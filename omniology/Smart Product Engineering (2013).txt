Lecture Notes in Production Engineering 
Smart Product
Engineering
Michael Abramovici
Rainer Stark
Editors
Proceedings of the 23rd CIRP Design Conference,
Bochum, Germany, March 11th–13th, 2013
German Academic Society for
Product Development WiGep
Berliner Kreis & WGMK

Lecture Notes in Production Engineering
For further volumes:
http://www.springer.com/series/10642

Michael Abramovici and Rainer Stark (Eds.)
Smart Product Engineering
Proceedings of the 23rd CIRP Design
Conference, Bochum, Germany,
March 11th–13th, 2013
ABC

Editors
Michael Abramovici
Ruhr-Universität Bochum
IT in Mechanical Engineering
Bochum
Germany
Rainer Stark
Technische Universität Berlin
Industrial Information Technology
Berlin
Germany
ISSN 2194-0525
ISSN 2194-0533
(electronic)
ISBN 978-3-642-30816-1
ISBN 978-3-642-30817-8
(eBook)
DOI 10.1007/978-3-642-30817-8
Springer Heidelberg New York Dordrecht London
Library of Congress Control Number: 2013932091
c⃝Springer-Verlag Berlin Heidelberg 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied speciﬁcally for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher’s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
Dramatic progress in the ﬁelds of embedded microdevices, mobile communica-
tion, software and computing power have not only changed our daily life but will
rapidly reshape industrial products, engineering processes and organizational
structures. “Industry 4.0”, “Cyber-Physical Systems”, “Ubiquitous Computing”
and “Internet of Things” are only a few examples of buzzwords trying to reﬂect
these revolutionary changes which will lead to a convergence of the real physical
world and the permanently growing digital world.
Smart Product Engineering – the topic of this conference – attempts to address
these tremendous changes of both industrial products and engineering processes.
In the context of product creation“smart”is not only a new fashionable word but
refers to the following meanings:“clever“, “intelligent“, “ingenious“ and “agile“.
Smart products are a new generation of products equipped with microsen-
sors, computing power and mobile communication capabilities i.e. smartphones.
However, not only consumer goods but also industrial products can become
“smart”if embedded intelligence is applied.Then they are able to react instantly
to environmental changes and to communicate with IT infrastructures or with
other products. Smart Product Engineering describes processes, methods
and tools for the creation of these smart products. Engineering processes could
also exploit the newest IT developments in order to reduce and better integrate
the increasing task complexity. In the last ﬁve years, a wave of research ini-
tiatives, start-up companies and marketing campaigns have addressed the new
“smart” topic.
These “smart” developments oﬀer huge opportunities to enhance task ef-
ﬁciency and to create more sustainable products. In orderto exploit these
opportunities it is necessary to provide new, highly interdisciplinary meth-
ods,organization concepts and IT tools. The papers included in this book give
an overview of the main research activities and the industrial practice towards
Smart Product Engineering.
The 23rd CIRP Design Conference continuesa long tradition ofprestigious
design conferences organized under the aegis of the International Academy for
Production Engineering (CIRP). The conference was jointly organized by Ruhr-

VI
Preface
Universit¨at Bochum (RUB) and byTechnischeUniversit¨at Berlin (TU Berlin). For
over 40 years, both organizing universities have been worldwide pioneers in the
development of product design methods and tools. The conference was organized
in cooperation with the German Academic Society for Product Development -
WiGeP.
Over 160 proposals were submitted for the conference. The international sci-
entiﬁc program committee selected 98 academic and industrial papers from over
20 countries for presentation during the conference and for publication in these
proceedings.
We would like to express our gratitude to all paper authors, keynote speakers,
session’s chairs and all participants for their contribution to the success of the
conference. Our grateful thanks also goto all supporting industrial partners who
made this conference possible. We also thank the conference organizing commit-
tee, especially the chief organizers Mr. Akamitl Quezada (RUB) and Mr. Maik
Auricht (TU Berlin). Finally, we thank the publisher as well as the typesetting
team for their support throughout the publication process.
We hope that the content of this book will oﬀer useful andvaluable input for
research, teaching and industry.
Bochum and Berlin, January 2013
Michael Abramovici (RUB)
Rainer Stark (TU Berlin)
Chairmen of the 23rd CIRP Design Conference Bochum 2013

Organization
Organization Committee
Chairmen
M. Abramovici
Ruhr-Universit¨at Bochum, Germany
R. Stark
Technische Universit¨at Berlin, Germany
Chief Organizers
A. Quezada
Ruhr-Universit¨at Bochum, Germany
M. Auricht
Technische Universit¨at Berlin, Germany
Organizing Assistants
H. Lagemann
Ruhr-Universit¨at Bochum, Germany
A. Haniecka
Ruhr-Universit¨at Bochum, Germany
A. Peters
Ruhr-Universit¨at Bochum, Germany
T. Dorka
Ruhr-Universit¨at Bochum, Germany
J. Thiele
Technische Universit¨at Berlin, Germany
International Scientiﬁc Committee
M. Abramovici
Ruhr-Universit¨at Bochum, Germany
R. Anderl
Technical University of Darmstadt, Germany
A. Bernard
Ecole Centrale de Nantes, France
H. Binz
University of Stuttgart, Germany
D. Brissaud
Grenoble Institute of Technology, France
C.A. Brown
Worchester Polytechnic Institute, USA
J.R. Duﬂou
University of Leuven, Belgium
H. ElMaraghy
University of Windsor, Canada
W. ElMaraghy
University of Windsor, Canada

VIII
Organization
J. Feldhusen
RWTH Aachen University, Germany
A. Fischer
Israel Institute of Technology, Israel
L. Galantucci
DIMEG, Politecnico di Bari, Italy
D. Gerhard
Technical University of Vienna, Austria
K.-H. Grote
University Magdeburg, Germany
P. Gu
University of Calgary, Canada
B. Hon
University of Liverpool, UK
I.S. Jawahir
University of Kentucky, USA
B. Kaftanoglu
Baskent University, Turkey
A. Katzenbach
University of Stuttgart, Germany
S. Kim
Massachusetts Institute of Technology, USA
F. Kimura
The University of Tokyo, Japan
T. Kjellberg
KTH Royal Institute of Technology, Sweden
D. Krause
Technical University Hamburg-Harburg,
Germany
F.-L. Krause
Technische Universit¨at Berlin, Germany
S. Kumara
Pennsylvania State University, USA
B. Leuwers
University of Leuven, Belgium
E. Lutters
University of Twente, Netherlands
S. Lu
University of Southern California, USA
P. Maropoulos
University of Bath, UK
L. Mathieu
Universite Paris Sud, France
N. Masayuki
The University of Tokyo, Japan
L. Monostori
Hungarian Academy of Science, Hungary
D. Mourtzis
University of Patras, Greece
A. Nee
National University of Singapore, Singapore
J. Ovcharova
Karlsruhe Institute of Technology, Germany
R. Roy
Cranﬁeld University, UK
K. Sch¨utzer
UNIMEP, Brasil
G. Seliger
Technische Universit¨at Berlin, Germany
M. Shpitalni
Israel Institute of Technology, Israel
R. Stark
Technische Universit¨at Berlin, Germany
R. Stelzer
Technical University of Dresden, Germany
N.P. Suh
Korea Advanced Institute of Science and
Technology, South Korea
S. Tichkiewitch
Grenoble Institute of Technology, France
T. Tolio
Technical University of Milan, Italy
T. Tomiyama
Delft University of Technology, Netherlands
D. Trippner
BMW AG, Germany
M. Tseng
Hong Kong University of Science and
Technology, Hong Kong
F. van Houten
University of Twente, Netherlands
J. Vancza
Hungarian Academy of Science, Hungary
S. Warzack
University Erlangen-N¨urnberg, Germany
R. Wilhelm
UNC Charlotte, USA

Organization
IX
Industrial Partners of the 23rd CIRP Design
Conference
Platinum: 
 
 
 
 
Gold: 
 
 
Silver: 
 
 

X
Organization
Supported by:
 

Contents
Section: Management Approaches I
Smart Engineering for Smart Products . . . . . . . . . . . . . . . . . . . . . . . . .
1
Reiner Anderl, Andr´e Picard, Katharina Albrecht
New Perspectives in the Quest for Uniﬁcation of ‘Lean’ with
Traditional Engineering Design Methodology . . . . . . . . . . . . . . . . . . .
11
S¨oren Ulonska, Torgeir Welo
Procedural Model for the Virtual Commissioning on the Basis
of Model-Based Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
Tanja Schm¨udderrich, Ansgar Tr¨achtler, Jan Br¨okelmann,
J¨urgen Gausemeier
A Template for Design for eXcellence (DfX) Methods . . . . . . . . . .
33
Juan Manuel Jauregui Becker, Wessel W. Wits
Section: PLM Applications
Enhancing Interpretation-Quality of Requirements Using
PLM Integrated Requirements-Communication in Cross
Company Development Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
Martin Rebel, J¨org W. Fischer, Armin Haße, Cees Michielsen
PLM-Centered Support of Active Virtual Customer
Integration into the Product Creation Process . . . . . . . . . . . . . . . . . .
51
Thomas Damerau, Haygazun Hayka, Rainer Stark
High Deﬁnition Product Lifecycle Management an Immersive
Decision Making Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
Carsten Burchardt
Implications of Open Innovation Approaches on Future PLM . . .
71
Andrea Denger, Detlef Gerhard, Christian Kaiser

XII
Contents
Section: Systems Engineering
Challenges of Model-Based Systems Engineering: A Study
towards Uniﬁed Term Understanding and the State of Usage
of SysML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
Albert Albers, Christian Zingel
Model Based Design with Systems Engineering Based on
RFLP Using V6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Sven Kleiner, Christoph Kramer
Consolidating Product and Process Information of
Connections – A System-Theoretical Approach . . . . . . . . . . . . . . . . .
103
Fabian Rusitschka, Efstratia Zafeiriou, Hansgeorg Binz, Daniel Roth
A Structured Approach for Function Based Decomposition of
Complex Multi-disciplinary Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Felician Campean, Edwin Henshall, Unal Yildirim, Amad Uddin,
Huw Williams
Section: Engineering of Manufacturing Systems
Methodology for Identiﬁcation of Adaptive Reusable Modules
in Automated Production Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
Konstantin Kernschmidt, Philipp Klein, Nasser Jazdi, Peter G¨ohner,
Michael Weyrich, Birgit Vogel-Heuser
Reverse Engineering for Manufacturing Approach: Based on
the Combination of 3D and Knowledge Information . . . . . . . . . . . .
137
Salam Ali, Alexandre Durupt, Pierre Antoine Adragna
A Kinematic Approach for 6-DOF Part Positioning . . . . . . . . . . . . .
147
Sajid Ullah Butt, Jean-Francois Antoine, Patrick Martin
Control Architecture for Plug-and-Play Intelligent Axes
within Fast Reconﬁgurable Manufacturing Equipments . . . . . . . . .
159
Mircea Murar, Stelian Brad
Section: Process Management
Product Development Process Modeling: State of the Art and
Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
Carolina Rom´an Amigo, Diego Rodrigues Iritani, Henrique Rozenfeld,
Aldo Ometto
Activity-Based Modeling and Analysis of Product Engineering
Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
Andreas Braun, Bj¨orn Ebel, Albert Albers

Contents
XIII
An Enhanced Interface Analysis Method for Engineering
Change Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
Unal Yildirim, Felician Campean
Optimizing Overlap between Testing and Design in
Engineering Product Development Processes . . . . . . . . . . . . . . . . . . .
201
Khadija Tahera, Claudia M. Eckert, Chris F. Earl
Section: PLM Architectures
Do(PLM)Con: An Instrument for Systematic Design of
Integrated PLM-Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
J¨org W. Fischer, Bernhard Lammel, Dirk Hosenfeld,
Deodatt Bawachkar, Bernd Brinkmeier
Meta-data-Model for the Development of Adaptronic
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
Roland Nattermann, Reiner Anderl
Web Services to Product, Processes and Resources Data
Integration: Results and Perspectives of FedMan Project . . . . . . .
231
Klaus Sch¨utzer, Antonio ´Alvaro de Assis Moura, Reiner Anderl,
Andr´e Picard
Product Lifecycle Management Functional Reference Model
for Software Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
Eduardo Zancul, Luiz Piccini, Sophia Berglehner, Lena Lachenmaier
Section: Mechatronic Design
Design of Reconﬁgurable Automotive Framing System . . . . . . . . . .
253
Abdo Al-Zaher, Waguih ElMaraghy, Z.J. Pasek, Hoda ElMaraghy
Mechatronic Machine Elements: On Their Relevance in
Cyber-Physical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
Marius St¨ucheli, Mirko Meboldt
Approach for an Early Validation of Mechatronic Systems
Using Idealized Simulation Models within the Conceptual
Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
Frank Bauer, J¨urgen Gausemeier, Daniel K¨ochling, Felix Oesters¨otebier
Design for Testability for Micro-mechatronic Systems. . . . . . . . . . .
283
Gisela Lanza, Thomas Blank, Benjamin Haefner
Section: Assembly Simulation
A Reference Framework for Manual Assembly Simulation . . . . . .
293
N´estor Andr´es Arteaga Mart´ın, Thomas B¨ar, Rainer Stark

XIV
Contents
Product Assembly Information to Improve Virtual Product
Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303
Martin Eigner, Joscha Ernst, Daniil Roubanov, Jochen Deuse,
Julian Schallow, Olga Erohin
Assisted Decision-Making for Assembly Technique Selection
and Geometrical Tolerance Allocation . . . . . . . . . . . . . . . . . . . . . . . . . .
315
Lo¨ıc Andolfatto, Fran¸cois Thi´ebaut, Claire Lartigue, Marc Douilly
Automated DHM Modeling for Integrated Alpha-Numeric
and Geometric Assembly Planning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
Martin Manns, N´estor Andr´es Arteaga Mart´ın
Section: Customer and Feedback Integration
3DExperiences – Dassault Syst`emes Strategy to Support
New Processes in Product Development and Early Customer
Involvement: A Software Tool Editor’s View to Challenge the
Smart Product Engineering Revolution . . . . . . . . . . . . . . . . . . . . . . . . .
335
Marc Frouin
Data Fusion and 3D Geometric Modeling from Multi-scale
Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
Dmitry Tansky, Anath Fischer
Exploiting Service Data of Similar Product Items for the
Development of Improved Product Generations by Using
Smart Input Devices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357
Michael Abramovici, Andreas Krebs, Andreas Lindner
Design of a Clip Product Based on Customer Needs for
Playing Acoustic Music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
367
Julien Veytizou, Hugo Xuereb, Guillaume Thomann
Section: Management Approaches II
Technology Roadmapping Based on Key Performance
Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
Felix Spangenberg, Dietmar G¨ohlich
Technical Risk Management for an Ensured and Eﬃcient
Product Development on the Example of Medical
Engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
387
Thomas Zentis, Robert Schmitt
A Simultaneous Engineering Approach for Free Form
Bifurcated Sheet Metal Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
Anselm Sch¨ule, Reiner Anderl

Contents
XV
Complexity Management in Product/Process Simultaneous
Design for Implementing a Fresnel Thermodynamic Solar
Plant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
411
Roozbeh Babaeizadeh Malmiry, Nicolas Perry
Section: Embodiment Design
Embodiment Discrete Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
421
Sebastian Pena Serna, Andre Stork, Dieter W. Fellner
A Virtual Prototyping Approach Based on DOE Analysis to
Support the Design of a Centrifugal Impeller . . . . . . . . . . . . . . . . . . .
431
Paolo Cicconi, Daniele Landi, Michele Germani
Towards a Context-Driven Front End in New Product
Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
441
Christer W. Elverum, Torgeir Welo
Multistate Feature Modelling of a Very Complex Design
Feature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
451
Done Ugarte, Alberto Izaguirre
Section: Manufacturing Modeling and Simulation
Virtual Validation of the Manual Assembly of a Power
Electronic Unit via Motion Capturing Connected with a
Simulation Tool Using a Human Model . . . . . . . . . . . . . . . . . . . . . . . . .
463
Jochen B¨onig, Christian Fischer, Matthias Brossog, Martin Bittner,
Markus Fuchs, Holger Weckend, J¨org Franke
Simulation of Variation in Assembly Forces Due to Variation
in Spot Weld Position . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
473
Kristina W¨armefjord, Rikard S¨oderberg, Lars Lindkvist
Cutting Tool Data Representation and Implementation Based
on STEP AP242 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
483
Yujiang Li, Mikael Hedlind, Torsten Kjellberg, Gunilla Sivard
Assessment of Sensitivity of Numerical Simulation in Sheet
Metal Forming Process Applied for Robust Design . . . . . . . . . . . . .
493
Von Dim Nguyen, Pierre-Antoine Adragna, Pascal Lafon
Section: User Centered Design
Analysis of Automatic Online Lead User Identiﬁcation. . . . . . . . . .
505
Sanjin Pajo, Paul-Armand Verhaegen, Dennis Vandevenne,
Joost R. Duﬂou

XVI
Contents
Improving Result Quality in Engineering Design by Better
Linking Employee’s and Task’s Features . . . . . . . . . . . . . . . . . . . . . . . .
515
Malte Hinsch, Jan Erik Heller, Raymond Djaloeis,
Christopher M. Schlick, J¨org Feldhusen
Internal Innovation Communities from a User’s Perspective:
How to Foster Motivation for Participation . . . . . . . . . . . . . . . . . . . . .
525
Albert Albers, Ludwig Maul, Nikola Bursac
Section: Product Architecture Design
Determining Granularity Level in Product Design
Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
535
Tarek AlGeddawy, Hoda ElMaraghy
Developing Modular Product Families with Perspectives for
the Product Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
543
Dieter Krause, Sandra Eilmus, Henry Jonas
A Modular Dynamic Products Platforms Design Model . . . . . . . .
553
Mohmmad Hanafy, Hoda ElMaraghy
Section: Early Design Stages
Design Automation with the Characteristics Properties Model
and Property Driven Design for Redesign . . . . . . . . . . . . . . . . . . . . . .
563
A.J. Qureshi, Boris Eisenbart, Jean-Yves Dantan, Lucienne Blessing
The Implications of the Skin Model Concept for Computer
Aided Tolerancing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
573
Benjamin Schleich, Sandro Wartzack
Can a Pre-sketching Activity Improve Idea Generation? . . . . . . . .
583
Emily Worinkeng, Joshua D. Summers, Shraddha Joshi
Section: Design of Manufacturing Systems
Design Approach for an Adaptable Highly Integrated
Hydraulic Feed Axis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
593
J¨org Bauer, J¨urgen Fleischer
Automated Conﬁguration of a Machine Simulation Based on
a Modular Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
603
Michael Weyrich, Frank Steden
System Design of PLC-Controlled Specialized Production
Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
613
Gernot Frank, Engelbert Westk¨amper, Wolfgang Schl¨ogl,
Matthias Lenord

Contents
XVII
Section: Human Centered Design
A New Method for Human Reliability Analysis in New
Product Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
623
Raymond Djaloeis, S¨onke Duckwitz, Malte Hinsch, J¨org Feldhusen,
Christopher M. Schlick
Design for Usability by Ubiquitous Product Documentation . . . .
633
Michael Abramovici, Andreas Krebs, Thomas Schindler
Simulation in Human-Centered Design – Past, Present and
Tomorrow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
643
J¨org Miehling, Daniel Kr¨uger, Sandro Wartzack
Section: Digital Collaboration
Distance Collaboration Support Environment . . . . . . . . . . . . . . . . . . .
653
Roy Damgrave, Eric Lutters
Technology Framework for Product Design . . . . . . . . . . . . . . . . . . . . .
663
Michael A. Bitzer, Michael Vielhaber
Digital Representations of Intelligent Products: Product
Avatar 2.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
675
Thorsten Wuest, Karl Hribernik, Klaus-Dieter Thoben
Section: Design Optimization
Product Evolution and Optimization Based on Gentelligent
Components and Product Life Cycle Data . . . . . . . . . . . . . . . . . . . . . .
685
Roland Lachmayer, Iryna Mozgova, Bastian Sauthoﬀ, Philipp Gottwald
Early Development of Weight-Optimized Mechatronic
Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
695
Tobias Luedeke, Michael Vielhaber
Statistical Tolerance-Cost-Optimization of Systems in Motion
Taking into Account Diﬀerent Kinds of Deviations . . . . . . . . . . . . . .
705
Michael Walter, Sandro Wartzack
Section: Design/LC Assessment
Forecasting Environmental Proﬁles in the Early Stages of
Product Development by Using an Ontological Approach . . . . . . .
715
Hesam Ostad-Ahmad-Ghorabi, Touba Rahmani, Detlef Gerhard
Leveraging Product Development for a Sustainable Future:
Energy and Resource Eﬃciency in Lifecycle Analysis . . . . . . . . . . .
725
Martin Eigner, Patrick D. Sch¨afer, Hristo Apostolov

XVIII
Contents
Exploring Opportunities to Improve Life Cycle Environmental
Performance of a Complex Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
735
Minjung Kwak, Harrison Kim
Section: Product Assessment
Cost-Eﬀects of Product Modularity – An Approach to
Describe Manufacturing Costs as a Function of Modularity . . . . .
745
Thomas Hohnen, Judith Pollmanns, J¨org Feldhusen
Proposal of a Research Methodology to Increase the
Robustness of the Conjoint Trends Analysis Method through
Its Formalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
755
Angela Cadavid, Jorge Maya
Analyzing the Deviation of Product Value Judgment . . . . . . . . . . .
765
Payam Amini, Robert Schmitt
Complexity Connectivity Metrics – Predicting Assembly
Times with Low Fidelity Assembly CAD Models . . . . . . . . . . . . . . .
777
Essam Z. Namouz, Joshua D. Summers
Section: VR Environments
The Virtual Reality Lab as a Synthetic Environment: From
Strategic Approach to Practical Implement . . . . . . . . . . . . . . . . . . . . .
787
Roy Damgrave, Eric Lutters, Fred J.A.M. van Houten
A Tool Proposition to Support Multidisciplinary Convergence
in Immersive Virtual Environment: Virtusketches . . . . . . . . . . . . . .
795
Ahmad Al Khatib, Morad Mahdjoub, Jean-Bernard Bluntzer,
Jean-Claude Sagot
A Visual Language for the Collaborative Visualization of
Integrated Conceptual Models in Product Development
Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
805
Johannes Herter, Ross Brown, Jivka Ovtcharova
Section: Knowledge Engineering
The Right Knowledge Management Strategy for Engineering
Analysis SME: A Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
815
Christophe Chevalier, Franck Pourroy, Fran¸cois Villeneuve,
Alex Du Pasquier
An Ontological Approach to Integrated Product and Process
Knowledge Modeling for Intelligent Design Repositories . . . . . . . .
825
Farhad Ameri, Stephen Allen

Contents
XIX
Know-How Identiﬁcation, Scoring, and Classifying in Product
Development Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
835
Mathias Lojewski, Christoph B¨armann, Frank Mantwill
Section: Eco/Sustainable Design
How the Integration of Environmental Concerns Modiﬁes the
Integrated Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
845
Maud Dufrene, Peggy Zwolinski, Daniel Brissaud
Towards Integrating Sustainability in the Development of
Product/Packaging Combinations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
855
Jos de Lange, Ellen Oude Luttikhuis, Roland ten Klooster, Eric Lutters
Tolerance Speciﬁcation Optimization for Economic and
Ecological Sustainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
865
Steven Hoﬀenson, Andreas Dagman, Rikard S¨oderberg
Material Selection for Eco-design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
875
Julie Rockizki, Zwolinski Peggy
Section: Design Process Assessment
An Information Model of the Design Process for the
Estimation of Product Development Eﬀort . . . . . . . . . . . . . . . . . . . . .
885
Judith Pollmanns, Thomas Hohnen, J¨org Feldhusen
Assessing the Relationship between New Product
Development Practices and Performance in the Norwegian
Manufacturing Industry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
895
Torgeir Welo, Silje H. Aschehoug, Geir Ringen
An Indicator-Based Process Monitoring Cockpit for
Controlling and Enhancing Product Development Processes –
An Industrial Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
905
Thomas Luft, Simon Smoll, Sandro Wartzack
Section: VR/AR Environments
A Method to Design a Smart Home Interface . . . . . . . . . . . . . . . . . . .
915
Silvia Ceccacci, Michele Germani, Maura Mengoni
Virtual Reality Coupled with Adapted Physical Interface for
a Better Evaluation of the Innovative Surgical Instrument . . . . . .
927
Duy Minh Phan Nguyen, J´erˆome Tonetti, Guillaume Thomann
Application of AR Technologies to Sheet Metal Forming in
Shipbuilding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
937
Kohei Matsuo, Uwe Rothenburg, Rainer Stark

XX
Contents
Section: Knowledge Modeling / Engineering
Are Smart Products Foiling Automated Design? . . . . . . . . . . . . . . . .
947
Patrick Klein, Johannes L¨utzenberger, Klaus-Dieter Thoben
Management of Cost Knowledge in Product Design –
Integration of Upstream and Downstream Life Cycle Phases . . . .
957
Susann K¨ohler, Annett Bierer, Uwe G¨otze
Functional Shape Elements Integrating Design and
Manufacturing Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
969
Volkmar Wenzel, Alexander Christ, Daniel Strang, Jan Tim Jagenberg,
Reiner Anderl, Thomas Bornkessel
Section: Eco Design Assistance
Decision-Making Support for Sustainable Product
Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
979
Kai Lindow, Oliver Heimann, Sebastian Adolphy, Haygazun Hayka,
Rainer Stark
Industry Requirements for an Assistant System Supporting
Energy-Eﬃcient Product Development in the Automotive
Supply Industry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
989
Michael Abramovici, Akamitl Quezada, Thomas Schindler
Early System Simulation to Support EcoDesign of Vehicle
Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
999
Fabio Dohr, Pascal Stoﬀels, Michael Vielhaber
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1009

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 1–10. 
DOI: 10.1007/978-3-642-30817-8_1 
© Springer-Verlag Berlin Heidelberg 2013 
 
Smart Engineering for Smart Products 
Reiner Anderl, André Picard, and Katharina Albrecht 
Technische Universität Darmstadt, Department of Computer Integrated Design,  
Petersenstraße 30, Darmstadt, Hessen, Germany 
{anderl,picard,albrecht}@dik.tu-darmstadt.de 
Abstract. Smart Engineering aims at a new approach for describing, designing 
and dimensioning Smart Products. Design methodology is far advanced and 
provides both a systematic approach to develop new products as well as appro-
priate methods to support development tasks in specific development phases. 
Within this development process the communication capabilities of Smart 
Products, the structure of communication among Smart Products as well as ex-
ecuting functional operations triggered by communicated messages is not de-
scribed yet. 
This new approach for the description of Smart Products introduces prod-
ucts’ states specification to derive the description of functional behavior as well 
as the execution of working procedures. In this contribution a basic systematic 
analysis of both sensors for communication and internet based communication 
protocols is presented to enable appropriate products’ states. Based on this 
analysis a framework approach for processing events will be presented. The 
Smart Engineering approach will be demonstrated finally within an application 
scenario. 
Keywords: Smart Engineering, Smart Products, Cross-Product Communication. 
1 
Introduction 
Smart Engineering comprises a new procedural paradigm for describing, designing 
and dimensioning Smart Products. The new innovative approach of Smart Products 
results from the concept of Cyber-Physical Systems which requires products being 
equipped with embedded systems, sensors and actuators as well as the ability to 
communicate with other Smart Products. These new features challenge engineering as 
product engineering and development has to take into account consistent and reliable 
product behavior as well as product states in a products’ application context using a 
Cyber-Physical Systems environment. 
The concept of Cyber-Physical Systems is considered as a fundamental approach 
to enable the development of Smart Products. The main features of Cyber-Physical 
Systems are sensors, actuators and embedded intelligence as known from mechatronic 
and adaptronic systems. The basic difference to existing approaches is the intersection 
of theories of computation and dynamical systems theories [1]. Lee describes two 
complementary approaches called “cyberizing the physical” for specifying physical 
subsystems with computational abstractions and interfaces and “physicalizing the 

2 
R. Anderl, A. Picard, and K. Albrecht 
 
cyber” for expressing abstractions and interfaces of software and network components 
to represent physical systems’ dynamics in time [1]. Furthermore Cyber-Physical 
Systems are considered as a breakthrough technology to develop the “Internet of 
Things” [2, 3]. A major approach for developing Cyber-Physical Systems is the de-
sign of a logical level, where both, the functionality of the physical systems and the 
system states, are to be specified. Brooks et. al. have studied model engineering using 
multimodeling techniques showing benefits of the augmented state chart model [4]. 
Own experiments using a LEGO Mindstorms environment confirm the increasing 
importance of specifying the logical state occurrence and the logical level of Cyber-
Physical Systems [5]. 
While Cyber-Physical Systems are considered as a fundamental basis of Smart 
Products, their features are even more advanced due to their embedded intelligence. 
Smart Products are aiming at embedded intelligence enabling products reacting auto-
nomously due to their communication with other Smart Products in a predefined  
environment e. g. a production environment. Due to the high industrial potential the 
German Ministry of Education and Research has identified Smart Products and Smart 
Factories as a key element of their initiative called “Industry 4.0” [6, 7]. 
To enable the development of Smart Products an advanced approach for engineer-
ing is required. This approach is called Smart Engineering and its target is to describe, 
to design and to engineer Smart Products. The importance of Smart Engineering has 
been discussed and clearly identified by acatech [8]. The main requirements comprise 
advanced methods for multidisciplinary product development, awareness for human 
acceptance and advanced engineering education [8]. Furthermore a major task in 
Smart Engineering is the precise and quantified definition of the products’ states, 
functionality and behavior. For Smart Engineering a profound knowledge of sensor 
technologies, actuator technologies, control logics and communication protocols is 
required. Therefore, a basic analysis of sensor and actuator technologies as well as an 
analysis of communication protocols is provided. 
Within this paper a new approach for describing, designing and dimensioning 
Smart Products is presented. Smart Engineering is based on the products’ states speci-
fication to derive the description of the functional behavior and the execution of 
working procedures due to the received messages. Received messages are sent after 
being triggered by both, internal and external events. In this paper externally triggered 
events using internet based communication protocols are of major interest. Further-
more the integration of this approach into the product development process is  
described. 
2 
Smart Engineering 
Virtual Product Development is far advanced. Appropriate product development and 
design methodologies have been developed and extended continuously. Pahl and 
Beitz [10], VDI guideline 2221 [11] and the so-called V-model (VDI guideline 2206) 
[12] are considered as state of the art. New approaches are aiming to integrate the 
challenging innovation of Smart Products into a product development and design 

 
Smart Engineering for Smart Products 
3 
 
methodology as proposed by Nattermann and Anderl [13]. The latter is a consequent 
extension of the V-model to the so-called W-model. Major progress is the integration 
of systems engineering approaches into the design methodology enabling cross-
discipline product development and design. 
Smart Engineering even extends systems engineering by enlarging system borders 
to the environment where Smart Products communicate and operate. Communication 
between Smart Products and their situative operation is a fundamentally new feature 
and a challenge for engineering. To meet this challenge a profound understanding of 
products’ states is necessary which requires appropriate sensors, actuators and com-
munication protocols. Furthermore it is strongly required to understand the Smart 
Products’ states as states dedicated to the single products as well as to corresponding 
states of multiple Smart Products in an application environment. 
2.1 
Smart Products 
Smart Products are mechatronic products equipped additionally with embedded sys-
tems enabling communication with other Smart Products using in particular existing 
internet technologies. An innovative approach is the application of Cyber-Physical 
systems in mechatronic products. Smart Products are enabled to know about their 
operational states, to monitor and to control their physical processes. They obtain 
awareness about their environment and their operational states, they interact with their 
environment by sending messages and triggering events, they are designed to draw 
decisions due to situation analysis and they are enabled to act autonomously.  
Classification of Smart Sensors. Smart Products are created by the application of 
Cyber-Physical systems to mechatronic products. They add communication capabili-
ties to Cyber-Physical mechatronic components. The communicating devices of such 
Smart Products consist of sensor systems, actuator systems and modules with embed-
ded control software for data processing. 
Such actuator systems consist of an active element and a sensor element which 
measures the current states of the active element. In the traditional understanding 
both, sensor and actuator systems are typically able to communicate internally con-
trolled by their embedded systems.  
Fraden, Isermann, Nordmann and White therefore developed classifications, 
grouping e.g. sensors, mainly regarding the measurement category or the power 
source [16, 17, 18, 19]. Other classification exists like Isermann who classified sen-
sors as thermal, mechanical, electrical, chemical or physical sensors [17]. 
The existing classifications of Fraden, Isermann Nordmann and White are not  
sufficiently specified for the description of sensors for Smart Products. A new classi-
fication is needed which considers different criteria. To explain these criteria it is 
necessary to clarify the overall definition of sensors and their impact. Sensors will not 
only be used within the entire Smart Products, but for the communication between 
Smart Products. Extending the traditional understanding of Smart Sensors including 
only communication using message exchange via Bus systems [20], within this paper 
the understanding of Smart Sensors explicitly includes the communication via  

4 
R. Anderl, A. Picard, and K. Albrecht 
 
potential bus systems and internet connections. The major progress is that future 
Smart Sensors will be identifiable through an IP address (internet protocol address). 
Consequently communication between Smart Sensors is performed based on digitally 
represented messages. Mechanisms for avoiding measurement uncertainties and ob-
servational errors are provided by the sensors themselves. The mechanisms are not 
taken into account within the classification, but must be considered in the require-
ments of the product development process. 
The main difference between Smart Sensors and Smart Products is the higher com-
plexity resulting from the integration of embedded control software in Smart Prod-
ucts. Thus Smart Sensors can only react on predefined states and Smart Products are 
able to handle complex operations. Smart Sensors as well as Smart Products interact 
within their communication network. Both are treated to be smart, located however in 
different categories shown in the metric in the following chapter 2.2. In order to re-
flect the new understanding the traditional sensor classification of Nordmann, Fraden 
and Isermann is used and extended additionally. 
Communication between Smart Products. Smart Products are enhanced by com-
munication capabilities. In comparison to mechatronic products communication is no 
longer restricted between components within one product. Smart Products instead are 
able to additionally exchange information with other Smart Products and trigger 
events on each other in order to transform their corresponding internal states. Thus 
they send messages including specific information. Information and events both can 
contain sensors states, but in comparison events additionally envelop a request for a 
specific action [14]. 
To communicate with other Smart Products, a Smart Product uses its embedded 
control software as an active sender. Thus every time the Smart Products reaches a 
phase to act and to communicate, it is able to send data which another Smart Product 
is able to receive. There are three possibilities of communication between Smart 
Products: synchronous - the system reacts on the other system immediately, asyn-
chronous - the system takes time to react on the other system and quasi-synchronous. 
Cyber-Physical systems aim to communicate synchronously, but in reality communi-
cation is mostly quasi-synchronous using online connections based on internet proto-
cols. A physical and a logical connection as well as standardized communication  
protocols for a fast communication of Smart Products are requested. 
Smart Products’ communication is differentiated into different grades of mobility: 
wired, wireless or a mixed form of both. Wired communication uses physical wired 
connections like the modular connector RJ-45 or USB cables. They are physically 
connected; sometimes using multiple, additional hops like repeater, bridges and hubs. 
Using wireless communication instead Smart Products are not physical connected. 
They are linked using wireless telecommunication technologies like radio frequency 
communication, microwave communication or infrared communication. Due to alter-
able infrastructure for communication, a mixed form of wired and wireless communi-
cation exists. It uses both types, but only one form exclusively at one time. 

 
Smart Engineering for Smart Products 
5 
 
Connecting and arranging multiple Smart Product results in different communica-
tion networks. Within these networks various protocols for communication are used. 
The analysis of commonly used internet protocols [21] like FTP, OSCAR or XMPP, 
shows that the selection of the protocol is not restricted. Theoretically every open or 
propriety format can be chosen allowing quasi-synchronous or synchronous  
communication. 
For engineering Smart Products the understanding of joint dynamics of software, 
networks and physical processes is required. A classification of Smart Products’ 
communication within an appropriate metric for the different combinations of Smart 
Products’ communication is discussed below. 
2.2 
Methods for the Development of Smart Products 
For describing, designing and dimensioning Smart Products, a framework for 
processing the communication addressing these communication combinations is 
needed. Although design methodology for mechatronic products is far advanced a 
framework for developing Smart Products does not exist. Approaches like the V-
Model [12] do not provide systematic approaches to develop Smart Products’ com-
munication. The framework presented in this paper introduces a systematic approach 
based on the W-Model to treat triggered events and received messages. Capabilities to 
fully integrate this framework are given in order to support development tasks in the 
specific development phase. 
Metric of Smart Products’ Communication. Concerning the given classification of 
Smart Products’ interaction states four combinations of different communication  
configurations can occur: passive-passive, active-passive/passive-active and active-
active. During the functional analysis of Smart Products within the product develop-
ment process a combination of Smart Products can be ranked using this metric (see 
Fig. 1):  
─ Passive-passive: Case 1 represents two Smart Products, sender and receiver. Both 
can only send their states and do not trigger events. 
─ Passive-active/active-passive: In case 2 and 3 one of the Smart Products, either 
sender or receiver, acts active and is sending its states and events. The other Smart 
Products may send its states and will process the received events with predefined 
actions. 
─ Active-active: Case 4 describes the active interaction of two Smart Products. There 
exists a toggle of sending and receiving states and events. The processing of events 
may be the execution of predefined operations or the receiver is able to ask actively 
for states of the Sender. The states and events sending and processing can be ex-
ecuted in parallel.  
Interaction States of Smart Products. Smart Products’ interaction is based on the 
exchange of messages. Triggering events and processing information transform the 
internal states and impact the relation between Smart Products. In order to understand 
and describe the Smart Products’ communication, this approach defines seven differ-
ent Smart Products´ interaction states (see Fig. 2): 

6 
R. Anderl, A. Picard, and K. Albrecht 
 
 
Fig. 1. Metric for Classification of Communication of Smart Products 
─ Pending: Initially Smart Products are in pending states. No knowledge and hence, 
no connection between any Smart Products exists. 
─ Paired: A pairing operation is performed by coupling two devices together. No 
exchange of information or events has been executed yet. 
─ Exchanging: During the process of sending or receiving Smart Products start ex-
changing messages. 
In case of processing events: 
─ Processing: After exchanging messages, information is processed within the Smart 
Product. Any form of internal processing or external interaction like new events or 
message exchanges can be initiated. 
─ (Paired): The sender of messages falls back to the paired states described above. 
 
Fig. 2. Interaction States of Smart Products 
 

 
Smart Engineering for Smart Products 
7 
 
In case of triggering events: 
─ Waiting: The events’ sender stays on hold waiting for a response. Control mechan-
isms in case of failure e. g. after a timeout can be executed. 
─ Executing: The receiving Smart Product proceeds to the executing state. 
Processing received messages and received requests, the Smart Product initiates an 
operation or a response message. The Smart Product has to respond based on the 
received request. 
─ Blocking: If needed, the Smart Products’ interaction state is transformed to 
“blocked”.  
 
 
 
 
(a) 
 
 
 
(b) 
Fig. 3. Scheme for processing simple logics: (a) processing events and (b) messages 
Exchanging messages and triggering events during Smart Products’ communication 
involves not only the transformation of the internal state, but also the transformation 
of the above presented interaction states. A Smart Product passes through its states. 
Therefore, transformation logics to transform between appropriate states have to be 
provided. In Fig. 3 two examples for processing information and triggering an event 
are given. Complex interactions can be described by combining these logic examples. 
Further extension of the logic examples is possible. 
Integration into the Functional Design Phase. To support the development process 
of Smart Products the approach described is used as a first step in the functional  
design phase. 

8 
R. Anderl, A. Picard, and K. Albrecht 
 
Based on predefined requirements in earlier phases of product development the 
communication is categorized using the introduced classification and then is further 
detailed by the application of the given metric. Sending of information, triggering of 
events and controlling of executed procedures is then modeled resulting in different 
interaction states. Using these product states’ and the traditional understanding, the 
required sensors for each Smart Product can be chosen. The approach therefore fits 
into the W-Model [13]. 
3 
A Smart Engineering Scenario with LEGO Mindstorms 
Based on the presented framework a representative scenario built with LEGO 
Mindstorms has been developed. The scenario consists of two Smart Products: a robot 
and a bottle. The bottle will be opened by the robot. The bottle is equipped with a 
display showing the states of the bottle top. All scenarios illustrated in the metric 
description shown in Fig. 1 are covered by this application. 
To clarify the application of the given framework an active/passive combination of 
Smart Products is described further. The bottle actively searches for a robot to open 
its bottle top. It can trigger two events: gathering information about the robot’s cur-
rent states e.g. if it is in blocked state and triggering the opening process. The robot is 
only able to react on two simple predefined tasks: send a message enveloping its in-
formation and open a bottle at a specific requested location. It is able to navigate  
independently, but self-controlled by detecting objects and calculating an appropriate 
path to a destination. 
 
Fig. 4. LEGO Mindstorms robots as Smart Product 
Performing the functional design description this state based approach helps to cla-
rify the collaboration between the two Smart Products, also classifying it as an active-
passive combination. Thus, based on the given metric this Smart Engineering task is a 
combination of a Smart Product which is able to processes complex events and a 
Smart Product which is only able to process elementary events. Elementary events 
correspond directly to the framework approach introduced in chapter 2.2. 

 
Smart Engineering for Smart Products 
9 
 
4 
Conclusion 
The development of mechatronic products is far advanced, but does not yet provide 
methods to engineer and develop Smart Products. The Smart Engineering approach 
introduced in this paper introduces a method to describe, to design and to dimension 
Smart Products based on a specification of products’ states embedded in the function-
al phase of the product development process. 
Based on a classification of Smart Products’ communication new products are de-
veloped using this metric. By the application of this metric development of Smart 
Products and their communication can be identified in order to describe and to design 
the exchange of messages between Smart Products, including information and trigger-
ing events. 
The described approach of this paper is currently further evaluated. The improve-
ment of the used methods and the technical support tools are subject of this  
evaluation.  
References 
1. Lee, A.-E.: CPS Foundations. In: Design Automation Conference. ACM (2010)  
2. Broy, M. (Hrsg.): Cyber-Physical Systems. Innovation durch Software-intensive eingebet-
tete Systeme. acatech DISKUTIERT. Springer, Heidelberg (2010)  
3. Acatech (Hrsg.): Cyber-Physical Systems. Innovationsmotor für Mobilität, Gesundheit, 
Energie und Produktion. acatech POSITION. Springer, Heidelberg (2011)  
4. Brooks, C., Cheng, C., Feng, T.H., Lee, E.A., Hanxleden, R., von Hanxleden, R.: Model 
Engineering using Multimodeling. In: 1st International Workshop on Model Co-Evolution 
and Consistency Management, MCCM (2008) 
5. Anderl, R.: Smart Product Engineering. 17° Seminário Internatcional de Alta Tecnologia, 
Santa Bárbara d’Oeste, Brazil (2012)  
6. N. N.: Industry 4.0 (2012),  
http://www.hightech-strategie.de/de/2676.php  
7. Kagermann, H., Wahlster, W., Held, J. (Hrsg.): Bericht der Promotorengruppe Kommuni-
kation. Im Fokus: Das Zukunftsprojekt Industrie 4.0. Handlungsempfehlungen zur Umset-
zung, Forschungsunion (2012)  
8. Anderl, R., Eigner, M., Sendler, U., Stark, R. (Hrsg.): Smart Engineering. Interdisziplinäre 
Produktentstehung. acatech DISKUTIERT. Springer, Heidelberg (2012)  
9. Warnecke, H.-J., Hüser, M.: Die Fraktale Fabrik. Revolution der Unternehmenskultur. 
Rowohlt (1996) 
10. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.: Konstruktionslehre. Grundlagen erfolgreicher 
Produktentwicklung. Methoden und Anwendung. Springer, Heidelberg (2006) 
11. Verein Deutscher Ingenieure: Systematic approach to the development and design of tech-
nical systems and products. VDI 2221 (1993) 
12. Verein Deutscher Ingenieure: Design methodology for mechatronic systems. VDI 2206 
(2004) 
13. Nattermann, R., Anderl, R.: Approach for a Data Management System and a Proceeding 
Model for the Development of Adaptronic Systems. In: Proceedings for the ASME Inter-
national Mechanical Engineering Congress and Exposition, IMECE 2010, Vancouver, 
Canada (2010)  

10 
R. Anderl, A. Picard, and K. Albrecht 
 
14. Picard, A., Albrecht, K., Anderl, R.: New Teaching Methods and Tools for Development 
and Virtual Testing of Mechatronic Systems. In: IPD-Workshop, Magdeburg (2012)  
15. Bicsi, B.: Network Design Basics for Cabling Professionals. McGraw-Hill Professional, 
City (2002) 
16. Fraden, J.: Handbook of Modern Sensors – Physics, Designs, and Applications. Springer 
(2004) 
17. Isermann, R.: Mechatronische Systme – Grundlagen. Springer (2008) 
18. Nordmann, R.: Mechatronische Systeme im Maschinenbau I. Shaker Verlag (2005) 
19. White, R. M.: A Sensor Classification Scheme. IEEE Transactions on Ultrasonics, Ferroe-
lectrics, and Frequency Control UFFC-34(2) (1987)  
20. Makinwa, K.A.A., Pertijs, M.A.P., van de Meer, J.C., Huijsing, J.H.: Smart Sensor Design: 
The Art of Compensation and Cancellation. IEEE (2007) 
21. Picard, A.: Integration von synchroner Kommunikation in die rechnergestützte Modellie-
rung, Darmstadt (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 11–21. 
DOI: 10.1007/978-3-642-30817-8_2    © Springer-Verlag Berlin Heidelberg 2013 
New Perspectives in the Quest for Unification  
of ‘Lean’ with Traditional Engineering Design 
Methodology 
Sören Ulonska and Torgeir Welo 
Department of Engineering Design and Materials, Norwegian University of Science  
and Technology, Richard Birkelands veg 2B, 7491 Trondheim, Norway 
{soren.ulonska,torgeir.welo}@ntnu.no 
Abstract. In an increasingly competitive business world, engineering compa-
nies need to improve their capability in developing products that offer high  
value to customers. In this connection, the Toyota Product Development Sys-
tem⎯commonly referred to as ‘Lean Product Development’⎯is a benchmark 
for effective, new practices across industries. Lean contains many of the same 
elements as traditional engineering design methodologies, developed in the 
1970-80s, which describe systematic design and engineering processes. How-
ever, the former differs through its philosophical nature⎯rather than being a 
methodology or tool⎯as well as its focus on increasing effectiveness through 
waste reduction. 
In this paper, a literature review of the traditional, systematic product engi-
neering/development methodologies and the more recent lean concept is con-
ducted. Both approaches are analyzed, providing a discussion as to what extent 
traditional methodologies include elements of lean-thinking and to what extent 
the associated product engineering processes are lean. 
Keywords: product development, lean, design methods research. 
1 
Introduction 
Nowadays, engineering companies operate more and more globally in increasingly 
competitive markets. Outsourcing of production and algorithmic engineering tasks to 
so-called low-cost countries is an obvious countermeasure to increase company bene-
fits in terms of cost reduction; however, this does not guarantee long-term competi-
tiveness. The only permanent solution is to improve a firm’s capability in inventing, 
developing, and producing innovative, new products that provide high value to cus-
tomers. In addition, companies need to launch new products earlier than their compet-
itors⎯before new technology emerges or the market changes. These challenges raise 
the need for more effective engineering design methodologies for developing and 
bringing valid, new products to the market place. To establish a basis for effective and 
efficient new-product development (NPD) strategies, it is necessary to understand 
their origin and evolution by considering the history and the context in which these 
methodologies have been developed.  

12 
S. Ulonska and T. Welo 
Traditional methodologies, developed in 1970-80s, describe processes to systemat-
ically design and engineer a product [5-7], [14], [19-20], [23]. More recently, in the 
context of effectiveness in manufacturing and product development, Toyota’s way of 
solving engineering problems is often referred to as the benchmark. Multiple re-
searchers have studied Toyota’s Product Development System (TPDS), commonly 
denoted Lean Product Development (LPD), concluding that Toyota’s practices are 
superior to any other firm with regard to productivity in NPD [8-10], [21]. The lean 
concept⎯whose primary goals are to reduce waste, time-to-market, and cost while 
improving quality⎯has more recently been applied to the process of solving design 
and engineering problems in product development (PD). It seems that many of the 
elements found in traditional PD are applied under a new terminology in LPD, but 
with a somewhat different focus. While traditional PD provides specific, detailed 
step-by-step guidance to designers and engineers, LPD represents more a mind-set 
with basis in a set of principles, focusing on the entire system and its practices. 
In the following, a literature review of the traditional, systematic PD methodolo-
gies and the more recent LPD concept is conducted. Both approaches will be syste-
matically analyzed at detail level, providing a discussion as to what extent traditional 
methodologies include lean-thinking and to what extent the processes are lean. In this 
context, the main research questions are: What is new about lean? What does the lean 
notion bring to NPD⎯and what is the origin of the methods employed? What is lean 
about traditional product engineering⎯and what are the differences, the commonali-
ties and the complementary attributes of traditional and lean methodologies?  
2 
Traditional Product Development Methodology 
Renowned researchers as Rodenacker [19], Pahl and Beitz [14], Hubka [7], Roth [20], 
and several others, describe methodologies for PD and engineering, developed in the 
1970s and 80s, guiding designers and engineers to systematically find solutions to 
technical problems. Their aim is to provide a methodology to design, engineer and 
develop desirable solutions that satisfy a set of requirements. However, these metho-
dologies are not the first approaches for systematic engineering and PD. The origin of 
systematic engineering methods is back in the 1940s [15], [17], and are developed 
from system theory, machine elements, and product specific approaches. In the devel-
opment to follow, the PD research community was concerned with increasing the 
number of engineering principles within the framework of an increasingly structured 
engineering process, which was divided into different phases (e.g. VDI 2221 [25]). 
The classical approaches mentioned in the beginning of this section are benchmarks 
in this context, representing the so-called traditional PD methodology. These metho-
dologies have been adapted to trends and state-of-the-art during the last few decades, 
for example axiomatic designs [16], [22], product structuring in modules, platforms, 
and architectures [15], [25], or stronger focus on customization and the whole product 
life-cycle, while the PD-phases remained essentially the same.  
All the above-mentioned authors more or less describe a holistic approach to engi-
neering design; each one providing an individual contribution. In addition, everyone 

 
New Perspectives in the Quest for Unification of ‘Lean’ 
13 
uses the same main structure to develop a product, which can be summarized through 
the following phases: At first, the main task has to be defined, including in-depth 
understanding of the problem, which is defined in a requirement list. Then the prob-
lem is abstracted into ‘black-boxes’ [7] or functions, which are decomposed to more 
abstract sub-functions. In the next phase, different principal solutions are combined to 
establish (several) concepts. After an evaluation the most promising concepts are 
chosen for further work. Then, the preliminary layout or the basic product structure is 
defined, followed by elaboration of the detailed solution, which includes all design 
features, bill of materials, production methods, etc. All the examined approaches in-
troduced a well-defined engineering methodology, guiding product engineers through 
the process step by step. The primary emphasis is on tasks required to find solutions 
to technical problems at design and engineering levels; ones that are driven by engi-
neering excellence rather than process efficiency and cost. 
3 
Lean Product Development 
The TPDS is the main source to what many, right or wrong, consider synonymous 
with so-called LPD. The concept emerged in the mid-1990s and has its origin in lean 
manufacturing, starting with the Lean Automotive Factory and evolving into the Lean 
Factory with emphasis on cost reduction, quality improvement, and delivery [8-10], 
[12], [24], using a system perspective. Based on an excessive study of TPDS, Morgan 
and Liker [13] introduced 13 lean principles within the dimensions of process, tech-
nology and people. The process-principles are the most interesting ones in terms of 
the contents of this paper, since the two other dimensions touch more on factors in 
execution environments outside product engineering. The primary objectives of LPD 
are to minimize waste, improve quality, reduce time-to-market and cost, all driven by 
the desire to create value to the customer. Here value may be characterized as any 
activity that transforms a new product design in a way that the customer is both aware 
of it and willing to pay for [10]. While waste is easy to detect in manufacturing (visi-
ble, physical objects), separating value from waste is more difficult in PD since the 
work-product is information and there are no physical objects to which value can be 
assigned. In general, waste can be divided into two categories. Type 1 waste includes 
activities that do not create value that the customer is aware of, but is still necessary to 
enable value generation (e.g. administration, coordination, testing, validation, checks, 
etc.). Type 2 waste is pure waste that does not create any value (e.g. defects, waiting, 
underutilization of people, etc.).  
An important part of the lean philosophy is learning and continuous improve-
ment [13]. Based on the Deming-Cycle [11] improvements and iterations are done 
continuously in small steps, aiming to reach the ultimate goal of a perfect solution by 
following a learning-spiral with each cycle closer to the target than the previous one. 
Although these iterations could be considered waste (type 1) at micro-process level, 
they are necessary to maximize the value of the overall outcome seen in a system 
perspective. In addition, by capturing knowledge for later reuse the learning cycle is a 
source of organizational learning, providing strategic value for the company. In the 

14 
S. Ulonska and T. Welo 
lean literature, the learning cycle is called PDCA-cycle (Plan, Do, Check, Act) [21] or 
LAMDA-cycle (Look, Ask, Model, Discuss, Act) [22]. In the first step (Look) the 
problem is observed and data are collected. Then, it has to be checked what is known 
about the problem and why this problem exists. Following, a model (prototype, 
sketch, etc.) to support articulate thinking is established. As the fourth step (Discuss), 
the problem and possible solutions are discussed with experts, and finally the solution 
is implemented (Act). In the quest for perfection, the cycle does not stop here but 
restarts from the first step again; this time at a higher level of knowledge. In the LPD 
philosophy, knowledge is effectively captured and communicated using ‘knowledge-
briefs’ [8], or so-called A3 reports [21] named by the paper size format used, aiming 
to visualize problem, goal, process, and solution, and risk elements in a standardized 
form, depending on the application and problem formulation. 
One methodology, often referred in the context of LPD is the so-called set-based 
concurrent engineering (SBCE) [10], [12]. In contrast to a single (point-based) ap-
proach, multiple alternatives are explored in parallel and systematically narrowed 
down through analysis and testing. Within the set of concepts, one is a proven no-risk 
alternative concept that can be selected as a fall-back in case the others do not suc-
ceed. The weaker concepts are successively ‘killed’ on the way, following a ‘survival-
of-the- fittest’ strategy. Lastly, only the best and most robust solution that fulfills all 
requirements remains, hence increasing the opportunity for innovation while reducing 
risk and development time. SBCE is a method aimed at frontloading resources to 
reduce late and expensive design iterations.  
In summary, LPD it is not just a methodology for engineers, it is a way of working, 
organizing, and making the PD processes more effective, considering both engineer-
ing and product management (PM) problems at engineering and management levels.  
4 
Comparison of Traditional Product Development and Lean 
Product Development 
It appears that traditional PD and LPD cannot be directly compared to each other, 
since their overall goals are different. Traditional PD describes a systematic approach 
of well-defined steps, explaining engineers what to do to create a product that solves a 
given (technical) problem. LPD, on the other hand, introduces a way to make engi-
neering processes more effective to improve the outcome for a company with value 
being the driver. It describes how processes have to be done to make a company more 
competitive by pulling value from customers and up the value chain. Lean is more a 
philosophy and a mind-set, rather than a detailed methodology to solve engineering 
problems [27]. Hence, traditional PD explains which steps have to be conducted and 
what has to be done in these steps, whereas LPD describes the working philosophy 
around the PD process. However, LPD and traditional PD are not contradictory in any 
respect. It is possible to apply the lean principles to (all) known engineering methods 
defined in traditional PD. Lean complements traditional methods by including mana-
gerial factors such as effectiveness (e.g. short time-to-market) and waste reduction 
(e.g. people, money, rework). Table 1 summarizes some key characteristics of both.  

 
New Perspectives in the Quest for Unification of ‘Lean’ 
15 
Table 1. Characteristics of Traditional Product Development and Lean Product Development 
Goals of Traditional Product Development 
Goals of Lean Product Development 
Gives specific ‘work instructions’ to mainly engineers at 
detail level 
Gives visionary and directional strategies for the entire 
company at system level with PD being the core compo-
nent 
Methodology that provides engineers with tools for 
solving a wide range of technical problems, and develop-
ing and designing products  
A company-wide PD system aimed at maximizing value 
to the customer or user, within the constraints of value to 
other stakeholders [1] 
Focusing on developing the best technical solution (high 
quality) with basis in engineering excellence 
Focusing on using an effective process to develop an 
overall optimal (customer) solution from a system pers-
pective, including operational and strategic management  
Use of knowledge and ideas to create solutions for 
technical problems  
Effective capturing and reuse of knowledge and ideas for 
increased learning, and to develop solutions with highest 
possible value in the eyes of the customer 
Can solve unknown problems and improve existing 
products; i.e., offering methodologies for both 
Strong basis in known processes with predictable out-
come (continuous improvement), minimizing technical 
risk within PD, i.e. after program definition  
Follows parallel or sequential processes, aiming to solve 
the task as well as possible 
Follows parallel processes, aiming to solve the task fast 
with effective use of resources 
 
In the following, traditional PD will be examined with regard to lean elements in 
order to answer the following question: In which way are traditional PD approaches 
lean? Six different approaches in the category of traditional PD methodologies and 
one approach of integrated PD⎯ones that are commonly referred as benchmarks in 
traditional PD⎯are analyzed in the context of lean. The findings are summarized in 
Table 2, which relates a set of lean principles to the reviewed approaches of tradition-
al PD. The lean ‘principles’ chosen here represent a broad selection of lean compo-
nents, which are based on the ones introduced by Morgan and Liker [13] and adapted 
to the scope of this paper. Notice that if a lean component is indicated with an ‘x’ it is 
a part of the traditional PD approach, and vice-versa. 
Rodenacker’s [19] approach is one of the early ones in systematic engineering de-
sign, with the basic approach still being applied in methodologies today. Rodenacker 
aims to find solutions for the cause-effect relations stepwise through logical, physical, 
and structural working principles. He uses a learning cycle similar to PDCA with the 
steps: information retrieval, information processing, information output, and check-
ing. Capture, reuse and extension of knowledge all are part of Rodenacker’s approach, 
which are important for continuous improvement.  
Tjalve’s [23] contribution to the design methodology is mainly form variation. 
Product solutions and alternatives are developed by systematically varying size, num-
ber, structure and shape of the design elements. Tjalve uses a learning cycle, called 
‘product synthesis’, similar to lean. He proposes that the criteria vary from phase to 
phase and have an increasing number of details, based on details from the former step. 
This reflects the lean principles continuous learning and improvement. 
Pahl and Beitz [14] provide a linear, holistic, systematic engineering design 
process to help design engineers find solutions for products by the use of different 
tools. They suggest that a PD methodology should save time, reduce work load, 
speed-up understanding and help maintain active interest. Further, they want the  

16 
S. Ulonska and T. Welo 
different functions concerned with development of a product to collaborate early. 
Problems should be detected early and clearly defined in the requirement list together 
with customer needs. Pahl and Beitz refer to a learning cycle, similar to the LAMDA 
cycle: confrontation, information, definition, creation, evaluation, decision, solution. 
They interpret the design process as a dynamic control process that continues until the 
information (content) has reached a level for optimum solution. Here it should be 
noted that many lean approaches follow the same strategy. 
Roth [20] introduces design catalogs for engineers. ‘Effects’, ‘effect owners’, ma-
terials, etc. are systematically structured in catalogs, which make knowledge capture 
and reuse simple, providing the design engineers a set of standard solutions and rec-
ommendations. Roth states that it is important to define the correct problem statement 
early and to attack problems at the root cause. He does not explicitly use expressions 
such customer or customer value, which are important drivers within LPD. However, 
customer (value) may still be considered as part of his approach since customer satis-
faction is mandatory for the success of a product. Roth applies engineering catalogs, 
which is essentially similar to the knowledge-brief approach [8], [21] within lean. 
Experiences, standards, and former product solutions can be documented in a visual 
engineering-friendly way by both approaches. The catalogs, which give fast and clear 
overview of alternatives, represent a knowledge-based approach to product develop-
ment. Catalogs can be adapted to the design process of a certain company, and can 
also be extended. An additional core component of lean is the use of standardization 
and checklists. For instance, standard tables (and check lists) are used for the gather-
ing of requirements, and these can be adjusted and extended to meet new challenges. 
In LPD a similar approach is employed by alternative concepts such as house of 
quality and quality function deployment (QFD). 
Ehrlenspiel [5] discusses the influence of engineering design on product costs, in-
cluding life-cycle costs. He proposes a number of opportunities to reduce product cost 
by correct selection of design features, production methods, materials, and good col-
laboration between different departments inside a company. Cost reduction opportuni-
ties lie in standardization of products, which is lean, by for instance using modular 
product concepts with standard parts or assemblies and customer-specific adaption of 
parts and assemblies. Ehrlenspiel uses value analysis to identify unnecessary costs, 
aiming to determine which product functions are absolutely necessary to accommo-
date the task that has to be accommodated to satisfy the customer, which can be  
associated with reduction of waste, meaning lean design. This methodology is also 
consistent with value engineering, which was developed during World War II [27]. 
Further, Ehrlenspiel encourages close communication between teams and short lines 
of communication, which supports the pull concept in lean. However, his approach is 
a more specific approach, guiding engineers to use cost reduction methods in detail, 
whereas LPD to a more extent approaches system problems. 
Hubka et. al. [7] introduce a theory for technical systems, which needs to have 
transformations (functions), organs (e.g. functional interfaces) and parts (compo-
nents), where the organs represent the link between two components or one compo-
nent and the user. Hubka proposes a kind of SBCE; several concepts, which are  
determined after each design phase, are developed in parallel up to a certain detail 

 
New Perspectives in the Quest for Unification of ‘Lean’ 
17 
level and evaluated. Concepts that are strong enough are carried forward. The evalua-
tion at the end of each phase is based on the status, the experience and learning of 
previous work, and the customer specifications. This resembles the lean principles of 
continuous learning, reuse of knowledge, and focus on customer value. 
Hein et al. [6] introduce one approach that considers PD in a broader perspective, 
so-called integrated product development (IPD). This is a more holistic approach that 
includes engineering design, production, marketing, and organization. IPD seeks to 
integrate methodologies used in different departments of a company toward common 
goals, procedures, and attitudes. The customer is of key importance, since s/he  
ultimately decides if the product becomes a success or not. Hein points out that the 
market is getting more competitive, which requires shorter development time, less 
production costs, and fast and continuous implementation of new technology for ac-
tive adaption and renewal of today’s products. Focus is not just on the product itself, 
but the entire execution environment, which is necessary to make the product success-
ful in the market place. Hence, IPD makes a step forward from pure engineering de-
sign methodology in the direction of LPD and product management (PM). 
Table 2. Lean Elements in Traditional Product Development Methodology (Legend: - not 
mentioned; (x) implicitly mentioned; x mentioned) 
Lean Principle 
Ro-
den-
acker 
Tjalve 
Pahl, 
Beitz 
Roth 
Ehr-
len-
spiel 
Hubka 
Hein 
Continuous control of requirements 
- 
x 
x 
x 
(x) 
x 
x 
Front load of the PD process 
- 
- 
x 
x 
x 
- 
x 
Understanding the customer 
- 
(x) 
x 
- 
x 
x 
x 
Integrate customer and supplier in com-
plete development 
- 
- 
- 
- 
- 
- 
- 
Parallel processes 
- 
x 
- 
- 
(x) 
x 
x 
Increase standardization, reduce variation 
- 
x 
x 
x 
x 
x 
(x) 
Continuous improvement of product 
x 
(x) 
x 
x 
x 
x 
x 
Continuous improvement of process 
(x) 
- 
x 
- 
- 
- 
x 
Capturing and reuse of knowledge and 
experience 
x 
(x) 
x 
x 
(x) 
(x) 
x 
Capturing past knowledge in checklists 
(x) 
- 
- 
x 
x 
(x) 
- 
Short and precise knowledge capture 
- 
- 
- 
x 
- 
- 
(x) 
Early include all different departments 
- 
- 
(x) 
(x) 
x 
- 
x 
Learning Cycle 
x 
x 
x 
(x) 
x 
x 
x 
Set-based concurrent engineering 
- 
- 
- 
(x) 
- 
x 
- 
Solving the roots of problems 
(x) 
- 
x 
x 
x 
- 
x 
This literature review shows that many elements of the LPD concept have been de-
veloped under different headings many years before the term lean was coined in the 
Western PD vocabulary. Learning cycles, knowledge capture and reuse, continuous 

18 
S. Ulonska and T. Welo 
improvements, and customer value all have been elements of the product engineering 
literature for several decades. What is new, associated with lean, however, is its 
strong focus on effectiveness and waste elimination. Hence, traditional PD methodol-
ogy delivers engineering tools for development of high-quality products, whereas 
LPD in addition targets effectiveness. 
5 
Product Development, Product Management and Lean 
Product Development in a Historical Perspective 
In the section above it has been shown that many elements of LPD have their origin 
from the traditional product design and engineering research community. LPD does 
reuse traditional approaches to a great extent, applying a different terminology in 
many cases. Moreover, basic engineering methodology is not part of the lean litera-
ture, which rather represents a holistic approach to improve the PD productivity. 
Some of this may be explained by the historical development of PD or LPD. Figure 1 
shows a principal interpretation of historical progress of PD, PM and LPD literature, 
illustrating the development of the three fields and an increased overlap towards right.  
 
Fig. 1. Development of traditional PD, PM and LPD literature 
First, traditional PD started out as a research field in the 1970s, describing  
methodologies to systematically solve engineering problems and develop advanced 
products.  
Later, throughout the 1980s and 1990s, the amount of PM research increased  
gradually. In PM, approaches to improve financial performance, innovation, differen-
tiation and new-products’ success in the market are introduced as well a holistic busi-
ness view of marked, product and production in integrated PD [6]. Cooper [2-3], for 
instance, introduced strategies for successfully driving products to market, like prod-
uct and technology strategies, portfolio management, and stage gate processes. PM 
and PD complement each other, since both are important to successfully create and 
deliver the right product but from different perspectives. This may be illustrated by 
the two approaches increasingly overlapping each other.  

 
New Perspectives in the Quest for Unification of ‘Lean’ 
19 
In the late 1990s, yet another approach, namely LPD, emerged from (US automo-
tive) companies’ need of being competitive in a global market. Supplementary to the 
other two approaches, lean puts emphasis on customer, value, waste reduction, and 
increased effectiveness primarily with basis in the engineering perspective. Lean me-
thods can be applied to⎯and are becoming increasingly part of⎯both PM and PD, as 
symbolized by the overlapping shaded areas. For instance, Cooper [4] realized several 
of the problems associated with the PM perspective that forms the basis for the clas-
sical stage-gate process, and updated his view towards a more process-driven organi-
zation, introducing 5-6 concepts directly from LPD.  
Today’s strong focus on lean methods can be explained through increasing market 
pressure, forcing companies to reduce time-to-marked and cost while improving in-
novation. This means that the competitive frontiers drift from, say, engineering  
excellence and workmanship towards efficiency of process, multi-disciplinary teams, 
collaboration, supplier integration, networks, knowledge management, organizational 
learning etc. In this respect, LPD seems to be an important strategy for bridging the 
gap between traditional engineering-oriented PD and more business-oriented PM.  
6 
Conclusions 
This review and discussion helps to better understand the differences of PD approach-
es and their historical development. The results show that many of the core elements 
in LPD have their roots in traditional PD, but under different names and headings. It 
appears that several classical methods have been reborn under a new common termi-
nology called lean. Lean has its origin⎯or should we say rebirth⎯in Japan, and was 
brought into the context of product development by US researchers [8-10], [12-13], 
[24], [26]; in many cases⎯purposely or accidentally⎯not fully considering the me-
thods’ original references in the design and engineering community. The good thing 
about this is that the new ‘wrapping’ helps bring the methods out to a greater commu-
nity outside the academic world, including practical engineers, managers and CEOs, 
boosted by popularization of an approach to an outermost important challenge for 
many of today’s companies: NPD performance.  
Nevertheless there are new elements in LPD. LPD adds effectiveness, waste reduc-
tion and competiveness to the traditional approaches and makes them evolve and 
adapt them to today’s competitive challenges. It is also demonstrated that the lean 
concept, when applied to PD, to some extent fills the gap between traditional product 
engineering (in the engineering community) focusing on micro-processes, and  
product innovation management (in business-economics community) focusing on 
macro-processes. To be successful in the marketplace, a combination of both tradi-
tional, PM, and LPD appears to be a good approach, applying both the engineering 
guidance of traditional PD and making processes effective by LPD. 
Some very interesting questions in this context are: How did Toyota develop a lean 
culture and from whom did they adopt their methodology; and how did US and Euro-
pean companies develop the revolutionary products and technologies that have served 
as a fundamental pillar of productivity growth in the 20th century, decades before the 
notions ‘lean’ and ‘lean product development’ were coined?  

20 
S. Ulonska and T. Welo 
References 
1. Browning, T.R.: On customer value and improvements in product development processes. 
Systems Engineering 6(1), 49–61 (2003) 
2. Cooper, R.G.: How New Product Strategies Impact on Performance. J. Product Innovation 
Management 1(1), 5–18 (1984) 
3. Cooper, R.G., Kleinschmidt, E.J.: An Investigation into the New Product Process. J. Prod-
uct Innovation Management 3(2), 71–85 (1986) 
4. Cooper, R.: Re-Thinking the Stage-Gate Process – A Reply to the Critics. Management 
Roundtable Inc. (2006) 
5. Ehrlenspiel, K.: Kostengünstig Konstruieren: Kostenwissen, Kosteneinflüsse, Kostensen-
kung. Konstruktionsbücher Band 35. Springer, Heidelberg (1985) 
6. Hein, L., Andreasen, M.M.: Integreret produktudvikling. Jernets Arbejdsgiverforening, 
Copenhagen (1985) 
7. Hubka, V., Andreasen, M., Eder, W.E.: Practical Studies in Systematic Design. Butter-
worth & Co., London (1988) 
8. Kennedy, M.: Product Development for the Lean Enterprise: Why Toyota’s System is 
Four Times More Productive and How You Can Implement it. The Oklea Press (2010) 
9. Liker, J.K.: The Toyota Way – 14 Management Principles from the world’s greatest Man-
ufacturer. McGraw-Hill, New York (2003) 
10. Mascitelli, R.: The Lean Product Development Guidebook. Technology Perspectives, 
Northridge (2007) 
11. Moen, R., Norman, C.: Evolution of the PDCA Cycle 
12. Morgan, J.M.: High performance Product Development: A Systems Approach to a Lean 
Product Development Process. PhD Thesis, University of Michigan, Ann Arbor (2002) 
13. Morgan, M., Liker, J.K.: The Toyota Product Development System. Productivity Press, 
New York (2006) 
14. Pahl, G., Beitz, W.: Engineering Design. The Design Council, 1st English edn. Springer, 
London (1984) 
15. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Konstruktionslehre, 7th edn. Springer, 
Heidelberg (2007) 
16. Park, G.J.: Analytic Methods for Design Practice. Springer (2007) 
17. Pulm, U.: Eine systemtheoretische Betrachtung der Produktentwicklung. PhD thesis, Insti-
tute of Product Development, Technical University of Munich (2004) 
18. Reinertsen, D.G.: Managing the Design Factory - A Product Developer’s Toolkit. The Free 
Press, New York (1997) 
19. Rodenacker, W.G.: Methodisches Konstruieren. Konstruktionsbücher Band 27. Springer, 
Berlin (1970) 
20. Roth, K.: Konstruieren mit Konstruktionskatalogen. Band 1-2. Springer (2000) 
21. Sobek, D.K., Smalley, A.: Understanding A3 thinking. Productivity Press, Boca Raton 
(2008) 
22. Suh, N.P.: Axiomatic design theory of systems. In: Research in Engineering Design, 
vol. 10, pp. 189–209. Springer-Verlag London (1998) 
23. Tjalve, E.: Systematic Design of Industrial Products. Institute for Product Development, 
Technical University of Denmark, Newnes-Butterworths, London (1979) 
24. Ward, A.C., Liker, J.K., Christiano, J.J., Sobek, D.K.: The second Toyota paradox. Sloan 
Management Review, 43–61 (1995) 
25. Ulrich, K.T., Eppinger, S.D.: Product Design and Development. McGraw Hill (2011) 

 
New Perspectives in the Quest for Unification of ‘Lean’ 
21 
26. Ward, A.C.: Lean Product and Process Development. Lean Enterprise Institute Inc., Cam-
bridge (2007) 
27. Welo, T.: On the application of lean principles in Product Development: a commentary on 
models and practices. Int. J. Product Development 13(4) (2011) 
28. VDI-Richtlinie 2221: Methodik zum Entwickeln und Konstruieren technischer Systeme 
und Produkte. VDI-Verlag, Düsseldorf (1993) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 23–32. 
DOI: 10.1007/978-3-642-30817-8_3 
© Springer-Verlag Berlin Heidelberg 2013 
 
Procedural Model for the Virtual Commissioning  
on the Basis of Model-Based Design 
Tanja Schmüdderrich1, Ansgar Trächtler1, Jan Brökelmann2, and Jürgen Gausemeier2  
1 Heinz Nixdorf Institute, Control Engineering and Mechatronics, University of Paderborn 
Fürstenallee 11, 33102 Paderborn, Germany  
2 Heinz Nixdorf Institute, Product Engineering, University of Paderborn  
Fürstenallee 11, 33102 Paderborn, Germany 
{tanja.schmuedderrich,jan.broekelmann}@hni.upb.de 
Abstract. There are many methods for the development of new products. One 
of them is model-based design. This article is dedicated to the usage of this de-
sign method for the reduction of the time required for the commissioning of 
manufacturing plants. For this purpose, the method of virtual commissioning is 
combined with the model-based design which makes it possible to create a pro-
cedural model. The presented approach assists the developer in order to save 
time during the actual commissioning and thus reduce costs. The presented ap-
proach is validated on an application example. 
Keywords: virtual commissioning, production system development, model-
based design, systems engineering. 
1 
Introduction 
Growing competitive pressure requires the commissioning of manufacturing plants to 
be done in shorter time. However, this is restrictedby the increasing complexity of 
such plants. Today the achievement of modern mechanical products (like these plants) 
is affected by the close interaction of mechanics, electrics/electronics, control engi-
neering and software engineering. This circumstance is expressed by the term “me-
chatronics”. Advanced mechatronic systems lead to a considerably increased system 
complexity due to their functionality and internal interaction. In addition, the in-
volvement of different domains requires an effective and continuous cooperation and 
communication between all developers during the whole development process. The 
conceivable development of information technology opens up fascinating perspec-
tives for the design of future technical systems, which go far beyond current  
standards. 
To analyze the system behavior as early as possible models are increasingly used. 
There are many possibilities for the design of mechatronic systems such as the  
"Design Methodology for Mechatronic Systems" [1] or the "Design of Intelligent 
Mechatronics (ENTIME)" [2]. The foundation for these development methods is a 
model-based approach. During the model-based design both the functions and the 
behavior of a mechatronic system are considered. Models that are created in this 
manner can be used for several analysis purposes [1]. 

24 
T. Schmüdderrich et al. 
The foundations which are formed in the design phases turn into real plants which 
have to be commissioned. During the commissioning of a plant often different topics 
merge: the clarification of the functionality of components and plants, the continua-
tion of software development, the functional test of the software, the parameterization 
and optimization of the automation software and the acceptance test. Those work 
contents are temporally rather undefined. They are cost extensive because they take 
place during the installation process. Hence they are fraught with risks and can delay 
the planned commissioning drastically [3]. 
An approach to reduce the time until start of production is virtual commissioning, 
which is a commissioning of the real control system and the original software in com-
bination with a virtual plant. For the control system the behavior of the virtual plant is 
adequate for starting up and running both. The simulation model for the virtual com-
missioning consists of the behavior model, reproducing the logical behavior of the 
machine and the kinematics model visualizing the movement behavior of the machine 
in all relevant degrees of freedom. Research has already shown that thereby the time 
for the commissioning decreases and the quality of the control program of the produc-
tion system increases [4-5]. However, the effort for the modeling is very high and 
often compensates the time advantage which leads to the fact that the cumulated time 
required until start of production of the plant is not reduced [6]. 
The basic proceeding of the virtual commissioning in the presented approach is 
similar to the procedure suggested in [4]. In [4] new models are created for each 
commissioning process. Our approach is to adapt and simultaneously use the models 
created during the model-based design also for the virtual commissioning. In this way 
time is saved and hence the overall costs are reduced. 
By this integrative approach, synergy effects can be utilized. Both the functions 
and the behavior of a production system are modeled. Such models can subsequently 
be used for analyses, which make it possible to reduce the costs and the development 
time of new production systems. A further advantage results from the use of solution 
patterns1. They allow the use of existing solutions for specific system elements or 
system components, in particular the simulation models.  
This contribution focusses the procedural model, in which the virtual commission-
ing is integrated in the model-based design process. For this purpose, we point out in 
section 2 the model-based design adapted for the planning of engineering plants. We 
consider also the choice of the modeling depth for the behavior models in the model-
based design. In section 3 we present the procedural model itself. The procedural 
model is validated on an example in section 4. Finally the major points are summa-
rized in section 5. 
2 
Model-Based Production System Design 
The following section treats the adaption of the model-based production system de-
sign described in [7] for the purposes of virtual commissioning. First a general over-
view is given followed by a detailed explanation of the main design phases. 
                                                           
1  A solution pattern is an abstract representation of a class of solution elements and describes 
[...] its structure and behavior in a generalized form (see [7]). 

Procedural Model for the Virtual Commissioning on the Basis of Model-Based Design 
25 
The life cycle of manufacturing plants comprises the phases conceptual design, de-
tail design, manufacturing and assembly, operational phase and finally redistribution. 
The procedural model of virtual commissioning on the basis of model-based design 
presented in this paper focuses on the first three phases (Fig. 1). It is modeled in the 
style of the VDI 2206 guideline [1]. The phases conceptual design as well as detail 
design are each divided into the tasks of the basic cycle of system design determina-
tion of objectives, synthesis and analysis [8]. 
 
Fig. 1. Production of Engineering Plants 
The process images are displayed in an abstract way. Iterations may occur at any 
time. As seen in Fig. 1 the conceptual design phase ends with the principle solution. 
This is the starting point for the detail design in which the concrete solution is devel-
oped in the particular specialized disciplines (e. g. mechanical engineering or control 
engineering). The manufacturing and assembly starts with the concrete solution and 
ends with the start of production. In the following are describe the three phases in 
detail.  
2.1 
Conceptual Design 
The conceptual design phase starts with the determination of objectives. Fig. 2 illu-
strates the certain steps of this phase. First of all an analysis of tasks and environmen-
tal boundary conditions is conducted. As soon as the application scenarios have been 
defined and the description of the requirements is established the function hierarchy 
can be drawn up, which is the last step of the completion of the determination of ob-
jectives of the conceptual design. 
The phases run through during the process of synthesis are presented in Fig. 3. It 
starts with the building of active structures and the system modularization. The mod-
ularization should be carried out under the aspect of a modular product concept being 
focused on the recovery of components. It is structured under a design-oriented and 
functional view. Subsequently, the solution patterns for the single module are  
 

26 
T. Schmüdderrich et al. 
 
Fig. 2. Process steps of the determination of objectives as part of the conceptual design [7] 
examined and selected. Solution patterns in plant construction are such modules being 
previously built or being purchased by third parties. It is especially identified whether 
a solution pattern already exists or whether a new development is required. In the 
following the behavioral model and the first approximate shaping are being estab-
lished. Afterwards, the selected modules are detailed. This phase is necessary, if a 
superior machine control is to be used. It should be specified, for example, to which 
module the sensors and actuators have to be assigned, or whether they are indepen-
dent modules. Finally, the modules have to be integrated into the entire system. The 
modularized entire system represents the completion of the synthesis phase. 
 
Fig. 3. Process steps of the synthesis as part of the conceptual design (see [7]) 
The last step of the conceptual design is the analysis (Fig. 4). At first the object of 
analyzing is defined. This is done by comparison with the task settings from the de-
termination of objectives phase. The established system design has to fulfill the re-
quirements. Then, the analysis is performed. Finally, analysis results are evaluated, 
which leads to a principle solution. This depicts the completion of the analysis phase 
and the whole conceptual design simultaneously. In the principle solution it is stated 
of which line components the plant consists and which manufacturing technologies 
are applied. 

Procedural Model for the Virtual Commissioning on the Basis of Model-Based Design 
27 
 
Fig. 4. Process steps of the analysis as part of the conceptual design [7] 
2.2 
Detail Design  
The detail design phase starts with the principle solution that has been developed in 
the conceptual design. The detail design also comprises three steps: determination of 
objectives, synthesis and analysis. These are carried out in the different domains 
(Fig. 5). As they are not relevant for the virtual commissioning, they are only shortly 
described, see [7] for more detailed information. Important for the detail design is the 
selection of solution elements2 for the beforehand used solution patterns in the first 
step. In the course of the process the results are being matched. For this purpose be-
havioral models are applied to make sure that the individual modules are compatible. 
The results of the individual domains are integrated into an entire solution (Fig. 5). At 
the end of this phase a concrete solution is available. 
 
Fig. 5. Concretization of the principle solution [7] 
2.3 
Manufacturing and Assembly 
The manufacturing and assembly phase consists of four steps (Fig. 6). These are the 
mechanical and electrical manufacturing as well as the assembly planning, which are 
performed in parallel. Subsequently, the assembly takes place. First of all the mechan-
ical structure is arranged and then the electric installation is executed. This phase is 
terminated by the commissioning process. Here the plant is transferred to the expected 
state. As the case may be, assembly and commissioning process can be repeated, 
when the plant has to be set up at the manufacturer’s site already. At the end of this  
 
                                                           
2  Solution elements are implemented and proven solutions - assemblies, modules, software 
libraries, etc. - to meet a function of the entire system (see [7]). 
Module 1
Module n

28 
T. Schmüdderrich et al. 
process the integration into the consisting production street and the acceptance of the 
plant is conducted. The phase is accomplished with the transition into regular opera-
tion, the start of production. 
 
Fig. 6. Process steps of the manufacturing and assembly 
2.4 
Modeling Depth 
For many of the previously described phases behavioral models are needed. The de-
veloper of these models has to decide individually what level of detail (modeling 
depth) he indicates. In general, models should be as detailed as necessary but as ab-
stract as possible. This decision is very subjective and needs a high level of system 
understanding and experience [9]. For this reason, in [10], a methodology for select-
ing the modeling depth was defined. This is also of utmost importance for the virtual 
commissioning. Therefore, it is explained shortly at this place. 
Central point of the methodology is the definition of different modeling depths. For 
the virtual commissioning three levels are necessary. These are the idealized function, 
the basic feasibility and the system-specific behavior. In the following the three levels 
are shortly defined (for detailed information and examples see [10]). 
Level 1 – idealized function 
The models are logically and not physically modeled. It means that these models con-
tain time-discrete state machines. This level does not allow the representation of the 
dynamic behavior. Only time-discrete values can be reproduced. 
Level 2 – basic feasibility 
These models also comprise time behavior and can be modeled physically. The mod-
els are strongly idealized without consideration of side effects (e.g. friction). At this 
level, solution patterns are employed for modeling. 
Level 3 – system-specific behavior 
This level contains models that are physically modeled and side effects are considered 
in a simple manner (e.g. linear friction). The solution patterns at this level are re-
placed by solution elements. 
3 
Procedural Model 
The procedural model correlates the previously defined phases with the levels of 
modeling depth. Basically all system components can be modeled in different levels 
of modeling depth. Each modeling depth represents the time from which it can be 

Procedural Model for the Virtual Commissioning on the Basis of Model-Based Design 
29 
used. For those modules which can be modeled in an abstract level of detail, the mod-
eling process is completed at an earlier stage. It specifies what information must be 
available to proceed with a more detailed modeling depth. The advantage of the  
procedural model is the well-planned approach in which the level of detail increases 
continuously. 
In Fig. 7 the process model with the integrated virtual commissioning is presented. 
The virtual commissioning consists of the phases modeling, realization and analyzing. 
In the modeling phase, the behavioral models of the individual modules are set up. 
The virtual commissioning is performed parallel with the model-based design. 
 
Fig. 7. Procedural model for the virtual commissioning on the basis of model-based design 
The modeling at the level of idealized function can already begin in the conceptual 
design phase in particular if the entire system is modularized. At this time, the phases 
of module conception and integration of the conceptual design are run through and the 
entire system is divided into subsystems. Now the modeling depth for each module 
can be defined considering the certain requirements (see [10]). If the module consists 
of already existent solution patterns, the existing behavioral models can be used. Oth-
erwise new models must be created. The models that arise in model-based design in 
the conceptual design phase are adjusted for the virtual commissioning to provide it 
with information (such as mass, length, etc.). They are applied in the analysis phase of 
the conceptual design. The analysis results can also be used in the model-based design 
to expedite the development. For the analysis phase at least modules of the idealized 
function of each module are required. Afterwards the module integration is evaluated 
and the principle solution is created. If modules need to be modeled in a more detailed 
modeling depth, the modeling process can be started after the synthesis of the concep-
tual design. All the information will lead to the already described principle solution. 
Parallel to the detail design the models of level 3, the system-specific behavior can be 
0
0
0
0

30 
T. Schmüdderrich et 
modeled. These are feasibl
After selecting a solution e
be used in this case. Now t
necessary for the detailed b
system element has to go th
stop after the models which
in a sorting system is not ne
also the software engineeri
control program for the pla
on the generated models. In
level to a lower level can
commissioning allows an e
the manufacturing and asse
eliminated in advance. In 
Moreover, even an integrati
the verification of software 
commissioning is finished w
4 
Application Exa
The above described proce
(Fig. 8) to demonstrate its u
explained.  
F
The dough production sy
dosing station, filling the in
per, to tip out the finished
which the dough is mixed a
carrier is used, which carri
al. 
le from the time when the solution elements are selec
element, the decision has been made which element sho
the component dependent information is available whic
behavior models of the system-specific behavior. Not ev
hrough all the modeling steps. For some it may be usefu
h represent the idealized function. For example the stor
eeded very detailed (see [10]). In the detail design inter 
ng is performed. Once the concrete solution is created 
ant exists. From this time, the controlling can be simula
n the virtual commissioning even the change from a hig
n be reasonable to increase the simulation speed. Virt
early test of the control program. It takes place paralle
embly phase. Thus, errors of the control program can
this manner, the time of plant shut down is shorten
ion of the manufacturing system can be simulated. Besi
the training of operating personnel is possible. The virt
with the putting into operation of the plant. 
ample: Dough Production System 
edural model is now applied to a dough production syst
usage. At first, the structure and function of the system 
 
Fig. 8. Dough production system [2] 
ystem consists of modular processing stations. These ar
ngredients, a mixer to mix the ingredients and a dump 
 dough for further production. Further there are bowls
and transported. For the transport of the bowls the so cal
es the bowls from one station to the next. The product
ted. 
ould 
h is 
very 
ul to 
rage 
alia 
the 
ated 
gher 
tual 
el to 
n be 
ned. 
ides 
tual 
tem 
are 
re a 
tip-
s in 
lled 
tion 

Procedural Model for the Virtual Commissioning on the Basis of Model-Based Design 
31 
process is basically always the same. At first the ingredients are filled into an empty 
bowl at the dosing station. Then the carrier moves the bowl to the mixer. After the 
ingredients were mixed the dough has to rest, which is done on a resting. After rest-
ing, the dough is finished, and is tipped out for the further production process by 
means of the dump tipper. The system has two controllers. Firstly, the carrier control-
ler, which controls the movements of the carrier, secondly, the central controller, 
which supervises and controls the entire system [2]. 
Below, a short overview of the model-based design of the dough production sys-
tem is given. Afterwards the interaction with the virtual commissioning is pointed out. 
In the conceptual design phase as well as in the requirement analysis a functional 
hierarchy was created. At the highest level, the system had to fulfill the function of 
making dough. This function was divided into the sub-functions dose ingredients, 
transport, mix, rest and tip out. In addition, the operator security had to be considered. 
Then an active structure was built. This was functionally structured and modularized. 
From the above described production system modules were formed: carrier, bowl, 
dosing station, mixer and dump tipper as well as control and safety fence. Solution 
patterns could be found for the individual modules because the modules had already 
been used as processing stations in the past. These modules were part of the same 
type of the dough production system, and as the number and arrangement of the sta-
tions depend on the requirements of the dough production system and the available 
space they could be reused. For all modules also behavioral models existed because 
known processing stations could be used. The analysis phase of the design process 
had shown that the carrier did not meet the requirements. The carrier did not allow 
enough bowl changes and this module needed to be newly developed. For the new 
development the behavioral models of the other processing stations already existed, 
so in the conceptual design phase the integration into the entire system could be 
checked. This means that the correct interaction of the various models had already 
been ensured. In the detail design phase, the individual modules were concretized. In 
this case some adjustments to the existing structure were made. System integration of 
the individual models is possible at any time. So after the development of a new carri-
er the old one could be replaced. 
During the detail design the control programs were developed. They could also be 
made of existing software modules. For the control of the new carrier also new pro-
grams were developed. 
After the concretization of the entire solution, the virtual commissioning could be 
performed. Here, both the control of the dough production system and the carrier 
control were tested and the compatibility was checked. Parallel to this, the plant as-
sembly took place. The virtual commissioning provided an early integration testing 
and a reduction of the commissioning time. The benefit of behavioral models in-
creased, as they could be used for the design of new components and for the commis-
sioning simultaneously. 
5 
Summary 
In this paper the benefits of the integration of the virtual commissioning into the mod-
el-based design have been shown. The individual phases, which are passed through, 

32 
T. Schmüdderrich et al. 
have been described in detail. From this context, a procedural model has been  
developed which supports the developer in the virtual commissioning process. The 
different modeling depths which occur in the virtual commissioning were also briefly 
discussed. This classification helps the developer to create the models of the plant 
suitable. Finally, the procedure has been explained on a short industrial example. 
Acknowledgments. This contribution was developed in the project “Methodology for 
virtual commissioning of mechanical engineering systems on basis of object-oriented 
behavioral models” funded by the German Research Foundation (DFG). 
References 
1. Verein Deutscher Ingenieure: Design methodology for mechatronic systems. Beuth Ver-
lag, Berlin ICS 03.100.40; 31.220(VDI 2206) (2004) 
2. Ostersötebier, F., Just, V., Trächtler, A., Bauer, F., Dziwok, S.: Model-Based Design of 
Mechatronic Systems by means of Semantic Web Ontologies and Reusable Solution Ele-
ments. In: ASME (ed.) ASME 2012 11th Biennial Conference on Engineering Systems 
Design and Analysis (ESDA 2012), Nantes, France, July 2-4 (2012) 
3. Barth, M.: Automatisch generierte Simulationsmodelle verfahrenstechnischer Anlagen für 
den Steuerungstest. VDI-Verl., Düsseldorf (2011) 
4. Wünsch, G.: Methoden für die virtuelle Inbetriebnahme automatisierter Produktionssys-
teme. Forschungsberichte IWB, vol. 215. Herbert Utz Verlag GmbH, München (2008) 
5. Zäh, M., Wünsch, G., Hensel, T., Lindworsky, A.: Feldstudie - Virtuelle Inbetriebnahme. 
wt Werkstattstechnik Online 96(10) (2006) 
6. Hoffmann, P., Schumann, R., Maksoud, T.M.A., Premier, G.C.: Virtual commissioning of 
manufacturing systems a review and new approaches for simplification. In: Proceedings - 
24th European Conference on Modelling and Simulation, ECMS 2010, pp. 175–181 
(2010) 
7. Gausemeier, J., Schäfer, W., Anacker, H., Bauer, F., Dziwok, S.: Einsatz semantischer 
Technologien im Entwurf mechatronischer Systeme. In: Gausemeier, J., Rammig, F., 
Schäfer, W., Trächtler, A. (eds.) Wissenschaftsforum 2011. Intelligente Technische Sys-
teme. 8. Paderborner Workshop Entwurf Mechatronischer Systeme, May 19-20. HNI Ver-
lagschriftenreihe, Paderborn (2011) 
8. Pahl, G., Wallace, K., Blessing, L.: Engineering design: A systematic approach, 3rd edn. 
Springer, London (2007) 
9. Kufner, A.: Automatisierte Erstellung von Maschinenmodellen für die Hardware-in-the-
Loop-Simulation von Montagemaschinen. Jost-Jetter, Heimsheim (2012) 
10. Lochbiechler, M., Schmüdderrich, T., Brökelmann, J., Trächtler, A.: Methodology for Se-
lecting the Modeling Depth of Object-Oriented Behavioral Models. In: WASET (ed.) En-
gineering and Technology, Zurich (July 2012) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 33–42. 
DOI: 10.1007/978-3-642-30817-8_4 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Template for Design for eXcellence (DfX) Methods  
Juan Manuel Jauregui Becker and Wessel W. Wits 
Lab of Design, Production and Management, University of Twente,  
P.O. Box 217, 7500 AE, Enschede, The Netherlands 
{j.m.jaureguibecker,w.w.wits}@utwente.nl 
Abstract. Design for eXcellence (DfX) entails a wide range of goal specific de-
sign methods targeting different phases of a product’s lifecycle. These methods 
are often not standardized and sometimes even have contradicting rules among 
them. As a consequence, design processes experience an increase in organiza-
tional entropy. This paper presents a template for DfX methods. The goal is to 
assist the industry in setting up lean design processes. The results of this re-
search are threefold: (1) standardization of DfX design tasks and information 
flows, (2) facilitating the implementation of DfX methods and (3) enable ben-
chmarking of DfX methods to purify design processes. 
Keywords: Design method, Knowledge management, Design for eXcellence 
(DfX). 
1 
Introduction 
Design for eXcellence (DfX) entails a wide range of goal specific design methods. 
The first DfX methods targeted the manufacturability of products and were developed 
after the 2nd world war with the aim of aiding the emerging concurrent design prac-
tices. The three most popular types of methods during this period were Design for 
Manufacturing (DfM), Design for Assembly (DfA) and Design for Disassembly 
(DfD) (all three the topic of the CIRP Dn keynote in 1992 [1]). Since then, most of 
the lifecycle phases of products have been the target of such methods; for instance, 
Design for Maintainability (DfMa), Design for Sustainability (DfS), Design for Obso-
lescence (DfO), Design for Supply Chain (DfSC), Design for Logistics (DfL), Design 
for Network (DfN), Design for Recycling (DfR) and many more. Characteristic to 
DfX methods is that the target is not the functional related performance of the product 
itself, but rather the product development aspects aiming to realize more competitive 
products. DfX methods are often based on designers’ expertise. Its implementation 
depends on the company’s strategies, product design philosophy and available re-
sources (i.e. energy availability, raw materials disposition, staff education level, pro-
duction system characteristics, consumer preferences, etc.). Although DfX methods 
are popular and widely spread in industry, their structure is highly variant as pointed 
out by Chiu and Okudan [2]. Furthermore, the efficiency of DfX methods as a 
benchmark has neither been studied nor addressed, leading to design suggestions with 
unknown consequences for the product lifecycle. 

34 
J.M.J. Becker and W.W. Wits 
 
Presently, established product development procedures in industry are populated 
with large numbers of DfX methods. Most of these methods are in fact developed in  
an industrial setting to support company-specific issues of Product Development 
Processes (PDP) or as protocols for standardizing design approaches and improving 
production parameters. As also concluded by Meerkamm et al. [3], such (industrial) 
methods are not presented in uniform structures and often do not target all steps of the 
design process. This potentially leads to complex concurrent design processes and 
increases a company’s organizational entropy. The core problem is the lack of stan-
dardization of DfX methods in terms of structure and completeness such that tasks 
and information flows are clearly defined. As an answer to this problem, this paper 
presents a template for structuring DfX methods. The template is based on well ac-
cepted design theories. The end goal is to assist industry in setting up lean Product 
Development Processes. By providing a clear DfX structure, we strive for: (1) the 
standardization of DfX procedures and information flows, (2) the integration of DfX 
methods in a systematic and structured manner, and (3) the presentation of criteria for 
benchmarking DfX methods such that companies can purify their DfX procedures. 
All these results target the goal of leaning up design processes.  
The template is the result of an extensive literature review in the fields of DfX 
(Section 2) and the application of design theory and methodology (Section 3). On the 
one hand, DfX related literature misses formal and concise descriptions of the design 
tasks and information types to be included into a DfX method. On the other hand, 
design theory and methodology literature endorses different frameworks that describe 
the basic building blocks of design methods from both an information flow point-of-
view and from a design procedures point-of-view. To fine-tune the template, specific 
DfX methods have been evaluated against the developed template (Section 4). Papers 
are selected from the fields of Design for Manufacturing (DfM) [4-10] and Design for 
Sustainability (DfS) [11-17]. According to Chiu and Okudan [2], DfM and DfS are 
the most mature fields within DfX. The assessment concluded that the template in-
deed captures the required design rationales DfX methods have to contemplate. 
2 
DfX Structure in Literature 
Given the vast amount of literature on this topic, this literature overview presents a 
summarized analysis of DfX review papers. The objective of this overview is to un-
derline how other DfX researchers have addressed the issues of structure and organi-
zation in DfX methods. 
Chiu and Okudan [2] have performed an extensive review of DfX methods. Their 
review includes a mapping between DfX concepts and methods onto the different 
design process phases (i.e. requirements, conceptual design, detail design and evalua-
tion). Also, the interrelation between different DfX methods and the calculation of a 
maturity index that represents the level of development of the different DfX fields 
was studied by them. Regarding the structure, DfX methods are classified into 5 cate-
gories based on the nature of the presented tools. Organized in increasing levels of 
detail, these categories are: guidelines, checklists, metrics, mathematical models and 
overall methods. Guidelines provide directions to be followed; checklists prescribe 
the items that should be taken into account during certain design processes; metrics 

 
A Template for Design for eXcellence (DfX) Methods 
35 
 
are used to quantitatively evaluate how good a product is in relation to its design goal 
(the X in DfX); mathematical models involve validated equations and formulas that 
are used to calculate system performance; finally, overall methods describe clear and 
systematic procedures involving several of the previous categories. This categoriza-
tion describes the type of structures in a DfX method; however it does not express 
which design tasks are addressed by each of the methods nor the order in which the 
tasks should be fulfilled. 
Meerkamm et al. [3] treat the structure of DfX in two ways. The first is as the rela-
tion between different DfX methods within the concurrent design philosophy. The 
second is in relation to the form of the DfX method. For example, if the method is 
made out of guidelines or in the form of checklists. There is however no statement 
about the design procedures and information flow within the method, nor how this 
flow is organized within a concurrent design. 
Kuo and Zhang [18] present a review of concepts, applications and perspectives in 
DfX. A large number of methods is reviewed and best practices are summarized. 
However, there is no unified structure for framing the different procedures and infor-
mation flows that each method entails. 
Huang [19] proposes a framework (the DfX shell) that can be used to develop DfX 
tools quickly and with consistent quality. The framework consists of 7 steps that take 
the method developer from analysing the method’s requirements through documenta-
tion and method verification. Tichem [20] developed a tool to support DfM, DfA and 
DfD during the conceptual design phase. The tool supports three main design func-
tions, namely, product and lifecycle modelling, decision making and design coordina-
tion. From all reviewed literature, both cited references (i.e. 19 & 20) are the closest 
to presenting a structure for DfX methods. However, neither of both addresses the 
required design procedures and information flows in a systematic way. 
3 
Design Tasks and Information Flow 
In general, the design process can be divided into 4 main phases [21]: (1) planning 
and clarifying the task; (2) conceptual design; (3) embodiment design; and (4) detail 
design. The first phase regards the problem statement, while the last phase aims at 
planning the manufacturing. Both are organizational processes. It is in the conceptual 
and embodiment design phases where the artefact is actually designed. Both phases 
are accomplished by following 4 basic design tasks: synthesis, analysis, evaluation 
and adjustment [22]. These 4 tasks are recursively invoked during design and the 
order in which they are organized defines the design strategy that is used. The synthe-
sis task transforms a set of input requirements into a candidate solution. The analysis 
task calculates (either quantitatively or qualitatively) the solution’s performance. The 
resulting performances and solutions are evaluated in order to decide whether to mod-
ify, reject or accept the candidate solution. The adjustment task is applied when the 
quality of a candidate solution can be improved by small alterations. 
The types of information flowing among design tasks can also be categorized into 
3 groups according to their role within the design process, namely embodiment, sce-
nario and performance [22]. Embodiment regards the set of parameters describing the 
design object; for instance, its topology or its (material) properties. Scenario is related 
to the set of entities describing the flow of energy, mass or information the  

36 
J.M.J. Becker and W.W. Wits 
 
embodiment is exposed to. Finally, performance determines how the embodiment 
behaves under a certain (group of) scenario(s). Several design frameworks in acade-
mia (e.g. McMahon [23] and Webber [24]) have classified design information in these 
3 groups as the information per group is used distinctly different during design. 
Under this view, analysis tasks quantify and/or qualify the performance of an em-
bodiment undergoing a given scenario. Vice versa, synthesis tasks are the process of 
specifying embodiment parameters such that they meet certain performance values for 
a given scenario. Both process flows are shown in Figure 1. 
 
   
 
Fig. 1. Information flow for analysis and synthesis processes 
Standardization of design tasks and information flow has been studied especially in 
the field of computational synthesis as this is fundamental for design automation (e.g. 
the work of [25] and [26]). A template for DfX must have methods to support each 
design task (i.e. synthesis, analysis, evaluation and adjustment) and a classification of 
the information type (i.e. embodiment, scenario and performance). Together they 
guarantee the structure and completeness of the DfX method. The structure describes 
which method to use for which task together with the information flow of each me-
thod. Completeness describes whether all design tasks are supported by the DfX  
method. The latter is not required; however, designers should be aware of the com-
pleteness of the methods they use. 
4 
The Design for X Template 
The developed DfX template consists of 3 main parts: the strategy declaration, the 
information type declaration and the design task support method. As shown in the 
taxonomy of the DfX template in Figure 2, the information declaration is further clas-
sified into 3 groups: embodiment, scenario and performance; while the design task 
support method is classified into 4 groups: synthesis, analysis, evaluation and adjust-
ment.  
The flow of information according to their type can be integrated with the 4 design 
task support methods, as shown in Fig. 3. As the figure shows, the DfX template also 
distinguishes between known initial information and design emergent information. 
The following subsections elaborate on this DfX template and present examples from 
existing DfX methods in literature to demonstrate the use and viability of this  
template. 

 
A Template for Design for eXcellence (DfX) Methods 
37 
 
 
Fig. 2. Taxonomy of the DfX template 
 
Fig. 3. Information flow and design task support in DfX template 
4.1 
Strategy Declaration 
DfX methods are inspired and developed by taking into account company specific 
strategies. These strategies serve as general goals that drive company rules and guide-
lines. Examples of such strategies are, for instance, to save energy, to save resources, 

38 
J.M.J. Becker and W.W. Wits 
 
to minimize manufacturing process types or to minimize complexity. Strategies 
emerge from a company’s strategic plan. For example, one strategy could be DfM 
with the goal of reusing as many existing production machinery as possible, while for 
another company DfM entails outsourcing as many processes as possible. Das et al. 
[27] present a strategy where DfX is defined as an approach for designing a product 
quickly, with low manufacturing cost, a minimum number of processes and handling 
requirements, and attains its designed level of quality. 
4.2 
Information Declaration 
Information declaration aims to obtain a map of the parameters that play a role in the 
DfX method and help designers determine if the method suits their problem. The 
declaration, according to the template, is done by defining both the parameter names 
and their type. Next, the 3 information types are discussed. 
Embodiment. In DfX, embodiment parameters are the properties of the artefact being 
designed upon which a design decision has to be made. Depending on the scope of the 
method, embodiment parameters describe the geometry, topology and material cha-
racteristics of the artefact. For DfX, the embodiment information declaration does not 
change compared to the original declarations. 
Scenario. Within the scope of DfX, scenario parameters describe the boundary condi-
tions for which the method is developed. For instance, in the case of a DfM method, 
manufacturing conditions would give input to the scenario parameters. As such, sce-
nario parameters describe, among others, the characteristics of the available machine 
processes in a given industrial setting. 
Performance. In DfX, performance is directly related to either a maximization or 
minimization of the X in the method. Performance has different levels of perception: 
product, system and eco-system [2]. For DfX, performances are related to measure-
ments of material resources usage, energy usage or time usage. For instance, the “uni-
versal virtue” areas described by Fabricius [4]: cost, quality, flexibility, risk, lead 
time, efficiency and environment. 
4.3 
Design Task Support Method 
Design task support methods for DfX consist of knowledge rules, procedures and 
information flows. The first are expressed in different forms; for example, equations, 
guidelines and checklists. Procedures determine the way in which knowledge rules 
are used and when and in what order a method should be executed. The information 
flow is discussed for each design task hereafter. 
 
Synthesis Design Task. According to the template, the synthesis design task support 
method aims at aiding the designer in developing high quality solutions at low design 
efforts. The synthesis task is supported by design procedures and knowledge rules. As 
shown in Figure 3, the input information of the synthesis task is a set of known scena-
rio specifications and desired performance requirements. 

 
A Template for Design for eXcellence (DfX) Methods 
39 
 
Design procedures determine the logical order in which knowledge rules should be 
applied. This has the aim of avoiding inconsistencies and improving design  
efficiency. 
Knowledge rules of a synthesis support method are termed as design rules. They 
serve as shortcuts that determine the embodiment of a design given a certain scenario 
while aiming at sufficient performance. Design rules are often the result of (past) 
design experience. 
Design rules need to be treated carefully as they impose restrictions to the solution 
space and have the ability to constrain more creative solutions. This can be managed 
by properly documenting its rationales such that modifications can be easily carried 
out when new insights are developed. Design rules in DfX methods are typically ex-
pressed in the form of guidelines.  
Examples of such rules from the field of DfM are “minimize the variety of parts”, 
“simplify the structure” and “use standard parts” [5]; typical from the field of DfS are 
“reduce use of energy”, “reduce emissions” and “Increase amount of recyclable mate-
rials” [11]. 
 
Analysis Design Task. Also in the DfX template, the analysis design task support 
method has an inverted effect compared to the synthesis method where scenario and 
performance parameters help define the embodiment variables. The analysis support 
method enables the designer to calculate the performances of a designed embodiment 
within the scope of the DfX target scenario. Contrary to the synthesis task, analysis is 
based on models (e.g. factual, analytical, numerical, etc.) and the analysis results 
represent reality (either qualitatively or quantitatively). As DfX focuses on issues that 
are related to the product lifecycle rather than the product functionality itself, analysis 
methods tend to be qualitative and the synthesis support method tends to be based on 
guidelines. 
Knowledge rules in analysis support methods are termed design equations. Design 
equations can be represented using different mathematical models (e.g. logic models, 
fuzzy models, algebraic models or look-up table models). Analogue to synthesis, 
analysis procedures determine how analysis equations are used to determine the per-
formance of an embodiment. As analysis methods do not embed complex decision 
making processes, they can be automated by computer software. 
Analysis equation can in the field of DfM, for instance, be formulated using histor-
ical records of manufacturing companies including figures for material, tooling, set up 
and labour cost as it is described by Goncalves-Coelho and Mourao [7]; in the field of 
DfS equations are often based on Life Cycle Analyses. Also, the eco-functional ma-
trix presented by Short and Lynch in [15] is an exemplary analysis method. 
 
Evaluation Design Task. The goal of the evaluation support method is to offer the 
designer a clear path to determine whether an obtained design solution should be ac-
cepted, adjusted or dismissed. Evaluation is often delegated to the designer’s judge-
ment. However, for the aim of using DfX as a tool for standardization, it is important 
to have uniform evaluation criteria that guarantee corporate quality standards. There-
fore, it is important to include evaluation design tasks into the template as a formal 
sub-method. 

40 
J.M.J. Becker and W.W. Wits 
 
Knowledge rules in evaluation support methods are termed evaluation criteria. 
Evaluation criteria are presented in the form of logic relations and, in DfX, they are 
often found as checklists. 
Evaluation procedures define the order in which the criteria have to be evaluated. 
In case the evaluation process determines to continue with an adjustment task to bring 
about improvements to the obtained solution, protocols have to prescribe how the 
target embodiment variables are identified and what their goal is during the adjust-
ment task. 
For instance, Van Vliet and Van Luttervelt [10] constructed a clear procedure us-
ing IF-THEN statements for their DfM method. Performance parameters are eva-
luated and the design is subsequently steered in the appropriate direction. 
 
Adjustment Design Task. Adjustment support methods support designers in bringing 
improvements to existing solutions. The input to an improvement process is a set of 
embodiment parameters. Knowledge rules are termed as adjustment rules. Similar to 
design rules, they specify the embodiment as a function of scenario and performance 
parameters. The adjustment procedures determine which adjustment rules apply de-
pending on the targeted embodiment parameters.  
Typical examples of adjustment rules for DfM are “Wrong” and “Right” represen-
tations of a designed component, for instance for casting components [8]. 
5 
Summary 
This paper presents a DfX template as the result of researching the structure of DfX 
methods. The research was motivated by the need to standardize DfX information 
flow and design support methods. The template strives for both structure and com-
pleteness. This would avoid a further increase in design process entropy. First, a lite-
rature review was carried out to search for existing frameworks within DfX literature. 
As no formal frameworks were identified, a literature study of existing design frame-
works in the field of design theory and methodology was performed. The results were 
used to propose a template that incorporates all relevant rationales of DfX. To finish, 
DfX methods in literature were assessed and projected onto the template to determine 
its feasibility. 
 
Acknowledgements. The authors like to acknowledge the support Dr. Tim Short for 
complementing to the conducted literature study. 
References 
1. Boothroyd, G., Alting, L.: Design for Assembly and Disassembly. CIRP Annals - Manu-
facturing Technology 41(2), 625–636 (1992) 
2. Chiu, M.-C., Okudan, G.: Investigation of the Applicability of Design for X Tools during 
Design Concept Evolution: A Literature Review. International Journal of Product Devel-
opment (September 2010) (in press) 

 
A Template for Design for eXcellence (DfX) Methods 
41 
 
3. Meerkamm, H., Koch, M., Clarkson, J.: Design for X. Design process improvement, pp. 
306–323. Springer, London (2005) 
4. Fabricius, F.: A seven step procedure for design for manufacture. World Class Design for 
Manufacture 1(2), 23–30 (1994) 
5. Boothroyd, G.: Product design for manufacture and assembly. Computer Aided De-
sign 26(7), 505–520 (1994) 
6. Boothroyd, G., Radovanovic, P.: Estimating the Cost of Machined Components During the 
Conceptual Design of a Product. CIRP Annals - Manufacturing Technology 38(1), 157–
160 (1989) 
7. Goncalves-Coelho, A.M., Mourao, A.J.F.: Axiomatic design as support for decision-
making in a design for manufacturing context: a case study. Int. J. Production Econom-
ics 109(1-2), 81–89 (2007) 
8. Huang, G.Q., Mak, K.L.: Design for manufacture and assembly on the internet. Computers 
in Industry 38(1), 17–30 (1999) 
9. La Trobe-Bateman, J., Wild, D.: Design for manufacturing: use of a spreadsheet model of 
manufacturability to optimize product design and development. Research in Engineering 
Design 14(2), 107–117 (2003) 
10. van Vliet, H.W., van Luttervelt, K.: Development and application of a mixed prod-
uct/process-based DFM methodology. Int. J. Computer Integrated Manufacturing 17(3), 
224–234 (2004) 
11. Ljungberg, L.Y.: Materials selection and design for development of sustainable products. 
Materials and Design 28(2), 466–479 (2007) 
12. Gu, P., Hashemian, M., Nee, A.Y.C.: Adaptable design. CIRP Annals – Manufacturing 
Technology 53(2), 539–557 (2004) 
13. Kasarda, M.E., Terpenny, J.P., Inman, D., Precoda, K.R., Jelesko, J., Sahin, A., Park, J.: 
Design for Adaptability (DFAD) – a new concept for achieving sustainable design. Robot-
ics and Computer-Integrated Manufacturing 23(6), 727–734 (2007) 
14. Waage, S.A.: Re-considering product design: a practical ‘road-map’ for integration of sus-
tainability issues. J. of Cleaner Production 15(7), 638–649 (2007) 
15. Short, T.D., Lynch, C.A.: Beyond the eco-functional matrix – design for sustainability and 
the Durham methodology. In: Design and Manufacture for Sustainable Development 
Conf., Loughborough, UK, September 1-2 (2004) 
16. Howarth, G., Hadfield, M.: A sustainable product design model. Materials and De-
sign 27(10), 1128–1133 (2006) 
17. Ijomah, W.L., McMahon, C.A., Hammond, G.P., Newman, S.T.: Development of robust 
design-for-remanufacturing guidelines to further the aims of sustainable development. Int. 
J. of Production Research 45(18-19), 4513–4536 (2007) 
18. Kuo, T.C., Zhang, H.-C.: Design for manufacturability and design for “X”: concepts, ap-
plications, and perspectives. In: 17th IEEE/CPMT International. Manufacturing Technolo-
gies - Present and Future, pp. 446–459 (1995) 
19. Huang, G.Q.: Towards a generic Design for X (DfX) shell. In: Gill, R., Syan, C.S. (eds.) 
Proceedings of 12th International Conference on CADCAM, Robotics, and Factories of 
the Future, pp. 940–945. Middlesex University, London (1996) 
20. Tichem, M.: A design coordination approach to design for X. Delft University of Technol-
ogy, Autores (1996) 
21. Tomiyama, T., Gul, P., Jin, Y., Lutters, E., Kind, C., Kimura, F.: Design methodologies: 
Industrial and educational applications. CIRP Annals 58(2), 543–565 (2009) 

42 
J.M.J. Becker and W.W. Wits 
 
22. Schotborgh, W.O., McMahon, C.A., van Houten, F.: A knowledge acquisition method to 
model parametric engineering design processes. Int. J. Computer Aided Engineering and 
Technology (2012) (paper accepted for publication) 
23. McMahon, C.A.: Observations on modes of incremental change in design. Journal of En-
gineering Design 5(3), 195 (1994) 
24. Weber, C.: CPM/PDD – an extended theoretical approach to modelling products and prod-
uct development processes. In: Proceedings of the 2nd German-Israeli Symposium on Ad-
vances in Methods and Systems for Development of Products and Processes, pp. 159–179. 
Fraunhofer-IRB-Verlag, Stuttgart (2005) 
25. Cagan, J., Campbell, M.I., Finger, S., Tomiyama, T.: A framework for computational de-
sign synthesis: Model and applications. Journal of Computing and Information Science in 
Engineering 5(3), 171–181 (2005) 
26. Jauregui-Becker, J.M., Tragter, H., van Houten, F.J.A.M.: Structure and models of artifac-
tual routine design problems for computational synthesis. CIRP Journal of Manufacturing 
Science and Technology 1(3), 120–125 (2009) 
27. Das, S.K., Datla, V., Samir, G.: DFQM - An approach for improving the quality of assem-
bled products. International Journal of Production Research 38(2), 457–477 (2000) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 43–50. 
DOI: 10.1007/978-3-642-30817-8_5 
© Springer-Verlag Berlin Heidelberg 2013 
 
Enhancing Interpretation-Quality of Requirements Using 
PLM Integrated Requirements-Communication in Cross 
Company Development Processes 
Martin Rebel1, Jörg W. Fischer2, Armin Haße2, and Cees Michielsen3 
1 Siemens Industry Software GmbH & Co. KG, Nonnendammallee 101,  
Berlin, 10555, Germany 
2 Siemens Industry Software GmbH & Co. KG, Liebknechtstr. 35, Stuttgart, 70565, Germany 
3 R&D Studio b.v., Valkenlaan 15, SG Maarheeze, 6026, The Netherlands 
{martin.rebel,joerg.fischer,armin.hasse}@siemens.com, 
cees@rnd-studio.net 
Abstract. Fulfilling customer requirements significantly influences product 
success. Improving the quality of documentation is a primary objective of  
requirements engineering and management. Despite properly outlined require-
ments however, it is very common that mistakes emerge throughout the product 
development lifecycle due to diverging interpretations. Because if this enhanc-
ing the quality of requirements documentation to ensure a common understand-
ing is increasingly the focus of quality improvement considerations. In response 
to this need, an approach was developed which uses Secure Information Trans-
formation from Input to Output (SITIO) as an aspect of PLM integrated  
requirements management to provide a sustainable enhancement of interpreta-
tion-quality through linguistic pragmatics. 
Keywords: Requirements Management, Quality, PLM, Product Lifecycle 
Management, SITIO. 
1 
Prerequisites for Product Success 
Sustainable customer satisfaction is only achievable if customer requirements are 
fulfilled. Therefore, the success of a product critically depends on conformity to de-
fined and fulfilled requirements. Companies are now realizing that, due to the increas-
ing complexity of their products, the inevitable integration of different domains (e.g. 
mechanics, electrics and electronics) as well as necessary global collaboration, an 
exact design can only be achieved by superior quality of requirements documentation 
throughout the entire product development process. 
In order to completely and correctly implement design requirements, these re-
quirements must be clearly identifiable, easily and completely understood as well as 
correctly interpreted by the product developer responsible for realization. The process 
of ensuring consistent requirements interpretation is becoming increasingly compli-
cated due to the growing trend of decentralized product creation. Because of this, 
current requirements definition methods are no longer sufficient. 

44 
M. Rebel et al. 
Therefore, new methods and IT tools, which offer adequate support for the 
processes of defining requirements, their distribution within and between companies 
and implementation of requirements are imperative for product success. 
2 
Transformation of Requirements in the Course of the 
Product Development Process 
A compilation of all requirements for a new product is the first result in the develop-
ment and construction process according to VDI-Richtlinie 2221 and 2222. Usually, 
requirements are verbally formulated up to this point. Kramer [1] proposes to divide 
the specification of requirements into three levels of granularity: stage 1 (statement), 
stage 2 (consolidation) and stage 3 (detailed definition). 
Kramer‘s proposal also broadly matches the actions of the automotive industry. 
Within the automotive industry, customer requirements – typically requirements from 
a marketing perspective – are at first derived into specific properties and functions. In 
automotive engineering, requirements on fuel consumption, safety or spacing would 
be represented on a vehicle level. Subsequently, properties and functions are then 
detailed through different levels of granularity down to the level of an individual 
component (fig. 1, right side of pyramid). 
The process of refinement thereby occurs across different departments and even 
companies – due to globally distributed suppliers. It cannot be assumed that commu-
nication between departments and suppliers refers only to a single defined level of 
granularity (fig. 1, total). 
 
 
Fig. 1. Typical levels of granularity of the automobile industry 
Marketing
System
Subsystem
Component
Vehicle
OEM
Supplier
company border
Granularity
department
border

 
Enhancing Interpretation-Quality of Requirements 
45 
Until a detailed and final definition of a product through requirements is achieved, 
several steps of transforming information across the dimensions “granularity” and 
“department or company boarders”, during which requirements are verbally formu-
lated, explained, discussed and subsequently detailed, are necessary. Within each of 
those transformational steps there is a risk that requirements are misinterpreted, fun-
damental elements are missing, or some context is lost. 
If requirements quality cannot be ensured between each transformational step, a 
high degree of customer satisfaction is hardly achieved. 
3 
Quality of Requirements 
Consequently, it is necessary to question the meaning of “quality of requirements”. In 
general, quality is the degree to which a sum of inherent characteristics fulfills the 
stipulations that are postulated for an object [2]. Today, requirements management 
essentially focuses on completeness and formal correctness of requirements. ISO 
9000:2005 constitutes that high-quality requirements are achieved through adequacy, 
completeness, unambiguousness and consistency as well as through the ability to 
verify them. The VDA-standard with a proposal for a structure for component  
requirement specifications takes this one step further. The VDA-standard [3] lists 
unambiguousness, identifiability, traceability, necessity, non-redundancy, comprehen-
sibility, completeness and consistency as significant criteria. 
The aforementioned requirement characteristics often results in a demand to for-
mulate more extensively detailed requirements than previously practiced. This can 
lead to higher levels of effort and therefore, to an increasing need for resources during 
requirements documentation. In order to counter-balance this, another approach for 
automatic test requirements would be necessary. Technically, this is done by checking 
requirements on so-called “weak” and “stop” words. Thereby, the primary goal is to 
ensure unambiguity by avoiding fuzzy wording like could, should, or would for  
example. 
The described procedure to ensure high-quality requirements is not sufficient since 
a significant weakness still exists. This can be recognized by considering the commu-
nication model (fig. 2 ). 
Previous approaches focus on the sending end (fig. 2, detail A). These approaches 
are formulated on the idea that it is only necessary to stipulate complete and correct 
requirements and that the receiving end will automatically interpret them correctly. 
However, practice shows that correct interpretation of formulated requirements is an 
essential problem in requirements management (fig. 2, detail B). 
An additional problem results from the existing mindset: compilation and interpre-
tation of detailed requirements costs resources and time. In most cases, especially in 
well-rehearsed development teams, experience shows that it is not necessary, perhaps 
even cumbersome, to formulate requirements in every detail. While interpreting such 
requirements, employees are losing time by evaluating information that is already 
known. 

46 
M. Rebel et al. 
Hence, the goal must be to find approaches for requirements management which 
succeed in dramatically enhancing interpretation-quality of requirements while flexi-
bly adjusting the level of detail for formulation of requirements on the receiving end. 
 
 
Fig. 2. Communication Model according to Shannon & Weaver [4] 
4 
Aspects of Linguistic Pragmatics in Requirements 
Management 
This problem focuses on linguistic pragmatics. In essence, the context of a person that 
interprets or encodes information also has to be considered in requirements manage-
ment. This context can be embraced with the personal frame of reference of a person. 
The personal frame of reference describes how a person views and interpretes the 
world and the information he consumes. This is partly dependent on the learned 
schema in which information is internally filed. It also depends on one‘s point of 
view, which results from the role in which the person is acting within the interpreta-
tion [5]. Based on the employees in the product development process, the relevant 
frame of reference is basically illustrated in the aspects represented in figure 3. Alter-
nately, it is contingent on aspects of the learned scheme, which includes one’s native 
language, cultural context, and accumulated work experience (fig. 3, point A). It can 
also be driven by aspects of the role that a person occupies in a company (fig. 3, point 
B). The personal frame of reference is mainly influenced by the professional domain 
(mechanical, electrical / electronics, mechatronics), the role and the associated tech-
nical issue. The analysis and classification of influences of the frame of reference is a 
complex issue that necessitates further research in the context of requirement man-
agement. However, it can be stated that people who exchange requirements must 
match their frames of reference in order to interpret them correctly (for example 
through conversation and documentation). The design of the previously described 
SITIO method raised the question: how can this aspect be easily implemented in re-
quirements management? The aim is to adjust and anchor the frame of reference in 
the methodology as well as take advantage of existing highly matching frames of 
references of persons in the process chain so that in such cases, the effort in formulat-
ing requirements decreases with a simultaneous increase in quality. 
Usually not considered (B)
So far considered (A)
Source of
information
Sender
Information
Source of
interference
Receiver
Destination of
information
Information

 
Enhancing Interpretation-Quality of Requirements 
47 
 
 
Fig. 3. Relevant aspects of the personal frame of reference under considerations of the product 
development process 
5 
Secure Information Transformation from Input to Output 
In the context of industrial research, the authors have developed a methodical ap-
proach, which incorporates an all-supported integration of requirements management 
into an open PLM platform, such as a Teamcenter. A particular overvalue of such 
integration is the system supported reproducibility of the dependencies of require-
ments among themselves and their practical implementation in a component design or 
an actual construction. This allows a quick and inexpensive handling of changes of 
the product without delay and interference of the system or break in communication. 
One aspect of this methodology is SITIO (Secure Information Transformation from 
Input to Output). It focuses on ensuring the quality of requirements transformation on 
the dimensions of "granularity" and "departmental or enterprise boundaries". Through 
the use of SITIO, it can be ensured that the participants in the process chain maintain 
a common understanding during all transformation steps of requirements. 
Figure 4 shows the process of a transformation of a formulated requirement. Per-
son A formulates the document of requirements. This input document is passed to 
person B for further detailing. Person B transforms the requirements to a more de-
tailed level of granularity. Then, person B passes the resulting output document to the 
successor in the process chain (comp. in figure 4, person C) who, for example, ac-
complishes the construction task. In recent requirements management, the quality is 
essentially ensured by tests, which by SITIO are considered by realizing the process 
step "Craftsmanship". In this process step, (fig. 4, ) [6] a responsible person from 
the department of person B tests the output document on formal compliance with 
agreed quality standards and compliance with the established process flow. The focus 
is on reviewing on non-redundancy and consistency of the by person B prepared in-
formation. SITIO adds two other elements to the process: "Fitness-for-use" (fig. 4,  
& ) and "Conformance" (fig. 4, ). The step "Fitness-for-use" (fig. 4, ) focuses 
on the correct interpretation of the input document by person B. This person confirms 
Native language
Cultural context
Qualification
Accumulated work
experience
Enterprise philosophy
Domain/Department
Role
Associated technical issue
Aspect (A) 
From learned schema
Aspect (B)
From role in company
Personal frame of reference

48 
M. Rebel et al. 
which points of the document are in his view fully understandable and reflects his 
understanding in an outcome document or in a work meeting with person A. Person A 
contrasts his original intention with the understanding of person B. If Person A be-
lieves that person B has understood all requirements of the input unambiguously, 
completely and understandably; person A releases the information for actual 
processing by person B. If the input document displays any weaknesses, the input has 
to be reviewed by the respective author of the information until all weaknesses are 
eliminated. 
In succession, person B conducts the actual transformation process, by, for exam-
ple, specifying the requirements of the input document to a finer level of granularity 
and by recording the results in an output document. The first step to release the so-
created document, as described above, is the step "Craftsmanship". To ensure that the 
output document properly reflects all requirements that are specified in the input  
document, the step "Conformance" (fig. 4, ) follows. Here, person A examines the 
output document accordingly. Only then, the output-document is released and trans-
mitted to the follow-up activity and the procedure described begins again. By the two 
elements "Fitness-for-use" (fig. 4,  & ) and "Conformance" (fig. 4, ), a review 
of the requirements on the receiver side is realized. Moreover a benefit is the precise 
assignment of the organizational responsibility for the process steps. Thus, person A 
cannot withdraw on having described all necessary information in the input  
document. She also has to take the responsibility that person B has understood the 
statements correctly. 
Fig. 4. Basic concept of SITIO 
6 
IT-Supported SITIO Process and the Use of Personal Frame 
of Reference 
At the first glance, the introduction of additional steps in requirements management 
through SITIO appears to be more time consuming, especially in the context of the 
currently prevailing tendency to demand more detailed definition of requirements. 
However, upon closer inspection, it becomes apparent that an additional expense is 
already justified by ensuring requirements are of high quality and by avoiding subse-
quent errors. The two process steps "Fitness-for-use" and "Conformance" are already 
used in the industry because an adjustment to match the frame of reference of the 
Input-
Docu-
ment
Person A
Output-
Docu-
ment
Person B
Person C
Fitness-for-use
Fitness-for-use
Craftmanship
Conformance
1
2
4
3

 
Enhancing Interpretation-Quality of Requirements 
49 
participants is essential for a proper understanding of the requirements. However, 
these are implicit processes which are only conducted by sheer necessity and driven 
by the participants themselves. If the transformation of requirements is implemented 
on a IT basis by means of SITIO, the formerly implicit communication processes are 
then explicit. This results in the organizational systemization of communication 
processes being based on defined workflows, which are handled by the PLM platform 
and are traceable and transparent. 
In addition, effects of decreased effort by matching frames of references could be 
used and anchored in the company by implementing SITIO. Such an implementation 
could lead to a cost and effort reduction when compared to the previous process. 
The implementation is realized, for example, with the open PLM platform Team-
center. The demonstrated process flow in figure 4 is modeled by Teamcenter through 
the "workflow engine". Thus, the communication, the document exchange and release 
of documents are realized directly through the system. It is thereby possible to main-
tain auditable traceability (i.e. the link and Traceability) between input and output 
documents at subchapter and paragraph level on the level of revisions. 
The steps "Fitness-for-use" and "Conformance" account for the effect of effort  
reduction from already existing common frames of references. Teamcenter allows 
mapping roles and organizational classification together with the qualifications of the 
participants in requirements management, so that during the procedure of transforma-
tion an analysis of participants can be conducted and subsequently the degree of 
commonality of the frames of references can be analyzed. 
Because of the analyzed overlap of the frames of reference, it is possible to provide 
different documentation templates based on the existing frames of reference. The 
various documentation templates require a contextual detail of the requirement docu-
mentation so that unnecessary, because already known, information can be omitted. In 
this way, the communication effort between the parties of process chain at elevated 
quality of the information transformation could be reduced accordingly. As a result, 
the processes accelerate and therefore, reduce costs while increasing quality. 
7 
Summary 
Through the use of SITIO information loss and distortion of information in the 
process of requirement transformation can be minimized. 
Currently used criteria to maintain quality were expanded through integration of 
"Conformance" and "Fitness-for-use", the process will be systematized and embedded 
in the organization. 
The transformation of requirements is bundled and can be system-based imple-
mented by means of open PLM platform, such as Teamcenter. 
The work effort for the preparation of requirements can be reduced by the utiliza-
tion of the effects of the personal frame of reference while the quality of requirements 
increases. Therefore, SITIO provides a methodological tool for requirements  
management, which improves the process. 

50 
M. Rebel et al. 
References 
1. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Konstruktionslehre – Grundlagen erfol-
greicher Produktentwicklung – Methoden und Anwendung. Springer, Heidelberg (2005) 
2. DIN EN ISO 9000:2005: Qualitätsmanagementsysteme – Grundlagen und Begriffe. Dreis-
prachige Fassung. Beuth Verlag, Heidelberg (2005)  
3. VDA/QMC: Das gemeinsame Qualitätsmanagement in der Lieferkette – Automotive VDA-
Standardstruktur Komponentenlastenheft. 1. Auflage (2007) 
4. Shannon, C.E., Weaver, W.: The mathematical Theory of Communication. University of 
Illinois Press (1949)  
5. Gabriel, P.: Personal Transformation: The Relationship of Transformative Learning Expe-
riences and Transformational Leadership. Washington University (2008)  
6. Deming, E.W.: Out of the Crisis. MIT Press, Cambridge (1986) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 51–60. 
DOI: 10.1007/978-3-642-30817-8_6 
© Springer-Verlag Berlin Heidelberg 2013 
 
PLM-Centered Support of Active Virtual Customer 
Integration into the Product Creation Process  
Thomas Damerau, Haygazun Hayka, and Rainer Stark 
Division Virtual Product Creation, Fraunhofer Institute for Production Systems and Design 
Technology, Pascalstr. 8-9, Berlin, 10587, Germany 
{thomas.damerau,haygazun.hayka,rainer.stark}@ipk.fraunhofer.de 
Abstract. Within the last decades the position of customers has changed from a 
passive recipient to an active co-designer in the value creation. Successful in-
novators use competence within an extended network which particularly in-
cludes the competence of customers. Therefore the ability to allow information 
about customers and their needs to flow into the process of product creation is 
decisive. But todays methods and IT-Tools often do not achieve the desired  
results in innovation projects. In order to improve the active virtual customer in-
tegration into the process of product creation; this paper presents a Product Li-
fecycle Management (PLM)-centered research environment with a ubiquitous 
mobile frontend. A process outline for the provision of product descriptive data 
from a PLM system to a mobile device and the information return, back into the 
product creation process is included. 
Keywords: Virtual customer integration, Open innovation, Product Life Cycle 
Management, Mobile computing, Research environment. 
1 
Introduction 
Due to the ongoing globalization and the shortening of the product life cycles, the 
pressure increases on companies to improve their innovation processes. Technological 
change has reduced both, the development time of products and their life massively, 
which is very clearly seen in products from consumer technology and the automotive 
industry. Due to the dynamics of global trade high-wage countries, post-industrial 
societies need to compensate for the disadvantages of location compared to low-wage 
countries through knowledge and innovation. Gary Hamel summarizes this difficulty:  
 
“We’ve reached the end of incrementalism. Only those companies that are capable of 
creating industry revolutions will prosper in the new economy." 
 
The focus in innovation research, therefore, no longer fails to occupy the role of inno-
vation as a driver of growth and profitability. This was clearly proven by the science, 
e.g. Thomke [1]. Now it is important to explain how innovation occurs and how inno-
vation processes can be optimized. A key approach is based on the paradigm shift to 

52 
T. Damerau, H. Hayka, and R. Stark 
open the innovation process and to integrate external perspectives and participants. 
For this purpose, the term open innovation was formed by Chesbrough [2], [3]. 
Within the last decades the position of customers has changed from a passive reci-
pient to an active co-designer in the creation of value. Successful innovators use com-
petence within an extended network which particularly includes the competence of 
customers [4], [5]. Numerous studies have shown that early integration of customers 
in the development process can increase the success of innovations clearly. In many 
industries, it is the customers who are responsible for the most successful develop-
ments [6]. Therefore, the ability to allow information about customers and their needs 
to flow into the process of product creation is decisive [4]. 
In this context, numerous methods (e.g. lead user) and tools (e.g. user toolkits, vir-
tual customer integration platforms (VCI) and virtual customer environments (VCE)) 
have been developed to integrate both, in-house and external customers in the innova-
tion process, respectively the product development process. But as Schroll [7] points 
out those methods and IT-Tools often do not achieve the desired results in innovation 
projects. So it is necessary to reconsider existing approaches. 
This paper describes the current state of ongoing research work which has set itself 
the goal to establish a link between the so far singular considered innovation and 
product creation processes and to reduce the existing deficits of customer integration 
tools. For this purpose the development of a ubiquitous active customer integration 
environment which is directly integrated with the product creation IT-Backbone, the 
Product Life Cycle Management (PLM) system is presented. 
After a general introduction in chapter 1, chapter 2 gives an overview of existing 
relevant solutions and the roles of customers in the customer integration. In chapter 3, 
the research environment is presented. Chapter 4 provides a summary and an outlook. 
1.1 
Challenges 
The aim of the so-called virtual customer integration (VCI) platforms is the collection 
of need information and implicit and explicit solution knowledge in all innovation 
process phases. This is achieved by the digital implementation of the methods of cus-
tomer integration in a fully virtual environment like an internet platform. As Rohr-
beck et. al. [4] showed, in the year 2010 thirteen out of the Euro Stoxx 50 companies 
had established a VCI platform. The state of the art (see chapter 2), however, has 
many shortcomings concerning methodical, organizational and technical aspects as 
well, both from a customer perspective and from a business perspective. 
Nowadays for a potential co-designer there are many barriers to cross before par-
ticipating in the innovation process. On the basis of an already existing product the 
customer first needs to identify the product (e.g. Golf V). If it’s the case that different 
configurations and/or variants of the product exist, a bijective identification is the best 
case (e.g. Golf V, station wagon, edition Rolling Stone, build year 2007, manufac-
tured by Volkswagen AG) to ensure that the customer feedback or contribution can be 
associate with the right product data. But now, the customer needs to look-up a com-
pany-specific and adequate feedback channel (e.g. feedback formula on website) 
which is in best-case a VCI platform that supports the innovation process phase the 

 
PLM-Centered Support of Active Virtual Customer Integration 
53 
customer wants to contribute to (e.g. idea generation for a next generation infotain-
ment system), not knowing if such channel exists. In a last step the customer de-
scribes his idea or feedback, in best-case using structured input fields or toolsets that 
offer certain degrees of freedom to virtually design ideas. 
In the described case, customer-generated data is largely unstructured, leaves room 
for interpretation, and is not consolidated and not assessed. Customer's innovation 
potential remains untapped because the customer has only a few additional inputs 
obtained for its innovating activities. For example interdisciplinary discourse and the 
provision of technical background information e.g. drawings are missing in this 
process. According to studies, these are particularly useful for the generation of ideas 
[8]. 
If large groups of people shall be involved virtually and active in the product  
creation process, the challenges for companies are summarized as follows. 
• Providing a technical solution that enables ubiquitous active customer integration. 
• Providing a technical solution that supports innovative customers with background 
information from the product development process, allowing self-assessment and 
refinement of contributions. 
• Establishing a process that supports customers with qualified feedback and assis-
tance from the product creation perspective to enable independent further devel-
opment.  
• Establishing a process that supports the collaborative aggregation, exploration and 
assessment of customer generated inputs in a customer community. 
• Providing a technical and procedural solution enabling the direct usage of customer 
generated inputs in the product creation process. 
1.2 
Object of Research and Approach 
The research concentrates on developing a holistic approach that addresses the above 
mentioned challenges. A modular research environment is in development that 
enables active virtual customer integration by using a ubiquitous innovation frontend 
on a mobile device with interface to a PLM system. This test environment serves the 
purpose to answer the research questions: 
• Which data from the product creation process supports customers to innovate?  
• Which data helps the product creation engineers to innovate? 
• How must a technical solution be designed to integrate innovation related activities 
of customer and product creation engineers in a continuous process?  
• How can this process be integrated with existing product creation processes? 
To achieve the goal and answer the research questions, existing procedural and tech-
nical solutions have been analyzed in terms of their deficits and reusability. In a next 
step, test scenarios must be developed and realized in the research environment using 
the results of qualitative and quantitative studies concerning investment goods. Final-
ly the developed solution needs to be evaluated and optimized for the integration with 
product creation processes.  

54 
T. Damerau, H. Hayka, and R. Stark 
The presented considerations and results are based on the results of the joint re-
search projects BMBF-ISYPROM [9]1 and BMBF-INNOPEP2, started in 2012. 
2 
State of the Art 
The chapter gives a brief overview of existing approaches for active virtual customer 
integration. In this paper, virtual customer integration is accordingly understood as a 
type of intensive interaction between manufacturers and customers, which is more 
than market research. In other words, customers adopt the role of active co-designers 
of the process of innovation [6]. Active customer integration goes one step further. 
The manufacturer is not only opening a direct channel to collect the need informa-
tion’s of the customers directly and systematically, but it allows the customers to 
innovate themselves constantly in cooperation with the companies. As Reichwald et. 
al. points out, this approach is based on the recognition that customers are not only in 
possession of solution information. In fact, many customers have comprehensive and 
detailed knowledge of how their unfulfilled desires can be realized [6] 
2.1 
Roles of Customers in Product Creation Process 
The term customer can be interpreted and defined very diverse [4], [5], [6], [7]. From 
an engineering perspective the differentiation of customers following the concept of 
Design for Innovation is applicable. The concept can be separated in four core com-
ponents defined as “Design for Purchasing” (by buyers), “Design for Adoption” (by 
user), “Design for Impact” (on the beneficiary) and “Design for Externalities” (on 
outsiders) [10]. 
For the development of the research environment solution it is important to define 
the roles those customers can take up in the product creation process. A good orienta-
tion provides Nambisan (Table 1.) [11]. In his research Nambisan refers to a virtual 
customer environment (VCE). Concerning the mobile frontend of the PLM-centered 
research environment his role definition is suitable because VCEs combine 
lightweight virtual product creation technologies with a physical space. 
2.2 
Methods of Customer Integration 
In the literature, a distinction between methods and technologies for customer integra-
tion is made only rarely. Bretschneider et. al. [12] offers a good stocktaking. This 
paper only presents essential and to a large extent technology-driven methods. 
Virtual Customer Integration Platforms. VCI platforms using the internet as a uni- 
or bidirectional communication channel between customer and companies e.g. the 
customer can post ideas for new products, or discussions with a development  
 
                                                           
1 www.isyprom.de 
2 www.innopep.de 

 
PLM-Centered Support of Active Virtual Customer Integration 
55 
Table 1. Roles of customers in accordance to Nambisan [11] 
 
Product 
Concep-
tualizer 
Product 
Designer 
Product 
Tester 
Product 
Support 
Specialist 
Product 
Marketer 
Nature of 
Customer 
Contribu-
tions 
Suggestions 
and ideas 
for new 
product 
and/or for 
product 
improve-
ment 
Specifica-
tion of new 
product 
design; 
inputs on 
product 
features and 
design 
trade-offs 
Identifica-
tion of 
product 
design 
flaws; input 
on product 
prototypes 
Delivery of 
product 
support 
services to 
peer cus-
tomers 
Diffusion 
of new 
product 
informa-
tion; shap-
ing peer 
customer’s 
purchase 
behavior 
Dominant 
Nature of 
Customer 
Interactions 
Customer – 
Customer 
Customer - 
Company 
Customer - 
Tool 
Customer – 
Company 
Customer - 
Tool 
Customer - 
Company 
Customer – 
Customer 
Customer – 
Customer 
Customer – 
Customer 
Customer – 
Tool 
Typical 
VCE Tech-
nologies 
Discussion 
forums 
Knowledge 
centers 
Blogs, wi-
kis 
Virtual 
product 
design and 
prototyping 
tools 
Messaging 
tools 
Virtual 
product 
simulation 
tools 
Messaging 
tools  
Discussion 
forums 
Knowledge 
centers 
Discussion 
forums 
Virtual 
product 
simulation 
tools 
 
 
engineer. The level of richness can vary from text communication to multi-modal 
interfaces or user-innovation toolkits, where the customer can manipulate the final 
product [4]. VCI tools can address customers individually or via online communities 
allowing to in-source creativity by enabling users to create and evaluate products, 
bypassing intermediaries such as market-research firms [4], [6], [12], [13]. 
 
Virtual Customer Environment. The term virtual customer environment (VCE) 
strongly overlaps with VCI platforms. Different to VCI platforms VCEs can offer 
virtual product creation technologies in a physical space enabling direct interaction 
between customers and engineers. Some examples for VCEs are BMW´s Customer 
Innovation Lab, Volvo´s Concept Lab or Ducati´s Tech Café [11]. 
 
PLM-Based Customer Integration. The idea of using PLM systems for customer 
integration so far has only little observance in literature. Schulte [14] developed an 
approach offering a frontend collecting customer feedback for virtual prototypes 
which is link as requirements to product structure elements in the PLM system. A 
similar approach was presented by Stark et. al. [9]. In the joint research project 
ISYPROM a model-based integration of a collaborative idea management tool and the 
requirements engineering component of a PLM system was realized. Table 2 shows 
the delta of presented methods and the desired PLM-centered solution. 

56 
T. Damerau, H. Hayka, and R. Stark 
Table 2. Delta between characteristics of existing and desired solution 
 
VCI 
VCE 
PLM 
PLM-centered 
Product identification 
- 
- 
- 
x 
Mobile frontend 
partially 
- 
x 
x 
Ubiquity 
partially 
- 
- 
x 
PLM integration 
- 
partially 
n.a. 
x 
Integration type: passive/active 
x/x 
x/x 
x/- 
x/x 
 
Fundamentally new within this approach is the automated access to an innovation 
environment via product identification. Hence, in comparison barriers for customer 
contributions to the product creation process are reduce drastically. While other cus-
tomer integration solutions have different drawbacks especially concerning the PLM-
integration, the PLM-centered virtual active customer integration approach offers a 
seamless integration between company-internal and external innovation and product 
creation activities and a vivid collaboration between the involved roles. 
3 
PLM-Centered Research Environment 
Chapter 3 presents the alignment of the proposed solutions to the PLM process, a 
detailed description of the technical process and the current state of development. 
3.1 
Alignment in the PLM Process 
Figure 1 depicts the alignment of the proposed research environment in the PLM 
process and the information flows between OEM and customer. 
 
Fig. 1. Alignment of research environment and information flows in the PLM process 
 
Support processes
Sales
Marketing
Supply Chain
Real 
Product
Real product life
Information flows
OEM
Customer
Production
Product creation
Usage    
Operative
production
Disposal   
&  Recycling  
Planning
Development 
(Construction, 
Testing)
Production 
planning
Active virtual customer integration

 
PLM-Centered Support of Active Virtual Customer Integration 
57 
The bi-directional exchange of information between the customer and OEM takes 
place in the phases Usage and Disposal & Recycling, so in the real product life, and 
the phases planning and development in the product creation process. The Section 3.2 
describes the technical details of the process active customer integration with refer-
ence to a scenario. 
3.2 
Technical Process Outline 
The process, as depicted in figure 2, consists of six phases, descripted below. 
 
Data Release and Preparation. As pointed out technical background information 
about existing or planed products can support self-imitated and autonomous customer 
innovation activities. Hence, in this process phase relevant product descriptive data in 
the PLM system is selected for release and prepared, in a sense of automated file for-
mat transformation, detail level reduction and information aggregation, for use in the 
downstream process stages. As well the addressed customer community (company 
internal, extend or global) needs to be selected. This process phase has to be tailored 
concerning different aspect due to company specific interpretations of openness (e.g. 
compliance to intellectual property guidelines). 
 
Data Provision. Depending on the addressed community the released and prepared 
data needs to be made accessible for the customer using a suitable IT-Infrastructure 
(e.g. Intranet, Internet). This process phase can be fully automated using the defined 
parameters from process phase one. It must be ensured that the data provided remains 
still associated with the initial data respectively its product. This guaranties the tra-
ceability between the source and target of customer-generated information. 
 
Fig. 2. Technical process for PLM-centered customer integration 
Product Identification and Data Retrieval. To meet the requirements of a ubiquit-
ous customer integration environment data retrieval needs to be designed barrier-free 
(see chapter Challenges). Therefore in this process phase the access to the environ-
ment is enabled by a bijective product identification. By pointing a mobile device on a 
product relevant data the customer likes to contribute to is retrieved. 
 

58 
T. Damerau, H. Hayka, and R. Stark 
Data Manipulation. Using a suitable communication and interaction interface the 
customer is enabled to initiate or contribute to existing innovation projects. Such con-
tributions can be unidirectional, e.g. the customer can post ideas for new products or 
incremental innovations, vote for innovation ideas, or bidirectional, allowing discus-
sions between the customers and engineers (see process phase data consolidation). 
Data Consolidation. In this phase the customer-generated information is made ac-
cessible to a wider community for further development. In a sense of active customer 
integration during this phase customers and engineers can take up different roles us-
ing a web-based platform for design activities. The phase data consolidation and data 
manipulation are running iteratively. 
Data Retransistion. Using company-specific metrics, the consolidated data are  
evaluated in terms of maturity. When a certain maturity level is reached data will be 
return into the PLM system for further development activities executed by product 
planer or development engineers. 
3.3 
Current State of Technical Realization 
The current research and development activities are focused on the implementation of 
a ubiquitous mobile customer integration frontend respectively the process phase 
“Product Identification and Data Retrieval” and “Data Manipulation”. For that, a 
model-based tracking was implemented on an Android device. This allows using 
tracking models generated from geometric data in the PLM system to identify real 
products, without using special marking approaches e.g. QR-Codes, markers, RFID 
etc., and tracking. Thus a translation and rotation stable overlaying of a product and 
its geometry was realized. 
 
 
Fig. 3. PLM-centered roundtrip 
 
Engineering department
PLM system
Customer-modified
product data
Product creation and innovation community
Real products
Mobile access to
product data
Data manipulation and consolidation 
by community members

 
PLM-Centered Support of Active Virtual Customer Integration 
59 
The developed application so far offers opportunities to leave annotations and to 
document own “real world” modifications with snapshots. In both cases the customer-
generated data is associated with the initial data in the PLM system e.g. drawings in 
the product structure. Figure 3 depicts the mobile frontend in the context of the re-
search environment. 
As seen in the figure, geometric data from the engineering department and a so 
called tracking model are made accessible via a PLM system. In this case Siemens 
Teamcenter PLM 9.0 was used. The provisioned data can be accessed by pointing the 
camera of the Android device at the corresponding product. After modifications have 
been made by the customer, data is published in a web-community for further devel-
opment and assessment. After passing the innovation funnel, community-hedged in-
novation ideas and corresponding data is transitioned to the PLM system, respectively 
to the engineering department, for realization. 
4 
Summary and Outlook 
The paper presents a new approach for the virtual active customer integration into the 
product creation process using a mobile device. In addition to the notion of the inte-
gration process results from the technical implementation are explained. The major 
befits compared to existing solutions are the drastic reduction of barriers for the par-
ticipation of customers in the product creation process and the seamless integration 
with the PLM system on data level. Beyond that, the presented solution is capable to 
function as a unified cross-company access to innovation platforms. 
While proof of concept has been provided for the usage of product descriptive data 
as basis for a ubiquitous identification of real products using a mobile device, various 
research needs to be conducted concerning procedural, technical and particular soci-
ocultural aspects as well. Hence, the presented technical solution serves as a research 
environment for further investigations. 
The design of the PLM system frontend components in development will be ad-
justed to the existing results and yet to be performed qualitative and quantitative sur-
veys within investment goods manufacturers. This relates primarily to the aspects: 
• mapping of customer contributions to requirements 
• visualization of customer contributions in mock-ups 
• feature integration for seamless community-interaction 
For this purpose the project partners will be interviewed in a first stage. Based on the 
results a scenario will be developed and implemented which enables further assess-
ment concerning the quality and quantity of customer generated contributions to the 
product creation process and refinement of the research environment itself as well. 
Acknowledgments. The research and development project INNOPEP (02PJ1138) is 
funded by the German Federal Ministry of Education and Research (BMBF) within 
the Framework Concept ”Research for Tomorrow’s Production” and managed by the 
Project Management Agency Karlsruhe (PTKA). The authors are responsible for the 
contents of this publication. 

60 
T. Damerau, H. Hayka, and R. Stark 
References 
1. Thomke, S.: Enlightened Experimentation: The New Imperative for Innovation. Harvard 
Business Review 79(2), 66–75 (2001) 
2. Chesbrough, H.W.: Open Innovation: The New Imperative for Creating and Profiting from 
Technology. Harvard Business School Press, Boston (2003) 
3. Chesbrough, H.W.: Open Innovation: A New Paradigm for Unterstanding Industrial Inno-
vation. In: Chesbrough, H.W., Vanhaverbeke, W., West, J. (Hrsg.) Open Innovation: Re-
searching a New Paradigm, pp. 1–12. Oxford University Press (2006) 
4. Rohrbeck, R., Steinhoff, F., Perder, F.: Sourcing innovation from you customer. How mul-
tinational enterprises use Web platforms for virtual customer integration. Technology 
Analysis & Strategic Management 22(4), S.117–S.131 (2010) 
5. Prahalad, C.K., Ramaswamy, V.: Co-opting customer competence. Harvard Business Re-
view 78(1), 79–90 (2000) 
6. Reichwald, R., Engelmann, M., Meyer, A., Walcher, D.: Der Kunde als Innovationspart-
ner. Konsumenten integrieren, Flop-Raten reduzieren, Angebote verbessern. Betriebswirt-
schaftlicher Verlag Dr. Th. Gabler | GWV Fachverlage GmbH Wiesbaden, Wiesbaden 
(2007), http://dx.doi.org/10.1007/978-3-8349-9226-0  
7. Schroll, A.: Community Based Innovation: Einsatz von Innovation Communities. GRIN 
Verlag (2008) 
8. Spur, G.: Produktionsinnovation im Fokus. Jahrbuch der inpro-Innovationsakademie 2011. 
Hanser Carl., München (2012) 
9. Stark, R., Hayka, H., Damerau, T.: PLM-basierte Innovationsbeschleunigung als integraler 
Teil des Systems Engineering. In: Gausemeier, J., Schäfer, W., Ramming, F., Trächtler, A. 
(Hg.) Entwurf Mechatronischer Systeme. HNI-Verlagsschriftenreihe (2011) 
10. Cantamessa, M., Cascini, G., Montagna, F.: Design for Innovation. In: Design Conference 
(2012)  
11. Nambisan, S., Nambisan, P.: How to Profit from a Better ’Virtual Customer Environment’. 
MIT Sloan Management Review 49(3), 53–61 (2008)  
12. Bretschneider, U., Leimeister, J.M., Krcmar, H.: Methoden der Kundenintegration in den 
Innovationsprozess: Eine Bestandsaufnahme. Arbeitspapier. Hg. v. H. Prof. Dr. Krcmar. 
Lehrstuhl für Wirtschaftsinformatik, Technische Universität München. München (34). On-
line verfügbar unter (2009),  
http://www.winfobase.de/lehrstuhl/publikat.nsf/c56a0adbefeb6
29c12576af002bb529/016d2abc30d795bcc1257610003a00f2/$FILE/ 
ArbeitspapierNr.34.pdf  
13. Füller, F., Matzler, K.: Virtual product experience and customer participation‐A chance 
for customer-centred, really new products. Technovation 27(6-7), S.378–S.387 (2007) 
14. Schulte, S.: Integration von Kundenfeedback in die Produktentwicklung zur Optimierung 
der Kundenzufriedenheit. Univ., Bochum, Bochum (2006),  
http://www-brs.ub.ruhr-uni-bochum.de/ 
netahtml/HSS/Diss/SchulteStefan/diss.pdf  

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 61–70. 
DOI: 10.1007/978-3-642-30817-8_7 
© Springer-Verlag Berlin Heidelberg 2013 
 
High Definition Product Lifecycle Management  
an Immersive Decision Making Environment   
Carsten Burchardt 
Siemens Industry Software GmbH & Co. KG, Werner-von-Siemens-Platz 1, 
30 880 Hannover, Germany  
carsten.burchardt@siemens.com 
Abstract. This paper describes High-Definition PLM (HD-PLM), Siemens 
PLM Software’s vision for supporting companies make smarter decisions that 
result in better products. Working from the knowledge that companies in all in-
dustries are faced with increased complexity in both products and processes, 
HD-PLM vision builds on three core concepts: intelligently integrated informa-
tion, a future-proof architecture, and a high-definition user experience. The 
High-Definition PLM vision enables decision makers throughout the product li-
fecycle to make better informed decisions more efficiently and with a higher 
level of confidence. This paper explains the network of technologies and capa-
bilities to achieve a defined set of strategic objectives and the technology foun-
dation of the core HD-PLM concept. The basics and environment are described 
and how HD-PLM supports a cross-domain decision making by uniting users 
with the people, tools and precise product-related information they need to in-
telligently evaluate decision alternatives. 
Keywords: HD-PLM, immersive decision making environment, complexity, 
product lifecycle management. 
1 
Introduction 
All industrial sectors must deal with multiple challenges and issues in bringing new 
products to the market and maintaining market presence once they establish a niche. 
This objective has grown complicated by the increased complexity of the products 
themselves as well as their development processes. The sheer volume of information 
being driven into and around the product is so great that some companies have opted 
to remove content rather than attempt to manage the complexity. Product develop-
ment has become complicated by environmental, safety, and government regulations; 
worldwide development practices; and diverse global market requirements. This re-
quires hundreds or even thousands of critical decisions to be made throughout the 
entire product lifecycle by different individuals from different disciplines all along the 
value chain. The quality and speed of these decisions can have a profound impact on 
the market success of a product. Yet each decision may be based on a vast and  
constantly expanding universe of distributed digital data stored in a wide variety of 
formats and originating from multiple disparate sources. Compounding this  

62 
C. Burchardt 
overwhelming challenge is the fact that many product related decisions rely on mul-
tiple areas of expertise that may be distributed across the globe. 
In all of the processes a company goes through to produce a product, thousands, if 
not tens or hundreds of thousands, of decisions must be made, from start all the way 
to the end of the product’s lifecycle. The faster decisions are made, the faster the 
process goes. The more accurately decisions are made, the fewer the problems that 
occur downstream. Fast, accurate decision-making is no easy feat, given all of the 
information that must be considered, as well as the fact that information and know-
ledge are spread throughout a company, its supply base, its joint venture partners, and 
of course, its customers. 
It is no longer enough to capture, manage and integrate information. Information 
must be given intelligence. It must understand what it is, how and why it relates to 
other information, and how and where it should be used so that people don’t have to 
hunt for it. In this approach, information is proactive, not reactive. It’s there at exactly 
the right time, in the right context, and in the precise level of detail so that this deci-
sion – the one being considered right now – can be made as quickly and accurately as 
possible. The critical question for today’s companies is: “Will this complexity sink 
you, or can you turn it into a competitive advantage?”  This question offers two very 
real possibilities.  
The HD-PLM vision can turn this massive, widely distributed and heterogeneous 
collection of data into knowledge, through a tightly integrated set of solutions that 
will permeate Siemens PLM Software’s entire suite of enterprise applications. HD-
PLM provides a comprehensive inter-disciplinary source of information across all 
product lifecycle domains and represents a virtual 3D repository of information that 
includes design models, simulation, manufacturing processes, and all data stakehold-
ers need in the product development lifecycle to make the right decisions in the  
development process. The idea is to enable users to move through the planning, de-
velopment, manufacturing, and support stages for the product lifecycle with all infor-
mation necessary and accessible to accomplish each stage. HD-PLM is designed to 
allow and achieve the latter, thriving in the current environment by turning complexi-
ty’s challenges into opportunities for customer satisfaction to provide intelligent in-
formation at exactly the right time, in the correct context, and at the precise level of 
detail that each person needs.  
These solutions will significantly enhance decision making throughout the product 
lifecycle by taking users into the realm of advanced data interaction that actively applies 
meaning to data and intuitively presents rich information in a way that facilitates under-
standing. The aim of the vision is to reduce costs through more optimal engineering 
trade-off and engineering change decisions, by reducing unforeseen downstream negative 
effects.  
 
HD-PLM Vision: Everyone in the product lifecycle makes decisions appropriate to 
their work, but too often it´s difficult to access the information they need from differ-
ent disciplines in order to make the best decisions. Organizations can improve  
decision-making capacity by intensifying the use of pervasive visual information. By 
enabling access to product information from multiple visual and non-visual and struc-
tured and unstructured sources, and synthesizing it into a highly usable visual  
information, organizations can create a more complete and exploitable context for 

 
HD-PLM an Immersive Decision Making Environment 
63 
effective decision making.  HD-PLM based solutions will personalize the perspective 
to the users' role to enable decision makers to more quickly access and understand 
information that used to take hours or even days to assemble and digest. Product-
related information, presented in a visual context and shared across widely accessible 
and easily usable collaborative interfaces, helps level the playing field for collabora-
tion across business functions, technologies, and enterprises and can contribute to 
manufacturers making more effective product-related decisions throughout the com-
plete product life cycle [1]. These decisions additional will be captured for future 
reference when similar criteria are encountered by others. The accumulation of best 
practices and experiential knowledge will enable users to validate that their decisions 
are the best choice to increase confidence in each decision [2].  
These HD-PLM advancements also have a significant impact on innovation and 
product competitiveness. Most product innovations are born out of market need  
and design creativity. By exposing deeper levels of product data to design teams and 
greatly facilitating its understanding, those teams have more knowledge available to 
fuel creativity and more time to implement it. The technology framework will help to 
eliminate decisions with unforeseen impacts on product quality, reliability and prod-
uct performance. All phases of process workflows can be streamlined with everyone 
having rich information personalized to their task.  
2 
High Definition Product Lifecycle Management an Immersive 
Decision Making Environment 
Over the years, methodologies and practices have evolved for organizing and under-
standing the information that drives decision making. The vision of HD-PLM is that 
all information is delivered to the user in the context of his or her current task, intui-
tively, without having to search. HD-PLM requires an architecture that doesn’t need 
re-invention but instead adapts and grows with every IT innovation and change in the 
business environment. Siemens PLM Software is delivering on the HD-PLM vision 
by focusing on three core concepts [3]:   
• 
An intelligent integrated information to delivering the right information 
to the right people. Understanding the semantics of the relationship between 
the product-life cycle data to understand why a relationship exists. take more 
insightful action than by just knowing that a relationship exists. By under-
standing and breaking down customer requirements, the user can assign them 
to the functional groups that develop a product feature, while maintaining vi-
sibility of the product as a whole. In this way, the user can engage with the 
specific users who are impacted by a change, instead of updating everyone 
every time a change is made.  
• 
Future-proof architecture to making sure that the information user 
receives is in a form he can use. What good is a PLM system that is only 
available in a single deployment scheme, locks into a certain set of authoring 
tools, or is only available on one or two devices? The focus on future-proof 
architecture should deploy to suit the user particular needs, whether it is a sin-
gle desktop or a globally distributed supply chain. Future-proof architecture 

64 
C. Burchardt 
must also allows to use the devices, tools and data that the user or the partners 
already have, while keeping the options open for what the user do next.  
• 
A high-definition user experience to making decisions effectively. This is 
where the value of intelligently integrated information and future-proof ar-
chitecture is most evident to the average user. The user get only the informa-
tion that a user particular needs to make a decision−based on their role and 
the decision at hand−cutting down on information overload and the time it 
takes to find relevant data. That means the user is always in the right context 
to do his work, accomplish his tasks, presenting the information intuitively, 
validate the use decision rationale. 
2.1 
Intelligently Integrated Information Architecture  
An intelligently integrated information architecture should bring plant and production 
information into planning and product development. This allows a much higher rate 
of production success – in terms of items such as cost, quality, and throughput – be-
cause what actually happens at the plant level is fed back into product development 
and manufacturing planning. The virtual actions of defining product and processes 
become much more predictive of what will happen in manufacturing. This eliminates 
the need for adjustments, prove outs and other time and labor-intensive remedies that 
were previously required when virtual planning wasn’t able to address real-world 
plant conditions. By connecting the physical devices in the plant and the software 
used to plan and manage plant operations within the PLM backbone, much more 
knowledge of what actually happens in manufacturing can be captured and driven into 
the early planning stages of product and process development. To create an environ-
ment for fast and accurate decisions PLM must support: 
 
Systems engineering to provide a consistent process framework across mechani-
cal, electrical, software and electronic domains 
 
Integration of all BOMs and BOPs to provide a comprehensive definition of the 
product and processes 
 
Integration of product development with production to provide closed-loop feed-
back 
from 
production 
to 
product 
development 
and 
manufacturing  
engineering 
 
Systems engineering supports companies to capture, manage and organize informa-
tion and knowledge, beginning with the voice of the customer and continuing through 
to service, support, and end of life. By modeling requirements and allocating them 
through functional and logical decompositions to physical implementation, they 
achieve a significant level of traceability throughout the product. They also gain a 
thorough understanding of the dependencies within the model. Another significant 
benefit is that systems modeling help drive alignment between engineering domains 
(mechanical, electrical, software, electronics). When coupled with configuration and 
change management, systems engineering can serve as a consistent process frame-
work that drives efficiency and accuracy during development and validation 
processes. Synchronized, cross-domain product development is realized through a 
systems engineering process that leverages a comprehensive understanding of  
 

 
HD-PLM an Immersive Decision Making Environment 
65 
 
Fig. 1. Intelligently Integrated Information – Three Key Technologies 
functional behavior and enables knowledge capture and re-use on the systems level. 
This same systems engineering process can also be applied to production, where there 
are four additional instantiations of the physical representation. These include: part, 
process, plant and the actual physical object that is produced. Incorporating produc-
tion into the systems model provides a comprehensive systems view including prod-
uct, process and plant.  
Today’s design world is dominated by solids and constraints while the business 
perspective is dominated by parts, features, quantity and cost. There is limited con-
nectivity between the virtual design and business worlds where actual configuration 
and change happen. This prevents early virtual validation (on geometry data as well 
as electric and electronic simulations and other CAE-based validations). It also makes 
it impossible to validate saleable product configurations. As a result, it is critical to 
unite the different product BOMs and BOPs, and even more importantly, to align the 
semantics of these different views (usage versus product structure, for example). The 
integrated product definition, combined with the consistent process framework that 
systems engineering provides, delivers a number of benefits. For example, aligning 
the business BOM with the bill of design and manufacturing processes provides the 
ability to virtually validate the product by applying the configuration rules in the 
business BOM across all engineering domains. Now entire mechatronic systems  
can be virtually validated in the exact configuration in which they’ll be sold. Also, the 
process for building those saleable different configurations can be validated in the 
exact ways the different plants will build them. For the first time, companies will be 
able to virtually validate exactly what they’ll produce at a specific plant. This will 
dramatically reduce the amount of physical validation required while also improving 
first time quality. 
To make intelligently integrated information happen, it is necessary to understand 
the meaning, or the semantics, of all the information to generate to plan, develop, 
build and support the product. For example:  Systems engineering is getting a lot of 

66 
C. Burchardt 
visibility today, especially in the discrete industries.  The complexity of the products 
and the dependencies between systems in the product has become so great that com-
panies are looking for ways to deal with it all.  One of the things SE does is to help 
people understand how requirements are allocated to functions => how functions are 
implemented through the logical architecture.  And that is realized in the physical 
design – across all disciplines – mechanical, electronic and software.  Building these 
systems models inside of PLM captures are important of the meaning and intent about 
the product.  A true semantic model begins to emerge where the way information is 
related and the context of the relationship itself is valuable and can start to be leve-
raged.  The capability to understand and leverage SE in the tool helps to build intelli-
gently integrated information right into all of the products.  Another example is Bill of 
Material /Bill of Production integration [4].  At this topic the relation/understanding 
how different BOM’s relate to one another, and to build that understanding into the 
PLM tools.  The entire product, process and production lifecycle can be thoroughly 
planned and analyzed in a systems engineering context. The technology can deliver 
this intelligently integrated view of product, process and production. By intelligently 
organizing and integrating systems engineering, providing an integrated definition of 
the product, and closing the loop between product and production, HD-PLM drives 
real step-change in product development and production, improving productivity, 
time to-market, first-time quality and ultimately helping you build the right product, 
and build it in the right way. 
HD-PLM has high value and will resonate with everyone in the PLM landscape. 
Most PLM professionals make decisions as the primary aspect of their job. Whether it 
is deciding what material to specify: who to include in a collaborative meeting; or 
how something was done in the past, PLM data is a repository of what customers 
need to support their decision-making process [5]. Everyone in the product lifecycle 
makes decisions appropriate to their work, but too often it is difficult to access the 
information they need from different disciplines in order to make the best decisions. 
The technology framework based solutions will personalize the perspective to the 
users’ role to enable decision makers to more quickly access and understand informa-
tion that used to take hours or even days to assemble and digest. Decisions will then 
be captured for future reference when similar criteria are encountered by others. The 
accumulation of best practices and experiential knowledge will enable users to vali-
date that their decisions are the best choice to increase confidence in each decision.  
The primary purpose of information is to support decision making. The faster an  
organization can deliver people the right information to make the right decisions, 
earlier, the more efficient their processes will be - and the lower their risk of product 
mistakes and costly delays. Providing this level of decision support requires a system 
that can bring together scattered information from across the product and production 
lifecycles - from planning and development through manufacturing engineering, onto 
the factory floor, and even into service and support - and deliver the information in a 
visual intuitive way [6]. 
2.2 
Future-Proof Architecture 
To provide an effective decision-support environment, an architecture must never 
become obsolete. It must be upgradeable and expandable to permit the introduction of 

 
HD-PLM an Immersive Decision Making Environment 
67 
new technologies and innovations. It must easily integrate with other systems because 
not everything is stored directly in the PLM system. It must change and morph to 
adapt to the changes in your business.  Following core principles must guide in archi-
tecting HD-PLM. 
Openness - open technology and open information. Open technology means that 
other companies getting  actively support to release, publish and use the used technol-
ogy (such as JT™, Parasolid®) to help grow in the PLM market. Open technology 
also means open standards, meaning to accepted standards and, if none exist, work to 
create standards that improve the entire PLM ecosystem. Open information means 
that a customer, having implemented tools to manage all aspects of their product and 
process development, has invested in something that is extremely valuable it must be 
transparent  about what that information is, how it is organized and how they can 
access it, import to it, or export from it => to give them full and ultimate control over 
their information. Future PLM architecture: 
 
Open 
 
Rich API set- SOA, NX Open 
 
Toolkits / published formats – PLM, XML, SDK, JT, Parasolid 
Scalable 
 
Four-tier architecture 
 
Transparent adjustment or needs based on demands 
Flexible 
 
Layered platform services 
 
Codeless customization 
 
Data model extensibility 
Preferred device support - people who need to interact with PLM aren’t just sitting 
at their workstations anymore. New devices such as smart phones, tablet computers 
and other handheld platforms are becoming the main productivity tool for many. With 
HD-PLM, field engineers must now visualize a part, mark it up and log issues into the 
PLM system from a number of different handheld devices. An issue can proceed 
through a review process while the field engineer is still on site, creating the real pos-
sibility for instant analysis and feedback with the customer. Sales personnel could 
configure a product while the customer views it on a handheld device, even to the 
point of visualizing configuration changes as he or she makes them. 
2.3 
High Definition User Experience 
The topics regarding the architecture and functionality of HD-PLM are complex. The 
challenge is to provide a user experience that is not. To accomplish this, the HD-PLM 
interface must leverages the same intelligence used to deliver precisely tailored in-
formation to each user, eliminating many of the administrative tasks required to find, 
enter and maintain information. To create an HD user experience, four key concepts 
should be addressed: 
 Put the user in right context for his work 
 Help the user accomplish his tasks 

68 
C. Burchardt 
 Present the information intuitively 
 Help the user validate decision rationale 
Context and Situational Sensitivity - HD-PLM should know who the user is. It 
should know what role the user plays and what he worked on last time he were in the 
system. It should know who the user collaborate with. All of this information, and 
more, can place the user in the right context every time he or she enters the system, 
reducing the amount of work he or she must do to get the necessary information on 
the screen to do work. 
Help Users Accomplish Their Task - Web 3.0 introduces the concept of robots or 
agents to help people navigate their way around tasks and functions. HD-PLM adopt 
this same concept. Agents in HD-PLM can help the user with a variety of functions. 
Agents can be active or proactive depending on the type of task the user want to per-
form them. For example, if the user is working on an issue, an agent could enter all of 
the information required to log the issue in the system, simply through its context and 
situational sensitivity. Once the information is complete, the agent could then as the 
user for approval to submit the issue to a specified process.  
Present Information Intuitively -Different users interact with information different-
ly. For some users, 3D is how user wants to view information. Others prefer a spread-
sheet view. Still others prefer graphs or charts. What “intuitive” means is dependent 
on the user and the task he’s trying to perform. To present information intuitively, it 
must be in the right format, the right context, and at the right level of granularity. 
Presenting too much information requires significant work to find what the user want 
or need. The system should be able to present just the right amount for the task at 
hand, while giving the user the option to get more detail if required. 
Validate against decision Rationale - Understanding why the use made a certain 
decision is critically important for a variety of reasons, including traceability, issue 
resolution and definition of best practices. The ability to capture the rationale the user 
used to make any decision – at any point in the product development process – is key 
to tracing the root cause of problems. It also allows the user to reapply the same ratio-
nale to arrive at the same decision, thus forming the basis for a best practice.  
2.4 
First HD-PLM Implementations 
The implementation of HD-PLM technology is being delivered in NX™ and Team-
center® via the HD 3D environment which includes Visual Reporting capabilities – 
such as color coding based on search results or 3D flags indicating warnings or rule 
violations – that provide a deeper, more intuitive understanding about the product to 
support effective decision-making. The technology framework based solutions also 
will have a very significant impact on innovation and product competitiveness. Most 
product innovations are born out of market need and design creativity. By exposing 
deeper levels of product data to design teams and greatly facilitating its understand-
ing, those teams have more knowledge available to fuel creativity and more time to 
implement it. The technology framework will help to eliminate decisions with unfore-
seen impacts on product quality, reliability and product performance. Speed to market 
is a major benefit gained by faster decision-making at all levels. All phases of process 

 
HD-PLM an Immersive Decision Making Environment 
69 
workflows can be streamlined with everyone having rich information personalized to 
their task. HD-PLM Framework based solutions eliminate wasted time getting to the 
information needed to perform tasks and makes multi-disciplinary information availa-
ble sooner to make decisions earlier. 
 
Fig. 2. Continues Improvement in HD-PLM User Interaction 
3 
Concluding 
The question “will complexity sink you, or can you turn it into a competitive advan-
tage?” offers two very real possibilities. The HD-PLM technology vision allows to 
achieve the latter, thriving in the current environment by turning complexity’s chal-
lenges into opportunities for customer satisfaction. By providing intelligent informa-
tion that is there at exactly the right time, in the correct context, and at the precise 
level of detail that each person needs, can help manufacturers in all industries achieve 
a new level of productivity and quality. Intelligently integrated information which 
allows users to manage large volumes of data but only use data in context to make the 
best decision. It is not the quality or the quantity of, but having the right data at the 
right time [7]. The HD-PLM vision builds on Siemens PLM Software’s experience 
while expanding on state-of-the-art innovations and leveraging emerging software, 
hardware and connectivity technology.  
The aim of this vision will be to provide a technology foundation to enable a global 
collaborative product development team to produce a common set of integrated soft-
ware tools that will identify, capture and collate the massive amount of information 
available both inside and outside of manufacturing enterprises, and then apply mean-
ing to that data using a consistent, compelling and intuitive visual environment. It is a 
fundamentally new way to discuss PLM and represents the beginning of a vision and 
development of a direction for the next several years to result in a continuous flow of 
innovations in products to support decision-making over the course of the next years. 

70 
C. Burchardt 
References 
1. Slansky, D.: Siemens PLM Realizes the Vision of End-to-End Design / Build Lifecycle, 
Arc View, MA, USA (July 29, 2010) 
2. Lessning, M.: Technology HD3D, CADCAM Report no.7 2010. In: CE-NET: The Concur-
rent Enterprising Network of Excellence. Dressler, Heidelberg (2010), WWW page  
http://www.ce-net.org/index.html 
3. White Paper, Make smarter decisions that results in better products with high definition 
PLM, Issued by: Siemens PLM Software. ©, Siemens Product Lifecycle Management 
Software Inc., X5 26286 11/11 C (2011), http://www.siemens.com/plm 
4. Siemens Presentation: HD-PLM Vision: Issued by: Siemens PLM Software. © (2012). 
Siemens Product Lifecycle Management Software Inc., 
http://www.siemens.com/plm 
5. White Paper, Role of PLM in the Software Lifecycle, Issued by: Siemens PLM Software. © 
2010. Siemens Product Lifecycle Management Software Inc., X4 15825 12/10 C (2010), 
http://www.siemens.com/plm  
6. Barkai, J.: IDC Manufacturing Insights Opinion, IN Manufacturing Insights: Product: Life-
Cycle Strategies: Methods and Practices, MA, USA (June 2010) 
7. Nucleus Research: Research Note – Siemens Commits to Expanding PLM Scope and Usa-
bility, Document M 109 Boston, USA (September 2012) 
 
 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 71–81. 
DOI: 10.1007/978-3-642-30817-8_8 
© Springer-Verlag Berlin Heidelberg 2013 
 
Implications of Open Innovation Approaches  
on Future PLM  
Andrea Denger1, Detlef Gerhard2, and Christian Kaiser1 
1 VIRTUAL VEHICLE Research Center, Information and Process Management,  
Inffeldgasse 21A, Graz, 8010, Austria 
2 Technische Universität Wien, Institut für Konstruktionswissenschaften und Technische 
Logistik, Getreidemarkt 9, Wien, 1060, Austria 
{andrea.denger,christian.kaiser}@v2c2.at,  
{detlef.gerhard}@tuwien.ac.at 
Abstract. Doing business globally causes the increase of very complex and dy-
namic processes. It demands a high level of flexibility and adaptability from the 
companies involved. The aim of companies is to find efficient ways in order to 
produce new products and to better meet customer demands. Open Innovation 
(OI) approaches contribute to this aim and therefore drive the evolution of fu-
ture Product Lifecycle Management (PLM). Based on four scenarios, developed 
using scenario planning method, the implications are shown on different levels 
of PLM. A catalogue of requirements for PLM 2020 was drawn up on the basis 
of the results of interdisciplinary panels, qualitative and quantitative interviews, 
and of a sector-specific use case for the automotive industry. The paper discov-
ers ways in which PLM can be made more successful in future and pinpoint 
challenges that PLM will have to meet with regard to OI concepts. 
Keywords: PLM, Open Innovation, Scenario Planning Method, Data Manage-
ment, Data Security, Human Factor. 
1 
Introduction 
For manufacturers today, innovation is the engine of growth [1]. In order to produce 
new products and better meet customer demands, companies i.e. innovate in produc-
tion, processes and business models. To access a broader creative potential within 
innovation processes, external knowledge and ideas have to be taken into account. In 
this sense, the term OI determines new approaches to involve people from outside the 
borders of the organization in the product innovation process. The key driver here is 
the Internet which offers access to a vast supply of resources and workforce. It is 
concretized in a variety of possible forms of organization of the innovation process 
between companies and the market. However, these relationships do need an overall 
organizing principle and this is one aspect that PLM intends to be. Directions of po-
tential PLM evolutions and requirements have been in focus of the research project 
“Future PLM”, conducted at VIRTUAL VEHICLE research center in Graz, Austria. 
At present the term PLM is used in several different ways, and often it is wrongly 

72 
A. Denger, D. Gerhard, and C. Kaiser 
understood as meaning merely an IT system. Its full definition is as an approach for 
managing a product, including the relevant intellectual property, over its entire life 
cycle. PLM consists of processes, organizational structures, methods and related IT 
systems. Used as an overall approach, PLM has the potential to give companies the 
structure and orientation they need to be profitable under competitive conditions. One 
particular aspect, which has been considered in the Future PLM project, is the impli-
cation of OI on future PLM. Considering OI in PLM, challenges occur in different 
topics, e.g. acquisition and management of customer feedback data, data security and 
intellectual property protection, or human factors. 
2 
Overview of Open Innovation and Research Project 
FuturePLM 
2.1 
Open Innovation 
Reichwald and Piller [2] distinguish “requirement information” from “solution infor-
mation” in OI processes. Requirement information is information about the customer 
and market needs, i.e. information about the preferences, desires, satisfaction factors 
and buying motives of current and potential customers or users of services. Solution 
information comprises the information needed to solve specific problems and critical 
issues in product development and innovation processes. Management of solution 
information is closely related to crowdsourcing which – as stated before - is not ga-
thering support of corporate functions and structures from the supply chain, but from 
the intelligence and workforce of a mass of free-time workers on the Internet. Reich-
wald and Piller state that this is necessary to keep the innovation-process as efficient 
and effective as needed. Depending on the branch often more than 50% of new devel-
oped products are not able to satisfy user-needs [3]. The development-, production-, 
distribution- and advertisement costs are not justified in this case and are simply gone. 
Therefore the “how to innovate” is focused to reduce insecurity to market and tech-
nology in an early phase of the innovation process. Innovation has to be an iterative 
process between company and market to use the creativity potentials of external 
sources. User innovations are often considered to be not radical enough because they 
are based on other concepts the user already is used to and therefore incremental, but 
this is disproved by surveys. [2] 
A different definition from Henry Chesbrough is to open the company-boarders 
permanently to gain the necessary potential that is needed to be innovation-leader in a 
particular market and to gain the chance to emerge in new markets. “OI is a paradigm 
that assumes that firms can and should use external ideas as well as internal ideas, and internal 
and external pathways to market, as the firms look to advance their technology. OI combines 
internal and external ideas into architectures and systems whose requirements are defined by a 
business model.“ [4] Fig. 1 shows H. Chesbrough’s approach. The illustrations depict 
the difference between closed- and open innovation: 

 
Impli
Fig. 1. Clo
Chesbrough’s approach 
phasis on the condition of 
open resource needs not au
tive price for access to intel
definition of OI by Pénin m
1. Voluntary knowledge dis
2. knowledge being open (w
able” [6]), and  
3. continuous and dynamic 
2.2 
Future PLM 
Within the research project
investigated future demand
manages roles and particip
which PLM can be made 
PLM will have to meet. Th
of contributions from all pro
First, a common PLM d
this paper as well: Product 
for managing intellectual pr
er, in order to foster a holis
and methods is a necessity. 
• Managing the entire lifec
• Collaboration between d
• Globally distributed deve
• Integration of customers 
Further a catalogue of req
the results of interdiscipli
ications of Open Innovation Approaches on Future PLM 
osed innovation (a) vs. open innovation (b) [4] 
was criticized later by [5] that it does not put enough e
availability of knowledge. Pénin states that access to
utomatically to be free of charge and claims that any po
llectual property potentially restricts access. The sugges
must encompass three constitutive elements: 
sclosure from participants,  
which is equivalent to say that “spillovers are not contr
interactions among participants. 
t Future PLM, the Virtual Vehicle Research Center in G
ds for product life cycle management in terms of the wa
pation of people. The project intended to discover ways
more successful in future and to pinpoint challenges t
he results presented here have been worked out on the ba
oject partners. 
definition had been set up that will be used in the scope
Lifecycle Management is seen as a strategic concept u
roperties of a product over the entire lifecycle [7]. How
stic PLM within a company, the improvement of proces
Challenges in future product development include: 
cycle of a product. 
different disciplines and cultures. 
elopment locations. 
and suppliers. 
quirements for PLM 2020 was drawn up on the basis
inary panels, four derived future scenarios on prod
73 
 
em-
o an 
osi-
sted 
roll-
Graz 
ay it 
s in 
that 
asis 
e of 
used 
wev-
sses 
s of  
duct  

74 
A. Denger, D. Gerhard, and C. Kaiser 
development in 2020, qualitative and quantitative interviews in automotive industry 
[8]. For a better understanding, the catalogue of requirements is inset in a matrix with 
specific levels versus topics [9]. In the catalogue of requirements the previously col-
lected ideas and suggested solutions are clarified and documented along with their 
chain of effects. In this paper the focus is set on matrix intersections with the topic OI.  
3 
Analyzed Challenges in the Field of Open Innovation and 
PLM 
The next subsections pinpoint some challenges in the field of OI that PLM will have 
to meet. 
3.1 
Data Management and Data Security 
From the point of view of data which is generated within the innovation process and 
has to be managed, two major aspects have to be distinguished: marketing and engi-
neering. Marketing data results from customer feedback and customer requirements. 
Customer feedback can be collected in the usage or Mid of Life (MoL) phase of a 
product based on the real existing product and customer experiences (usage, mainten-
ance, service). Customer requirements can also be collected based on concepts and 
virtual products in early phases of the product development process. 
Both can be considered as base to define and fix the requirements set for the devel-
opment of new or refurbishment of existing products. Especially, social media or web 
2.0 tools like forums offer a vast supply of different information sources, are develop-
ing fast, and can be utilized to collect marketing data but this data has to be analyzed 
and structured in order to be valuable for the definition of requirements. Another op-
tion is to develop special, questionnaire like web tools to collect data at least in a semi 
structured form or even enhance products with capabilities to return feedback data 
from usage. Another point of distinction is whether feedback data is collected on a 
product class in general or a single instance basis (identified product instance) which 
would require respective PLM enhancements.  
On the other hand the crowd sourcing aspect of OI requires handling and exchange 
of various types of typical engineering data though here are conceptual contradictions 
between the required structuredness of enterprise information management and the 
creativity of the OI processes. The main question is how transparency in decision 
making and evaluation of alternatives in the OI process can be ensured and what en-
hancement of PLM functionalities have to be provided to suit the demands outside the 
inner circle of the own organization and even outside the outer circle of business part-
ners such as suppliers or contractors. This is predominantly not a challenge with re-
spect to IT issues since all available IT systems in this area support ubiquitous web 
technology but essentially with respect to legal aspects (accountability, liability, ex-
port restrictions etc.) and organizational aspects (transparency in processes, definition 
of the granularity of tasks and information to be transferred into the crowd sourcing 
community, management of the crowd etc.). Resulting data of crowd sourcing 

 
Implications of Open Innovation Approaches on Future PLM 
75 
processes does not only contribute to requirements but to various tasks of engineering 
design and product development.  
The major issue in the context of data management in crowd sourcing processes is 
certainly the security issue. The term Intellectual Property Protection (IPP) represents 
the business objective to protect the know-how of a company as part of a supply chain 
or engineering network against risk of industrial espionage, patent violation, and pla-
giarism. IPP also comprises Data Leakage Prevention (DLP), i.e. means to prevent 
data leakage incidents where sensitive data is disclosed to unauthorized personnel 
either by malicious intent or inadvertent mistake [10]. To Implement IPP, different so 
called Enterprise Rights Management (ERM) approaches using encryption technology 
are available. These control access to corporate documents by selectively granting 
access to certain portions of the digital content or certain operations [11] and thereby 
enable companies to extend security to third-party partners, suppliers and customers 
[12].  
ERM is based on identity management (user authentication) and requires a policy 
server in which rights are defined, an encryption mechanism that controls access to 
the data, and a software or device that enforces the policy. ERM solutions focus on 
document exchange security which represents static content, but in typical IT solu-
tions for PLM, content is tied to a business process and dynamically changing, i.e. 
rights on a document are not only defined by the identity and role of a user but espe-
cially by its status or maturity. In multiple party crowd sourcing processes, user iden-
tification maintenance of roles with respect to PLM environments will be an intricate 
task and prone to failure. Main requirements are that protections stay together with 
document wherever the document travels and the owner remains in full control, i.e. 
access rights can be modified or revoked at any time. This means that each access to 
protected documents requires access to a server as independent authority which stores 
access rights and decryption information and raises questions of organizational as-
pects for managing identities, defining roles, or classifying data. 
3.2 
Process  
During the last few years, there has been rapid technological development and new 
possibilities have emerged for collaboration, communication and the management of 
product lifecycle information and knowledge. Some of the major changes are related 
to the novel possibilities offered by the emergence and, in the business sense, the 
maturing of web 2.0 and social media–based approaches (e.g. [13]). Social media 
integration in PLM has been an important trend of major PLM vendors, allowing e.g. 
the use and sharing of non-structured and tacit knowledge, which are problematic in 
traditional PDM and PLM systems. According to Stocker and Tochtermann [14] web 
2.0 focusses technologies that enable users to communicate, create content and share 
it with each other via communities, social networks and virtual worlds - faster and 
easier than ever before. They emphasize the power of users to select, filter, publish 
and edit information, as well as to participate in the creation of content in social me-
dia [15]. To sum up, web 2.0 and social media provides quite novel and useful ways 

76 
A. Denger, D. Gerhard, and C. Kaiser 
of interacting and collaborating in the innovation process, as well as for creating new 
information and knowledge for innovations.  
Based on Chesbrough the authors Gassmann and Enkel [16] define three processes 
to integrate OI into the development process as shown in Fig. 2. The processes are:  
• The outside-in process where ideas generated outside the company are used inside,  
• the inside-out process where knowledge or products are exploited outside the cur-
rent market, and  
• the coupled-process where outside-in and the inside-out processes are combined. 
 
Fig. 2. Core-processes to integrate OI into the development process [16] 
Sherhan, Albers and Miller [17] show outside-in and inside-out methods in the au-
tomotive industry where German Original Equipment Manufacturers (OEM) and 
supplier in the automotive industry are surveyed and analyzed. The study also rec-
ommended two further steps in the innovation process: An innovation impulse step 
for managing internal and external innovation inputs and an innovation transfer step 
to maximize the benefit in R&D productivity [17]. 
3.3 
Human Factor 
According to Golas [18] a company in its environment represents an open and  
targeted social system. Humans in the field of automotive industry are working in 
complex socio-technical systems. No matter how much technically dominated this 
operational environment is – the creative, social and individual facets of people re-
main very important. Working in complex systems is on the one hand characterized 
by routines and on the other hand by exceptional situations or crises. These situations 
require e.g. that people make decisions under time pressure and high risk or find new 

 
Implications of Open Innovation Approaches on Future PLM 
77 
innovative solutions within very limited time. Therefore people are the highest poten-
tial in a company and have to get major support in communication and collaboration 
to achieve the corporate goals. They need the best possible support by a suitable envi-
ronment - a well-balanced system between human factors, organization and technolo-
gy [19]. In the workplace of the future access to information and technology-related 
knowledge will be much more open and flexible. This simultaneously increases the 
available range of information. In response, employees are increasingly moving into 
information flows and create independent and self-organized individual information 
as well as their own system environment. The transparency of knowledge and know-
ledge holders will continually rise in companies. For practitioners, it is often more 
important to identify the relevant knowledge holders in the companies than the expli-
cit knowledge itself. [19] 
A typical pitfall which has been observed in projects is the NIH syndrome (not in-
vented here) coming from the co-workers within a company. A solution which was 
not developed inside of internal R&D is often not considered trustful and is different 
from solutions of the company. This often results in resistance from the people who 
are working for this company [20]. 
4 
Identified Requirements for Open Innovation in PLM 2020 
The perspectives of the scenarios, the statements of the experts and the analyzed chal-
lenges in the field of PLM are combined to perform a requirements catalogue for 
future PLM. The requirements catalogue is divided into five classes which are  
product, human, organization, process, and IT, taking into account that single re-
quirements can be in one, some or all classes.  
• Product: relationship between overall product and product parts (customer wish - 
product requirements, complexity). 
• Humans/team: individuals and their relationships (team), common understanding 
of mission, associations and threads. 
• Organization: cross-domain and global communication and collaboration  
• Process: interaction of total product development process and sub-processes, trans-
disciplinarity. 
• IT: new technology and methods for new information and communication tools 
(e.g. Web 2.0). 
As a research result the relations of all 317 requirements and classes are shown. This 
paper deals with the catalogue entries that include the topic of OI. Some of them are 
described in detail below. At each headline, the assignment of classes is listed in pa-
rentheses. 
4.1 
Strong Cooperation across Domains (Human, Organization, Process, IT) 
OI processes are benefited in the case that employees are independent because it 
forces them to contact companies and communities to share knowledge with people 

78 
A. Denger, D. Gerhard, and C. Kaiser 
from different areas of interests. OI can be seen as integration of customer-knowledge 
into the existing one. Customers all over the world are able to take part in creation or 
improvement of the products. Requirements to PLM 2020 are particularly: Support 
interaction with the customers in the product development process; integration of 
analytical tools; integration of alternative procurement strategies. 
4.2 
Virtual Work of the Company (Human, Organization, IT) 
Virtual work is one of the key trends of the future. Due to “cloud” technology, fast 
internet-based information exchange as well as communication channels for interac-
tion between customers, employees and all participants of the supply chain in a prod-
uct development process will become an essential part. Problems can be published to 
a community which provides recommendations for the problems and additionally 
makes changes and edits solutions. Information synchronization as a key concept of 
cloud technology forces interactivity and interdisciplinary working what leads to radi-
cal innovations. Creating social networks inside a company will help to share infor-
mation between different departments, discuss tasks and solutions online with the 
required personal of the company. Trust between employees will increase.  
4.3 
Communication in Product Development (Product, Human, Organization, 
Process, IT) 
Customers can take direct participation in the development of the products. All com-
ments of the customers have to be implemented or at least labeled to be considered. 
Language and cultural barriers have to be solved. Power and quickness of the product 
development depends on the interaction between employees and customers. Quick 
review of the development process allows a faster development in general. The com-
munication between two or more participants has to be structured. 
4.4 
Workstation of the Future (Human, Organization, IT) 
Due to increasing mobility each employee needs external access to working data. 
Specialists in small areas of interest will be available on demand and work for one or 
more companies, assisted by virtual assistants. The product service system is inter-
sected with OI as well when the customers give their feedback using different possi-
bilities. One of them is to classify the product in a simple way, e.g. mechanisms as the 
“like” button on “Facebook”. 
5 
Discussion 
OI in the automotive industry is no longer an empty phrase. Sheran, Albers and Miller 
[17] state that in the next 10 years the way of creating and profiting from innovation 
will change completely in the automotive industry. They also delineate that OI is a 
phenomenon that has become increasingly important over the last few years in the 

 
Implications of Open Innovation Approaches on Future PLM 
79 
automotive industry [17]. Gassmann [16] argues that some trends like globalization, 
new technologies, or new business models will foster OI concepts in industry. Gass-
mann´s arguments recover most of the aspects in the scenarios which were developed 
in the FuturePLM project, especially the mega trend globalization described in the 
scenario “Globalization Extreme!” [7].  
The scenario “People take center stages” is formed upon the following assump-
tions: (1) Recognition of importance of employees (2) Mutual trust within companies 
increases acceptance and understanding of PLM due to deployment of new technolo-
gies, processes and organization forms (3) High cultural diversity in companies (4) 
Deep PLM integration, and (5) Flexible infrastructure and working conditions in 
complex business environments [7]. In this regard, people working from diverse loca-
tions can use social software such as wikis and blogs [13] as a modern way of disse-
minating information and knowledge within the company and out of company, which 
creates a simple form of community. Contained in the general assumption that the 
way companies are organized will change dramatically in future is the implication 
that the definition of workplaces and working time models will change as well [19]. 
All shifts recommended above will require equivalent changes in PLM implementa-
tion models, which will affect how goals are defined, how the system is introduced 
and how it is used [8].  
6 
Conclusion and Outlook 
Being able to innovate is the key factor of success for companies in high tech 
branches. OI on the one hand side means customer integration rather than customer 
orientation. On the other hand side, OI approaches leverage the work force and crea-
tivity of the mass of smart and talented people from all around to enable new ways of 
idea generation to solve product development problems. Since our society heavily 
depends on innovative products, the new approaches of innovation processes have to 
be implemented in companies and have to be supported adequately by means such as 
PLM.  
PLM is a concept of how to manage a product in terms of people, workplace and 
organization. The main driver will be the evolution of the concepts of employees as 
part of the value creation process and the growing importance of individual human 
potentials. To manage this successfully, a wide-ranging dialogue with the people af-
fected will be necessary. A new culture of how information is shared needs to be de-
veloped. The technological systems development must follow the developing needs of 
the people and create solutions which meet the needs of both users and tasks in such a 
way that people can use them with enthusiasm.  
The combination of concepts in the field of OI and PLM needs to change the ad-
justment of the company with regards of openness. New methods in research and 
development phases should afford an opportunity to look outside of company  
boundaries.   

80 
A. Denger, D. Gerhard, and C. Kaiser 
Acknowledgments. The authors would like to acknowledge the financial support of 
"COMET K2-Research Centres for Excellent Technologies Programme" of the  
Austrian Federal Ministry for Transport, Innovation and Technology (BMVIT), the 
Austrian Federal Ministry of Economy, Family and Youth (BMWFJ), the Austrian 
Research Promotion Agency (FFG), the Province of Styria and the Styrian Business 
Promotion Agency (SFG).  
References 
1. Koudal, P.: Mastering Innovation: Exploiting Ideas for Profitable Growth. A Deloitte Re-
search Global Manufacturing Study. Deloitte Research (2005) 
2. Reichwald, R., Piller, F.: Open Innovation: Kunden als Partner im Innovationsprozess. Pe-
ter Lang Verlag, Berlin (2005) 
3. Christensen, C.: The innovator’s dilemma. Harper Business New York (2000) 
4. Chesbrough, H.W.: Open Innovation. The New Imperative for Creating and Profiting from 
Technology. Harvard Business School Press, Boston (2003) 
5. Pénin, J.: More open than open innovation? Rethinking the concept of openness in innova-
tion studies. Secétariat du BETA, Strasbourg (2008) 
6. West, J., Gallagher, S.: Patterns of Open Innovation in Open Source Software. In: Che-
sbrough, H., Vanhaverbeke, W., West, J. (eds.) Open Innovation: Researching a New Pa-
radigm, pp. 82–106. Oxford University Press, Oxford (2006) 
7. Schmeja, M., Denger, A.: FuturePLM - Ansätze für ein mitarbeiterzentriertes PLM. In: 
Proceedings of Product Life Live 2011, pp. 129–136 (2011) 
8. Denger, A., Schmeja, M., Unzeitig, W., Schlüter, W., Peschke, F.: Product Lifecycle Man-
agement (PML) - Ein Blick auf den Mitarbeiter von Morgen. ProduktDaten Journal 18(2), 
55–59 (2011) 
9. Denger, A., Unzeitig, W.: Future Product Lifecycle Management (PLM) – A Considera-
tion of Informal Communication as a Key Enabler for Future Product Development. In: 
Rivest, L., Bouras, A., Louhichi, B. (eds.) PLM 2012. IFIP AICT, vol. 388, pp. 435–444. 
Springer, Heidelberg (2012) 
10. Shabtai, A., Elovici, Y., Rokach, L.: A Survey of Data Leakage Detection and Prevention 
Solutions. Springer, New York (2012) 
11. ProSTEP iViP: SP2 – Secure Product Creation Processes, White Paper (2012), 
http://www.prostep.org 
12. Gaudet, E.: DRM vs. ERM: battle to control data. Network World (2006), 
http://www.networkworld.com/news/tech/2006/ 
121806techupdate.html  
13. Gartner: Gartner’s the Top 10 Strategic Technologies for 2011 (2011), 
http://www.gartner.com/it/page.jsp?id=1454221 
14. Stocker, A., Tochtermann, K.: Wissenstransfer mit Wikis und Weblogs. Fallstudien zum 
erfolgreichen Einsatz von Web 2.0 im Unternehmen. Gabler-Verlag, Wiesbaden (2010) 
15. Tredinnick, L.: Web 2.0 and business: a pointer to the intranets of the future? Business In-
formation Review 23(4), 228–234 (2006) 
16. Gassmann, O., Enkel, E.: Open Innovation. Die Öffnung des Innovationsprozesses erhöht 
das Innovationspotential. In: Zfo, 3/2006 (75. Jg.), pp. 132–138 (2006) 
 
 

 
Implications of Open Innovation Approaches on Future PLM 
81 
17. Sherhan, I., Albers, A., Miller, S.: Open innovation in the automotive industry. R&D Man-
agement 40(3) (2010) 
18. Golas, H.G.: Der Mitarbeiter, 9th edn. Verlag Cornelsen Lehrbuch, Berlin (1997) 
19. Denger, A., Stocker, A., Schmeja, M.: Future Workplace. Eine Untersuchung sozio-
technischer Einflüsse auf den Arbeitsplatz der Zukunft. Shaker Verlag Aachen (2012) 
20. Gassmann, O., Sutter, P.: Praxiswissen Innovationsmanagement: von der Idee zum Mark-
terfolg. Carl Hanser Verlag, München (2011) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 83–92. 
DOI: 10.1007/978-3-642-30817-8_9 
© Springer-Verlag Berlin Heidelberg 2013 
 
Challenges of Model-Based Systems Engineering:  
A Study towards Unified Term Understanding  
and the State of Usage of SysML  
Albert Albers and Christian Zingel 
IPEK – Institute of Product Engineering at Karlsruhe Institute of Technology (KIT),  
Kaiserstr. 10, 76131 Karlsruhe, Germany 
{albert.albers,christian.zingel}@kit.edu 
Abstract. Model-Based Systems Engineering is on everyone’s lips as innova-
tive approach to overcome traditional, error-prone document-based product de-
velopment. The Systems Modeling Language (SysML) is the most popular tool 
for model-based development of multidisciplinary systems. Several research 
works and industrial pilot projects have applied the OMG-standardized lan-
guage in the last years, but it has still not become widely accepted. Previous ex-
periences of the authors from several research projects with industry underline 
this statement and have shown that engineers still have trouble in applying 
SysML. This paper investigates possible reasons for this issue and presents re-
sults of a survey regarding term understanding of engineers as well as accep-
tance of SysML. 
Keywords: Term understanding, Common Engineering Language, SysML, 
Systems Modeling Language, Acceptance. 
1 
Model-Based Systems Engineering – Potentials and 
Challenges 
Model-Based Systems Engineering (MBSE) is on everyone’s lips as an innovative 
approach to overcome traditional, error-prone, document-based product development. 
The advantages of using formal models to specify a complex technical system are 
manifold: fewer inconsistencies, less redundancies and concurrently the basis for clear 
communication and sustainable documentation. Furthermore, models can help to 
force systemic thinking, which is often expressed as holistic and function-based think-
ing [1]. Over the last years, several modeling languages and approaches have been 
presented. They all promised to enable their users to model multidisciplinary and 
complex technical systems. The most popular modeling language is SysML, which is 
based on UML, a widely accepted, object-oriented graphical software modeling lan-
guage. Observations of the authors have shown that software engineers and electron-
ics engineers cope well with the provided diagrams for modeling several system  
aspects like structures, sequences or states. Emerging graphical modeling languages 
for embedded systems like MARTE, AUTOSAR or EAST-ADL using similar  

84 
A. Albers and C. Zingel 
 
principles and technologies like the meta-modeling standard MOF (Meta Object Fa-
cility) seem to underline this observation. On the contrary, mechanical engineers have 
much more trouble dealing with such kind of modeling languages. A possible reason 
is that technical terminologies between disciplines differ significantly. Furthermore, 
there is a huge leap of abstraction from concrete discipline-specific models to abstract 
multi-disciplinary system models. An online survey presented in this paper was con-
ducted in order to identify the understanding of some crucial terms in the field of 
MBSE and to determine the state of application of SysML. The survey was sent to 
selected engineers from different disciplines in industry and academia in Germany, 
who are familiar with Systems Engineering (i.e. members of the German Chapter of 
INCOSE, the GfSE). Knowing that the answers will not be representative for all engi-
neers, it was rather intended to provide an indication about term understanding and 
SysML application among “Systems Engineers” as spearheads in establishing MBSE 
in academia and industry. 50 responses (23 from academia, 27 from industry) were 
evaluated. Before presenting findings from this survey, the next chapter will give an 
overview of adjacent research efforts and their results. 
2 
State of Research 
Several studies and academic or industrial pilot projects aimed to gain insights about 
the applicability of MBSE methods and tools. The ProSTEP iViP society conducted a 
survey in cooperation with the Fraunhofer IPK called “PEP2015 – Challenges in 
modern Product Engineering Processes”, which evaluated needs and visions of indus-
try and tool vendors in terms of Systems Engineering [2]. The results showed that 
Systems Engineering methods are applied only occasionally within software engineer-
ing and electrics/electronics engineering. Discipline-specific tools are well-
established; transdisciplinary system architecture interaction is still an unsolved issue 
in industrial practice. Bone and Cloutier determined from another study that especial-
ly large companies are widely aware of the benefit of MBSE and increasingly adopt 
corresponding programs and projects [3]. Their focus is set on architecture modeling, 
requirements traceability and conceptual design of products. Thus, the value for soft-
ware and systems engineers is much more obvious than for hardware engineers or 
managers. Existing organizational structures are frequently not compatible with trans-
disciplinary systems engineering [1], which can be substantiated by missing metho-
dologies for the application of existing standards or modeling languages. Therefore, 
Estefan conducted a survey on the most prominent MBSE methodologies in 2008, 
aiming to mainstream them in industrial application [4]. None of those methodologies 
has significantly established over the last years after this survey. Kasser discusses 
seven myths of Systems Engineering, due to persistent discussions about possible 
reasons for the lacking acceptance and application in industrial product engineering 
[5]. He found out, that there is neither a single broad agreement upon systems engi-
neering processes nor on the adequate application of tools and methods to handle 
system complexity. 

 
Challenges of Model-Based Systems Engineering 
85 
 
Several pilot projects have taken place in order to determine best practices or to 
evaluate first applications of MBSE tools and methods. Friedenthal presented several 
findings from the application of SysML [6]. He stated that MBSE is a cultural change 
and requires well-defined methodologies and handling them requires training in lan-
guage, methods and tools. Karban et al. state challenges in using SysML, which have 
been figured out in the APE (Active Phasing Experiment) project of the SE^2 chal-
lenge team of the GfSE [7]. They propose several tasks for the advancement of 
SysML, which underlines that the language is still under development and will be 
further advanced in the future.  
Other research efforts deal with the definition and understanding of frequently used 
terms in engineering disciplines. An example for such a term is “function”, which is 
for instance understood in software engineering as a piece of software code that 
processes input information towards a certain output. Mechanical engineers on the 
contrary have different connotations for “function”: it can either describe, what a 
system to develop is intended to do or what a system solution actually does. Moreo-
ver, a function is often distinguished between a desired function and an undesirable 
one. Others would name an undesired function an appearing phenomenon, an effect or 
a behavior. Several efforts have addressed the understanding of terms, so has Eckert 
et al. [8] for instance investigated the different notions of the previously mentioned 
example “function” in engineering design. They identified the 5-key-concept of Ver-
maas as the most valuable, but also differentiating definition of this term, meeting 
most of the previously mentioned examples [9]. Vermaas concludes that different 
meanings are required in different situations instead of pursuing a single definition of 
“function” through emphasizing that different meanings are in fact necessary to de-
scribe devices in engineering design. 
Literature review has shown that the challenges in application of MBSE are mani-
fold and leads to the awareness, that the “cultural change” from traditional document-
based towards a model-based development approach has still not taken place. The aim 
of this paper is to identify the cause for this issue and to point out fundamental actions 
to be taken in order to advance MBSE tools and methods. 
3 
Motivation for Further Research 
MBSE aims to improve communication and collaboration between engineers from 
different disciplines and management. Communicating efficiently means to easily 
gain the desired information, provided in a comprehensible and coherent manner, 
which is one basic goal of MBSE. Unified term understanding is crucial for establish-
ing a coherent, formal and coincidentally intelligible modeling language for multidis-
ciplinary systems. Considering all relevant terms and every specialized discipline 
would either lead to a very generic solution like SysML or to a very extensive set of 
specific languages. Even if the idea to apply a common language for all involved 
individuals is promising, none of the existing approaches has established in industrial 
development yet. Possible reasons are a persistent lack of common term understand-
ing or insufficient information representation within existing modeling languages. 

86 
A. Albers and C. Zingel 
 
The approach at hand concentrates on engineers, who are familiar with Systems Engi-
neering paradigms or concerned with Systems Engineering research. They form the 
basis for the harmonization of term understanding and establishing a common lan-
guage in industrial product development. The aim of the survey conducted by the 
authors of this paper is to answer two research questions: 
• 
What is the understanding of the basic terms “function”, “behavior” and “impact 
chain” among Systems Engineers? 
• 
To what extent is SysML applied, what is the perception of the added value of 
SysML for the daily work today and where is improvement potential? 
The results of this survey shall help to harmonize term understanding by clustering 
consistent statements and complement previously identified definitions. Furthermore, 
the demand for certain modeling aspects shall be identified and the suitability of pro-
vided diagrams in SysML for describing those aspects shall be evaluated. The long-
term goal of this ongoing research work is to advance MBSE languages and coevally 
according modeling approaches. 
4 
An Approach towards a Unified Term Understanding 
The presented data in this chapter result from a survey, conducted among German and 
Austrian Systems Engineers from academia and industry. Altogether, 50 responses 
(23 from academia, 27 from industry) have been evaluated. The academic participants 
are PhD-students (14 out of 23), students or postdoctoral researchers. The industrial 
participants range from development engineers over trainers and consultants to prod-
uct-, project- and department-managers. The spectrum of the participants’ expertise is 
shown in Fig. 1. 
 
Fig. 1. Range of expertise of survey participants 
The participating Systems Engineers have expertise is numerous disciplines, which 
helps to gain information from diverse viewpoints. However, this survey is not in-
tended to meet representative statements with statistical evidence, but rather to point 
out tendencies and to collect statements from experts. The survey was divided into 
two sections: the first section asked for the personal understanding or definition of the 
three terms “function”, “behavior” and “impact chain”. The answers regarding term 

 
Challenges of Model-Based Systems Engineering 
87 
 
understanding were given as free statements, whereas every participant could state 
multiple answers per definition (i.e. to subdivide the definitions into multiple aspects). 
Altogether, the participants posed 78 statements for “function”, 70 for “behavior” and 
58 for “impact chain”. The answers were characterized by a high degree of diversity. 
In the following, frequently made contradictory statements are contrasted: 
Table 1. Contrasting pairs of statements towards the term “function“ 
Functions describe the purpose of a system 
Functions realize functional requirements 
Functions describe the role of persons in a 
company 
Functions describe the transformation of 
matter/energy/information inputs into ac-
cording outputs 
Functions are solution-neutral 
Functions are solution-afflicted 
Functions are abstract specifications of 
transformations 
Functions can be described in mathematical 
terms 
Functions describe an active behavior 
Function are an interaction of components to 
achieve a certain behavior 
Concluding, the interpretation of the term “function” is very heterogeneous, even 
among Systems Engineers. The next question asked for the definition of “behavior” 
with explicit distinction to “function”. Behavior is often associated with the perfor-
mance of functions, but beyond that the survey identified a very diverse understand-
ing of this term. Many statements differentiate between static functions and dynamic 
behavior, regarding the latter as a time-dependent aspect. Where functions only define 
the desired behavior, the description of behavior itself can also comprise misconduct 
(i.e. system crash) or undesired functions (i.e. noise emission). Behavior is often seen 
as system reaction towards environmental input stimuli under certain boundary condi-
tions and (measureable) characteristics, which are differentiated between discrete (i.e. 
the event “press button”) and continuous (i.e. transmit torque). Unfortunately, the 
opinions occasionally interfere with others and there is a lack of common understand-
ing. For instance, one participant stated that functions are perceivable, another 
attributes this to behavior. Several statements contradict others regarding the question 
whether “behavior” describes the external view and “function” the internal view on a 
system or vice versa. Some participants confine behavior on the transition between 
system states, others acknowledge behavior to be component-afflicted; still others 
attest behavior to be uncontrollable. Concluding, the statements were highly diverse, 
but none of them embraced all mentioned aspects, which indicates a lack of unified 
understanding of behavior. Some statements told behavior to be a “chain of func-
tions”. Where functions are often modeled as tree structures or using logical control 
flows and the input-processing-output-principle for object flows, “impact chains” 
intend to represent a certain sequence of functions. The resulting statements regarding 
the understanding of “impact chain” are discussed in the next paragraph. 
In contrast to “function” and “behavior”, this term was not known to every partici-
pant. Two of them wrote that they had never heard this term before. The majority of 
the statements define an impact chain as a chain of functions, where input values of a 
function are the output values of the previous function. Some participants regarded 
these chains as high-level linking of systems, others as the internal progress of  

88 
A. Albers and C. Zingel 
 
activities within a function which results in a system behavior. Some statements again 
contrasted: they described impact chains as synonym to traceability from require-
ments over functions towards implementation and test. Impact chains and active 
structure sound similar in German language (“Wirkketten” and “Wirkstrukturen”), but 
are fundamentally different. Impact chains deal with functions, active structures or 
working structures with components. The appearance of several terms in different 
meanings can lead to communication being confusing. This is why the next paragraph 
presents a graphical proposal (Fig. 2), illustrating semantic contexts of the important 
and frequently reoccurring term “function”. The goal of this graphical representation 
is to harmonize the understanding of semantic coherences between frequently reoc-
curring terms in Systems Engineering. The depicted aspects embrace literature re-
search as well as own experiences made in several development projects. 
 
Fig. 2. Proposal for semantic context of “function” 
The definition of function at hand embraces the approach to regard a function as an 
activity, which processes input values (information, energy, material) to output val-
ues. An input equals a cause in terms of triggering (discrete, i.e. “button pressed”) or 
exciting (continuous, i.e. “torque flows”) a function by certain input values. The 
processing of the input values in activities relies on logical (software), physical or 
chemical (mechanical and electrical systems) effects, which can be specified by equa-
tions, presuming a comprehensive knowledge about the system. The output flows 
have an impact on other functions (they can trigger or excite other functions) or result 
in perceivable phenomena (i.e. forces, noise or fields). Additionally, function-relevant 
physical parameters are factored into the processing of flows. Therefore, from the 
viewpoint of the authors, functions are not completely solution-neutral, but they are 
also not component-afflicted. The important awareness is to consider relevant  
properties for feasibility of functions, but not to anticipate an entire solution  
(i.e. component). This is one of the basic principles of the Contact & Channel – Ap-
proach (C&C²-A) for the integrated analysis and synthesis of functions and form of 
technical systems [10]. The graphical representation at hand was part of deriving and 

 
 
formalizing extending aspe
quirements of engineering 
simple example in Fig. 2 in
A similar graphical repres
quently to the questions reg
distort the respective statem
of five statements concerni
and discussed subsequently
Fig. 3. Com
All aspects were predom
opportunity to make annou
contents. Conspicuous is th
deep level of detail, where 
However, several participa
hand and using equations w
other hand. Therefore, the 
mum for the applied conte
always be possible to be sp
complex system “combustio
tic map only using load and
value, depending on the m
3) was criticized, because 
therefore been rephrased to
more certain inputs”. The 
ferred into one output. The
multiple inputs and multipl
conducted in a variety of w
which will become compr
One participant stated that p
ing component, but also fro
Challenges of Model-Based Systems Engineering 
ects for a SysML-profile in order to better meet the 
designers towards application of MBSE [11], [12]. T
ntends to make these theoretical coherences more tangib
sentation was presented to the survey participants sub
garding term definitions in order not to influence or even
ments. The participants were asked to rate the applicabi
ing the presented figure. The results are depicted in Fig
y. 
mprehensibility of semantic context of function 
minantly rated as well applicable. The participants had 
uncements or proposals for improvement of the grap
he second aspect, which is well comprehensible on a v
mostly a few or even one equation can express this eff
nts remarked that this would cause high effort on the 
will not be possible or suffice on low levels of detail on 
level of accuracy should be limited to the necessary m
ext. Considering this condition, a kind of relation sho
pecified between input and output. For instance, the hig
on engine” could be sufficiently modeled by a characte
d engine speed as input values and engine torque as out
modeling purpose. Furthermore, the first statement (cf. F
not every input triggers an activity. This statement 
o “processing activities are triggered or excited by one
schematic graphic only shows one input, which is tra
e meaning behind is not that stringent: an activity can h
le outputs. Moreover, the processing of the inputs may
ways, depending on the characteristics of the input valu
rehensible through further decomposition of the activ
processing does not only apply parameters of the perfo
om adjacent systems.  
89 
re-
The 
ble. 
bse-
n to 
ility 
g. 3 
 
the 
phic 
very 
fect. 
one 
the 
mini-
ould 
ghly 
eris-
tput 
Fig. 
has 
e or 
ans-
have 
y be 
ues, 
vity. 
rm-

90 
A. Albers and C. Zingel 
 
Concluding, the term definitions as well as the remarks contributed to the ad-
vancements of the definition of a common language and associated semantic connec-
tions. The identified inaccuracies have meanwhile been revised in the specification. 
The common language forms the basis for the definition of a model implementation in 
SysML using extending profiles. Therefore, the application of SysML as a leading 
MBSE tool was assessed as well, combined with the identification of relevant model-
ing aspects for engineering tasks as well as improvement potential regarding SysML. 
The results are presented in the next chapter. 
5 
Application State of SysML and Current Advancement Issues 
Firstly, the participants were asked to rate their own SysML experience. Only two out 
of 50 responses had no SysML experience and have therefore not answered the ques-
tions concerning SysML. 7 participants claim their selves as SysML experts, 5 as 
advanced modelers and 19 as modelers with basic experience. The remaining 16 par-
ticipants have no modeling experience, but know SysML diagrams from literature. 
Regarding application of the provided diagram types of SysML, the participants 
were asked to evaluate their particular benefit in representing the desired information 
of a modeled system. The results are illustrated in Fig. 4. The most frequently applied 
diagram type is the Internal Block Diagram for modeling internal system structures. 
Its benefit was rated as “crucial” by 40% of all participants. The benefit of Activity 
Diagrams was also rated as “crucial” by 40% at a little less application ratio. The 
most unknown diagram type is the Constraints Diagram, which is intended to 
represent constraints between model entities like parameters or requirements, merely 
48% know this diagram type and only 4% rate its benefit as “crucial”. 
 
Fig. 4. Benefit of SysML diagrams for representation of particular modeling aspects 

 
Challenges of Model-Based Systems Engineering 
91 
 
The next question demanded for the added value of SysML for modeling major 
tasks of discipline-crossing systems. In contrast to the provided diagram types, func-
tional modeling has the most added value for users, which could indeed be conducted 
by IBD’s, but would presume to apply according methods like the FAS-method 
(Functional Architectures for Systems, cf. [13], [14]), which applies this diagram type 
for enabling an additional view (containing functional blocks) in order to overcome 
the gap between solution-neutral modeling of activities and flows and the performing 
(physical) structure with interfaces in IBD’s.  
The following question asked for the importance of the previously mentioned as-
pects towards their general importance for the participant’s work tasks. The compari-
son points out that the added value of SysML fits the user’s need pretty well, but the 
results have shown that modeling of a system is much more demanded than data ex-
change between tools. Finally, the users had the opportunity to remark improvement 
potential regarding SysML in order to facilitate broader application of the modeling 
language in academic and industrial product engineering. 24 more or less detailed 
remarks were stated. Half of the participants remarked that not SysML itself should 
be improved, but rather the provided modeling tools, especially regarding usability 
(i.e. navigation through models, support of special views like matrices or special dia-
grams, handling etc.). Furthermore, six participants remarked missing modeling me-
thods or guidelines and a high learning effort, five users missed particular aspects (i.e. 
decision tables, chances and risks). Insufficient Model2Model-transformation-support 
and variant modeling was also mentioned multiple times. 
6 
Conclusion and Outlook 
The paper at hand has clarified that term understanding even among Systems Engi-
neers in academia and industry is still very heterogeneous, but features tendencies 
towards corresponding aspects. This enables the opportunity to harmonize the under-
standing of basic terms like function and behavior in order to provide a basis to for-
malize those terms within modeling languages with according entities, attributes and 
relations. A graphical representation has been presented to the survey participants, 
which encountered predominantly positive responses. Hence, a formal specification of 
modeling elements can be derived incorporating minor advancements. Furthermore, 
the survey results have shown that SysML seems to be an adequate modeling lan-
guage to cope with important modeling aspects supporting daily engineering work. 
Nevertheless, several advancements of SysML and in particular the modeling tools 
are still necessary in order to enable a wide application of Model-Based Systems En-
gineering in product development processes. Therefore, the IPEK conducts continuing 
high efforts in development of new, extending modeling aspects realizing the needs of 
product designers and managers (i.e. [11], [12]). Furthermore, a SysML extension for 
function-based modeling with derivation of dynamic structures through further im-
plementation of the paradigms of the Contact & Channel – Approach (C&C²-A) [10] 
is under development in order to obtain better acceptance among model users [15]. 
The long-term goal is to achieve more human-centered MBSE tools and methods. 

92 
A. Albers and C. Zingel 
 
References 
1. Friedenthal, S., Moore, A., Steiner, R.: A practical Guide to SysML – The Systems Model-
ing Language, 2nd edn. Elsevier, Amsterdam (2011) 
2. Tazir, N.: Viel diskutiert, selten wirklich angewendet - Systems Engineering. In: Produkt-
Daten Journal (February 2011)  
3. Bone, M., Cloutier, R.: MBSE Survey Results (Summary). Stevens Institute of Technology 
(2012) 
4. Estefan, J.: Survey of Model-Based Systems Engineering (MBSE) Methodologies. 
INCOSE MBSE Focus Group (2008), http://omgsysml.org  
5. Kasser, J.: Seven Systems Engineering Myths and the corresponding Realities. In: Pro-
ceedings of the Systems Engineering Test and Evaluation Conference, Adelaide, Australia 
(2010) 
6. Friedenthal, S.: SysML: Lessons from Early Applications and Future Directions. In: 
INCOSE INSIGHT, vol. 12(4). INCOSE (2009) 
7. Karban, R., Hauber, R., Weilkiens, T.: MBSE in Telescope Modeling. In: INCOSE 
INSIGHT, vol. 12(4). INCOSE (2009) 
8. Eckert, C., Alink, T., Ruckpaul, A., Albers, A.: Different notions of function: results from 
an experiment on the analysis of an existing product. Journal of Engineering Design 
(2011) 
9. Vermaas, P.: Technical Functions: Towards Accepting Different Engineering. In: Interna-
tional Symposium Series on Tools and Methods of Competitive Engineering (TMCE), An-
cona, Italy (2010) 
10. Matthiesen, S.: Seven Years of Product Development in Industry – Experiences and Re-
quirements for Supporting Engineering Design with ‘thinking tools’. In: Proceedings of 
the International Conference on Engineering Design (ICED), Copenhagen (2011) 
11. Albers, A., Zingel, C.: Interdisciplinary Systems Modeling Using the Contact & Channel-
model for SysML. In: Proceedings of the International Conference on Engineering Design 
(ICED 2011), Copenhagen, Denmark (2011) 
12. Zingel, C., Albers, A., Matthiesen, S., Maletz, M.: Experiences and Advancements from 
One Year of Explorative Application of an Integrated Model-Based Development Tech-
nique Using C&C2-A in SysML. IAENG International Journal of Computer Science 39(2) 
(2012) 
13. Lamm, J.G., Weilkiens, T.: Functional Architectures in SysML. In: Proceedings of the Tag 
des Systems Engineering (TdSE), Munich, Germany (2010) 
14. Korff, A., Lamm, J.G., Weilkiens, T.: Werkzeuge für den Schmied funktionaler Architek-
turen. In: Proceedings of the Tag des Systems Engineering (TdSE 2011), Hamburg, Ger-
many (2011) 
15. Albers, A., Zingel, C.: Extending SysML for Engineering Designers by Integration of the 
Contact & Channel – Approach (C&C2-A) for Function-Based Modeling of Technical Sys-
tems. In: Proceedings of the Conference on Systems Engineering Research (CSER 2013), 
Atlanta, Georgia, USA (accepted 2013) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 93–102. 
DOI: 10.1007/978-3-642-30817-8_10 
© Springer-Verlag Berlin Heidelberg 2013 
 
Model Based Design with Systems Engineering  
Based on RFLP Using V6 
Sven Kleiner and Christoph Kramer 
:em Engineering Methods AG, Rheinstr. 97, 64295 Darmstadt, Germany 
{kleiner,kramer}@em.ag 
Abstract. Today, coping with the different workflows, methods and tools of 
this inter-disciplinary approach to product development throughout a product’s 
life-cycle is the key challenge for a company. There is evidently a need for re-
quirements engineering and management, as well as model-based design and 
engineering. More specifically, however, what is required is a unique and inte-
grated methodology for requirements engineering and management, functional 
and logical design, as well as physical design in different domains for the multi-
disciplinary development process based on a Systems Engineering  approach 
early in the design process. In this paper, the RFLP approach (Requirements – 
Functional – Logical – Physical) will be presented as the baseline for model-
based design with Systems Engineering  that enable close interaction and col-
laboration between the different engineering disciplines render resources and 
processes more efficient, enhance quality, and ensure that the target system ul-
timately meets the requirements, while reducing design cycle time and engi-
neering lead time. 
Keywords: Systems Engineering, RFLP, Requirements Engineering and Man-
agement, Functional & Logical Design, Mechanical Design, V6. 
1 
Introduction 
In recent times, the term Systems Engineering  has increasingly cropped up in the 
context of developing, testing and validating technical systems [1-3]. The “Gesell-
schaft für Systems Engineering” (GfSE) describes it as a comprehensive engineering 
activity necessary for the development of complex products [4]. Besides the tradition-
al fields of application (the aerospace industry), the method is becoming increasingly 
important in other areas such as the automobile industry and medical technology. 
Here, Systems Engineering  serves as a construct for solving complex problems at 
system level. This procedural model incorporates methods, procedures and resources 
for developing and implementing technically complex systems, from requirements 
definition, system analysis and system development all the way to integration of the 
finished system. Systems Engineering  focuses on the process of problem solving, 
which comprises the two components of system design and project management; its 
fundamental criterion is “systems thinking”. Hierarchization, in other words the 

94 
S. Kleiner and C. Kramer 
 
process of moving from the general to the particular, is just as fundamental a feature 
as the application and creation of models as structuring aids [5]. The model-based 
abstracting of complex systems often enables the latter to be broken down into their 
component parts and the minimized complexity of the real system to be depicted [6]. 
Emerging from this development, the systems thinking approach has taken center 
stage in product development with the intention of producing innovative and globally 
competitive products that meet customer requirements in full [7]. Thanks to the multi-
tude of disciplines (mechanics, electrics/electronics, software) involved in today’s 
innovative products in particular, a command of Systems Engineering  is increasingly 
becoming a genuine competitive advantage for companies which develop and manu-
facture products. For example, implementing a competitive system for developing 
mechatronic products would be difficult to achieve with any degree of success with-
out first introducing a method of Systems Engineering  and model-based develop-
ment techniques. It is for this reason that VDI Guideline 2206 was modeled on the 
Systems Engineering  methodology with the aim of ensuring methodical support for 
the cross-domain development of mechatronic systems while proposing an end-to-end 
development environment for mechatronic systems [8].  
2 
The RFLP Approach 
When it comes to designing and developing complex, multi-disciplinary products, it is 
essential that the manifold customer requirements, system functions and operating 
principles of the various disciplines are described and integrated within a common 
product model. In virtual product development, the various disciplines such as me-
chanical engineering, electrical/electronic engineering and information technology 
have access to specific methods and CAx systems which, as a rule, are employed only 
at certain stages of the product development cycle (e.g. in design, computation and 
simulation). For this reason, integrated development environments are needed to 
maintain a holistic view and provide end-to-end support for all systems [9].  
2.1 
The Systems Engineering Procedural Model 
In mechatronic product development, Systems Engineering  is divided into three 
main phases: system analysis, system development and system integration. In system 
analysis, the product being developed is described theoretically in the worksteps re-
quirements definition, functional analysis, logical architecture design and component 
specification, and specified further via the resulting product models. The system  
development phase creates product development data (including e.g. 3D CAD mod-
els, behavior models). In the system integration phase, the developed components are 
simulated and tested, integrated in the system, and subjected to continuous verifica-
tion and validation. In Figure 1, the so-called V model is used to illustrate this  
procedure. 

 
Model Based Design with Systems Engineering Based on RFLP Using V6 
95 
 
 
Fig. 1. Virtual product development based on Systems Engineering and RFLP 
2.2 
General Description of the RFLP Approach 
Previous IT systems used in product development offered only limited support of 
Systems Engineering  methods and data of the kind detailed e.g. in ISO Standard 
10303 AP 233 “Systems Engineering”. This was due to the absence of an integrated 
information model spanning all product development phases and disciplines, and 
because many of the IT tools used were geared to specific application areas and con-
sequently were unable to be integrated adequately [9]. For the first time, the so-called 
RFLP approach for developing mechatronic products could succeed in providing the 
called-for holistic support for design and development projects based on Systems 
Engineering . The acronym RFLP stands for Requirements engineering, Functional 
design, Logical design and Physical design (the 3D CAD model), and describes the 
process of systematic product development from system analysis to system develop-
ment, comprising the descending branch of the aforementioned V model based on 
VDI Guideline 2206 “Development methodology for mechatronic systems” 
(VDI2206).  
The RFLP approach was first implemented as a basis for Systems Engineering  in 
the conventional V6 PLM environment of Dassault Systemes. This approach was 
tested with the ENOVIA and CATIA systems, taking this solution as an example. 
Figure 1 shows the artifacts R-F-L-P, along with the development phases system 
analysis, system development and system integration as supported by the CATIA V6 
CAx system and the PLM platform ENOVIA V6 for Systems Engineering. 
 

96 
S. Kleiner and C. Kr
 
2.3 
Requirements Engi
The first phase of the RFL
prises requirements enginee
with VDI Guidelines 2206 
question are recorded and m
the V6 environment in clos
be imported directly into E
ENOVIA environment, th
available to the CATIA CA
Any changes to these spec
components and are therefo
environment. Figure 2 dem
ENOVIA V6 and CATIA V
 
Fig. 2. Requirements engineer
2.4 
Functional Design 
At the Functional design sta
to link the requirements st
structure. The main and sub
be displayed and structured
in terms of specification a
main and sub-functions fo
structure, whereby sub-func
function (or part function) 
in CATIA V6, these are re
can comprise one of two t
connections are defined as 
flow” connections can be m
ramer 
ineering and Management 
LP approach is called requirements engineering and co
ering and management of all specified requirements. In l
and 2221, client requirements with regard to the produc
managed [8, 10]. First, a requirements model is created
se synchronicity with ENOVIA V6. The requirements 
ENOVIA from Microsoft Word, for example. Once in 
he requirements are managed and made simultaneou
Ax platform based on the V6 integrated information mo
cifications are automatically synchronized between syst
ore always up-to-date and globally available within the 
monstrates the support of and interaction between Wo
V6 systems during requirements engineering. 
 
ring and management using Word, ENOVIA and CATIA syste
age, the VPM Functional Logical Editor workbench is u
tructure to the functional structure and the logical syst
b-functions, and the logical components of the system 
d graphically. The functions are derived from requireme
nd design, and thus expand the requirements model. T
rm a basic framework with which to create a functio
ctions are linked to form main or general functions. A s
is a transaction broken down into its individual eleme
epresented in blocks. Links between individual functi
types: according to engineering design [11], “data flo
a flow of energy, materials or signals. In addition, “con
modeled as so-called activation flows. 
om-
line 
ct in 
d in 
can 
the 
usly 
del. 
tem 
V6 
ord, 
ems 
used 
tem 
can 
ents 
The 
onal 
sub-
nts; 
ions 
ow” 
ntrol 

 
Model Based Design with Systems Engineering Based on RFLP Using V6 
97 
 
2.5 
Logical Design 
By integrating the Dymola IT solution in CATIA V6, it is possible to create the logi-
cal model and then generate a dynamic behavior description using the open Modelica 
modeling language. Model-based design first involves defining a logical system mod-
el as a 2D graph in CATIA V6, connecting the relationships between the various 
components used. On this basis, an architecture concept is created for the system that 
describes operating principles as solutions for the defined logical components. With 
the aid of what is known as Dynamic Behavior Modeling in V6, a specific system 
behavior modeled with Modelica is stored for each logical component, thus gradually 
forming a complete, simulation-ready system. Figure 3 shows a sample logical model 
resulting from behavior modeling in V6. 
 
 
Fig. 3. Functional design and schematic diagram for electronic scales 
2.6 
Physical Design 
With RFLP-based design and development, full simulation of a virtual prototype is 
facilitated with the aid of a 3D CAD design model (Physical view). To this end, a 3D 
CAD model is created using the design tools already available in CATIA V5. The 3D 
parts and assemblies generated via the CATIA/Modelica integration process are now 
incorporated into the Dymola simulation environment, where they supplement the 
logical model – now equipped with the behavior description – with the physical cha-
racteristics set out in the 3D CAD model. Thus, alterations made to the CAD model 
directly affect the simulation model and are fully linked to the logical system behavior 
description in the integrated information model. 
 

98 
S. Kleiner and C. Kramer 
 
2.7 
Sample Application for Evaluating the RFLP Approach in V6 
The RFLP approach in V6 was evaluated using the example of a LEGO Mindstorm 
NXT 2.0 robot, and this is discussed below. Modeled on a Segway personal transpor-
ter, the robot in question was a two-wheeled, single-axle, electrically driven vehicle, 
able to maintain balance during forward motion with the aid of an electronic drive 
control. Management of the robot’s forward motion and rotation about its own axis 
for the purpose of turning right or left was by remote control. A LEGO Mindstorm 
NXT 2.0 kit comprising two servo-motors with integrated rotary sensor, a gyro sensor 
for determining position, data communication lines and a set of LEGO modules were 
available to build the robot. The behavior of the NXT robot with CATIA V6 system 
was modeled, simulated and optimized, starting with an analysis of the requirements 
and creation of a requirements list, followed by the design of the functional structure 
and system architecture. 
A list of requirements for the sample application was compiled in MS Word, so 
that it could be imported into ENOVIA V6 using Microsoft Office Requirements 
Management Integration. For this purpose, certain passages in the requirements list 
were defined as requirements, comments or chapters and marked in color in the doc-
ument. It was possible to configure these definitions individually during the import 
process. In this step, the list of requirements was declared e.g. as a set of system re-
quirement specifications, and a distinction drawn between client/user requirements, 
and functional/non-functional requirements. On completion of the import process, the 
requirements specifications have been integrated into the ENOVIA platform and are 
depicted both as a requirements model in the RFLP structure tree (Requirements 
view) and in the form of a 2D graph in CATIA V6 systems (see Figure 4). 
 
 
Fig. 4. Physical design of the LEGO Mindstorm NXT 2.0 robot 

 
Model Based Design with Systems Engineering Based on RFLP Using V6 
99 
 
In the VPM Functional Logical Editor of V6 Systems Workbench, the functional 
model was then extrapolated from the requirements model and stored in the RFLP 
structure tree (Functional view). The functions of the NXT robot are displayed in the 
form of a block diagram, differentiated by main and sub-function and linked together 
by means of functional flows .  
Fig. 5 shows the robot’s general mode of operation. The robot’s general task of 
"maintaining balance while moving" is formulated in the main function and divided 
into the three sub-functions "Maintain balance", "Manage movement" and "Generate 
electrical energy". These sub-functions are detailed further into sub-functions of lesser 
complexity, extending all the way to functions such as "Transform electrical energy 
into mechanical energy", "Query motion specification" and "Adapt mechanical ener-
gy". Connecting the individual sub-functions together produces a functional design, 
i.e. the robot’s general mode of operation. 
 
Fig. 5. Requirements for LEGO Mindstorm NXT 2.0 robot 
The logical design details the functional architecture via the model-based devel-
opment of a simulation-ready behavior model for the robot based on defined operat-
ing principles and selected technology solutions.  
The robot’s logical architecture comprises the elements sensor, controller, actuator 
and control loop. For the sensor, a gyro sensor was adapted to measure the robot’s 
angle (tilt) and position. Mechanical energy is converted into electrical power (actua-
tor) by two electrical servo-motors, which transmit parameters of the gearbox setting 
to the controller via an integrated rotary sensor. The controller (a 32-bit microproces-
sor) evaluates and calculates all recorded signals and commands. The control loop is 

100 
S. Kleiner and C. Kramer 
 
interpreted as an inverted pendulum that establishes contact with the ground via two 
wheels and a single axle, and is governed by a PID regulator integrated in the  
controller. 
 
Fig. 6. Functional design of the LEGO Mindstorm NXT 2.0 robot 
With the aid of the Dynamic Behavior Modeling workbench and Modelica, beha-
vior descriptions are stored for the functions and logical modules and these are then 
calculated and simulated on the basis of Dymola in CATIA V6 systems. Figure 7 
shows the Lego Mindstorm NXT 2.0 robot’s electrical drive system created using 
Modelica. 
 
 
Fig. 7. Dynamic behavioral modeling of the LEGO Mindstorm NXT 2.0 robot 

 
Model Based Design with Systems Engineering Based on RFLP Using V6 
101 
 
CAD-based modeling of individual components and assemblies in CATIA, and the 
associated definition of a system’s form and mechanical properties have been around 
for years. But the integrated CATIA 3D Body Modelica Library now allows the CAD 
model to exert a direct influence on the simulation behavior, merging the physical 
model with the behavior model. This Modelica library allows parameters such as 
mass, inertia and density to be transferred to the simulation environment directly from 
the geometry or 3D CAD model. After calculation, the system behavior is described 
with the aid of the simulation in the form of diagrams, and the movement of the 3D 
model visualized in an animated sequence. The simulation run is recorded via live 
plots, which are also used for evaluation and optimization purposes. 
3 
Systems Engineering and the RFLP Approach in Practice 
The development of complex technical products calls for comprehensive, multi-
disciplinary engineering and a holistic approach in the early phases of development. 
The RFLP approach facilitates multi-disciplinary, model-based development and 
supports the early analysis, evaluation and optimization of the technical system. This 
approach was tested using the V6 PLM platform and, specifically, CATIA systems.  
The special feature of CATIA V6 systems is that the items of RFLP information 
are represented on the basis of an integrated data model, so that links between indi-
vidual development phases and models can be established as and when required. 
RFLP partial models are linked to each other via so-called implement relations and 
allow the user e.g. to jump from an individual function – whether main or sub-
function – to the implemented requirement or the logical design. In addition, the 
transparency and traceability of development steps and results are secured by means 
of informative representations (e.g. the traceability report). The links between the 
individual RFLP representations mean that data can be validated and verified on a 
continuous basis.  
With V6, the V model can be used to develop technical products in a process that 
extends from requirements engineering and management via the functional analysis 
and logical architecture all the way to the physical design of the product in question. 
The engineer can therefore see at a glance which requirements led to the implementa-
tion of a certain logical behavior and the effect this had on the 3D geometry. The V6 
PLM platform thus provides an end-to-end Systems Engineering  solution along with 
options for generating and managing the objects that are created during product de-
velopment. To this end, all modeling results and product information, as well as simu-
lation data and results, are stored in the central ENOVIA PLM environment, where 
they are available on the database server in an object-oriented, structured format. 
Which means that a common product model spanning all disciplines and featuring 
neither interfaces nor breaks in the media chain can be used for Systems Engineering. 
The model-based development of technical systems requires engineers to have 
knowledge of model-based development techniques and of appropriate modeling 
languages. Model-based development in an interdisciplinary team of several engi-
neers calls for appropriate modeling methods and techniques (e.g. modeling rules and 

102 
S. Kleiner and C. Kramer 
 
conventions) in order to craft models that are at once transparent, robust and main-
tainable [1]. V6 currently offers a proprietary modeling language for use in the func-
tional and logical design stages of a project. In practice, however, SysML appears to 
be prevailing as the standard language. Support for and integration of SysML and 
other standards for system development, e.g. AUTOSAR, in the automobile industry 
are necessary for enterprise-wide, model-based development.  
Today we are seeing other technologies and trends already playing a certain role or 
coming to the fore in practical terms – and thereby complementing Systems Engineer-
ing  in general and model-based development based on RFLP in particular (e.g. mod-
el-based testing, automatic code generation, co-simulation). 
Systems Engineering  is a general, procedure-based model that is not geared to 
specific development challenges and which provides multi-disciplinary development 
teams with a common means of communication [12]. In addition, the RFLP approach 
is a successful means of supporting model-based development. The methods and pro-
cedures presented here have been tried and tested successfully in practice in a sample 
V6 application. While the selection and implementation of appropriate integrated IT 
development tools is necessary when introducing Systems Engineering  and the 
RFLP method, this is not sufficient by itself. The necessary development and support 
processes must be established, and the project team members involved in Systems 
Engineering  must be well qualified and motivated in order to implement this com-
prehensive methodology successfully in practice. 
References 
1. Alt, M.: Modellbasierte Systementwicklung mit SysML. Carl Hanser Verlag, München 
(2012) 
2. Sendler, U.: http://www.plmportal.org (accessed March 22, 2012) 
3. Stark, R., Beier, G., Rothenburg, U., Woll, R.: Modellbasiertes Systems Engineering – 
Durchgängige Entwicklung mit erlebbaren Prototypen. Digital Engineering Magazin 
(March 2012) 
4. Gesellschaft für Systems Engineering e.V.: GfSE e.V. und Systems Engineering, 
http://www.gfse.de (accessed on April 17, 2012) 
5. Daenzer, W. F., Huber, F.: Systems Engineering Methoden und Praxis. Verlag für Indu-
strielle Organisation, Zürich (1997)  
6. Cellier, F.E.: Continous System Modeling. Springer, New York (1991) 
7. Janschek, K.: Systementwurf mechatronischer Systeme. Springer, Heidelberg (2010) 
8. Verein Deutscher Ingenieure: VDI-Richtlinie 2206, Entwicklungsmethodik für mechatro-
nische Systeme. VDI-Gesellschaft, Düsseldorf (2003)  
9. Kleiner, S.: Föderatives Informationsmodell zur Systemintegration für die Entwicklung 
mechatronischer Produkte. Shaker Verlag, Aachen (2003)  
10. Verein Deutscher Ingenieure: VDI-Richtlinie 2221, Methodik zum Entwickeln und Kon-
struieren technischer Systeme und Produkte. VDI-Gesellschaft, Düsseldorf (1993) 
11. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Konstruktionslehre, Grundlagen. Springer, 
Heidelberg (2012) 
12. Züst, R.: Einstieg ins Systems Engineering. Orell Füssli Verlag, Zürich (2004) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 103–112. 
DOI: 10.1007/978-3-642-30817-8_11 
© Springer-Verlag Berlin Heidelberg 2013 
 
Consolidating Product and Process Information  
of Connections – A System-Theoretical Approach 
Fabian Rusitschka1,2, Efstratia Zafeiriou2, Hansgeorg Binz3, and Daniel Roth3 
1 Graduate School of Excellence Advanced Manufacturing Engineering (GSaME),  
University of Stuttgart, Nobelstr. 12, 70569 Stuttgart, Germany 
2 AUDI AG, 85045 Ingolstadt, Germany 
3 Institute for Engineering Design and Industrial Design (IKTD), University of Stuttgart,  
Pfaffenwaldring 9, 70569 Stuttgart, Germany  
fabian.rusitschka@{gsame.uni-stuttgart.de,audi.de}  
Abstract. The selection of the optimal connection is one of the most crucial 
parts in product development as it determines both, product properties and pro-
duction processes. In multi-variant series production, functionality presumed, 
process cost is more and more in the focus of the selection. Designers face a big 
challenge in evaluating the feasible connections and surveying them holistical-
ly. Main challenge is the consideration of both, product and process related  
characteristics as they are documented according to different methodologies and 
in different sources. Therefore, a system-theoretical approach for consolidating 
product and process information is developed enabling designers effective de-
signing and surveying the possible connections for given parts to be assembled. 
A case study evaluates impact on design time and connection quality. 
Keywords: product and process documentation, mechanical connection, DFA, 
process selection, systems-theory. 
1 
Connections in Multi-variant Series Production 
Mechanical connections strongly influence product properties and production 
processes. Studies for the metal working industry indicate that connections determine 
up to 50% of the labor time [1] and 70 – 80% of the manufacturing costs [2].  
Selecting the connection during concept design determines product characteristics 
that occur either in production, during times of product utilization or at the end of the 
lifecycle. Influencing those characteristics at a later point in development is either not 
possible at all or only with high effort. For this reason, selection of the most appropri-
ate connection is one of the most crucial tasks in product development.  
From an OEM perspective two major trends will increase the significance of these 
figures: individualization and sustainability. Due to individualization the number of 
feature options and consequently the number of different parts to be assembled will 
rise further [3]. Each new assembly requires an individual connection that needs to be 
designed separately. The second relevant trend - sustainability - currently is fulfilled 
in product development either through new sustainable materials (e.g. natural fiber) or 

104 
F. Rusitschka et al. 
via lightweight materials (e
depend on new connection
spectively not optimal be ap
Described tendencies inc
connection further, as varie
dered) and changes (numbe
In reference to the prod
while selecting the connec
ness or positioning accurac
result is a multi-criteria pr
space, defined by the numb
those.  
According to an applic
scribed (Section 2), existin
(Section 3) before a new ap
a case study of bolted co
Verweisquelle konnte nich
2 
Challenges in th
The solution space is defin
the type of connection used
the number of solutions, wh
challenge of selecting the 
surveyed: the assembly of a
 
(a) control unit
To estimate the number 
surveyed assembly, each of
An internal study of 3214 
database for the results desc
e.g. magnesium, titanium, carbon fiber). Both approac
n techniques as the conventional techniques cannot, or 
pplied to the new materials and material mixes.  
crease the complexity of the task of selecting the optim
ety (number of different connection techniques to be con
er of different connections to be designed) increase.  
duct, designers need to keep various requirements in m
tion. These are functional requirements like forces, tig
cy but also process-related aspects like assembly cost. T
roblem that needs to be applied to an extensive solut
ber of connections, type of connections and specification
cation-oriented approach the context of discovery is 
ng approaches are surveyed and requirements are deri
pproach is developed (Section 4) that is finally evaluated
onnections in the automotive industry (Section Fehl
ht gefunden werden.). 
he Selection of Connections 
ned by three dimensions: the number of connection poi
d and the dimension of the connections. In order to quan
hich are part of the solution space and thereby evaluate 
optimal connection, a standard assembly (see Fig. 1
a control unit (Fig. 1a) to a metal hold (Fig. 1b). 
 
t 
(b) metal hold 
Fig. 1. Assembly control unit 
of different connections, part of the solution space of 
f the solution space’s dimensions is analyzed individua
bolted connections in a modern medium-class car is 
cribed in the following. 
ches 
 re-
mal 
nsi-
mind 
ght-
The 
tion 
n of 
de-
ived 
d in 
ler! 
ints, 
ntify 
the 
) is 
the 
ally. 
the 

 
Consolidating Product and Process Information of Connections 
105 
The number of connection points theoretical possible is defined by the size of the 
part to be assembled and the minimal distance between the connection points. Even 
when smaller than the theoretical possible number of connection points, for further 
calculation 15 connection points per part assembled are taken as a reference corres-
ponding to the study (see Fig. 2). 
 
Fig. 2. Number of connection points (authors own study) 
For the second dimension of the solutions space, the different connection types are 
surveyed. The study describes ten different kinds of bolted connections (see Fig. 3). 
 
Fig. 3. Bolted connections (authors own study) 
# connection 
points /assembly 
1 
2 
3 
14 
10 
8 
7 
6 
5 
4 
# assemblies 
 
a        b       c       d      e        f       g       h        i       j        k       l 
 
connection 
technique 
# assemblies 
a: bolt and weld nut 
c: bolt and through hole (metric thread) 
e: bolt and blind hole (coarse thread) 
g: bolt and straddling dowel 
 i: bolt and nut and through hole 
k: bolt and blind rivet nut 
b: nut and weld stud (metric thread) 
d: bolt and snap nut  
f: bolt and blind hole (metric thread) 
h: bolt and through hole (coarse thread) 
j: nut and integrated bolt (coarse thread) 
l: bolt and punched nut (metric thread) 

106 
F. Rusitschka et al. 
The variation in dimension of the connection types as third dimension of the solu-
tion spaces is even more versatile than connection points and connection types. The 
combination of five different diameters, three different thread types and thirty differ-
ent lengths lead to more than 100 different dimensions - differences in the characteris-
tic of bolt and nut as head geometry not considered. 
Combination of the described dimensions demonstrates for the comparatively sim-
ple assembly of a control unit more than ten thousand possible solutions. Necessary 
condition for the identification of the optimal connection, due to a given set of re-
quirements, is the derivation of all connections part of the solution space and the abili-
ty to assess those methodologically corresponding to the set of requirements. 
Due to the proven large number of theoretical possible connections, designers de-
pend on methodic support and computational help for this task. As the different con-
nections need to be evaluated holistically [4] according to described functional and 
nonfunctional criteria like process cost, the derivation of the different connections 
needs to include not only product, but also process information. 
That calls for a general description of the connection between parts of an assembly,  
the ability to variegate the configuration of the connection and the consideration of 
both, product and process information. Following existing approaches of product and 
process information structure are surveyed towards their ability to solve described 
challenges. 
3 
Approaches towards Product and Process Documentation 
Existing approaches towards the integration of product and process information (Sec-
tion 3.1) are analyzed for their applicability to the described challenge of connection 
selection (Section 3.1).  
3.1 
Integrated Product and Process Documentation of Connections 
Beside the broad work on product and on process documentation few approaches 
towards the integration of the two fields can be found in literature. The most relevant 
approaches will be detailed in the following. 
 
Grabowski95. With the integrated product and production data model Grabowski [6] 
offers a approach towards the improvement of data management in product develop-
ment. For this purpose a object oriented data model was developed, that describes 
information about the entire lifecycle of a product or production tool. With the de-
scription of manufacturing processes, individuals and workplaces linked to the prod-
uct, the integration of process information into the product model is achieved. 
 
Munoz06. Munoz [7] describes an approach for the identification of inconsistencies 
in connections and supports thereby design for assembly with a model for the evalua-
tion and analyze of connections. The model consists of a functional and a structural 
part. The structural part allows the description of geometrical and physical  
characteristics of the assembly, the functional part describes a method for the analysis 
of the connections. 

 
Consolidating Product and Process Information of Connections 
107 
Therefore, elementary attributes (PLUG attributes) are defined for the description 
of the parts. Besides geometrical information like the position and characteristics of 
the contact surface of the parts to be assembled, also information about the connection 
element is documented in the structural model. With the help of multi agent systems 
PLUG attributes of the parts to be assembled are compared and analyzed in order to 
identify inconsistencies. From this, an expert can determine the according production 
processes. 
 
Groll08. Groll [8] offers with interconnection based product and process documenta-
tion a method for the customers related series manufacturers aiming for the optimiza-
tion of data structure and management through an alternative to the existing methods 
of hierarchically oriented product structures. Products are therefore configured as a 
web of parts and interconnections. An interconnection describes the connection be-
tween two specific parts and holds process specific information like capabilities, time, 
instructions, procedures, utilities and tooling.  
3.2 
Conclusion 
Described state of the art presents a broad base in the fields of product and process 
information, as well as several approaches towards the integration of those. Especially 
the work of Groll [8] offers an important approach towards the described research 
question. Even though, as the motivation of the approaches is either the documenta-
tion of product and process [6, 8], or the analyze and checking of the connection  
[7], none of the described approaches offers the required level of detail in order to 
enable the assisted design of connections regarding economic criteria. 
3.3 
Requirements to the Consolidation of Product and Process 
Documentation 
To attain the assisted design of connections, following requirements towards the do-
cumentation of product and process of connection need to be fulfilled: 
1. Solution independent description of the assembly (parts to be assembled and con-
nection) 
2. Different degrees of detail and different views (manufacturing, assembly) 
3. Holistic description of all processes 
4. Evaluation of resulting changes due to modification of the parts to be assembled 
The requirements result from a survey among a group of 15 persons, involved in the 
process of designing, planning and assembling connections in the automotive  
industry. 
4 
Product and Process Information of Connections –  
A System-Theoretical Approach 
In order to fulfill above defined requirements (Section Fehler! Verweisquelle konnte 
nicht gefunden werden.) towards the consolidation of product and process informa-
tion of connections, a system-theoretical approach was developed that describes a 

108 
F. Rusitschka et al. 
connection not longer as the relation between the assembled parts (see Fig. 4), but as 
an independent element in the system of the assembly.  
 
Fig. 4. Connection as element of the system assembly 
Thereby, the connection element defines an independent subsystem consisting out 
of the individual joints. Each of the joints itself defines an individual subsystem, con-
sisting of joining elements and joining process (see Fig. 5). These three layers allow 
for the documentation of all information about the connection, including assembly 
and manufacturing processes. As of this closed description, the connection can be 
evaluated and compared holistically to others. 
Following the different layers (4.1) and elements of the models (4.2) are described 
in detail. 
4.1 
Layers of the Connection Model 
Corresponding to the systems-theory the system consists of elements and relations 
between them. These have functional and structural relations. As the elements can 
define (sub-) systems themselves, they can inhere also hierarchical relations: 
 
Fig. 5. System-theoretical three layer description of a connection 
1st layer. General description of the assembly, consisting of two or more parts and 
the connection. The first level defines the parts to be assembled (pt1, pt2), the connec-
tion system (c) as well as the requirements towards the assembly. 
 
pt1: part 1 
pt2: part 2 
c: connection 
pt1 
pt2 
c 
pt1 
pt2 
c 
1st layer 
2nd layer 
3rd layer 

 
Consolidating Product and Process Information of Connections 
109 
2nd layer. Subsystem of the connection of the 1st level constitutes from the individu-
al connection points c1,…,i and their configuration. 
 
3rd layer. Subsystems of the connection points c1,…,i constitutes from the connec-
tion elements and production processes. 
4.2 
Elements of the Connection Model 
While 1st and 2nd layer represent the structural and hierarchical relations, product and 
process information is represented in the 3rd layer. Therefore, a detailed description of 
the 3rd layer’s subsystems:  
Each of the systems contains connection elements and related processes. Assump-
tion is the assembly of a product by starting with one part and then successively add-
ing parts until the completion of the product. Thereby, the part closer to the core of 
the product (mount) carries index 1, while the part fixed carries index 2 (when con-
necting the door hinge to the a-pillar of car, the a-pillar is closer to the core, takes the 
function of a mount and carries index 1, the hinge accordingly index 2. But when 
connecting the door to the hinge, the hinge is closer to the core and carries index 1, 
the door accordingly index 2). In order to define the single elements, above described 
assembly of a control unit, using a bolted connection is surveyed. Each of the bolted 
connections is a single 3rd layer subsystem, consisting of four individual elements (see 
Fig. 6). 
• Connection Element (ce) defines the part of the mechanical connection added to 
the assembly in the production process 
• Function Element (fe) defines the part of the mechanical connection that is part of 
the assembly (mount) 
• Assisting Element (ae1) defines the modification of part 1 necessary to enable the 
application of the connection technique 
• Assisting Element (ae2) defines the modification of part 2 necessary to enable the 
application of the connection technique 
 
Fig. 6. System-theoretical description of a bolted connection with connection elements 
 
bolt:  
connection element (ce) 
Through hole:  
assisting element (ae2) 
weldnut:  
function element (fe) 
Through hole:  
assisting element (ae1) 
ae2 
ce 
fe 
ae1 

110 
F. Rusitschka et al. 
The described elements represent the physical parts of the connection as shown in 
Fig. 6 and define the connection sufficient. This representation is completed by the 
assignment of the related processes. Therefore, five different processes are defined 
and linked to the corresponding elements of the 3rd layer (see Fig. 7). Furthermore, 
the related processes are described: 
• Production Process Connection (ppc) defines the interaction of connection element 
and function element – screwing bolt in weldnut 
• Production Process Connection Element (ppce) defines the interaction between the 
connection element and part1 – inserting bolt in through hole 
• Production Process Function Element (ppfe) defines the interaction between the 
function element and part 2 – welding the weldnut to part 2 
• Production Process Assisting Element1 (ppae1) defines the preparation of part 1 – 
adding the through hole into part 1 
• Production Process Assisting Element2 (ppae2) defines the preparation of part 2 – 
adding the through hole into part 2  
 
Fig. 7. Sufficient description of a connection with all corresponding processes 
Each of the processes can be further detailed into assembly processes, tools, logistic 
processes and informational processes. The production processes can be derived au-
tomatically or manually by the designer, due to the related elements. 
For evaluation the initially described assembly of the control unit is redesigned us-
ing the system-theoretical approach towards consolidation of product and process 
information. 
5 
Evaluation 
In order to evaluate the developed system-theoretical approach towards the integra-
tion of product and process information, the surveyed connection of the control unit 
(Section 2) was redesigned. Therefore in the first step the characteristics (weight, size, 
material, etc.) of the parts to be assembled (part1, part2) and requirements towards the 
ae1 
ppae1 
ce 
ppce 
fe 
ae2 
ppc 
ppae2 
ppfe 

 
Consolidating Product and Process Information of Connections 
111 
connection (c) (forces, tightness, position accuracy, etc.) were defined (1st layer). 
Based on this information, different connection scenarios were created by the varia-
tion of the number of connection points and their orientation (2nd layer). For the sce-
narios, the load on every connection point was calculated and the connection points 
were dimensioned according to the requirements (3rd layer). By the allocation of the 
production processes to the elements, the connection models were completed, allow-
ing for the derivation of the assembly expenditures using a methodology described in 
[5]. Corresponding to this derivation the functional and economic optimal connection 
was selected.  
With the support of the system-theoretical approach towards the integration of prod-
uct and process information a new connection was designed, that offers the same func-
tionality as the known design but is characterized by less and smaller connection points. 
Thus the assembly causes 40 percent less expenditures than the actual connection. 
6 
Summary and Further Work 
Current methods of product description like variant parts list, open/closed variant 
parts list, rule based parts list etc., only focus on the parts to be assembled, while me-
thods of process description like assembling planning, variant work plans etc., only 
focus on the process of joining the parts. None of the existing methods is capable to 
join and structure product and process information sufficiently. 
The approach allows the representation of the assembly and production processes 
as well as the relations and dependencies between those in a structured model and 
expands thereby the state of the art. 
Designers and planners benefit from the approach by the ability to survey all suita-
ble connections of a given solution space corresponding to a given set of requirements 
quick and comprehensible. Via the application on different automotive assemblies the 
approach is evaluated, demonstrating the definition of the entire solution space, as-
sessment of all resulting connections and selection of the optima, demonstrating sig-
nificant improvement during the design process and in the quality of the solution. 
Described system-theoretical approach towards the consolidation of product and 
process information of connections is a contribution to the field of information and 
knowledge management in product development. The approach is limited to produc-
tion processes that are characterized by fixed and standardized manual work cycles as 
they can be found in multi variant series production.  
Further studies could focus on either the survey of different joining processes or on 
the implementation of the approach in a software tool. 
References 
1. Ehrlenspiel, K., Danner, S., Schlüter, A.: Verbindungsgestaltung für montagegerechte Pro-
dukte. In: Montage und Demontage, Aspekte Erfolgreicher Produktkonstruktion, pp. 179–
206. VDI-Verlag, Düsseldorf (1992) 
2. Bauer, O., Althof, W., Haferkamp, H., Hamkens, H., Kaschner, M., Koller, R., Stellberg, 
M.: Handbuch der Verbindungstechnik. Carl Hanser Verlag, München (1991) 

112 
F. Rusitschka et al. 
3. Herlym, W.J.: Zur Abbildung variantenreicher Erzeugnisse in der Automobilindustrie. VDI 
Progress Reports, vol. 16(52). VDI, Düsseldorf (1990) 
4. Ponn, J., Lindemann, U.: Konzeptentwicklung und Gestaltung technischer Produkte, p. 227. 
Springer, Heidelberg (2011) 
5. Rusitschka, F., Binz, H., Bunte, J.: Methodology for a holistic view on assembly expendi-
tures. In: 21st International Conference on Production Research: Innovation in Product and 
Production. Fraunhofer Press, Stuttgart (2011) 
6. Grabowski, H., Meis, E., Hain, K.: Integriertes Produkt- und Produktionsmodell. Informa-
tionstechnik und Technische Informatik 37(5), 32–38 (1995) 
7. Munoz Toledo, M.B.: Agentenbasierte Modellierung und Analyse von Verbindungen im 
Produktentstehungsprozess, pp. 34–73. Shaker Verlag, Aachen (2006) 
8. Groll, M.: Interconnection based product and process documentation, pp. 77–86. Ph. D. the-
sis, Twente (2008) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 113–123. 
DOI: 10.1007/978-3-642-30817-8_12      © Springer-Verlag Berlin Heidelberg 2013 
A Structured Approach for Function Based 
Decomposition of Complex Multi-disciplinary Systems 
Felician Campean, Edwin Henshall, Unal Yildirim, Amad Uddin, and Huw Williams 
University of Bradford, EDT3 Ford Suite, Bradford, BD7 1DP, UK 
F.Campean@braford.ac.uk 
Abstract. The aim of this paper is to introduce the Systems State Flow Diagram 
as a structured approach to high level solution-independent function based de-
composition of a complex multi-disciplinary system. The approach is discussed 
in the context of existing function modelling frameworks and in relation to cur-
rent practice in industry. A generic case study is used to introduce the approach 
and to highlight the salient features, followed by an illustration on its applica-
tion to the analysis of an electric vehicle powertrain. Experience with the prac-
tical application of the approach with engineering teams is discussed. 
Keywords: Systems engineering, function decomposition, system state, func-
tion flows, product development. 
1 
Introduction 
The complexity of automotive systems has increased dramatically over the past 
couple of decades driven by the accelerated pace of innovation and introduction of 
new technologies to enhance customer satisfaction and to mitigate environmental and 
safety concerns in a highly competitive marketplace. This escalation in complexity 
has brought significant technical challenges, compounded by the increase in sophisti-
cation of the control systems employed to manage the integration of the new technol-
ogies within the system. Managing multi-disciplinary system integration is recognized 
as being a very difficult task [1-3], which is not well supported by the existing 
frameworks and tools for function decomposition of complex systems [1]. The partic-
ular difficulty with the analysis of multi-disciplinary (i.e. electro-mechanical, elec-
tronic, control, software) complex systems is that system to system interactions are 
very difficult to assess or predict early in the systems engineering design process.  
Observation of current systems engineering design practice in industry has hig-
hlighted the prevalence of a structural based decomposition of systems, generally 
underpinned by clustering analysis on a design structure matrix (DSM) [4-5]. Alloca-
tion of design responsibilities to engineering design teams mirrors the system decom-
position, which means that design teams have responsibility for design units or 
chunks, and not for functions, which often means that “system integration” functions 
are left to chance. For illustration, taking the example of an exhaust system (based on 
 

114 
F. Campean et al. 
 
the authors’ observations across a number of OEMs), the design responsibility is typi-
cally split between 4 teams on the basis of the disciplinary grouping of the sub-
systems involved: exhaust pipework, after treatment components, sensors, and control 
software. There is no overall responsibility for the integration of the system on the 
basis of its main function (to transport and condition exhaust gas), which is ultimately 
the root cause for common field issues, e.g. associated with the regeneration of the 
exhaust particulate traps (a well-known cross-industry issue). This illustrates that the 
conventional approach of structure - function decomposition works well for the inte-
gration of reasonably small electro-mechanical systems, but it does not provide a 
good approach for complex mechatronic systems, which is a conclusion similar to 
literature findings [1-2].  
The aim of this paper is to introduce the Systems State Flow Diagram (SSFD) as a 
simple but effective tool to support the high level analysis and function based decom-
position of a complex system. The method will be explained with a generic case 
study, followed by an illustration of an industry-based example of an electric vehicle 
powertrain, and a discussion of the experience with the application of the method on a 
broader basis for complex automotive systems. The organization of the paper is as 
follows: section 2 gives an overview of common methods for functional decomposi-
tion of complex systems; section 3 outlines the proposed method, followed by discus-
sion and conclusions in section 4. 
2 
Overview of Function Modeling Approaches 
The function of an engineered system is commonly defined in relation to the trans-
formation of the inputs to the outputs; the inputs and outputs are usually considered in 
terms of types of energy (E), material (M) and information (I) [6-7]. Design decom-
position is based on mapping the flows of energy, material and information through 
the system [7]; sub-functions are defined as successive operations on flows, with out-
put from one sub-function providing the input to the next. Several graphical dia-
grammatic representations have been developed to visualize the system’s functional 
structure and to facilitate the analysis by teams of engineers, e.g. Function Trees, 
System Block Diagrams, System Boundary Diagrams, Function Flow Block Diagram, 
FAST Diagrams, Integrated Definition for Function Modeling (IDEF0) [8-9]. 
The description of the function commonly follows a verb – noun structure [9]. The 
“functional basis” approach by Stone and Wood [10] provides a consistent framework 
including a taxonomy for functions and a coherent representation of the overall func-
tion in terms of interconnected sub-functions, defined as operations on flows (of E, 
M, I). Design development is carried out through successive decompositions of func-
tions and sub-functions into lower level sub-functions. This is usually done iteratively 
within the design synthesis, i.e. an iterative decomposition in the functional and the 
design solution domains, referred to as zigzagging in the context of axiomatic design 
[11]. At any level, system integration is supported through interface analysis, facili-
tated by a design structure matrix (DSM) type approach [4-5], which aims to identify 

 
A Structured Approach for Function Based Decomposition 
115 
 
the relationships or linkages [12] between components. Clustering analysis on the 
DSM is used to group components into structure-function units [4].  
Several other methods for defining functions have been proposed (see for example 
[13] for a review), aiming to support different phases of the design process, in particu-
lar by enhancing the understanding of the relationship between function (defined in 
relation to the utility to the user or customer) and the intended behavior and structure 
of the system. The Function – Behavior – State (FBS) [14] framework suggests that 
functions are defined in relation to states of the structures (design objects), which are 
represented by entities, their attributes, and relations between entities. Within FBS the 
functions are defined by the combination of verb-object which relate to the designer 
intentions, and behaviors expressed through adjectives, which can instantiate the 
function, thus embedding the time dimension into the function definition. While pro-
viding a strong framework for functional modeling which lends itself to useful soft-
ware implementation and automation, the FBS framework has been found rather dif-
ficult to implement in engineering practice [15]. 
The Object-Attribute-Function framework [16] proposes a similar approach, with 
the input and output defined as generic objects thought of as tangible entities that 
have attributes (such as mass), or information (expressed as signals that can be de-
tected). Within the OAF framework, the input and output objects are described by 
their measurable attributes, with a clear taxonomy developed to describe both.  
A generic issue with all frameworks is that they do not support the high level func-
tional analysis / decomposition of a complex system in a solution independent man-
ner. At any level, decomposition tends to be based on brainstorming, i.e. by asking the 
question “how is this function achieved”. The Contact and Channel (CC) framework 
[17-18] addresses this issue by providing a coherent structure for functional decompo-
sition based on identification of working surface pairs (WSPs) at the system input and 
output, as well as the channel that connects the WSPs within the engineered system. 
A working surface is described in terms of a state characterized by measurable 
attributes, and the system function defined as “transfer of one state into another” [19]. 
The functional decomposition is carried out by defining surface pairs with the chan-
nel, which correspond to design subsystems. While this framework is structured and 
powerful, it uses a taxonomy which is not always conducive to the analysis of multi-
disciplinary systems (e.g. control or software systems engineers are unlikely to adopt 
the language of working surface pairs). 
3 
System State Flow Diagram 
A review of current frameworks for function modeling pointed out the need for a 
more structured tool to support high level solution-independent function analysis and 
decomposition for complex multi-disciplinary systems. Analysis of current methods 
and practices in industry, discussed in [20], has also highlighted the need for a struc-
tured tool to address the heavy reliance on less structured approaches (largely based 
on brainstorming) in carrying out practical function decomposition analysis. The re-
quirements for such a tool can be summarized as follows: 

116 
F. Campean et al. 
 
• To be integrated with other tools commonly used in industry – such as Boundary 
Diagrams and Interface Matrices [8], to encourage broad take-up of the tool; 
• To have a graphical (diagram based) representation to facilitate the development of 
shared mental models [21] within the engineering team carrying out the analysis; 
• To be portable across disciplines (electro-mechanical, control and software) and 
domains (design / process);  
• To promote axiomatic design principles of domain separation and primacy of func-
tion and solution-neutral thinking in systems engineering design analysis. 
The System State Flow Diagram (SSFD) was first outlined in [22], and further  
discussed in [20] with a comprehensive example of application to the analysis of an 
electric vehicle powertrain. The SSFD has been further applied to the analysis of  
automotive systems, giving a rich experience and feedback from many teams of  
engineers. 
This paper will explain the principle of the approach on the basis of a generic ex-
ample – design analysis of a generic Bread Toasting System (BTS), followed by an 
illustration of the application in an industry based context. 
3.1 
Principles of a State Flow Diagram 
Block Diagrams are commonly used to represent an engineered system. At high level, 
a system (conceptually thought of in terms of its function and physical structure / 
design solution) is represented as a black box, showing the inputs and the outputs to 
the system, as shown in Figure 1. Coherent with FBS [14] and OAF [16] function 
modeling frameworks, the input and the output can be thought of as generic objects 
described by a set of measurable attributes. For the BTS example, the sliced bread 
(input) and toast (output) can be thought of as objects characterized by physical and 
chemical attributes (e.g. density, humidity, porosity) and geometry (e.g. thickness).  
As discussed, a generic system function definition is “the transferring of one state 
into another” [19]. The SSFD embeds this definition of the system function in a 
graphical representation, shown in Figure 2. The SSFD follows the general principles 
of state diagrams (such as state transition diagram or reliability state diagram [23]), in 
that by convention the boxes denote the states of the objects and the arrows denote the 
functions required to achieve the transfer from one state to another. The important 
feature of this representation is that it divorces the consideration of function from the 
consideration of the design solution. In the SSFD framework the functions can be 
thought and articulated purely in terms of transformation between states of objects, as 
 
 
Fig. 1. BTS System Block Diagram 
Fig. 2. BTS High level SSFD  

 
A Structured Approach for Function Based Decomposition 
117 
 
described by their observable and measurable attributes. So the function of the BTS to 
“toast bread” can be more clearly expressed as “transform bread into toast”, which 
can be defined and assessed in terms of the change in the physical (e.g. humidity, 
density, thickness, weight), chemical (e.g. oxidation) and geometrical attributes. 
3.2 
Function Decomposition Based on State Flow Diagram 
The function decomposition using the SSFD is based on identifying intermediate 
states between the input state and the output state. The definition of the intermediate 
states should follow the same logic and structure as the one used for the input and 
output; i.e. consistent with the OAF framework, we need to think of observable states 
characterized by the measurable attributes of the objects they relate to. The SSFD 
maps the flow of the states through the system and the functions required to achieve 
the transitions between the states. Figure 3 illustrates the development of the SSFD 
for the BTS system, showing only the main flow through the system, which in this 
case is that of bread. The functions defined can be mapped in terms of the object 
attribute changes required to transition between states. For example:  
• “F1 – load bread” and “F3 – remove toast” are associated with changes in the at-
tributes relating to spatial location and orientation;  
• “F2 – Toast Bread” relates to the change in physical and chemical attributes of the 
bread when it is converted into toast. 
 
Fig. 3. SSFD for the Main Flow in BTS 
The box around the system defines the “system boundary”, i.e. the limits of the scope 
for responsibility for the design team. Once the main flow is depicted, the engineering 
design analysis and synthesis can be carried out to identify the best way in which the 
functions can be delivered through the engineered system. Based on the understand-
ing of the science of toasting, the “working principle” of a BTS can summarized as 
“reduce bread moisture content and oxidize the surface of the bread”. The BTS engi-
neering design task associated with function F2 is to find a way of delivering the 
function; in this case this relates to the delivery of heat energy from a given source to 
the “Bread retained in BTS”. Figure 4 shows an updated SSFD which maps the en-
ergy states flow through the BTS system. This shows an “Energy Source” as another 
input to the system, an intermediate state of “Heat”, and a function F4 to “Convert 
Energy into Heat”. Given that the SSFD delivered a solution independent analysis, the 
BTS engineer has the freedom to consider a variety of sources of energy (electrical, 
gas, chemical, sunlight), and a range of design concepts as ways of converting 
 

118 
F. Campean et al. 
 
this energy into heat to achieve the F2- “toast bread” function.  Therefore, the attrib-
utes of the 2 new states shown in Figure 4 (i.e. “Energy Source” and “Heat”) cannot 
be fully defined until technology and system design decisions have been made.  
Function F2 – “Toast Bread” is achieved directly by “Applying Heat to the Bread 
retained in the BTS”, i.e. heat will change the physical and chemical attributes of 
bread, transforming it into toast. In general, coherent with the OAF framework, an 
engineered function is completely defined in terms of the triad of (1) input object 
state, (2) output object state and (3) transforming energy or process. The functional 
decomposition is complete when functions are fully defined, i.e. specified in terms of 
the triad defined above, typically requiring several function decomposition iterations. 
 
 
Fig. 4. SSFD for the Bread Toasting System (BTS) 
The key feature of the functional representation in Figure 4 is that it is fully in the 
functional domain and solution-independent. As such, the SSFD in Figure 4 could 
provide an adequate representation for a range of BTS designs, e.g. a common house-
hold electric bread toaster (which holds the bread in a case), a hotel type bread toaster 
which uses a conveyor belt, or an ecological bread toaster where focused sunlight heat 
is used to toast the bread. The SSFD diagram in Figure 4 could equally represent a 
process of toasting bread under a gas grill or over a barbeque, if all the transportation 
functions on the main flow are performed by the user. 
From this SSFD representation we can directly extract a high level BTS Function 
Tree, Figure 5. This has been derived from a structured decomposition of the system 
in a solution neutral way, and not based on directed brainstorming (How-Why) which 
is the typical approach in practice [9]. 
 
 
Fig. 5. High Level Function Tree for the BTS 
 
 

 
A Structured Approach for Function Based Decomposition 
119 
 
3.3 
Design Analysis of System Control Features Using SSFD 
Most modern systems are required to have a control system in place to support the 
robust achievement of the consumer requirements. Consumer requirements are gener-
ally directed at the attributes of the output object state. For example, the “browning” 
level of the toast is a critical attribute that can be related to the customer requirement 
of “consistent good toast”. The browning level as an attribute of the output state, de-
pends on the attributes of the input state and the functions (transformations) in the 
system. Figure 6 illustrates an attribute transformation matrix for the BTS system that 
supports the identification of the functions which can influence the browning level of 
bread. As discussed, functions F1 and F3 are associated with a change in the location 
and orientation attributes, and do not affect the browning level. However, the input 
state (in terms of the physical and chemical properties of the bread – such as moisture 
content, density, structure, chemical composition), and function F2 (in terms of the 
heat rate and overall heat exchanged) have a clear influence onto the browning level. 
The designer has little control over the attributes of the input state (arbitrary choice by 
the user), hence the control strategy must be directed at function F2, i.e. control the 
heat rate and the overall heat applied to the bread. The way in which a control feature 
is designed and implemented depends on the design solution adopted and the required 
level of control. For example, on a common domestic toaster the heat control can be 
based on time or heat rate setting, whereas for a hotel type bread toasting system the 
control is based on the belt speed and / or heat rate settings. The BTS control system 
 
Critical output 
state attribute 
Input 
State 
F1: Load Bread 
in BTS 
F2: Apply Heat to 
Bread in BTS 
F3: Remove 
Toast 
Browning level 
X 
 
X 
 
Fig. 6. Attribute Transformation Matrix 
 
Fig. 7. SSFD for a BTS with Control System 

120 
F. Campean et al. 
 
can be manual (i.e. adjusted by the user), or automated if based on in process mea-
surements (e.g. sensing the “browning” level). For illustration, Figure 7 shows a 
SSFD “customized” for a common household electric bread toaster, showing two 
options of control system. 
The function “Convert energy into heat” has been further decomposed into “F4: 
Control supply of EE ” and “F5: Convert EE into Heat”, reflecting the design choices 
made – i.e. to use mains electrical energy (EE) as energy source, which is converted 
into radiated heat (applied to the bread, function F2) through a heat converter. The 
design elements that achieve functions F4 and F5 (i.e. the switched connector and the 
heat converter) have also been indicated on the SSFD. Figure 7 illustrates 2 control 
strategies: 
(1) Manual control based on timing, where the user selected setting for the 
browning level is converted to a timing control signal (F6.1), which controls 
the switch connector (F6.2);  
(2) Automatic control, based on sensing the toast browning level (F7.1), the 
conversion of the sensed browning level into a control signal (F7.2), which 
controls the switch connector (F7.3).  
It is important to note that the control functions are still solution independent; e.g. 
sensing the toast browning level can be achieved in a number of different ways which 
can be considered by the design engineers. Fundamentally, the only design decisions 
that have been made relate to the use of mains electric energy to power the BTS. The 
function tree can be updated to include the control functions derived from the SSFD, 
as sub-functions to “convert energy into heat” function on the high level BTS function 
tree shown in Figure 5.  
3.4 
SSFD Illustration for an Electric Vehicle Powertrain System  
Figure 8 illustrates the application of the SSFD to the analysis of an electric vehicle 
powertrain (EVP) for a small truck application [20]. In an EVP system there are 3 
main flows, i.e. (i) to charge and store energy; (ii) to deliver controlled torque to the 
rear axle; (iii) to provide power for vehicle consumer units. The SSFD analysis shown 
in Figure 8 integrates these 3 flows into a compact functional representation of the 
 
 
Fig. 8. SSFD for an Electric Vehicle Powertrain 
 

 
A Structured Approach for Function Based Decomposition 
121 
 
EVP system. This analysis was conducted by a multi-disciplinary team of engineers, 
and used to define the functional breakdown of the system and the allocation of asso-
ciated design responsibilities on a functional basis. The subsequent analysis of the 
functional integration of the EVP system, at the system level, was developed through 
an enhanced interface analysis, described in [20]. 
4 
Discussion and Conclusions 
The main aim of this paper was to introduce the Systems State Flow Diagram as a 
structured approach to high level solution-independent function based decomposition 
of a complex multi-disciplinary system. The review of current frameworks and tools 
for functional modelling as well as the discussion of current industrial practice 
pointed out the need for the development of such a tool. The SSFD draws on the FBS 
and OAF frameworks for function modelling, and introduces a state transition based 
graphical representation, which is intuitive to use, yet powerful in terms of maintain-
ing the discipline of solution-independent thinking in the analysis of system decom-
position on a function basis. The reasoning structure underpinning SSFD is similar in 
principle with the Contact and Channel framework [17-19], but it has the advantage 
that it offers a more straightforward graphical representation, it is more portable 
across multiple engineering disciplines (including mechatronics and control systems), 
and easier to integrate with other tools commonly used in industrial practice. 
The Bread Toasting System case study illustrated the development of the SSFD 
and showed that the requirements outlined in section 3 are met. The integration with 
other tools was illustrated in terms of the development of the Function Tree (Figure 5) 
– which is a common basis for engineering design deployment and analysis. The 
broader integration with other engineering tools (including interface analysis and 
Failure Modes and Effects Analysis) was discussed in [20]. The SSFD provides a 
system representation that is easy to understand, thus supporting the achievement 
within an engineering team of a common understanding of the functional decomposi-
tion of the system in a fundamental, solution independent way. Figure 7 clearly illus-
trates the ability of the SSFD to support multi-disciplinary analysis, by showing that 
control features can be accommodated in a seamless way within a SSFD.  
The SSFD has been rolled out with two major automotive OEMs, and feedback 
from the engineering teams has been extremely positive, in that it is a clear and easy 
to use tool, supporting a thorough and objective analysis of the system. It supports a 
better understanding of the functions that need to be delivered by the engineered sys-
tem, and the way in which engineering design tasks can be allocated to teams to en-
sure a better integration of the system, focused on the customer required functions. A 
strong feature of the SSFD is that it improves communication between disciplines in 
the sense that it is a tool that can be equally used by engine component design engi-
neers and engine calibration engineers, responsible for control feature development. 
The SSFD can be applied at all levels of the systems engineering cascade, and pro-
motes a seamless integration between product and process engineering design on the 
basis that the SSFD is a similar representation to a process flow map. 

122 
F. Campean et al. 
 
The authors’ experience of using the SSFD discussed in this paper has been mainly 
in conjunction with complex automotive systems. However, the SSFD framework and 
tool can be applied to any complex system. 
References 
1. Van Beek, T.J., Tomiyama, T.: Requirements for Complex Systems Modeling. In: The 
18th CIRP Design Conference – Design Synthesis (2008) 
2. D’Ameilo, V., Chmarra, M., Tomiyama, T.: Early design interference detection based on 
qualitative physics. Res. Eng. Design 22, 223–243 (2011) 
3. Lindemann, U., Maurer, M.: Facing Multi-Domain Complexity in Product Development. 
In: Proc. 17th CIRP Design Conference. Springer, Berlin (2007) 
4. Pimmler, T.U., Eppinger, S.D.: Integration analysis of product decompositions. In: ASME 
Design Theory and Methodology Conference, Minneapolis (September 1994) 
5. Browning, T.: Applying the Design Structure Matrix to System Decomposition and Inte-
gration Problems: A Review and New Directions. IEEE Transactions on Engineering 
Management 48(3), 292–306 (2001) 
6. Pahl, P., Beitz, W., Feldusen, J., Grote, K.H.: Engineering Design: A systematic approach, 
3rd edn. Springer (2007) 
7. Ulrich, K.T., Eppinger, S.D.: Product design and development, 3rd edn. McGraw-
Hill/Irwin, New York (2003) 
8. Webb, R.D.: Investigation into the application of robustness and reliability tools to the de-
sign process. MSc thesis, University of Bradford (2002) 
9. Otto, K., Wood, K.: Product design: Techniques in reverse engineering and new product 
development. Prentice Hall, New Jersey (2001) 
10. Stone, R.B., Wood, K.L.: Development of a Functional Basis for design. Journal of Me-
chanical Design 122, 359–370 (2000) 
11. Suh, N.P.: The principles of design. Oxford University Press, New York (1990) 
12. Jarratt, T.A.W.: A model-based approach to support the management of engineering 
change. PhD thesis, University of Cambridge (2004) 
13. Srinivasan, V., Chakrabarti, A., Lindemann, U.: A Framework for Describing Functions in 
Design. In: Proc. 12th Int. Design Conference DESIGN 2012, Dubrovnik, pp. 1111–1122 
(2012)  
14. Umeda, Y., Tomiyama, T.: Functional Reasoning in Design. IEEE Expert: Intelligent Sys-
tems and their Applications 12, 275–288 (1997) 
15. Van Beek, T.J., Tomiyama, T.: Integrating Conventional System Views with Function-
Behaviour-State Modelling. In: The 19th CIRP Design Conference – Design Synthesis 
(2009)  
16. Sickafus, E.N.: Unified Structured Inventive Thinking: How to Invent. Michigan (1997)  
17. Albers, A., Ohmer, M., Eckert, C.: Engineering design in a different way: cognitive pers-
pective on the contact and channel model approach. In: Visual and Spatial Reasoning in 
Design, July 22-23, 2003. University of Sydney, Cambridge (2004) 
18. Albers, A., Braun, A., Clarkson, P.J., Enkler, H.-J., Wynn, D.: Contact and channel model-
ling to support early design of technical systems. In: International Conference on Engi-
neering Design – ICED 2009, Stanford, August 24-27 (2009) 
19. Albers, A., Oerding, J., Alink, T.: Abstract objectives can become more tangible with the 
contact and channel model (C&CM). In: Models and Methods for Variation Management 
in Global Product Development: Proc. 20th CIRP Design Conference, pp. 203–213 (2011) 

 
A Structured Approach for Function Based Decomposition 
123 
 
20. Campean, I.F., Henshall, E., Brunson, D., Day, A., McLellan, R., Hartley, J.: A structured 
approach for function analysis of complex automotive systems. SAE Int. J. Mater. Ma-
nuf. 4(1), 1255–1267 (2011) 
21. Henshall, E., Campean, I.F., Brunson, D.: Robust and Reliable Teamwork within Engi-
neering Projects, SAE Technical Paper 2011-01-1273 (2011), doi:10.4271/2011-01-1273 
22. Campean, F., Henshall, E.: A function failure approach to fault tree analysis for automo-
tive systems. Society of Automotive Engineers Technical Paper: 2008-01-0846 (2008) 
23. Rausand, M., Hoyland, A.: System Reliability Theory: Models, Statistical Methods, and 
Applications, 2nd edn. Wiley Series in Probability and Statistics (2003) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 125–135. 
DOI: 10.1007/978-3-642-30817-8_13 
© Springer-Verlag Berlin Heidelberg 2013 
 
Methodology for Identification of Adaptive Reusable 
Modules in Automated Production Systems 
Konstantin Kernschmidt1, Philipp Klein2, Nasser Jazdi3, Peter Göhner3,  
Michael Weyrich2, and Birgit Vogel-Heuser1 
1 Institute of Automation and Information Systems, Technische Universität München, Germany 
{kernschmidt,vogel-heuser}@ais.mw.tum.de 
2 Chair of Automated Manufacturing and Assembly, Universität Siegen, Germany 
{philipp.klein,michael.weyrich}@uni-siegen.de  
3 Institute of Industrial Automation and Software Engineering, Universität Stuttgart, Germany 
{nasser.jazdi,peter.goehner}@ias.uni-stuttgart.de   
Abstract. The development and design of mechatronic systems require detailed 
knowledge in mechanical, electrical and software engineering. In order to face 
challenges, like reduced time-to-market, reduced costs and increased variability, 
complex systems should be modularized and the identified mechatronic 
modules reused for the development of new variants or versions. This paper 
focuses on the identification of adaptive reusable modules with an appropriate 
level of granularity and the representation of the deduced modules to support 
the development. Based on the collection of deduced modules, with defined 
functions and structure, new systems can be designed through a combination of 
the appropriate modules.In this contribution, the methodology will be presented 
through a use case. This example shows how appropriate modules are identified 
in a first step. In the second step, the impact on the engineering process is 
shown by the support of the selection and design of the modules. 
Keywords: Modularization, Development and Design, Automated Production 
Systems, Engineering tool. 
1 
Introduction 
Automated production systems nowadays are complex mechatronic systems and de-
mand engineering solutions tailored to the customers’ needs and commissioned within 
a restricted time. In order to stay competitive the innovation cycle, the time-to-market, 
and the costs have to be decreased, while the quality has to stay on a high level. 
Interviews with companies in the machine and plant manufacturing industry 
showed that modularization is an important innovation factor to face the described 
challenges, while missing solutions for module management act as an innovation 
inhibitor. However, the benefits of a reuse of modules are not capitalized, because the 
reuse of the modules is limited to discipline specific tools, module searching mechan-
isms are not developed efficiently, and interdisciplinary modules are not clearly  

126 
K. Kernschmidt et al. 
defined [1]. A literature study on the integration of knowledge in the engineering 
process has shown that the automatic combination of components and modules to 
complex production systems is a recent development trend [2]. A prerequisite for an 
automatic combination is identifying adaptable reusable modules. This contribution 
focuses on interdisciplinary modules in the field of automated production systems. 
The researched aspects comprise the modularization of existing systems, the identifi-
cation of the right module-granularity, and the modeling of the identified modules. 
2 
State of the Art 
Despite ongoing efforts in the field of research and in industry to implement reuses 
systematically, there are still many aspects which are strongly dependent on the expe-
rience of the particular developer. One such aspect is determining the appropriate 
granularity of a reusable module in order to reuse it successfully [3,4]. The difficulty 
is to decide which level of granularity the module must have in order to function ap-
propriately in a future project. The term granularity stands for the number of subsets 
of an element. In software architecture granularity differentiates between fine, me-
dium and coarse-grained levels. In the case of reusable modules we speak of fine-
grained, medium-grained and coarse-grained modules [5]. A fine-grained module is 
mostly passive and has a short processing time. In contrast, a medium-grained module 
is mostly active or interactive. Finally, coarse-grained modules are active units which 
check and control processes units and have a longer processing time as well. Fine-
grained modules can be reused in many projects. Additionally, fine-grained modules 
show greater flexibility because only those modules which match a project-specific 
application are chosen. However, many adjustments are necessary and mistakes are 
possible during integration so that the stability of the whole system can be at risk [6]. 
On the other hand, if the reusable modules are coarse grained, a major part of the 
functionality or structure of the automated systems will be covered. Only few adapta-
tions are then required in order to construct the model of the entire automated system. 
However, this greater efficiency comes at the cost of flexibility and the amount of 
reusability. The reason for this is the reduced probability that the modeled features at 
the coarse-grained level will be appropriate for further automated systems. A module-
based engineering method of automated production systems has been described by 
Weyrich et al. [7]. The blue print that results from this method can be verified by 
simulation tools with regard to the function, performance or energy consumption [8]. 
Various approaches have been analyzed to increase the reusability of the identified 
modules [9]. Other approaches are pursuing the issue of reuse of very small, mecha-
tronic units [10]. The demand for the adaptability of the modules results primarily 
from the aim to modularize production systems, not products. The production volume 
and the degree of standardization are benefits that can be used in the design of product 
architectures [11], while automated production systems are manufactured for specific 
applications in small number. Therefore, the adaptability and reusability are require-
ments for the configuration of production systems in contrast to the configuration of 
 

 
Metho
products [12]. The modular
ble to a modularization in th
tion for configuration syste
complexity. In contrast, the
proving the manufacturabili
is an aspect of the systema
Fay et al. [15]. Especially t
ing phases requires adaptab
modules should be impleme
of the possibility of cooper
opment of a new system fo
sented in [17]. This tool all
In contrast to the adaptation
ponents has been already 
reuse of modules, the deriv
defined inputs and outputs
within a module and betwe
line-specific as well as inte
port based approach for the
proposed in [19]. Each com
taining either a flow of a ph
an application of reusable 
tools. Maga et al. [20] ident
with the dependencies betw
propose a hierarchical SysM
machine and plant manufa
levels of granularity and e
modularity was not the focu
3 
Methodology for
3.1 
Modularization 
Existing automated product
adaptive modules, which co
ponents of these production
matrix (DSM) (Fig. 1). 
Fig. 1. Design S
dology for Identification of Adaptive Reusable Modules 
r design of automated manufacturing systems is compa
he process industry [13]. The aim of the module identifi
ems is to increase the reusability of the modules with h
e modularization for configuration of products aims at 
ity or suitability of service [14]. The reusability of artifa
atic improvement of the engineering process described
the comprehensive reuse of modules in different engine
bility of the modules. The reuse of adaptive, mechatro
ented as part of integrated engineering tools. An evaluat
ration of such tools has been presented in [16]. The dev
or conceptual design of mechatronic systems has been p
lows specifying systems by reuse of mechatronic modu
n of mechatronic modules the adaptation of software co
analyzed [18]. For an efficient development through 
ved modules of existing systems have to be modeled w
. In this way an analysis of dependencies of the eleme
een different modules is possible. In order to show disc
erdisciplinary dependencies of the elements in a modul
e modeling of mechatronic modules and their function
mponent is defined through its input and output-ports, c
hysical quantity or an information flow. A prerequisite 
modules in industry is the compatibility to engineer
tified that currently available engineering tools cannot d
ween software and hardware components. Bassi et al. [
ML (Systems Modeling Language) [22] framework for 
acturing industry. The system is modeled with differ
ach level can be mapped onto the other levels. Howev
us. 
r Modularization 
tion systems are analyzed for the identification of reusab
onsist of several components. The dependence of the co
n systems to one another can be shown in a design struct
 
Structure Matrix (left unstructured, right structured)  
127 
ara-
fica-
high 
im-
acts 
d by 
eer-
onic 
tion 
vel-
pre-
ules. 
om-
the 
with 
ents 
cip-
le a 
ns is 
con-
 for 
ring 
deal 
[21] 
the 
rent 
ver, 
ble, 
om-
ture 

128 
K. Kernschmidt et al. 
The illustrated relationships between the components emphasize that a component 
depends on another component, in order to fulfill its function. The representation in 
the DSM allows analyzing the relationships between the components. The compo-
nents which have strong dependencies among each other, can be summarized by sort-
ing of rows and columns. This allows the clustering of components into modules 
(marked red in Fig. 1). The resulting modules are assigned to functions in order to 
systematize them. This enables the identification and selection of appropriate modules 
based on a functional description of the automated production system. This systemati-
zation also allows the interchangeability of modules. Optimization within the  
engineering and a conceptual revision of existing systems are further outputs. A  
performance evaluation of the modules enables selecting not only modules which 
perform a function, but modules that perform the function under certain criteria best. 
The reusability of the modules is improved by various adaptations e.g. adapting the 
granularity. 
3.2 
Adequate Granularity 
Reusable modules include structure and performance or take into account overlapping 
aspects in regard to automated systems. In this way requirements, software compo-
nents, electrical circuit plans, reference architecture, test cases or processes can be 
conceived and modeled in a reusable way [23]. However, this requires that the  
modules feature an appropriate granularity. Following this approach the creation of 
reusable modules takes place independently of a project. These modules can be im-
plemented during the actual realization of a project. The method offers the instruction 
on how to determine the level of granularity of the module as well as how to raise the 
quality of the reutilization [24]. Thereby the following points should be taken into 
account: 
A1: 
Conflicting Requirement 
In most cases the domain requirements are competing, since they intend to fulfill a 
broad field of different functionalities. The reusable modules should be created as 
detailed as possible, in order to offer concrete support during application engineering 
[25]. In order to deal with competing requirements, it is necessary to build reusable 
modules in a modular manner. The modules should have a similar level of granularity, 
in order to be interchangeable. 
A2: 
Different Levels of Granularity necessary for Reuse in separate Disciplines 
The different disciplines imply different levels of granularity. Even within one discip-
line, different project phases require different levels of granularity. It is very difficult 
to propose a certain level of granularity for a reusable module if this changes depend-
ing on the project phase in which it is instantiated. Therefore, it is reasonable to  
provide modules that cover different levels of granularity. This can be achieved by 
designing reusable modules in a hierarchical manner. The top-level of such a reusable 
module should be coarse-grained, with many configuration and parameterization pos-
sibilities. The bottom-level of a nested module should be fine-grained. This should be 
detailed, specific and easy to change. However, it is crucial to ensure consistency 

 
Methodology for Identification of Adaptive Reusable Modules 
129 
between nested modules. In addition, the used tool chain should support the stepwise 
creation of nested reusable modules [26]. 
A3: 
Different Levels of Granularity in Domain and in project development 
In advance, it is sometimes unclear which level of granularity is required for a certain 
module. In order to mitigate this problem, we propose carefully analyzing the applica-
tion development process. Which modules are required, depends on the concrete 
process. Next, the granularity-level necessary for the created reusable modules is 
analyzed. Then it should be mentioned whether a reusable module created can provide 
the required level of granularity. Combining lower level components and higher level 
template guides for the integration of the components is essential for successful reuse.  
A4: 
Mismatch between Reusable Modules 
Structure, behavior and crosscutting aspects should be bundled to large blocks, at-
tached to the same module in the reference architecture and finally reused together. If 
these aspects are modeled at different levels of granularity, it is very difficult to group 
them to a bundle and to reuse them together. The systematic development of reusable 
modules could solve this problem. In this case, the different disciplines are obliged to 
work together from the very beginning. This increases understanding for adjacent 
disciplines. In regard to the granularity level of the modules, it is necessary to enable 
connections between reusable modules which have the same level of granularity.  
A5: 
Thorough documentation 
Reusable modules should be well documented, in order to be found in the domain 
repository, recognized as appropriate for the specific project and finally to be reused. 
Reusable modules should contain the description of their functionality and the de-
scription of how to be reused. Behavior, structure, origin and quality of reusable 
modules should be included in the documentation of the model. In the case of coarse-
grained modules, providing a concrete description of configuration and parameteriza-
tion possibilities is indispensable. It should be clear which variants are covered by the 
module and where changes are necessary, in order to obtain the variant required by a 
specific project. In case of fine-grained modules, a description of functionality and 
interfaces should be provided. Also, a mechanism to find reusable modules with an 
appropriate level of granularity for a concrete project phase or discipline has to be 
realized and links to required, recommended or optional modules should be offered. 
3.3 
Adaptability 
The reusability of modules in the development of new machines or plants requires a 
modular structure of the system’s model. The SysML framework offers the possibility 
to decompose a system into sub-systems, or modules, which can be modeled separate-
ly. Included in the models are the requirements, the behavior, and the structure of the 
system or module. For a representation of the models the SysML offers nine diagram-
types as shown (Fig. 2). The diagrams, which are the most useful to model adaptive 
reusable modules, are highlighted in Fig. 2. 

130 
K. Kernschmidt et al. 
The requirement diagram is used to show the requirements which have to be ful-
filled by the specific module and their relationships. The modeling of requirements 
for the reuse of modules is necessary, as the functions executed by the module and the 
used components derive from the requirements. The activity diagram represents  
the functions or the ‘workflow’ of the module. This diagram is especially important as 
the modularization of the system in this methodology is based on the functional de-
pendencies of the elements (see section 3.1). The block definition diagram (bdd) and 
the internal block diagram (ibd) show the structure of the system. While the bdd is 
used to show which modules form the required system, the ibd illustrates how the 
different elements are deployed within a module and which logical relationships exist 
between the different elements. An important prerequisite for the reuse of modules is 
their adaptability. New requirements as well as forced innovations, e.g. the withdraw-
al of a component, make changes in modules necessary. These can affect the devel-
opment of a new system, but also can have an influence on existing systems. The 
identification of change influences is important, as the exchange of an element can 
result in a multitude of required changes of other elements, leading to unexpected 
costs and time delays. As the influences of an element on others often are interdiscip-
linary, we propose to form interdisciplinary modules, including mechanics, electron-
ics, and software and to integrate all views into the model. 
 
 
Fig. 2. SysML diagrams [22]. Diagrams, necessary for reusable modules, are highlighted. 
4 
Use Case 
For the described methodology a bench-scale model of a stamping and sorting plant 
serves as a use case. It consists of a stack depot, a crane, a stamp module and a sorting 
belt and executes typical steps of a manufacturing process.  
4.1 
Modularization 
The described system is modularized by the method described in section 3.1. An ex-
tract of the DSM and some resulting modules are shown in Fig 3. If they include at 
least four relationships, the components are clustered into modules. The relationships 

 
Methodology for Identification of Adaptive Reusable Modules 
131 
are expressed by numbers in the following. A one indicates that a relationship exists. 
Central components such as control are split for modularization. Therefore, it is as-
sumed that the production systems are distributed and decentralized. In this case, the 
sorting of the rows and columns and the definition of the modules was done manually. 
The matrix, in which the relationships between the components of the system are 
shown, is not symmetrical. Consequently, a component in a row can affect the com-
ponent in a column, but not necessarily vice versa. Thirteen modules result from the 
modularization of the exemplary system described. The example shows that the re-
sulting modules map the functions of the example system. These functions are stor-
ing, passing, stamping, passing and sorting. Exemplarily the functions of passing are 
described which are realized by a crane. The functions of the crane are the material 
transfer from the storage and from the stamp to the conveyor belt. Both functions are 
represented by appropriate modules. In addition, control modules have been identified 
for these function modules. Other modules are for communication and for data input. 
Fig. 3. Extract of the structured Design Structure Matrix 
4.2 
Granularity 
The adaptive reusable approach mentioned above has been concretely deployed in the 
case study for the domain of stamping and sorting plants. We created reusable mod-
ules of physical structure and of the behavior of the stamping and sorting plant and 
stored them in a repository. Subsequently, we executed the activities of engineering, 
in order to simulate the construction of a customer-specific stamping and sorting 
plant. The goal of the case study was twofold: first, to evaluate the approach and iden-
tify necessary refinements, changes or completions. Second, to investigate the granu-
larity levels appropriate for the reusable modules. In order to provide an optimal  
support for application engineering, adaptive reusable modules shall be created as 
detailed as possible. Templates, customizable CAD-drawings, and customizable wir-
ing diagrams shall be created independent of a certain customer order. They should 
contain all basic information necessary to accomplish a well-defined engineering step. 
This implies first, appropriate activities should be included in engineering to prepare 
templates. Second, information regarding required forms of reusable artifacts and 
their level of granularity shall be fed back to engineering process. It is difficult to find 
the appropriate levels of granularity for adaptive reusable modules. For this purpose 

132 
K. Kernschmidt et al. 
«block»
Stampingplant
parts
 : Cylinder
 : Cylinder control unit
 : Cylinder
 : Cylinder control unit
 : Capacitive sensor
«block»
Stampingunit
parts
 : Optical Sensor
 : Capacitive sensor
 : Motor 
 : PLC
 : Optical Sensor
 : Motor control unit
 : Inductive sensor
 : Optical Sensor
 : Band-conveyor
 : Cylinder
 : Cylinder control unit
 : Cylinder
 : Cylinder control unit
«block»
Sortingsystem
parts
 : Cylinder
 : Cylinder control unit
 : Capacitive sensor
 : Optical Sensor
«block»
Storage
parts 
 : Cylinder
 : Cylinder control unit
 : Motor
 : Motor control unit
«block»
Crane
1
1
1
*
1
2
3
4
t
Sorting Belt
Stamp
Crane
Stack
depot
4.
0°
180°
Legend
work piece
Pneumatic cylinder
(Control: 1 Bit)
Pneumatic cylinder
(Control: 2 Bit)
1
2
3
4
Sorting cylinder 1
Sorting cylinder 2
Stack
cylinder
Lift
cylinder
Stamp
cylinder
Clamp cylinder
1.
2.
3.
4.
we should consider both the domain requirements and the intended reuse of the con-
cerned modules. As general recommendation, we suggest to use well-documented, 
hierarchical and nested modules, which provide different levels of granularity depend-
ing on the required functionality [27].  
4.3 
Modeling 
A modularization of the stamping and sorting plant through a DSM (see section 4.1) 
has shown that the system consists of the interdisciplinary modules storage, crane, 
stamp, and sorting system. In a first step the system is divided into this structure in a 
bdd (Fig 4). The different modules contain a list of the used parts, however the inter-
nal structure is not depicted. Thus, this representation has a very coarse granularity, 
and is used to give an overview of the entire production system and its modules. 
 
 
 
 
 
 
 
 
 
 
 
Fig. 4. Extract of the bdd of the stamping and sorting plant 
In the next step the modules are modeled in detail. Each module executes certain 
activities within the system. The main activity is detailed to sub-activities in an activi-
ty diagram. The sub-activities conducted by the sorting-system would be for example: 
Notice workpiece (WP), Move WP (Pos. 0-1), Identify metal WP, Sort out metal WP, 
Move plastic WP (Pos. 1-2), Identify white plastic WP, Sort out white plastic WP, 
Move black plastic WP (Pos. 2-3). The described activities are conducted through 
components of the respective module, whose structure is modeled in the ibd. It  
includes mechanical components, e.g. band-conveyors, and electrical/electronic com-
ponents, e.g. sensors. The software steps are excluded to keep the diagram clear. 
However, the interfaces between software and other components are included in the 
ibd through the integration of input and output ports of the PLC. In this way the out-
put port (e.g of a sensor) can be connected to an input port of the PLC. The software 
steps can be modeled in a separate view (as sub diagram). The modules contain next 
to the ports of the elements also global ports (ports on the border of the ibd), which 
constitute the interfaces to other modules. By modeling the identified modules in the 
described way they can be reused in the development of a new production system. As 
the influences of each component on other components, discipline-specific as well as 

 
Methodology for Identification of Adaptive Reusable Modules 
133 
interdisciplinary, are shown through the connections of the respective input and out-
put ports, the modules stay adaptable and change influences can be analyzed. 
5 
Results of the Use Case 
The employment of the methodology on the use case has shown that adaptive mod-
ules with an appropriate level of granularity can be identified in existing systems and 
modeled for a reuse in new systems. However it is difficult to develop detailed arti-
facts, because they have to cover more requirements than a specific product has to. 
Many requirements contradict each other, so that they cannot be integrated in one 
module. If coarse-grained modules are developed, they can cover the requirements of 
an entire production line. Unfortunately, they cannot be utilized/applied for specific 
problems. On the other hand, fine-grained modules, although they can be applied for 
specific problems, don’t have a high quality level of reusability. The case study has 
shown that the modules that cover a wide range in a domain are coarse-grained. How-
ever, for the specific realization of a requirement fine-grained modules are needed. 
6 
Conclusions and Outlook 
In this paper a methodology for the identification and the modeling of adaptive reusa-
ble modules for automated production systems was presented. The methodology is 
based on three steps: First an existing system is analyzed on functional dependencies 
between its components and is clustered accordingly. Second, the right level of granu-
larity has to be identified. Thereby a tradeoff between reusability and number of  
included components has to be made. In the third step the identified modules are 
modeled in the SysML framework, to make an effective reuse in new projects possi-
ble. The modules for automated production systems hereby are interdisciplinary  
(mechanics, electric/electronic, software) as many dependencies exist between the 
different domains. The methodology was applied to a bench-scale model of a stamp-
ing and sorting plant. The steps were carried out thereby manually. Thus, in future 
research a suitable tool will be developed, which makes an automatic component 
clustering to modules with an appropriate level of granularity possible. This tool 
should have an interface to common modeling tools, to import the identified modules 
directly. The long-term goal is the support of the design of modularized plants. There-
for the interdisciplinary modules should be stored in a library. Based on the collection 
of modules, new machines or plants can be designed by combining different modules.  
References 
1. Li, F., Bayrak, G., Kernschmidt, K., Vogel-Heuser, B.: Specification of the Requirements 
to Support Information Technology-Cycles in the Machine and Plant Manufacturing In-
dustry. In: 14th IFAC Symp. on Inform. Control Prob. in Manufacturing, Bucharest (2012) 

134 
K. Kernschmidt et al. 
2. Weyrich, M., Klein, P., Löwen, U., Schäffler, T., Vollmar, J.: Knowledge Based Engineer-
ing in der Anwendung - Anwendungen und Trends wissensbasierter Engineeringmethoden 
und Werkzeuge. Industrie und Management (March 2012) 
3. Griffel, F.: Componentware. Konzepte und Techniken eines Softwareparadigmas. Dpunkt, 
Heidelberg (1998) 
4. Burd, E., Munro, M., Wezeman, C.: Extracting Reusable Modules from Legacy Code: 
Considering the Issues of Module Granularity. In: Proc. of the 3rd Working Conf. on Re-
verse Engineering, pp. 189–196. IEEE Press, New York (1996) 
5. Szyperski, C., Gruntz, D., Murer, S.: Component software: beyond object-oriented pro-
gramming, 2nd edn. ACM Press, New York (2002) 
6. Szdzuy, A.: Aspects of granularity for components. In: Workshop on Component-based 
Software Development. Technical University Berlin (2002) 
7. Weyrich, M., Klein, P.: Modulbasiertes Engineering von Produktionsanlagen - Wissensba-
sierte Konzeption basierend auf funktionsorientierter Modularisierung. wt Werkstattstech-
nik Online 102(9) (2012) 
8. Weyrich, M., Klein, P.: Engineering of automated Manufacturing Systems with Mecha-
tronic Objects. In: Proc. of 38th Annual Conf. of the IEEE IECON, Montréal (2012) 
9. Weyrich, M., Klein, P.: Assisted Engineering for mechatronic Manufacturing Systems 
based on a Modularization Concept. In: 17th IEEE Conf. on ETFA, Cracow, Poland 
(2012)  
10. Lüder, A., Foehr, L.H.M., Wagner, T., Zaddach, J.-J., Holm, T.: Manufacturing System 
Engineering with Mechatronical Units. In: Proc. of IEEE Conf. on ETFA, Bilbao, Spain 
(2010) 
11. Schuh, G., Arnoscht, J., Nußbaum, C.: Produktarchitekturen richtig gestalten. Industrie 
und Management (June 2007) 
12. Korajda, I., Seyfarth, M., Pritschow, G.: Disziplinübergreifende Baukastensysteme. wt 
Werkstattstechnik Online 94(5), 215–219 (2004) 
13. Urbas, L., Doherr, F., Krause, A., Obst, M.: Modularisierung und Prozessführung. Chemie 
Ingenieur Technik 84(5), 615–623 (2012) 
14. Shpitalni, M., Stiassnie, E.: Axiomatic modukar systen design for service oriented prod-
ucts. In: Proc. of the 6th Int. Conf. on Axiomatic Design, Daejeon (2011) 
15. Fay, A., Schleipen, M., Mühlhause, M.: Wie kann man den Engineering-Prozess systema-
tisch verbessern? Automatisierungstechnische Praxis, 80–85 (February 1, 2009)  
16. Barth, M., Drath, R., Fay, A., Zimmer, F., Eckert, K.: Evaluation of the openness of auto-
mation tools for interoperability in engineering tool chains. In: 17th IEEE Conf. on ETFA, 
Cracow, Poland (2012) 
17. Anacker, H., Dorociak, R., Dumitrescu, R., Gausemeier, J.: Integrated tool-based approach 
for the conceptual design of advanced mechatronic systems. In: 12th IEEE Int. Systems 
Conf. (SysCon), Heidelberg and Mannheim (2011) 
18. Morel, B., Alexander, P.: Automating component adaptation for reuse. In: Proc. of the 
18th IEEE Int. Conf. on Automated Software Engineering, Tokyo, pp. 142–151 (2003) 
19. Kernschmidt, K., Bayrak, G., Vogel-Heuser, B.: A port-based approach for modeling the 
structure of mechatronic modules. In: Proceedings of the 14th Int. Dependency and Struc-
ture Modelling Conf. (DSM 2012), Kyoto, Japan, pp. 111–123 (2012) 
20. Maga, C., Jazdi, N., Göhner, P.: Requirements on Engineering Tools for Increasing Reuse 
in Industrial Automation. In: Proc. of 16th IEEE Conf. on ETFA, Toulouse, France (2011) 
21. Bassi, L., Secchi, C., Bonfe, M., Fantuzzi, C.: A SysML-based methodology for manufac-
turing machinery modeling and design. IEEE Trans. Mechatron 16(6), 1049–1062 (2011) 

 
Methodology for Identification of Adaptive Reusable Modules 
135 
22. Object Management Group: OMG Systems Modeling Language (OMG SysML), Version 
1.3. OMG Document Number: formal/2012-06-01 (June 2012) 
23. Maga, C., Jazdi, N., Ehben, T., Tetzner, T.: Domain Engineering–Improved Systematisa-
tion in Industrial Solutions Business. In: Proc. of the Automation Congr., Baden-Baden 
(2009) 
24. Stoics, A.: On hardware Evolvability and Levels of Granularity. Center for Space Micro-
electronics Technology. Jet Propulsion Laboratory, California Institute of Technology 
(1997), 
http://trs-new.jpl.nasa.gov/dspace/bitstream/2014/22583/ 
1/97-1093.pdf  
25. Maga, C., Jazdi, N.: A Survey on Determining Factors for Modeling Reference Architec-
tures. In: Proceedings of OOPSLA, Orlando, Florida, USA (2009) 
26. Maga, C., Jazdi, N., Göhner, P.: Requirements on Engineering Tools for Increasing Reuse 
in Industrial Automation. In: Proc. of 16th IEEE Conf. on ETFA, Toulouse, France (2011) 
27. Maga, C., Jazdi, N., Göhner, P.: Reusable Models in Industrial Automation: Experiences 
in Defining Appropriate Levels of Granularity. In: 18th World Congress of the Interna-
tional Federation of Automatic Control (IFAC 2011), Milano, Italy (2011) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 137–146. 
DOI: 10.1007/978-3-642-30817-8_14 
© Springer-Verlag Berlin Heidelberg 2013 
 
Reverse Engineering for Manufacturing Approach: 
Based on the Combination of 3D and Knowledge 
Information 
Salam Ali1, Alexandre Durupt2, and Pierre Antoine Adragna1 
1 Université de Technologie de Troyes, Troyes, France 
2 Université de Technologie de Compiègne, Compiègne, France 
{salam.ali,pierre_antoine.adragna}@utt.fr, 
Alexandre.durupt@utc.fr 
Abstract. Today industrial companies are still trying to optimize in terms of 
time and cost the re-manufacturing of mechanical components. They need to  
directly define a new process planning from 3D information (points cloud, 
drawings…). This paper proposes an approach called Reverse Engineering For 
Manufacturing (REFM) which allows to directly obtain a CAPP (Computer 
Aided Process Planning) model from 3D and knowledge information. Routine 
tasks will also be taken into account. In this paper, RE is considered as a specif-
ic domain and concerns parts where no information is available on them. The 
system management is based on Design For Manufacturing (DFM) approach 
and enables to manage manufacturing information (the number of fixtures, the 
kind of milling operations...). Additionally, the future REFM system will have 
to propose alternatives for CAPP models. Therefore the main innovative point 
of REFM is to develop “manufacturing knowledge extraction” phase which is 
the aim of this paper.  
Keywords: reverse engineering, manufacturing, knowledge extraction, process 
planning, Design For manufacturing. 
1 
Introduction 
Reverse Engineering (RE) is the process that is used in several domains like mechani-
cal engineering, electrical engineering, computer sciences and so on… In this paper, 
we focus on reverse engineering of mechanical parts. In this context, RE is used to 
create a CAPP model of an existing physical part from 3D information. The 3D in-
formation like 3D points cloud is obtained using 3D scanning technologies. In this 
study, the following context is considered: there is no information on the part (no plan 
or scheme); only the physical part is available. So to remanufacture this kind of parts, 
we propose an approach called Reverse Engineering For Manufacturing (REFM). 
REFM will allow to define a method of RE of mechanical parts, supported by a soft-
ware demonstrator. One hypothesis of this research is to consider that the methodolo-
gy developed will be based on DFM approach. Indeed, the re-engineering of a part 

138 
S. Ali, A. Durupt, and P.A. Adragna 
needs to be done through a systematic process that integrates the work of the designer 
and the product development team from identification of the problem until the final 
design of the product, offering a greater chance of success. REFM consists in provid-
ing a Computer Aided Process Planning (CAPP) model including a new manufactur-
ing tree. This tree must be selected by optimizing the manufacturing sequence and 
define alternatives operations which aim to facilitate and optimize the re-
manufacturing. REFM aims to integrate databases that contain all the necessary  
information for the construction of the CAPP model. In this context, the system of 
Ashby et al. [1] CES4.5 is adapted. The major advantage of this method is to integrate 
at the earliest the manufacturing constraints in the product’s lifecycle. In addition, RE 
can also be a recursive process; routine tasks will also be taken into account. Nowa-
days, RE approaches including routine tasks begin to be supported by Knowledge 
Base Engineering Systems (KBS) [2]. These systems are efficient to quickly obtain 
CAD models based on functional features. These CAD models are successful for re-
designing activities and then for defining a process planning. So, these systems are 
not adopted to obtain directly a CAPP model in a routine RE context. The main prob-
lematic of this paper is to explore how to adapt DFM to the RE context. The contribu-
tion of the paper is limited here to propose a prospective approach in a milling process 
context. This paper is structured as follow: section 2 presents a state of the art of 
knowledge based system for RE in order to highlight the way to support routine con-
text, and a state of the art of DFM for RE context; next, section 3 proposes a prospec-
tive approach in a milling process context. A top of reducer is considered in this paper 
in order to illustrate our system. This case study will provide the basis of analysis of 
the system REFM.  
2 
The State of the Art 
2.1 
Knowledge Based System for Reverse Engineering 
RE methodologies are able to duplicate complex parts; however they can capture a 
very low level semantics. Or, a CAD model is not only a geometrical model. Addi-
tionally in design, functional aspects are often attached to geometric shapes. So today, 
it is necessary to integrate in a RE process these semantics to the geometry. Actually, 
many researches are discussed the importance of knowledge management of RE. For 
example, Mohaghegh et al. [3] propose to involve a pre-knowledge on the part before 
performing the reverse engineering activities. The works of Fisher [4] explore the 
possibility to extract features even in very noisy data and that by using “knowledge 
based” techniques. To select surface types and manufacturing actions, he exploits 
engineering knowledge and functional constraints with some user assistance. Or in 
their works, the knowledge is implicit and is not driven by a methodology. Thompson 
et al. [5] describe a classical geometric features-based reverse engineering system 
(Reverse Engineering Feature Based - REFAB). The developed prototype creates 
interactively the CAD model of a part where the user selects predefined features in a 
list and chooses where these features are located in the 3D points cloud. So,  

 
Reverse Engineering for Manufacturing Approach 
139 
manufacturing knowledge extraction is achieved implicitly by the user. Only five 
manufacturing features (such as types of pockets and holes) are performed and 2.5 is 
considered. 
Certain types of knowledge allow extraction of geometrical primitives. As an ex-
ample, the VPERI [6] (Virtual Parts Engineering Research Initiative) project was 
created by the US Army Research Office in order to provide the vision, strategy, and 
methodology to help solving problems of long life cycle product maintenance. The 
knowledge of the geometric shape is necessary but not sufficient to reproduce the 
part. Re-engineering and re-design need functional specifications. A design interface 
is used to allow the additional of knowledge in the form of algebraic equations that 
represent engineering knowledge such as the functional behavior of the components, 
the physical laws that govern the behavior, etc.  
The KBE for reverse engineering context is a good solution to reverse a part and 
obtain a CAD part. It is often based on functional knowledge to reverse the part. So, 
the manufacturing knowledge is not really integrated. In the scientific literature, CAD 
model is obtained from points cloud. Then, process planning is redefined from this 
CAD model. In this case, feature extraction/recognition based approaches are used 
and often characterized as knowledge based. For instance, Zhou et al. [7] use feature 
recognition/extraction and feature based design to integrate CAD and CAPP systems. 
Or, our approach REFM consists in identifying directly the CAPP model from the 
points cloud. Hence, KBE is used to extract knowledge on manufacturing. This know-
ledge explores the possibility to adapt the concept of DFM to the RE context. 
2.2 
Design For Manufacturing for RE Context 
As mentioned above, REFM is a RE methodology that aims to directly define a new 
process planning of a mechanical part. This approach is based on the combination of 
3D and knowledge information. These knowledge should be manage and should be 
integrate in the re-design stage to reach an optimal CAPP model and then to achieve a 
successful RE process. It is for these reasons that DFM methodologies are more ap-
propriated. In the literature, Kerbrat et al. [8] bring a new DFM approach to multi-
process manufacturing. This research considers that the choice of the manufacturing 
processes is based on the determination of the manufacturability complexity and the 
time/cost estimation at the design stage. Zhao and Shah [9] proposed a DFM shell for 
aid to manufacturing analysis in taking into account technics and economics data. 
Other work aims to reduce the manufacturing cost and time, so it turns to optimize the 
product form, material selection, and resource selection [10]. Gupta et al. [11] pro-
posed an approach to select processes and materials during embodiment design based 
on the cost estimation. CES4.5 (Cambridge Engineering Selector) system of Ashby et 
al. [1] includes a database oriented on the triple characteristics: Process, Material and 
Geometry. In this database, all numbered characteristics are limited by intervals 
which show the manufacturability. For this paper, the DFM approach is limited to the 
context where a designer has to define a product in the point of view of manufacturing 
process. The manufacturing process view in REFM has to be in the technical data 
with accurate details such the fixtures, kind of machines, kind of tools and so on… 

140 
S. Ali, A. Durupt, and P.A. Adragna 
REFM has to integrate databases which include all these information according to the 
manufacturing resources of the company. In this context, we utilize a database which 
combines the system of Ashby et al. [1] with other information from handbooks such 
as [12]. In the following section of this paper, the REFM method will be revealed and 
the prospective interfaces will be proposed through a case of study.  
3 
REFM Methodology 
REFM is a methodology which concerns components that are get out of the product 
lifecycle. The inputs points of REFM method are the digitized part and the manufac-
tured part. To recall, all precedent capitalizations of the original product lifecycle are 
lost. So, Manufacturing knowledge extraction phase will be based on user’s supposi-
tions. The aim of REFM could be considered such as the combination of geometrical 
approaches (segmentation) and aided process planning methodologies (manufacturing 
knowledge extraction) of design context. The main innovative point of REFM is to 
develop “manufacturing knowledge extraction” and to define how it is possible to 
adapt to RE context in this contribution. Figure 1 shows the REFM methodology in 
details where Manufacturing knowledge extraction phase is developed. The different 
modules used in our methodology are described in the following sections.  
 
 
Fig. 1. REFM methodology in details 
3.1 
Segmentation 
To start, according to related works on RE, REFM should import RE files such as 3D 
points cloud or STL (STéréo Lithographie) file. As a supposition REFM starts when 
treatment operations of cleaning STL or points cloud are previously executed. The 
segmentation phase is used here in order to detect surfaces (plan, cylinder, spherical, 
and conical surfaces) without geometric parameters. It consists in the division of the 
3D points cloud of a given part into a set of n points clouds representing the n surfac-
es that compose this part. In the RE process, this phase can be performed by one of 
the following three segmentation techniques: Region-based technique [13], Edge-
based technique, and Hybrid technique. 

 
Reverse Engineering for Manufacturing Approach 
141 
If we apply the first technique on the top of reducer, the 3D points cloud will be 
divided into 36 surfaces (figure 2). Note that if some surfaces are not recognized, the 
user can enter the data for unrecognized surfaces. This aspect of the software enables 
the handling of any complicated part. This paper does not deal with the segmentation 
phase. It is mainly focusing on Manufacturing knowledge extraction phase. 
3.2 
Manufacturing Knowledge Extraction 
a. Material and blank selection 
After the segmentation phase, the DFM process can start. To select the material of the 
part to be re-engineered, REFM asks the user to enter its mass. Then, the system cal-
culates the volume of the part from the 3D points cloud file and that to obtain the 
density. Once REFM has the density, the system CES4.5 of Ashby et al. [1] is to be 
used. The system proposes some materials and according to their needs and their ex-
periences; the user chooses the material that he finds the most appropriate. If the user 
did not find the suitable material, he can add additional material. 
The following step of DFM analysis is to determine the original blank of the part. 
To recall, REFM concerns milled parts. So, the blank comes from a precedent process 
step of the part. REFM can propose an original blank from primary processes: the 
extraction process of raw (rolling, extrusion…) or the process of shaping (casting, 
forging…). 
b. Surface precedence graph 
The surface precedence graph connects machining surfaces between them by starting 
from raw surfaces (figure2). Each surface is represented by a circle containing the 
type of the surface (B: raw surface, F or A: machined surface). The arrow starts from 
a reference surface and ends at a referenced surface. Geometric and dimensional  
tolerances help the user to draw this graph. Hence, references surfaces are to be ma-
chined prior to the machining of the referenced surfaces, or the reference and the refe-
renced surfaces should be machined in one set-up. So, based on the combination of 
the Ashby database and using a manufacturing method of analysis: REFM asks the 
user to select a machined surface and its reference one, then REFM proposes one or 
more tolerances and the user can chooses the tolerances that he finds the most appro-
priate, as we show in figure 2. After that, the user enters by supposition the tolerance 
class and the roughness of the surface (the user can measure the roughness by a 
roughness meter).  
c. Machining operations 
Using the above data and based on a cutting tools database [14], the user searches for 
a logical grouping of machined surfaces. Indeed, the accessible surfaces by the same 
tool should be grouped to be machined at the same time. For example, selected sur-
faces (F3 and F6i; i = 1 to 8) in figure 3 are combined in a group called GF. In addi-
tion, REFM can propose groups of surfaces in the case of routine tasks. 
 

142 
S. Ali, A. Durupt, and P.A. Adragna 
 
Fig. 2. Interface of the surface precedence graph in REFM 
 
Fig. 3. The selection of machining operations in REFM 

 
Reverse Engineering for Manufacturing Approach 
143 
Many details affect the selection of machining operations such as: the shape, accu-
racy and surface finish requirement of the surface, the overall structure of the part, 
and the workpiece material. In fact, Ashby et al. [1] explain that the best solution of 
design for manufacturing is retained if decisions of materials, geometry and processes 
are taken into account simultaneously. And based on the surface roughness, the sys-
tem determines the number of operations to reach the final surface finish requirement 
(rough, semi-finish, finish). REFM tries to select alternatives routes to machine each 
surface or group of surfaces. And thus, the user has the option to choose its appropri-
ate route. Note that the user can change the machining operations according to re-
quirements. After that, standard features can be generated. Indeed, a feature is the 
combination of surfaces coupled to an operation.  
d. 3D identification 
The previous steps allow the user to obtain machining operations, standard features 
and so on. Each operation, for example, the contouring of the feature selected in the 
figure 4 should be linked to geometry. This geometry will start from the blank and 
will decompose to the final part. 3D identification serves to translate operation steps 
of manufacturing process in geometry. To make this modeling, REFM uses Skin and 
Skeleton concept. In fact, for each skin and Skeleton element [15], an included script 
in the database performs an algorithm based on the least squares approximation. This 
step is extremely important in our approach; it is already addressed in our previous 
work [16]. The output of this module is a primitive CAPP model including machining 
operations that are not yet defined in order (figure 4). 
 
 
Fig. 4. 3D identification module in REFM 

144 
S. Ali, A. Durupt, and P.A. Adragna 
e. Define the order of machining features & Define set-ups 
The order of machining features of the re-engineered part depends on non-geometric 
information such as geometric dimensions and tolerances. So to reach feature se-
quencing, REFM returns to the data mentioned in the Surface precedence graph mod-
ule. In addition, REFM will integrate simple rules taken from Handbooks such as [12] 
which include constraints on the optimization of cutting conditions to perfect the or-
der of machining features. For instance, if the part to re-engineer contains a hole on an 
inclined surface, so it is optimal to machine the hole before the inclined surface since 
holes cannot be machined accurately on an inclined surface. Or, if the part contains a 
hole on a flat and smooth surface, so we start by the milling operation and that to not 
plug the hole as in our case. After that, REFM groups the features into set-ups. Set-up 
design should be such that a maximum number of features can be machined with a 
minimum number of set-ups. Before proceeding to the next step, REFM asks the user 
if he is satisfied with the proposed sequence. If not, he is allowed to change the order, 
based on his own experience and knowledge. 
Next, fixture planning module will to be achieved. According to 3-2-1 method (lo-
cating method for prismatic parts), the user can select the surfaces of fixtures. Then, 
REFM selects for each feature, from the cutting tools database of Sandvik 
(www.sandvik.com), 2 or 3 cutting tools based on geometric parameters and surface 
finish requirements of the correspondent feature. And then the user can choose the 
suitable one according to him. Finally, REFM selects for each operation one or more 
machines. To choose the best one, it considers a set of criteria. For example, the most 
suitable machine among those that are previously candidate, is the machine that  
 
 
Fig. 5. The final CAPP tree and the inspection with the RE files 

 
Reverse Engineering for Manufacturing Approach 
145 
realizes the maximum number of operations with the maximum number of set-ups. 
Note that the user can enter information on the means of production available in his 
enterprise, which allows constraining the suggestions provided by the system. 
Thereby, the process plan is generated by REFM system (figure 5). In addition, the 
user can show by inspection, the distances between the RE files and the CAPP model 
decomposed in machining operation steps. 
4 
Conclusion 
This paper proposes a new method for re-manufacturing mechanical components. 
Indeed, industrial companies of the cluster NOGENTECH have to define a new 
process planning from 3D information (points cloud, drawings, etc). Or, late commer-
cial solutions, such as GeomagicTM, RapidFormTM and CATIATM are more effi-
cient to obtain a CAD model. Nevertheless, the industrial who needs to define a 
CAPP model redefines the process planning from this CAD model. REFM is a me-
thodology based on DFM approaches and focuses on the milling process. According 
to the related works, the Ashby et al. [1] classification seems to be a way of resolu-
tion. The future REFM system provides a Computer Aided Process Planning (CAPP) 
model including new manufacturing tree. This tree must be selected by optimizing the 
manufacturing sequence and define alternatives operations which aim to facilitate and 
optimize the re-manufacturing. Each milling operation is a Skin and Skelton feature 
which is fitted in the 3D information. The aim of REFM system is really to propose a 
prototype software which can be coupled in CATIA V5 and Solidworks. It means that 
REFM system is independent but could use the geometrical resources of commercial 
softwares. A final version is planned based on PYTHONOCC (OpencasCadeTM) 
resources in order to propose a complete independent software. After that, a next way 
will to be adding in the future created database: the cost aspect (evaluating the cost 
milling), the time consuming (the time of the process milling) and the sustainable 
aspect (to produce milling part in respect of environment).  
 
Acknowledgments. The acknowledgment concerns the region Champagne Ardenne 
through the ESSAIMAGE program. The works are financed by the region. 
References  
1. Ashby, M.F., Brechet, Y., Cebon, D., Salvo, L.: Selection strategies for materials and 
processes. Materials & Design 1, 51–67 (2004) 
2. Durupt, A., Remy, S., Ducellier, G.: Knowledge based reverse engineering: An approach 
for reverse engineering of a mechanical part. Journal of Computing and Information 
Science in Engineering 10, 1–4 (2010a) 
3. Mohaghegh, K., Sadeghi, M.H., Abdullah, A.: Reverse engineering of turbine blades on 
design intents. International Journal of Advanced Manufacturing Technology 32, 1009–
1020 (2006) 

146 
S. Ali, A. Durupt, and P.A. Adragna 
4. Fisher, B.: Applying knowledge to reverse engineering problems. Computer Aided De-
sign 36, 501–510 (2004) 
5. Thompson, W., Owen, J., De St Germain, H., Stark, S., Henderson, T.: Feature based re-
verse engineering of mechanical parts. IEEE Transactions on Robotics and Automation 15, 
57–67 (1999) 
6. VPERI.: Army research office virtuals parts engineering research initiative final report. 
Workshop, Salt Lake City, USA (2003), 
http://www.cs.utah.edu/gdc/Viper/Collaborations/ 
VPERI-Final-Report.pdf 
7. Zhou, X., Qiu, Y., Hua, G., Wang, H., Ruan, X.: A feasible approach to the integration of 
CAD and CAPP. Computer-Aided Design 39, 324–338 (2007) 
8. Kerbrat, O., Mognol, P., Hascoet, J.: A new DFM approach to combine machining and ad-
ditive manufacturing. Computers in Industry 62, 684–692 (2011) 
9. Zhao, Z., Shah, J.: Domain independent shell for DFM and its application to sheet metal 
forming and injection molding. Computer-Aided Design, 881–898 (2005) 
10. Feng, S., Song, E.: A manufacturing process information model for design and process 
planning integration. Journal of Manufacturing System 22(1) (2003) 
11. Gupta, S., Chen, Y.: A system for generating process and material selection advice during 
embodiment design of mechanical components. Journal of Manufacturing System 22, 28–
45 (2003) 
12. Chevalier, A., Bohan, J.: Guide du technicien en productique. Hachette Technique, éd. 13 
(2010) ISBN 978-2-0116-7584-2 
13. Besl, P., Jain, R.: Segmentation through variable order surface fitting. IEEE Transaction 
on Pattern Analysis and Machine Intelligence 10, 167–191 (1988) 
14. http://www.sandvik.com/ 
15. Skander, A., Roucoules, L., Klein Meyer, J.S.: Design and Manufacturing Interface model-
ing for manufacturing processes selection and knowledge synthesis in design. International 
Journal of Advanced Manufacturing Technology 37, 443–454 (2008) 
16. Durupt, A., Remy, S., Ducellier, G.: KBRE: a knowledge based reverse engineering for 
mechanical components. Computer-Aided Design and Applications, 279–289 (2010b) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 147–157. 
DOI: 10.1007/978-3-642-30817-8_15 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Kinematic Approach for 6-DOF Part Positioning 
Sajid Ullah Butt1, Jean-Francois Antoine2, and Patrick Martin3 
1 CEME, National University of Sciences and Technology (NUST), Islamabad, Pakistan 
2 GMP, Le Montet, Rue du Doyen Urion, 54601, Villers-lès-Nancy, France 
3 LCFC, Art et Métiers, 4 Rue Augustin Fresnel, 57078 Metz, France 
sajidullahbutt@yahoo.com,  
jean-francois.antoine@univ-lorraine.fr, 
Patrick.martin@ensam.eu 
Abstract. This article proposes a fixturing system consists of a cuboid basep-
late located through a 3-2-1 configuration of locators. The locators are mounted 
on machine table/pallet and posses one axial DOF. The workpiece is mounted 
on the baseplate and all the elements are assumed to be rigid with zero friction. 
The positioning error of the workpiece is calculated and the compensation is 
performed by the axial movement of the locators. The proposed analytical mod-
el is verified by the simulation performed in the CAD model. 
Keywords: Analytical model, Fixturing system, Part positioning, kinematic 
model. 
1 
Introduction 
There is a competition in the manufacturing industry to design and deliver a variety of 
high quality products to their customers in the shortest time. Due to rapid change in 
production technology and customer demand, the manufacturers need to develop flex-
ible manufacturing practices to achieve a rapid turnaround in product development 
[1]. Among other factors, the use of feasible fixtures is one of the factors influencing 
the final part’s quality. Fixtures are devices used to support, locate and hold a  
workpiece at a desired position and orientation in machine’s workspace during manu-
facturing. The final part’s quality is influenced by the capability of the fixture to pre-
cisely hold and locate it on the machine considering different functional conditions 
during fabrication. About 10-20% of total manufacturing cost is associated with the 
fixtures in traditional FMS systems [2]. The design of fixtures is important to precise-
ly hold the workpiece and compensate the errors that the workpiece can encounter 
during machining or assembling operation, so that higher product’s quality can be 
ensured [3]. 
The need of high quality production, at lower cost, has accelerated the research ef-
forts in fixture design. To cope with current market demand, Ryll et al. [4] emphasize 
on the need of “intelligent” fixtures which should be capable of self-configuring; 
reducing and compensating dimensional errors; providing stability and adapting 

148 
S.U. Butt, J.-F. Antoine, and P. Martin 
clamping forces to guarantee optimum performances. This fixture should be generic 
and should be able to adapt to different workpiece configurations. 
2 
Positioning Errors 
Dimensional errors of the parts from a part family cause the initial misplacement be-
tween the workpiece and machine tool affecting the final product quality. The possi-
ble causes of the positioning errors between the machine tool and the workpiece are 
shown in Figure 1, which are:  
• Error due to the placement of locators [5–8] 
• Geometric/form defects of the workpiece [9–12] 
• Errors due to deformation of locators [13–18] 
• Kinematic defects/ machine tool errors [19–26] 
• Misc. errors due to tool wear, heat, NC codes, etc… 
Part
Pallet
Column
Base
Spindle
Kinematics defects
Locators placement
Geometric/form 
defects
Deformation 
due to forces
Tool wear
Effect of heat
NC Code errors
Tool
 
Fig. 1. Errors between the machine tool and the workpiece 
Rough workpiece’s dimensions are varied from one part to another, so the machin-
ing allowances have to be added. Even after the addition of allowances, the rough 
workpiece may not be completely included in required position, which causes the 
wastage of the workpiece due to incomplete machining. To avoid the loss of time and 
material, it is necessary to precisely place each new part relative to machine tool. But 
this placement needs a mobilization mechanism on the machine. This mechanism 
should assure the kinematic transformation to place the workpiece at an optimal posi-
tion by compensating the positioning error between the workpiece and the machine-
tool. A high number of degrees of freedom (DOF) machine would be an easy way to 
perform this compensation.  
In an existing serial production environment, the global choice of 5-axis machines 
in the whole production line is not an economically feasible choice. So a new fixtur-
ing system is proposed. This fixturing system is able to perform a 6 DOF workpiece’s 
repositioning on a low DOF production machine through the axial motion of 6 sup-

 
A Kinematic Approach for 6-DOF Part Positioning 
149 
porting locators placed at 3-2-1 configuration. The initial and final positions of the 
workpiece are given as the input data and an algorithm calculates the positioning error 
and the axial displacement of each locator required to compensate this positioning 
error.  
The proposed system can be used on the existing machines as well as on automatic 
production lines where the number of axis is limited for each station. The proposed 
system allows better positioning of the workpiece on the fixture and hence limiting 
the required allowances. It also insures a prepositioning of complex parts for precise 
machining operations. The necessary geometric and kinematic models of the proposed 
fixturing system are presented in this article. 
Y3
Z3
X3
X
Z
Y
O
b
P
XP
ZP
YP
6
4
5
2
3
1
 
Fig. 2. Proposed fixturing system principle 
3 
Proposed Fixturing System 
This article proposes a fixturing system consists of a set of six locators whose posi-
tions and orientations are defined through locating holes of the machine table/pallet, a 
cuboid baseplate, and a workpiece fixed on the baseplate as shown in Fig. 2. Hip 
prosthesis is chosen as the demonstrated workpiece because it requires repetitive ma-
chining operation on expensive material and the dimensions of part change according 
to patient need. The baseplate is introduced because when the locators are directly in 
contact with the rough workpiece surfaces, it is impossible to attain the precise  
positioning of the workpiece through the axial displacement of 6 locators due to un-
certainty of the contacting points caused by the local geometrical defect at rough con-
tacts. The positioning surfaces of the baseplate are considered to be perfectly plane 
and orthogonal. This assumption causes the surface normals to always remain parallel 
to the contacts’ normals, which enables us to predict the exact location of the work-
piece by the locators’ positions. Thus the addition of intermediate baseplate avoids 
this positioning uncertainty: kinematic model will be independent of part geometry. 
The locators are assumed to be in a 3-2-1r configuration [27] and possess only one 
axial DOF. The lateral position of each locator is chosen by considering the con-
straints of accessibility, stability of the workpiece and manufacturing knowledge. It is 

150 
S.U. Butt, J.-F. Antoine, and P. Martin 
also assumed that the workpiece is mounted rigidly on the baseplate and no additional 
deformation occurs between workpiece and baseplate except those caused during 
clamping the workpiece.  
3.1 
Analytical Formulation 
For kinematic analysis, all the elements of the ﬁxturing system are assumed to be 
rigid. It is assumed that the positioning error of the baseplate is negligible as com-
pared to the positioning error of the workpiece. Also the unknown initial position of 
the workpiece could imply large displacements (LD) during correction phase; the 
kinematic model is built using homogeneous transformation matrices (HTM) and LD 
formulation. The initial position of the workpiece can be measured through CMM 
while its final position is the position according to which the machine tool is pro-
grammed. This position is known by the part program. These positions are compared 
and if the difference is more than the allowed tolerance, the algorithm calculates the 
unique relative axial position of each locator to relocate the workpiece at the required 
position.   
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
-10.00
10.00
30.00
50.00
70.00
90.00
110.00
130.00
Min Material (Chebyshev)
Y
X
Stem
Neck
P
 
Fig. 3. 2D Demonstration of measurement though CMM 
The measurement principle of the hip prosthesis though CMM (in 2D) is shown in 
Fig. 3. Rough part dimensions are larger than the final product.  Random measured 
points are generated in MS Excel for stem and neck of the hip prosthesis. RMS and 
Chebyshevs’ surface association criteria are presented [28], [29], and theoretical cen-
terlines (for neck and stem) are then deduced. The angle between these centerlines 
should be under the tolerance range. Point P denotes the intersection of centerlines. In 
3D space, the definition of point P, in machine reference, cancels 3 DOF; the defini-
tion of the XY plane cancels two more DOFs and the last DOF is canceled by defin-
ing the angle of stem axis with XZ plane, completing workpiece placement in the 
machine space. Some position variations among the parts of the same part family will 
remain. Random measuring points are generated and the point P is calculated for each 
set of measuring points. The generated distribution of P is also presented in Fig. 3. 

 
A Kinematic Approach for 6-DOF Part Positioning 
151 
The HTM of cuboid baseplate position is the function of its surface normals calcu-
lated from the positions of the six locators [12]. This HTM is shown in Eq. (1) where 
a, b and c are the unit vector components; 1, 2 and 3 are the unit vectors in Z, Y and X 
directions while xb, yb and zb are the coordinates of baseplate origin.  
 
[
]












=
1
0
0
0
z
c
c
c
y
b
b
b
x
a
a
a
P
b
1
2
3
b
1
2
3
b
1
2
3
Ob
 
(1) 
Similarly, the HTM of the workpiece position in machine coordinates is defined con-
sidering YPR transformation as shown in Eq. (2) with α, β and γ being the rotations 
along Z, X and Y axes respectively.  
 
[
]












γ
β
β
γ
β
−
γ
β
α
−
γ
α
β
α
γ
β
α
+
γ
α
γ
β
α
+
γ
α
β
α
−
γ
β
α
−
γ
α
=
1
0
0
0
z
cos
cos
sin
sin
cos
y
cos
sin
cos
sin
sin
cos
cos
sin
sin
cos
cos
sin
x
cos
sin
sin
sin
cos
cos
sin
sin
sin
sin
cos
cos
P
P
P
P
OP
 
(2) 
Positioning transformation scheme of the proposed ﬁxturing system is shown in Fig. 4 
where Xi represents the position vector of reference i while [Pij] represents the trans-
formation matrix from position i to j.  The HTM of the baseplate with respect to ma-
chine reference ([POb]) is calculated from the locators’ initial positions. The transfor-
mation of the workpiece relative to the machine ([POP]) can be measured through 
CMM. Thus the required transformation of workpiece with respect to baseplate ([PbP]) 
is deduced and HTM of the error compensation ([POb’]) is calculated as shown in Eq. 
(3). Final absolute positions of all the six locators, required to compensate the work-
piece positioning error, are shown in Eq. (4).  
 
XO
XP
XF
Xb
Xb’
[Pob]
[POb’]
[PbP ]
[Pb’F]
[PPF]
Machine reference = Pallet reference
Initial measured position of the workpiece
Final position of the workpiece (Objective)
Initial position of baseplate
Final position of baseplate
HTM of baseplate in machine reference 
HTM of baseplate in machine reference 
HTM from baseplate to workpiece
HTM from baseplate to workpiece (Pb’F=PbP)
Workpiece positioning error (Pbb’ =PPF )
Calculation path
Rigid link
XF
Xb’
XO
Xb
XP
[PPF]
[PbP]
[Pb’F]=[PbP]
[POb’]
[POb]
Correction
[PPF ]
Error to be corrected
Rigid link
Correction 
through 
locators
Initial 
placement of 
baseplate on 
the locators
 
Fig. 4. Fixturing system reference transformation 
 
[
] [
] [
]
[
] [
][
] 1
bP
OF
'
Ob
OP
1
Ob
bP
P
P
P
P
P
P
−
−
=
=
 
(3) 

152 
S.U. Butt, J.-F. Antoine, and P. Martin 
 
[
]
[
] [
] [
]
(
)
1
'
Ob
1
Ob
OF
'
b
'
1
'
2
'
3
'
b
'
1
'
2
'
3
'
b
'
1
'
2
'
3
'
Ob
P
P
P
1
0
0
0
z
c
c
c
y
b
b
b
x
a
a
a
P
−
−
=














=
 
(4) 
The resolution of the above equations give the positions of locators which are imposs-
ible to attain because the contacting points of locators on the baseplate change as a 
result of rigid body motion of the baseplate on locators. This is shown with a 2D ex-
ample in Fig. 5, where the final calculated positions of the arc centers of locators are 
shown by 1* and 2*. Due to the constraint of uniaxial motion, the locators cannot be 
advanced to these positions. To overcome this mathematical issue, a line is drawn 
between the points 1* and 2* (plane in our case of 3D), and the points of intersections 
of this line with the locators’ axes are calculated. Moving the locators at these calcu-
lated positions will enable us to perform the required workpiece transformation. In the 
same manner, axial advancements of all the six locators are calculated through the 
contacting points of all three contacting surfaces. The final axial position of locator 1 
is shown in Eq. (5) with
'
1
a ,
'
1
b  and 
'
1c  being the unit vector components of the ba-
seplate surface. The advancements of the rest of the locators are deduced similarly. 
 
'
1
1
'
1
1
'
1
1
'
1
c
y
b
x
a
D
z
−
−
=
 
(5) 
Z
X
1*
2*
1’
2’
1
2
Z
X
1*
2*
1’
2’
1
2
D
cz
ax
D
cz
ax
'
1
'
1
*
1
*
1
=
+
=
+
c
ax
D
z
'
1
'
1
−
=
(a ) d rawi ng line and calcula ting  ne w axia l p ositio ns
(b) p e rfor m ad va nc e m e n t
 
Fig. 5. Calculating the axial advancements of locators  
3.2 
Case Study 
In order to validate the kinematic model, a case study is performed on a hip prosthesis 
repositioning through CATIA® simulation.  A CPT® 12/14 Hip Prosthesis by Zimmer 
[30] is chosen as a demonstrative workpiece. The part is created in CATIA® with 
slightly larger dimensions and supports are added. It is supposed that this workpiece 
is clamped rigidly on the baseplate which is further located through six rigid locators. 
An inverse impression of the workpiece (like a half die) is created with the original 
hip prosthesis dimensions and is placed on a fixed position with reference to the ma-
chine origin. This position represents the tool path on the machine as the tool moves 
with reference to machine and not with reference to workpiece. Boolean operation is 

 
A Kinematic Approach for 6-DOF Part Positioning 
153 
performed to simulate the machining operation by subtracting the common material 
from the workpiece. Two slots are made in the supports during machining of the first 
half part which will help to place the workpiece on two well positioned blocks after 
inverting. 
The analytical model is implemented in a worksheet directly linked to CATIA® 
model which furnishes the initial position ([POP]) of the workpiece as shown in table 
1(a). This position should be obtained by CMM in real environment. The initial posi-
tion of the baseplate ([POb]) is a function of locators’ positions shown in Table 1(b). 
The machining performed on this initially roughly placed workpiece is shown in Fig. 
6.  The workpiece should be repositioned at the required position ([POF]) to perform a 
precise machining operation. This final position is known by the part program and is 
shown in the Table 2. 
Table 1. Initial positions of locators and the workpiece 
(a) Initial locators’ positions (Axial po-
sitions are highlighted) 
(b) Initial workpiece position 
 
Table 2. Required position of the workpiece (Objective) 
 
Gray: Machined surface
Orange: Rough surface
Final 
Product
Machining 
simulation  
Fig. 6. Machining simulation on the workpiece at initial position 
The algorithm calculates the final axial positions of all the six locators (Table 3) to 
compensate the workpiece positioning error. The locators are moved to these new 

154 
S.U. Butt, J.-F. Antoine, and P. Martin 
positions and the machining simulation is re-performed. This time the material re-
moval was uniform throughout the workpiece as shown in Figure 7. 
Final 
Product
Machining 
simulation
 
Fig. 7. Machining simulation on the workpiece after repositioning 
Table 3. Calculated final position of the six locators (Axial positions are highlighted) 
 
 
Simple investigation reveals that the workpiece was not at the exact required posi-
tion. The 6 DOF repositioning error of the workpiece is shown in Table 4(a) while the 
same for the second side is shown in Table 4(b). This positioning uncertainty is due to 
the limited advancement precision (10 μm) of locators. This positioning uncertainty 
can be expressed as robustness of the proposed model. 
Table 4. Workpiece positioning error due to locators' precision 
(a) First side of the workpiece 
(a) Second side of the workpiece 
 
3.3 
Robustness of the Model 
The workpiece position uncertainty is calculated from the Plucker coordinates[31] as 
the function of precision of locators’ advancements. In our case, using the locators’ 
input positions (Table 1. Initial positions of locators and the workpiece(a)), the uncer-
tainty at reference point P (Table 2) is deduced as a function of six advancements, 
 

 
A Kinematic Approach for 6-DOF Part Positioning 
155 
 
 
(6) 
 
where, dz1, dz2, dz3, dy4, dy5 and dx6 are uncertainties of the locators’ advancements. 
In order to calculate the maximum positioning error, all the term are arranged so that 
their effect is added to the positioning error. The right most vector in Eq. (6) is the 
maximum positioning error as the function of precision of locators’ advancements ξ, 
in our case, assumed to 10μm. 
4 
Conclusion 
A fixturing system has been proposed which is capable of performing the compensa-
tion of the positioning error of the workpiece through the advancement of six locators. 
To allow a repetitive repositioning of irregular parts, a baseplate has been placed in 
between the machine table and the workpiece. The baseplate has been located through 
a 3-2-1 locating configuration and all the fixturing elements were considered to be 
rigid. The kinematic model calculated the locators’ advancements which enabled us to 
relocate the workpiece indirectly by baseplate relocation. The kinematic model has 
been simulated in CATIA and the results verified the analytical model. 
References 
1. Boyle, I., Rong, Y., Brown, D.C.: A review and analysis of current computer-aided fixture 
design approaches. Robotics and Computer-Integrated Manufacturing 27(1), 1–12 (2011) 
2. Zhang, W.: Flexible fixture design and automation: review, issues and future directions. 
International Journal of Production Research 39(13) (2001) 
3. Butt, S.U., Antoine, J.F., Martin, P.: An Analytical Model for Repositioning of 6 D.O.F 
Fixturing System. Mechanics & Industry, 13 (2012) (in Press) 
4. Ryll, M., Papastathis, T.N., Ratchev, S.: Towards an intelligent fixturing system with rapid 
reconfiguration and part positioning. Journal of Materials Processing Technology 201, 
198–203 (2008) 
5. Li, B., Melkote, S.N.: Improved workpiece location accuracy through fixture layout opti-
mization. International Journal of Machine Tools and Manufacture 39(6), 871–883 (1999) 
6. Somashekar, S.: Fixturing features selection in feature-based systems. Computers in Indus-
try 48(2), 99–108 (2002) 
7. Roy, U., Liao, J.: Fixturing Analysis For Stability Consideration in an Automated Fixture 
Design System. J. Manuf. Sci. Eng. 124(1), 98–104 (2002) 
8. Wang, M.Y.: Tolerance analysis for fixture layout design. Assembly Automation 22(2), 
153–162 (2002) 
9. Bourdet, P.: Logiciels des machines à mesurer tridimensionnelles. Techniques de 
l’ingénieur. Mesures et contrôle, no. R1316, pp. R1316–1 (1999) 

156 
S.U. Butt, J.-F. Antoine, and P. Martin 
10. Clement, A., Bourdet, P.: A Study of Optimal-Criteria Identification Based on the Small-
Displacement Screw Model. CIRP Annals - Manufacturing Technology 37(1), 503–506 
(1988) 
11. Villeneuve, F., Legoff, O., Landon, Y.: Tolerancing for manufacturing: a three-
dimensional model. International Journal of Production Research 39(8), 1625–1648 (2001) 
12. Asante, J.N.: A small displacement torsor model for tolerance analysis in a workpiece-
fixture assembly. Proceedings of the Institution of Mechanical Engineers, Part B: Journal 
of Engineering Manufacture 223(8), 1005–1020 (2009) 
13. Jayaram, S., El-Khasawneh, B.S., Beutel, D.E., Merchant, M.E.: A Fast Analytical Method 
to Compute Optimum Stiffness of Fixturing Locators. CIRP Annals - Manufacturing 
Technology 49(1), 317–320 (2000) 
14. Raghu, A., Melkote, S.N.: Modeling of workpiece location error due to fixture geometric 
error and fixture-workpiece compliance. Journal of Manufacturing Science and Engineer-
ing 127, 75 (2005) 
15. Hurtado, J.F., Melkote, S.N.: Improved Algorithm for Tolerance-Based Stiffness Optimi-
zation of Machining Fixtures. J. Manuf. Sci. Eng. 123(4), 720–730 (2001) 
16. Asante, J.N.: Effect of fixture compliance and cutting conditions on workpiece stability. 
The International Journal of Advanced Manufacturing Technology 48(1), 33–43 (2010) 
17. Marin, R.A., Ferreira, P.M.: Analysis of the Influence of Fixture Locator Errors on the 
Compliance of Work Part Features to Geometric Tolerance Specifications. J. Manuf. Sci. 
Eng. 125(3), 609–616 (2003) 
18. Liao, Y.G., Hu, S.J.: An Integrated Model of a Fixture-Workpiece System for Surface 
Quality Prediction. The International Journal of Advanced Manufacturing Technology 17, 
810–818 (2001) 
19. Hsu, Y.Y., Wang, S.S.: A new compensation method for geometry errors of five-axis ma-
chine tools. International Journal of Machine Tools and Manufacture 47(2), 352–360 
(2007) 
20. Lin, Y., Shen, Y.-L.: A Generic Kinematic Error Model for Machine Tools. Citeseer 
(2000) 
21. Ahn, K.G., Cho, D.W.: An analysis of the volumetric error uncertainty of a three-axis ma-
chine tool by beta distribution. International Journal of Machine Tools and Manufac-
ture 40(15), 2235–2248 (2000) 
22. Choi, J.P., Min, B.K., Lee, S.J.: Reduction of machining errors of a three-axis machine 
tool by on-machine measurement and error compensation system. Journal of Materials 
Processing Technology 155, 2056–2064 (2004) 
23. Jha, B.K., Kumar, A.: Analysis of geometric errors associated with five-axis machining 
centre in improving the quality of cam profile. International Journal of Machine Tools and 
Manufacture 43(6), 629–636 (2003) 
24. Zhu, S., Ding, G., Qin, S., Lei, J., Zhuang, L., Yan, K.: Integrated geometric error model-
ing, identification and compensation of CNC machine tools. International Journal of Ma-
chine Tools and Manufacture 52(1), 24–29 (2012) 
25. Martin, P., Dantan, J.Y., D’Acunto, A.: Virtual manufacturing: prediction of work piece 
geometric quality by considering machine and set-up accuracy. International Journal of 
Computer Integrated Manufacturing 24(7), 610–626 (2011) 
26. Wan, X.-J., Xiong, C.-H., Zhao, C., Wang, X.-F.: A unified framework of error evaluation 
and adjustment in machining. International Journal of Machine Tools and Manufac-
ture 48(11), 1198–1210 (2008) 
27. Paris, H.: Contribution à la conception automatique des gammes d’usinage: le probléme du 
posage et du bridge des pièces. Université Joseph Fourier Grenoble (1995) 

 
A Kinematic Approach for 6-DOF Part Positioning 
157 
28. Dursapt, M.: Aide-mémoire métrologie dimensionnelle. Dunod (2009) 
29. Bourdet, P., Schneider, F.: Spécification géométrique des produits: cotation & tolé-
rancement ISO (Coll. Technique & ingénierie). DUNOD, Paris (2007) 
30. zimmer, CPT 12/14 cemented stems (September 2011), 
http://www.zimmer.co.uk/web/...14_Hip_Syste_97-8114-
01_rev_1.pdf 
31. Halevi, G., Weill, R.D.: Principles of Process Planning: a Logical approach. Chapman and 
Hall, London (1995) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 159–168. 
DOI: 10.1007/978-3-642-30817-8_16 
© Springer-Verlag Berlin Heidelberg 2013 
 
Control Architecture for Plug-and-Play Intelligent Axes 
within Fast Reconfigurable Manufacturing Equipments 
Mircea Murar and Stelian Brad*  
Engineering Design and Robotics, Technical University of Cluj-Napoca, 
28 Memorandumului Str., 400114, Cluj-Napoca, Cluj, Romania 
murar_mircea@yahoo.com, stelian.brad@staff.utcluj.ro 
Abstract. A new perspective on a control architecture, capable to increase the 
overall performance of the manufacturing equipment by endowing it with  
distributed intelligence, thus providing short ramp-up times, plug-and-play ca-
pability, great integrability and scalability, together with cost reduction, is  
presented in this paper. The architecture concept, based on the outcome of ap-
plying specific problem solving methods, shows an intelligent axis equipped 
with a network of smart sensors and units controlled and supervised by a master 
control unit. The proposed solution is based on both electronic and software de-
signs in order to expand equipment performances and take a step forward in 
supporting manufacturing systems towards reconfigurability. An experimental 
testing bench has been constructed around a mechanical axis for exploring con-
trol architecture performances and intelligent axis concept feasibility. Results 
have shown that the proposed control architecture is functional, highly reconfi-
gurable, cost effective, and the concept of intelligent axis is feasible. 
Keywords: intelligent axis, reconfigurable system, control architecture, smart 
units, distributed intelligence. 
1 
Introduction 
Starting over a decade ago, manufacturing environment and thus manufacturing sys-
tems became subject to continuous changes due to increased customer needs and  
rapid technological developments. 
A manufacturing concept called reconfigurable manufacturing system meant to 
quickly respond to the forthcoming manufacturing needs is introduced by Koren and 
Mehrabi in the late 90s [1-2]. This concept comes with some advantages to the dy-
namically changing product varieties and batches, including lower costs, shorter 
ramp-up time and time to the market, easier to debug, reduced risk of becoming obso-
lete, etc. [3]. 
Modularity, integrability, customization, convertibility and diagnosability are the 
core functions of reconfigurable manufacturing systems (RMS). Among enabling 
                                                           
* Corresponding author. 

160 
M. Murar and S. Brad  
technologies, modular machine tools and software, open control architecture, plug-
and-play equipments and heterogeneous platforms have been identified [3]. 
Even if RMS concept it's not new, its core functions and enabling technologies are 
considered actual research directions in the Factories-of-the-Future (FoF) initiative 
roadmaps [4-5]; some of the research directions to which this paperwork is related 
are: open-control architecture, adaptability, reconfigurability, embedded intelligence, 
smart and Plug-and-Produce equipments.  
This initiative is meant to help European Union (EU) small to medium sized enter-
prises (SMEs) to achieve sustainability and competitiveness in the turbulent global 
market [6]. One question that comes up is: What are the costs of implementing such 
technological developments that have to be supported by manufacturing SMEs? 
Most of the current manufacturing systems of SMEs are characterized by tradition-
al control architectures, built around programmable logic controllers (PLCs), which 
are still not able to provide the desired level of reconfigurability, as well as the back-
ground for implementing required functionalities on the go and thus they might not 
represent reliable solutions to the future control architectures for manufacturing  
systems. 
This paperwork introduces the authors’ view of an enhanced control architecture 
capable to increase the overall performance of manufacturing equipments by endow-
ing it with distributed intelligence, thus providing short ramp-up times, plug-and-play 
capability, great integrability and scalability, extended configurability and control 
options, rapid customization, real-time assistance for system building and mainten-
ance, still being cost effective, easy to build and develop. 
The proposed solution is based on both electronic and software designs in order to 
expand equipment performances and take a step forward in supporting manufacturing 
systems towards reconfigurability. Equipment prioritization, system expandability, 
self-integration, communication options, short control loops, reporting of events,  
analog and digital data processing with respect to chosen configuration options and 
decision taking are investigated. 
2 
The Problem 
Considering the current world economic crisis, when the EU manufacturing sector has 
experienced the biggest decline in the number of SMEs from all sectors [6], the global 
market instability and customer continuous changing requirements, strengthening 
manufacturing SMEs with sustainability and competitiveness is a must-target. 
Lack of reconfigurability, modular structure of both software and hardware, intero-
perability between equipments, advanced diagnosability and limited functionalities of 
which traditional manufacturing systems are characterized, together with the desirable 
level of technological progress to be brought by implementing the FoF concept, 
represent a big step forward for manufacturing SMEs but also generate inevitable 
challenges: lifecycle cost management and effective development of the required 
infrastructure.  

 
Control Architecture for Plug-and-Play Intelligent Axes 
161 
This paperwork presents a simpler concept of control architecture for plug-and-
play intelligent axes that can be used within RMS, which might be less expensive if it 
is integrated and developed from the perspective of open-source philosophy. 
3 
Background 
A smart sensor or unit is an embedded solution, built from the sensor or unit itself and 
a microcontroller [7-8]. In the microcontroller, distributed intelligence is implemented 
by software means. In this configuration, increasing the performance of the embedded 
system is a matter of creativity and innovation with respect to sensor capabilities and 
embedded hardware design constraints.  
Also a similar concept to the one of smart sensors, to which the paper will refer 
from now on as smart units (e.g. smart motors, smart circuit breakers, smart contacts), 
is going to be introduced in this paperwork. An experimental bench, where an embed-
ded design is employed to boost up the functionalities and control options of an elec-
tro-mechanical axis, is also considered. 
The link between the control architecture, smart sensors, smart units and the human 
machine interface (HMI) will be done over the I2C communication protocol devel-
oped by Phillips [9]. The major facts that led to this communication protocol are pre-
sented by Murar and Brad in [7]; however, the communication protocol should be 
selected in order to better suit the served process. 
4 
Guidelines towards Innovation 
In order to achieve the proposed objectives, innovative problem solving tools have 
been considered. The use of CSDT method [10] in combination with TRIZ method 
[11] have been used to approach two conflicting generic design problems: increased 
capacity of the system from the perspective of reconfiguration while keeping low 
costs for development and integration; and increased versatility/adaptability of the 
system while keeping low costs for development and integration. 
For the first design challenge, the vector towards innovation is to change the con-
centration of functions and modularity. For the second design challenge, three vectors 
of innovation have to be integrated: change the concentration of functions, develop-
ment of non-uniform structures and make some characteristics of the system’s com-
ponents to change. The most impacting generic module of the control architecture is 
the information management software, followed by the algorithms for equipment 
control and the interface with process information. For the smart sensors, the most 
impacting module is the interface between the configuration options and the operating 
rules, followed by the operating algorithms and the interface with the communication 
protocol. The same impacts are reflected in the case of other smart units (e.g. smart 
motors). Thus, the focus was on developing sensors, motors, etc. that are independent 
in terms of intelligence (building them as self-intelligent units, able to carry their own 
past events), ensuring that these intelligent units can change some of their operational 
 

162 
M. Murar and S. Brad  
functions (using software means for resetting), and making them capable to commu-
nicate immediately with other intelligent units for self-reconfiguration in new opera-
tional systems. In-process configuration without losing information is another issue 
that should be solved by means of special buffers. 
5 
Application Example 
A simplified conceptual schematic of the control architecture, based on the design 
vectors from the previous section, is illustrated in figure 1. The characteristics and 
functionality of its components are briefly described below. 
 
Fig. 1. Simplified conceptual schematic of control architecture 
The control unit is an embedded solution built on such hardware and software plat-
forms that allow monitoring and management of information together with the control 
of smart equipments that are part of the served process. It is characterized by:   

 
Control Architecture for Plug-and-Play Intelligent Axes 
163 
• Scalability: the number of high priority equipments that can be connected is limited 
by the number of general purpose input output pins (GPIOs) available on the used 
microcontroller; in this case study, up to twenty five equipments. Theoretically, for 
the low priority equipments there is no limit, but process constraints and communi-
cation protocol parameters have to be considered. 
• Configurability: is considered with respect to the direction (input or output) of the 
direct connection between control unit GPIOs and high priority equipments, which 
are not predefined, thus resulting in the ability of having connected either only in-
puts or outputs or both, as well as replacing an output with an input equipment at 
any time. This depends of the process needs. 
• Plug-and-play: the control unit offers software support for automatic detection and 
integration of connected equipments, providing the operator with real-time assis-
tance together with all the information and configuration options that the connected 
equipments have. The operator just has to select the desired level of functionality, 
in a friendly manner, thus eliminating the need of specialized personnel. 
• Diagnosability: checking and identifying of problems or incompatibilities between 
equipments or cascade-connected equipments are supported by the information 
management algorithms and by the distributed intelligence enclosed in smart 
equipments, resulting in a high control level with direct impact on system fitness, 
functionality and stability. 
Since smart sensors and their characteristics are already presented in [7], still it makes 
worth to mention that they have been enhanced in this research work in order to 
achieve an extended orientation towards modularity and scalability, the electronic 
design allowing to quickly creating new smart sensors solutions for accommodating 
more sensors with different or special requirements. 
Smart units represent intelligent output manufacturing equipments which make use 
of embedded systems, software creativity and microcontrollers in order to boost up 
the performances and functionalities of normal output equipments.  
Employing both hardware and software designs it was possible to implement the 
concept from figure 1 on a electro-mechanical axis driven by a 24 VDC 5 phases 
stepper motor with windings in a pentagon connection, resulting in an intelligent elec-
tro-mechanical axis with the following functionalities: 
• Distributed intelligence: it was implemented by placing relevant information about 
the electro-mechanical axis, controlling algorithms, available functionalities and 
options inside microcontrollers’ memories. 
• Plug-and-play: it is achieved by exchanging important information about the intel-
ligent axis with the control unit using a communication protocol that makes the in-
tegration and configuration processes of the intelligent axis simpler and quicker. 
• Controlling: a broad range of motor control options have been implemented (but 
they are not limited to this range): half step, full step, forward, backward, speed 
control and all are accessible by software. 
• Remote control and learning: once the intelligent axis is integrated, the user has the 
full control upon its functionalities and can decide how and where to drive the axis 

164 
M. Murar and S. Brad  
and what moves to be learned and introduced in its working sequences in relation 
with other connected equipments, if any. 
• Short control loops and independent decision taking: they have been obtained via 
the level of distributed intelligence implemented inside the microcontroller and via 
the information provided from data management algorithms and logic from this 
equipment or from others that can influence its functionalities, as specified in the 
configuration process by the operator. 
• Parameters monitoring: the electronic part is designed such as to allow electric 
parameters monitoring and, together with data management algorithms, to identify 
possible malfunctions situations, stop the control algorithms on equipment level 
and let the control unit and user knowing about these situations. 
• Preventive maintenance: simple algorithms are implemented to keep track of work-
ing hours and conditions in order to alert the user when processes similar to: 
equipment inspection, greasing, adjustments and re-calibration are needed in order 
to prevent faults to occur. Even more advance features could be implemented if 
linked with the information from the monitored parameters. 
Adapters are additional hardware parts in the system. They are used to prioritize smart 
equipments and they can be of two types: high or low priority. By prioritizing the 
control unit, special procedures can be quickly triggered on the smart units’ side. As 
well, smart sensors can quickly trigger special procedures on the control unit side. A 
secondary role of the high priority equipments is to adjust voltage logic levels of the 
direct connection between smart equipments 24 VDC and control unit 3.3 VDC. 
Human machine interface (HMI) is the connection of the user to the control unit or 
smart equipments and it is used for selecting between configuration options, remote 
control of smart units and other specific issues or to monitor process parameters. 
6 
Tests and Results 
An experimental testing bench has been constructed in order to test the feasibility of 
the developed control architecture and the related intelligent axis concept, as it can be 
seen in figure 2. 
It consists of one master control unit (1), one power and signal distribution unit (2), 
three smart sensor units (one Smart Temperature Measurement Unit (5) for measuring 
temperature that can accommodate up to four LM35 or equivalent temperature sen-
sors, one Smart Magnetic Field Measurement Unit (6) for measuring magnetic field 
that can accommodate up to four SS49 Honeywell Hall effect sensors or equivalent, 
one Smart IR Barriers Unit (7) capable to accommodate up to three pairs of IR emitter 
and receptor for realizing IR barriers), one High Priority Smart Axis Unit (4) for me-
chanical-axis control, one human machine interface unit (3) used for configuring the 
connected smart equipments, high and low priority adapters (12).  
Each smart-equipment has its own microcontroller, where distributed intelligence 
and specific test control algorithms have been developed and implemented by soft-
ware means for concept testing purposes. 

 
Fig. 2. Expe
Current equipment conf
process in which an electro
smart sensors configuration
used to detect if the part h
process it, the second IR ba
range of the electro-mechan
when the sliding part of th
home position (a small mag
temperature sensor (8) is pl
Control Architecture for Plug-and-Play Intelligent Axes 
erimental test bench (without connecting wires) 
figuration that can be seen in figure 2 could represen
omechanical axis (4) is used to move a part (11), the act
n can be interpreted as follows: one smart IR barrier (9
has been pushed over a point so that another equipmen
arrier is used to detect if the part has exceeded the operat
nical axis, the Hall effect smart sensor (10) is used to de
he electro-mechanical axis (4) has been brought back
gnet is placed on the bottom of the sliding part), the sm
laced on the stator of the electromechanical axis’ (4) mo
165 
 
nt a 
tual 
9) is 
nt to 
ting 
etect 
k to 
mart 
otor 

166 
M. Murar and S. Bra
and can be used to keep te
experienced and to prevent 
If sensors are repositione
rations and functionalities c
For a better understandin
sor and smart unit from soft
tionalities. The functionaliti
bench are left to reader crea
ities that can be implemente
Fig. 3. Example of f
Fig. 4. Process pr
ad  
emperature data logging, to detect if high temperature 
axis deterioration by shutting down the control. 
ed and additional software is developed, different confi
can be achieved for the overall system. 
ng of the concept, the paperwork dives into one smart s
tware point of view and exemplifies the implemented fu
ies of the other smart sensors used in this experimental 
ativity to deal with. Bellow, in figure 3, a list of function
ed into specific equipments is introduced. 
 
functionalities that can be implemented into smart units 
rogramming example related to specified equipments 
are 
igu-
sen-
unc-
test 
nal-
 

 
Control Architecture for Plug-and-Play Intelligent Axes 
167 
After connecting and auto-integrating the equipment, specifying the control algo-
rithm of a unit by the operator using HMI physical interface is inspired from object 
oriented programming (OOP) since it is very intuitive. The HMI will real-time actual-
ize and populate the list of available options, based on equipment configuration  
options, other connected equipments and their configuration options, operators, spe-
cific actions and options. Thus, a small fracture of the control architecture is presented 
in figure 4. Reduced representation regarding the list of available options and what 
they imply can be viewed in figure 4, representing a line of the control algorithm.  
7 
Discussions 
Intensive testing on the case study for recognizing specific characteristics has shown 
that the proposed control architecture is functional, highly reconfigurable, cost-
effective, and the concept of intelligent axis is feasible. Some of the features of great 
importance in dealing with future manufacturing system problems and market needs 
can be also found in our experimental test bench: reconfigurability, master control 
unit has the capacity to configure on-the-go and at any time the direction of its pins 
(i.e. the connection to the external world) with respect to the connected device; the 
operator can ask a specific connected equipment to provide its configurability options 
and choose between these options a desired way of how an equipment will act with 
respect to the implemented software and hardware configuration; plug-and-play, mas-
ter control unit detects when an equipment is disconnected or connected on the com-
munication bus and takes care to integrate and configure the connected part without 
corrupting data transfer on the communication bus; real-time assistance, all informa-
tion about a device is inside the memory of the attached microcontroller, thus  
connecting two or more incompatible devices will result in alerting the operator and 
blocking any attempt of driving or controlling that device; scalability, has been ob-
tained by conferring a priority level to equipments: high or low priority (note: the 
number of high priority equipments is restricted to the number of available pins on the 
master control unit, thus having a direct connectivity to master control unit for quick 
triggering of actions; the number of low priority equipments is restricted by the time 
constraints of the deserved process and the bus electric capacitance); independent 
decision taking, like preventive actions, in the case of smart units it is based on 
process information and data statistics regarding process values. The level of decen-
tralization is another important feature identified in our design. It is obtained from the 
symbiosis between the given equipment and a microcontroller connected by a com-
munication network to other equipments and to the higher management level. 
Test performed on a communication protocol’s speed of 400 kbit/s has shown that 
a time period under 5 seconds is needed by the main control unit to detect and auto-
integrate any connected equipment. Also, an average of 5 minutes is needed by a 
skilled operator to configure the connected equipment. 

168 
M. Murar and S. Brad  
8 
Conclusions 
This research introduces a control architecture that enables building up intelligent 
axes for fast integration within technical systems. Tests have shown that the proposed 
control architecture is suitable for supporting the development of small scale, highly 
reconfigurable systems using smart equipments at affordable costs. In comparison 
with control architectures build around programmable logic controllers, the proposed 
architecture takes a step closer to what reconfigurable manufacturing systems concept 
is based on. 
One noticeable limitation of the proposed solution is related to the communication 
protocol performances where significant propagation delays are experienced over 100 
meters at high data rates, as stated by the producers. In has to be considered that this 
communication protocol was employed to emphasize the concept of the control archi-
tecture, if data transmissions over larger distances have to be covered, protocols like 
CAN and LAN could be used together with the set of hardware and software frame-
work that they require. 
The obtained results encourage to further work on this concept for reaching its full 
potential. The first envisaged research direction would be on increasing the simplicity 
of equipment and process configuration by developing an enhanced HMI that  
considers a PC-software together with the USB support of the main control unit mi-
crocontroller. The second research direction is about developing an online open-
source data base with information, control algorithms, drivers, etc. about different 
smart equipments.   
References 
1. Koren, Y., Heisel, U., Jovane, F., Moriwaki, T., et al.: Reconfigurable manufacturing sys-
tems. Annals of the CIRP 48/2 (1999) 
2. Mehrabi, M.G., Ulsoy, A.G., Koren, Y.: Reconfigurable manufacturing systems: Key to 
future manufacturing. Journal of Intelligent Manufacturing 11, 403–419 (2000) 
3. Mehrabi, M.G., Ulsoy, A.G., Koren, Y., Heytler, P.: Trends and perspectives in flexible 
and reconfigurable manufacturing systems. Journal of Intelligent Manufacturing 13, 135–
146 (2002) 
4. Factories of the Future PPP Strategic Multi-Annual Roadmap 2007-2013. Publications of-
fice of the European Union, Luxembourg (2010) 
5. Factories of the Future PPP FoF 2020 Roadmap Consultation document (2011) 
6. Wymenga, P., Spanikova, V., Derbyshire, J., Barker, A.: Are EU SMEs recovering? An-
nual Report on EU SMEs 2010/2011, Rotterdam, Cambridge (2011) 
7. Murar, M., Brad, S.: Providing Configurability and Plug-and-play Capability to Simple 
Sensors: A Step towards Smart Sensors for Smart Factories. Applied Mechanics and Mate-
rials 162, 597–606 (2012) 
8. Dubey, D., Anjaneyulu, T.: Smart Sensors. EE Dept, IIT Bombay (2002) 
9. Irazabal, M.J., Blozis, S.: I2C Manual (AN10216-01), San Jose (2003) 
10. Brad, S.: Complex System Design Technique. International Journal of Production Re-
search 46/21, 5979–6008 (2008) 
11. Altshuller, G.: The innovation algorithm: TRIZ, Worchester, Technical Innovation Center 
(2000) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 169–179. 
DOI: 10.1007/978-3-642-30817-8_17      © Springer-Verlag Berlin Heidelberg 2013 
Product Development Process Modeling:  
State of the Art and Classification  
Carolina Román Amigo, Diego Rodrigues Iritani, Henrique Rozenfeld,  
and Aldo Ometto 
Production Engineering Department, University of São Paulo, Av. Trabalhador São Carlense, 
400, São Carlos, SP 13566-590, Brazil 
carolamigo@gmail.com, {diritani,roz,aometto}@sc.usp.br 
Abstract. Process modeling is a set of activities to be followed to create one or 
more models of a process for a certain purpose. Some modeling methods are 
more suitable for a given purpose than others, an essential fact to remember 
when choosing a modeling method. Some literature reviews about product de-
velopment process modeling and their purposes are available on the literature; 
however, none of them intend to deplete the subject. Therefore, this research 
aims to provide a state of the art about product development process modeling 
methods and propose a detailed and comprehensive classification of them based 
on their purposes. To this end, a systematic literature review is conducted, fol-
lowed by the elaboration of a matrix that relates modeling methods to their pur-
poses. The resulting matrix can serve as a starting point for the elaboration of a 
framework for modeling method selection.  
Keywords: product development, process models, process modeling, process 
representation, process visualization, models purposes. 
1 
Introduction 
Among basic organizational capabilities, the ability to innovate by developing new 
products provides the greatest competitive advantage [1, 2]. Product development 
(PD) is the process by which an organization transforms market opportunities and 
technical possibilities into valuable information for commercial production [3]. 
Unlike business processes designed to produce predictable results, PD is intended to 
create something new. A business process with distinct characteristics, PD involves 
creativity and innovation, is non-linear, and iterative [4, 5]. 
Process modeling is an activity set to be followed to create one or more models of 
a process for a certain purpose, usually the representation, explanation, design, 
specification, analysis, or control of a given process [6]. It is essential to choose a 
model appropriate to its purpose [5–7]; some process modeling methods are more 
suitable for a given purpose than others. For example, a design structured matrix will 
show process activity dependencies more clearly than a simple flowchart and will not 

170 
C.R. Amigo et al. 
highlight process improvements opportunities as efficiently as a value stream map 
can.  
As well as other processes, it is possible and useful to build models for product 
development processes [8]. For example, process models can help a development 
team focus on value-adding activities, provide current situation transparency and 
visibility to a workforce, indicate process-related best practices, provide a baseline for 
process management, allow process change analyses, and assist the comprehension of 
complex processes, among others [5].  
The business process modeling literature is extensive. Literature reviews on both 
general modeling methods [2, 7, 9] and specific PD process modeling [5, 8, 10–12] 
have been conducted. Although these authors do not agree on the definition of the 
term “modeling methods” (but instead use terms like “frameworks,” “approach,”  
techniques,” “languages,” and “views”), they discuss with varying abstraction levels 
and foci a similar set of PD process modeling methods (e.g., event process chain, 
design structure matrix, flowcharts).  
Some of these reviews describe modeling method types and their typical purposes 
[5, 10], group modeling methods in categories regarding their purposes [8], or analyze 
the fitness of modeling methods for various purposes [12]. However, none of these 
reviews intend to deplete the subject. Methods described in one review are sometimes 
not cited in others, and none of the reviews aggregates all of the extant methods. The 
current classification based on process purposes is largely generic (as it refers to 
modeling method purposes collectively rather than to each method’s individual 
purpose) and is often partial. For example, Browning [12] examines the attributes that 
a model view is able to represent and that are useful to a given purpose to elaborate a 
matrix showing the alignment between model views and their purposes. The matrix is 
a very interesting contribution, but, as it relies only on information about attributes, 
does not consider other aspects important to a model’s fitness for its purpose, like 
intuitiveness and ease of use.  
Therefore, this research aims to aggregate, complement, and update the extant 
literature reviews in order to provide a current overview of PD process modeling 
methods, and propose a more detailed and comprehensive classification of them based 
on their purposes. 
2 
Research Method 
2.1 
Systematic Literature Review 
The following systematic literature review was based on the roadmap proposed by 
Conforto et al. [13], adapted from other knowledge areas [14, 15] to guide systematic 
literature reviews on operations management. This roadmap’s main characteristics are 
its research strings tests and refinements, the iterative processing of its results, and its 
references by references search. The roadmap phases will be described, detailing the 
methodology employed.  

 
Product Development Process Modeling: State of the Art and Classification 
171 
Phase 1: Inputs. In this phase, the systematic literature review was planned and its 
inputs defined. The resulting plan as the inputs defined are shown below.   
1. Objective definition: identify the modeling methods used in PD process modeling. 
2. Database definition: qualified experts, and ISI Web of Science (Thomson Reuters) 
and SciVerse Scopus (Elsevier) authoritative sources. Articles and conference 
proceedings available in English, free of charge, and authenticated by the 
researchers’ institutions should be considered. Searches should be conducted using 
the “title,” “abstract,” and “keywords” fields. 
3. Strings definition: the keywords were selected from the list of articles identified by 
the experts. Three iterations were then carried out for strings refinement. 
4. Inclusion criteria definition: the established criteria for articles including was 
“proposition, description or application of modeling frameworks, methods, 
techniques or approaches to PD process modeling” and “proposition, description or 
application of new PD process models”. 
5. Searching: searching the selected databases, eliminating duplicates, and exporting 
results to a table for filters application 
6. Filters with inclusion criteria application: 1st iteration with article’s title, keywords 
and abstract reading; 2nd iteration with article’s introduction, results and conclusion 
reading; 3rd iteration with article’s full reading. 
7. References by references search: should be performed using the references of the 
selected articles. 
8. Data extracting to synthesis table, obtained from deep reading of selected articles.  
9. Articles cataloging and storing in bibliographic reference manager software. 
Phase 2: Processing. A systematic literature review search, results analysis, and 
documentation were performed. Searching using the chosen string produced 5646 
articles (counting both databases). Of this total, 1394 articles were duplicates across 
the two databases (a 33% overlap). Thus, 4252 articles were iteratively subjected to 
the filters defined in the previous phase. During articles’ full reading, it is normal to 
find citations to other relevant articles that did not appear in the references by 
references search. In our systematic literature review, 36 articles were found through 
the references by references search. Finally, 101 articles were analyzed, 65 from the 
filters selection and 36 from the references by references search. Data extraction from 
the selected articles to a synthesis table then occurred, listing all the modeling 
methods found and their purposes. The only information considered was what could 
be retrieved from the analyzed set of articles; no critical analysis occurred at this 
point. 
Phase 3: Outputs. The literature review’s main output was the synthesis table 
(reflecting 52 modeling methods) and the identification of the main authors and 
journals about the subject. Most of the selected articles were drawn from the 
engineering and computing fields.  

172 
C.R. Amigo et al. 
2.2 
Elaboration of the Modeling Methods x Purposes Matrix 
The raw data collected into the synthesis table after the systematic literature review 
enabled the elaboration of the modeling methods x purposes matrix. This data were 
submitted to a refining critical analysis consisting of two phases. In the first, the 
modeling methods were analyzed; in the second, their purposes were examined. The 
critical analysis of the first phase occurred in two steps: duplicates were eliminated 
(some authors referred to a single modeling method using different names), and the 
selected modeling methods were confirmed to lie within the scope of this research. In 
the second phase, the examination of the modeling methods’ purposes was refined in 
two steps. First, the purposes assigned to each method according to what could be 
found in the literature were examined for duplicates and discrepancies and were then 
rewritten in standard format (i.e., using a verb followed by a substantive). These steps 
were repeated in three iterations in order to produce a refined set of purposes (see 
Figure 1). Finally, a matrix was elaborated based on the refined set of modeling 
methods and purposes.  
 
Fig. 1. Method for synthesis table refinement 
3 
Results  
The main result is a matrix relating the modeling methods to their purposes (see Table 
1). The modeling methods, along with their main references, occur in the rows, which 
include descriptions and more detail.1 The purposes, represented by an ID, occur in 
the columns and are further explained in Table 2, which also shows the respective 
matrix ID.  
                                                           
1 Due to the page restriction, only a small subset of the consulted articles is referenced in the 
matrix. Please contact the authors for additional references.  

 
Product Development Process Modeling: State of the Art and Classification 
173 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 1. Matrix relating modeling methods to purposes 
Instructions for authors, p. 2, 2013. 
© Springer-Verlag Berlin Heidelberg 2013 
Purposes 
P1 
P2 
P3 
P4 
P5 
P6 
P7 
P8 
P9 
P10 
P11 
P12 
P13 
P14 
P15 
P16 
P17 
P18 
P19 
P20 
P21 
P22 
P23 
P24 
P25 
P26 
P27 
P28 
 
Modelling Method [main references] 
A1 
7 
7 
12 
21 
4 
6 
40 
6 
20 
3 
29 
8 
33 
15 
5 
2 
27 
16 
5 
9 
9 
17 
9 
28 
11 
4 
13 
34 
 B2 
Event process chain diagram (EPC)[11, 16] 
  
X 
  
  
X 
  
X X 
  
  
X 
  
X X X 
  
  
X X X 
  
X X X 
  
  
X X 16 
Business process modeling notation (BPMN)[5, 17] 
X 
  
  
  
X 
  
X 
  
  
  
X 
  
  
X 
  
X X X X X 
  
  
X X 
  
  
X X 14 
New modelling framework [18] 
  
  
X X 
  
  
  
  
X X X 
  
X 
  
X 
  
X X 
  
  
X X 
  
X 
  
  
  
X 13 
Signposting [5, 10, 19, 20] 
X 
  
X X 
  
  
X 
  
  
X X X X 
  
  
  
X 
  
  
  
  
  
X X 
  
  
X X 13 
Design structure matrix (DSM)[5, 10, 18, 21–23] 
  
  
X X 
  
  
X 
  
  
  
X 
  
X 
  
  
  
X X 
  
X X X 
  
X 
  
  
  
X 12 
Work transformation matrix (WTM)[5, 8, 18, 24, 25] 
  
  
  
X 
  
  
X 
  
X 
  
X 
  
X X 
  
  
X X 
  
  
X X 
  
X 
  
  
  
X 12 
Design roadmap [18, 21] 
  
  
  
  
  
  
X 
  
  
  
X X X 
  
  
  
  
X X X X 
  
X X 
  
  
X 
  
11 
Q-GERT model [5, 8, 26] 
X 
  
  
X 
  
  
X 
  
X 
  
X 
  
X 
  
  
  
X 
  
X 
  
  
X 
  
X 
  
  
X 
  
11 
Fuzzy DSM [5, 27] 
  
  
X X 
  
  
X 
  
  
  
X 
  
X X 
  
  
X 
  
  
  
  
X 
  
X 
  
  
X X 11 
Graphical evaluate review technique (GERT)[5, 11, 18, 28] 
X 
  
  
X 
  
  
X 
  
X 
  
X 
  
X 
  
  
  
X 
  
X 
  
  
X 
  
X 
  
  
X 
  
11 
Activity-on-arc diagram [5, 11, 29] 
X 
  
  
X 
  
X X 
  
X 
  
  
  
X 
  
  
  
  
X 
  
  
  
  
  
  
X X 
  
X 10 
Gantt chart [7, 11, 18] 
  
  
  
X 
  
X X X X 
  
X 
  
X 
  
  
  
  
X 
  
  
  
X 
  
  
X 
  
  
  
10 
D-critical path method [5, 30] 
  
  
  
X 
  
  
X 
  
X 
  
X 
  
X X 
  
  
X 
  
  
  
  
X 
  
X 
  
  
  
  
9 
Sequential iteration model [5, 8, 25] 
  
  
  
X 
  
X X 
  
  
  
  
  
X X 
  
  
X 
  
  
  
  
X 
  
  
  
  
X X 9 
Markov models [5, 25, 31] 
  
  
  
X 
  
  
X 
  
X 
  
X 
  
  
  
  
  
X X 
  
  
  
  
  
X 
  
  
X X 9 
Value stream mapping [5, 11, 32] 
X X 
  
  
  
  
X 
  
  
  
  
X X X 
  
  
  
  
  
  
  
  
X 
  
  
X 
  
X 9 
Integration definition for function modeling (IDEF3)[5, 7, 
10, 11, 18, 21, 33] 
  
  
X 
  
  
  
X 
  
  
  
  
  
X 
  
  
  
X X 
  
  
  
  
X X 
  
  
  
X 8 
Activity module decomposition model[5, 8, 34] 
  
  
  
  
  
  
X 
  
  
  
X 
  
X X 
  
  
X 
  
  
  
X X 
  
  
  
  
  
X 8 
Function block diagram [21] 
  
  
  
  
  
  
X 
  
  
  
X X X 
  
  
  
X 
  
  
  
X 
  
X X 
  
  
  
  
8 
High-level “life cycle” models [11, 35] 
  
  
  
  
  
  
X 
  
  
  
X 
  
  
  
X 
  
  
  
  
X X 
  
  
X X 
  
  
X 8 
Modeling the release time of a single uncertain iterative 
activity [36] 
  
  
  
  
  
  
X 
  
X 
  
  
X X 
  
  
  
X 
  
  
  
X 
  
  
X 
  
  
  
X 8 
Iterative cycle time estimation[5, 8] 
  
  
  
  
  
  
X 
  
X 
  
X 
  
X 
  
  
  
X X 
  
  
  
X 
  
  
  
  
  
X 8 
Entry-task-validation-exit (ETVX) diagram [5, 11, 38] 
  
  
  
  
  
  
X X 
  
  
X 
  
X 
  
  
  
X 
  
  
X 
  
  
  
X 
  
  
  
X 8 
                                                             
1 A: Total of modeling methods that fulfill this purpose. 
2 B: Total of purposes that are fulfilled by this modeling method. 

174 
C.R. Amigo et al. 
 
 
 
 
 
Table 1. (continued) 
Purposes 
P1 
P2 
P3 
P4 
P5 
P6 
P7 
P8 
P9 
P10 
P11 
P12 
P13 
P14 
P15 
P16 
P17 
P18 
P19 
P20 
P21 
P22 
P23 
P24 
P25 
P26 
P27 
P28 
 
Modelling Method [main references] 
A2 
7 
7 
12 
21 
4 
6 
40 
6 
20 
3 
29 
8 
33 
15 
5 
2 
27 
16 
5 
9 
9 
17 
9 
28 
11 
4 
13 
34 
 B3 
Structured analysis and design technique (SADT)[5, 21, 33, 
37] 
  
  
X 
  
  
  
X 
  
  
  
  
X X X 
  
  
  
  
  
  
  
  
X X 
  
  
  
X 8 
Integration definition for function modeling (IDEF0)[5, 7, 
10, 11, 18, 21, 33, 39] 
  
  
X 
  
  
  
X 
  
  
  
  
X X X 
  
  
  
  
  
  
  
  
X X 
  
  
  
X 8 
Petri net [5, 7, 10, 18, 21, 40] 
  
X 
  
  
X 
  
X 
  
  
  
  
  
X 
  
  
  
X X 
  
  
  
  
  
X 
  
  
X 
  
8 
Feedbacks and crossovers [5, 8] 
  
  
  
X 
  
  
X 
  
  
  
  
  
  
  
  
  
X 
  
  
  
X X 
  
X 
  
  
X X 8 
Activity-on-node task graph [5, 29] 
  
  
  
X 
  
X X 
  
X 
  
  
  
X X 
  
  
  
  
  
  
  
  
  
  
  
  
  
X 7 
Project evaluation and review technique (PERT)[5, 10, 18, 
21, 36] 
  
  
  
X 
  
X X 
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
X 
  
  
X 
  
  
X 7 
Colored petri nets [5, 7, 18, 41] 
  
X 
  
  
  
  
X 
  
  
  
X 
  
X X 
  
  
  
  
  
  
  
  
  
  
  
  
X X 7 
Activity-activity incidence matrix [18, 33, 42, 43] 
  
  
X 
  
  
  
X 
  
  
  
  
  
X X 
  
  
X 
  
  
  
  
X 
  
  
  
  
  
X 7 
Data flow diagram [7, 12] 
  
  
  
  
  
  
  
  
  
  
X 
  
  
X 
  
  
  
X 
  
X 
  
  
  
X 
  
X 
  
X 7 
Critical path method (CPM)[5, 10, 18, 36] 
  
  
  
X 
  
X X 
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
X 
  
  
X 
  
  
X 7 
Flow chart [5, 7, 11] 
  
  
  
  
X 
  
X 
  
  
  
  
  
  
  
  
X X X 
  
  
  
  
  
X 
  
  
  
X 7 
Module scheduling [5, 8] 
  
  
X X 
  
  
X X X 
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
X 7 
Stage overlapping [5, 8, 44] 
  
X 
  
X 
  
  
X 
  
X 
  
  
  
X 
  
  
  
X 
  
  
  
  
  
  
X 
  
  
  
  
7 
Digraph [18, 21, 27] 
  
  
  
  
  
  
X 
  
X 
  
  
  
X 
  
  
  
X X 
  
  
  
  
  
  
  
  
  
X 6 
Stage targeting [5, 8] 
  
  
X X 
  
  
X X 
  
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
X 6 
Queuing network model [5, 8] 
  
  
  
  
  
  
X 
  
X 
  
X X X 
  
  
  
  
  
  
  
  
X 
  
  
  
  
  
  
6 
Network of commitments [5] 
  
  
  
  
  
  
X 
  
  
  
X 
  
X X 
  
  
  
  
  
  
  
  
  
X 
  
  
  
X 6 
Strategic information flows [23] 
  
  
  
  
  
  
  
  
  
  
X 
  
  
  
  
  
X 
  
  
  
  
  
  
X X 
  
  
X 5 
Product release [8] 
  
X 
  
X 
  
  
  
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
X 
  
X 
  
5 
SIPOC diagram [5, 11] 
  
  
  
  
  
  
  
  
  
  
X 
  
X 
  
  
  
  
  
  
X 
  
  
  
  
X 
  
  
X 5 
Textual narrative [11, 12] 
  
  
  
  
  
  
X 
  
  
  
  
  
  
  
X 
  
  
  
  
X 
  
  
  
X 
  
  
  
X 5 
Analysis of interconnected decision areas (AIDA)[8, 45] 
X 
  
  
  
  
  
  
  
  
X 
  
  
X 
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
  
4 
Block diagrams [5, 24] 
  
  
X 
  
  
  
  
  
X 
  
X 
  
  
  
  
  
X 
  
  
  
  
  
  
  
  
  
  
  
4 
Parallel scheduling [5, 8, 46] 
  
  
  
X 
  
  
X 
  
  
  
  
  
X 
  
  
  
  
  
  
  
  
  
  
X 
  
  
  
  
4 
Signal flow graph [18, 47] 
  
  
X 
  
  
  
  
  
X 
  
X 
  
  
  
  
  
X 
  
  
  
  
  
  
  
  
  
  
  
4 
Stock-and-flow diagram [11, 12] 
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
X 
  
  
  
  
  
  
X 
  
  
  
3 
Timing of design reviews [5, 8] 
  
  
  
  
  
  
  
X 
  
  
  
  
  
  
  
  
X 
  
  
  
  
  
  
  
X 
  
  
  
3 
Create-read-update-delete (CRUD)  [11] 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
X X 
  
  
2 
Responsibility assignment matrix [12, 48] 
  
  
  
  
  
  
  
  
  
  
X 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
1 

 
Product Development Process Modeling: State of the Art and Classification 
175 
Table 2. Purposes of modeling methods 
Matrix ID—Purpose: Explanation 
Matrix ID—Purpose: Explanation 
P1: Evaluate design process complexity: level of 
complexity of development process (e.g., by number of 
activities, dependencies, or resources required). 
P15: Indicate standard practices/tools: provide 
information about best practices and PD tools based on 
literature, benchmarking, and/or successful projects. 
P2: Evaluate design process performance: in time, cost, 
productivity, number of error/reworks, etc. Also outputs 
performance (product quality, time to release, etc.). 
P16: Interchange process data: provide interchange 
between different computer systems through an 
integrated database or common language. 
P3: Evaluate design process risks/reliability: risks, 
stability, confidence level, failure modes, and their effects. 
P17: Identify/manage process iteration: minimize, 
eliminate process loops/circuits. Rework management. 
P4: Evaluate design process time-cost/ quality tradeoffs: 
support decision making in tradeoffs to minimize time and 
cost expenditures and maximize quality. Also project 
improvement to minimize time and cost expenditures. 
P18: Monitor/control process/activities: show key 
performance indicators as activity status, completion 
percentage, and time to completion to allow process or 
project monitoring/control. 
P5: Automate process: workflow modeling/management. 
Describe a process that can be performed by computer 
agents. 
P19: Multiple process integration/coordination 
management: allow integration/coordination of multiple 
interacting processes and/or multi-party collaborations. 
P6: Calculate slack/float time: deriving the degree of float 
(slack) or scheduling flexibility for each process activity. 
P20: Organize process knowledge: documentation/ 
register of design process data/information about the 
work and how to do it. Manage process data. 
P7: Define/show activities/sequences: process or project 
activities linear ordering, sequencing. 
P21: Reduce complexity of design process: decompose 
process models to reduce complexity. 
P8: Determine the timing of design reviews/gates: design 
reviews tradeoff (unnecessary reviews x great amount of 
rework) to determine optimal time for realization. 
P22: Scheduling design activities/tasks: determine 
starting and finishing times for process activities and 
tasks. 
P9: Estimate completion time of a process or project by 
showing the critical path or indicating the completion time 
for each activity. Estimate product time-to-market. 
P23: Show activities hierarchy (parent/children): show 
parent/children relationship, structure decomposition. 
Indicate hierarchical levels of PD process. 
P10: Handle the changes: analysis of process/project 
modifications. Provide alternative routing for process 
changes or allow model updating. 
P24: Show flow of data or information: show how 
information enters and leaves the process (process or 
activities inputs/outputs). 
P11: Identify and organize/allocate required resources: 
assign roles/responsibilities; optimize resource allocation; 
analyze resource sharing between processes; show 
organizational units involved; show stakeholders. 
P25: Show process milestones/deliverables: highlight 
important process or project events. Show process or 
project times for a determined work package 
delivery/analysis. 
P12: Identify constraints that can interfere in the PD 
process (e.g., paucity of financial resources or staff, 
regulation requirements, infrastructure limitations). 
P26: Shows activities’ effects on deliverables/flow of 
information: connect activities with deliverables and 
indicate cause and effect relations. 
P13: Identify dependencies/precedence of activities/ 
functions: enable concurrent engineering (show 
parallelism, coupling, minimize overlapping); allow 
network analysis; show technical product dependencies. 
P27: Simulate design process: predict process behavior 
in different scenarios. 
P14: Improve/continuously improve design process: 
increase process efficiency; process reengineering. 
P28: Visualize/understand design process: provide 
concise representation; communicate, explain process. 
4 
Discussion and Conclusion 
Although the performed systematic literature review allows a comprehensive analysis 
of modeling methods and their purposes, it has limitations. The analyzed papers 
address the modeling methods from several perspectives and on varying levels of 
detail. Thus, the resulting set of purposes includes highly abstract purposes as “show 
flow of data and information” and less abstract ones as “define/show activities 
sequence.” The same is true of the modeling methods, where a high-level model 
contrasts with a Gantt chart. Future research should classify the methods and  
 
 

176 
C.R. Amigo et al. 
purposes, identifying the groups that would help users with matrix comprehension 
and method selection. Moreover, the literature describes some modeling methods 
better than others, implying that more details about purposes are available for some 
methods than for others. This can produce a misconception about modeling methods’ 
appropriateness to their purposes. For example, a modeling method poorly detailed in 
the literature can appear unsuitable for a purpose it can satisfy. This also turns 
infeasible to indicate the degree of purpose fulfillment for each method. The authors’ 
critical analysis during the synthesis table refinement helped alleviate these problems, 
but further analyses and validation by experts and practitioners would be valuable. 
Despite their importance to the PD process, some purposes—like “handle with 
changes” and “indicate standard practices/tools”—are attended by only a few 
modeling methods. This may result from the adaptation of methods originally created 
for process modeling, given that most PD process modeling methods are derived from 
methods created to model repetitive processes, or to the inherent limitations of the 
traditional interfaces and tools used for process representations (i.e., printed posters or 
software with limited functionalities). Furthermore, some papers describe modeling 
method purposes in combination with software tools or platforms, while others 
describe the purposes of modeling methods alone. Future studies should clarify which 
purposes flow from this combined use. 
Despite these limitations, this study provides insights that can help researchers and 
practitioners interested in PD process modeling. The study’s modeling methods list 
aggregates all the modeling methods found in the available literature reviews and 
provides additional ones. Although an exhaustive list is unfeasible, this research 
constitutes a comprehensive reference for future studies. The purposes list is also a 
valuable resource, as few studies have sought to identify PD modeling methods 
purposes. This study’s matrix can help PD process modelers select the best modeling 
method for a given purpose. Future studies could give it more detail, showing the 
degree of purpose fulfillment provided by each modeling method (i.e., partial or full), 
which was not possible with the methodology applied in this research. It could also 
serve as a basis for a modeling method selection framework that combines process 
model purposes with other selection criteria, such as model change permissiveness. 
Moreover, it could serve as the starting point for future efforts to develop new process 
modeling methods more suitable for PD, by indicating unattended PD process model 
purposes and related shortcomings. 
 
Acknowledgments. The authors are grateful for the financial support provided by the 
Comissão de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) and the 
Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP). 
 

 
Product Development Process Modeling: State of the Art and Classification 
177 
References 
1. Slater, S.F.: The Challenge of Sustaining Competitive Advantage. Management 
Science 86, 79–86 (1996) 
2. Kalpic, B., Bernus, P.: Business Process Modelling in Industry: The Powerful Tool in En-
terprise Management. Computers in Industry 47, 299–318 (2002) 
3. Clark, K.B., Fujimoto, T.: Product Development Performance: Strategy, Organization, and 
Management in the World Auto Industry. Harvard Business School Press (1991) 
4. Kline, S.J.: Innovation is Not a Linear Process. Research Management 26, 36–45 (1985) 
5. Browning, T.R., Fricke, E., Negele, H.: Key Concepts in Modeling Product Development 
Processes. Systems Engineering 9, 104–128 (2006) 
6. Vernadat, F.: Enterprise Modeling and Integration: Principles and Applications. Springer 
(1996) 
7. Aguilarsaven, R.: Business Process Modelling: Review and Framework. International 
Journal of Production Economics 90, 129–149 (2004) 
8. Smith, R.P., Morrow, J.A.: Product Development Process Modeling. Design Studies 20, 
237–261 (1999) 
9. Kettinger, W.J., Teng, J.T.C., Guha, S.: Business Process Change: A Study of Methodolo-
gies, Techniques, and Tools. MIS Quarterly 21, 55 (1997) 
10. O’Donovan, Browning, T.R., Eckert, C.M., Clarkson, P.J.: Design Planning and Modeling. 
In: Design Process Improvement: A Review of Current Practic, pp. 60–87. Springer (2005) 
11. Browning, T.R.: The Many Views of a Process: Toward a Process Architecture Frame-
work for Product Development Processes. Systems Engineering 12, 69–90 (2008).  
12. Browning, T.R.: On the Alignment of the Purposes and Views of Process Models in 
Project Management. Journal of Operations Management 28, 316–332 (2010) 
13. Conforto, E.C., Amaral, D.C., Silva, S.L.D.: Roteiro para Revisão Bibliográfica Sistemáti-
ca: Aplicação no Desenvolvimento de Produtos e Gerenciamento de Projetos. In: 8o Con-
gresso Brasileiro de Gestão de Desenvolvimento de Produto - CBGDP, Porto Alegre, pp. 
1–12 (2011) 
14. Levy, Y., Ellis, T.J.: A Systems Approach to Conduct an Effective Literature Review in 
Support of Information Systems Research. Science Journal 9 (2006) 
15. de Almeida Biolchini, J.C., Mian, P.G., Natali, A.C.C., Conte, T.U., Travassos, G.H.: 
Scientific Research Ontology to Support Systematic Review in Software Engineering. Ad-
vanced Engineering Informatics 21, 133–151 (2007) 
16. Scheer, A., Thomas, O., Adam, O.: Process Modeling Using Event-Driven Process Chains, 
pp. 119–145 (2005) 
17. Arkin, A.: Business Process Modeling Language (2002) 
18. Jun, H., Suh, H.: A Modeling Framework for Product Development Process Considering 
Its Characteristics. IEEE Transactions on Engineering Management 55, 103–119 (2008) 
19. O’Donovan, B.D., Clarkson, P.J., Eckert, C.: Signposting: Modelling Uncertainty in De-
sign Processes. In: Proceedings of the 14th International Conference on Engineering De-
sign, Stockholm, Sweden, pp. 19–21 (2003) 
20. Clarkson, P.J., Hamilton, J.R.: “Signposting”, A Parameter-driven Task-based Model of 
the Design Process. Research in Engineering Design 12, 18–38 (2000) 
21. Park, H., Cutkosky, M.R.: Framework for Modeling Dependencies in Collaborative Engi-
neering Processes. Research in Engineering Design 11, 84–102 (1999) 
22. Eppinger, S.D., Whitney, D.E., Smith, R.P., Gebala, D.A.: A Model-Based Method for 
Organizing Tasks in Product Development. Research in Engineering Design 6, 1–13 
(1994) 

178 
C.R. Amigo et al. 
23. Lewis, W., Cangshan, L.: The Timely Allocation of Resources in the Concurrent Design of 
New Products. Journal of Engineering Design 8 (1997) 
24. Lee, S.G., Ong, K.L., Khoo, L.P.: Control and Monitoring of Concurrent Design Tasks in a 
Dynamic Environment. Concurrent Engineering: Research and Applications 12, 59–66 
(2004) 
25. Smith, R.P., Eppinger, S.D.: A Predictive Model of Sequential Iteration in Engineering 
Design. Management Science 43, 1104–1120 (1997) 
26. Taylor III, B.W., Moore, L.J.: R&D Project Planning with Q-GERT Network Modeling 
and Simulation. Management Science 26, 44 (1980) 
27. Ko, Y., Kuo, P., Yu, C.: Modelling Concurrent Workflow for New Product Development 
Management. Matrix. 474–479(2010) 
28. Moore, L.J., Taylor III, B.W.: Multiteam, Multiproject Research and Development Plan-
ning with GERT. Management Science 24, 401 (1977) 
29. Elmaghraby, S.E.: Activity Nets: A Guided Tour through some Recent Developments. Eu-
ropean Journal of Operational Research, 383–408 (1995) 
30. Hang, L.: The Application Research on Process Planning Model of Product Development 
Based on D-CPM Method. In: Msie 2011, pp. 460–463 (2011) 
31. Ahmadi, R., Wang, R.H.: Managing Development Risk in Product Design Processes. Op-
erations Research 47, 235–246 (1999) 
32. Mcmanus, H.L.: Product Development Value Stream Mapping (PDVSM) Manual Tech-
nology (2005) 
33. Kusiak, A., Nick Larson, T., Wang, J. (R.): Reengineering of Design and Manufacturing 
Processes. Computers & Industrial Engineering 26, 521–536 (1994) 
34. Kusiak, A., Park, K.: Concurrent Design: Decomposition of Design Activities. In: Pro-
ceedings of the Rensselaer’s 2nd International Conference on Computer Integrated Manu-
facturing, pp. 557–563 (1990) 
35. Cooper, R.G.: Winning at New Products: Accelerating the Process from Idea to Launch. 
Perseus, Cambridge (2001) 
36. Sonnemans, P., Geudens, W., Brombacher, A.: Organizing Product Releases in Time !Dri-
ven Development Processes: A Probabilistic Analysis. IMA Journal of Management Ma-
thematics 14, 337–356 (2003) 
37. Ross, D.T.: Structured Analysis (SA): A Language for - Communicating Ideas. IEEE 
Transactions on Software Engineering SE-3, 16–34 (1977) 
38. Fricke, E., Schulz, A., Wehlitz, P., Negele, H.: A Generic Approach to Implement Infor-
mation-Based System Development. In: Proceeding of the 10th Annual INCOSE Confe-
rence, Minneapolis, USA (2000) 
39. NIST: Integration Definition for Function Modeling, IDEF0 (1993) 
40. Krause, F., Kind, C.: Adaptive Modelling and Simulation of Product Development 
Processes. CIRP Annals-Manufacturing (2004) 
41. Deng, Q.W., Yang, L.P.: Research on Product Development Resource Allocation Model-
ing Based on Hierarchical Colored Petri Net. Applied Mechanics and Materials 44-47, 
138–142 (2010) 
42. Kusiak, A., Zakarian, A.: Reliability Evaluation of Process Models. IEEE Transactions on 
Components, Packaging, and Manufacturing Technology: Part A 19, 268–275 (1996) 
43. Kusiak, A., Wang, J.R., He, D.W., Feng, C.: A Structured Approach for Analysis of De-
sign Processes. IEEE Transactions on Components, Packaging, and Manufacturing Tech-
nology: Part A 18, 664–673 (1995) 
44. Krishnan, V., Eppinger, S.D., Whitney, D.E.: A Model-Based Framework to Overlap 
Product Development Activities. Management Science 43, 437–451 (1997) 

 
Product Development Process Modeling: State of the Art and Classification 
179 
45. Harary, F., Jessop, N., Luckman, J., Stringer, J.: Analysis of Interconnected Decision 
Areas: An Algorithm for Project Development. Nature 205, 118 (1965) 
46. Aitsahlia, F., Johnson, E., Will, P.: Is Concurrent Engineering Always a Sensible Proposi-
tion? IEEE Transactions on Engineering Management 42, 166–170 (1995) 
47. Eppinger, S.D., Nukala, M.V., Whitney, D.E.: Generalised Models of Design Iteration Us-
ing Signal Flow Graphs. Research in Engineering Design 9, 112–123 (1997) 
48. (U.S.), P.M.I. ed.: A Guide to The Project Management Body of Knowledge (PMBOK® 
Guide). Project Management Institute, Inc., Newtown Square, Pa. (2008) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 181–190. 
DOI: 10.1007/978-3-642-30817-8_18     © Springer-Verlag Berlin Heidelberg 2013 
Activity-Based Modeling and Analysis  
of Product Engineering Processes  
Andreas Braun, Björn Ebel, and Albert Albers 
Karlsruhe Institute of Technology (KIT), 
IPEK – Institute of Product Engineering, 
Kaiserstraße 10, 76131 Karlsruhe 
{andreas.braun2,bjoern.ebel,albert.albers}@kit.edu 
Abstract. Product engineering processes are subject to increasing complexity. 
In order to address this problem, various model-based simulation or optimiza-
tion techniques have been published. However, these techniques are limited to 
the structure and quality of the data that they work with. In addition, modeling 
effort needs to be minimized, in order to efficiently support product engineer-
ing. In this contribution an approach for the activity-based analysis of product 
engineering processes is presented. It is intended to support modeling of com-
plex processes with a focus on easy information acquisition. The overall suita-
bility of the approach as well as ways to minimize modeling effort are discussed 
in two case studies. 
Keywords: modeling, product engineering process, information acquisition,  
Integrated Product Engineering Model. 
1 
Introduction 
Product engineering processes can be considered as complex systems [1]. They com-
prise myriad system elements such as goals or objectives, people, activities, resources, 
resulting objects etc. and myriad relations between them. High dynamics lead to a 
continuously changing system structure. Hence, uncertainty is an immanent aspect of 
product engineering (cp. [2]). Successful product engineering, however, demands 
high quality of both product and process. Effectivity and efficiency stand opposite to 
uncertainty and dynamics. Therefore, suitable approaches to manage the complexity 
of product engineering and to handle dynamic system changes flexibly are needed. 
Explicit modeling of information is a necessary condition to establish transparency 
among the relevant aspects. 
The major challenge is keeping an overview of the manifold dependencies between 
the domains product and process, where relevant information flows occur (cp. [3] and 
[4]). The many dependencies are not only caused by the product itself but especially 
by the involved people (designers, managers etc.) and their activities. Hence, a suita-
ble modeling of product engineering processes has to represent especially the people, 

182 
A. Braun, B. Ebel, and A. Albers 
 
their activities and the dependencies of these. With this, purposive process modeling 
aiming at transparency, improved coordination and planning can be realized (cp. [5]).  
In this contribution, an activity-based approach for the analysis of product engi-
neering processes is presented (Section 3). It is grounded on a systemic view on prod-
uct engineering (cp. [6]) and uses elements of the Integrated Product Engineering 
Model iPeM (cp. [1]). With this, it is possible to address product- as well as process-
related aspects from the perspectives of designers and managers. In two case studies, 
the overall modeling concept is applied in real engineering projects (Section 4). The 
exemplary applications are used to discuss the wholeness and the continuousness of 
the approach (cp. [7]), i.e. its suitability to compass any relevant element at necessary 
levels of detail (Section 5). Furthermore, different modeling strategies are reflected on 
with regard to efficiency in information acquisition. These issues are the background 
for the development of sophisticated support tools based on the iPeM in further work. 
In the next section the modeling approach is placed within the state of the art. 
2 
Modeling of Product Engineering Processes 
2.1 
Approaches for Modeling of Product Engineering Processes 
A generic consideration of product engineering is the systemic ZHO-Model. It de-
scribes how socio-technic Operation Systems transform elements of Systems of Ob-
jectives into elements of Systems of Objects [8]. A derivative is the ZOPH-Model of 
Negele that adds a process system to the ZHO-Model [5]. Integrated Product Devel-
opment of Ehrlenspiel considers also operation systems, systems of objectives and 
objects and integrates this consideration into a concept for interdisciplinary projects. 
A methodic conduction of individual process steps is approached with a problem 
solving cycle. It seperates problem containment, search for solutions and selection of 
solutions [9]. Also the Munich Procedure Model can be used as a problem solving 
cycle. It contains also aspects of planning and structuring, analysis and decision [10]. 
Also the iPeM as described in Section 3 is based on a systemic view on product engi-
neering and a generic problem solving process. Based on this, models of concrete 
projects can be represented. 
Literature on modeling of product engineering is multifarious. Wynn and Clarkson 
describe many approaches that aim at establishing transparency in the system of 
product engineering – each from a distinct perspective. The authors distinguish phase- 
and activity-based models, solution- and problem-oriented models, abstract, proce-
dural and analytic models with a focus on either design or management [11]. There is 
a gap between these focuses: well-established models such as the approach of Pahl 
and Beitz are used in design activities, for instance to ensure a methodological proce-
dure. For managerial purposes, especially quality gate processes which separate  
development stages with milestone gates where decision criteria are evaluated are 
widely spread. There is no integrated view that would allow a communication be-
tween these disciplines based on their models (cp. [1]). Consequently, product engi-
neering processes today cannot be improved through one modeling approach alone.  

 
Activity-Based Modeling and Analysis of Product Engineering Processes 
183 
 
In this paper, a modeling approach is presented that is assumed to be comprehen-
sive enough to comprise engineering processes holistically. Given this, analysis tech-
niques are expected to be a suitable concept for process improvement [11]. Such 
techniques have been discussed in literature as well. They are based on formal 
frameworks such as PERT (process evaluation and review technique), CPM (critical-
path method), IDEF (Integration DEFinition), Signposting, Petri nets or DSM (design 
or dependency structure matrix). They allow amongst others calculating or simulating 
process duration or likely process outputs. Furthermore, an analysis of failure modes 
and their impact on cost and scheduling become possible. Strategies of process im-
provement generally aim at a limitation of risk promoters or a re-arrangement of re-
sources, people and activities [12]. However, the authors point out again, that there is 
no modeling framework yet, that would be suitable to compass any relevant aspect.  
Such a holistic consideration can be found in Systems Engineering. It focuses on 
the human being as an actor, but also as a stakeholder or as “the user”. In his 5-Layer-
Model Hitchins considers product-, project- or system-, business-, industry- (e.g. 
Supply Chains) and socio-economic aspects to describe the overall dependencies of 
product engineering continuously [13]. The approach of Haberfellner et al. can be 
divided into an overall philosophy, a problem-solving process and explicit techniques 
of design and project management [14]. However, Systems Engineering today is only 
applied in large projects such as aerospace. A wide industrial application is missing 
(cp. [7]). 
2.2 
Applications of Product Engineering Processes Modeling 
The modeling approaches presented in the section beforehand have been applied in 
several research studies more or less successfully. Smith and Eppinger describe an 
approach to determine the duration of iterative processes. It is based on the coupling 
of “tasks” in DSM networks [15]. The approach uses analysis methods such as di-
rected graphs, PERT, SADT (Structured Analysis and Design Technique) and matric-
es. The latter allow algorithms like partitioning, triangularization or ordering within 
triangularized blocks (“tearing“) [16]. Browning describes DSM application using 
“clustering“ and “sequencing“ for component-based, team-based, activity-based and 
parameter-based models. A more sophisticated simulation environment based on 
DSM is presented by Cho and Eppinger for the management of complex projects. The 
simulation heuristic comprises stochastic, resource-oriented planning problems consi-
dering iteration. It aims at process optimization through determining relevant leverage 
points [17]. 
Approaches such as DSM offer many algorithms for analysis. However, their bene-
fit is limited to the kind and quality of the information in the matrices. It was already 
1999 when Wallace et al. stated that DSM representations are limited in their usability 
by modeling effort and the effort of information acquisition. Consequently, they are 
often reduced to static, very abstract models. In order to reduce the effort of informa-
tion acquisition the authors follow a software-based approach in which the process 
model is built automatically with the help of a service-network during engineering 
activities [18]. Stacey and Eckert describe what designers “can do” and “should do” 

184 
A. Braun, B. Ebel, and A. Albers 
 
as „knowledge level models“ – based on cognitive science. Their approach to model-
ing engineering processes is close to social science [19]. Again, the design activities 
are central elements. 
Summing up, there are many modeling and analysis techniques for product  
engineering processes and also simulation and optimization is being done. Today, 
common approaches address either design or management. Nevertheless, all these 
approaches are limited by the available information basis. For distinct purposes, a 
consideration of engineering processes that is based on activities has been found help-
ful to focus on relevant aspects in many cases. This can also be justified with the help 
of system theory and the central role of activities: they connect various information 
(e.g. goals, people, and results) as pointed out in the following section. 
3 
Activity-Based Modeling of Product Engineering Processes  
3.1 
Integrated Product Engineering Model iPeM 
The Integrated Product Engineering Model iPeM is based on the system triple of 
product engineering. It describes product engineering as a continuous interaction of 
three systems: the System of Objectives, the System of Objects and the Operation 
System. The System of Objectives contains all explicit objectives (i.e. goals and their 
constraints) as well as their interrelations (e.g. conflicts) and justification (Design 
Rationale). It represents expectations towards the future product (cp. [20]). The sys-
tem of Objects contains all synthesized artifacts (virtual and physical). This is the 
final product but also any intermediate object. 
The Operation System is a socio-technic system that contains structured activities, 
methods and processes. Additionally, it contains the involved people and required 
resources. The Operation System analyzes and synthesizes the Systems of Objectives 
and of Objects in an explorative, iterative and co-evolutionary process (cp. [20]). It 
combines generic activities of product engineering with generic activities of problem 
solving (German acronym “SPALTEN” representing the individual steps of problem 
solving [1]). These form an Activities Matrix which allows a well-structured, granular 
consideration of the Operation System (see Figure 1).  
The iPeM makes it possible to model activities, the acting people and used re-
sources in explicit relation to the objectives that they are based on and to the resulting 
objects. Since the sub systems of the system triple can be modeled hierarchically at 
various levels of detail (cp. [21]), a purposive consideration can be achieved as dis-
cussed in the next section. The iPeM is an integrated approach that aims at supporting 
both design and management. While activities and knowledge work can be assisted by 
the Activities Matrix, time-dependencies can be visualized in sub models which 
represent general best practice patterns (Reference Model), individual plans (Imple-
mentation Model) and actual courses of individual projects (Application Model – cp. 
right side of Figure 1). These allow set-actual-comparisons that can be used to derive 
management ratios. 
 

 
Activity-Based M
 
Fig. 1. In
The iPeM approach appe
analysis of engineering pr
managers. 
3.2 
Activity-Based Mod
Useful modeling of produ
modeling purpose (cp. [5]).
process (descriptively) or 
consideration is inevitable s
must not spare relevant inf
modeled information allow
system triple that the iPeM 
ested in a representation of 
plan. Here, one would focu
tones etc. Another purpose 
a System of Objectives in o
Information acquisition n
sulting model has to be a r
activities, resources) and th
lows further analysis and/
ensures that a reduced amo
abstraction. In this paper it
Operation System is a suita
ing with the iPeM is that i
overall management. The cr
ity gate process are gener
needs an overall picture, th
level. In the next sections i
Modeling and Analysis of Product Engineering Processes 
ntegrated Product Engineering Model iPeM 
ears to be suitable for holistic and continuous modeling 
rocesses with the aim of supporting both designers 
deling of Product Engineering Processes with the iPe
uct engineering processes requires a specification of 
. This can be analysis for mere or better understanding o
the intent to optimize a process (prescriptively). Suc
since models are always a reduction of reality. For this, 
formation (cp. [7]). At the same time, a reduction of 
ws to reduce modeling effort to an acceptable amount. T
is based on can help focusing a model: one could be in
an actual (real) course of a process in contrast to an ini
s on activities and their duration, decision criteria at mi
could be the representation of the continuous evolution
order to learn for a new product generation. 
needs to be based on the specified modeling purpose. A
representation of all relevant elements (objectives, obje
heir interrelations. It has to be structured in a way that
/or improvement. This includes hierarchic modeling t
ount of information can be focused on at higher levels
t is shown that the iPeM and the focus on activities in
able way to achieve this. A benefit of the integrated mod
it allows bridging the gap between basic design work 
riteria that are needed for decisions at milestones in a qu
rated during design activities. Even though managem
he required data originates from activities at the operat
t is shown, that the iPeM approach is capacious enough
185 
 
and 
and  
eM 
the 
of a 
ch a 
one 
the 
The 
nter-
itial 
iles-
n of 
A re-
ects, 
t al-
that 
s of 
n its 
del-
and 
ual-
ment 
tive 
h to 

186 
A. Braun, B. Ebel, and A. Albers 
 
comprise all relevant aspects on the one hand, and allows analyzing process models 
continuously over distinct levels of detail on the other hand. The fractal iPeM struc-
ture ensures consistent model use. With this, it is possible to observe processes and 
criteria such as maturity levels during actual operation and to condense the informa-
tion to overall models that can be used for managerial decisions. 
4 
Analysis of Product Engineering Processes 
In this section, two different studies are presented. They show the suitability of the 
iPeM modeling approach and illustrate strengths and weaknesses of the respective 
applied modeling strategies. 
4.1 
Case Study 1: Manual Modeling of a Student Project with Interviews 
The academic course integrated product development (IP) of the Karlsruhe Education 
Model for Product Development (KaLeP) [22] has been chosen as a use case for the 
test. Even though IP is a student project, it can be considered to be close to reality. 
The students are all in their final year and thus almost fully educated. Boundary con-
ditions such as working environment, working hours etc. are deliberately close to 
reality as well.  It is a four-month PE process with an industrial partner that includes 
all stages and challenges of a (totally) new design – all the way from the definition of 
the market niche to the production of functional prototypes – as well as project man-
agement (time, real budget etc.). The project’s initial task description is very vague; 
hence uncertainty is particularly high in IP. However, the entire process is well ob-
servable as the supervisors have access to all intermediate files, sketches, documents, 
project plans etc. 
Information Acquisition. Information about the engineering process of one team was 
gained through weekly open interviews. The interviewee was the team leader; the 
researcher took notes. The information was structured with the help of the iPeM  
afterwards by the researcher. It was modeled manually as a network where the sub 
systems of the iPeM (of Objectives, of Operation and of Objects) served as clusters in 
which the respective information was modeled hierarchically. Connections between 
these ‘model elements’ were indicated by arrows where e.g. objectives and resources 
were input to an activity, or where an object resulted. 
Analysis of the Process. Even though modeling effort was considerable (200 hours of 
interviews and model creation for 800 interlinked ‘model elements’), the explicit 
representation of information is remarkable. All aspects that were mentioned in the 
interviews could be modeled at comprehensive levels of detail. Hence, the iPeM can 
be considered to be suitable for representing the deep fractal character of product 
engineering processes (cp. [21]). The fact that any information that occurred could be 
modeled explicitly indicates the wholeness of the approach. However, with the aim of 
efficiency, a suitable level of reduction has to be defined to focus on relevant (i.e. 

 
Activity-Based Modeling and Analysis of Product Engineering Processes 
187 
 
purposive) information. Limitations of manual modeling in terms of ergonomics, for 
instance when retrieving information again from the model, can be addressed with 
computer supported approaches as discussed in the following. 
4.2 
Case Study 2: Model Creation with a Collaborative Work Environment 
Information Acquisition. One lesson learned from case study 1 is that reducing 
modeling effort is inevitable. Hence, the iPeM approach has been used to model 
another student project differently in the following year: by the students themselves. 
For this, a MS SharePoint environment was configured according to the iPeM’s sub 
systems. The students were then advised to use this platform to do their project plan-
ning. They were asked to plan their team activities in detail and with the respective 
underlying objectives and resources. 
Analysis of the Process. The modeling resulted in seven explicit models. The student 
teams modeled an average of 130 activities which belonged to average of 82 objec-
tives. Figure 2 shows models of two teams. The layout of the models represents the 
iPeM (cp. Figure 1). Note that connections between the elements are in the SharePoint 
database but not depicted in the figure. Gray shades indicate the number of modeled 
elements of objectives, activities and resources (larger numbers are indicated by dark-
er background color). The right in each model shows the activities on a time bar. One 
can see that the students planned their processes differently – even though they were 
given the same task (cp. uniqueness of engineering processes in [21]). Also obvious: a 
process does not proceed sequentially, but activities overlap and may reoccur  
iteratively. 
 
Fig. 2. Comparison of two resulting models 
Another interesting observation can be made when the seven models are summed 
up. Figure 3 shows a ‘common’ model of the seven teams. The time bar here resem-
bles the process course that the students were taught. This taught model is a reference 
process that obviously was used as a pattern for planning. One can conclude that the 
iPeM approach is suitable to compass different model levels continuously – from 
reference patterns to individual plans and back to a consolidated model. 

188 
A. Braun, B. Ebel, an
 
Fi
5 
Discussion and O
5.1 
Wholeness and Con
The case studies imply tha
product engineering that a
students when they planned
tion for purposeful modeli
initially. In order to result i
the information that has to b
The continuousness of th
first study confirmed that c
possible with the granular u
The second study revealed
representation is possible. H
zation is given. This make
pattern as shown in the stu
compared with the referenc
Another important insig
comparisons between diffe
issues such as the focus tha
hand it is still generalized e
ences – for instance with th
5.2 
Applicability – Pur
In a general perspective, a
two major purposes: metho
terms of planning and cont
proach is suitable to bridg
nd A. Albers 
ig. 3. Consolidated model of all teams 
Outlook 
ntinuousness of the Modeling Approach 
at the iPeM approach is suitable to compass any aspec
appeared to be relevant during the interviews and for 
d their processes. Hence, wholeness as a necessary con
ing and analysis of product engineering can be assum
n an efficient modeling approach, a purposive reduction
be represented in models has to be defined (Section 5.3)
he approach is also substantiated by the case studies. T
comprehensive modeling at user-defined levels of detai
understanding of activities in the iPeM approach (cp. [2
d that also aggregating models to a common, consolida
Hence, continuousness both in detail as well as in gener
es it possible to adjust a project plan to a given refere
udy. The consolidation of individual project plans can
e in turn. 
ght is that the activity-based modeling approach allo
erent projects. It is detailed enough to comprise individ
at the individual teams laid in their projects. On the ot
enough to allow quick comparisons and recognizing dif
he help of simple shading as illustrated in Figure 2. 
rposive Use vs. Efficiency 
application of process models in industrial practice ser
odological support of design activities or managemen
trolling. The second case study implies that the iPeM 
e this gap. It allows compassing activities at the level
 
t of 
the 
ndi-
med 
n of 
). 
The 
il is 
23]). 
ated 
rali-
ence 
n be 
ows 
dual 
ther 
ffer-
rves 
t in 
ap-
l of 

 
Activity-Based Modeling and Analysis of Product Engineering Processes 
189 
 
actual operation. Here, in daily engineering work, designers can be supported e.g. 
with explicit objectives. Cumulating these to coherent phases on a time bar, on the 
other hand, allows managers to control projects in terms of stages and quality gates.  
As introduced in Section 2 there are various analysis and optimization techniques 
to improve product engineering processes. They have in common that they depend on 
the available information basis. The case studies presented above point out in two 
directions. On the one hand, a modeling approach needs to be comprehensive enough 
to compass relevant aspects of engineering processes entirely. On the other hand it is 
no solution to model “anything” since modeling effort is proportional to the model 
element’s count. A purposive reduction of the overall information to the core of ne-
cessary elements is the key. The positive experiences of the two studies presented 
beforehand motivate the application of the activity-based iPeM approach for both 
information acquisition and model creation. Computer supported approaches further-
more ease model use e.g. with searching or filtering functionalities. 
5.3 
Outlook 
Further work consequently addresses a classification of iPeM elements that are to be 
modeled for distinct purposes. This includes checklists of information elements as 
well as standardized interfaces to analysis tools. 
An important finding of case study 2 is the applicability of collaborative work en-
vironments such as MS SharePoint for explicit model creation. The configuration of 
the user interface according to the iPeM approach made it possible to integrate the 
modeling tool in daily workflows (here: for planning purposes). The model was 
created, updated and checked for internal consistency “on the fly”. This is a promising 
perspective for efficient application in industry but also for research. As showed in 
the study, it is possible to gain a comprehensive database directly without biasing 
through interpretation of the interviewer etc. Future efforts will follow this direction. 
However, also improvements of the usability of the work environment are to be 
addressed in order to result in a modeling platform for productive use in industry. In 
future work, not only planning in advance, but also tracking of actual courses of 
projects needs to be integrated. Automation (e.g. with the help of time trackers) can 
help to reduce effort through easy modeling of the IS-process in addition to the SET-
process. This would allow a set-actual-comparison and be the basis for advanced 
process optimization at the level of individual activities – even during running 
projects. 
References 
1. Albers, A., Braun, A.: A Generalised Framework to Compass and to Support Complex 
Product Engineering Processes. J. Prod. Dev. 15(1/2/3), 6–25 (2011) 
2. Suh, N.P.: A Theory of Complexity, Periodicity and the Design Axioms. Res. Eng. De-
sign 11, 116–131 (1999) 

190 
A. Braun, B. Ebel, and A. Albers 
 
3. Earl, C., Johnson, J., Eckert, C.: Complexity. In: Clarkson, P.J., Eckert, C. (eds.) Design 
Process Improvement – A Review of Current Practice, pp. 174–197. Springer, London 
(2005) 
4. Browning, T.R., Fricke, E., Negele, H.: Key Concepts in Modelling Product Development 
Processes. Syst. Eng. 9(2), 104–128 (2006) 
5. Negele, H., Fricke, E., Schrepfer, L., Härtlein, N.: Modelling of Integrated Product Devel-
opment Processes. In: 9th Annual Symposium of INCOSE, UK (1999) 
6. Albers, A., Lohmeyer, Q., Ebel, B.: Dimensions of Objectives in Interdisciplinary Product 
Development Projects. In: International Conference on Engineering Design, ICED 2011, 
Copenhagen, vol. 2, pp. 256–265 (2011) 
7. Albers, A., Lohmeyer, Q.: Advanced Systems Engineering - Towards a Model-Based and 
Human-Centered Methodology. In: International Symposium Series on Tools and Methods 
of Competitive Engineering, TMCE 2012, Karlsruhe, pp. 407–416 (2012) 
8. Ropohl, G.: Einleitung in die Systemtechnik. In: Ropohl, G. (ed.) Systemtechnik – Grun-
dlagen und Antworten. Carl Hanser Verlag, München (1975) 
9. Ehrlenspiel, K.: Integrierte Produktentwicklung – Denkabläufe, Methodeneinsatz, Zusam-
menarbeit, vol. 4. Carl Hanser Verlag, München (2009) 
10. Lindemann, U.: Methodische Entwicklung technischer Produkte – Methoden flexibel und 
situationsgerecht anwenden. Springer, Heidelberg (2007) 
11. Wynn, D., Clarkson, P.J.: Models of Designing. In: Clarkson, P.J., Eckert, C. (eds.) Design 
Process Improvement – A Review of Current Practice, pp. 34–59. Springer, London 
(2005) 
12. O’Donovan, B., Eckert, C., Clarkson, P.J., Browning, T.R.: Design planning and model-
ing. In: Clarkson, J., Eckert, C. (eds.) Design Process Improvement – A Review of Current 
Practice, pp. 60–87. Springer, London (2005) 
13. Hitchins, D.K.: Systems Engineering – A 21st Century Systems Methodology. John Wiley 
& Sons, West Sussex (2007) 
14. Haberfellner, R., deWeck, O., Fricke, E., Vössner, S.: Systems Engineering – Grundlagen 
und Anwendung. Orell Füssli Verlag, Zürich (2012)  
15. Smith, R.P., Eppinger, S.D.: A Predictive Model of Sequential Iteration in Engineering 
Design. Man. Sci. 43(8), 1104–1120 (1997) 
16. Gebala, D.A., Eppinger, S.D.: Methods for Analyzing Design Procedures. ASME Des. 
Theory & Meth. 31 (1991) 
17. Cho, S.-H., Eppinger, S.D.: A Simulation-Based Process Model for Managing Complex 
Design Projects. IEEE Transactions on Engineering Management 52, 3 (2005) 
18. Wallace, D., Abrahamson, S., Borland, N.: Design Process Elicitation through the Evalua-
tion of Integrated Model Structures. In: Proceedings of DETC 1999. ASME, Las Vegas 
(1999) 
19. Stacey, M., Eckert, C.: An Ethnographic Methodology for Design Process Analysis. In: In-
ternational Conference on Engineering Design, ICED 1999, Munich (1999) 
20. Albers, A., Ebel, B., Lohmeyer, Q.: Systems of Objectives in Complex Product Develop-
ment. In: International Symposium Series on Tools and Methods of Competitive Engineer-
ing, TMCE 2012, Karlsruhe, pp. 267–278 (2012) 
21. Albers, A., Braun, A., Muschik, S.: Uniqueness and the Multiple Fractal Character of 
Product Engineering Processes. In: 1st International Conference on Modelling and Man-
agement of Engineering Processes. Springer, London (2010) 
22. Albers, A., Burkardt, N., Deigendesch, T., Meboldt, M.: Enabling Key Competencies by 
Educational Project Work Exemplified by Teamwork and Cooperation. In: International 
Conference on Engineering and Product Design Education, EPDE 2008, Barcelona (2008) 
23. Albers, A., Muschik, S., Braun, A.: Ein Beitrag zum Verständnis des Aktivitätsbegriffs im 
System der Produktentstehung. In: Tag des Systems Engineering, München (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 191–200. 
DOI: 10.1007/978-3-642-30817-8_19 
© Springer-Verlag Berlin Heidelberg 2013 
 
An Enhanced Interface Analysis Method  
for Engineering Change Management 
Unal Yildirim and Felician Campean 
University of Bradford, EDT3 Ford Suite, BD7 1DP, UK 
u.yildirim@bradford.ac.uk 
Abstract. The complexity of automotive systems has increased dramatically, 
driven by the requirement to address environmental and safety concerns and the 
pressure to offer higher level consumer technologies. This places a great chal-
lenge on product development organizations to manage the multidisciplinary 
systems integration in a reliable and robust manner. Engineering changes, 
which are integral part of the iterative automotive product development process, 
need to be managed in a way that efficiently addresses the integration require-
ments of complex multidisciplinary systems. The aim of this paper is to present 
a structured approach for engineering change management which is based on an 
enhanced interface analysis method which aims to identify comprehensively the 
system integration functional requirements as the basis for both engineering 
change prediction and support of robust engineering change design. The 
framework will be illustrated with an industrial case study on the development 
of an electric vehicle powertrain. The effectiveness of the proposed approach 
will be discussed in contrast with other methods for engineering change man-
agement. 
Keywords: Interface Matrix, Interface Analysis Table, ECM. 
1 
The Business Case for Engineering Change Management 
The engineering challenge within automotive systems engineering design has been 
increasing rapidly over the past couple of decades, largely due to the accelerated pace 
of introduction of new technologies to address environmental concerns and the drive 
to enhance customer satisfaction. In spite of the development and use of enhanced 
CAE and virtual engineering tools, the efficiency of automotive product development 
(PD) process has not increased as expected. This is clearly illustrated by the pattern 
and cost of engineering changes; for example the research reported by Cash [1] 
showed that on a typical new vehicle development program there were on average 5 
engineering changes per part. These results are confirmed by a larger survey reported 
by Wasmer [2], who showed that across the supply chains for 3 major OEMs there 
were over 350,000 engineering changes per year, with an estimated cost of $50k per 
engineering change. While the iterative nature of the automotive systems design pro-
vides some justification for a certain level of engineering change within product  

192 
U. Yildirim and F. Campean 
design and development, research [1] showed that the requirement for engineering 
changes is largely due to issues discovered late in the design process, which require 
countermeasure development. This also explains the spiraling costs of engineering 
changes, as fixing failure modes late in the design process is difficult and has knock-
on effects to other subsystems. 
There is also recognition that the technical difficulty of automotive systems engi-
neering design is related to the complexity and the multi-disciplinary nature (mechan-
ical, electronic, software, control) of modern automotive systems. The role of systems 
integration in system engineering design is demonstrated by evidence [3] that the 
overwhelming majority of failures in the field are due to system interactions not being 
adequately managed during the design, leading to robustness failures. A structured 
approach using a coherent set of supporting tools is required to address effectively the 
complexity of the multi-disciplinary integration during system engineering design.  
Research in engineering change management for complex systems [4-7] has fo-
cused extensively on predicting the effect and the impact of engineering changes. 
Most ECM approaches are underpinned by the Design Structure Matrix (DSM) [8-9] 
which is a concise, matrix based, representation of the system architecture, focused on 
identifying links between components or subsystems within the scope of the overall 
system. Change prediction approaches are substantially focused on identifying propa-
gation paths for the effect of the engineering change based on the linkages identified 
in the DSM [6,7], and the associated risks assessment of the likelihood and impact of 
the effect of the change [4,5]. A criticism that can be brought to many of these ap-
proaches is that they are strongly focused on predicting the impact of an engineering 
change, and less focused on supporting the engineering design process that underpins 
the development of robust design countermeasures.  
The Contact and Channel Model (CC) proposes a structured methodology for sys-
tem decomposition, which could be utilized to support engineering change analysis 
and management. The method is based on analysis of a system through representing 
its functional and physical elements simultaneously. This is achieved via describing 
Working Surface Pairs (WSP) and Channel and Support Structures (CSS) to connect 
the WSPs of the system [12]. However, the method has not been implemented on a 
complex multidisciplinary system [13]. The approach to engineering change man-
agement presented in this paper is underpinned by a Failure Mode Avoidance (FMA) 
framework [10-11], which has been developed and adopted by the automotive indus-
try as a way of enhancing the effectiveness of product development by “getting it 
right first time”. The FMA framework aims to avoid late engineering changes by pro-
moting early identification of potential failure modes and the development of robust 
countermeasures. The FMA framework is based on coherent information flow between 
a series of structured tools that support the systems engineering design integration. The 
FMA process places a strong emphasis on tools that support function analysis and de-
composition, and promotes the use of an enhanced interface analysis to identify func-
tional requirements for managing interfaces and ensuring systems integration. 
This aim of this paper is to discuss the use of this enhanced interface analysis me-
thod to support engineering change analysis and management. The approach will be 
illustrated with an industry based case study of an electric vehicle powertrain for a 
small truck. 

 
An Enhanced Interface Analysis Method for Engineering Change Management 
193 
2 
Interface Analysis Method 
Within a complex and multi-disciplinary system engineering design framework, man-
agement of system integration on a functional basis has the utmost importance in 
ensuring that the system delivers the customer required functions robustly and relia-
bly. The functional analysis of the system must provide a coherent structure for the 
vertical integration of the system with its subsystems and components, as well as being 
comprehensive in identifying all functions required at a certain level to manage inter-
faces between systems and subsystems. Thus, the functional architecture of the system 
must be fully be detailed to achieve the system integration through documenting:  
(i) 
the main functions of the system, associated with the main flows through the 
system;  
(ii) 
the integration of functional requirements for components to work within the 
system, i.e. to manage system interfaces. 
Pimmler and Eppinger [15] discussed the principles of system integration analysis 
through interface analysis, providing the basis for a tool (Interface Matrix, IM) com-
monly used in the automotive industry to support the engineering systems design 
analysis, in particular focused on the development of robust design verification plans 
[3]. While the IM is structurally similar to DSM (i.e. relies on a contingency matrix 
between system design elements or subsystems), its aim is to systematically identify 
all the exchanges at interfaces which could affect the robust operation of the system. 
The IM proposes that the interface exchanges between components are characterized 
based on the general framework of Energy (E), Material (M) and Signal or Informa-
tion (I) [14]. In addition, Pimmler and Eppinger [15] have introduced the Spatial (S) 
type interaction to describe requirements for adjacency or orientation between two 
interfacing components; in the industrial application of the interface matrix [3], the 
spatial interface exchanges are commonly referred to as “Physical” (P) interface. 
The interface analysis framework has been considerably enhanced and extended 
[16] through the introduction of an Interface Analysis Table (IAT) to document func-
tional requirements associated with interface exchanges. This supports the integration 
of the interface analysis with the systems engineering design analysis, by ensuring 
that all functional requirements (i.e. both main functions and interface management 
functions) are cascaded through the systems engineering process [17]. 
To illustrate this enhanced interface analysis approach we consider a case study on 
the system level design analysis of an electric vehicle powertrain for a small truck, 
discussed in [16]. The main functions of the electric vehicle powertrain EVP were 
defined as (i) to provide controlled torque at the rear axle, (ii) to provide power for 
low voltage (LV) vehicle consumers and (iii) to charge and store electrical energy. 
Figure 1 summarizes the structure of the system in terms of a Boundary Diagram 
[16] constructed based on a structured functional decomposition of the EVP main 
functions using its System State Flow Diagram and Function Tree [16]. A Boundary 
Diagram identifies the inputs and outputs of the system, the boundaries of the system 
(thus defining the scope of responsibility for the system design team), the subsystems 
and components that achieve the system function (shown inside the system  

194 
U. Yildirim and F. Campean 
boundary), and the external systems that interface with the system. The arrows depict 
the flow (of Energy, Materials and Information) through the system, from the inputs 
(Driver Demand and Mains Energy) through to the outputs (Controlled torque at rear 
axle and LV/DC for vehicle consumer). The relationships between the system and 
neighboring systems are shown on the boundary diagram as a double-headed arrow 
(denoting that exchanges can take place in both directions). 
 
Fig. 1. System Boundary Diagram for Electric Powertrain  
The purpose of interface analysis is to systematically identify and characterize all 
possible interfaces between subsystems, both within the system boundaries and with 
systems outside the boundaries. We define an interface between two subsystems in 
terms of the relationship between them, which, consistent with [15] and current auto-
motive practice [3], can be defined as an exchange of material (M), energy (E) or 
information (I), or a physical (P) relationship relating to the spatial adjacency and 
orientation (e.g. packaging requirements). There could be multiple exchanges at an 
interface, and it is important that these are identified and characterized, because they 
can affect the overall performance of the system, either as “noise factors” or as “di-
verted output” [10-11]. Figure 2 illustrates an Interface Matrix (IM) which is a tool 
commonly used to document interface analysis. 
In the IM, each cell documents the existence of an interface between the two sub-
systems on the corresponding row / column, with a summary description of the type 
of exchange (P, E, I or M). For example, at the interface between the “Battery Charg-
er” and the “Battery Pack” there is an energy exchange (electric power from charger 
to the battery) and also an information exchange relating to the battery condition 
(state of charge and temperature) needed to the charger in order to control the  
charging rate. 
 

 
An Enhanced Interface Analysis Method for Engineering Change Management 
195 
 
Fig. 2. Interface Matrix for Electric Vehicle Powertrain 
While the IM provides a compact analysis of exchanges at interfaces, both internal 
and external, it does not capture their details, normally discussed by the engineering 
team while analyzing a particular interface. This is particularly important as there 
could be multiple exchanges of the same type at an interface. More significantly, if an 
exchange (i.e. a flow of E, M or I) is identified at an interface, then a functional re-
quirement must be specified to manage this exchange. An Interface Analysis Table 
(IAT) has been suggested [17] as an enhancement to the IM. The IAT, illustrated in 
Figure 3, shows two interfaces – one internal (Charger – Battery Pack) and one exter-
nal (Battery Pack – Chassis Frame). The IAT includes a description of the exchange, 
the target/unit of the exchange, a statement of the engineering function required to 
manage the exchange and an evaluation of the effect of the interface exchange on the 
main (“high level”) function and related high level function is also documented on the 
table. Coherent with [15] the rating of the effect on the main function uses a numeric 
scale from -2 to +2, the “-“sign indicating that the effect is detrimental to the main 
function and the exchange must be prevented to achieve the function, whereas the “+” 
sign indicates that the exchange must be provided to support a main function of the 
system.  
3 
ECM Based on Interface Analysis Method 
Interface analysis, supported and documented by the Interface Matrix and the Inter-
face Analysis Table, provides a structured framework for both systems engineering 
design integration (of a new system) and engineering change management. Focusing 
on engineering change management (ECM), the interface analysis supports and 
guides the ECM at three levels: 
1. Identify the system impact of an engineering change: The Interface Matrix  
provides an effective way of identifying the impact of an engineering change by 

196 
U. Yildirim and F. Campean 
showing the interfaces between the component or subsystem that will incur the 
change and other subsystems both within and outside the system boundary. The in-
terface exchanges are characterized in terms of P, E, I and M, with a description of 
the exchange being given in the Interface Analysis Table. 
 
Fig. 3. Interface Analysis Table for EVP 
 
2. Evaluate the impact and difficulty of an engineering change: In order to support 
engineering change decisions it is useful to be able to evaluate or quantify the im-
pact and difficulty of an engineering change to a subsystem. The information 
available in the IM and IAT provides a powerful way of evaluating the impact of 
the engineering change by looking, for example, at the number of interfaces (ex-
tracted from the IM), the complexity of the interface exchanges (extracted from the 
IM in terms of the P, E, I, M types of exchanges), the number of functions that 
need to be provided for the system integration (extracted from the IAT), and the 
importance of the interface functions evaluated from the effect of the interface  
exchange, also documented in the IAT. In this way, the interface analysis method 
also provides an objective justification of the propagation path of an engineering 
change. 
3. Support the implementation of an engineering change: The Interface Analysis 
Table specifies the functions that need to be engineered in through systems engi-
neering design in order to manage interface exchanges. We can therefore extract all 
the functions that need to be engineered in the subsystems that are subject to 
change, and ensure that these functions are delivered by the new design. In other 
words, the system design requirements for the system that is the subject of the  
engineering change should be cascaded from the IAT in order to ensure a flawless 
integration of the modified design with the system. 
We will illustrate these concepts on the basis of the electric vehicle powertrain exam-
ple. Assume that the potential launch of the vehicle for a new market use requires a 
slightly enhanced powertrain performance in terms of an increased range. An engi-
neering solution proposed is based on enhancing the storage capacity of the battery 

 
An Enhanced Interface Analysis Method for Engineering Change Management 
197 
pack. The engineering question associated with this proposed design change relates to 
its likely impact to the system, in terms of both design requirements and failure mod-
es. We will answer this question on the basis of the EVP interface analysis summa-
rized in the IM (shown in Figure 2) and the IAT (shown in Figure 3). 
Figure 4 below illustrates (as an extract from the IM shown in Figure 2) the inter-
faces of the Battery Pack as a subsystem, with other subsystems within the EVP and 
external systems. This shows that the battery pack has interfaces within the EVP sys-
tem with the Battery Charger, the Motor Control Unit and the DC-DC Converter. The 
Battery pack also has interfaces with the driver, the environment and the chassis 
frame. The IM gives an indication of the nature of these exchanges (P, E, I, M), with 
the full description given in the IAT (battery pack-charger and battery pack-chassis 
frame interfaces are illustrated in Figure 3 as an example). All interface exchanges 
need to be taken into account in the engineering design of the system with the new 
battery pack.  
 
Fig. 4. Battery Pack Interfaces within the EVP Analysis 
It is convenient to carry out the systems engineering design analysis on the basis of 
functional requirements; the IAT contains a comprehensive list of interface functional 
requirements which need to be engineered in the system, which provide a focus for 
the engineering change analysis. Each interface function needs to be considered in 
turn to evaluate the impact of the proposed engineering change.  
For example, the IM extract in Figure 4 shows that at the Battery Pack - Charger 
interface there are both energy and information exchanges. The IAT (Figure 3) docu-
ments further detail of these exchanges as “transmit electrical power from charger to 
battery”, and “transmit battery state of charge and temperature information to charg-
er”. These interface function requirements are cascaded to both the Battery Pack and 
to the Charger, and the impact of the engineering change must be evaluated in terms 
of the internal complexity (i.e. within the Battery Pack) and transmitted complexity to 
the interfacing subsystems / components. For example, an internal requirement cas-
caded to the Battery Pack is to “transmit battery temperature information to the 
Charger”, which is achieved by a temperature sensing system (which includes a tem-
perature sensor and the associated electronic circuit and harness connectors). Thus, 
the engineering team need to evaluate (i) internal complexity – i.e. whether the tem-
perature sensing system can be carried over or a new system is required following the 
change of the battery capacity; and (ii) transmitted complexity – whether there is a 
requirement to have an engineering design change for the charger in order to fulfill 
this interface function with the new battery pack.  

198 
U. Yildirim and F. Campean 
The external interface requirements are addressed in a similar way. The IM extract 
in Figure 4 shows that there are physical contact and material and energy exchanges 
at the Battery – Chassis frame interface. The IAT in Figure 3 shows the interface 
functions which the new battery pack must satisfy. These interface requirement func-
tions are cascaded to both the Battery Pack and to the Chassis frame and the impact of 
the change must be evaluated in terms of internal complexity and transmitted com-
plexity. For example, the “locate Battery Pack onto Chassis” interface function re-
quires considering whether a new component should be used or an engineering design 
change is needed for the chassis to meet this interface function requirement. 
In terms of evaluating the impact of the engineering change, the interface table 
(Figure 3) shows that the Battery pack has numerous exchanges with 6 subsystems 
out of 12 subsystems of the EVP. Also, the examination of the full IAT shows that the 
battery pack has 17 interface function requirements that must be managed to ensure 
the system integration. The IAT also details descriptions of these exchanges and all 
this information along with the IM gives an objective view regarding the impact and 
propagation of the battery change on the main system and into other subsystems. 
4 
Discussion and Conclusions 
The aim of this paper was to present a structured approach for engineering change 
management based on an enhanced interface analysis method. The key features of this 
interface analysis method are that:  
(i) It provides a comprehensive analysis of exchanges (expressed in terms of P, E, I, 
M) between subsystems both within the system boundary and external;  
(ii) It provides a description of the interface exchange and a function requirement 
specification in order to manage the exchange at the interface to ensure the sys-
tem integration. 
This information is documented in the IAT, which provides a comprehensive basis for 
evaluating the impact of an engineering change in terms of the number of interfaces 
that need to be managed and the number of functions required, and also supports the 
implementation of the engineering change by providing a list of function requirements 
for system integration that must be satisfied.  
This has been illustrated through the EVP case study discussion, based on an engi-
neering change at the powertrain system level (upgrade to the battery pack). The in-
terface analysis is normally cascaded through the systems engineering decomposition, 
so the approach will be applicable at all levels in the system hierarchy. For example, 
if an engineering change is required for the battery temperature sensor (e.g. due to the 
lack of robustness in the field of the current design), then the component level inter-
face analysis, developed on the basis of a cascade of interface exchanges and func-
tional requirements from the higher level systems, will guide the engineering change 
analysis by specifying the functional requirements for the new sensor. 
The interface analysis method described in this paper is strongly integrated with 
other tools supporting the FMA framework. In particular, as discussed in [10] the 

 
An Enhanced Interface Analysis Method for Engineering Change Management 
199 
interface analysis underpins the development of the DFMEA, which provides a struc-
tured approach to the evaluation of the criticality of function failure modes with their 
potential countermeasures. This enhances significantly the engineering change analy-
sis by showing not only the functions that need to be provided to ensure the system 
integration, but also the potential critical function failure modes and their counter-
measures. This provides a better platform of support for the engineering change deci-
sion – by quantifying the impact of the engineering change in terms of both interface 
function requirements and function failure modes. 
This is a significantly different approach compared to the linkage modeling 
(LMM) and change prediction (CPM) methods which suggest managing changes by 
evaluating subjectively the likelihood (the average probability that a change in a sub-
system may propagate into other systems) and impact (the average proportion of the 
design rework required to be done if the change propagates) of component changes 
[4]. Another disadvantage is that these methods seem to be focused exclusively on 
internal interfaces, and do not appear to formally capture and evaluate interfaces with 
external factors. It can be argued that the interface analysis requires extensive re-
source investment for a complete analysis, which appears to provide some justifica-
tion for a subjective evaluation of the impact of an engineering change as a more 
pragmatic approach. However, the authors’ extensive experience with automotive 
systems engineering design shows that interface analysis is treated as the backbone of 
the systems engineering cascade, and part of the normal engineering design process. 
Further work will concentrate on the development of a more substantive approach 
to engineering change management based on the enhanced interface analysis method, 
to include a comprehensive risk management framework for engineering changes 
based on critical parameter management through failure mode and effect analysis 
(FMEA) and robust design verification. This framework will be illustrated through 
industrial case studies developed in partnership with industrial sponsors and collabo-
rators to the University of Bradford Engineering Quality Improvement Centre 
(BEQIC). 
References 
1. Cash, P.A.: Right first time production releases. MSc thesis, University of Bradford (2003) 
2. Wasmer, A., et al.: An industry approach to shared, cross-organizational engineering 
change handling—The road towards standards for product data processing. Computer-
Aided Design (2010), doi: 10.1016/j.cad.2010.10.002 
3. Webb, R.D.: Investigation into the application of robustness and reliability tools to the de-
sign process. MSc thesis, University of Bradford (2002) 
4. Clarkson, P.J., Simons, C., Eckert, C.: Predicting change propagation in complex design. 
In: ASME 2001 Design Engineering Technical Conferences and Computers and Informa-
tion in Engineering Conference, Proceedings of DETC 2001, Pittsburgh, September 9-12 
(2001) 
5. Clarkson, J., Simons, C., Eckert, C.: Change prediction for product redesign. In: Interna-
tional Conference on Design-ICED 2001, Conference Proceedings, Glasgow, August 21-
23 (2001) 

200 
U. Yildirim and F. Campean 
6. Jarratt, T., Eckert, C., Clarkson, P.J.: Development of a product model to support engineer-
ing change management. In: Proceedings of the TCME 2004, Lausanne, April 12-16 
(2004)  
7. Jarratt, T.A.W.: A model-based approach to support the management of engineering 
change. PhD thesis, University of Cambridge (2004) 
8. Browning, T.R.: Applying the design structure matrix to system decomposition and inte-
gration problems: A review and new directions. IEEE Transactions on Engineering Man-
agement, Conference Proceedings, Processing 48(3), 292–306 (2001) 
9. Danilovic, M., Browning, T.R.: Managing complex product development projects with de-
sign structure matrices and domain mapping matrices. International Journal of Manage-
ment 25, 300–314 (2007) 
10. Henshall, E., Campean, F.: Implementing failure mode avoidance. Society of Automotive 
Engineers Technical paper: 2009-01-0990 (2009) 
11. Campean, I.F., Henshall, E., Brunson, D.: Failure mode avoidance paradigm in automotive 
engineering design. In: International Congress on Automotive and Transport Engineering – 
CONAT, Conference Proceedings, Brasov, October 25-27, pp. 207–214 (2010)  
12. Albers, A., Braun, A., Clarkson, P.J., Enkler, H.-J., Wynn, D.: Contact and channel model-
ling to support early design of technical systems. In: International Conference on Engi-
neering Design – ICED 2009, Stanford, August 24-27 (2009) 
13. Albers, A., Oerding, J., Alink, T.: Abstract objectives can become more tangible with the 
contact and channel model (C&CM). In: Models and Methods for Variation Management 
in Global Product Development: Proceedings of the 20th CIRP Design Conference, Ecole 
Centrale de Nantes, Nantes, pp. 203–213. Springer, Berlin (2011) 
14. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.H.: Engineering Design: A systematic ap-
proach, 3rd edn. Springer, London (2007) 
15. Pimmler, T.U., Eppinger, S.D.: Integration analysis of product decompositions. In: ASME 
Design Theory and Methodology Conference, Minneapolis (September 1994) 
16. Campean, I.F., Henshall, E., Brunson, D., Day, A., McLellan, R., Hartley, J.: A structured 
approach for function analysis of complex automotive systems. Society of Automotive 
Engineers Technical paper: 2011-01-1268 (2011) 
17. Campean, I.F., Henshall, E.J.: Systems Engineering Design through Failure Mode Avoid-
ance – an Automotive Industry Perspective. In: Proceedings of the 1st International Confe-
rence on Through-life Engineering Services, pp. 99–107 (2012) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 201–210. 
DOI: 10.1007/978-3-642-30817-8_20 
© Springer-Verlag Berlin Heidelberg 2013 
 
Optimizing Overlap between Testing and Design  
in Engineering Product Development Processes 
Khadija Tahera, Claudia M. Eckert, and Chris F. Earl  
Department of Design, Development, Environment and Materials,  
The Open University, Milton Keynes, MK7 6AA, UK 
{K.Tahera,C.M.Eckert,C.F.Earl}@open.ac.uk 
Abstract. To reduce product development time, upstream testing and down-
stream design processes are often overlapped. Existing studies do not recom-
mend overlapping in situations where test results may have a significant effect 
on downstream redesign. However, this study identifies that, due to long pro-
curement time and lengthy physical tests, companies may have no choice but to 
overlap these tasks to meet product delivery deadlines. This research investi-
gates how a case study company manages these overlaps, and proposes a model 
to support the overlapping of testing and subsequent redesign phases during the 
development phases. 
Keywords: Product development process, physical testing, virtual testing,  
overlapping. 
1 
Introduction 
For engineered products to succeed in competitive markets, performance, reliability, 
safety and durability are critical issues. A potential design may fail to meet customer 
requirements, have technical design faults, or raise issues about manufacturability and 
maintainability [1, 2]. Testing can identify these problems and is central to product 
development (PD) [3]. Product development is not a linear process of “design-build-
test”; rather, the design process and the testing process are intertwined. However, 
physical testing can take a long time, and delayed or negative results in one phase 
potentially jeopardise project schedules. Therefore, design for the next phase often 
starts before testing is complete, causing testing and design activities to overlap. 
Overlapping testing and design activities can incur risk, since redesigning without 
test results might perpetuate faults or miss opportunities to respond to emerging prob-
lems. The literature suggests that overlap models of product development do not ap-
ply well where most changes occur towards the end of the process perhaps due to long 
duration testing (ie slow evolution) and where substantial redesign results from these 
changes (ie high sensitivity). This paper proposes modifications to the product devel-
opment process structure which identify a more prominent role for virtual testing and, 
allow overlap models to be applied more effectively.  
2 
Background  
Overlapping occurs when a downstream activity is started before completing an up-
stream activity. This can reduce development time. The advantage of overlapping has 

202 
K. Tahera, C.M. Eckert, and C.F. Earl 
 
been recognized in several studies [4-7]. Clark and Fujimoto [4] suggest that optimal 
overlapping may depend on organizational characteristics and effective communica-
tion. Overlapping might identify design flaws [5], but may allow accidental omission 
of key steps [6] and may introduce uncertainties which can increase iterations [7]. In 
the worst case, development costs may increase and product quality may worsen [7] .  
Two studies are particularly relevant in setting the context and background for this 
research in modifying process structure for effective overlap. First, Krishnan et al. [7] 
develop a model which formalizes the tradeoffs based on two key concepts: upstream 
evolution and downstream sensitivity. If the primary information about a product’s 
parameter values are given as intervals, as the product development progresses the 
intervals are narrowed and finalized, some faster than others. When the final values 
are achieved early in the process this is called fast evolution, whilst slow evolution 
occurs if most design changes happen towards the end. In low downstream sensitivity, 
substantial changes in the upstream tasks can be accommodated readily in the down-
stream activities. High downstream sensitivity happens when small upstream changes 
require large amounts of iteration in downstream activity. This analysis concludes that 
in general a fast evolution and low sensitivity situation is favorable to overlapping, 
and conversely, high sensitivity and slow evolution is less favourable.   
Second, Terwiesch and Loch [8] present a statistical measurement of the effec-
tiveness of overlapping development activities in reducing project completion time. 
Fast uncertainty resolution projects benefit from overlapping.  This is similar to 
Krishnan’s conclusion above. This paper also identifies that testing in projects with 
fast uncertainty resolution seems to have a delaying rather than an accelerating effect. 
These conclusions might implies that testing with long lead time and slow uncertainty 
resolution is not favorable for overlapping test with redesign unless accompanied by 
structural changes in the product development process.     
It is observed that engineering companies overlap testing and design as essential 
practice; regardless of the situation with respect to evolution, sensitivities and resolu-
tion, and this happens in the case study company. This paper proposes modifications 
to the structure of design and testing processes which allow effective overlap for fast 
evolution and low sensitivity as well as in situations without fast uncertainty resolu-
tion. The benefits of overlapping can then be realized more widely in practice.   
3 
Methodology and Case Study  
A case study was undertaken at a UK-based company that designs and manufactures 
diesel engines; complex, highly regulated products with high levels of testing to meet 
customer requirements, performance standards and statutory regulations. Interviews 
were carried out, recorded and transcribed, between March 2011 to May 2012 with six 
engineers: a senior engineer, a development engineer, a CAE engineer, a verification 
& validation manager and a validation team leader.  
Complex overlapping activities were observed in the company, but this study fo-
cused on single layer overlapping where much of the existing research has been con-
ducted. There were two main objectives: 
 

 
Optimizing Overlap between Testing and Design 
203 
 
1. To identify a means of effective overlap, even where the upstream evolution of 
information is slow and downstream sensitivity is high.  
2. To identify ways to speed up testing to give quicker uncertainty resolution.  
The next section starts by reviewing the process structure and overlapping in the case 
study company, then section 4 analyses the ways that the company overlaps testing 
and design, and section 5 proposes changes to the process structure for more effective 
overlapping of testing and design. 
3.1 
Process Structure in the Case Study Company  
The case study company has a structured gateway process for New Product 
Introduction (NPI) (Fig. 1). It has eight stages starting from “Launch” to “Gateway 
7”. Most of the testing occurs between Gateway 2 (GW2) to Gateway 4 (GW4), thus 
this research focuses on these three main phases of the PD process (as in Fig. 2).  
 
Fig. 1. An outline of the company's gateway process 
 
 
Fig. 2. A schematic of the PD process from Gateway 2 to Gateway 4 
Among the large number of activities in these stages, Re/Design, Computer Aided 
Engineering (CAE) (eg Simulation), and Procurement (of test prototypes) are consi-
dered as drivers for testing. For simplicity Fig. 2 presents these activities as time  
limited boxes, but in reality, a core team keeps working on Design and CAE, and 
Testing goes on almost continuously, in parallel to these activities. Design, CAE, 
Procurement and Testing undergo at least three iterations from GW2 to GW 4, and 
serve different purposes in each stage. 
Market need 
Identified
Groundwork 
research New 
technology 
introduction
Technology 
testing/ 
Concept 
demonstration 
(SD)
Technology 
chosen, 
design 
verification 
(DV) 
Product 
validation (PD), 
engine 
productionalized
Production 
release, 
manufacturing 
process starts
Start of 
Production
Review to 
capture 
issues from 
production 
or operation
Launch
Gateway 1
Gateway 2
Gateway 3
Gateway 4
Gateway 5
Gateway 6
Gateway 7
Market need 
Identified
Market need 
Identified
Groundwork 
research New 
technology 
introduction
Groundwork 
research New 
technology 
introduction
Technology 
testing/ 
Concept 
demonstration 
(SD)
Technology 
testing/ 
Concept 
demonstration 
(SD)
Technology 
chosen, 
design 
verification 
(DV) 
Technology 
chosen, 
design 
verification 
(DV) 
Product 
validation (PD), 
engine 
productionalized
Product 
validation (PD), 
engine 
productionalized
Production 
release, 
manufacturing 
process starts
Production 
release, 
manufacturing 
process starts
Start of 
Production
Start of 
Production
Review to 
capture 
issues from 
production 
or operation
Launch
Gateway 1
Gateway 2
Gateway 3
Gateway 4
Gateway 5
Gateway 6
Gateway 7
Redesign
CAE
Procure
P&E Engine Testing (DV)
Design
CAE
Procure
P&E Engine Testing (SD)
Redesign
CAE
Procure
P&E Engine Testing (PV)
Mech. Engine Testing (DV)
Mech. Engine Testing (SD)
Mech. Engine Testing (PV)
GW2
GW3
GW4
Redesign
CAE
Procure
P&E Engine Testing (DV)
Design
CAE
Procure
P&E Engine Testing (SD)
Redesign
CAE
Procure
P&E Engine Testing (PV)
Mech. Engine Testing (DV)
Mech. Engine Testing (SD)
Mech. Engine Testing (PV)
GW2
GW3
GW4

204 
K. Tahera, C.M. Eckert, and C.F. Earl 
 
Three phases of testing are distinguished: (i) Concept/System Demonstration (SD) 
shows that the technology can deliver the required performance; (ii) Design Verifica-
tion (DV) aims to ensure that design outputs meet the given requirements under dif-
ferent use conditions, and (iii) Product Validation (PV) which tests the product 
against customer requirements and specifications. Both the product’s characteristics: 
Performance and Emission (P&E) and the mechanical durability and reliability are 
tested in each of the three phases. The mandatory tests required for acceptance usually 
occur during PV phases. The engine level testing blocks (Fig. 2) contain a large num-
ber of tests. Some tests are grouped and some are individual. Some test results can be 
obtained quickly whereas some require running the tests till very end of the testing 
phase. 
3.2 
Overlaps with Testing in a Single Product Development Stage  
In each gateway stage there are overlaps between activities.  Design, CAE and 
Procurement overlap but the focus is on their overlaps with testing:  
• CAE - P&E testing: CAE analysis, e.g. Computational Fluid Dynamics (CFD) is 
used to support the design, optimization and verification of an engine’s fluid sys-
tem. As the company freezes the design, some CAE work is still ongoing, and/or 
additional CAE work might be required parallel to the physical testing.  
• CAE-mechanical testing: Finite Element Analysis (FEA) provides accurate, timely 
and cost-effective guidance when testing the durability of engine components. 
• Procurement-testing: Testing starts as soon as a component arrives at site to mi-
nimize the testing lead time. Even system-level testing continues with prototype 
parts. 
• P&E testing-mechanical testing:  Early results from P&E enable the mechanical 
testing phase to start earlier.  
Note that the significant overlaps which occur between testing and (re)design in the 
next phase, is an area of interest of this paper. Fig. 2 illustrates how engines are tested 
in sequence for SD, then DV and PV.  However, in reality, several versions of the 
same engine are tested simultaneously in parallel test-beds. Some components are 
tested for concept demonstration whereas others are tested for design verification. 
Therefore, each testing phase overlaps in a complex manner. 
4 
Analysis of Overlapping Design and Testing  
in the Case Study Company 
In analysing the company’s design and testing processes, two key issues emerge in 
overlapping tasks. Firstly, long lead time procurement and secondly, the long duration 
physical tests.  
4.1 
Long Lead-Time for Procurement  
There are some cases, for example during design verification (DV), when the 
company needs to start a certain test to meet the schedule of the next GW stage, but a 
core hardware component is not available from the supplier. The company cannot 

 
Optimizing Overlap between Testing and Design 
205 
 
afford delay, and instead tests using alternative components. The validation managers 
need to identify suitable alternatives and calculate trade-offs. For example, an engine 
requires a piston to run a test, but the piston will not be delivered until a later date, so 
they will either continue physical tests with a prototype piston, or else simulate the 
ideal engine computationally and identify the associated risk. These alternative tests 
may give a risk reduction of, for example, 30% instead of the planned 50%.  In this 
scenario the product cannot be signed off yet, and physical testing of the new piston in 
an engine is still necessary for verification or validation. This situation causes the DV 
or PV phases to extend over two GW stages instead of one.  
 
Fig. 3. Overlapping between testing and redesign in two phases 
4.2 
Lengthy Physical Tests 
Testing physical prototypes is essentially a slow and expensive process. However, it is 
a high fidelity method for ensuring the product’s characteristics and in some cases is 
mandatory for acceptance and essential for assuring the product’s function and 
behaviour. Ideally, physical testing results from one phase should drive the design and 
CAE of the next phase. However since testing takes a long time, it is not often viable 
to wait. For instance, the SD phase testing may still be on-going while the (re)design 
for the DV phase is started (and sometimes finished), and while procurement for the 
subsequent DV testing begins, as seen in Fig. 3.  
Without the testing results being available, there will be uncertainties in redesign-
ing and procuring for the next phase. This is a case where upstream evolution of  
testing information is slow and has high sensitivity on downstream design phase, 
resulting in significant number of iterations in subsequent phases.  
4.3 
The Current Approach to the Issues  
To overcome the issues mentioned above, the company has developed two main 
methods: firstly frontloading the tasks, and secondly reducing physical testing through 
supporting CAE. 

206 
K. Tahera, C.M. Eckert, and C.F. Earl 
 
Front loading (a) increases the rate of problem solving cycles at early stages 
(activity frontloading) or (b) uses prior knowledge about past problem solving 
(knowledge frontloading) to reduce the necessary number of problem solving cycles 
at later stages [4]. To minimize long lead time procurement, a clear and accurate 
specification of the product is required. CAE analysis drives design requirements and 
engine settings for test. Optimization takes place earlier in the product development 
cycle (front loaded), to improve product specification to the supplier. The company 
makes virtual prototypes with many iterations to enable the first physical prototype to 
be built closer to target. One engineer commented, “computer simulation is becoming 
increasingly important to the companies to minimize the effort and expense involved 
in product development”.  
Reducing physical testing through CAE analysis and simulation, can identify im-
proved boundary conditions for physical test, which then becomes more focused. For 
example in a performance test, simulation can predict when to measure a value or condi-
tions, so less time is spent on the physical test. Physical test results validate the product as 
well as the simulation model, which is then reusable for future products, reducing time 
and cost for subsequent iterations of the model. Iteration in physical product testing re-
quires building a new physical prototype and might involve redesigning, reordering, 
building and testing, In contrast, virtual testing supports fine tuning of selected parame-
ters and rapidly produces new models of components or products. 
5 
Proposed Process Structure 
In the review of literature in Section 2 two key papers [7, 8] were identified.  
Krishnan, V. et al. [7]  recommend circumstances where activities should be 
overlapped. From their model, the worst case is where the upstream evolution is slow 
and the downstream sensitivity is high; in this case overlapping is not recommended. 
In this situation, it is suggested that exchanges of information should be 
disaggregated, to see if any information can evolve faster, or can be practically 
transferred in a primary form.  
On the other hand, in the other key paper, Terwiesch and Loch [8] indicated that 
lengthy testing might have a delaying effect on a fast uncertainty resolution project. 
For this case study company, it is difficult to gauge the speed of uncertainty resolu-
tion. The company has to finish a project on a given timeline and bring the product to 
the market. Even for a complex new product, the timeline may vary little. Terwiesch 
and Loch [8] also suggest that “if the uncertainty resolution over the course of the 
project is unfavourable for overlapping activities and cannot be sufficiently accele-
rated by defining standards and architectures, the project organization has to search 
for other means of uncertainty resolution” [8]. For the case study company, testing is 
the primary method for uncertainty confirmation and identification. Subsequent tasks 
such as redesign are uncertainty elimination tasks. Testing is a slow process; so the 
company undertakes downstream design activities before testing is finished. Knowing 
the associated risk of an extensive rework, the company has no choice but to overlap 
these design tasks with testing, a design proposal is needed to commence another 
lengthy procurement process. Thus for this case, a way of accelerating the testing 
process was essential. 

 
Optimizing Overlap between Testing and Design 
207 
 
 
Fig. 4. The proposed process structure 
It is suggested for this case that these issues can be solved by introducing virtual 
testing parallel to the physical testing in each PD phase, as shown in the model pre-
sented in Fig. 4. Simulation or virtual testing can be regarded as distinct from CAE 
analysis proper. Initial CAE analyses may check interference and stress on compo-
nents and assemblies using general purpose tools, such as FEA. A virtual test is  
designed specifically for a given situation and conditions and is representative of a 
physical test. Virtual testing of a piston should create a use scenario over the full 
range of parameters on a piston which might be encountered in a test bed. This virtual 
test for a piston would not be appropriate for another component like a connecting 
rod. Such virtual test models are founded on the expertise of engineers and the soft-
ware development team in formulating mathematical models for the interacting en-
gine components, writing appropriate numerical solution algorithms, and integrating 
the resultant programs into workable analysis. However, it is also noted that physical 
test results help to improve and validate virtual test models. Early CAE analysis helps 
to reduce uncertainty, thus frontloading activities. In contrast virtual testing is aimed 
more at reducing the time and effort of physical testing. 
The proposed model separates virtual testing from the initial CAE analysis. Initial 
CAE analysis should define the specification for procurement and virtual testing 
should assist the physical testing. Not all physical tests require virtual testing, or 
might be assisted by it. Initially, it is necessary to build a virtual test model, which is 
representative of the physical test, and can be validated in physically. Engineering 
experience, prior understanding of the product, previous product testing and historical 
data should all contribute to the boundary conditions for the virtual test model. The 
model is further validated against the values gained from the physical tests. The limits 
of variation in the variables are adjusted in the virtual testing model through several 
iterations until the simulation model is representative of the physical tests.   
5.1 
Benefit from Virtual Testing 
Integrating virtual testing into the process structure can help to address the two key 
objectives in Section 3. The first objective is to create effective overlap when 

208 
K. Tahera, C.M. Eckert, and C.F. Earl 
 
upstream evolution is slow and downstream sensitivity is high. For instance, in cases 
where results from a physical test cannot be delivered before the end of the test, the 
durability testing of a new engine component may not produce any failure until very 
late in the testing process. This type of failure can prompt modifications with serious 
consequences (such as material changes) and may lead to an additional iteration in 
design and procurement.  As information does not evolve quickly in upstream 
testing, which has a high level of sensitivity to downstream design, overlapping is not 
favorable. This paper suggests using parallel virtual testing. When the sensitivity in 
the downstream design is high, the faster evolution of useful test results is required to 
make the overlapping possible. In this case, this paper suggests starting the 
downstream design work once the virtual testing has produced results which are 
representative of the physical testing results. Virtual test model simulation will predict 
parameter values faster than a physical test, and the faster evolution or disaggregation 
of useful results will be possible. Early prediction or indication of failure can support 
an early design decision.   
The second objective is to make testing faster. Different tests benefit from inte-
grating virtual testing with physical testing in different ways. Some benefit by focus-
ing the tests, and identifying future values to minimize the number of iterations to 
yield a satisfactory design, while others require running for shorter periods of time. 
For example, for constant speed and load, an engine has its intakes of fuel and air 
regulated, with the goal of achieving desired power ratings. An engine might require 
several iterations in design and test to achieve these desired power ratings. A virtual 
testing using a mature model can predict the likely consequences of certain values of 
fuel and air intake of the engine, thus suggesting appropriate values for next iteration.  
Reliability and durability tests ensure performance without failure over an ex-
tended period of time. When a virtual test is able to accurately predict the behaviour 
of the engine, then the number of physical testing hours for durability can be mini-
mized, saving time and reducing cost. The virtual testing might also indicate the 
points where the product might fail, making it possible to avoid unnecessary testing, 
or to replace a component before it fails and damages the whole engine. 
5.2 
Costs for Introducing Parallel Virtual Testing  
Companies might be reluctant to accept the introduction of a virtual testing model if 
the costs are higher than the benefit. The cost will depend on two main factors: 
communication cost and virtual testing model establishment cost. Effective 
communication between physical testing and the CAE team is a key success factor for 
this structure of parallel physical and virtual testing. 
 
Fig. 5. Information exchange between virtual testing, physical testing and design 
C
Virtual Testing
Engine Physical Testing
Re-design
CAE 

 
Optimizing Overlap between Testing and Design 
209 
 
Initially, virtual (simulated) and physical testing results may differ in several ways. 
Discrepancies may determine the number of meetings required, and increase with the 
level of uncertainty and potential dependencies [9]. The cost for introducing the 
virtual testing block can be calculated as follows. Initially a fixed cost C is required to 
build the virtual model (as shown in Fig. 5). This cost will depend on the company’s 
capability in CAE modelling and simulation. With a well established CAE department 
then this cost might be lower than outsourcing. We are assuming that the cost for each 
meeting is Xi, for meetings i = 1, 2,.. n. After the model is mature, the frequency of 
meetings is reduced. Each meeting results in modifications and further simulation in 
the virtual model, at cost Yi. A regular maintenance and opportunity cost M is 
incurred per unit time, for the virtual test duration TV. If a company has committed 
human resources for CAE analysis throughout the process, this maintenance might 
not add extra marginal costs.  Thus the cost of additional virtual testing model is:  
CVT = C + ∑ (Xi + Yi) + M TV     
(1)
Savings denoted CT will be accumulated in several ways. Learning from the parallel 
virtual testing will reduce the uncertainties in design and procurement. The gain is 
highly dependent on the sensitivity of the downstream work. It is assumed that this 
virtual testing will make the physical tests shorter without any quality loss, and that 
the virtual test is representative of the physical testing.  A benefit in using parallel 
virtual testing will accrue when CT > CVT.  However, the real benefit of using parallel 
virtual testing continues during iterations as this might avoid extending a testing into a 
subsequent gateway. Even with another iteration (of DV for example), the cost of 
running the virtual testing phase will be approximately ∑ (Xi + Yi) + MTV, as the 
model building cost C will be small as the virtual testing model is already mature, the 
number of meetings will also be relatively low. The duration of physical testing in this 
phase will be shorter, and uncertainty decreased. Thus larger savings in physical test-
ing are possible.   
6 
Discussion and Conclusion 
The question remains as to whether such virtual testing models can be constructed. 
The case study company has partially done this, both to assist the physical testing and 
to apply when physical components are not ready. The performance, reliability and 
durability predictions of engine components using CAE is developing rapidly. For 
example, the material and structural analysis group’s understanding of the principles 
of fatigue behaviour in complex materials, combined with historical data from high 
temperature applications, modelled in commercial (and internal) software, with a 
comprehensive materials database means that the durability of engine components can 
be reliably predicted and probability distributions applied to perform failure rate 
calculations.  Whilst the company recognises there are still many technical 
challenges to overcome, ongoing investigative work in virtual testing currently 
includes gas flows and combustion chemistry, cavitations in bearing oil films and 
metal fatigue under extreme temperatures.  

210 
K. Tahera, C.M. Eckert, and C.F. Earl 
 
This research suggests a model to reduce the uncertainties associated with over-
lapping between testing and redesign. This paper has considered the scenario where 
the information evolution of upstream testing is slow and the sensitivity on down-
stream design is high a case which the literature suggests do not provide favourable 
conditions for overlapping. However, companies often have no other choice but to 
practice overlapping. The proposed model suggests a possible strategy for overlap-
ping providing several benefits: (1) reduced uncertainty in design and procurement, 
(2) focused physical testing, (3) reduced duration of physical tests (4) reduced itera-
tion and overall cost saving.  
Further work will extend validation of this model in an industrial context, includ-
ing the original case study company. In particular, overlapping considerations for the 
design and testing of products at different scale, complexity and maturity will be 
compared. The model will be extended to consider multiple layered overlapping.  
References 
1. Thomke, S., Bell, D.E.: Sequential Testing in Product Development. Management Science, 
308–323 (2001) 
2. Qian, Y., Xie, M., Goh, T.N.: Optimal Testing Strategies in Overlapped Design Process. 
European Journal of Operational Research 206(1), 131–143 (2010) 
3. Thomke, S.H.: Experimentation matters: unlocking the potential of new technologies for 
innovation. Harvard Business Press (2003) 
4. Clark, K.B., Fujimoto, T.: Product development performance: Strategy, organization, and 
management in the world auto industry. Harvard Business Press (1991) 
5. Ha, A.Y., Porteus, E.L.: Optimal Timing of Reviews in Concurrent Design for Manufactu-
rability. Management Science, 1431–1447 (1995) 
6. Smith, P.G., Reinertsen, D.G.: Shortening the Product Development Cycle. Research Tech-
nology Management 35(3), 44–49 (1992) 
7. Krishnan, V., Eppinger, S.D., Whitney, D.E.: A Model Based Framework to Overlap Prod-
uct Development Activities. Management Science, 437–451 (1997) 
8. Terwiesch, C., Loch, C.H.: Measuring the Effectiveness of Overlapping Development Ac-
tivities. Management Science, 455–465 (1999) 
9. Loch, C. H., Terwiesch, C.: Communication and Uncertainty in Concurrent Engineering. 
Management Science, 1032–1048 (1998)  

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 211–220. 
DOI: 10.1007/978-3-642-30817-8_21 
© Springer-Verlag Berlin Heidelberg 2013 
 
Do(PLM)Con: An Instrument for Systematic Design  
of Integrated PLM-Architectures  
Jörg W. Fischer, Bernhard Lammel, Dirk Hosenfeld, Deodatt Bawachkar,  
and Bernd Brinkmeier 
Siemens Industry Software GmbH & Co. KG, Liebknechtstr. 35, Stuttgart, 70565, Germany 
{joerg.fischer,bernhard.lammel,dirk.hosenfeld,deodatt.bawachkar,
bernd.brinkmeier}@siemens.com 
Abstract. Today, the purpose of many PLM implementation projects is the rea-
lization of PLM-Architectures which aim on product data integration over the 
dimensions domain, business unit and product line along the Product Develop-
ment Process. Currently, these projects are mostly addressed like IT-Projects 
for implementation of domain specific products (e.g. CAx, PDM- or ERP-
Systems). Hence the implementation focuses on functional aspects. This focus 
usually lacks consideration of semantic and progression conception which are 
key aspects for PLM implementation. To improve this situation the 
Do(PLM)Con methodology was developed. It provides methods for abstract de-
sign of integrated PLM-Architectures including semantics and progression. The 
following paper will explain the terms semantics and progression in PLM and 
describe the Do(PLM)Con method as an instrument for systematic design of in-
tegrated PLM-Architectures. 
Keywords: PLM, Product Lifecycle Management, PLM-Architectures, 
Do(PLM)Con. 
1 
Current Discussion of Integrated PLM-Architectures 
Today industry is working on digitizing the entire process chain from product to pro-
duction. One of the essential goals of this is to bridge the gap between virtual and real 
world. To do so, it is necessary to design integrated virtual Product and Production 
Models across the product lifecycle which presupposes the implementation of inte-
grated PLM-Architectures. A major task to realize this is to define a suitable IT-
System landscape.  
An ideal picture discussed in this context (see Figure 1) is the concept of service 
oriented PLM-Architecture which aims on separation of business logic from applica-
tions and database layer [1]. The idea behind this approach is that IT-Systems offer 
standardized "services" which can be used by other IT-Systems. This approach of 
loose coupling has substantial potential, but does not solve persistent semantic inte-
gration which is a key aspect while implementing integrated PLM-Architectures. 

212 
J.W. Fischer et al. 
 
Service Bus for
Product Development
Legend:
Product Development Process
Data Management Systems
Service Bus
PDM
ERP
Config. 
System
System - X
 
Fig. 1. Ideal picture of an integrated PLM-Architecture 
2 
Semantic and Progression as Key Aspects for  
PLM-Realization  
Product models are realized as product structures within IT-Systems. IT-functions 
used in product development traverse these product structures and extract portions of 
data to be authored by the user. To do so, they demand specific semantics from the 
respective product structure. If the structure does not meet the demanded semantics, 
the function is not or only limited available. This issue becomes particularly relevant 
when building integrated PLM-Architectures since integration requires the consolida-
tion of different product structures with distinct semantic aspects. Hence the resulting 
consolidated structure usually contains significant semantic changes.  
Figure 2 shows an example for such a case. The business intent of the scenario is 
providing a digital product prototype in a correct positional arrangement that can be 
configured based on customer-selectable variant options. The business intent should 
be realized within a PDM-System. The relevant functions to fulfill the use cases, as 
the most important the visualization of virtual prototypes is mentioned, will take place 
in the PDM-Client.  
On the left hand side of Figure 2 the implemented "Current State" Scenario is 
shown. A product structure is maintained in a PDM-System. The Client in pairing 
with the business logic of the PDM-System traverses these product structure and ex-
tracts the data which has to be visualized for and authored by the user (Figure 2, De-
tail A). 
For realization of the integrated "Future State" (Figure 2, right hand side), structure 
extracts with specific semantics from the BOM-System are inserted in the product 
structure of the PDM-System (Figure 2, Detail B). In this specific case it is the posi-
tion, the part and the usage including the variant condition. To complete the model-
ing, the incorporated structure extracts have to be semantically integrated. In this 
specific case semantic integration is done by Product Engineers through assignment 
of CAD documents to the respective design usages. This nourishes the product struc-
ture with prior to this not explicitly documented information of geometric variance. 
As a result of the described semantic integration, the basis of information for realizing 
the new visualization use cases is set. 

 Do(PLM)Con: An Instrument for Systematic Design of Integrated PLM-Architectures 
213 
 
Legend:
Data Management 
System
Application
Use Case
PDM
Client
structure extract
B
C
BOM-
System
PDM
Client A
Extraction of 
Authoring Volume
Current State
integrated Future State
 
Fig. 2. Example for an industrial integration scenario 
The use cases are performed by the user with the help of IT-Functions. These 
Functions (see Figure 2, Detail C) access the specific product structure represented in 
the PDM-System, extract an amount of data and provide these data to the user. Pre-
condition for the function to perform successful is that the semantic consistency the 
functions expect from the product structure in the PDM-system is met. Therefore, a 
function could be considered only in pairing with the semantics of the given product 
structure. 
Taking into account that semantic necessities are driven by methodical, organiza-
tional and technical aspects [2] whereas the first two are typically customer specific 
and that integration presuppose conjoining diametrical semantics it becomes obvious 
that semantics is a key aspect of the design of integrated PLM-Architectures. 
Another important aspect to be considered is the progression of product informa-
tion during product lifecycle. Figure 3 outlines the principle dimensions of progres-
sion within PLM. Entity progression refers to the gradual development of product 
information across product lifecycle. Typically known concepts to depict this aspect 
in product structures are revisions and releases. The dimension granularity modifica-
tion denotes on changes of data granularity happening regularly during daily work. It 
occurs if information has to be transferred in another level of detail. A typical exam-
ple for a situation could be that the designer recognizes that the axle he wanted to 
design as a single part has to be subdivided in different parts connected via a gearbox 
or a BOM-Engineer notes that filter and sealing ring have to be ordered separately, 
thus he has to transfer the previous component into two separated parts within the 
BOM-System. Content separation contemplates that a partial portion of a structured 
information including semantics is taken to use it somewhere else. Export to Excel or 
XML would be the standard example for this kind of progression. The important con-
sequence of this case is the fact that in the moment this data is separated a parallel 
redundant lifecycle of the separated data accrues, which might has to be adjusted by 
manual or automatic activity during the development process.  

214 
J.W. Fischer et al. 
 
Granularity 
Modification
Entity
Progression
content
seperation and
transformation
Label
Effectivity
Legend:
 
Fig. 3. Progression dimensions 
The aspects of progression are substantially known and handled in common PDM-
Systems today, but considering integration, the progressions of different product 
structures have to be synchronized. Since this is, due to organizational premises, in 
numerous cases not possible asynchronous progression has to be realized instead. 
This is usually challenging and has significant impact on structure semantics and 
semantic consistency of integrated structures. 
In consequence of these considerations PLM-Functions can not be seen on their 
own anymore. They have to be considered in combination with semantics and pro-
gression. Thus, semantics and progression becomes a central design aspect while 
realizing integrated PLM-Architectures. Today’s typically taken functional view often 
leads to the obstacle that the necessity of carefully semantic and progression concep-
tion is not considered as part of the project. Thus in the course of the project inte-
grated structures evolve, but their semantics and semantic integrity is adapted through 
an exhausting "trial and error" process until they fulfill their purpose.  
To solve this, an instrument is needed which allows to design integrated PLM-
Architectures including the key aspects semantics and progression. The Do(PLM)Con 
method is such an instrument. 
3 
An Abstract Layer  
During conception of the Do(PLM)Con method it became obvious, that a construct 
was needed, which allows to depict functional requirements including belonging se-
mantic and progression necessities. Beyond this, the construct should be able to sim-
plify the complex coherencies of PLM and detach PLM-Architecture discussion from 
limitations of already given IT-Systems or IT-technology.  
When considering the Product Development Process it can be seen that such a con-
struct exists, but its significance has not been recognized sufficiently. During the 
Product Development Process teams of Product Engineers of different field of appli-
cations create and use models of the future product. Each of these models reflects 

 Do(PLM)Con: An Instrument for Systematic Design of Integrated PLM-Architectures 
215 
 
partial aspects of it. Typically known product models are solid CAx-Models com-
bined as product structures in PDM-Systems, BOM-Models realized in BOM-
Systems or Requirement Models created in Requirement Management Systems (for 
additional examples see [3]). Further, usually unsought models can be found all 
across product development departments in standard tools like Excel and Access or as 
piece of paper in the drawer of an engineer. Key to successful product development is 
to create, align and combine these partial product models until the real product can be 
build out of them.  
 
Fig. 4. Partial (Product)Model 
Hence, Do(PLM)Con takes "Partial (Product)Model" as its abstract core element 
(see Figure 4). A "Partial (Product)Model" in Do(PLM)Con is seen as a depiction of a 
sum of relevant aspects of the product out of the perspective of a specific field of 
application or users (a more detailed definition is given in [4]). In Figure 4 the aspects 
of the Do(PLM)Con definition of Partial Models are delineated. A Partial Model ex-
ists because of its purpose (Figure 4, Business Intent). To realize this purpose, it has 
to cover defined use cases of product development. The use cases themselves require 
specific semantics from the Partial Model. The semantics subsumes the areas struc-
tures, information entities and relations between information entities. A Partial Model 
is designed for an explicit content, for example to subsume a single product or a com-
plete product program etc. Due to the aspect of content a Partial Model possesses 
behavior. Behavior can be subdivided into the categories progression, access and 
extraction whereas progression is the most important aspect of the behavior (see pro-
gression dimensions in Figure 3).  
With the help of the Partial (Product)Model concept the various models generated 
by product engineers within the Product Development Process can be easily  

216 
J.W. Fischer et al. 
 
expressed. If in addition also analyzing the lifecycle interaction between the Partial 
Model, the Partial Model concept becomes an essential basis for understanding the 
customers individual Product Development Process.  
To support this the Do(PLM)Con-Method provides different diagrams which are 
able to depict Partial Models including their interaction in Current (meaning already 
implemented) and Future State. To give an impression a planned "Future State" out-
lined in the "Lifetime & Information Flow Diagram" is shown in Figure 5. The exam-
ple focuses on the scenario of Figure 2 but beyond this includes additional Partial 
Models which have to be considered. 
Pre-SOP
Post-SOP
SOP
ERP-Material
BOM-Usage
Configurable Digital Prototype
Previous Design
Design
Design Template 
Legend:
Release/Baseline
Cross Model Type Alignment
Partial Model 
Cross Model Instance Link
Cross Model Link
Cross Model Clone
Cross Model Copy
 
Fig. 5. Example for working with abstract Partial Models 
The Lifetime & Information Flow Diagram enables a simplified overview of rele-
vant Partial Models, their rough lifecycle and their lifecycle interaction. The models 
are shown along a time axis which subdivides into Pre- and Post-SOP (Start of Pro-
duction) of a given product. Rectangles depict Partial Models in their rough lifetime. 
The type of arrows between the Models represent the principle model integration and 
the rhombus indicates releases within the Partial Model. All different symbols have 
specific methodical significance. 
The Business Intent behind the scenario shown in Figure 5 is a typical integration 
scenario in industries today. It aims on visualizing a digital product prototype that can 
be configured based on customer-selectable variant options. In the depicted Future 
State scenario a new Partial Model for "Configurable Digital Prototype" is realized. It 
integrates information from the "BOM-Usage" Model via "Cross Model Clone" which 
indicates that the information (e.g. the part and usage) transferred is cloned to the 
"Configurable Digital Prototype" Model. This solution concept allows for a progres-
sion splits between the two Partial Models, which enables working on both models 
simultaneously. The "Design" Model is connected very closely through "Cross Model 
Instance Links". Linking Models via a "Cross Model Instance Link" denotes that 
Model A and B are sharing some identical Information Entities but the instant link 

 Do(PLM)Con: An Instrument for Systematic Design of Integrated PLM-Architectures 
217 
 
adds model specific context information (e.g. Design Variants or Variant Specific 
position information). This kind of a design near realization of the "Configurable 
Digital Prototype" Model is chosen because one business intent of the scenario is to 
support the Pre-SOP design with the configurable visualization. 
As seen above the Do(PLM)Con Partial Model concept and diagrams allow for 
simplified analyses of integrated PLM-Architectures consisting of interacting Partial 
Models on pure methodical level. This creates a neutral base to discuss and define 
future concepts not being influenced by already realized IT-Systems. This kind of 
reduction to the substantial demand generates high transparency of requirements and 
enables savings in implementation costs since only requirements which are really 
necessary will be implemented. 
4 
There is No Green Field 
Management of the product lifecycle is nothing new. Every manufacturing company 
has to manage the lifecycle of their products and they did it long before PLM became 
a significant topic in manufacturing industries.  
PDM
ERP
Lifecycle
SOP
Product Development 
Process (PDP)
 
Fig. 6. Typical "Current State" IT-System landscape 
The purpose of realizing PLM-Strategies is essentially based on the fact that an ef-
fective management of the product lifecycle contains enormous improvement poten-
tial which could realize high competitive advantages. Also it is necessary to manage 
the increasing product complexity. Thus there is no green field while realizing PLM. 
In every manufacturing company a "Current Situation" is found and the aim of PLM 
is to improve this current situation. In Figure 6 a typical "Current State" IT-System 
landscape is shown. A grown Product Development Process is mainly supported by 
standard IT-Solutions (e.g. PDM- and ERP-Systems). Additionally, a number of 
mostly individual systems is used.  
In a first implementation stage of PLM industries aimed on reducing the number of 
individual systems by integrating them in standard solutions such as PDM and ERP. 
Currently there is a tendency to think in a large-scale on digitizing the entire process 
chain from product to production in a multi disciplinary Engineering Network Reposi-
tory [5]. 

218 
J.W. Fischer et al. 
 
5 
"Value Design PLM" with Do(PLM)Con 
For this each company has to find and define an individual "Future State" PLM-
Architecture. Do(PLM)Con is an instrument, containing systematic approaches, tools 
and best practice methods which help to design such a "Future State". Do(PLM)Con 
is based on some principle paradigms which are listed below. 
• Integrity of functions, semantics and progression  
• Model before process  
• Stay Focused  
• Value first  
The paradigm "Integrity of function, semantic and progression" refers to the ne-
cessity to consider the aspect function only in context of semantics and progression 
and to ensure decoupling of the architectural discussion from functional blocks in IT-
System landscape maps by using the Do(PLM)Con concept of "Partial Models". 
"Model before Process" denotes that while analyzing a "Current State" situation it is 
easiest to first focus on Partial Models, secondly on the engineering tasks which are 
performed with the help of these models and thirdly on the engineering processes 
following out of this. The reason for this is the experience that Partial Models are the 
seed crystals of the interrelated engineering processes in product development. "Stay 
Focused" indicates that there should be a clearly defined PLM-Area which is in focus 
to avoid the risk of overcharging the project. "Value first" emphasizes that it is only 
reasonable to implement a Partial Model if it is possible to describe the kind of busi-
ness intent it fulfills and which value this will bring for the Product Development 
Process. 
The Do(PLM)Con approach follows a procedure model (see Figure 7) which de-
fines different realization phases. The two most important phases are  
• Pre-Framing and  
• Realization Packaging 
In "Pre-Framing" the "Current State" of implementation and the Product Development 
Process is investigated. The fundamental tasks in this phase are analyzing existing 
Partial Models, identifying their business intent and investigating the user groups 
maintaining and using these models (cf. Figure 7, "Current State" from Implementa-
tion to Partial Model Layer). In a directly following step interactions between Partial 
Models are also examined.  
The outcome of the Pre-Framing analyses is documented in the diverse 
Do(PLM)Con simplified graphical representation diagrams (see as example Figure 5).  
Following these investigations “Value Design PLM” is done as part of Pre-
Framing. “Value Design PLM” is based on similar ideas as "Value Stream Design" in 
"Lean Management". As "Value Stream Design", "Value Design PLM" is oriented at 
customer view [6]. The customers in case of Do(PLM)Con are the users of the differ-
ent Partial Model, their needs and the core tasks which they have to perform with the 
Partial Models. Consequently, that connotes that if a Partial Model does not support a 

 Do(PLM)Con: An Instrument for Systematic Design of Integrated PLM-Architectures 
219 
 
defined business intent it is not needed and thus the necessity to create and maintain 
it, is to be questioned. Also it is to be called into question whether or not two models 
are necessary if supporting similar Business Intents. Following out of this “Value 
Design PLM’s” first goal is to reduce the number of Partial Models.  
Realization Packaging
Definition of 
Realization Alternatives
Alternative Evaluation
Architecture 
Release
Prototyping
Continues Build and Test 
Pre Framing
Implementation
Layer
Do(PLM)Con:
Partial Model Layer
Future State
Current State 
PDM
ERP
Value Design PLM
Organizational 
Reconciliation
Semantic
Architecture 
PLM
ERP
 
Fig. 7. Do(PLM)Con Procedure Diagram 
As a further step a new lean integration of the different models is designed. This 
has on one hand the target to realize self-acting, automatic interaction wherever it is 
possible and on the other hand the goal to discover the integration potential of differ-
ent Partial Models by analyzing their communality. The aspect of communality and 
possibilities to use them for “Value Design PLM” has a specific significance for inte-
gration approaches but is too complex to be discussed in terms of this paper having an 
overall focus. Outcome of “Value Design PLM” is a Future State of Partial Models 
including their semantics and progression and the definition of necessary integration 
and interaction processes. 
In the "Realization Packaging" phase the desired "Future State" is realized in dif-
ferent concepts of possible implementation alternatives (cf. Figure 7, "Future State" 
from Partial Model to Implementation Layer). For this Do(PLM)Con defines the con-
struct of Realization Package (RP). A RP encapsulates a complete scenario for an 
implementation landscape to be carrying the different Partial Models. A RP also in-
cludes the definition of necessary function packages and PLM-Applications realizing 
this functionality. A RP is thus a concrete definition of a specific framework for a 
possible solution that can be evaluated, simplified depicted and thus discussed trans-
parently on management level. 
 Since chosen IT-Systems and applications are providing a semantic and behavior 
frame as a next step "Future State" Partial Models have to be fitted into the semantic 

220 
J.W. Fischer et al. 
 
and behavior frame of the Carrier IT-Systems given by the specific RP. Do(PLM)Con 
supports the fitting with the help of "System Mules". "System Mules" in 
Do(PLM)Con are constructs similar to development mule in the automotive industry. 
System-Mules are IT-System installations equipped with experimental components 
for testing to try out aspects before starting the full customization, implementation 
and roll out. Significant goal is to discover typical issues, especially semantic incon-
sistencies which might occur due to intensive integration of Partial Models. 
The "Realization Packaging" ends with the release of one specific realization pack-
age which should be implemented as the future integrated PLM-Architecture. After 
this the chosen Realization Package serves as a guiding model for the implementation 
in following project phases. 
6 
Conclusions 
Do(PLM)Con provides instruments that help to derive lean and flexible concepts of 
integrated PLM-Architectures. By Pre-Framing and Realization Packaging as the 
underlying main design steps it is possible to provide a specific design proposal al-
ready in an early project phase which could be used as a basis for discussion, scientif-
ic object and experimental environment. The systematic analysis helps to avoid com-
plex amendment and to speed up usually long drawn-out integration projects. 
Do(PLM)Con therefore enables an effort-reducing implementation of integrated 
PLM-Architectures. Also the expenses for the maintenance and care of the structure 
during the application are substantially reduced. 
References 
1. Bergsjö, D., Catic, A., Malmqvist, J.: Implementing a service oriented PLM architecture 
using PLM Services 2.0. In: International Design Conference - Design (2008) 
2. Fischer, J.W., Glauche, M.: Skizzierung eines Gestaltungsrahmens für Produktstrukturen. 
ZWF Zeitschrift für Wirtschaftlichen Fabrikbetrieb 3, 127–132 (2011) 
3. Eigner, M., Stelzer, R.: Product Lifecycle Management. Ein Leitfaden für Product Devel-
opment und Life Cycle Management. Springer, Heidelberg (2009) 
4. Fischer, J.W., Brinkmeier, B.: Do(PLM)Con - Proposal of a taxonomy for the development 
of PLM-Architectures. Product Data Journal 1, 24–28 (2012) 
5. Eigner, M.: PLM versus PPS: Zukünftige Lösungen für ein durchgängiges Produkt- und 
Prozessmanagement. In: eDM Report; Electronic Data Management, vol. 1, pp. S.20–S.23. 
Hoppenstedt Publishing, Darmstadt (2012) 
6. Rother, M., Shook, J.: Learning to See: Value Stream Mapping to Add Value and Eliminate 
MUDA, p. S.17. Taylor & Francis (1999) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 221–230. 
DOI: 10.1007/978-3-642-30817-8_22 
© Springer-Verlag Berlin Heidelberg 2013 
 
Meta-data-Model for the Development  
of Adaptronic Systems  
Roland Nattermann and Reiner Anderl 
Department of Computer Integrated Design, TU Darmstadt, Petersenstraße 30,  
64287 Darmstadt, Germany 
nattermann@dik.tu-darmstadt.de 
Abstract. Active systems are characterized by an integration of electronic com-
ponents into mechanical systems and can be distinguished into mechatronics 
and adaptronics. In adaptronic systems the electronic components are full struc-
tural integrated. The Department of Computer Integrated Design (DiK) has  
developed the W-model as a new proceeding model for the integrated develop-
ment of adaptronic systems which is based upon a virtual cross-disciplinary in-
tegration and verification in all phases of product development. Therefore the 
W-Model insists on the use of special development environment which is based 
on the product data management functionalities. In this paper a proposal for a 
meta-data-model for such an environment is presented. Following the W-model 
the meta-data-model allows the definition, import and analysis of complex sys-
tem models, the derivation of hierarchic requirements-, function- and product 
structures and  the  definition of integrated system simulations for the virtual 
verification of results. 
Keywords: Adaptronic, Development, Systems Engineering, W-Model. 
1 
Introduction 
The development of active systems is characterized by an increasing integration of 
electronic components. In addition, these devices are no longer additions to mechani-
cal systems but more and more structural integrated. This leads to a further develop-
ment of mechatronics towards adaptronics. The increasing level of integration is an 
additional challenge for product development, as the variety of dependencies between 
systems elements must also be considered in the development process. 
The application of existing process models for the development of mechatronic 
systems has proven to be an issue due to increasing complexity. Therefore, a new 
process model for the development of active systems, as an extension of existing 
models, has been developed. This new model, called the W-model, insists on the use 
of a specific development environment. 
The aim of the work presented was to develop a meta-data-model for such an  
environment. The W-model as well as the meta-data-model is presented below and a 
possible implementation using a representative example is shown. 

222 
R. Nattermann and R. Anderl 
2 
Adaptronic Systems 
Active systems are distinguished into mechatronics and adaptronics and are characte-
rized by an adaptation of the mechanical behavior to changing external and internal 
conditions. 
Mechatronics refer to systems in which, in addition to different active principles 
for functional performance, other disciplines such as control, regulation, and informa-
tion technology are used [1]. 
In adaptronic products the sensors and actuators are structurally integrated. This 
means they are embedded in the load path of the basic mechanical structure. In adap-
tronic systems sensors and actuators are often made from multifunctional materials 
(e.g. piezo electric materials). Due to the adaptive materials, the material properties 
can be adjusted by the control system to achieve the desired characteristics [2], [3]. 
The difference between mechatronics and adaptronics is described by using an Eu-
ler column with actively controlled buckling behavior. In Figure 1, the column is 
shown both in a mechatronic (left) and in an adaptronic (right) configuration. 
 
Fig. 1. Euler column in mechatronic (left) and adaptronic (right) configuration (cf. [4] 
The large numbers of dependencies between the involved domains in adaptronics, 
compared to mechatronics, have to be considered in the product development. This 
requires not only improvements in the domains involved but also an adaption and 
improvement of methods and tools used in product development. For the further de-
velopment of adaptronics the German federal state of Hessen has founded the AdRIA-
Center (AdRIA – Adaptronic, Research, Innovation, Application) in 2008, as part of 
the LOEWE-project (LOEWE – federal initiative for the regarding of scientific  
excellence). The aim of the LOEWE-Center AdRIA is the further development of 
adaptronics in scientific depth and width to the industrial usability. The ADRIA-
center has been established as a cooperation between the Technical University of 
Darmstadt, the University of Applied Sciences Darmstadt and the Fraunhofer  
Institution LBF [3], [5]. 

 
Meta-data-Model for the Development of Adaptronic Systems 
223 
3 
State of the Art 
Current industrial development projects are often based on the V-model for the devel-
opment of mechatronic systems. The V-Model proposes a development process which 
consists of three steps [6]. In the first step an analysis of the requirements takes place, 
a system model is defined and the development tasks are distributed into the involved 
technical disciplines. In the second step the detailed development is done parallel by 
the corresponding disciplines. When the disciplines have reached a specific level of 
maturity, the discipline specific solutions are integrated in a third step. The integrated 
solution is analyzed with respect to the systems behavior and the fulfillment of the 
requirements. Then the necessary changes and adaptions are determined and the dis-
cipline specific development process starts again [6]. 
Analyses of industrial development projects which use the process defined in the 
V-Model have shown that problems occur during the integration of the discipline-
specific solutions, caused by a lack of communication [7]. The cross-disciplinary 
communication deficits lead to divergent development processes during parallel de-
velopment, resulting in a large number of failures in the phase of system integration. 
The corrections must be done by the individual disciplines, leading to an iterative 
process with little progress between the iteration steps. To deal with increasing com-
plexity of development processes of active systems typically two different approaches 
are used. 
The first approach is to analyze the systems to be developed with respect to their 
elements, the allocation of the elements to the various disciplines and their mutual 
functional dependencies. A method that is often used in this context is systems engi-
neering. Systems engineering is based on the concept of systems theory. According to 
the approach of systems theory, a complex system is structured by the decomposition 
into various elements as well as the documentation of the relations between the sys-
tem elements [8]. 
In this context also a number of methods for managing the complexity of system 
models exist. As an example, the Dependency Structure Matrix (DSM) is mentioned. 
The DSM provides a systematic mapping of system elements and their relationship 
[9]. Through the use and analysis of the DSM, in particular indirect relations between 
system elements are identified. 
The second approach is to increase the communication and the alignment of results 
between the disciplines through the use of additional tools. As an example the cross 
disciplinary integration platform developed by Bellalouna is mentioned [10]. Here, a 
product data management system is used as a platform to centralize the data-sets of 
the various disciplines. The connection to the discipline specific data management 
solutions is accomplished by the use of a service oriented architecture (SOA).  
Gräb describes a mechatronic product development process on the basis of parame-
tric product models. Thereby the discipline-specific results can be matched by the use 
a common parameter structure [11].  
In this paper a meta-data-model for an integrated development environment as de-
scribed in the W-model is presented and the single packages referring to the different 
parts of the environment are described. 

224 
R. Nattermann and R. Anderl 
Furthermore a possible implementation of the development environment based on 
commercial PDM systems is explained. 
Concluding it can be stated, that the V-model results in a large number of iterations 
if applied on the development of highly structural integrated systems. In the past a 
number of research projects aimed to resolve this problem. Nevertheless there is cur-
rently no continuous methodology for integrated development of active systems, tak-
ing into account and addressing the increased capabilities of computer-based design 
methods. 
4 
The W-Model for the Development of Adaptronic Systems 
To counter the increasing degree of structural integration Nattermann and Anderl 
propose the W-model as a new process model for the development of active systems 
[12], shown in Figure 2. The W-model is based on the previous V-model and extends 
current developments in the field of mechatronic product development.  
Contrary to the three steps of the V-model the W-model consists of five steps 
which are explained briefly below. 
 
Fig. 2. W-Model for the development of adaptronic systems [12] 
System Analyzing. In the first step of the W Model, the system to be developed and 
the requirements, documented in the requirement specification, are analyzed. In  
addition, a first definition of the system states and the functions, based on the re-
quirements, as well as a breakdown of the system development tasks in the various 
disciplines is performed [6], [12]. 
Specific Solutions and Dependency Analysis. In the second step the W-model pro-
poses basic discipline-specific problem solving. This is performed in parallel and 
 

 
Meta-data-Model for the Development of Adaptronic Systems 
225 
independently of each other in the individual technical disciplines involved. Here the 
required product development methods are applied to define the discipline-specific 
solution space. 
Additionally, a network of dependencies between system-critical parameters and 
properties of the elements of the solution space is created [12]. 
Virtual System Integration. Following the definition of the discipline-specific  
solution space the integration and limitation of a cross-disciplinary solution space and 
the derivation of potential alternative solutions is developed. For the later version 
evaluation criteria based on the requirements are determined. 
For each solution variant then interdisciplinary, holistic system models are estab-
lished. These system models contain the atomic requirements, functions and system 
components, as well as their known interdependencies. After the creation of the sys-
tem model various techniques of complexity management can be applied, in particular 
to analyze the indirect dependencies between model elements. To support the later 
detailed hierarchical product structures are derived from the system model. 
System simulations are generated from the system model. These simulations are 
used for analyzing the global system's behavior on the basis of critical system charac-
teristics and parameters. The defined system simulations are bidirectionally linked 
with corresponding system model elements. 
Following Nattermann and Anderl, a specific development environment is used for 
the definition and documentation of the system models and system simulations, which 
is based on the functionality of the product data management [12]. 
Model Analysis and Detailed Development. The definition of the system model is 
followed by a detailed discipline-specific development. Therefore the hierarchical 
product structures, derived from the system model in the third step, are used. During 
detailed design the system model created in the third step model is used throughout 
the development process in order to perform a cross-disciplinary integration of the 
discipline-specific development results. This is performed e.g. by the application of 
appropriate parametric geometric modeling systems. 
Changes to data sets of individual disciplines can thereby be automatically trans-
ferred into the system model. In addition, the defined system simulations are used to 
analyze changes during discipline-specific development in relation to the overall  
system behavior and cross-discipline dependencies. Through the permanent virtual 
validation in the context of the entire system diverging development paths can be 
avoided [12]. 
System Integration. In the fifth step the W model describes system integration. Here, 
the integration with both physical (e.g. prototypes) and virtual results are provided, 
possibly to meet one validation milestone. Also a verification of integrated results in 
terms of requirements fulfillment takes place. 

226 
R. Nattermann and R. Anderl 
5 
Meta-data-Model 
In the following, the developed meta-data model for an integrated development envi-
ronment is presented. The meta-data model consists of several packages that represent 
the creation of system models, the derivation of hierarchical product structures, the 
definition of system simulation approaches and the creation of parameters-property-
networks.  
Package ActiveSystem. Central class of the package is the class ActiveSystem which 
describes the System combining data-sets involved in development. This class there-
fore provides compositional relationships with system models, system simulation 
approaches, and documents used for the development of components, functions and 
requirements. These are hierarchical structures in which the elements may include any 
sub-elements. The structural elements themselves are realizations of the information 
contained in the system model elements. It is important to preserve the relationship 
information contained in the system models during the conversion into hierarchical 
structures allowing traceability. 
 
Fig. 3. Package ActiveSystem 
Package SystemModel. The package SystemModel contains the definition of system 
models through their elements and their mutual relations. The package allows the 
definition of new system models as well as the import of existing system models. The 
package not only supports methods for analyzing system models with respect to  

 
Meta-data-Model for the Development of Adaptronic Systems 
227 
indirect dependencies but also includes methods for the control of model complexity 
described in [9]. Since the W-model proposes the linking of system models, system 
simulation approaches and discipline-specific documents by the use of parametric 
structures, the individual system model elements also combines links both to proper-
ties as well as parameters. In this context, parameters are values which are not depen-
dent on other values and freely modifiable (e.g. geometric dimensioning). Properties 
depend upon parameters or other properties and can be adjusted only by adaptions of 
parameters (e.g. volume) [13]. 
 
Fig. 4. Package SystemModel 
Package SystemSimulation. In the package SystemSimulation the definition of inte-
gration simulation approaches is described. The meta-data model supports the metho-
dology for the modularization of system simulations and the development of library 
modules for typical system elements developed by Herold et al. [14]. The input and 
output files of the simulation blocks are linked with the parameters and properties. It 
should be noted that the simulations don’t access the parameters and properties direct-
ly but via links. 
The relationship between these links and the actual parameters as well as the rela-
tionships between simulation approaches, parameters and properties are presented in 
the following package descriptions. 
Packages SystemParameters and SystemProperties. In the packages SystemParame-
ters and SystemProperties the meta-data-model represents the definition of a central 
parameter-property-network. Parameters and properties are explicitly distinguished in 
the meta-data model, as parameters with their values are not mutually dependent; 
properties on the other hand are depended solely on other properties and parameters 
[13]. In both cases, the meta-data-model provides the establishment of clusters to 
create groups of parameters properties belonging to the same system sections. 

228 
R. Nattermann and R. Anderl 
 
Fig. 5. Package SytemSimulation 
The access and use of the system parameters and properties is not possible directly 
but through the use of appropriate links synchronizing and controlling the access. This 
avoids redundancies when using a single parameter in multiple documents. 
 
Fig. 6. Package ParameterPropertyRelation 
Package ParamterPropertyRelation. The package represents the relationship be-
tween system properties and parameters. The meta-data model utilizes the approach 
of Property-Driven-Design, after which parameters are transferred into properties 
[13]. The meta-data-model allows the transfer of parameters into properties through 
documents as well as integrated simulations. Thereby simulations can be used to de-
termine system critical properties without having to access discipline-specific  

 
Meta-data-Model for the Development of Adaptronic Systems 
229 
documents. Thereby simultaneously with changes to properties and parameters, the 
impact on the global system behavior can be analyzed. 
6 
Implementation  
For the implementation of the presented meta-data model the commercial product 
data management system ENOVIA SmarTeam V5 was chosen to be used as the basis 
of the integrated development environment. The decision is in line with the proposed 
W-model allowing the application of an adapted product data management system. 
One advantage of this approach is that in the standard product data management func-
tionalities such as workflow, privileges and file management are already available and 
can be used. Currently, the adjustment of the data-model of the selected product data 
management system to the presented meta- data-model is developed. For this purpose, 
the classes of the meta-data-model are transferred in to the data management system 
and already existing classes and functionalities are adapted. 
For the implementation of the integrated simulation, the mathematical simulation 
solution MATLAB from Mathworks with the graphical modeling environment Simu-
link is used. The simulations are based on the concept of modular system simulations 
developed by Herold et al. [14]. 
After implementation of the meta-data-model, the development environment will 
be validated using a representative example. For this purpose an adaptronic engine 
bearing, developed in the LOEWE-Center AdRIA [15], is used. 
7 
Conclusion 
Adaptronic systems are characterized by the structural integration of electronic com-
ponents into mechanical structures. The mutual impact resulting from the high degree 
of structural integration must be considered in all phases of product development. It 
has been shown that existing process models for the development of mechatronic 
systems lead to issues with an increasing degree of structural integration. For this 
reason, the W-model for the development of active systems is developed by the De-
partment of Computer Integrated Design. The W-model is based on a continuously 
integrated development and proposes the use of a central integrated development 
environment. In the present work, the W-model was explained in detail and a meta-
data-model for a corresponding development environment is presented. The single 
packages of the meta-data-model were explained and an outlook on the currently 
ongoing implementation and validation is given. 
References 
1. Isermann, R.: Mechatronic systems: Fundamentals. Springer, London (2005) 
2. Hanselka, H., Bein, T., Breitbach, E., Krajenski, V.: Mechatronik/Adaptronik. In: Hering, 
E., Bein, T. (eds.) Grundwissen des Ingenieurs. Mit 265 Tabellen, Fachbuchverl, 13th edn. 
Leipzig im Carl Hanser Verl., München (2002) 

230 
R. Nattermann and R. Anderl 
3. Bein, T., Hanselka, H.: Das LOEWE-Zentrum AdRIA: Adaptronik - Research, Innovation, 
Application. 3. Konferenz des DVM - Arbeitskreises "Zuverlässigkeit mechatronischer 
und adaptronischer Systeme", Darmstadt (2010) 
4. Malzacher, J.: Leistungssystem zur integrativen virutellen Produktentwicklung adaptro-
nischer Systeme. Shaker, Aachen (2010) 
5. Bös, J., Hanselka, H.: LOEWE-Zentrum AdRIA - a multidisciplinary research project on 
the advancement of active systems. In: Proceedings of the 16th International Congress on 
Sound and Vibration, Kraków (2009) 
6. Verein deutscher Ingenieure: Entwicklungsmethodik für mechatronische Systeme. Beuth 
Verlag, Berlin (2206) (2004) 
7. ProSTEP iViP Documentation: Mechanic Process Integration (MPI), Darmstadt (2009) 
8. Daenzer, W.F., Haberfellner, R.: Systems engineering: Methodik und Praxis, 11th edn. 
Verl. Industrielle Organisation, Zürich (2002) 
9. Lindemann, U., Maurer, M., Braun, T.: Structural complexity management: An approach 
for the field of product design. Springer, Berlin (2009) 
10. Bellalouna, F.: Integrationsplattform für eine interdisziplinäre Entwicklung mechatro-
nischer Produkte. Shaker, Aachen (2009) 
11. Gräb, R.: Parametrische Integration von Produktmodellen für die Entwicklung mechatro-
nischer Produkte. Shaker, Aachen (2001) 
12. Nattermann, R., Anderl, R.: Simulation Data Management Approach for Developing 
Adaptronic Systems - The W-Model Methodology. In: Proceedings of the WASET 2011 
International Confernce on Software and Data Engineering (ICSDE), Bangkok, pp. 429–
435 (2011) 
13. Weber, C.: CPM / PDD - An extended theoretical approach to modelling prod-ucts and 
product development processes. In: Bley, H. (ed.) Advances in Methods and Systems for 
the Development of Products and Processes. Proceedings: 2. German-Israeli Symposium 
on Design and Manufacture, vol. 7, pp. 159–179. Fraunhofer IRB, Stuttgart (2005) 
14. Herold, S., Jungblut, T., Kurch, M.: A Systematic Approach To Simulate Active Mechani-
cal Structures. In: Multi-Disciplinary Simulations - The Future of Virtual Product Devel-
opment, Wiesbaden (2009) 
15. Kauba, M., Herold, S., Koch, T., Mayer, D., Melz, T.: Design and application of an active 
vibration control system for a marine engine mount. In: International Conference on Noise 
and Vibration Engineering, Leuven, pp. 241–255 (2008) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 231–241. 
DOI: 10.1007/978-3-642-30817-8_23 
© Springer-Verlag Berlin Heidelberg 2013 
 
Web Services to Product, Processes and Resources Data 
Integration: Results and Perspectives of FEDMAN Project 
Klaus Schützer1, Antonio Álvaro de Assis Moura 1, Reiner Anderl2, and André Picard2 
1 Laboratory for Computer Application in Design and Manufacturing, Methodist University of 
Piracicaba, Rod. Luis Ometto, Km 1, Santa Bárbara d'Oeste, 13451-900, Brazil 
2 Department of Computer Integrated Design, Darmstadt University of Technology,  
Petersenstr. 30, Darmstadt, 64287, Germany 
{schuetzer,atamoura}@unimep.br, 
{anderl,picard}@dik.tu-darmstadt.br 
Abstract. A production system can be treated at various levels beginning on 
components and going on machinery, plant, factory and even company. As for 
each level there are specific IT tools, every level necessarily implies an increase 
of the amount of data to be managed. To accomplish this integration of multiple 
sources the FEDMAN project uses the concept of federative databases and orga-
nizes this data in a model with three orthogonal domains: Product, Processes 
and Resources. The integration is based on a service-oriented architecture, 
which provides the access to specific information via web services requesting to 
the owner of information. This federative approach treats the data encapsulated 
in order to keep the partial autonomy of specific tools, without significant loss 
of flexibility. This model of integration is promising in the sense of integrating 
large amounts of data involved in a production system, maintaining the flexibil-
ity of each information technology tool. 
Keywords: Web Services, Data Integration, Federative Databases. 
1 
Introduction 
Nowadays the management of a production system can be considered as a distributed 
and collaborative activity. For collaborative means that activities over time are per-
formed by different actors, who must work in a common way. As these actors are 
scattered over both space and in relation to time this activity can be considered distri-
buted. The complexity arising from the inherent characteristics of any distributed 
activity can be managed through information systems, but there are still some re-
quirements that need to be improved, being the first data exchange between different 
systems since each of the actors’ process uses proprietary solutions, developed ac-
cording to specific needs, which adds data to the heterogeneous process. 
Consider each activity as a service and to structure these services within a service-
oriented architecture is a possible solution to handle the complexity and heterogenei-
ty, so that each stage of the requisite services and receive the information needed to 
progress. The service-oriented architecture is also shown as a solution for dealing 

232 
K. Schützer et al. 
with legacy systems, enabling the system to aggregate existing solutions still in use, 
but with the development and maintenance discontinued. Being services dedicated to 
the development process of the product available on the web, any actor's process can 
have access at any time and place and, therefore, reach the requirement to attend a 
distributed system. Web services are defined as software agents designed to work 
with interaction between networked computers with an interface described in a format 
processed by computers. Other systems can interact with the Web service using a 
message sent and received via a simple object access protocol (SOAP). Web services 
usually use hypertext transfer protocol (HTTP), with data described in extensible 
Markup Language (XML). 
This paper presents a proposal for integrating data from a production system 
through a service-based architecture (SOA), with the use of Web services to exchange 
messages written in XML and enveloped in SOAP. So that integration can take place 
in a distributed and collaborative environment dealing with the issues of flexibility 
and dynamism. 
2 
Production System Information Management 
Managing a production system represents a set of activities with collaborative and 
distributed issues, demanding enterprises that must be flexible and adaptable, and also 
demanding processes carried out in increasingly interrelated and independent of time 
and place in which they are applying the concepts of concurrent engineering and agile 
production [1]. These characteristics enable complex and innovative products availa-
ble to the market in increasingly short time. The systems that support these activities 
within the Product Development Process belong to PDM/CAD/CAM/CNC chain, and 
only for those systems there are different ways of data management. 
If all these systems are within the same container, the data can be stored in a single 
database, making the necessary information available to all systems. But even with all 
the effort of the major system developers to consolidate this model, adding to a mono-
lithic database with all necessary information, this is not the reality found in produc-
tion systems today. The phases existing in a production system are executed, each by 
experts in a highly distributed network and are therefore distributing the information 
pertaining to each system. The knowledge that involves production activity is spread 
across various systems and persons in different organizations, in different places, and 
you cannot leave aside this knowledge especially with the possibility now existing to 
access all this information. By involving several individuals and organizations, in 
which all has the control of a specific part of the process and no one have an absolute 
control of it, the production system is seen as a collaborative and distributed process, 
where each partner implements its own business process [2]. 
The framework of information technology that supports these processes should re-
flect this distributed environment, so ensuring the right information at the right time 
for decision making, regardless of which actor holds this information. 

 
Web Services to Product, Processes and Resources Data Integration 
233 
3 
Federative Databases for a Production System 
The scenario of a distributed production system can be understood as a complex sce-
nario, especially when taking into account that for each type of existing production 
systems or applications, they have their own characteristics. The product development 
also considers factors from external fields of knowledge which act by increasing the 
number of interactions and the variety of information. At the same time that is re-
quired of these systems to perform the management of information coordinately [3]. 
Another important consideration is about legacy systems, since historically produc-
tion systems working with autonomous systems that are often chosen for strategic 
criteria rather than for operational criteria [4]. These legacy systems should be incor-
porated into the product data management system allowing the aggregation of its 
information to the core model. Besides these factors, the presence of more than one 
factory in the same scenario, with different operating systems, only increases this 
complexity [4]. 
To obtain a framework to support product development that is more flexible, able 
to adapt quickly to market needs, responding to pressures such as launching new 
products with quality assurance, it is essential to develop information technology 
tools capable of working with high volume of data in a decentralized environment. 
The data generated by several and independent information technology tools are 
heterogeneous because each application has its isolated autonomy to generate and to 
manage data. The integration of these data requires the resolution of the conflict be-
tween transformation and heterogeneity of documents and data sources into an inte-
grated concept [5]. Even though the data management systems like PDM and ERP are 
fairly matured, however the collaboration between these systems does not present the 
same level of development. All domains involved in product development have to 
collaborate with the central model; currently the data integration problems arise and 
must be solved. 
The distributed data management in a collaborative manner can be done in three 
different ways according to [6, 7, 8] as shown in Figure 1. 
The maintenance of data in isolated applications ensures a high degree of distribu-
tion, and preserves the autonomy of data that can be handled independently by each 
application. Each application, in a isolated way, is responsible for the coupling to the 
central product model. This model has little flexibility and low degree of integration 
due to the implicit coupling between the tools of information technology that make up 
the central model. One difficulty with this model is the way to deal with the data of 
coupled systems, which require a specific application for each insertion to the central 
model. 
Another option is the option for a fully integrated model, where the data of the par-
tial models are integrated into the core model, generating a monolithic database. In 
this model there is no data distribution, since all applications are integrated and the 
connection to the central model occurs through direct links. As with the previous 
model, this does not present much flexibility due to the fact that every change in an 
application, or a partial model, impacts on change in central model, which is a further 
 

234 
K. Schützer et al. 
difficulty, especially when dealing with existing systems. The high degree of interac-
tion allows for total control of the system, but the loss of autonomy, because the ap-
plications are no longer responsible for generating or manipulating data on partial 
models. 
 
Fig. 1. Data management models [6, 7, 8] 
The intermediate solution between the two models presented is to allow data to be 
distributed in isolated applications, but the construction of the central model is made 
by means of loosing couples. Thus, the characteristic of autonomy for managing the 
data for each application is maintained, while preserves the consistence of the central 
model. This intermediate solution keeps the independence of each application and 
makes every part collaborate with all without losing its own features [3]. 
4 
Applications Developed for the FEDMAN Project 
This paper considers the problems generated by information management of a produc-
tion process, even if generated from different sources, in different formats, in large 
quantities and relationships in n-dimensional, can be reduced to simple problems. The 
information applications can be decomposed, analyzed and synthesized keeping an 
order, which is nothing more than the assumption of the scientific method. 
Based on a theoretical model of federative database applications were developed a 
model linked to current commercial systems and also linked with web services for the 
domains of product, processes and services. An application was developed for the 
demonstration of the federative management and integration of information. 
Applications for CAD and CAM systems are developed using the NX Open appli-
cation programming interface library, provided by Siemens PLM to access the func-
tions of computer-aided systems. Web services are developed using the language and 

 
Web Services to Product, Processes and Resources Data Integration 
235 
tools of ASP.NET inside of .Net-Framework, within the paradigm of object orienta-
tion. The services are made available through an Internet Information Server. 
A prototype implementation for the integration of product, processes and resources 
through Web services was developed using the layers concept. In the innermost layer 
were developed applications within CAD and CAM systems for product data and 
process data, respectively. For the availability of data in the Web services, a layer 
Web services was developed for reading and analyzing the data available on the Web 
to answer requests to the application that makes the role of integrating all data of a 
production system. Finally, the application developed is also able to request to web 
services data of product, process and resources [9, 10]. A scheme of internal applica-
tions, web services and application integration is presented in Figure 2. 
All development was done using the .NET-Framework within the paradigm of ob-
ject orientation. In this development the Web services environment, such as internal 
applications to CAD and CAM systems, as well as the application and services are 
implemented in ASP.NET and VB.NET applications. 
 
Fig. 2. Schema for Applications and Web Services 
4.1 
Add-On in CAx Systems 
Computer Aided Systems are available for several areas of engineering. The devel-
opment of these systems follows the general pace of development of information 
technology, especially for support systems to design and manufacture, new versions 
are released annually. Such speed renovation makes it a common practice of the end 
consumer not to update all versions, but waiting for the deployment of two or more 
updates. 
These releases meet the demands of consumers, which, however, may have a lack 
for very specific needs, so that to make possible a more specialized support, computer 
systems are developed allowing applications that work internally. These system appli-
cations are known as "Add-on". 

236 
K. Schützer et al. 
From the original system it is possible to start the application that can access  
the internal functions of the system using the same user interface to make the use of 
the application quite friendly. For the demonstration of the concepts proposed in  
this paper applications for managing product releases and manufacturing are  
developed. 
As the system is used for demonstration modules integrating CAD and CAM are 
developed using a common interface for both systems, which also includes the  
features for estimating the time of a machining operation and the determination of 
measuring points and vectors for a Coordinate Measuring Machine. The FEDMAN 
functionality is included in the title bar of the program, which opens the possibility of 
release management and consequently opens a dialog box for the functions described, 
as can be seen in Figure 3. 
 
Fig. 3. Application Dialog Box 
This dialog box is divided into three blocks: Processes; Product and Extras. 
The product functionality interacts with the Web service that provides access to the 
federative databases’ information, including the geometrical model and material of 
each component, its location in the coordinate system, the material, and the total 
amount of the same part in the assembly. The file maintains the hierarchical structure 
of the assembly. 
The processes functionality, inside of CAM System, can interact with a Web ser-
vice and provide the necessary information tools for the manufacturing process and 
also provide the specific machining processes for each component. 

 
Web Services to Product, Processes and Resources Data Integration 
237 
4.2 
CAD Application 
The application for the CAD system that provides product information is accessible in 
the CAD System via the button "Release Assembly Parts", inserted in the "Product", 
as showed in Figure 3. This button activates a method to collect in their CAD product 
components, including - at this stage of development - the position, material and the 
amount of times the component appears in the assembly. These are then compiled into 
an XML file and sent to a distributed database via a Web service. 
The CAD system keeps the product structure that has a role in the integration of 
the systems, because it defines the physical relationship between modules and com-
ponents comprising the product [11]. The links between the product components are 
stored in this structure, which may contain sets and subsets of chained mode, the 
structure can be presented in the form of tree or indented list [12]. 
To guarantee that all levels of the tree construction are read and the information in 
the product structure is kept, a recursive method is designed which gets the data from 
child components to its innermost level. the method continues to search for child 
components into the lowest level, when data is collected and made available in an 
XML object. All these objects are transferred to a XML file using its own methods of 
.Net platform and a file XML is created and transferred to a distributed database. 
4.3 
CAM Application 
The information system of Computer Aided Manufacturing are within the scope of 
information regarding processes, considered here as manufacturing processes, such as 
drilling, tapping, milling, etc.. Also information concerning the resources can be ob-
tained, since the CAM system contains information about the equipment used in the 
manufacturing process. 
Likewise the internal application of the CAD system, this application is accessible 
via the button "Release Operations" within group "Resources" (Figure 3). When 
pressing this button a sweep is made of all machining processes envisaged for this 
part. The transactions are then compiled into an XML file which is sent in turn to a 
distributed database. The step of transmission to the database is made via a Web  
Service.  
Each operation is linked to a specific tool. The CAM system provides information 
on the operation being one of the options that can open the dialog window that shows 
the information about the tool. The system stores the CAM operations within an ob-
ject of type collection of objects, which are the information for each operation and its 
characteristics. The application gets access to that object and searches the information 
by doing a scan all operations therein. 
The XML file generated and sent to the federative database has a list like structure 
containing the attributes of the operation and tool used, indicating for example: the 
diameter and length. Because of its extensibility, other transactions can be entered 
without prejudice to the operation of the system. Once transferred to the federative 
database is reported to the user a dialog box informing the operation’s success. 

238 
K. Schützer et al. 
4.4 
Product Data Analysis and Recovering 
The product data information are in the Web in a XML format file, the access to this 
information is made through a Web service that can return the requested data. The 
product data available through the CAD system are stored in the Web service which 
offers six functions for product data recovery. 
It is possible with these functions to access the product information necessary for 
integrating the fields of product, processes and resources. For example, to obtain in-
formation of the material of a existing component in an assembled a set up the follow-
ing sequence of operations is used: “AskAllPartsNameInAssembly” (to get the name 
of the components) and “AskPartMaterial” (to get the material with the component 
name). 
The answer to these functions is an XML file with the names of the components, 
with component names. It must be pointed that all communication with the web ser-
vice is made through objects encapsulated by the SOAP protocol, both the name of 
the component to which you want to know the material and the response of the  
material from which the component is made. However, all this communication is 
transparent to the user of the service. 
4.5 
Processes and Resources Data Analysis and Recovering 
The process data are stored in an XML file in Web for retrieval and the information 
contained therein is accessed through a web service with other six specific functions. 
With these functions it is possible to access the complete information of the 
processes. For example, to know what kind of an operation defined for a product you 
must first obtain a list of all existing operations. This is done via the “AskAllOpera-
tionsNames” which returns an XML file with the list of operations. This information 
can now be obtained with the use of the function “AskOperationType” to get the type 
of operation, using the operation’s name as a parameter. All communication is done 
with the information described in XML and enveloped in a simple object protocol 
(SOAP). 
The resources considered here are the tools used in machining processes. The in-
formation about the tools were obtained directly from the CAM system for a specific 
application and transferred to the Web in a file in XML format. For these functions is 
required to provide the name of the tool you want to get the information. These func-
tions can be verified in any browser with internet access. 
4.6 
Data Integration Browser 
The integration of product data, processes and resources is done through an applica-
tion that accesses Web services and enables the creation of links between the three 
domains. This application was developed in VB.Net, within the .Net platform, in the 
same way that other software components. 
The application displays in the upper left three buttons of type radio to indicate the 
relationship between the three domains included. Once chosen the two domains is 

 
Web Services to Product, Processes and Resources Data Integration 
239 
possible to request data from the federative database through the button "Request 
Web Service". Figure 4 shows in the first column, the result of data relating to the 
components of the pneumatic cylinder designed in the CAD system. Informing the 
number of times the component appears and the name of the component, in the first 
line are presented the tools listed in the CAM file. 
The option to view the resource domains and processes is possible in the same pro-
cedure. In this case the tools are displayed in the same way as the previous machining 
processes and are listed in the first column. 
 
Fig. 4. Browser Screenshot with product data and resources 
At any time the designer can create a link between product and process, or product 
and resources or resources to processes. These links are possible by the radio button 
shown in the windows above. The links can be archived and retrieved later. 
5 
Conclusions 
The amount of information involved in a production system is growing and no indica-
tions that this process will stagnate or even fall back, on the contrary. 
The concept proposed in this work is a promising alternative for the management 
of this large amount of data layers defining and structuring services. It also meets the 
requirements for incorporation of legacy systems, allowing applications already dis-
continued to be inserted into a central model. 
The mentioned services maintain the possibility of constant updating, which en-
sures the end user to always obtain updated information. Due to this model be done in 
layers in case of services update there are no need to changes in other layers. 
A scenario of multiple networks of customers and suppliers can be managed with 
this model through the implementation of various services. This scenario maintain 
compatibility with legacy systems and the possibility of updates. 

240 
K. Schützer et al. 
6 
Future Trends 
Considerations about information security should be aggregated to this proposal. Just 
as a managing roles and visions for each of the user, who have access to information. 
Security issues should involve both the permission to access and ensuring correct 
usage. These two topics are present in the second phase of the project FEDMAN as 
well as an improved and increased application portability considering the use of  
mobile devices. 
The use of mobile devices is growing and allows greater freedom of location, 
commercial applications for smartphones and tablets are available constantly and it is 
natural that this advance also covers applications related to production systems. 
Another interesting possibility is the application of semantic concepts to enable the 
use of inference engines and artificial intelligence for information retrieval. 
Acknowledgements. The FEDMAN project is funded by the DFG (Deutsche 
Forschungsgemeinschaft), by CAPES (Coordenação de Aperfeiçoamento de Pessoal 
de Nível Superior), by FINEP (Financiadora de Estudos e Projetos) and by CNPq 
(Conselho Nacional de Desenvolvimento Científico e Tecnológico) within the 
program BRAGECRIM. 
References 
1. Godinho Filho, M., Fernandes, F.C.F.: Paradigmas Estratégicos de Gestão da Manufatura 
(PEGEMs): elementos-chave e modelo conceitual. Gestão & Produção 12(3) (December 
2005) 
2. Teófilo, A., Silva, A.R.D.: CBPEL - Linguagem para definição de processos de negócio 
interorganizacionais Descrição de processos interorganizacionais. In: XML: Aplicações e 
Tecnologias Associadas, pp. 155–166 (2005) 
3. Schwarzenbacher, K., Wagner, J.: The Federative Principle in Business Architecture 2 
Business Architecture: Vision and Reality. In: Interoperability of Enterprise Software and 
Applications, pp. 567–579 (2005) 
4. Masson, C.: SOA can light up manufacturing IT. Computer Weekly (2006) 
http://www.computerweekly.com/Articles/2006/11/09/219795/ 
soa-can-light-up-manufacturing-it.htm (accessed: July 27, 2012)  
5. Lubell, J., Peak, R.S., Srinivasan, V., Waterbury, S.C.: STEP, XML, and UML: Comple-
mentary Technologies. In: 24th Computers and Information in Engineering Conference, 
vol. 4, pp. 915–923 (2004) 
6. Abeln, O.: Innovationspotentiale in der Produktentwicklung: Das CAD-Referenzmodell in 
der Praxis. Teubner Verlag, Stuttgart (1997)  
7. Montau, R.: Föderatives Produktdatenmanagement anhand semantischer Informationsmo-
dellierung. VDI Verlag, Düsseldorf (1995) 
8. Pedersen, S.: Interoperabilität heterogener Informationsquellen im Gesundheitswesen auf 
Grundlage von Standards für die medizinische Kommunikation und Dokumentation Dis-
sertation Susanne Pedersen. Universität Oldenburg (2005) 

 
Web Services to Product, Processes and Resources Data Integration 
241 
9. Schützer, K., Moura, A.Á.A., Anderl, R., Mosch, C.: Implemented Web Services for Data 
Integration in a Federative Environment. In: 21st International Congress of Mechanical 
Engineering, COBEM (2011) 
10. Mosch, C., Anderl, R., Moura, A.Á.A., Schützer, K.: Prototype of a Federative Factory 
Data Management for the Support of Factory Planning Processes. World Academy of 
Science, Engineering and Technology, 237–241 (2011) 
11. Schuh, G., Rozenfeld, H., Assmus, D., Zancul, E.D.S.: Process oriented framework to sup-
port PLM implementation. Computers in Industry 59(2-3), 210–218 (2008) 
12. Laurindo, F.J.B., Mesquita, M.A.D.: Material Requirements Planning: 25 Anos de Historia 
- Uma Revisão do Passado e Prospecção do Futuro. Gestão & Produção 7(3), 320–337 
(2000) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 243–251. 
DOI: 10.1007/978-3-642-30817-8_24 
© Springer-Verlag Berlin Heidelberg 2013 
 
Product Lifecycle Management Functional Reference 
Model for Software Support 
Eduardo Zancul1, Luiz Piccini1, Sophia Berglehner2, and Lena Lachenmaier3 
1 Production Engineering Department, University of Sao Paulo, Av. Prof. Almeida Prado, 
trav.2, 128, Sao Paulo, 05508-070, Brazil 
ezancul@usp.br 
2 Natural Sciences and Industrial Engineering, Hochschule Deggendorf, Edlmairstraße 6 und 8, 
Deggendorf, 94469, Germany 
3 International Technical Sales, Aalen University, Beethovenstraße 1, Aalen, 73430, Germany 
Abstract. The adoption of product lifecycle management (PLM) approach re-
quires the selection and implementation of IT systems. Currently, there is a wide 
variety of IT systems focused on PLM available in the market. The problem that 
motivates this research is the difficulty faced by manufacturing companies to eva-
luate and to compare existing PLM systems available commercially, in order to 
select the most appropriate alternative to their business processes needs. At aca-
demia, the main problem is the lack of a widely accepted definition of the soft-
ware functions that should be considered within the scope of PLM. This paper 
presents a PLM functional reference model for software support. The reference 
model is comprehensive and neutral, establishing a common base to facilitate 
software comparison and thus support software selection at manufacturing com-
panies. Additionally, the presented reference model establishes a generic defini-
tion of PLM to be discussed at academy and industry.  
Keywords: Product Lifecycle Management, PLM System, Reference Model. 
1 
Introduction 
Manufacturing companies have been adopting Product Lifecycle Management (PLM) 
approach to increase productivity and effectiveness of product development process 
and to improve the management of product data along the entire product life cycle. 
PLM improves information distribution and flows, supporting better decision making 
on early stages of the product development. For instance, product designers can better 
access field and service data from existing products in order to improve new product 
generations. Moreover, PLM enables management of product data until end of life, 
supporting extended services like continuous monitoring and maintenance. 
The adoption of product lifecycle management business approach requires the se-
lection, implementation and application of IT systems. Currently, there is a wide va-
riety of systems focused on PLM available commercially in the market. The existing 
systems stem mainly from three different software categories: evolution of CAD 
(Computer Aided Design) systems, pure player PDM (Product Data Management) 

244 
E. Zancul et al. 
provider and expansion of ERP (Enterprise Resource Planning) systems. The hetero-
geneous stems and market focus of each software house resulted in systems that have 
different functional capabilities. 
In this scenario, scope differences among existing software hamper the selection of 
IT systems to support PLM at manufacturing companies. The problem that motivates 
this research is the difficulty faced by manufacturing companies to evaluate and to 
compare existing PLM systems available in the market, in order to select the most 
appropriate alternative to fit their business processes needs. At the same time, for 
academic research, the main problem is the lack of a widely accepted theoretical defi-
nition of the software functions that should be considered within the scope of PLM. 
The aim of this paper is to present a PLM functional reference model for software 
support. The reference model is comprehensive and neutral from any software vendor, 
establishing a common base to facilitate software comparison and thus support soft-
ware selection at manufacturing companies. Additionally, the presented reference 
model establishes a generic definition of PLM to be discussed at academy and indus-
try. The model presented on this paper is an evolution of previous work conducted 
since 2006 [1-4]. 
The main expected applications for the model are: serve as the basis for the com-
parison of different commercial systems on selection processes at the industry; contri-
bute to the definition of the theoretical scope of PLM systems; serve as reference for 
software companies to prioritize the inclusion of new features in their systems. 
This article is organized in five sections. Section 2 provides information on litera-
ture review and field research about current software capabilities. Section 3 discusses 
the definition of reference model considered in this paper and presents the research 
method applied to construct the proposed PLM model. Section 4 presents the  
proposed reference model. Finally, section 5 discusses the conclusions and future 
research. 
2 
PLM Definition and Current Software Capabilities 
2.1 
PLM Definition 
PLM is defined as an approach for the integrated management of all product related 
information and processes through the entire lifecycle, from the initial idea to end-of-
life [5], [6]. Most authors currently agree that PLM does not only refer to individual 
computer software, but, moreover, it is related to a broad management concept which 
depends on the integration of multiple software components [5-8]. 
Based on this conceptual foundation, in this research PLM is defined as a business 
approach for the integrated management of business process and information related 
to products through the entire lifecycle. This approach requires integrated information 
systems to support collaboration over the extend enterprise throughout the product 
lifecycle [3]. 

 
Product Lifecycle Management Functional Reference Model for Software Support 
245 
PLM definition focusing on business process and information related with products 
allows the definition of a process scope, which includes but is not limited to the fol-
lowing business processes: 
• Innovation planning; 
• Proposal development; 
• Technology development; 
• Product and service development; 
• Middle-of-life product updates; 
• End-of-live product transformation. 
 
In order to support theses business processes, software vendors offer PLM solutions 
described in the next section. 
2.2 
PLM Current Software Capabilities 
PLM systems have been evolving within the limits set by other business applications. 
The boundaries of PLM systems are delimited with: CRM (Customer Relationship 
Management), used to manage customer data; SCM (Supply Chain Management), 
focused in production management and logistics; and ERP (Enterprise Resource 
Planning), with broad scope, including finance and HR management. 
Within these boundaries well recognized in the software market, PLM innovates in 
defining the product as the central element that is used to aggregate information from 
various sources. Moreover, PLM considers the complete lifecycle as the time dimen-
sion used to integrate information. As a result, any product information can be  
accessed directly by every authorized person at any time [3], [4]. 
The analysis of current PLM software capabilities involved data gathering from ten 
relevant software suppliers available in the market (PTC Windchill, Siemens  
Teamcenter, Dassault Enovia, Oracle Agile, SAP PLM, IBM PLM, Audros, SofTech, 
ProFile Procad and Autodesk PLM 300). 
The analysis shows that besides typical PDM functions, such as document man-
agement and product structuring, PLM has evolved to encompass other functions, 
including functions in the field of product management (e.g. requirements manage-
ment), service and maintenance. 
3 
PLM Functional Reference Model Construction 
3.1 
Reference Model Definition and Types 
The documentation of business processes may be performed by process models. A 
model is a representation of reality, usually with graphical notation, which describes 
the operation of processes in a logical, schematic manner. There are several methods 
for designing process models, and the degree of detail of a process model depends on 
the objective considered. Regardless of the possible semantic and notation variations, 

246 
E. Zancul et al. 
process models usually represent the following aspects: activities and their sequence, 
input and output information for each activity (information flow), organizational areas 
responsible for conducting each activity, and the resources used to perform the activi-
ties (e.g. a function of an information system). 
A special class of business processes models is made up of more comprehensive 
models of wide application and benchmark character, called reference models. 
Reference models of business processes are representations of business processes 
containing best practices. In addition, reference models are generic, so that they can 
reflect the reality found in various companies and various business situations. This 
allows reference models to be adapted for application in different contexts [9-10]. 
The specification of reference models can occur in two alternative forms. A refer-
ence model can be created inductively, based on the compilation of knowledge of 
several empirical cases and information systems. Alternatively, reference models can 
be deduced from the theory [9], [11]. 
In terms of application, reference models can be configured in specific models. The 
instantiation of a particular model from a reference model helps ensure that best practic-
es are considered into the resulting model. With the use of reference models, it is ex-
pected that the deployment of specific models is faster and that the result is of better 
quality. Another possible application for reference models is in the evaluation of specif-
ic models. In this situation, the reference model provides a basis of comparison for iden-
tifying problems and opportunities for improvement in specific model [9], [12]. 
In addition to the reference models of business processes, there are other types of 
reference models. In the context of this paper, the reference models of information 
systems are relevant. These models represent the functions available in an information 
system [13-14]. 
3.2 
Research Method 
Research method involved four phases. First, PLM scope and its boundaries were 
defined considering PLM definitions on literature and requirements of business 
process related to PLM. Second, ten systems commercially available were researched 
and analyzed. Data gathering occurred in the second half of 2012. Considering the 
boundaries set on previous phase of the research, software functions identified in 
commercial solutions were grouped in modules according to their similarities in 
scope. Third, a three levels hierarchy was defined to organize the functions in mod-
ules (level 1), functional groups (level 2) and functions (level 3). On the last phase, a 
tabular description of the whole model was prepared. 
4 
PLM Functional Model for Software Support 
The PLM system reference model focuses on the functions used to support compa-
nies’ business processes. It is, therefore, a reference model of systems´ functionalities. 
Other aspects of PLM systems, such as supplier characteristics (e.g. size, geographical 
coverage, financial performance) are not considered. 

 
Product Lifecycle Management Functional Reference Model for Software Support 
247 
Regarding its structure, the reference model of PLM systems is organized into 
three levels of detail: modules, groups of functions and functions. At the first level of 
detail, the model consists of 9 modules, illustrated by Fig 1. 
 
 
Fig. 1. PLM reference model modules (model level 1) 
The layout of Fig. 1 indicates modules’ role within the PLM system. On the left 
hand side of the figure, are displayed modules related to the planning and manage-
ment of product life cycle. In top and bottom of Fig. 2, are displayed the modules 
related to functions that cover the whole lifecycle: document management and colla-
boration support. In the middle of Fig 1. is located the product structuring module, 
responsible to central data management, and modules focused on product data crea-
tion – manufacturing process, quality and sustainability. Finally, on the right hand 
side is located the service module, related to the downstream processes and activities 
of the product lifecycle. 
The second view shows the modules and groups of functions (Table 1). 
Table 1. Modules and corresponding group of functions of the PLM reference model 
Level 1 - Modules 
Level 2 – Group of functions 
1. Product management 
Ideas management 
Requirements management 
Portfolio management 
2. Project management 
Project planning 
Project control 
3. Product structuring 
Parts management 
Classification 
Bill of materials 
Variant management 
Product cost 
Engineering change management 
1. Product
management
2. Project 
management
4. Document management
9. Collaboration
5. 
Manufacturing
process
planning
6. Quality
management
7. 
Sustainability
8. Services
3. Product structuring

248 
E. Zancul et al. 
Table 1. (continued) 
4. Document management 
Documents records 
Visualization 
Technical documentation  
5. Manufacturing process planning 
Tools and resources management 
Manufacturing process planning 
6. Quality management 
Quality methods (e.g. FMEA) 
Quality control planning 
Quality control and actions 
Customer complaints 
7. Sustainability 
Materials catalogue 
Lifecycle assessment  
Regulatory compliance 
8. Service 
Service parts management 
Service and maintenance plans 
Service and maintenance execution/records 
9. Collaboration 
Workflow 
Virtual workspaces 
Online meeting 
 
The content of each module is summarized below. 
Product Management. This module comprises functions that support planning and 
management of the product offering. It embraces functions concerning ideas, re-
quirements and portfolio management. 
Project Management. Includes functions needed to plan projects (e.g. activities, 
resource allocation, timeframe) and to control project execution (e.g. activities com-
pletion, budget). 
Product Structuring. Product structuring module comprises functions which relate 
to the product engineering, allowing the user to create and manage items and prod-
ucts, including product variants that will exist through the lifecycle. The groups of 
functions within this module are parts management, classification, bill of materials, 
variant management, product cost and engineering change management. 
Document Management. This module encompasses functions needed to manage 
every document created within the lifecycle. System manages document meta-data 
and controls access rights and versioning. Visualization functions allows document 
viewing and collaboration by multiple users. Technical documentation group of func-
tions support preparation of service and support manuals. 
Manufacturing Process Planning. This module refers to the production process, 
supporting users by management of resources needed to manufacture the products, 

 
Product Lifecycle Management Functional Reference Model for Software Support 
249 
including machine and tools that are applied in the process. Moreover, the module 
includes functions to support manufacturing process definition. 
Quality Management. The object of quality management module includes quality 
methods, like FMEA (Failure Mode and Effects Analysis). Also the functions quality 
planning and quality control and actions are considered. Furthermore, customer com-
plaints are registered in the system related to specific items or products to be accessed 
by other areas of the company, helping to close the information loop within the prod-
uct lifecycle.  
Sustainability. Comprises the management of hazardous materials through specific 
materials catalogues. Furthermore the environmental impact of the whole product 
lifecycle may be analyzed. Finally regulatory compliance management supports users 
to deal with regulatory documentation. 
Service. In order to guarantee product consistency, it is necessary to provide services 
through the whole lifecycle. To enable this, the PLM software provides the tools to 
support this task, which are organized in three groups of functions: service parts man-
agement, service and maintenance plans and service and maintenance execution / 
records management. 
Collaboration. Workflow management, virtual workspaces and online meetings en-
hance teamwork and collaboration in the extended enterprise. 
Finally, the third view of the model in tabular format comprises all the three levels 
of detail. Table 2 illustrates this view including module, group of functions and func-
tions for one specific group of functions, namely Documents records of Document 
management module. 
Table 2. Example of the third view of the model encompassing module, groups of functions 
and functions – Documents records of Document management 
Level 1 - Mod-
ules 
Level 2 – Group 
of functions 
Level 3 - Functions 
4. Document  
management 
Documents  
records 
Define document types and their characteris-
tics
Create document and generate document ID 
Create document based on template 
Include document metadata
Define document structure
Classify documents based on classification 
scheme
Search for documents
Check-in and check-out documents 
Control documents version and status 
View document history

250 
E. Zancul et al. 
5 
Conclusions 
This paper presents an updated version of the PLM functional reference model for 
software support. The model characteristics – vendor neutral and it´s hierarchic  
structure in three levels (modules, group of functions and functions) – is intended to 
facilitate comparison with commercially available software in software selection pro-
cedures at the industry, defining a benchmark to compare the different systems in the 
market. 
Moreover, PLM functional reference model contributes creating a common reference 
of PLM systems to be discussed at the academy level. Last but not least, software sup-
pliers may also take advantage of the PLM reference model in order to identify gaps 
and define a development roadmap for the inclusion of new functionalities. 
Future research on this field includes a survey with experts from academy and in-
dustry to enhance the third level of the PLM reference model (functions) and case 
examples of the application of the presented model at industry to support software 
selection. 
Acknowledgments. The authors extends their sincere thanks to FAPESP (Fundação 
de Amparo à Pesquisa do Estado de São Paulo) for funding the research and to 
DAAD (German Academic Exchange Service) for RISE scholarships. 
References 
1. Assmus, D., Meier, J., Treutlein, P., Zancul, E.S.: Marktspiegel Business Software – PLM 
/ PDM 2006/2007. WZL Werkzeugmaschinenlabor der RWTH Aachen / Trovarit AG, Aa-
chen (2006)  
2. Schuh, G., Rozenfeld, H., Assmus, D., Zancul, E.: Process oriented framework to support 
PLM implementation. Computers in Industry 59(2-3), 210–218 (2008)  
3. Zancul, E.S.: Gestão do ciclo de vida de produtos: seleção de sistemas PLM com base em 
modelos de referência. Escola de Engenharia de São Carlos. Universidade de São Paulo, 
São Carlos (2009) 
4. Zancul, E. S.: PLM reference model: a preliminary proposal for reference model evolution. 
In: 9th International Conference on Product Lifecycle Management. Product Lifecycle 
Management: Towards Knowledge-Rich Enterprises, Montreal (2012)  
5. Saaksvuori, A., Immonen, A.: Product lifecycle management. Springer, Berlin (2004) 
6. Arnold, V., Dettmering, H., Engel, T., Karcher, A.: Product Lifecycle Management beherr-
schen: ein Anwenderhandbuch für den Mittelstand. Springer, Berlin (2005) 
7. Abramovici, M., Schulte, S.: Product Lifecycle Management – Logische Fortsetzung der 
PDM-Ansätze oder Neuauflage des CIM-Debakels? In: VDI-Berichte 1819: I2P-Integrierte 
Informationsverarbeitung in der Produktentstehung – (k)ein Gegensatz zwischen Innova-
tion und Kostensenkung, VDI (2004) 
8. Scheer, A.-W., Boczanski, M., Muth, M., Schmitz, W.G., Segelbacher, U.: Prozessorien-
tiertes Product Lifecycle Management. Springer, Berlin (2006) 
9. Schwegmann, A., Laske, M.: Istmodellierung und Istanalyse. In: Becker, J., Kugeler, M., 
Rosemann, M. (eds.) Prozessmanagement. Ein Leitfaden zur prozessorientierten Organisa-
tionsgestaltung, pp. 155–184. Springer, Berlin (2005) 

 
Product Lifecycle Management Functional Reference Model for Software Support 
251 
10. Fettke, P., Loos, P.: Using Reference Models for Business Engineering-State-of-the-Art 
and Future Developments. In: Innovations in Information Technology, pp. 1–5. IEEE, Du-
bai (2006) 
11. Fettke, P., Loos, P., Zwicker, J.: Business process reference models: Survey and classifica-
tion. In: 3rd International Conference on Business Process Management, pp. 469–483. 
Springer, Nancy (2005) 
12. Vernadat, F.B.: Enterprise Modeling and Intergration: principles and applications. Springer 
(1996) 
13. Curran, T., Keller, G., Ladd, A.: SAP R/3 business blueprint: understanding the business 
process reference model. Prentice-Hall, Upper Saddle River (1997) 
14. Keller, G., Teufel, T.: SAP R/3 Process Oriented Implementation. Addison-Wesley, Bos-
ton (1998) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 253–262. 
DOI: 10.1007/978-3-642-30817-8_25 
© Springer-Verlag Berlin Heidelberg 2013 
 
Design of Reconfigurable Automotive Framing System 
Abdo Al-Zaher, Waguih ElMaraghy*, Z.J. Pasek, and Hoda ElMaraghy 
Industrial and Manufacturing Systems Engineering 
University of Windsor, 401 Sunset Ave. Windsor, Ontario N9B 3P4, Canada 
{alzahea,wem,zjpasek,hae}@uwindsor.ca 
Abstract. This research introduces new systematic methods dealing with a 
complete end-to-end design process to production systems, where the uncertain-
ty of product variety is mapped to product attributes and manufacturing 
processes, then mapped into a manufacturing and production line using product 
decomposition into systems, sub-systems, and modular assembly. Graph net-
work (NW), change propagation index (CPI) and hybrid design structure matrix 
(HDSM) were introduced to: (1) establish connectivity between sub-systems 
(modules) before mapping design changes, (2) measure the degree of changes 
to the state of systems due to changes propagated through the entire systems, 
(3) estimate how much embedded flexibility is needed for these elements  
(design variables) to absorb future changes. A practical example of actual pro-
duction systems was presented. Hybrid Design Structure Matrix (HDSM) is 
used to transmit knowledge gained, detailing design of production systems. 
Keywords: Reconfigurable manufacturing systems, Automotive framing sys-
tems, Design methodology, Digital manufacturing. 
1 
Introduction 
Manufacturing enterprises are forced to reassess their production paradigms, so that 
their products can be designed to maximize potential achievable variants, while the 
manufacturing and production systems can be designed to operate efficiently by ro-
bustly accommodating future product changes (product upgrading), minimizing time 
to market and providing a reliable production base. Typically, more than 80 % of the 
tooling and equipment in a body shop are not specific to an individual model but can 
be used for all models produced. The hypothesis is that if the right subsets of car body 
elements (product- product family) and production capabilities are designed with 
proper care for future flexibility based on the flexibility and reconfigurability prin-
ciples, then the production system can better accommodate body styling changes, 
variants of family production without the need for tooling changeover with significant 
increase in throughputs. 
                                                           
*  Corresponding author. 

254 
A. Al-Zaher et al. 
1.1 
The State of Practice of Vehicle Framing Systems 
The ‘Framing system’ is a process and the related infrastructure for a precise position-
ing and securing under-body platform with the upper body components [4]. Most 
passenger vehicles made today have a (structural) body that comprises 100–150 
stamped metal panels, assembled into 18-22 modules (sub-assemblies) [1]. These 
modules are assembled into 3 systems to create the vehicle body formed (prior to 
painting, is referred to as body-in-white) in a multi-step manufacturing process, dur-
ing which the modules are joined together by welding. Overall, a vehicle framing 
system can be divided into three subsystems, as shown in Fig. 1:  
 
 
Fig. 1. Assembly Process of BIW Framing Systems- product decomposition 
During the framing process, all modules have to be position-ally secured with re-
spect to the under-body-platform and inner/outer skin, as only then the welds attach-
ing the roof skin can be placed. In most production systems, the time required for all 
these activities is set between 45 to 56 s, depending on the vehicle size (smaller times 
for smaller bodies). The time required for framing also determines the cycle time 
(CT) of the whole line, and hence causes a bottleneck operation.  
Historically, all current framing systems have evolved from the Open-Gate and 
Robo-Gate system, initially developed in the 1980’s. When a body style change is 
required, the changeover can be performed in an average amount of time, 60 - 90 
minutes. Otherwise, the operation of the gate is the same as described previously for 
Robo-Gate. The cost of lost production due to the changeover is estimated 70- 120 job 
with cost about 300.000 USD. Currently the automakers framing in North America, 
still using dedicated gates for each style at the final assembly see Fig. 2, to accommo-
date multiple styles gate storage mechanism were needed. As results of using dedicate 
gate systems for each style, there are key issues can be summarized as follows: (a) 
Changeover times have to be accommodated for in the production plan, as the line has 

 
Design of Reconfigurable Automotive Framing System 
255 
to stop running, (b) Gates storage systems require significant amount of floor space, 
(c) Overall high cost, high lead-times for engineering and build time, (d) Very expen-
sive to manufacture and maintain.  (e) High cost of lost production due to downtime 
(breakdown or retooling).   (f) High risk in capital investment. 
 
Fig. 2. Current practice of framing systems using dedicated gate with gate storage 
1.2 
The Goals of the Proposed Methods of the Framing Systems Are as 
Follows  
The goal of the proposed methods is 1) to create a common understanding between 
product developer and systems design to execute the engineering changes due to 
product upgrading (markets segmentation), 2) To use a common tool to evaluate 
changes in product and equipment of the assembly line And 3) Joining process as-
sumption needs to be made upfront to plan flexibility at the early stage prior to final 
design of production line see Fig. 3.  
Fig.4 shows the proposed Reconfigurable Open-Gate Framing Systems (ROGFS); 
new modular structure with embedded flexibility and reconfigurability for the open 
gate to uncouple the top units devices and rear units that correspond to geometry styl-
ing due to engineering changes. With the proposed framing systems, top units devices 
are programmable, design constrain must be clear for the product designer to stan-
dardize product features across a product family, such as pin diameter and orientation 
of tooling access  to secure the assembly (assembly details: non-functional features).  
 
Fig. 3. The proposed systems: to run more styles using same Gate  
 

256 
A. Al-Zaher et al. 
The main aim of concurrent engineering is to integrate product and process devel-
opment in order to reduce the design lead-time and improve the quality and cost.  
Design problems become complex due to multiple components such as tooling, posi-
tioning devices, transfer equipments, and joining process. In the last few years,  
markets increasingly require more customized products with shorter life cycles. In 
response, RMS systems have evolved from mass production techniques through flexi-
ble automation and mass customization, to produce at mass production costs [6][8]. 
 
 
Fig. 4. Proposed Reconfigurable Open Gate Framing Systems (ROGFS) 
2 
RMS Framework and the Design Methodology  
2.1 
The RMS Framework of Automotive Framing Systems (Proposed) 
The framework provides a guideline to support and to structure the different stages of 
the design methodology. This framework is mainly based on system life-cycle con-
cept. Fig. 6 shows the main 4 stages of proposed framework; first, 3 transitional stag-
es and lastly, the parallel stage: 
(1) Manufacturing systems analysis, (2) Manufacturing systems design, (3) Manu-
facturing systems operation & maintenance, (3’) is the reconfigurability stage or the 
life cycle extension of production systems, and (4) is the refine offline gate combined 
with manufacturing support centre (Teamcenter). A brief explanation for each  
stage is: 
 

 
Design of Reconfigurable Automotive Framing System 
257 
 
Fig. 5. Framework for RMS of Automotive framing systems 
Stage 1: Manufacturing Systems Analysis – Analyzing Stage 
The manufacturing systems analysis is the first stage of the life-cycle where the for-
mulation and definition of the manufacturing system is performed to satisfy specific 
needs. The main constraints at this stage are the manufacturing strategy, the characte-
ristics of the product, and the process. The automotive framing systems have complex 
product and processes; systems designers and product developers need to have a per-
fect knowledge of the decomposition and integration of all the modules and compo-
nents of each module (interface components and their process) in order to upgrade to 
new vehicle styling. 
Stage 2: Manufacturing Systems Design – Synthesizing Stage 
The manufacturing system design is the second stage of the manufacturing system 
life-cycle. The main elements of this stage are:  The main inputs for this stage are the 
requirements of the manufacturing system in terms of reconfigurability, which are the 
results of the assessment. 
Stage 3: Manufacturing systems operation & maintenance  
The third stage of the framework is the implementation or the launching of the manu-
facturing systems. Once the manufacturing systems are operating, it is important to 
establish operational matrices aligned with the design objective and the performance 
of the production line rate. 
Stage 3’:  Reconfigurability stage -- Evolvabilty and survivability of the systems  
Reconfigurability means enhancing the systems and extending the life-cycles of the 
systems [10].  
Stage 4: Refine offline gate – Evaluation and testing stage 
During the design activities, more detail is needed. The manufacturing characteristics 
such as product, operations, processes, and alternatives of layout are designed.  

258 
A. Al-Zaher et al. 
2.2 
The Design Structure Matrix (DSM) Used to Identify and Measure 
Interaction 
Design structure matrix (DSM-Component-Based Architecture) is used in the auto-
motive industries as a tool and technique [9]. It provides a simple, compact, and  
visual representation of a complex system that supports innovative solutions to de-
composition and integration problems. It is important to note that DSM models 
represent extensive system knowledge. Hence, DSMs can be difficult to build, espe-
cially initially, as they depict data that are not always at hand, easily gathered, or 
quickly assimilated. For a given change, it is important to establish how changes 
propagate throughout the systems. Fig. 6 (a), shows network (NW) representation of 
the systems that consist of eight modules M1 to M8, and shows how the final systems 
configuration is due to ∆X Changes in one element. Changes are applied M1, then 
changes are propagated throughout the systems.  The direction of changes is propa-
gated and classified based on the classification of element to changes by Eckert 
(2004). The classification were used to measure the degree of reaction in the system 
due to each of the changes of critical elements, there is a new metric called change 
propagation index (CPI) using equation (1). 
The question is how can these classifications be identified and quantified to help 
systems engineers create better flexible and reconfigurable production systems?  
 
(1)
 
 
Fig. 6. Change Propagation (CPI) Due to ∆X: Eng Changes (adapted from [7])   

 
Design of Reconfigurable Automotive Framing System 
259 
The rows and columns in DSM is equal to the number of modulus/components of the 
systems 8x8 as shown in Fig. 6 (b). CPI can be measured as follows; for each mod-
ules i.e. M3 receives one input from M1 and send 2 outputs to M2 and M5,  
CPI= ∆Eout, M3 - ∆Ein, M3= 2-1 =1 which is classified as a multiplier. The clas-
sifications for the rest of the system components are shown in Fig.6 (b). 
The challenge is that the designer needs to determine how to eliminate or reduce 
the impact of physical interaction between modules. Multiplier elements can be turned 
to absorber or carrier by building flexibility abounded [7]. The answer to this question 
is presented in the next section, new tooling introduced and used in the proposed me-
thodology. 
2.3 
The RMS Design Methodology of Automotive Framing Systems 
Lastly, the methodology is the integration of these four stages by the RMS design 
framework which is proposed to decompose each stage into activities to analyze, eva-
luate, and synthesize the inputs and outputs of each stage in order to design/ reconfi-
gure manufacturing system. Fig.7 shows the main stages of the methodology with 
PDM/DM; Teamcenter manufacturing support as a hub to the design methodology 
based on the proposed framework. Network graph representation and Design Struc-
ture Matrix (DSM - Component-Based Architecture) is used as an effective method 
for integrating low-level design processes based on physical design parameter rela-
tionships. Furthermore, it displays the relationship(s) between modules or components 
(which depends on the level of details) of a system in a compact, visual, and analyti-
cal format. It is important to note that DSM models represent extensive system know-
ledge. Concurrent processing to all steps using the new methods stated in three stages 
as follows: 
 
Fig. 7. PLM/DM is the hub of RMS design methodology 

260 
A. Al-Zaher et al. 
I):  The first stage is evaluating the impact of engineering changes:  
The evaluation of engineering changes (∆X) on the manufacturing and production 
systems; usually received as a complete kit of data - geometry changes, weld data and 
processes (simulation input). Network representation for car body structures is built as 
shown in Fig. 8. These links between modules represent joining process such as weld 
spots, arc welding, laser welding or any other methods of joining upper modules to 
the lower-body. The network graphical represents two types of physical connections: 
As shown in Fig. 8, connection between modules of upper-body (M2, M3 and M4) to 
lower-body (M1 assembly) easily identifies changes in propagation throughout the 
system.  Once the evaluation is completed, CPI can be calculated and then used to 
measure the physical interaction between modules assembly; the outputs of this stage 
are the identification of the key elements or components that need to have flexibility.  
 
 
Fig. 8. Network representation of car body structures BIW 
II):  The second stage is the system design:   
Start with conceptual design with virtual evaluation and make changes to the systems 
to achieve the design objective. New systems configuration HDSM used for mapping 
of product function to design configuration, as shown in Fig. 9; (two stages). The 
structure of HDSM is a high level representation of modules interaction of upper-
body (M2, M3 and M4) with lower-body (M1); changes are initiated in M2 which is 
represented by square matrix 7x7 ( the size depend on car body modules structures), 
M3 to the right, then M4 at the top (roof modules) and M1 (under-body complete). It 
is a straightforward mapping of low-level physical interaction as first stage indicates. 
The second stage is the evaluation for the positioning units (tooling) & joining 
processes (robot programs) of gate tooling using DM simulation as shown in Fig. 10. 
 
 

 
Design of Reconfigurable Automotive Framing System 
261 
III):  The third stage is the manufacturing processes:  
The output of this stage is the production data including tooling functionality, se-
quencing of operation, systems layout, cycle time for stations and robotics. Program-
ming and automation to run virtual simulation of the production was done using Del-
mia IGRIP.  Simulation outputs are used and can refine the design prior to 
building and installation of the production line.  
 
Fig. 9. HDSM key elements physical interaction for all modules 
 
Fig. 10. HDSM evaluation of changes in positioning units and robot programs 

262 
A. Al-Zaher et al. 
3 
Conclusions and Future Work 
The proposed design method shows how to systematically pinpoint and value flexible 
elements in modules, despite a 30-50% higher initial investment in equipment and 
tooling. In return, it offers significant savings of up to 50% for each new style addi-
tion. When simulation is connected to process design and builds, lead-time is reduced 
by 80%. This work is the first systematic attempt at mapping product design-driven 
flexibility into manufacturing system capabilities.   
HDSM, introduced in this paper as a new application of DSM to the automotive 
framing systems, can be classified as a special use in the automotive industry. The 
new configuration of DSM provides a new relationship perspective of dependency 
between modules. The use of this application helps in the following ways: a) To help 
systems designers understand changes in product design and mapping to physical 
domains (production systems) and to evaluate the effect of changes by connecting the 
process to DM simulation. b) Used to map engineering changes of car body structures 
to production systems; including positioning devices and joining process at the high-
est level  
The future work will aim to: Continue to develop more low level details for multi 
domain interface product design to production systems with more interactive  
simulation.  
References 
[1] Al-Zaher, A., ElMaraghy, W., Pasek, Z.J.: Robust and Cost-Effective Design of Automo-
tive Framing Systems Using Reconfigurability Principles. In: The 2nd Canadian Quality 
Congress, Toronto, ON, Canada (2010) 
[2] Al-Zaher, A., ElMaraghy, W., Pasek, Z.J.: Enabling Car Body Customization through 
Automotive Framing Systems Design. In: ElMaraghy, H.A. (ed.) Enabling Manufacturing 
Competitiveness and Economic Sustainability. Springer, Heidelberg (2011) 
[3] Automobilesreview.com. Body structure, 
http://www.automobilesreview.com/category/car-makes/holden/ 
(accessed) 
[4] Baulier, D.: Automotive Vehicle Framing Systems. Canada patent application 10/668,525 
(2006) 
[5] ElMaraghy, H.A.: Flexible and reconfigurable manufacturing systems paradigms. Inter-
national Journal of Flexible Manufacturing Systems 17, 261–276 (2005) 
[6] ElMaraghy, H.A.: Changeable and Reconfigurable Manufacturing Systems. Springer, 
London (2009) 
[7] Eun Suk, S., de Week, O.L., Chang, D.: Flexible product platforms: framework and case 
study. Research in Engineering Design 18, 67–89 (2007) 
[8] Koren, Y.: The Global Manufacturing Revolution: Product-Process-Business Integration 
and Reconfigurable Systems. Wiley (2010) 
[9] Kusiak, A.: Interface Structure Matrix for Analysis of Products and Processes. In: CIRP 
International Conference on Life Cycle Engineering (2008) 
[10] Siddiqi, A., de Weck, O.L.: Modeling methods and conceptual design principles for re-
configurable systems. Journal of Mechanical Design 130, 101102-1 (2008) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 263–272. 
DOI: 10.1007/978-3-642-30817-8_26 
© Springer-Verlag Berlin Heidelberg 2013 
 
Mechatronic Machine Elements: On Their Relevance  
in Cyber-Physical Systems  
Marius Stücheli and Mirko Meboldt 
Department Mechanical and Process Engineering, pd|z Product Development Group Zurich,  
ETH Zurich, 8092 Zurich, Switzerland 
{mstuecheli,meboldtm}@ethz.ch 
Abstract. This paper discusses the appearance of mechatronic machine ele-
ments (MME) in the greater context of cyber-physical systems (CPS). For this 
purpose it establishes classifications for CPS. Three groups of MME are identi-
fied, characterized and illustrated with examples. Regarding the advantages of 
MME, the text explains how they allow for new functions in mechanical sys-
tems and how they help to reduce the development effort for complex products. 
As desirable characteristics for MME in general are identified independent 
communication abilities, “plug-and-play” integration in networks and mechan-
ics and the preprocessing of sensor data on the element itself. CPS with  
specifically designed MME can further become a valuable tool in product  
development for information collection.  
Keywords: Cyber-Physical Systems, Mechatronics, Machine Elements,  
Product Development. 
1 
Introduction 
In 2006 a group of computer scientists opened a new research field under the term 
Cyber-Physical Systems (CPS), which was coined by Helen Gill [1]. Expressing the 
new understanding of embedded systems in the context of the interaction of  
computers with physical processes, it is an indicator of an ongoing development in 
engineering and computer science. At first mechanical products were enhanced with 
electronics and programmable computing units, what climaxed in the 1990ies in the 
rise of the field of mechatronics. Now networking is being added to the physical sys-
tems. Embedded and remote computers are being connected within a single machine 
and over large distances as to collectively gather and exchange data about the physical 
world. This new stage in integrating engineering disciplines requires new questions to 
be asked and new vocabulary to be developed.  
Lee and Seshia [1] define CPS as follows: “A cyber-physical system is an integra-
tion of computation with physical processes. Embedded computers and networks 
monitor and control the physical processes, usually with feedback loops where physi-
cal processes affect computations and vice versa.” 
Of course, many of the arising issues like network security, the computational han-
dling of concurring events, and so on, are mainly to be discussed and solved by com-
puter scientists. When it comes however to the contact point between the “cyber” and 

264 
M. Stücheli and M. Meboldt 
the “physical”, there are also issues in the mechanical engineering field that are well 
worth to be considered in the wider context of CPS. This also becomes clear from a 
position paper where Tabuada [2] presents a vision of a programmable sports car, able 
to emulate an arbitrary driving experience. The much one can argue about the tech-
nical feasibility of such a system, it is clear that developing such a system would ne-
cessitate new mechatronic solutions.  
One of the issues to be considered in this new context are machine elements. Most 
machine elements used today are well understood and applied for decades. Recent 
developments mostly concern increase in robustness, miniaturization or better preci-
sion. However, over the past twenty years a development has gained momentum, 
which is about to revolutionize the way, complex mechatronic products are designed. 
Even more this development will make new mechatronic designs possible. It is about 
the increasingly tight integration of sensing, actuating and even computing and net-
working functions with conventional machine elements, resulting in what the authors 
call Mechatronic Machine Elements (MME). Why this expression has been chosen is 
explained in section 4. 
An ideal MME the authors define as an element which has a mechanical function, 
is either able to generate and communicate information or to apply received informa-
tion to a mechanical system, and is an independent instance in a data network. As an 
element is understood a small assembly of components which cannot be divided into 
subassemblies while any of its main functions is maintained.  
The advent of MME is interpreted as the result of a technology push and a market 
pull. The advances in mechatronics push the development as they allow for an every 
tighter and more miniaturized integration of functions. At the same time the technical 
systems are increasingly dependent on comprehensive information about the state and 
condition of subsystems and its environment (i.e. they are developing towards every 
more complex CPS). This results in a pull for more mechatronic solutions and deeper 
mechatronic integration to extract information from and apply control to the inmost 
parts of the subsystems. 
This paper aims at discussing the observed trend towards mechatronic machine 
elements in the context of cyber-physical systems, to set up a rough classification of 
different types of CPS and to illustrate three groups of MME. It further outlines how 
CPS in general and MME in particular can improve not only marketable products but 
also the process of developing them. Section 2 describes the initial observation of the 
trend towards MME which is then put into the context of CPS in section 3. In sec-
tion 4 general considerations and examples of three distinct groups of MME are pre-
sented. Section 5 gives an outlook how CPS can be beneficially applied also in  
product development. The last section presents the conclusions. 
2 
Observation of Dissolving Borders 
Starting from the 1960s with fast advances, first in electronics and subsequently in 
computing, machines and mechanical devices became more and more integrated with 
electronics and software. While devices and machines, mechatronic on a product lev-
el, for a long time could be clearly divided into mechanical and electronic compo-
nents, a second step of integration is taking place at the moment on a machine  

 
Mechatronic Machine Elements: On Their Relevance in Cyber-Physical Systems 
265 
element level. Traditional borders are dissolving and we can observe both: There are 
conventional machine elements enhanced in their capabilities by adding electronics to 
them and there are novel machine elements that were developed as mechatronic ele-
ments from scratch and did not exist before. 
Good examples for the first are roller bearings. Probably already known in the an-
cient world was their sole purpose to support a rotating axis with little friction. In a 
recent patent [3] a roller bearing was presented that integrates an angular encoder in a 
cost and space saving way into the bearing ring. Machine element and sensor are in-
separably merged to a MME. 
A bit more difficult to classify as machine elements are active magnetic bearings 
[4]. They are rather complex systems by themselves. But looking at their function 
within a system they clearly serve as machine elements and they are elements from 
the perspective that their core function cannot be achieved when removing parts of it. 
Active magnetic bearings can be regarded as the first MME that were not convention-
al machine elements enhanced with mechatronic features, but mechatronic from the 
core. 
3 
The Web Reaching into Devices 
The observed development of appearing mechatronic machine elements the authors 
would like to consider in the context of Cyber-Physical Systems. To date, many of the 
CPS discussed in the computer science community, which is primarily researching in 
this topic, are networks of independent devices or plants. An example are Advanced 
Electric Power Grids [5]. In such a network, instances like power plants, energy sto-
rages, control computers and consumers are connected on different levels in digital, 
physical or often both ways. These instances can in general be produced, installed and 
connected to the network independently. For the network existing outside the single 
products, this type of CPS shall be referred to as External Network CPS. 
The great interest of automotive and aerospace companies in CPS topics leads us to 
a second type of CPS [6]. External network CPS like smart power grids are a web of 
things1 with the things being plants and devices. Modern cars and airplanes are things 
containing an own web inside them. Their subsystems are mechanically connected 
and digitally linked to each other. The subsystems are controlled by electronic control 
units (ECU), e.g. to decide on the activation of the airbags, which are networked to-
gether. The single subsystems are still an assembly of mechanical parts, sensors, elec-
tronics and actuators, often provided by different suppliers and as individual elements. 
This type will be further referred to as Internal System Network CPS. 
With the emergence of mechatronic machine elements the web is beginning to 
reach even further down, into the element level. The angular decoder-integrated roller 
bearing, described above, or seals equipped with sensors for condition monitoring [8] 
are machine elements, also able to generate information. It is a small step to MME 
                                                           
1  The use of the term web of things is inspired by, but not necessarily identical to its use by 
Guinard and Trifa [7]. It is meant in this context as a network digitally connecting physical 
objects in its widest sense. 

266 
M. Stücheli and M. Meboldt 
acting as independent instances in a computer network, able to communicate over 
standard bus protocols. Similarly, but the other way round, work machine elements 
receiving commands or information out of a bus environment, processing and apply-
ing it to actively adapt their properties according to the information. When in a CPS 
not only subsystems but even machine elements are digitally connected and act and 
communicate independently, the authors call that an Internal Element Network CPS 
(see Fig. 1). 
 
(a) 
 
(b) 
Fig. 1. Principle of an internal systems network CPS (a) and an internal element network CPS 
(b). The network nodes in (a) are subsystems, complex themselves, whereas in (b) the network 
reaches down to a machine element level. 
 
Fig. 2. Overview of the classification of cyber-physical systems used in this paper 
 
external network
internal network
system level
element level
external network CPS
internal system network CPS
internal element network CPS
pervasive network CPS
SS
SS
SS
SS
SS
SS
SS
SS
MME
Subsystem
mit ECU
 
MME 
Subsystem
with ECU
Subsystem
with ECU 
Subsystem 
with ECU 
  

 
Mechatronic Machine Elements: On Their Relevance in Cyber-Physical Systems 
267 
This still being rather a vision than an observable development, it can even be im-
agined CPS, at the same time reaching down to an element level inside one machine 
and out to other devices. Imagine the timing belt of your car detecting a precarious 
wear on its own, before breaking. Through the internal element network of the car, the 
information will be transmitted to a mobile communication interface. From there a 
message is sent through external networks to your smartphone. The smartphone then 
helps you to schedule an appointment at a garage and sends a specific message to the 
spare part ordering system of your garage of choice. This type of CPS combining 
internal element networks and external networks shall be referred to as Pervasive 
Network CPS. 
4 
Mechatronic Machine Elements 
4.1 
Discussion of the Term 
The term mechatronic machine element seems to have rarely been used to date. A 
Google search for the precise expression produces in September 2012 four hits. One 
of them leads to the encoder integrated bearing described in section 2. In the only 
publication found, using the term[9], it does not play a major role, nor is it defined. 
The term was chosen for the following reasons: The part machine element refers 
first to the similar role these items have within a mechatronic product to conventional 
machine elements, especially in terms of mechanical function. Secondly it stresses out 
the element character as being a small integral assembly which only develops useful-
ness when all parts are contained. Mechatronic brings in the combination of mechan-
ics, electronics and digital computing. 
4.2 
Standard Solutions Reduce Design Effort  
As briefly mentioned above, typical CPS products which contain Internal System 
Networks are automotive vehicles and airplanes. These highly complex systems are 
developed by huge design teams. The high development efforts can be afforded due to 
the large number of vehicles produced or the high price paid for an airplane. Other 
products, as medical rehabilitation devices for example, are too specific to be  
produced in hundreds of thousands and cannot cost millions of euros, neither. In con-
sequence the development effort has to be kept comparably low. Thus the cyber-
physical integration is often minimal, although it might be highly desirable that they 
exchange more information with other devices and adjust better to the current task. 
So we can see on one side the trend towards CPS and on the other side products for 
which the development effort to design complex internal system network CPS cannot 
be afforded. Here MME may close the gap. Having – in future – at hand a wide range 
of MME, which work plug-and-play in a bus system and can be selected and mechan-
ically integrated by the development engineers as easily as today a gear box, this will 
enable also small teams to develop sophisticated CPS.  
A similar development already took place in electronics a few years ago, looking at 
digital camera sensors for example, or GPS modules. In 1990 GPS receivers were 

268 
M. Stücheli and M. Meboldt 
bulky devices and due to their price only used professionally. Ten years later, GPS 
navigation devices were already handy and wide spread for navigation in cars or out-
door sports. Today the small size and cheap price of a GPS module makes them en-
hance devices like mobile phones and digital cameras with secondary functionality. 
But it also contributes to their broad application that developers do not need to under-
stand the whole GPS technique anymore. It is enough for them to know the physical 
interface of the module and the processed digital values that it gives out.  
 
 
 
 
(a) 
(b) 
(c) 
Fig. 3. Historical development of GPS receivers, from bulky positioning devices (a) over smart 
navigation devices with map functions (b) to small modules integrated into smart phones (c) 
From the example above can be deduced that mechatronic machine elements 
should have the following traits in order to have a great impact on future product de-
velopment. Of course they should have reasonable costs and a compact design. Then 
they should process as much information as possible directly on the element. Thus the 
amount of data transmitted is reduced, and the transmitted data can more easily be 
interpreted by the integrating development engineers. For easy network integration it 
is important that the elements communicate according to an appropriate standard. 
Also regarding their mechanical design MME should follow general standards.  
Three groups of MME, where important developments are going on, are examined 
more closely in the next three subsections. The examples given may not be MME 
exactly as defined above but they illustrate the key characteristics of each group. 
4.3 
Sensor-Integrated Machine Elements 
An example of the group of MME which the authors call Sensor-Integrated Machine 
Elements is the roller bearing including an angular encoder, described in section 2 [3]. 
That this is truly one element can be seen from the fact that the angular structure de-
tected by the transducer is manufactured into a bearing ring. Thus sensor and machine 
element are inseparable. While here the advantage of the fusion lies mainly in cost 
and space saving, sensor-integrated machine elements can offer more. 

 
Mechatronic Machine Elements: On Their Relevance in Cyber-Physical Systems 
269 
Dickerhof [10] showed in his research that it is possible to detect dry running and 
rib contact running of roller bearings through ultrasound analysis. Thus impending 
failure can be detected before it occurs. This detection is generally desired for roller 
bearings, as both, failure and unnecessarily early replacement is often very costly. So 
the demand for standard elements combining bearing and ultrasound analysis is given.  
Now, from an ultrasound measurement the data flow is obviously large while one 
only needs one single value: okay or not okay. The analysis to get to this value re-
quires considerable knowledge of the physical system, i.e. the spectrum of vibrations 
occurring in “healthy” and “unhealthy” bearings. Therefore it is useful that the manu-
facturer implements the raw data processing directly on the machine element, which 
then sends to other systems only the relevant okay/not okay status. Thereby it requires 
only minimum communication bandwidth. 
Another advantage of sensor-integrated machine elements is also illustrated by 
Dickerhof’s research. It is that damages on bearing races can be detected already from 
low impact energies, i.e. at low speed or with small damage, when the vibration is 
measured directly at the bearing. Often only a very tight sensor-mechanical integra-
tion regarding space and component parts leads to sufficient signal quality to extract 
relevant information from the system. 
4.4 
Semi-Actuators 
A second group of MME is referred to as Semi-Actuators. While the main function of 
an actuator is to transform a certain form of energy into mechanical energy, the main 
function of a semi-actuator is mechanically passive. However, a semi-actuator can 
adjust the properties of its main function actively. Thus semi-actuators have generally 
the properties that they add degrees of freedom to the system, consume little energy 
and require often not so fast control cycles as principal actuators. 
An example for this type of MME comes from robotics, where there is a lot of re-
search going on in the field of variable impedance joints. The DLR VS-Joint [11] is a 
passive spring joint that produces in both directions forces towards a neutral position. 
Through an actuated spindle the pretension of the springs can be adjusted in order to 
change the joint stiffness. So this actuator does not actuate the joints but just influ-
ences the way the links are passively coupled through the joint.  
Novel semi-actuators have the potential to make completely new mechanical solu-
tions possible. 
4.5 
Sensor-Actuator Fusion 
A third group of MME are Sensor-Actuator Fusions. Similar to sensor-integrated 
machine elements sensor function is deeply integrated into another element – here 
into an actuator. One motivation is again to save space. Another reason for such ele-
ments is to measure directly at the “hotspot of action”. As an actuator by definition 
produces a change to a system, one often wants to measure the effects of it. On the 
one hand the measurement of the resulting effects of an actuator input is usually the 
most relevant the closest to their source, i.e. the actuator itself. On the other hand, as 
certain sensors are often deployed together with an actuator in a system, it can be very 
useful to develop and distribute them as a single element. 

270 
M. Stücheli and M. Meboldt 
One example for a sensor-actuator fusion element is a self-sensing short stroke li-
near drive, developed for a force feedback keyboard [12]. It uses the identical hard-
ware to produce force and to sense position. The position is derived from the damping 
of electric oscillations in the system, which is proportional to the position of the fer-
romagnetic core in the coil. A second example pointing in the direction of a sensor-
actuator fusion is a highly integrated light-weight robot actuator [13]. Here the motor 
and sensors are still separable, but designed for a tight spatial integration. One cylin-
drical assembly contains a hollow shaft motor, motor and joint position sensors, tor-
que sensors, a brake, a gear, plus ring shaped electronics boards.  
5 
Outlook: Insights for Product Development 
So far cyber-physical systems are mainly discussed as marketable products or net-
works of such. The value of the intense exchange of data lies in operation states and 
services which are specifically optimized to the current situation, thanks to more situ-
ation awareness of the machines. Or, in another case, the value can be to free the hu-
man user from the task of acquiring, processing and deciding upon data. But CPS can 
also play an important role in the development process of products which are them-
selves low mechatronic. 
To design good products, engineers need correct and precise specifications regard-
ing the application of a product. CPS can provide information from prototypes in field 
testing in real time around the globe. This can be for example data on the frequency 
and duration of usage or the ambient temperature and humidity in the place of appli-
cation. Much more powerful becomes the database if it provides information on the 
dynamics of the product in interaction with the user and the application. 
As an example can serve the testing of hand-held power tools. Small units inte-
grated in the prototypes record the operation of the switch and measured accelerations 
and temperatures. The data is sent via GSM network to the R&D headquarters of the 
company. It should be noted that relevant accelerations are to be measured at the force 
bearing structures inside the device and not at the housing. Additionally, not to alter 
the dynamics significantly, the data acquisition unit has to be light compared to the 
mass of the product. 
With the need to measure relevant properties deep in the core structure of the pro-
totype, this example again indicates the usefulness of mechatronic machine elements. 
New solutions should be actively sought for MME specifically designed to collect 
data for design purposes. The authors think for example of sensor-integrated machine 
elements, which are in the end product exchanged for conventional machine elements 
to save costs. 
6 
Conclusions 
This paper in the first place documents the observation of first mechatronic machine 
elements being developed. This observation is brought into context of a general trend 
towards cyber-physical systems in engineering.  
 
 

 
Mechatronic Machine Elements: On Their Relevance in Cyber-Physical Systems 
271 
A major contribution is the introduction of a nomenclature for both, categorizing 
observed MME and existing or envisioned CPS. Four types of CPS are identified and 
named external network CPS, internal system network CPS, internal element network 
CPS and pervasive network CPS. Three groups of MME are identified so far. They 
are sensor-integrated machine elements, semi-actuators and sensor-actuator fusions 
(see Table 1). These groups are characterized and illustrated with examples. 
Table 1. Comparison of the three proposed types of mechatronic machine elements 
MME type 
Short description 
Add mechan-
ical energy 
Change  
mechanical 
properties 
Sensor  
output 
Sensor-integrated  
machine element 
Passive machine element with deeply 
integrated sensing capability 
 
 
X 
Semi-actuator 
Basically passive mechanical beha-
viour, which is actively adjustable 
 
X 
 
Sensor-actuator 
fusion 
Deep integration of actuator and  
sensor functions into single element 
X 
X 
X 
 
While the emergence of MME is a consequence of the increasing integration of 
mechanics, sensing, computation and networking, they are at the same time a prere-
quisite for types of CPS that reach deeply into the machines, namely internal element 
network CPS and pervasive network CPS. Advantages of MME are most obviously 
saving space and better data quality, as they can sense very close to the source of 
measured effects. But they can also reduce the development effort to design complex 
products as they provide already more functionality, coordinately engineered and 
readily integrated in one element. Some MME also create mechanical functionality 
which can only be achieved through a mechatronic solution.  
It is argued that for a broad success of MME it is crucial that they simplify further 
the work of the engineers deploying them in products they build. Therefore MME 
should follow mechanical and communication standards. For sensing MME it is im-
portant to implement as much preprocessing as possible on the element itself in order 
to boil down the data to the relevant information. This helps the deploying engineers 
in understanding and using the transmitted data, and it also helps to reduce the re-
quired bandwidth of the network. 
In an outlook there is a great potential identified for building CPS not only as mar-
ketable products but also as a tool for gathering information in the product develop-
ment process. It is suggested to develop MME specifically for this purpose to make 
best use of this potential. 

272 
M. Stücheli and M. Meboldt 
References 
1. Lee, E.A., Seshia, S.A.: Introduction to Embedded Systems: A Cyber-Physical Systems 
Approach, 1st edn. LeeSeshia.org (2011) 
2. Tabuada, P.: Cyber-Physical Systems: Position Paper. In: NSF Workshop on Cyber-
Physical Systems, Austin, TX (2006) 
3. Schmid, G.: Roller Bearing With Integrated Rotary Shaft Encoder. Patent WO 
2008/006645 A1 
4. Schweitzer, G.: Mechatronics - A Concept With Examples in Active Magnetic Bearings. 
Mechatronics 2(1), 65–74 (1992) 
5. Rajkumar, R., Lee, I., Sha, L., Stankovic, J.: Cyber-Physical Systems: The Next Compu-
ting Revolution. In: Proceedings of the 47th Design Automation Conference, Anaheim, 
CA, June 13-18 (2010) 
6. Work, D., Bayen, A., Jacobson, Q.: Automotive Cyber Physical Systems in the Context of 
Human Mobility. In: National Workshop on High-Confidence Automotive, Troy, MI 
(2008) 
7. Guinard, D., Trifa, V.: Towards theWeb of Things: Web Mashups for Embedded Devices. 
In: Workshop on Mashups, Enterprise Mashups and Lightweight Composition on the Web 
(MEM 2009), Madrid, Spain (April 2009) 
8. Freudenberg Simrit: Produktinformation Simmerring MSS1+ Condition Monitoring. Freu-
denberg Simrit GmbH & Co. KG, http://www.simrit.de/files/0000097E.pdf (accessed Sep-
tember 28, 2012) 
9. Karavaev, Y., Abramov, I.V.: Building a knowledge base for intelligent control system of 
mechatronic machining center. In: Proceedings of 14th International Conference on Me-
chatronics MECHATRONIKA, Trencianske Teplice, Slovakia, June 1-3, pp. 93–94 (2011) 
10. Dickerhof, M.: Potentiale der Schallemissionsanalyse zur Überwachung und Diagnose tri-
bologischer Systeme. Doctoral thesis, Karlsruher Institut für Technologie (KIT) (2011) 
11. Wolf, S., Hirzinger, G.: A new variable stiffness design: Matching requirements of the 
next robot generation. In: IEEE International Conference on Robotics and Automation, 
ICRA, May 19-23, pp. 1741–1746 (2008) 
12. Savioz, G., Perriard, Y.: Self-sensing of linear short-stroke actuators for multi-finger hap-
tic interfaces using induced high frequency oscillations. In: IEEE/ASME International 
Conference on Advanced Intelligent Mechatronics (AIM), Kachsiung, July 11-14, pp. 
764–769 (2012) 
13. Hirzinger, G., Albu-Schaffer, A., Hahnle, M., Schaefer, I., Sporer, N.: On a new genera-
tion of torque controlled light-weight robots. In: IEEE International Conference on Robot-
ics and Automation, ICRA, COEX, Seoul, Korea, pp. 3356–3363 (2001) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 273–282. 
DOI: 10.1007/978-3-642-30817-8_27 
© Springer-Verlag Berlin Heidelberg 2013 
 
Approach for an Early Validation of Mechatronic 
Systems Using Idealized Simulation Models  
within the Conceptual Design 
Frank Bauer1, Jürgen Gausemeier1, Daniel Köchling1, and Felix Oestersötebier2 
1 Chair of Product Engineering, University of Paderborn,  
Fürstenallee 11, 33102 Paderborn, Germany 
2 Chair of Control Engineering and Mechatronics, University of Paderborn,  
Fürstenallee 11, 33102 Paderborn, Germany 
{frank.bauer,felix.oestersoetebier}@hni.uni-paderborn.de  
Abstract. The success of mechatronic systems is determined by the close inte-
raction of mechanical engineering, electrical engineering, control engineering 
and software engineering. During the design processes developers often rely on 
established and proven solutions. In order to ensure the desired system behavior 
simulations are carried out. This leads to considerable modeling effort to create 
the necessary simulation model. In this paper an approach for early validation 
of mechatronic systems is presented. The aim is to reduce the modeling effort 
for simulation models and to allow early simulations of different concepts. 
Therefore idealized simulation models are combined to a simulation-ready 
Modelica model of the system. The model is generated (semi-)automatically 
based on information provided by the active structure, a shape model and a be-
havior model. All models are part of a coherent system that describes the prin-
ciple solution. Adjustments and changes in the simulation model are transferred 
back into the partial model. 
Keywords: Mechatronics, Conceptual Design, Model-Based System Design, 
Principle Solution, Dynamic Analysis. 
1 
Introduction 
Today the success of complex products is determined by the close interaction of me-
chanical engineering, electrical engineering, control engineering and software engi-
neering. The design of such mechatronic systems requires a domain-spanning devel-
opment process as well as a continuous validation of the system design. In order to 
improve the development of mechatronic systems, especially the early design phase, 
which the VDI-guideline 2206 calls system design, is of high importance. This part of 
the development is the conceptual design, where a domain-spanning system model is 
developed resulting in the principle solution as a first solution concept. Early analyses 
allow the discovery of existing weaknesses, the elimination of errors and the identifi-
cation of possible improvements. The necessary analysis depends on the type of sys-
tem in development and the application scenarios according to further  

274 
F. Bauer et al. 
 
operation. Examples are the thermal behavior of individual actuators, the electromag-
netic interference of components or the dynamic behavior of multi body systems. The 
benefits of early, simulative verification are for example time- and money-saving 
during the development and an improvement of the product quality. Especially small 
and medium-sized enterprises (SMEs) decline the use of simulation and visualization 
tools as the effort seems too high and the benefits are not clear. Many of these com-
panies do not have their own simulation department which is specialized on simula-
tion models and environments. As a result the modeling and analysis must be done by 
the system engineers themselves. The required modeling effort depends of the devel-
opment task at hand and restricts the developer with respect to the planned work. 
Another barrier is the high investment costs for the required simulation programs. 
These are profitable only in continuous use [1]. 
In this paper an approach for early verification of the dynamic behavior of mecha-
tronic systems is presented. To reduce the modeling effort, the simulation model for 
analysis in multi-domain simulation is generated semi-automatically. A prerequisite 
for this task is a system model as well as the use of specially prepared and idealized 
simulation models for selected subparts. Comparable approaches are presented in [2]. 
In that contribution, two case studies are described for system modeling with SysML 
and the simulation analysis with Modelica. The first possibility is the continuous 
enrichment of the system model (SysML) with simulation relevant information. This 
allows a direct generation of the Modelica model using SysML4Modelica. Hence no 
simulation expert is required to create the simulation model. The developer of the 
system model however needs extensive knowledge of the simulation options and the 
required information. In the second case, only the basic structure of the simulation 
model is created based on the system model. Therefore the system model is not as 
extensive as in the first case. The elaboration of the simulation is performed by a si-
mulation expert.  
The approach presented here is a compromise between the two outlined ways. The 
basic idea is to build a fundamental simulation model based on a domain spanning 
system model. For the elaboration of the simulation model the predefined idealized 
simulation models will be combined. It is assumed that these are purchased from an 
external partner (e.g. supplier) or are available in-house. The effort of the simulation 
expert is reduced significantly, without overloading the system model with too much 
simulation specific information. The complexity of the system model thus remains 
manageable and the common understanding between the developers is still guaran-
teed. This contribution focuses the early stage of development and the verification of 
the system design. Chapter 2 deals with the development process within the concep-
tual design phase, in which a first version of the system model (active structure) is 
created. Chapter 3 shows the construction of idealized dynamic models and the semi-
automated model building. The analysis of the system and the feedback of the simula-
tion results will be discussed in Chapter 4. As an example we use the development 
and verification of a sorting station. The main task of this station is to sort small cy-
lindrical samples in different stores. The samples differ in material and color. They 
are fed by a central storage to the station, inspected and finally transported to the  
desired store.  

 
Approach for an Early Validation of Mechatronic Systems 
275 
 
2 
Conceptual Design of Mechatronic Systems 
According to the VDI-guideline 2206 the design of mechatronic systems is divided 
into the domain-spanning conceptual design and domain-specific design [3]. These 
two phases can be further divided into the three processing steps determination of 
objectives, synthesis and analysis. Figure 1 shows the structure of the conceptual de-
sign phase and the results of each step in detail. The approach that is presented in this 
paper is based on a function-oriented design. This was defined by PAHL/BEITZ in [4]. 
To comply with the requirements of Model Based Systems Engineering a semi-formal 
specification technique is used to describe the principle solution of mechatronic sys-
tems during the conceptual design. The specification technique is CONSENS - "Con-
ceptual Design Specification technique for the ENgineering of Complex Systems” [5]. 
The result of the conceptual design phase is a coherent system of partial models, 
which describes the principle solution of the new system holistically. The developers 
often rely on proven solutions and subsystems to create a new system. As a conse-
quence the proposed system model combines all the information of the selected solu-
tions like system elements and geometry parts. Although the different aspects of the 
system are represented by so-called partial models, the system model cannot be simu-
lated directly within a simulation program. 
 
Fig. 1. Design process of mechatronic systems including an early validation 
During the determination of objectives, the development tasks will be clarified and 
the environment model, the application scenarios and the requirements list will be 
created. For example it is defined, that the sorting station should receive commands 
from an operator (environment), store metal samples and plastic samples (use cases) 
or may operate with max. 230 Volt (requirement). It is advisable to create these mod-
els during workshops. This ensures a common understanding of the development task 
among all participating developers. In addition, members from other divisions as for 
example marketing or service can participate to contribute their views on the system. 
Afterwards the functional description of the system will be made. The result is the 
function hierarchy in which the sorting process is split up into required sub functions. 
Examples are "to eject sample", "to move sample to store" or "to identify sample  
material". 
Workshop
synthesis
choose solution pattern
model active structure 
model behavior
model shape 
determination of 
objectives
clarify & define the task 
define functions
analysis
adjust simulation
start simulation
evaluate simulation 
Results
Environment
Application scenarios
Requirements
Function hierarchy
domain-spanning conceptual design
Mechatronic Modeller / 
Workshop
Results
Active structure
Shape
Behavior
Dymola / 
Mechatronic Modeller
Results
Simulation model 
Results of analyses
Principle Solution

276 
F. Bauer et al. 
 
The selection of suitable solution patterns at the beginning of the synthesis is an 
important step in the development process. A solution pattern describes a specific 
product or solution element in an abstract form. Additional information about solution 
patterns and the synthesis are described in [6]. Using different solution pattern within 
the development process, various concepts for a new system are developed. While all 
concepts share the same functional description, they implement them differently. 
They vary in structure, shape and behavior. For example, one concept could use a 
pick and place robot to move the samples to the stores while a second concept uses 
pneumatic cylinders and a conveyor belt. Therefore an active structure and a shape 
model for possible concepts are defined. In Figure 2 these two models are shown for 
the second concept of the sorting station.  
 
Fig. 2. Active structure and initial 2D shape model of the sorting station 
The sample will be moved by pneumatic cylinders from the central storage to the 
conveyor belt and from the conveyor belt to the stores. Light barriers detect the posi-
tion on the conveyor belt while it is moving. The active structure shows the different 
system elements and some of the flow relations between them. To keep it simple only 
two stores are shown. The 2D shape model outlines the position of each part on the 
frame. In addition to these models, the control of the sorting station is defined in a 
first behavior model. The simulation relevant information about the system is mapped 
to these three models. The internal computer-representation is performed by using the 
Mechatronic Modeller. The Mechatronic Modeller is especially designed for the spe-
cification technique CONSENS. All partial models can be modeled graphically and 
they can be parameterized [5]. In the following the modeling of the active structure 
and the annotation of simulation-relevant information from the shape model will be 
explained in more detail. 
Each system element in the active structure is associated with one solution pattern. 
A system element represents a part of a system that is not yet defined finally in the  
 
mech. 
connection
mech. 
connection
mech. 
connection
mech. 
connection
mech. 
connection
mech. 
connection
mech. 
connection
frame
control
cylinder 2
cylinder 1
eject 
cylinder
sample
induction 
sensor
store 2
store 1
system element
information flow
energy flow
Legend
material flow
light 
barrier 2
light 
barrier 1
conveyor
sensor signal 1 control signal 1 sensor signal 2 control signal 2
mech. 
connection
mech. connection
mech. 
connection
1
1
pointer
sorting station
mech. 
connection
Active structure
frame
X
Y
sample
conveyor
store 2
cylinder 2
light 
barrier 2
cylinder 1
light 
barrier 1
induction sensor
eject 
cylinder
store 1
2D Shape model

 
Approach for an Early Validation of Mechatronic Systems 
277 
 
conceptual design phase. The system element is described by parameters and has 
input and output ports. The elements are connected thought the system flows energy, 
material and information. To ensure the automatic generation of the simulation model, 
the modeling of the active structure has to follow some restrictions. These will ensure 
a clear link between system elements and the idealized simulation models: 
• 
At each system element the class of the idealized simulation model has to be 
annotated. The name of the system element is still arbitrary as it was defined in 
the workshop for example. 
• 
The input and output ports of the system elements are defined using the interface 
type and a port name. When no kinematic coupling exists between mechanical 
elements the type of the contact surface has to be indicated.  
At the stage of concept development, no detailed CAD model is available in most 
cases. Nevertheless the simulation model requires some geometrical information. 
Therefore, the necessary shape information from sketches or rough 3D drafts is anno-
tated to each system element. This includes the location and the coupling points of 
elements. The restrictions to annotate the shape information are as follows: 
• 
The coordinates of the position are annotated in the system element through the 
label position = {x, y, z}. The rotation is specified by angle = {φy φx, φz}. 
• 
The names of the contact surfaces are integrated in the ports of the system ele-
ments. The combination of two contact surfaces determines the choice of the con-
tact block. This block will be integrated in the simulation model. 
• 
Fixed points will be defined through the specification of a port. They have the 
purpose to displace and rotate the components in the model. 
The basic control of the system is modeled in a state diagram. The modeling is done 
by using the Mechatronic Modeller. In accordance with the DIN EN 61131-3 simpli-
fied sequential controls for the system are created [7]. The connections between sys-
tem elements are needed to generate a state diagram in Modelica. Restrictions are: 
• 
Each state diagram is defined by an initial step and a final step. 
• 
To define and use a flag (Boolean interim result) the annotation “flag” has to be 
set as an output value of a state in the state diagram. 
• 
A concretization of a state it realized by sub states.  
3 
Model-Based Analysis of the Dynamic Behavior 
During the conceptual design of a new mechatronic system the principle solution will 
be build, checked and revised. This is done gradually and with several iterations. Con-
trolling the motion of the sorting station is a task where model-based synthesizing and 
analyzing methods offer great potential. Furthermore, to evaluate the functional capa-
bility, idealized control structures have to be designed. Therefore idealized simulation 
models are used which are assigned to the corresponding solution patterns. 

278 
F. Bauer et al. 
 
3.1 
Idealized Simulatio
Considering the conceptual
time of the design process.
So simulation models that a
ical principles in an idealiz
constraints and physical lim
a full product datasheet at 
easy to determine. Rough c
remaining parameters are 
structure of the idealized c
pattern. Its main componen
the two stops. The latter rep
use-case. Furthermore the 
means of a custom-made bl
Fig. 3. Idealized M
In idealized models know
lations of a solution pattern
able to reduce the modelin
validating the chosen confi
actors for the sorting station
via an interface model, the 
tric, pneumatic or hydraul
cylinder pushes the sample
simulation model the design
on Models for the Early Validation 
l stage, concrete solution elements cannot be chosen at 
. The designer only decides on the technology in gene
are especially designed for this purpose represent the ph
zed way. However they have to take into account techn
mits. The parameterization must be possible without hav
hand. Only a few parameters are required which are qu
calculations to determine respective default values for 
provided by the models [8]. Figure 3 shows the inter
cylinder model, which corresponds to the chosen solut
nts are the two pneumatic chambers, the piston mass 
present limits of the actors, which are very important in 
contact surface of the pusher is defined separately 
lock. 
 
Modelica model of a pneumatic cylinder (Screenshot) 
wledge about the structure as well as about the physical
n is stored and offered. Using these models the designe
ng effort while comparing different principle concepts 
iguration. This means for example the test of sensors 
n. As the interfaces of the models are defined and inheri
designer is able to exchange them and compare e.g. el
lic actuators [8]. In the described example a pneum
es onto the conveyor belt. In the corresponding ideali
ner has to define the diameters of the pneumatic chamb
this 
eral. 
hys-
nical 
ving 
uite 
the 
rnal 
tion 
and 
this 
by 
l re-
er is 
and 
and 
ited 
lec-
matic  
ized 
bers, 

 
Approach for an Early Validation of Mechatronic Systems 
279 
 
the diameter of the rod and the stroke. These are parameters that can be obtained from 
the given requirements. A simple example for a calculated parameter is the required 
value for the mass in movement. This is estimated by knowing the piston geometry 
and by assuming that it is made of steel. By setting these values and checking them 
with the help of the simulation model of the sorting station, the designer defines target 
values to search for solution elements which are available in the market.  
3.2 
Automated Generation of Simulation Models 
To evaluate the functional capability of the system by simulation, the modeling lan-
guage Modelica is chosen [9]. Modelica allows an object-oriented, equation-based 
description of complex technical systems. The combination of closed sub-models for 
the components is possible [10], [11]. Numerous libraries that are free of charge al-
ready offer blocks which allow a relatively easy modeling of solution pattern models. 
We furthermore use the possibility of creating custom libraries to build up the “solu-
tion pattern library (SoPatLib)”. The text-based description of models is another ad-
vantage of Modelica. It facilitates the automated compilation of the simulation model 
for the designed system. To simulate and edit the model we use Dymola. Dymola is a 
simulation tool for Modelica which offers graphical modeling as well as different 
analyzing possibilities. The latter comprise the graphical visualization of variables as 
well as 3D animations of the mechanical components of the system [12]. 
The automated generation of simulation models is divided into two phases. First, 
the sub-models are instantiated and their parameters are defined. Then the connec-
tions of the sub-models are described in the equation section of the model. The  
necessary information is described in the partial models according to the presented 
restrictions. A converter extracts the information and builds up the text based Modeli-
ca file automatically. It extracts the relevant data from the Mechatronic Modeller file 
and hence creates the Modelica model by combining, parameterizing and connecting 
the sub models. The idealized models are identified via an annotation in the system 
element. Figure 4 illustrates the transition of a selected part of the active structure. In 
the system element sample the corresponding complete path SoPatLib.Components. 
SampleRound of the simulation model is given. So the respective sub-models can be 
loaded and instantiated in the Modelica code of the sorting station model. Figure 4 
displays also examples for the necessary port information. This implies a description, 
the port-type and a possible description of the contact surface. This information is 
used to generate the connections in the equation section of the Modelica code. Here 
the ports of the system elements frame, cylinder and sample are defined as mechani-
cal_3D. The consistency of the connection can be checked in advance. The system 
elements are arranged according to the position statement with respect to inertial 
coordinates. Therefore a FixedRotation block is inserted and parameterized. Further-
more the surface port of the sample and the pusher port of the cylinder defines the 
contact surface. The combination of the two contact surfaces, namely a round and a 
plain surface, led to the appropriate idealized contact model. This is inserted from the 
contact library. At last the connections are transferred by means of the connect  
statements. 

280 
F. Bauer et al. 
 
Fig. 4. Transition from
To test the automated sor
Modelica StateGraph2 libra
the developer defines the b
creates a state machine wit
The automated generation 
proach for the generation o
a screenshot of the created 
positions the model can be 
started and the eject cylinde
is a starting point for furthe
Fig. 5. Sc
sample
cylinder 1
frame
Description: thread
Type: mechanical_3D
Contact surface: -
Description: flange
Type: mechanical_3D
Contact surface: -
Position
{0.5,0.1
Name: sample
Class of Mode
SoPatLib.Compon
s.SampleRoun
Description: pusher
Type: mechanical_3D
Contact surface: plain
Description: surface
Type: mechanical_3D
Contact surface: round
Active structure
m the active structure to a simulation-ready Dymola model 
rting process a first control sequence is generated using 
ary. The starting point is the partial model behavior. Her
basic sequence of the sorting process. For this purpose
thin the Mechatronic Modeller using states and transitio
of StateGraph2 model is analogous to the presented 
f the structure model of the sorting station. Figure 5 sho
simulation model. According to the selected parameter 
simulated immediately. In the Figure 5 sorting process j
er pushes a sample on the conveyor. The simulation mo
er optimization of the system design. 
 
creenshot of the simulation model in Dymola 
model sorting_station 
SoPatLib.Components.Framework frame a;
SoPatLib.Components.SampleRound sample a;
SoPatLib.Components.PneumaticCylinder cylinder_1 a;
SoPatLib.Couplings.line_contact contact1 a;
Modelica.Mechanics.MultiBody.Parts.FixedRotation
fixedRotation1(r={0.5,0.1,0})    a;
equation 
connect(cylinder_1.pusher, contact1.contactFrame_a) a;
connect(contact1.contactFrame_b, sample.surface) a;
connect(frame.thread, fixedRotation1.frame_a) a;
connect(fixedRotation1.frame_b, cylinder_1.flange) a;
connect(frame.fix_point, …
connect(cylinder_1.air_a, …
connect(cylinder_1.air_b, …
connect(sample.bottom, …
end sorting_station;
[
n = 
,0}
e
el:
nent
nd
sample
frame
contact1
Modelica code
Dymola mode
 
the 
rein 
e he 
ons. 
ap-
ows 
and 
just 
odel 
l

 
Approach for an Early Validation of Mechatronic Systems 
281 
 
4 
Simulation and Adjustment of the Principle Solution Models  
The use of the simulation environment Dymola allows an easy modification and ad-
justment of initially created systems. As shown in the previous section the names of 
the Modelica components match the names of the system elements within the active 
structure. Due to the graphical representation in the Mechatronic Modeller and in 
Dymola the developer can model the system similar in both tools. Within the simula-
tion environment, the developer refines the parameters, adjusts the positions of ele-
ments or adapts the control. This process continues until the system shows the desired 
behavior. The obtained results are important for the further development of the sys-
tem. They have direct influence on the selection of appropriate solution elements as 
well as the development of the shape model in the CAD system. As the system model 
is the basis for all involved experts during the concretization phase the results of the 
simulation have to be transferred back into the partial models. 
An example of modifications and adaptations on the sorting station are the posi-
tions of the light barriers and the speed of the conveyor. Both parameters are crucial 
for the accurate sorting of the samples. Since the conveyor belt does not stop imme-
diately when the control signal is set, the light barriers have to detect the sample be-
fore they reach the store. The parameters must be aligned to ensure a smooth running 
of the sorting process. During the simulation the developer can easily modify the 
speed of the conveyor and reposition the light barriers to test different combinations. 
Furthermore, the speed of the belt impacts on the stand of the samples. Although all 
samples have the same geometry they are made of different materials. As a result the 
friction between samples and conveyor belt and the inertia varies. Too rapid accelera-
tion and deceleration ramps can cause an overturn and hence lead to problems during 
the sorting process. These are a typical problems within a virtual commissioning of a 
new system. 
The mentioned changes and enhancements to the simulation model influence main-
ly the active structure including the system elements and parameters. Three types of 
changes can be distinguished. The addition and subtraction of simulation models, the 
refining or setting parameter values as well as the adaptation of flows relations be-
tween elements. Each of these modifications will affect the design of the active struc-
ture. So far the changes like position or parameter values have to be transferred back 
manually into the active structure. The influences on the other partial models like the 
function hierarchy or the environment are captured in the Mechatronic Modeller. This 
ensures that the principle of solution is kept consistent. 
5 
Summary 
The presented work shows a way to secure and optimize mechatronic systems within 
the conceptual design phase. The basic idea is the use of predefined idealized simula-
tion models of subparts. These are automatically combined into a simulation ready 
Modelica model which allows an early simulation and analysis of the dynamic beha-
vior. Starting point is the active structure of the new system with all system elements 

282 
F. Bauer et al. 
 
and the flow relation between them. Using a converter, the information is extracted, 
combined and converted to a Modelica model. The automated transfer of information 
allows developers to test and compare different approaches easily. Detailed know-
ledge about the simulation models is not required as the idealized models can be seen 
as a black box. This simplifies the use of simulation models in the early development 
phase. The secured principle solution improves the development of the system by the 
involved experts and supports the selection of suitable solution elements.  
Acknowledgments. This work was developed in the project “ENTIME: Ent-
wurfstechnik Intelligente Mechatronik” (Design Methods for Intelligent Mechatronic 
Systems). The project ENTIME is funded by the state of North Rhine-Westphalia 
(NRW), Germany and the EUROPEAN UNION, European Regional Development 
Fund, “Investing in your future”. 
References 
1. Döbler, T.: Simulation und Visualisierung in der Produktentwicklung. FAZIT-
Schriftenreihe Band 12. MFG Stiftung Baden-Württemberg, Stuttgart (2008) 
2. Votintseva, A., Witschel, P., Regnat, N., Stelzig, P.E.: Comparative Study of Model-Based 
and Multi-Domain System Engineering Approaches for Industrial Settings. In: Vallecillo, 
A., Tolvanen, J.-P., Kindler, E., Störrle, H., Kolovos, D. (eds.) ECMFA 2012. LNCS, 
vol. 7349, pp. 20–31. Springer, Heidelberg (2012) 
3. The Association of German Engineers (VDI): Design methodology for mechatronic sys-
tems. VDI-Guideline 2206. Beuth-Verlag, Berlin (2004) 
4. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Engineering Design: A Systematic Ap-
proach. Springer, Heidelberg (2006) 
5. Gausemeier, J., Lanza, G., Lindemann, U. (Hrsg.): Produkte und Produktionssysteme inte-
grativ konzipieren – Modellbildung und Analyse in der frühen Phase der Produktentste-
hung. Carl Hanser Verlag, München (2012) 
6. Anacker, H., Bauer, F., Dumitrescu, R., Gausemeier, J.: Computer support for the identifi-
cation of solution patterns for the conceptual design of advanced mechatronic systems. In: 
Proceedings of the 11th Biennial Conference on Engineering Systems Design and Analysis 
(ESDA 2012), Nantes (2012) 
7. IEC 61131-3: Programmable controllers – Part 3: Programming. Beuth Verlag, Berlin 
(2003) 
8. Oestersötebier, F., Just, V., Trächtler, A., Bauer, F., Dziwok, S.: Model-Based Design of 
Mechatronic Systems by means of Semantic Web Ontologies and Reusable Solution Ele-
ments. In: Proceedings of the 11th Biennial Conference on Engineering Systems Design 
and Analysis (ESDA 2012), Nantes (2012) 
9. Modelica Association: Modelica® A Unified Object-Oriented Language for Physical Sys-
tems Modeling – Language Specification – Version 3.1, Linköping, Sweden (2009) 
10. Paredis, C.J.J., Diaz-Calderon, A., Sinha, R., Khosla, P.K.: Composable Models for Simu-
lation-Based Design. In: Engineering with Computers. Springer, London (2001) 
11. Lee, E.A., Sangiovanni-Vincentelli, A.: Component-Based Design for the Future. In: Pro-
ceedings of the Design, Automation & Test in Europe Conference & Exhibition (DATE), 
Grenoble (2011) 
12. Dynasim, A.B.: Dynamic Modeling Laboratory – Manual. Ideon Sciene Park, Lund (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 283–292. 
DOI: 10.1007/978-3-642-30817-8_28 
© Springer-Verlag Berlin Heidelberg 2013 
 
Design for Testability for Micro-mechatronic Systems  
Gisela Lanza1, Thomas Blank2, and Benjamin Haefner1 
1 wbk Institute of Production Science, Karlsruhe Institute of Technology (KIT),  
Kaiserstraße 12, 76131 Karlsruhe, Germany 
2 Institute for Data Processing and Electronics, Karlsruhe Institute of Technology (KIT),  
Kaiserstraße 12, 76131 Karlsruhe, Germany 
Benjamin.Haefner@kit.edu 
Abstract. The development and manufacturing of highly precise micro-
mechatronic systems, such as MEMS applications, is a challenging task due to 
the complexity and variety of their manufacturing technologies, as well as their 
high quality requirements. Within the context of the product engineering 
process of micro-mechatronic systems, quality inspection by means of produc-
tion measurement technology is a crucial factor. This paper presents a survey of 
the challenges regarding quality inspection of micro-mechatronic systems. Fur-
thermore, a Design for Testability approach for these types of products is  
described and exemplary applications of its implementation are shown. 
Keywords: Design for Testability, Simultaneous Engineering, micro-
mechatronics, MEMS, MID, TSV. 
1 
Introduction 
Due to their complexity and their high degree of miniaturization various functions can 
be realized by manifold micro-mechatronic systems. Smart phones, intelligent sensors 
for private homes, or micro robots are only a very small selection of possible applica-
tions. However, because of their specific characteristics, quality assurance of micro-
mechatronic systems and, in particular their testing, is a challenging task. 
The goal of this article is the optimization of quality inspections within the produc-
tion process of micro-mechatronic systems. It is elaborated, how this aim can be 
achieved most effectively, if the entire product engineering process, including the 
design phase, is taken into global perspective. 
The article is organized as follows. Section 2 introduces micro-mechatronic sys-
tems and summarizes the state-of-research regarding their product engineering 
process. Furthermore, it contains a short introduction into different quality assurance 
methods. In section 3, the concept of Design for Testability is discussed with regard 
to micro-electronic and micro-mechatronic systems. In section 4, the latter is further 
elaborated and a methodological approach for the integration of Design for Testability 
into the product engineering process of micro-mechatronic systems is presented. Sec-
tion 5 shows the implementation of this approach for different exemplary systems. 

284 
G. Lanza, T. Blank, a
2 
Literature Revie
2.1 
Micro-mechatronic
The term mechatronics de
which is based on the clas
neering and information tec
getic integration of mechan
control in the design and m
Isermann emphasizes th
tional and spatial system in
ent disciplines [3]. Yet, this
The integrative approach
tems. Most of today’s micr
ogies (layer deposition, lit
technologies, which are use
electro-mechanical systems
which are important for 3D
[4], IC packaging technolog
technology for stacked ICs
[6], and the Flexible Circuit
Typical examples of m
copes used as motion senso
2.2 
Methods of Integra
Systems 
Due to their interdisciplina
high degree of miniaturizat
ucts involving interdisciplin
methods providing an integ
A design methodology 
German guideline VDI 220
domain solution concept is
design is developed, which
and B. Haefner 
ew 
c Systems 
escribes an interdisciplinary field of engineering scien
sic disciplines of mechanical engineering, electrical en
chnology (cf. Figure 1) [1]. It is often defined as the syn
nical engineering with electronic and intelligent compu
anufacturing of industrial products and processes [2]. 
hat mechatronic systems provide the opportunity of fu
tegration, which is more than a mere addition of the dif
s also leads to a higher complexity of the systems. 
h of mechatronics can be also applied to miniaturized s
o systems are based on semiconductor production techn
thography, and etching). Besides classical semiconduc
ed for the production of integrated circuits (ICs) and mic
s (MEMS) devices, there are various other technolog
D micro-mechatronic systems, such as the LIGA technolo
gies (wire bonding, flip chip) [4], the Through-Silicon-V
s [5], the Molded Interconnect Device (MID) technolo
t Technology [7]. 
micro-mechatronic systems are accelerometers and gyr
ors in e.g. automotive and consumer electronics. 
 
Fig. 1. Definition Mechatronics 
ated Product and Production Design for Mechatro
ary nature, their cross-functional dependencies, and th
ion, micro-mechatronic systems usually are complex pr
nary product engineering processes, which is dealt with
grated view on the product engineering process [8]. 
for mechatronic systems has been standardized in 
06. By means of the so called V model, an iterative, cro
s presented (cf. Figure 2) [9]. Within this, first, a syst
h involves the segmentation of the overall function i
nces 
ngi-
ner-
uter 
unc-
ffer-
sys-
nol-
ctor 
cro-
gies 
ogy 
Vias 
ogy 
ros-
onic 
heir 
rod-
h by 
the  
oss-
tem 
into  

 
sub-functions. On this bas
integrated into a common p
further concretizing the fina
model also for the design o
tance of intermeshing produ
This integration, howev
(SE) approach. SE is chara
and parallel development o
product development proce
Fig. 2. V model for iterative
Fig. 3. Parallelization of pro
2.3 
Quality Assurance 
The production quality is a
systems. Nowadays, custom
al products, such as smart p
Design for Testability for Micro-mechatronic Systems 
sis domain-specific designs are elaborated and, hereaf
product. This process is repeated in several iteration cyc
al product. The guideline suggests the application of th
of the production system and comments on the great imp
uct and production design. 
er, is further elaborated in the Simultaneous Engineer
acterized by a target-oriented, interdisciplinary cooperat
of product, production and sales with regard to the en
ss by means of consequent project management [8, 10].
 
, cross-domain product development according to VDI 2206 [
 
oduct and production design by Simultaneous Engineering [10
of Micro-mechatronic Systems 
a very important characteristic for most micro-mechatro
mers require a 100% quality standard when buying techn
phones or video projectors. As the production quality of 
285 
fter, 
cles 
e V 
por-
ring 
tion 
ntire 
  
 
9] 
0] 
onic 
nic-
f the 

286 
G. Lanza, T. Blank, a
respective micro-mechatron
have to invest enormous eff
Quality can be defined 
customers’ requirements [1
based either on an inadequa
process. A zero defects qua
approach such as Total Qu
the product engineering pro
Obviously, it is most adv
their occurrence. Hence, th
tion Deployment (QFD), Pr
within the so called Design
(cf. Figure 4) [12]. The sam
process (e.g. Process FMEA
However, because of t
mechatronics and their com
volved production processe
detection of defects is cruci
be only realized by adequat
optical sensors (e.g. confo
force microscopes (AFM), o
Fig. 4. Quality assuranc
3 
Design for Testa
3.1 
Design for Testabil
In the product developmen
tion process is adjusted by t
al test devices (e.g. in-cir
product design of the circui
The objective of these de
tability (DFT), is to suppor
them easier, faster and che
became necessary due to th
sults in changes to the circu
and B. Haefner 
nic systems is crucial to their functionality, manufactur
forts to ensure the quality of their products. 
as the realized condition of a product in relation to 
11]. Quality defects of micro-mechatronic systems can
ate design of the product or a defect within the product
ality level usually can be only achieved by a comprehens
ality Management (TQM), which involves all steps wit
ocess [12] 
vantageous to prevent defects of a technical product bef
e application of preventive methods such as Quality Fu
roduct Failure Mode and Effects Analysis (Product FME
n for Quality approach in the design phase is very effect
me accounts for preventive methods during the product
A, Poka Yoke, Preventive Maintenance) [11]. 
the broad variety of production technologies in mic
mplexity, it is particularly difficult to control all the 
es of an entire micro-mechatronic system. Therefore, 
ial for the quality of micro-mechatronic systems, which 
te methods of production measurement technology such
ocal microscopes, interferometers), tactile sensors, atom
or scanning electron microscopes (SEM). 
 
ce within product development and the production process 
ability 
ity in Micro-electronics 
nt process of microelectronic circuits not only the prod
the implementation of testing equipment, such as functi
rcuit tests), but also certain considerations regarding 
it hardware are taken into account. 
esign techniques, which are summarized as Design for T
rt the application of manufacturing tests in order to m
eaper [13]. In particular, for microelectronic systems, 
he dramatic increase of ICs per hardware module and 
uit design on the board. 
rers 
the  
n be 
tion 
sive 
thin 
fore 
unc-
EA) 
tive 
tion 
cro-
in-
the 
can 
h as 
mic 
duc-
ion-
the 
Tes-
make 
this 
 re-

 
Design for Testability for Micro-mechatronic Systems 
287 
3.2 
Challenges of Micro-mechatronic Products and Possible Applications of 
Design for Testability 
For micro-mechatronic systems, which deal with micro-mechanical functionalities in 
addition to the aforementioned electrical ones, DFT would also be a suitable approach 
in order to support quality inspections within the production process. Especially for 
the micro-mechanical components of these systems, the manufacturing tests are  
often not taken into account explicitly within the product development process. 
Besides, even the electronic design within micro-mechatronic systems can require 
new approaches for Design for Testability. For instance, 3D-shaped Molded Intercon-
nect Devices (MID) or 3D integrated circuits with through-silicon-vias (TSV) archi-
tectures, which are becoming more and more important for the further miniaturization 
of micro-mechatronic systems, require new considerations to implement DFT [15]. If 
these technologies are used within micro-mechatronic systems, the complexity for 
testing and quality assurance is also increased. 
4 
Methodical Approach 
4.1 
Principles of Design for Testability for Micro-mechatronic Systems 
According to Weckenmann et al. the quality of a measurement result is influenced by 
the measurement object itself, the measurement procedure, the measurement device, 
the operator of the measurement device and the measurement environment [15]. The 
latter three of those influences can be optimized within the field of production  
metrology without crucial dependencies to the development process of the product. 
On the contrary, the properties of the product as a measurement object highly depend 
on its design. This, furthermore, has an influence on the choice of the measurement 
procedure, as well. Therefore, it is advantageous to take the requirements of the test-
ing into account already within the development process of the product. For micro-
mechatronic systems, this implies both, the mechanical design and the electrical  
circuit design. 
The degree of how suitable the design of a micro-mechatronic system is for its  
testing depends on many different aspects, which are specific to the respective appli-
cation and the involved measurement technologies. However, the following generic 
principles can be identified, which enhance the testability of various micro-
mechatronic systems: 
• high accessibility of the measurement attribute 
• high stability of the measurement conditions 
• high observability of the product condition 
• high durability of the product 
• low noise interference with the measurement due to the product 
• high in-line testability 
• high ability for test parallelization 
• short testing time 

288 
G. Lanza, T. Blank, and B. Haefner 
The accessibility of the measurement attribute is fulfilled, if the required information 
of the measurement attribute can be observed easily by means of the measurement 
device. While, for instance, the contacts of a standard surface-mounted device (SMD) 
can be inspected without difficulty by AOI, this is impossible for a ball grid array 
(BGA) with its contact balls face-down. 
Stability of the measurement conditions means that all relevant influences to the 
measurement remain sufficiently constant for a certain period which is necessary to 
achieve a valid measurement result. If e.g. the length of a very elastic MEMS canti-
lever, which cannot be fixed in a well-defined condition, was critical to quality, the 
stability of the measurement conditions would be an issue. 
The observability of the product condition during the measurement can be defined 
as the precise knowledge of the current state of the system during the measurement, 
e.g. the evaluation of the aforementioned MEMS, whether it is exactly straight during 
an optical measurement. 
The durability of the product refers to its reaction to the measurement device. E.g. 
a typical Through-Silicon-Via contact pad with a diameter of approximately 5 µm is 
easily damaged by the common contact pins of an electric inspection such as ICT. 
Noise interference with the measurement due to the product occurs, if parts of the 
product, which are not subject to the measurement, bias the measurement result, e.g. 
diffused light reflected by the product within an optical inspection. 
In-line testability, the ability for test parallelization and the testing time are criteria 
which determine the efficiency of the test. The total time required for all measurement 
tasks can be minimized, if the product design allows the inspection to be integrated in 
the work cycle of the production process, several measurement attributes to be ob-
served simultaneously and short testing periods. 
If the product design of a micro-mechatronic system complies with these principles 
of DFT, the quality of the measurement results can be enhanced significantly. 
4.2 
Integration of Design for Testability within the Product Engineering 
Process of Micro-mechatronic Systems 
The aforementioned principles of Design for Testability can be easily integrated into 
the state-of-the-art planning approaches of the product engineering process of micro-
mechatronic systems. In the following a methodology for their integration into the V 
model and the SE approach is presented. 
Within the V model, for an iterative development of the product design, the DFT 
principles serve as additional guidelines in the phase of the system design (cf. Figure 
2). The criteria build additional requirements, which have to be considered within the 
definition process of domain-specific sub-functions. By means of the derived sub-
functions they are incorporated into the domain-specific co-design of mechanical en-
gineering, electrical engineering, and computer science. Finally, these developments 
are integrated into the total product and the next cycle for further refinement starts. 
In parallel to the product design, the design of the production system is developed. 
As an iterative approach, the V model can be applied similarly. As the conception and 
installation of the required production measurement technology is also part of the 
production system, these are integrated within this process. 

 
Design for Testability for Micro-mechatronic Systems 
289 
According to the Simultaneous Engineering approach, the production system de-
sign starts as early as possible to enable a strong cooperation between product and 
production design (cf. Figure 3). Within this process, the product design according to 
the DFT principles and the composition of the production measurement technology 
are matched and adapted to each other in order so finally select an optimal configura-
tion for testing purposes. Considerations regarding preventive methods of quality  
assurance are combined within this process to determine which degree of testing is 
necessary (cf. Figure 4). 
Thus, in total, a TQM system covering the entire product engineering process of 
micro-mechatronic systems can be realized which smoothly integrates the Design for 
Testability approach. 
5 
Exemplary Applications of the Methodical Approach  
Currently, the Institute of Production Science (wbk) and the Institute for Data 
Processing and Electronics (IPE) at the Karlsruhe Institute of Technology (KIT) are 
working on possible implementations of the presented methodical approach. As a first 
demonstrator a micro vibration switch was realized at IPE, which is presented in the 
following. For further illustration, two additional commonly available systems are 
describes, which also comply with the aforementioned DFT principles. 
5.1 
Micro Vibration Switch 
A micro vibration switch is a low-cost micro sensing device to detect motion (cf. 
Figure 5) [16]. It is integrated in battery operated, movable system, to activate (or 
deactivate) the device, while the system is agitated. Typical applications are bicycle 
backlights, remote controls, GPS tracking systems and illuminated dog collars. The 
sensor is realized as a functional element in a PCB. By integrating a movable micro-
ball inside the PCB a resistance change can be detected, if the sensor is in motion. For 
efficient production, 1300 single sensors are arranged on a single production lot (cf. 
Figure 6). 
 
 
 
Fig. 5. Micro vibration switch, 
single sensor (with friendly admis-
sion of Sensolute GmbH) 
Fig. 6. Production lot of 
micro vibration switches 
Fig. 7. Test station for 
micro vibration switches 

290 
G. Lanza, T. Blank, a
In order to optimize the 
system was concurrently 
(cf. Figure 7). 
The co-design and co-de
on wafer level, where multi
to each other. Thus, DFT 
switches. In particular, it w
DFT criteria of high ability
design, 50 % of conductive
be reduced by a factor of 15
5.2 
Bosch Pressure Sen
The pressure sensor DS8 by
which is produced by 2-com
two polymers can be metall
connects the actual measu
automotive system. By mea
was achieved by Bosch. 
Despite the miniaturizat
mainly the criterion of the
neck. Quality inspection typ
be easily realized, as test p
AOI and that can be contac
line testing. 
Fig. 8. Design for Testabi
5.3 
3D Stacked ICs wit
Through-Silicon-Via (TSV
ICs in a space-efficient ma
(diameter 5 µm, pitch 10 µm
stacked on each other, cont
within very limited space. 
and B. Haefner 
testing of the sensors within the production process, a 
developed with the production layout of the sen
evelopment process lead to a patented sensor arrangem
iple sensors are electrically connected in series and para
could be realized very effectively for the micro vibrat
as possible to find an optimized solution with regard to 
y for test parallelization and short testing time. Due to 
e through holes could be saved and the testing time co
50. 1300 sensors are tested in less than 10 seconds [17]. 
nsor with MID Technology 
y Bosch for an automotive application is a 3D MID syst
mponent injection molding (cf. Figure 8) [18]. One of 
lized after molding, thus forming a 3D circuit board wh
uring cell and a printed circuit board (PCB) to the m
ans of the MID technology a miniaturization of the syst
tion, DFT is realized very sufficiently. For MID syste
e accessibility of the measurement attributes is the bot
pically is carried out by AOI and contact testing. Both 
pads have been designed that are all on the same plane 
ted from the outside. This also qualifies for parallel and
lity of Bosch pressure sensor DS8 with MID technology [16] 
th Through-Silicon-Vias 
) is a very capable technology for stacking and connect
anner (cf. Figure 9) [19]. TSVs are very thin connecti
m) which are drilled through silicon chips. Thus, if ICs 
tact pads of both sides of adjacent chips can be connec
test 
nsor  
ment 
allel 
tion 
the 
this 
ould 
em, 
the 
hich 
main  
tem 
ems 
ttle-
can 
for 
d in-
 
ting 
ions 
are 
cted 

 
For a complex system c
tial, which means that first 
system is iteratively combin
However, because of th
contact testing equipment s
accessed precisely and are 
and durability are not fulfill
Yet, a DFT-conform con
probe pads and the adaption
In-Self-Test) to the 3D circ
can be selected for larger p
ment. This selection is mad
Fig. 9. Design for Testabi
6 
Summary 
In conclusion, in this article
systems with a focus in defe
As a solution concept, 
electrical properties of mi
proach consisting of gen
mechatronic systems, as we
product engineering proces
illustrated by three exempl
realized by the Institute for
system, and a typical 3D St
Based on the experience
approach can be summariz
as it enables testing in case
and universal applicability 
model and SE. However, 
which typically involves s
useful, if a new product is d
Design for Testability for Micro-mechatronic Systems 
consisting of stacked ICs modular contact testing is ess
each chip has to be tested by pre-bond testing, before 
ned and tested by post-bond testing [19, 20]. 
he fine pitches and the fragile contacts of TSVs, comm
such as ICT is not applicable. The small contacts cannot
easily damaged. Hence, the DFT criteria of accessibi
led. 
nfiguration can be achieved by the implementation of lar
n of DFT techniques (Boundary Scan Test including Bu
cuit [19]. Only a limited number of contacts of the IC st
probe pads that can be accessed by the available test equ
de in accordance with a suitable DFT algorithm.   
 
lity of Bosch pressure sensor DS8 with MID technology [20] 
e the difficulty of quality assurance for micro-mechatro
ect detection is described. 
Design for Testability considering both mechanical 
icro-mechatronic systems is proposed. A methodical 
neric principles of Design for Testability for mic
ell as a methodology for the integration of DFT into th
s is presented. The implementation of the DFT approac
lary micro-mechatronic systems: a micro vibration swi
r Data Processing and Electronics (IPE), a Bosch 3D M
tacked ICs with TSVs. 
es with these applications, the main advantages of the n
ed by its high impact on difficult quality inspection tas
es, where this has been impossible before, and by its e
combined with standard engineering tools such as the
as the approach requires design changes to the prod
ignificant effort, from an economic viewpoint, it is o
developed or if there are crucial reasons for its redesign.
291 
sen-
the 
mon 
t be 
ility 
rger 
uild-
tack 
uip-
onic 
and 
ap-
cro-
heir 
ch is 
itch 
MID 
new 
sks, 
easy 
e V 
uct, 
only 
 

292 
G. Lanza, T. Blank, and B. Haefner 
References 
1. Schweitzer, G.: Mechatronik - Aufgaben und Lösungen. VDI-Berichte, vol. 787. VDI Ver-
lag, Düsseldorf (1989) 
2. Harashima, F., Tomizuka, M., Fukuda, T.: Mechatronics – “What Is It, Why, and How?’’: 
An Editorial. IEEE/ASME Transactions on Mechatronics 1(1), 1–4 (1996) 
3. Isermann, R.: Mechatronic systems, fundametals. Springer, London (2005) 
4. Menz, W., Mohr, J., Paul, O.: Mikrosystemtechnik für Ingenieure. Wiley-VCH, Weinheim 
(2005) 
5. Tummala, R.R., Swaminathan, M.: Introduction to System-on-Package (SOP): Miniaturi-
zation of the Entire System. McGraw-Hill, New York (2008) 
6. Feldmann, K.: Technologie 3D-MID. Hanser, München (2004) 
7. Fjelstad, J.: Flexible Circuit Technology, 3rd edn. BR Publishing, Seaside (2006) 
8. Ehrenspiel, K.: Integrierte Produktentwicklung. Hanser, München (2007) 
9. Verein Deutscher Ingenieure: VDI guideline 2206: Design methodology for mechatronic 
systems. Beuth, Berlin (2004) 
10. Eversheim, W., Bochtler, W., Gräßler, R., Kölscheid, W.: Simultaneous engineering ap-
proach to an integrated design and process planning. European Journal of Operational Re-
search 100, 327–337 (1997) 
11. Geiger, W., Kotte, W.: Handbuch Qualität, 4th edn. Vieweg, Wiesbaden (2005) 
12. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Konstruktionslehre: Grundlagen erfol-
greicher Produktentwicklung: Methoden und Anwendung, 7th edn. Springer, Berlin (2007) 
13. Wu, P.: Design for Testability. In: Seventh National Conference on Artificial Intelligence, 
pp. 358–363 (1988) 
14. Noia, B., Chakrabarty, K.: Testing and Design-for-Testability Techniques for 3D Inte-
grated Circuits. In: Twentieth Asian Test Symposium, pp. 474–479 (2011) 
15. Weckenmann, A., Gawande, B.: Koordinatenmesstechnik: Flexible Messstrategien für 
Maß, Form und Lage. Hanser, München (1999) 
16. Blank, T., Gemmeke, H., Kühner, T., Schlote-Holubek, K., Wüstling, S.: Von der Mikro-
komponente zum System: Aufbau hybrider Mikrosysteme am IPE. In: 10th GMM Work-
shop “Methoden und Werkzeuge für den Entwurf von Mikrosystemen“ (2004) 
17. Blank, T.: Test and Characterization of Omnidirectional Sensitive Micro Vibration Sen-
sors. Mechatronik Tage Karlsruhe (2008) 
18. Schlitzkus, M., Rohde, G.: MID Pressure Sensor for automotive application. In: 10th In-
ternational Congress Molded Interconnect Devices (2012) 
19. Marinissen, E.J., Zorian, Y.: Testing 3D Chips Containing Through-Silicon Vias. In: In-
ternational Test Conference (2009) 
20. Chakrabarty, K.: Testing of 3-D Stacking Devices. Tutorial. In: 21st IEEE North Atlantic 
Test Workshop (2012) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 293–302. 
DOI: 10.1007/978-3-642-30817-8_29 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Reference Framework for Manual Assembly Simulation 
Néstor Andrés Arteaga Martín1, Thomas Bär1, and Rainer Stark2 
1 Production Oriented Product Validation, Daimler AG, P.O. Box 2360, 89013 Ulm, Germany 
2 Fraunhofer Institute for Production Systems and Design Technology IPK,  
Pascalstraße 8-9, 10857 Berlin, Germany 
{nestor_andres.arteaga_martin,thomas.baer}@daimler.com, 
{rainer.stark}@ipk.fraunhofer.de 
Abstract. This paper starts highlighting the importance of simulating manual 
assembly processes in the automotive industry and shortly presenting current 
available modeling possibilities. It then introduces a reference framework for 
modeling and simulation in the context of manual assembly process verification 
and presents a set of available simulation approaches. It concludes by proposing 
a methodology to assess the quality of different methods and technologies to 
verify planned manual assembly processes without using the classical physical 
prototype based approach. 
Keywords: assembly simulation, manual assembly, automotive assembly. 
1 
Motivation 
1.1 
Increasing Product Variety and Manual Assembly 
Seeking to acquire key market shares, original equipment manufacturers in the auto-
motive industry are moving from mass production to mass customization offering 
more customized vehicles and more model variants. For instance, while in 1993 Mer-
cedes-Benz was offering nine models, by 2015 it is planning to add at least 10 new 
models to the 22 of its 2012 portfolio. Increasing product variety, the need to include 
more model variants into existing production lines and shorter production planning 
cycles, make faster and more secure ramp-up processes —which are demanded by 
modern automotive assembly systems— more difficult to achieve and affect the way 
in which automobiles are developed and verified [1-2]. 
Although product variety impacts all production departments, it must be noted that 
product customization is mostly handled at the final assembly stage where manual 
labor is mainly used [1]. Manual assembly enables model variety by e.g. allowing 
installation of different interior trimmings for sedan and station wagon, and customi-
zation by e.g. adding optional features like driver assistance systems without any 
changeover of the assembly system. Since in the foreseeable future it is not expected 
that any mechanical assembly system will have the flexibility of the human being [3], 
the human ability to work in a number of different workstations and to perform sev-
eral assembly tasks is probably the best way of handling the increasing demand for 
larger product variability and efficiently using available assembly capacities [1] [3]. 

294 
N.A.A. Martín, T. Bär, and R. Stark 
1.2 
Reducing Physical Prototypes 
Prototype building serves as an opportunity to evaluate, verify and optimize planned 
manual assembly processes. The assembly planning team conducts several prototype 
build workshops before the start of production using digital models, as well as early, 
confirmation, and preseries physical prototypes. The goal of these workshops is to 
achieve an optimal manual assembly process, through which efficient, stable and 
robust processes are defined, product quality is guaranteed, ergonomic conditions are 
optimal, and production ramp-up can be completed faster. Physical prototype building 
and testing is one of the major cost factors in vehicle development [4-5]. Reducing or 
completely eliminating these prototypes is in line with the goal and trend of producing 
more with less, i.e. less material and energy consumption, as well as less time and 
money [6-7]. 
In spite of the increasingly accurate digital models, and analysis and simulation 
tools used during product design and development, real tests on physical prototypes 
are still required at least for final product validation. Nevertheless, the increasing 
number of errors corrected using digital models has made the development process 
more cost and time efficient [5][7], e.g. in areas such as passive safety, the use of 
digital models and simulations has significantly reduced the use of physical proto-
types [4]. In vehicle assembly, digital buildability workshops are a well established 
process to check vehicle assembly feasibility before any physical prototypes are built 
[4-5]. As physical prototypes continue to be reduced, an increasing number of verifi-
cations and evaluations of manual assembly processes could no longer take place 
using these prototypes. Alternatives to efficiently and effectively evaluate and verify 
manual assembly processes as early as possible must be sought and evaluated [2], 
since waiting for the next available prototype will entail identifying assembly process 
issues on a later stage, cause ramp-up delays and increase development costs. 
2 
Reference Framework 
2.1 
Modeling Manual Assembly Processes 
So as to further reduce the number of physical prototypes, it is necessary to explore 
what possibilities exist to model and simulate manual assembly processes. Manual 
assembly processes comprise all assembly related operations carried out by a human 
worker without the use of automatic machines to bring assembly parts onto a base 
part in order to create a final product. The area where assembly takes place includes 
the space required for equipment and workers, as well as the space required for the 
storage of components and finished products [8]. 
The worker can be abstracted as a human representation. All other elements can be 
abstracted as objects. For modeling the worker it is possible to use a real human, a 
digital human model (DHM) controlled by digitalized real human movements, i.e. 
motion capture see e.g. [9], or a DHM whose movements are mainly computer gener-
ated, e.g. [10-11]. Motion capture allows depicting the movements of an actual human 
through a DHM, in this case the DHM becomes an avatar, i.e. a virtual human repre-
sentation controlled by a human. In contrast, following the manual or automatic  
approach the DHM becomes an agent, i.e. a human representation controlled by an 

 
A Reference Framework for Manual Assembly Simulation 
295 
algorithm [12]. To model the assembly relevant objects it is possible to use physical 
prototypes, physical mock-ups (PMU) or digital mock-ups (DMU). See Figure 1. 
2.2 
Reference Framework for Product Assessment 
The use of prototypes for product assessment is an established process in industry. 
Some applications include the evaluation of product design for usability, aesthetics, 
maintenance and ergonomics [13]. In [13] a reference framework for product assess-
ment considering physical and digital approaches is presented. At first two dimen-
sions are introduced, i.e. user and prototype, each dimension having the possibility to 
be real or virtual, thus yielding four possible approaches, i.e. real user–real prototype, 
real user–virtual prototype, virtual user–virtual prototype and virtual user–real proto-
type. It is also possible to have a mix of real and virtual prototypes, where an envi-
ronment is made up of both physical and digital objects. The amount of physical and 
digital objects present in the environment can vary from purely real to purely virtual. 
 
Fig. 1. Elements of manual assembly processes, abstraction, modeling possibilities and domain 
of action 
To further differentiate possible approaches, a third dimension, i.e. interaction, is 
also introduced. According to [13], the interaction of the real users with the proto-
types can either be direct or indirect. Indirect interaction refers to interaction that is 
mainly visual, while direct refers to visual interaction enriched by the possibility of 
touching the models. Mixed prototype scenarios based on interaction devices allowing 
the user to directly interact with mixed objects are also possible. 
Vision is the primary source of information about the outside world and it is com-
monly used to double-check the accuracy of other senses. Visual feedback from the 
virtual domain is possible through different devices like head-mounted displays, 
CAVE systems, large projection screens or high resolution monitors with or without 
stereoscopic eyeglasses, just to name a few [9]. 
Haptic feedback will enhance interaction by simulating touch and force and feed-
ing them back to the user via an input/output device which tracks the user’s intentions 
and provides a proper response [9]. Lindeman et al. [14] further differentiate between 

296 
N.A.A. Martín, T. Bär, and R. Stark 
passive and active haptic feedback. Passive haptic devices are physical objects that 
provide feedback simply by their shape, weight, or other inherent properties, while in 
active devices the feedback is computer generated. 
2.3 
Reference Framework for Manual Assembly Process Simulation 
Analogously to [13], three dimensions, i.e. the worker, the objects and the interaction, 
are defined for the reference framework for manual assembly process simulation. The 
objects and the worker in this case can take on three different levels, i.e. real, digital 
or mixed, and the interaction can either be visual or visual and haptic. Figure 2 shows 
the two basic dimensions, i.e. objects and worker. 
Digital
Real
Mixed
Digital
Real
Mixed
 
Fig. 2. Objects and worker dimension of the reference framework for manual assembly process 
simulation, (a) mixed objects and (b) mixed worker 
For the objects it is possible to take on three different levels, i.e. purely real objects 
like prototypes or PMUs, purely digital objects in the form of DMUs and a mix of 
both, in which e.g. dummy parts are tracked and visualized in a digital environment. 
As Figure 2 (a) shows, it is possible to have different levels of real and digital objects 
in the mixed scenario. Once manual assembly process verification begins, detailed 
digital models of the assembly parts and the base parts, as well as standard and carry 
over tools, equipment and elements of the materials zone are available. Detailed digi-
tal models of some special purpose tools like handling devices, or customized bins 
and racks might not be available, since these are usually the consequence of the as-
sembly verification process.  
During assembly simulation it will be possible to e.g. physically have a cardboard 
box representing the assembly part being investigated and a table depicting the racks 
on the material disposition zone, while all assembly relevant objects will be available 
as DMUs in the digital domain. In this case the mix of assembly relevant objects will 
be mostly made up of digital objects. If on the other hand, mostly physical objects are  
available and a few elements are digitally visualized and used during assembly simu-
lation, the mix will be made up of mostly physical objects, the dotted line in Figure 2 
(a) would depict such a situation. 

 
A Reference Framework for Manual Assembly Simulation 
297 
To represent the worker it is possible to use a real worker, who carries out the 
planned processes, a DHM in the form of an agent in the digital domain, or a DHM in 
the form of an avatar. Unlike in the objects dimension, the mixed worker in this case 
only takes on one mix level, see the dotted line in Figure 2 (b), which for the sake of 
simplicity will be 50% digital, i.e. the DHM, and 50% real, i.e. the real worker being 
tracked through motion capture. Nonetheless, it must be noted that it would also be 
possible to have other worker mix levels, e.g. if only a digital model of the hands is 
available in the digital domain, while a real worker is being tracked, this will be a mix 
made up of mostly a physical worker. The focus will be on complete DHMs, because 
not including the complete DHM will limit the verification possibilities of the mixed 
worker modelling approach. 
As in [13] two interaction modes are available, i.e. visual, and visual and haptic. 
Visual interaction occurs when e.g. a real worker interacts by means of a pointing 
device with DMU objects projected on a screen. If on the other hand, the worker han-
dles e.g. PMU or prototype parts, the interaction will be visual and haptic. 
Digital
Real
Mixed
Digital
Real
Mixed
 
Fig. 3. Manual assembly process simulation approaches with (a) visual interaction and (b) 
visual and haptic interaction 
As can be seen, it is possible to experiment with the models of the worker and the 
objects in combination with different technologies to simulate the manual assembly 
process. The domain where the simulation takes place could be physical, like in the 
current verification process, digital where digital models of the elements are used to 
depict the worker and its interactions with the objects, or a combination of both where 
actions/interactions of/with real objects are mapped to the digital models of the ele-
ments. Based on the points mentioned at the beginning of this section a framework 
depicting the possible manual simulation approaches is proposed, see Figure 3. These 
approaches are briefly described below in sections 2.4 and 2.5. 

298 
N.A.A. Martín, T. Bär, and R. Stark 
2.4 
Visual Interaction Approaches 
Digital worker–digital objects. Currently, manual workstations can be designed, visu-
alized and evaluated by means of DHMs, which can be used alongside the digital 
models of the workstation and the product to perform product interaction, posture, 
reachability, visibility, as well as assemblability analysis [10]. In practice such simu-
lations are static and mainly used for critical cases. Dynamic simulations require de-
fining key frames and letting the tools create the in betweens, which is very time  
consuming and could result in implausible or unnatural movements of the DHM. 
Available tools are complex and using them requires expertise in ergonomics, CAD 
skills and also detailed knowledge of the processes and products being evaluated [15]. 
Although solutions for the automated generation of dynamic simulations have been 
implemented [11], time consuming preparation, flexibility of the model, lack of real-
ism, lack of interactivity and lack of acceptance are the most prominent weaknesses 
of this approach. 
Mixed worker–digital objects. This is the case of motion capture based simulations, in 
which the interaction mainly occurs through visual feedback. This approach is based 
on the possibility of using motion capture technology to directly map human motion 
onto a DHM, and pointing devices to select and manipulate DMU objects. The motion 
capture driven DHM will interact with DMU objects in the digital domain according 
to the input given by the worker being tracked. This approach has the advantage of 
not requiring any modeling of the DHM movements, thus being faster and generating 
more confidence about the correctness of the results. Different technologies ranging 
from easy to use low cost markerless motion capture —the authors are working on 
solutions based on consumer electronics hardware— to more complex multi camera 
optical marker based motion capture systems can be used. 
Real worker–digital objects. In this case a real user interacts with the DMU objects 
without the mediation of any DHM mainly using visual feedback to perform the veri-
fication and usually a mouse, or other pointing device, to manipulate the objects. This 
is typical of, e.g. the digital buildability workshop, in which the assembly is digitally 
evaluated without specifically taking the human aspects into consideration. 
Real worker–mixed objects. In [16] an example of an augmented reality based system 
is presented in which hand gestures are identified and used to manipulate virtual tools 
and parts that are projected on a work table in order to complete and evaluate an as-
sembly task. In this case the interaction with the objects can be more intuitive and 
human factors aspects could be considered. 
Mixed worker-mixed objects. This is the case of the mixed worker–digital objects 
approach enriched with physical objects, i.e. the real worker–mixed objects approach. 
Since physical objects will provide cues about the virtual environment in the real 
world, the worker being tracked could concentrate more on the task at hand and less 
on orientating itself in the virtual environment. 

 
A Reference Framework for Manual Assembly Simulation 
299 
2.5 
Visual and Haptic Interaction Approaches 
Mixed worker–digital objects. This is the same case as the mixed worker–digital ob-
jects with visual interaction presented in 2.4 above, but enriched with haptic feedback 
from the virtual environment. Haptic interaction occurs through active haptic devices, 
like e.g. the Virtuose6D [17], which allow controlling the hand of a DHM to directly 
manipulate an object in the virtual environment. 
Real worker–digital objects. In contrast to the approach with only visual interaction, 
here it is possible to manipulate the DMU objects using haptic interfaces. As in the 
case immediately presented above active haptic devices are used, but no DHM is 
present in the digital domain. 
Real worker–mixed objects. Having a mix of virtual and real objects allows enriching 
haptic interaction through prototype and PMU parts. Active haptic interfaces can be 
used, e.g. the smart hybrid prototyping approach for product design presented in [18] 
can be adapted to be used for manual assembly verification. Passive haptic approach-
es are also possible in which prototype or PMU parts depicting the form and/or 
weight of the digital objects are tracked. 
Mixed worker–mixed objects. This can be seen as the combination of the mixed work-
er–digital objects and the real worker–mixed objects approaches. It enriches the  
virtual environment and the motion capture with physical objects, thus making inte-
raction with the digital domain more intuitive and allowing the real worker to concen-
trate more on the tasks to be verified and less on orienting themselves in the virtual 
environment. 
Real worker–real objects. This is the case of the current assembly process verification 
workshops. In this case the simulation is carried out building a prototype under simu-
lated, i.e. non-series, conditions. 
2.6 
Not Relevant Approaches 
Digital worker-real objects/mixed objects. Although physical agents could be used 
along real objects for product assessment [13], in the case of manual assembly it will 
not be practical. 
Mixed worker-real objects. Depicting the movements of a worker through a DHM 
while performing assembly tasks with real objects, if not accompanied by the DMU 
representations, will not immediately add any value to the manual assembly process 
verification workshop. 
3 
Evaluating the Different Approaches 
Since the purpose of a simulation is to reproduce some of the characteristics of a situ-
ation or a system, e.g. manual assembly processes, without reproducing others, e.g. 
the costs of prototypes, the simulation cannot duplicate the system it simulates, i.e. the 
stimulus on the simulator will never be identical to those of the simulated system. In 
order to evaluate stimulus fidelity it is possible to split it into two categories, i.e. fidel-
ity of subjective experience or experiential fidelity also referred as presence, and  
fidelity of performance or action fidelity. Metrics of action fidelity are more useful as 

300 
N.A.A. Martín, T. Bär, and R. Stark 
constraints on the design and evaluation of simulators. Action fidelity exists when 
performance in the simulator transfers to behavior in the simulated system and is 
measured in terms of task performance. Since transfer of skills from the simulator to 
the simulated system may occur despite departures from stimulus fidelity, it is ex-
pected that a valid simulation does not necessarily need to occur inside a high end 
fully immersive virtual environment [19]. 
So as to evaluate how well a simulation approach represents the system, it will be 
possible to set up different manual assembly scenarios with known issues related to 
the manual assembly process verification requirements. The focus will be on process 
description and ergonomics. For process description two critical issues will be eva-
luated, i.e. process completeness and assembly order. For ergonomics reachability 
will be the main focus. Addressing process completeness and assembly order assesses 
how well the given simulation approach depicts the assembly task allowing the work-
er and the observers to identify the completeness of the process and the feasibility of 
the assembly order. On the other hand, a reachability issue directly affects the ergo-
nomic wellness of an assembly operation and could indirectly affect process descrip-
tion. For instance, if an operation is planned to be completed without needing to step 
inside the vehicle, but this is not possible, the result will be an incomplete process 
description as well as a critical ergonomic assessment. In this case the ability of the 
simulator to correctly depict the assembly situation allowing or not the worker and the 
observers to identify the reachability issue is assessed. 
These scenarios will be then modeled in different forms and simulated with differ-
ent approaches. If the focus falls on action fidelity, i.e. task performance, it can be 
said that the more issues that are identified on a manual assembly process simulator, 
the better the approach can address the verification requirements. Other task perfor-
mance indicators such as time to completion might as well be considered. It must be 
noted that in practice assembly planners tend to favor simple and easy to use solutions 
allowing them to achieve their objectives in the most efficient way, hence the impor-
tance of understanding what elements of the simulator play a key role in addressing 
the manual assembly verification goals. Therefore, the focus will be on comparing 
low cost – low fidelity and high end – high fidelity simulators and in understanding 
how much value different experiential fidelity levels deliver to the end user, i.e. the 
assembly planning team, so as to establish the most relevant aspects to consider when 
designing manual assembly simulators.  
4 
Conclusions 
There are many possibilities to address the reduction of physical prototypes for ma-
nual assembly process verification. As can be seen in Figure 4, these possibilities 
cover a substantial portion of the proposed reference framework. It is worth noting 
that 100% coverage of the topics addressed during the prototype based verification is 
not possible using the alternative methods and technologies. These alternative ap-
proaches have their limitations and cannot fully simulate all the interactions and ef-
fects arising during assembly [10] [19]. Some technologies and methods are more 
adequate to address a certain type of evaluation or verification task. For instance, an 

 
A Reference Framework for Manual Assembly Simulation 
301 
undesired posture during material collection is relatively easy to spot by manually 
modeling a DHM. Using motion capture to evaluate if getting inside the car is re-
quired to complete a task is faster and more accepted than modeling the DHM by 
hand. 
Digital
Real
Mixed
 
Fig. 4. Possible approaches for manual assembly process simulation according to the proposed 
reference framework 
As it has been presented, there are many ways in which such a simulation can be 
built as well as technologies that could be used to evaluate and verify the planned 
assembly processes. They range from purely digital simulation of the assembly 
process modeling DHMs by hand to e.g. the use of high fidelity PMUs of the entire 
assembly in a fully immersive virtual environment with active haptic interaction ca-
pabilities. Therefore, from all the probable ways in which the digital and physical 
models could be used to simulate the manual assembly processes, it is not possible to 
establish a priori which combination will best suit the challenges and requirements of 
the assembly planning teams during manual assembly process verification. Careful 
exploration of available alternatives needs to be addressed so as to better understand 
the effects of different alternatives —e.g. the fidelity of PMUs or active vs. passive 
haptic feedback— on the value the simulation delivers to the user, as well as on the 
efficiency of the verification process. 
References 
1. Michalos, G., Makris, S., Papakostas, N., Mourtzis, D., Chryssolouris, G.: Automotive as-
sembly technologies review: challenges and outlook for a flexible and adaptive approach. 
CIRP Journal of Manufacturing Science and Technology 2(2), 81–91 (2010) 
2. Maropoulos, P.G., Ceglarek, D.: Design verification and validation in product lifecycle. 
CIRP Annals - Manufacturing Technology 59(2), 40–759 (2010) 

302 
N.A.A. Martín, T. Bär, and R. Stark 
3. Feldmann, K., Slama, S.: Highly flexible Assembly – Scope and Justification. CIRP An-
nals - Manufacturing Technology 50(2), 489–498 (2001) 
4. Weber, J.: Automotive Development Processes - Processes for Successful Customer 
Oriented Vehicle Development. Springer, Heidelberg (2009) 
5. Morello, L., Rosti Rossini, L., Pia, G., Tonoli, A.: The Automotive Body - Volume I: 
Components Design. Springer Science + Business Media B.V. (2011)  
6. Chryssolouris, G., Papakostas, N., Mavrikios, D.: A perspective on manufacturing strate-
gy: Produce more with less. CIRP Journal of Manufacturing Science and Technology 1(1), 
45–52 (2008) 
7. Stark, R., Krause, F.-L., Kind, C., Rothenburg, U., Müller, P., Hayka, H., Stöckert, H.: 
Competing in engineering design—The role of Virtual Product Creation. CIRP Journal of 
Manufacturing Science and Technology 3(3), 175–184 (2010) 
8. CIRP: Dictionary of Production Engineering. Springer, Heidelberg (2012) 
9. Gutiérrez, M.A., Vexo, F., Thalmann, D.: Stepping into Virtual Reality. Springer-Verlag 
London Limited, London (2008) 
10. Lämkull, D., Hanson, L., Örtengren, R.: A comparative study of digital human modelling 
simulation results and their outcomes in reality: A case study within manual assembly of 
automobiles. International Journal of Industrial Ergonomics 39(2), 428–441 (2009) 
11. Fritzsche, L., Jendrusch, R., Leidholdt, W., Bauer, S., Jäckel, T., Pirger, A.: Introducing 
ema (Editor for Manual Work Activities) – A New Tool for Enhancing Accuracy and Ef-
ficiency of Human Simulations in Digital Production Planning. In: Duffy, V.G. (ed.) 
ICDHM 2011. LNCS, vol. 6777, pp. 272–281. Springer, Heidelberg (2011) 
12. Fox, J., Arena, D., Bailenson, J.N.: Virtual Reality - A Survival Guide for the Social 
Scientist. Journal of Media Psychology: Theories, Methods, and Applications 21(3),  
95–113 (2009) 
13. Bordegoni, M., Cugini, U., Caruso, G., Polistina, S.: Mixed prototyping for product as-
sessment: a reference Framework. International Journal on Interactive Design and Manu-
facturing 3(3), 177–187 (2009) 
14. Lindeman, R.W., Sibert, J.L., Hahn, J.K.: Hand-held windows: Towards effective 2D inte-
raction in immersive virtual environments. In: IEEE Virtual Reality 1999, pp. 205–212 
(1999) 
15. Lockett, J.F., Assmann, E., Green, R., Reed, M.P., Raschke, U., Verriest, J.-P.: Digital 
Human Modelling Research and Development User Needs Panel. In: 2005 SAE Digital 
Human Modeling for Design and Engineering Symposium, Iowa-city (2005) 
16. Ong, S.K., Wang, Z.B.: Augmented assembly technologies based on 3D bare-hand interac-
tion. CIRP Annals – Manufacturing Technology 60, 1–4 (2011) 
17. Haption, S.A.: Hardware, http://www.haption.com/site/index.php/en/ 
products-menu-en/hardware-menu-en 
18. Stark, R., Beckmann-Dobrev, B., Schulze, E.-E., Adenauer, J., Israel, J.H.: Smart Hybrid 
Prototyping zur multimodalen Erlebbarkeit virtueller Prototypen innerhalb der Produk-
tentstehung. In: Lichtenstein, A. (ed.) Der Mensch im Mittelpunkt technischer Systeme. 8. 
Berliner Werkstatt Mensch-Maschine-Systeme 2009. VDI-Verlag, Berlin (2009) 
19. Stoffregen, T.A., Pagulayan, R., Smart, L.J., Bardy, B.G.: On the Nature and Evaluation of 
Fidelity in Virtual Environments. In: Hettinger, L.J., Haas, M. (eds.) Virtual and Adaptive 
Environments, Applications, Implications, and Human Performance Issues, pp. 111–128. 
LEA Publishers Mahwah, New Jersey (2003) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 303–313. 
DOI: 10.1007/978-3-642-30817-8_30 
© Springer-Verlag Berlin Heidelberg 2013 
 
Product Assembly Information to Improve Virtual 
Product Development 
Martin Eigner1, Joscha Ernst1, Daniil Roubanov1, Jochen Deuse2,  
Julian Schallow2, and Olga Erohin2 
1 Institute for Virtual Product Engineering, TU Kaiserslautern University,  
Gottlieb-Daimler-Straße 44, Kaiserslautern, Germany 
2 Institute of Production Systems, Chair of Industrial Engineering, TU Dortmund University, 
Leonhard-Euler-Straße 5, Dortmund, Germany 
{joscha.ernst,roubanov}@mv.uni-kl.de  
Abstract. Integrating product engineering and process planning (e. g. assembly 
process planning) is a relevant research theme of the last years. Primary objec-
tive is the reduction of costs and time-to-market in modern product engineering 
processes. In this paper the concept of Product Assembly Information is pre-
sented to support the assembly-oriented product engineering and to reinforce 
the integration of product development and assembly process planning. Product 
Assembly Information contains assembly time-relevant product criteria and is 
an essential part of continuous data exchange between product development and 
process planning. 
Keywords: Product Assembly Information, Assembly Feature, Connecting 
Element, Improvement of CAD, Virtual Product Development. 
1 
Introduction 
Within the last few years research activities in the field of virtual product engineering 
are focusing on a stronger parallelization of product development and process plan-
ning. Main aim is the reduction of total planning time and therefore of time-to-market 
for new products. For this purpose a lot of methods and concepts such as Concurrent 
or Simultaneous Engineering, Frontloading, Rapid Prototyping or Design for X ap-
proaches were developed and practically applied [1-3]. 
In order to realize a significant reduction of time-to-market methods and tools of 
information technology are used to enforce the integration of different planning tasks 
within the product engineering process (PEP). Especially crucial in this context is the 
integration of product development and assembly process planning [4], [5]. The pro-
duction costs are mainly determined during the early phases of the PEP where a lot of 
far reaching strategic decisions have to be made. Therefore it is of primary impor-
tance to develop decision and planning support for those early planning stages [6]. 
One concept to integrate assembly planning aspects in product development was de-
veloped in the late 1980s and is known as “Design for Assembly” [2], [7]. It provides 

304 
M. Eigner et al. 
product development with some general recommendations for an assembly-oriented 
design. A complete and data-based integration of this concept is still often missing. 
Therefore this paper presents aims and potentials for the integration of assembly-
relevant knowledge in the product development process by defining and implement-
ing Product Assembly Information (PAI). In order to illustrate the scope of the paper 
Section 2 surveys the state of the art and identifies need for research. Section 3 
presents the concept of PAI, including its definition, application and implementation. 
In Section 4 the case study for connecting elements is realized. Finally, Section 5 
summarizes the results and gives a scientific outlook for further developments. 
2 
State of the Art and Motivation 
2.1 
Current Integration Gaps within PEP 
Currently existing gaps between product development and assembly planning are 
caused not only by conceptual weak points, but also by the two-part IT infrastructure 
in manufacturing industry. On the one hand, Computer Aided Design (CAD) and 
Product Data Management (PDM) systems provide support to product development 
processes. On the other hand, e. g. Enterprise Resource Planning (ERP) systems ad-
ministrate the continuously increasing amount of production-related data and support 
operative production processes (see Figure 1).  
 
Fig. 1. Two-part IT infrastructure in manufacturing industry [8] 
Due to the integration of product and process planning the concepts of Product  
Lifecycle Management (PLM) and Digital Factory (DF) were developed and are of 
high relevance nowadays [9], [10]. Thus, PLM and DF solutions represent the current 
approaches to provide integrated IT infrastructure in the context of Simultaneous 
Engineering. In spite of their high acceptance and wide dissemination in manufactur-
ing companies the utilization of assembly-relevant knowledge, e. g. in form of assem-
bly features, is not often realized in a structured way [11]. A typical example for an 
integrated utilization of directly represented (explicit), manufacturing-relevant know-
ledge is the creation of a numerical control (NC) work plan based on a CAD model 
[12], [13]. Due to the complexity of the application field assembly process planning, a 

 
Product Assembly Information to Improve Virtual Product Development 
305 
uniform representation of assembly-relevant information and its consistent usage 
along the PEP are not established yet. 
2.2 
Initial Approaches in Product Development 
The classical tasks of assembly planning, the creation of a Manufacturing Bill of Ma-
terials (MBOM) based on an Engineering Bill of Materials (EBOM), is much more 
focused on using assembly process planning expert know-how. An explicit descrip-
tion of the planning procedures and the planning logic is often not available. As a 
consequence, this planning knowledge remains personalized and therefore implicit.  
Nevertheless, some approaches for an explicit documentation of assembly-relevant 
parameters in CAD systems for example in features exist [14], [15]. However their 
consistent usage in practical application is still limited [11]. For example Siemens - 
Solid Edge ST 4 contains the assembly feature “screw connection” to select the re-
quired nuts, washers, bolts, etc. After the joint is completed and the feature assistant is 
closed, Solid Edge creates a new node in CAD according to the new joint, which con-
tains all participant parts (see Figure 2). 
 
Fig. 2. Screw connection in Solid Edge ST4  
The main weakness of CAD assembly features is their rare utilization in the ongo-
ing PEP. In most cases feature information is not consistently used in assembly 
process planning. This is caused by system interfaces and consequential information 
loss during data exchange e. g. between CAD and PDM system. In this example, the 
information transferred is often limited to a pure Bill of Materials (BOM) without 
detailed assembly specification. 
2.3 
Information Content within PEP 
The illustrated problem of maintaining information available in early stages of prod-
uct development for downstream planning processes (e. g. assembly planning) is not 
exclusively caused by the two-part IT infrastructure. Product development cannot 
gather all important information which is required for assembly planning. A certain 
set of information can only be added during the assembly planning process. This 
enrichment of assembly planning information on the basis of product development 
data is the primary task of assembly planning departments. The know-how gap of 
determined and required information cannot be bridged only by IT systems  
themselves.  

306 
M. Eigner et al. 
Additionally this gap between required and available information is often enlarged 
by insufficient utilization of already existing information, not only on the system 
boundary, but even within product development and process planning. For example 
assembly-relevant product development information is often not explicitly stored in 
IT systems (e. g. CAD, PDM) or saved unstructured and hence not locatable. This 
results in a non-ideal behavior of the information curves in Figure 3. This effect is 
enforced by the already mentioned two-part IT infrastructure and herewith a “wall” 
between product development and (assembly) planning departments. This leads to a 
growing lack of planning information which has to be added by the assembly plan-
ning under steadily increasing time and cost pressure. 
 
Fig. 3. Information contents of product development and assembly planning 
To face the described challenges, gathering and storing of assembly-relevant in-
formation within PEP has to be supported. On the one hand, the concept of PAI (as 
detailed later on in this paper) will offer the opportunity of conserving and structuring 
assembly-relevant information, as it is already available in product development. On 
the other hand, it will structure the enrichment of this information in downstream 
planning processes in close collaboration between product development and assembly 
planning. The current state of information content development along the PEP and 
anticipated improvements realized by PAI concept in future are illustrated in Figure 3. 
2.4 
Requirements Specification for PAI Conception 
To summarize the identified need for research methodical requirements (primarily the 
essential content and organizational integration) and technical requirements (issues of 
IT supported PAI realization and implementation) for the development of PAI are 
detailed in following. Methodical requirements are: 

 
Product Assembly Information to Improve Virtual Product Development 
307 
• Providing the fundamental opportunity for additional input and exchange of as-
sembly-relevant (in part implicit). 
• Minimizing the effort for the enrichment of planning information. 
• Defining assembly-relevant information in structured and standardized way. 
• Enabling both standard contents of PAI and company-specific adjustments. 
Following technical requirements can be identified on this stage of PAI development: 
• Dealing with machine-readable, standard data formats to be stored and applied in 
different IT systems. 
• Allowing information transfer across the variety of PEP-relevant IT systems. 
• Enabling context-based and problem-related representation of planning informa-
tion along the PEP. 
• Creating a basis for the application of Data mining on assembly-relevant data. 
3 
Concept of Product Assembly Information (PAI)  
As described in Section 2, the transfer of information within PEP is still a key chal-
lenge for manufacturing industry. To address this issue, it is necessary to create a 
methodical and systemic way to store assembly-relevant information. Therefore, the 
Institute for Virtual Product Engineering (VPE, TU Kaiserslautern) in collaboration 
with the Chair of Industrial Engineering (APS, TU Dortmund) developed the concept 
of PAI in order to extend existing PDM data models. PAI contains and structures all 
assembly-relevant information of an assembly or a part (analog to the idea of Product 
Manufacturing Information [16]). This information can vary depending on the func-
tional behavior of the part or its hierarchical position in the product structure. 
3.1 
Definition of PAI Content 
There are two different general approaches to define the information content which 
has to be represented within PAI (see Figure 4). The first one is to develop PAI based 
on know-how from product development and assembly process planning, the so 
called knowledge-based compilation of PAI. For this aim all assembly-relevant  
aspects and their typical values have to be defined and aggregated in attributes befo-
rehand. Deriving specific values for the predefined attributes can be automatically 
detected from CAD system, selected or added by product designer. 
The second approach follows the data-based definition of relevant attributes and 
their value classes. Therefore, the application of machine learning techniques is re-
quired for analyzing digital databases of different IT systems in order to identify and 
adjust relevant attributes. 
Both ways to define the required PAI can lead to one and the same or two different 
PAI contents (attributes and value classes) depending on their data- or knowledge-
based creation. To get valid and usable PAI both approaches have to be combined. 
The comparison of the results will lead to a PAI content meeting the requirements of 
both planning processes and their data-based support within the PEP. 

308 
M. Eigner et al. 
Fig. 4. Con
3.2 
Application of PAI 
To transfer assembly-relev
process planning, the conce
assembly information of a j
A PAI consists of one or
vant information about a sc
data unit: Nm; data value
relevant data as a comment
planers e. g. determine the a
which are sometimes only
copy. This disadvantage can
tion about connecting elem
by process planers (see Figu
Fig. 5
ncept of Product Assembly Information (PAI) 
vant information from product development to assem
ept of PAI is utilized. Thereby PAI contains and structu
oint or a connecting element.  
r more triple of attribute, unit and value. For example, re
crew torque has the following content: data name: torq
: 19. Nowadays, product designers often paste assemb
t on engineering drawings. Later in PEP assembly proc
assembly time by searching and analyzing these comme
y accessible as non-digital engineering drawings in h
n be eliminated by application of PAI. As soon as inform
ments is available, it can be extracted from the PDM syst
ure 5). 
 
5. Improved information transfer via PAI 
 
mbly 
ures 
ele-
que; 
bly-
cess 
ents 
hard 
ma-
tem 

 
Product Assembl
3.3 
Implementation of 
For the implementation 
essential. Therefore, releva
CAD.  
The example in Figure 6
CAD. Thereby, assembly a
fined attributes and highligh
head is very high, so the 
achieve an assembly improv
controlled torque are defin
clamation mark in the line 
sembly. Combining and vi
window (see Figure 6) allo
could be a reduction of wei
Fig. 
4 
Case Study for C
Section 3.1 presented two a
case study a knowledge-ba
connecting elements are ex
and their suitability for the
selected, because they are
products and often specified
4.1 
Classification of Co
Connecting elements can b
principle in a not-detachab
tachable connecting eleme
 
ly Information to Improve Virtual Product Development 
PAI in CAD 
of PAI, PDM and CAD data model extensions 
ant attributes can be attached as an assistant function
6 shows the assembly of motor block and cylinder head
assistant function is represented as a dialog checking 
hting problems. In this example, the weight of the cylin
assistant function of CAD suggests weight reduction
vement. In addition, parameters e. g. number of screws 
ed and their values traced (marked with a check). The 
additional elements indicates a missing washer in the 
isualizing all assembly-relevant information in one sin
ows the identification of potentials for improvement. T
ght and an accomplishment of washers.  
 
6. PAI-based assistant function in CAD 
Connecting Elements 
alternatives for detecting PAI-relevant information. In 
ased definition of attributes is realized. For this purpo
xamined regarding their assembly-relevant characteris
e development of PAI. Thereby connecting elements 
e frequently applied during the design of manufactur
d in a standardized way. 
onnecting Elements 
be classified according to the detachment and operat
ble and a detachable category (see Figure 7). Thereby 
ents are subdivided in two classes: non-destructive 
309 
are  
n in 
d in 
de-
nder 
n to 
and 
ex-
as-
ngle 
This 
this 
ose, 
stics 
are 
ring 
ting 
de-
and  

310 
M. Eigner et al. 
 
Fig. 7. Classification of connecting elements [17] 
destructive detachable ones. The first class of connection can be reused after  
disassembly. Destructive connections in contrast are developed for only one-time 
assembly and are destroyed during a disassembly process. A rivet connection e. g. 
cannot be reused, because it must be drilled during the disassembly process.  
The most frequently used connecting elements in mechanical engineering are bolts 
and screws [17]. Therefore, assembly-relevant information of screw connection is 
analyzed in the following to build a basis for the development of screw-PAI. The aim 
of analysis is to detect attributes, which describe a screw connection from both engi-
neering and assembly planning view.  
4.2 
Attributes and Values of Screw Connections 
Table 1 illustrates an extract of assembly-relevant attributes of a screw connection 
which have been identified knowledge-based considering the rules of Methods-Time 
Measurement (MTM). MTM is a Predetermined Motion Time System (PMTS) and 
offers a standard time data catalogue for manual assembly operations depending on 
different product and process attributes.  
First assembly-relevant attributes and values of screw elements (see first two col-
umns in Table 1) are gathered based on product development and process planning 
know-how. Secondly identified attributes are analyzed concerning their information 
sources. According to Figure 4, the identified attributes are categorized (see Table 1) 
in information which can be automatically extracted from a CAD-Part (column 3); 
needs product engineering know-how to be extracted (column 4) or needs to be de-
fined by assembly planning (column 5). 
 
 
 

 
Product Assembly Information to Improve Virtual Product Development 
311 
Table 1. Example of attributes and values of a screw connection 
Attributes 
Values 
Source of information  
CAD 
Product 
engineering 
Assembly 
planning 
additional elements 
washer, nut 
x 
  
x 
automatic screwing 
automatic/drill screwdriver 
  
  
x 
controlled torque 
yes/no 
x 
  
  
glove used 
yes/no 
  
  
x 
lack of space 
yes/no 
  
x 
x 
magnetic screw 
yes/no 
  
x 
  
manual screwing 
cross recess; open-end spanner; … 
  
  
x 
material 
metal sheet, wood… 
x 
  
  
obstructed view 
yes/no 
  
  
x 
oiled parts/ lock Tide 
yes/no 
  
x 
x 
screw head diameter 
mm 
x 
  
  
screw type 
tapping screw, metric screw 
x 
  
x 
thread pitch search 
yes/no 
  
  
x 
thread type 
fine thread / coarse thread 
x 
 
x  
5 
Summary, Further Development and Scientific Outlook 
The concept of PAI is a standardized and aggregated way to describe and store as-
sembly-relevant information within PEP. PAI will scale down the “wall” still existing 
between product development and assembly process planning. The uniform represen-
tation of PAI is not limited to the use within CAD systems for product design support. 
It is supposed to be an adequate extension for current PDM systems as connector to 
further downstream planning solutions as well. The paper presents connecting ele-
ments, their classification and standardized description as a possible key structure for 
an applicable assembly planning support by PAI. An exemplary case study identifies 
assembly-relevant parameters for screw connections. The classification of these pa-
rameters gives an idea of further potentials of PAI-usage within PEP.  
The technical feasibility of the proposed solution has to be shown by implementa-
tion of the PAI concept in CAD and PDM systems. As a scientific addition, the possi-
bilities of modern machine learning applications have to be implemented within the 
concept. In the future Data Mining for example could help to overcome the disadvan-
tages of high initial efforts in defining relevant PAI content. It can also reduce insuffi-
cient company-specific planning support, leading to a more flexible and adoptable 

312 
M. Eigner et al. 
concept for the usage of PAI. Especially important in this context is the data-based 
identification and standardized representation of assembly-relevant information with-
in the huge amount of digital data. For an improved utilization of the PAI concept 
new methods for analyzing assembly-relevant information stored in existing PAI 
should be developed and applied. This will raise the final scientific question of a stan-
dardized representation and seamless usage of information in following planning 
processes as realized in DF or PLM solutions. 
Acknowledgments. This paper represents the background, objectives and first results 
of the research project “Prospective Determination of assembly work content in Digi-
tal Factory (Pro Mondi)”. This research and development project is funded by the 
German Federal Ministry of Education and Research (BMBF) within the Framework 
Concept ”Research for Tomorrow’s Production” (funding number 02PJ1110) and 
managed by the Project Management Agency Karlsruhe (PTKA). The authors are 
responsible for the contents of this publication. 
References 
1. Bretsche, B., Bullinger, H.-J.: Entwicklung und Erprobung innovativer Produkte - Rapid 
Prototyping: Grundlagen, Rahmenbedingungen und Realisierung. Springer, Berlin (2007) 
2. Ponn, J., Lindemann, U.: Konzeptentwicklung und Gestaltung technischer Produkte: Sys-
tematisch von Anforderungen zu Konzepten und Gestaltlösungen, pp. 225–245. Springer, 
Heidelberg (2011) 
3. Erohin, O., Kuhlang, P., Schallow, J., Deuse, J.: Intelligent Utilisation of Digital Databases 
for Assembly Time Determination in Early Phases of Product Emergence. In: Procedia 
CIRP- 45th CIRP Conference on Manufacturing Systems 2012, vol. 3, pp. 424–429 (2012) 
4. Bley, H., Franke, C.: Integration of Product Design and Assembly Planning in the Digital 
Factory. Annals of the CIRP 53/1, 25–30 (2004) 
5. Petzelt, D., Schallow, J., Deuse, J., Ferstl, H.: Produktionsgerechte Produkte durch tech-
nische Mitgestaltung aus der Produktionsplanung. In: ZWF, vol. 100, pp. 988–992 (2009) 
6. Anderl, R., Eigner, M., Sendler, U., Stark, R.: Smart Engineering - Interdisziplinäre Pro-
duktentstehung (acatech DISKUSSION). Springer, Heidelberg (2012) 
7. Molloy, E., Yang, H., Browne, J.: Feature-based modeling in design for assembly. Interna-
tional Journal of Computer Integrated Manufacturing 6(1-2), 119–125 (1993) 
8. Eigner, M., Ernst, J., Roubanov, D.: Erfahrungsbasierte Unterstützung des Entwick-
lungsprozesses mit Fokus auf Verbindungsauslegung. In: DfX-Symposium, Bamberg, Oc-
tober 4-5 (2012) 
9. Eigner, M., Stelzer, R.: Product Lifecycle Management. Springer, Berlin (2009) 
10. VDI 4499: Digital Factory – Fundamentals. Beuth, Berlin (2008) 
11. Vajna, S., Weber, C., Bley, H., Zeman, H.: CAx für Ingenieure – Eine praxisbezogene 
Einführung, pp. 391–404. Springer, Heidelberg (2009) 
12. Hehenberger, P.: Computerunterstützte Fertigung – Eine kompakte Einführung. Springer, 
Heidelberg (2011) 
13. Berger, U., Kretzschmann, R., Arnold, K.-P.: A Heuristic STEP-NC Based Process Plan-
ning Tool For Sequencing NC-Machining Operations. In: Xun, X., Nee, A. (eds.) Ad-
vanced Design and Manufacturing Based On STEP, pp. 49–78. Springer, London (2009) 

 
Product Assembly Information to Improve Virtual Product Development 
313 
14. VDI 2218: Information technology in product development – Feature Technology. Beuth, 
Berlin (2003) 
15. Eigner, M., Handschuh, S.: Gerhardt, Fl.: Concept to Enrichen Lightweight, Neutral Data 
Formats with CAD-based Feature Technology. Computer-Aided Design & Applica-
tions 7(1), 89–99 (2010) 
16. Andre, P., Sorito, R.: Product Manufacturing Information (PMI) in 3D models: a basis for 
collaborative engineering in Product Creation Process (PCP). In: Proceedings 14th Euro-
pean Simulation Symposium, SCS Europe BVBA (2002) 
17. Albers, A., Sauer, B., Steinhilper, W.: Konstruktionselemente des Maschinenbaus. Sprin-
ger, Berlin (2008) 

Assisted Decision-Making for Assembly Technique
Selection and Geometrical Tolerance Allocation
Lo¨ıc Andolfatto1,2, Franc¸ois Thi´ebaut2,3, Claire Lartigue2,3, and Marc Douilly1
1 EADS Innovation Works, 12 rue Pasteur, 92150 Suresnes, France
2 LURPA, ENS de Cachan, 61 avenue du Pr´esident Wilson, 94235 Cachan Cedex, France
3 IUT de Cachan, 9 Avenue de la Division Leclerc, 94235 Cachan Cedex, France
loic.andolfatto@{eads.net,lurpa.ens-cachan.fr}
Abstract. Assembly process planning involves many aspects from geometrical
matters to operational research. Though, the literature shows very few works
about assembly technique selection.
This paper deals with an original method to select assembly techniques and to
allocate component geometrical tolerances in order to minimize the product cost
and to maximize the conformity rate associated with the assembly plan.
The data structures used to deﬁne a parametric assembly plan is detailed. This
data structure is used to formulate a multi-objective optimization problem reﬂect-
ing the concerns of the study.
The entire method is illustrated trough a case study. The results obtained are
presented and followed by a discussion about the potential beneﬁts of its appli-
cation in an industrial context. The useful support that this method can provide
to decision-making is highlighted. Its shared point of view from product design-
ers to manufacturing process designers makes it an efﬁcient tool for concurrent
engineering.
Keywords: Assembly process planning, assembly technique selection, geomet-
rical tolerance allocation, multi-objective optimization, concurrent engineering.
1
Introduction
1.1
Design of an Assembly Process Plan
The assembly of large mechanical structures, such as aeronautical ones, may account for
a large share of their total delivery cost. Boothroyd and Dewhurst stressed out the impor-
tance for manufacturing companies to assess a product’s design by designing assembly
process plan as soon as 1 in order to reach the maximum performance [1]. Computer-
aided assembly process planning has been the subject of many research works generally
aiming at ﬁnding the minimum lead time and/or cost. The performance of the assembly
process plans is evaluated according to several indicators such as, for example, tool-
ing needs, reorientations of sub-assemblies, technological similarities in consecutive
operations and so on [2].
1 What is called design for manufacturing and assembly (DFMA).
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 315–324.
DOI: 10.1007/978-3-642-30817-8_31
c⃝Springer-Verlag Berlin Heidelberg 2013

316
L. Andolfatto et al.
Assembly process plan description:
Performance indicators:
Cost
Conformity rate
Delivery rate
...
Assembly plan 
evaluation
Assembly sequence
Assembly techniques
Component geometrical tolerances
Assembly system organisation
Boundary of the 
study
Fig. 1. Schematic view of the description and evaluation of an assembly process plan with the
proposed approach boundary
As summarized in Fig. 1, the description of an assembly process plan can be split into
an assembly sequence, the assembly technique selected to make each attachment of the
product, the geometrical tolerances allocated to each component and the description
of the assembly system2. Most of the works presented in the literature only focus on
speciﬁc aspects of the problem.
Assembly sequence planning, either deﬁned as the part introduction ordering [3–5]
or as the joint realization ordering [6, 7], has been widely studied in the literature [2].
The joint-based approach is close to the activity-based approach proposed by Cao and
Sanderson [8], and reﬂects some of the issues considered for the organisation of the
assembly system commonly treated in operational research [9].
But the generation of assembly sequences and the organisation of the assembly sys-
tem are not the only activities encapsulated in assembly process planning. Assembly
technique3 selection [10] is seldom addressed in the literature even though it may have
the greatest impact on production cost, according to Abdullah et al. [11].
Selecting assembly techniques implies setting the associated geometrical capabili-
ties. The concern of geometrical quality of the assembled product (commonly called
tolerancing) is therefore coupled with the assembly process planning. Up to this point,
the geometrical tolerances allocated to the components to be assembled may be part of
the information contained in a comprehensive assembly process plan [7, 12, 13].
1.2
Proposed Approach
Massive automation has been the key solution to answer the need for decreasing manu-
facturing cost and increasing delivery rate in many manufacturing domains. But in the
ﬁeld of aeronautical structure assembly, the complexity of some attachments, the high
level of geometrical requirements and the high working area required are several hur-
dles to an efﬁcient use of automation with respect to manual assembly. Trades-off have
to be made.
This paper aims at proposing an original method to select assembly techniques and
to allocate component geometrical tolerances in order to minimize the product cost
and to maximize the conformity rate associated with the assembly plan. The assembly
2 Either spatial or temporal.
3 Also called assembly process by some authors.

Assisted Decision-Making for Assembly Technique Selection
317
sequence is assumed to be already deﬁned and the organisation of the assembly system
is not considered.
Section 2 details how an assembly process plan can be described according to a set
of decision variables and how it can be evaluated according to several performance
indicators. This formal view of the problem serves the deﬁnition and the resolution of
a multi objective optimization problem.
A simple use case is presented in Sect. 3 and the results obtained are detailed in
Sect. 4. Section 5 concludes on the potential beneﬁts of this approach in a concurrent
engineering context.
2
Optimization Problem
2.1
Multi-objective Optimization
Considering a given product and a given assembly sequence, the purpose of the pre-
sented work is to propose a method to ﬁnd a set of assembly techniques and a set of
geometrical tolerances that minimize the non-conformity rate and the delivery cost of a
product at the same time.
A mathematical expression of this problem is given in (1), where x is a decision
variable vector representing an assembly plan, X is the set of feasible assembly plans
and f is a function associating a ﬁtness to an assembly plan. The construction of the
decision variable vector x ∈X is later described in Sect. 2.2.
inf{f(x) : x ∈X}
(1)
In the present case, the ﬁtness function f must cope with several objectives (non-
conformity and cost). Moreover, these objectives are likely to conﬂict with each other.
A common solution to this issue is to combine the objectives in a single-valued ﬁt-
ness function. But it can prove complicated to model the actual objectives into a single
performance indicator, which results in making trades-off a priori.
Solving a multi objective optimization problem seems to be more appropriate within
the industrial context. From the assembly process planner point of view, an interesting
point x∗describes an assembly process plan for which one can not ﬁnd a solution that
provides both lower non-conformity rate and lower cost at the same time, as pictured in
Fig. 2.
Assuming f = [NCR(x), C(x)], non-conformity rate function and delivery cost
function described in Sect. 2.3, a solution of the problem given in (1) is a point x∗
satisfying (2), x∗is called a non-dominated point.
̸ ∃x ∈X, NCR(x) < NCR(x∗) and C(x) < C(x∗)
(2)
The underlying mathematical problem of this work consists in ﬁnding the set of non-
dominated points (or a sufﬁcient amount of points if this set is not ﬁnite). The assembly
process planner ﬁnally has to select the plan that provides the best trade-off among
the set of non-dominated points a posteriori. The high level, difﬁcult-to-make, decision
appears with the rough results in hand instead of during the modelling phase required
to build a black-box optimizer. Modelling every single actual objective of the problem
alone is likely to be an easier task.

318
L. Andolfatto et al.
x*
x*
Fig. 2. Non-dominated assembly process plans (circles) among a population (squares) consid-
ering the simultaneous minimization of the non-conformity rate and the minimization of the
delivery cost
2.2
Parametric Assembly Plan
Generalities. Running the optimization problem presented in (1) requires to convert
the assembly process plan description known in technical terms into a mathematical
vector x called assembly plan vector in the paper.
A product is a set of components linked together through joints during the assembly
process. Each component is subject to geometrical variations bounded by geometrical
tolerances. Each joint is made using an assembly technique. The requirement on the
assembly plan vector x can be deduced out of those three assertions. It must include
elements representing the technique selected for each joint of the product and elements
representing the components geometrical deviations.
Assembly Techniques. Considering a product with Nj joints, the Nj ﬁrst elements of
x are dedicated to describe which assembly technique is associated to each of the Nj
joints. Assembly techniques are stored and indexed in a library with several attributes:
index, associated list of assembly operations, list of costing information, geometrical
capabilities, etc.
The technique assigned to the joint j is the one the index of which equals the value
of the jth element of x, as pictured in Fig. 3.
The assembly process planner has to deﬁne the list of techniques suitable to make
each joint of the product. Some additional constraints can be set among those Nj ﬁrst
elements: two joints can be forced to be made with the same technique for example.
This reduces the set X of feasible assembly plans to the technically admissible ones.
Geometrical Variations and Geometrical Tolerances. Assuming that the probability
distribution of each geometrical variation is known, allocating tolerances amounts to
setting these distribution parameters, e.g. the lower bound and upper bound for a uni-
form probability distribution or the mean value and the standard deviation for a normal
distribution. This is illustrated in Fig. 3. If the product’s components have Ngv geo-
metrical variations, and each of them has k distribution parameters, then the assembly
plan vector x also has Nvp = Ngv
i=1 ki elements to describe the geometrical tolerances
allocated.
Intrinsic constraints exist among those Nvp elements, such as a lower bound smaller
than an upper bound for instance. Extrinsic constraints can also be declared by the user
to deﬁne other technical limits, such as the minimum size of a tolerance zone. This also
reduces the set X of feasible assembly plans.

Assisted Decision-Making for Assembly Technique Selection
319
x =
...
tj
...
...
pi1
pi2
...
Grid drilling + manual fastening
Automated riveting cell
Robot drilling + robot fastening
...
tj-1
tj
tj+1
...
#
Assembly techniques
Nj elements
Nvp elements
Parameters assigned to the 
Probability Density Function of 
the ith geometrical deviation
Tolerance to allocate to the ith
geometrical deviation
a
?
a
??
Technique chosen 
for the joint j
...
...
Assembly plan 
vector
pi2
pi1
Fig. 3. Translation of an assembly process plan into a decision variable vector x
x =
...
tj
...
...
pi1
pi2
...
Nj elements
Nvp elements
Assembly plan 
vector
S . l = KC
Tolerance analysis
Monte Carlo approach
Sampling
NCR(x)
Cost vs. Tolerance law
Ctol(x) = 
ctol(T)
Activity-based cost
Non-recurring cost
Cnr(x) = 
cnr
j=1
Nj
Technique j
Cop(x) = 
cop
j=1
Nj
Technique j
+
+ +
C(x)
NCR
C
Graphical
representation 
of the fitness
Fig. 4. Evaluation scheme and ﬁtness representation of an assembly plan deﬁned by its mathe-
matical representation x
2.3
Assembly Plan Evaluation
Non-conformity Rate. The conformity of a product can be assessed by verifying that
some of its characteristics – called key characteristics (KC) in [7] – are kept within a
requirement domain deﬁned during the functional analysis. The Non-Conformity Rate
of a product is deﬁned as the probability for a product to have at least one of its KC
outside of its requirement domain. In this work, only geometrical KC are considered.
Tolerancing studies commonly provide a sensitivity matrix S to link the geometrical
deviations δl4 to the KC deviations δKC as in (3) [12, 14, 13].
S · δl = δKC
(3)
Samples of δl vectors can be associated to an assembly plan vector using both PDFs
of the component geometrical variations and assembly technique capabilities. A Monte
4 Gathering both geometrical deviations of the components and deviations due to the assembly
techniques employed.

320
L. Andolfatto et al.
Carlo approach is ﬁnally used to associate the non-conformity rate NCR(x) to the
assembly plan described by x.
Delivery Cost. The delivery cost can be split into two cost sources: the cost associated
to the assembly operations and the cost resulting from the allocation of geometrical
tolerances.
Each assembly technique is characterized by an interdependent list of assembly op-
erations (an example is given in Table 3). Each operation has a ﬁxed cost cf representing
consumables and unitary tool wear for example. It also uses several resources (of hourly
rate hri) during a certain amount of time top. The elementary cost of an assembly op-
eration is deﬁned in (4):
cop = cf + top ×

i
hri
(4)
The total activity-based cost Cop is the sum of the elementary cost of each operation,
the list of which is derived from the list of selected assembly techniques given by x.
The non-recurring cost per product Cnr, due to the acquisition cost of all the re-
sources required for the assembly divided by the presumable amount of product to be
assembled, is also included to the total delivery cost. As the list of the resources re-
quired to assemble the product depends on the assembly technique selection, Cnr is a
function of x.
The cost associated to a geometrical tolerance allocated is modelled by (5) (adapted
from [15]). T is the size of the tolerance interval allocated, Tlim, a, b, m and k are
function parameters identiﬁed according to experimental data.
ctol(T ) = a + b · e−m(T −Tlim) ·(T −Tlim)−k
(5)
The total cost Ctol associated to the geometrical tolerances allocated to the product’s
components is the sum of all the elementary costs evaluated thanks to (5) according to
the tolerances described by the vector x.
Finally, the sum of Cop, Cnr and Ctol provides the delivery cost C(x) associated to
an assembly plan, as depicted in Fig. 4.
3
Use case
3.1
Product to Be Assembled
The method described in the previous section is applied to the assembly of a simple
mechanical structure composed of four components (see Fig. 5). Three additional tem-
porary components are also used during the assembly, as exposed in Fig. 6 in which the
assembly sequence is also given (as the order in which the joints are made).

Assisted Decision-Making for Assembly Technique Selection
321
KC1
KC2
KC3
KC5
KC6
3 - Splice
4 - Cover
KC4
L1
L2
L3
L2
1 – L_Flank
2 – R_Flank
Fig. 5. Details of the product’s key characteristics (KCi), components and their characteristics
considered as subject to geometrical variation (Lj)
Tool 1
Tool 2
Tool 3
tj1
tj2
tj3
tj4
tj5
tj6
J1
tj7
J2
J5
J6
J3
J4
Assembly sequence: tj1, tj3, tj2, tj4, tj5, J1-J2, tj6, tj7, J3-J4, J5, J6
Fig. 6. Temporary components used during assembly (referred to as Tool k), joints of the product
and assembly sequence
Table 1. Parameters of the Cost vs. Tolerance law deﬁned by (5)
Tlim
a
b
m
k
(mm)
(cost unit)
(cost unit / mm) (mm−1)
L1 and L2
0.01
0
200
1
1
L3 and L4
0.01
0
200
1
1
The geometrical variation propagation problem is reduced to a one-dimensional
study with six key characteristics and four component’s dimensions subject to geo-
metrical variations. The capabilities of the technique selected for the joints tj3 to tj7 are
also impacting the conformity rate as these joints are positioning joints involved in the
KC values. Deviations on KC1 and KC2 must be kept within ±0.6 mm. The value for
KC4 to KC6 is ±0.3 mm.
The parameters of the cost vs. tolerance law deﬁned by (5) are given in Table 1. The
sensitivity matrix S deﬁned in (3) is not detailed in the paper.
3.2
Assembly Technique Library
The assembly technique library is not entirely described in this paper but the Table 2
presents the techniques considered. The Table 3 details the information stored in the

322
L. Andolfatto et al.
Table 2. List of the assembly techniques available associated to the joints they can be used for
Index Name
Feasible joints
1
Positioning with tool
tj3 to tj7
2
Positioning with adjustable tool
tj3 to tj7
3
Positioning with a robot as tool
tj3 to tj7
4
Positioning with a speciﬁc automated station tj3 to tj7
5
Back-to-back positioning
tj1 and tj2
6
Back-to-back positioning with splitting
J3 and J5
7
Traditional bonding
J1 and J2
8
Rapid bonding
J1 and J2
9
Drilling with grid and Manual fastening
J4 and J6
10
Drilling and Fastening with robot
J4 and J6
11
Drilling and Fastening with a speciﬁc station J4 and J6
Table 3. Details of the technique Positioning with a robot as tool
Technique name:
Positioning with a robot as tool
Operations
Fixed cost Duration Resource(s) Quantity(ies) Capabilities
Robot referencing
0
1
Robot
1
Component Grabbing
0
0.5
Robot
1
Component Positioning
0
0.5
Robot
1
U(−0.07, 0.07)
Resource hourly rate: Robot, 0.7 cost unit / mn
Resource acquisition cost: Robot, cnr =50000 cost units
library for the Positioning with a robot as tool technique. In addition to the feasibility
constraints detailed in Table 2, the joints tj3 to tj5 must be done with the same tech-
nique. So do the joints tj6 and tj7.
4
Results
The optimization problem deﬁned in (1) is solved using the Non-Sorting Genetic Algo-
rithm II (implemented in the Inspyred Python library [16]) with a population composed
of 200 individuals evolving during 20 generations. The sensitivity matrix S is obtained
using the tolerance analysis software AnaTole [14] and the non-conformity rate is eval-
uated with a Monte Carlo method implemented in OpenTURNS [17].
The non-dominated points obtained are displayed in Fig. 7. The assembly process
plan corresponding to the square point in Fig. 7 is detailed in Table 4.
The results illustrate the interest of a multi objective optimization. Considering a
single objective for the ﬁtness and the other one as constraint would give an arbitrary
boundary between acceptable and non-acceptable solution, leading to an optimal point
not necessarily better than other ones. Here, a decision-making team can adapt the ﬁnal
choice with more information in hand.
A deeper analysis of the results displayed in Fig. 7 shows that the points can be
classiﬁed into four zones for this use case. In each zone, the assembly techniques se-
lected are identical and only tolerances allocated are varying. It is therefore possible to

Assisted Decision-Making for Assembly Technique Selection
323



	
Fig. 7. Non-dominated points obtained after optimization, with four separate zones of iso-
technique and varying allocated tolerances.
Table 4. Assembly process plan corresponding to the square point in Fig. 7
Joint
Assembly technique selected
Dimension Allocated tolerance
tj1 and tj2 Back-to-back positioning
L1
U(−0.40, 0.30)
tj3 to tj7
Positioning with robot as a tool
L2
U(−0.40, 0.30)
J1 and J2 Traditional bonding
L3
U(−0.22, 0.21)
J3 and J5 Back-to-back positioning with splitting
L4
U(−0.17, 0.16)
J4 and J6 Drilling with grid and Manual Fastening
identify the most relevant set of assembly techniques early during the design of the
product and to reﬁne the tolerance allocation along its design process.
5
Conclusion
Selecting assembly techniques and allocating geometrical tolerances for a product re-
quires making quality vs. cost trades-off. The method proposed in this paper helps the
assembly process planner to ﬁnd a set of good5 solutions among which he can select
the one that suits the best his interests.
The method is based on an assembly technique library in which the company know-
how can be stored and upon a geometrical variation propagation relation associated to
the assembly sequence. A wide range of potential assembly process plans can therefore
be investigated and evaluated from an objective point of view. Results can be obtained
with various assembly sequence scenarios and even with various product architecture
scenarios. Decisions about the product’s design and its assembly process plan can be
taken from a point of view shared by product designers and manufacturers, enhancing
collaborative and concurrent engineering.
5 The non-dominated points, from the mathematical point of view.

324
L. Andolfatto et al.
Acknowledgment. This research work has been carried out in partnership between
EADS Innovation Works and the LURPA-ENS de Cachan in the frame of the GRC-
Flexible Assembly of the INNO’CAMPUS program.
References
1. Boothroyd, G., Dewhurst, P.: Product design for manufacture and assembly. Manufacturing
Engineering 100, 42–46 (1988)
2. Wang, L., Keshavarzmanesh, S., Feng, H.Y., Buchal, R.: Assembly process planning and
its future in collaborative manufacturing: a review. The International Journal of Advanced
Manufacturing Technology 41, 132–144 (2009)
3. Bourjault, A.: Contribution `a une Approche M´ethodologique de L’Assemblage Automatis´e:
´Elaboration Automatique des Sequences Op´eratoires. Ph.D. thesis, Universite de Franche-
Comte (1984)
4. De Fazio, T., Whitney, D.: Simpliﬁed generation of all mechanical assembly sequences.
IEEE Journal of Robotics and Automation 3(6), 640–658 (1987)
5. Dini, G., Failli, F., Lazzerini, B., Marcelloni, F.: Generation of optimized assembly sequences
using genetic algorithms. CIRP Annals - Manufacturing Technology 48(1), 17–20 (1999)
6. Homem de Mello, L., Sanderson, A.: A correct and complete algorithm for the generation
of mechanical assembly sequences. IEEE Transactions on Robotics and Automation 7(2),
228–240 (1991)
7. Mantripragada, R., Whitney, D.: The datum ﬂow chain: A systematic approach to assembly
design and modeling. Research in Engineering Design 10(3), 150–165 (1998)
8. Cao, T., Sanderson, A.: Task decomposition and analysis of robotic assembly task plans
using petri nets. IEEE Transactions on Industrial Electronics 41(6), 620–630 (1994)
9. Becker, C., Scholl, A.: A survey on problems and methods in generalized assembly line
balancing. European Journal of Operational Research 168(3), 694–715 (2006)
10. CIRP: Dictionnary of Production Engineering; vol. IV - Assembly (2012)
11. Abdullah, T.A., Popplewell, K., Page, C.J.: A review of the support tools for the process
of assembly method selection and assembly planning. International Journal of Production
Research 41(11), 2391–2410 (2003)
12. Camelio, J., Hu, S.J., Ceglarek, D.: Modeling variation propagation of multi-station assembly
systems with compliant parts. Journal of Mechanical Design 125(4), 673–681 (2003)
13. Mounaud, M., Thi´ebaut, F., Bourdet, P., Falgarone, H., Chevassus, N.: Assembly sequence
inﬂuence on geometric deviations propagation of compliant parts. International Journal of
Production Research 49(4), 1021–1043 (2011)
14. Marguet, B., Chevassus, N., Falgarone, H., Bourdet, P.: Geometrical behavior laws for com-
puter aided tolerancing: Anatole a tool for structural assembly tolerance analysis. In: 8th
CIRP Seminar on Computer-Aided Tolerancing, Charlotte, NC USA (2003)
15. Chase, K., Greenwood, W., Loosli, B., Hauglund, L.: Least cost tolerance allocation for
mechanical assemblies with automated process selection. Manufacturing Review 3(1), 49–
59 (1990)
16. Inspyred 1.0: Bio-inspired Algorithms in Python (May 2012),
http://inspyred.github.com/ (consulted on May 4, 2012)
17. Open TURNS - Reference Guide - version 1.0 (April 2012),
http://doc.openturns.org/openturns-1.0/openturns-doc
april2012/pdf/OpenTURNS ReferenceGuide.pdf

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 325–334. 
DOI: 10.1007/978-3-642-30817-8_32 
© Springer-Verlag Berlin Heidelberg 2013 
 
Automated DHM Modeling for Integrated  
Alpha-Numeric and Geometric Assembly Planning  
Martin Manns and Néstor Andrés Arteaga Martín 
Daimler AG, Wilhelm-Runge-Str. 11, 89073 Ulm, Germany 
{martin.manns,nestor_andres.arteaga_martin}@daimler.com 
Abstract. Digital Human Model (DHM) simulations are established as alterna-
tive to prototype based assembly verification. Prohibitive modeling effort con-
strains its application to selected tasks. In this work, a novel methodology is  
introduced to automatically generate DHM assembly simulations from textually 
planned assembly processes. The methodology is based on the software ema, 
which employs a human movement database mapped to building blocks. The 
methodology is evaluated using a model for a car interior pre-assembly station 
at Daimler AG. It is compared to state of the art process verification with proto-
types, classical DHM modeling and manual application of ema. Most of the 
processes can be identified and half of them are realistically modeled. The pre-
sented methodology is a promising approach towards automating DHM model-
ing for process verification. Such a feature could help integrating today’s  
alpha-numerical process planning to 3D geometric planning because no CAD 
expertise is required. 
Keywords: Assembly planning, digital human model simulation, controlled 
natural language. 
1 
Trends in Automotive Assembly Planning 
Automotive assembly systems need to adjust to a continuously changing market. Try-
ing to keep up with market demands, automotive OEMs are offering an increasing 
number of customized vehicles and model variants. The main challenges faced by 
modern automotive assembly systems are faster and more secure ramp-up processes, 
which can manage shorter production planning cycles and increasing product variety, 
as well as the need to include new variants into existing production lines [1]. 
The availability of accurate digital models and simulation tools has significantly 
reduced development time and costs by being able to identify and correct problems 
before any physical prototypes are built. For assembly planning digital buildability 
check is a well established process, in which digital models of the vehicle’s parts are 
checked for geometric consistency, thus guaranteeing that all parts can be assembled 
before any prototype parts are manufactured [2]. Geometrical process simulation is 
mostly used by experts, who address critical issues, because of high modeling effort. 
Automation has been addressed by e.g. [3] [4]. 

326 
M. Manns and N.A.A. Martín 
The use of computer aided planning systems and the increasing standardization of 
processes has opened the possibility of documenting production planning from the 
early stages of product development. In this way, it is possible to have a preliminary 
assembly plan that is based on a standard process plan at the beginning of assembly 
planning. In this assembly plan information and performance indicators are initially 
documented with default values. Once sufficient product and process information is 
available, these preliminary values are replaced by actual ones. As product develop-
ment progresses and assembly planning is refined, these indicators start to give a 
more realistic view of the planned production processes. 
2 
Concurrent Assembly Planning in Automotive Industry 
In order to address high product variety and shorter product development times, tech-
niques like concurrent engineering are employed to achieve early integration of prod-
uct and process planning [5]. Concurrent engineering, which is common practice in 
today’s automotive development, has led to shorter development times, allowing early 
problem identification and reducing correction costs [6]. However, in early planning 
there are uncertainties about product details that complicate communication between 
development process parties (Figure 1). Considerable research has been conducted in 
modeling these uncertainties such as Derichs’ fuzzy database approach [7]. 
 
Fig. 1. Information reliability in Simultaneous Engineering (translated from [7]) 
In practical assembly planning environments, simpler approaches can be found. 
One is to differentiate levels of detail in each process step and plan as far as possible, 
i.e. fill out the data fields that are known at the moment, and document possible un-
certainties as free text (c. [8]). This common approach has drawbacks. Management 
normally tries to shorten development times urging planners to fill in uncertain solu-
tions. Uncertainties are documented in an individual text form that cannot automati-
cally be analyzed. Since aggregation is time consuming, uncertainty information stays 
hidden. Thus, planning documentation tends to give a too optimistic view of the  
Sequential product and process design
SE: Integrated product and process design
Process design
Product design
Process design
Product design
Time
TIME TO MARKET
Communication volume
Reliable
information
Product
idea
Market
launch
Time
TIME TO MARKET
Communication volume
Reliable
information
Reduced
communication effort
Uncertain information
Reduced
development time
-Δt
Market
launch
Product
idea
Sequential product and process design
SE: Integrated product and process design
Process design
Product design
Process design
Product design
Time
TIME TO MARKET
Communication volume
Reliable
information
Product
idea
Market
launch
Time
TIME TO MARKET
Communication volume
Reliable
information
Reduced
communication effort
Uncertain information
Reduced
development time
-Δt
Market
launch
Product
idea

 
Automated DHM Modeling for Integrated Alpha-Numeric 
327 
planning progress. In these individual planning and notification texts, uncertainty is 
mostly described using vague terms. Since there is no vagueness measure, getting an 
overview of planning progress is time consuming. Additionally, there is no link be-
tween a vague term in a textual description and geometric parts that are linked to this 
description. Thus, tracking if process descriptions are up to date after product design 
changes is a challenging task. 
3 
Manual Assembly Process Verification 
Prototype building serves as an opportunity to evaluate, verify and optimize produc-
tion planning [2]. The assembly planning team conducts several prototype builds 
before the start of production. During these prototype builds, the planner reads the 
planned processes out loud, and a worker interprets and follows the instructions, 
while experts from different fields perform different evaluations and verifications. 
The goal of these builds is to achieve an optimal production process preparation 
through which efficient, ergonomically optimal, stable and robust processes are de-
fined, product quality is guaranteed, and production ramp-up can be completed faster. 
As product variety increases, it becomes more difficult and costly to build physical 
prototypes of all variants. Reducing prototypes reduces the number of manual assem-
bly process verifications that can be done using prototypes. An established method to 
address non prototype based manual assembly process verification is the use of digital 
human model (DHM) tools that are available inside PLM systems, such as Dassault’s 
Delmia. These tools are widely used in automotive industry to simulate manual as-
sembly processes [9]. Unfortunately, these are complex tools, that to be correctly 
applied require CAD expertise, as well as knowledge of process and product [10]. 
In practice, mostly static analyses of critical assembly situations are addressed in 
this way. This is due to time consuming modeling, inflexibility to changes on process 
and product, and to often occurring unnatural movements of the DHM if dynamic 
processes are modeled. Typically, key frames, i.e. static postures, are defined and the 
DHM interpolates between every pair of key frames to generate the so called in bet-
weens depicting the movement of the DHM. Modeling a simple task, e.g. walking to a 
rack and grabbing a part form a bin, requires to define various key frames, i.e. defin-
ing the position of every single joint of the DHM, and any possible interaction with 
other objects in the scene. If simple changes on the scene occur, e.g. a new position of 
the rack and a new version of the part, new modeling of the whole process is usually 
required. Even after carefully modeling the key frames, unnatural behavior of the 
DHM’s movements could arise, e.g. the arms will go through the rack. So as to avoid 
this issue, usually more key frames need to be defined, thus increasing modeling time. 
Alternatives, like imk automotive’s ema, address the time consuming and inflexi-
ble approach of classical DHM tools by allowing a more abstract modeling of the 
DHM movements and its interactions. This approach is based on the decomposition of 
tasks into basic operations and the use of parameterized movement generation. The 
underlying movements appear more natural due to the fact that these were synthesized 
by recording and analyzing the movements of real workers executing diverse manual 

328 
M. Manns and N.A.A. Martín 
tasks [3]. A DHM simulation with ema is built by dragging and dropping basic opera-
tion building blocks onto a time line depicting the process. ema is able to generate 
DHM key frames that depict the intended action and task. This generation depends on 
building blocks, parts, location and specific movement information such as the worker 
model being restricted in moving its upper torso. Thus, it is possible to redefine the 
position of the rack and the version of the assembly part, and the new required post-
ures of the DHM to reach the part are automatically generated based on the current 
part information with much less effort than with the traditional approach. 
In the following section, a novel DHM assembly simulation methodology is intro-
duced. This methodology directly links the textually planned assembly process with 
the ema activity building blocks approach just presented above. It is shown, how 
DHM assembly process simulations could be generated, without a user needing to 
read, interpret and model the process, thus being faster than in the traditional DHM 
and the ema approaches. 
4 
Methodology for Automatically Generating Digital Human 
Model Simulations from Alpha-Numerical Process Plans 
4.1 
Controlled Natural Language for Assembly Planning 
Since the proposed concepts of systematically storing information about abstraction 
have not found adoption, a method that is based on a controlled natural language 
(CNL) has been implemented, the method employs the language that practitioners use 
today. The CNL is focused on staying as close as possible to today’s planning texts. 
An analysis of 1824 textual process descriptions yielded a basic understanding of how 
such texts are structured. From these texts, a set of semantic roles has been derived: 
• ACTIVITY 
Main process activity 
• THEME 
Part that is manipulated 
• GOAL 
Part that the THEME is joint to 
• SOURCE 
Part that the THEME is separated from 
• FASTENER 
Part that secures a fastening 
• TOOL 
Part that is used to conduct the ACTIVITY 
• INSPECTION Objective of an inspection ACTIVITY 
The grammar of the CNL was kept as simple as possible. It consists of one fixed role 
sequence per activity. The text style closely resembles the style of the process de-
scriptions, which are clearly made to be understandable by experts. The comparative-
ly simple grammar allows simple parsing, so that a parser for the CNL was built and 
tried out on the process descriptions. At this stage, only 321 process descriptions 
could be parsed correctly so that no automatic transition to the CNL was considered 
realistic. In order to reduce migration of legacy process descriptions, an input mask 
for easy validation of planning input has been developed. In close collaboration with 
planners, role contents have been optimized. This has led to a CNL that is able to 
cover between 70% and 90% of the process descriptions depending on the planning 

 
Automated DHM Modeling for Integrated Alpha-Numeric 
329 
domain. This CNL represents the basis of the proposed methodology. In all its roles, 
several levels of abstraction are available and can be automatically analyzed. In the 
case of activities, each of the 104 activity terms is mapped onto the respective produc-
tion task category from DIN 8580ff. [10]. These categories have different levels of 
abstraction, for example “joining” is considered more abstract than “assembling” (in 
German Zusammensetzen). This mapping allows tracking of each process descrip-
tion’s abstraction level. 
4.2 
Mapping Approach 
The methodology of automatically deriving DHM models from textual process  
descriptions starts with mapping the semantic activity roles of the CNL to basic op-
eration building blocks. Usually, the semantic roles are more abstract than basic oper-
ation building block parameters. Therefore, concretization is necessary. Since there 
are normally several concrete ways of realizing an abstract process description, the 
objective is to derive a concretization that is plausible for both planners and workers. 
Parts have to be mapped from their abstract representations, (e. g. “Outside mirror, 
left”) to their names in the CAD tree (e.g. “MIRROR OTR LHD LH ASPH TWA 
STV 360-DEGREE-VIEW”). First, a set of possible CAD objects is retrieved from 
the bill of material. This list is filtered using the car variant that is employed for simu-
lation and the process-product mapping from the product process resource tree of the 
digital planning software. All remaining parts are displayed in a 3D-model so that the 
correct node of the CAD tree can be selected. Geometric locations are given in ab-
stract ways in the semantic roles. They can incorporate prepositions such as “above” 
another part, or comprise hints such as cable colors. These descriptions are meant to 
be understood by a worker who carries out the planned assembly process. Therefore, 
parts are virtually moved in the scene until the state of the respective process step is 
reached. Candidates for locations that are relative to parts are derived by projecting 
the bounding box of the basis part into the given direction and selecting a point on the 
near side of all parts that collide with the projection. If a color attribute is given,  
the visibility from the worker position is taken to generate points in the middle of the 
visible colored area. For movement information, reachability issues are predominant, 
i.e. it has to be determined whether or not the worker is able to easily grasp an object. 
Therefore, the DHM simulation is put into the loop. Given that all prior process steps 
are successfully planned, a series of parameter sets that describes process and ergo-
nomic attributes is provided. Simulation is done for each parameter set until success-
ful. If no simulation is successful, no DHM simulation model is generated. 
5 
Evaluation 
5.1 
Scenario 
In order to evaluate effectiveness and quality of the presented methodology, a pre-
assembly station in automotive end assembly has been modeled for process verifica-
tion. In the scenario, a center console is taken from a rack and positioned in a fixture. 

330 
M. Manns and N.A.A. Martín 
An antenna is contacted and assembled as well as a cup holder, two cover panels, a 
control module and a telephone keypad. The process is planned in 11 process tasks. 
The station consists of a roller conveyor, two racks and a work place with a special 
fixture. Only hand tools are employed. It does not require any special movements 
such as kneeling, which should facilitate automatic process creation. Four different 
models of the station process have been modeled independently for comparison: 
• A physical prototype in which a worker assembles prototype parts and process 
validation experts watch. This represents today’s established methodology. 
• A digital model that has been created with existing functionality of the software 
package Delmia V5. This model follows the process that is used for ergonomics 
analysis of critical situations today. 
• A digital model that has been created with the software ema using the proposed 
methodology of the vendor. This methodology promises reducing modeling time 
and improving process time predictions. 
• A digital model that has been derived from the methodology presented in section 4. 
5.2 
Evaluation Methodology 
For each model, process or simulation results have been reviewed by two experts. 
Criteria address core issues with integrating DHM into assembly validation: The 
processes that are validated have to be modeled in a realistic way and the process time 
that results from simulation should be similar to the time that an average worker in a 
series production would require. Therefore, two experts assessed each process step 
according to the following criteria: 
• Modeled tasks. A task is marked as modeled if both experts have noticed the 
process step being conducted from watching the worker or the DHM, i.e. it can be 
marked as not modeled if there is not such a task in a simulated process. 
• Realistic process. A task is marked as showing a realistic process if both experts 
consider that the process step is conducted similarly to the future process on the as-
sembly line. 
• Realistic human motions. A task is marked as showing realistic human motions if 
both experts consider that the movements of the worker or of the DHM are similar 
to the future worker movements on the assembly line. 
• Process time. The process time is measured from the actual process time of the 
worker or in case of a simulation from the simulated time.  
Results of quality measures have been aggregated on the task level. If a criterion is 
only met for part of a task then the result shows “partial” instead of “yes” or “no”. In 
addition to quality criteria, the four models have been compared for modeling effort. 
For simulation models, modeling time has been derived from experienced users to 
reduce impact of the early learning curve. For the physical prototype, modeling time 
has not been assessed because much of this time was spent producing and acquiring 
prototype parts. 

 
Automated DHM Modeling for Integrated Alpha-Numeric 
331 
5.3 
Results 
The percentage of modeled tasks (see Figure 2) shows that the prototype based ap-
proach has deficiencies in depicting all processes. This limitation is mostly caused by 
missing parts or parts that cannot be assembled because of geometric deviations. Fur-
thermore, the currently planned sequence could not be replicated because some parts 
had been outdated at the time of verification. 
 
Fig. 2. Ratio of visibly modeled tasks for the four verification models 
In the manually modeled Delmia V5 and the fully automated ema simulation, all 
tasks were visible. The 100% coverage of Delmia V5 resulted from manually adjust-
ing each required joint angle, which means prohibitive effort for large and complex 
scenarios. Good coverage of the ema automated approach stems from a methodologi-
cal inaccuracy, i.e. a movement artifact that is visible whenever a new task is con-
ducted. This artifact makes the worker movement less realistic but strongly hints at 
some action being part of a new task. In the manually modeled ema simulation, only 
seven tasks could be clearly identified. The simulation does not show the artifact of 
the automated ema simulation. 
 
Fig. 3. Process realism for the four verification models 
Regarding process realism (see Figure 3), 91% of the processes are evaluated as 
realistic in the manually created Delmia V5 model, 54% in the automatically generat-
ed ema simulation, 36% in the physical prototype approach, while only 18% of the 
processes follow the plan in the manually created ema simulation. Most process  
0%
20%
40%
60%
80%
100%
Prototype
Delmia V5
ema manual
ema automated
Modeled tasks
no
partially
yes
0%
20%
40%
60%
80%
100%
Prototype
Delmia V5
ema manual
ema automated
Realistic processes
no
partially
yes

332 
M. Manns and N.A.A. Martín 
deviations of the worker with the physical prototype result from parts that are missing 
or difficult to assemble due to tolerance issues, as well as from assembly problems. 
Both ema models deviate considerably because fine motor skills are not mapped onto 
worker movements. Therefore, the respective processes may be identified as being 
present but are not evaluated as realistically following the planned tasks. The ratio of 
these tasks can be seen in the proposed automatic model generation with ema. If a 
process is modeled at all then it matches the plan. 
Realism of human motions (see Figure 4) is, as expected, best for a real human 
conducting a task. In the prototype based approach, one process step could only be 
hinted at, which was evaluated as unrealistic. The Delmia V5 simulation shows al-
most similar movement realism, while half of the processes of the manually derived 
ema model include unrealistic human movements. In the proposed methodology, only 
18% of the processes have been evaluated as depicting realistic human motions. Most 
processes deviate considerably from possible human movements due to collisions and 
overlapping of the DHM with other objects in the scene. 
 
Fig. 4. Human motion realism for the four verification models 
Table 1 summarizes differences between MTM expert estimates for process times 
and the predicted times of the considered approaches. Only the eight tasks modeled 
by all approaches are considered. Time measurement of the worker shows considera-
bly longer times than the MTM (methods-time measurement) expert estimates. This 
result is not necessarily wrong because the MTM values refer to a mature process 
executed on the assembly line after ramp-up rather than prototype building. Consider-
ing the median of the process times, the manually generated ema model predicts 90% 
of the planned time. This may hint at optimization potential because even in processes 
that are completely modeled, process times are lower than planned. The considerable 
standard deviation of 0.87s results partly from process steps that were not modeled. 
Note that two processes are not taken into account for the manual ema model because 
they were not recognizable and therefore not included in the time assessment. The 
other models show processing times above the prototype processing times. The Del-
mia V5 model used Delmia’s standard procedure without MTM based optimization. 
Therefore, considering time predictions, such analyses have to be conducted addition-
ally. The proposed methodology yields the longest time predictions. Correlation of the 
planned times to the DHM simulated times rank as expected. However, the physical 
0%
20%
40%
60%
80%
100%
Prototype
Delmia V5
ema manual
ema automated
Realistic human motions
no
partially
yes

 
Automated DHM Modeling for Integrated Alpha-Numeric 
333 
prototype is completely uncorrelated, which emphasizes the little resemblance of 
assembly time in prototype building to those in mass production. Therefore, none of 
the investigated simulation approaches should be considered a replacement of MTM 
analysis. 
Table 1. Median and standard deviation for eight ratios of predicted time to MTM expert esti-
mate, and correlation of predicted process times to MTM expert estimates 
 
Prototype 
Delmia V5 
ema 
manual 
ema 
automated 
Median [s] 
1.25 
1.64 
0.83 
1.72 
Std. dev. [s] 
1.09 
1.37 
0.91 
0.87 
Correlation 
-0.02 
0.65 
0.66 
0.55 
 
DHM simulation modeling effort is differentiated into the categories data acquisi-
tion and process modeling. Data acquisition mostly consists of gathering CAD models 
of the respective car variants in their considered assembly state, CAD models of tools, 
handling equipment and building and process descriptions and their mapping to CAD 
objects. Data acquisition has been identical for all DHM simulations. For the simu-
lated scenario, data acquisition amounted to 7.15 hours. For the proposed approach 
process modeling consists of 10.1 minutes of computer processing time on a standard 
office notebook. Manual modeling with ema took about 10.5 hours. This was mainly 
due to required tuning of the worker trajectories. The Delmia V5 expert needed 21 
hours, because of manual optimization of human movement by explicit entry of 
worker joint angles. Modeling effort for the physical setup mostly consists of acquir-
ing prototypes and prototype parts, which had been conducted by a range of depart-
ments making effort estimation difficult. However, overall cost and effort clearly 
exceeded those of digital modeling in the considered scenario. 
6 
Discussion and Outlook 
The proposed methodology is a promising approach towards automating DHM mod-
eling for process verification in early stages of assembly planning. Even though hu-
man movements are currently rated very unrealistic, a complete process has been 
automatically modeled. If needed, this process could be an initial point for further 
manual detailing of the DHM movements. 
Shortcomings in movement realism could be overcome by replacing the bounding 
box based collision model with a more geometrically plausible one. Furthermore, 
temporary state changes in the 3D model that result from DHM actions should be 
modeled in an ontology. This ontology may be extended by implicit information that 
is given from the point of view of the DHM, e.g. a red cable that is mentioned is most 
probably the one that is at the time visible for the DHM. Such an approach also would 
allow plausibility checks, which would refine the forced process modeling that is 
currently done regardless of restrictions and uncertainties. In order to gain consistent 
motions in the presented approach artificial activities, e.g. turning in direction, have 

334 
M. Manns and N.A.A. Martín 
been introduced. These artificial activities should be included into ema’s basic opera-
tion building blocks, which would increase realism at the cost of reduced visibility of 
the model tasks. 
With sufficient modeling effort realistic human movements in DHM simulations 
can be achieved. However, modeling effort is prohibitive for systematically simulat-
ing the complete process with DHM simulations. The proposed methodology offers 
DHM modeling from planning texts without the need of CAD expertise. Such a fea-
ture could be a first step towards bridging the gap between today’s alpha-numerical 
process planning and 3D geometric planning. 
References 
1. Michalos, G., Makris, S., Papakostas, N., Mourtzis, D., Chryssolouris, G.: Automotive as-
sembly technologies review: challenges and outlook for a flexible and adaptive approach. 
CIRP Journal of Manufacturing Science and Technology 2(2), 81–91 (2010) 
2. Weber, J.: Automotive Development Processes - Processes for Successful Customer 
Oriented Vehicle Development. Springer, Heidelberg (2009) 
3. Fritzsche, L., Jendrusch, R., Leidholdt, W., Bauer, S., Jäckel, T., Pirger, A.: Introducing 
ema (Editor for Manual Work Activities) – A New Tool for Enhancing Accuracy and Ef-
ficiency of Human Simulations in Digital Production Planning. In: Duffy, V.G. (ed.) 
ICDHM 2011. LNCS, vol. 6777, pp. 272–281. Springer, Heidelberg (2011) 
4. Min, J., Chai, J.: Motion graphs++: a compact generative model for semantic motion anal-
ysis and synthesis. In: ACM Transactions on Graphics (TOG) - Proceedings of ACM 
SIGGRAPH Asia 2012 TOG (2011) 
5. Chryssolouris, G.: Manufacturing Systems: Theory and Practice, 2nd edn. Springer 
Science-i-Business Media, Inc., New York (2006) 
6. Eversheim, W., Bochtler, W., Laufenberg, L.: Simultaneous engineering. Springer, Berlin 
(1995) 
7. Derichs, T.: Informations management in Simultaneous Engineering. Shaker Verlag, Aa-
chen (1997) 
8. Klauke, S.: Methoden und Datenmodell der ”Offenen Virtuellen Fabrik” zur Optimierung 
simultaner Produktionsprozesse. VDI-Verlag, Düsseldorf (2002) 
9. Lämkull, D., Hanson, L., Örtengren, R.: A comparative study of digital human modelling 
simulation results and their outcomes in reality: A case study within manual assembly of 
automobiles. International Journal of Industrial Ergonomics 39(2), 428–441 (2009) 
10. Lockett, J.F., Assmann, E., Green, R., Reed, M.P., Raschke, U., Verriest, J.-P.: Digital 
Human Modelling Research and Development User Needs Panel. In: 2005 SAE Digital 
Human Modeling for Design and Engineering Symposium, Iowa-city (2005) 
11. DIN 8580, Fertigungsverfahren. DIN Deutsches Normungsinstitut e.V. Beuth Verlag, Ber-
lin, Germany (2003) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 335–344. 
DOI: 10.1007/978-3-642-30817-8_33 
© Springer-Verlag Berlin Heidelberg 2013 
 
3DExperiences – Dassault Systèmes Strategy  
to Support New Processes in Product Development  
and Early Customer Involvement 
A Software Tool Editor’s View to Challenge the Smart Product 
Engineering Revolution 
Marc Frouin 
Senior Director CATIA Systems R&D,  
Dassault Systèmes  
10, Rue Marcel Dassault 
78140 Vélizy-Villacoublay 
France 
marc.frouin@3ds.com 
Abstract. In many industries the market sees a lot of change happening these 
days; new trends and challenges obviously need to be addressed as part of the 
product creation process. 
Dassault Systèmes (3DS) is introducing a 3DEXPERIENCE PLATFORM to 
expand the usability of the digital smart product development in an accurate vir-
tual universe beyond Product Lifecycle Management (PLM). 
The target is to provide the ability to place the customer at the heart of a sys-
tem, integrating both company business processes and product development 
processes. It is a combination of science, art and technology to bring value to 
our customers by helping them to respond to the needs of their customers and 
creating ‘magnetic’ products with strong market appeals. 
3DExperiences can be a catalyst for innovation, enabling any enterprise 
stakeholder to participate in the innovation process, contributing to drive value 
for the end consumer and create smart products from design to recycling. 
This presentation highlights some aspects of the new Dassault Systèmes 
strategy; shows some of the solution experiences and how customers respond to 
them. 3DS is a scientific company positioned among the top 10 software com-
panies worldwide and for more than 30 years has been helping companies to 
transform the way they design and produce their products.  
The conclusion is based on an explanation to leverage the virtual product 
with accuracy through the production of a Probabilistic Certificate of Correct-
ness for complex cyber physical smart product definition. 
Keywords: Smart Product, Simulation, Mechatronics, Multi Physics, 
3DExperience, SIMULIA, Abaqus, DMU, Functional Mockup Interface, FMI. 
1 
Why Is Change Necessary? 
It is a complex challenge to move from traditional engineering to Smart Product  
development. Forward-thinking executives are, quite rightly, demanding more and 

336 
M. Frouin 
better consumer insight and smarter research to measure impacts of potential new 
innovations. Companies can gain an understanding of their customers which is better, 
faster, and less expensive by doing four things called the “digital research revolution”: 
1. Leverage the digital society power to rapidly create and deliver detailed informa-
tion about what would be products customer acceptance information. 
2. Recognize the limitations of the measurement systems based on projected past or 
isolated focus groups.  
3. Seek to understand what customers value the most and how technologies and inno-
vation can deliver and enhance the product value. 
4. Link investigations into consumer insights based on virtual product candidates that 
are sorting value very close to their customer relationships management systems 
and create criteria such as “Cool Feature”, “Must Have”, etc. 
Leverage the Power of the Digital Society 
The Internet is fast. It enables any type of digital exchange and can leverage data in 
many forms plus has the ability to create many more iterations with a turnaround in a 
few days, rather than weeks required for traditional methods.  
Marketing strategy segmentations for innovation that used to take months can be 
completed in weeks. Working digitally is also less expensive than prototyping/ face-
to-face studies and can be conducted with dramatically larger sample sizes.  
Finally, customers’ online comments, search behavior, and other revelations offer a 
trove of data.  
Decrease Reliance on Traditional Prototypes and Focus Groups 
Focus groups were once the default qualitative research approach. Companies ga-
thered small numbers of consumers in a facility around real prototypes for some 
hours, promised them a small payment or free products, and asked them to give sim-
ple written or oral answers to questions that may or may not have provided real  
insight. Despite the emergence of better alternatives, little has changed yet.  
Full usage of Virtual Product candidates now enables leading marketers to create 
deep immersion experiences with a large number of users organized through com-
munities to generate consumer insights and new products; consider values and posi-
tioning; and redefine a product’s competitive set. These ideas can then be tested  
quantitatively, either through further research or a market pilot. 
Understand How Successful Products are Created 
Companies have long focused on what and why people buy. Research data from con-
sulting companies like Oliver Wyman show that this is not sufficient. They clearly 
show that only few high-quality products are magnetic1, defining “magnetic” by a 
simple equation: M = F×E. Magnetic equals best functionality times most powerful 
emotional connection with customers. Very good functionality is obviously not 
enough anymore. 
Connect What You Know about Consumer Value 
Why do consumers disproportionately demand one product over a seemingly similar 
one, often by a factor of four or five to one? Functionally and technically, products 
might be close; emotionally, they can be worlds apart. Why do seemingly similar 
products produce radically different demand curves? 
                                                           
1  Magnetic = having a great power of attraction over people. 

 
3DExperiences –
Demand creators, people
sessed with understanding 
value chains to fix it. They
map to recognize the huge 
including emotionally — an
Demand creators crack the 
job answering a small set of
2 
The Concept of 
How can a Software tool ve
3DS has initialized a large 
product creators with the n
create Smart Products from
together to create a “Virtual
To reach customers but 
of a product that can actuall
Product needs to be digitall
creators if it is not also pos
ing of the environment in w
And of course measurin
magnetic products describe
ities and finally partners in
interacting socially in fast a
3DS has started to create
to complete the digital land
• Facebook as a social plat
• Google as a search platfo
• Wikipedia as a knowledg
• App stores as a services 
• Android as a developmen
• YouTube, iTunes and oth
Fig. 1. 3DExperien
– Dassault Systèmes Strategy to Support New Processes 
e who design products that truly excite consumers, are 
customer values, and connecting the dots from multi
y don’t assume that buying = wanting. They use the va
gaps between what people buy and what they really va
nd use those value map as a springboard to see differen
mystery of the demand equation by doing a radically be
f critical value and differentiation questions very well. 
3DExperiences by Dassault Systèmes 
endor support demand creators?  
change to apply the power of the Digital Society to h
new challenges described above: Introduce the ability
m an unified digital representation that are all interact
l” Smart Product. 
also to ensure that fast development cycles based on va
ly be produced can be achieved digitally, a “Virtual” Sm
ly accurate. This product accuracy is not sufficient to h
sible to benefit from similar accuracy in the digital mod
which the products will be used.  
ng value and making product creators able to deliver 
d above means that ability to interface with large comm
ncluding suppliers, channels and consumers are request
and efficient digital loops. 
e a Product Creator Digital Platform that has the ambit
scape compared to other platform technologies  
tform 
orm 
ge platform 
and applications platform 
nt platform 
hers as a content platform 
 
nce Platform compared to other platform technologies 
337 
ob-
iple 
alue 
alue 
ntly. 
etter 
help 
y to 
ting 
alue 
mart 
help 
del-
the 
mun-
ting 
tion 

338 
M. Frouin 
This Product Creator Digi
signed to socially link the l
required to create Smart Pro
The integration is done 
are leveraging the 3DS 20
around 3D based digital eng
The target is to provide 
other engineering domains.
the magnetic product creat
nufacturability of the produ
Lifecycle Management mar
The representation of th
software and data platform 
grated application environm
Fig
3 
User Experience
The 3DS new 3DExperienc
duce Smart Products includ
Automotive, Aerospace an
volume Consumer Package
and Business Services, Life
Fig. 3. 
 
 
ital Platform is named 3DExperience platform and is 
large communities of contributors in product developme
oducts.  
around real accurate Digital Virtual Smart Products t
0 + years of experience in creating accurate engineer
gineering.  
the same accuracy that was obtained on geometry to 
. The target is not to provide game platforms but to ena
tion to be done socially based on the real physics and m
ucts - this can be seen as a natural extension to the Prod
rket (PLM). 
his strategy is indicated by a user interface to access 
m in the form of a compass as a synonym for a highly in
ment and a platform concept. 
g. 2. 3DExperience Platform Compass 
e by Industries 
ce platform is to be delivered for 12 industries – that p
ding the more traditional customers of the company such
nd Industrial Equipment to new customers such as h
ed Goods, Retail, Energy, Process and Utilities, Finan
e Science and more. 
Dassault Systèmes supports 12 Industries 
de-
ents 
that 
ring 
the 
able 
ma-
duct 
the 
nte-
 
pro-
h as 
high 
ncial 
 

 
3DExperiences –
3.1 
PLM Innovation b
Engineered Busines
Experience 
Overall the changes, wheth
er it is the market, the con
sumer buying behavior, new
products, changed mobilit
etc. have a significant im
pact on the way engineerin
will work in the future.  
4 
Examples of Ind
4.1 
Transportation 
& Mobility 
(T&M2) Solution 
Experience3 
As part of the transfor-
mation process toward 
integrated Smart Product 
delivery 
the 
new 
3DExperience platform 
is addressing 5 major 
areas in the T&M do-
main: 
My Car Experience 
The purpose of this experie
next generation of the mobi
their needs and identify the
around new concepts and so
the industrial innovation pr
showroom, with augmented
A virtual in-car feeling b
rience will lead to new desi
the time to market and redu
                                            
2  T&M includes automotive
segments. 
3  Information about other So
www.3ds.com. 
– Dassault Systèmes Strategy to Support New Processes 
by 
ss 
h-
n-
w 
ty 
m-
ng 
dustry Solution Experience  
ence is to understand how the customers are expecting 
ility services by providing some innovative ways to col
e market trends, to enable crowd-sourcing and co-creat
ocial innovative design. Powerful 3D design tools to bo
rocess and efficient tools to perform virtual car clinics 
d reality and immersive reality, will be introduced. 
by enabling new in-car experience, including driving ex
ign and engineering front loading experience, thus reduc
ucing physical prototypes. 
               
e, truck, motorcycle, motorsport and transportation indu
lution Experiences from other Industries can be obtained f
Fig. 4. PLM Innovation 
Fig. 5. Transportation and Mobility Industry Solution Experie
339 
the 
llect 
tion 
oost 
and 
xpe-
cing 
ustry  
from 
 
 
ence 

340 
M. Frouin 
Lean, Green and Compliant 
The purpose of this experience is to improve vehicle energy efficiency upfront with 
light weight and green propulsion and to explore new ways of progress by optimizing 
the energy balance and the weight. Providing an end-to-end solution for EV and HEV 
propulsion concepts development accompany companies in their will to be more envi-
ronmentally responsible. An integrated platform for vehicle validation and certifica-
tion to define, manage and monitor the validation and verification process (physical 
and virtual test) insures requirements validation. The compliance towards global regu-
lations and certifications (material compliance, FMVSS, CAVA, passive safety, etc.) 
help to optimize the vehicle performance globally and by providing a multi-physics 
simulation platform to evaluate and optimize all the performance of the vehicle, in-
creased product quality and satisfying sustainability results can be harvested. 
Smart, Safe and Connected 
The market today is challenged by a re-invention of the mobility experience as 80% 
of vehicle innovations are coming from embedded systems and development moves 
from vehicle attributes to vehicle experience. A huge complexity in the number of 
ECUs, lines of code of software and an increasing number of functional requirements 
makes car design error prone, resulting in 20 to 40% of vehicle development cost 
spent in testing and diagnostics. The market demands to improve quality and safety 
and decreasing the number of warranty cost and cars recalls. To demonstrate func-
tional safety against class actions lawsuits and to standardize on industry initiatives 
such as Software re-use (AUTOSAR), Strengthen safety (ISO26262) and early valida-
tion are mandatory to support next generation car developments. The concept of 
Functional Mock up that integrates mechanical, electronics, software and simulation, 
is key to design and simulate the car of the Future: smarter, safer and always con-
nected. 
Modular, Glocal4 and Secure 
This experience allows defining the technical requirements from a marketing analysis 
and ideation process done on unstructured data, to define the vehicle portfolio and 
monitor the program: project dashboards, program 360° and quality plans (incl. sup-
pliers). Platforms and modules, interfaces as well as the vehicle architecture (vo-
lumes, technological choices, etc.) are managed along the defined configurations. The 
management of the EBOM is based on a modular approach and support the Digital 
Mock-up review processes (Design, Manufacturing and Simulation) in a distributed 
environment (Suppliers, R&D centers,). The MBOM manages the assembly processes 
per manufacturing plant and the supply chain, from engineering to after sales, includ-
ing sourcing, RFQ/Supplier choice process, and the contract and cost management, 
etc. are integrated as well. 
Target Zero Defect 
To ensure a zero defect process End-to-End on all the domains, extend the zero geo-
metry defect to the End-to-End, and taking full benefit of new system engineering and 
collaboration environments is the purpose of this experience. 
                                                           
4  Glocal = Global and local. 

 
3DExperiences – Dassault Systèmes Strategy to Support New Processes 
341 
Capitalizing and reusing knowledge globally for design, manufacturing and  
simulation will promote innovative tools and methodologies in Architecture and Con-
ceptual phases of the processes. Augmented simulation contribution and key differen-
tiators in all steps of the processes will improve productivity and efficiency to reduce 
cycles by providing Best-of-Class solutions. 
5 
Complexity of Delivery with Accuracy 
Most product transformations towards Smart Products have been done in complexity 
stages from integration of independent mechatronic systems and intelligent sen-
sors/actuators and interfaces to the current Cyber Physical Systems. 
Delivering accurate Cyber Physical Systems in Smart Products requires a tight 
coupling of computational and physical elements, and therefore the behavior of geo-
metry (deformations, kinematics), physics and controls need to be certified over a 
very high dimensional space.  
Because of the infinite number of potential failure modes of these systems, 3DS 
has developed new concepts for the systems integration. 
 
The System Probabilistic Certificate of Correctness (PCC) quantifies the proba-
bility of satisfying requirements with consistent statistical confidence in integration 
between system components inside a Smart Product. This metric is based on the no-
tion that the fidelity, applicability, tolerances and accuracy of simulation models 
along with the predicted results are equally important for verification and validation 
of all the dimension of the product. To compute PCC at every stage of the product 
lifecycle a large set of technologies has to be deployed, namely virtual prototyping, 
simulation tolerancing, tiered abstraction modeling and automated simulation frame-
works that are available inside the 3DExperience platform.  
PCC can be implemented as a scalable engineering practice for certifying complex 
system behavior at every milestone in the product lifecycle and provides real im-
provement over the V-cycle, because verification and validation happens at every 
stage of the system engineering process thus reducing rework in the more expensive 
implementation and physical certification phase.  
Today’s complex systems have to meet hundreds of top level product acceptance 
requirements which in turn reference numerous standards and sub requirements.  
Validation is the process of establishing evidence that the product meets the cus-
tomer’s requirements. A large variety of techniques are used today to certify cyber 
physical system requirements. Examples are reachability and fault tree analysis, 
Monte Carlo simulation, virtual prototyping as well as quality assurance processes 
used in software development based on mean time between failure, defect density and 
other metrics. 
In leveraging the 3DExperience Platform a combination of these concepts for the 
certification of top level probabilistic requirements is being deployed.  
This is done at virtual product certification level rather than physical product certi-
fication with the understanding that both are essential parts of the certification 

342 
M. Frouin 
process. Especially in the e
not be available and the e
Later in the life cycle, but
essential because it is not fe
Hardware in the loop ex
and the simulated behavio
change and it becomes inte
mission. At that point meas
improve the fidelity of the v
5.1 
Simulation Toleran
The process to define system
Tolerances are widely 
dimensions, physical prop
virtual prototype simulated 
on the computed stochastic
mentation of the use of tol
type. For this to be a scalab
input and output) would h
implementation and shared
sume a uniform maximum e
The confidence interval 
fication of similar models w
5.2 
Tiered Abstraction
Tiered abstractions are a ke
Multi- tier abstraction p
factor of five Tiered abstrac
Fig. 6. Co
early stages of design, physical prototypes most likely w
end user will have to rely heavily on simulated behav
t before product launch relying on simulated behavio
easible to carry out physical experiments.  
xperiments often involves a physical (electronic) contro
or. The configuration and/or mission of the product m
eresting to know if the product can be certified for a n
sured operational data for the product which can be used
virtual prototypes is made available. 
ncing 
m tolerances is called tolerancing.  
used in engineering to indicate the limits in geome
perties, and measured values. Tolerances are a powe
concept in terms 
contractual 
agr
ments as well. W
ranties can be iss
that certain toleran
are 
met. 
Tigh
component toleran
are marketed as hi
er quality produ
which command 
higher price.  
Today there is 
accepted practice 
deploy tolerances 
behavior. Unfortunately, the effect of simulation accur
c results is very large and requires the system wide imp
lerances for each computed behavior of the virtual pro
ble and easy to adopt solution, parameter tolerances (b
ave to be implemented as parameter attributes in a PL
d between applications. Conservatively it’s allowed to 
entropy probability distribution.  
is determined through a quality assurance process of v
with the same assumptions and the same solver settings. 
n Modeling 
ey enabler to efficient PCC calculations.  
process can speed up the engineering design process b
ction modeling processes.  
onfidence Interval 
will 
vior. 
or is 
oller 
may 
new 
d to 
etric  
erful 
 of 
ree-
War-
ued 
nces 
hter 
nces 
igh-
ucts 
 a 
no 
 to 
for 
racy 
ple-
oto-
both 
LM 
as-
veri-
 
by a 

 
3DExperiences – Dassault Systèmes Strategy to Support New Processes 
343 
In the functional Tier 1 the focus is on fast modeling of the product behavior as it 
pertains directly to the customer requirements.  
Trading off customer PCC requirements as well as cost and model tolerances to de-
termine the critical behavior models is required. Critical behavior models are those 
models for which the PCC relevant behavior most efficiently can be influenced by 
increasing their fidelity. For the critical behavior models high fidelity abstractions 
based on high fidelity (CAE) virtual prototypes need to be authored. These models are 
then used as replacements for (initial) critical behavior models in the functional tra-
deoff. This process is repeated until the product is verified and validated for a given 
design milestone. This eliminates costly rework as compared to late stage V-cycle. 
5.3 
Results  
Virtual prototypes are mainly used as a replacement of physical prototypes in the V-
cycle system engineering process. 
3DS targets with 3DExperience platform can show an improvement over the tradi-
tional V-cycle because verification and validation happens at every process stage 
from concept to manufacturing using multi fidelity virtual prototypes. This approach 
identifies problems early and reduces rework in the more expensive implementation 
and physical customer or certification phase. 
In practice the ability to integrate in a single platform all the requested tools to 
create a scalable engineering practice for studying complex system behavior can for 
example enable creation of accurate probabilistic certificate of correctness at every 
milestone in the product lifecycle by 
• Capturing methods by process flow automation 
• Making upfront capture of certification methods and implementation of simulation 
workflows to automate the product design process, thereby enabling rapid loops 
until the proper results are obtained including in case of user / value change. 
• Creating virtual prototypes at different fidelity and abstraction levels 
• Enabling efficient simulation and integrating these models into a simulation 
process flow. Hierarchically nested approximations dramatically reduce simulation 
process flow execution times while maintaining acceptable model accuracy. 
• Verifying requirements in parallel by deploying virtual prototypes across large 
organizations. 
• Scalability by reducing cycle time proportional to additional computational re-
sources. 
• Trading off in real time system sizing, modeling accuracy, technology selection 
and manufacturing tolerances against requirements and costs. This tradeoff process 
detects (human) modeling errors because of the need to explain the computed 
trade-offs. 
6 
Summary 
Acknowledgements. Bernard Charles, Monica Menghini, Alex van der Velden, Steffi 
Dondit, Volker Klare, Eliane Fourgeau. 

344 
M. Frouin 
Copyrights: CATIA, ENOVIA, SIMULIA, DELMIA, Modelica, Dymola, Abaqus 
are DS trademarks. 
References 
1. 3DS Corporate Strategy presentation - Bernard Charles, Monica Menghini (2012) 
2. 3DS 3DExperience Universe for Sustainable Innovation, 
http://www.3ds.com/company/about-dassault-systemes/ 
our-vision/  
3. 3DS Transportation and Mobility Industry Referential presentation 2012 - Olivier Sappin, 
Guillaume Belloncle  
4. 3DS Transportation and Mobility 3DExperience Platform, 
http://www.3ds.com/solutions/ 
transportation-mobility/overview/  
5. Transportation and Mobility Smart, Safe and Connected Solution Experience, 
http://www.3ds.com/solutions/ 
transportation-mobility/industry-experiences/ 
smart-safe-and-connected/ 
6. CATIA Systems Engineering; Behavior modeling and Simulation, 
http://www.3ds.com/products/catia/solutions/ 
catia-systems-engineering/  
7. Proceedings of the ASME 2012 International Design Engineering Technical Conferences 
and Computers and Information in Engineering Conference, IDETC/CIE 2012 (2012) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 345–355. 
DOI: 10.1007/978-3-642-30817-8_34 
© Springer-Verlag Berlin Heidelberg 2013 
 
Data Fusion and 3D Geometric Modeling  
from Multi-scale Sensors 
Dmitry Tansky and Anath Fischer 
Faculty of Mechanical Engineering, Technion, Haifa 32000, Israel 
{tansky,meranath}@technion.ac.il 
Abstract. The past several decades have seen major advances in sensor tech-
nologies, including surface scanning at multi-scales. While state-of-the-art re-
search focuses on methods for integrating diverse scanned data into a single 
geometric model for inspection analysis, these methods still cannot handle mul-
ti-scale data. This paper proposes a new approach for data fusion from multi-
scale sensors by defining two generic frameworks for data fusion: Single-Level 
Multi-Sensor (SLMS) for multi-scale data merged on one level and Hierarchical 
Multi-Sensor (HMS) for hierarchically merged multi-scale data. These frame-
works are based on state-of-the-art generic frameworks and use the properties 
of multi-scale sensors properties. The feasibility of the proposed approach is 
demonstrated on 2.5D surfaces scanned by CMM touch probes and laser scan-
ners and on 3D multi-scale synthetic data from CAD models. 
Keywords: data fusion, multi-sensors, multi-scale, inspection analysis. 
1 
Introduction  
3D inspection analysis has progressed significantly as advanced sensors have evolved 
from single sensors into multi-sensors [1-3]. Multi-sensors have several advantages: 
(a) Different inspection technologies can be used; (b) The number and type of sensors 
are not limited; (c) Diverse data can be added adaptively; and (d) Multi-scale data can 
be merged into a single multi-scale model. Sensors are classified as contact and non-
contact types [3-5] based on the interaction between sensor and inspected part. A 
typical multi-sensor configuration includes both contact sensors (e.g. touch probes) 
and non-contact sensors (e.g., video cameras, laser scanners and micro-probes). The 
multi-sensor head of the Nikon scanning system [6] is depicted in Figure 1. 
Contact and non-contact sensors each have their own working principles and  
properties, simultaneously providing diverse and complementary data that can consi-
derably improve inspection. Contact sensors provide sparse and very accurate high 
resolution (HR) data, while non-contact sensors provide dense and less accurate low 
resolution (LR) data. Due to their differing accuracies, these two data sets can be  
regarded as multi-scale data. The main challenge lies in how to utilize the LR data 
despite its lower accuracy. 

346 
D. Tansky and A. Fi
Fig. 1. Multi-sensor he
Customers continue to d
metric solutions. Multi-sen
multiple single-sensors [6-8
In the literature, data fus
lowing parameters [2-3]: (
(b) arrangement: serial, pa
dundant), complementary, a
feedback only after the fin
historical barriers to techno
terminology that crosses ap
Surface features can be 
Points of Interest (POIs). A
tures, such as edges, corner
a part of some feature. PO
sufficient information, whi
belong to ROIs and can be
According to [2-3], fusion 
(a) alignment of coordinate
(b) feature detection on th
reconstructing the fused mo
Feature detection: Most
[5], [9], [10]. Sipiran and 
Detector [12] for 3D mesh
dratic surfaces and selecting
Matching data sets: Fea
detected feature that can be
grams [13] are descriptors 
spherical sectors (Figure 2c
the feature center of mass. 
bution, enclosed into shells
dratic distance function is 
used for POIs [14]. 
This paper uses an appro
tors, provided by the multi-
ischer 
 
ead [6] with a laser scanner (left) and a touch probe (right) 
demand smaller, faster, cheaper, easier and more prec
nsors can meet these demands more effectively than 
8].  
sion methods for multi-sensors are characterized by the 
(a) data flow: centralized, decentralized, and hierarchi
arallel, and combined; (c) configuration: competitive 
and cooperative; and (d) feedback: without feedback, w
nal stage, and with feedback after every stage. One of 
ology transfer in data fusion has been the lack of unifo
pplication-specific boundaries.  
classified into two groups: Regions of Interest (ROIs) 
A ROI is a region on a model that includes important f
rs and holes. A POI is a point that belongs to the ROI an
OIs are thought to be more unique than ROIs and to ca
ile being less complex to use and store. In addition, P
e useful for matching and merging between sampled d
between geometrical data comprises the following ste
e systems/references (not within the scope of this pap
he data sets; (c) matching data sets; and (d) merging 
odel.  
t feature detection methods are based on curvature analy
Bustos [11] recently proposed an extension of the Ha
hes based on calculating local neighborhoods, fitting q
g POIs.  
ature detection methods usually provide a descriptor of 
e compared with descriptors of other features. Shape hi
 that can be used for matching. First, shells (Figure 2
c) and their combination (Figure 2b) are constructed arou
Next, histograms are calculated of the surface area dis
s, sectors and their combination (Figure 2). Finally, a q
used to compare the histograms. This descriptor can
oach combining curvature analysis and geometric desc
scale shape descriptors-based method [9]. 
cise 
can 
fol-
ical; 
(re-
with 
the 
orm 
and 
fea-
nd is 
arry 
POIs 
data. 
eps: 
per); 
and 
ysis 
arris 
qua-
the 
sto-
2a), 
und 
stri-
qua-
n be 
rip-

 
Data Fusion and 3D Geometric Modeling from Multi-scale Sensors 
347 
Merging and reconstructing the fused model: Since data processing methods [9], 
[15-16] are usually applied on meshes, a mesh should be reconstructed from the 
scanned points. The data can be merged into a single model using strategies such as 
(a) merging all data sets into one cloud of points, (b) stitching different data sets, and 
(c) correction of one data set according to another. This paper uses a variation of 
shape histograms in which similarity is calculated according to the physical properties 
of the data acquisition and then applied on meshes that were reconstructed by the 
Power Crust method [17].  
 
 
 
 
 
(a) 
(b) 
(c) 
Fig. 2. Shape histograms [13] 
Framework models can provide data fusion methods with a theoretical base. 
Among such models in the literature are (a) the JDL data fusion process model [1], 
(b) the Thomopoulos model [18]; (c) the multi-sensor integration fusion model [19]; 
(d) the behavioral knowledge-based data fusion model [20]; and (e) the waterfall 
model [21]. Table 1 compares these models according to data flow, sensor arrange-
ment, sensor configuration and feedback integration. According to the table, the  
 
Table 1. Comparison of models 
Model 
Data flow  
Sensor  
arrangement  
Sensor configuration 
Feedback  
integration 
JDL Data Fusion 
Process Model 
Centralized / 
Decentralized / 
Hierarchical 
Serial /  
Parallel / 
Combined 
Competitive /  
Complementary / 
Cooperative 
After every step 
Thomopoulos 
model 
Centralized /  
Decentralized / 
Hierarchical 
Serial /  
Parallel / 
Combined 
Competitive /  
Complementary / 
Cooperative 
After every step 
Multi-sensor 
integration fusion 
model 
Hierarchical 
Serial /  
Combined 
Complementary / 
Cooperative 
After every step 
Behavioral know-
ledge-based data 
fusion model 
Centralized 
Parallel 
Competitive /  
Complementary / 
Cooperative 
- 
Waterfall model 
Centralized 
Serial /  
Parallel / 
Combined 
Competitive /  
Complementary / 
Cooperative 
After final step 

348 
D. Tansky and A. Fischer 
multi-sensor integration fusion model fits our requirements. It is based on local data 
fusion from different types of sensors, including contact and laser sensors, works with 
serial/combined sensor arrangements, and provides feedback after every step. In addi-
tion, when global data fusion of all data is needed at every step, the behavioral know-
ledge-based data fusion model or the waterfall model can be used. 
2 
Approach  
Multi-sensor systems are usually built modularly, with single sensors integrated into a 
common framework. Each single sensor gives the local data of a measured region, 
and data from all the sensors is integrated into the complete data set. This integration 
can also be defined as a hierarchical tree, in which the leaves are the local data sets 
and the root is the fused model. Thus, basing of the multi-sensor system on the SLMS 
and HMS generic frameworks is proposed.  
In the Single-Level Multi-Sensor (SLMS) framework, the sensors are arranged on 
one level (Figure 3). The behavioral knowledge-based data fusion model or the water-
fall model can represent SLMS using multi single-sensors that scan simultaneously. 
In the Hierarchical Multi-Sensor (HMS) framework, the sensors are arranged hie-
rarchically as a binary tree (Figure 4). The multi-sensor integration fusion model can 
serve as a model for HMS, with every level involving fusion of two different datasets.  
A common SLMS and HMS core can be defined (red dotted rectangle in Figure 3 
and Figure 4). This core should be flexible enough to deal with all types of diverse 
scanned data. The basic core comprises six main stages (Figure 5): (a) data acquisi-
tion; (b) mesh reconstruction; (c) ROI and POI detection; (d) matching data sets; (e) 
merging data sets; and (f) reconstructing the multi-scale model.  
3 
Implementation 
The proposed data fusion frameworks were partially implemented using C++ with the 
OpenGL graphic library and partially using Matlab. Synthetic data was also used for 
feasibility testing. 
 
 
 
Fig. 3. SLMS data fusion framework – general scheme 
Sensor 1 
Sensor 2 
Sensor 3 
Sensor n 
…
Single-Level Multi-Sensor (SLMS) Data Fusion 
Multi-Scale model 

 
Data Fusion and 3D Geometric Modeling from Multi-scale Sensors 
349 
 
Fig. 4. HMS data fusion framework – general scheme 
Data acquisition from multi-sensor system: The sampled data takes the form of 
point clouds when a CAD model is also available. Some of the models were built with 
SolidWorks. Triangle meshes at different resolutions were generated and point clouds 
were extracted. These point clouds are accurate, while sampled data is noisy. Normal-
ly distributed noise is therefore added to the synthetic points in the direction of  
surface normals.  
Mesh reconstruction from sampled data: The Power Crust method was used for 
mesh reconstruction. If the mesh is already provided, this step can be omitted.  
ROI and POI detection on sampled data: The ROIs on both sampled data sets are 
detected, based on Shmukler and Fischer [9]. POIs are then found for each data set. 
Figure 6 shows a sample with marked ROIs (left) and POIs (right). 
Matching between sets of sampled data: The sets of sampled data are matched us-
ing Shape Histograms. Shells and spherical sectors are built for the POIs.  
Calculation of POIs histograms: For each POI, the surface area of the sub-shells is 
defined. The histograms are then calculated for each sub-shell. 
Correlation map between POIs histograms: Corresponding POIs have similar or 
approximated histograms. In contrast to the shape histograms method, in this study 
the physical properties of the data acquisition are also integrated. Every measured 
surface can be represented as a superposition of surface properties, such as roughness 
and waviness [22]. Figure 7 shows an example of two measurements of the same area 
using different sensors. A comparison of Figure 7a and Figure 7b shows that if the 
observing window is big enough, the similarity between profiles can be detected and 
corresponding regions identified. For 2D measurements, the similarity will be in the 
curves, while in 2.5D/3D the surfaces will be similar. Thus, corresponding POIs 
should have similar histograms, and correlations between these histograms should 
 
Sensor 1 
Sensor 2 
Sensor 3
Sensor n 
… 
Data Fusion 
Multi-Scale model 1 
Data Fusion 
Multi-Scale model 2 
Sensor 4
Multi-Scale model m-1
Multi-Scale model m

350 
D. Tansky and A. Fi
Fig
Fig. 6. Mod
Matching 
 
 
 
 
 Sa
Hi
Data 
ROIs and POIs 
Detection  
Merging of the  
multi-scale model 
Reconstruction of 
the multi-scale 
model 
 
Mesh  
reconstruction 
ischer 
g. 5. Proposed approach for basic core 
 
del with ROIs (left) and model with POIs (right) 
ampled Data 1 
POIs 1 
istograms for  
POIs 1
Correlation map 
Selective merging / Retentive merging 
Sampled Data 2 
POIs 2 
Histograms for  
POIs 2
Reconstruct mesh from merged point cloud 
CAD Model 
Reconstruct mesh from point cloud  
Short legend: 
Optiona
Step 
  
al 

 
Data Fusion
yield relatively high values
are built for shell and sector
lated between every pair of
defined as correlation map
correlations are chosen as 
between matched POIs, the
come noises and measurem
between corresponding POI
(a) Example of 
(b) Example of 
Fig. 7. Example of surf
Merging LR dense data
posed approach deals with 
data that is very accurate bu
rate but dense. Hence, HR
merging approaches were u
selective merging, only som
the merged model. The erro
between the measured poin
as a point selection criterio
calculated. Finally, the thr
retentive merging, all point
model, and some of the po
according to the error map
number of points in one set 
set, the error map is not effe
This study used two corr
correction function. The GP
surfaces only and refers to 
n and 3D Geometric Modeling from Multi-scale Sensors 
s [23-24]. Correlation maps between two sampled data 
r histograms. Linear correlation coefficients are then cal
f vectors for each type of histogram. These coefficients 
ps. Based on the threshold value, pairs of POIs with h
candidates for correspondence. If there is a correlat
e ones with the minimum distance value is chosen to ov
ment errors. The result is a correlation map with connecti
Is from sampled data sets. 
 
surface profile, measured by low resolution sensor  
 
surface profile, measured by high resolution sensor  
face profiles, measured by sensors with different sensitivity 
a with HR sparse data into the multi-scale model: The p
two types of noisy sampled data: (a) high resolution (H
ut sparse, and (b) low resolution (LR) data that is less ac
R data is much more reliable than LR data. Two types
used: (a) selective merging and (b) retentive merging. 
me of the points from each set of sampled data are added
or at the measured point, defined as the Euclidian dista
nt and the corresponding point on the CAD model, is u
on. Then, a normalized error map of the sampled dat
eshold of error value is defined to filter noisy points
ts from each set of sampled data are added to the mer
oints are corrected using various approaches, for exam
p defined by the selective merging. Nevertheless, if 
of the sampled data is significantly smaller than in anot
ective.  
rection approaches: Gaussian Processes (GP) and B-Sp
P approach of Colosimo and Pacella [25] works with 2
data from a pair of contact (HR data) and non-contact (
351 
sets 
lcu-
are 
high 
tion  
ver-
ions 
pro-
HR) 
ccu-
s of 
 In 
d to 
ance 
used 
a is 
. In 
rged 
mple 
the 
ther 
line 
.5D 
(LR 

352 
D. Tansky and A. Fi
data) sensors measured on t
and HR data and then to co
and location adjustment fu
with local support with resp
tion of a given degree c
B-Splines of that same deg
considered a low-pass filter
correction function for LR d
Multi-scale model recon
mesh for reconstructing a m
4 
Examples  
Two examples show differe
of the selective merging o
densities, noise values and l
 
 
(a) 
(e) 
F
For two data sets (Figur
Figure 8d), error maps we
selective merging was carr
error map of the multi-scal
point cloud with mesh; blac
set 2. The overall error rate 
tive merging using the B-S
ischer 
the same surface. A two-stage model is used to connect 
orrect dense LR data relative to sparse HR data, using sc
unctions. In the CAD field, a B-Spline is a Spline funct
pect to a degree parameter [26-27]. Every polynomial fu
can be uniquely represented as a linear combination
gree and smoothness. In fact, a B-Spline function can
r [28-29], so that B-Spline curves/surfaces can be used a
data. 
nstruction: The Power Crust method is used to create 
multi-scale model from the multi-scale cloud of points.  
ent types of merging and data. Figure 8 shows an exam
of two synthetically sampled 3D data sets, with differ
level of details, simulating partial sampling by one senso
 
(b) 
(c) 
(d) 
(f) 
(g) 
(h) 
Fig. 8. Example of selective merging 
re 8a and Figure 8b) POIs were calculated (Figure 8c 
ere built for each data set (Figure 8e and Figure 8f), 
ried out according to the threshold. Figure 8g shows 
le merged data. Figure 8h depicts the multi-scale mer
ck points obtained from data set 1 and red points from d
of data set 2 is lower. Figure 9 shows an example of ret
Spline correction function. The merging is done betw
LR 
cale 
tion 
unc-
n of  
n be 
as a 
the 
 
mple 
rent 
or. 
 
and 
and 
the 
rged 
data 
ten-
ween 

 
Data Fusion and 3D Geometric Modeling from Multi-scale Sensors 
353 
two synthetically sampled 2.5D data sets with different densities (Figure 9a and Fig-
ure 9b). Due to the low density of the data set, in order to visualize shape Figure 9a 
includes also the mesh. Multi-scale merged data can be seen at Figure 9c. 
 
 
(a) 
(b) 
(c) 
Fig. 9. Example of retentive merging 
5 
Summary  
This paper has described the SLMS and HMS frameworks that were developed for 
data fusion from multi-scale sensors. These frameworks offer the following advantag-
es: (a) integration of advanced and traditional inspection technologies; (b) unlimited 
number and type of sensors; (c) adaptive addition of diverse data sets; (d) merging 
multi-scale data into a hierarchical multi-scale model. In the future, performance can 
be improved by using an Octree-based data structure for efficient storage and fast 
neighbor searching. Moreover, additional information, such as texture, normals and 
color from the multi-sensors, can be incorporated. 
 
Acknowledgments. This study was partially supported by the Technion Bernard M. 
Gordon Center for Systems Engineering. Partial funding has been received from the 
European Union's Seventh Framework Programme [FP7/2007-2013] under grant 
agreement n° 285075 – MuProD. 
The authors would like to thank Dr. Lev Podshivalov for collaboration and useful 
discussions. The authors also would like to thank Prof. Bianca M. Colosimo of the 
Manufacturing and Production Systems Laboratory at the Dipartimento di Meccanica 
of Politecnico di Milano for collaboration.  
References 
1. Llinas, J., Hall, D.L.: An Introduction to Multisensor Data Fusion. In: Proceedings of 
IEEE, pp. 6–23. IEEE Press, New York (1997) 
2. Esteban, J., Starr, A., Willetts, R., Hannah, P., Bryanston-Cross, P.: A Review of data fu-
sion models and architectures: towards engineering guidelines. Neural Computing and Ap-
plications 14(4), 273–281 (2005) 
3. Weckenmann, A., Jiang, X., Sommer, K.D., Neuschaefer-Rube, U., Seewig, J., Shaw, L., 
Estler, T.: Multisensor data fusion in dimensional metrology. CIRP Annals - Manufactur-
ing Technology 58(2), 701–721 (2009) 

354 
D. Tansky and A. Fischer 
4. Fraden, J.: Handbook of Modern Sensors: Physics, Designs, and Applications, 4th edn. 
Springer, New York (2010) 
5. Miropolsky, A., Fischer, A.: A Uniform Approach for Utilizing Synergy between Inspec-
tion Technologies and Computational Methods. CIRP Annals - Manufacturing Technolo-
gy 55(1), 123–126 (2006) 
6. Nikon. On-line Documentation, http://www.nikonmetrology.com 
7. Carl Zeiss. On-line Documentation, http://metrology.zeiss.com 
8. Hexagon. On-line Documentation, http://www.hexagonmetrology.com 
9. Shmukler, A., Fischer, A.: Verification of 3D freeform parts by registration of multiscale 
shape descriptors. The International Journal of Advanced Manufacturing Technology 49, 
1093–1106 (2009) 
10. Azernikov, S., Fischer, A.: Surface Reconstruction of Freeform Objects Based on Hierar-
chical Space Decomposition. International Journal of Shape Modeling 9(2), 177–190 
(2003) 
11. Sipiran, I., Bustos, B.: Harris 3D: a robust extension of the Harris operator for interest 
point detection on 3D meshes. The Visual Computer 27(11), 963–976 (2011) 
12. Harris, C., Stephens, M.: A combined corner and edge detector. In: Proceedings of the 4th 
Alvey Vision Conference, pp. 147–151. Controller HMSO, London (1988) 
13. Ankerst, M., Kastenmüller, G., Kriegel, H.-P., Seidl, T.: 3D Shape Histograms for Similar-
ity Search and Classification in Spatial Databases. In: Güting, R.H., Papadias, D., Lo-
chovsky, F.H. (eds.) SSD 1999. LNCS, vol. 1651, pp. 207–228. Springer, Heidelberg 
(1999) 
14. Bustos, B., Keim, D.A., Saupe, D., Schreck, T., Vrani, D.V.: Feature-based similarity 
search in 3D object databases. ACM Comput. Surv. 37(4), 345–387 (2005) 
15. Jamshidi, J., Wyn Owen, G., Mileham, A.R.: A New Data Fusion Method for Scanned 
Models. Journal of Computing and Information Science in Engineering 6(4), 340–348 
(2006) 
16. Zheng, H., Saupe, D.: Complex 3D shape recovery using hybrid geometric shape features 
in a hierarchical shape segmentation approach. In: 12th IEEE International Conference on 
Computer Vision Workshops (ICCV Workshops), pp. 1662–1669. IEEE Press, Los Alami-
tos (2009) 
17. Amenta, N., Choi, S., Kolluri, R.K.: The power crust. In: Proceedings of the Sixth ACM 
Symposium on Solid Modeling and Applications, pp. 249–266. ACM, Ann Arbor (2001) 
18. Thomopoulos, S.C.: Sensor integration and data fusion. In: Proceedings of SPIE: Sensor 
Fusion II: Human and Machine Strategies, pp. 178–191. SPIE, Philadelphia (1989) 
19. Luo, R., Kay, M.: Multi-sensor integration and fusion: issues and approaches. In: Proceed-
ings of SPIE: Sensor Fusion, pp. 42–49. SPIE, Orlando (1988) 
20. Pau, L.F.: Sensor data fusion. J. Int. Robot. Syst. 1, 103–116 (1988) 
21. Harris, C.J., Bailey, A., Dodd, T.J.: Multi-sensor data fusion in defense and aerospace. 
Aeronaut. J. 102(1015), 229–244 (1998) 
22. Kalpakjian, S., Schmid, S.R.: Manufacturing Processes for Engineering Materials, 4th edn. 
Prentice Hall, New Jersey (2003) 
23. Gibbons, J.D.: Nonparametric Statistical Inference, 2nd edn. Marcel Dekker, New York 
(1985) 
24. Hollander, M., Wolfe, D.A.: Nonparametric Statistical Methods. Wiley, New York (1973) 
25. Colosimo, B.M., Pacella, M.: On Integrating Multi-sensor Data for Quality Inspection and 
Monitoring. In: X Convegno AITeM. AITeM, Napoli (2011) 
 

 
Data Fusion and 3D Geometric Modeling from Multi-scale Sensors 
355 
26. Farin, G., Hoschek, J., Kim, M.-S.: Handbook of Computer Aided Geometric Design, 1st 
edn. North-Holland, Amsterdam (2002) 
27. Bartels, R.H., Beatty, J.C., Barsky, B.A.: An Introduction to Splines for Use in Computer 
Graphics and Geometric Modeling. Morgan Kaufmann, San Mateo (1987) 
28. Goshtasby, A., Cheng, F., Barsky, B.A.: B-spline curves and surfaces viewed as digital fil-
ters. Comput. Vision Graph. Image Process. 52(2), 264–275 (1990) 
29. Unser, M., Aldroubi, A., Eden, M.: B-spline signal processing. IEEE Transactions on Sig-
nal Processing: Theory 41(2), 821–833 (1993) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 357–366. 
DOI: 10.1007/978-3-642-30817-8_35 
© Springer-Verlag Berlin Heidelberg 2013 
 
Exploiting Service Data of Similar Product Items for the 
Development of Improved Product Generations by Using 
Smart Input Devices 
Michael Abramovici, Andreas Krebs, and Andreas Lindner 
Ruhr-Universität Bochum, IT in Mechanical Engineering, Universitätsstraße 150,  
Bochum, 44801, Germany 
{michael.abramovici,andreas.krebs,andreas.lindner}@itm.rub.de 
Abstract. State-of-the-art industrial products generate large amounts of data 
that is not lead back into product development. The use of modern technologies 
like mobile devices and Auto-ID to identify product items and collect data of-
fers a new, rich data source that can be used by product development. To gain 
an understanding of the products in practical application, the collected data can 
be processed by assistant systems for the improvement of products. The assis-
tant system outlined in the paper in hand uses statistical methods and methods 
derived from risk management to provide a comprehensive analysis. A short 
overview of the analysis results is presented to the product developer as a prese-
lection of parts worth improving. Based on the preselection, the product devel-
oper can choose specific parts and gain further information via the assistant  
system. The system itself provides that information using different methods of 
product and information visualization. 
Keywords: Data Mining, Information flow, Internet of the things, Knowledge 
Engineering, Product use information, QR Code, RFID, mobile computing. 
1 
Introduction 
Throughout the lifecycle of industrial products, a large quantity of data is generated. 
During the early phases of product life (i.e. the design and manufacturing phases) 
mainly unchanging data is generated. This data includes information about the prod-
uct itself (e.g. type information, serial number) and it does not change throughout 
product life. In the later phases of product life, namely the product use phase, differ-
ent types of changing data are generated. That data can e.g. contain information about 
the configuration, the condition, or the environment of a particular product item. This 
data provides a snapshot of the current condition of a given product item. The present 
use of product use data is limited to the specific purposes it has been generated for. As 
a matter of fact, with regards to the improvement of maintenance or later product 
generations, the high potential within that data is currently not considered at all. 
The approach presented in the paper in hand employs mobile computing to facili-
tate the collection of service data that can provide detailed information about the 

358 
M. Abramovici, A. Krebs, and A. Lindner 
usage and maintenance of product items. To gain structured data from different data 
sources (e.g. in-house services, product manufacturer services, third party services), 
the approach implies that a service event can be carried out facilitated by mobile de-
vices and using predefined user dialogs (e.g. check boxes) for specific, frequently 
occurring actions (e.g. the exchange of a component due to a breakdown). The stored 
information is crucial for the service provider and needs to be lead back to product 
development, as it shows great potential for product improvement. Comprehensive 
information about the usage and maintenance of large numbers of product items can 
be analyzed for a variety of purposes. The aggregation of all product use data of an 
individual product item allows an improved design of later product generations. 
Based on this aggregated data, an assistant system is proposed. The proposed system 
utilizes statistical methods and methods derived from risk management. The intro-
duced approach is versatilely applicable for new product generations in the design 
phase and also for products in the later lifecycle phases. 
2 
Related Work 
Feedback Data in Product Development 
Conventional material and information flows are directed forwards and connected to 
the lifecycle of material products. Exceptions comprise recycling where products are 
dismantled and no value or direct benefit is created. For several years, customer feed-
back also has been established as a backwards directed type of information flow [1]. 
Customer feedback is lead from the product use phase of a product into product de-
velopment. The goal is to make products that fulfill customer demands. Customer 
feedback is considered subjective feedback. Therefore it is not free of personal opi-
nions and feelings. The transformation of subjective feedback into objective informa-
tion has been addressed frequently in literature [1]. 
The acquisition of objective feedback is a relatively recent research topic, which 
has evolved together with the development of smart products and new business mod-
els. The latter have lead to a stronger connection between providers and customers, 
and therefore enable the leading back of product-related information. Objective feed-
back is data or information derived from sensors of a product, sensors in the environ-
ment of a product, data generated by service staff, data generated about service staff, 
or quality parameters. [2] Objective feedback is free of personal opinions and can be 
either structured or unstructured. 
Structured data has a homogeneous structure as found, for instance, in databases. 
Unstructured data has no such structure and is e.g. information that is hidden inside a 
document or texts. Unstructured data cannot be captured without further processing 
[3]. Methods of extracting information from text are currently under development. 
When using objective and structured data, a further processing of that data is more 
effective due to fewer efforts converting the data into understandable information. 
Another advantage of objective data is that this data can be generated, collected, and 
aggregated automatically. Therefore the effort of collecting such data is very low. A 
basic problem when using objective data is that important information for a correct 

 
Exploiting Service Data of Similar Product Items 
359 
interpretation of the data may be missing. Hence, comprehensive monitoring of the 
whole product is favored. 
Within different research projects, objective data has been taken into closer consid-
eration. The PROMISE EU research project considers product-embedded information 
devices for information tracking and tracing. A concept for leading information into 
product development and production has been created [4] [5]. 
WiRPro, a further research project, has extended the PLM approach by processing 
product use information chiefly based on the use of Bayesian Networks [6] [7]. This 
project has yielded a feedback assistant system that consists of four modules: analysis, 
diagnosis, decision support, and prediction [8]. These modules have been designed to 
directly support the product developer in improving existing product generations. 
Based on the above-mentioned characteristics and different types of feedback,  
service data, which is collected by smart input devices, will be used. This data can 
either be sensor data or generated by service staff. As the input interfaces of the ser-
vice staffs are expected to be structured, only objective and structured information is 
gathered. 
Use of Automatic Identification Technologies with Smart Input Devices 
Throughout the last decade, the number of smart input devices like smartphones and 
tablet PCs has increased significantly. These devices facilitate mobile computing and 
mobile communication. Some devices also feature embedded cameras or even near 
filed communication (NFC) or radio frequency identification (RFID) reader module, 
and can be used as a reader for automatic identification (Auto-ID) technologies [9]. 
The term Auto-ID technologies refers to a set of different technologies like bar-
codes, e.g. QR codes (readable by camera systems), and RFID tags, e.g. NFC tags 
(readable and rewritable by electromagnetic high frequency transceivers). Auto-ID 
technologies provide an opportunity for automated identification of physical product 
items without manual errors like assignment or typing errors, and can provide cross-
media references between the physical world and the virtual world inside computers. 
They thus constitute a basis for the vision of the Internet of things [9]. The use of the 
Internet of things as a source of information about product items requires marking 
product items by affixation or embedding of marking technologies like RFID tags or 
barcodes onto the item, and to store a unique URL or ID within the marking technol-
ogy. These marking technologies can be read by commonly used mobile devices, to 
access and alter different data sets of a product item in data bases via the internet. 
Summary and Focus of the Research Work 
In a literature research, several approaches have been found that deal with leading 
back data into product development. Still, these approaches are highly specific, as 
they only consider particular data sources for special use scenarios more closely. The 
use of mobile devices and Auto-ID technologies has been taken into consideration 
only rarely, never to gain subjective nor objective feedback information. Moreover, 
the field of leading data from service staff into product development to improve cur-
rent product generations has only been treated scarcely. 

360 
M. Abramovici, A. Krebs, and A. Lindner 
3 
Proposed Approach 
3.1 
Overview 
The presented approach focuses on industrial products, which are cost-intensive and 
used several years. Throughout the product use phase of such products, maintenance 
costs exceed investment costs. Thus, a large number of industrial products are 
equipped with embedded sensors. Due to the sensors and regular servicing, mainten-
ance is the main data source during the product use phase. 
Based on the combined service data (e.g. small errors, total failures) of several 
product items from the same or similar product types, analysis methods can be per-
formed to support the development of further product generations. Product improve-
ment can affect the whole product or just components of the product. 
 
Fig. 1. Concept for the assistant system  
An overview of the presented approach is provided by Fig. 1. The left side of Fig. 
1 shows the relevant data sources of the approach. The data source can be e.g. main-
tenance information collected by service staff with smart devices and embedded sen-
sors. The staff uses smart input devices to generate and collect data and information 
from different product instances. The collection of maintenance information with 
smart devices is performed in several steps. First, the marking technologies are read to 
provide an identification of a unique product item. Based on the identification, the 
smart device can access existing datasets related to this product item via the internet. 
That information can be displayed on the mobile device and be used to assist the ser-
vice staff during inspection. The existing dataset can be updated depending on the 
collected data and the performed actions by the service staff. 
Each particular dataset is stored in a local database of the service provider who is 
in charge of maintenance of the related product item. To provide a sufficiently large 

 
Exploiting Service Data of Similar Product Items 
361 
number for statistical analysis, the different heterogeneous databases have to be  
aggregated. The aggregated data is stored in a central database, which is part of the 
proposed assistant system. 
To efficiently support the product developer in improving existing products, the  
assistant system consists of four interacting modules for the 
• 
identification of critical parts, 
• 
information provision, 
• 
impact analysis of product improvements, and 
• 
visualization. 
Based on the aggregated data, the module for the identification of critical parts pro-
vides the developer first with condensed information about components that require 
improvement. The visualization module provides that information in an environment 
(PLM and CAD system) known to the product developer. To gain further information 
about a component, the developer can access further information via the information 
provision module. The module for the impact analysis of product improvements inhe-
rits knowledge about components like earlier improvements and their impact. That 
knowledge is stored as lessons learned. 
3.2 
Feedback-Relevant Data Sources 
The approach presented in this paper can use different kinds of data sources to gain 
detailed information about the usage and maintenance of product items for the same 
or a similar product type. These data sources can be data collected by service techni-
cians and product users, as well as sensor-derived data, which are integrated into in-
dustrial goods. To make the approach versatilely applicable for a great variety of 
present product types, the presented approach is mainly focused on data of service 
technicians and product users. Using service technicians and product users as a source 
of data makes it possible to consider simple products, without any sensors. That way, 
the proposed method can be used for products that are in the development phase or in 
the later lifecycle phases. This data source can provide a great detail even about  
complex or unforeseen situations, which cannot be fully recorded via sensors e.g. 
complete breakdowns, disturbing environmental conditions on the installation site, or 
an interplay among errors. 
To assist service technicians and product users in gathering data, the outlined  
approach suggests the use of mobile devices that can read marks of automatic identi-
fication technologies such as barcodes and RFID tags, and can access the data sets, 
which are related to the ID or URL, that are stored inside the marks via the internet. 
This information can be used and displayed as a part of guided user dialogs, which 
can lead through a systematic inspection of the product item. The order of the inspec-
tion steps can be based on the structure of the product type and the known errors of 
the type. Each inspection step can be assisted by short information. The result of the 
inspection step can be noted in the form of damage codes or check boxes if necessary. 
Through the use of chiefly predefined user dialogs on the mobile devices, data 
gained during the inspections can be considered structured data. The collected, struc-

362 
M. Abramovici, A. Krebs, and A. Lindner 
tured data from different data sources (e.g. in-house services, product manufacturer 
services, third party services) can be integrated into a central database, which contains 
extensive data about errors that have occurred with a large number of similar product 
items. 
3.3 
Modules of the Assistant System 
Module for the Identification of Critical Parts  
The presented approach uses consolidated, structured data of different data sources 
(e.g. in-house services, product manufacturer services, third party services) to gain 
comprehensive information about the errors of a vast number of similar product items, 
which is derived from the same or similar product types. The information about the 
usage and maintenance of larger quantities of product items can be analyzed for a 
variety of purposes. The main focus of the introduced module is the automated identi-
fication of critical parts of product components that need to be improved and consi-
dered for redesigning or a new design. 
To provide an adequate selection of components, which needs to be reviewed by a 
product designer, the module for the identification of critical parts uses simple but 
effective methods. The basic idea of these methods lies in a comparison between the 
actual and the target performance of the considered product items. One of these me-
thods uses statistical methods to analyze the achievement of target performance cha-
racteristics. Another method is based risk management to analyze the occurrence of 
minor errors and total failures. 
The method based on statistical methods analyzes the achievement of target per-
formance characteristics by using average values and statistical frequency analyses. 
These target performance characteristics are provided by the values that are listed in 
the specifications, regulations and manufacturer’s quality requirements. The relevant 
regulations can be listed in standards, legislative regulations, or obligations of treaties. 
The manufacturer’s quality requirements are not necessarily documented in the prod-
uct specifications but should be orientated towards customary business practice in the 
related sector and the expected customer requirements. As a first step of the method, 
the module for the identification of critical parts uses data collected form a large 
number of similar product items to calculate average values of different product cha-
racteristics. In a second step, the module for the identification of critical parts per-
forms a statistical frequency analysis and calculates how many product items have 
reached certain percentages of an estimated value. Fig. 2 shows a visual example of a 
result of the statistical method based on the meantime between failures (MTBF). 
The method based on risk management analyzes the occurrence of negative effects 
like minor errors and total failures by using a risk matrix. The risk matrix is a method 
that is used in risk management approaches like ISO 31000 or ISO27005 to provide 
an overview of all considered risks, and it also shows the importance of the various 
 

 
Exploiting Service Data of Similar Product Items 
363 
 
Fig. 2. Example for the MTBF assessment of one component 
risks. Since risk is often defined as to equal the product of the likelihood of the nega-
tive incident multiplied with the impact of the unwanted incident, the risk matrix uses 
these dimensions as axes (cf. Fig. 3). The likelihood of errors and total failures, which 
is represented on the horizontal axis, can be calculated statistically. The impact of an 
incident shown on the vertical axis can be measured based on repair efforts and fol-
low-up costs. Fig. 3 shows a visual example of a result from the proposed method 
regarding the risk of component errors and total failures of a machine. 
 
Fig. 3. Example for the risk analysis of different components 
The first result of the two proposed methods is a classification of the product and 
its components into three different categories. Depending on the results of the two 

364 
M. Abramovici, A. Krebs, and A. Lindner 
methods, a product and its components can be classified as “sufficient no review”, 
“review is advisable” or “review is crucial”. This classification is used by the visuali-
zation module to offer product designers a preselection of products and components, 
which require improvement. Detailed graphical output as presented in Fig. 2 and Fig. 
3 is given to the module for information provision. 
Module for Information Provision 
The main function of the module for information provision is to provide additional 
information to preselected products and components. To gain the additional informa-
tion, the product developer has to choose preselected parts, which are classified as 
“review is advisable” or “review is crucial” of the product and need to be considered 
in more detail. Examples for additional information can be the output generated by 
the two preselection methods (cf. Fig. 2 and Fig. 3) and the collected and aggregated 
data itself. 
To display the information in a context that is customary to product developers, the 
module for information provision processes the additional information for a presenta-
tion via the visualization module. To depict all additional information in a compre-
hensive overview, the visualization module uses methods of data and information 
visualization (e.g. area charts, hyperbolic trees). 
Module for the Impact Analysis of Product Improvements 
A major task of the module for the impact analysis of product improvements is to 
assist product developers in checking and analyzing the effectiveness and efficiency 
of already performed improvements. To enable a deeper analysis, the module for the 
impact analysis of product improvements provides the possibility of tracing back the 
impact of modifications made on parts or components over many changes and product 
generations. The discoveries are stored as “lessons learned” and provided to the prod-
uct developer in later product changes.  
Another task of the module for the impact analysis of product improvements is the 
management of dependencies and interactions between positive and negative impacts 
of changes made, which have been detected by the product developers. That kind of 
knowledge also enriches the “lessons learned”. 
For an effective knowledge provision, knowledge visualization methods are used 
(e.g. decision maps). 
Visualization Module 
The visualization module supports the product developer in improving existing prod-
ucts by displaying crucial information in a simple and easy to understand way. The 
information and knowledge for the identification of critical parts gained in the mod-
ules, the information provision, and the impact analysis of product improvements  
will be presented within the working environment of the product developer. For this 
task digital engineering visualization methods are used in combination with the  
information and knowledge visualization methods of the modules for information 

 
Exploiting Service Data of Similar Product Items 
365 
provision and impact analyses of product improvements. These methods have to be 
combined and adapted in the approach as a whole. 
The example of the visualization module presents an early outlook on a possible 
later implementation of the visualization module. The visualization module displays 
the results of the modules for the identification of critical parts, information, and im-
pact analyses of product improvements. 
Often JT technology is used to generate small sized 3D models. In this particular 
case, the 3D pdf technology has been used to create the model (cf. Fig. 4). Next to the 
visualization of the 3D model, the product structure is presented. 
The product developer can interact with the 3D model and the product structure. 
By double clicking a part in the 3D model, that part will be highlighted in the product 
structure and additional information or a dialog open up. The product developer then 
has the opportunity to choose the information he wishes to see. 
 
 
Fig. 4. Visualization example 
4 
Outlook and Future Work 
The presented concept provides an approach for the analysis of structured service data 
for the purpose of product improvement. That data can be collected by the use of 
mobile devices and Auto-ID technologies. The data from different sources can be 
aggregated in one central database, which provides the possibility for a detailed anal-
ysis using statistical methods and such derived from risk management. This analysis 
assists product developers in identifying critical products and components with the 
most need for improvement. For enhanced integration into product development, the 
visualization of this analysis can be integrated into existing tools like PLM and CAD 
systems. 
Future research work will also explore unstructured data like texts for the en-
hancement of the product development process. Further research will feature the  
integration of the approach into the WiRPro research project, which mainly considers 
structured sensor data collected from industrial products.  

366 
M. Abramovici, A. Krebs, and A. Lindner 
References 
1. Schulte, S.: Integration von Kundenfeedback in die Produktentwicklung zur Optimierung 
der Kundenzufriedenheit. Shaker Verlag, Aachen (2007) 
2. Abramovici, M., Lindner, A., Walde, F., Fathi, M., Dienst, S.: Decision support for improv-
ing the design of hydraulic systems by leading feedback into product development. In: Pro-
ceedings of the 18th International Conference on Engineering Design (ICED), Copenhagen 
(2011) 
3. Bracht, U., Geckler, D., Wenzel, S.: Digitale Fabrik - Methoden und Praxisbeispiele. Sprin-
ger, Heidelberg (2011) 
4. Kiritsis, D.: Closed-loop PLM for intelligent products in the era of the internet of things. In: 
Proceedings of Computer-Aided Design (2011) 
5. Rostad, C., Myklebust, O., Moseng, B.: Closing the product lifecycle information loops. In: 
18th International Conference on Production Research, Fisciamo, Italy (2005) 
6. Abramovici, M., Lindner, A.: Providing product use knowledge for the design of improved 
product generations. In: CIRP Annals - Manufacturing Technology, Budapest, Hungary 
(2011) 
7. Dienst, S., Fathi, M., Abramovici, M., Lindner, A.: A Conceptual Data Management Model 
of a Feedback Assistance System to support Product Improvement. In: IEEE International 
Conference on Systems, Man and Cybernetics (IEEE SMC 2011), Anchorage, Alaska 
(2011) 
8. Abramovici, M., Lindner, A., Dienst, S.: Use Case of providing Decision Support for Prod-
uct Developers in Product Improvement Processes. In: Proceedings of the 5th International 
Conference on Integrated Systems Design and Technology (ISDT), Mallorca, Spain (2012) 
9. Fleisch, E., Friedemann, M.: Das Internet der Dinge: Ubiquitous Computing und RFID in 
der Praxis: Visionen, Technologien, Anwendungen, Handlungsanleitungen. Springer, Ber-
lin (2005) 
 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 367–376. 
DOI: 10.1007/978-3-642-30817-8_36 
© Springer-Verlag Berlin Heidelberg 2013 
 
Design of a Clip Product Based on Customer Needs  
for Playing Acoustic Music  
Julien Veytizou, Hugo Xuereb, and Guillaume Thomann 
GSCOP – Grenoble Laboratory for Sciences of Design, Optimisation and Production  
46, avenue Félix Viallet - 38031 Grenoble Cedex 1 - France 
Julien.Veytizou@grenoble-inp.fr 
Abstract. Inclusion of uses and users in product design remains a challenge to 
take up; especially when their characterizations are very specific (it’s the case 
with disabled persons). In the musical domain, a lot of adapted interfaces are 
manufactured to enable users with disabilities to play music from digital audio. 
But few of them allow the music practice on acoustic instruments, which is one 
of the goal identified by the AE2M non-profit association (Ergonomic Adapta-
tion of the Musical Material). In this specific context, this paper presents the de-
sign of a universal product which transforms any user environment object to a 
personalized interface, to play percussion instruments. 
Keywords: user environment, design, disabled people, acoustic music, robotic 
musical. 
1 
Introduction 
Traditionally, designers create products or interfaces for able-bodied users, assumed 
to have normal motor skills [1]. Atypical users, including people with disabilities, 
remain outside of these skills and capabilities standards. They must adapt to with 
existing systems, using technical aids. Unfortunately, these technologies are unusable 
for people with temporary impairments. They are also not adapted to users whose 
abilities change over time. The data, detailed in a research report conducted by INSEE 
(France's National Institute of Statistics and Economic Studies), show a significant 
need of assistance in handling (only 35.3% satisfaction for remote handling) [2]. Nev-
ertheless, these means are often abandoned because of their price and complex use. 
The main reasons for giving up are due to a minimum consideration of future users in 
design activities. 
There are two main groups among system designers for disabled people [3]:  
• 
The first which go to certain well-defined needs and seek a technological solu-
tion (social pull). These proponents generally have a good knowledge of disabili-
ties. They are looking primarily practical systems, efficient and robust,  
sometimes ignoring the more advanced technologies. The risk here might be to 
focus too much on obsolete technology, unusable or too expensive. 

368 
J. Veytizou, H. Xuereb, and G. Thomann 
• 
The second which go to a technology they have mastered and are looking for 
applications in the field of disabilities (technology push). The risk here might be 
to propose solutions for imaginary or non-priority needs. 
Integrate multidisciplinary teams in the design process can avoid these risks. 
Our research work consists to the development and the manufacture of physical in-
terfaces facilitating access to instrumental music for children with heavy physical 
disabilities. Currently, disabled user environment are not adapted to their needs and 
human machine interface. One solution is a custom design of products for each user 
motor skills. But this process can be lengthy and difficult because of the myriad of 
user profiles. Moreover, one of the most objectives is to reach low cost production 
and the large diffusion for disabled customers.  
 
The research question can be written as below: How to design a universal prod-
uct which would transform any user environment to a personalized interface?  
To answer this question, our methodology is the following: 
Section 2 presents an identification of needs in our context thanks to a literature   
review. Three main parts are exposed: future user characteristics, existing adapted 
system to play music, human machine interaction in musical practice. Thanks to the 
needs list defined previously, section 3 offers a first prototype based on piezoelectric 
sensors technology. Its structure is defined with a representation of human machine 
interaction in our specific context. Its architecture is described in functional         
component decomposition. After a presentation of test methods, section 4 shows the 
first results when using this interface for a musical playing. 
2 
Identification of Needs 
As the NF EN ISO 9241-210 UCD Process1 recommends, designers take into account 
the contextual factors as well as the requirements of the future users before producing 
design solutions.  
2.1 
User with Disability 
User characteristics define the context in which the product is used. The futures users 
are children with disability. We are focused on the disability ICF-CY definition          
(International Classification of Functioning, Disability and Health for Children and 
Youth), proposed by the WHO (World Health Organization) to understand the     
terminology of disability [4]. This definition provides a common language that can 
record the problems occurring during infancy, childhood and adolescence. It is based 
on body functions and body structures, activity limitations, restrictions participation 
and environmental factors significant for children and youth (Figure 1).  
 
1. NF EN ISO 9241-210: 'Human-Centred Design Processes for Interactive Systems' 
Genève, Switzerland, International Organization for Standardization, January, 2011. 
 

 
Design of a Clip Produc
Fig. 1. ICF-CY definition: Inte
Child and Youth 
Disability therefore resu
problems and contextual fa
modifications of body func
Limitations are difficulties 
eating, playing etc...). Part
participate in a situation of 
2.2 
Existing Adapted S
This section analyzes exist
reveal needs, problems and 
2.2.1 
To play Music w
Interfaces designed for disa
• 
The BAO-PAO fits pe
to operate a computer 
each terminated by two
per sphere and the low
it with a drumstick or h
• 
TouchTone is an elect
ability, to develop bim
children with hemipleg
• 
The GUI//RO is a styl
feedback. The system i
by Arduino board. Sou
stylus in touch-sensitiv
 
ct Based on Customer Needs for Playing Acoustic Music 
 
ernational Classification of Functioning, Disability and Health
ults from the interaction between a person with hea
actors (personal and environmental). Impairments refer
ctions or anatomical structures, such as paralysis. Activ
an individual may experience in these activities (walki
ticipation restrictions are problems a person may mee
daily life, such as transportation inaccessibility.  
System 
ting systems for the practice of music. This analysis 
constraints that have not been taken into account. 
with Digital Audio 
abled people allow musical practice from digital audio:  
erfectly with the computer-assisted music because it ne
and special software. It consists of four rigid steel arch
o spheres at the ends. A laser beam passes between the 
wer sphere. It generates a sound when the musician cros
hand [5].  
tronic musical instrument. Its goal is to develop mus
manual coordination and to increase social participation
gic cerebral palsy [6].  
lus controlled instrument, augmented with resistive fo
is manufactured through an electromagnetic coil control
unds and haptic effects are generated by the position on 
ve screen [7].  
369 
h for 
alth  
r to 
vity 
ing, 
t to 
can 
 
eeds 
hes, 
up-
ssed 
ical 
n of 
orce 
lled 
the 

370 
J. Veytizou, H. Xuer
With these solutions, user
Nerveless, the musical instr
a real musical instrumen
equipment installation can
disabled people. 
2.2.2 
To Play Acousti
Few systems allow the mus
AE2M non-profit associatio
provides access to instrum
autonomy level as non-disa
three main applications spe
instrumental music and mu
specialist therapists) and En
 
Fig. 2
For a better understandin
the roles and the work of th
with the properties of the 
they consult the music spec
with the paramedical spec
They know some physical 
development.  
Generally, interfaces m
play percussion instrument
tem to allow a strike of the
activates the system by the
positioned around him acc
system, the musician could
the preparation of musical
profiles. Depending on mot
(wheelchair, tablet ...), this 
reb, and G. Thomann 
rs can play any instrument thanks to synthesize sou
rument is dematerialized. Indeed, the user doesn’t visua
nt for the music game. Moreover, transportation 
n be difficult and not adapted with the environment
ic Music with AE2M Project 
sic practice on acoustic instruments. This is the goal of 
on (Ergonomic Adaptation of the Musical Material) [8]
mental music for children with disabilities, with the sa
abled. It’s a multidisciplinary team which is surrounded
ecialties (Figure 2): Musicians (by specialized professor
usical manufactory), Paramedical specialists (by doct
ngineers (principally by teachers and engineering studen
 
. Competencies triangle of AE2M project 
ng of this competencies triangle, it is necessary to deve
hese specialties in the project. Engineers should be fami
musical instrument which children would try to play.
cialists of the project. Moreover, engineers have to disc
cialists who spend all their times with disabled childr
capacities of these children, those needed for the proj
anufactured by the association AE2M allow children
s A specific solution realized is an electromechanical s
e drumstick on percussion instruments (Figure 3). The u
e means of accessories (push button, pressure effort, e
cording to his ability to perform body motions. With 
dn’t perform different strike velocity on the instrument 
 activities could be long because of the  myriad of u
tor skills (movement, strength ...) and the user environm
system may have difficulty to be integrated naturally. 
und. 
alize 
and      
t of    
f the 
]. It 
ame 
d by 
rs in 
ors, 
nts).  
elop 
iliar 
 So 
cuss 
ren. 
oject 
n to 
sys-
user 
etc.) 
this 
and 
user  
ment 

 
Design of a Clip Produc
Fig. 3. Use o
2.3 
Human Machine In
To define the user tasks, it 
proposes a generic definiti
fills the space between the 
gy) and musical sounds (w
“instrumental gesture” conc
on the physical world and a
ing information. Then, his d
nication modality specific a
• 
It applies to a material o
• 
In this context of intera
• 
These phenomena can t
2.4 
Synthesis 
These different parts allow 
face realization. In our mus
lar health condition, a prod
ties) and even to participate
non-disabled (body structur
system must be easy to inst
the user. In addition, the de
tem response must be trans
ten milliseconds (between 
system). The system must a
engineers, the interface mu
ance must be performed, it 
in the user environment as a
ct Based on Customer Needs for Playing Acoustic Music 
 
of electromechanical magnet for a playing game 
nteraction for Musical Practice 
is important to design the user-system interaction. Me
ion that considers the “musical gesture” as anything t
musical intentions (cognition, psychology, and musico
waveforms physical) [9]. “Musical gesture” includes 
cept. Cadoz defines “Gestural Canal” as a means of act
as a means of two way communication: sending and rece
definition considers the Instrumental Gesture as a comm
at “Gestural Canal” as follow [10]: 
object and there is physical interaction with it, 
action, it will occur physical phenomena,  
then become communicational messages. 
describing the needs as well as requirements of our in
sical context, we must offer to disabled child with parti
uct which would allow them to enjoy musical play (act
e in concert (participation) with the same autonomy leve
re and function). For musicians and paramedical team, 
tall and must be integrated naturally to the environmen
elay between the musical intention of the user and the s
sparent. The musician recommends a delay time less t
the action of the children and the action of mechan
also perform different strike velocity on the instrument. 
st contain a minimum of electronic equipment. If maint
should be easy. Our proposition is to transform any obj
an action unit on electromechanical systems.  
371 
tois 
that 
olo-
the 
ting 
eiv-
mu-
nter-
icu-
tivi-
el as 
the 
nt of 
sys-
than 
nical 
For 
ten-
ject 

372 
J. Veytizou, H. Xuereb, and G. Thomann 
3 
Concept Solution Proposition 
This section offers a concept solution using the previous synthesis. The product struc-
ture is defined with a representation of human machine interaction in our specific 
context. Its architecture is described in functional component decomposition. 
3.1 
Product Structure 
The product structure is based on our proper representation of human machine inte-
raction using the definition of “musical and instrumental gesture”. In our context, the 
user has a musical intention (Figure 4). He will perform an action on an object envi-
ronment. This interaction will produce physical phenomena (force, pressure or vibra-
tion) that can be controlled by the user (duration, amplitude). These phenomena can 
thus become communicational messages to the electromechanical system for the 
strike of the drumstick on the percussion instrument. The essential elements in design-
ing of our future product are: 
• 
A clip sensor for the physical phenomena detection.  
• 
An electronic board to transform these phenomena in action message for actuator 
positioned on the musical instruments.  
 
The development is inspired from the different work realize by the LEMUR (League 
of Electronic Musical Urban Robots) which is the producer of the famous Pat Methe-
ny's Orchestrion composed of forty robotic musical instruments. It uses principally 
stepper motor, hobby servo and solenoids as actuator. They are controlled by a Pulse 
Width Modulation Signal (PWM) [11][12]. 
 
 
Fig. 4. Human Machine interaction of our interface 

 
Design of a Clip Product Based on Customer Needs for Playing Acoustic Music 
373 
3.2 
Product Architecture 
From this previous definition as well as the identification of needs, this part describes 
the product architecture. It’s divided principally in two parts (Figure 5):  
• 
Control part with sensor clip, electronic board and microcontroller input  
• 
Actuator part with microcontroller output, electronic board and mechanical  
system 
The sensor emits a signal. It is interpreted by a microcontroller to control a mechani-
cal system placed on the instrument. These control actions are based on the musical 
intentions of the user. In our case, the Arduino board corresponds to our expectations. 
This material is open source, free and easy for everyone to use. After this interpreta-
tion, Arduino can generate PWM signals to control a solenoid. But the output current 
is too weak to command this system. A power signal processing is therefore neces-
sary. All this process must be realized in less than 10 milliseconds. 
 
 
Fig. 5. Architecture of the smart clip sensor 
4 
Evaluation of the Solution 
This section shows an evaluation of the design solution against user needs and re-
quirements. Test methods are presented for the sensor choice and the study of the 
strike velocity. 
4.1 
Tests Methods 
The sensor choice will be based on the following main criteria: accuracy, sensitivity, 
size and of price. Three different sensors performances are studied. They are different 
in size and form: two circular (Multicomp and Murata) and one rectangular (Pro-
wave). They are arranged in a circular arc at an equal distance (d) from a disturbance 
area where it will be throw, at the same height, different weights. Sensitivity is meas-
ure by an oscilloscope (Figure 6). To study the strike velocity, Musician asks to the 
user to perform three different strikes (Piano, Mezzo, and Forte) on a plastic box 
where the clip sensor is located (n°1 on Figure 7). After signal processing (n°2 on 

374 
J. Veytizou, H. Xuer
Figure 7), the drumstick m
ure 7) is filmed. Speed strik
source solution for video an
ists and ergonomics enginee
 
Fig. 6. Test for sensor c
4.2 
Sensor Choice 
Figure 8 shows the sensitiv
and Prowave. The signal a
disturbance area. For a 500
comp 9.8 volts and Prowav
than 120 grams, Murata fo
For the next experimentatio
 
Fig. 8. S
reb, and G. Thomann 
movement (n°3 on Figure 7) on the instrument (n°4 on F
ke is analyzed with the Kinovea software, a free and o
nalysis. It is mostly used by sports coaches, animation 
ers. 
choice  
Fig. 7. Test for signal processing and velocity
vity analyzes of piezoelectric sensors Murata, Multico
amplitude is measured when the weight is throwing in 
0 grams weight, Murata gives a signal of 30.2 volts, Mu
ve 2.9 volts. Multicomp exceeds 5 volts for weight grea
or few grams. However, Prowave never exceed this val
on, the strike velocity is analyzed with Murata. 
 
Sensitivity analyze of piezoelectric sensors 
Fig-
open 
art-
 
y 
omp 
the 
ulti-
ater 
lue. 

 
Design of a Clip Product Based on Customer Needs for Playing Acoustic Music 
375 
4.3 
Strike Velocity 
Figure 9 shows the strike speed of the drumstick controlled by the mechanical      
system. Three strike speeds are saved: the first at 0.4 m/s, the second at 0.7 m/s and 
the third at 1.9 m/s. 
 
 
Fig. 9. Strike velocity study with Murata sensor 
5 
Discussion 
Our interface allows transforming any user environment object to a percussion musi-
cal instrument. Its architecture is simple with low-cost and easy system Arduino, little 
electronic equipment to facilitate maintenance. Our system is integrated more natural-
ly to the environment of user. For the sensor choice, Murata sensitivity is more  
efficient. Indeed, a child with low motion disabilities could control musical instru-
ment. The response time of the system is less than two milliseconds (inferior at ten 
milliseconds recommended by the musician). It's possible to adjust the strike velocity 
on the instrument with different actions on an object.  
6 
Conclusion 
Identification of needs and collaboration with the AE2M multidisciplinary team has 
allows to generate uses specifications of a product. These specifications have been 
destined to a team of Research and Development, expert in new technology. Thus, 
this methodology has led to the design of a universal product which would transform 
any user environment to a personalized interface. The product structure has defined 

376 
J. Veytizou, H. Xuereb, and G. Thomann 
by a representation of human machine interaction in our context. The product archi-
tecture is described in functional component decomposition. After these first encour-
aging results, this system facilities access to instrumental music for users with heavy 
physical disabilities, but also to increase the uses performance by musicians. The 
perspectives are focused by the clip design, the increase of the performance of the 
mechanical system and the “intelligence” of this system (the actuator part is able to 
adapt to user motor skills). 
References 
1. Keates, S., Clarkson, P.J., Harrison, L.-A., Robinson, P.: Towards a practical inclusive de-
sign approach. In: Proceedings on the 2000 Conference on Universal Usability, New York, 
NY, USA, pp. 45–52 (2000) 
2. Brouard, C.: Le handicap en chiffres. CTNERHI Centre Technique National d’Etudes et de 
Recherches sur les Handicaps et les Inadaptations (2004) 
3. Sperandio, J.-C.: Designing technological devices for a normal population, namely also in-
cluding disabled people and the elderly. Pistes 9(2) (2007) 
4. ICF: International Classification of Functioning, Disability and Health (ICF) Disability and 
Health (2001) 
5. BAO- PAO (2012), http://www.bao-pao.com/index.php 
6. Bhat, S.: TouchTone: an electronic musical instrument for children with hemiplegic cere-
bral palsy. In: Proceedings of the Fourth International Conference on Tangible, Embedded, 
and Embodied Interaction, New York, NY, USA, pp. 305–306 (2010) 
7. Müller, A., Hemmert, F., Wintergerst, G., Jagodzinski, R.: Reflective Haptics: Resistive 
Force Feedback for Musical Performances with Stylus-Controlled Instruments. In: Pro-
ceedings of the International Conference on New Interfaces for Musical Expression,  
pp. 477–478 (2010) 
8. AE2M, AE2M Project (2012), http://projetae2m.free.fr/ 
9. Métois, E.: Musical Sound Information - Musical Gestures and Embedding Systems. PhD 
thesis, Massachusetts Institut of Technology (1996) 
10. Cadoz, C.: Gestural channel of the man machine communication: instrumental communi-
cation. TSI. Technique et Science Informatiques 13(1) 
11. Kapur, A., Singer, E., Suleman, A., Tzanetakis, G.: A comparison of solenoid-based strat-
egies for robotic drumming. ICMC Copenhagen (2007) 
12. Kapur, A., Darling, M.: A Pedagogical Paradigm for Musical Robotics. Presented at the 
New Interfaces for Musical Expression, Sydney, Australia (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 377–386. 
DOI: 10.1007/978-3-642-30817-8_37 
© Springer-Verlag Berlin Heidelberg 2013 
 
Technology Roadmapping Based  
on Key Performance Indicators  
Felix Spangenberg and Dietmar Göhlich 
 Product Development Methods and Mechatronics, Technische Universität Berlin,  
Strasse des 17. Juni 135, Berlin, 10623, Germany 
{felix.spangenberg,dietmar.goehlich}@tu-berlin.de 
Abstract. In this paper a new technology roadmapping method, based on the 
stochastic simulation of key performance indicators, is presented. Initially the 
specific challenges and the system model associated with the introduction of an 
innovative electric bus systems are worked out. The future behavior of this 
complex technological system is forecasted based on a Monte Carlo simulation 
method with stochastic input to take uncertainties into account and a determi-
nistic model which is derived from the structure of the technological system. 
Stochastic outputs are obtained for technology readiness, economic feasibility 
and environmental sustainability. The data are evaluated with a technology in-
formation management system to generate technology roadmaps and to support 
decision making on development strategy and planning of time to market. The 
method is exemplary illustrated for an urban electric bus system with a special 
focus on the technology readiness level as a performance indicator.  
Keywords: Roadmapping, Systems Simulation, Monte Carlo Method, Key Per-
formance Indicators, Electric Bus System. 
1 
Introduction 
Today’s technological innovations typically comprise different integrated sub-
systems. Examples for such systems innovations are the emerging electric mobility in 
private and public transportation and the smart electricity grids.  
The proper planning of research and development (R&D) efforts to develop the 
underlying technologies depends on many influencing parameters and it has to con-
sider technological and economic forecasts. Therefore finding the right timing for a 
successful market launch of systems innovations and starting the necessary R&D 
activities accordingly is a major challenge of innovation management. 
An example is the development of a new urban public bus transportation system 
with electric buses and inductive charging infrastructure at certain waypoints. Here 
the technology readiness levels (TRL) of the electric drive train, the charging system 
and the battery have to match the requirements. Besides the TRL of the subsystems 
other key performance indicators (KPI) are for example life cycle cost (LCC) or the 
specific emissions factor per output unit (SEF).  
KPIs are an important instrument for fact based technology decision and in this 
context the method of technology roadmapping (TRM) is widely used. TRM is  

378 
F. Spangenberg and D. Göhlich 
usually based on workshops to define market needs, product specifications, technolo-
gies and resources required. However the quality of the resulting roadmap depends on 
the specific knowledge, availability and skills of the workshop participants. Addition-
ally the results of such a workshop may be biased by personal attitudes and erroneous 
estimations.  
In this paper a new method is presented, which combines data derived from work-
shops with quantitative system simulation. The input parameters of the system are 
stochastically modeled to take uncertainties in the system environment into account. 
Based on a Monte Carlo simulation different technologies can be evaluated in their 
system context. The KPI mean values give a measure of the expected system perfor-
mance while the standard deviations represent the forecast uncertainty. These indica-
tors are computed over the considered time span and compiled in a management  
information system which serves as a tool for decision making regarding the right 
timing for systems innovations market launch. Furthermore this information system 
supports the planning of the different product development processes.  
The applicability of this method is demonstrated for a new electric urban bus sys-
tem. The TRL-levels of the different subsystems, fuel cost, price of electricity, carbon 
dioxide emissions and performance of battery system are considered as stochastic 
inputs. Based on the predicted TRL, generated by the system simulation, different 
stakeholders can derive their specific TRM.  
2 
Background and Objectives 
2.1 
Systems Innovations 
A technological system is a set of interdependent components that are delimited by a 
system boundary. The system objective is achieved by an interaction of the compo-
nents and well defined input-output relations. Weiss and Wettengl define systems 
innovations as the implementation of a system of compatible and complementary 
components and their commercialization by an industrial consortium [1]. Systems 
innovations consist of product and process innovations and initiate a change from one 
socio-economic system to another [2]. 
An example is the introduction of electric vehicles, also known as electromobility. 
This systems innovation is based on product innovations like new batteries as well as 
process innovations like charging management or smartphone based energy payment. 
All sub-components are strongly interdependent as for example the value of an elec-
tric vehicle is limited if compatible charging stations are not available. An electric car 
only pitches its value added, in comparison to conventional cars, if all necessary  
sub-systems like charging infrastructure, power plants, energy storages, cars and regu-
latory framework are compatible and complementary.  
2.2 
Electric Bus System Innovation 
Electrical bus systems meet rising emission reduction targets, because they do not 
produce any local CO2, CO, NOx or particle emissions. Furthermore their noise  

 
Technology Roadmapping Based on Key Performance Indicators 
379 
emissions are quite low. Electric trolley bus systems have been around for many dec-
ades but the installation of overhead wires is not possible or desirable in a lot of cities 
and furthermore the trolley concept doesn’t allow flexible alternative routing. Storing 
the energy for the operation of a complete day will reduce the payload (number of 
passengers) due to extensive weight of the very large batteries. Hence a systems inno-
vation - roadside charging - is needed which comprises several new sub-systems  
(figure 1). 
Electric busses can only replace conventional diesel buses, if all its components 
have a sufficient technology readiness level (TRL) and it is crucial for OEMs and 
suppliers to have sound information about the TRL landscape. If for example the 
battery technology isn’t ready, it’s useless to bring an inductive charging system on 
the market.  
This paper presents an approach to forecast future technology readiness as well as 
ecological and financial performance of a new electric bus system by applying tech-
nology roadmapping. 
 
 
Fig. 1. New urban public bus transportation system [3] 
2.3 
Technology Roadmapping 
Typically technology roadmapping is based on a qualitative methodology and identi-
fies the relationship between technology and time. Recently quantitative techniques 
have been introduced into technology roadmapping as well. 
Qualitative techniques are often based on structured workshops where a group of 
ex-perts identifies the relevant elements of the TRM. The contents are derived in a 
subjective form from the know-how and expertise of experts. A good example of such 
an approach is the T-Plan method [4]. 
The quantitative approach is based on objective data and mathematical algorithms 
rather than subjective judgments. However, data generation and data processing are 
not free from errors due to misinterpretations and false estimates. 

380 
F. Spangenberg and D. Göhlich 
Hence an approach, which combines the positive aspects of both approaches, is fa-
vorable [5-6]. In this Paper a combined simulation based method, customized for the 
challenges of systems innovations like the electric bus, is presented.    
3 
Simulation Based TRM 
3.1 
Methodology 
Simulation based TRM allows forecasting the future system behavior and provides 
indications about future requirements for technological solutions. Based on the find-
ings, the right time for a market entry can be estimated, which provides a base to plan 
the product development processes. 
By modeling the system with its elements, relationships and boundaries in a system 
model [7], the structure of a system becomes more transparent and manageable. It 
allows finding interdependences, sensitivities and possible threshold values. A major 
advantage over classic workshop based approaches is a higher degree of objectivity 
and reproducibility of the results. 
The structure of the technological system defines the stochastic model of the simu-
lation (as shown in figure 2). Hence the systems structure needed for a simulation 
based TRM depends on the elements of the system, their relationship and input/output 
relationships. System engineering provides a set of tools that support an analysis of 
the system structure. One exemplary tool is the design structure matrix method, which 
provides a methodology for the structuring of complex systems into interrelated sub-
systems and components [8].   
 
Fig. 2. Key Performance Indicator System 

 
Technology Roadmapping Based on Key Performance Indicators 
381 
For the electric bus system three KPIs are evaluated. The first performance indica-
tor measures the LCC as an economic objective. The second indicator is the SEF, 
which measures CO2-emissions per output unit as an environmental objective. The 
third indicator is the system technology readiness level (STRL), which is an indicator 
for the maturity of technologies and possible technical problems associated with sys-
tem integration. 
Based on the system structure a deterministic model for the Monte Carlo simula-
tion can be defined. Input parameters are stochastically modeled variables which are 
driven by out-side influences.  Based on stochastic input data the Monte Carlo simu-
lation iteratively computes the stochastic distribution of the KPIs. 
For the modeling of the input parameters, the project evaluated and review tech-
nique (PERT) [9] is used. PERT has its origin in the military network planning. This 
technique is widely used to plan projects under uncertainty which is normally the case 
in R&D programs. As an input for the PERT distribution estimations of the optimistic 
(o), most likely (m) and pessimistic (p) value of an input parameter are carried out. 
With these three parameters and the factor k, which represents the weight of the most 
likely scenario, a beta-distribution is derived [10]. 
The expected mean value of the Beta-PERT distribution can be easily obtained 
from: 
 
ߤ=
௢ା௞௠ା௣
௞ାଶ
   
(1) 
With the standard deviation of a Beta-PERT distribution:  
 
 
ߪ=
௣ି௢
௞ାଶ  
(2) 
Input data can be obtained from internal or external studies, as well as subjective ex-
pert estimations. Specialized internal or external experts with specific knowledge in a 
particular field (e. g. battery technology) can estimate certain input parameters even if 
they do not have a grasp of the entire system. This is an important enabler for an ef-
fective information search. 
For every input parameter specific values for the variables a, o and p are estimated. 
The focus on only three values makes the methodology manageable and convenient 
for the application in industry practice. If the experts involved can not agree about the 
input values the arithmetic average of the expert’s estimations is taken. 
The readiness of components technologies is an important indicator, for the risk as-
sociated with the development of a technological system. Therefore the STRL should 
be one of the three KPIs. 
The STRL methodology is based on the TRL scheme, developed by the National 
Aeronautics and Space Administration (NASA) which is based on a 1-9 scale. TRL 1 
represents the level of basic research whereas TRL 9 stands for field tested (figure 3). 
 
 
 

382 
F. Spangenberg and D. Göhlich 
TRL 9 
Actual system flight proven through successful mission opera-
tions    
TRL 8 
Actual System completed and flight qualified through test and 
demonstration (ground and flight) 
TRL 7 
System prototype demonstration in a space environment 
TRL 6 
System/subsystem model or prototype demonstration in a rele-
vant environment (ground or space) 
TRL 5 
Component and/or breadboard validation in relevant environ-
ment 
TRL 4 
Component and/or breadboard validation in laboratory envi-
ronment 
TRL 3 
Analytical and experimental critical function and/or characte-
ristic proof-of-concept 
TRL 2 
Technology concept and/or application formulated 
TRL 1 
Basic principles observed and reported 
Fig. 3. TRL levels proposed by NASA [11] 
The estimation of a TRL at the system level requires more than just single TRLs. 
The criticality of the various components must also be considered. Therefore it’s pro-
posed to weight the TRLs according to its criticality if they are aggregated to a system 
technology level. The criticality value of the components should be measured as pro-
posed by Hicks and Larsson (see figure 4).      
 
Criticality Value 
Definition 
3 
Technology is ‘enabling’ to the core functionality of the prod-
uct. No work-around or substitution possible. 
2 
Bold Technology fulfills a vital role in the product’s functio-
nality. However, a work-around may be possible using substi-
tute technologies that will incur an acceptable penalty. 
1 
Technology is ‘enhancing’ to the product’s performance, cost 
etc. Several alternative technologies exist that could be substi-
tuted and incur a minimal penalty. 
Fig. 4. Definition of the criticality value according to Hicks and Larsson [12]  
The STRL value is computed from:  
 
ܴܵܶܮ=
∑ሺ்ோ௅௫஼௥௜௧௜௖௔௟௜௧௬ሻ
∑ሺ஼௥௜௧௜௖௔௟௜௧௬ሻ
  
(3) 
In this paper the concept of Hicks and Larsson is extended to a multi-layer concept, 
which also considers subsystems. The criticality of a superior system should be de-
fined as the arithmetic average of it included subsystems respectively components.  
The deterministic model for the STRL calculation in the case of an electric bus sys-
tem is shown in figure 5. 

 
Technology Roadmapping Based on Key Performance Indicators 
383 
 
Fig. 5. Structure of the system simulation in the case of an electric bus (excerpt) 
The stochastic modeling and simulation of the other KPIs (SEF and LCC) has been 
described in more detail by Spangenberg and Goehlich [13].  
3.2 
Technology Information Management System 
For the Interpretation of the calculated KPI distributions a transparent and managea-
ble method has to be provided. 
All single KPIs like LCC, TRL and SFE [CO2/km] could be weighted and aggre-
gated to one high level KPI, which represents the overall performance of the technol-
ogical system [14]. However this leads to the problem of finding reasonable weights 
for each KPI. Furthermore there is a danger of not recognizing a critical single KPI 
because it is compensated by others. Therefore it is expedient to define specific min-
imum values for each KPI and to monitor the KPIs with respect to mean value and 
standard deviation over time.  
The evaluation is carried out with the following comprehensible traffic light logic: 
A traffic light is “green” if for the new technological system the KPI mean value is 
better and the standard deviation is smaller than the existing system. It’s “yellow” if 
either the mean value or the standard deviation is better. Naturally it’s red, if both 
characteristics are worse [13].  
In the specific case of the TRL KPI the traffic light becomes green, if the mean 
value of the TRL forecast exceeds TRLmin and the p-value lies above a predefined 
threshold (e. g. 0.95). At that point, the technology is ready enough to shift its devel-
opment from scientific institutions like universities to the corporate R&D depart-
ments, which integrates it into the system context.   

384 
F. Spangenberg and D. Göhlich 
The stochastic simulation is executed, based on different input parameters, for dif-
ferent years so that the probability distribution of the system performance over time is 
generated. Figure 6 shows the simulated TRLs of an urban bus system for the years 
2012, 2016 and 2020.   
 
Fig. 6. Exemplary results of the stochastic TRL simulation   
 
Fig. 7. “Technology Information Management System” (TIMS) as an decision tool 
When the TRM is constructed, all three KPIs have to be considered. Therefore a 
Technology Information Management System (TIMS) like in figure 7 can give sub-
stantial support. This TIMS displays the traffic light forecast for every KPI over time. 
If all traffic lights are green, the new system is favorable in contrast to the consisting 
system, so that it makes sense to switch to the new technology.  
 

 
Technology Roadmapping Based on Key Performance Indicators 
385 
From that point of time the R&D roadmap is planned backwards. The project kick-
off for the separate subsystem development projects is planed top down from the time 
to market point, based on the estimated project time and the interdependencies be-
tween the projects. In the example in figure 7 the R&D project for subsystem SS2 
starts in 2012 whereas project SS1 in 2014.   
4 
Conclusions 
The new TRM method presented in this paper could successfully be applied to an 
electric bus system innovation. The concept of stochastically modeled KPIs allows a 
forecast of the expected system performance and takes uncertainties of the system 
parameters into account. Through the use of a consistently structured model and iden-
tical input parameters for the simulation of the different KPIs valid results are en-
sured. The proposed technology information management system provides a suitable 
decision making tool. Since TIMS can be easily updated, continuous information 
about the best time to market can be provided. Furthermore this information system 
supports the planning of the different product development processes. 
A bus manufacturer can obtain a suitable TRM for the electromobility R&D strate-
gy while a public transport authority can use the results to plan the introduction of 
electric busses in its fleet. 
The adaption of the presented approach to systems innovations in other industries 
is currently under investigation. 
References 
1. Weiss, E., Wettengl, S.: Abwarteblockaden bei technologischen Systeminnovationen. In: 
Weiss, E., Pfeiffer, W. (eds.) Innovative Unternehmensführung, Forschungs- und Bera-
tungsgruppe für Innovative Unternehmensführung, Nürnberg (1998) 
2. Geels, F.W.: Technological transitions and system innovations: A co-evolutionary and so-
cio-technical analysis. Elgar, Cheltenham (2005) 
3. Bombardier: Primove Bus: Elektrisch unterwegs (July 2012), 
http://primove.bombardier.com/de/anwendungen/busse/ 
4. Phaal, R., Farrukh, C.J.P., Mitchell, R., Probert, D.R.: Starting-up Roadmapping fast. Re-
search – Technology Management 31(3), 52–58 (2003) 
5. Kostoff, R.N., Schaller, R.R.: Science and technology roadmaps. IEEE Transactions on 
Engineering Management 48(2), 132–143 (2001) 
6. Ma, T., Liu, S., Nakamori, Y.: Roadmapping as a way of knowledge management for sup-
porting scientific research in academia. Systems Research and Behavioral Science 23(6), 
743–755 (2006) 
7. Ropohl, G.: Allgemeine Technologie: Eine Systemtheorie der Technik, vol. 3. Univ.-
Verlag Karlsruhe, Karlsruhe (2009) 
8. Lindemann, U., Maurer, M., Braun, T.: Structural Complexity Management: An Approach 
for the Field of Product Design. Springer, Heidelberg (2009) 
9. Wiest, J.D., Levy, F.K.: A management guide to PERT/CPM: with GERT/PDM/DCPM 
and other networks. Prentice-Hall, Englewood Cliffs (1969) 

386 
F. Spangenberg and D. Göhlich 
10. Moder, J.J., Phillips, C.R.: Project Management with CPM and PERT. Reinhold Industrial 
Engineering and Management Sciences. Textbook Series. Reinhold Publishing Corpora-
tion, New York (1964) 
11. Mankins, J.C.: Technology readiness and risk assessments: A new approach. Acta Astro-
nautica 65(9–10), 1208–1215 (2009) 
12. Hicks, B., Larsson, A., Culley, S., Larsson, T.: A Methodology for Evaluating Technology 
Readiness During Product Development. In: International Conference on Engineering De-
sign, ICED 2009, Stanford, pp. 157–168 (2009) 
13. Spangenberg, F., Goehlich, D.: Unterstützung des Technology Roadmapping bei Syste-
minnovationen durch stochastische Simulation. In: 8. Symposium für Vorausschau und 
Technologieplanung, Berlin (2012) 
14. Reinhart, G., Schindler, S., Krebs, P.: Bewertung von Produktionstechnologien aus strate-
gischer Sicht. In: 7. Symposium für Vorausschau und Technologieplanung, Berlin (2011) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 387–398. 
DOI: 10.1007/978-3-642-30817-8_38 
© Springer-Verlag Berlin Heidelberg 2013 
 
Technical Risk Management for an Ensured  
and Efficient Product Development on the Example  
of Medical Engineering 
Thomas Zentis1 and Robert Schmitt2 
1 Fraunhofer Institute for Production Technology IPT,  
Steinbachstr. 17, 52074 Aachen, Germany 
2 Laboratory for Machine Tools and Production Engineering (WZL) RWTH Aachen University, 
Fraunhofer Institute for Production Technology IPT,  
Steinbachstr. 17, 52074 Aachen, Germany 
{thomas.zentis,robert.schmitt}@ipt.fraunhofer.de 
Abstract. Nowadays, industry and trading companies are facing a lot of risks. 
These risks occur in a variety of different fields such as in the corporate strate-
gy, the development of new products or in the introduction of new business and 
production processes. Due to external influences in particular technical risks are 
increasing, e. g. by regulations, increasing price and quality competition or 
shorter product life cycles, reasoned by innovative products. Therefore, an ef-
fective and efficient technical risk management system is crucial, to keep com-
petitive advantages. A survey of the FRAUNHOFER INSTITUTE FOR PRODUCTION 
TECHNOLOGY IPT, regarding technical risk management (TRM) in manufactur-
ing companies, confirms the described importance, because 75% of the compa-
nies assign TRM a very high importance within the product and process  
development. Against the background of shown challenges, the FRAUNHOFER 
IPT developed an approach to make the assessment and control of technical 
risks more efficient.  
Keywords: Risk management, risk assessment, risk control, quality management. 
1 
Introduction 
In a survey conducted by the FRAUNHOFER INSTITUTE FOR PRODUCTION TECHNOLOGY 
in 2011, 180 manufacturing companies were asked what methods they use for as-
sessment of technical risks and what challenges they face by assessing and controlling 
technical risks. The results show that the Failure Mode and Effects Analysis (FMEA) 
with 60.3% is the most utilized method. Also highly widespread is risk assessment in 
workshops and team meetings (52.5%). The methods DRBFM (9.5%) and FTA 
(7.3%) are less utilized within risk analysis. 
When asked about the biggest challenges within analysis and evaluation of risks 
(see Figure 1), the lack of clarity and precision of the methods becomes apparent. One 
 

388 
T. Zentis and R. Schmitt 
reason for this is, for example, that often subjective assessments do not lead to a 
common risk definition and do not deliver enough objective bases of valuation by 
appropriate scales. The reason for the lack of quantification is reasoned by difficult 
surveyable data and therefore only partially applicable statistical models. An imme-
diately following problem is the dissatisfaction of employees by conducting risk  
analysis and assessments, because the results are not clear enough (for example same 
values of Risk Priority Number RPN). In addition, employees are frustrated by the 
lack of implementation or tracking of measures to treat risks. Another reason for the 
lack of precision may lie in the universal character of the methods. The methods are 
not individual enough to specific industries and products, for example in medical 
engineering, where risks for the patient have to be evaluated. So individualized con-
cepts that regard organization, corporate culture as well as deployable resources of a 
company are required.  
 
Fig. 1. Challenges related to risk analysis and assessment 
The sustained success of risk management is possible only through the control of 
risks. Therefore it is important to assess the effectiveness and to draw conclusions 
from the risk assessment for future projects. The high costs (time, money, personnel) 
are criticized as a challenge related to the control of risks. From the fact that 38.6% of 
the enterprises consider the benefits of risk control as too low, can be concluded that 
the potential of risk management is not yet fully recognized and exploited. Concern-
ing the acceptance of risk management 33.5% of the participants of the survey fear 
personal consequences in the analysis and assessment of risks. 
The following paper treats this problem by introducing an integrated model for as-
sessment and control of technical risks. Therefore existing approaches for risk as-
sessment and control as well as their central deficits are illustrated. On this basis a 
method for technical risk assessment and a model for technical risk control are de-
scribed. The paper closes with the illustration of the model validation results in the 
medical industry. 
The bases of valuation leave too much
space for interpretation
The methods of risk assessment and
analysis are to expensive
The methods of risk assessment are not precise
enough, to determine the primary risks
The methods of risk analysis are not suitable
for determining the cause
The methods of risk analysis are not suitable
for determining the cause
Other
0%
20%
40%
60%
80%
100%

 
Technical Risk Management for an Ensured and Efficient Product Development 
389 
2 
Current Approaches to Assess and Control Technical Risks 
2.1 
Methods to Assess Technical Risks 
As the results of the survey show, FMEA is the most utilized method. Amongst others 
the reason for this are different normative requirements (e. g. ISO 14971 in medical 
engineering or ISO TS 16949 in automotive sector etc.). Along the product develop-
ment process different tools supporting risk management exist. The methods are in-
troduced in the following. 
FMEA. The goal of a Failure Mode and Effects Analysis FMEA is the analysis of 
products and processes during the early stages of development concerning potential 
failures and initiating measures for failure prevention through an integrated risk anal-
ysis [1-6]. In effect, an FMEA reduces development time and costs, while increasing 
the quality by avoiding product defects [3]. Therefore FMEA is a valuable tool for 
risk management [6]. The most common arguments against FMEA are the high costs 
of implementation, the influence of subjective perceptions and the difficult interpreta-
tion of the risk priority number RPN that is not to be considered as an absolute degree 
for risk [5-7]. The monetary quantification of risk on the basis of the risk priority 
number is not possible either. Thus, the sole application of FMEA does not lead to an 
objective quantification of risk and solutions to eliminate the causes of failures, but it 
allows a systematic and structured collection of explicit and implicit knowledge about 
possible failures [6, 8-9].  
QRC. The QuickRiskCheck QRC is a simple method for the identification of critical 
process risks. It was developed at the FRAUNHOFER IPT. In the first step a process is 
mapped, divided into process steps and the main risks of these steps – in form of fail-
ures occurred in the past – are identified. Afterwards the most risky process steps are 
identified through pairwise comparison. The risks of these process steps are analyzed 
in detail. The occurrence probability and effect of every identified risk is aggregated 
and assessed by a 3-stage scale. In the following, measures are defined and their ef-
fect on every single risk is evaluated on a 4-stage scale. Through this evaluation of the 
aggregated benefit of the measure, a ranking of all measures is developed. Further-
more the possibility of determination of the costs of each measure exists. [10-13] 
FTA. The Failure Tree Analysis FTA is a method for systematic identification of 
failures in complex and safety related systems [14-16]. The starting point is an analy-
sis of the technical system under consideration (product / manufacturing process) for 
which an undesirable event is to be defined and a failure tree will be developed. For 
this failure all possible combinations of malfunctions are identified and displayed in a 
tree structure. [14-15] As all single causes of a failure respectively damage have to be 
identified, the costs rise over proportional to the complexity of the system, so that the 
construction of the failure tree is very challenging [16-17].  
ETA. The Event Tree Analysis ETA was first used in the atomic energy field and 
gradually extended to other fields such as chemical engineering and mechanical engi-
neering [18]. ETA is an inductive logic and diagrammatic method for identifying the 

390 
T. Zentis and R. Schmitt 
various possible outcomes of a given initiating event such as the malfunctioning of a 
system, process or construction [19]. This bottom-up method leads to a tree consisting 
of an initiating event, probable subsequent events and final results caused by the se-
quence of events. Probable subsequent events are independent to each other and the 
specific final result depends only on the initiating event and the subsequent events 
following [20]. In this way failure initiation and escalation are represented by a series 
of trees based on binary logic trees [21-22].  
DRBFM. A method that has a very close connection to FMEA, as it was derived in 
large parts from her, is the Design Review Based on Failure Mode DRBFM. Both 
methods – DRBFM and FMEA – identify and eliminate product and process inherent 
risks systematically. While FMEA analyzes potential risks of an entire product or 
process, the DRBFM concentrates on changes to existing developments and designs. 
The DRBFM emerged from the realization that changes contain the highest potential 
for failure and are often implemented without a structured examination of the influ-
ences on functions. The goal of DRBFM is to identify potential risks in the develop-
ment that are aroused by changes on existing products through systematic procedure 
and to minimize these risks through changes within the product design. [23-25] 
2.2 
Methods to Control Technical Risks 
Risk control as the last phase of the risk management process is a continuous task that 
concentrates on monitoring the risk situation in the organization [26-28]. The function 
of risk control is not limited only to monitoring alone, but extends to the review and 
improvement of existing approaches as lessons learned are fed back into the risk man-
agement system [28-29]. 
With the fulfillment of the tasks of risk control the risk management process is a 
repetitive closed loop (see figure 2). The information feedback from the phase after 
development and production allows a steady examination of the current relative to the 
desired risk position and the derivation of appropriate measures in the preliminary 
phases of the risk management process. [29-31]  
 
Fig. 2. Tasks of risk control in the risk management system 
risk 
identification 
risk 
analysis 
risk 
treatment 
risk control: 
continuous monitoring of risk situation 
identification 
of new risks 
effectivity of 
measures 
objectification 
of assessment 
risk strategy 
field data 
products 
products 

 
Technical Risk Management for an Ensured and Efficient Product Development 
391 
Flawless Startup. A method for continuous monitoring and controlling risks, devel-
oped at FRAUNHOFER IPT in cooperation with MAN DIESEL&TURBO SE is Flaw-
less Startup. The methods defines a structure in which all risks and problems (so 
called Flaws), for example in a design project, are collected and structured, including 
all existing information. Within the systematic all necessary information is requested 
and the necessary measures for treatment determined. Through continuous review-
meetings, the status of every flaw is monitored and problems within treatment are 
documented. The use of Flawless Startup provides a learning effect for new design 
projects. [32-33] 
2.3 
Central Deficits and Need for Action 
Existing approaches for risk assessment only provide a qualitative rating, for example 
by using risk portfolios. These methods provide an insufficient image of the probabili-
ty of occurrence and monetary consequences of risks. Reasons for this are the diffi-
culty of predicting events without historic data and the combination of a risk with its 
financial consequences because of the delay of some consequences, for example war-
ranty claims or loss of image. Reasoned by the lack of methods for quantification the 
value of risk management is often considered too low, as the consequences of risks 
cannot be described and quantified on a monetary scale while decisions are based on 
costs and benefits in many industries. Another deficit is the simulation of probability 
of occurrence and monetary consequences of risks, as the existing models of simula-
tion and approaches are too complicated or necessary data can only be received with 
high costs. 
Risk control sustainably ensures the success of technical risk management. The 
most existing approaches only provide general process models without practical help. 
With the feedback of data relevant to risk management from field data, the risk man-
agement process of an organization becomes a repeating closed loop. Along this 
closed loop exists a deficit on practice-oriented methods. 
3 
Methodical Approach  
In order to amend the existing methods of risk assessment and control the 
FRAUNHOFER IPT developed a model for technical risk assessment as well as a 
process model for risk control.  
Goal of the method and the model is the elimination of deficits within the assess-
ment of interdependencies in risk consequences and the efficient control of technical 
risks. The control of technical risks completes the closed loop of risk management 
that contains the process phases identification, analysis and assessment and control. 
This way the overall gain of knowledge in form of a validation of risk assessments is 
realized. The module of risk inventory and its use in medical industry is illustrated in 
extracts. The application shows how relevant data can be collected and used in future 
activities of risk management. Thus, the benefit of risk management is increased sus-
tainably – in the opinion of the companies surveyed. 

392 
T. Zentis and R. Schmitt 
In the following the method for assessment of the interdependencies in risk conse-
quences is described at first. Afterwards the model for control of technical risks, 
which completes the closed loop of risk management, is illustrated. In this closed loop 
the method of risk assessment introduced before as well as the methods of controlling, 
e. g. the risk inventory are located. 
3.1 
Method for Technical Risk Assessment 
As basis for the method, an event tree is developed in the first step. In the style of a 
failure tree, used to determine the causes of a risk, the event tree systematically identi-
fies potential risk consequences. The goal is the transparent illustration of all risk 
consequences, which then will be assessed qualitatively and quantitatively. The selec-
tion of the risk to be assessed is based on the knowledge about its consequences, so 
that expert’s opinions are often consulted. The effort of the methods varies with the 
possible risk consequences. In order to simplify the method and reduce the effort, 
assessments can be standardized based on the results of former detailed assessments.  
The goal of the qualitative assessment is in particular the illustration and evaluation 
of interaction between risk consequences. For this, the RISK STRUCTURE MATRIX 
RSM is developed that identifies risk consequences with the highest potential nega-
tive influence on other risk consequences and for this reason should be avoided. The 
developed RSM can be used for risk assessment in product and process development 
as well as in production. 
 
Development of Potential Risk Consequences by Event Trees. Goal of the devel-
opment and assessment of potential risk consequences is, to systematically determine 
the most critical consequences of a risk. Critical means that the consequence with the 
highest expected damage and probability of occurrence become transparent to allow a 
decision concerning the treatment of the risk. For the identification of risk conse-
quences, existing approaches of visualizing risk consequences, in form of event tree 
analysis, are used. [34] 
The choice which risk has to be assessed is reasoned on the basis of risk analysis 
executed in the process step before. For the chosen risk an event tree is designed with 
the help of methods mentioned before under the use of the Boolean algebra. The illu-
stration is done as detail-oriented as possible, i. e. risk consequences are divided into 
their smallest logical sequences. Figure 3 shows the schematic illustration of an event 
tree and the risk consequences contained. After the tree structure is finalized, the 
probabilities and monetary consequences of the single branches have to be deter-
mined. In contrast to the FTA, the probability of detection of the potential failure and 
the predicted monetary damage is assigned to the branches besides the probability of 
occurrence. These key figures match the RPN of FMEA and can be used in a subse-
quent detailed analysis of ETA. In case of unknown consequences this can be the 
initiating point for the execution of further analysis, for example in form of Design of 
Experiments DoE. 

 
Technical Risk Management for an Ensured and Efficient Product Development 
393 
 
Fig. 3. Schematic illustration of an Event Tree 
Determination of the Critical Path through Minimal Cut Sets. After the tree struc-
ture has been developed and the necessary input data has been determined, the analy-
sis can be started by determination of the minimal cut sets of the tree. A minimal cut 
set contains the minimum amount of risk consequences that result from the assessed 
risk. The goal is the identification of the critical path or the minimal cut set that con-
tains the risk consequence with the highest probability occurrence or damage, or the 
lowest probability of detection. Cut sets can be derived from the structure of the Event 
Tree in which the logic element “and” can be interpreted as conjunction and the logic 
element “or” as disjunction. [35] 
On the basis of the derived cut sets, risk consequences that are most likely to occur 
and least likely to be detected can be derived. These consequences represent the most 
critical consequences and have to be assessed on basis of existing risk tolerance inter-
vals with the highest priority. The total probability of occurrence of a cut set is deter-
mined through multiplication of the probabilities of each contained risk consequence. 
The resulting probability of the assessed risk is determined trough addition of the 
probabilities of all minimal cut sets. 
 
Risk Structure Matrix for Assessment of Interactions between Risk Consequences 
The created Event Trees, including the determined minimal cut sets can be input of a 
systematic approach for transparent visualization and assessment of interaction be-
tween risks. Starting point are cut sets, that can be transferred in the so called Risk 
Structure Matrix RSM. The goal of the RSM is to assess risk consequences within the 
cut sets, concerning their interaction on risk consequences in other cut sets, i. e. in-
crease or diminish the risk or have no effect. 
The RSM is a continuative method to the ETA and the quantitative assessment. It 
allows the qualitative assessment within risk consequences to identify the greatest risk 
drivers. On the basis of the results a direct risk treatment of the critical risk conse-
quences is possible. The results of the method are integrated into the risk inventory 
and serve the continuous risk assessment to its final elimination.  
assessed risk 
RC 1 
RC 2 
RC 3 
RC 4 
RC 5 
RC 6 
RC 7 
RC 8 
≥1 
& 
≥1 
≥1 
& 
AND-link 
≥1 
OR-link 
risk consequence 
RC  

394 
T. Zentis and R. Schmitt 
3.2 
Concept of a Process Model for Risk Control 
The following chapter contains the introduction of a process model for risk control. 
The process model can be divided into an event-driven part, with relation to the col-
lection of field data and a part for continuous review of the risk situation within  
the organization. The model completes the aforementioned closed loop of risk  
management. 
In the event-driven part field data are analyzed regarding potential risks. The iden-
tification of risks after receiving field data is necessary as single risks can demand  
immediate action because of their possible damage. Furthermore data are analyzed 
regarding to content and formal aspects, to ensure a high quality of information. Cen-
tral challenge is the focus on data that pictures product or process risks adequately. 
Result of this process step is the determination what data has to be gathered in which 
way. As risks that have not been identified are not considered in the further progress, 
this step is highly relevant for the complete visualization of the organization’s risk 
situation. 
 
Fig. 4. Concept of the process model for risk management 
The sequence of the event-driven part of the process model is based on the first 
steps of the 8D-method that serves for systematic analysis and correction of problems 
and is e. g. frequently used within the execution of reclamations by manufacturers in 
the automotive sector [5]. Besides identifying the problem, immediate measures are 
executed, to ensure that the customer is not further affected by the problem. Further-
more failure causes are determined and measures are initiated to fix the problem. In 
the process model all incoming product risks are not treated in this complex way. In 
fact a decision based on the assessment of possible consequences of the single risks to 
the customer is made, whether immediate measures are necessary. 
 
immediate measures 
field data collection 
risk inventory 
risk aggregation 
product 
failure 
risk 
situation 
aggregated  
risk information 
risk analysis 
risk  
control 
FMEA 
risk 
treatment 
concept/ 
design phase 
DRBFM , FTA, ETA… 
initial 
assessment 
field 
data 
event-driven 
continuous 
review 

 
Technical Risk Management for an Ensured and Efficient Product Development 
395 
In the second part of the process model the identified single risks are aggregated 
for continuous review of the organization risk situation. The aggregated risks picture 
the risk situation of a product in field, to initiate risk treatment measures, whenever 
the situation deviate from the goals defined in risk strategy. With the initiation of 
measures the loop is closed, as gained knowledge from the field data is fed back into 
the product development process. Central instrument of risk control is the risk inven-
tory in which all risks occurred in the field and each result of the activities along the 
risk management process is entered. The data structure enables the aggregation of 
single risks and the address-related supply of information relevant to risk manage-
ment. Innerhalb des Modelles kann die zuvor präsentierte Methode der RSM 
innerhalb der Risikoanalyse verortet werden. Hier kommen ebenso Methoden wie die 
FMEA oder die FTA zum Einsatz. 
 
Module Risk Control. In the module of risk control the actual risk situation pictured 
in the risk inventory is compared to the defined targets in risk strategy. The module 
serves as an actuator that initiates more detailed risk analysis when deviations occur. 
The cause analysis for risks occurred in field is executed in the following module 
of risk analysis and takes place using existing methods from product development 
(e. g. FMEA, DRBFM, FTA or ETA). On the one hand impulses for the execution of 
risk analysis come from the module of risk control. On the other hand risk analysis 
are initiated after the development of a new product concept. In the following the 
module risk inventory is introduced more detailed. 
 
Module Risk Inventory. Task of the risk inventory is the structured collection of all 
risks occurred regarding a product. In addition to every product risk in the field, the 
risk inventory must allow the registration of additional information, like initial as-
sessments of the risk in the product development, taken measures or responsibilities. 
As the initial point of information relevant to risk management is the product itself, 
the representation of single risks is done in accordance to the product structure – as it 
is usual in methods of technical risk management like e. g. FMEA. Every product can 
be divided into assemblies, which can further be divided into single parts. The smal-
lest unit of a product is a part that cannot be further deconstructed respectively is not 
further deconstructed by reason of defined criteria. 
 
Fig. 5. General structure of risk inventory 
part-related risks
interaction risks
use-related risks
assembly 1.1
product 1
assembly 1.2
part 1.3
part 1.2.1
part 1.2.2

396 
T. Zentis and R. Schmitt 
Three risk categories can be distinguished. On the one hand risks that can be as-
signed to a part, as for example the mechanical breakdown of the part due to construc-
tion failures or failures in manufacturing. Furthermore risks can origin from the  
interaction of different components (parts or assemblies), for example by the incom-
patibility of several components. The third risk category describes risks that occur 
within the use of the product, for example in human-machine-interaction or through 
environmental effects. 
The collection of product risks in accordance to the product structure has several 
advantages. On the one hand the gathering is done in accordance to the existing and 
supporting methods of risk management. On the other hand this structure allows the 
aggregation of risks.  
 
Practical Application in Medical Industry. The validation of the module risk inven-
tory was done based on interviews with the goal to collect the existing approaches in 
practice of medical engineering more detailed. By this the designed structure of the 
model can be compared to practical approaches and improvement potentials can be 
identified and implemented. 
In the interviews the imprecise definition of parts was criticized, as it is not clear, 
how to handle purchased assemblies and how the term is associated with the term 
component. The application of a level-model consisting of several sub-components or 
systems was seen as potential for improvement. In reference to bought-in parts it was 
observed that often the correctness of the manufactures data regarding the failure 
probability is not further questioned or analyzed. Knowledge of part-related risks is 
hence hardly possible. Only in rare cases the analysis of interaction between assem-
blies or parts can be used to gain data regarding risks of single parts. 
Another note was that the sequential procedure within risk assessment and with it 
the structure of the inventory for a new product should be amended with an iterative 
process. Within a practice example it was shown that first part- and assembly-related 
risks along the production process are analyzed, before in the next steps also risks of 
the human-machine-interface are considered. The results of the use-oriented FMEA 
are finally implemented in the development process. This process is executed until the 
remaining risk is on an acceptable level. The existing model has to be amended with a 
back loop depending on the scenario of application. 
Overall, the method and model have been seen as facilitative tools within the con-
trol and assessment of technical risks. The reasonable extent of use of the method is 
still to be tested. 
Acknowledgements. Support of the Federal Ministry of Economics and Technology 
(BMWi) as part of a program promoting “Industrial Joint Research (IGF)” within the 
framework of the Confederation of Industrial Research Associations (AiF) and super-
vision by the Federation for Quality Research and Science (FQS). 

 
Technical Risk Management for an Ensured and Efficient Product Development 
397 
References 
1. Hartung, S.: Methoden des Qualitätsmanagements für die Produktplanung und -
entwicklung. Dissertation RWTH Aachen. Shaker, Aachen (1994) 
2. Rhee, S.J., Kosuke, I.: Using cost based FMEA to enhance reliability and serviceability. 
Advanced Engineering Informatics 17, 179–188 (2003) 
3. Schloske, A.: Risikomanagement mit der FMEA. In: Kamsike, G.F. (pub.) Qualitätstech-
niken für Ingenieure. 2. Aufl., pp. 285–328. Symposion, Düsseldorf (2009) 
4. Schmitt, R., Hense, K.: Wissensbasierte FMEA. Symposion, Düsseldorf (2005) 
5. Schmitt, R., Pfeifer, T.: Qualitätsmanagement: Strategien, Methoden, Techniken. Hanser, 
München (2010) 
6. Schuler, W.: FMEA – Ein Instrument des Risikomanagements. Oder: Wie könnte ein 
Manager FMEA sehen und benutzen? QZ Qualität und Zuverlässigkeit 35(8), 444–448 
(1990) 
7. Haffner, T.: Ein Modell zur Bestimmung der monetären Einsparungspotenziale bei der 
Durchführung einer Fehlermöglichkeits- und Einflussanalyse (FMEA). Dissertation, Uni 
Stuttgart, Stuttgart (2005) 
8. Kistner, W.: FMEA noch besser anwenden. QZ Qualität und Zuverlässigkeit 41(7), 827–
830 (1996) 
9. Westkämper, E.: TÜV Rheinland Köln - Integrationspfad Qualität: Leitfaden zum Erfolg. 
Springer, Berlin (1991) 
10. Zentis, T., Schmitt, R.: Risk minimized procurement in low wage countries. In: 22nd CIRP 
Design Conference Proceedings, Bangalore (2012)  
11. Schmitt, R., Kukolja, J., Zentis, T.: Risikominimierte Beschaffung in Niedriglohnländern. 
Forschungsgemeinschaft Qualität FQS, DGQ Band 88-05, Frankfurt a. M. (2010) 
12. Schmitt, R., Kukolja, J., Zentis, T.: Effizient und abgesichert – Risiko- und Kostenmini-
mierung bei internationaler Beschaffung. QZ – Qualität und Zuverlässigkeit 55(4) (2010) 
13. Schmitt, R., Zentis, T.: Risikominimierte Beschaffung in Niedriglohnländern. Productivity 
Management 1(1) (2010) 
14. DIN 25424-2: Fehlerbaumanalyse; Handrechenverfahren zur Auswertung eines Fehler-
baumes. Issue 1990-4. Beuth, Berlin (1990) 
15. Gleißner, W., Romeike, F.: Risikomanagement – Umsetzung, Werkzeuge, Risikobewer-
tung. Haufe Mediengruppe, München (2005) 
16. Schmitt, R., Betzold, M., Hense, K.: Das Aachener Qualitätsmanagementmodell. In: 
Schmitt, R., Pfeifer, T. (pub.) Masing - Handbuch Qualitätsmanagement. 5. Aufl., pp. 35–
38. Hanser, München (2007) 
17. Dahmen, J.: Prozessorientiertes Risikomanagement zur Handhabung von Produktrisiken. 
Dissertation RWTH Aachen. Shaker (2002) 
18. Beim, G.K., Hobbs, B.F.: Event tree analysis of lock closure risks. Journal of Water Re-
sources Planning and Management ASCE 123, 137–198 (1997) 
19. Huang, D., Chen, T., Wang, M.J.: A fuzzy set approach for event tree analysis. Fuzzy Sets 
and Systems 118, 153–165 (2001) 
20. Hong, E., Lee, I., Shin, H., Nam, S., Kong, J.: Quantitative risk evaluation based on event 
tree analysis technique: Application to the design of shield TBM. Tunnelling and Under-
ground Space Technology 24, 269–277 (2009) 
21. Jun, C., Chang, S., Hong, Y., Yang, H.: A Bayesian approach to prediction of system fail-
ure rates by criticalities under event trees. Int. J. Production Economics 60-61, 623–628 
(1999) 

398 
T. Zentis and R. Schmitt 
22. Ericson II, C.A.: Hazard Analysis Techniques for System Safety, pp. 223–234. John Wiley 
& Sons (2005) 
23. Werdich, M.: FMEA – Einführung und Moderation. Durch systematische Entwicklung zur 
übersichtlichen Risikominimierung (inkl. Methoden im Umfeld). 1. Aufl. Vie-
weg+Teubner, Wiesbaden (2011) 
24. Gamweger, J., Jöbstl, O., Strohrmann, M., Suchowerskyj, W.: Design for Six Sigma. Kun-
denorientierte Produkte und Prozesse fehlerfrei entwickeln. Hanser, München (2009) 
25. Schorn, M., Kapust, A.: Qualitätsmanagement. DRBFM – die Toyota-Methode. VDI-Z. 
147(7/8), 67–69 (2005)  
26. Brühwiler, B.: Risikomanagement als Führungsaufgabe. ISO 31000 ONR 49000 wirksam 
umsetzen. Haupt, Bern (2011) 
27. Gräf, J.: Risikomanagement: Umsetzung und Integration in das Führungssystem. In: 
Gleich, R., Klein, A. (pub.) Risikomanagement und Risiko- Controlling. Organisation und 
Dokumentation im Unternehmen, Datenerhebung und Risikobewertung, Integration in die 
Führungs- und Reportingsysteme, Umsetzungsbeispiele aus der Praxis (Reihe: Der Con-
trolling-Berater, Bd. 16), pp. 51–74. Haufe, Freiburg (2011) 
28. Altstetter, S.: Bedeutung und Umsetzung einer ganzheitlichen Risikoidentifikation zur 
Etablierung eines effizienten Risikomanagement-Systems in Unternehmen. Diss. Univer-
sität Kassel (2008) 
29. Schorcht, H.: Risikomanagement und Risikocontrolling junger Unternehmen in Wach-
stumsbranchen. Konzeption eines theoriegeleiteten Handlungsrahmens für die praxisindu-
zierte Unternehmenssteuerung. In: Brösel, G., Keuper, F. (pub.) Schriften zum Konver-
genzmanagement (Bd. 1). Logos, Berlin (2004) 
30. Bläsing, J.: Medizinprodukte: Risikomanagement im Lebenszyklusmodell nach DIN EN 
ISO 14971. Beobachtungs- und Meldesysteme. 2. überarb. TQU Verlag, Aufl. Ulm (2008)  
31. Dahmen, J.W.: Prozeßorientiertes Risikomanagement zur Handhabung von Produktrisiken. 
Diss. RWTH Aachen (2002) 
32. Gut, H.: System of Excellence als Beitrag zum nachhaltigen Qualitätsmanagement. In: 
Schmitt, R. (pub.) Produkt- und Prozessqualität Präventiv Absichern und Messbar Ma-
chen. Tagungsband 15. Business Forum Qualität. Apprimus, Aachen (2011) 
33. Schmitt, R., Ottong, A., Gut, H.: Mitigating Technical Risks by Creative Problem Solving 
Approaches. In: Proc. Ann. Reliability & Maintainability Symp. Institute of Electrical & 
Electronics Engineers, IEEE (2011) 
34. DIN 25419: Deutsches Normungsinstitut - Ereignisablaufanalyse - Verfahren, graphische 
Symbole und Auswertung. Beuth Verlag, Berlin (1985) 
35. Thums, A.: Formale Fehlerbaumanalyse. Dissertation Universität Augsburg (2004) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 399–409. 
DOI: 10.1007/978-3-642-30817-8_39 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Simultaneous Engineering Approach for Free Form 
Bifurcated Sheet Metal Products 
Anselm Schüle and Reiner Anderl 
Department of Computer Integrated Design, Technische Universität Darmstadt, 
Darmstadt, Germany 
{schuele,anderl}@dik.tu-darmstadt.de 
Abstract. In this paper a simultaneous engineering approach is introduced to 
cope with the complex and elaborative engineering design process for bifur-
cated sheet metal products. The design process comprises product design, sheet 
blank design, and tool design. Free form surfaces derived from mathematical 
optimization processes are the starting point for product, tool, and sheet blank 
design. Part design is strongly connected and references to the underlying free 
form surface. For the 3D-CAD models a modeling concept is introduced where 
free form surfaces are referenced from an external source. A workflow to man-
age the design process and to deal with possible changes and iterations during 
product development is also developed and presented. 
Keywords: CRC 666, simultaneous engineering, free form modeling. 
1 
Introduction 
The members of the collaborative research center 666 “Integral Sheet Metal Design 
with Higher Order Bifurcations - Development, Production, and Evaluation” research 
and develop tools and methods for the development, production and evaluation of 
sheet metal products with bifurcations. Linear flow splitting and its derivative linear 
bend splitting are the underlying manufacturing techniques that enable the production 
of sheet metal profiles with bifurcations made in integral style. Today, multi-
chambered profiles can be made in an appealing quality. Subsequent deformation of 
sheet metal stringers produced by bend splitting further increases the product range of 
bifurcated sheet metal. The product development process for this kind of products is 
highly integrated and includes the domains of product development, mathematics and 
production. The collaborative research center 666 developed an algorithm-based 
product development process for sheet metal profiles that aims to generate optimized 
solutions from product requirements. To achieve this aim product requirements are 
transformed into optimized product topology and geometry by mathematical optimi-
zation processes. The generated geometry is subsequently transformed into 3D-CAD 
models that build the basis for detail design and manufacturing [1]. For free form 
sheet metal products the algorithm-based approach is reconsidered and adapted to the 
new requirements. The mathematical optimization now begins with a starting geome-
try that is optimized with regard to parameters and objective functions derived from 

400 
A. Schüle and R. Anderl 
product requirements. The target functions for optimization can be for example max-
imum stiffness, minimal weight, or maximized heat transport. The optimization 
process is described in section 3.1 
In this paper a simultaneous engineering approach for development and production 
of this new kind of products is presented. First state of the art and the main ideas of 
simultaneous engineering are presented in section 2. It is also explained why simulta-
neous engineering is necessary and valuable for free form sheet metal products with 
bifurcations especially when a mathematical optimization process is executed. Then a 
brief introduction to linear bend splitting and the forming process is given to identify 
the special requirements to product development processes from bifurcated sheet 
metal products. Section 3 describes the developed concept to enable simultaneous 
engineering for bifurcated sheet metal and the associated development processes. The 
different product models are described and a method to link external geometry to 
CAD models to enable simultaneous engineering is explained. 
2 
Fundamentals of Simultaneous Engineering 
Simultaneous engineering (SE) is a powerful approach to improve the product crea-
tion process. SE describes methods for integrated product and manufacturing process 
development. The major goal of SE is an optimization of product creation with regard 
to the “magic triangle”, i.e. shorter time-to-market, reduction of costs, or an increase 
in quality [18, 19]. The decrease in product development time allows a quick reaction 
to trends and demands on the market which is the most promising and effective way 
for company’s success [18]. 
Although many definitions for SE exist the main characteristics can be identified as 
integration and parallelization of processes. Process parallelization implies that 
processes are started with uncertain or incomplete information. To achieve paralleli-
zation of processes a detailed process analysis is required to identify sub-processes 
with their minimal required information. In a composition phase the sub-processes 
can then be rearranged and reordered to a simultaneous engineering workflow. 
Reconsidering the primary aim of simultaneous engineering, the parallelization of 
processes and a decrease in time-to-market an additional advantage of simultaneous 
engineering becomes obvious: the early accumulation of knowledge by interdiscipli-
nary teams and the high amount of communication between formerly sequential 
processes leads to a broad understanding of the product and the manufacturing con-
straints. Errors and disadvantages can be identified and corrected in an early stage of 
product development. This effect is called “front-loading”. It increases the product 
development process and leads to a decrease in the total product development costs 
but shifts those towards the early phases [19]. 
Applicability of Simultaneous Engineering 
The mentioned advantages and characteristics of simultaneous engineering implies 
that simultaneous engineering is an obvious and generally suitable method for  
 

A Simultaneous Engineering Approach for Free Form Bifurcated Sheet Metal Products 
401 
every product and company. In fact simultaneous engineering requires complete  
restructuring of the company’s processes, workflows and working culture. It is also 
required to be well prepared if the desired product shall be suitable for simultaneous 
engineering. Products with a high volatility in requirements, products that are based 
on new technologies, and products with uncertain success are better designed by se-
quential product development process. 
3 
Product Creation of Bifurcated Sheet Metal 
3.1 
Mathematical Shape Optimization 
The product development approach within the collaborative research center 666  
applies mathematical optimization to generate the product geometry. The shape opti-
mization uses product requirements and manufacturing constraints from product plan-
ning to derive optimized product geometry. The process starts in a conceptual phase 
by collection of product requirements and the creation of a preliminary product shape. 
The shape is a free form surface with defined parameterization for example a NxM 
control point matrix with a defined B-Spline degree. For the mathematical optimiza-
tion process input data from requirements is transformed into objective functions, 
constraints and optimization parameters. A finite element calculation is executed as 
described in [1]. The outputs of this process are new control point coordinates for the 
starting free form surface and the relative position and parameters of the stiffeners, 
i.e. the bend-splitted flanges. The output is used in CAD modeling to automatically 
generate 3D-CAD models from the preliminary product shape. For details on the  
mathematical optimization please refer to Ulbrich et al. [21]. 
3.2 
CAD Modeling 
Most 3D-CAD systems apply solid modeling techniques to generate 3D-CAD models 
[7]. For simplification and standardization of the modeling process feature technology 
is used. A feature is set of geometry and/or information with a certain purpose for 
example manufacturing features, quality features, or assembly features. The most 
commonly used features are form features (e.g. block, cylinder, and cone) and manu-
facturing features (e.g. holes, chamfers, and threads) [8]. To generate product models 
for bifurcated sheet metal products modeling features for linear bend splitting and 
linear flow splitting were developed [9] and [10]. To create flanges on non-linear 
profiles and on free form surfaces a new feature approach had to be developed that 
processes B-Spline based reverences. This feature approach was presented in [11]. 
Within the collaborative research center 666 the created 3D-CAD models are used for 
manufacturing and finite element analysis of the deep-drawing process (see  
section 1.1). 
 

402 
A. Schüle and R. Anderl 
3.3 
Manufacturing Processes  
Flow Splitting and Bend Splitting 
Linear flow splitting and its derivative linear bend splitting (fig. 1) are cold massive 
forming techniques that enable the creation of bifurcated sheet metal products without 
material doubling or joining [2]. Linear flow splitting is an incremental forming 
process. Sheet metal is “split” at the sheet edge by a set of a forming roll and two 
support rollers and formed to a Y-shaped profile. For linear bend splitting the sheet 
metal strip is initially bent by roll forming in a 90 degree angle. Thus a new edge is 
created and the forming is conducted by a splitting roll and a set of support rollers to 
back the sheet metal from buckling. Bend splitting increases the product range of 
bifurcated sheet metal by enabling the creation of flanges at almost any position on 
the sheet surface [3]. Sheet metal stringers made in integral style are profiles with two 
or more flanges. The forming process induces an increased strength in the deforma-
tion zone and in the created flanges [4, 5]. 
 
Fig. 1. Linear Flow Splitting (left) and Linear Bend Splitting (right) [20] 
The term “linear” with flow and bend splitting describes the geometry of the edge 
where deformation takes place. New approaches enable the creation of non-linear 
flow-splitted parts, i.e. profiles with variable cross sections. In this case the sheet edge 
is preliminary curved by high speed milling or laser cutting and then formed by a 
moving tool set [16]. For non-linear bend splitting the edge is curved by flexible roll 
forming. This technique becomes relevant in particular when deep drawn stinger pro-
files ought to have flanges with specific distance and orientation on the product. Dur-
ing the forming process the relative position of the flanges differ slightly on the sheet 
surface because of the unequal material movement into the mold. Thus the orientation 
on the sheet blank must be calculated and manufactured so that it will fit the position-
ing requirements after the deformation process. 
Hydroforming of Bifurcated Sheet Metal 
Bifurcated sheet metal stringers made from bend splitting are well suited as semi-
finished products for hydro-forming. With this forming process the created flanges 
can be formed within certain limits. The working fluid supports the flanges and pre-
vents their deformation. To create sheet blanks from bend-splitted stringer profiles a 
sealing zone is milled. The blank is then inserted into the tool. A hydraulic press pro-
vides the pressure for closing the bottom and top mold and a pump builds up the 

A Simultaneous Engineering Approach for Free Form Bifurcated Sheet Metal Products 
403 
forming pressure from which the sheet blank is deformed and pressed into the tool 
[5]. A schematic picture of the process is shown in fig. 2. The process is very re-
stricted concerning the degree of deformation and possible flange heights i.e. flange 
to sheet thickness ratio. As in any forming process the product underlies a back-spring 
effect, caused by the elastic behavior of the material. It is significantly increased by 
the bend-splitted flanges. The back-spring effect depends on various parameters such 
as geometry, material, the retaining force, the applied tool, and friction. For bifurcated 
sheet metal products a computer aided simulation of back-spring effects is required as 
common assumptions and simplifications are no longer valid [6]. Besides product 
shape the back-spring effect is the most important factor for tool design. To derive 
process parameters and to ensure manufacturability, the process is simulated using 
finite element analysis. Detailed information about the forming process and the para-
meter identification can be found in [4] and [17]. 
 
Fig. 2. Hydroforming of Bifurcated Sheet Metal and Example Product [5] 
4 
Simultaneous Engineering for Bifurcated Sheet Metal 
The introduced advantages and properties of simultaneous engineering do fit well for 
the product creation processes in the collaborative research center 666 where different 
domains collaborate with respect to a single product. In fig. 3 the development 
process for bifurcated sheet metal products is shown. As described in section 1, the 
engineering design process for bifurcated sheet metal is very complex and requires 
different domains to collaborate and to process information. Anyhow, the products 
created within the product range of bifurcated sheet metal are similar in their general 
structure and manner. A general concept can be developed that is valid for the whole 
product range. The differences from one design project to another are the product 
shape, the free form surface, the number and orientation of flanges. But in general the 
structure of each product is quite similar. Product development and manufacturing 
planning are the domains involved in the engineering design process. The main idea 
of this paper is to enable a simultaneous engineering design by external geometry 
linked to CAD models from product design and manufacturing planning. The external 
geometry is easily accessible by the different domains and serves as input for the 

404 
A. Schüle and R. Anderl 
mathematical optimization process. A XML file is used to store the geometry data. It 
is independent from CAD models and other proprietary file formats. The aim of the 
approach is to have different domains working simultaneously with yet uncertain 
geometry information. By linking this external geometry to 3D-CAD models the 
geometry can be optimized while the CAD models are in detail design or used for 
simulation processes. The external geometry then works as placeholder within the 
models.  
Product creation of bifurcated sheet metal requires three different 3D-CAD mod-
els: The product model, the tool model (i.e. lower die), and the sheet blank model. 
Each CAD model has its own characteristics and modeling structure. The product 
 
 
Fig. 3. Simultaneous Engineering Process for Bifurcated Sheet Metal 

A Simultaneous Engineering Approach for Free Form Bifurcated Sheet Metal Products 
405 
model describes the product itself in its deep drawn and cut out state. The tool de-
scribes the tool geometry with all required process specific components. Back-spring 
effects are considered in the tool to ensure, that the demanded product geometry is 
achieved successfully. The sheet blank describes flat geometry before the forming 
process. It is of special interest how flanges need to be oriented so that they will be 
placed in the desired position on the product after the forming process. 
All parts have similarities and shared geometry. This concept uses these similari-
ties to generate a base structure of geometry that is referenced from an external 
source. All CAD-models adapt automatically to changes in this external source. 
4.1 
Model Dependencies 
It is required to identify dependencies between the CAD models and the external 
geometry to further detail the development processes and to ensure a consistent in-
formation flow. 
• Product model: 
The product free form is the basic geometry for the whole product development 
process and the driving input in development. Nevertheless changes in the free 
form can be necessary if problems occur in tool design or sheet blank design. 
• Tool model:  
The free form surface of the tool is derived from the product free form and the 
compensation of back-spring effects (derived from a finite element analysis). The 
flanges on the product significantly influence the back-spring effect. 
• Sheet blank model: 
The flange positions and orientation on the sheet blank depend on the tool and 
product design. The flange positions are derived from manufacturing simulations.  
Considering these dependencies it is obvious that a general sequence of product de-
sign, tool design and sheet blank design is required to ensure a consistent information 
flow but it is also obvious, that iterations are required and that models need to be 
changed during development. The identified dependencies and the conclusions drawn 
from that analysis are important to create a simultaneous engineering concept and 
build the foundation for process detailing. 
4.2 
Referencing External Geometry 
To enable the introduced simultaneous engineering processes the external geometry 
needs to be linked to the 3D-CAD models of tool, product and sheet blank. In this 
section a method is described that references an external source file, e. g. a XML 
document that contains the free form data. As mentioned above, the free form surface 
is described by NxM control points and a B-Spline degree. To use this geometry in 
the respective CAD files an import filter is defined for each part. This filter uses the 
control point coordinates from the XML file and processes those for each element. 
The tool for example has a slightly different free form surface than the product. To 

406 
A. Schüle and R. Anderl 
the product’s control point coordinates the back-spring compensation is added. For 
the sheet blank the product’s free form and the relative position of the flanges are 
used to calculate the position and orientation on the blank.  
The 3D-CAD models of tool, sheet blank, deep drawn product and the final prod-
uct are shown of an example product. The design process was started with a simple 
tray. An example load case was defined and the mathematical optimization created a 
free form surface. The surface is then used in the different models by a reference to a 
XML file. Thus the parts could be predesigned while mathematical optimization was 
still in progress. The created product models were used for finite element analysis and 
the solutions from this analysis were in turn used for the final shape design. Each 
product model will automatically adapt to changes in the referenced free form surface 
basic geometry. 
 
Fig. 4. Tool Design (simplified, right) and Sheet Blank Design (right) 
In figure 4 a simplified 3D-CAD model of the tool is shown. The light grey area il-
lustrates the geometry that is referenced to an external source. It will adapt to the 
changes in the XML file. 
In figure 4 the sheet blank with flanges is shown. The sealing zone is also visible. 
The flange position (darkened) is referenced to the external source. 
 
Fig. 5. Deep-Drawn Part before (left) and after (right) cut-out 
The product model is shown in figure 5. The free form geometry is linked to the 
external source. It is the driving input for the product development process yet it also 
underlies changes that might impact manufacturing planning. 

A Simultaneous Engineering Approach for Free Form Bifurcated Sheet Metal Products 
407 
4.3 
Data Management and Change Management 
Within the CRC 666 an information model was developed that describes a data struc-
ture for bifurcated sheet metal and the information flows as well as workflows  
involved in the development process [1]. The structure is used to define the XML 
structure and to identify relevant processes and actors in the design of bifurcated sheet 
metal. To support an effective simultaneous engineering process it is crucial to deliver 
the required data in time to wherever it is needed in the workflow. For bifurcated 
sheet metal characteristic information is the free form surface and the position and 
orientation of the bend-splitted flanges. Both are stored in a XML file. The file itself 
can be stored in a PDM system or any data base.  
For effective simultaneous engineering, change management processes are  
required. Thus everyone concerned by a performed change in the base geometry is 
notified automatically by the implemented workflows. These notifications provide 
information about changes that affect the work of the recipient. He can then use the 
changes for further detailing or to check if the work done is still valid. 
Two different cases of changes can be distinguished in the scenario of bifurcated 
sheet metal products: One represents changes that affect the free form face and/or the 
flanges (change type 1), the other represents all other changes that have no effect on 
the free form surface like changes in holes or other detail design features on the prod-
uct (change type 2). Each change in the flanges or the free form surface requires 
changes in tool design and sheet blank design. Thus it is not necessary to categorize 
changes any further. 
The start of the process chain begins always with the generation of the product 
geometry: In case of a change in product geometry because of new features, the 
change affects the free form surface and thus the flanges (change type 1). As soon as 
the change process is started the subsequent processes, in this case tool design and 
sheet blank design are notified and supplied with the new geometry data. 
5 
Conclusions and Future Work 
In this paper a simultaneous engineering approach for sheet metal products with bi-
furcations is introduced. To specify the required information flows the geometrical 
connections between the 3D-CAD parts were identified and described. Thus it was 
possible to develop a process for data exchange between product design and produc-
tion planning in a time-dependent workflow model. In addition a method for change 
notification and support was introduced considering change types, information flow 
and relevant actors. The interaction and links between the 3D-CAD part files for tool, 
sheet blank, and product were demonstrated by an example product.  
The referenced geometry is a single patch free form surface. In complex products 
free form surfaces are often built form multiple patches. The concept needs to be ex-
tended to such geometry. Until now only product development and manufacturing are 
considered in the concept. It could be extended to other domains and steps of the 
product creation process. 
 

408 
A. Schüle and R. Anderl 
Acknowledgments. The results presented in this paper were determined within the 
framework of the subprojects A4 and A5 of the Collaborative Research Center 
CRC 666 fund by the German Research Foundation1. 
References 
1. Anderl, R., Weitzmann, O., Rollmann, T., Schüle, A., Göllner, T.: An Object-Oriented In-
formation Model for the Representation of Free Form Sheet Metal Parts in Integral Style. 
In: Proceedings of TMCE 2012, May 7-11 (2012) 
2. Groche, P., Vucic, D., Jöckel, M.: Basics of linear flow splitting. Journal of Materials 
Processing Technology 183, 249–255 (2007) 
3. Groche, P., Ringler, J.: Spaltbiegen - Ein neues Verfahren für integrale Verzweigungen aus 
der Blechmitte. In: 6. Fachtagung Walzprofilieren Darmstadt, pp. 63–71 (2008) 
4. Bäcker, F., Ertugrul, M., Groche, P.: A new process chain for forming individually curved 
sheet stringers. International Journal of Material Forming 3(suppl. 1), 837–840 (2008) 
5. Doege, E., Behrens, B.-A.: Handbuch Umformtechnik. Springer (2007) 
6. Lee, K.: Principles of CAD / CAM / CAE Systems. Addison-Wesley (1999) 
7. VDI Richtlinie 2209 – 3d product modeling, Düsseldorf (2009) 
8. Schüle, A., Rollmann, T., Anderl, R.: Realization of design features for linear flow split-
ting in nx 6. World Academy of Science Engineering and technology 58, 496–501 (2009) 
9. Rollmann, T., Anderl, R., Chahadi, Y., Schüle, A.: Three-dimensional kernel development 
with parasolid for integral sheet metal design with higher order bifurcations. In: Proceed-
ings of the ASME IDETC/CIE, San Diego, USA (2009) 
10. Schüle, A., Weitzmann, O., Anderl, R.: Feature-based Modeling of Bifurcated Sheet Metal 
Products. In: Proceedings of the 12th APIEMS, Beijing, China (2011) 
11. Wu, Z., Chahadi, Y., Anderl, R., Rollmann, T.: Algorithm-based product development - 
refined concepts and example applications. In: ASME IDETC/CIE, Boston, USA (2008) 
12. Pernot, J.-P., Falcidieno, B., Giannini, F.: Modelling free-form surfaces using a feature-
based approach. In: Proceedings of the Eighth ACM Symposium on Solid Modeling and 
Applications, pp. 16–20 (2003) 
13. Carter, D.E., Baker, B.S.: CE — Concurrent Engineering: The Product Development Envi-
ronment for the 1990s. Addison-Wesley (1992) 
14. Eversheim, W., Bochtler, W., Gräßler, R., Kölscheid, W.: Simultaneous engineering ap-
proach to an integrated design and process planning. European Journal of Operational Re-
search 100(2), S.327–S.337 (1997) 
15. Kummle, R., Weber, H., Storbeck, M., Beiter, P., Berner, S., Schmitt, W., Groche, P.: 11. 
Umformtechnisches Kolloquium Darmstadt, März, Darmstadt, Deutschland, pp. 141–151 
(2012) 
16. Koller, D., Ulbrich, S.: Optimal control of hydroforming processes. In: PAMM, vol. 11(1), 
pp. 795–796 (2011) 
17. Büyüközkan, G., Dereli, T., Baykasoglu, A.: A survey on the methods and tools of concur-
rent new product development and agile manufacturing. Journal of Intelligent Manufactur-
ing 15(6), 731–751 (2004) 
18. Eversheim, W., Bochtler, W., Gräßler, R., Kölscheid, W.: Simultaneous engineering ap-
proach to an integrated design and process planning. European Journal of Operational Re-
search 100(2), S.327–S.337 (1997) 
                                                           
1 Deutsche Forschungsgemeinschaft, http://www.dfg.de 

A Simultaneous Engineering Approach for Free Form Bifurcated Sheet Metal Products 
409 
19. Pfeiffer, T.: Qualitätsmanagement. Hanser Verlag (2001) 
20. Groche, P. (ed.): Tagungsband Sonderforschungsbereich 666: 1. Zwischenkolloquium 06. 
März 2007 Meisenbach (2007) 
21. Göllner, T., Hess, W., Ulbrich, S.: Geometry Optimization of Branched Sheet Metal Prod-
ucts. In: Proceedings in Applied Mathematics and Mechanics (2012) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 411–420. 
DOI: 10.1007/978-3-642-30817-8_40 
© Springer-Verlag Berlin Heidelberg 2013 
 
Complexity Management in Product/Process 
Simultaneous Design for Implementing a Fresnel 
Thermodynamic Solar Plant 
Roozbeh Babaeizadeh Malmiry and Nicolas Perry 
I2M - UMR 5295 - Arts et Métiers ParisTech, F-33400 Talence, France 
{roozbeh.babaeizadeh-malmiry,nicolas.perry}@ensam.eu 
Abstract. Fresnel mirror in thermodynamic solar power plant technology is an 
efficient power resource in many potential countries. The key challenge is cost 
effectiveness of the product and its settling. The importance of using this tech-
nology redounded in writing this paper to propose a methodology in order to 
design this system considering its complexities. Because of the dependency of 
the product with process of production and installation of this solar plant, a si-
multaneous design methodology is required. Moreover, a concept of movable 
factory, from one solar farm site to another one, is also discussed which is  
identified as a proper solution for such project with reusability. 
Keywords: product-process interaction, design methodology, mobile factory 
design, complexity management, solar power plant, Fresnel. 
1 
Introduction 
As Tsao stated, “In any given hour, more energy from the sun reaches earth than is 
used by the entire human population in a given year” [1]. In addition to this fact, solar 
power, with free and available source of energy in many countries, has the minimum 
environmental damages among different types of renewable energies [2]. Considering 
these advantages, although, recently there was a fast growth in solar generation but 
still according to US Energy information administration; only 13% of energy is sup-
plied from renewable energies which less than 1% of that is from solar energy [3], [4]. 
Therefore, solar energy and specifically the technology of Fresnel mirror has chosen 
as target market for this paper.  
Fresnel Thermodynamic Solar (FTS) technology is a new generation of concen-
trated solar plants to use the thermal energy of the sun. A FTS system uses the heat of 
sunray as solar energy and reflects and concentrates this energy by a Fresnel mirrors 
over a flow of fluid to create steam. This paper focuses on this technology of concen-
trated solar power plant but the method and discussions can be applied to any other 
type of project with similar nature. 
The target project is to produce and install a 500,000 m² solar farm (i.e. 50,000 ref-
lectors of 10m² and 136 receptors of 240m long each) in 6 months. This plant is sup-
posed to supply energy for at least 40 years [5]. Logistics aspects suggest a movable 

412 
R.B. Malmiry and N. Perry 
factory as a relevant solution instead of manufacturing in one place and shipping all 
the material. In addition, the temporary factory needs to be movable to produce for at 
least in five different locations.  
One of the main issues is the interaction of product with production. Therefore, in 
the product development aspects, the ease of production and installation method in the 
process of product design needs to be considered. So, it would be a concurrent design-
ing. To sum up, lack of a systematic methodology for this concurrent designing, to-
gether with time limit and the concept of movable factory, makes the design process 
relatively complex.  
Thus, the hypothesis of this paper is to introduce a methodology to design a prod-
uct and process for planting a FTS system taking into account the complexity of the 
design process. 
2 
Complexity and Variability Management in Design 
According to Seth Lloyd, there are several ways to use the word “complexity” [6]. It 
can be the amount of computation, scale of measure, or amount of effort which is 
needed to manufacture a product [7]. The design method of this solar plant is consi-
dered as a complex system because; 
1. The scale of project considering the required effort is relatively large, 
2. The target time limit compared with the size of the project is short, 
3. The concept of movable factory is needed to be applied, 
4. The product and process is closely dependent on each other. 
So, with a methodological design, by means of movable factory, a large part of this 
complexity would be solved. In addition, different levels of variability as presented by 
ElMaraghy, have to be faced in this project which leads to a high level of complexity 
that should be managed [8]. Al-Hakima promotes graph representation to map this 
complexity [9]. At the information level, the interactions and the exchanged know-
ledge also have to be mapped [10]. 
First, the product is designed in a parameterized way (1st level of complexity).  
Indeed this design has to be adapted to the process of production and installation. 
Taking into account this potential product variability, the mobile factory has to be 
designed and optimized, (2nd level of complexity). Moreover, the manufacturing 
system has its geographical dependencies in terms of logistic aspects, power and wa-
ter supply as well as potential local supply (3rd level of complexity). All these design 
solutions should minimize the cost of solar field considering the concentration factor 
and on the other side the cost of mobile factory should have return on investment after 
five times solar plant installation [11]. 
Dealing with solar field design requires defining the objective. This objective 
mainly could be to have maximum energy from a field, to have minimum field area to 
achieve a desired amount of energy, or to maximize the energy obtained from collec-
tor unit area of a field [12]. 

 
Complexity Management in Product/Process Simultaneous Design 
413 
There are other related complexities in the process as well. For instance the diffi-
culty of protect the installation process from environmental effects because of large 
project area and environment condition of locations such as deserts.  
3 
Methodology 
In the past, many designs have been made empirically, iteratively and intuitively, 
based on years of experience and creativity with involvement of much trial and error. 
Therefore, the methodological design has been created to establish a theoretical foun-
dation for engineers in order to increase the creativity and reduce the iterative  
trial-and-error processes [7]. 
The design process in this 
project starts from early stage of 
product/ process definitions. Thus, 
a good orientation could be a top-
down approach from macro model 
of extracting energy from sun to 
micro model of components and 
characteristics. This model is 
shown in figure 1. 
As Nam Suh stated in the book of “Axiomatic design”, design is the process of 
mapping between “what we want to achieve” and “how we want to achieve” [7]. In 
one interpretation, this definition can be interpreted as mapping between prod-
uct/implantation itself and the process of designing the product/implantation. But, this 
definition can be adapted to our goal which is the design of product, and design of the 
process of achieving the product. Since the process must be based on the product and 
on the other side, in the process of designing the product, the production and installa-
tion method also must be considered. Product/Process and Behavioral modeling ap-
proaches give possible representation of these interactions [13], [14]. 
According to axiomatic design, first the aim of the project should be identified and 
then the functional domain which includes the functional requirements. These func-
tional requirements map the physical domain which includes design parameters and 
finally the process domain (see figure 2) [15]. 
The process needs to be divided into sections to be evaluated. For each section, in 
the process domain, after literature review and examining the basic available methods 
based on the stages of the process, there are three simple steps; first, choosing the best 
related methods among the traditional methods; second, combining the traditional 
methods to create a new method; and third, the innovative ideas which are obtained 
by brainstorming and taking ideas from similar methods. 
This approach has led to create some possible conceptual solutions with keeping an 
eye on the limitations of the project. These limitations are time, budget, resource 
availability, and required specifications for the product with high quality and long-
term reliability. Then the best method according to the criteria should be chosen. 
 
 
 
Fig. 1. Top-down approach for design of FTS [5] 
Solar energy 
Transforming solar into heat 
Product design 
Reflector 
Receptor 
Process design 
Mobility 
Speed 
Transforming 
heat into other 
types of 
energy 

414 
R.B. Malmiry and N. Perry 
 
Fig. 2. Four domains for design of FTS 
In every stage of design, all 
the effective factors need to be 
considered. Figure 3 shows the 
main factors for a thermodynam-
ic solar plant as IDEF0 (The 
complete IDEF3 diagram for this 
project is available in [5]). As it 
is illustrated in this chart, in the 
design process, all the material 
flow, processes, controls and 
resources should be taken into 
account. The economical solu-
tion will depend on the manufac-
turing processes and mainly on organizational issues such as material purchase, im-
plementation time, energy and fluid access for the factory. 
3.1 
Product Design 
In brief, the product in this project consists of two main parts, “reflectors” and “recep-
tors”. These two parts are supposed to be installed and work together. So, the process 
of designing depends on each other. According to the proposed process in Figure 2, 
first, the characteristics of the product need to be examined. Since the product doesn’t 
exist to be examined, the aim of the product is scrutinized. For instance, the reflectors 
are expected to reflect the sunray in the most efficient way to the receptors. Moreover, 
it needs to be rotatable to follow the sunray during the day. Based on these primary 
objectives, one can characterise the product and shall proceed to the next steps for 
designing the rotators, connectors, mirrors and the main challenge which is the sup-
port. After, these characteristics become more in detail such as size and geometry, 
structure of support and so on to satisfy the required accuracy and rigidity. 
 
Fig. 3. Key parameters for FTS system analysis [5] 

 
Complexity Management in Product/Process Simultaneous Design 
415 
For designing the receptor, with the same procedure, the objective is to receive the 
concentrated sunray. To obtain this goal, the receptor must contain pipes for the flux 
and also should have an insulator to prevent heat release. This system must implement 
in the height. Therefore, the receptor must contain several tubes with special material 
to resist from heat while it conducts the heat to the flux. These pipes need to be 
hanged using holders. Moreover, the cover of receptor, needs to have flexibility to 
keep straighten in case of thermal expansion which might be different in top and the 
bottom side. Also, this cover must carry the insulator too. 
3.2 
Process Design 
Beside the product characteristics, the method of production and installation of the 
plant also needs to be specified. This method is considered as “process” in this paper. 
As process, managing resources such as material, machines, labor as well as facilities 
and working conditions are also included. Since our product has two parts of reflector 
and receptor, one production process has to be designed for each product. Because of 
dependency of production method with product, the number of process to design 
would be the combination of these two. It means that, if there are x number of solu-
tions for designing of reflectors and y number of solutions for designing of receptors, 
there needs to be z number of solutions for processes which z ≥ x×y considering x and 
y are dependent. In spite of manufacturing, we also have to design a method(s) to 
install these elements. Note that, because of the interaction of these two parts, the 
installation of them must be adopted to each other and also to the criteria. The main 
criteria for process are: i) ability to produce the whole solar farm elements within 6 
months, ii) reusability for at least 5 times, iii) install and uninstall of the factory in 
two months, iv) ability to move, v) adoptability with different locations. 
So, factors such as time, reusability, reliability, adoptability and mobility need 
to be considered during the process of design (figure 2). In this case also the similar 
methodology of design should be followed. Different stages of the production and 
assembly line as well as installation stage need to be examined. In most of the produc-
tion and installation stages, the process is known and could be applied. By consider-
ing the criteria, the most suitable method should be chosen and any method needs to 
be adjusted along with the goal domain. 
The production environment has an important role in this project. For instance 
access to energy, working temperature, dust and etc. will affect the plant and working 
facilities design. The installation equipment should be portable to cover large area. 
Levels of portability which will be discussed in next section also need to be chosen. 
In several design solutions, dividing the processes of production and installation is not 
appropriate. Modular design of the factory can be applied using shipping containers as 
structure module. 
4 
Product/Process Evaluation Tool 
As it is mentioned earlier, one of the most complex issues in this project is the interac-
tion of design factors in this project. Interaction between the different levels and 
product-process is mapped using graph representation as illustrated in figure 4.  

416 
R.B. Malmiry and N. Perry 
 
Fig. 4. Product/process elements interactions [5] 
After designing stage, regarding these links, each design solution need to be eva-
luated based on: 1) Process steps and sub-steps, 2) Cycle times, 3) Throughput time, 
4) Human resource (number and skills), 5) Types and cost of material, 6) Type and 
cost of machines, 7) Number of container needed to move the factory, 8) Number of 
container needed to supply the materials, 9) Bottle necks, 10) Machines’ efficiency, 
11) Product mass and weight, as presented in Appendix A. 
For cost evaluation, reusability of the mobile factory needs to be considered. As-
suming five times for reusability, the total cost will be calculated as; 
ܶ݋ݐ݈ܽ ܿ݋ݏݐ ݋݂ ૚ ݏ݋݈ܽݎ ݌݈ܽ݊ݐ=
ሾ૞ൈܥ݋ݏݐ ݋݂ ݉ܽݐ݁ݎ݈݅ܽ+ ૞ൈܥ݋ݏݐ ݋݂ሺܶݎܽ݊ݏ݌݋ݎݐܽݐ݅݋݊, ܮܾܽ݋ݎ… ሻ+ ૚ൈܥ݋ݏݐ ݋݂ ݂ܽܿݐ݋ݎݕሿ
૞
ൗ      (1) 
5 
Movable Factory Design 
As an initial idea for this project, the project supposed to be implemented in North 
Africa but the special resources are available in source country which is considered as 
France. By special resource, it is meant materials and machines which are not found 
in many places (especially North Africa) as well as experts, robots and customized 
equipment. In addition, the process supposed to be used for more than one project. In 
this case, the first idea could be to build the product in origin and transport to the 
destination. 
As table 1 illustrates, the cost of transportation for only reflectors panel will be 
about 775 k€ which makes the project non economical. While, if the material is trans-
ported instead of ready panels, the cost would be around 32 k€ which is significantly 

 
Complexity Management in Product/Process Simultaneous Design 
417 
cheaper. This difference, even without considering the reduction in possible damages 
during the transportation can justify transporting labor, equipment and etc. and pro-
ducing in the destination point in case of receptors, the 200m length requirement, 
rationalizes the proposed transportation method even more. Hence, the concept of 
mobile factory is used in this project. 
Table 1. Cost of transportation for ready panels vs. materials from France to North Africa 
40ft Container 
Size: 12m×2.35m×2.4m , Volume limit: 37 m3 , Weight limit: 36500 kg 
transportation cost of each container: 340 € 
Fixed transportation cost:  2,300 € 
 
Required container 
Estimated cost of transportation 
Prepared panels 
2273 
775 k€ 
Materials (Glass & Aluminum) 
88 
32 k€ 
 
As DesJardin presents, there are three major level of mobility for a factory; Fixed, 
transportable and portable. Various factors should be considered to make a factory in 
high level of portability such as size reduction, weight reduction and power source 
[16]. In this project, there are two aspects of mobility. First, because of the size and 
complexity of the product, it’s not possible to move the finished goods. So, the idea is 
to move the material, equipment, labor, and energy source to the destination and then 
produce and install at site.  
The concept of movable factory is not limited to solar plant. Offshore workshop, 
used in Navy, is a good example of movable production system which enables carry-
ing the missions’ maintenance system anywhere in the world. Movable assembly line 
is another example of using this concept in aerospace industry to have a leaner and 
more efficient production system. 
Second aspect is to use portable factory. A portable factory which is also dubbed as 
“factory in a box” is a pre-assembled group of equipment which is transportable and 
can be used as plug-and-play in site. These kinds of buildings are just placed in the 
site and can be easily removed and transported to another place afterwards. In such 
buildings, features such as HVAC system, energy generators, etc. could be housed in 
a 20ft shipping container which can be shipped, trucked or airlifted to any location. 
Thus, in the process of production and installation design for such project, movable 
factory based on the level of mobility, factory life cycle, interdependency of process 
and product, and other issues needs to be designed.  
Before starting to design, the life cycle of the process to build the solar plant needs 
to be examined. This cycle includes preparation, storage, transportation, installation, 
use, and uninstallation. The transportation stage means either to move from the source 
to the project site or from one project to another. In this stage, three categories of 
human resource, machines and robots, and material need to be transported. In each 
category some elements could be used by local resources while others need to be 
transported. Table 2 is the summery of these elements and the way to utilize. The 
cycle starts from the preparation of equipment, materials, labors, etc. 

418 
R.B. Malmiry and N. Perry 
The installation stage means to install the temporary factory (e.g. in 6 months) in 
order to build the solar plant and in “use” stage the factory is used to produce the 
components and install it. 
Table 2. Using local resources vs. transporting resources 
 
 
Because of the difficulty of components like receptor, and time shortage, these two 
stages need to be done in parallel with a proper scheduling. For fast establishment of 
large area foundation, machines such as concert slip forming machine, asphalt finish-
er, track renewal train and etc. could be occupied. These machines create a flat  
surface which makes it easy for equipment and labor to move in the project area espe-
cially for portable factories and also it can be used to install the solar plant compo-
nents. After installation and use, with considering the concept of green factory, all the 
equipment of plant production must be collected from the site except those which are 
necessary for the utilizing the solar plant. 
6 
Conclusion and Discussions 
This paper proposed an example and the starting elements of a methodology in order 
to deal with complexity in design of product and production/installation process with 
bearing in mind the cost, quality and time. The main complexity which is faced first 
in this project is the interaction of product with process which made it necessary for a 
concurrent designing. The next level that must be integrated is the adaptability and 
reliability requirements due to mobility.  
To evaluate a design as a “good design”, at first the goal has to be identified. So, 
this methodology is started from goal domain. This goal maps the functional require-
ment which maps the physical domain for product and process. Finally, in the process 
domain, the procedure needs to be divided properly and the three steps apply for each 
section. 
After designing the product and processes, the solution needs to be evaluated by 
the “evaluation tool” which was proposed. By means of this tool, different solutions 
of product design would be examined and subsequently, the design of process is eva-
luated. Regarding the goal domain, the design decision is made based on the result 
which includes cost, scheduling, number of containers, etc. 
After the conceptual design phases, other aspects such as movable factory are also 
integrated which according to the project, the level of movability needs to be chosen. 
This movability makes the process less expensive in transportation, more efficient for 

 
Complexity Management in Product/Process Simultaneous Design 
419 
covering large area, and possible to be reused for at least five times. The reason of 
choosing “five” as reusability is that considering transportation time, inventory wait-
ing time and production time, five times of implanting a solar power plant takes about 
5 to 7 years. After this time most probably the product is no longer on top of technol-
ogy and should be replaced by a competing product which gives a better performance.  
Since the time is so limited in this project in comparison with size of the project, 
milestones are substantial to be scheduled and therefore each process needs to be 
illustrated as simogram. This Gaunt chart is fundamental as far as so many processes 
may happen in parallel in this project and it’s crucial to have them all on time. 
This methodology, using the evaluation tool, works as a dynamic tool for “what-if” 
scenario. At any stage, one factor can be modified and the result is being observed. 
However, the discussion is for the early stage of design and the next developments 
will give more precise rules for evaluation (costs or time). These sharper equations 
can be easily integrated in the evaluation tool and use its final results (input/output 
data). After evaluation of the solutions, if the result is not satisfactory, the characteris-
tics of domains change and then the method applies again until the desired result is 
achieved.  
References 
1. Tsao, J., Lewis, N., Crabtree, G.: Solar FAQs. US department of Energy (2006) 
2. Jayakumar, P.: Resource Assessment Handbook, APCTT (2009) 
3. US National Academy of Engineering: A Grand Challenge for the Twenty-first Century: 
Commercializing Solar Energy. Strategic Direction 24(7), 33–36 (2008) 
4. Quoilin, S.: Concentrating Solar Power Plants. Université de Liège (2007) 
5. Babaeizadeh Malmiry, R.: Product-Process Interaction Analysis: Design of Mobile Factory 
for Fresnel Thermodynamic Solar Plant. Master Thesis, Arts et Metiers – ParisTech (2012) 
6. Lloyd, S., Pagels, H.: Complexity as Thermodynamic Depth. Annals of Physics 188, 186–
213 (1998) 
7. Suh, N.P.: Axiomatic Design: advances and applications. Oxford University Press, New 
York (1936) 
8. ElMaraghy, W., ElMaraghy, H., Tomiyama, T., Monostori, L.: Complexity in Engineering 
Design and Manufacturing, vol. 61/2 (2012) 
9. Al-Hakima, L., Kusiak, A., Mathew, J.: A Graph-theoretic Approach to Conceptual Design 
with Functional Perspectives. Computer-Aided Design 32(14), 867–875 (2000) 
10. Bernard, A., Labrousse, M., Perry, N.: LC Universal Model for the Enterprise Information 
System Structure: Innovation in Life Cycle Engineering and Sustainable Development, pp. 
429–448. Springer (2005) 
11. SYNDICAT, Directory of the French Solar Thermodynamic Industry (2011) 
12. Weinstock, D., Appelbaum, J.: Optimal solar field design of stationary collectors. Journal 
of Solar Energy Engineering, 898–905 (2004) 
13. Qian, L., Gero, J.S.: Function-Behavior-Structure Paths and Their Role in Analogy-Based 
Design. AIEDAM, 289–312 (1996) 
14. Umeda, Y.: Functional Reasoning in Design. IEEE Expert, 42–48 (1997) 
15. Labrousse, M., Bernard, A., Véron, P.: Generic FBS concept for Process/Product/Resource 
integration. In: Tools and Methods of Competitive Engineering, vol. 1, pp. 384–394 
16. DesJardin, L.: Downsizing with VXIbus: Opportunities and Limitations in Factory, Field, 
and Portable environments. pp. 55–62. IEEE (1989) 

420 
R.B. Malmiry and N. Perry 
 
 Appendix A. Evaluation tool’s results environment: Process, Evaluation factors, Process time & 
Product choices [5] 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 421–429. 
DOI: 10.1007/978-3-642-30817-8_41 
© Springer-Verlag Berlin Heidelberg 2013 
 
Embodiment Discrete Processing 
Sebastian Pena Serna1, Andre Stork1,2, and Dieter W. Fellner1,2 
1 Fraunhofer IGD, Fraunhoferstr. 5, Darmstadt, Germany 
2 TU Darmstadt, Fraunhoferstr. 5, Darmstadt, Germany 
{sebastian.pena.serna,andre.stork,d.fellner}@igd.fraunhofer.de 
Abstract. The phases of the embodiment stage are sequentially conceived and 
in some domains even cyclic conceived. Nevertheless, there is no seamless in-
tegration between these, causing longer development processes, increment of 
time lags, loss of inertia, greater misunderstandings, and conflicts. Embodiment 
Discrete Processing enables the seamless integration of three building blocks. 
1) Dynamic Discrete Representation: it is capable to concurrently handle the 
design and the analysis phases. 2) Dynamic Discrete Design: it deals with the 
needed modeling operations while keeping the consistency of the discrete 
shape. 3) Dynamic Discrete Analysis: it efficiently maps the dynamic changes 
of the shape within the design phase, while streamlining the interpretation 
processes. These integrated building blocks support the multidisciplinary work 
between designers and analysts, which was previously unusual. It creates a new 
understanding of what an integral processing is, whose phases were regarded as 
independent. Finally, it renders new opportunities toward a general purpose 
processing. 
Keywords: Embodiment, Design, Analysis, Dynamic Meshes, Mesh  
Modifications. 
1 
Introduction 
The use of a digital representation of an (either existing or conceptual) object or shape 
in different domains, such as in engineering, entertainment, cultural heritage or medi-
cine, is essential for representing the 3D physical reality. This is true for several shape 
representation schemes such as point clouds, isosurfaces, subdivision surfaces, mesh-
es, parametric surfaces, and B-reps, among others. These representation schemes have 
a tight inherited relationship with their corresponding production technique. On the 
one hand, in the case of physically born objects, for example a laser scanner can typi-
cally generate a point cloud suitable for triangulation, while a computed tomography - 
CT scanner - can usually generate volume data suitable for isosurfacing. On the other 
hand and referring to digitally born objects, CAD systems ([1]) classically model with 
B-reps or Constructive Solid Geometry - CSG - schemes and freeform surface model-
ing systems normally work with parametric, polygonal or subdivision surfaces. 
Although the representation schemes for digitally born objects can easily be trans-
formed into the representation schemes of the physically born objects, the opposite 
case is not a straightforward process. Furthermore, the domain application also influ-
ences the utilization of specific representation schemes. For instance, in engineering 

422 
S.P. Serna, A. Stork, and D.W. Fellner 
 
B-reps, parametric surfaces and meshes are commonly used; polygonal, subdivision 
or parametric surfaces are frequently employed in the entertainment industry; in the 
cultural heritage field subdivision or polygonal surfaces and meshes are widely estab-
lished; while B-reps, parametric surfaces and meshes are extensively exploited in the 
medicine sector. 
Nevertheless, regardless of the domain application and the corresponding represen-
tation scheme, producing and using a shape demand dedicated processes. In the me-
chanical engineering domain, a workflow in the context of the Systematic Approach 
to Engineering Design, proposed by Pahl and Beitz [2], presents this more clearly. 
This workflow has four stages: a) task clarification, b) conceptual design, c) embodi-
ment, and d) detail design. Task clarification mainly deals with collecting the  
requirements for the given problem; conceptual design aims to propose a feasible 
solution(s) based on the identified requirements; embodiment builds a digital repre-
sentation of the solution and verifies its performance against the expected functionali-
ty; and detail design generates the specification of the evaluated solution toward  
manufacturing. 
The embodiment stage illustrates the dedicated processes for producing and using a 
shape and it is subdivided into two main phases: i) design, and ii) analysis. The design 
phase entails the different processes, which are needed to produce a shape for the 
analysis phase, e.g.: digitization, processing, modeling, or synthesis. The result of the 
design phase is a shape, whose geometric characteristics are described in a specific 
representation scheme. The analysis phase involves the different processes for inter-
preting the shape, in other words, for correlating the shape with other properties, such 
as semantics, materials, physics, and functions, among others. 
2 
Related Work 
Although the phases of the embodiment stage are sequentially conceived and in some 
domains even cyclic conceived, there is no seamless integration between these, caus-
ing longer development processes, increment of time lags, loss of inertia, greater mi-
sunderstandings, and conflicts, etc. The lack of integration is motivated by several 
aspects: i) representational - designers require a geometric representation and analysts 
require an analytic representation, ii) functional - designers constantly change the 
geometry of the shape and analysts normally take a snapshot of the geometry, iii) 
deterministic - designers want to describe a shape, while the analysts want to infer 
properties from the shape, and iv) cultural - designers and analysts do not communi-
cate with the same terminology. This situation is found in several domains and indus-
tries. 
This is the case for producing a mechanical part in the engineering domain, where 
on the one hand, the embodiment design is carried out using a CAD system with an 
internal representation schema based on B-reps, which supports several modeling 
operations for specifying and constructing the geometric characteristics of the part. 
And on the other hand, the embodiment analysis is performed using a CAE system 
with a discrete representation (e.g. a tetrahedral mesh) for simulating the dimensions 
of the part with its material, in order to satisfy a functionality described by physical 
laws. Thus, there is a need for an extra representation, in order to correlate the geome-
tric characteristic of the part with its functionality. These cases become even more 

 
Embodiment Discrete Processing 
423 
 
demanding, if the analysis produces new requirements toward the design and these 
need to be communicated by the analysts to the designers, since there is no estab-
lished transition between analysis and design, as its counterpart between design and 
analysis. 
This situation of multiple representation schemes for two highly interdependent 
phases poses a major challenge: how to efficiently handle the different representations 
for streamlining the process and for achieving multidisciplinary environments? Dif-
ferent strategies can be followed in order to address this challenge: 1) improving the 
transition techniques between the different representations, 2) complementing existing 
representations with some of the missing capabilities of the other phase, and 3) devel-
oping an existing or new representation, which is able to efficiently handle the design 
and the analysis phases. 
In the context of transition techniques, the research community has pursued the de-
velopment of efficient and robust meshing techniques ([3] and [4]), nonetheless this 
transition process is not interactive for volume meshes and it is not always automatic. 
In terms of enhancing representations, they also proposed the isogeometric analysis 
for 3D NURBS ([5] and [6]), in order to perform mechanical simulation on parame-
tric representations, however it still requires a transition from the 2D NURBS of the 
B-rep to the 3D NURBS for the analysis and so far it is only possible to simulate sim-
ple shapes. Additionally, there are also some approaches dealing with mesh deforma-
tion, either only the geometry (morphing [7]) or the geometry with the topology 
(smoothing and re-meshing [8]), but for the first case there are issues with the quality 
of the elements during the deformation and it could also insert inconsistencies in the 
mesh, for the second case the performance is a drawback and the range of operations 
is limited. 
3 
Building Blocks 
There is so far no effort devoted to developing an existing or new representation able 
to concurrently handle the design and the analysis phases. In order to develop such an 
integral solution, which can consistently and robustly deal with the embodiment stage, 
the following research questions need to be resolved: 1) What is a suitable representa-
tion scheme to dynamically deal with the changes of the shape in the design phase and 
to capture the needed properties for the interpretation process of the analysis phase in 
a concurrent manner? 2) What is the set of operations that such a dynamic representa-
tion needs to support, in order to efficiently and robustly enable the required geome-
tric changes of the shape, while keeping its quality? 3) How can the interpretation 
process be streamlined given the multiple changes in the characteristics represented 
by the shape? 
These research questions are addressed and integrated in this work ([9], [10], [11], 
[12], [13]). This work is based on a deep analysis of the related work and the research 
opportunities, which strengthen the selection of a discrete representation (i.e. mesh). 
Thus, the Dynamic Discrete Representation is capable to concurrently handle the 
design and the analysis phases. The Dynamic Discrete Design deals with the needed 
modeling operations while keeping the consistency of the discrete shape. The Dynam-
ic Discrete Analysis system efficiently manages the dynamic changes of the shape 
within the design phase, while streamlining the interpretation processes. The seamless 
integration of these three building blocks is called Embodiment Discrete Processing. 

424 
S.P. Serna, A. Stork,
 
This environment allows fo
lysts, which was previous
integral processing is, who
new opportunities toward a 
3.1 
Dynamic Discrete R
Compact representations ar
of the mesh demand a re-e
mostly dealing with multire
solution model is usually p
existing representations, th
volume) or the quality (com
Thus, representing meshes 
in order to reliably compu
the topology of the changin
 
Fig
The Dynamic Discrete R
a consistent representation 
enables a generic and robu
ponding interrelationships, 
geometry and the topology 
, and D.W. Fellner 
or the multidisciplinary work between designers and a
sly unusual. It creates a new understanding of what
ose phases were regarded as independent. And it rend
general purpose processing. 
Representation 
re only feasible for static meshes; changes in the topolo
encoding of the connectivity. Dynamic representations 
esolution problems; hence the computation of the mult
performed offline. Regardless of the characteristic of 
e current solutions are limited to the type (e.g. surface
mpleteness, orientation or manifoldness) of the input me
with dynamic changes require generic and robust metho
ute the neighboring information and consistently updat
ng mesh. 
 
g. 1. Dynamic Discrete Representation 
Representation is the building block responsible for keep
of the shape. It is a dynamic mesh data structure, wh
ust description of the topological entities and their corr
while efficiently allowing the dynamic modification of 
of the mesh. The main characteristics of the data struct
ana-
t an 
ders 
ogy 
are 
tire-
the 
e or 
esh. 
ods, 
ting 
ping 
hich 
rres-
f the 
ture 

 
 
(see Fig. 1) are: a) balance
hierarchical construction, ii
mesh. 
3.2 
Dynamic Discrete D
The design of a discrete sha
the shape, while preserving
ever, many of the propose
ments, which could bias the
global shape, do not perfor
tion techniques only deal 
constant and generating art
dealing with the topology (
for interactive performance
The Dynamic Discrete 
measure, which computes th
ing of the local mesh confi
the 1-ring vertex neighborh
the deviation between the 
mean vector (see Fig. 2). 
range [0, 1], which quantifi
a better understanding of th
provides clues on the ill-co
improving its interpolating 
ing the 1-ring neighborhoo
and therefore accuracy in th
of the 1-ring vertex, regar
Embodiment Discrete Processing 
ed trade-off between memory and performance, b) rob
ii) efficient query support, and iv) dynamic changes on 
Design 
ape requires the interactive modification of the element
g the quality for the following interpretation process. Ho
ed assessment methods only consider the individual e
e assessment process and the few methods considering 
rm at interactive rates. On the other hand, many modifi
with the geometry of the shape, keeping the topolo
tifacts for large deformation. Additionally, the techniq
(e.g. meshing, remeshing, multiresolution) are not suita
. 
 
Fig. 2. Dynamic Discrete Design 
Design profits from a novel neighboring based qua
he quality of 1-ring vertices, capturing a better understa
iguration. This quality measure evaluates the condition
hood, by means of computing for every involved simp
direction of the gradient vector and the direction of 
These deviations are averaged, providing a value in 
ies the quality of the 1-ring vertex. In addition of captur
he local mesh configuration, the 1-ring quality measure a
nditioning of the 1-ring neighborhood, which is tackled
condition and therefore the gradient of the vertex rega
od, leading to a low condition number of the linear syst
he solution. In order to improve the interpolating condit
rding the neighboring vertices, the algorithm applies
425 
bust 
the 
s of 
ow-
ele-
the 
fica-
ogy  
ques 
able 
ality 
and-
n of 
plex 
the 
the 
ring 
also 
d by 
ard-
tem 
tion 
 an  

426 
S.P. Serna, A. Stork, and D.W. Fellner 
 
optimization procedure, which combines complex topological operations and smooth-
ing techniques, aiming to generate optimal local configurations. The optimal configu-
rations are found by averaging the height direction of the affected simplices,  
regarding the 1-ring vertex. The resulting direction suggests an infinite line, where an 
optimal configuration can be found for the ill-conditioned area, such that the 1-ring-
vertex can improve its gradient. The possible configurations are found by either: i) 
modifying the neighborhood (by displacing the 1-ring vertex over the line), ii) creat-
ing a new neighborhood (by adding a new vertex over the line) or iii) eliminating a 
neighborhood (by collapsing the 1-ring vertex to a vertex in the neighborhood). The 
evaluation of the possible configurations is based on an objective function, which 
maximizes the 1-ring quality for the set of predefined configurations. 
3.3 
Dynamic Discrete Analysis 
The suitability of a shape of being able to be interpreted is what makes a shape versa-
tile. However, only a limited set of representation schemes are suitable to be inter-
preted. Discrete shapes are a versatile representation, since these can deal with design 
and analysis processes. One of the interpretation processes supported by a discrete 
representation is the understanding of its intrinsic semantic properties. The semantic 
properties cannot only be derived from the geometry, the context in which the shape 
is conceived, need to be considered as well. Additionally, the resolution of the shape 
should not affect its semantics and changes in the form of the shape should also be 
handled by the interpretation process at interactive rates. On the other hand, discrete 
shapes also support the evaluation of functionality. However current approaches only 
handle static shapes or shapes, whose connectivity remains constant, while the geo-
metry is deformed. This is motivated by the performance cost, associated with the 
changes on the geometry and the topology of the shape, which need to be replicated to 
the linear system, responsible of capturing the correlation of the different properties 
(e.g. dimensions, materials, physical laws). 
The digital representation of a shape, either a conceptual or an existing one, is es-
sential for the digital processing of products (in a general extent). However, the shape 
with its geometric description does not integrally support this process, since the  
functionality of the shape needs to be interpreted as well. In the context of the Embo-
diment Discrete Processing pipeline, the shape is represented with the Dynamic Dis-
crete Representation and the shape is modified (and possibly created) with the  
Dynamic Discrete Design processes. Now, the shape is interpreted with the Dynamic 
Discrete Analysis system, which is able to flexibly handle the dynamic changes of the 
shape, in order to streamline the integral embodiment pipeline. This flexibility is 
achieved, by means of creating an equivalent system (see Fig. 3), which is able to 
efficiently correlate the properties related to the shape. The equivalent system exploits 
the updated neighboring information, which is kept by the Dynamic Discrete Repre-
sentation, enabling the dynamic changes without performing a rebuild of the system 
for every change. The main characteristic of the system resides on the flexible and 
efficient correlation of the properties of the system without the need of a static encod-
ing, achieving an alternative representation for linear systems of equations. 

 
Embodiment Discrete Processing 
427 
 
 
Fig. 3. Dynamic Discrete Analysis 
4 
Conclusions 
The interplay of these three building blocks will enable a multidisciplinary work be-
tween designers and analysts, a new understanding of what an integral processing is, 
and new notion of general purpose processing (see Fig. 4). One of the possible new 
scenarios would be: in a single environment (without the need for external interfaces), 
designers and analysts work together in the solution of a problem, the designer builds 
the shape, while explaining the analyst the reason of specific styling characteristics. 
The analyst understands the situation and invites the designer to evaluate the current 
design against the expected functionality. Hence, the analyst takes over the control of 
the environment and sets a couple of constraints and conditions, in order to perform 
the evaluation. The result is shown to both experts and the analyst explains the de-
signer, where critical conditions for the current design could affect the functionality of 
the solution. Thus, the designer operates the environment, in order to modify the 
shape, in view of the mitigation of the critical conditions, while maintaining the styl-
ing characteristics. Finally, the analyst repeats the evaluation and verifies the im-
provements achieved with the changes on the shape within a single and dynamic  
environment for multidisciplinary collaboration. 
 

428 
S.P. Serna, A. Stork,
 
Fig
Acknowledgments. This 
VISTRA (FP7-FoF-ICT-20
Green Regional Aircraft (G
References 
1. Lee, K.: Principles of CAD
2. Pahl, G., Beitz, W.: Engin
don (1996) 
3. Labelle, F., Shewchuk, J.
dral angles. ACM Transac
4. Klingner, B.M., Shewchu
ings of the 16th Internatio
2007) 
5. Hughes, T.J.R., Cottrell, 
nurbs, exact geometry an
4135–4195 (2005) 
6. Verhoosel, C.V., Scott, M
higher-order gradient dam
12, The Institute for Com
Austin (May 2011)  
7. Athanasiadis, T., Fudos, 
geometrically constrained
Symposium on Applied C
8. Wicke, M., Ritchie, D., K
ic local remeshing for e
(2010) 
9. Serna, S.P., Stork, A.: Co
the International Conferen
329–332. IEEE Computer
10. Serna, S.P., Stork, A.: Me
of NAFEMS World Cong
mulation for Today and B
, and D.W. Fellner 
g. 4. Embodiment Discrete Processing 
work is partially supported by the European proje
011.7.4-285176) and Clean Sky JTI (FP7) within the IT
GRA) and SMART Fixed-Wing Aircraft (SFWA). 
D/CAM/CAE Systems. Addison-Wesley (1999) 
neering Design: A Systematic Approach, 2nd edn. Springer, L
.R.: Isosurface stuffing: Fast tetrahedral meshes with good d
ctions on Graphics 26(3), 57.1–57.10 (2007) 
uk, J.R.: Aggressive tetrahedral mesh improvement. In: Proce
onal Meshing Roundtable, IMR, Seattle, USA, pp. 3–23 (Octo
J.A., Bazilevs, Y.: Isogeometric analysis: Cad, finitr eleme
nd mesh refinement. Comput. Methods Appl. Mech. Engrg. 
M.A., Borden, M.J., Hughes, T.J.R., de Borst, R.: Discretizatio
mage models using isogeometric finite elements. ICES Report
mputational Engineering and Sciences, The University of Texa
I., Nikou, C., Stamati, V.: Feature-based 3d morphing based
d sphere mapping optimization. In: Proceedings of the 2010 A
Computing, SAC 2010, pp. 1258–1265. ACM, New York (201
Klingner, B.M., Burke, S., Shewchuk, J.R., O’Brien, J.F.: Dyn
elastoplastic simulation. ACM Trans. Graph. 29(4), 49:1–49
omputer design in computational mechanics. In: Proceeding
nce on Computing, Engineering and Information, ICC 2009,
r Society, Los Alamitos (2009) 
esh-based modeling towards engineering analysis. In: Proceedi
gress: The Analysis Advantage: Perspectives on Engineering
Beyond, NWC 2009, pp. 1–11. NAFEMS, Glasgow (2009) 
 
ects 
TDs: 
Lon-
dihe-
eed-
ober 
ents, 
194, 
on of 
t 11-
as at 
d on 
ACM 
0) 
nam-
9:11 
gs of 
, pp. 
ings 
g Si-

 
Embodiment Discrete Processing 
429 
 
11. Serna, S.P., Stork, A., Fellner, D.W.: Tetrahedral mesh-based embodiment design. In: Pro-
ceedings of the ASME International Design Engineering Technical Conferences & Com-
puters and Information in Engineering Conference, IDETC CIE 2010, vol. 3, pp. 131–140. 
ASME, New York (2010) 
12. Serna, S.P., Stork, A., Fellner, D.W.: Embodiment mesh processing. In: Fischer, X., Na-
deau, J.-P. (eds.) Research in Interactive Design. Virtual, Interactive and Integrated Prod-
uct Design and Manufacturing for Industrial Innovation. IDMME - Virtual Concept 2010, 
vol. 3, pp. 1–6. Springer, Berlin (2010) 
13. Serna, S.P., Stork, A., Fellner, D.W.: Interactive exploration of design variations. In: Pro-
ceedings of the NAFEMS World Congress: A World of Engineering Simulation: Industrial 
Needs, Best Practice, Visions for the Future, NWC 2011, pp. 1–18. NAFEMS, Glasgow 
(2011) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 431–440. 
DOI: 10.1007/978-3-642-30817-8_42 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Virtual Prototyping Approach Based on DOE Analysis 
to Support the Design of a Centrifugal Impeller  
Paolo Cicconi, Daniele Landi, and Michele Germani 
Department of Industrial Engineering and Mathematical Sciences,  
Università Politecnica delle Marche, via Brecce Bianche, Ancona, 60131, Italy 
{p.cicconi,d.landi,m.germani}@univpm.it 
Abstract. The development of appliances focuses on more efficient and high 
functionality products. The recent European legislation limits energy consump-
tion for several domestic devices, including kitchen hoods which encloses an 
impeller moved by an electrical motor. A correct impeller design is important to 
guarantee fluid dynamic performance and the reduction of resistance torque in a 
cooker hood system. The traditional design approach is based on pilot manufac-
turing and related experimental tests. This paper aims to present a methodology 
design for centrifugal impellers integrating a multi-level approach based on a 
Virtual Prototyping tool, Design of Experiments (DOE) method and Rapid Pro-
totyping system in order to support the project design. The proposed research 
has been validated during the design of a fan wheel for domestic cooker hoods. 
A DOE approach, based on main impeller parameters, has been applied to the 
virtual experiment with CFD tool to reduce cost and time-to-market. 
Keywords: impeller, kitchen hood, Virtual Prototyping, Design of Experi-
ments, Rapid Prototyping. 
1 
Introduction 
European directives govern household appliance manufacturers in the research of 
more efficient products with less environmental impact [1], while the market also 
demands higher performance [2] and extended functionalities [3]. The context of 
cooker hoods regards specifications related to air-ventilation, air-quality, noise control 
and electricity consumption. This appliance is often installed in wide living rooms, 
where ventilation and air-quality is necessity [4]. An adequate fan removes smoke, 
volatile organic compounds, grease particles and vapour from the kitchen area [5]. 
These issues require a flexible design methodology which would be able to support 
the engineer in a virtual physical analysis and also in rapid decision making, in order 
to increase product quality.  
Nowadays, Virtual Prototyping technologies are often used in design processes to 
guarantee flexibility and reliability during the first project phase [6]. These solutions 
reduce the project development lead-time and the physical prototyping, according to 
the needs of kitchen hood development. The cooker hood design concerns different 
project levels such as aesthetical design, structural analysis, fluid dynamics perform-
ance and electricity consumption. The ventilator, which groups the electric motor and 

432 
P. Cicconi, D. Landi, and M. Germani 
fan wheel, is the main component in the cooker hood architecture. The integration of 
virtual and rapid prototyping tools allows the blower’s geometrical configuration to 
be optimized and reduces many experimental loops. From a medium-large sized pro-
duction point of view, this means an efficient redistribution of the available resources. 
2 
Background Research 
The design of  kitchen hood impellers concerns repetitive phases: parametric model-
ling, pilot mould manufacturing and experimental tests to investigate the performance 
of several prototypes. This procedure increases cost and development time in finding 
an optimal product configuration, which means spending company resources. In lit-
erature, some design procedures are available to improve the design and engineering 
of products like kitchen hood blowers. Therefore these tools and methods are often 
used alone. The main subdivision is between the real testing procedure and virtual 
prototyping. 
The first step in improving impeller design is to introduce a rapid prototyping (RP) 
approach in order to reduce the more expensive pilot manufacturing and development 
time. This allows a quick response to business opportunities thus ensuring company 
competitiveness. The RP is a 3D printing process which forms items by adding plastic 
material layers [7]. The RP systems are relatively faster and cheaper than traditional 
machine tools. The main objective is the realization of early physical prototypes, in 
order to validate the model performance with a low manufacturing cost of pilots. The 
advantage of using the RP technique regards the product development in order to 
reduce the time-to-market. While the use of RP tools reduces the pilot cost and the 
related manufacturing time, the number of the experimental tests and the necessary 
pilots is uncertain. The design of kitchen hoods concerns many geometrical parame-
ters which are correlated with each other, so an RP approach has to provide several 
3D printed models to achieve the required characteristics. 
A design of experiments (DOE) approach is the solution to plan the physical tests 
according to the paradigm of Robust Design. This second design approach allows the 
pilot manufacturing and related experiments to be reduced. In particular, through the 
formulation of orthogonal matrices provided by Taguchi’s method, it is possible to 
investigate a large number of parameters and to obtain important results with a mini-
mum number of experiments [8][9]. This approach is suitable for the kitchen hoods 
application in order to support the engineer in the main parameter definition and to 
plan a little quantity of tests and prototypes. After the experiment analysis, the conse-
quent result is the optimum parameter configuration which satisfies the specifications. 
Therefore, this approach requires a certain number of physical experiments, depend-
ing on the number of parameters. Therefore, an investigation of many parameters (n) 
requires a similar number of experiments (n+1). 
The third available solution to increase impeller design is the introduction of simu-
lation tools for a computational fluid dynamics (CFD) analysis [10]. Even if the fan 
geometry is simple, a fluid dynamic study provides important information to improve 
hood performance and the airflow distribution in the kitchen [11]. In order to evaluate 
the airflow distribution and the local temperature gradient, these tools also support the 

 
A Virtual Prototyping Approach Based on DOE Analysis 
433 
designer in the study of innovative solutions for improving the air quality in domestic 
kitchens. The main difficulty concerns studying the  turbulent air flow elaborated by a 
rotating wheel [12].  
In literature it is possible to find many examples of virtual prototyping methods, 
rapid prototyping approach and DOE analysis, but the difficulty lies in integrating 
these methods into a design process for product development. 
This paper aims to validate a design methodology to support the designing phases 
for centrifugal impellers. This research work proposes a DOE (Design of Experi-
ments) approach based on virtual simulations, which are planned with an L4 orthogo-
nal array in following Taguchi’s theory. The proposed analysis allows the designer to 
define an optimum parameter configuration in order to maximize the inlet flow rate 
and efficiency. Then, the rapid prototyping tools enable a quick pilot in order to com-
pare the simulation with real-world physical tests. The main objective is the reduction 
of time and cost due to trial and error experiments. 
3 
Methodology 
This section presents the methodology studied to support the fan wheel design for a 
kitchen hood. The achieved methodology integrates three different levels of analysis: 
virtual prototyping, Design of Experiment (DOE) and rapid prototyping. The virtual 
analysis allows the geometrical parameterization of a simplified model to be defined 
and simulates the performance of each configuration at working conditions (Figure 1).  
The proposed DOE approach is based on virtual experiments according to the ne-
cessity of reducing time and costs during the first design phase. The optimum parame-
ters configuration, defined by the previous step, is useful to define the geometry of a 
reliable virtual model. The final level is the realization of a 3D ventilator with a rapid 
prototyping printer. The obtained component is now evaluable at the test bench to 
investigate the air flow rate and the electric power consumption. 
The virtual prototyping level concerns the phases of model simplification, geome-
trical parameterization and virtual simulation. The simplification of the virtual model 
is the first step where early analysis identifies the less important geometrical entities. 
At this level the engineer interacts with CAD tools to reduce the geometrical com-
plexity of the real model. The resulting geometry is a closed volume which excludes 
through holes, threads, small fillets and chambers, electrical components, etc. The 
simplified model does not consider the geometrical details which not influence the 
fluid dynamics analysis. The next step includes the parameterization of the main 
geometrical dimensions. Therefore, the parameter choice is related to the DOE analy-
sis which requires an orthogonal array to plan the virtual experiments. 
The DOE approach guides the analysis of virtual simulation  by identifying a cer-
tain number of parameters which influence the performance. The simulation basically 
regards CFD analysis which reproduces system behaviour without physical manufac-
turing. The DOE level provides the experiment plan definition related to the parame-
ters chosen in virtual modelling. The engineer can use his know-how to set the  
parameter range and to evaluate the most suitable configuration. According to Ta-
guchi’ methods, a reduced number of experiments is required to elaborate the final  
 

434 
P. Cicconi, D. Landi, and M. Germani 
optimum condition. Each test includes a combination of the set values in order to 
investigate the influence of  each parameter. The objective function includes two  
levels of specifications: the maximization of air flow rate and the reduction of the 
necessary motor torque, which means the efficiency increasing with low energy con-
sumption. Analysing the CFD results, it is possible to evaluate the optimum condition 
and to simulate the elaborated configuration. 
 
 
Fig. 1. The research methodology 
The result of the DOE approach, including the virtual experiments,  provides a bet-
ter parameter configuration. The elaborated settings could be simulated to evaluate 
the performance with virtual tools. The next step is the rapid prototyping of the better 
candidate parameter configuration, using a 3D printer tool. At the test bench, the real 
world experiments can confirm the final analyzed configuration of the printed model. 
4 
Test Case 
The proposed approach has been validated during the design of a fan wheel for do-
mestic hoods in collaboration with a large company working on kitchen hoods. The 
applicative target concerns the fluid dynamic optimization of an existing blower in 
order to increase ventilator efficiency, air flow rate and to reduce energy consump-
tion. The optimization process, described below, concerns the investigation of the 
wheel blades, while the fan volute and wheel dimensions are fixed for this  
application. 
 

 
A Virtual Prototyping Approach Based on DOE Analysis 
435 
The model simplification is the first step in the CFD analysis. This activity con-
cerns the reproduction of a less detailed model including the essential parts useful for 
the fluid dynamic analysis. Figure 2 shows the difference between a detailed model 
and a related simplified body for the proposed test case. However, the virtual simpli-
fied model includes the original geometry of blade shape and internal volute, and the 
parametric relations to support the configuration changes. The geometry characteriza-
tion is important to reduce the compute time due to the model complexity. 
 
  
Fig. 2. A comparison between the initial and the simplified model  
During this phase, it is fundamental to analyze the characteristic curves in order to 
evaluate the motor operating point at the free-delivery condition. This supports the 
engineer in the early design phase of the impeller-volute system. The operating point 
of the blower system is the intersection between two characteristic curves:  the torque 
of the electric motor and the fan prevalence. Defining a motor behavior, a blade geo-
metry has been investigated to increase air-flow rate and reduce the resistance torque.  
4.1 
Geometrical Parameterization and DOE Approach 
Following the proposed methodology, a DOE plan is defined to organize the required 
virtual experiments. Table 2 reports the L4 orthogonal array analysed in this test case. 
The three chosen parameters (Figure 3) are: the blade number (P1), the blade outlet 
angle (P2) and the blade chord angle (P3). Each parameter can hold two possible con-
figurations (level -1 and level +1), and the total number of experiments is 2n (4 in this 
case), where n is the number of investigated parameters.  
The objective function takes into account the mass flow rate and the necessary 
torque, assigning the same weight (0,5) to each criterion. 
ܻ=0,5·ܥݎ1+0,5·ܥݎ2 
(1)
The term ܥݎ1 identifies the criterion related to the mass flow rate, calculated with the 
method Big Is Better (BiB), while the term ܥݎ2 concerns the necessary torque and is 
evaluated with the Small Is Better (SiB) methods. This is because the increasing mass 
flow-rate is a desired effect, whilst the resistance torque is a value which has to be 
reduced to optimize electricity consumption. 
 

436 
P. Cicconi, D. Landi, and M. Germani 
 
Fig. 3. The geometrical parameters investigated in the proposed test case: the blade number 
(P1), the blade outlet angle (P2) and the blade chord angle (P3) 
Table 1. DOE parameters defined for the proposed test case 
Parameters 
Initial value 
Level -1 
Level +1 
Blade number 
55 
50 
60 
Blade outlet angle 
125° 
120° 
130° 
Blade chord angle 
7° 
5° 
9° 
4.2 
Virtual Experiments 
The need to reduce the design cost and the time-to-market leads designers to reduce 
the number of experimental tests and promotes the use of virtual simulations in order 
to predict the behaviour of a real system without building the physical prototype. 
After the model simplification, shown in previous sections, the mathematical geome-
try has been meshed in many elementary elements. Figure 4 shows the discretized 
model  with different levels of thickening. A thick mesh provides a simulation close 
to real world behaviour, but which increases computational resources. 
 
 
Fig. 4. The mesh related to the test case model 
The last column of Table 2 reports the DOE results calculated using Equation 1 
with the related criteria. Figure 5, 6 and 7 show the variation of the objective function 
values depending on analyzed parameters (P1, P2, P3),  and the major inclination 
indicates a more important impact on the system. The maximum is obtained by  

 
A Virtual Prototyping Approach Based on DOE Analysis 
437 
positioning parameter P1 on level -1, P2 to level +1 and P3 to level -1. Thus, the op-
timal configuration provides 55 blades, a blade outlet angle of 130 °, and a blade 
chord angle of 5 °. This condition has to maximize the air-flow rate and the  
efficiency. 
 
 
Fig. 5. The incidence of P1 parameter on the objective function 
 
Fig. 6. The incidence of P2 parameter on the objective function 
 
Fig. 7. The incidence of P3 parameter on the objective function 

438 
P. Cicconi, D. Landi, and M. Germani 
Table 2. The orthogonal L4 array 
Experiment 
P1 
P2 
P3 
Y 
1 
-1 
-1 
-1 
0,33 
2 
-1 
+1 
+1 
0,48 
3 
+1 
-1 
+1 
0,28 
4 
+1 
+1 
-1 
0,48 
4.3 
The Optimum Configuration 
For each experiment reported in Table 2, the air-flow rate and the resistance torque 
values have been calculated through using the previously described Equation 1. Bear-
ing in mind the previous consideration, Table 3 shows the optimal condition calcu-
lated for the proposed test case. The best configuration candidate has been first  
verified through a virtual simulation and then tested at the test benches with a 3D 
printed model (Figure 8). Figure 9 shows the trend of pressure (a) and the velocity 
vectors (b), which are obtained through the virtual simulation with a CFD tool. This 
report is also useful to investigate the velocity field in the more critical zones in order 
to maximize the efficiency of the system. The test bench used for air-flow rate detec-
tion is comply with the standard ISO 5167-1 “Measurement of fluid flow by means of 
pressure differential devices inserted in circular cross-section conduits running full”. 
 
 
Fig. 8. The test case prototype printed with a RP machine 
The chosen configuration concerns 50 forward blades optimized to maximize the 
air flow rate and the efficiency. As reported in Table 3, the angle of blade outlet is 
130° in forward direction, while the chord angle is 5° related to the center of the 
wheel. The virtual model achieves an air flow rate of 875 m3/h and a torque of 4.95 
N·cm. This result is better than the previous configurations. After the virtual valida-
tion, a rapid prototype (Figure 8) has been printed to confirm the simulated result. The 
real air flow tester provides a maximum value of 840 m3/h, with a related torque of 
5.25 N·cm. Thus, the CFD analysis includes an average error of 5% and can be used 
as alternative to physical prototyping. 

 
A Virtual Prototyping Approach Based on DOE Analysis 
439 
Table 3. The best parameter configuration and performance result after CFD analysis and real 
testing on rapid prototype printed. 
Blade 
number 
Blade 
outlet 
angle 
Blade 
chord 
angle 
CFD results 
Prototype Test   
Results 
Airflow 
[m3/h] 
Torque 
[N·cm] 
Airflow 
[m3/h] 
Torque 
[N·cm] 
50 
130° 
5° 
875 
4,95 
840 
5,25 
 
 
                           (a)                                                             (b) 
Fig. 9. Some CFD reports: a) total pressure; b) velocity vectors 
5 
Conclusions  
The proposed approach has been validated during the design of a fan wheel for do-
mestic cooker hoods. The main geometrical parameters have been analysed and has 
been defined a L4 orthogonal array in order to plan the necessary virtual experiments. 
Using a CFD tool has been simulated the fluid dynamic performance for each planned 
experiments. The geometrical model, which includes the impeller and its volute, has 
been previously simplified using a parametrical approach. An objective function has 
been formulated in order to give a value to each virtual experiments. The valuating 
criteria are based on air flow rate and resistance torque. After the simulation compu-
ting, an optimum configuration  has been found. The new configuration has been first 
validated by a CFD analysis and then printed with a rapid prototyping machine. The 
physical prototype has confirmed the CFD results with a variation of almost 5%. 
The use of the DOE method allows a decreases in the number of experiments, 
while the introduction of virtual prototyping reduces pilot manufacturing. The use of 
rapid prototyping for the final impeller configuration results the best solution to verify 
quickly the effective result. 
As a future development, the proposed methodology will be extended to the design 
of a new fan for domestic kitchen hood. In particular a different type of volute and 
impeller will be analysed in order to validate the design approach. This future test 
case would include a L8 orthogonal matrix with more parameters. 

440 
P. Cicconi, D. Landi, and M. Germani 
Acknowledgments. The authors wish to thank Faber SPA for their precious contribu-
tion in the development of this research program. 
References 
1. Regulating Energy Supranationally: EU Energy Policy. European Yearbook of Interna-
tional Economic Law 3(pt. 1), 371–404 (2012) 
2. Gynther, L., Mikkonen, I., Smits, A.: Evaluation of European energy behavioural change 
programmes. Energy Efficiency 5, 67–82 (2012) 
3. Paetz, A., Dütschke, E., Fichtner, W.: Smart Homes as a Means to Sustainable Energy 
Consumption: A Study of Consumer Perceptions. J. Consum. Policy 35, 23–41 (2012) 
4. Charytonowicz, J., Latala, D.: Evolution of Domestic Kitchen. In: Stephanidis, C. (ed.) 
HCII 2011 and UAHCI 2011, Part III. LNCS, vol. 6767, pp. 348–357. Springer, Heidel-
berg (2011) 
5. Zhao, K., Zhou, X., Zhao, B.: Cooking generated particles’ impact on indoor air quality of 
university cafeteria. Build Simul. 3, 15–20 (2010) 
6. Choi, S.H., Chan, A.M.M.: A virtual prototyping system for rapid product development. 
Computer-Aided Design 36, 401–412 (2004) 
7. Ding, Y., Lan, H., Hong, J., Wu, D.: An integrated manufacturing system for rapid tooling 
based on rapid prototyping. Robotics and Computer-Integrated Manufacturing 20, 281–
288 (2004) 
8. Taguchi, G.: System of experimental design: Engineering methods to optimize quality and 
minimize costs. UNIPUB/Kraus International Publications (1987) 
9. McCreary, M.L.: Tips and Tricks for Using Simulation Doe to Assess the Complex Inte-
ractions of Your Process. In: Proceedings of Simulation Conference Winter (2007) 
10. Eberlinc, M., Sirok, B., Hocevar, M., Dular, M.: Numerical and experimental investigation 
of axial fan with trailing edge self-induced blowing. Forsch Ingenieurwes 73, 129–138 
(2009) 
11. Lim, K., Lee, C.: A numerical study on the characteristics of flow field, temperature and 
concentration distribution according to changing the shape of separation plate of kitchen 
hood system. Energy and Buildings 40, 175–184 (2008) 
12. Abanto, J., Reggio, M.: Numerical investigation of the flow in a kitchen hood system. 
Building and Environment 41, 288–296 (2006) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 441–450. 
DOI: 10.1007/978-3-642-30817-8_43          © Springer-Verlag Berlin Heidelberg 2013 
Towards a Context-Driven Front End in New Product 
Development 
Christer W. Elverum and Torgeir Welo 
Department of Engineering Design and Materials, NTNU, Richard Birkelands vei 2B,  
Trondheim, Norway 
{christer.elverum,torgeir.welo}@ntnu.no 
Abstract. The fuzzy front end (FFE) of new product development (NPD) has 
been an important area of research during the last decade. The FFE is recog-
nized by several researchers as the part of the innovation process where most 
substantial improvements can be achieved with minimum cost. This paper in-
vestigates former research to determine if there is a common perception of the 
FFE. Referring to state of the art and the nature of the FFE, it is argued that an 
‘ideal’ FFE model brings limited value to a firm. A number of contextual fac-
tors that influence the characteristics of the FFE are presented to further argue 
this finding. That is, the context is far too important to be overlooked in the 
search for a FFE strategy. 
Keywords: fuzzy front end, innovation, concept development. 
1 
Introduction 
Innovation is vital for survival of any organization in the 21st century. Globalization 
has forced companies to bring products to market faster than ever before. With the 
emergence of low-cost countries, such as India and China, advanced manufacturing 
has become a commodity, forcing companies that operate in high-cost countries to 
create products with higher customer value than its competitors operating in low cost 
countries. In order to achieve increased customer value, it is necessary to innovate, 
and develop new and better solutions than its competitors. To quote the CEO of SRI 
International, Curtis Carlson: “The goal of every innovation is to create and deliver 
customer value that is greater than the competitors” [1]. 
According to Koen and Ajamian [2], the innovation process can be divided into 
three stages: the fuzzy front end (FFE), the new product development (NPD) 
(process) and the commercialization stages. The NPD process has been under exten-
sive research for several decades. The focus has in large parts been on creating an 
ideal process for developing and managing the development of new products. Well-
known approaches such as Robert G. Cooper’s Stage-Gate® process [3] and the 
PACE® approach have been used for managing the product development process with 
great results [4]. For the FFE, however, a well-established, commonly accepted 
process does not seem to exist.  

442 
C.W. Elverum and T. Welo 
This paper considers different FFE models proposed by several authors and com-
pares them to see where there is agreement and where there is not. Answers to the 
following research questions are sought out: 
RQ1. Is there a common understanding of what the FFE is, where it starts, where it 
ends and what activities it should include to create products of superior value to cus-
tomers?  
RQ2. Can the FFE be generalized to the degree that it is covered by one single model? 
What are the dimensions that make this possible or impossible, and how are they re-
lated to each other? 
2 
Former FFE Research 
The FFE is a term popularized by Smith and Reinertsen [5] and is most commonly 
defined as the portion of the new product development cycle between when work on a 
new idea could start and when it actually starts [6]. The phase is described as rather 
chaotic and unstructured [7]. 
Several authors have identified the FFE as the part of the innovation process where 
the most substantial product improvements and cost reductions can be made [8-11]. 
Extensive research has been conducted in the FFE field in the recent years. Case studies 
have brought into light how the phase is actually conducted in certain (type of) indus-
tries, for example the automotive industry [8], aerospace [12] and process firms [13]. 
Numerous attempts for developing a common language and a framework have been 
made [14-16]. It is argued that it would make it easier for researchers to build upon one 
another’s work if there is a common understanding of the definition and a framework 
for the FFE. To be able to compare different models it is necessary to distinguish be-
tween two different approaches for investigating the FFE. Most of the research deals 
with the FFE within the company – best practices, tools, activities and so on, while Reid 
and de Brentani [15] propose a theoretical model that explains how information flows in 
radical innovations. They thus explain how radical innovation occurs by examining the 
initiation of the innovation. They suggest that information flows in the opposite direc-
tion of incremental innovations; that is, from the outside environment into the firm. This 
paper will for the most part focus on FFE research within the company sphere, compar-
ing the different models proposed by various authors. 
There appears to be at least two common ways of viewing the FFE within a firm. 
One way to interpret and model the phase is as a structured process where there exists 
an ‘ideal process’. The other is as a more contextual, situational-driven view where 
there does not exist one ideal FFE model. Several authors argue for the process view 
of the FFE. Khurana and Rosenthal [17] divide the front end for incremental innova-
tions into three phases: pre-phase zero, phase zero and phase one without implying 
that there might occur feedback or iterations between and within the phases. Even 
though they state that the front end process needs to be adapted to the product, mar-
ket, etc., the model presented is still a process view of the front end.  
One example from the other end of the spectrum is the framework presented by 
Koen and Ajamian [14]. Their model takes into account that the FFE is indeed fuzzy 
and unstructured. Suggesting that there are five elements driving the front end: oppor-
tunity identification, opportunity analysis, idea genesis, idea selection and concept & 

 
Towards a C
technology development. In
from the outside environm
agement. This model does 
ments proceed in a random
and Kohn [16] is perhaps th
research. It is based on ove
for front end activities and 
result of the work is a FFE
fied illustration. The learni
surrounded by factors that c
as senior management, corp
FFE team, etc. Further awa
controllable by the manage
and organizational macro s
ers, age, market dynamic, e
research institutions and un
 
Fig. 1. Sim
Tab
Author  
Cooper 
(1983) 
 
 
[19] 
a
Activities 
 
Opportunity 
identification 
 
Idea generation 
x 
Concept generation 
 
Concept screening 
 
Concept definition 
x 
Business analysis 
x 
Project planning 
 
Context-Driven Front End in New Product Development 
n addition to the five elements there are influencing fact
ent and the support from senior and executive-level m
not present the FFE as a straight-forward process; the e
m and iterative fashion. The framework proposed by Hü
he most comprehensive one in terms of building on form
er sixty empirical NPD studies that identify relevant fact
thirty specific conceptual and explorative FFE papers. T
E framework for future research; see Figure 1 for a sim
ing organization is placed in the center of the framewo
can (to some extent) be controlled by the corporation, s
porate creativity, horizontal co-operation, cross functio
ay from the center, the contextual factors get gradually l
ement. These factors include the firm context: its age, s
structure, industry context: structure, competitors, custo
etc., and finally the macro environment: natural resourc
niversities, regulation and so on. 
 
mplified illustration of FFE framework by [18] 
ble 1. Activities in different FFE models 
Nobelius 
and Trygg 
(2002) 
 
[20] 
Khurana and 
Rosenthal (1998) 
 
 
[17] 
Koen et al. 
(2001) 
 
 
[14] 
Ulrich and 
Eppinger 
(2012) 
 
[21] 
Hüs
and
Koh
(200
[16
 
 
 
 
 
 
x 
x 
 
x 
 
x 
x 
 
x 
x 
x 
“Product Concept” 
x 
“Concept and 
Technology 
Development” 
x 
 
x 
x 
 
x 
 
x 
x 
 
 
 
x 
x 
x 
x 
 
x 
x 
443 
tors 
man-
ele-
üsig 
mer 
tors 
The 
mpli-
ork, 
uch 
onal 
less 
size 
om-
ces, 
sig 
d 
hn 
03) 
6] 

444 
C.W. Elverum and T. Welo 
When different FFE models are compared, it has to be taken into account that the 
authors use different terms and provide various degree of details, thereby making it 
difficult to compare them directly; in other words, a common language does not exist. 
An example of this can be seen in Table 1. All concept activities have been merged 
into one cell for [17]. The reason for this is that they vaguely describe one of their 
phases as “Product Concept”, indirectly implying that all of the concept development 
activities have to be conducted in order to end up with a product concept. As the table 
indicates, there is no consensus on the activities included in the FFE. [21] provide a 
schematic overview of the different phases in the entire product development cycle, 
starting with planning and ending with production ramp-up. Phase 1 – Concept De-
velopment is described as ‘The Front End Process’ and is clearly separated from op-
portunity identification. The same goes for the synthesized model by [20], i.e., the 
opportunity identification is treated as an input instead of a part of the front end itself. 
The concept seems to be the most important and agreed upon part of the FFE. Even 
though it is not consistent what part of the concept development each of the authors 
includes, it is recognized as an important activity in all of the models.   
3 
Discussion 
3.1 
The Importance of Context 
Taking the Context into Consideration 
Despite the attempts of creating a unified model, there is no consensus of exactly how 
the FFE looks like, and what elements it consists of. However there does, to some 
degree, seem to be an agreement of the definition of when the FFE starts and ends. 
The FFE starts with an idea or opportunity, and ends with a go/no-go decision. Some 
of the authors have, however, treated the opportunity identification as a part of the 
front end, see Table 1. 
Whether the phase should be treated as a structured process, or if a less structured 
guiding framework is more suitable, is an ongoing discussion. Nobelius and Trygg 
[20] argue that there is no point in chasing the front end process. Through a case study 
they show that each individual project is too different to be adapted to one single 
model. Hüsig and Kohn [16] recommend a stronger focus on the context to see how 
changes to various parameters affect the FFE. They also indicate that the future re-
search challenges could be found on a more micro level. For instance, the influence of 
factors like type of technology, size of project and interrelations between projects in a 
historical perspective needs further investigation. Based on the statements and the 
former research discussed above, it is argued that a common FFE model will be of 
limited value to a firm. The context (type of industry, level of innovation etc.) is of 
such great importance that if the FFE were to be generalized across several types of 
industries and products, this would have to be on such a basic level that it would re-
sult in a superficial model, unable to satisfy any firm or industry. The nature of the 
FFE is vastly different than the more structured NPD-process [2], and most likely less 
suitable for standardization. This does not, however, imply that there cannot be a 
somewhat common perception of what the FFE is; i.e. where it starts, where it ends, 
and to a certain degree what the activities and outcome should be. A common  

 
Towards a Context-Driven Front End in New Product Development 
445 
language for the researchers will be favorable in terms of bringing the research for-
ward but will most likely not give a firm within a specific industry the level of details 
needed to improve its front end phase.  
There are most likely numerous of important contextual factors that will affect the 
FFE, but it is impossible to take all factors into consideration here. Therefore, the 
purpose of the following discussion is to emphasize some of the key factors that will 
play a major role in the front end. The elements presented in the next section are 
meant to highlight some of the product and innovation specific contextual factors. The 
scale presented for each of the elements is by no means to be considered absolute or 
definite. The purpose is to illustrate the span and the variation of each element by 
presenting its extremes. Their interdependence and actual influence on the front end 
need further investigation and validation.  
3.2 
Contextual Factors 
Degree of Innovation 
Different levels of innovation require different approaches to the FFE. Before the 
importance of degree of innovation can be discussed, however, it is necessary to de-
fine the various degrees of innovation.  
Following the definitions used by [15] there are three different degrees of innova-
tion, incremental, “really new” and “radically new”. Incremental innovations typically 
involve product improvements using existing technologies targeted toward existing 
markets. To be classified as a “really new” innovation, changes to existing technolo-
gical or marketing infrastructure are required, while “radically new” implies changes 
to both. Discontinuous innovation is a term that covers “really new” and “radically 
new” innovations [22], the definitions are summarized in Figure 2. 
 
 
Fig. 2. Visual presentation, the various degrees of innovation 
According to de Brentani and Reid [23] “discontinuous innovations typically are 
not the result of explicit and structured organizational processes, as is the case for 
incremental innovations.” Their research shows that discontinuous innovations are 
more dependent on contact with the environment through individuals called ‘Boun-
dary Spanners’ at the Boundary Interface, the interface between the firm and the out-
side environment. Firms that operate in the incremental innovation area acquire most 
of the knowledge needed from within the firm, thus they need to focus more on opti-
mizing internal parts and processes to be successful at innovating. For firms operating 

446 
C.W. Elverum and T. Welo 
in the discontinuous innovation area, on the other hand, the interface with the outside 
environment is the most critical part to focus on. Most former research uses the  
degree of innovation to separate different firms. Khurana and Rosenthal [17], for 
example, investigate companies that range from consumer packaged goods to elec-
tronics and industrial products in the same case study. This might be useful for creat-
ing a generalized model that explains what the FFE is and to a certain degree what 
activities it consists of. However, it might not be specific enough to explain how to 
conduct the FFE. 
Concept Driving-Force 
The concept driving-force may affect the entire innovation process and is closely tied 
to the customer characteristics and the degree of innovation. Backman and Borjesson 
[8] report that customer and market concepts (ones that are less technology-driven) 
need a contextualization to be more easily understood and granted funding for further 
development. Contextualization in this case, according to Backman and Borjesson [8], 
means that ”the concept is dressed in a way to fit an NPD context, facilitating both 
understanding and evaluation.” In their case-study, one example of contextualization 
is the development of a concept car. By placing the less technology-driven concept in 
a larger context, there is a greater chance that its value will be recognized by the man-
agement and continue into the NPD process. This finding suggests that there might be 
different optimal approaches based on whether the concept is technology-driven or 
based on customer or market knowledge. They report that pure technology-driven 
products found their way into the development cycle considerably easier. This has 
several possible explanations, one of them being that the NPD process and manage-
ment in the organization studied are used to evaluating technological concepts. How-
ever, if a generalization is to be made, it might also be due to the fact that it is easier 
to convey and argue for a new, tangible technology where you can clearly see the 
benefit. 
Customer Characteristics 
The customer is always crucial part in new product development, whether this is 
through direct or indirect involvement. Without a customer there is no need for a 
product. The translation of customer needs into product requirements may require 
deep customer understanding. The method for acquiring the needed knowledge de-
pends on several factors; degree of innovation is one of them. It is obvious that an 
incremental improvement of an existing product, driven by a single factor such as 
price, weight, new material etc. requires a vastly different approach and user-insight 
than creating a radically innovative product with new technology for new market 
segments. In the former case, the firm already has a lot of knowledge and experience 
from managing similar products and a structured approach might be satisfactory. For 
radical innovation, on the other hand, more uncertainty and a great deal of risk is 
usually involved, and the validity of a concept is related to uniqueness and variation 
[24]. Flint [25] discusses the importance (and lack of) building the voice of the cus-
tomer into the FFE. He claims that one of the standard brainstorming rules ‘quantity 
breeds quality’ leads firms into believing that they have too many ideas, hence assum-
ing that the ideation stage is not the problem. He states that it is not only about  

 
Towards a Context-Driven Front End in New Product Development 
447 
coming up with creative ideas but incorporating deep customer understanding into the 
ideation stage. The nature of customer’s needs and wants can be placed within the 
scale ranging from overt to latent [26]. The more latent the needs are, the harder it is 
to uncover what the customer actually wants. These kinds of needs can be tied to 
more radical innovations, where the market segment is either new or the existing one 
is transformed [27].  
 
 
Fig. 3. (a) Contextual factors, product, need and technology. (b) Innovation, customer needs 
and technological feasibility. 
Summary of Contextual Factors 
To improve the FFE, a deeper understanding of the factors influencing the phase is 
needed. Some of the factors that will influence the characteristics of the FFE have 
been discussed in this paper. To summarize, the findings have been placed together in 
Figure 3. The need and the technology enable a product to be created. In order to 
generate the product requirements, customer needs have to be identified, understood 
and translated. The customer needs can range from being overt to latent, meaning that 
a range of various tools and methods may be required in the front end in order to un-
cover the needs, depending where on the scale one is located. The driving-force for 
creating a product can range from market/customer-driven to pure technology-driven. 
As discussed earlier, the level of contextualization needed varies with the driving-
force of the concept. Technological feasibility reflects something about how technol-
ogically demanding the product is. This is a factor that is strongly correlated with the 
degree of technology innovation, and consequently affects the FFE. 
According to several researchers it seems like the degree of innovation to a large 
extent dictates the characteristics of the FFE. [15, 17, 28] are some of the authors who 
use the degree of innovation to differentiate between companies. The degree of inno-
vation is determined by numerous of factors. The customer’s needs and the technolo-
gical feasibility can be linked to each axis of the degree of innovation, as illustrated in 
Figure 3 (b). For discontinuous innovations in terms of technology, it can be directly 
connected to complex technology feasibility for the product. For discontinuous  

448 
C.W. Elverum and T. Welo 
innovations in terms of generating a new or transforming the market, it is most likely 
that the customer needs are well hidden, requiring a substantial effort to be uncovered. 
Hence, they are latent needs, according to [27]. 
3.3 
A Framework and a Definition 
Two Different Stakeholders 
Throughout this paper it has been discussed that there are inconsistent perceptions of 
the FFE among various researchers and a lack of details in the models when it comes 
to providing firms with information on how to conduct and improve the FFE. Based 
on these findings it seems appropriate to differentiate between two different stake-
holders: the research community and the industry. These two stakeholders have dif-
ferent needs and goals, making it difficult to satisfy both with one single approach.  
To bring the research forward it is at first important to define, understand and de-
scribe the FFE on a less detailed, basic level. This will not be of any direct value to 
the end users (the industry), but it will ensure that researchers speak the same lan-
guage and stop confusion about terms and contents within the FFE. This is a topic 
discussed by both [14] and [16]. It would be more convenient and effective conduct-
ing research within the FFE field if there was a common language and an agreed upon 
definition of the FFE and its activities. The point being that there is one agreed upon 
definition, today most researchers use their own definition. This is imprecise, confus-
ing and problematic when it comes to comparing different models and discussing the 
FFE in a general context. Furthermore, the boundary and the activities of the FFE 
should be defined. The purpose is not to go into detail of each of the activities; it is to 
define what is considered a part of the FFE and what is not. The outcome of the FFE 
should be ‘something’ that allows the firm to make a thorough decision whether to 
launch or kill the project. That is, after all, the main purpose of the FFE; to evaluate 
whether an opportunity is worth pursuing or not.  
To provide the industry with valuable applied research, the contextual factors have 
to be taken into account to create a more detailed, (custom-)tailored approach. It is 
necessary to dig deeper into the details to understand how to improve the front end; 
that is, it is not sufficient to describe what it is and what the inputs and outputs are. 
4 
Summary, Conclusion and Future Research 
Former research has been investigated by comparing the work of various researchers. 
It is evident that there is no agreement on what activities the FFE consists of, or 
whether the phase should be regarded as a process or a less structured framework. 
Several arguments support the assumption that an ‘ideal’ FFE model is not optimal 
and that a less structured framework is more suitable. Some of the contextual factors 
that seemingly cause the FFE to be less suitable for standardization are presented. 
Furthermore, due to different needs, it has been suggested to differentiate the research 
between two different groups of interests. For the research community a simplified 
model defining what the FFE is to create a common language among the researchers 
 

 
Towards a Context-Driven Front End in New Product Development 
449 
is suggested. For the industry, on the other hand, a more contextual approach is pro-
posed; to satisfy the specific firms in terms of improving the FFE by investigating 
how to best conduct the phase. 
Based on the identified inconsistencies associated with definition of the FFE, the 
following future research is suggested:  
• Develop a definition of the FFE and a common language for the researchers; a 
definition of what the FFE is, where it starts and where it ends. This will prevent 
each researcher from making his or her own definition of the FFE. This again will 
make it more manageable to compare research and bring research forward. This 
definition has to be on such a basic level that it is general and not affected by con-
textual factors. 
• Identify the industry, company and project-specific factors, and how they relate to 
one another. Some factors are proposed in this paper, but far more research is ne-
cessary to examine, test and validate the factors with regard to their individual or 
collective influence on the FFE. 
References 
1. Carlson, C.R., Wilmot, W.W.: Innovation: The Five Disciplines for Creating What Cus-
tomers Want. Crown Business, US (2006) 
2. Koen, P., et al.: Fuzzy Front End: Effective Methods, Tools, and Techniques. In: Belli-
veau, P., Griffin, A., Somermeyer, S. (eds.) The PDMA ToolBook for New Product De-
velopment 1, pp. 5–35. John Wiley & Sons, Inc. (2002) 
3. Cooper, R.G.: Winning At New Products. KOGAN PAGE, New York (1988) 
4. Koen, P.: The Fuzzy Front End for Incremental, Platform, and Breakthrough Products. In: 
Kahn, K.B. (ed.) The PDMA Handbook of New Product Development, pp. 81–91. John 
Wiley & Sons, Inc., New York (2004) 
5. Smith, P.G., Reinertsen, D.G.: The Strategist’s Role in Shortening Product Development. 
The Journal of Business Strategy 12(4), 18–22 (1991) 
6. Reinertsen, D.G.: Taking the fuzziness out of the fuzzy front end. Research-Technology 
Management 42(6), 25–31 (1999) 
7. Breuer, H., Hewing, M., Steinhoff, F.: Divergent Innovation: Fostering and Managing the 
Fuzzy Front End of Innovation. In: Proceedings of Picmet 2009 - Technology Manage-
ment in the Age of Fundamental Change, vols. 1-5, pp. 744–751 (2009) 
8. Backman, M., Borjesson, S., Setterberg, S.: Working with concepts in the fuzzy front end: 
exploring the context for innovation for different types of concepts at Volvo Cars. R & D 
Management 37(1), 17–28 (2007) 
9. Chang, H.W., Wei, C.C., Lin, R.J.: A model for selecting product ideas in fuzzy front end. 
Concurrent Engineering-Research and Applications 16(2), 121–128 (2008) 
10. Kiewiet, D.J., Van Engelen, J., Achterkamp, M.: The fuzziness of the fuzzy front end: the 
influence of non-technical factors. In: ISMOT 2007: Proceedings of the Fifth International 
Symposium on Management of Technology, vol. 1 and 2, pp. 330–333 (2007) 
11. Kim, J., Wilemon, D.: Focusing the fuzzy front-end in new product development. R & D 
Management 32(4), 269–279 (2002) 

450 
C.W. Elverum and T. Welo 
12. Corallo, A., et al.: Enhancing Knowledge sharing in “Fuzzy Front End” of NPD: An Aero-
space Case Study. In: Proceedings of the 11th European Conference on Knowledge Man-
agement, vol. 1 and 2, pp. 288–295 (2010) 
13. Kurkkio, M., Frishammar, J., Lichtenthaler, U.: Where process development begins: A 
multiple case study of front end activities in process firms. Technovation 31(9), 490–504 
(2011) 
14. Koen, P., et al.: Providing clarity and a common language to the “Fuzzy Front End”. Re-
search-Technology Management 44(2), 46–55 (2001) 
15. Reid, S.E., de Brentani, U.: The fuzzy front end of new product development for disconti-
nuous innovations: A theoretical model. Journal of Product Innovation Management 21(3), 
170–184 (2004) 
16. Hüsig, S., Kohn, S.: Factors influencing the front end of the innovation process: A com-
prehensive review of selected empirical NPD and explorative FFE studies. In: 10th Inter-
national Product Development Management Conference 2003, Brussels, Belgium (2003) 
17. Khurana, A., Rosenthal, S.R.: Towards holistic “front ends” in new product development. 
Journal of Product Innovation Management 15(1), 57–74 (1998) 
18. Hüsig, S., Kohn, S.: Factors influencing the front end of the innovation process: A com-
prehensive review of selected empirical NPD and explorative FFE studies. In: 10th Inter-
national Product Development Management Conference 2003, Brussels, Belgium, p. 15 
(2003) 
19. Cooper, R.G.: A Process Model for Industrial New Product Development. IEEE Transac-
tions on Engineering Management 30(1), 2–11 (1983) 
20. Nobelius, D., Trygg, L.: Stop chasing the Front End process — management of the early 
phases in product development projects. International Journal of Project Manage-
ment 20(5), 331–340 (2002) 
21. Ulrich, T.K., Eppinger, D.S.: Product Design and Development. McGraw-Hill (2012) 
22. Garcia, R., Calantone, R.: A critical look at technological innovation typology and innova-
tiveness terminology: a literature review. Journal of Product Innovation Manage-
ment 19(2), 110–132 (2002) 
23. de Brentani, U., Reid, S.E.: The Fuzzy Front-End of Discontinuous Innovation: Insights 
for Research and Management. Journal of Product Innovation Management 29(1), 70–87 
(2012) 
24. Martin, R.L.: The Design of Business: Why Design Thinking is the Next Competitive Ad-
vantage. Harvard Business School Press, Boston (2009) 
25. Flint, D.J.: Compressing new product success-to-success cycle time - Deep customer value 
understanding and idea generation. Industrial Marketing Management 31(4), 305–315 
(2002) 
26. Vonhippel, E.: Customer-Active Paradigm for Industrial Product Idea Generation. Re-
search Policy 7(3), 240–266 (1978) 
27. Leifer, R., O’Connor, G.C., Rice, M.: Implementing radical innovation in mature firms: 
The role of hubs. Academy of Management Executive 15(3), 102–113 (2001) 
28. McDermott, C.M., O’Connor, G.C.: Managing radical innovation: an overview of emer-
gent strategy issues. Journal of Product Innovation Management 19(6), 424–438 (2002) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 451–461. 
DOI: 10.1007/978-3-642-30817-8_44 
© Springer-Verlag Berlin Heidelberg 2013 
 
Multistate Feature Modelling  
of a Very Complex Design Feature 
Done Ugarte and Alberto Izaguirre 
Mondragon Unibertsitatea, Loramendi kalea 4, Mondragón, Gipuzkoa, Spain 
{dugarte,aizaguirre}@mondragon.edu 
Abstract. This paper presents a Knowledge Based Engineering (KBE) applica-
tion developed for the aeronautical sector. Specifically, this paper describes a 
KBE application that aids on the definition of the contact interface between  
turbine discs and blades of low pressure turbines, namely the firtree. Designing 
firtrees requires more than 60 different entry parameters, a check for static 
stresses, de-featuring for fatigue finite element analysis and as cast design for 
manufacturing and weight management purposes. The application delivered at 
first the typical benefits of KBE applications: significant time savings, error re-
duction and standardization. However, the challenges posed by the aeronautical 
industry, quickly pushed designers out of the standard agreed topology for the 
firtree. This raised the typical issue on development time and maintainability of 
KBE applications. The challenge was partially solved through designer knowl-
edge capture and reuse in the form of a User Defined Feature library integrated 
in to the original KBE application. 
Keywords: Knowledge Management, Design Automation, NX, Unigraphics, 
Knowledge Fusion, Firtree, User Defined Feature, Feature Based Design. 
1 
Introduction 
In order to be able to respond to the ever increasing demands of the aeronautical sec-
tor and society in general, of reduced development costs and time, fuel consumption 
and emissions, and at the same time, increased safety and more strict regulations in 
aero engines, a Low Pressure Turbine (LPT) supplier started an initiative to capture 
LPT Systems Life Cycle Knowledge into a Knowledge Based Engineering (KBE) 
application. 
After an Added-Value analysis of the different components, turbine discs and 
blades were chosen for the development of the first applications. Specifically, this 
paper describes a Knowledge Fusion application that aids on the definition of the 
contact interface between turbine discs and blades, i.e., the firtree. 
Firtrees used to be defined on a different in-house application based on a legacy 
KBE language, the Adaptive Modeling Language (AML®) by Technosoft, as re-
garded by the LPT supplier. This legacy application, helped designers in defining this 

452 
D. Ugarte and A. Izaguirre 
complex interface’s profile with more than 60 different entry parameters, check for 
static stresses and output an IGES file. 
Later on, the IGES file had to be imported into NX for detailed mechanical design. 
As IGES only supports plain geometry, the non-parametric firtree profile was inte-
grated into a parametric disc through non-parametric operations, losing the advantage 
of the parametric nature of the disc. Thus, any change to the input parameters, used to 
lead to a return to the AML® based application and lots of manual rework. 
Besides, parametric standards for downstream states of the feature do actually exist 
on the company's standard engineering processes that were manually generated in 
NX6. Two additional parametric states lacking software support were identified: 
• de-featured for fatigue Finite Element Analysis (FEA), 
• as cast for manufacturing and weight management purposes, 
Thus, a requirement to develop the new firtree application in NX ®’s KBE language 
Knowledge Fusion® was set out from the very beginning to achieve full integration 
into NX® and had to cover: 
• helping the profile definition phase with its more than 60 input parameters, 
• checking static stresses, 
• developing the solid features for the disc in detailed and de-featured for FEA 
states, 
• developing the solid features for the blade in detailed, de-featured for FEA and 
casting states, 
• text-file output for archiving purposes, 
Therefore, according to the following literature review the expected benefits of the 
application were: 
• time saving due to the integration of the Firtree Design tool into NX, 
• time saving in the development of solid models in the different states, 
• error reduction due to elimination of manual data re-input, 
• standardisation of feature solid modelling, 
• reduced training time for new disc & blade design engineers. 
2 
Definition of Knowledge Based Engineering 
From an academic perspective the definition of what KBE is, is an open-ended ques-
tion as recently pointed out in a KBE review [1]. 
Technically, nearly all KBE systems share that they are a declarative, demand-
driven, value-caching, Knowledge Based Systems (KBS) linked to a Computer Aided 
(CAx) system, written on an Object Oriented Programming language. The rationale 
behind such architecture is, to use the KBS to capture, reuse and make integrated 
multi-disciplinary decisions in order to automate the repetitive, non-creative tasks an 
engineer should perform over the CAx system. A very detailed technology-oriented 
description can be found in [2]. 

 
Multistate Feature Modelling of a Very Complex Design Feature 
453 
Originally, the focus of KBE was on integrating multi-disciplinary knowledge on 
the early design stages and it used to automate only the geometrical engineering tasks 
performed on Computer Aided Design (CAD) systems. For instance, in [3] Bermell-
García et al, reported a reduction in development time of a wind tunnel from 2 weeks 
to 2 hours, due to the generation of an specific application that linked a KBS to a 
CAD. In [4] Rios et al, developed another specific application that linked a KBS to 
CAD system to design and manufacture fixtures for High Speed Machining. 
In order to alleviate the engineers from searching the optimum response solution, a 
desired design response that could be determined by an analytical equation based on 
the design, and an optimization system that could drive the designs to an optimum, 
were added to a KBS linked to a CAD system, creating a Multi-disciplinary Design 
Optimization (MDO) system for composite panels of aero-structures in [5]. 
In order to be able to find responses in designs where no analytical equations were 
available, the analytical equation was replaced by a Computer Aided Engineering 
(CAE) model pre-processing, solving and post-processing. Thus, another CAx tool 
was integrated into the KBE system in [6]. 
Approaching a more different engineering discipline, in [7–9] a KBS was built 
around a Computer Aided Manufacturing (CAM) system, in order to automate 5-axis 
machining process planning. 
Therefore, KBE practitioners must be thorough in defining what the scope of their 
KBE application is. In particular, the work presented in this paper addresses the 
knowledge capture and reuse, and reduction of repetitive engineering tasks by the 
generation of Higher Level Primitives as named in [5], design features as named in 
[10] or standard features as named in [11]. In all cases, they refer to wrapping several 
commercial off the shelf CAD commands into a semantically richer one in the context 
of application. Thus, a design feature contains by definition the knowledge about the 
construction procedure in the CAD system and any further constraints its parameters 
might be bound to, as stated about basic features in classical textbooks on parametric 
and feature based modelling systems [12] and [13]. 
Specifically, the KBE application will define a single design feature which has two 
different linked representations on two different parts: the wheel and the blade. The 
constraints of the parameters are linked to a historical database due to the complexity 
of modifying the manufacturing process in terms of cost. Besides, the representation 
on the wheel must support two different states: detailed and truncated for CAE pur-
poses. Finally, the blade representation must also support three different states: de-
tailed, as-cast and truncated for CAE purposes. 
3 
Methodologies for Knowledge Based Engineering  
In order to properly elicitate, capture, store and maintain the expert’s knowledge into 
the KBE application several methodologies have been defined. The most remarkable 
is MOKA (Methodologies and software tools Oriented to Knowledge-Based Engi-
neering Applications) [14], [15]. MOKA defines a KBE application lifecycle in six 
steps: Identify, Justify, Capture, Formalize, Package and Activate. It focuses on 

454 
D. Ugarte and A. Iza
adapting existing KBS deve
and Formalize steps to the 
system, in KBE applicatio
ICARE forms (Illustration
knowledge that is going to
the format of a knowledge
the formal MOKA model i
Model. In the latter, it def
guage (MML), in order to 
ment by linking it to the ICA
Similarly, in [7], [8], [16
pability to handle the kno
KBS systems linked to CAM
However, if the scope 
(MDO), the KNOMAD me
In this work, as the desig
dard practices, no particula
Nevertheless, the MOKA m
4 
Definition of Fir
As shown in Figure 1, a f
blade (c) in a LPT. It has 
Both of them are composed
wheel and solid in the blad
others representation param
are tightly linked. 
                    (a)  
Fig. 1. W
aguirre 
elopment methodologies by providing adapting the Capt
specificities derived from the linkage of a KBS to a C
ons. More precisely, in the Capture step it defines 
, Constraint, Activity, Rule and Entity forms) where 
o be integrated into the KBE application is registered
e book, or informal model. Regarding the Formalize s
is composed of the Product Model and the Design Proc
fines a UML variant, namely the MOKA Modeling L
adapt the modelling language for OOP to KBE devel
ARE forms previously defined. 
6] the MOKA methodology was adapted to increase its 
wledge related to 5-axis machining process-planning 
M systems. 
of application is multidisciplinary design optimizat
thodology adapted for so can be found in [17]. 
gn feature was already modelled across several internal st
ar KBE methodology has been used, as diagnosed by 
methodology could have been adequate for this purpose. 
rtree 
firtree is a design feature (b) that links the wheel (a) 
two different representations on the wheel and the bla
d by an extruded complex profile, which are: a slot in 
de. The profiles share some parameters and depend on 
meters to be geometrically fully defined, thus, the prof
 
                          (b)                (c) 
Wheel and blade assembly through the firtree 
ture 
AD 
the 
the 
d in  
tep, 
cess 
Lan-
lop-
ca-
for 
tion 
tan-
[1]. 
and 
ade. 
the 
the 
files 

 
Multistate Feature Modelling of a Very Complex Design Feature 
455 
 
Fig. 2. Supported firtree configurations 
The profile topology contains many configurable elements. As shown Figure 2 fir-
trees with lobe number ranging from 5 (a) to 1 (e) are supported. The overall pressure 
angle can be modified (f) and (g), and the parameters of the two arcs present in each 
lobe can be specified individually (h). Finally, the bottom of firtree profile in wheels 
is composed of several arcs that may not be tangent on every design (i). In total, more 
than 60 parameters have to be specified to fully define the profiles of the firtrees. 
5 
Firtree Development Process and States 
The firtrees as the whole wheels and blades, have to be modeled in different devel-
opment states throughout their development processes. 
The development process of this linking design feature starts by reviewing the va-
lidity of existing ones, due to the high cost of their tailor-made manufacturing tools 
for the wheels, i.e., broaches. 
Following, if the designer finds out that none of the existing firtree designs is valid 
for the new project. It starts to generate new firtree profiles. During preliminary  
design iterations, static stress checks are performed to ensure certain validity of the 
proposed designs. Thus, the detailed firtree design is proposed, see Figure 3 (a). 

456 
D. Ugarte and A. Iza
           (a)           
Fig. 3. Different states of the s
When the preliminary de
though CAE analysis, and t
to internal CAE standard pr
Following, when the bla
be generated and the firtree
gy completely, see Figure 3
Finally, the approved w
models are submitted to the
ing processes. 
6 
KBE Applicatio
As aforementioned, the am
amount of calculations to b
was architected as shown in
Fi
 
aguirre 
 
 
         (b)                           (c) 
same firtree during the development process of wheels and bla
esign gets accepted, it is thoroughly checked for fatigue 
the lobes need to truncated and slightly modified accord
ractices, see Figure 3 (b). 
de design is approved, an as-cast model of the blade ha
e is covered up with stock material and changes its topo
3 (c). 
heel and blade designs’ raw material models and detai
e Process Planning department to set out their manufac
on Development 
mount of parameters to be gathered from the users and 
be performed are quite big. Therefore, the KBE applicat
n Figure 4. 
 
ig. 4. KBE application class hierarchy 
ades 
life 
ding 
as to 
olo-
iled 
tur-
the 
tion 

 
Multistate Feature Modelling of a Very Complex Design Feature 
457 
A class linked to the main dialog is used to mainly gather the desired state of the 
firtree and the profile positioning parameters in its attributes. All these attributes are 
directly accessible in any other object generated by itself or any descendant, due to 
the automatic inheritance of attributes in Knowledge Fusion. As the parameters to 
define the profiles are too many, they were moved to a sub-dialog. These parameters 
are passed into the Geometry Solver by reference chain to the main class. 
Thus, all parameters are available at the Geometry Solver class that analytically 
solves the detailed profile points. As all the classes that generate geometry are created 
by this class, the solution is inherited and ready to be drawn in the different states. 
The Profile class supports the engineer during the preliminary geometrical design 
checks and simply draws the solutions with curves on the XY plane. The Static Stress 
class generates the profiles as sheet bodies to get the area of the profiles and performs 
the static stress calculations to aid the engineer during the preliminary checks. The 
Detailed Solid class positions the profile on the blade or wheel and extrudes the cor-
responding profile to generate the corresponding solid for the Boolean operation. The 
CAE Solid class derives the truncations out of the inherited solution and again gene-
rates the solid by extruding the corresponding profile. Finally, the Cast Solid is only 
applicable to the blade and calculates the outmost lobe to add stock at a distance from 
it recalculating a brand new cast profile. 
Regarding the user interface a decision to integrate everything within a single di-
alog was made to simplify the interaction for the user. The tabs present in the dialog, 
represent the different steps along the development process to make it flow naturally. 
Figure 5 (a), shows the initial dialog with several options to load different designs and 
main profile position parameters. Pushing the Parameters Dialog button, the dialog n 
Figure 5 (b) is displayed to allow the user modify profile parameters. Figure 5 (c) 
corresponds to the preliminary static stress check step. Finally, the dialog in Figure 5 
(d) allows the user to integrate the designed firtree profile in the corresponding wheel 
or blade solid in the desired state: detailed, truncated for CAE or (in blades) as-cast. 
A remarkable effort was made to increase the productivity of users in defining the 
big amount of input parameters. Therefore, a close look to Figure 5 (a) show a dis-
played pop-up menu showing many different ways of loading existing definitions. 
The “New” option allows the engineer to start from scratch. 
Options starting with “Existing …” load parameters from a library of text files, 
where the parameters of manufactured broaches are stored. “Existing Standard” con-
strains the user and does not allow editing the parameters related to the broach, at all. 
“Existing Standard” corresponds to broaches that have been manufactures but as-
sessed as non-valid. Whilst, “Existing and Edit” is a way to start a new design, taking 
as starting point an existing one. 
Those options containing “HD” refer to design iterations that have been saved to 
the hard drive for future re-use. In this case, the engineer can decide whether he is 
allowed to change the parameters or not. 

458 
D. Ugarte and A. Izaguirre 
 
 
                          (a)                     (b) 
 
 
                          (c)                       (d) 
Fig. 5. KBE application dialog and tab content 

 
Multistate Feature Modelling of a Very Complex Design Feature 
459 
Finally, the “From Blade” is designed to be used by the engineers that design 
wheels. This option allows reading the definition of the firtree profile from the NX 
part file where the KBE application has been used to define the firtree of the blade. 
This is possible because typically the development time of discs is longer than for 
blades. 
7 
KBE Application Validation 
The KBE application went through two validation processes: 1) against the existing 
AML KBE application, and 2) against the NX experts within the LPT supplier. 
On the first validation, all pre-existing firtree designs where fed into the Know-
ledge Fusion based firtree application and compared the outputs between the IGES 
file that the AML based application output with the curves generated in NX by Know-
ledge Fusion. A 100% of matching results were achieved. 
On the validation against internal NX experts, the check was performed regarding 
usability of the application. The usability of the KBE application was assessed as 
good. 
Thus, the KBE application was approved and activated. 
8 
KBE Application Maintenance 
Despite the initial approval, after some successful firtree designs a new project came 
in. First iterations were performed based on the existing KBE application and, on the 
opinion of design engineers, no satisfactory solution was found based on the approved 
topology (type and amount of curves present in the profile). Design engineers re-
quired the bottom of wheel groove to be curved and not flat (cylindrical in 3D) in the 
firtree profile for the wheel. 
As can be spotted in Figure 2, all supported configurations relied on a bottom flat 
groove profile, and no curved bottom was supported by the KBE application. 
Therefore, a knowledge management challenge was confronted. What should be 
done in such a case? Should the application include the new firtree configuration? 
Should engineering management force design engineers to constrain themselves to the 
standard configuration? 
In this case, modifying the Geometric Solver class to support the new configura-
tion would have been costly, as it would have required the class to be rewritten from 
scratch. 
The latter, forcing engineers to follow the standard topology, is a two folded deci-
sion. Any standard should provide the benefit of reducing costs. However, at the same 
time, they may also hinder innovation. So it is definitely a complex management  
decision. 
Therefore, an intermediate solution was taken in this work. 
NX, as a fully-fledged feature based design CAD application, supports User  
Defined Features (UDF) definition, management and instantiation. UDFs are custo-
mization elements. During UDF definition, the way a design feature is geometrically 

460 
D. Ugarte and A. Izaguirre 
constructed is captured. During the UDFs definition some constraints on parameter 
values can be set, but their semantics are very limited. Most importantly for the KBE 
field, UDF definition happens over an NX part any design engineer may have con-
structed. Therefore, capturing the geometrical construction knowledge can be done 
very quickly through UDFs. Thus, only UDF’s geometrical construction capabilities 
were exploited in the solution. 
As an intermediate solution, the user was also enabled to choose between a set of 
firtree UDFs during the definition phase. This was achieved by including the “New 
UDF” option on the displayed popup-menu on Figure 5 (a) The benefit of such an 
approach is the ability to quickly partially maintain the KBE application. The counter-
side of such a solution is that there is no support for the following states of the firtree. 
9 
Conclusions 
The firtree KBE application did meet its expectations and significant time reductions 
were achieved, as expected. However, as described in [1], one of the critical aspects 
of KBE applications is knowledge management. Within this context, to manage 
knowledge means to take the decision when a new knowledge item (a new firtree 
configuration, in this case) deserves to be integrated into the knowledge repositories 
or KBE applications of the company and when it has to be regarded as a single excep-
tion solution. 
Whenever the KBE application requires any maintenance it is convenient to con-
sider the observation made in [18], where the authors state that typically customiza-
tion/UDFs have a much better return on investment, than automation/re-coding KBE 
application. 
References 
1. Verhagen, W.J.C., Bermell-Garcia, P., van Dijk, R.E.C., Curran, R.: A critical review of 
Knowledge-Based Engineering: An identification of research challenges. Advanced Engi-
neering Informatics 26, 5–15 (2012) 
2. Rocca, G.L.: Knowledge based engineering: Between AI and CAD. Review of a language 
based technology to support engineering design. Advanced Engineering Informatics 26, 
159–179 (2012) 
3. Bermell-Garcia, P., Fan, I.: A kbe system for the design of wind tunnel models using reus-
able knowledge components. In: Proceedings of the VI International Conference on 
Project Engineering (2002) 
4. Rios, J., Jimenez, J.V., Perez, J., Van Iza, A., Menendez, J.L., Mas, F.: KBE Application 
for the Design and Manufacture of HSM Fixtures. Acta Polytechnica 45, 17–24 (2005) 
5. van Tooren, J.M., Nawijn, M., Berends, J.P.T.J., Schut, E.J.: Aircraft Design Support using 
Knowledge Engineering and Optimisation Techniques. In: 46 th AIAA/ASME/ASCE/AHS/ 
ASC Structures, Structural Dynamics and Materials Conference, Austin, Texas, USA 
(2005) 
6. Rocca, G.L., Tooren, M.V.: A knowledge based engineering approach to support automat-
ic generation of FE models in aircraft design. In: 45th AIAA Aerospace Sciences (2007) 

 
Multistate Feature Modelling of a Very Complex Design Feature 
461 
7. Ammar-Khodja, S., Perry, N., Bernard, A.: Processing Knowledge to Support Knowledge-
based Engineering Systems Specification. Concurrent Engineering 16, 89–101 (2008) 
8. Ammar-Khodja, S., Perry, N., Bernard, A.: Processing Knowledge to Support Knowledge-
based Engineering Systems Specification. Concurrent Engineering 16, 89–101 (2008) 
9. Candlot, A., Perry, N., Bernard, A., Ammar-Khodja, S.: Case Study, USIQUICK Project: 
Methods to Capitalise and Reuse Knowledge in Process Planning. In: Bernard, A., Tich-
kiewitch, S. (eds.) Methods and Tools for Effective Knowledge Life-Cycle-Management, 
pp. 487–506. Springer, Berlin (2008) 
10. Lupa, N., Danjou, S., Koehler, P.: Approach for a Modular Knowledge-based CAD Mod-
eling Process. In: 18th CIRP Design Conference (2008) 
11. Jagenberg, J.T., Gilsdorf, E.A., Anderl, R., Bornkessel, T.: Knowledge Driven Design Fea-
tures for the Product Life Cycle of Engine Parts. In: ASME 2009 International Design En-
gineering Technical Conferences & Computers and Information in Engineering Confe-
rence, pp. 1–8 (2009) 
12. Kunwoo, L.: Principles of CAD/CAM/CAE Systems. Addison Wesley Longman Limited, 
England (1998) 
13. Shah, J.J., Mantyla, M.: Parametric and Feature Based CAD/Cam: Concepts, Techniques, 
and Applications, 1st edn. John Wiley & Sons, Inc., New York (1995) 
14. Stokes, M. (ed.): Managing Engineering Knowledge: MOKA Methodology for Knowledge 
Based Engineering Applications (2001) 
15. Klein, R.: Knowledge Modelling in Design – the MOKA Framework. Artificial Intelli-
gence in Design, 77–102 (2000) 
16. Ammar-Khodja, S., Perry, N., Bernard, A.: Processing Knowledge to Support Knowledge-
based Engineering Systems Specification. Concurrent Engineering 16, 89–101 (2008) 
17. Curran, R., Verhagen, W.J.C., van Tooren, M.J.L., van der Laan, T.H.: A multidisciplinary 
implementation methodology for knowledge based engineering: KNOMAD. Expert Sys-
tems with Applications 37, 7336–7350 (2010) 
18. Lamarche, B., Rivest, L.: Dynamic Product Modeling with Inter-Features Associations: 
Comparing Customization and Automation. Computer-Aided Design and Applications 4, 
877–886 (2007) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 463–472. 
DOI: 10.1007/978-3-642-30817-8_45 
© Springer-Verlag Berlin Heidelberg 2013 
 
Virtual Validation of the Manual Assembly of a Power 
Electronic Unit via Motion Capturing Connected  
with a Simulation Tool Using a Human Model 
Jochen Bönig, Christian Fischer, Matthias Brossog, Martin Bittner, Markus Fuchs, 
Holger Weckend, and Jörg Franke 
Institute for Factory Automation and Production Systems, Friedrich-Alexander-University of 
Erlangen-Nuremberg, Egerlandstr. 7-9, 91058 Erlangen, Germany 
Jochen.Boenig@faps.uni-erlangen.de 
Abstract. A key challenge for gaining important time and cost potentials in 
production engineering projects is an early virtual validation during the pre-
series. Under the premise to replace physical by digital mock-ups, we will 
present requirements and solutions of a virtual validation focused on manual as-
sembly of power electronics in automotive industry. Using a digital human 
model for dynamic analysis is not very prevalent, because of the high modeling 
complexity in the digital environment. The resulting motions of the human 
model are furthermore unrealistic. Hence the need for research is a time saving 
and, regardless, a realistic movement design for virtual validation by a human 
model. To achieve this goal, we use an experimental setup including a variable 
eight camera motion capture system, a data glove and an interface for the  
connection to the digital validation software. 
Keywords: virtual validation, manual assembly, motion capture. 
1 
Introduction 
The future competitive situation in the automotive industry and the increasing market 
demands will inevitably lead to higher complexity in product development, produc-
tion planning and pre-series. Longer lifecycles of assembly systems based on highly 
flexible production cells with a rising number of integrated processes call for new 
solutions, especially in production engineering. An early virtual validation is helpful 
for simultaneous engineering and leads to production-oriented product design and 
development. [1] In this early stage only digital mock-ups are available. Human mod-
els are used for the virtual validation of manual assembly tasks. The movement of the 
worker is either unrealistic, or time expensive to realize. Hence the need for research 
is an optimization of virtual validation using a human model. 
A motion capture (mocap) system, a data glove and an interface for the connection 
to the validation software is used for improvement. This setup allows an object and 
human tracking at the same time. The human tracking is connected with virtual affix-
ing of the assembly. With this experimental setup the correlation of the real and  

464 
J. Bönig et al. 
virtual situation is tested. The collision calculation and a dynamic Rapid Upper Limb 
Assessment (RULA) analysis are possible in real time as well as a later processing 
with stored data after the tracking. Additional a transient ergonomic analysis with the 
Ergonomic Assesment Work Sheet (EAWS) is realized with the ALASKA software 
from the Institute of Mechatronics (IfM) in Chemnitz, because EAWS is not yet im-
plemented in DELMIA from Dassault Systemes. The flexible system setup concept 
with eight cameras is also presented. This concept is developed for a highly flexible 
camera positioning for different assembly tasks to avoid occlusion. The visualization 
of the digital environment is realized in 3D. 
2 
State of the Art 
It is difficult to create postures or movements of human models in simulation software 
like DELMIA. Software upgrades for the analysis workbenches are necessary. For 
different postures and especially for movements and time depending analyses, scien-
tific models are missing. The manual assembly of limp components is not possible to 
validate in conventional software for example. Besides the time consuming teaching 
of the human model, the resulting movement of the worker in the virtual environment 
is unrealistic and hence not a good base for ergonomic analysis. Regarding the ap-
praisal possibilities of the analysis there is also need for action in order to be able to 
filter critical situations, postures, tasks etc. clearer and faster and additionally to de-
fine the elements for an optimization of the situation. Important functions are view 
and reachability analysis. More complex analyses like posture analysis are used less 
often. [1–5] 
A lot of manual teaching of the human model is necessary to simulate and accor-
dingly validate a manual assembly task. Generally there are two different convention-
al ways of setting the manikin to the desired position or orientation. The most time 
expensive option is to move every single body part manually with a function that is 
usually called forward kinematics. The second possibility is using some kind of in-
verse kinematics. The operator has to set the final position and orientation of a certain 
body part, e.g. the hand, and the posture prediction system generates the correspond-
ing movements of the other involved body parts, e.g. the arm and shoulder. Although 
the option of using a posture prediction system is much faster in comparison with the 
forward kinematics function, one crucial disadvantage persists: The generated move-
ments are often unnatural and an appropriate examination is difficult [6]. 
The different ergonomic analyses depend on the joint angles of the human model, 
so a realistic movement is fundamental for acceptable results. Filming the real assem-
bly movement is a good way to create comprehensible movements of the human 
models. That’s because the postures and positions of the relevant body parts such as 
hands, arms, torso and legs, can be taken directly from the video documentation. De-
spite good analysis results, this method is very time consuming. To create rapidly and 
more detailed ergonomic simulations the possibilities of motion capture are the most 
promising. 

 
Virtual Validation of the Manual Assembly of a Power Electronic Unit 
465 
To avoid problems in the production process, because of the high number of va-
riants, virtual validation is a useful tool. The subsequent integration of high voltage 
components in existing car concepts is an actual example for the increasing variety in 
automotive production. Therefore different requirements arise from the assembly 
situation to produce different engine concepts in one production line. [7] 
Especially for automotive industry, an increasing interest in an effective way for 
virtual validation is observable. Insufficient experience and expert know-how in the 
field of e-mobility concerning the assembly and the mounting of electric components 
are trademarks of today’s production environments. Anticipatory planning of the en-
tire production process in an early stage of product development – including a colli-
sion-free feasibility and accessibility, validation of assembly sequences and posture 
analysis of the mechanic for ergonomic reasons – is the future for automotive indus-
try. [8, 9] Research in this field is also interesting because of the medical aspect [4]. 
3 
Generating Realistic Movements of the Human Model 
The underlying idea is to use an experimental setup including a variable eight camera 
motion capture system, a data glove and an interface for the connection to the valida-
tion software, in order to solve the different problems. To record the movement in a 
digital environment the motion capture system is used. To simplify the teaching of the 
human model the captured data is transferred from the tracking program directly to 
the analysis program via a data interface. In this course the problem of unrealistic 
movement is solved too. With the system setup shown in figure 1 the following expe-
riments are carried out. 
 
Fig. 1. System Setup for generating realistic human model motions [12, 13, 22] 
The advantages of virtual in comparison to physical validation are defined by Flick 
and Riedl [10, 11]. 
3.1 
Motion Tracking/Capture System  
To capture the movement of a worker during a manual assembly task a motion track-
ing system is used. Since optical systems have the highest accuracy of the common 

466 
J. Bönig et al. 
motion capture technologie
Tracking (A.R.T.) GmbH. 
to measure the relative mo
around a 3.5*3.5 m² worki
flash. The actor wears a sen
cal markers. The targets re
light. At least two camera
degrees-of-freedom-(6DOF
tain target, the tracking sig
of an optical motion capture
refers to the defined origin 
optical MCS the opportuni
movement of any object an
problem is to avoid occlus
himself. We try to solve thi
application-specific setup b
Fig. 2. CAD
The application softwar
graphical user interface to t
data for further use. [12] 
3.2 
Data Glove 
Data gloves are used to me
realization of these measur
current data gloves. The da
its wireless version. It con
Namely these are three ben
for thumb crossover, palm a
The thin gauges are mad
is bent in consequence of e
effect makes it possible to
s, we use an eight camera system from Advanced Realti
Other technologies use e.g. mechanical or inertial sens
otions of the human joint angles. The cameras are pla
ing area and they send out a synchronized infra-red-(I
nsor set of 17 targets, consisting of retro-reflecting sph
eflect the IR-radiation into the direction of the incom
as must receive the reflected signal, to calculate the 
F)-orientation of the targets. If more cameras cover a c
nal is redundant, but more accurate. Every tracking sig
e system (MCS) is calculated with its absolute position 
and orientation of the room coordinate system. This gi
ity to not only track human motion, but also capture 
nd their interaction, when a target is fixed on it. The m
ion due to obstacles in the recording area or by the ac
is problem by rearranging the cameras in order to create
by visualizing the cones of light (see figure 2). 
D-tool for the flexible application-specific setup 
e DTrack2 does all necessary calculations. It serves a
the controller and offers the position and rotation track
easure the finger angles of hand and finger joints. For 
rements various measuring principles are used in differ
ata glove used in our experiments was the CyberGlove I
ntains 22 sensors based on the principle of strain gaug
ding sensors per finger, four abduction sensors and sens
arch, wrist flexion and wrist abduction. [13] 
de up of an electrically conductive material. When a ga
e. g. the flexion of a finger, its resistivity is increased. T
o determine the corresponding finger movement with 
ime 
sors 
aced 
IR)-
heri-
ming 
six-
cer-
gnal 
and 
ives 
the 
main 
ctor 
e an 
 
as a 
king 
the 
rent 
II in 
ges. 
sors 
auge 
This 
the 

 
Virtual Validation of the Manual Assembly of a Power Electronic Unit 
467 
help of a calibration process. The use of strain gauges makes the CyberGlove more 
efficient than other data gloves. [14] 
The CyberGlove offers no ability to calculate the position and orientation of the 
hand according to the room coordinate system. This is why an additional hand track-
ing is obligatory. As the CyberGlove itself has no ability to give the user any kind of 
force feedback, CyberGlove Systems distribute solutions for force feedback which 
can be also used in combination with the CyberGlove.  
3.3 
Interface Software  
The Haption RTID software is an interface to connect the CyberGlove and the motion 
tracking software with the simulation tool DELMIA V5. RTID stands for Real Time 
Interaction for DELMIA and consists of the components RTID Core and Human. The 
application is based on the physics-engine Interactive Physics Simulation Interface 
(IPSI) and allows the processing of all prevalent V5-formats. Movements, captured 
with various mocap-applications, can be recorded as move to posture-activities or as 
.xml-data file. Because of the Client/Server-principle of the IPSI-engine, the main 
computing is done by the IPSI-software and can be sourced out on another processor. 
Therefore the client only has to do the visualization. Haption also offers possibilities 
for the integration of collision mechanisms. [15, 16] 
3.4 
Validation Software 
In this study, the digital human model and work environment are designed in 
DELMIA V5. It’s the basis software to do the digital analysis for the manual assem-
bly. The human model used for DELMIA is human builder. It includes all the func-
tionalities that human models should have for the purpose of using them in ergonomic 
simulations [17] and also to study the interaction between the man-machine relation. 
The sophisticated manikin structure consists of 99 independent links and 148 degrees 
of freedom and has access to data from multiple populations. In addition, the manikin 
possesses fully articulated hand, spine, shoulder, and neck models to accurately re-
produce natural movement, which includes default inverse kinematics for manikin 
motion [18]. Also the natural constraints of the joint angles are considered. 
The Human Activity Analysis tool allows static posture analysis, lifting/carrying 
according to Niosh and push/pull according to Snook & Ciriello, hand-arm studies 
(RULA), motion, ergonomics, and handling investigations. The Human Posture Anal-
ysis provides a qualitative and quantitative real-time analysis of attitude with color 
marked single values for each joint. An automatic posture optimization completes the 
feature set. [3] 
For an accurate visualization of the human model, the anthropometric data of the 
actor must be assumed for the virtual manikin. 
4 
Experimental Setup 
To realize the above mentioned idea it is necessary to connect the different hard and 
software parts to a running system. For the accomplishment of the planned experi-
ments, a separated room surrounded with black curtains was created. This conduced 

468 
J. Bönig et al. 
to the avoidance of reflections, what is important for the proper working of the optical 
tracking system. In the recording area, different objects building the assembly scene 
were placed. These were a demonstrator vehicle, an assembly table, a cordless screw-
driver and a power electronic unit. Furthermore a silver screen was added to show the 
captured scene to the particular test person and to relieve the communication between 
the operator and the test person. Eight IR cameras were placed around the scene and 
the test person, who should provide the motion data for the manikin, was equipped 
with the optical targets and the CyberGlove II wireless. The digital virtualized expe-
rimental setup is shown in the following figure 3. 
 
Fig. 3. Experimental setup in the digital environment of DELMIA V5 
The IR cameras were connected with the DTrack Controller. The main computer, a 
CAD workstation, was connected witch the DTrack Controller, with the screens for 
the operator and for the test person and, via Bluetooth, to the CyberGlove II. In our 
experiments the software systems DELMIA, RTID, DYNAMICUS and VirtualHand 
were all installed on one PC, which carried out the main processing as well as the 
visualization. For future work an outsourcing of the main processing on a second 
computer is planned. 
4.1 
Experiments and First Experiences  
The intention of the performed experiments was to gather information about the pos-
sibilities and limitations of the described hardware and software constellation.  

 
Virtual Validation of the Manual Assembly of a Power Electronic Unit 
469 
The aim is the ability to capture the movements of humans and objects, to record 
them both together and to replay and analyze the generated movement sequences. 
Therefore an exemplary assembly situation was created, where a worker should fit a 
power electronic unit in the engine compartment of a demonstrator vehicle. The cor-
responding manikin movements should be analyzed in real time and also in recorded 
form. In Haption RTID the motion tracking of humans or objects is relatively simple 
once the required calibrations are made. The recording of the correspondent move-
ment sequences is a little more challenging. Movement of the Manikin can be record-
ed as a DELMIA-internal “move to posture” or as well in the .xml format. Generally 
there are two ways of integrating movement of objects in the simulation process. On 
the one hand virtual objects can be attached to single body parts irrespective whether 
a physical counterpart is used or not. On the other hand optical targets can be fixed to 
real objects. When a linkage of the target and the corresponding virtual object is 
created in Haption RTID, moving the real object also determines a movement of the 
virtual pendant. For a realistic representation of the tracked movements, the matching 
of the real and the virtual subjects and objects is of great importance. This means, that 
the positions of the real and the virtual targets have to be brought in accurate accor-
dance with each other in the course of the calibration process. If the calibration is not 
carried out properly, there might appear deviations between the real and the virtual 
position and orientation. During our studies we could for example observe gaps be-
tween the manikin’s hand and a bored object. An easy possibility to add objects to 
recorded human movements is to insert a pick and place function into the sequence of 
the created move to postures. Unfortunately this method has nothing to do with the 
tracking of real objects. Another option is to add a target to a real object like de-
scribed above. The problem lies in recording and replaying these sequences. Record-
ing and replaying human and object movements apart from each other and replaying 
them all together is possible, but to perform this task, it is necessary to use 
IPSI scripting, a python-based SDK-application for the easy integration of changes 
within the software context. Once the appropriate scripts are created, it works very 
well. A further possibility to capture and analyze motion data is offered by IfM. The 
simulation tool ALASKA is a multi-body-simulation tool for the analysis of dynamic 
mechatronic systems. The software tool is modular, so that just the required plug-ins 
needs to be integrated. The DYNAMICUS/Recorder is a tool to record human track-
ing data as well as object movement. The motion data of any target is saved in a sin-
gle .xml-file. For single ergonomic analysis the data can be transferred into 
DYNAMICUS/EAWS. For further interaction the motion data can be integrated into 
an ALASKA simulation environment. [19] 
With the right calibration the capturing of finger movement with the CyberGlove II 
works quite well in DELMIA. This is again strongly depending on the corresponding 
calibration process. The current version of DYNAMICUS does not offer a Cyber-
Glove interface. 
4.2 
Analysis  
In order to visualize the captured motion data and interpreting it in relation to ergo-
nomic matters, two different software systems were tested. These were on the one 
Hand DELMIA V5 with the corresponding plugin Haption RTID and on the other 
hand DYNAMICUS/EAWS. Both tools allow the ergonomic analysis of human 

470 
J. Bönig et al. 
movement but with two different approaches. While DELMIA offers ergonomic tests 
according to different international standards of which we chose the RULA analysis, 
DYNAMICUS/EAWS uses the European Assembly Worksheet EAWS. The basics of 
the two methods and the corresponding differences are illuminated below. 
 
Fig. 4. The RULA and EAWS analysis with the corresponding human models 
RULA. Is a survey method developed for use in ergonomics investigations of 
workplaces where work-related upper limb disorders are reported. This tool requires 
no special equipment in providing a quick assessment of the postures of the neck, 
trunk and upper limbs along with muscle function and the external loads experienced 
by the body. A coding system is used to generate an action list which indicates the 
level of intervention required to reduce the risks of injury due to physical loading on 
the operator. [20] 
EAWS. In accordance to the German BAuA and the Toyota method, IAD tools grant 
load points for ergonomically unfavorable conditions. Dependent on the score a traf-
fic light three zone rating system is associated with respect to the demands of the 
Machinery Directive. The EAWS consists of four sections for the evaluation of work-
ing postures and movements with low additional physical efforts (< 30-40 N or 3-4 kg 
respectively), action forces of the whole body or hand-finger system, manual mate-
rials handling and repetitive loads of the upper limbs. 
Sections one to three base their evaluation on physiological and biomechanical cri-
teria; section four is based on medical and epidemiological data. With respect to the 
different evaluation approaches the results of sections one to three are combined to a 
“whole body” exposure, whereas section four indicates the load situation of the “up-
per limbs”. Both approaches are rated in a 3 zone rating system as shown in figure 3. 
The overall estimation is the worst case of “whole body” and “upper limbs”. The 
dashed line in between the green, yellow and red zones indicate, that they are no con-
crete border lines, but transient areas. This means 47 and 53 points represent the same 
“orange” color, i.e. colors do not “switch from yellow to red”, when crossing a score 
of 50. [21] 

 
Virtual Validation of the Manual Assembly of a Power Electronic Unit 
471 
5 
Conclusion 
In this paper a virtual validation methodology for the manual assembly of a power 
electronic unit, using a virtual human model, has been presented. First, the problems 
of virtual validation and the state of the art are described. With the identified prob-
lems the need for research in this field is proved and leads to the idea of using mocap 
with a direct interface to the validation software. Second, the single systems con-
nected with each other to a setup that allows the stipulated possibilities are presented. 
This concept enables us to detect possible sources of errors and hazards at an early 
stage, allowing installing preventive measures. Third, in addition to the product-
related virtual validation, the modeling and simulation-based design of a manual 
workstation is performed with DELMIA to integrate the virtual human model to per-
form further ergonomic analysis within the virtual validation. Additional the EAWS 
analysis is performed with the ALASKA software. Summarized, this setup allows a 
time saving teaching of the human model, a realistic movement of the worker, so that 
digital becomes virtual, and an easy way to give an ergonomic feedback about the 
human-machine-system. 
6 
Future Work 
This work has focused the application of the presented system regarding integration 
and testing the possibilities. Some remaining steps, i.e. force feedback and collision 
analysis, present future work. A particular problem therein is the virtual validation of 
flexible components, which is currently being researched. In order to evaluate the 
ergonomics of the necessary assembly steps, an extension to the virtual human model 
providing such information is a key research objective in the current eProduction 
research project and thus also presents future work. 
Acknowledgments. This research and development project was funded by the Ger-
man Federal Ministry of Education and Research (BMBF) within the grant 
13N12036. The authors are deeply grateful for this support and are responsible for the 
contents of this publication. Furthermore the authors are grateful to Norman Hofmann 
and Dr. Andreas Müller from IfM and Dr. Jerome Perret from Haption for the soft-
ware support. 
References 
1. Wack, K.-J., Bär, T., Straßburger, S.: Limitations of digital ramp-up validation. In: Zülch, 
G., Stock, P. (eds.) Integration Aspects of Simulation: Equipment, Organization and Per-
sonnel, October 7-8, pp. 45–52. KIT Scientific Publ., Karlsruhe (2010) 
2. Bönig, J., Fischer, C., Marquardt, V., Matzka, S., Franke, J.: Methodical Integration of As-
sembly Specific Influences concerning High-Voltage Components into the Virtual Valida-
tion Process. In: Franke, J. (ed.) 2nd International Electric Drives Production Conference, 
EDPC 2012, Nuremberg, October 16-17 (2012) 

472 
J. Bönig et al. 
3. Mühlstedt, J., Kaußler, H., Spanner-Ulmer, B.: Programme in Menschengestalt: digitale 
Menschmodelle für CAx- und PLM-Systeme. The software incarnate. Zeitschrift für Ar-
beitswissenschaft 62(2), 79–86 (2008) 
4. Chaffin, D.B.: Digital human modeling for vehicle and workplace design. In: SAE-R, 
vol. 276. Society of Automotive Engineers, Warrendale (2001) 
5. Chaffin, D.B.: Improving digital human modelling for proactive ergonomics in design. Er-
gonomics 48(5), 478–491 (2005), doi:10.1080/00140130400029191 
6. Farell, K.: Kinematic Human Modeling and Simulation using Optimization-Based Posture 
Prediction. Master Thesis, University of Iowa (2006) (accessed September 14, 2012) 
7. Maschke, P.: Herausforderungen in der Montage von Elektrofahrzeugen. Hochvoltkompo-
nenten. ATZ Produktion 3(4), 24–27 (2010) 
8. Schlick, C., Bruder, R., Luczak, H.: Arbeitswissenschaft, 3rd edn. Springer, Heidelberg 
(2010) 
9. Schreiber, W., Zimmermann, P. (eds.): Virtuelle Techniken im industriellen Umfeld. 
Springer, Heidelberg (2011) 
10. Flick, D.R.: Virtuelle Absicherung manueller Fahrzeugmontagevorgänge mittels 3-D-
Menschmodell - Optimierung der Mensch-Computer-Interaktion. Dissertation, Technische 
Universität München (2010) (accessed February 21, 2012) 
11. Riedl, M.: Potential eines virtuellen Fahrerplatzmodells in der Fahrzeugkonzeptentwick-
lung. Dissertation, Technische Universität München (accessed August 27, 2012) 
12. Advanced Realtime Tracking GmbH: ART DTrack2 user manual 
13. CyberGlove Systems LLC: CyberGlove® II Wireless Data Glove - User Guide (2009) 
14. Fuchs, P., Moreau, G., Guitton, P.: Virtual reality. Concepts and technologies. CRC Press, 
Boca Raton (2011) 
15. Haption: Real-Time Interaction for Delmia Human, RTID (2012) 
16. Haption: Interactive Physics Simulation Interface, IPSI (2012) 
17. Kühn, W.: Digitale Fabrik. Fabriksimulation für Produktionsplaner. Hanser, München 
(2006) 
18. Dassault Systemes: DELMIA: Virtual Ergonomics Solution (2008), 
http://www.3ds.com/fileadmin/PRODUCTS/DELMIA/OFFERS/ 
Virtual-Ergonomics-Solutions/PDF/DELMIA-Virtual-Ergonomics-
brochure.pdf (accessed August 29, 2012) 
19. Härtel, T., Keil, A., Hoffmeyer, A., Toledo Munoz, B.: Capturing and Assessment of Hu-
man Motion during Manual Assembly Operation. In: First International Symposium on 
Digital Human Modeling, Lyon, France, June 14-16 (2011) 
20. McAtamney, L., Corlett, E.N.: RULA: a survey method for the investigation of world-
related upper limb disorders. Applied Ergonomics 24(2), 91–99 (1993) 
21. Schaub, K., Caragnano, G., Britzke, B., Bruder, R.: The European Assembly Worksheet. 
Theoretical Issues in Ergonomics Science, 1–23 (2012) 
22. Dell GmbH, http://www.dell.de (accessed August 23, 2012) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 473–482. 
DOI: 10.1007/978-3-642-30817-8_46 
© Springer-Verlag Berlin Heidelberg 2013 
 
Simulation of Variation in Assembly Forces Due  
to Variation in Spot Weld Position 
Kristina Wärmefjord, Rikard Söderberg, and Lars Lindkvist 
Department of Product and Production Development, Chalmers University of Technology,  
SE-412 96 Göteborg, Sweden 
{kristina.warmefjord,rikard.soderberg,lali}@chalmers.se 
Abstract. Resistance Spot Welding is frequently used for joining of sheet metal 
assemblies. Geometrical variation in parts to be joined, variation in fixtures and 
lack of repeatability in welding guns and robots, result in variation in spot weld 
position. Analysis of industrial scanning data showed deviations of spot weld 
positions of magnitudes up to 19 mm. This variation leads to variation in the in-
itial gap between the parts to be joined and therefore, also to variation in the as-
sembly force required to join the parts. In this paper, a simulation method for 
prediction of variation in assembly forces due to variation in spot weld position 
is presented.  
Keywords: resistance spot welding, variation, welding gun, welding force, as-
sembly force, joining, tolerances, variation simulation. 
1 
Introduction 
Resistance spot welding (RSW) is the dominant joining method in automotive indus-
try. Spot welds of good quality are important in order to fulfill key performance cha-
racteristics of the car, such as stiffness and crash behavior. Welding force, together 
with welding current and time, are the most important parameters which affect the 
functional performance of a spot weld. In RSW, a force must be applied to press the 
parts to be joined against each other in order to allow the current to pass through both 
parts. If the force is too low or too high the resulting joint will probably decrease in 
strength. Due to variation in individual manufacturing processes, the parts may not be 
nominal, and this, together with variation in the assembly process, may lead to gaps 
between the parts in the locations of the weld points. Those initial gaps need to be 
closed, and some of the predefined welding forces are then consumed in order to close 
the gaps. Therefore, it is important to take the variation in magnitude of the initial 
gaps into consideration when defining suitable welding forces. The initial gap is 
strongly affected by the position of the spot weld. In this paper the coupling between 
variation in spot weld position and variation in the force needed to close the initial 
gap is investigated and a simulation method for predicting variation in assembly 
forces is proposed. The term assembly force will be used to describe the force  

474 
K. Wärmefjord, R. Söderberg, and L. Lindkvist 
 
required to close the initial gap, while welding force refers to the total force applied 
by the welding gun.  
In the following two sections, introductions to methods for variation simulation 
and spot welding in automotive industry are given. In those sections, also an overview 
of relevant literature is given.  
2 
Methods for Variation Simulation 
Geometrical variation in parts is a result of previous manufacturing processes. The 
variation propagates from parts throughout the assembly systems and may lead to 
products not fulfilling requirements, with high costs and reduced competitiveness as a 
consequence. To foresee, and thereby also possibly reduce, these kinds of problems in 
a final assembly, methods to predict geometrical variation are crucial tools [1]. Ac-
tivities of this kind, often referred to as variation simulation, are performed in early 
stages of the product development process in automotive industry. With increased 
demands on sustainability, virtual tools for variation simulation will be even more 
important in the future, since they can reduce the need of physical tests and pre-series. 
Variation simulation is in many cases based on Direct Monte Carlo (DMC) simula-
tion [2], where statistical distributions for the parameters affecting a critical dimen-
sion are defined. By sampling those distributions, the resulting value in the critical 
dimension can be computed. This procedure is repeated a number of times and there-
by a distribution of the critical dimension can be approximated. By incorporating FEA 
techniques, non-rigid tolerance analysis may be performed. This analysis may be used 
to simulate how non-rigid parts or subassemblies of non-rigid parts, such as sheet 
metal and plastic components, behave after assembly.  
Lee et al. [3] examine how welding sequences for continuous welding can be in-
cluded in variation simulations by using a pre-generated database. Wärmefjord et al. 
[4] include the welding sequence in variation simulation and verify the result on an 
industrial case study. Hu et al. [5] investigate the effect of the welding sequence on a 
dash panel assembly.  
3 
RSW in Automotive Industry 
During all welding, heat is generated. This may lead to deformations of the parts. For 
spot welding, this deformation is usually of minor importance [20] and is not included 
in this work. 
The RSW process is mainly influenced by the welding time, welding current and 
welding force. The welding force, or electrode force, applied ensures the electrical 
contact and retains the weld nuggets from weld expulsion, which is ejection of molten 
material from the weld nugget during welding. Zhang et al. [6] conclude that the elec-
trode force is the second most important parameter to control expulsion. Hamedi et al. 
[8] minimize the deformation of the final assembly by optimizing welding current, 
welding time and electrode force. The RSW process significantly affects crashworthi-
ness of a vehicle [9].  

 Simulation of Variation in A
 
Before welding, the part
after stamping, influence fr
deviation from its nominal 
to be welded. This initial ga
trode force can be increase
the welding contact zone an
small nugget size. Both ex
strength. Therefore, it is im
find a suitable electrode fo
initial gap is investigated b
and approximation curves 
initial gap varies of course 
portance to be able to pred
As shown later in the paper
variation in spot weld posit
to close the gap. 
Most of the previous wo
necessary joining force trie
a Monte Carlo based variat
the initial gap, an interval
described later in this paper
 
Fig. 1. The actual spot weld p
black circles) 
The positions of the spo
such as stiffness and geom
affect the required welding 
 
Assembly Forces Due to Variation in Spot Weld Position 
ts are clamped in the assembly fixture. Due to springb
om previous assembly steps, positioning errors, weld po
position etc., there will be gaps between the parts/flan
ap influences expulsion [10]. To avoid expulsion, the el
ed [11]. A too high electrode force will however incre
nd thereby decrease the electric resistance, leading to a 
xpulsion and a too small nugget size lead to reduced w
mportant to take the initial gap into consideration in orde
orce for a sound spot weld. The force needed to close
by Murakawa and Ueda [12], who apply parametric cur
to two simple structures. The force required to close 
with the variation in the gap. Therefore, it is of great 
dict this gap/force variation in order to design good RS
r, the initial gap is dependent on the spot weld position 
tion leads therefore to variation in assembly forces requi
ork investigating the correlation between initial gap and 
s a couple of different values of the initial gap. By runn
tion simulation, aiming to predict the force needed to cl
l of probable values of this force can be simulated,
r.  
 
position compared to nominal spot weld position (indicated w
ot welds affect many characteristics of the final prod
metrical deviations. As shown later in the paper they a
force. The variation in spot weld position is caused by: 
475 
back 
oint 
nges 
lec-
ease 
too 
weld 
er to 
e an 
rves 
the 
im-
SW. 
and 
ired 
the 
ning 
lose 
, as  
with 
duct, 
also 

476 
K. Wärmefjord, R. Söderberg, and L. Lindkvist 
 
• geometrical variation on the part level in areas where spot welds are located;  
• variation in the positioning of the parts to be assembled; 
• wearing of electrodes on the welding gun;  
• lack of repeatability in the robot and the welding gun.  
In [13], the effect of variation in spot weld position on geometrical variation in a 
number of critical measures was analyzed.  
To investigate the magnitude of the variation in spot weld position in automotive 
industry, spot-welded assemblies were scanned and analyzed. The instrument used 
was a 3D scanner with phase-based distance sensor. In Fig. 1.  a scanning of a spot-
welded assembly from an automotive manufacturer is shown. The deviation between 
nominal and actual positions of the spot welds is in many cases quite large. For a 
number of more closely analyzed spot welds, the maximum deviation from nominal 
position amounted to 19 mm. The mean deviation was 9 mm. 
4 
Simulation Methodology 
To be able to simulate the effect from variation in spot weld position on required 
welding forces and to build a distribution for the welding forces in all different weld 
points, Monte Carlo simulation is used. The Monte Carlo simulation is in this case 
enclosed in the software RD&T [14], where a total sensitivity matrix is implicitly 
defined in a FEA-based simulation model describing all mating conditions, kinematic 
relations and non-rigid behavior. 
The inputs to a general variation simulation are, in the case of non-rigid simulation, 
part meshes, material properties, locating schemes, tolerances of the parts and fix-
tures, and information about the joining process – i.e. joining method, position of the 
joining points and joining sequence.   
Direct Monte Carlo simulation combined with FEA is a standard technique for var-
iation prediction of compliant parts. However, since a large number of replications are 
required for achieving satisfactory accuracy, the method is very time-consuming if a 
new FEA calculation is to be executed in each replication. Liu and Hu [15] presented 
a technique called Method of Influence Coefficients (MIC) to overcome this draw-
back, and this method is used in the work described in this paper. The idea of MIC is 
to find a linear relationship between part deviations and assembly springback devia-
tions. The sensitivity matrix, constructed using FEA, describes that linear relation-
ship. This sensitivity matrix is then used in the simulation, and a large number of FEA 
calculations can be saved. 
Contact modeling is another important aspect to include in non-rigid variation si-
mulations. It is a way to avoid that parts in the virtual model penetrate each other due 
to possible imperfections and variations in parts and tools. Instead, the resulting 
forces due to collisions are transferred to the parts via contact surfaces. Here, a point 
based contact algorithm is used [16].  
The force needed to close an initial gap in welding point i (wpi) is highly dependent 
on active contact conditions in the surrounding geometry. The distance to the closest 
contact points affects the simulated welding force. In order to minimize differences 

 Simulation of Variation in Assembly Forces Due to Variation in Spot Weld Position 
477 
 
due to this effect, contact points are placed in all nodes adjacent to the welding point. 
This is illustrated in Fig. 2. Even though the position of the welding point is varied, 
the welding point is always surrounded by contact points.  
 
 
 
Fig. 2. A weld point and its surrounding nodes. Contact points are placed in all nodes adjacent 
to the weld point (i.e. in Node 1-7 in the figure). 
The joining sequence is also taken into consideration in the simulation methodolo-
gy. For a non-rigid assembly, the following steps are generally necessary to take into 
consideration when predicting variation and deviation in critical dimensions of the 
final assembly:  
• Step 1: The parts are positioned/clamped in their fixtures, and over-constrained 
locating systems (i.e. clamps) are applied. Forces are applied to clamp non-nominal 
parts. 
• Step 2: The parts are welded together in a pre-defined welding sequence. The gaps 
in the weld points are closed, one by one. 
• Step 3: After the last joint is set, the assembly is unclamped and is allowed to 
springback. 
The third step is not of interest in this work, since the focus is on determining the 
welding forces required in Step 2.  
In the work by Wärmefjord et al. [17], Step 1-3 above is described in detail for 
RSW when a balanced gun is used. The assembly forces are calculated in each MC 
replication. The simulation method to do this is based on the following steps: 
Modeling of step 1: Clamping the parts in the fixture 
The parts a and b are positioned in their fixtures and over-constrained locating sys-
tems are applied. The gaps to be closed in the clamping points are gathered in the 
vectors ሼܝܘ
܉ሽ and ሼܝܘ
܊ሽ respectively. To close the gaps in the clamping points, forces 
൛۴ܘ
܉ൟ and ൛۴ܘ
܊ൟ respectively are applied. The part stiffness matrices are denoted ሾࡷܘ
܉] 
and ሾࡷܘ
܊] respectively. The relations between forces and gaps can be described as: 
 

478 
K. Wärmefjord, R. Söderberg, and L. Lindkvist 
 
 
൛۴ܘ
܉ൟ= ൣࡷܘ
܉൧ሼܝܘ
܉ሽ       
(1) 
 
൛۴ܘ
܊ൟ= ൣࡷܘ
܊൧ሼܝܘ
܊ሽ 
(2) 
Modeling of step 2: Welding the parts, weld point (wp) i=1,...,N 
To set wp i, a force ሼ۴ۯ
ܑሽ, where the index A stands for assembly, is applied and the 
relation becomes:  
 
ሼ۴ۯ
ܑሽ= ሾࡷۯ 
ܑି૚ሿሼܝۯ
ܑି૚ሽ 
(3) 
After wp i is set, the assembly is released from its fixture and will then springback 
(the release step is executed since it gives clearer and easier calculations compared to 
accumulating the resulting forces for each joint). The stiffness matrix ൣࡷۯ
ܑ൧, used to 
calculate the springback, describes the stiffness of the assembly after wp i is set and is 
determined in Equation (4).  
After the springback, the assembly is brought back to its position by applying the 
clamps once more, and the required force ሼ۴ۯ
ܑሽ to do this, taking both clamping forces 
and forces due to contact modeling into consideration, is registered. 
The stiffness matrix ሾࡷۯ
ܑሿ is updated for every new wp by adding, to the stiffness 
matrix from the previous step, a new matrix ሾࡷܟܘሺܑ܉,ܑ܊ሻ
ܑ
ሿ locking three translations and 
three rotations corresponding to the added joint. This means that 
 
ሾ۹ۯ
ܑሿ= ሾ۹ۯ
ܑି૚ሿ+ ሾ۹ܟܘሺܑ܉,ܑ܊ሻሿ 
(4) 
For the very first wp, the matrix ሾ۹ۯ
૙ሿ refers to the original part stiffness matrices, i.e. 
one for each part. The deviation used for the first welding point, ሼܝۯ
૙ሽ, corresponds to 
the part deviations.  
It should be noted that, with the terminology used in this paper, the assembly force 
ሼ۴ۯ
ܑሽ only refers to the force required to close the initial gap in wp i, and does not 
include the force needed to form a sound weld. 
5 
Case Study 
In this section an industrial case study is presented. The case study is an A-pillar as-
sembly, consisting of two parts: the A-pillar and its extension, shown in Fig. 3. The 
A-pillar and the extension are welded together with eight spot welds. The welding 
sequence is also illustrated in Fig. 4. For this case study, the position of the spot welds 
will be varied and the resulting variation in assembly forces will be investigated. The 
inputs in the simulation consist of: 
• scan data, represented by FE meshes, of both of the  geometries included in the 
assembly on part level; 
• tolerances for spot weld position. 
Variation simulation for this case study (without variation in spot weld positions, but 
based on the scanned geometries) aiming at predicting the deviation from nominal 

 Simulation of Variation in Assembly Forces Due to Variation in Spot Weld Position 
479 
 
values in a number of inspection points has previously been conducted and the results 
have been validated against industrial scanning data of the complete assembly [18], 
[21]. The correlation between simulated results and scanning data of a complete as-
sembly amounted to 0.94 for inspection points located on the A-pillar and 0.87 for 
inspection points on the extension.  
 
Fig. 3. The A-pillar assembly consists of two parts, the A-pillar and its extension, which are 
joined with eight spot welds. The figures show the welding sequence. 
The outcome from the scanning shown in Fig. 2 is used to estimate a reasonable to-
lerance for the spot weld position. To each spot weld position, a circular tolerance 
consisting of an angle and a radius is allocated. The angle is uniformly distributed 
between 0 and 2π and the radius is uniformly distributed on [0, 7.5] mm. The element 
lengths in the mesh are approximately 3 mm, and since the node closest to the ran-
domly generated spot weld position (according to the defined tolerance) is chosen as 
the weld point, the actual spot weld position may vary within a circle with radius up 
to 9 mm. A Monte Carlo simulation with 5000 replications is run. In each replication, 
a disturbance for every spot weld position is randomly generated and the node pair 
closest to the new position is chosen as the new position of the spot weld. For each 
new position of a spot weld, contact points are placed in nodes adjacent to the spot 
weld node. No other sources of variation, except the variation in spot weld position, 
are added. As already mentioned, the meshes for the individual parts are, however, 
based on non-nominal scanning data from industrial production. 
In Fig. 4 the distribution of the assembly forces for each one of the eight spot 
welds are shown. As can be seen, there are quite large variation in the force required 
to close the gap between the two parts to be joined for each spot weld. For wp7, the 
minimum value is 0.01 kN, while the maximum required assembly force amounts to 
2.7 kN. For wp8 there are actually three replications resulting in a value of approx-
imately 6.2 kN, this is however just three values out of 5000. The fourth largest value 
for wp8 is 2 kN.  
It can be emphasized once more that this variation in assembly forces is  due to 
variation in spot weld position, where the size of the variation is estimated from  
industrial scan data, and from part deviations from nominal, which in this case is 
based on scanned geometries from industrial production. 

480 
K. Wärmefjord, R. Söderberg, and L. Lindkvist 
 
 
Fig. 4. Distribution of assembly forces in the eight spot welds 
6 
Discussion 
The results presented in the previous section indicate that there can be large variation 
in the assembly force, i.e. the force required to close the initial gap. For the welding 
points showing the largest range in values, the assembly force goes from 0.01 kN to 
2.7 kN.  This can be compared to the values of typical welding forces between 1.5 kN 
and 3 kN given by Dennison et al. [18] or the values from a large spot welder manu-
facturer [19] where 3.9-5.2 kN is said to be a typical welding force for uncoated low 
carbon steel of thickness 1.6 mm if high force, short time conditions are used. If me-
dium force and long time conditions are used, the corresponding value is 2.6-2.9 kN.  
Assuming that a welding force of 3 kN is chosen, most of the force will in many 
cases be consumed in order to close the gaps, which may lead to welds of bad quality. 
If a higher welding force is chosen, in order to have some safety margins, the welding 
force will sometimes be too high, leading to too small welding nuggets.  
The investigations of assembly forces for RSW included in this paper are applied 
to a case study consisting of low carbon steel, and as stated by Shen et al. [20], the 
critical value of the initial gap necessary to avoid expulsion is much smaller when 
using dual phase steel rather than low carbon steel. That means that dual phase steel is 
even more sensitive to variation in welding forces than the low carbon steel used in 
the case study shown here. 
In order to avoid the problems with varying assembly forces due to variation in 
spot weld position, the variation must be reduced, the welding process must be robust 
to this kind of variation or an adaptive welding force with sensors identifying when 
the initial gap is closed must be used.  

 Simulation of Variation in Assembly Forces Due to Variation in Spot Weld Position 
481 
 
None of those points are very simple or economically viable to achieve. The main 
aim of this paper is to call attention to the problem and to demonstrate the effects of 
the variation in spot weld position. To reduce the variation in spot weld position, the 
contribution from the different sources listed in Section 3 should be investigated. 
Repeatability studies of the robot and welding gun are probably a good start.  
7 
Conclusions 
In this paper the effect of variation in spot weld position with respect to assembly 
forces has been investigated. Assembly force is defined as the force required to close 
the initial gap between two parts to be joined. The assembly force is a subset of the 
total welding force, which is the total force applied from the welding gun. It is well 
known that the welding force is an important parameter in a welding scheme aiming 
at spot welds of good quality.  
In order to get realistic values of the presumed values of the tolerance for spot weld 
position, scanning of industrial case studies were carried out. Those showed a  
deviation of the spot weld position from nominal value with up to 19 mm and a mean 
deviation of 9 mm. In the simulation in this paper, a tolerance with a radius of 9 mm 
was used.  
To investigate the effect of the variation in spot weld position with respect to  
assembly forces, an industrial case study was used. The two parts of an A-pillar as-
sembly, taken from industrial production, were scanned and this scanning was used as 
input to the simulations. On those scanned geometries, the weld point position was 
varied within given tolerances. 
The results showed that the variation in weld point position leads to quite large var-
iations in assembly forces. For the weld point showing the largest range in assembly 
forces, the required force varied between 0.01 kN and 2.7 kN. Since the welding force 
normally is fixed and in the magnitude of 2-5 kN for this kind of case, the variation in 
assembly forces due to variation in spot weld location can lead to spot welds of poor 
quality. 
Acknowledgements. This work was carried out at the Wingquist Laboratory VINN 
Excellence Centre within the Area of Advance – Production at Chalmers, supported 
by the Swedish Governmental Agency for Innovation Systems (VINNOVA). The 
support is gratefully acknowledged. 
References 
1. Söderberg, R., Lindkvist, L., Carlson, J.: Virtual Geometry Assurance for Effective Prod-
uct Realization. In: NordPLM 2006, Göteborg, Sweden (2006) 
2. Gao, J., Chase, K., Magleby, S.: Comparision of Assembly Tolerance Analysis by the Di-
rect Linearization and Modified Monte Carlo Simulation Methods. In: Proc. ASME DETC 
(2009) 
 

482 
K. Wärmefjord, R. Söderberg, and L. Lindkvist 
 
3. Lee, D., Kwon, K.E., Lee, J., Jee, H., Yim, H., Cho, S.W., et al.: Tolerance Analysis Con-
sidering Weld Distortion by Use of Pregenerated Database. J. Manuf. Sci. Eng. 131 (2009) 
4. Wärmefjord, K., Söderberg, R., Lindkvist, L.: Strategies for Optimization of Spot Welding 
Sequence With Respect to Geometrical Variation in Sheet Metal Assemblies. In: Proc. 
ASME IMECE (2010) 
5. Hu, M., Lin, Z., Lai, X., Ni, J.: Simulation and analysis of assembly processes considering 
compliant, non-ideal parts and tooling variations. Int. J. of Machine Tools and Ma-
nuf. 41(15), 2233–2243 (2001) 
6. Zhang, H., Hu, S.J., Senkara, J., Cheng, S.: A Statistical Analysis of Expulsion Limits in 
Resistance Spot Welding. J. Manuf. Sci. Eng. 122, 501–510 (2000) 
7. Hamedi, M., Shariatpanahi, M., Monsourzadeh, A.: Optimizing spot welding parameters in 
a sheet metal assembly by neural networks and genetic algorithm. Proc. Institution of Me-
chanical Engineers, Part B: Journal of Engineering Manufacture 221 (2007) 
8. Sun, X., Stephens, E.V., Khaleel, M.A.: Effects of fusion zone size and failure mode on 
peak load and energy absorption of advanced high strength steel spot welds under lap shear 
loading conditions. Engineering Failure Analysis 15(4), 356–367 (2008) 
9. Murakawa, H.: Simulation of Resistance Welding for Selection of Optimum Welding 
Conditions and Process Control. Trans. JWRI 32(1) (2003) 
10. Shen, J., Zhang, Y.S., Lai, X.M.: Effect of Electrode Force on Expulsion in Resistance 
Spot Welding with Initial Gap. Materials Science Forum, 675–677, 795–798 (2011) 
11. Murakawa, H., Ueda, Y.: Mechanical study on the effect of the initial gap upon the welda-
bility of spot weld joint. Trans. of JWRI, 51–58 (1989) 
12. Söderberg, R., Wärmefjord, K., Lindkvist, L., Berlin, R.: The influence of spot weld posi-
tion variation on geometrical quality. CIRP Annals - Manufacturing Technology (2012) 
13. RD&T Manual, Mölndal, ver. (1.07) (2009)  
14. Liu, S.C., Hu, S.J.: Variation Simulation for Deformable Sheet Metal Assemblies Using 
Finite Element Methods. J. of Manuf. Sci. and Eng. 119, 368–374 (1997) 
15. Wärmefjord, K., Söderberg, R., Lindkvist, L.: Tolerance simulation of compliant sheet 
metal assemblies using automatic node-based contact detection. In: Proc ASME IMECE 
(2008) 
16. Wärmefjord, K., Söderberg, R., Lindkvist, L.: Variation Simulation of Spot Welding Se-
quence for Sheet Metal Assemblies. In: Proc. 8th International NordDesign Conference 
2010, Gothenburg, Sweden, pp. 519–528 (2010) 
17. Dennison, A., Toncich, D., Masood, S.: Control and Process-Based Optimisation of Spot-
Welding in Manufacturing Systems. Int. J. Adv. Manuf. Technol. 13, 256–263 (1997) 
18. PW Resistance Welding Products Ltd. PW Resistance Welding Products Ltd. (2011), 
http://www.portablewelders.com/spot-weld-technical.html  
(retrieved March 05, 2011) 
19. Shen, J., Zhang, Y.S., Lai, X.M.: Influence of initial gap on weld expulsion in resistance 
spot welding of dual phase steel. Sci. and Tech. of Welding & Joining 15(5), 386–392 
(2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 483–492. 
DOI: 10.1007/978-3-642-30817-8_47 
© Springer-Verlag Berlin Heidelberg 2013 
 
Cutting Tool Data Representation and Implementation 
Based on STEP AP242 
Yujiang Li, Mikael Hedlind, Torsten Kjellberg, and Gunilla Sivard 
Production Engineering, KTH Royal Institute of Technology, Stockholm, 10044, Sweden 
{yujiang.li,mikael.hedlind,torsten.kjellberg, 
gunilla.sivard}@iip.kth.se 
Abstract. For cutting tool data exchange in manufacturing CAx (Computer-
Aided technologies), standardized representation and classification of items and 
properties is important. ISO 13399 (Cutting tool data representation and ex-
change) provides a solution to represent cutting tool data classified with an ISO 
13584 (Parts Library, PLib) based dictionary. However, ISO 13399 does not 
support classification of shape geometry directly, which limits its use. Another 
limitation is representing GD&T (Geometric Dimensioning and Tolerancing) as 
simplified general properties, which does not fulfill high semantic precision and 
validation rules. This research provides a unified solution to represent cutting 
tool parameters integrated with geometry and dedicated properties based on 
STEP AP242 (ISO 10303-242 Managed model-based 3D engineering). Stan-
dardized libraries such as the ISO 13399 dictionary can be reused with the 
modeling approach for AP242 cutting tool representation. Software is devel-
oped to validate and demonstrate how this solution facilitates the data integra-
tion process to support CAx applications. 
Keywords: Modeling, STEP AP242, Cutting tool, Classification. 
1 
Introduction 
Collaborations between enterprises put high requirements on seamless digital infor-
mation exchange and sharing between software systems for industry. Standardized 
representation and classification of items and properties is fundamental for cutting 
tool data exchange between applications of PLM, CADCAM and CNC (Product Life-
cycle Management, Computer-Aided Design, Computer-Aided Manufacturing, and 
Computer Numerical Control). With PLM applications, engineering information of 
cutting tools is integrated to support its development and deployment through the 
lifecycle. With CAD applications, cutting tool geometry, assembly structure, and 
relevant properties are defined in different viewpoints for production and utilization. 
With CAM applications, cutting tool requirements and usages are defined for opera-
tions with tool paths and cutting parameters. With CNC applications, operations are 
executed with input on cutting tool types, main dimensions, and tolerances on tool 
wear. For decades, information management has been regarded as the essence of  

484 
Y. Li et al. 
cutting tool management for efficient computerized manufacturing control [1]. How-
ever, differences in terminology and data format between different computer-aided 
software are blocking cross-system interoperability.  
An unambiguous way for the cutting tool data modeling and exchange is ontology 
based on industrial standards. A standardized hierarchy of classes and properties is the 
basic structure to describe taxonomies, as for cutting tool ontology. An important 
contribution in this area is the dictionary of cutting tool classes and property types 
within ISO 13399 (Cutting tool data representation and exchange). The dictionary is 
standardized underlying PLib (ISO 13584, Parts Library). A data set conforming to 
ISO 13399 is cutting tool data representation including its classification referencing 
the dictionary. On the other hand, shape representation is set outside the scope of 
ISO 13399. If needed, geometric models in other formats, e.g. STEP (ISO 10303, 
STandard for the Exchange of Product data), can be referenced from the data set. This 
separation approach is traditional for PLM systems.  
A barrier for implementation based on ISO 13399 is the multiple data sets and as-
sociated data schemas that must be managed. For instance, an ISO 13399 cutting tool 
data set is transferred from a tool supplier to its customer with a corresponding geo-
metry data set of STEP AP214 (ISO 10303-214, Application protocol: Core data for 
automotive mechanical design processes). System developers have to establish and 
maintain data integration based on the two standards and find a convenient way to 
convey both data sets without data loss. ISO 13399-1 is mainly a subset of AP214, 
which makes the practical situation even worse. As a result, there are two standards 
with mainly equivalent schemas to be synchronized for implementation and mainten-
ance. Besides, the ISO 13399-150 usage guidelines describe implementation  
differently from how it is done for STEP AP214. It results in great efforts for imple-
menters to develop and coordinate separate implementations.  
The design of schemas also makes ISO 13399 differ from STEP in representation 
structure. It can be observed that both standards share the same functionality of 
GD&T (Geometric Dimensioning and Tolerancing) representation but with different 
modeling approaches. In STEP, the dedicated representation schema for GD&T is a 
basic functionality that is preferably reused for cutting tools, and the connection to 
geometry can be easily established. ISO 13399 instead uses general properties without 
a geometric context for shape dimensions. This does not meet high requirements for 
data consistency and interpretation precision in future CADCAM and CNC applica-
tions. A lack of a common schema for the geometry and GD&T results in a lack of a 
complete context and an unambiguous representation. With the representation of e.g. 
diameter classified as cutting diameter, the link to a specific geometry shape element 
is important information required in industry. 
A standardized approach to represent cutting tool information is important for 
modern industry. The dictionary provided by ISO 13399 is promising for a system 
independent cutting tool library of CAM systems, which is able to establish interfaces 
between various tool makers and their customers [2]. Since it fulfills its defined scope 
for cutting tool data representation, ISO 13399 has contributed to several researches. 
Kaymakci et al. [3] adopt concepts from ISO 13399 for a general prediction model  
of inserted cutters, where the demands for “a unified geometric, kinematic, and  

 
Cutting Tool Data Representation and Implementation Based on STEP AP242 
485 
mechanics model” are not what ISO 13399 is able to meet solely. Helgoson and Kal-
hori [4] use ISO 13399 for cutting tool data exchange in the context of machining 
process planning, where the solid model and cutting tool parameters are separately 
represented with STEP format and ISO 13399. Chungoora et al. [5] highlight the 
problems for joints usage of standard and present an ontology-based framework to 
consolidate various production information standards, e.g. ISO 10303, ISO 13399, 
ISO 13584, and ISO 15531. Generally speaking, integrated geometry models are 
commonly required regarding the adoption of ISO 13399. It becomes a must that 
multiple standards are integrated to achieve a complete modeling solution. Therefore, 
a modeling approach with a unified standard architecture such as STEP will be help-
ful for cutting tool data exchange. 
Within the framework of STEP AP214, dimensions and tolerances are defined in a 
specific UoF (unit of functionality). Classification of items based on PLib is sup-
ported by AP214 in another UoF, which provides association with any dictionary 
conforming to PLib, e.g. the ISO 13399 cutting tool library. The draft standard AP242 
(ISO 10303-242 Managed model-based 3D engineering) is going to replace AP214. 
Added capabilities in AP242 include supports for the Geometrical Product Specifica-
tions (GPS) standard. Dedicated schemas for GD&T and external references are inte-
grated into this protocol and can be used to represent the needed information for 
CADCAM and CNC cutting tool data exchange as well as PLM applications. 
In this research, STEP AP242 and ISO 13399 cutting tool library are combined for 
the comprehensive cutting tool modeling. Products, GD&T, features, general proper-
ties, and classes are basic elements in the model. The following section introduces the 
standardized modeling approach based on AP242, and the mapping strategy to a UML 
data model for implementation. The third section presents a prototype implementation 
which is able to classify AP242 cutting tool data sets with the ISO 13399 PLib-based 
dictionary.  
2 
Cutting Tool Modeling 
Multiple types of information regarding cutting tool data are considered in this re-
search, which requires data integration among schemas. The standardized product 
generic information modeling schema in STEP is able to support association of 
classes and properties defined in the ISO 13399 dictionary with products, shape ele-
ments, GD&T, features, and other elements. This capability also applies for different 
levels of details for different user requirements e.g. geometric level of details. For 
instance, tool suppliers use a complete model with a high level of detail for internal 
data exchange, but tool customers may only need the basic cutting tool parameters. 
Thus, exact geometry and other detailed design requirements, which in many cases 
also are confidential or unnecessary, should be trimmed from the complete model for 
external data exchange. 
Figure 1 presents a STEP AP242 data excerpt of a diameter representation asso-
ciated with a face on a solid body representation. In the model, a diameter of 22.0 mm 
 

486 
Y. Li et al. 
with a tolerance of 0.05 mm and -0.1 mm is associated with a face. All geometric 
information is represented in the established way for STEP, which is omitted in the 
empty block. Using the standard modeling concept of shape aspect, dimension defini-
tion, measure representation and other properties are precisely integrated.  
 
Fig. 1. Example of geometric dimension modeling in STEP AP242 
 
Fig. 2. Example of classification modeling in STEP AP242 
 

 
Cutting Tool Data Representation and Implementation Based on STEP AP242 
487 
Business cases may require hidden shape information, and then there will be no 
link between the dimension definition and the face. Nevertheless, the dimension itself 
is represented in a fixed way independent of existence of shape information. For a 
complete definition of GD&T, a geometric context is needed to establish coordinate 
system for the dimensions, e.g. supplemental geometry. Supplemental geometry is 
also known as help geometry or constructive geometry. Example data types are 
placements, points, curves and faces. These data types are not used to define shapes, 
but to support the definition of design requirements. It is a common CAD functionali-
ty also available in STEP. Recommended practices for implementing supplemental 
geometry are published by the CAx Implementers forum [6]. 
Figure 2 exemplifies the classification modeling approach within AP242. The 
ISO 13399 cutting tool dictionary is referenced in the model. The description of an 
end mill with a usable length is retrieved from the library and associated with the 
corresponding product and dimension.  
3 
Implementation 
As an information modeling approach for computational applications, the result of 
this research should be validated with software implementation. The presented proto-
type software aims to evaluate the feasibility of the proposed solution and to provide a 
development strategy for industrial applications in the future. 
The major function of the software is to classify products and other properties 
based on AP242 referencing the ISO 13399 cutting tool library based on PLib. The 
input is a p21 (ISO 10303-21, clear text encoding of the exchange structure) file of 
AP242 with its assembly structure, shape features, dimensions with tolerances, and 
other properties. Shape representation or supplementary geometry is optional. In a 
case study, the input STEP file is exported from a plug-in to Siemens NX 8.0 that is 
developed in this research project, of which the integration strategy is demonstrated in 
[7]. Assembly tree of cutting tool with the identification of each occurrence is dis-
played as a product part list breakdown. Thus, product parts, shape features, GD&Ts, 
and general properties can be classified according to PLib in an unambiguous way.  
The development of this software reuses and contributes to a Java-based develop-
ment project, STEP Toolbox. The scope of the STEP Toolbox project has a larger 
scope than product classification, e.g. kinematics and geometric errors [8]. In general, 
this project aims to significantly improve the usability of STEP standard for its major 
readers, CAx system developers. A complete solution to simplify STEP implementa-
tion is presented as a programming interface with high maintainability, reusability, 
and extensibility. The toolbox provides modularized functions to process different 
types of product information in engineering oriented perspective, i.e. to manage inte-
grated geometry, kinematics, classification and other kinds of product data. Without 
requiring STEP knowledge, developers can produce their own standalone applications 
or plugins for CAD/CAM software based on the toolbox. The following data model 
design is a result of the research presented in this paper as well as a contribution of 
STEP Toolbox. 

488 
Y. Li et al. 
3.1 
Data Model 
STEP Toolbox aims to provide a friendly programming interface for CAx developers, 
rather than designers in a specific industrial domain. Therefore, concepts should be 
defined in a domain-independent way so that elements are reusable in multiple pro-
gramming modules in the toolbox, e.g. the component is a widely used concept in 
geometry, kinematics, and GD&T. Thus, the toolbox solution adopts ontology model 
mapping from EXPRESS (ISO 10303-11) schemas in order to generate a proper de-
sign of computer interpretable UML model for programming. The ontology also helps 
advanced programmers to understand the conceptual relationship between EXPRESS 
models and UML models. Using the modeling principle outlined by Kjellberg et al. 
[9], an ontology model for cutting tool implementation has been used for mapping 
from AP242 in EXPRESS to a UML model (see Figure 3).  
 
Fig. 3. Cutting tool implementation ontology 
Note that the presented ontology model mainly serves the purpose of supporting 
computational implementation going from EXPRESS to UML. It does not necessarily 
express semantic relationship precisely. For example, a component actually does not 
own PLib class, but it can be multiply defined by several classes, which leads to such 
a composition relationship, e.g. a multi-functional cutting tool can be classified as an 
end mill and a drill. Another example is that semantically the general feature should 
not be defined as a sub-class of the component. However, it also can be classified and 
has classifiable properties, which indicates all the requirements of the general feature 
implementation can be met by the attributes and operations of the component. As a 
result, the general feature is set as the subclass of the component here. Geometry is a 
general concept, of which the subclasses include bodies, faces, edges, and points, 
besides the displayed components in the figure. Both the PLib class and the compo-
nent have self-references to indicate the tree structure for presentation. Properties 
such as general properties and dimensions are associated with certain components or 
general features, which are defined semantically by PLib Property, e.g. a linear dis-
tance associated with a component is defined as usable length in a PLib dictionary 
(see Figure 2). PLib properties have strict ownership from certain PLib classes, which 

 
Cutting Tool Data Representation and Implementation Based on STEP AP242 
489 
can only classify the properties of the components classified with corresponding own-
er PLib classes. 
Programmers can use the ontology model to understand concepts easily, but a well-
designed UML (Unified Modelling Language) model is fundamental for practical 
implementation. As a core part of UML, a class diagram is important of such a Java-
based programming interface. In Figure 4, a simplified class diagram for cutting tool 
is illustrated. Methods of most classes are not displayed, but they are important for 
implementation, such as getters, setters, and constructors. As a part of STEP Toolbox, 
modules are divided and controlled by managers, such as the property manager and 
the classification manager. The STEP model manager provides basic operations for 
STEP data set, such as initialize, export, and close. Differences from ontology may 
occur for specific data model implementation. For example, both components and 
PLib classes have tree structures, but PLib classes also need to record a list of parents, 
rather than only the direct parent which is the case for component. This is caused by 
the inheritance of PLib properties from all relative parents. 
 
Fig. 4. Simplified class diagram for cutting tool development 

490 
Y. Li et al. 
3.2 
System Development 
An MVC (Model-View-Controller) structure design is illustrated in the Figure 5. The 
relative part of STEP Toolbox is integrated as the Model layer. The system starts with 
a file opening dialog. If the file exists and is acceptable, the Initializer triggers the 
STEP model manager to read the file and invokes the PLib parser to generate the 
standard definitions of classes and properties. The GD&T manager and the classifica-
tion manager need to analyse the STEP model, read dimensions, general properties, 
and classification information. Then the assembly controller collects all the data and 
invokes the assembly viewer to display the assembly with GD&T in the graphical 
user interface (e.g. see Figure 6). The operation of classification is started by a se-
lected component from assembly. A classification dialog is used to organise the PLib 
classes and properties in a displayable and selectable way (e.g. see Figure 7). The 
classifier in the controller classifies the products and properties only in the level of 
Java data model rather than the STEP model, and records the updated component 
whenever there is a user operation. Then, the exporter triggered by interface performs 
all the changes to the data set and exports it with specific configurations. 
 
Fig. 5. MVC system design 
3.3 
Interface Design 
The interface is designed to present and manage different elements in the data set with 
a clear structure. Figure 6 illustrates the basic design of the main user interface. Com-
ponents are presented in a tree structure as commonly used in CAx systems. Features 
are displayed with a darker background color and a special category name. Both com-
ponents and features can be selected to classify. The bottom table displays related 
properties of the selected item in the above tree table. 

 
Cutting Tool Data Representation and Implementation Based on STEP AP242 
491 
 
Fig. 6. Main user interface 
The major function of this software is performed after the click of the “Change…” 
button. Then the classification dialog (see Figure 7) pops up with two lists of selec-
tion: a tree list of PLib classes that can be multiply selected, and a table list of PLib 
properties of the selected class. The combo list contains all available properties owned 
by the selected component/feature to be specified, e.g. the linear distance is specified 
as the function length of an end mill in Figure 7. 
 
Fig. 7. Classification dialog 

492 
Y. Li et al. 
4 
Conclusion 
This research focuses on the description and evaluation of a standardized cutting tool 
data modeling approach. A unified computer interpretable model for data exchange 
and data integration is proposed and implemented. Benefits of the comprehensive 
representation structure of STEP AP242 are utilized and demonstrated in developed 
application. External standardized library, such as the ISO 13399 PLib-based dictio-
nary, can be easily reused and associated with the shape representation. Practical data 
exchange is promising by integration developments with current commercial CAx 
systems. With the unified model, industrial practitioners can skip the barrier to coor-
dinate multiple standards and achieve integration development smoothly. 
Acknowledgments. We are grateful for the support from VINNOVA and XPRES 
(Initiative for excellence in production research), and for fruitful discussions with 
members of ISO TC184 SC4 WG3 T24 and industrial practitioners from Sandvik 
Coromant. 
References 
1. Veeramani, D., Upton, D.M., Barash, M.M.: Cutting-Tool Management in Computer-
Integrated Manufacturing. International Journal of Flexible Manufacturing Systems 4(3/4), 
237–265 (1992) 
2. Zelinski, P.: Coming Soon: Universal, CAM-Independent Cutting Tool Library. Modern 
Machine Shop 84(4), 22–24 (2011) 
3. Kaymakci, M., Kilic, Z.M., Altintas, Y.: Unified Cutting Force Model for Turning, Boring, 
Drilling and Milling Operations. International Journal of Machine Tools and Manufac-
ture 54-55, 34–45 (2012) 
4. Helgoson, M., Kalhori, V.: A Conceptual Model for Knowledge Integration in Process 
Planning. Procedia CIRP 3, 573–578 (2012) 
5. Chungoora, N., Cutting-Decelle, A.-F., Young, R.I.M., Gunendran, G., Usman, Z., Harding, 
J.A., Case, K.: Towards the ontology-based consolidation of production-centric standards. 
International Journal of Production Research, 1–19 (2011) 
6. Bay, J., Rosché, P.: Recommended Practices for Supplemental Geometry, Release 1.0. CAx 
Implementor Forum (2010) 
7. Li, Y., Hedlind, M., Kjellberg, T.: Implementation of Kinematic Mechanism Data Exchange 
Based on STEP. In: 7th CIRP-Sponsored International Conference on Digital Enterprise 
Technology, Athens, pp. 152–159 (2011) 
8. Li, Y., Hedlind, M., Kjellberg, T.: Kinematic Error modeling Based on STEP AP242. In: 
1st CIRP Sponsored Conference on Virtual Machining Process Technology, Montreal 
(2012) 
9. Kjellberg, T., Euler-Chelpin, A.V., Hedlind, M., Lundgren, M., Sivard, G., Chen, D.: The 
Machine Tool Model — A Core Part of the Digital Factory. CIRP Annals - Manufacturing 
Technology 58(1), 425–428 (2009) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 493–503. 
DOI: 10.1007/978-3-642-30817-8_48 
© Springer-Verlag Berlin Heidelberg 2013 
 
Assessment of Sensitivity of Numerical Simulation in 
Sheet Metal Forming Process Applied for Robust Design 
Von Dim Nguyen, Pierre-Antoine Adragna, and Pascal Lafon 
Laboratory of Mechanical Systems and Concurrent Engineering (LASMIS), University of 
Technology of Troyes, 12 Rue Marie Curie, CS 42060, 10004 Troyes Cedex, France 
{von_dim.nguyen1,pierre_antoine.adragna,pascal.lafon}@utt.fr 
Abstract. Considering variation of influent factors is a critical issue to enhance 
the robustness of sheet metal forming process in the product design process. 
The stochastic variability of uncontrollable factors results in the variations on 
the formed part which can lead to rejected parts. Since the inherent sources of 
variation in the sheet metal forming process comes from part-to-part, within 
batch and batch-to-batch variation. Therefore, the prediction and control of the 
variability influencing on the performance of the product is an essential demand 
of automotive and aeronautic manufacturers. Moreover, it is very necessary to 
have a numerically dedicated tool which predicts the process variability with a 
good confidence.  In this paper, prediction of the variations of the formed part 
due to the variabilities of the sheet stamping process and the workpiece by nu-
merical simulation will be carried out.  
Keywords: robust design, variability, sheet metal forming, sensitivity analysis. 
1 
Introduction 
Stamping process is an effective process applied for fabricating body panels in auto-
motive manufacturers. The fact that there are around 100 to 150 stamped metal panels 
on vehicles produced nowadays. The process is mainly used in production of large 
batch because designing and manufacturing the stamping tools are very expensive and 
time-consuming [1]. Hence, reduction of time in the tooling design phase as well as 
elimination of expensive physical experiments is considered as objectives which the 
manufacturers would like to obtain. As a solution for this issue, FEA software has 
been a rapid and effective tool for design and verification of new product propositions 
in automotive industries in the last few years. Nevertheless, quality of produced parts 
is one of the most important issues which need to take into account to satisfy custom-
er’s specifications. The stamped parts must respect functional, geometrical aesthetic 
requirements. Thereby, enhancement of reliability of the design by using numerical 
simulation is a focus of this research work.  
In industrial practice, the industrial actors often cope with several defects occurred 
on the stamped parts in which shape defects due to springback, thinning and  
wrinkling are principal problems. The sources provoking these defects are from input 

494 
V.D. Nguyen, P.-A. Adragna, and P. Lafon 
parameters’ variations of the forming process. The variation sources of draw bending 
process are synthesized in Figure 1.  
As a consequence, the variabilities result in poor product quality. In order to en-
hance the robustness of the sheet metal forming processes or in other words, to mi-
nimize the reject rate, the fluctuations must be taken into account in the part and tools 
design stage.  
As mentioned above, the FEM numerical simulation is the solution for shortening 
the lead-time and saving the cost for the experiments in the sheet metal forming 
process. Presently, the FEM software can evaluate any virtual forming process with 
an acceptable accuracy. However, there is still difference between results from nu-
merical simulations and results from physical experiments. The cause of the differ-
ence may be due to inconsistent FE models or incorrect inputs parameters or deviation 
of the input variables [3]. In other words, although the geometry and the material 
properties of the tools and the sheet blank are fixed, the variations in the method of 
FE modeling by users may lead to various results [2]. 
Previously, there were several research works which investigated the effects of 
numerical factors such as the element size of sheet trip, the hardening law, the preci-
sion of modeling tool radii and the dynamic effect on the springback results of the U-
draw bending benchmark problem [4]. He and Wagoner [5] investigated the impact of 
the finite element mesh system of the blank on springback results using the same 
benchmark problem. The effect of the dynamic term on springback was evaluated by 
Chung et al. [6]. Numerical factors affecting springback including contact damping 
parameter, penalty parameter, blank element size, number of corner elements were 
investigated by Lee and Yang, [2]. For the last few years, a couple of investigations in 
relation to the effectiveness of numerical models have been also taken into considera-
tion making comparison between numerical predictions and experimental results [7]. 
Particularly, the influence of numerical parameters comprising the type of the utilized 
element, the number of integration points, the hardening rule and so forth, with the 
aim to improve the effectiveness and reliability of the numerical results. 
 
Fig. 1. Draw bending process and its variation sources 

 Assessment of Sensitivity of Numerical Simulation in Sheet Metal Forming Process 
495 
Xu et al. [8] analyzed the effect of sensitive factors in a U-bending process of Nu-
misheet’93 benchmark problem using a fully explicit solution scheme in which the 
impact of integration points number, blank element size and punch velocity was  
researched.  
It can be seen that all mentioned literatures concentrate on considering the effects 
of numerical parameters on the virtually formed parts, there were hardly any studies 
concerning evaluation of reliability of FEA software. In other words, qualifying the 
sensitive level of numerical simulation tools with very small variations of the scatter-
ing parameters in the sheet metal forming process is crucial to enhance the robustness 
of digital programs in the prediction of variability of the process. Since very small 
variability of the sheet’s material properties, the blank thickness and tooling parame-
ters influence on finished part, particularly, springback variation in sheet metal  
forming process.   
Therefore, the purpose of this research work is to focus on evaluation of prediction 
capability of the stamped part’s variation derived from the input parameters’ variabili-
ty using commercial FEA software.  
In general, in this study, the objective is to analyze the reliability of FE numerical 
simulation tool, namely ABAQUS software, when having very small variability of 
input parameters, so then whether output results, particularly springback variations, 
are sensitive with the variability or not. Meaning that the software can be sensitive to 
how small percent of variability is. In the following sections, problem modeling will 
be presented in which a case study, springback measurements and numerical model-
ing and simulation will be discussed in Section 2. In section 3, investigation of  
reliability of numerical simulation will be presented. Evaluation of sensitivity of nu-
merical simulation will be shown in Section 4. The last section is conclusion.  
2 
Problem Modeling 
2.1 
Case Study 
The U-shaped part, a benchmark problem of NUMISHEET’93 International Confe-
rence [10], is investigated in this paper. However, the part’s geometrical dimensions 
are modified according to industrial requirements and the part is named open-channel 
part. This part is a representative product commonly used in automotive industry to 
reinforce for body panel or base. A schematic view of die, punch, blank and their 
dimensions for the draw bending process is shown in Fig. 2 which is used in this 
study. Table 1 shows dimensions for the draw bending process. 
Table 1. Dimensions for the draw bending process  
Parameters 
W1 
W2 
W3 
W4 
R1 
R2 
G1 
Stroke 
Dimensions 
(mm) 
57.7 
60 
150 
150 
5 
10 
1.15 
60 
 

496 
V.D. Nguyen, P.-A. Adragna, and P. Lafon 
 
Fig. 2. A schematic view of tools and dimensions for the open-channel part 
The simulation work in this study is carried out based on the experimental results 
of Ledoux et al. [1]. The blank is obtained from rolled sheet of 0.8 mm thick, 300 mm 
long and 300 mm width. The accuracy of the length and width dimensions of the 
blank is 0.5 mm. The blank holder force of 90 KN is applied in this case. Blank ma-
terial is DC04 steel with the material properties presented in Table 2.  
Table 2. Blank’s material properties [1] 
DC04 material 
Young’s modulus  
206.62 GPa 
Yield strength 
175 MPa 
Poisson’s ratio 
0.298 
Lankford’s coefficient 
r0° = 2.09 
r45° = 1.56 
r90° = 2.72 
Density 
7200 kg/m3 
Strain hardening’s coefficient 
K = 466 MPa 
n = 0.2056 
 
Moreover, experimental measurements prove that part profiles remain symmetric. 
Therefore, simulation of numerical experiment will be performed on half of the profile.  
2.2 
Springback Measurements 
In order to characterize the total springback distortion, three measurements including 
the springback of wall opening angle (β1), the springback of flange angle (β2) and 
sidewall curl radius (ρ) are shown in Fig. 3. They describe the variation of the part’s 
cross-sectional shape obtained before and after removing the tools. For calculating the 
springback measurements, it is necessary to determine the measurements before and 
after springback. To do so, the least square method is applied to identify the points  
of A0, B0, C0, D0 and E0 on the formed part’s profile according to given x and y  
coordinates. 

 Assessment of Sensitivity of
Based on the known po
(ߠଶ
଴) before springback are 
defined on the part’s profile
Fig. 3. Schem
They are then used to c
springback. The side wall c
three points A, B and C to c
2.3 
Numerical Modelin
To predict the springback v
numerical simulation is an e
ing process of the open-cha
problem is modeled accord
tools configuration are app
simulation are shown in Tab
Table 3. T
Element type 
Number of elements 
Integration points 
Yield function/Plastic potential 
Hardening rule 
Tool type 
3-D draw bending  
Springback 
f Numerical Simulation in Sheet Metal Forming Process 
oint coordinates, the wall angle (ߠଵ
଴) and the flange an
computed. Similarly, other points of A, B, C, D and E 
e which the tools have been removed. 
 
matic view of springback profile and parameters 
alculate the wall angle (θ1) and the flange angle (θ2) a
curl radius is estimated by a curve fitting technique throu
construct a circular arc. 
ng and Simulation 
variations derived from the variability of input paramet
efficient solution. The FE simulation of the 3D draw be
annel part is carried out by the ABAQUS/CAE 6.11-2. T
ing to the schema of Fig. 2 and the process parameters 
plied as in Table 1. The key characteristics of numer
ble 3. 
he key characteristics of numerical simulation 
Blank  
Shell S4R 
5340 
7 
Hill48 [9] 
Isotropic, Swift model ߪ= ܭሺߝ଴+ ߝሻ௡ 
Tools 
Analytical rigid surface 
General aspect of the code 
Dynamic, Explicit 
Static, General 
Friction coefficient: 0.15 
497 
ngle 
are 
after 
ugh 
ters, 
end-
The 
and 
rical 

498 
V.D. Nguyen, P.-A. Adragna, and P. Lafon 
As mentioned above, the half of problem is modeled. Hence, boundary conditions 
and symmetric condition are applied on the half part of the model.  
3 
Investigation of the Reliability of Numerical Simulation 
With the purpose of investigation of sensitive level of numerical simulation software 
so that a global approach is proposed and illustrated as Figure 4 in which input para-
meters are run by using method of Design of Experiments (DOE) to make variations, 
and then, the variation of input parameters are used as input parameters of numerical 
simulation. As a result, the simulated part will be calculated in the Matlab to define 
the responses. In this study, only part-to-part variation is considered, namely the blank 
thickness variation is regarded as an input variable of this investigation. 
 
Fig. 4. The globally proposed approach in evaluation of sensitivity of numerical simulation 
In particular, the investigated model is demonstrated in Figure 5. Starting from the 
nominal input parameters of the material, tooling and process as Section 2, numerical 
simulation of the draw bending process of the open-channel part is run in the 
ABAQUS/CAE 6.11-2 in which the part shape is performed in two steps of forming 
and springback step.  
 
Fig. 5. The investigated model of reliability of numerical simulation 
Consequently, nodal coordinates of deformed part are extracted from the 
ABAQUS. Afterwards, they are used to compute the springback parameters of β1, β2, 
and ρ in the Matlab. After calculation, the results of the part’s measurements before 
and after springback are listed in Table 4. 
 

 Assessment of Sensitivity of Numerical Simulation in Sheet Metal Forming Process 
499 
Table 4. The part’s measurements before and after springback 
Before springback 
After springback 
Springback measurements 
Measurements 
ߠଵ
଴ 
ߠଶ
଴ 
θ1 
θ2 
β1
β2 
ρ (mm) 
Results 
90.5° 
90.5° 
97.59° 
84.93° 
7.096° 
5.561° 
236 
 
A comparison between the numerical simulation result and the experimental result 
implemented by Ledoux et al. [1], the numerical result is very close to the experimen-
tal one. The deviation between experimental and numerical results is less than 1 mm 
which shows good prediction by FE numerical simulation.  
To calculate automatically, the design of experiments, numerical simulation and 
response calculation are coupled in the workflow of ModeFRONTIER™. The 
workflow in the ModeFRONTIER™ is presented in Figure 6. 
 
 
Fig. 6. The workflow of the proposed approach 
4 
Evaluation of Sensitivity of Numerical Simulation 
Sensitivity function is used to evaluate the reliability of the numerical simulation 
software as below: 
• Smaller and bigger value: 
 
α = 
డ௙
డ௫ሺݔ଴ሻ=
௙ሺ௫బା∆೐ሻି௙ሺ௫బି∆೐ሻ
ଶ∆೐
  
 (1) 
• Smaller and nominal value:  
 
α = 
డ௙
డ௫ሺݔ଴ሻ=
௙ሺ௫బሻି௙ሺ௫బି∆೐ሻ
∆೐
  
 (2) 

500 
V.D. Nguyen, P.-A. Adragna, and P. Lafon 
• Nominal and bigger value: 
 
α = 
డ௙
డ௫ሺݔ଴ሻ=
௙ሺ௫బశ∆೐ሻି௙ሺ௫బሻ
∆೐
  
 (3) 
Where, x0 corresponds to nominal thickness and Δe is variation range between  
thickness values. 
Thickness variation parameters and sensitivity values are shown in Table 5. In par-
ticular, the variation is expressed through gradual decrease of variation percent of 
nominal thickness. It is assumed that the thickness in the whole part is constant. The 
 
Table 5. Thickness variation parameters and sensitivity values 
Variation (%) 
Δe (mm) 
Thickness varia-
tion (mm) 
α(β1) (°/mm) 
α(β2) (°/mm) 
α(ρ) (mm/mm) 
  
  
0.64 
14.4212 
11.3241 
-359.3706 
20 
0.16 
0.8 
6.3059 
5.1569 
-65.7868 
  
  
0.96 
-1.8094 
-1.0104 
227.7971 
  
  
0.72 
13.4137 
4.4931 
-263.4972 
10 
0.08 
0.8 
4.9432 
1.2885 
33.5350 
  
  
0.88 
-3.5273 
-1.9161 
330.5673 
  
  
0.76 
13.8783 
1.7104 
-92.5804 
5 
0.04 
0.8 
6.9821 
2.4945 
49.0883 
  
  
0.84 
0.0859 
3.2787 
190.7571 
  
  
0.784 
38.6269 
4.7084 
-284.1957 
2 
0.016 
0.8 
21.9402 
5.1717 
-67.8929 
  
  
0.816 
5.2534 
5.6349 
148.4098 
  
  
0.788 
1.2409 
-2.5120 
115.8318 
1.5 
0.012 
0.8 
-2.8300 
0.2649 
126.1214 
  
  
0.812 
-6.9010 
3.0419 
136.4111 
  
  
0.792 
4.5147 
-9.6161 
353.8053 
1 
0.008 
0.8 
-1.9217 
-1.9243 
287.6912 
  
  
0.808 
-8.3580 
5.7675 
221.5770 
  
  
0.7936 
21.0978 
9.4679 
-848.9659 
0.8 
0.0064 
0.8 
4.1734 
9.6291 
-433.2124 
  
  
0.8064 
-12.7511 
9.7903 
-17.4588 
  
  
0.796 
170.8839 
42.9871 
-2025.5843 
0.5 
0.004 
0.8 
83.1412 
32.0626 
-902.5713 
  
  
0.804 
-4.6014 
21.1382 
220.4417 
  
  
0.7984 
62.9189 
-53.2250 
462.2954 
0.2 
0.0016 
0.8 
-136.9051 
-38.8243 
1393.4893 
  
  
0.8016 
-336.7291 
-24.4236 
2324.6832 
  
  
0.7992 
-33.7328 
-81.3877 
4490.6412 
0.1 
0.0008 
0.8 
-90.2131 
24.8825 
2733.6190 
  
  
0.8008 
-146.6935 
131.1527 
976.5967 
  
  
0.7996 
1453.4986 
202.5773 
-5962.3799 
0.05 
0.0004 
0.8 
692.7517 
174.3740 
-5902.7587 
  
  
0.8004 
-67.9952 
146.1708 
-5843.1374 
  
  
0.79992 
2023.2555 
278.3391 
-16901.7565 
0.01 
0.00008 
0.8 
-2675.0272 
11.9680 
14256.7433 
  
  
0.80008 
-7373.3098 
-254.4031 
45415.2431 

 Assessment of Sensitivity of Numerical Simulation in Sheet Metal Forming Process 
501 
other parameters of the process are fixed with nominal values. Numerical modeling 
and simulation of the process still remains as mentioned in Section 2. Output parame-
ters are springback measurements of β1, β2 and ρ. 
Sensitivity analysis results are shown Figure 7. Observation from sensitivity analy-
sis graphs shows that the sensitivity of numerical simulation software in this case can 
reach for level of blank thickness variation of 5 % of nominal thickness. It can be seen 
that three sensitive lines of three springback responses converge at the point of 5% of 
nominal thickness. Due to the fact that the smaller the variation range, the more 
closed three sensitive lines are.  
 
 
 
Fig. 7. Sensitivity of numerical simulation α(β1), α(β2), α(ρ) respectively with thickness  
variation   
 
10
-2
10
-1
10
0
10
1
10
2
-150
-100
-50
0
50
100
150
Variation (%)
Sensitivity (°/mm)
 
 
Smaller and nominal
Smaller and bigger
Nominal and bigger
10
-2
10
-1
10
0
10
1
10
2
-20
-15
-10
-5
0
5
10
15
20
Variation (%)
Sensitivity (°/mm)
 
 
Smaller and nominal
Smaller and bigger
Nominal and bigger

502 
V.D. Nguyen, P.-A. Adragna, and P. Lafon 
 
Fig. 7. (continued) 
5 
Conclusion 
Numerical modeling and simulation of U-shaped draw bending process has been car-
ried out to show the effect of blank thickness variation on the variability of output 
response of springback.  
Additionally, the paper has been investigated the sensitivity of numerical simula-
tion software, namely ABAQUS, in modeling and simulating the U-shaped draw 
bending process. The result has shown that the reliability of the software can reach for 
level of blank thickness variation of 5 % of nominal thickness in this case study. The 
sensitivity evaluation is built and calculated automatically in the ModeFRONTIER™. 
The proposed approach has been developing which provides a tool in taking into ac-
count the variation of input parameters affecting on output responses in the sheet met-
al forming process by using FEM contributing to improvement robust parameter  
design methodology. 
This project is being in the research process. Construction of metamodels based on 
approximating surface response to predict variation of other factors will be employed 
in the next works.  
Acknowledgments. The authors acknowledge the financial supports of Champagne-
Ardenne region to implement this project.  
References 
1. Ledoux, Y., Sébastian, P., Samper, S.: Optimization method for stamping tolls under relia-
bility constraints using genetic algorithms and finite element simulations. Journal of Mate-
rials Processing Technology 210, 474–486 (2010) 
2. Lee, S.W., Yang, D.Y.: An assessment of numerical parameters influencing springback in 
explicit finite element analysis of sheet metal forming process. Journal of Materials 
Processing Technology 80-81, 60–67 (1998) 
10
-2
10
-1
10
0
10
1
10
2
-500
-400
-300
-200
-100
0
100
200
300
400
500
Variation (%)
Sensitivity (mm/mm)
 
 
Smaller and nominal
Smaller and bigger
Nominal and bigger

 Assessment of Sensitivity of Numerical Simulation in Sheet Metal Forming Process 
503 
3. Jansson, T., Nilsson, L., Moshfegh, R.: Reliability analysis of a sheet metal forming 
process using Monte Carlo analysis and metamodels. Journal of Materials Processing 
Technology 202, 255–268 (2008) 
4. Mattiasson, K., Thilderkvist, P., Strange, A., Samuelsson, A.: Simulation of springback in 
sheet metal forming. In: Shen, S., Dawson, P.R. (eds.) Proc. NUMIFORM 1995, Balkema, 
Rotterdam, The Netherlands, pp. 115–124 (1995) 
5. He, N., Wagoner, R.H.: Springback simulation in sheet metal forming. In: Lee, J.K., Kin-
zel, G.L., Wagoner, R.H. (eds.) Proc. NUMISHEET 1996, pp. 308–315. Ohio State Uni-
versity, OH (1996) 
6. Chung, W.J., Cho, J.W., Belytschko, T.: A study on dynamics effects of dynamic explicit 
FEM in sheet metal forming. In: Lee, J.K., Kinzel, G.L., Wagoner, R.H. (eds.) Proc. 
NUMISHEET 1996, pp. 414–420. Ohio State University (1996) 
7. Li, K., Carden, W.P., Wagoner, R.H.: Simulation of springback. International Journal of 
Mechanical Sciences 44, 79–101 (2002) 
8. Xu, W.L., Ma, C.H., Feng, W.J.: Sensitivity factors in springback simulation for sheet 
metal forming. Journal of Materials Processing Technology 22, 151–217 (2004) 
9. Hill, R.: A theory of the yielding and plastic flow of anisotropic metal. Proc. R. Soc. Lond. 
A 193, 281–297 (1948) 
10. Taylor, L.M., Cao, J., Karafillis, A.P., Boyce, M.C.: Numerical Simulation of Sheet Metal 
Forming. In: Proceedings of 2nd International Conference, NUMISHEET 1993, Ishehara, 
Japan (1993) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 505–514. 
DOI: 10.1007/978-3-642-30817-8_49 
© Springer-Verlag Berlin Heidelberg 2013 
 
Analysis of Automatic Online Lead User Identification  
Sanjin Pajo, Paul-Armand Verhaegen, Dennis Vandevenne, and Joost R. Duflou 
Mechanical Engineering Department, Katholieke Universiteit Leuven, Celestijnenlaan 300 bus 
2422, Heverlee, 3001, Belgium 
{Sanjin.Pajo,PaulArmand.Verhaegen,Dennis.Vandevenne, 
Joost.Duflou}@cib.kuleuven.be 
Abstract. Lead user identification is a systematic approach to uncovering prod-
uct development opportunities by identifying lead users, individuals or groups 
actively involved in modifying or developing products for personal benefit. In 
this paper, a systematic approach called Fast Lead User IDentification (FLUID) 
based on online data mining, specifically of the Twitter micro-blogging site, is 
proposed. Topic classification, sentiment and intent of a given tweet or user-
metadata can be automatically determined using various text mining techniques. 
The described FLUID system makes use of such techniques to rank retrieved 
users based on indexes derived from well-established lead user characteristics. 
In the initial analysis phase collection of relevant artifacts and contextual in-
quiry allow for measuring impact of each index toward delineating lead users 
from other non-lead users. Through refinement based on statistical analysis of 
expert assessments the effectiveness of the FLUID system is optimized.  
Keywords: lead user identification, data mining, micro-blogging, social net-
works. 
1 
Introduction 
In recent years, research has shown that experienced users driven by the possibility of 
own benefit contribute greatly to the development and innovation of new and existing 
products [1]. Often referred to as lead users, they are among the first to see high de-
gree of benefits in adopting products and services and are first to diffuse adaptations, 
new prototypes or services into the marketplace [1] [2]. Lead users have deep and 
extensive knowledge of products and services and their needs eventually become 
general demands of the marketplace [1] [3]. In addressing their needs, lead users ap-
ply products and services in unique and novel ways to generate new prototypes or 
solutions [4]. Luthje [5] characterizes the described lead user behavior with six  
characteristics: ahead of trend, dissatisfaction, product-related knowledge, use expe-
rience, involvement and opinion leadership. Furthermore, product satisfaction, adap-
tability and evolution can all be observed and learned by studying lead users [5] [6].  
Systematic identification allows for reduction in costs incurred during the research 
and conceptualization stages of the engineering design process. By engaging lead 
users, enterprises or companies reduce the effort and cost of design and product  

506 
S. Pajo et al. 
adaptation [7] [8]. It is also a way to judge the feasibility and adaptability of a product 
in an engineering design process to meet the changing needs or requirements. Addi-
tionally, with their extensive knowledge of products and services, lead users can easi-
ly be integrated into work groups within enterprises, one of the main reasons being 
that they do not need extensive on-the-job training. 
Two current methods for identifying lead user are screening and netnography. 
Screening is one the original approaches to identifying lead users, a technique ad-
vanced by Eric Van Hippel [6]. During the screening process, experts in the product 
domain create and map a network of lead users and gather innovation ideas. Based on 
the preliminary research of the target population, they set up semi-structured tele-
phone interviews to evaluate potential candidates and explore their product related 
knowledge. In some cases, depending on the feasibility, on-site interviews are per-
formed. Over a lengthy time-period, the screening method results in a small network 
of verified lead users [8]. An advantage of the described method is to indubitably 
disassociate lead users from non-lead users by having a direct or over the phone con-
tact with the potential candidates. Through exploratory interviews and on-site visits, 
experts are able to ascertain if a candidate is a lead user. Several drawbacks are asso-
ciated with the screening method. The identification process can last a few months 
and a significant number of potential lead users are overlooked by the screening 
process, resulting in a small sample efficiency in a large population [8] [6]. There is 
also high demand on human resources and heavy reliance on self-assessment of res-
pondents [6].  
Netnography is a more recent systematic approach to identifying lead users. As the 
name implies, it is an ethnographic study with the web as the target platform [6].  
Initially, relevant user online metadata and discussions are collected through observa-
tions of forums or communities. Similarly to the screening method, the approach ne-
cessitates expert users to analyze and process the available data to delineate potential 
lead and non-lead users. Verification is done on the downloaded material and, in con-
trasts to the screening method, no direct communication is enacted between the  
experts and the selected lead users [6]. The approach focuses on a much smaller 
community formed around a common interest, a product or service, allowing it to 
have a better sample efficiency than the screening approach [6]. Unfortunately, the 
resource and time costs are as high as with the previous approach. The experts ob-
serve and parse through vast amounts of online data, a time consuming effort. Other 
major drawbacks of this method are the scarcity of online information for a significant 
number of users on online communities or forums and lack of proper direct verifica-
tion of lead users. A portion of users is cast out of the analysis process due to lack of 
substantial online user information, adding to the error margin in uncovering potential 
lead users. Both methods ask for utilization of expert users, persons that have deep 
knowledge of the material or service and are capable of analyzing vast amounts of 
user discussions, comments, attitudes and opinions. The authors aim to reduce re-
source and time costs by using a fast, automated and systematic approach while  
retaining high attainment of lead users from a large population. 

 
Analysis of Automatic Online Lead User Identification 
507 
2 
FLUID 
Fast Lead User IDentifaction (FLUID) is a systematic approach to finding and identi-
fying lead users. The method aims to automate the process by focusing on the virtual 
or web based communities and sites and by using data mining algorithms on publicly 
available information to separate lead users from non-lead users. The selected target 
platform is the web with the aim to increase the sample efficiency and minimize the 
errors introduced by relying on self-assessment of respondents. To strengthen the 
reliability of the results, in the final stages of the approach the authors aim to make 
use of expert verification of rendered lead users. Netnography has shown that we can 
detect user needs, desires, experiences, motivations, attitudes and perceptions using 
available online data [9]. More and more people are also sharing their experiences and 
opinions on the web. According to Pang [10], 81% of users have done online research 
on a product, 32% have provided a rating on a product or service via an online rating 
system and 30% have posted an online comment. In addition, patterns in relationships 
between the online users can be detected, that can provide more clues in predicting 
lead user characteristics and behavior [6]. The advantage of the automated data min-
ing tool is the ability to speedily evaluate large quantities of user metadata and activi-
ties without relying on interactive human interference. In the approach, the first step is 
to obtain and consolidate or select data. The next step is preprocessing data, which 
entails filtering spam and irrelevant data or users. Thereafter data are interpreted and 
evaluated for knowledge to identify potential lead users. Figure 1 shows the three 
principal steps in fast lead user identification leading to actualization and also integral 
impact scoring based on FLUID indexes, described in the following sections.   
 
 
Fig. 1. FLUID system 
2.1 
Target Platform 
For the target platform, the authors selected the micro-blogging site Twitter. Since its 
creation, the number of users has exponentially increased and currently Twitter 
represents a large, rich and complex data set. Although it contains enormous amounts 
of insignificant or spam information [11], data mining Twitter has provided signifi-
cant amounts of useful qualitative and quantitative data for interpreting user know-
ledge and behavior [12] [13]. It has been used to successfully predict box office  
revenues for movies and political election results [14] [15]. Twitter is a real-time, 
opinion rich resource making it a possible platform for analyzing behavior and atti-
tudes towards products and services. Daily tweets discuss product experience and 

508 
S. Pajo et al. 
adaptation and can make companies aware of new components added to the products 
or replacements and reconfiguration of the existing components [12] [16]. Product 
use, benefits and potential redesigns, including opinions and feelings, can be gathered 
through real time tweet monitoring and analysis [12] [13]. Other important factors 
regarding Twitter are the feature richness and ease of access and use. Majority of the 
features including tweets and the user metadata can be accessed through the Twitter 
application interface. Twitter provides a programming method for most of the site 
features. Extensive user metadata, i.e. description, URL, location, lists etc. may be 
accessed to better support the classification process. However, data mining micro-
blogs like Twitter still poses a challenge. Tweet language is constantly evolving with 
numerous non-uniform linguistic conventions. Because of the 140 character limit, 
users make use of nonstandard vocabulary for informal conversations, missing proper 
names and terms. At the same time, Twitter has specific conventions for marking up 
words that are valuable for data mining. For example, Twitter user names are prefixed 
with a @ sign, i.e. @plumb. Hashtags # denote subjects or topics, i.e. #election. 
Another commonly used convention is RT that stands for retweet, where the text of a 
user is being echoed. Consequently, data mining methods are adapted to process 
tweets and Twitter user metadata [12] [13].  
2.2 
Application 
To test the applicability of the FLUID approach, a test platform was built. Initially, 
the FLUID application takes a product name or a hashtag and searches real time for 
relevant tweets, e.g. bicycle. All retrieved tweets and included user metadata are 
stored in the local database to be processed. After retrieval data are analyzed and 
processed before being displayed. Spammers, promoters, and other automated-script 
style Twitter accounts are discounted, filtering irrelevant data. In the follow-up steps, 
parsed tweets and user metadata are ranked based on five proposed criteria or indexes: 
activity, trend, influence, relevance and sentiment. After several iterations based on a 
manually selected score threshold, users with a high rank are identified as potential 
lead users. In the following section, the FLUID ranking criteria are discussed.  
2.3 
Rank Indexes 
Existing research has identified six characteristics of lead users: ahead of trend, dissa-
tisfaction with the product, product related knowledge, use experience, involvement 
and opinion leadership [6] [7]. The characteristics are reformulated into indexes for 
automated scoring of user data to facilitate identification of lead users online. The five 
indexes are as follows: activity, trend, influence, relevance and sentiment. Table 1 
shows the mapping of lead user characteristics to FLUID online indexes characterized 
by Twitter user attributes.  
Lead users have an extensive experience using products and see potential benefits 
of product redesign or modification. They tend to show high level of engagement in 
product use and development and these traits have been denoted as user involvement. 
Since user involvement is not limited to the web, in the context of online behavior the 

 
Analysis of Automatic Online Lead User Identification 
509 
authors defined the trait as user online product related activity. The frequency and 
quality of online discussions or conversations regarding a product are indicators of 
product related activity. For Twitter users, high product related activity can be ob-
served in the number of relevant statuses, URLs, hashtags, favorites, etc, as shown in 
Table 1. It can also be observed in the number of relevant networks users form, the 
number of subscription or member lists and the number of like-minded friends. Activ-
ity related actions increase the rank value of a Twitter user in the FLUID application, 
as enthusiastic and engaged users and consumers are viewed as probable lead users. 
Table 1. FLUID Metrics 
Lead 
User 
Characte-
ristics 
# of Statuses 
# of Followers 
# of Friends 
# of Favorites 
# of Lists (Member) 
# of Rel. Lists (Member) 
# of Hashtags 
# of Rel. Hashtags 
# of Lists (Subs.) 
# of Rel. Lists (Subs.) 
# of URLs 
# of Retweets 
Image Relevance 
URL Relevance 
Description Relevance 
Tweet Relevance 
Tweet Sentiment 
FLUID 
Indexes 
Ahead  
of  
Trend 
 
 
 
x  
x
 
x
 
x
 
x
 
x
 
 
 
Trend 
Knowledge 
 
 
 
 
 
x
 
x
 
x
 
 
 
x
 
x
 
Relevance 
Use  
Experience 
 
 
 
 
 
x
 
x
 
 
 
 
x
x
x
x
 
Involve-
ment 
x x  
x x x
x
x
 
 
x
x
 
 
 
 
 
Activity 
Opinion 
Leadership 
 
x x  
 
 
 
 
x x  
x  
x  
 
 
Influence 
Dissatis-
faction 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
x Sentiment 
 
The lead user characteristic, ahead of trend, is also an index component of the 
FLUID model, called trend. This user characteristic means that a lead user is among 
the first users to recognize benefits of a product or service and its potential adaptation 
or redesign. Lead users are among the first to use and modify the product. In the con-
text of online behavior, judging foresight is a challenge due to lack of direct contact 
with the user, but knowledge and use of new products or services can be observed and 
is therefore denoted as trend. Twitter examples of such behavior are retweets of new 
product developments, subscriptions to relevant lists and the number of potential lead 
user Twitter friends and favorites. Similarly to the activity index, trend-like Twitter 
actions add to the overall rank score of the user. 
The influence component of the FLUID model is inferred from the opinion leader-
ship characteristic of the lead user. Lead users diffuse innovation ideas or prototypes 
into the marketplace, influencing future product development. In the context of online 

510 
S. Pajo et al. 
behavior, user’s clout or influence is measured. Online use experience, knowledge, 
needs sharing and high activity attract like-minded individuals and followers, which 
helps facilitate circulation of innovation solutions. On the Twitter platform a user’s 
influence and popularity is measured by monitoring the number of product relevant 
retweets and mentions by other users. In addition, the number of potential lead users’ 
followers is a significant indicator in how far-reaching the user’s message is.   
Within the web based FLUID model the relevance index is perceived as both prod-
uct knowledge and experience. Without direct contact or on-site visits there is an 
inadequate amount of information to determine product knowledge and experience, 
but online the extent of user’s tweet relevance to the target product is measured. Rele-
vant conversations to the target product or service, relevant lists and follower net-
works are good initial indicators of an assiduous user. The importance of relevance in 
processing is one of the primary indicators of significance of target related product 
behavior. On Twitter, user status updates, links and hashtags are analyzed for their 
relevance. Lead users will often subscribe to lists or discussion boards focused on the 
target product or service or will find themselves added to those lists by other Twitter 
users. Relevance of tweets is measured using natural language processing tools to 
provide a numeric estimate for the index. Filtering and extraction algorithms are ap-
plied to discern the essential components, the size of the post, presence of a specific 
category and keywords for that category. A tweet is relevant when properties i.e. 
function, and characteristics, i.e. structure, shape of a product can be uncovered in the 
message.  User lists and hashtags are matched up against automatically or manually 
generated lists and hashtags to give additional indicators of relevance. The relevance 
scoring is based on the tweet and metadata analysis and contributes to the rank used to 
identify lead users.  
Final important indicator in identification of lead users is sentiment or product re-
lated disposition. It is one of the main sparks behind human behavior or actions. It is 
defined as a thought, view or attitude and it is derived from related research indicating 
that lead users tend to express a greater amount of dissatisfaction with offered prod-
ucts or services [6]. The sentiment of online posts, in this case tweets, is measured 
and a noticeable negative sentiment is used to separate lead users from other users. 
The process makes use of emoticons that approximate human emotions using lan-
guage characters. Lastly, scores for each index in the application are weighted and 
combined to give an overall rank of the retrieved user. Manually keyed in rank thre-
shold based on observed difference in scores is used to set lead users apart from non-
lead users.  
3 
FLUID Effectiveness Analysis 
3.1 
Test Description 
A study was conducted to evaluate the ranked results obtained by the FLUID applica-
tion. The primary goal of the study was to verify the lead and non-lead users  
discovered by the FLUID tool. The secondary goal of the study was to identify the 
significance or strength of difference in relevance, trend, influence, activity and  

 
Analysis of Automatic Online Lead User Identification 
511 
sentiment between the two groups of users.  For the target product the authors se-
lected bicycle products. In preparation for the study, the FLUID application utilized 
the Twitter search engine to search for Twitter users using manually chosen relevant 
bicycle terms and Twitter hashtags, e.g. bicycle, #cycle. Twitter users were aggre-
gated by the FLUID application and ranked using the five indexes. 0.07 % had a  
distinctly high rank score based on the indexes, which was selected as the threshold 
designating users as lead users.  For the effectiveness analysis 50 users were selected 
from the total number of users retrieved by the FLUID application. 20 users were 
uniformly selected from the 0.07% of FLUID users labeled as lead users and 30 from 
the remaining list of FLUID non-lead users. The test took place at KU Leuven with 
five experts, knowledgeable in lead user characteristics, evaluating the selected 50 
users by their tweets. Before starting the study a brief introduction regarding the study 
was given to the experts. Next, they were given 14 random tweets of each of the 50 
users and were asked to rank on a five point Likert scale whether they agreed or disa-
greed that the tweets belonged to a lead user. The order of Twitter users was altered 
for each participant to minimize ordering bias.  The results, a lead and non-lead user 
group, were used in the second part of the study to score and compare user metadata, 
tweet relevance and tweet sentiment. 
3.2 
Experimental Results 
The experts identified 14 lead users and 36 non-lead users out of the selected list of 
50 Twitter users. Twitter users rated with an average score between 4 (agree) and 5 
(strongly agree) were noted as lead users. Users with an average score between 1 
(strongly disagree) and 4 noted as non-lead users. An interrater reliability analysis for 
the 5 experts was performed using the Fleiss’ Kappa statistic. The strength of the 
agreement between the experts was found to be substantial, with Fleiss’ Kappa for 50 
cases with two possible classes (Lead User and Non-Lead User) 0.7124, 95% CI 
(0.6247, 0.8000). As shown in Table 2, 14 of the 20 FLUID lead users were noted as 
lead users (LU) by the experts and remaining 36 were noted as non-lead users (NLU). 
Table 2. Expert and FLUID results matrix 
Predicted class (FLUID) 
 
 
LU 
NLU 
 
Evaluated  
Class (Experts) 
LU 
14 
0 
 
NLU 
6 
30 
 
 
The next step was to quantify the performance of the model in the case of bicycle 
products. A confusion matrix was created with two classes for FLUID and experts, 
shown in Table 2. Calculated precision that a FLUID retrieved lead user is affirmed 
by experts to be a lead user is 0.7 and calculated recall that a randomly confirmed lead 
user is retrieved by FLUID is 1. The accuracy of the model base on the results shown 
in the table is 0.88.  

512 
S. Pajo et al. 
In the second part of the study, the identified lead user metadata were compared to the 
non-lead user metadata with regard to the five indexes. Significant differences in 
Twitter user metadata and tweets between the two groups are shown in Table 3. For 
the influence index, the number of retweets for lead users was considerably greater 
than for the non-lead users. Lead users are also involved in networks relevant to the 
target product. Looking at the lists that users are subscribed to by other users, the 
number of relevant lists that the Twitter user is member of is considerably higher for 
lead users than for the non-lead users. For the relevance index, lead users’ tweets, 
URLs, descriptions, and images were found to be more relevant to the product domain 
than the non-lead users’. For the sentiment index, authors found no difference in the 
overall sentiment in tweets between the two groups. The lead user sentiment (M=3.17, 
SD=0.14) was adjudged to be similar to the sentiment of non-lead users (M=3.10, 
SD=0.17), t(29)= 1.52, p=0.13. Lead users did not appear to be more dissatisfied with 
the bicycle products as expected from the related research [6].  
Examining the activity index, the identified lead users tend to share more in-depth 
information regarding products, but also notably tend to be less active on Twitter. 
Lead users retrieved by utilizing Twitter search tend to tweet less than non-lead users. 
No other significant indicators of difference in activity between the two groups were 
found. Finally, for the trend index, the percent of relevant lists that the user subscribes 
to is considerably higher for lead users. There was no noticeable difference in the 
percent of relevant hashtags, taking into account the bias introduced by manual selec-
tion of queried hashtags in the FLUID application. 
Table 3. Significant Differences for Twitter data 
 
Lead Users 
Non-Lead Users 
 
 
Mean 
SD 
Mean 
SD 
T 
Df 
p 
Tweet 
2.87 
0.29 
1.84 
0.92 
5.95 
47 
0.000 
URL 
4.21 
0.69 
2.64 
1.25 
5.20 
39 
0.000 
Description 
3.83 
1.11 
2.27 
1.18 
4.11 
20 
0.000 
Images 
2.85 
1.29 
1.85 
1.17 
2.43 
24 
0.022 
# of Tweets 
1.95 
2.71 
8.74 
11.47 
-3.32 
43 
0.002 
# of Retweets 
7.57 
5.01 
3.77 
5.00 
2.40 
48 
0.020 
Lists (Member) 
0.54 
0.21 
0.18 
0.26 
4.94 
31 
0.000 
Lists (Subscriptions) 
0.62 
0.30 
0.17 
0.31 
2.86 
17 
0.024 
4 
Discussion 
The purpose of the research is to evaluate automatic classification of Twitter data by 
the FLUID application. For the selected case study, as indicated in the previous sec-
tion, precision and recall in identifying lead users are 0.7 and 1, respectively, leading 
to an overall accuracy of the FLUID application of 88% when evaluated by experts. 
The effectiveness analysis results, shown in Table 2, imply high agreement between 
the experts and FLUID application on non-lead users and moderate agreement on lead 
users for bicycle products. The study also shows that the user characteristics can be 

 
Analysis of Automatic Online Lead User Identification 
513 
translated into criteria to categorize online lead users from non-lead users. Relevance, 
whether of tweets, URLs, favorites or lists is the primary influence factor in separat-
ing lead users from other Twitter users. Activity, trend and influence are additional 
strengthening components in ascertaining the lead user identification. The strength of 
each index in delineating between the two groups gives a notion of possible correction 
of weights for the FLUID application in ranking Twitter users for follow up studies. 
The sentiment of a tweet was found to have no significant impact in separating the 
two groups. Analyzing Twitter data for relevance and sentiment is challenging. Sepa-
rate studies should be performed to increase efficiency and verify techniques used. 
Additionally, with the growth of Twitter, spam messages have become more and 
more of a problem. A targeted classifier should be used to track down such tweets. 
Some of the remaining challenges in identifying lead users are limited resources, 
Twitter data restrictions, and bias introduced in manually selecting queried product 
names and hashtags. Further studies in other domains with refinements in FLUID 
weights from the results obtained in the study will have to be performed to validate 
and improve on the results in identifying lead users. In regards to related research, the 
FLUID method further supports the netnography method in an online approach to 
lead user identification. Additionally, it adds a systematic and automatic approach to 
the method. The FLUID approach can also be one of the initial steps in the screening 
process to minimize the time costs in mapping a network of potential lead users be-
fore an in-person or on-site verification process starts. The model may be applicable 
to other social networking sites, e.g., Facebook, Myspace, Flickr that may provide 
access to user posts and also user meta-data, which will require similar mapping to 
FLUID indexes. Nevertheless, successful data gathering and analysis on social net-
working sites is dependent on evolving user or site privacy restrictions on available 
online data.      
5 
Conclusion 
In this paper, a systematic and automated FLUID approach to identifying lead users is 
described and its performance effectiveness is analyzed on data retrieved from the 
social networking site Twitter for the bicycle product. These results indicate further 
need for research on the proposed approach, although they have to be interpreted 
cautiously. The results are based on a single case study and further refinements 
through several iterations are necessary. Data mining methods rapidly and effectively 
extract and classify user discussions and metadata available on Twitter although sig-
nificant challenges remain. The next step in the FLUID model development is to veri-
fy the method with different target products or services and possibly by having on site 
visits. During on-site visits, context of product use and development can be observed 
that is difficult to gather through data mining. During the verification stage, the au-
thors look to further ascertain the viability of the approach in identifying the lead 
users.  Identification of lead users and analysis of online discussions on social media 
sites like Twitter or other online communities provide many opportunities, especially 
for the private sector. Systematic and automated lead user identification is bound to 
play an important role in the early stages of product innovation. 

514 
S. Pajo et al. 
References 
1. Von Hippel, E.: Lead Users: An Important Source of Novel Product Concepts. Manage-
ment Science 32, 791–805 (1986) 
2. Morrison, P.D., Roberts, J.H., Midgley, D.F.: The nature of lead users and measurement of 
leading edge status. Res. Policy 33(2), 351–362 (2004) 
3. Von Hippel, E.: The Source of Innovation. Oxford University Press, New York (1988) 
4. Srivastava, J., Shu, L.H.: Designing Products to Encourage Conservation: Applying the 
Discretization Principle. In: Leveraging Technology for a Sustainable World. J. Mol. Biol., 
pp. 569–574. Springer, Heidelberg (1981) 
5. Luthje, C., Herstatt, C.: The lead user method: An outline of empirical findings and issues 
for future research. R & D Management 34(5), 553–568 (2004) 
6. Von Hippel, E., Thomke, S., Sonnack, M.: Creating breakthroughs at 3M. Harvard Bus. 
Rev. 77(8), 47–57 (1999) 
7. Von Hippel, E.: Democratizing Innovation. In: Democratizing Innovation. MIT Press, 
Cambridge (2005) 
8. Belz, F., Baumbach, W.: Netnography as a Method of Lead User Identification. Creativity 
and Innovation Management 19(3), 304–313 (2010) 
9. Bartl, M.: Netnography: Einblicke in die Welt der Kunden. Planung & Analyse 5, 83–89 
(2007) 
10. Pang, B., Lee, L.: Opinion mining and sentiment analysis. Foundations and Trends in In-
formation Retrieval 2(1-2), 1–135 (2008) 
11. Wang, A.: Don’t follow me: Spam detection in twitter. In: Int’l Conference on Security 
and Cryptography, SECRYPT (2010) 
12. Saif, H., He, Y., Alani, H.: Alleviating data sparsity for twitter sentiment analysis. In: The 
2nd Workshop on Making Sense of Microposts (2012) 
13. Bifet, A., Frank, E.: Sentiment Knowledge Discovery in Twitter Streaming Data. In: Pfa-
hringer, B., Holmes, G., Hoffmann, A. (eds.) DS 2010. LNCS, vol. 6332, pp. 1–15. Sprin-
ger, Heidelberg (2010) 
14. Asur, S., Huberman, B.A.: Predicting the future with social media. arXiv:1003.5699v1 
{cs.CY} (2010) 
15. Chung, J.E., Mustafaraj, E.: Can collective sentiment expressed on twitter predict political 
elections? In: Burgard, W., Roth, D. (eds.) Proceedings of the Twenty-Fifth AAAI Confe-
rence on Artificial Intelligence, AAAI 2011, pp. 1768–1769. AAAI Press, Menlo Park 
(2011) 
16. Alexander, P., Patrick, P.: Twitter as Corpus for Sentiment Analysis and Opinion Mining. 
In: Proceedings of the 7th Conference on Language Resources and Evaluation (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 515–524. 
DOI: 10.1007/978-3-642-30817-8_50 
© Springer-Verlag Berlin Heidelberg 2013 
 
Improving Result Quality in Engineering Design by 
Better Linking Employee’s and Task’s Features 
Malte Hinsch1, Jan Erik Heller1, Raymond Djaloeis2, Christopher M. Schlick2,  
and Jörg Feldhusen1 
1 Chair and Institute for Engineering Design, Steinbachstraße 54B, 52074 Aachen, Germany 
{hinsch,heller,feldhusen}@ikt.rwth-aachen.de 
2 Institute of Industrial Engineering and Ergonomics, Bergdriesch 27, 52062 Aachen, Germany 
{r.djaloeis,c.schlick}@iaw.rwth-aachen.de 
Abstract. Human interference with engineers’ activities negatively influences 
engineering tasks’ success. Regarding human influence is therefore a promising 
approach to improve that success. This Paper examines the aspect of man-
induced disturbances in Systematic Engineering Design processes. Based on a 
lab study done with engineering students and interviews with engineers in in-
dustry a model of how to link engineering tasks with the most appropriate prob-
lem solving design engineer is developed. To underline the importance of the 
model, specific background information on Design Theory approaches is given. 
Keywords: engineering design, design methodology, result quality of engineer-
ing tasks, linking employees and tasks. 
1 
Introduction 
Quality of products is measured by how well requirements derived both from customers’ 
needs and products’ development, which is typically comprised of different single tasks 
e.g. Computer Aided Design (CAD), are met. According to the German VDI 2221 guide-
line, the quality of the result is on one hand related to the demands of the product devel-
opment task, and on the other to the competence of the design engineer [1]. Each task 
demands a certain profile of technical, social and personal competence, and each design 
engineer possesses a different profile of these competences. If task’s demands and com-
petences do not match, inferior product quality can occur. Therefore, to ensure a high 
quality of the product, it is a first step to find competent design engineers, but more im-
portant to find out which of them provides the best fit for the specific product develop-
ment task. This contributes to a deeper understanding of the relation between the human 
factor in product development and its consequences for the quality of the result. The 
paper gives a more detailed introduction of the lab study performed to analyze the scena-
rios. Furthermore, an insight on how and why the different lab study assignments were 
compiled, is presented. Finally, the paper examines the conclusions that can be drawn 
from the results of the three scenarios and proposes a method to evaluate the background 
of an employee in engineering design, in order to match it with the right kind of task, 
aiming at the successful solving of engineering tasks. 
In chapter two, the state of the art of Systematic Engineering Design and the role of 
man-induced disturbances in product development along with challenges design  

516 
M. Hinsch et al. 
engineers face in early phases of the product development process are introduced. 
Chapter three establishes the research on error trees in order to explain the necessity 
for the presented lab study performed with students of RWTH Aachen University. 
Based on the lab study and interviews with engineers in industry the need for a model 
of both engineering tasks and employees in engineering is developed in chapter four, 
while the two parts of the model are linked in chapter five. Chapter six summarizes 
the positions of the paper and gives an outlook on further research. 
2 
State of the Art 
2.1 
Systematic Engineering Design 
Product development as considered by Systematic Engineering Design (SED) is a 
sequence of distinctive actions starting at the original conception through design and 
production and leading up to the product’s introduction into the manufacturing 
process. Several methodologies and approaches have been developed in the course of 
the last fifty years. All that happened in order to systematize product development and 
to minimize the effect of human action. Research on SED has been undertaken in 
various countries for about 60 years. As this research progressed simultaneously sev-
eral methodologies for SED have been developed. 
The Theory of Inventive Problem Solving (TRIZ) is focused on early stages of de-
velopment [2], [3]. Quality Function Deployment was developed in Japan resulting in 
the “House of Quality” [4], [2] and Axiomatic Design was published in the USA pre-
senting a method in order to achieve high quality products by axiomatically consider-
ing independence and least information content [5]. A more holistic methodology of 
SED was developed in German speaking countries since the mid-20th century. Joint 
efforts of several groups led to the guideline VDI 2221 [6], [1]. 
2.2 
Methods and Approaches within SED 
Despite the different approaches, Systematic Engineering Design techniques can be 
divided into Discursive Methods, Methods of Analogy Consideration and Heuristic 
Methods. An overview of methods and their classification is given in Figure 1 [4], [7].  
Common with all methods is that humans act with their deficiencies from irrational 
behavior. All three kinds of approaches make up the entirety of SED problem solving 
methods. 
 
Fig. 1. Methods in Systematic Engineering Design 
Discursive Methods
Heuristic Methods
Methods of 
Analogy Consideration
solution field
• brainstorming
• 6-3-5 brainwriting
• gallery method
• Delphi method
• synectics
• combination 
of methods
• systematic examination 
of physical relationships
• systematic examination 
by means of 
classification
• use of design catalogues
• TRIZ
• literature study
• analysis of existing 
technical systems
• analogy considerations
• technical systems
• natural systems 
(bionics)
• measurements and 
model tests

 
Improving Result Quality in Engineering Design by Better Linking Employee’s 
517 
2.3 
Man-Induced Disturbances 
Considering the matrix of human error factors in SED by Djaloeis et al. [8], the role 
of man-induced disturbances was focused on. In order to examine human interfe-
rences with SED approaches they have to be named first. According to Dylla, interfe-
rences are related to individual as well as external influences [9]. Every design engi-
neer’s individual problem solving is initially influenced by his individual background, 
which is made up by individual personality, acquired knowledge and accumulated 
education but also results from the emotional situation. In most cases, these influences 
have to be taken as given. They are always related to external settings, the task and 
the structure of the organization executing the SED process. They can be kept at bay 
to a certain extent by well executed project management. Figure 2 shows the relations 
between SED and individual along with external influences. [9] 
 
Fig. 2. Individual and external influences on SED 
2.4 
Challenges in Early Phases of Product Development 
Especially for early phases of product development, Bender advocates “opportunistic 
planning”, which includes dynamic changes according to new requirements, partial 
solutions, and intuition, often based on heuristics [10]. He identifies a detailed prob-
lem and requirement analysis as the fundament of product development planning, 
especially in early stages. After generating possible problem approaches, a systemati-
cal evaluation of the most promising principle solutions is advisable. However, the 
result quality is closely connected to the ability to decompose the general problem 
into more manageable subproblems. This requires high levels of experience, both 
theoretical and practical competence, and the ability to dynamically switch between 
problem-, solution- and process-oriented points of view. Therefore, technical profi-
ciency is highlighted. 
2.5 
Conclusion 
One general aspect of all SED approaches is that they are put into action by human 
beings who always bring their distinct and unchangeable deficiencies with them [9]. 
The important influences of human factors are also underlined by research done by 
Duckwitz, Djaloeis et al [11]. These influences interfere continuously with the proper 
individual influences
external influences
assignments
available 
information
available means 
and equipment
working environment
external decisions
social and organizational 
integration  
available time
fact knowledge
operational 
knowledge
skills
style of action
style of thought
value systems
emotions
motivation and 
productivity
Systematic Engineering Design

518 
M. Hinsch et al. 
execution of SED methods. Error trees, which will be introduced in the next chapter, 
can be used to examine problem solvers’ influence on the success of actions. 
3 
Lab Study 
3.1 
Error Tree 
In product development, the product must conform to the requirements listed in the 
product specification. The quality requirements as well as their respective acceptance 
limits on the functional, working and constructional interrelationship levels can be 
derived [4].  
• The functional interrelationship level expresses the intended relation between input 
and output. Common errors are e.g. failure to understand the main function or nam-
ing a physical law or a product component instead of a function.  
• On the working interrelationship level, relevant laws of physics as well as geome-
trical and material constraints are listed. Common errors are e.g. choosing the 
wrong physical effect.  
• The constructional interrelationship level highlights components, joints and assem-
bly points. Common errors are e.g. naming a function or a physical law instead of a 
defined component, value or dimension or wrong dimensioning.  
Pahl et al. elaborate that establishing functional relationships can be challenging, be-
cause the high level of abstraction is difficult to handle [4]. Based on the error model 
of Hacker [12], a three-level error tree was defined. In each of the three relevant sys-
tematic engineering levels, the respective requirement R can be evaluated using the 
following procedure: at first, it is asked whether the designer has included R: if not, it 
is an error of omission, and if yes, it is asked if the implementation of R exceeded the 
design range. If yes, it is an error of execution, if no, R is fulfilled. This error tree 
provides a valid method to categorize and count errors on various levels of systematic 
design. The above mentioned approach towards error trees was verified in a lab study 
which will be described in the next chapter. 
3.2 
Study with Engineering Design Students 
In order to examine different influences like (e.g. amount of time or personal back-
ground) on the quality of the outcome of a product development process, a lab study 
was conducted. The participants solved five tasks in the engineering design process, 
described as followed. First task was to conceive the structure of the product with the 
help of a picture of the crankset. For guidance the participants got an example of a 
structure with the task definition. Focus of this task was the examination of the human 
capability in abstraction of a given issue and structuring a given product. In the 
second task, based on the same picture, the function of each part had to be conceived 
and displayed in a hierarchical structure. Then subsequently the structures had to be 
connected by the participants to show the coherences between parts and functions. 
Beyond the pure abstraction of the bicycle crankset the participants had to develop 

 
Improving Result Quality in Engineering Design by Better Linking Employee’s 
519 
functions. In the third task an improved solution of the crankset had to be designed by 
the participants. Creative development was required. For the fourth task the partici-
pants were handed an incomplete technical drawing of a part of the crankset with a lot 
of freedoms in design. The part had to be modeled in CAD software. Demanded skills 
were handling of the software, but mostly the self-reliant recognition of the problem 
and the modeling of the part. In the fifth and last task CAD parts had to be assembled 
in CAD software. Central requirements to the participants were finding reasonable 
connection types, self-reliant picking of an assembly order and handling the software. 
To get an insight into systematic interrelationships of participants’ backgrounds and 
performance in the given engineering tasks, three scenarios analyzing both back-
ground data and results were set up. 
3.3 
Scenarios 
In this chapter scenarios are laid out for all four groups of participants and the results 
will be interpreted. Scenario A examines in what way the personal backgrounds, 
which are independent from the test and only depend on the participant, are connected 
to the result quality of the design process. To test the correlations between test data 
the provided results were split into three scenarios. Spearman’s rank correlation coef-
ficient was calculated bivariate for the ordinal scaled data in different categories for 
each scenario. Spearman’s rank correlation is used because the variables are non-
linear to each other. Furthermore Spearman’s rank correlation is robust against out-
liers. The scenarios are as followed: 
Hypothesis A (correlation between Background and Results) 
Hypothesis A examines the correlation between the backgrounds of each participant, 
gathered before the experiment, and the results achieved by the participants in the test. 
The evaluation in SPSS (IBM Statistical Product and Service Solutions), a software 
package used for statistical analysis, also states the correlation in the backgrounds and 
in the results itself, which lead to conclusions for types of problem solvers. In detail 
the following criteria are analyzed: As part of an often used intelligence test results 
from a test for spatial awareness are used [13]. The test was performed by the partici-
pants before the main test. Different rotated dice had to be matched to a set of dice. 
The results were converted into grades from 1 to 4, where 4 is the best grade and 1 the 
worst. Other background information is experience with technical drawings, engineer-
ing design, CAD and with the item examined in the test, the crankset of a bicycle. 
Again a scale from 1 to 4 was used by the participants, where very much experience 
was rated with a value of 4. On the site of the results the quality of the solution was 
rated. The rating was done regarding aspects of completeness, correctness and clean-
ness of the displayed results. Best results got a 4. Worst results a 1. Scenario A could 
be confirmed with the test results.  
Figure 3 shows an excerpt of the result from the lab study. It is easy to see, that 
there is a strong relation between the average subjectively perceived spatial percep-
tion and the actual result from the above described engineering tasks. Similar results 
were drawn from the other tasks conducted in the lab study. 

520 
M. Hinsch et al. 
 
Fig. 3. Interrelationship between average spatial perception and results in lab study 
Hypothesis B (correlation between backgrounds and task load) 
Hypothesis B examines the correlation between personal backgrounds of the partici-
pants and subjective task load. The same criterions as with hypothesis A were used. A 
scale from 1 to 4 was used by the participants, where very much experience was rated 
with 4. In the following the task load criteria are introduced. They were recorded on 
standardized questionnaires from the NASA TLX method [14]. On the questionnaires 
the participants could score their task load on a scale from 1 to 10. For the evaluation 
the score was converted to a scale from 1 to 4. The task load was recorded together 
for tasks one and two and separately for the task three to five. The NASA TLX meas-
ures mental demands, physical demands, temporal demands, own performance, effort 
and frustration. Results delivered ambiguous data for this scenario. 
Hypothesis C (Correlation between task load and results) 
Hypothesis C evaluates the correlation between subjective task load and the result 
quality. The criteria were recorded on standardized questionnaires from the NASA 
TLX method [14] exactly like hypothesis B. On the side of the results the quality of 
the solution was rated. The rating was done regarding aspects of completeness, cor-
rectness and cleanness of the results. Best results got a 4. Worst results a 1. Results 
delivered unclear data for this scenario. Figure 4 exemplarily displays the relationship 
between task load expressed through temporal demand and results. Further details on 
the results of the scenarios are summarized by Hinsch et al. [15]. 
 
Fig. 4. Interrelationship between temporal demand and results in lab study 
0
0,5
1
1,5
2
2,5
3
3,5
4
4,5
0
5
10
15
20
25
30
Leistung_Gesamt
Würfeltest_skala
Pot.(Leistung_Gesamt)
Pot.(Würfeltest_skala)
total result
spatial perception
avg. total result
avg. spatial perception
Group
0
1
2
3
4
1
4
temporal demand
assessment task 1
temporal demand
assessment task 3b
0
0
4
0
1
2
3
4
1

 
Improving Result Quality in Engineering Design by Better Linking Employee’s 
521 
Additional research by Djaloeis et al. [11] confirmed that there is an unclear, 
though trending, relation between available time and subjective workload: especially 
a paradoxical relation between frustration and quality of the result was highlighted. 
These results provide an approach for improving an existing actor-oriented workflow 
simulation model. A first theoretical approach was conducted by Duckwitz et al. [16], 
with the intention to sustainably increase the predictive accuracy of the simulation 
model as a planning tool for project managers in design projects.  
4 
Need for an Employee and Task Model 
Participants in a set of interviews held in engineering companies as well as the lab 
study results confirmed strong influences of man induced disturbances and the choice 
of problem solver on SED task execution. Thus, SED task execution requires a well 
thought choice of problem solver and therefore models of both tasks and employees. 
4.1 
Task 
Adapting a model from Schreyögg, a task can be categorized by five scales – variabil-
ity, novelty, interdependence, unambiguousness and knowledge/skills [17]. Variabili-
ty is defined as differences in the conditions of the performance of work over time. A 
more variable task frequently changes its conditions of successful task execution over 
time, where a less variable task features fewer changes. Novelty is examined from the 
number of exceptions that the task executioner maybe confronted with. A new task 
will force a high number of exceptions. Interdependence measures the dependence to 
other tasks before or after the task viewed as well as the dependence to other people. 
A higher rating in interdependence states a high dependence on other tasks and 
people. 
Unambiguousness rates how explicit the task is or can be defined. A higher rating 
in this scale results in a more defined task with less room for interpretation. Added to 
the mentioned four directly adapted scales from Schreyögg the scale knowledge/skills 
defines what knowledge is demanded by the task and how much the knowledge influ-
ences the task execution. The demands are scaled in a range from 0 to 4. The factors 
were validated by examining 35 typical engineering tasks and exercise assignments 
from university. The tasks were executed in final theses since 2008 in cooperation 
with the industry. For each task five items were evaluated. In total only 25 questions 
have to be answered in order to describe a task. 
4.2 
Employee 
The employee model is adapted from a personality test – the NEO-FFI [18]. The five 
scales in the employee model are agreeableness, openness for experience, conscien-
tiousness, problem solving and competence. The first three scales are directly meas-
ured with the personality test. Employees with high scores on the scale compatibility 
are altruistic, compassionate, understanding and sympathetic. They tend to interper-
sonal trust, cooperation, compliance and they have a strong need for harmony. Sub-
jects with high scores on openness to experience are characterized by a high degree of 
appreciation for new experiences, prefer variety, are inquisitive, creative, imaginative 

522 
M. Hinsch et al. 
and independent in their judgment. They have varied cultural interests and are inter-
ested in public events. The conscientiousness scale differs neat, reliable, hard-
working, disciplined, punctual, meticulous, ambitious and systematic of careless and 
indifferent persons. The scale problem solving is based on research on problem solv-
ing among engineers. Subjects with a higher rating in problem solving deliver better 
results in most cases. The scale competence is based on common tests for evaluation 
of IQ and EQ as well as expertise. The questionnaires for employees are more exten-
sive compared to the task questionnaires. This is done because the variety of em-
ployee characteristics typically exceeds task variety, as all tasks originate in product 
design. 
5 
Linking of Task and Employee Model 
Figure 5 displays the linking of task and employee model. For each of the five sectors 
attached to each pentagon, the link is established by first conducting surveys with 
predefined questionnaires. The results are stored in databases. Through a correlation 
matrix that is established by intense statistical analysis (like the above described) and 
thoroughly based on the lab study’s results the link is created. All five links together 
form the model. The implemented software prototype features an easy to use interface 
that helps people from the management level to assign the right task to the right em-
ployee. 
 
Fig. 5. Application of task and employee model and assignment scheme 
The correlation within the prototype is calculated in an Excel based matrix, while 
the applying engineer will only enter the given values into a software-based form. The 
matrix then calculates the best match based on the given entries and proposes an em-
ployee-task combination to be chosen. The values concerning the employee can be 
imported from already existing systems to evaluate employees. In order to best con-
sider these systems in the development of the tool, a survey on practical application of 
these systems will shortly be done in engineering companies. 
task
knowledge and skills
novelty
variability
interdependence
unambiguousness
employee
competence
openness
agreeableness
conscientiousness
problem solving cap.
Assignment
helps
database
questionnaires
database
correlation matrix
questionnaires
interface
software tool

 
Improving Result Quality in Engineering Design by Better Linking Employee’s 
523 
6 
Conclusion and Outlook 
This paper intends to underline the importance of considering human influence in 
engineering task execution. Therefore it first gave a short insight into Systematic En-
gineering Design approaches and methods. Next, it subsequently presented error trees 
to examine the effects of the influences along with a lab study based on these error 
trees. The results of the lab study contributed to the establishment of a model on how 
to meet the challenges from human influence in engineering tasks. Finally, the model 
was introduced and presented in its current state. The model is subject of current re-
search and will be broadly enhanced. While further developing the model, the back-
ground of its development as presented in this paper will again be used to underline 
features of the model. E.g. insights into interrelationships between engineers’ problem 
solving will be taken from the analysis of lab study results. Next, the factors of the 
model as presented here are developed supported by an extensive series of interviews 
with engineers in the field. Thus, the validity and applicability of the model can be 
granted. Last, task and employee linking will be enhanced from the current state as 
already outlined. Once finished, the applicable model will increase engineering tasks’ 
success, as negative effects of human influence will be reduced. 
References 
1. VDI 2221: Methodik zum Entwickeln und Konstruieren technischer Systeme und Pro-
dukte. Verein Deutscher Ingenieure. Beuth, Berlin (1993) 
2. Schulz, I.: Multimethodische Arbeitsumgebung für die Produktentwicklung. Schriftenreihe 
Produktentwicklung und Konstruktionsmethodik, vol. 4. Shaker, Aachen (2008) 
3. Livotov, P., Petrov, V.: Innovationstechnologie TRIZ: Produktentwicklung und Prob-
lemlösung. TriSolver Consulting, Hannover (2002) 
4. Pahl, G., Beitz, W.: Engineering Design: A Systematic Approach. Springer (2005) 
5. Suh, N.P.: Axiomatic Design: Advances and Applications. Oxford University Press, New 
York (2001) 
6. Kreimeyer, M., Heymann, M., Lauer, W., Lindemann, U.: Die Konstruktionsmethodik im 
Wandel der Zeit: Ein Überblick zum 100. Geburtstag von Prof. Wolf Rodenacker. Kon-
struktion 58(10), 72–74 (2006) 
7. Feldhusen, J.: Lecture Konstruktionslehre I, Aachen (2010) 
8. Djaloeis, R., Frenz, M., Schlick, C.M.: Einflußfaktoren zur Analyse menschlicher Fehler 
und Zuverlässigkeit in der Produktentwicklung. In: Schütte, M. (ed.) Gestaltung nachhal-
tiger Arbeitssysteme: Wege zur Gesunden, Effizienten und Sicheren Arbeit; Bericht zum 
58. Kongress der Gesellschaft für Arbeitswissenschaft vom 22. bis 24. February 
2012/GfA, Gesellschaft für Arbeitswissenschaft e.V. Jahresdokumentation/Gesellschaft für 
Arbeitswissenschaft e.V. GfA-Press, Dortmund (2012) 
9. Dylla, N.: Denk- und Handlungsabläufe beim Konstruieren. Hanser, München (1991) 
10. Bender, B.: Erfolgreiche individuelle Vorgehensstrategien in frühen Phasen der Produk-
tentwicklung. Fortschritts-Berichte VDI, vol. 377. VDI-Verlag, Düsseldorf (2004) 
 
 

524 
M. Hinsch et al. 
11. Duckwitz, S., Djaloeis, R., Hinsch, M., Feldhusen, J., Schlick, C.M.: Consideration of 
Human Reliability in Actor-Oriented Simulation of New Product Development. In: 2012 
IEEE Conference of Systems, Man, and Cybernetics (SMC), Seoul (2012)  
(accepted paper) 
12. Hacker, W.: Allgemeine Arbeitspsychologie – psychische Regulation von Wissens-, Denk- 
und körperlicher Arbeit, 2nd edn. Hans Huber, Bern (2005) 
13. Liepmann, D., Beauducel, A., Brocke, B., Amthauer, R.: Intelligenz-Struktur-Test 2000 R: 
extended Edition. Hogrefe, Göttingen (2007) 
14. Hart, S.G.: NASA-Taks Load Index (NASA-TLX); 20 Years Later. In: Proceedings of the 
Human Factors and Ergonomics Society. 50th Annual Meeting, pp. 904–908 (2006) 
15. Hinsch, M., Heller, J.E., Feldhusen, J.: Improved Application of Design Methodology: 
Taking Man-Induced Disturbances into Account. In: Buck, L., Frateuer, G., Ion, W., 
McMahon, C., Baelus, C., De Grande, G., Verwulgen, S. (eds.) Design Education for Fu-
ture Wellbeing. Proceedings of the 14th International Conference on Engineering and 
Product Design Education, Antwerp (2012) 
16. Duckwitz, S., Djaloeis, R., Hinsch, M., Feldhusen, J., Schlick, C.M.: Consideration of 
Human Reliability in Actor-Oriented Simulation of New Product Development. In: IEEE 
International Conference on Industrial Engineering and Engineering Management (IEEM), 
Hong Kong (2012) (accepted paper) 
17. Schreyögg, G.: Grundlagen der Organisation: Basiswissen für Studium und Praxis. Gabler, 
Wiesbaden (2012) 
18. Borkenau, P., Ostendorf, F.: NEO-Fünf-Faktoren-Inventar nach Costa und McCrae (NEO-
FFI): Manual, 2nd edn. Hogrefe, Göttingen (2008) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 525–534. 
DOI: 10.1007/978-3-642-30817-8_51 
© Springer-Verlag Berlin Heidelberg 2013 
 
Internal Innovation Communities from a User’s 
Perspective: How to Foster Motivation for Participation 
Albert Albers1, Ludwig Maul2, and Nikola Bursac2 
1 IPEK - Institute of Product Engineering,  
Karlsruhe Institute of Technology, Karlsruhe, Germany 
2 Design Methods and Management, IPEK - Institute of Product Engineering,  
Karlsruhe Institute of Technology, Karlsruhe, Germany 
{albert.albers,nikola.bursac}@kit.edu, 
maul@ipek.uka.de 
Abstract. In order to further include the employees’ creativity in the innovation 
processes, companies provide social software platforms for internal innovation 
communities to share, discuss and evaluate ideas. The main challenge for orga-
nizing such communities is to foster motivation for participation. In this paper, 
motivation theories are put in context with an innovation community concept 
developed at the automotive manufacturer Dr. Ing. h.c. F. Porsche AG. Firstly, 
an interview study analysis of this concept is used to identify new relevant ex-
pectations, hopes, needs and abilities of employees. Secondly, measures pro-
posed in the past are evaluated. With the help of 20 semi-structured interviews 
it can be shown that each of the employees is unique and has own motivational 
deficiencies. Thus it is important that a concept for fostering motivation in-
cludes several measures which motivate the various users to participate. These 
measures are integrated into a holistic concept presented in this paper. 
Keywords: tools for innovation, collaboration, innovation community, user 
motivation, enterprise 2.0. 
1 
Introduction 
Researching the open innovation paradigm, Chesbrough suggests that organizations 
should integrate new sources for innovative product ideas from outside into their in-
novation processes [5]. However, Gassmann and Enkel found that limited absorptive 
capacities make it challenging to transfer external knowledge through organization 
boundaries [8]. Only recently more and more companies identify their own employees 
from various divisions inside the company as useful sources for new product ideas. 
Often, however, the accumulated knowledge and creativity necessary for generating 
such ideas is spread out widely through the organization. From a perspective of know-
ledge management these findings implicate two major tasks. Firstly, effective 
processes and tools for collaboration have to be provided through a software platform. 
Secondly, a culture of participation, curiosity and knowledge sharing has to be lived. 

526 
A. Albers, L. Maul, and N. Bursac 
2 
State of the Art 
2.1 
Research on Innovation Communities 
Koch and Richter suggest that Social Software is suitable for collaboration and know-
ledge sharing especially in the case of large organizations [11]. As can be seen  
on popular internet applications, people are able and willing to share knowledge (e.g. 
on Wikipedia.org), discuss issues (e.g. on Facebook.com) and evaluate products  
(e.g. on Amazon.com) through social network functionalities. These typical functio-
nalities for communication, collaboration and knowledge sharing can be transferred 
and adapted to an organization’s internal platform for an innovation community, 
enabling employees to share, discuss and evaluate new product ideas. The Innovation 
Jam carried out by IBM in 2006 shows the potential of such platforms. Bjelland and 
Wood give an overview on the results of this online brainstorming session initiated by 
the IBM Top Management with the aim to bring together the creativity of more than 
300.000 employees. As a result, in only 72 hours 150.000 employees, family mem-
bers, business partners, clients and university researchers generated 46.000 ideas re-
sulting in 10 new business units [4]. Since then Reinhardt and others have developed 
a variety of concepts for processes, structures and IT landscapes for social networks 
to support collaborative idea generation within organizations. However, after installa-
tion in organizations many of such innovation communities lack user participation.  
One main reason for that can be found in studies of Albers et al: While in recent 
decades more and more effort was put into the development of computer tools for 
product engineering, the role of humans was neglected [2]. In order to organize a 
community for effective idea generation, the user has to be put in the center of a 
community concept. The innovation potential of an internal community can only be 
fully utilized if the user’s expectations, hopes, needs and abilities are met. Hence, user 
acceptance is believed to be one of the keys to the success of any tool that is supposed 
to support the design process. This is why motivation theories from social sciences 
are in this paper put in the context of an innovation community platform developed at 
the automotive manufacturer Dr. Ing. h.c. F. Porsche AG. 
2.2 
Research on Motivation 
Thus far, research has rarely considered the role of motivation in the context of com-
munity innovation. Schattke and Kehr analyzed motivation in innovation communities 
with ‘the compensatory model of work motivation and volition’. [13] Kehr’s model 
includes three components: Explicit motives, implicit motives and perceived abilities. 
[9] Explicit motives can be expressed by a person, they are consciously accessible and 
they constitute the reason for their actions. In contrast to that, implicit motives lead to 
behavioral impulses and are subconscious. Additionally, perceived abilities are the 
basis for people to perform certain acts. [9] With regard to implicit motives McClel-
land differentiates between the need for affiliation, the need for achievement and the 
need for power. The need for affiliation describes the desire of people to enter new 
social relationships and stay in touch with their friends. The need for achievement 
prompts people to explore what they are able to, so they can grow with new  

 
Internal Innovation Communities from a User’s Perspective 
527 
challenges and expand their own limits. People who are motivated by the need of 
power aspire to have power and to keep it. They focus mainly on strength and control. 
Each of the types of need can appear more or less dominant to different individuals. 
[12]  
Kehr states, that if explicit and implicit motives of a person are congruent, the per-
son is enabled to be intrinsically motivated. If the perceived abilities also match these 
motives, the person can immerse into a ‘flow’ experience. [9] This is a state, in which 
a person has undivided attention to a task, a changed sense of time and no disturbing 
thoughts. [6] Von Cube relates ‘flow’ to the experience of mastering a challenge and 
go beyond one’s expectations as in mountain climbing or complex problem solving. 
[7] This element is also addressed in ‘the compensatory model of work motivation 
and volition’. It is found within the need for achievement. 
For the purpose of an innovation community platform, where participation is vo-
luntary, it is necessary to make sure that the users are able to have a flow experience. 
They need to be motivated explicitly as well as implicitly and must be enabled by 
their perceived abilities. Although Kehr offers different approaches to motivate 
people who have deficiencies in one of the three components of his model, Schattke 
and Kehr mainly concentrate on implicit motives and the need for affiliation, 
achievement and power. From these types of need the Authors derive recommenda-
tions for measures to motivate different types of employees.  
A person who has deficiencies in explicit motives is not convinced that the action 
is required. Such a Person can be motivated by measures that focus on cognitive as-
pects. These are e.g. argumentation for the necessity of the action, setting goals and 
solving of conflicts of objectives. [10] A person who has deficiencies in implicit mo-
tives can be motivated by stimulating the needs for affiliation, achievement and pow-
er. Depending on the personal preferences, the need for affiliation can be stimulated 
by teamwork, the need for achievement can be addressed by challenging assignments 
and the need for power can be satisfied by the chance to earn prestige and responsibil-
ity. [10] A person who has deficiencies in perceived abilities is not able to solve the 
task with his abilities. These deficiencies can be overcome by making the task easier 
or by improving the person’s abilities by assistance. [10] 
3 
Development of an Integrated Concept 
3.1 
Aim of Research 
The authors are convinced that humans and their individual factors should be positioned 
in the center of product engineering when developing new methods and processes (see 
also [1]). In the case of an innovation community, an undefined number of users can be 
involved. Every one of them is a unique individual and will respond differently to meas-
ures of community management. Thus, when implementing a community platform, it is 
crucial to motivate all these different types of users. While approaches for stimulating 
implicit motives already exist [10], this paper aims at the identification of new relevant 
expectations, hopes (explicit motivation) and abilities (perceived abilities) of employees, 
the evaluation of existing measures for fostering (implicit) motivation, the development 
of new measures for innovation community management and the integration of these 
measures into one holistic concept. 

528 
A. Albers, L. Maul, and N. Bursac 
3.2 
Methodology 
‘The compensatory model of work motivation and volition’ offers different perspec-
tives on the motivation of users for participation within the community. Because of its 
broad approach this model has been chosen to support the framework for the further 
interview study analysis and the development of the concept introduced in this paper.  
The interview study was conducted with the automotive manufacturer Dr. Ing. h.c. 
F. Porsche AG. Two workshops with innovation management experts have been held 
to identify relevant aspects for the following interviews. In addition, two innovation 
community platforms for demonstrational and test purposes were introduced to pro-
vide interviewees with a deeper understanding of variations in basic features of such 
platforms. Semi-structured interviews with 20 employees including current and poten-
tial users of an innovation community form the basis for the identification of relevant 
motives and the evaluation of possible management measures. Questions asked during 
the interviews concern the three areas explicit motives, implicit motives and per-
ceived abilities. Thereby probable reasons for a lack of explicit motivation or per-
ceived abilities have been identified and first potential solutions have been derived. 
Regarding implicit motivation 15 possible features for innovation community plat-
forms were presented to the interviewees, who were asked to rank these according to 
how much they would like to use them. On the basis of the interview study, measures 
have been developed to foster motivation. Newly identified and evaluated measures 
are then checked for consistency because several measures can cause conflicts be-
tween various objectives. Those are identified and solved through a holistic concept. 
 
Theoretical
framework
Employee
Interviews
Holistic
Concept
Expert Workshops
Demonstrational Platforms
 
Fig. 1. Overview of this paper's research 
4 
Findings 
4.1 
Explicit Motives 
Explicit motives are consciously accessible and the cognitive reason for people to 
undertake an action. Two major aspects of these motives were found in the experts 
workshops. Firstly, knowing about the relevance of the innovation community in a 
company and secondly, understanding the relevance of the specific innovation task 
that is to be solved by the community.  
Relevance of the Innovation Community 
The interviews revealed that “the projects [the employees are] working on don’t leave 
a lot of time. If [they] spend time working in the community […] it should be valua-
ble for the company.” [Interviewee 15] Furthermore they say that “working in the 
community is definitely not prioritized.” [Interviewee 5] In summary because of the 
lack of time it is important for the users that ideas can be efficiently added. In addi-
tion, reasons for the relevance of an innovation community need to be obvious. There-
fore it is important that the employees see the necessity for innovation on the one 
hand and the suitability of a community to generate innovation on the other hand. 

 
Internal Innovation Communities from a User’s Perspective 
529 
Relevance of the Innovation Task 
The attention of an innovation community can be led by specific innovation tasks. 
Even if an employee is convinced by the relevance of an innovation community, it is 
further necessary to give arguments why the particular innovation task is important. 
One interview partner who worked in a workshop with a test community platform 
says that “in order to generate an additional value for the company, [he] always tried 
to solve the given task.” [Interviewee 15] Furthermore an interview partner explains 
that “the main point is what the benefit for the [company] is.” [Interviewee 2] In 
summary the users expect the tasks to be within a strategically relevant area and on a 
question, on which answers from the community can make a noticeable impact. 
4.2 
Implicit Motives 
Implicit motives can be strengthened by stimulating the needs for affiliation, 
achievement and power. This can be accomplished by implementing specific func-
tions in the innovation community platform. In the expert workshops, in literature and 
in case studies from existing platforms such functions were collected. After an intro-
duction of the different functions that stimulate the need for affiliation, achievement 
and power, the results of the evaluation are analyzed and visualized in this chapter.   
Functions to Stimulate the Need for Affiliation, Achievement and Power 
Functions to stimulate the need for affiliation must address the desire of people to 
establish new relationships and stay in touch with existing relationships. Therefore 
functions for communication are important to people who are motivated by the need 
for affiliation. Examples for this kind of functions are the possibility to send personal 
messages and a chat function. Furthermore it can be shown to users who else is online 
in the community. [13] In addition, further functions can include the possibility to set 
up personal profiles and to link with colleagues.  
Functions to stimulate the need for achievement are effective if they give the em-
ployees the chance to engage in new challenges and improve their skills. Therefore a 
function for asking experienced members for feedback can be interesting. Ratings 
given from others also provide feedback on one’s achievement. [13] Furthermore 
tools for visualization give the community members the chance to give feedback more 
easily and improve the quality of their posts. The personal skills can be improved by 
following categories in order to learn more about specific topics.  
Functions to stimulate the need for power are supposed to give the employees the 
chance to gain prestige. This can for example be reached with the name and the por-
trait of the idea generator placed next to his idea. Motivation of employees that focus 
on career can be achieved through a function to inform the supervisor by automatical-
ly forwarding own ideas. Power can also be gained by becoming a moderator. Em-
ployees can be motivated by having their ideas compared to others in a ranking and 
earning a title like ‘innovator of the month’. [13] Additional statistics provide the 
possibility to compete with others. 
Evaluation and Visualization of the Functions  
Fifteen potential users were asked to rank the suggested functions by their personal 
preference. The result of that analysis can be seen in figure 2. 

530 
A. Albers, L. Maul, and N. Bursac 
 
Fig. 2. Ranking of functions according to personal preferences of interviewees 
 
Fig. 3. Multidimensional unfolding to visualize the personal preferences towards different 
functions within a community platform  
Except the function ‘rating ideas’, which is a basic function and expected by the in-
terviewees, the distribution of the ranking is heterogeneous. The function for automat-
ic idea ‘forwarding to supervisors’ is for example ranked first by two employees and 
ranked last by three others. One possible explanation for such discrepancy is that 
power motivated users focus on that function, whereas employees who are motivated 
by the need for affiliation and achievement do not consider this function as important. 
0
1
2
3
4
5
6
7
8
Number of times ranked
Ranked 1st
Ranked 2nd
Ranked 3rd
Ranked 4th
Ranked 5th
Ranked 6th
Ranked 7th
Ranked 8th
Ranked 9th
Ranked 10th
Ranked 11th
Ranked 12th
Ranked 13th
Ranked 14th
Ranked 15th
0
1
2
3
4
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15
Number of times ranked
Ranked
forwarding to supervisor

 
Internal Innovation Communities from a User’s Perspective 
531 
If the decision whether to implement certain functions in the community platform or 
not was based on the averaged rating of the potential users, functions would not be 
taken into account although they are most important to some users. In addition in 
figure 3 the data is visualized by multidimensional unfolding. This statistic method 
allows to project objects and subjects by their similarity in a two dimensional space. 
[3] Thus functions that are ranked high by an interviewee are placed close to him, 
whereas functions that he dislikes have a larger distance to him. 
By looking at figure 3 it can be seen that similar functions like the ‘chat function’ 
and the ‘online visibility’ function are placed close to each other. Furthermore the 
functions that stimulate the need for affiliation are clustered, as well as the functions 
that stimulate the need for power and achievement.  
4.3 
Perceived Abilities 
Usability 
Most interviewees stated that they have experience with internet applications like 
Wikipedia, Facebook and Amazon. However they say that “even small technical dif-
ficulties can demotivate potential users.” [Interviewee 16] For example the communi-
ty platforms for demonstrational and test purposes had a few deficiencies in this area, 
which “threw [users] back in [their] motivation.” [Interviewee 15] Furthermore an 
interviewee says “even though [he] consider[s the innovation community] as very 
important, [he expects] that the access and the handling with the community platform 
is very easy and uncomplicated.” [Interviewee 16] In addition care should be taken 
that the “effort is as low as possible to work” [Interviewee 10] in the community plat-
form and “ideas [can] be entered efficiently.” [Interviewee 5] In summary the inter-
viewees expect an intuitive community platform with a comfortable access.  
Selection of Innovation Tasks  
One interviewee is "sure, that [he] can contribute to some innovation tasks more than 
to others, based on [his] experience." [Interviewee 3] Another interviewee "believe[s] 
that everyone has his favorite topics." [Interviewee 15]. By the sample of interviewees 
it was confirmed that different users have individual knowledge, specific technology 
or market expertise and personal intellectual skills. Depending on the area, topic and 
the question of the innovation task, users felt more or less creative. If a task makes it 
too hard for a user to contribute at all, he might get frustrated. Thus, an innovation 
task is only motivating, as long as it appears solvable to users. On the other hand, 
users who are driven by the need for achievement might get bored if a given task is 
too simple and lets them only generate ideas which are already obvious. For such 
users the optimal level of excitement can be reached, when a task appears challenging 
and demands just the very best effort of the user to solve.  
5 
Application in an Example Case 
At the example of a community concept developed at Dr. Ing. h.c. F. Porsche AG it is 
shown how the findings from Chapter 4 can be applied on a community platform. 
Firstly measures are recommended, secondly they are analyzed in a holistic context. 

532 
A. Albers, L. Maul, and N. Bursac 
5.1 
Measures to Foster Motivation for Participation 
Five major issues have been identified in the interview study: The user’s understand-
ing of the relevance of an innovation community as well as the specific innovation 
tasks, providing functions to stimulate the users’ implicit motives, ensuring usability 
of the software and the adequate selection of innovation tasks. In the example case 
these issues are addressed with the following recommendations: 
1. In order to show to the users the relevance of the innovation community platform it 
is recommended, that the top management publishes statements for innovation 
communities in the platform, articles are printed in the company magazine and the 
CEO sends an e-mail to the users directly calling for participation. Furthermore the 
priority in working in the platform can be improved if the top management also 
participates in the community. In addition, videos of successful cases from the past 
can be presented to show the suitability of a community to generate innovation. 
2. In order to show the users the relevance of the specific innovation tasks it is rec-
ommended that the tasks are related to the organization’s strategy, endorsed by sta-
tistics and studies explaining why each task is going to be important for the future. 
3. Considering that every potential member of the community is an individual, the in-
novation community platform should offer a multitude of different functions to 
motivate all kinds of users. On the other hand, there are hardly any users who want 
to use every possible function. As a consequence every member should be able to 
decide on his own if he wants to use a function or not, e.g. rankings. 
4. For high usability the access to the community platform should be quickly found, 
for example via a link on the intranet homepage. After the computer login, an addi-
tional login request for the community platform should be avoided. A video  
explaining the platform and answers to frequently asked questions can help inexpe-
rienced users. To identify difficulties, the community platform should be estab-
lished in several steps, starting with testers who are not easily frustrated. 
5. Since every user will define his perceived abilities differently, it is suggested, that 
several innovation tasks are given to the community at the same time. The users 
can then decide which task they would like to work on. Just like different goals are 
chosen by mountain climbers according to their own physical fitness, different 
tasks should cover various levels of complexity and various topics. In addition, a 
‘free ideas’ task should be opened to address ideas which don’t match given topics.   
5.2 
Holistic Concept on Motivation in an Innovation Community Platform 
Since the employees have different motivational deficits the recommended measures 
address all three components of ‘the compensatory model of work motivation and 
volition’. These various measures have to be checked for consistency. It is recom-
mended that the community platform has a large number of different functions. How-
ever, it should be easy and uncomplicated to work with.  
One interviewee describes one testing platform as “extremely confusing.” [Inter-
viewee 16] Another interviewee says about the same testing platform, that “it is a 
matter of taste, what is too much [or] too few. [Apparently, he is] already used to 
other community platforms” [Interviewee 3] and, “because [he has] seen a lot of  
platforms, [he is] able to imagine something behind all the functions.” [Interviewee 3] 

 
Internal Innovation Communities from a User’s Perspective 
533 
Thus different users with different experience perceive the usability of the same plat-
form more or less easy to handle.  
In order to equip the platform with interesting functions but prevent an overload, 
an analogy to smartphones may be considered. Smartphones can have a lot of differ-
ent functions. Their handling, however, is perceived intuitive. One reason for that is 
that there are only a small number of functions installed, when delivered. Once a user 
gains more experience, he can then enhance the smartphone through downloading and 
installing new applications of his choice. This logic can be transferred to innovation 
communities. Like smartphones functions the community platforms functions should 
be modular. In the beginning, every user should only see basic functions to ensure a 
good usability. However, he should then be able to customize the platform according 
to his needs. From the user’s view, such self-determination is important for the accep-
tance of the platform. That is why as few functions as possible should be enforced 
upon a user or withheld from him.  
6 
Conclusion and Outlook 
The findings on the basis of ‘the compensatory model of work motivation and  
volition’ indicate the complexity of motivating users to participate in an innovation 
community. Since every user is unique and responds differently to measures of com-
munity management, a ‘one-size-fits-all’-approach is rarely adequate. With the exam-
ple case it is illustrated how motivation for participation can be fostered. A broad set 
of measures regarding layout design, processes and communication of a community 
platform is recommended. Furthermore a modular platform is proposed which can be 
individually configured by the users according to their motives and abilities. 
Kehr’s ‘compensatory model of work motivation and volition’ showed to be suita-
ble to analyze motivation in innovation communities. For future research on human 
factors further theories from the social sciences may be considered. In order to solidi-
fy the findings and validate the described measures for the example case further anal-
ysis on cases from other organizations should be carried out . The presented concept 
needs to be further developed and additional objectives and requirements within the 
context of organizations have to be considered, This includes intellectual property 
management, information technology security, data protection, human resources, cost 
optimization and processes of moderation, transfer and implementation of ideas. 
Whilst research on innovation communities has started only recently and methods and 
processes for community management will become more and more enhanced in the 
future, putting humans in the centre will be the most important success factor. 
References 
1. Albers, A.: Five Hypotheses about Engineering Processes and their Consequences. In: 
Horváth, I., Mandorli, F., Rusák, Z. (Hrsg.) Proceedings of the TMCE 2010, Organizing 
Committee of TMCE (2010) 
2. Albers, A., Sadowski, E., Marxen, L.: A new Perspective on Product Engineering – Over-
coming Sequential Process Models. In: The Future of Design Methodology. Birkhofer, 
Herbert (2011) 

534 
A. Albers, L. Maul, and N. Bursac 
3. Backhaus, K., Erichson, B., Weiber, R.: Fortgeschrittene Multivariate Analysemethoden: 
Eine anwendungsorientierte Einführung. Springer (2010) 
4. Bjelland, O.M., Wood, R.C.: An Inside View of IBM’s Innovation Jam. MIT Sloan Man-
agement Review 49, 32–40 (Fall 2008) 
5. Chesbrough, H.W.: The Era of Open Innovation. MIT Sloan Management Review 44(3), 
35–41 (2003) 
6. Csikszentmihalyi, M., Aebli, H., Aeschbacher, U.: Das Flow-Erlebnis: Jenseits von Angst 
und Langeweile: im Tun aufgehen. Klett Cotta Verlag (2008) 
7. von Cube, F.: Lust an Leistung - die Naturgesetze der Führung. Piper, München (1997) 
8. Gassmann, O., Enkel, E.: Open Innovation – Die Öffnung des Innovationsprozesses erhöht 
das Innovationspotetial. In: ZfO 0.3/2006, pp. 132–138 (2006) 
9. Kehr, H.: Integrating implicit motives, explicit motives, and perceived abilities: the com-
pensatory model of work motivation and volition. Academy of Management Review 29(3), 
479–499 (2004) 
10. Kehr, H.: Das Kompensationsmodell von Motivation und Volation als Basis für die 
Führung von Mitarbeitern. In: Vollmeyer, R., Brunstein, J.C. (eds.) Motivationspsycholo-
gie und Ihre Anwendungen. 1., Aufl., pp. 130–150. Kohlhammer (2005) 
11. Koch, M., Richter, A.: Enterprise 2.0 – Planung, Einführung und erfolgreicher Einsatz von 
Social Software in Unternehmen, 2. Auflage (2009) 
12. McClelland, D.C., et al.: The relationship of affiliative arousal to dopamine release. Moti-
vation and Emotion 11(1), 51–66 (1987) 
13. Schattke, K., Kehr, H.: Motivation zur Open Innovation. In: Zerfaß, A., Möslein, K.M. 
(eds.) Kommunikation als Erfolgsfaktor im Innovationsmanagement, pp. 121–140. Gabler 
Verlag (2009) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 535–542. 
DOI: 10.1007/978-3-642-30817-8_52 
© Springer-Verlag Berlin Heidelberg 2013 
 
Determining Granularity Level  
in Product Design Architecture 
Tarek AlGeddawy and Hoda ElMaraghy  
 Intelligent Manufacturing Systems Centre, University of Windsor, Ontario, Canada 
{algedd,hae}@uwindsor.ca 
Abstract. Product architecture represents components grouped into modules 
that can be assembled later to constitute a specific variant. Literature provides 
Methods of clustering components into weakly related modules with strong in-
terconnections between components within modules. The number of modules 
and their hierarchical relationships shape product architecture and determine the 
balance between modular design and components integration. A novel hierar-
chical clustering approach, based on the biological Cladistics analysis, has been 
developed to cluster Design Structure Matrix (DSM) widely used to promote 
modularity. It evaluates different granularity levels of the resulting hierarchy 
and finds the best granularity level for maximum modularity. An automotive 
Body-in-White of 38 different components is used as a case study. Results 
showed the superiority of the recommended modularity pattern and synthesized 
product architecture over other clustering techniques. 
Keywords: Product Design, Architecture, Modularity, Granularity. 
1 
Introduction 
Product architecture defines the components of that product in terms of their function 
and the topology of their interfaces. Product architecture allows further detailed de-
sign, testing, manufacturing, supplying, etc. of those components [1]. In an era  
characterized by proliferation of product variety and mass customization, family of 
products and product platforms are typical solutions for product design and manufac-
turing [2]. A modular architecture instigates the appropriate product structure consist-
ing of a group of modules, each with distinguishable function(s), with minimal  
interaction. Many types of modularity can be identified in a product architecture [1], 
1) bus modularity where all modules are connected to a single common module, 2) 
sectional modularity where product variants are built from specific combinations of 
modules with a unified interface, 3) scalable modularity where some components with 
scalable parameters are combined with standard components. 
Granularity level of a product refers to the number of decomposition stages of its 
components’ hierarchy and which components exist at each stage. Granularity level of 
modular product architecture is of prime importance. Sharing common modules 
across many product variants facilitates economy of scale, while modules differentia-
tion increases product variants and enables economy of scope. Effective trade-off 
between the two types of economies depends on the appropriate identification of 

536 
T. AlGeddawy and H. ElMaraghy 
 
common and different modules and their interaction. A product architecture,  
especially in a setup of family of products, should consider the appropriate level of 
aggregation [3]. 
This paper introduces a new model that is capable of determining the appropriate 
level of granularity for a product design, as well as the structure of its architecture, 
based on the interactions among its components. The model also includes a new clus-
tering tool to group components into modules in a hierarchical structure revealing the 
appropriate product architecture. 
2 
Product Modularity 
Product components are usually grouped into modules assembled according to a spe-
cific design architecture to facilitate future design changes, product variety, mass 
customization and manufacturing processes using delayed product differentiation [4]. 
Design Structure Matrix (DSM) is the most common tool used to represent interac-
tions among components (component-based DSM) in a system [5]. DSM elements are 
usually binary; however, other values can be used to express the intensity of compo-
nent interactions. In binary DSMs, ‘1’ indicates interaction existence, while ‘0’ indi-
cates lack of interaction. Grouping product components into modules can be  
accomplished by clustering DSM into blocks of ‘1’ elements. 
There are few techniques available to cluster a DSM for modularity.  The differ-
ences between those techniques are mainly in the clustering objective function. Coor-
dination cost minimization was one of the first methods used for clustering [6], in 
which each DSM element is placed in an individual cluster and bids evaluated from 
all other clusters. Clustering is a tradeoff between the costs of a component being 
within vs. outside a cluster. The maximum number of components in a cluster was 
predetermined to prevent large clusters formation, and iterations were performed us-
ing simulated annealing. 
Minimal Description Length (MDL) is another objective function introduced to 
cluster a DSM [7]. MDL is an evaluation function that compares clustered DSMs to a 
targeted DSM structure and considers their mismatched elements. Although the me-
thod allows the possibility of having overlaps among modules and performing bus 
clustering, it is highly dependent on the target DSM and its clustering scheme. The 
objective of clustering process was to minimize MDL, while GAs were used to gener-
ate chromosomes for clusters. Number of clusters is determined beforehand based on 
the given targeted DSM structure. A clustering efficiency (CE) index was also used to 
direct DSM clustering process [8]. CE is a weighed count of zero elements inside 
clusters and non-zero elements outside clusters. A neural network optimization ap-
proach was used for clusters generation. If the number of clusters is not predefined, 
the model would form fewer clusters than desired. One problem with applying auto-
mated clustering algorithms to some complex DSMs is the difficulty of extracting the 
relevant information from the data, and  conveying it to the user [9]. 
The review of existing clustering techniques and modularity measures reveals two 
points. First, the objective function for DSM clustering is normally closely related to 

 
Determining Granularity Level in Product Design Architecture 
537 
 
the existence of interactions within and between the resulting clusters. Chiriac et al. 
[10] compared modularity of several products based on different indices, and all of 
them were consistent and yielded the same  results. Since many formulae of modular-
ity indices exist without clear advantages, the simplest formulation would always be 
preferable. In this paper, a modified clustering efficiency (CE) index [8] formula is 
adopted in a much simpler form. 
Second, available DSM clustering techniques reveal only one dimensional modules 
formation, even when compared to bus modularity architecture. Yu et al. [7] imposed 
bus architecture on the clustering process based on prior designer knowledge. The 
true nature of product architecture and its granularity is difficult to be revealed using 
clustered DSM only, especially for large matrices representing complex systems [9], 
without human intervention. 
This paper introduces a new DSM clustering technique for product modularity. The 
technique finds the best granularity level of modular product architecture without 
forcing a specific modularity structure or pre-defining the number of modules or 
number of components per module. 
3 
Product Architecture Granularity Model 
This new model uses hierarchal clustering to divide a DSM into the best modular 
architecture. Cladistics, a classification tool extensively used in Biology [11], reveals 
the evolution hypothesis and speciation scheme of a studied group of entities. Cladis-
tics was first introduced to propose evolution hypotheses to the world of artifacts in 
ElMaraghy et al. [12]. Cladistics analysis was applied to the extracted features of 
automobile engine blocks using historical data to study their evolution and plan their 
future designs. Those variants have a total of 10 characters/features as shown in Fig.1, 
in which (1) refers to presence of a character (derived state), while (0) refers to the 
absence of that character (primitive state). 
Cladogram construction is performed to obtain the most parsimonious data repre-
sentation with the minimum information content. It is referred to as the cladogram 
length, which is the number of characters appearing on the branches of the cladistic 
tree. Fig.1 shows that the total number of derived character states (1) is 25, which is 
the upper value of the information content, while the number of characters is 10, 
which is the lower value. The presented cladogram has a length of 17 derived charac-
ters, i.e. there is a reduction of (26-17=9) in the information content. A handful of 
specialized software is dedicated to cladogram construction such as Hennig86, PAUP, 
NONA, PeeWee, Phylip, etc.  and can perform very fast clustering on huge data [13]. 
NONA is used in this paper for cladogram construction. 
Cladogram construction process is modified to allow for DSM clustering where re-
lationships are considered as characters. The self-relationships of components to 
themselves are represented by the ‘1’ diagonal elements of the original DSM. To 
determine the best granularity level on a resulting cladogram, a simple modularity 
index (MI) is used to specify the depth of the cladogram topology that is equivalent to 
the best granularity level and given in equation 1. 

538 
T. AlGeddawy and H. ElMaraghy 
 
MI=I+Z 
(1)
Where I is the number of ‘1’ elements in the DSM outside formed clusters, and Z is 
the number of zero elements of those clusters. The aim is to minimize the value of 
MI. MI value changes from a granularity level to another depending on the developed 
clusters. Best granularity level corresponds to the minimum value of MI. 
 
 
Fig. 1. An example of a set of engine blocks showing the hierarchal clustering technique of 
Cladistics analysis. Adapted from [12] 
4 
Automotive Body-in-White Case Study 
This case study was published in [8, 14] to evaluate their modularity measures and 
clustering technique, which use neural networks for optimization. A body-in-white 
(BIW) is a structure of large number of metal parts to be welded in the body shop in 
preparation for painting later in the paint shop (Fig.2).  The DSM that presents rela-
tionships among BIW components is given in Fig.3. The BIW components in the 
DSM are listed alphabetically. Self-relationships of components are replaced with ‘1’ 
elements so they can be used as an input to the proposed cladistics clustering analysis. 

 
Determ
 
Fig. 2. An exp
Fig. 3. The DSM
mining Granularity Level in Product Design Architecture 
 
ploded view of automotive BIW components [15] 
M of automotive BIW case study (Adapted from [8]) 
539 
  

540 
T. AlGeddawy and H. ElMaraghy 
 
5 
Results and Discussion 
The DSM of the BIW has been used to construct a cladogram using NONA cladistics 
software. The resulting cladogram has a length of 127 steps and shown in Fig.4. Cla-
dogram topology shows 10 possible levels of granularity between the top root level 
and the terminal level of individual components. To determine the best granularity 
level, Modularity Index (MI) is calculated for each possible granularity level pre-
sented by the cladogram topology. The ordered DSM is used to draw cluster bounda-
ries at each level to identify the ‘1’ elements outside cluster boundaries and the ‘0’ 
elements inside those boundaries. Results show that level 6 scores the least MI=160, 
hence, it is the best granularity level. It has 9 modules, module {36, 1, 11, 3, 4}, mod-
ule {9, 2, 35 ,34}, module {30, 8, 29, 28, 25, 19}, module {27, 15}, module {38, 23, 
24}, module {17, 14, 18, 13, 10}, module {26, 21, 5, 7, 6}, module {20, 12, 37} and 
finally module {33, 32, 31, 16, 22}. 
 
Fig. 4. Cladogram showing the different architecture granularity levels and component modules 
of the BIW case study 
Cladogram topology above dotted line of level 6 in Fig.4 is the granularity map of 
the desired BIW product architecture. It can be converted to a product architecture 
layout similar to a Bill-of-Material (BOM) tree in Fig.5 by adding nodes of sub-
assemblies at each level to illustrate unions of component modules. Modules {38, 1, 
11, 3, 4} and {9, 2, 35, 34} exist at the bottom of the tree indicating that these are 
core modules with the most relationships with other modules. They have the least 
probability of change over product generations and the highest potential of being used 
as a product platform. The MI of clustering results obtained by Pandremenos and 
Chryssolouris [8] is 204, a higher value than MI=160 obtained from the proposed 
cladogram based method. Therefore, the clustering model presented in this paper 
 

 
Determining Granularity Level in Product Design Architecture 
541 
 
 
Fig. 5. The BIW architecture showing the obtained best granularity level and its modules 
produces more efficient clustering compared to the latest models proposed in the  
literature [8]. In addition, the method is also capable of determining the architecture 
structure including number of granularity levels, components, modules and subas-
semblies at each level of the BIW case study. 
6 
Conclusions 
System architecture represents the main design of system components layout and their 
interactions. In addition, a modular architecture is extremely useful for designing 
product variants especially for mass customization, since it facilitates variants custo-
mizing using modules removal, addition and switching.  The interactions among 
components are the driver for grouping components into specific modules. The main 
objective of components clustering is to maximize component integration within each 
module and minimize interactions among modules. 
This paper presented a new clustering model that uses cladistics, a hierarchical 
classification tool commonly used in biology, to 1) find the architecture structure of 
analyzed product design, 2) group product components into the optimum number of 
modules, and 3) specify the best granularity level of the system that minimizes the 
overall interactions between modules and subassemblies at all granularity levels. 
A case study of 38 sheet metal parts to be welded together to form an automotive 
Body-in-White (BIW) is analyzed using the new model. Results showed that: 1) Hie-
rarchal clustering of products using cladistics is able to reveal their architecture, 2) the 
new model determines the optimal granularity level of the system, 3) Modularity re-
sults, based on the new model, are superior to those reported in literature. 
 
 

542 
T. AlGeddawy and H. ElMaraghy 
 
References 
1. Ulrich, K.T., Eppinger, S.D.: Product design and development, 5th edn. McGraw-Hill  
Irwin, Boston (2012) 
2. ElMaraghy, H.: Changing and Evolving Products and Systems - Models and Enablers. In: 
ElMaraghy, H. (ed.) Changeable and Reconfigurable Manufacturing Systems, ch. 2, pp. 
25–45. Springer, London (2009) 
3. Xuehong, D., Jianxin, J., Tseng, M.M.: Architecture of product family: fundamentals and 
methodology. Concurrent Engineering: Research and Applications 9(Copyright 2002, 
IEE), 309–325 (2001) 
4. AlGeddawy, T., ElMaraghy, H.: Product Variety Management in Design and Manufac-
turing: Challenges and Strategies. In: Proceeding of the 4th International Conference on 
Changeable, Agile, Reconfigurable and Virtual Production (CARV 2011), Montreal, Can-
ada (2011) 
5. Eppinger, S.D., Browning, T.R.: Design structure matrix methods and applications. In: 
Engineering Systems 2012, xii, 334 p. MIT Press, Cambridge (2012) 
6. Thebeau, R.E.: Knowledge Management of System Interfaces and Interactions for Product 
development Processes. In: System Design and Management 2001. Massachusetts Institute 
of Technology (2001) 
7. Tian-Li, Y., Yassine, A.A., Goldberg, D.E.: An information theoretic method for de-
veloping modular architectures using genetic algorithms. Research in Engineering De-
sign 18(2), 91–109 (2007) 
8. Pandremenos, J., Chryssolouris, G.: A neural network approach for the development of 
modular product architectures. International Journal of Computer Integrated Manufactur-
ing 24(10), 879–887 (2011) 
9. Sharman, D.M., Yassine, A.A.: Characterizing complex product architectures. Systems 
Engineering 7(1), 35–59 (2004) 
10. Chiriac, N., Holtta-Otto, K., Lysy, D., Eun Suk, S.: Level of Modularity and Different Le-
vels of System Granularity. Journal of Mechanical Design 133(10), 101007 (2011) 
11. Hennig, W.: Phylogenitic Systematics. University of Illinois Press, Urbana (1966); repub-
lished in 1999 
12. ElMaraghy, H., AlGeddawy, T., Azab, A.: Modelling evolution in manufacturing: A bio-
logical analogy. CIRP Annals - Manufacturing Technology 57(1), 467–472 (2008) 
13. Nixon, K.C.: The Parsimony Ratchet, a New Method for Rapid for Rapid Parsimony 
Analysis. Cladistics 15(1), 407–414 (1999) 
14. Pandremenos, J., Chatzikomis, C., Chryssolouris, G.: On the Quantification of Interface 
Design Architectures. AIJSTPME 2(3), 41–48 (2009) 
15. Millet, D., Yvars, P.-A., Tonnelier, P.: A method for identifying the worst recycling case: 
Application on a range of vehicles in the automotive sector. Resources, Conservation and 
Recycling 68(0), 1–13 (2012) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 543–552. 
DOI: 10.1007/978-3-642-30817-8_53     © Springer-Verlag Berlin Heidelberg 2013 
Developing Modular Product Families with Perspectives 
for the Product Program 
Dieter Krause, Sandra Eilmus, and Henry Jonas 
Institute for Product Development and Mechanical Engineering Design (PKT),  
Hamburg University of Technology (TUHH), Denickestraße 17, Hamburg, D-21073, Germany 
sandra.eilmus@tuhh.de 
Abstract. Reducing internal variety is a major challenge for industrial enter-
prises. Several approaches have been presented in literature supporting the  
development of modular product families in order to accomplish variety reduc-
tion. The Integrated PKT-Approach for Developing Modular Product Families 
integrates aspects of design for variety with technical-functional and product -
strategic modularization methods. It furthermore proposes how to embed  
product family development into a holistic strategy for the specific corporate 
product program. 
Keywords: Product family, product program, design for variety, modularization. 
1 
Strategic Background of Product Family Development 
The product development phase determines to which extent a product meets the chal-
lenges of modern market situations. It is important to address contradictory require-
ments and to balance compromises. Globally intense pricing competition as well as 
the megatrend of individualization is reflected in the conflicting customer require-
ments of low prices and personalized, highly variant products. These requirements 
result in two product development strategies. On the one hand, the aim is to develop 
standard mass-market products to offer competitive prices - the focus being on the 
advantage of large quantities of the same products. On the other hand, to be able to 
make a profit, a high number of individualized products can be a successful way to 
meet individual customer requirements. Both strategies involve chances and risks. In 
product development, the strategy of developing modular product families is a prom-
ising way for combining the advantages, such as individual customer demands, with 
low costs, to offer competitive and robust product programs.   
2 
An Example of a Product Family 
A family of herbicide spraying systems is used to demonstrate the use of tools and 
methods in the following. The MANKAR-Roll family by Mantis ULV consists of 
Ultra Low Volume (ULV) Spraying Systems that enable eco-efficient distribution of 

544 
D. Krause, S. Eilmus
herbicides. The existing pro
as well as 24 additional va
these variants adjust the sp
the customers working wit
different spray widths or siz
Fig. 1. Product variants of the 
3 
Methods Suppor
Families 
To support the developme
proaches have been develop
ing of product components
or according to product-stra
dered more detailed in appr
architecture approaches (e.g
partly conducting them in w
as applying them on indust
gained.  Fig. 2 (left) show
herbicide spraying system. 
tween all components are a
are shifted to the diagonal
DSM is a powerful tool in
modular structure. It was e
tural Complexity Managem
people and documents in th
tool within MFD [4] deals 
tegic reasons why compone
an example of the MIM app
components are allocated to
designated as components 
Experiences in workshops 
need the support that both t
functional and product-str
matrix-based approaches, p
perception are missing. The
ponent variety within the pr
s, and H. Jonas 
oduct families consist of twelve actively advertised varia
ariants provided on special request (Fig. 1). For examp
praying systems to the individual application conditions
thin professional in-row cultivations or public places 
zes of wheels. 
product family of herbicide spraying system MANKAR-Roll
rting the Development of Modular Product 
ent of modular product families different scientific 
ped. Approaches on modularization deal with the regro
s either according to technical-functional (e.g. [2] and 
ategic (e.g. [4]) module drivers. Product variety is recon
roaches on design for variety (e.g. [5]) as well as in prod
g. [6]). Researching these approaches in theory or even
workshops with industrial practitioners and students as w
trial examples, experiences on their convenience could
s how the Design Structure Matrix [2, 7] is applied to 
Spatial, energetic, informatics and material couplings 
allocated. The matrix is then re-sorted so that the coupli
. This forms clusters that indicate possible modules. T
n understanding the technical-functional conditions fo
enhanced by integrating the view of organization in Str
ment using even matrices to analyze the couplings betw
he company [8]. The Module Indication Matrix (MIM) a
with product-strategic module drivers, describing the s
ents should be integrated into modules. Fig. 2 (right) sho
plied to the family of herbicide spraying systems. Here 
o the module drivers that they are affected by. Modules 
sharing similar module drivers or module driver patter
and case studies showed that method users appreciate 
tools (DSM and MIM) provide in understanding technic
rategic module drivers. However, when working w
product related visualizations that enable more intuit
e methods give no direct indication of how to reduce co
roduct family. 
ants 
mple, 
s of 
via 
 
l [1] 
ap-
oup-
[3]) 
nsi-
duct 
n by 
well 
d be 
the 
be-
ings 
The 
or a 
ruc-
ween 
as a 
stra-
ows 
the 
are 
rns. 
and 
cal-
with  
tive 
om-

 
Developing Modular Prod
Fig. 2. DSM (left) and
4 
The Integrated P
Product Familie
Combining the strengths of
weaknesses, the Integrated 
[1] was developed at the
(TUHH). This approach for
• Combining the product-
program variety 
• Integrating technical-fun
product life phases 
• Redesign of components
• Fostering team discussio
related visualizations 
• Support for reducing var
Fig. 3. The Integrated PK
duct Families with Perspectives for the Product Program 
d MIM (right) of the family of herbicide spraying systems 
PKT-Approach for Developing Modular 
es 
f the approaches analyzed before and reacting on poten
PKT-Approach for Developing Modular Product Fami
e Institute PKT at Hamburg University of Technolo
r reducing internal variety has the following aims: 
-oriented view with the process-oriented view of prod
nctional and product-strategic module drivers along 
s to enable a variety-optimized product structure 
on and integration of experts by specific product fam
riety tailored to corporate needs. 
 
KT-Approach for Developing Modular Product Families [9] 
545 
 
ntial 
ilies 
ogy 
duct 
the 
mily 

546 
D. Krause, S. Eilmus, and H. Jonas 
Several method units are integrated within an approach reaching towards that aim 
(Fig. 3). Two of them, Design for Variety and Life Phases Modularization, are pre-
sented below. In section 5 tools for Product Program Planning and the Development 
of Modular Product Programs are described. 
4.1 
Design for Variety  
Design for Variety [10,11] aims to bring the product families closer to an ideal of a 
variety-oriented product structure derived from literature research: 
• Clear differentiation between standard components and variant components 
• Reduction of the variant components to the carrier of a differentiating properties 
• One-to-one mapping between differentiating properties and variant components 
• Minimal degree of coupling of variant components to other components. 
In the first step of the method, the external market-based and internal company v 
arieties of the product family are analyzed. A Tree of External Variety (TEV) aids 
analysis of the external variety (Fig. 4). This tree visualizes the selection process of 
the customer by linking variant product properties relevant to customers and the of-
fered product variants.  
 
Fig. 4. Tools for the analysis of product variety [9] 
Internal variety is analyzed at the levels of functions, working principles and com-
ponents. The variety of functions is shown in the Product Family Functional structure 
(PFS) that makes representation of variant and optional functions possible. The Mod-
ule Interface Graph (MIG) is used to visualize and analyses the variety of components 
and connecting flows (Fig. 4). All this information is visualized in the Variety Alloca-
tion Model (VAM, Fig. 5 ). The connections between the four levels demonstrate the 
allocations between differentiating properties, functions, working principles and com-
ponents. In this way, the VAM allows analysis of the degree of fulfillment of the ideal 
of a variety-oriented product structure described before. For variant conformity, any 

 
Developing Modular Product Families with Perspectives for the Product Program 
547 
weak points in the design can be identified at all levels of abstraction. Thus, the VAM 
is the basis for solution finding. In Fig. 5 (left) the differentiating property ‘Selective 
Spraying’ influences various functions, working principles and components that were 
influenced by other differentiating properties in turn. Realizing this property via a 
magnetic clutch instead of a magnetic valve a solution was found with fewer interde-
pendencies between the differentiating properties (Fig. 5, right). By this multiplication 
effects of variance are avoided, with the result that each component is required in only 
a small number of variants and configuration of the product family is simplified. 
 
Fig. 5. Applying the Variety Allocation Model (VAM) as a tool to optimize the product family 
of herbicide spraying systems [9, 10] 
4.2 
Life Phases Modularization  
Life Phases Modularization [10, 12] transfers the results of Design for Variety for 
each relevant product life phase to a continual module structure, while checking con-
sistency and adjustment. Product family structure requirements can be better met by 
considering different product family structures for individual phases. The procedure is 
divided into the following steps: 
1. Development of a technical-functional modularization 
2. Development of modularizations for all stakeholders and product life phases 
3. Combination of modularizations  
4. Derivation of the modular product family structure. 
The starting point is the technical-functional modularization of the product develop-
ment phase. Modules are provided that are largely decoupled to reduce the complexity 
of the development task and allow parallel development of modules. Technical func-
tional approaches (Section 3), such as that described by Stone [3] or the DSM [2,7], 
can be applied at this step. The development of modularization perspectives for all 
relevant product life phases is made by module drivers associated with individual life 
phases. For instance, the production phase is mapped by the module driver ‘Separate 

548 
D. Krause, S. Eilmus
Testing’ (Fig. 6). The mod
Deployment (Section 3, [4]
to develop modules. For th
demonstrate the product-sp
tions are linked to the comp
by grouping the component
one module. Subsequent to
the individual life phases, t
sistency between life phase
structure cannot be realized
is important that the modul
nuous but not 100 % cong
module that is as large as 
module in the form of sm
adapted structure, must not 
parently combines the persp
process more clear (Fig. 6).
 
Fig. 6. The Module Process C
driver specifications to module
4.3 
Results from Indus
The redesign of a family o
for Variety and Life Phases 
on 32 instead of 46 compo
components was reduced fr
the new product family con
the existing and new produ
s, and H. Jonas 
dule drivers are a concept known from Modular Funct
]) that has been supplemented with concrete specificati
he module driver ‘Separate Testing’, the tests carried 
ecific specifications. In network diagrams, these specifi
ponents of the product. The preparation of modules is m
ts that relate to a common module driver specification i
 the development of modular product family structures 
the modularizations are visualized in a MIG to check c
es and find conflicts. It was found that the same mod
d for all life phases because of the contradictory criteria
e structures of the individual phases are adapted and co
gruent. For assembly, it may be advantageous to insta
possible. For purchase, it may be necessary to buy 
maller modules from different suppliers which, in a w
be contradictory. The Module Process Chart (MPC) tra
pectives of different life phases and makes the coordinat
.  
Chart (MPC) as a tool for allocating module drivers and mod
es [10,12] 
strial Application 
of herbicide spraying systems by the method units Des
 Modularization led to a new product family concept ba
onents in total (reduction of 31%). The number of vari
rom 31 to 14 (reduction of 55%). Fig. 7 (left) shows h
ncept enables easy configuration. Furthermore the MIG
uct family concept (Fig. 7, right) show which compone
tion 
ions 
out 
fica-
made 
into 
 for 
con-
dule 
a. It 
onti-
all a 
this 
well-
ans-
tion 
 
dule 
sign 
ased 
iant 
how 
s of 
ents 

 
Developing Modular Prod
were standardized (white). 
studies that were performed
 
Fig. 7. Configurational concep
and MIGs showing the optimiz
For the total number of 
52% could be gained in ave
variant before and only 48%
nents was enhanced from 1
case studies further need 
usability of the integrated P
open the focus of activities 
to a holistic corporate produ
 
Fig. 8. Achieved results in red
5 
Perspectives for
In order to gain a product p
tion according to the specif
be embedded within a corp
by a market-driven produc
modular product programs b
duct Families with Perspectives for the Product Program 
Similar results were achieved in six further industrial c
d for evaluating the approach (Fig. 8). 
pt of the new product family of herbicide spraying system (l
zation of component variety (right) [1] 
components the product families consist of, a reduction
erage. In average 85% of product family components w
% after the optimization. The share of standardized com
5% to 47% in average. Performing evaluation in indust
for research was identified concerning applicability 
PKT-approach [13]. A major need of the companies is
for reducing variety from focus on single product fami
uct program strategy.  
ducing variety of diverse product families (industrial case stud
r the Product Program 
program balanced between standardization and differen
fic corporate context, product family development need
orate product program strategy. This strategy is influen
ct program planning on one hand and the development
based on the carry-over potential on the other hand.  
549 
case 
 
left) 
n of 
were 
mpo-
trial 
and 
s to 
ilies 
 
dies) 
ntia-
s to 
nced 
t of 

550 
D. Krause, S. Eilmus, and H. Jonas 
5.1 
Product Program Planning 
The method unit of Product Program Planning consists of two major phases [14]. 
The first phase uses a new portfolio model which shows hierarchy and economic key 
figures of the whole program as a status analysis. The visualization called the Pro-
gram Structuring Model (PSM) is shown in Fig. 9. In addition to the dimensions, 
traffic light colors can be added to represent the margin of each hierarchical level. 
After analysis of internal and external trend factors, scenarios to the future structure 
of the product program are elaborated in a workshop and visualized by the model. 
Based on the developed scenarios, in the second phase of the method strategic car-
ryover-components are conceptualized to the product program. Aim of this phase is a 
maximization of potential carryover-components, internally of product families as 
well as crossing different product families. 
 
 
Fig. 9. Program Structuring Model (PSM) [14] 
The developed carryover concepts are visualized in the Carryover Assignment Plan 
(CAP), Fig. 10. Outcomes of the method are the planned variety of the program 
represented by a TEV and product concepts including carryover-candidates visualized 
by MIGs. The outcomes represent also the modular product structure of the life phase 
“product planning”. For the next step, the CAP gives the information whether car-
ryover components can be realized rather within or across product families. 
 
 
Fig. 10. Carryover Assignment Plan (CAP) [14] 
Product Family 2
Product 2.1
Product 2.2
Product Family 1
Product 1.1
Product 1.2
Housing 2.2
Enclosure
2.2
Circuitboard 
2.2
Housing 2.1
Enclosure
2.1
Circuitboard 
2.1
Housing 1.1
Circuitboard 
1.1
Display1.2
Circuitboard 
1.2
Housing 1.2
Products
Components
=already
carryover
=carryover
candidate
…
Display 1.1
Sealings
Battery 1.2
=variant
Sealings
Battery 2.2
Frontcover 
1.1

 
Developing Modular Prod
5.2 
Development of Mo
There are different strategi
activities can be subdivide
helps to allocate and name
activities that raise product
might be high potential fo
recommended that a classi
(Fig. 11, left). Other produ
parts, components and mod
configurable modular syste
across product families (Fig
tial for both directions of c
middle). During Product P
(Fig. 10) which gives direc
uct program shows best pot
choice of product structure
within and/or across produ
supported by Design for Va
 
Fig. 11. Product
6 
Summary and P
Reducing internal variety is
ly, particularly product dev
Integrated PKT-Approach 
from literature research an
experience and creativity of
to foster discussion and exc
the method units into w
duct Families with Perspectives for the Product Program 
odular Product Programs 
ies how to develop a modular product program. Initia
ed into activities within and across product families. T
e different activities. These activities mean product des
t commonality. Depending on the product program, th
or product family internal commonality. In this case, i
ical product family oriented platform strategy is follow
uct programs might have high potential for carryover
dules across product families. They should be designed a
em of smaller modules with a strong focus on carryo
g. 11, right). A lot of product programs might show pot
ommonality – within and across product families (Fig. 
Program Planning this potential is analyzed using the C
ct indication for which product structure strategy the pr
tential to internal variety, see the indicator in Fig. 11. A
e strategy the design tasks can be performed accordin
uct families. These design tasks are performed as proje
ariety and Life Phase Modularization, as described abov
t structure strategies and indication of potential [15] 
Prospects 
s a challenge that touches all life phases of a product fa
velopment. To give a support in product development 
for Developing Modular Product Families was deri
nd industrial case studies. The branch-specific knowled
f a company’s engineers are integrated using graphical to
change of concepts. This is also supported by incorporat
orkshop-based procedures, focusing on interdisciplin
551 
ally, 
This 
sign 
here 
it is 
wed 
r of 
as a 
over 
ten-
11, 
CAP 
rod-
After 
ngly 
ects 
ve.  
 
ami-
the 
ived 
dge, 
ools 
ting 
nary  

552 
D. Krause, S. Eilmus, and H. Jonas 
exchange within the company. In 7 case studies, a reduction in components of about 
52% on average was achieved by applying the method units Design for Variety and Life 
Phases Modularization. Even more significant is the reduction from 85% to 48% in the 
share of variant components achieved. Method units for Product Program Planning and 
Development enable a market-oriented product program planning as well as choice of 
product structure strategy according to the specific corporate carryover potential. By 
this product family development is embedded within a holistic corporate strategy. In 
future the method units Product Program Planning and Development of Modular 
Product Programs will be evaluated through industrial cases. 
References 
1. Krause, D., Eilmus, S.: Methodical Support for the Development of Modular Product Fam-
ilies. In: Birkhofer, H. (ed.) The Future of Design Methodology, pp. 35–45. Springer, Ber-
lin (2011) 
2. Pimmler, T., Eppinger, S.: Integration Analysis of Product Decompositions. In: Proceed-
ings of the 6th Design Theory and Methodology Conference, New York, pp. 343–351 
(1994) 
3. Stone, R.B.: Towards a Theory of Modular Design, Austin, Texas (1997) 
4. Erixon, G.: Modular function deployment: a method for product modularisation. The Roy-
al Inst. of Technology, Stockholm (1998) 
5. Martin, M., Ishii, K.: Design for variety: developing standardized and modularized product 
platform architectures. Res. Eng. Design (13), 213–235 (2002) 
6. Du, X., Jiao, J., Tseng, M.M.: Architecture of Product Family: Fundamentals and Metho-
dology. Concurrent Engineering 9(4), 309–325 (2001) 
7. Eppinger, S.D., Browning, T.R.: Design structure matrix methods and applications. MIT 
Press, Cambridge (2012) 
8. Lindemann, U., Maurer, M., Braun, T.: Structural Complexity Management - An Ap-
proach for the Field of Product Design. Springer, Berlin (2009) 
9. Brosch, M., et al.: Design for Value Chain – An Integration of Value Chain Requirements 
into the Product Development Process. In: Proceedings of Norddesign 2012, Aalborg 
(2012) 
10. Blees, C., Kipp, T., et al.: Development of Modular Product Families: Integration of De-
sign for Variety and Modularization. In: Proceedings of Norddesign 2010, Gothenburg, pp. 
159–168 (2010) 
11. Kipp, T., Blees, C., Krause, D.: Anwendung einer integrierten Methode zur Entwicklung 
modularer Produktfamilien. In: Design for X. Beiträge zum 21. DfX-Symposium, pp. 157–
168. TuTech Verl., Hamburg (2010) 
12. Blees, C.: Eine Methode zur Entwicklung modularer Produktfamilien, 1st edn. TuTech-
Verl., Hamburg (2011) 
13. Eilmus, S., et al.: Evaluating a methodical approach for developing modular product fami-
lies in industrial case studies. In: Dorian, M., Mario, S., Neven, P., Nenad, B. (eds.) Pro-
ceedings of DESIGN 2012, Zagreb, pp. 959–968 (2012) 
14. Jonas, H., Gebhardt, N., Krause, D.: Towards a strategic development of Modular Product 
Programs. In: Dorian, M., Mario, S., Neven, P., Nenad, B. (eds.) Proceedings of DESIGN 
2012, Zagreb, pp. 959–968 (2012) 
15. Eilmus, S., Krause, D.: An Approach for reducing Variety across Product Families. In: 
Proceedings of Norddesign 2012, Aalborg (2012) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 553–562. 
DOI: 10.1007/978-3-642-30817-8_54 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Modular Dynamic Products Platforms Design Model  
Mohmmad Hanafy and Hoda ElMaraghy 
Intelligent Manufacturing Systems Center, 
Industrial and Manufacturing Systems Engineering, University of Windsor,  
Windsor, ON N9B 3P4, Canada 
{hanafym,hae}@uwindsor.ca 
Abstract. Mass customization is an effective means to economically produce 
the wide product variety that the market now demands. The product platform 
strategy has emerged as an important enabler, where common components used 
across different product variants are maximized to decrease design and manu-
facturing cost. A model for designing optimal products platforms was devel-
oped. It determines the best number, combinations, and composition of modular 
product platforms. It defines the optimum hierarchy of relations between the 
platform components and enables delayed product differentiation to achieve 
economy of scope. Unlike designing product platforms using common compo-
nents, the new model enables customization of the platforms themselves to suit 
the dynamically changing and evolving product families. It uses a mathematical 
nonlinear and linear mixed integer programming approach. A case study of a 
family of touch screen computer tablets is used to illustrate the application and 
advantages of the newly developed dynamic product platform design model. 
Keywords: Products Platforms Design, Modular, Dynamic. 
1 
Introduction 
Products variety is increasing in response to customers’ demands for new features and 
functionalities. This is in addition to fluctuating market conditions of supply and  
demands. Therefore, effective concepts and methodologies are needed to enable Mass 
Customization while keeping cost down and quality high in order to remain  
competitive.  
The product architecture concept and its impact on industrial corporations were 
discussed [1]. A clear differentiation between Modular and integral product architec-
tures were delineated and categories of modularity (such as slots, bus, and sectional 
types) were identified.  
2 
Literature Review 
In the Product Platform Formation, the research field includes, but is not limited to, 
commonality measures and indices and optimization using different criteria (e.g.  
maximum commonality and minimum functionality loss). Platforms have been  

554 
M. Hanafy and H. ElMaraghy 
 
categorized depending upon modularity, scalability and functionality. defined Product 
Platform as a: set of subsystems and interfaces that form a common structure, from 
which a stream of derivative products can be efficiently produced and developed [2]. 
Another paper discussed the main concept of product platforms, but called it Product 
Families Architecture [3]. The authors clearly defined the concept of “Modular Prod-
uct Platforms”, which can produce different products by varying some modular  
components. described another platform type they called Scalable Platform [4]. A 
Compromise Design Support Problem Formulation was proposed to design a product 
family of universal electric motors by varying some scalable parameters. This method 
can provide motors with very similar structures but with variable performances.  
Design Structure Matrix (DSM) method was used with a partitioning and genetic 
algorithms; to produce clear common platforms for complex products and groups 
interactions and applied it to Gas turbines, 22 cross-functional teams’ communication 
in GM corporation [5].  A multi agent model was developed to solve the platform 
configuration problem, where the functional model of the product was the primary 
concern and the hierarchy and the precedence of the components were ignored [6]. 
This model does not handle large product families.  
A non-linear mathematical formulation dealt with the multiple platform configura-
tions problem was proposed; to configure single and multiple platforms by adding or 
removing components to the platform; to form the final product but unfortunately 
their results could not be replicated [7]. This model needs the number of platforms to 
be defined a priori and yields negative costs when the maximum number of platforms 
is reached. The model includes number of plane cuts which neither improve the solu-
tion time nor guarantees optimality.  In that paper, this model will be completely re-
formulated to avoid all previous drawbacks. Then the concept of platform hierarchy 
will also be proposed to assist in the assembly of duplicate components that have 
multiple variants and/or different assembly positions. 
3 
Modular Products Dynamic Platforms Formation  
Many products have a modular nature such as power tools, toasters, microwaves, 
computer tablets, and computers consist of numerous components and modules.  
Multiple products with different structure and specifications are produced by inter-
changing these modules and components to target different market segments.  
The proposed model aims at combining different concepts into one holistic model. 
The concepts include: changeable product families  [8], changeable product platforms 
[9], delayed product differentiation, simultaneous assembly and disassembly 
processes, and the newly proposed platform hierarchy concept.  It employs an innova-
tive concept of a “Changeable modular platform configuration” where some modules 
may be removed or added after initial assembly to maximize responsiveness to chang-
ing customers and market demands. The Dynamic Products Platforms Design Model 
(DPPD) is mathematically described next.  

 
A Modular Dynamic Products Platforms Design Model 
555 
 
3.1 
The Mathematical Model 
As shown in Figure 1, the DPPD model describes many criteria which are considered 
crucial in the platform concept including the most recent criterion related to the  
hierarchy of the obtained platforms. Most platform design models in the literature 
discuss the product platform as a group of individual components without clearly 
defining the relations between the components of the platforms.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. IDEF0 of the Dynamic Products Platforms Design  
The clear definition of the relations between platforms components is very essen-
tial especially when one component can be assembled in different places (i.e. bolts, 
nuts and specially designed parts). The objective function of the proposed model has 
four main terms: 
• The first term describes the cost of the components and their mass production  
assembly. 
•  The second term describes the cost of adding components needed to obtain a new 
product in a manual station or stations that are not dedicated for mass production. 
•  The third term describes the cost of removing components needed to obtain a new 
product in a manual station or stations that are not dedicated for mass production. 
• The fourth term is used to add cost for each platform if the platform is used (i.e. 
costs associated mainly with training workers on new assembly processes/ 
products) 
The objective function (c) is the total cost of the above four cost items.  
DPPD Model 
Hierarchy (i.e. Prece-
dence of platforms  
components) 
     Precedence of Components 
Maximum number of platforms (Control) 
Optimal Number of Platforms 
Optimum numbers and 
members of Product  
Families
     Components of each product 
Demand of each product 
Costs of mass production  
Optimization Analysis and Mathematical  
Modelling 

556 
M. Hanafy and H. ElMaraghy 
 
Assume that: 
  cost୮: Cost of assembling single component j into platform 
  cୢ:  Cost of component d 
  cୟୢ : Labor cost of the assembly of a component d to a platform in station that  
          does other operations 
  c୰ୢ: Labor cost of disassembly of a component d from a platform in station that   
        does other operations. 
  c  : Cost of labor training to assemble a certain platform. 
 M: large integer number 
 
  v୨୩= ൜1 ,                                    if product k contains component j 
0,                                     otherwise                                              
 
 P୩୨ୢ= ൜1 ,                                   if component j precedes component d in product k 
0,                                     otherwise                                                                                
 
The decision variables are: 
 
  y୧୩   = ൜1 ,                                  if product k is clustered to platform i    
0,                                      otherwise                                                     
 a୧୨ୢ୩ = ൝
1 ,                                 if component d is assembled after component j         
 in platform i to form product k
0,                                    otherwise                                                                                
 
 
  r୧୨ୢ୩ = ൝
1 ,                                 if component d is disassembled from component j
 in platform i to form product k
0,                                   otherwise                                                                           
  
 
    p୧   = ൜1,                                   if platform i exists                              
0,                                  otherwise                                              
  
  s୧୨ୢ= ൜1 ,                   if component d is assembled after component j in platform i 
0,                    otherwise                                                                                                 
 
The objective function is given by: 
 
Min Cost c = 
∑
∑
∑
∑
൫cost୮+ cୢ൯s୧୨ୢ
୬
୩ୀଵ
୫
ୢୀଵ
୫
୨ୀଵ
୪
୧ୀଵ
y୧୩d୩+ ∑
∑
∑
∑
ሺcୟୢ+
୬
୩ୀଵ
୫
ୢୀଵ
୫
୨ୀଵ
୪
୧ୀଵ
cୢሻ a୧୨ୢ୩y୧୩d୩+ ∑
∑
∑
∑
ሺc୰ୢሻ r୧୨ୢ୩y୧୩d୩
୬
୩ୀଵ
୫
ୢୀଵ
୫
୨ୀଵ
୪
୧ୀଵ
+ ∑
cp୧
୪
୧ୀଵ
                             (1)           
 
The objective function obtained is nonlinear. This will significantly increase the time 
needed to solve real products and platforms with large number of components and 
products. Therefore, a basic linearization scheme is adopted from [10]. Each two bi-
nary variables are substituted by one variable (shown below) and two constraints 
(shown within the model itself) as follows: 

 
A Modular Dynamic Products Platforms Design Model 
557 
 
                                               zx୧୨ୢ୩= s୧୨ୢy୧୩                                                   (2) 
 
                                             zy୧୨ୢ୩= a୧୨ୢ୩y୧ୢ                                                  (3) 
 
                                             zz୧୨ୢ୩= r୧୨ୢ୩y୧ୢ                                                    (4) 
 
Hence, the objective function will be: 
 
Min   Cost c = 
∑
∑
∑
∑
൫cost୮+ cୢ൯zx୧୨ୢ୩
୬
୩ୀଵ
୫
ୢୀଵ
୫
୨ୀଵ
୪
୧ୀଵ
d୩+ ∑
∑
∑
∑
ሺcୟୢ+
୬
୩ୀଵ
୫
ୢୀଵ
୫
୨ୀଵ
୪
୧ୀଵ
cୢሻ zy୧୨ୢ୩d୩+ ∑
∑
∑
∑
ሺc୰ୢሻzz୧୨ୢ୩d୩
୬
୩ୀଵ
୫
ୢୀଵ
୫
୨ୀଵ
୪
୧ୀଵ
+ ∑
cp୧
୪
୧ୀଵ
                                    (5)                   
 
 
Subject to: 
                                        Ms୧୨ୢ൒zx୧୨ୢ୩                                                           (6)  
                                   zx୧୨ୢ୩൒y୧୩+ Mሺs୧୨ୢെ1ሻ                                             (7) 
                                   Ma୧୨ୢ୩൒zy୧୨ୢ୩                                                               (8) 
                                    zy୧୨ୢ୩൒y୧୩+ Mሺa୧୨ୢ୩െ1ሻ                                         (9) 
                                   Mr୧୨ୢ୩൒zz୧୨ୢ୩                                                                 (10) 
                                   zz୧୨ୢ୩൒y୧୩+ Mሺr୧୨ୢ୩െ1ሻ                                            (11) 
                                   ∑
y୧୩= 1  , k = 1, … , n  
୪
୧ୀଵ
                                             (12) 
                              a୧୨ୢ୩+ Mሺ3 െv୨୩െvୢ୩െP୩୨ୢ+ s୧୨ୢሻ൒y୧୩                      (13) 
                                    a୧୨ୢ୩൑My୧୩                                                                    (14) 
                                      r୧୨ୢ୩+ Mሺ1 െs୧୨ୢ+ P୩୨ୢሻ൒y୧୩                                   (15) 
                                ∑
y୧୩
୬
୩ୀଵ
൒p୧, i = 1, … , l                                                    (16) 
                                ∑
y୧୩
୬
୩ୀଵ
൑Mp୧, i = 1, … , l                                                 (17)                   
                                 ∑
s୧୨ୢ
୫
ୢୀଵ
൑2, i = 1, … , l                                                   (18) 
                                     ∑
y୧୩
୬
୩ୀଵ
൒s୧୨ୢ                                                                (19) 
               zx୧୨ୢ୩, zy୧୨ୢ୩, zz୧୨ୢ୩, p୧, s୧୨ୢ, y୧୩, a୧୨ୢ୩, r୧୨ୢ୩= ሼ0,1ሽ                           (20) 
 
Constraints 6 to 11 are necessary to linearize the three quantities (s୧୨ୢy୧୩, a୧୨ୢ୩y୧ୢ , 
r୧୨ୢ୩y୧ୢ). The set of constraints (12) ensures that every product belongs to only one 
family and one platform. Constraints set (13) assembles new component d to compo-
nent j in a certain platform i to form product k, if components d and j belong to that 
product, and j precedes d in the original product precedence matrix, and not in the 
obtained platform. Constraints set (14) prevents assembling any component to a plat-
form i,  if the product k is not in platform i. Constraint (15) removes component d 
from component j in platform I to form product k, if the product is in the family 
served by that platform, and component d is not in product k. The existence of a  
 
 

558 
M. Hanafy and H. ElMaraghy 
 
certain platform and, hence, addition of its associated fixed costs is determined by 
constraints (16-17). Constraint set (18) is component and product dependent, where 
the constraint determines the maximum number of components that can co-exist (as-
sembled) to a parent component. Constraint set (19) ensures that if a platform does 
not serve any product family then all its components should vanish. Finally, constraint 
set (20) forces all variables to be binary.  
The newly introduced platform hierarchy concept in this paper needs a modifica-
tion in the structure and hierarchy of data set of the products.  Hence, in the proposed 
dynamic platform model, a new dummy component with zero cost is added to each 
product to enable the model to account for the first component in any given product 
(e.g. steel mid frame components in the following tablet’s case study). 
4 
Case Study 
A case study of touch screen tablets product family consisting of six product variants 
is used to demonstrate the developed dynamic platform formulation model. The tab-
lets have different structures and different components, hence different precedence 
constraints as shown in Figure 2. The mathematical model was programmed using 
AMPL language and solved using CPLEX 11.2.1 solver. The inputs to the model 
were as follows: 
 
- 
cost୮ = $2.5 
- 
cୟୢ = $4.25 
- 
c୰ୢ = $4.25 
- 
c  = $900 
- 
Total demand (number of tablets to be produced) for the six products respectively 
= [9000 700 8000 400 9000 500] 
- 
Maximum number of platforms = 1 and 6 (two scenarios were analyzed) 
 
Figure 2 shows the precedence sequence for assembling the different components of 
the touch screen tablet family. The motherboard used has three different variations; 
the speaker is designed so that it has two different possible assembly locations: either 
after the assembly of the battery, or just after the power button. The dummy compo-
nent is an imaginary component used to decrease number of terms and their complexi-
ty in the objective function. Table 1 enumerates the components used in each tablet, 
and their costs as well. 
4.1 
Discussion 
The model was solved for single and multiple assembly lines. In the single assembly 
line, the product platform structure contains five main components: steel mid frame, 
power button, display, front panel assembly, and the speaker assembly in position 1. 
 
 

 
A Modular Dynamic Products Platforms Design Model 
559 
 
 
 
 
 
            
 
 
 
Fig. 2. General precedence diagram of the tablets family 
In Table 2, the model described clearly the position of the speaker assembly – 
since it had two positions – and decided to attach it to the power button. This is suita-
ble for products 1, 3, 5. For products 2, 4, 6, the speaker assembly will be removed 
from its position and added after the assembly of the battery. Then, according to the 
composition of each product, the components will be added as the product platform 
advances through the assembly line. The next step is to allow the maximum number 
 
Steel 
Mid 
Frame 
(1)
Display (12)
Front Panel 
Assembly (8)
Power 
Button 
(5)
Motherboard 
(9,10,11)
Touchscreen 
Controller 
(4)
Battery 
(2,3)
Speaker 
Assembly 
(position 
1) (6)
Back 
Cover 
(13)
Speaker Assembly 
(position 2) (7)
Front Panel Assembly 
Back Cover 
Steel mid 
frame 
Speakers 
Display 
Mother Board 
Power Button 
Battery 

560 
M. Hanafy and H. ElMaraghy 
 
of components in the platform to be the total number of product variants. This  
increases the model flexibility in assigning the products to more platforms and  
assembly lines, to minimize the cost of products components, assembly, and disas-
sembly. The total cost dropped by 2.89 % as a result. In that case, the model found 
that assigning each product to a separate assembly line is more economic than creat-
ing one unified platform for all of them. 
This would be expected if the cost of initiating the assembly line is small compared 
to the expected cost of the components, mass produced platforms, and their combined 
assembly and disassembly operations. It is worth noting that by changing any input 
factor (e.g. product demand, assemblies’ precedencies, and costs), all product plat-
forms and product families change accordingly and evolve flexibly and easily. The 
obtained product platforms possess more shared components across the tablet family; 
this is the key to delayed product differentiation. Use of the model decreased the 
number of differentiating elements in the mass production phase, so the delayed diffe-
rentiation process has become more efficient.  The computation time using 3.12 Ghz 
Xeon Processor and 4 GB RAM computer was less than 1 second for the one plat-
form, and less than 5 seconds for the 6 platforms, which proves the efficiency of the 
model and its potential ability to handle more complex products.  
Table 1. Costs and composition of each tablet 
 
 
Cost($) 
P1 
P2 
P3 
P4 
P5 
P6 
1 
Steel mid frame 
4 
x 
x 
x 
x 
x 
x 
2 
Battery 1 (4400 mAh) 
16.5 
x 
 
 
x 
 
x 
3 
Battery 2 (8000 mAh) 
30 
 
x 
x 
 
x 
 
4 
Touchscreen controller  
3 
x 
x 
x 
x 
x 
x 
5 
Power Button 
2.5 
x 
x 
x 
x 
x 
x 
6 
Speaker Assembly Posi-
tion 1 
10 
 
x 
 
x 
 
x 
7 
Speaker Assembly Posi-
tion 2 
10 
x 
 
x 
 
x 
 
8 
Front Panel Assembly 
42 
x 
x 
x 
x 
x 
x 
9 
Mother Board 1 (default) 
64.5 
x 
 
 
x 
x 
 
10 
Mother Board 2 (High 
memory) 
80 
 
x 
 
 
 
x 
11 
Mother Board 3 (High 
storage capacity) 
90 
 
 
x 
 
 
 
12 
Display 
45 
x 
x 
x 
x 
x 
x 
13 
Back Cover 
6 
x 
x 
x 
x 
x 
x 
 
 

 
 
Table 2
 
5 
Conclusions and
Innovative and efficient pro
for enabling mass produ
introduced to merge many d
form hierarchy concept for
both the addition and remov
ket demands. The model u
most product variants, whic
It defines the optimum pos
The merger of different 
demands, costs, platform h
hances the efficiency of del
forming product platform
different performance crite
explored.  
One Platform (i.e. one as
Platform Hierarchy for all
 
Dummy
5. Power 
Button
7. Speaker 
Assembly 
Position 2
1
1. Steel mid 
frame
A Modular Dynamic Products Platforms Design Model 
2. One and multiple platform configuration 
d Future Work 
oduct platform design and assembly strategies are essen
uct customization. A new mathematical model w
different product platform design aspects using a new p
r Dynamic Products Platforms Design (DPPD). It allo
val of components / modules to satisfy customers and m
utilizes the principle of assembling components shared
ch can be disassembled later to produce different varia
ition of each component in the dynamic product platfo
aspects of the product platform design (precedenc
hierarchy and combined assembly and disassembly) 
layed product differentiation and increase the flexibility
ms from assembly costs perspective. For future wo
eria (e.g. maintainability, torques, speeds... etc.) would
ssembly line) 
Cost ($) 
Maximum 
allowable 
Platforms 
(i.e. 6 
platforms) 
Cost ($) 
l products 
 
6459430 
Each 
product has 
its 
inde-
pendent 
platform 
(i.e. 
the 
product 
itself) and 
separate 
line 
of 
assembly. 
6272630 
12. Display
8. Front Panel 
Assembly 
561 
ntial 
was  
plat-
ows 
mar-
d by 
ants. 
orm. 
cies,  
en-
y of 
ork,  
d be 

562 
M. Hanafy and H. ElMaraghy 
 
References 
1. Ulrich, K.: Role of product architecture in the manufacturing firm. Research Policy 24(3), 
419–419 (1995) 
2. Meyer, M., Lehnerd, A.P.: The power of product platform – building value and cost lead-
ship. Free Press, New York (1997) 
3. Erens, F., Verhulst, K.: Architectures for product families. Computers in Industry 33(2-3), 
165–178 (1997) 
4. Simpson, T.W., Maier, J.R., Mistree, F.: Product platform design: Method and application. 
Research in Engineering Design - Theory, Applications, and Concurrent Engineer-
ing 13(1), 2–22 (2001) 
5. Tian-Li, Y., Yassine, A.A., Goldberg, D.E.: An information theoretic method for develop-
ing modular architectures using genetic algorithms. Research in Engineering Design 18(2), 
91–109 (2007) 
6. Park, J., Simpson, T., Moon, S.K., Kumara, S.R.T.: A dynamic multiagent system based 
on a negotiation mechanism for product family design. IEEE Transactions on Automation 
Science and Engineering 5(2), 234–244 (2008) 
7. Ben-Arieh, D., Easton, T., Choubey, A.M.: Solving the multiple platforms configuration 
problem. International Journal of Production Research 47(7), 1969–1988 (2009) 
8. ElMaraghy, H.: Reconfigurable Process Plans For Responsive Manufacturing Systems 
Digital Enterprise Technology. In: Cunha, P., Maropoulos, P. (eds.), pp. 35–44. Springer 
US (2007) 
9. ElMaraghy, H.A.: Changeable and Reconfigurable Manufacturing Systems. Springer, 
London (2009) 
10. Peterson, C.C.: A note on transforming the product of variables to linear form in linear 
programs (1971) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 563–572. 
DOI: 10.1007/978-3-642-30817-8_55     © Springer-Verlag Berlin Heidelberg 2013 
Design Automation with the Characteristics Properties 
Model and Property Driven Design for Redesign 
A.J. Qureshi1, Boris Eisenbart1, Jean-Yves Dantan2, and Lucienne Blessing1 
1 Research Unit in Engineering Sciences, Luxemboug University, L-1359, Luxembourg 
2 LCFC, Arts et Métiers ParisTech, 4 Rue Augustin Fresnel, Metz, 57078, France 
{ahmed.qureshi,boris.eisenbart,lucienne.blessing}@uni.lu,  
jean-yves.dantan@ensam.eu 
Abstract. This paper presents a framework consisting of a mathematical model 
and an algorithm for representation, analysis and exploration of the design 
space in redesign problems. The framework develops and extends the existing 
formalism of the Characteristics Properties Model (CPM) and Property Driven 
Design (PDD). A platform independent quantitative model based on formal log-
ic is presented to map the characteristics and properties, as well as the relations 
and dependencies between them, along with the necessary conditions for solu-
tion evaluation. The model is based on generalization of existing mathematical 
design models and is supported by the development of an algorithm enabling 
property driven design. The resulting framework offers a rich and flexible syn-
tax and vocabulary along with a mathematical and computational tool applica-
ble to mechanical product redesign.  
Keywords: Redesign, first order logic, design automation, simulation based de-
sign, characteristics properties model, property driven design. 
1 
Redesign and Adaptive Design 
Most design problems encountered in industry are related to redesign or adaptation of 
an existing product design [1]. Redesign, and adaptive design are common and widely 
practiced design tasks, wherein – starting from an existing solution – the designer 
creates a product to meet new requirement, needs and constraints [1]. The resulting 
product may be adapted for different requirements, or be an improved product version 
for the existing requirement. Redesign is also used for different versions of the prod-
uct to address different segments in the market. With advances in the manufacturing 
systems, shift from mass production to customized production, and global markets 
with specific demands, adaptive design and redesign are increasingly practiced. This 
allows the company to better adapt its product with regard to performance, functional-
ity, economic constraints or ethnographic preference, as required in a new market. 
While most of the literature in product design addresses methodologies and me-
thods for original design, the most common design activity remains to be redesign or 
adaptive design. The model presented in this article addresses these design problems 

564 
A.J. Qureshi et al. 
and assumes that earlier information about the product is readily available in form of 
simulation based non-physical models and the solution principle and design space is 
well understood from previous experiences in similar projects.   
CPM/PDD is conceived with goals of consolidation of the knowledge from design 
theory and methodologies; integration of existing models and strategies into a com-
mon framework; facilitating the integration of simulation based design and design 
automation in everyday design activities of practitioners [2]. The resulting ease of 
formalization of CPM/PDD due to consideration of these elements lends itself to the 
possibility of adapting it for the application in redesign problems.  
The degree of abstraction and detail in redesign tasks is significantly different from 
other design tasks. Redesign is characterized by a high level of concreteness exhibited 
by detailed models with quantifiable characteristics resulting from the existing prod-
uct documentation or design experience. In a redesign task information from earlier 
design project(s) is available, providing an idea of the design space already explored, 
which may be used to enhance the search for alternative solutions. The CPM/PDD 
framework can be developed and extended to provide a formalized support for design 
simulation and automation for such redesign problems. The following sections in the 
paper respectively provide an introduction to CPM/PDD model, a systematic, mathe-
matical and logical CPM based model for modeling redesign and an algorithmic  
iterative method for carrying out design space search for the solutions to a redesign 
problem at hand. Further, an illustrative example is presented. 
2 
Characteristics Property Model and Property Driven Design  
The Characteristics-Properties Modeling (CPM), respectively Property-Driven De-
velopment (PDD) approach is a rather recently developed approach to systematic 
product development [2, 3]. Therein, CPM comprises the product model, while PDD 
embodies the systematic scheme for the process of product development. The ap-
proach specifically aims at integrating knowledge about design from well-established 
design theory as well as existing modeling approaches and techniques into a common 
framework. Formalization is endorsed, so as to ease implementation in computer- 
based tools.  
The fundamental characteristic of the approach is the clear separation between 
characteristics of a product and its properties: 
• Characteristics (Ci) “describe the shape and the structure of a product (e.g. geo-
metry, BOM, materials etc.) and can be directly established, assigned and modified 
by the designer”; 
• Properties (Pj) “describe the current behavior of a product (e.g. weight, manufac-
turability, function, cost, user friendliness etc.) and cannot be directly established 
by the designer; they can only be indirectly influenced by changing the depending 
characteristics” [4]. They are the indicator of the actual performance of the prod-
uct, resulting from a given set of characteristics. 

 
Design Automation 
• Required Properties (P
designed artifact. Requi
while considering a custo
Product development strive
established product proper
(PRj), i.e. the difference Δ
driving the development pro
Relations (Rj) relate the 
cal behavior and tangible
physical objects (models, m
physical model (mathemati
be differentiated in: 
 
• Analysis: Based on a set
the respective embodied
riments, simulation, calc
• Synthesis: Based on a se
specific characteristics a
use calculation, tables, c
ic characteristics, to achi
In Figure 1 these two bas
interdependencies between 
to be respected during the a
ble solution space. 
 
  
a) 
Fig. 1. Model of t
 
with the CPM and Property Driven Design for Redesign 
PRj) describe the properties that have to be fulfilled by 
ired properties are the reference values which are fi
omer’s preferences. 
es to define a set of product characteristics such that 
rties are sufficiently close to a set of required proper
ΔPj = RPj - Pj  0. Thus, minimization of ΔPj is in f
ocess. 
characteristics to the properties through the laws of phy
e/intangible principles. Relations may be deduced fr
mockups, and prototypes) or they may be made in a n
cal, numerical, computer-based, graphical, etc.). They m
t of known or given characteristics (structural paramete
d properties are analytically determined through e.g. ex
culations etc. 
et of given (Pj) or required properties (RPj) the produ
nd corresponding values are established. The designer m
catalogues, experience etc. in order to determine the spe
ieve the desired properties.  
ic relations are modeled. Dependencies (Dx) address 
individual characteristics. External conditions (ECj) n
analysis and synthesis, as they directly constrain the ava
 
b) 
the central analysis a) and synthesis b) steps, after [2] 
565 
the 
ixed 
the 
rties 
fact 
ysi-
rom  
non-
may 
ers) 
xpe-
uct’s 
may 
ecif-
the 
need 
aila-
 

566 
A.J. Qureshi et al. 
The starting point of PDD (i.e. application of the described CPM theory) is a com-
prehensive requirements list, based on which the required properties (RPj) are de-
rived. The product characteristics and respective values – embodying the product 
properties – are gradually established, in order to meet the required properties. Essen-
tially, this is facilitated through synthesis steps. Synthesis is thus regarded to be the 
essential activity during product development. However, in PDD, synthesis is fre-
quently accompanied by analysis and evaluative steps to give more and more precise 
information about the conceived product’s properties (function, behavior etc.). 
3 
Mathematical Characteristics Properties Model for Redesign 
Different mathematical models exist in literature on modeling in design. Often these 
models describe a specific task with a specialized context and application and provide 
no link to a general framework in the design theory and methodology. Yvars [5] mod-
els the parametric design activity during detail design as a constraint satisfaction 
problem in the form of ሼܸ, ܦ, ܥሽ. Modeling of the parameter design and robust design 
activities has been proposed by different authors [5–8]. Dantan et al. [9] propose a 
model which enables specification, modeling and analysis of tolerancing activities 
using a model based on quantifiers (“there exists, ׌"  and “for all, ׊ "  ). 
A generalization of the above models (discussed in the previous paragraph) is poss-
ible by extending the definition of the quantifiers as given by Dantan et al [9]. An 
extended design model for redesign problems using the available syntax of formal 
logic can be formulated and applied to CPM. The notion of predicates and conditional 
evaluation available in formal logic can be extended to model relationships between 
different parts of the CPM as described in the preceding section. The following sec-
tion describes the model in detail with relevance to the parameter design of mechani-
cal systems.  
3.1 
Changes to CPM Nomenclature 
To facilitate the mathematical expression, the nomenclature of the CPM as described 
in [2, 3] is modified as follows. The mathematical representation of the individual 
characteristics  ܥ௜, properties  ܲ௝, relations  ܴ௞, dependencies  ܦ௫ and external 
conditions ܧܥ௡ are henceforth used in italics instead of upper case letter. The upper 
case denominations will henceforth be utilized for sets of individual terms as 
described below.   
3.2 
Mathematical Model 
The following text describes the general structure of the mathematical model of the 
characteristics properties model (mCPM). General definitions of the terms ‘characte-
ristics’, ‘properties’, ‘required properties’, ‘relations’ have been described in the pre-
vious section. The following text defines these terms mathematically. 

 
Design Automation with the CPM and Property Driven Design for Redesign 
567 
Characteristics 
The state of information in the product design in the later design state is in a degree of 
maturity that it can be represented to a large extent through quantitative data sets. The 
characteristics ܿ௜ belong to the set of the characteristics of the model ࡯. Characteris-
tics may be quantitatively represented in form of dependent and independent  
variables. 
The characteristics are defined as: 
 
࡯= ሼܿଵ, ܿଶ, … , ܿ௜ሽ 
(1) 
Each characteristic may be assigned one or a set of values which may be categorized 
into symbolic, numeric, discrete, or continuous type. From a viewpoint of definition, 
they may be categorized into symbolic or numeric, whereas from the viewpoint of 
continuity, these values may be categorized into discrete or continuous variables. 
Depending upon the nature of their value(s), a characteristic may be represented by a 
set or a tuple. 
Properties 
The properties ݌௝ belong to the set of the properties ࡼ. The set of properties is  
defined as: 
 
ࡼ= ൛݌ଵ, ݌ଶ, … , ݌௝ൟ  
(2) 
Required Properties 
The required properties are the properties that are to be achieved in order for the de-
sign process to succeed. Required properties ܴܲ௝ belong to the set of the required 
properties ࡾࡼ which is defined as: 
 
ࡼࡾ= ൛݌ݎଵ, ݌ݎଶ, … , ݌ݎ௝ൟ 
(3) 
The type of values of each individual ݎ݌௜ is same as the type of corresponding ݌௜. 
Design Space 
The characteristics are the variables to which values must be assigned. These 
assignments are taken from the design space. For redesign, this denotes the n-
dimensional space in which the solution is to be searched. Parts of design space have 
been explored during the earlier design activity for similar products. It is not entirely 
known but a partial understanding of the space is present from existing solutions 
corresponding to their specific required properties. The design space is defined as: 
 
ࡰ= ൫݀௖భ, ݀௖మ, … , ݀௖೔൯ 
(4) 
 
ܿ௜א ݀௖೔: ܿ௜௠௜௡൑݀௖೔ ൒ܿ௜௠௔௫  
(5)      
where  ܦ is the starting domain for the design problem. It contains the existing solu-
tions and the unexplored space that a designer intends to explore for possible  
solutions corresponding to the required properties. 

568 
A.J. Qureshi et al. 
Relations 
Relations provide the fundamental quantitative boundaries to constrain the design 
space for the expected solution. A relation or a set of relations may be required to 
model a specific property. In case of non-physical models, the whole set of the 
relations forms the analytical model of the product which can then be evaluated for 
properties in the presence of the given required properties. From a mathematical point 
of view, the relations are a function of characteristics and properties. In order to 
facilitate design automation or simulation, they may include required properties in 
form of an equality or non-equality. They are represented as a set of relations ࡾ with 
individual relations ݎ௜: 
 
ࡾ= ൛ݎଵ,ݎଶ… ݎ௞ൟ  
(6) 
The relations may be of any of the following general types:  
• Discrete i.e. of a catalog type, where choice between two or more discrete entities 
of numeric or symbolic nature is required; 
• Continuous, where all the variables involved are continuous real numbers; 
• Mixed, where both the continuous and discrete variables are present;  
• Based on relational database necessitating the use of rule based database selection; 
• Complex relations involving the use of expressions dealing with complex numbers; 
i.e. numbers having real and complex parts; 
• The relations may be of explicit type or of implicit type.  
The general form of relations may therefore be expressed as:  
 
ݎ௜= ݂ሺܥҧ, ݌௜ሻ: ܥҧ ܥ, ݌௜ ܲ, ݂ሺܥҧ, ݌௜ሻ א Թஶڀԧஶ ڀԺஶ 
(7) 
 
ݎ௜= ቄܫܨ…                      ܶܪܧܰ…
ܧܮܵܧܫܨ…           ܶܪܧܰ… 
(8) 
 
ݎ௜= ቐ
݂ሺܥҧ, ݌௜, ݎ݌௜ሻ൑0
݂ሺܥҧ, ݌௜, ݎ݌௜ሻ൒0
݂ሺܥҧ, ݌௜, ݎ݌௜ሻ= 0
 
(9) 
The terms of sets with bar accent ܥҧ denote that a specific relation may contain only a 
partial list of the members of that set. One or many relations may be required to 
model a specific property. 
Dependencies 
Each relation ݎ௜ contains a subset of characteristics ܥҧ from the set ܥ. The characte-
ristics present in a given relation are dependent on each other as defined in a given 
relation. These dependencies may be established between the characteristics in a  
given relation or between the characteristics and the property being modeled by a 
relation. A dependency involving different characteristics may also be present across 
different relations. This is often the case making the design problem a multi-objective 
constraint satisfaction and optimization problem. 

 
Design Automation with the CPM and Property Driven Design for Redesign 
569 
Solutions 
The values of characteristics for which the relations are satisfied are declared to be a 
solution. This necessitates that these values should be from design space and the dif-
ference between  ࡼࡾ and ࡼ should be the minimum acceptable. For a given design 
problem the set of solutions ࡿ may be defined as: 
 
 ࡿ= ൛ݏଵ,ݏଶ… ݏ௟ൟ 
(10) 
 
ݏ௜= ൛ݏ௖భ, ݏ௖మ… , ݏ௖೔ൟ  
(11) 
 
࢙࢏ك ࡰ, ݏ௖೔ك ݀௖௜  
(12) 
Mathematically the condition for a successful solution is described as: 
 
׌ݏ௜|ݏ௜ك ࡰ 
(13) 
 
ࡿ= ൛ڂ
ݏ௜٧ ׌࡯  ࡾሺ࡯, ࡼ, ࡾࡼሻ
௦೔אௌ
ൟ 
(14) 
The above expression lays down the basis of the standard parameter design by verify-
ing that for a solution to be valid, all the characteristics must have valid value(s) from 
design space and that all the relations must be satisfied according to the required 
properties.  
Having defined the necessary mathematical elements for CPM and the conditions 
for a successful solution, the next section describes an algorithm based on PDD for 
rapid visualization of design space for the solutions. 
4 
PDD Based Design Space Search Algorithm 
According to [3] the PDD process consists of four main steps: 
1. Establish the required properties ࡾࡼ and selecting initial set of characteristics ࡯ 
2. Analysis of the properties ࡼ based on the assigned characteristics 
3. Comparison of the calculated properties w.r.t. required properties ࡾࡼ 
4. Reducing the magnitude of difference vector between ࡾࡼ and ࡼ  
Based on these four steps, an algorithm is proposed to apply the CPM/PDD search for 
finding solutions in a design space via rapid visualization. The algorithm has been 
programmed in Mathematica® and is platform independent. It utilizes the mCPM 
model in conjunction with formal logic to search a given design space for a number of 
solutions that satisfy the required properties. 
Fig. 2 illustrates the flowchart of the developed algorithm. The algorithm is divided 
into three main parts (1-3). Part 1 corresponds to the first step in PDD wherein the 
designer specifies the set of  ࡼࡾ to be fulfilled. This is accompanied by linking them 
to the design space and a list of solutions known earlier. This results into the assign-
ment of initial values to the set of characteristics. The algorithm then systematically 
divides the given design space into subspaces for the analysis of the characteristics. 
These steps are carried out via interval arithmetic and mixed integer programming 
and branch and bound technique. 

570 
A.J. Qureshi et al. 
 
Fig. 2. PDD based design space search algorithm 
The second step comprises the step 2 and 3 of PDD. Herein the algorithm analyses 
the subsets of the design space to calculate the properties via specified relations. 
These properties are evaluated against the required properties. The outcome of this 
step is the evaluation if the part of the design space under analysis holds a valid solu-
tion. The evaluation takes place by iterative application of equation 14. This requires 
evaluation of the quantified expressions. As the scope of this paper consists of para-
metric design therefore the algorithm is limited to resolution of existential quantifier 
only. This is resolved via optimization based constraint propagation as minimization 
or maximization of relations with inequalities. 
The results from the step two are used in the step three of algorithm. This step cor-
responds with the last synthesis step of PDD. Results of the design space analysis are 
used to reduce the design space. The space without solutions is disregarded and the 
space with solutions is added to the set of solutions. The algorithm then proceeds to 
select the best direction to search for the properties. This is a directed search which 
prefers the direction according to the minimum distance between the required proper-
ties ࡼࡾ and the evaluated properties ࡼ of the analyzed design space i.e.: 
 
minௗ௜௙௙ሺ݂݂݀݅: ݂݂݀݅= |ࡼࡾെࡼ|ሻ 
(15)  
The results of the algorithm are obtained in form of the explored design space which 
is stored and manipulated in form of an n-dimensional hypercube having dimensions 
equal to the number of the characteristics. For analysis, the parts of the n-dimensional 
hypercube may be projected to a 3 dimensional projection space, allowing the 
designer to visually analyze the solution space for the number of alternative solutions 
available. The alternatives can then be analyzed for the adaption to a certain 
preference via the information available in the hypercube. The algorithm can be 
programmed to run until all the design space is explored or until a given number of 
solutions are found. 

 
Design Automation with the CPM and Property Driven Design for Redesign 
571 
5 
Application 
The developed framework is illustrated by an example problem of a structure design 
adapted from earlier research work (see [10]) as shown in Fig. 3. Contrary to the 
earlier version, the example is made complex by decoupling the dependencies 
between the two beams in terms of a fixed pin joint location and beam dimensions.  
The objective is to find out the values for the dimensional characteristics of two 
beams (length ܮ௜, width ݓ௜, thickness   ݐ௜ : i=1, 2) as well as the positioning of the 
pin joints and materials to support a weight W). The example has 8 characteristics, 5 
properties (system mass, max. stress, max. force, cost, and factor of safety) and 7 
relations linking the characteristics to properties. The relations are non-linear and 
there is a strong dependency between the characteristics. The mathematical model 
contains continuous as well as discrete variables representing different characteristics 
and properties. 
 
 
            
 
Fig. 3. Mechanical structure example with domain visualization 
The white cube in the Figure 4 is the starting design space whereas the green cubes 
correspond to the parametric solutions. Starting from an earlier point based solution, a 
number of alternative solutions are generated that satisfy the required properties and 
allow the designer to choose the suitable solution. 
6 
Conclusions 
CPM/PDD models products and product development processes in a flexible yet 
formalized way which is further developed in this paper for computer aided-design in 
redesign problems. These problems are routine design problems characterized by 
iterative design processes and activities to adapt an existing product to varying 
requirements with intensive CAx involvement. CPM is adapted for providing the 
basic product model and design process. However in order to apply CPM on CAx 
level, a mathematical model and an algorithmic process is required. This paper 
w2
t2
w1
t1
Cross section E-E
Cross section F-F
E
E
F
F
C
W
B
A
D
L1
L3
L2

572 
A.J. Qureshi et al. 
proposes a mathematical model and algorithm that intends to unify the two different 
views of design theory and methodology and computer aided design automation.  
The model and algorithm developed in this paper is based upon a generalization of 
existing mathematical models. It is flexible and may be developed further to 
encompass other considerations such as robust design and uncertainty management.  
The performance of the algorithm for using the model however currently depends 
heavily on the number of characteristics. Due to being a branch and bound method the 
computational complexity of calculation increases and results in high computational 
effort. It is currently being improved through direct integration with heuristic 
techniques. The model is also currently applicable only in situations of redesign and 
adaptive design i.e. where the design space is understood; where non-physical models 
(analytical/parametric/simulation based) can be constructed from the knowledge of 
the relations between different characteristics and the properties. 
The model can also be further developed and integrated with other concepts in 
CPM to provide a unified view of different downstream activities in design such as 
parameter design, tolerance design and analysis, robust design and manufacturing 
process selection which are interlinked with each other. 
References 
1. Beitz, W., Pahl, G.: Engineering design: a systematic approach. Springer (2007) 
2. Weber, C.: Looking at “DFX” and “Product Maturity” from the Perspective of a New Ap-
proach to Modelling Product and Product Development Processes. In: Proceedings of the 
17th CIRP Design Conference, pp. 85–104 (2007) 
3. Weber, C.: An Extended Theoretical Approach to Modelling Products and Product Devel-
opment Processes. In: Bley, H., Jansen, H., Krause, F.-L., Shpitalni, M. (eds.) Proceedings 
of the 2nd German-Israeli Symposium on Advances in Methods and Systems for Devel-
opment of Products and Processes, Berlin, pp. 159–179 (2005) 
4. Köhler, C., Jan, C., Weber, C.: A Matrix Representation of the CPM/PDD Approach as a 
Means for Change Impact Analysis. In: Proceedings of International Design Conference, 
DESIGN 2008 (2008) 
5. Yvars, P.A., Lafon, P., Zimmer, L.: Optimization of mechanical system: contribution of 
constraint satisfaction method. In: International Conference on Computers & Industrial 
Engineering, CIE 2009, pp. 1379–1384. IEEE (2009) 
6. Scaravetti, D., Sebastian, P.: Design space exploration in embodiment design: an applica-
tion to the design of aircraft air conditioners. International Journal of Product Develop-
ment 9, 292 (2009) 
7. Sébastian, P., Chenouard, R., Nadeau, J.-P., Fischer, X.: The embodiment design con-
straint satisfaction problem of the BOOTSTRAP facing interval analysis and genetic algo-
rithm based decision support tools. International Journal on Interactive Design and Manu-
facturing (IJIDeM) 1, 99–106 (2007) 
8. Thornton, Anna, C.: Variation Risk Management: Focusing Quality Improvements in 
Product Development and Production (2003) 978-0-471-44679-8 
9. Dantan, J.-Y., Mathieu, L., Ballu, A., Martin, P.: Tolerance synthesis: quantifier notion 
and virtual boundary. Computer-Aided Design. 37, 231–240 (2005) 
10. Scott, M.J., Antonsson, E.K.: Using indifference points in engineering decisions. In: 11th 
International Conference on Design Theory and Methodology, p. DETC2000/DTM-14559. 
ASME, Baltimore (2000) 

The Implications of the Skin Model Concept
for Computer Aided Tolerancing
Benjamin Schleich⋆and Sandro Wartzack
Friedrich-Alexander-University Erlangen-Nuremberg, Chair of Engineering Design
Martensstr. 9, 91058 Erlangen, Germany
{schleich,wartzack}@mfk.fau.de
http://www.mfk.fau.de
Abstract. During product origination many deviations occur which
manifest in unreliable functional behaviour and reduced product quality.
Thus, the observable geometric deviations have to be limited by geo-
metric tolerances of function-relevant part features. In this context the
standards for Geometrical Product Speciﬁcation and Veriﬁcation (GPS)
provide helpful tools for geometric tolerancing. A basic concept within
these standards is the skin model concept which can be understood as
a model of the physical interface between the workpiece and its envi-
ronment. Recent research tries to translate this concept into discrete
geometry and focuses upon the technical aspects related to generating
skin model shapes, whereas the implications of this concept and emerging
simulation possibilities for the geometric variations management process
stay disregarded. Therefore, this paper highlights the consequences of
the skin model concept and modern simulation tools for the computer
aided tolerancing process and the management of geometric deviations
during product development.
Keywords: Skin model, geometric variations management, tolerance
management, computer aided tolerancing.
1
Geometrical Variations Management during Product
Development
Geometrical variations management is an important issue in all phases of the
product life cycle, especial in product design [1,2]. The main reason is that
even though modern manufacturing processes achieve steadily increasing accu-
racy, geometric deviations are observable on every manufactured part, which are
covered by the axiom of manufacturing imprecision and the axiom of measure-
ment uncertainty [3,4]. These geometric deviations can have huge inﬂuences on
the functional behaviour as well as the product quality. In engineering design
there is therefore a necessity to manage and to control these geometric deviations
along the whole product life cycle [5] and thus to limit the observable geometric
⋆Corresponding author.
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 573–582.
DOI: 10.1007/978-3-642-30817-8_56
c⃝Springer-Verlag Berlin Heidelberg 2013

574
B. Schleich and S. Wartzack
deviations by geometric tolerances of function-relevant part features during pro-
duct development. In this context it is acknowledged that approximately 70% of
the manufacturing costs are related to insuﬃcient tolerance requirements [6,7]
since tight tolerances lead to high manufacturing costs due to additional manu-
facturing steps, slow and cost-intensive machining, and additional measuring
expenditure [8]. Against this background, it is of high interest to identify toler-
ances which assure the functional behaviour of the product during use causing
as low as possible costs. These activities, which can be referred to as tolerance
management or geometric variations management, help to increase the compe-
titive capability of modern quality-aware companies and should be integrated in
a complete and coherent tolerancing process [5].
In order to assure cost-eﬀective product design, manufacturing, and inspec-
tion as well as adequate product quality, the tolerance activities should be incor-
porated into the early conceptual design stage (following [9]) [1,10]. Quite few
approaches have been developed for this, such as e. g. the Integrated Tolerancing
Process proposed by Dantan et al. [1], a design synthesis process proposed by
Roy et al. [11] and a concurrent design method for functional tolerance proposed
by Heng at al. [10].
In these early engineering design stages only little information about the ﬁnal
product geometry is available, whereas in later stages of product development
the whole part geometry is known and could be used for tolerance analysis.
Nevertheless, this geometry information is not taken into account by many cur-
rent tolerance analysis models since severe simpliﬁcations are made and the part
deviations are reduced to dimensional and orientation defects [12,13].
However, nowadays computer aided tools support the product designer during
product development as well as in many other phases of the product life cycle.
Though, many of these established computer models for simulation and analysis
disregard environmental impacts and imperfections of manufacturing processes
[12], their accuracy and capability for taking uncertainties and geometric devia-
tions into account steadily increases. This trend is challenging for product devel-
opers and organisations since the available tools need to be integrated eﬃciently
within existing product development processes. Especial in computer aided toler-
ancing during product development where the allowable limits of geometric devia-
tions have to be set and a huge cost responsibility is predominant, the application
of suitable methods and their eﬃcient integration into the tolerancing process is
crucial [14]. Therefore, this paper proposes an approach for performing tolerance
analysis in later design stages based on the skin model concept which is a basic
concept of the standards for Geometrical Product Speciﬁcation and Veriﬁcation
(GPS). Since this approach is based on the workpiece geometry, we denote it as
an approach for geometry-based computer aided tolerancing.
This paper is structured as follows: in the next section the standards for GPS
as well as the skin model concept are brieﬂy highlighted. Thereafter, the proposed
approach for skin model inspired tolerance analysis is explained. Following that,
the ideas are illustrated in a simple case study. Finally, we summarize our results
and give an outlook for future research.

The Implications of the Skin Model Concept for Computer Aided Tolerancing
575
2
Geometrical Product Speciﬁcation and Veriﬁcation
As mentioned before, the main aim of the tolerancing activities is to ensure the
proper function of every manufactured product despite geometric deviations of
its parts and to ﬁnally satisfy the customer demands by deﬁning and setting lim-
its for these deviations. For this purpose, the standards for Geometrical Product
Speciﬁcation and Veriﬁcation [3] oﬀer beneﬁcial tools to the product developer
for setting geometric tolerances and hence simplify geometric variations man-
agement.
2.1
The Concept of GPS
Geometrical Product Speciﬁcation and Veriﬁcation are standards for the descrip-
tion of workpieces, so that they can be manufactured and measured indepen-
dently as single parts or assembly groups [3]. Certain requirements regarding the
geometry of a workpiece or a number of workpieces are known as GPS that com-
prise size and dimensions, geometric tolerances and geometrical surface ﬁnishes
[3]. However, the concept of GPS includes four diﬀerent kinds of standards, dif-
ferent kinds of geometric features, workpiece features as a result of diﬀerent
manufacturing processes and diﬀerent stages of product development [3].
2.2
The Skin Model Concept
The standards for GPS oﬀer a toolbox for geometric variations management
to the designer for deﬁning the range of tolerable geometric deviations of a
part’s set of features. These allowed geometric deviations are adapted to the
functional requirements of the part [3]. In this regard, a part is initially deﬁned
by its nominal geometry with perfect shape and ideal dimensions, which is called
the nominal model (see Figure 1 (a)) and can be identiﬁed as a ﬁnite model
[12]. However, the nominal model can impossibly be manufactured or measured,
since every manufacturing process is inherently imprecise and every measuring
process involves uncertainties [3,4]. Therefore, the skin model concept (see Figure
1 (b)) is developed within the GPS standards, which comprises the deviations
brought in by manufacturing, measurement and assembly processes [3]. Hence,
the skin model is a model of the physical interface between the workpiece and its
environment [3]. It exists in the mind of the product developer and represents
an inﬁnite model [12]. It can be seen as a tool for the designer to envision the
deviations of the part’s shape as well as to set geometric tolerances.
Geometrical operations such as partition, extraction, ﬁltration, association,
collection and construction are required to obtain certain ideal or non-ideal geo-
metrical features. These operations are the basis of the GeoSpelling model [2]
and are also described and deﬁned by GPS standards. They can be applied to
the nominal model as well as to the skin model [3,15]. Based on this, a compa-
rison procedure between the skin model and measurement results can be deﬁned
[3]. Here, parallel procedures of speciﬁcation and measurement of manufactured
workpieces are performed in order to check if the measured workpiece complies
with the design intent [3].

576
B. Schleich and S. Wartzack
(a) Nominal Model
(b) Skin Model
Fig. 1. Diﬀerence between the Nominal Model and the Skin Model
However, the skin model concept itself is an inﬁnite model and is not related
to any kind of geometry representation [12]. Recent research activities, though,
try to translate this concept into discrete geometry [4,16] and to create possible
and realistic outcomes of the workpiece involving geometric deviations which
may be referred to as skin model shapes. The idea behind creating these skin
model shapes is visualizing manufacturing dependent geometric deviations and
simulating their eﬀects on the product behaviour during the later stages of the
product life cycle. These approaches enable the product developer to identify
critical design issues as well as the impacts of geometric tolerances and there-
fore help to optimize geometric variations management. In doing so, simulation
models can be developed based on the skin model concept which can support the
product designer in tolerance simulation as it is an important tool in product
development [14]. But, however, so far no general framework for the integration
of these approaches in the product development and tolerancing process, especial
tolerance analysis, exists.
3
Implications of the Skin Model Concept on Computer
Aided Tolerancing
The basic design tasks in geometric tolerancing are tolerance speciﬁcation, tol-
erance allocation and tolerance analysis [17]. During tolerance speciﬁcation, the
questions of which types of tolerances are required and which datums and ref-
erences should be used to ensure the proper function of the product have to be
answered. During tolerance allocation values for these selected tolerance types
have to be identiﬁed. Finally, a tolerance validation has to be performed in order
to ensure that the selected tolerances and their values ensure that the product
meets its requirements and conforms to the design intent. This validation is re-
ferred to as tolerance analysis. Tolerance synthesis, however, can be regarded as
tolerance optimization taking into account inspection and manufacturing facets
and is an iterative process of tolerance allocation, tolerance analysis and ad-
justment of tolerance values. Computer Aided Tolerancing tries to solve these
highlighted tolerancing questions with the support of computer modelling and
analysis.

The Implications of the Skin Model Concept for Computer Aided Tolerancing
577
Nowadays, the product design process and thus also computer aided toler-
ancing is still performed based on the nominal model [12], i. e. severe simpliﬁca-
tions about the reality are made. This paper, thus, focuses on how skin model
inspired simulation models could be integrated in computer aided tolerancing
and how their application in tolerance analysis could be structured.
Since these approaches are based on the workpiece geometry, we denote them
as geometry-based tolerancing, which starts as soon as geometric information
about the workpieces are available (see Figure 2). The geometry-based CAT
builds up on the design intent and the functional requirements just like the
tolerancing process itself. Based on this information, tolerance speciﬁcation and
tolerance allocation can be performed. Thereafter, the tolerance analysis is con-
ducted. For this purpose, skin model shapes, that are likely outcomes of the
workpiece surface regarding the manufacturing process, are generated using ei-
ther results of manufacturing process simulations or mathematical approaches
such as e. g. proposed by Zhang et al. [4], Schleich et al. [16] or Stoll et al.
[18]. These skin model shapes represent deviated shapes of the workpiece that
fulﬁl all tolerance requirements, i. e. usually a virtual inspection routine has to
be employed to ensure this. These skin model shapes are then assembled virtu-
ally employing assembly simulations, as for example developed in [18] or [19].
Furthermore, usage simulations employing ﬁnite element methods, elastic multi-
body simulations or similar simulation techniques are employed and applied to
the single or assembled skin model shapes [20,21]. Due to this simulation chain,
obviously, geometric deviations from all stages of manufacturing, assembly and
use, which may lead to reduced product quality or malfunction, are represented
and taken into account. Finally, the functional key characteristics are checked
for conformance with the design intent and the functional requirements as ex-
plained in ISO 17450-1 where the simulation results can be considered as virtual
measurement results. Based on this comparison, a conclusion whether or not the
speciﬁed geometric tolerances (and even speciﬁed operating windows) assure the
product function can be drawn. If it can be seen that all simulated assemblies
pass the check for conformance then the speciﬁed tolerances can be added as
functional tolerances. Otherwise, the tolerance allocation and maybe even the
tolerance speciﬁcation steps have to be repeated as well as the tolerance analysis.
This computer aided tolerance analysis procedure reﬂects the reality in this
sense that possible outcomes of the workpieces are successively generated, in-
spected, and assembled. Thereafter, a functional check is performed whether or
not the product fulﬁls the requirements. By adjusting the tolerances, the func-
tional behaviour and the quality of the product can be inﬂuenced virtually. The
capability of this approach, of course, depends strongly on the power of the
simulation models employed for virtual manufacturing, virtual assembly, and
virtual use. However, since the capability of computer aided engineering sys-
tems is steadily increasing, it seems to be only a matter of time until powerful
simulation models and techniques for these purposes are available.
Furthermore, it should be mentioned that this approach does not depend on
any mathematical tolerance representation or tolerancing standard. Of course,

578
B. Schleich and S. Wartzack
Fig. 2. Skin Model inspired Framework for Computer Aided Tolerancing
the generated skin model shapes have to be checked for tolerance requirements.
In order to perform this, various tolerancing standards may be applied. However,
the proposed general tolerance analysis procedure stays unaﬀected with regard
to the selected tolerancing standards.
The application of a skin model based approach for computer aided toler-
ancing can decrease costly tolerancing errors in product development since it
reﬂects the designers native view on the product origination process enabling
the consideration of all sources of geometric deviations. It thus oﬀers a contri-
bution to integrated product development.
4
Case Study
The proposed skin model inspired tolerance analysis process is now illustrated
in a case study. For this purpose a simple tolerance stack-up of two identi-
cal solid cubes as illustrated in Figure 3 (a) serves as an example. The func-
tional key characteristic c is the height of the assembly consisting of both cubes.

The Implications of the Skin Model Concept for Computer Aided Tolerancing
579
(a) Tolerance Stack-Up
(b) Speciﬁed Tolerances
Fig. 3. Example Assembly
Fig. 4. Three Skin Model Shapes of the Cube
Some tolerances as can be seen from Figure 3 (b) are applied to the cube in order
to assure the functional key characteristic c lying in an interval of [100.00; 100.25].
The tolerance analysis procedure can now be performed based on skin model
shapes. Firstly, we therefore create N = 1, 500 skin model shapes of the cube
employing a random ﬁeld approach as e. g. proposed in [16], i. e. a surface mesh
of the nominal part is created by a proprietary software tool and then deformed
in the surface normal direction by spatially correlated random variables. For
the Gaussian random ﬁeld parameters we choose correlation lengths of lρ =
{1; 15; 30}, a mean of μ = 0 and standard deviations of σ = {0.1; 0.15}. The
created skin model shapes are then virtually inspected in order to determine their
geometric deviations. Figure 4 shows three skin model shapes of the cube where
the grayscales indicate the geometric deviations. It should be mentioned that
these skin model shapes show only random deviations. Systematic deviations, of
course, are depending on the manufacturing process, which is assumed to be not
known yet.
These skin model shapes are then assembled to obtain the assembly as shown
in Figure 3 (a). In this case we use an assembly simulation based on the min-
imization of the convex hull volume between the cubes and perform 5, 000 as-
sembly simulations where random skin model shapes of the population with
N = 1, 500 are assembled (note that we have m = (N−1)·N
2
= 1, 124, 250 possi-
ble assemblies). Thereafter, the functional key characteristic c of the assembly is
measured. This measurement is performed by determining the maximum height
of the assembly. Three exemplary results of the assembly simulation can be seen
from Figure 5.

580
B. Schleich and S. Wartzack
Fig. 5. Three resulting Assemblies
(a) No Tolerance Requirements
(b) Applied Tolerance Requirements
Fig. 6. Results of the Case Study
Figure 6 (a) shows 150 randomly selected results where the tolerances and
the resulting value of c are plotted in a so-called parallel coordinates plot. By
deﬁning values for the tolerance requirements, the eﬀect of these speciﬁcations
on the functional characteristic c can be determined. For example, if we choose
values of dima = 0.05, ft = fb = 0.15 and p = 0.3, the functional characteristic
c lies within an interval of [min(c) = 100.031; max(c) = 100.223] (see Figure
6 (b)) and thus conforms to the original speciﬁcation. It can be seen that the
ﬂatness tolerances ft and fb as well as the parallelism tolerance p help to assure
the functional requirements. However, based on the proposed approach the ef-
fects of adjusting these tolerances on the functional characteristic can easily be
analysed.

The Implications of the Skin Model Concept for Computer Aided Tolerancing
581
5
Summary and Conclusion
Geometric deviations are observable on every manufactured part. Since these
deviations aﬀect the functional behaviour of technical products, geometrical pro-
duct speciﬁcation is inevitable and tolerance speciﬁcations have to be set during
product development in order to assure proper functioning and product quality
targets. However, many commonly used tolerance analysis methods disregard
form defects of workpieces and take only dimensional and orientation defects
into account.
The skin model as a basic concept within the GPS standards as well as increas-
ing capabilities in computer aided engineering such as sophisticated simulation
methods change the established tolerancing models and processes. Therefore,
a framework for geometry-based computer aided tolerancing is proposed which
comprises the skin model concept and the related comparison for conformance
as well as new simulation possibilities. This framework enables the product de-
veloper to integrate gathered knowledge and experience as well as modern sim-
ulation techniques into the tolerancing process. Furthermore, based thereon a
straightforward tolerance analysis process is derived. The aim of ensuring the
product quality by geometric tolerancing and variations management can hence
be achieved more accurate and within a shorter period of time.
However, to develop an integrative tolerance simulation framework for com-
puter aided tolerancing based on the presented ideas, various improvements in
the ﬁeld of tolerance simulation have to be accomplished in the future.
References
1. Dantan, J.Y., Anwer, N., Mathieu, L.: Integrated tolerancing process for conceptual
design. CIRP Annals - Manufacturing Technology 52(1), 135–138 (2003)
2. Dantan, J.Y., Ballu, A., Mathieu, L.: Geometrical product speciﬁcations – model
for product life cycle. Computer Aided Design 40, 493–501 (2008)
3. EN ISO 17450-1:2011: Geometrical product speciﬁcations (gps) – general concepts
– part 1: Model for geometrical speciﬁcation and veriﬁcation (2011)
4. Zhang, M., Anwer, N., Mathieu, L., Zhao, H.: A discrete geometry framework
for geometrical product speciﬁcations. In: Proceedings of the 21st CIRP Design
Conference (2011)
5. Mathieu, L., Ballu, A.: A model for a coherent and complete tolerancing process.
In: Davidson, J.K. (ed.) Models for Computer Aided Tolerancing in Design and
Manufacturing, pp. 35–44. Springer Netherlands (2007)
6. Boothroyd, G., Dewhurst, P.: Product Design for Manufacture and Assembly. Mar-
cel Dekker, New York (1994)
7. Ceglarek, D., Huang, W., Zhou, S., Ding, Y., Kumar, R., Zhou, Y.: Time-
based competition in multistage manufacturing: Stream-of-variation analysis (sova)
methodology-review. International Journal of Flexible Manufacturing Systems 16,
11–44 (2004)
8. Shin, S., Kongsuwon, P., Cho, B.R.: Development of the parametric tolerance mod-
eling and optimization schemes and cost-eﬀective solutions. European Journal of
Operational Research 207, 1728–1741 (2010)

582
B. Schleich and S. Wartzack
9. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.: Konstruktionslehre. Springer (2007)
10. Heng, Z., Yanlong, C., Yanding, W., Jiangxin, Y.: A concurrent design method
for functional tolerance and structure based on the principle of decomposition and
reconstitution. In: Proceedings of the 12th CIRP Conference on Computer Aided
Tolerancing (2012)
11. Roy, U., Pramanik, N., Sudarsan, R., Sriram, R., Lyons, K.: Function-to-form
mapping: model, representation and applications in design synthesis. Computer
Aided Design 33, 699–719 (2001)
12. Charpentier, F., Ballu, A., Pailhes, J.: A scientiﬁc point of view of a simple indus-
trial tolerancing process. In: Proceedings of the 12th CIRP Conference on Com-
puter Aided Tolerancing (2012)
13. Hong, Y.S., Chang, T.C.: A comprehensive review of tolerancing research. Inter-
national Journal of Production Research 40(11), 2425–2459 (2002)
14. Wartzack, S., Meerkamm, H., Stockinger, A., Stoll, T., Stuppy, J., Voß, R., Walter,
M., Wittmann, S.: Lebenszyklusorientierte Toleranzsimulation zur funktionalen
und ¨asthetischen Produktabsicherung. Konstruktion 6, 63–74 (2011)
15. Mathieu, L., Ballu, A.: Univocal expression of functional and geometrical tolerances
for design, manufacturing and inspection. In: 4th CIRP Seminar on Computer
Aided Tolerancing, pp. 31–46 (1995)
16. Schleich, B., Anwer, N., Mathieu, L., Walter, M., Wartzack, S.: A comprehensive
framework for skin model simulation. In: Proceedings of the ASME 2012 11th
Biennial Conference on Engineering Systems Design and Analysis (2012)
17. Armillotta, A., Semeraro, Q.: Geometric Tolerance Speciﬁcation. In: Geometric
Tolerances – Impact on Product Design, Quality Inspection and Statistical Process
Monitoring. Springer, Heidelberg (2011)
18. Stoll, T., Wittmann, S., Meerkamm, H.: Tolerance Analysis with detailed Part
Modeling. In: Giordano, M., Mathieu, L., Villeneuve, F. (eds.) Product Life-Cycle
Management. Geometric Variations (2010)
19. Samper, S., Adragna, P.A., Favreliere, H., Pillet, M.: Modeling of 2d and 3d as-
semblies taking into account form errors of plane surfaces. Journal of Computing
and Information Science in Engineering 9(4) (2009)
20. Walter, M., Wartzack, S.: Analysis of the eﬀects of manufacturing-caused devia-
tions and varying operation parameters on operation-depending deviations of sys-
tems in motion. In: Proceedings of the 12th CIRP Conference on Computer Aided
Tolerancing (2012)
21. Schleich, B., Stockinger, A., Wartzack, S.: On the impact of geometric deviations
on the structural performance. In: Proceedings of the 12th CIRP Conference on
Computer Aided Tolerancing (2012)

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 583–592. 
DOI: 10.1007/978-3-642-30817-8_57 
© Springer-Verlag Berlin Heidelberg 2013 
 
Can a Pre-sketching Activity Improve Idea Generation? 
Emily Worinkeng, Joshua D. Summers, and Shraddha Joshi 
Mechanical Engineering, Clemson University, Clemson SC-29631, USA 
{eworink,jsummer,shraddj}@clemson.edu 
Abstract. This paper examines the influence of a pre-sketching activity on the 
quantity and novelty of the design solutions. A controlled experiment is used to 
investigate the influence of pre-sketching activity on quantity and novelty of the 
concepts. Two student groups sketched solutions for the same design problem; 
with one group given a pre-sketch activity before the design problem. Results 
reveal that this short pre-sketching activity positively affects the novelty of the 
solutions of the design problem (p-value=0.05 for novel means). Further, while 
the findings suggest that the pre-sketching activity resulted in more concepts 
generated, this was not found to be statistically significant (p-value=0.22). 
Therefore, it is recommended that idea generation methods be augmented with 
short pre-sketching activities before the sessions.  
Keywords: pre-sketching, design experiment, idea generation. 
1 
Motivation to Study Idea Generation Activities 
Several idea generation techniques support early stage conceptual design synthesis 
activities, such as brainstorming, brain sketching, C-sketch, Gallery method, and 6-3-
5. Four metrics can be used to evaluate and compare different idea generation me-
thods: quality, quantity, variety and novelty [1–3]. This paper explores novelty and 
quantity metrics as they are affected by the introduction of a “pre-sketching” activity 
to an idea generation session. Pre-sketching is a sketching exercise performed imme-
diately before ideating on the actual design problem. This proposed augmentation to 
existing ideation techniques is based on the premise that sketching activities can have 
a positive impact on ideation. For instance, sketching allows the evaluation of design 
ideas in terms of level of creativity and enables designers to obtain feedback on their 
designs [4]. Graphical representations are preferred over textual representation based 
on the fact that they produce higher quality and novel ideas [5]. Previous experiments 
suggest that sketching and graphical representations increase the quantity of ideas [6]. 
The pre-sketching activity is used to “prime” the designer to be thinking visually 
when they are presented with the new design problem. 
Researchers have explored the influence of using different idea generation methods 
on the quantity, quality, novelty and variety of solutions [7,8]. However, the influence 
that different types of sketching activities, such as sketching ideas for a novel design 
problem and sketching artifacts from memory, have on generating novel concepts for 
other design problems is relatively unexplored. Thus, this paper investigates the im-
pact that pre-sketching activities have on the quantity and novelty of ideas generated 
by students. To this end, this paper focuses on these research questions: 

584 
E. Worinkeng, J.D. Summers, and S. Joshi 
• What is the influence of pre-sketching activity on the quantity of design solutions? 
• What is the influence of pre-sketching activity on the novelty of design solutions? 
To answer these questions, a user study was conducted in a senior level mechanical 
engineering design class at Clemson University in the Fall 2011 semester. The class 
was divided into two groups of roughly the same number of students. One group was 
given a pre-sketch activity while the other group was not given any pre-sketch activi-
ty. The results from the two experimental groups are compared with respect to quanti-
ty and novelty.  
2 
Prior Research on Warm-Up Exercises 
Researchers have studied the impact that various pre-exercises have on ideation. In 
one experiment, students were given pictures of toys/creatures and were asked to 
produce new toys/creatures by retrieving a picture from each category. The instruc-
tions emphasized both novelty and quantity. This research shows there was not a sig-
nificant difference in the quantity but introducing examples can constrain generation 
of imaginative ideas [9]. In another experiment, analogical thinking was explored 
which showed that subjects can use a story analogy to guide generation of an analog-
ous solution to the target problem [10]. The study suggests that analogical thinking 
may play a role in creative problem solving and provide information about the mental 
processes involved in analogical problem solving. In these two studies, examples 
were provided to students to determine the impact that analogy or fixation had on 
ideation. 
Alternatively, others have studied how different types of training impact ideation 
and problem solving. In one experiment, students were given two unbalanced equa-
tions to balance them with coefficient as a warm up exercise and then given other 
chemical equations to solve. The results show that students learned how to rethink 
and change some features in their drawings to reflect on other equations [11]. Thus, 
graphical training immediately before an exercise had positive impact. The idea of 
doing “warm-up” exercises is common practice in athletics to prepare the body for the 
physical toll of the activities [12]. Here, we introduce an analog to these warm-ups 
through a pre-sketching activity to have the students practice the activity of sketching 
before addressing the design problem.  
3 
Sketching in Idea Generation 
Sketching is considered the principal approach by which engineers visualize the solu-
tions and it concretizes the thinking [13]. Designers sketch to externalize their con-
cepts and where the drawings provide visual clues for refinement and revision [14]. 
Alternatively, one might define a sketch as a rough drawing that addresses one or 
more requirements in design. In engineering domains such as software, architecture, 
and mechanical design, graphical communication plays a key role throughout the 
design process [15]. Sketching facilitates the evolution of concepts into products, 
thereby having a positive impact on the quality of the solution as well as the designer 
[19].  

 
Can a Pre-sketching Activity Improve Idea Generation? 
585 
In order to conduct empirical studies on ideation, one must address what should be 
measured and how it should be measured to evaluate the process or the outcome of 
the ideation [1]. Subsequently, much focus has been placed on evaluating the impact 
of sketching exercises on the designers. In one study, it was found that the quantity of 
sketches overall increases the likelihood of generated quality solutions [16]. Essential-
ly, the sketches are used as quick surrogates for evaluating the potential for ideation 
and other design activities to generate solutions. Analyzing these sketches poses a 
challenge and thus researchers have developed wide variety of metrics that may be 
used for analyzing these sketches.  
Quantity is most often measured while analyzing sketches [5,11], yet researchers 
count elements differently. For example, quantity might be a count of the number of 
distinct sketches generated [19,20] or it could be a count of only those sketches that 
represent solutions that satisfy targeted functions [8]. The quantity of ideas is impor-
tant in creativity studies because it is a measure of fluency with the assumption that 
generating multiple ideas increases the chance generating better ideas [1]. 
Examples of other metrics include quality, novelty, and variety, but again they are 
used differently [5,7]. Novelty might be the frequency at which a complete idea is 
found in a collection [7] or it might be based on the unanticipated solutions to func-
tions and sub functions [5]. Each of these different measures evaluates the solution 
found within the sketch, rather than on the sketch itself. 
4 
Experiment Design 
This experiment evaluates whether a pre-sketching activity has impact on novelty and 
quantity of solutions generated. To this end, two groups of students from senior me-
chanical engineering (ME) design course were given a problem for which to sketch 
solutions. In addition, one of the groups was given a “pre” treatment where they were 
instructed to generate sketches prior to the idea generation session. Two hypotheses 
are tested: (1) the students who participated in the pre-sketching activity will generate 
a greater quantity of concepts than those with no pre-sketching activity and (2) the 
number of novel concepts from the experimental group will be greater than the con-
trolled group. These hypotheses are an extension of [1]which suggest that any inter-
vention influencing the designer or the design process will be reflected in the output. 
As a result, if the outcome is a greater number and more novel solutions, then it will 
be assumed that the intervention had a positive impact.  
Many studies have been done using a population of undergraduate students for de-
sign activity research [9,11,27]. Therefore it is valid to use students for this particular 
study. Moreover, these senior students are less than five months from being consi-
dered practicing engineers or novices. Thirty-one students participated in this study. 
Characteristics such as personality, race, age, sex, or religion were out of scope for 
this study as students were randomly assigned to the experimental groups to avoid 
self-selection concerns. The students in this senior level class had previously com-
pleted the conceptual design phase of a semester long project to design wind tunnels 
for local elementary schools, thus giving the students similar baseline experiences  

586 
E. Worinkeng, J.D. Summers, and S. Joshi 
in design. Finally the students who participated in this activity were given extra credit 
within the course to ensure appropriate levels of effort in the study. This type of in 
class activity is common in the course and was therefore not considered an intrusive 
activity for the students.  
4.1 
Sketching “Warm Up” Activity and Design Problem 
The pre-sketch exercise administered to the experimental group was to sketch a dream 
home in Alaska. This problem was chosen based on its simplicity and familiarity to 
the students, to encourage flow of ideas and with the intention to “jump-start” the 
imagination of the participants. Specifically, it was felt that drawing a “dream home” 
in an “exotic” location would challenge the students without requiring them to ad-
dress explicit design requirements. The activity was developed so it would not be 
another challenging design problem in itself, but rather an accessible sketching activi-
ty. The experimental group was not given detailed instructions on how to sketch or 
render the “dream home”, allowing the students to provide perspective drawings, 
orthographic views or schematic drawings. They were given 10 minutes to complete 
the warm-up sketching task. The design problem (Fig. 1) give to both groups was to 
design a carpentry nail remover.  
 
While in his home workshop, a carpenter occasionally needs to remove an unwanted nail from a 
given project. For this project, the carpenter needs to remove a nail and replace it without causing 
damage to his almost completed project. Normally he might use the classic pry technique; however, 
the nail is in such a confined place that the pry will not work. The only angle he can approach the nail 
removal is along the axis of the nail. You are a designer at a Hobbyist Tools Inc. and have 30 minutes 
to design a nail removing device keeping the following requirements in mind. The device should: (1) 
Not damage the material, (2) Remove the nail parallel to its axis, (3) Remove the nail from a 4”x 4” 
space, (4) Be designed with a hobbyist carpenter’s budget in mind, (5) Be designed for high 
production rates of a tool company, and (6) Not enlarge nail hole during removal 
Fig. 1. Related Design Problem given to all participants 
None of the students have seen this problem before in other design classes, though 
this design problem has been used in other experimental studies [24]. Thus, the scale 
and challenge of the design problem have been previously tested and found suitable 
for senior ME students. The problem does not have a current commercial solution 
(unsolved), yet is easily accessible in understanding for the students (familiar). All 
students in the course have completed the ME lab sequence that includes basic fabri-
cation and woodworking. Therefore, they should be nominally familiar with chal-
lenges associated with removing nails from wood. 
4.2 
Execution 
Students were drawn from the course. The experiment was conducted in familiar, 
though segregated settings, during the regular scheduled course time period. Thus, 
discomfort in unfamiliar settings was reduced in the experiment. Students had  
 

 
Can a Pre-sketching Activity Improve Idea Generation? 
587 
sufficient workspace to remove social inhibition. Students worked independently and 
were not allowed to talk. No additional sketching training was administered before the 
experiment.  
The experimental group had 16 students and the control group had 15 students. 
Both groups were given thirty minutes to sketch solutions to the nail-removing prob-
lem. The students were not explicitly instructed to generate solutions based on quali-
ty, quantity, novelty, or variety to avoid biasing the results towards one of these  
metrics. After the instructions were given, students could ask clarification questions.  
Each student was instructed to sketch only a single solution per page provided. A 
pilot run of the experiment suggested that a dozen sketch sheets would be sufficient 
for student idea generation within the given time period. All sketches, including the 
pre-sketch activity results, and a post-experiment survey were collected, though only 
the sketches generated were used for experimental analysis.  
5 
Protocol for Analyzing Design Solution Sketches  
This experiment focused on quantity and novelty as metrics to analyze the sketches.. 
Quantity is studied to ensure that a pre-sketching activity does not negatively impact 
the number of sketches that are generated by designers. Novelty is the metric used to 
evaluate the positive impact of the activity. Variety and quality analysis are reserved 
for future studies. To test the hypotheses, the results of each group are compared. 
Quantity is based on the total number of sketches generated for each individual as 
solutions to the nail removal design problem for both the controlled group and the 
experimental group. Each sketch was documented on a separate sketch sheet by the 
individual, allowing for a simple quantity count of each sketch sheet that was com-
pleted for each individual. Only pages with sketches that addressed one or more re-
quirements were considered. For example if a page contained a drawing not related or 
addressing the problem statement, it was not added to the count.  
Novelty is defined as an unexpected idea that satisfies one or more requirements of 
the design problem. Novel solutions are sought in idea generation methods as they 
can help designers to shift from evolutionary to revolutionary design. To objectively 
evaluate the sketches generated by participants for novelty of solutions, a protocol 
was defined to score each sketch according to how novel specific elements of the 
sketch are. In this protocol, similar to that of [5], a collection of predicted solution 
fragments for the defined set of requirements in the nail removal problem, are devel-
oped a priori to avoid biasing. These fragments are represented in a morphological 
matrix (Table 1). The collection was drawn from the previous experiment using this 
design problem [24] and augmented by the authors. The sketches generated by the 
students are compared against the means in the morphological matrix. A fragment in a 
sketch that could not be mapped to one in the chart is deemed “novel”. Each sketch 
has a novelty score as the sum of the number of novel fragments found with a maxi-
mum score of five.  

588 
E. Worinkeng, J.D. Summers, and S. Joshi 
Table 1. Morphological Chart for Novelty Concepts 
Sub-Functions 
Means 
Secure Head of Nail 
Adhesive 
Magnet 
Matching Drill Bit 
Size 
Claws 
 
Generate Axial Force
Power Screw 
Hydraulic or 
Pneumatic Piston 
Manual Rotational 
Motion 
Ball Screw Human 
Force 
Apply Axial Force 
Linear Guide 
Linear Bearings 
Gear 
Pushing 
Nail 
Lever 
Adjust to Different 
Nail Sizes 
Several Drill Bit 
Sizes 
Self-Adjusting 
Mechanism 
 
 
 
Not Damage Material
Specify a 
Material 
Pushing Nail 
 
 
 
6 
Analysis and Results 
Seventy sketches were generated by the experimental group and fifty-seven sketches 
from the controlled group. An ANOVA test is performed to determine if the two 
groups are statically equivalent in the quantity of concepts generated. The p-value 
found is 0.283. Further in the analysis of the experiment, an outlier was found who 
generated five sketches with a much higher score for novelty compared to students in 
the controlled group. The ANOVA test was performed again with a p-value of 0.228. 
In both analyses, it is shown that the pre-sketching activity does not have a significant 
impact on the quantity, thus supporting the first hypothesis. 
For novelty, an evaluator assigns a score of 0 or 1 for each sub-function addressed 
in the sketch (0 if found in Table 1; 1 if not found). If a function is not addressed, it is 
scored a 0 also. The level of detail is not of interest and is not addressed in this study. 
An example sketch from a student in the experimental group is shown in Fig. 1. When 
evaluated against the protocol for novelty, this sketch received the score (0,0,0,0,0) as 
all means used are found in Table 1. For example, “Handle” meets sub-function S2 
with means Human force, “Clamp” meets sub-function S1 (Secure head of nail) by 
means of acting as claws, the edges can adjust to different nail sizes, therefore it 
meets sub-function S4 with the mean self-adjusting mechanism, and “Gear” meets 
sub-function S3. 
If a novel means is found, the matrix is not updated as only a priori novelty is of 
interest in this study. After scoring all the sketches, there were 16 novel solutions 
(sketches that have at least one novel mean) from the pre-sketching group and 9 novel 
solutions from the experimental group. While the protocol was developed to be as 
objective as possible, it was tested for inter-rater reliability to assess its robustness. A 
panel of four graduate student judges was used to score a subset of sketches. The 
distribution of analysis was done such that at each sketch was evaluated by at least 
two judges and one judge evaluated sketches shared with all other judges. Each judge 
analyzed the sketches individually and the agreement in the results from the analysis 
was tested using Joint Probability method of inter-rater reliability [33].  

 
Can a Pre-sketching Activity Improve Idea Generation? 
589 
 
Fig. 2. Example sketch detailing a completely un-novel solution of student A 
Rater A analyzed all the sketches (70 sketches from experimental group and 57 
from control group). If the Joint Probability Agreement was found to be sufficient, 
this set of analysis would be used for the inferential statistics. Rater B analyzed the 
second half of the sketches from both groups. Raters C and D scored the first half of 
the sketches of both pre-sketch and no pre-sketch groups. The joint probability 
agreement between the raters for the experimental group is (AB=0.70; AC=0.86, AD 
=0.71; CD=0.71) and the control group is (AB=0.90; AC=0.74; AD=0.81; CD=0.85). 
All inter-rater scores were above the target of 0.7, so the protocol is assumed robust 
and the analyses of Rater A are used for further analysis. 
An ANOVA test is done to determine whether the two groups are equivalent in 
their responses through the novelty scores. Four novelty permutations are assessed 
within each individual, as the student is the unit of study. The first score is the count 
of novel scores generated per designer (novel solution). Should a designer generate 
four sketches but only two determined to be novel, the score for this designer would 
be 0.5. The second score is the total number of novel solution fragments or means that 
were generated by an individual. The novel means might exist within a single sketch 
or may be distributed across multiple sketches without changing the score assigned to 
the designer. Third, the means density is the total number of novel means for a de-
signer divided by the total number of solution fragment possibilities in the number of 
sketches generated. The final score is the novel solution density where the number of 
solution sketches that had a novel fragment is divided by the total number of sketches 
generated. By examining both the novel solution and the novel solution fragment 
perspective, the pre-sketching activity can be examined for influence at different le-
vels of granularity. Moreover, by including the density for the number of sketches 
generated, these scores can be adjusted for those situations where a designer may 
generate many non-novel ideas against those where a designer may generate one or 
two highly novel solutions only.  

590 
E. Worinkeng, J.D. Summers, and S. Joshi 
The ANOVA test with single factor is performed (Table 2), with and alpha=0.15 
common in designer experimentation [37]. The lowest p-value for the four metrics 
was found to be 0.21, suggesting that the two groups are not statistically dissimilar.  
Table 2. ANOVA for Novelty without Outlier 
 
Novel Solution 
Novel Means 
Novel Density 
Novel Solution  
Density 
With 
WO
With
WO
With
WO
With 
WO 
Column Pre-Sketch 
16 
16
16
16
16
16
16 
16 
Column No Pre-Sketch 
15 
14
15
14
15
14
15 
14 
Sum Pre-Sketch 
16 
16
24
24
1.05
1.05
4.03 
4.02 
Sum No Pre-Sketch 
9 
7
18
8
0.79
0.39
2.1 
1.70 
Average Pre-Sketch 
1.00 
1
1.50
1.5
0.07
0.06
0.25 
0.25 
Average No Pre-Sketch 
0.60 
0.50
1.20
0.57
0.05
0.027
0.14 
0.12 
Variance Pre-Sketch 
0.67 
0.66
2.00
2.00
0.004
0.004
0.07 
0.07 
Variance No Pre-Sketch 
0.83 
0.73
6.89
1.03
0.01
0.003
0.05 
0.04 
P-value
0.21 
0.11
0.70
0.05
0.68
0.084
0.22 
0.15 
 
Upon closer investigation of the data, it is found that a single student generated ten 
novel means out of five sketches, approximately twice the amount of the next highest 
performer. This student was also recognized as a more creative student in the class 
based on a review of the student’s performance in other portions of the class. There-
fore, the ANOVA test was done again, this time without this outlier’s scores. In the 
adjusted analysis, all p-values are less than or equal to 0.15. These results, combined 
with higher pre-sketching quantity, though not statistically significantly, suggest that 
a pre-sketching can provide value in an ideation activity by increasing the likelihood 
of generating novel solutions. That said, as a single outlier can have this amount of 
impact on the statistical significance, it is recommended that additional testing is con-
ducted to expand the sample sizes to truly represent the population. 
7 
Conclusions/Future Work 
This paper presents the findings of an experiment studying whether a sketching 
“warm-up” activity has a positive influence on novelty and does not negatively im-
pact quantity. It was statistically found that novelty was positively impacted by the 
warm-up activity, if not considering the discovered outlier. Further, while the data 
suggests that quantity was also increased, though not statistically significant and thus 
a larger sample is needed to draw final conclusions.  
More types of activities related to sketching can be tested prior to idea generation 
sessions to determine if the results obtained in this paper can be attributed a causal 
relationship. Higher replication counts are needed to perform statistical analyses of 
these results. In order to determine causal relationships two more studies can be done, 
one with a pre-sketching activity with an engineering related topic and one with a pre-
activity that does not involve sketching i.e solving a design problem verbally. If re-
sults show that the activities yield to a greater number of concepts with an increase in 

 
Can a Pre-sketching Activity Improve Idea Generation? 
591 
novelty then one could determine that what causes these enhancements is the warm up 
activity before the idea generation session. At the very least, it is recommended that 
designers include a short “pre-sketching” warm-up exercise when they seek innova-
tive solutions to problems. 
 
Acknowledgments. We would like to thank Matthew Peterson, Ivan Mata, and all the 
graduate students in ME 893 who helped conduct this user study experiment in Fall 
2011. 
References 
1. Shah, J.J., Smith, S.M., Vargas-Hernández, N.: Metrics for Measuring Ideation Effective-
ness. Design Studies 24, 111–134 (2003) 
2. Nelson, B.A., Wilson, J.O., Rosen, D., Yen, J.: Refined metrics for measuring ideation ef-
fectiveness. Design Studies 30(6), 737–743 (2009) 
3. Peeters, J., Verhaegen, P.-A., Vandevenne, D., Duflou, J.R.: Refined Metrics for Measur-
ing Novelty in Ideation. In: IDMME Virtual Concept, p. 4. Springer, Bourdeaux (2010) 
4. Goldschmidt, G.: The dialectics of sketching. Creativity Research Journal 4(2), 123–143 
(1991) 
5. McKoy, F.L., Vargas-Hernández, N., Summers, J.D., Shah, J.J.: Influence of design repre-
sentation on effectiveness of idea generation. In: Proceedings of ASME International De-
sign Engineering Technical Conferences and Computers and Information in Engineering 
Conference, DTM-21685, Pittsburgh, PA (2001) 
6. Vargas-Hernández, N., Schmidt, L., Kremer, G.E.O.: An Empirical Study of the Effective-
ness of Selected Cognitive Aids on Multiple Design Tasks. In: Gero, J. (ed.) Design Com-
puting and Cognition, College Station, TX (2012) 
7. Linsey, J.S., Clauss, E.F., Kurtoglu, T., Murphy, J.T., Wood, K.L., Markman, A.B.: An 
Experimental Study of Group Idea Generation Techniques: Understanding the Roles of 
Idea Representation and Viewing Methods. Journal of Mechanical Design 133(3), 031008 
(2011) 
8. Linsey, J.S., Wood, K.L., Markman, A.B.: Increasing Innovation: Presentation and Evalua-
tion of the Wordtree Design-by-Analogy Method. In: 20th International Conference on 
Design Theory and Methodology; Second International Conference on Micro- and Nano-
systems, vol. 4, pp. 21–32. ASME (2008) 
9. Smith, S.M., Ward, T.B., Schumacher, J.S.: Constraining effects of examples in a creative 
generation task. Memory & Cognition 21(6), 837–845 (1993) 
10. Gick, M.L., Holyoak, K.J.: Analogical problem solving. Cognitive Psychology 12(3), 306–
355 (1980) 
11. Roseman, R., Mcbride, D.: Exploring Student Understanding of Chemical Equations 
through Representative Drawings. In: National Conference on Undergraduate Research, 
Ithaca College, NY, pp. 440–446 (2011) 
12. Sotiropoulos, K., Smilios, I., Christou, M., Barzouka, K., Spaias, A., Douda, H., Tokmaki-
dis, S.P.: Effects of Warm-Up on Vertical Jump Performance and Muscle Electrical Ac-
tivity Using Half-Squats at Low and Moderate Intensity. Journal of Sports Science and 
Medicine 9(2), 326–331 (2010) 
13. Mezughi, M.M.: The integral role of drawing in architectural conception. Thesis. Universi-
ty of Glasgow (1996) 

592 
E. Worinkeng, J.D. Summers, and S. Joshi 
14. Masaki Suwa, B.T.: What architects and students see in architectural design sketches: A 
protocol analysis. In: First International Symposium on Descriptive Models of Design 
(1996) 
15. Lau, K., Oehlberg, L., Agogino, A.: Sketching in Design Journals: an Analysis of Visual 
Representations in the Product Design Process. Engineering Design Graphics Jour-
nal 73(3) (2009) 
16. Song, S., Agogino, A.M.: Insights on Designers’ Sketching Activities in New Product De-
sign Teams. In: 16th International Conference on Design Theory and Methodology, vol. 
3a, pp. 351–360. ASME (2004) 
17. Yang, M.C.: Observations on Concept Generation and Sketching in Engineering Design. 
Research in Engineering Design 20, 1–11 (2009) 
18. Yang, M.C.: Concept Generation and Sketching: Correlations With Design Outcome. In: 
ASME International Design Engineering Technical Conferences and Computers and In-
formation in Engineering Conference, pp. 829–834. ASME, Chicago (2003) 
19. Yang, M.C.: A Study of Prototypes, Design Activity, and Design Outcome. Design Stu-
dies 26, 649–699 (2005) 
20. Ramachandran, R., Caldwell, B.W., Mocko, G.M.: A User Study to Evaluate the Function 
Model and Function Interaction Model for Concept Generation. In: 23rd International 
Conference on Design Theory and Methodology; 16th Design for Manufacturing and the 
Life Cycle Conference, vol. 9, pp. 273–284. ASME (2011) 
21. Ruder, J., Sobek, D.K.: Student System Level Design Activities: An Empirical Pilot Study 
on Improving Design Outcomes. In: 17th International Conference on Design Theory and 
Methodology, vol. 5a, pp. 19–25. ASME (2005) 
22. Ostergaard, K.J., Wetmore III, W.R., Divekar, A., Vitali, H., Summers, J.D.: An experi-
mental methodology for investigating communication in collaborative design review meet-
ings. Co-Design 1(3), 169–185 (2005) 
23. Wetmore III, W.R., Summers, J.D., Greenstein, J.S.: Experimental study of influence of 
group familiarity and information sharing on design review effectiveness. Journal of Engi-
neering Design 21(1), 111–126 (2010) 
24. Hess, T.A.: Investigation of Prototype Roles in Conceptual Design Using Case Study and 
Protocol Study Methods. MS Thesis, Mechanical Engineering, Clemson University, Clem-
son, SC (2012) 
25. Sim, J., Wright, C.C.: The kappa statistic in reliability studies: use, interpretation, and 
sample size requirements. Physical Therapy 85(3), 257–268 (2005) 
26. Banerjee, M., Capozzoli, M., McSweeney, L., Sinha, D.: Beyond kappa: A review of inter-
rater agreement measures. Canadian Journal of Statistics 27(1), 3–23 (1999) 
27. Verhaegen, P.-A., Peeters, J., Vandevenne, D., Dewulf, S., Duflou, J.R.: Effectiveness of 
the PAnDA ideation tool. Procedia Engineering 9, 63–76 (2011) 
28. Viswanathan, V.K., Linsey, J.S.: Physical Models in Idea Generation: Hindrance or Help? 
In: 22nd International Conference on Design Theory and Methodology, vol. 5, pp. 329–
339. ASME (2010) 
29. Miller, K.J., Vanni, M.: Inter-Rater Agreement Measures and the Refinement of Metrics in 
the PLATO MT Evaluation Paradigm. Technical Report, McLean, VA (2005) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 593–602. 
DOI: 10.1007/978-3-642-30817-8_58      © Springer-Verlag Berlin Heidelberg 2013 
Design Approach for an Adaptable Highly Integrated 
Hydraulic Feed Axis 
Jörg Bauer and Jürgen Fleischer 
wbk Institute of Production Science, Karlsruhe Institute of Technology (KIT),  
Kaiserstrasse 12, Karlsruhe, 76131, Germany 
Joerg.Bauer@kit.edu 
Abstract. Currently, micro-machines are often based on principles of macro-
machines. Results are unfavorable ratios between the build and the work space 
and also between the large moving masses and the work piece mass. To over-
come these shortcomings, it is necessary to develop new approaches for work 
piece adaptable micro machine components as well as new kinematic chains es-
pecially geared towards the requirements of micromachining.  For this reason, 
a work piece adaptable, highly integrated, piezohydraulic feed axis is devel-
oped. In this paper, the design of the hydraulic feed axis as well as a simulation 
and scaling tool are outlined. Furthermore, measurements regarding accuracy 
are presented. 
Keywords: Micromachining, conceptual design, hydraulic. 
1 
Introduction 
In product design, a significant decrease in part sizes but also an increase in the part’s 
complexity and produced units can be noticed. This is due to three main factors: 
Energy and resource shortness, enabling of functions and demand of comfort and 
customization. Decreased part size leads to less resource consumption during manu-
facturing and lesser energy consumption during operation. Small parts and machines 
enable functions such as the usage in minimal invasive surgery. Customers claim 
smaller devices e.g. mobile phones due to comfort reasons, but on the other hand they 
also wish to have personalized devices to meet their specific requirements. 
Therefore, production technology faces three main challenges: First, parts are be-
coming more complex while significantly reduced in part size. Second, the functional 
density of the devices is increasing. Third, the produced units are increasing while 
large numbers of varieties are required.  
Small parts can be defined, as shown in Fig. 1, with part size in the cm and sub-
mm range, structure sizes in the mm and sub-µm range as well as tolerance require-
ments in the µm and sub-µm range. Those parts can be exemplarily found in medical 
applications, consumables such as optics and mobile phones but also in automotive 
and industrial applications. This leads to high-volume production. 

594 
J. Bauer and J. Fleisc
Fig. 1. 
2 
State of the Art 
2.1 
Current Machine T
Current machine tools for 
based on macro machine to
fore, these machines feature
space (see Fig. 2). 
Fig. 2. Exemplary floor
 
This disproportion furth
backs. The production of 
material during production
larger moving masses. The
which heats up the machine
the high energy consumpt
cher 
 
Parts machined on small machine tools [1] 
Tools for Micromachining 
the production of parts in the specified range are of
ool principles, components and kinematic chains [2]. The
e a disproportionate build space in comparison to the w
 
r space and work envelope of a current micro machine tool  
her leads to ecological, economical and technical dr
a non-work piece adapted machine tool requires m
n and higher energy consumption during operation due
 powerful feed axis required for high dynamics emits h
e tool. This requires strong cooling systems. Economica
tion and the lack of portability of current machines 
ften 
ere-
work 
aw-
more  
e to 
heat 
ally, 
are  

 
Design Approach for an Adaptable Highly Integrated Hydraulic Feed Axis 
595 
unfavorable. Additionally, current machine tools are hard to reconfigure which will be 
required in the future due to high part varieties. Technically, current machine tools 
feature a large component size requiring clash conditions which reduce the achievable 
machine complexity. 
Current work piece adapted machine tools are shown in [2]. It can be stated that the 
feed units of the machine tools are one influencing factor for the machine tool’s size. 
Furthermore, reducing the size of the machine tool leads to reduced masses which in 
turn lead to higher eigenfrequencies that can be excited by e.g. a milling process. 
However, the production of small, complex parts requires high accuracy which can 
only be achieved by an increased damping within the feed axis. Therefore, the reduc-
tion of the feed axis dimension, the scaling of the feed axis as well as an increase of 
the damping in the guidance system is one approach to a work piece adapted machine 
tool. 
2.2 
Current Feed Axis for Micro Machine Tools 
Currently, a wide variety of feed axes is used in machine tools. In micro-machining 
where high accuracy is required hydrostatic guides are often used due to reduced fric-
tion, wear and damping. As drives for propulsion, ball screw drives, hydrostatic 
thread drives and linear direct drives can be found. In Table 1 and Table 2, different 
feed drives and guidance systems are compared. 
Table 1. Comparison of guidance systems [4] 
 
Concluding Table 1 a hydrostatic guidance system features a high damping ratio, 
enabling high precision for manufacturing. Hydrostatic feed axis feature a lack of 
stiffness. Due to small strokes within small machine tools the stiffness is considered 
sufficient. 
Requirements
Force 
density
Functional 
integration
Damping
Stiffness
Running 
smoothness
Scalability
Sum:
Aerostatic
Magnet-
guidance
Metal/
Metal
Metal/
Plastic
Ball
Roll
Cam 
roller
Hydrostatic
Sliding 
guide
Linear 
bearing

596 
J. Bauer and J. Fleischer 
Table 2. Comparison of feed drives [4] 
 
In conclusion it can be stated, that a combination of a hydraulic feed drive in com-
bination with a hydrostatic guidance system proposes a compact and highly dynamic 
feed axis with high damping for ultra-precision application. 
2.3 
Current Simulation Approaches 
Currently, simulation is widely used in machine tool design. It can be found in  
concept, design and realization [5]. While 3D-CAD design and kinematics optimiza-
tion used during concept activities is reduced in complexity due to widely changing 
models and the need for fast prediction of the feasibility of the proposed concept, 
simulation models and tools are significantly increasing in complexity towards the 
end of the design process of a machine tool. At the Institute of Production Science in 
Karlsruhe, an approach for simulation of machine tools with minimal effort is pro-
posed [6]. Within this integrated approach, the model’s complexity is increasing for 
each step of the development process. While in early stages of the product develop-
ment process the fast and easy testing of kinematic chains as well as the estimation of 
first design parameters is more important than the required accuracy, simple kinemat-
ic models are sufficient for this purpose. Along with the progress in the development 
process, the model’s complexity also has to increase, requiring more time for simula-
tion allowing more precise results.  
As configuration tools in enterprises, scalable CAD models are often used for con-
figuration of machine tool components. CAD systems often allow the parametric 
design of assemblies and parts. Therefore, internal and external parameters can be 
defined. Furthermore, parameters which cannot be varied can be defined as well as 
parameters which vary with fixed mathematical functions. For example, the feed of a 
feed axis consists of the traverse path as well as the width of components such as the 
carriage. It is on the basis of these specified parameters that the others can be  
calculated.  
Requirements
Force 
density
Functional 
integration
Damping
Stiffness
Dynamics
Scalability
Sum:
TGT
TGT: Acme screw drive
KGT
KGT: Ball screw
PGT
PGT: Planetary screw assembly
Piezo/
Mech
Piezo-
Hyd.
Piezo
Direct lin. actuator
Electro-
mechanical feed 
axis
Hybrid 
feed axis
Fluidic feed axis

 
Design Approach for an Adaptable Highly Integrated Hydraulic Feed Axis 
597 
3 
Approach 
The configuration and scaling of the work piece adapted feed axis will take place at 
the end of the product development process. Hence, the further outlined approach is 
based on results, methods, simulation tools and techniques especially proposed for the 
final stages of the development process outlined in [6]. These techniques and tools are 
adapted and geared in particular towards an easy and targeted configuration and scal-
ing of the highly integrated work piece adaptable hydraulic feed axis. 
In comparison to [6], the approach for a work piece adapted feed axis consists of 
two main parts: First, the targeted configuration, simulation and adaption of the feed 
axis is enabled by a simulation and scaling tool. Furthermore, this tool permits an 
efficient pre-tuning of the controller as well as an estimation of the dynamic behavior 
before the actual feed axis is manufactured. Results of this tool are the controller pa-
rameters, the dynamic behavior as well as the drawings of the parts to be produced.  
Since the basic design of the feed axis is the same for all variations, the feed axis 
varies in size, dynamic, stiffness as well as in force. In the following, the overall con-
cept will be outlined. 
3.1 
Simulation and Scaling Tool 
For an efficient and targeted configuration of the work piece adaptable hydraulic feed 
axis, a simulation and scaling tool has to be used (see Fig. 3). On one hand, it com-
bines a Matlab Simulink and on the other hand a parametric Catia model.  
Within this tool, the later described concept of a highly dynamic hydraulic feed  
axis is implemented in the Catia Model as well as in the Matlab Simulink model. At 
first, the requirements resulting from the work piece and the process, e.g. cutting 
force, dynamic, traverse path and stiffness, are inserted. With these parameters, the 
Matlab Simulink model calculates the design parameters, such as the sizes of the hy-
drostatic pockets of the guidance system, the piston diameters and the required tubing. 
Within the Matlab Simulink model the behavior of the feed axis is implemented ana-
lytical. The equations are obtained by using the pressure build-up equation. The hy-
draulic and mechanical parts are linked via the equilibrium of forces. For the piezo 
electric seat valves the mechanical and electrical equations are linked via the pressure 
build-up equation with the piston. In the first step not all piezo electric effects could 
be reproduced within the model possibly resulting in an inaccuracy of the model. In 
the next step these effects will be considered and the simulation model will be harmo-
nized with the actual feed axis.  
With these design parameters, the parametric Catia model is adjusted. The moving 
masses, the outer dimensions, the drawings as well as the resource consumption can 
be derived from this model. With the Catia model, FEM Simulations, e.g. with Ab-
aqus can be started for obtaining the systems eigenfrequencies as well as the materi-
al’s stresses. After transferring those parameters back to the Matlab Simulink model, 
the dynamic and control investigations can be executed. In the end, control parame-
ters, system behavior and energy consumption can be calculated. On one hand, the 
Matlab Simulink model offers a set of control parameters but on the other hand, also 

598 
J. Bauer and J. Fleisc
enables an offline optimiza
and scaling tool, an opt
manufactured. 
Fig. 3. Struct
Further research will fo
mining the significant influ
and resource consumption 
variation of the parameters 
evaluated. The aim of this a
determine the limits of bui
control parameters.  
Another focus will rest o
overall accuracy of the feed
sults for improving the co
Therefore, the model’s com
the significant parameters d
3.2 
Overall Concept of
The feed axis was develop
process is outlined in detail
of a hydrostatic guidance sy
ty enabling compact feed 
guidance, for propulsion as
Since equal stiffness in bo
both piston chambers are of
in standstill, further leads to
Since friction based effects 
to a continuous leak of oil
maintain the pressure. For t
cher 
ation of the parameters. With the results of the simulat
timized feed axis targeted to the work piece can 
 
ture and usage of the simulation and scaling tool 
cus on the design of an experimental approach for de
uence parameters on the feed axis build space, the ene
as well as the control parameters. Therefore, a system
will be executed and the significance of the parameter
approach is to further miniaturize the feed axis, but also
ild space, energy and resource consumption as well as 
on enabling the simulation and scaling tool to increase 
d axis. This is envisioned by actively using simulation
ontrol behavior and error compensation of the feed a
mplexity will be significantly reduced by concentrating
drawn from the design of experiment. 
f the Feed Axis 
ped and optimized using VDI Guideline 2221 [10]. T
l in [7]. The demand for high damping [7] leads to the 
ystem. Furthermore, hydraulics features a high force den
units. The multifunctional use of the hydraulic fluid 
 well as for temperature control enables high compactn
oth axial directions is required, the piston surface areas
f the same size. The demand for high stiffness, in particu
o equal pressure in both piston chambers during stands
have to be excluded, all seals are realized by gaps, lead
. Therefore, oil must be continuously supplied in orde
this reason, the control valves are integrated into the ret
tion 
be  
eter-
ergy 
matic 
rs is 
o to 
the 
the 
n re-
axis. 
g on 
The 
use 
nsi-
for 
ess. 
s in 
ular 
still. 
ding 
er to 
turn 

 
Design Approach f
flow of the actuator. This 
edges control. Adjustable th
ensure that an opening of th
the piston chambers.  
Fig. 4. 
Since moving masses ha
nary actuator housing and 
housing also comprises the 
tatic guidance system is alm
of all guidance systems. In
are reduced, realized by pi
grated into the piston rod (F
4 
Results 
In the following, the result
feed axis is prototypically 
valves (HAWE, EMP-21-V
Fig. 5. Func
for an Adaptable Highly Integrated Hydraulic Feed Axis 
control principle can be characterized as a two-meter
hrottles have been integrated into both supply aperture
he valve does not lead to an uncontrolled flow of fluid i
 
Design concept of the hydraulic feed axis 
ave to be reduced, the developed concept features a sta
piston rod with the moving piston housing. The pis
hydrostatic guidance system and the carriage. The hydr
most friction free and features the highest possible damp
n order to increase control dynamics, the fixed oil colum
iezoelectric proportional seat valves that are directly in
Fig. 4). For more information see [7]. 
ts of the characterization of the feed axis are shown. T
realized out of standard parts with standard proportio
V) (see Fig. 5).  
 
ctional prototype, build out of standard parts [7] 
599 
ring 
s to 
into 
atio-
ston 
ros-
ping 
mns 
nte-
The 
onal 

600 
J. Bauer and J. Fleischer 
The control principle is the same as specified above. This realization enables a first 
characterization of the feed axis behavior. Measurements have shown [7] that this 
type of axis features no friction based effects such as the stick-slip effect. For this 
reason, it was positioned slowly while measuring velocity and position. Furthermore, 
the stiffness of the feed axis was determined. It features a maximum stiffness of 250 
N/µm at a hydrostatic pressure of 2 MPa. This is considered to be sufficient for micro 
machining. 
In order to achieve a small positioning error, a control system was implemented. 
The controller is a PID controller with anti-windup [8] measures to control the inte-
grator. Fig. 6 shows the response of the controlled system for a stepwise increase of 
the carriage set position. It can be stated, that the feed axis achieves stationary accura-
cy of less than 10µm. The agitated characteristics can be explained by possible stick-
slip effects and nonlinearities within the standard valves. An increase in dynamics as 
well as an improvement in control characteristics is expected by using the highly dy-
namic, piezoelectric direct seat valves comprising an integrated hydrostatic guidance 
system.  
 
Fig. 6. Stepwise increase of the carriage’s set position 
An approach similar to the DIN ISO 230-2:2006 guideline [9] was applied to de-
termine the positioning error. Therefore, eleven different positions were approached 
and the distance between actual and set position was determined (xi in Fig. 7). The 
measurement was conducted with a dSpace system using an integrated incremental 
glass scale featuring a resolution of 0.4µm. The sampling rate was set to 1000Hz. The 
measurement was conducted as specified in the guideline: The positions were ap-
proached from one side left to right. After the position at the outer left at the stroke 
boundary was approached, the scheme was applied from right to left. The whole cycle 
was conducted 5 times which equals a total of 110 measurements, five measurements 
at one position approached from right to left as well as from left to right were con-
ducted. The position deviation was evaluated after the feed axis was in standstill for 
5s. During measurement, the temperature was monitored. A temperature of 43°  

 
Design Approach for an Adaptable Highly Integrated Hydraulic Feed Axis 
601 
Celsius was not exceeded. It can be stated, that the axis shows temperature stability 
after the heating up. All measurements were conducted after warming up to this tem-
perature. As outlined in the guideline, the measurement is evaluated statistically al-
lowing the determination of the positioning error based on statistics.  
It can be noted, that the feed axis features a bi-directional positioning error of 8µm 
(see A), a bi-directional positioning repeatability of 7.2µm (see R) and a mean bi-
directional positioning error of an axis of 2.48µm (see M, Fig. 7). For more details the 
guideline DIN ISO 230-2:2006 has to be considered. For micro machining 8 µm is 
not sufficient. The non-satisfying positioning error is due to the used seat valves 
which are not optimal for this purpose. The integration of the piezo electric seat 
valves promises improvements in positioning errors. 
 
Fig. 7. Positioning error similar to DIN ISO 230-2:2006 [9] 
Furthermore, an optimization of the feed axis build space was conducted. The new 
designed apparatus features a moving mass that decreased to one third and a build 
space that decreased to about one half of the shown functional prototype. This appara-
tus features the main parameters such as the prototype, e.g. piston diameter and  
traverse path. Since this design is far more compact than the shown prototype, stiff-
ness as well as dynamics are supposed to be improved significantly. 
5 
Outlook 
Further research will focus on an alignment of the simulation and scaling tool to the 
real prototype. Other major issues are the currently used seat valves. They will be 
replaced by the above mentioned piezoelectric seat valves. Due to the better control-
lability of the valves, an increase in control dynamics as well as in control accuracy is 
expected. Moreover, the designed concept will be manufactured and then characte-
rized. Also, further optimization, in particular on resource and energy consumption, is 

602 
J. Bauer and J. Fleischer 
required. In addition, it is planned to enable the simulation and scaling tool for online 
control adjustment to improve the overall system accuracy. 
6 
Conclusion 
This paper presents a new concept for a hydraulic feed axis. As required, this feed 
axis does not feature any friction based effects such as the stick-slip effect and pro-
vides high damping due to the hydrostatic guidance system. In parallel, a simulation 
and scaling tool is presented, enabling a targeted design, optimization, manufacturing 
and commissioning of such a feed axis.  
Acknowledgement. This paper is based on investigations of the collaborative  
research program SPP1476 which is kindly supported by the German Research Foun-
dation (DFG). 
References 
1. Schubert, A., Neugebauer, R., Schulz, B.: System concept and innovative component de-
sign for ultraprecision for assembly processes. In: Towards Synthesis of Micro-/Nano-
Systems 2007, Part 2. Springer (2007) 
2. Wulfsberg, J.P., Grimske, S., Kohrs, P., Kong, N.: Kleine Werkzeugmaschinen für kleine 
Werkstücke - Zielstellungen und Vorgehensweise des DFG-Schwerpunktprogramms 1476. 
wt Werkstattechnik Online, Jahrgang 100 11/12, 886–891 (2010) 
3. Altintas, Y., Verl, A., Brecher, C., Uriarte, C., Pritschow, G.: Machine tool feed drives. 
CIRP Annals - Manufacturing Technology 60, 779–796 (2011) 
4. Bosch Rexroth, A.G.: Handbuch Lineartechnik. Schweinfurt (2006) 
5. Altintas, Y., Brecher, M., Weck, M., Witt, S.: Virtual Machine Tool. CIRP Annals - Man-
ufacturing Technology 54(2), 115–138 (2005), 
http://www.sciencedirect.com/science/article/pii/ 
S0007850607600225 
6. Munzinger, C., Krausse, M., Kipfmueller, M.: Simulation of parallel kinematic machine 
toolswith minimal effort. Production Engineering 4(Heft/Band Issue 5) (2010) 
7. Fleischer, J., Bauer, J.: Highly integrated piezo-hydraulic feed axis. In: 5th CIRP Confe-
rence on High Performance Cutting 2012, Zurich, Switzerland, Band, June 4-6, Procedia 
CIRP 1, pp. 342–346 (2012)  
8. Geering, H.P., Shafai, E.: Regelungstechnik II. Scriptum control systems II, Institute for 
Dynamic Systems and Control, ETH Zurich, p. 8 (2004), 
http://www.imrtweb.ethz.ch/users/geering/ 
HPG-ES-RT2-Aufl2.pdf 
9. DIN ISO 230-2:2011-11: Test code for machine tools - Part 2: Determination of accuracy 
and repeatability of positioning numerically controlled axes (ISO 230-2:2006), Beuth 
(2011) 
10. VDI Guideline 2221: Systematic approach to the development and design of technical sys-
tems and products. VDI-Fachbereich Produktentwicklung und Mechatronik (1993) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 603–612. 
DOI: 10.1007/978-3-642-30817-8_59     © Springer-Verlag Berlin Heidelberg 2013 
Automated Configuration of a Machine Simulation  
Based on a Modular Approach 
Michael Weyrich and Frank Steden 
Institute of Production Engineering, University of Siegen, Paul-Bonatz-Straße 9-11,  
57068 Siegen, Germany 
{frank.steden,michael.weyrich}@uni-siegen.de 
Abstract. In this paper a module-based methodology for the automated confi-
guration of machine simulation is presented. The aim is the fast generation of 
different simulation models for control tests and virtual commissioning which 
are based on a module database. This approach can be integrated into current 
engineering processes of machine builders. Depending on the degree of detail 
of the module, the simulation can be used in individual engineering steps. 
Therefore, it is not necessary to have completed engineering data. The metho-
dology comprises several steps beginning with the systematic module identifi-
cation based on the machine components and its functions; followed by the 
evaluation and assessment of the identified modules. Finally the modules are 
modeled and validated for the automatic or manual configuration of simulation 
models from a module database. The impact of the methodology is examined 
exemplarily on a production machine. 
 
Keywords: Integration of Virtual Machine Engineering, Modularization, Man-
agement in Product Development, Virtual Commissioning. 
1 
Introduction 
Complex requirements from customers, increasing cost pressure and short delivery 
times present major challenges for the manufacturing of machine tools and production 
machines. In order to meet these challenges machine builders employ extensible ma-
chine platforms or construction kits. Additionally, the employment of machine simu-
lation for the control test and virtual commissioning responds to these challenges. 
With simulation early engineering steps can be reviewed. Concepts can be compared 
and early errors such as design errors can be detected. In subsequent steps the control 
software can be tested and pre-optimized before commissioning. The quality of the 
machines can be improved and single engineering steps are parallelized towards si-
multaneous engineering. Hence, the employment of machine simulation leads to a 
more efficient engineering process. 
The major obstacle in the application of machine simulation into company’s prac-
tice is the expense for the simulation modeling process. Different simulation models 
are created from mechanic, electric and control data for different tests in the life cycle 

604 
M. Weyrich and F. Steden 
of a machine [1, 2]. Another problem states the consistency of the data, as data is 
changed frequently and its level of detail changes during the engineering due to se-
quential process steps. Current approaches for the automated configuration of a ma-
chine simulation mainly base on multipurpose language which collects and transforms 
engineering data. It is complicated to put these approaches into the current engineer-
ing process. Not only the incomplete data, but also the lack of practical experience 
with those languages forms a problem. 
A module-based approach facilitates the creation of different variants of machine 
simulation, even if the data is incomplete. The modules required for this purpose must 
have specific properties. Based on the respective machine concept with its compo-
nents and functions, scalable and configurable modules can be combined easily and 
reused for other simulation projects. 
2 
State of the Art 
Simulation models are used to validate and compare different machine concepts at an 
early stage in the engineering process. Furthermore the control programs can be tested 
and a virtual commissioning can be executed. Therefore, it is necessary to create a 
functional performance model of the machine. Additionally, often a 3D-model has to 
be created to support special tests e.g. collusion tests. The simulation models are 
based on the data of mechanical, electrical and control engineering fields. 
Simulation models are often developed only for a single application and for a par-
ticular purpose. New requirements demand that the simulation models have to be 
reused for different purposes along the engineering and machine life-cycle [2]. There-
fore, it is necessary to create different variants of simulation models easily [1]. The 
economics of a simulation project can be measured with current approaches. It de-
pends, among other parameters, on the type of simulation creation and on the level of 
detail of the simulation model [3]. Simulation modeling can be carried out manually, 
semi-automatically or fully automated, whereby the manual modeling causes the larg-
est cost and must be carried out by simulation experts. In contrast, the implementation 
of the automated creation of a simulation model is very complex. 
Many approaches for the automatic creation of simulation models use standardized 
description language for the compilation and transformation of the required engineer-
ing and process data. For process plants, an approach for the automated creation of 
simulation models, which can be sufficiently accurate and complete to be used for 
control tests or virtual commissioning, is presented [4]. In order to transform the en-
gineering information, the description language XML is applied and used for the 
functional model of the system. Another approach describes the automatic creation of 
simulation modules for production machines [5]. In comparison to the approaches for 
process plants, 3D-models of the production machine have to be created and linked to 
a functional model. Fundamentals of this approach are the actual structure of the ma-
chine and its complete circuit diagrams. These approaches show successfully the pos-
sibilities for an automated creation of a simulation model. However these approaches 

 
Automated Configuration of a Machine Simulation Based on a Modular Approach 
605 
get complex in the implementation if for example the engineering data is incomplete 
or missing or because of unskilled personal. 
Another way to increase the efficiency and economics in simulation projects is the 
reuse of existing models and sub-models [1]. In research projects, the requirements on 
engineering systems for the processing of reusable models are analyzed [6]. In the 
VDI guideline 3633 the complete and partial reuse of simulation models is recom-
mended, but specifications of the modules properties are not discussed in detail [7]. 
First modular approaches demonstrate the use of simulation modules [8]. The simula-
tion modules are based on mechatronic engineering data such as circuit diagrams, 
fluid plans and 3D-CAD data. A special benefit of simulation modules is the ability to 
integrate this approach into current engineering processes [8]. Those modules are 
used for the automatic or semi-automated creation of simulation models. The semi-
automated modeling is done with a simulation kit. This construction kit contains basic 
function blocks which can be combined and completed for the simulation model of a 
machine. 
Other approaches show the benefits of using mechatronic modules in the engineer-
ing process and propose a component-based approach for the creation of simulation 
models based on these mechatronic modules [9-11]. 
Existing modularization strategies such as design structure matrix or the modular 
function deployment are only conditionally suitable for the identification of simula-
tion modules [12]. One reason is the limited degree of freedom in the determination of 
the modules. The machine concept and certain other conditions are already deter-
mined at the time of the module identification. In addition the technological capabili-
ties of the simulation software and the integration of the control software can only be 
partially considered in existing modularization strategies. 
3 
Motivation 
A recent study based on the survey "Implementation of the kinematic simulation in 
several engineering processes"1 has been conducted in 2012 at the Institute for Pro-
duction Engineering of the University of Siegen. Manufacturing companies of custom 
machine tools and special purpose machines have been interviewed about their engi-
neering processes, their application and connection of CAx-systems and their special 
requirements for the kinematic simulation. 
A striking result is the cognition that the engineering process is still dominated by 
the mechanical design construction and sequent process steps. Timely delayed, the 
mechanical construction is followed by the electrical design and the development of 
the control program. Instead of a mechatronic engineering process with comprehen-
sive parallel work, a special kind of simultaneous engineering is applied where single 
steps follow each other delayed. The development and engineering of customized 
machine tools or special purpose machines are mainly based on reference plants and 
modular construction kits which can be complemented by new designs of subsystems 
(Fig. 1a). 
                                                           
1 Number of evaluable responses n=14. 

606 
M. Weyrich and F. Steden 
 
Fig. 1. Statistical analysis of the surveys’ results 
The companies surveyed implement different CAx-systems in the mechanical and 
electrical engineering fields. The selection of these systems is based on special de-
mands, this is why no dominating system is detected. The use of 3D-CAD data is also 
important for the modeling of a kinematic simulation. Especially for the collision 
analysis of e.g. moved axis in the machine these data are necessary. All surveyed 
companies employed a 3D-CAD system, on top of that more than 30% of the compa-
nies are still using 2D-CAD systems for the engineering of sub systems (Fig.1 b). 
This can be explained by the use of 2D-drawings of reference plants, which can be 
easily changed but still are difficult in transforming into 3D-drawings. A fast and easy 
data exchange is necessary for an automated update of the simulation model, but the 
networking of the different CAx-systems with each other or with ERP or MES-
systems is implemented only fragmentarily. 
More than 66% of the surveyed companies already used the kinematic simulation 
for selected projects. The applied software is chosen according to the goals of the 
simulation of the company and also depends on the machine concept to be tested. In 
comparison to the study in 1997 where the integration of the simulation was a future 
trend [13], today the added value of the simulation is not mainly seen in the design 
review and concept tests, but in the virtual commissioning of the machines and in the 
early tests of the control programs. Nowadays simulation topics are maintained equal-
ly by the mechanical design and the control programming. 
The main advantages of using simulation techniques are primarily the shortening 
and quality improvement of individual process steps. The largest barrier for the appli-
cation of simulation (stated by 65% of the companies), far in front of the cost aspects, 
is the effort to model the simulation (Fig.1 c). This is consistent with a survey from 
2005 where 61% demand reusable models to decrease the modeling costs [14]. A 
methodology for identifying such reusable simulation modules will be presented in 
the following chapter. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
New
Construction
Construcion
based on
references
Use of
construction
kits
0%
20%
40%
60%
80%
100%
2D-CAD
3D-CAD
0%
20%
40%
60%
80%
100%
Modeling
Effort
Others
(a)
(b)
(c)

 
Automated Configuration of a Machine Simulation Based on a Modular Approach 
607 
4 
Modular Approach for the Configuration of a Simulation 
Model 
The presented methodological approach includes the identification and the assessment 
of reusable simulation modules. The aim is to use these modules for an easy manual, 
or automated configuration of specific simulation models based on a data basis. The 
approach depicted in Fig.2 is divided into five steps. 
 
Fig. 2. Overview of the modular approach and an example of a machine concept 
First, each step of the approach is explained in general and thereafter deepened in 
detailed aspects on the practical machine example of the Hekuma GmbH company. 
Their fully automated machines are for the manufacturing of plastic products and 
comprise the functions plastic powder injection, transport of the work pieces, assem-
bly of parts, quality control and packaging of finished products. For controlling dif-
ferent PLC-systems are employed. 
4.1 
Framework and Domain Specifically Issues 
The first step forms the basis for the subsequent domain-specific identification and 
assessment of the modules and takes special reference to the company’s particular 
situation. In this step the basic indication factors have to be determined and matched. 
This should be done for every simulation project by an interdisciplinary team of com-
pany’s experts.  
These factors depend on the different goals for the implementation of simulation, 
the control system to test and the machine concept. The factors have to be defined and 
prioritized in advance. In this example the specific objectives are: the early control 
tests of sub-modules of the machine as well as of the entire system, collision detection 
between the moving axes and a comparison of different conceptual options which 
determine the optimal cycle time. The early testing of the individual modules based 
on detailed engineering data is not possible in this example. As at the time of testing, 
these data are not all available or they are changed permanently during the engineer-
ing process. Here the test on the basis of reusable and amendable simulation is of 
advantage. 
1. Framework and domain specifical issues
2. Module 
Identification
3. Module 
Assessment
4. Modeling, Validation & Implementation
5. Configuration of machine simulation
models from data base

608 
M. Weyrich and F. Steden 
The defined and prioritized objectives of the simulation influence the choice of the 
simulation software. Further influences come from the applied control-system and the 
use of engineering data e.g. 3D-CAD data. Rarely in simulation projects completely 
new developed machines are tested. It is more common that new machines are devel-
oped and tested based on existing machine concepts. Therefore, the present machine 
concept has also to be considered for the identification of reusable modules. In the 
given example, the machine concept is based on an expandable construction kit. The 
customized machine is then only a combination of different components of the con-
struction kit and for specific requirements components are changed or newly con-
structed. 
4.2 
Module Identification 
The second step is the actual identification of the simulation modules. For the known 
machine concepts, existing machines are compared along their material flow in order 
to identify similar or recurring components. The functions and geometries of these 
components form the simulation modules. An initial rule for the identification process 
is that the modules should be large and functionally comprehensive. Such modules 
can be quickly and easily used for the creation of new simulation models. A positive 
side effect is the simplified data management. On the other hand, too large modules 
are complicated to scale and to alter; this is why they are difficult to handle. The op-
timal size for the application of modules is obtained by an iterative approach which 
comprises the steps module identification and their subsequent assessment. 
In the application example, the recurring modules are the plastic molding machine, 
robot systems, transport axes and quality control systems. For the simulation project 
not all functions of these modules are relevant. Therefore, the modules are divided in 
their functions and required 3D-geometries. The aim is to focus on simulation-related 
functions and geometries, and consequently reducing the complexity of the modules 
early in the approach. 
 
 
Fig. 3. Module identification process 
2. Module Identification Process
Established Machine Concepts
New Machine Concepts
Comparison of different 
machines along material flow
List of function and geometry
dependance
Module Identification
Rule: Identify comprehensive modules!
Module reduction to relevant functions and 3D-CAD data

 
Automated Configuration of a Machine Simulation Based on a Modular Approach 
609 
In the case of new machine concepts, the individual machine functions have to be 
determined first. Therefore, the simulation-related functions and geometries are listed 
and then evaluated according to their dependence. This can be done with different 
modularization strategies e.g. the design structure matrix. Functions with a strong 
dependency form a simulation module. The initial rule to identify comprehensive 
functional modules is the same as mentioned above. In Figure 3 the process of the 
module identification is shown. 
4.3 
Module Assessment 
The identified modules have to be assessed in order to determine their suitability for 
the domain-specific simulation task. For the assessment, the modular indication  
matrix and its module drivers are employed [12]. Especially the following module 
drivers can be adapted for the assessment of specific simulation aspects: 
• Carry over: Module is based on an existing module. 
• Technical specification: Module is changeable for customers’ needs. 
• Common unit: Module can be used for different machine concepts. 
• Separate testing: Separate testing and validation of single modules. 
The adapted module drivers consider the established objectives and framework of the 
simulation project for the assessment of the modules. The associated weightings have 
to be determined by the mentioned team of experts. It is important to keep the weight-
ing for different modules equal to enable the comparison of modules with similar 
functionality and also for the assessment of different variants of modules. 
The result of the assessment is based on the total score of the module. The higher 
the score the better is the suitability for the simulation project and for its reuse. How-
ever, there are elimination criteria that force to a re-identification process. If a single 
high weighted factor is rated low or zero, the module has to be changed. An example 
of the assessment of a specific module is shown in table 1. This module gets a total 
score of 43 out of 50 points.  
Table 1. Assessment matrix of a simulation module 
Assessment Factor 
Score 
(0-5) 
Weighting 
(1-3) 
Sum 
Separate testing 
0 
3 
0 
Implementation in simulation system 
5 
1 
5 
Scalability 
3 
2 
6 
… 
… 
… 
… 
Total Score 
 
 
43 
Considering only the total score, the assessment is positive and the module passes 
to the following steps. As in this example the high weighted factor “separate testing” 
is rated with zero, it states the elimination criteria for this module. The module has to 
be changed and be rated again. 

610 
M. Weyrich and F. Steden 
After the assessment of a module, another iteration loop for the identification fol-
lows. A negative assessment leads to changes in the module as long as all require-
ments are fulfilled; whereas after a positive assessment modules can be modeled and 
implemented in a database. The change of the module can be done by combining or 
splitting the module or by adding or removing functions. The number of iterations for 
each module depends on both, the assessment and the characteristics of the individual 
module.  
4.4 
Modeling, Validation and Implementation 
After a positive assessment, the modules can be modeled and implemented in a data-
base for further application and for the creation of simulation models. Due to the dif-
ferent number of identification and assessment loops, the modules have different sizes 
and different comprehensive functions. 
The modeling focusses on relevant functions of a module. For example the model-
ing of the module plastic molding machine can be reduced significantly. Related  
functions such as opening and closing of the mold have been mapped while internal 
functions have not been modeled. The effort of modeling also depends on whether the 
individual functions or variables are modeled scalable. An example for a scalable 
variable is the speed of the mold motion. On the one hand the scalable modeling 
means greater expenditure of time, on the other hand it allows a simpler alterability of 
the module. Further module properties are depicted in Fig. 4. 
 
 
Fig. 4. Module properties 
The modeled and pre-validated modules can be used for both manual and auto-
mated creation of simulation models. In this example, the database system of the  
simulation software is used for storing the created modules. In addition, it is also 
possible to use an external databases which are connected to the simulation software. 
5 
Discussion 
The quality of the identified modules is mainly pre-defined by detailed constrictions 
of the company-specific initial situation in the first process step. The identification- 
and assessment process can be conducted the better the specification and the aims are 
stated. In this status a suitable simulation system is chosen. The consideration of an 
Simulation 
module
properties
Reusable
Link to other
modules
Link to
control
system
Changeable
level of
detail
Consistent to
engineering
data

 
Automated Configuration of a Machine Simulation Based on a Modular Approach 
611 
interdisciplinary team eases the integration of simulation and its modules in the engi-
neering process. On the one hand each participant can state his specific requirements 
and on the other hand he should know about the potentials and restrictions in the  
simulation project. 
The presented approach aims to identify the reusable simulations modules for the 
creation of specific simulation models. The stored and pre-validated modules in the 
database are combined to machine simulation models, whereby the combination may 
take place manually or automatically. The reuse of the simulation modules can be 
realized in various ways: 
• direct reuse and recombination of modules; 
• scaling and resetting of the variables of the modules; 
• model fitting of the modules; 
The recombination of the modules enables the reuse of modules without the addition-
al effort. With existing and scalable simulation modules, similar components can be 
simulated faster. An example might be the simulation module of a robot system which 
can be adapted to the specification of various robots and robot systems. By the fast 
scalable and resettable modules, a simulation in the early stage is also possible with-
out engineering-data. The effort of rescaling existing modules is proportional to the 
degree of adjustment and can vary in individual cases. Depending on the effort, it 
might be reasonable to re-model the module or even to renew the identification and 
assessment phase. The constantly growing database eases the simulation modeling. 
The modeling and validation of simulation models ask for profound knowledge of 
simulation systems. Hence, these are executed by trained specialists. In comparison 
the modeling of the simulation model on pre-modeled and pre-validated modules 
require less expertise and can be performed by an operator of the simulation task. This 
fact promotes the integration of the simulation in the existing engineering process. 
6 
Conclusion and Further Work 
The presented modular approach enables and eases the generation of variants of  
machine simulation. The separation of the module-modeling and the creation of the  
simulation model from a database contribute to model-driven development. Conse-
quently the application of models in the engineering process is supported. The pre-
sented approach can also be employed on fragmented engineering-data because of the 
adaptability of the single modules. This is also the reason why the simulation model 
of a machine can be adopted and refined during the engineering. 
The scalability and resetting of the modules are striking factors for the reusability. 
The presented example shows that same modules can be identified and modeled with 
different degrees of scalability. The aim is not to have the maximum degree of scala-
bility, but to find the balance between scalability and the modeling effort. A similar 
problem results in the automated simulation configuration. Here, the implementation 
often requires additional software and the maintenance of the data base is also  
complicated. It has do be discussed if and how a marginal limit can be set to the  
automated configuration of simulation models. 

612 
M. Weyrich and F. Steden 
References 
1. ElMaraghy, H.A. (Hg.): New Methods to Create Variants of 3-D Simulation Models of 
Manufacturing Systems. Springer, Heidelberg (2012) 
2. Mayer, G., Spiekermann, S.: Life Cycle of Simulation Models – Requirements and case 
Studies in the automotive industry. Journal of Simulation 4(4), 255–259 (2010) 
3. Wünsch, G.: Methoden für die virtuelle Inbetriebnahme automatisierter Produktionssys-
teme. Utz Verlag, München (2008) 
4. Barth, M.: Automatisch generierte Simulationsmodelle verfahrenstechnischer Anlagen für 
den Steuerungstest. VDI-Verlag Düsseldorf, Hamburg (2011) 
5. Kufner, A.: Automatisierte Erstellung von Maschinenmodellen für die Hardware-in-the 
Loop-Simulation von Montagemaschinen. Jost-Jetter-Vlg., Hemsheim (2012) 
6. Maga, C., Jazdi, N., Göhner, P.: Requirements on Engineering Tools for Increasing Reuse 
in Industrial Automation. In: 16th IEEE International Conference on Emerging Technolo-
gies and Factory Automation, ETFA 2011, Toulouse, France (2011) 
7. N.N.: Verein Deutscher Ingenieure: VDI 3633 – Simulation von Logistik-, Materialfluss- 
und Produktionssystemen (1996) 
8. Schob, U.: Methode zur frühen Inbetriebnahme von Steuerungsprogrammen durch halbau-
tomatische Maschinenmodellbildung. Verlag wissenschaftliche Scripten, Chemnitz (2012) 
9. Weyrich, M., Steden, F., Wolf, J., Scharf, M.: Identification of mechatronic units based on 
an example of a flexible customized multi lathe machine tool. In: 16th IEEE International 
Conference on Emerging Technologies and Factory Automation, ETFA 2011, Toulouse, 
France (2011) 
10. Lüder, A., Foehr, L., Wagner, T., Zaddach, J.-J., Holm, T.: Manufacturing System Engi-
neering with Mechatronical Units. In: IEEE Conference on Emerging Technologies and 
Factory Automation, ETFA (2010) 
11. Weyrich, M., Steden, F.: Prozessintegration von Maschinensimulation zur Steuerungsinbe-
triebnahme bei Maschinenherstellern. In: Adolphs, P. (Hg.) Automation 2011, pp. S.339–
S.342. VDI Verlag, Düsseldorf (2011) 
12. Ericsson, A., Erixon, G.: Controlling design variants. Modular product platforms. Society 
of Manufacturing Engineers. ASME Press, Dearborn, New York (1999) 
13. Reinhart, G.: Simulation - Schlüsseltechnologie der Zukunft? Utz, Verl. Wissenschaft, 
München (1997) 
14. Bierschenk, S., Kuhlmann, T., Ritter, A.: Stand der digitalen Fabrik bei kleinen und mit-
telständischen Unternehmen. Fraunhofer-IRB-Verl., Stuttgart (2005) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 613–622. 
DOI: 10.1007/978-3-642-30817-8_60 
© Springer-Verlag Berlin Heidelberg 2013 
 
System Design of PLC-Controlled Specialized  
Production Machines  
Gernot Frank1, Engelbert Westkämper1, Wolfgang Schlögl2, and Matthias Lenord2 
1 GSaME, University of Stuttgart, Nobelstrasse 12, Stuttgart, 70569, Germany 
2 Siemens AG, Gleiwitzer Str. 555, Nuremburg, 90475, Germany 
gernot.frank@gsame.uni-stuttgart.de 
Abstract. PLC-controlled specialized production machines are usually em-
ployed to automate customer-specific production processes. Sequential engi-
neering processes are frequently used for designing these machines. To reduce 
the time and effort, parallelization of the different disciplines as well as increas-
ing the reuse of already designed modules has to be considered. Both of these 
activities originate in the system design phase. Therefore, the sequential design 
process is analyzed to define the information required for starting detailed engi-
neering in parallel. Additional requirements in the system design phase are de-
rived from the characteristics of the system design phase itself and the special 
situation when designing specialized production machines. A method how these 
requirements can be fulfilled is shown based on these requirements.  
Keywords: PLC, System Design, Systems Engineering, Specialized Machines, 
simulation, modularization. 
1 
Introduction 
Shorter product lifecycles result in the challenge of achieving short times-to-market 
for new products. Further, the individualization of products is increasing and ma-
chines are needed to manufacture these products. Based on the individualization of 
the products, the manufacturing technologies employed have to be adapted. This turns 
into challenges for machine builders to deliver customer-specific machines in a short-
er time. When using a sequential engineering process, the different discipline-specific 
phases cannot be shortened furthermore. However, to still reduce time, the various 
phases could be parallelized.[11]. Another approach that some machine-builders are 
using is a modular-machine design, which separates the development process into a 
module development process and an order-dependent development process. [3] In this 
case, the machine is individually adapted to the manufactured product by combining 
different modules. For this reason, a minimum number of built modules is necessary. 
But some products require high customer-specific production technology, which can-
not be achieved by just combining different modules. Therefore, specialized produc-
tion machines are built. Based on the very customer-specific production technology 
and the fact that the product is highly dependent on the production process, only one 
or two of these types of machines are built. 

614 
G. Frank et al. 
2 
Current Engineering Process for Specialized Production 
Machines  
Specialized production machines consist of purchased components such as motors, sen-
sors and mechanical parts, designed by the machine builders themselves. To automate the 
machine, programmable logical controllers (PLCs) are used. The current design process 
starts by defining the customer’s requirements regarding the machine. Here, the most 
important requirement is the specified cycle time for executing the manufacturing 
process. Based on this, mechanical engineers start by drafting possible solutions to fulfill 
these requirements. They are usually sketched by hand and implicitly contain information 
about the actions required, rough geometrical descriptions of parts, the first definition of 
assemblies, kinematic dependencies and module states during operation. This definition 
of a principle solution has a significant impact on the detailed engineering in other discip-
lines [2]. The next phase of the mechanical design process is characterized by detailing 
different geometrical parts in a mechanical CAD system (MCAD), where assembly 
groups are first defined. This detailed engineering goes hand-in-hand with defining the 
kinematics, the drive concept, the moved masses and the kinematic states within the 
process. The actuators to be ordered are calculated from these definitions and the re-
quired motion time between the kinematic states. The last mechanical engineering phase 
involves the detailed engineering of the different parts and assemblies. Specifying the 
sensors and actuators to the electrical department marks the transition from the mechani-
cal engineering to the electrical engineering department. Here, the electrical engineers 
create what is known as a functional structure in an electrical CAD system (ECAD) and 
add the sensors and actuators specified by the mechanical engineering department. In this 
phase, each sensor and actuator can be directly mapped to symbols in the circuit diagram. 
Further, the engineers must know the sequence of operation for the sensors and actuators, 
e.g. if two pneumatic cylinders should extend at the same time; however this can only be 
controlled if there is a common valve for both cylinders. Based on this information, the 
electrical engineers draw circuit diagrams. Once all of the signal interfaces of the devices 
being used and the mechanical results (mechanical design, kinematic information etc.) 
have been transferred, the automation department starts to define the various machine 
states. These states are generally kinematic states, e.g. “cylinder is extended” and the 
associated transitions, e.g. “cylinder extends”. Further, the transition conditions between 
the states are defined using the signal interfaces of the sensors being specified by the 
mechanical and electrical engineering departments. Detailed engineering adds additional 
states and transition conditions along with safety functions such as interlocking condi-
tions. The machine is finally assembled and commissioned. As mentioned previously, the 
decision regarding the principle solution has a significant impact on all of the disciplines 
involved. To obtain the best “mechatronics” solution, all disciplines should be involved 
in the system design phase [8]. Based on the individual customer requirements and the 
situation, the mechanical engineers frequently do not know whether a module has already 
been developed that can be used in this project assemblies, once developed, are infre-
quently reused [10]. The decision regarding the ability to reuse modules is also made 
directly by defining the principle solution. To reduce time within such a process, it 
should be possible to reuse modules as well as engineer new modules in parallel. To 
achieve both of these objectives, a method for designing the concept is needed before the 
various disciplines start detailed engineering in parallel.  

 
System Design of PLC-Controlled Specialized Production Machines 
615 
3 
Engineering and Design Approaches for System Design 
Engineering specialized production machines is characterized by developing a mecha-
tronic system, where the mechanical engineering is the leading discipline. In this case, 
the machine itself can be seen as the product of a machine builder. [13] suggests a 
development method for mechatronic systems, which is divided into three main phas-
es. It starts with a concept or system design where all of the disciplines are involved, 
followed by a phase where the detailed mechanical, electrical/electronics and software 
engineering is executed in parallel. The final phase is the overall system integration. 
Using this macro process as a basis, this paper focuses on the concept or system de-
sign phase with the requirements that have been derived: Parallelizing disciplines, 
facilitating the reuse of mechatronic assemblies of the machine and considering the 
cycle time specified by the customer. Further, [13] explains a general method more 
specifically for the system design. Based on the requirements, the crucial problems 
are abstracted, a functional structure is drawn up and solutions for these functions are 
searched for. The main solutions are then evaluated and an interdisciplinary concept 
of a general mechatronic system is defined. This basic procedure is also used by [9] 
and [14] for system design, but with the focus on the mechanical engineering. Regard-
ing the functional structure there, it is defined as functions connected to each other 
through energy, information or material flow. Based on this, principle solutions are 
identified for the different functions, combined and then a decision about the principle 
solution is made. [1] adapts this macro process to the development of PLC-controlled 
machines, by adding transition conditions between functions to model logical depen-
dencies of the sequence between them. These dependencies and the rules for identify-
ing sensors and actuators as a result of the information and energy flow, enable the 
automation department to start detailed engineering work. [1] also considers modula-
rization of the machine, but not to reuse the various modules in a mechatronic way. 
Instead, it defines work packages for detailed internal engineering work running in 
parallel. Further, defining sensors and actuators based on the functional description is 
too abstract to allow symbols in a circuit diagram to be mapped to them. This 
represents a gap, and the electrical engineering department is not able to start with 
detailed engineering. [6] also mentions that selecting a principle solution based on 
functional descriptions is not sufficient to start detailed engineering. Therefore, what 
are known as aspect models can be used, such as sketches for the mechanical system 
and attached to the principle solution. [6] also introduces assigning a principle solu-
tion directly to a single function. [5] extends this approach by automatically searching 
for the implementation by defining a verb-noun-verb combination and taxonomies for 
them, related to stored principle solutions in a database. The machine building ap-
proaches mentioned in the introduction also use the word function. [17] however indi-
cates that the term function in this context is understood and used differently than in 
the approaches previously discussed. The term function is strongly related to the im-
plementation of a function, such as a conveyor. Certainly, a conveyor is just an ab-
stract description of a component that can be ordered. The abstraction level (the term 
function in these approaches) can be compared with them, which corresponds to the 
name of a principle solution. This level is called the logical level in the following. 

616 
G. Frank et al. 
Similar approaches can be
modules and generating eng
4 
Requirements P
If the situation of the inten
requirements and the need 
tailed engineering processe
straction of the requiremen
the solution search field. T
are described in Chapter 2
start with, it can be seen t
available to start detailed e
ments on the system design
previous sections. An inter
ciplines to participate. The
also has to be given consid
placed on the system design
 
Fig. 1. System 
5 
System Design f
[17] mentions that system 
analysis, the definition of th
tecture. Transferring these 
chine equates to the first ac
be understood as the modul
takes place at the logical lev
e found in [7-8] and [16], where the focus is on reus
gineering data based on libraries.  
Placed on System Design 
nded development process is considered, starting with 
of an interdisciplinary system design phase, ending in 
es for specific disciplines running in parallel, then an 
nt definition is required, which in turn facilitates open
The engineering workflows within the different discipli
2. Based on the information that these workflows need
that a really detailed component-oriented view has to
engineering in parallel. Additionally, the general requ
n phase can be derived from the situation described in 
disciplinary description is required to allow all of the d
e cycle time is a key requirement for such machines, 
deration. Figure 1 is derived from all of these requireme
n phase. 
 
design requirements for PLC-controlled machines 
for PLC-Controlled Specialized Machines 
engineering consists of three activities, the requirem
he functional architecture and the link to the design arc
activities to Fig. 1, defining the requirements for the m
ctivity in system engineering. The design architecture 
larization and the abstract definition of components, wh
vel. This leads to two main stages within the system des
sing 
the 
de-
ab-
ning 
ines 
d to 
o be 
uire-
the 
dis-
and 
ents 
ment 
chi-
ma-
can 
hich 
sign 

 
System Desi
phase: Defining the functio
level. The specific steps wi
described in the following. 
5.1 
Functional Descrip
The functional description 
architecture. It is an interd
intended project and specif
engineers to narrow down 
sign architecture. The first 
of the crucial problem. For 
conveyor and placing it on
product”. Here, an abstract
word “product”, which is a
such as mentioned in [15]. T
ing functionality” is ignore
As a consequence, the “pic
hierarchy, below the “prov
be achieved by asking the 
quirement specification, ad
ucts”. This leads to a functi
 
Fig. 
The typical engineering 
functions with relationships
tion. Some links are quite 
uct” and “placing product”
adding such a relationship b
of possible principle solutio
based on a gripper. In this c
bing product” is realized de
be seen, if the “checking s
gripper can grab the produc
if “checking status” is perf
relationship has to be added
alternative 1 is a potential s
from defining the functiona
is not useful, and an iterativ
ign of PLC-Controlled Specialized Production Machines 
onal architecture and the design architecture at the log
ithin these two levels and the transition between them 
   
tion  
bridges the gap between the requirements and the des
disciplinary description and can be used for analyzing 
fying the interfaces. [17] These characteristics enable 
the general requirements on the machine to create the 
step of defining the functional description is an abstract
example, if the new machine is for “picking a box from
 a machine”, the crucial problem would be “providing 
tion of the term “box” was done by replacing it with 
associated with opening a wide area of principle solutio
The same applies to the word “placing”. Further, the “pi
ed, because the main functionality is “providing produ
cking product” function can be included into a functio
viding product” function. This hierarchical description 
question: “How is the function done?” Based on the 
dditional functions can be included such as “stocking pr
onal hierarchy such as shown in Fig. 2. 
 
 2. Drawing up the functional hierarchy 
methods, such as [1], [6] and [9], interlink the differ
s for three main flow types: energy, material and inform
obvious, such as a material flow between “stocking pr
”. When considering a simple example, it can be seen t
between the different functions narrows down the quan
ons. Figure 3 shows a principle solution for a system tha
case, the relationship between “checking status” and “gr
ependent on the position of the “checking function”. As 
status” function is performed directly at the position, 
ct, the relation of material flow is not required. Convers
formed before this gripping position, then a material fl
d. Depending on the relationship is drawn, decides whet
solution − or not. As a consequence, a top-down-appro
al structure and then evaluating possible principle soluti
ve procedure must be employed.  
617 
gical 
are 
sign 
the 
the 
de-
tion 
m a 
the 
the 
ons, 
ick-
uct”. 
onal 
can 
re-
rod-
rent 
ma-
rod-
that 
ntity 
at is 
rab-
can 
the 
ely, 
flow 
ther 
oach 
ions 

618 
G. Frank et al. 
 
Fig. 3. Dependencies between functions and principle solution (design architecture) 
 
Fig. 4. Gantt chart for breaking down cycle time into functions 
Based on the particular manufacturing process, it is quite easy to define time de-
pendencies between functions without knowing the flow types, e.g. “checking status” 
has to be done before “grabbing product”. Further, the diagrams above do not take 
into consideration planning the cycle time. In this case, the most important informa-
tion concerns which functionalities have to be done in a sequential way or which can 
be done in parallel. This information is provided in the form of Gantt charts. If a solu-
tion, e.g. a barcode-reader for fulfilling the “checking status” function, is known and 
the duration can be assigned, e.g. based on experiences or tests, it is possible to break 
down the required cycle time into functionalities as shown in Fig. 4. As a result of 
what has been identified about the dependencies between the functional architecture 
and the design architecture, the Gantt chart cannot be entirely defined using a top-
down approach. This means that the Gantt chart has to be created iteratively, which 
forces the engineers to think about the cycle time and implementations, which can 
achieve the process step required in the given time slot. This also helps engineers to 
see, e.g. if different functions can be implemented in parallel or if energy from a pre-
vious function can be used (indicated by the blue dashed line in Fig. 4). Regarding  
 
 

 
System Design of PLC-Controlled Specialized Production Machines 
619 
alternative 2 in Fig. 3, assigning the duration to the “material-flow” between the func-
tions “checking status” and “grabbing product” is suitable. However, defining the 
time behavior using Gantt charts requires that this relationship is modeled as a sepa-
rate function so that the duration can be assigned and visualized. As a consequence, 
the three-types of flows are interpreted discreetly with respect to time, but indicate the 
interfaces required for the functions, e.g. energy input or output. 
5.2 
Transition to the Logical Level 
A search is made for principle solutions based on the functional description. This 
could be done manually by using creativity methods [9] or by employing automatic 
searches within databases [5][17]. [17] uses hierarchical semantic definitions of the 
wording.  Identifying different solutions for functions can be visualized in a morpho-
logical box [9]. If automated searches are used, this box can be predefined using a 
software system. However, it should also be possible to add new principle solutions 
based on the creativity of the engineers. When taking a closer look at morphological 
boxes, it is possible to find principle solutions fulfilling different functionalities, e.g. a 
“conveyor” for “moving product” or “stocking product”. Based on this situation, deci-
sions have to be made if different functionalities can be integrated into a single prin-
ciple solution − or if it is useful to reuse the same principle solution for different  
functions. Integrating different functions leads to a n:1 relationship between functions 
and principle solutions. If a function is realized using a separate solution, then this 
represents a direct 1:1 relationship. Regarding modularization, the functional model-
ing and the relationships to principle solutions have to be done in such a way that no 
n:m relationship appears between functions and principle solutions. Otherwise, the 
system borders of the module cannot be identified clearly, which makes it difficult to 
reuse mechatronic modules. After selecting the principle solutions, the functional 
architecture has to be detailed and validated regarding the cycle time and the other 
constraints. 
5.3 
Logical Level  
To communicate with other departments involved in the detailed engineering, it is 
useful to structure the principle solutions. Regarding the method of [8], the material 
flow is one of the first definitions within a modularized engineering approach. There-
fore, this can be taken as criteria for structuring the principle solutions into a  
hierarchy. When compared to Fig. 3, the conveyor could be the child of an “Infeed-
Module”. Creating such a structure is very similar to the functional structure, which 
electrical engineers use when starting detailed engineering work in an ECAD tool. As 
previously mentioned, finding a principle solution is not sufficient to start detailed 
engineering. As far as Fig. 1 is concerned, such a principle solution has to be de-
scribed in a way so that all of the information required is shown on the right. As far as 
the information required is concerned, three main types of information can be defined: 
1. geometries and kinematics, 2. actuators and sensors with signal-interfaces, 3. se-
quences of operations. The information about the first category is usually provided by 
an MCAD tool. By assigning information about classified sensors and actuators, 

620 
G. Frank et al. 
which could be mapped to a symbol in a circuit diagram, it appears that this structure 
can be directly used as starting point for the electrical engineering in ECAD. The 
signal definitions at the PLC hardware represent the interface between the automation 
and electrical engineering disciplines. Therefore, in addition to the states and transi-
tion conditions, signals should also be described within the operating sequence. To 
provide information about the effect of setting or reading a signal, the connection 
between signals and the actuators should be available as context information. Another 
requirement on the sequence of operation involves the machine cycle time. The max-
imum time for executing the different movements is obtained by breaking down the 
required cycle time to a function and defining relationships using 1:1, n:1 and 1:n to 
the principle solutions. In conjunction with information about the geometrical posi-
tions, the start and end of motion and the moved masses, the actuators can be precise-
ly calculated to achieve the required cycle time. This in turn requires a time-based 
description for the sequence of operation, to break down the cycle times into the mo-
tion of the different principle solutions and to directly influence the selection of the 
actuators. A lot of description languages are available for defining sequences. As can 
be seen from Fig. 4, Gantt charts have a time-based description and the operations 
entered into them can be interpreted as states. The black arrows indicate transitions. 
Signals and transition conditions can be visualized, if it is possible to add signals and 
conditions to these arrows. [4] describes additional languages. Within pulse diagrams, 
the connection between sensors/actuators and signals is obvious, based on the re-
sources available. The transition conditions have to be added to the arrows, drawn for 
signals. PERT charts do not have a time axis like Gantt charts or pulse diagrams, but 
assign starting and end times to states. Logic networks, sequential function charts and 
state charts do not have information about time and no information about the signals 
connected to sensors or actuators. All other information that is required is directly 
visualized within these description languages. It can be seen that pulse diagrams 
represent the best fit for describing the sequence of operation of principle solutions, 
which has to be extended by adding conditions to the signal arrows. Regarding the 
requirements derived for the description of the principle solution for PLC-controlled, 
specialized production machines, a combination of a geometrical/kinematic MCAD 
model, the logical structure with sensors and actuators that have been classified and a 
pulse diagram contains all of the relevant information required to start engineering a 
machine in parallel.  
6 
Implementation 
To consider the requirement of [8] the phase of system design has to be supported by 
simulation. Therefor Mechatronics Concept Designer can be used to combine the 
geometrical and kinematics model of NX with a simulation model to visualize the 
sequence of motion for a specific module of the machine. This geometrical model is 
the same as the one used in the mechanical discipline. So this can be directly used for 
detailing. To execute the simulation, actuators for defining the motion of the kinemat-
ics, based on constraints for positions and speed are added. To detect, if a Geometric-
al-Part reaches a Position, sensors can be added, too. [12] To provide a tool for the 
interdisciplinary system design phase, fulfilling the requirements shown in the  

 
System Design of PLC-Controlled Specialized Production Machines 
621 
chapters above, this model should be controlled by a pulse-diagram. The description 
of those can be done in Sequence Designer, which provides the possibility to generate 
directly PLC-code. Connecting both tools, the actions of Sequence Designer lead to 
the motion of the kinematics, executed through actuators. Therefor the exact geome-
trical position is mapped to the given state of Sequence Designer. But a pulse-diagram 
does not give the information about: how a position is reached. This is solved through 
defining relation between the states and the actuator, which is responsible for reaching 
this state. Signals for the transition between the actions are available in Mechatronics 
Concept Designer by sensors. Mapping them to the given signals in Sequence De-
signer, the transition between the different actions is defined. To do this enrichment 
and the mapping an application was developed. Using this, it is possible to provide a 
simulation tool within the system-design phase, which owns enough information, to 
start detailing within the automation and mechanics discipline. If there would be the 
possibility of extending the information about sensors and actuators in Mechatronics 
Concept Designer by adding a detailed classification of the sensors and actuators, also 
the electrical department would be able to start detailing.  
7 
Summary  
To reduce time and the associated costs when designing specialized production ma-
chines, it is not possible to split the engineering process as proposed by [3]. To reduce 
time, it must be possible to reuse modules that have already been designed, and the 
various disciplines have to be parallelized. This could be done based on the macro 
process of [13]. Therefore, the current sequential process was analyzed to define the 
information required for starting all of the disciplines involved in parallel. Additional 
requirements for the system design phase are derived from the general characteristics 
of this phase. The design process of PLC-controlled specialized machines was pre-
dominantly adapted in 4 steps: 1. Functional breakdown of the requirements, 2. Add-
ing dependencies between functions with the types of: time-dependency (Gantt chart), 
material, information and energy, 3. Searching and selecting the principle solutions 
for functions, taking into consideration integration and reuse, 4. Drawing-up a logical 
structure, including linking discipline-specific descriptions as geometrical/kinematic 
for mechanics, sensors and actuators that have been classified for the electrical system 
and pulse diagrams for the automation. Based on this approach, the cycle time can be 
continuously planned down to the motion of a single axis. Further, all of the informa-
tion required for starting detailed engineering in the various disciplines is available. 
To support this phase an application was developed to connect the tools Mechatronics 
Concept Designer and Sequence Designer. 
References 
1. Bathelt, J.: Design Method for PLC-operated mechatronics systems. University of Zurich 
(2006) 
2. Baudisch, T., Westkämper, E., Schlögl, W., Frank, G.: Concept for seamless Engineering 
Tool Chain for PLC-controlled Specialized Production Machines. In: Proceedings of Inter-
national Conference on Mechanical Engineering and Mechatronics, ICMEM 2012. Inter-
national Academy of Science, Engineering and Technology, Ottawa (2012) 

622 
G. Frank et al. 
3. Doll, U.: Usage of mechatronics Design-Tools within the digital factory for Line-Projects. 
In: Heisel, U., Gadow, R., Liewald, M., Verl, A., Spath, D., Westkämper, E. (eds.) FTK 
2010, pp. 631–648 (2010) (in German) 
4. Drath, R., et al.: Data Exchange for Line-planning with Automation ML - Integration of 
CAEX, PLCopenXML and Collada. Springer, Berlin (2010) 
5. Gehrke, M.: Design of mechatronics Systems based on function hierarchies and System 
structures. University of Paderborn (2005) (in German) 
6. Kallmeyer, F.: A Method for modeling principle solutions of mechatronics systems. Uni-
versity of Paderborn (1998) (in German) 
7. Litto, M., et al.: Engineering based on building sets with Föderal – A Guideline for Ma-
chine- and line-Builders. VDMA-Verlag, Frankfurt a.M. (2004) (in German) 
8. Litto, M. et al.: Adaptable Modeling-tool and Qualification-Program for setting up compa-
ny specific mechatronics engineering processes – aquimo. VDMA-Verlag, Frankfurt a.M. 
(2010) (in German) 
9. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Theory of design – Base for successful 
product development and application. Springer, Berlin (2007) (in German)  
10. Schilke, M.: Usage of PDM-Systems in specialized machine building for the automobile 
industry. University of Saarbrucken (2010) (in German)  
11. Schlögl, W.: Digital Factory Fabrik 2.0 – The digital Factory at the transition to real Facto-
ry. In: Heisel, U., Gadow, R., Liewald, M., Verl, A., Spath, D., Westkämper, E. (eds.) FTK 
2008, pp. 453–462 (2008) (in German)  
12. Siemens Product Lifecycle Management Software Inc.: Mechatronics Concept Designer – 
A function based-approach for Machine-builders, Cologne (2010) 
13. VDI 2206: Method of Design for mechatronics Systems. Beuth Verlag, Berlin (2004) (in 
German) 
14. VDI 2222: Method of Design – methodical development of principle solutions. BeuthVer-
lag, Berlin (1997) (in German) 
15. VDI 2803: Function analysis – Fundamentals and method. BeuthVerlag, Berlin (1996) (in 
German) 
16. VDW-Richtlinie: Results of the Projectgroup 3: – Directive for functional description. 
VDW Verein Deutscher Werkzeugmaschinenfabriken e.V., Frankfurt a.M. (2001) (in 
German) 
17. Witte, M.: Systems Engineering, Plant Engineering and Functional Models. Software 
technik-Trends, Udo Kelter, Siegen (2012) (in German) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 623–632. 
DOI: 10.1007/978-3-642-30817-8_61 
© Springer-Verlag Berlin Heidelberg 2013 
 
A New Method for Human Reliability Analysis  
in New Product Development  
Raymond Djaloeis1, Sönke Duckwitz1, Malte Hinsch2, 
Jörg Feldhusen2, and Christopher M. Schlick1 
1 Institute of Industrial Engineering and Ergonomics at RWTH Aachen University,  
Bergdriesch 27, 52062 Aachen, Germany 
2 Institute for Engineering Design at RWTH Aachen University,  
Steinbachstraße 54 B, 52062 Aachen, Germany 
r.djaloeis@iaw.rwth-aachen.de 
Abstract. Human reliability is an essential quality of the complex and highly 
dynamic domain of new product development (NPD), because a low level of 
human error positively influences quality, cost and safety of a product. Howev-
er, there is lack of empirical research in this area, especially regarding methodo-
logical approaches. In this paper, based on the abstraction hierarchy and the 
multi-facet human error taxonomy of Rasmussen as well as on the requirements 
of systematic engineering design by Pahl et al., a theory for human reliability in 
NPD is described. Based on this theory, a framework for a large-scale laborato-
ry experiment (n=111) with three NPD-compliant design tasks was developed, 
which focused on the effects of limited available time on human reliability. The 
experiment provided a broad range of empirical results, which were quantita-
tively analyzed, and serves as the base for follow-up NPD experiments. 
Keywords: human reliability, human error, new product development, research 
methodology, systematic engineering design, time, empirical analysis. 
1 
Introduction 
Successful new product development (NPD) is essential to gain competitive advan-
tages in global markets [1]. In systematic engineering design, NPD is described as a 
dynamic, highly iterative process with numerous interdependencies between problem, 
process and product. This stems from the demands of a systematic requirements anal-
ysis, which stipulate that a product has to satisfy numerous requirements on the func-
tional, working, constructional and systematic interrelationship levels [2][3]. In NPD, 
a high level of human reliability is essential, because a faulty product not only puts 
material and spatial constraints at risk (constructional level), but also endangers the 
overarching physical effect (working level), which in turn endangers the product 
function (functional level) and in the worst case destroys the overarching product 
system (systems level). About 80% of all product-related errors and about 60% of all 
product recalls due to customer dissatisfaction are rooted in a flawed product devel-
opment process (e.g. unsystematic, myopic approaches) [2]. A high level of human 
reliability in NPD contributes to a higher quality, lower cost and greater safety of the 

624 
R. Djaloeis et al. 
product, and leads to a higher level of customer satisfaction, lower market price, 
punctual arrival on time, and most importantly, it ensures that the product in devel-
opment reaches its intended function [2].  
However, regarding human error in NPD, there is lack of empirical research, espe-
cially regarding methodological approaches adhering to the standards of systematic 
engineering design. Previously, Schütze et al. [4] and Dylla [5] experimentally ana-
lyzed product development processes, but only used a relatively small number of 
participants and/or limited themselves to simple tasks. In this paper, a theoretical 
framework for NPD analysis is presented, which is based on the abstraction hierarchy 
and the multi-facet human error taxonomy by Rasmussen [6], [7], and is adapted for 
the domain of NPD. The essential performance shaping factors are further categorized 
by adapting the structure model of individual human action by Busse and Lampe [8]. 
Using this approach, human reliability factors can be categorized into person-, 
process- and product-oriented variables. Based on this framework, a large-scale la-
boratory study (n=111) in new product development under temporal constraints was 
conducted, and results concerning engineering design quality and human reliability 
are presented here. 
2 
Human Error 
Rasmussen (1982) describes human errors as “very difficult to define satisfactorily”, 
because “frequently they are identified after the fact“. Therefore he proposes viewing 
errors in a greater context as “man-machine or man-task misfits”. Rasmussen distin-
guishes three levels of human error [6]: 
• Knowledge-based errors in unique and unfamiliar situations, in which detailed 
multi-level planning is required. 
• Rule-based errors in familiar situations, in which stored rules are used to regulate 
actions. Errors here are typically connected to recognition of wrong rules. 
• Skill-based errors in automated, stored patterns of behavior, which typically relate 
to a false time, space or force coordination. 
Concerning the most complex category of knowledge-based errors, Rasmussen estab-
lishes a five-level abstraction hierarchy [7]: 
• functional purpose, which clarifies system objectives 
• abstract function, which deals with abstract, causal input-output structures of mass, 
energy and information flows 
• generalized functions, which cover concrete technical functions 
• physical functions, which name electrical, mechanical or chemical processes 
• physical forms, which deal with the physical form and location 
According to Rasmussen, this approach is useful for the design and evaluation of 
complex technical systems: by mapping high-level purposes and low-level physical 
implementations onto each other, the satisfaction of functional, physical and material 
requirements can be facilitated. Rasmussen also describes a multi-facet taxonomy of 
human error [6]: 

 
A New Method for Human Reliability Analysis in New Product Development 
625 
• causes of human malfunction (e.g. external events, excessive task demand, intrinsic 
human variability, disturbances) 
• performance shaping factors (e.g. mental load, affective factors) 
• mechanisms of human malfunction (e.g. information processing/comprehension 
mistakes, false inference, false discrimination) 
• situational factors (e.g. task characteristics, work time characteristics) 
• internal human malfunctions (e.g. faulty detection, identification, action) 
• personnel tasks (e.g. equipment design, procedure design, fabrication) 
• external mode of malfunction (e.g. non-performance of a necessary task or com-
mitment of an erroneous or extraneous task, i.e. errors of omission, errors of execu-
tion, errors of intention) 
The variables of this taxonomy are only partly observable: whereas external malfunc-
tions, situational factors and performance shaping factors can be measured, the other 
parts are internal. Additional factors listed by Vicente are the quality of the human-
machine-interface, situational familiarity and situational anticipation [9][10]. Shappell 
and Wiegmann [11], who developed the Human Factor Analysis and Classification 
System (HFACS) based on the approach by Reason [12], describe human error as 
“factors in a mishap when mental or physical activities of the operator fail to achieve 
their intended outcome” and list problem misjudgement, lack of available time 
(“tough schedules”), poor spatial perception, inexperience, and external distractions. 
Similarly, Hacker describes human error as “unsatisfactory human execution of a 
tangible work task”, and stresses the lack of available time and limited human infor-
mation processing ability as important contributing factors [13]. Bartsch describes 
human error as “the ability of the person in the work system to bring a suitable quali-
fication and the corresponding physical and mental performance preconditions into a 
specific work process… whereby technical, economic, humanitarian and ecological 
criteria and a failure acceptance range are respected”, and lists a broad range of pre-
dominantly physical human error shaping factors (e.g. health, qualification, strain, 
stress, visual-spatial memory) [14]. Several of these authors list the limited amount of 
time as an important factor. However, time constraints are often underestimated. 
Deadline shortening puts product quality at high risk and lead to an exponential 
growth of complexity [4]. However, in NPD, the terms “human reliability” and “hu-
man error” are not well defined, which makes categorization of human reliability 
factors difficult. 
3 
Human Error in NPD 
In systematic engineering design, the product must conform to the requirements as 
decided on between the customer and the development team. Fulfillment is deemed as 
quality, and failure to meet these standards can be subdivided into errors of omission 
as well as errors of execution. All quality requirements as well as their respective 
acceptance limits on the functional, working and constructional interrelationship le-
vels can be categorized using the levels of systematic engineering design [2][3]: 

626 
R. Djaloeis et al. 
• The functional interrelationship level expresses the intended relation between input 
and output (i.e. the product function), and result is the functional structure. The de-
signed product must conform to function-specific requirements, generic require-
ments and logical requirements. Requirements on this level are very abstract. 
Common errors are failure to understand the main function; naming a physical law 
or a product component instead of a function; incorrect subdivision of main and 
auxiliary functions; incorrect subdivision of functions; illogical solutions etc. 
• On the working interrelationship level, relevant laws of physics as well as geome-
trical and material constraints are considered, and result is the working structure. 
The appropriate physical effect must be modeled with appropriate formula, geome-
trical and material constraints. Working requirements have a medium abstraction 
level. Common errors are choosing the wrong physical effect; naming a function or 
a product component instead of a physical law; picking the wrong formula; setting 
the initial and boundary conditions wrongly etc.  
• The constructional interrelationship level highlights components, joints and assem-
bly points, according to the requirements of assembly, transport etc., and result is 
the construction structure. Requirements on this level are concrete, i.e. the abstrac-
tion level is minimal. Each component, each connection and the assembly itself 
have to be correct. Common errors include naming a function or a physical law in-
stead of a concrete component, value or dimension; wrong dimensioning or order 
of assembly; creating spatially impossible components and/or connections etc.  
• System interrelationships deal with the role of the product as a part of an overarch-
ing system, especially regarding external input-output-relations. Because this paper 
only deals with small-scale NPD, the systems interrelationship is not included in 
further sections.  
Pahl et al. elaborate that establishing functional relationships can be challenging, be-
cause the high level of abstraction is difficult to interpret [2]. In general, there are 
similarities between the abstraction hierarchy of Rasmussen, which focuses on inter-
nal human mental models, and the levels of systematic engineering design by Pahl et 
al., which emphasize technical qualities (see Table 1). Additionally, both approaches 
emphasize the importance of high abstraction levels, and take into account that hu-
mans prefer to work on the lower levels. Whereas Rasmussen’s approach consists of 
mutually dependent internal and external modes of malfunction, and defines three 
possible external error modes (errors of omission, errors of intention, errors of execu-
tion). Pahl et al. focus on the subset of external, observable errors: “errors of inten-
tion” are not applicable, but become observable in the two remaining categories  
“errors of omission” and “errors of execution”. 
Table 1. Levels of abstraction compared to levels of systematic engineering design 
Abstraction hierarchy  
(Rasmussen) 
Systematic engineering design
(Pahl et al.) 
Common concepts 
Purpose 
Functional  
Interrelationship level 
Main function 
Abstract function 
Functional structure 
General functions 
Working  
interrelationship level 
Working structure,  
physical effects 
Physical functions 
Physical forms 
Constructional 
Interrelationship level 
Construction structure,  
assembled product 

 
A New Method for Human Reliability Analysis in New Product Development 
627 
In the error models described previously, a broad range of possible human error 
shaping factors is listed, but there is a lack of categorization. According to the struc-
ture model of individual action by Busse and Lampe, human action can be modeled as 
a circular interaction between subject and object, in which every element is connected 
to the other two [8].  
 
Person-related 
error shaping factors 
(excerpt)
• Work-relevant 
knowledge, rules and 
skills
• Physical workload
• Mental workload
• Visual-spatial memory
• Perceived workload
• Affection
• Experience
• Requirement 
comprehension
• Communicative abilities
• Health
Person
Process-related 
error shaping factors 
(excerpt)
• Available time
• Task complexity
• Objective strain
• Interruption 
• Equipment quality
• Level of abstraction
• Process familiarity
• Concurrent task 
processing
Process
Product-related 
error shaping factors 
(excerpt)
• Functional-level quality 
requirements
• Working-level quality 
requirements
• Constructional-level 
quality requirements
• General product 
properties
• Product complexity
• Product familiarity
Product
New Product Development
 
Fig. 1. Structure model for new product development 
The circular relation expresses the mutual interaction between the psyche of the 
subject and the properties of an object, which are influenced by both activity- and 
non-activity-related aspects. Based on this model, error shaping factors in the pre-
vious literature review can be classified whether they are person-oriented, process-
oriented or product-oriented. A NPD adaptation is depicted in Fig.1. Intuitively,  
several possible relations become evident, e.g. between “perceived workload” and 
“objective strain”. These error shaping factors could be additionally subdivided into 
NPD-related and non-NPD-related, but the boundaries are often not clearly definable. 
4 
Framework for NPD Analysis 
Using the model listed above, a theoretical framework for the analysis of human error 
of NPD in laboratory settings was developed. Firstly, it is essential to identify the 
product in development, because each product has a different purpose. Based on this, 

628 
R. Djaloeis et al. 
it is vitally important to list the necessary requirements on the functional, working and 
constructional interrelationship levels of systematic engineering design. Secondly, 
errors in fulfilling these requirements are categorised and counted as either “error of 
omission” or “error of execution”. Thirdly, in a laboratory experiment setting, it is 
essential to start with tasks relating the functional interrelationship level, proceed with 
tasks in the working interrelationship level, and end with tasks dealing with the con-
structional interrelationship level. 
Furthermore, it is essential to determine what error shaping factor the laboratory 
experiment should specifically focus on. There are a multitude of possible variables, 
as the NPD structure model in Fig.1 illustrates. The effects of limited available time 
on quality of the result (on the functional, working and constructional levels) are hig-
hlighted by several authors [2][6][15]16] and were therefore focused on. Lack of time 
poses a serious risk for product quality, cost and safety, as well as for the perceived 
workload on the respective product developers. Previous research on work under time 
pressure mostly focuses on simple tasks like speed typing. Speed typing is far less 
creative and dynamic than NPD, but already in this simple task, a non-linear relation-
ship between available time and human error probability (HEP) was found [15]. In a 
follow-up experiment, a similar non-linear relationship between available time and 
perceived workload on the other hand was formulated [16]: if time constraints be-
come too restrictive, HEP and perceived workload rapidly increase. 
5 
Laboratory Experiment 
Based on the theoretical framework described above, a laboratory study was con-
ducted, in which participants had to perform a multi-part NPD task under time  
constraints. The experiment was carried out in the laboratories of the Institute of In-
dustrial Engineering and Ergonomics at RWTH Aachen University. A between sub-
jects design was chosen, and each person participated exactly once in this experiment, 
i.e. no repeated measure was carried out. The product in question was a set of gear 
wheels with treadles, and prior to the experiment, a requirement list for every level of 
systematic engineering was compiled. Based on these lists, a multi-part NPD setting 
was developed, which deals with the functional, working and constructional levels of 
systematic engineering design: 
• In task F, the participants hierarchically established the functional relations. 
• In task W, the working relations were described: improvements based on a set of 
given deficiencies had to be developed and sketched in a principle solution. 
• In task C, the participants hierarchically sketched the constructional interrelations. 
To provide a degree of freedom, the participants could conduct F and C simultane-
ously. In this experiment, the main independent variable was the amount of available 
time. Several dependent variables were examined. Firstly, for every task, the quality 
of the design was evaluated based on the fulfilment of the respective functional-, 
working- and constructional-level requirement lists. Secondly, the perceived worko-
load level according to the NASA-TLX technique was evaluated. Finally, extraneous 

 
A New Method for Human Reliability Analysis in New Product Development 
629 
variables such as personal data, level of education, work experience and ability of 
three-dimensional visualisation were collected. The sample of participants consisted 
of 111 junior engineers, all of them students in higher semester engineering classes at 
RWTH Aachen University, were divided into four groups (group 1: 27 participants; 
group 2: 25 participants; group 3: 28 participants; group 4: 31 participants), each with 
a different time to complete the three tasks (see Table 2): 
Table 2. Available time per group 
Task 
Group 1 
Group 2 
Group 3 
Group 4 
F+C 
5 min 
8 min 
10 min 
12 min 
W 
10 min 
12 min 
16 min 
20 min 
 
The participants were mostly male (91%), had a mean age of 22.76 (SD=1.36), and 
generally evaluated themselves as ‘‘medium or highly experienced’’ in both technical 
drawing (80%), but only ‘‘little or not experienced’’ in the design of actual products 
(54%). The experiment itself was divided into three phases: Pretests, data acquisition 
and semi-structured interview. In the pretest phase, the participants filled in question-
naires in which personal data (age, gender etc.), level of education and work experi-
ence (technical drawings, manual labour etc.) were anonymously collected. After that, 
the participants carried out a preliminary IST 2000 R test [18] to measure their ability 
of three-dimensional visualisation, after which the main experiment was performed: 
In the data acquisition phase, the participants performed the tasks described above. 
Also, the perceived workload according to the NASA-TLX assessment technique was 
documented [19]. This multi-dimensional rating procedure derives workload using six 
subscales: Mental Demands, Physical Demands, Temporal Demands, Own Perfor-
mance, Effort, and Frustration. At the end, a 10-minute semi structured interview was 
conducted. Based on the sample of the 111 participants, a detailed analysis of the 
above mentioned hypotheses was carried out. For the analysis of the empirical data, 
the software Statistical Software for Social Sciences (SPSS, version 19) was used. At 
first, it was checked whether the empirical data met the conditions for a standard 
analysis of variance (ANOVA). A simple histogram analysis of the empirical data 
revealed that within the four groups, the dependent variables follow a normal distribu-
tion. The homogeneity of variance was checked with a Levene’s test. If the data 
passed this examination, an ANOVA was carried out with a Tukey post hoc analysis, 
and the chosen level of significance for each analysis was α=0.05. 
6 
Results 
The data analysis (see Table 3) shows that in each task, an almost linear relation be-
tween the success rate, the human error rates (both errors of omission and errors of 
execution) and the amount of available time is observable. According to the results of 
the corresponding ANOVA analysis, the available time significantly influences the 
quality of the design (p<0.001). The Tukey post hoc analysis revealed that both a 
significant differentiation of paired groups and ranking of the groups are possible. 

630 
R. Djaloeis et al. 
Interestingly, a descriptive analysis reveals that the frustration levels are highest in 
Group 4 (cumulated mean of the three tasks=109.8, SD=43.1), which had the most 
time available, and also the order of Group 1 (cumulated mean=102.9, SD=32.7), 
Group 2 (cumulated mean=105.0, SD=40.9) and Group 3 (cumulated mean=91.1, 
SD=31.5) was unexpected. This paradoxical result can be partly explained by the 
analysis of the semi-structured interviews. Participants of Group 4 said that they were 
regularly finished before the available time was up. In the remaining time, they con-
sciously reflected what errors they committed, without having enough time left to 
correct everything, which led to frustration. 
Table 3. Human error probabilities 
Task 
Error category 
Group 1 
Group 2 
Group 3 
Group 4 
F 
Omission 
56.33% 
47.20% 
33.81% 
29.25% 
Execution 
18.90% 
11.84% 
6.38% 
0.56% 
Success 
24.77% 
40.96% 
59.81% 
70.19% 
 
 
 
 
 
 
W 
Omission 
48.89% 
41.20% 
28.21% 
16.45% 
Execution 
31.22% 
20.83% 
21.40% 
5.56% 
Success 
19.89% 
37.97% 
50.39% 
77.99% 
 
 
 
 
 
 
C 
Omission 
27.60% 
24.00% 
18.57% 
8.39% 
Execution 
36.21% 
19.20% 
16.26% 
6.07% 
Success 
36.19% 
56.80% 
65.17% 
85.54% 
 
Additionally, for all groups, the rate for errors of omission is highest for the task F, 
in the middle for task W, and lowest in task C. For example, in Group 3, the rates 
were respectively 33.81%, 28.21% and 18.57%: the higher the level of abstraction, 
the higher the “task entry barrier” seems to become. However, once this threshold is 
surpassed, the solution seems easy: there was a very low rate for errors of execution 
in task F. On the other hand, for task C (constructional level), the rate for omission 
errors was continuously lower than for other tasks. This indicates that the “task entry 
barrier” was lower. However, the rate for execution errors was continuously high, 
which indicates that formulating concrete solutions seems challenging. For the work-
ing level, it is difficult to formulate exact results. For Groups 2 and 3, the rate for 
execution errors in task W was higher than in tasks F and C; however, for Groups 1 
and 4, this figure was higher than the respective execution error rate in task F, but 
lower than in task C. Moreover, Group 3 actually made more execution errors in task 
W than Group 2, although they had more time. 
7 
Summary and Conclusion 
In this paper, a new theoretical and methodological framework for human error analy-
sis in new product development (NPD) was presented. Based on the abstraction hie-
rarchy by Rasmussen, the three levels of systematic engineering design by Pahl et al., 

 
A New Method for Human Reliability Analysis in New Product Development 
631 
and the structure model of individual action by Busse and Lampe, a research frame-
work for NPD was established, which allows the categorization of human error shap-
ing factors, and the expression of their mutual relation. Based on these thoughts, a 
laboratory experiment (n=111) was conducted, in which the effect of limited available 
time was analyzed. Empirical analysis confirmed the impact of this human error shap-
ing factor on the quality of the result, and several non-intuitive relations between 
abstraction levels, and both category and rate of human errors were observed, which 
require deeper analysis. Further studies should concentrate on a detailed analysis of 
the specific execution errors on each level, and observe the influence of additional 
factors, such as disturbances or incomplete information, and integrate resulting find-
ings. In the long term, the experimental findings will lead to an enhancement of actor-
oriented work process simulation models by considering bounded human reliability 
and its influence on complex NPD projects [19]. 
 
Acknowledgements. The research was funded by the German Research Foundation 
(DFG) according to Grant SCHL 1805/3-3. 
References 
1. Mihm, J., Loch, J., Huchzemeier, A.: Problem-Solving Oscillations in Complex Engineer-
ing Projects. Management Science 46(6), 733–750 (2003) 
2. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.H.: Engineering Design: A Systematic Ap-
proach. Springer, Berlin (2007) 
3. Ponn, J., Lindemann, U.: Konzeptentwicklung und Gestaltung technischer Produkte: Sys-
tematisch von Anforderungen zu Konzepten und Gestaltlösungen. Springer, Berlin (2008) 
4. Schütze, M., Riemer, S., Steger, W., Mickan, R.: Zur Wechselwirkung von Gestalten und 
Berechnen im konstruktiven Entwurf aus denkpsychologischer Sicht – Hinweise aus dem 
Vorgehen Erfahrener. In: Mensch-Technik-Organisation, vol. 33, pp. 83–104. Rainer 
Hampp, Zürich (2002) 
5. Dylla, N.: Denk- und Handlungsabläufe beim Konstruieren. In: Konstruktionstechnik 
München, vol. 5. Hanser, Munich/Vienna (1991) 
6. Rasmussen, J.: Human Errors. A Taxonomy for Describing Human Malfunction in Indus-
trial Installations. Journal of Occupational Accidents 4, 311–333 (1982) 
7. Rasmussen, J.: Skills, Rules, and Knowledges: Signals, Signs, and Symbols and Other Dis-
tinctions in Human Performance Models. IEEE Transactions on Systems, Man, and Cy-
bernetics. SMC- 13(3), 257–266 (1983) 
8. Busse, S., Lampe, R.H.: Person-Handlung-Umwelt. Ein Strukturmodell zur individuellen 
Handlungsfähigkeit. Probleme und Ergebnisse psychologischer Forschung 8, vol. 5, pp. 1–
127. Karl-Marx-Universität Leipzig, Sektion Psychologie, Leipzig (1987) 
9. Vicente, J.: Ecological Interface Design: Theoretical Foundations. IEEE Transactions on 
Systems, Man, and Cybernetics 22, 589–606 (1992) 
10. Vicente, J.: Ecological Interface Design: Progress and Challenges. The Journal of Human 
Factors and Ergonomics Society 44, 62–78 (2002) 
11. Shappell, S.A., Wiegmann, D.A.: The Human Factors Analysis and Classification System 
HFACS. U.S. Department of Transportation, Federal Aviation Administration, Washing-
ton D.C. (2001) 
12. Reason, J.: Human Error. Cambridge Press, Cambridge (1992) 

632 
R. Djaloeis et al. 
13. Hacker, W.: Allgemeine Arbeitspsychologie – psychische Regulation von Wissens-, Denk- 
und körperlicher Arbeit. In: Schriften zur Arbeitspsychologie, vol. 58. Hans Huber, Bern 
(2005) 
14. Bartsch, H.: Menschliche Zuverlässigkeit in Flug-Arbeitssystemen. In: Verlässlichkeit der 
Mensch-Maschine-Interaktion: 46. Fachausschusssitzung Anthropotechnik der Deutschen 
Gesellschaft für Luft- und Raumfahrt e.V. DGLR-Bericht 2004-03, pp. 147–157. Deutsche 
Gesellschaft für Luft- und Raumfahrt e.V., Bonn (2004) 
15. Bubb, H., Jastrebska-Fraczek, I.: Human Error Probability depending on Time Pressure 
and Difficulty of Sequential Tasks. In: Safety and Reliablity, p. 681. Balkema, Rotterdam 
(1999) 
16. Mazloumi, A., Kumashiro, M., Izumi, H., Higuchi, Y.: Quantitative Overload: a Source of 
Stress in Data-Entry: VDT Work Induced By Time Pressure and Work Difficulty. In: In-
dustrial Health 2008, Fukuoka, Kawasaki, vol. 46, pp. 269–280 (2008) 
17. Liepmann, D., Beauducel, A., Brocke, B., Amthauer, R.: I-S-T 2000 R. Hogrefe, Göttin-
gen (2007) 
18. Hart, S.: NASA-Task Load Index (NASA-TLX): 20 Years Later. In: Proceedings of the 
Human Factors and Ergonomics Society 50th Annual Meeting, pp. 904–908. HFES, Santa 
Monica (2006) 
19. Duckwitz, S., Djaloeis, R.B., Hinsch, M.S., Feldhusen, J., Schlick, C.M.: Consideration of 
Human Reliability in Actor-Oriented Simulation of New Product Development. In: IEEE 
International Conference on Industrial Engineering and Engineering Management (IEEM), 
Hong Kong (2012) (accepted paper) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 633–641. 
DOI: 10.1007/978-3-642-30817-8_62 
© Springer-Verlag Berlin Heidelberg 2013 
 
Design for Usability by Ubiquitous Product 
Documentation 
Michael Abramovici, Andreas Krebs, and Thomas Schindler 
Ruhr-Universität Bochum, Chair of IT in Mechanical Engineering, Universitätsstraße 150, 
Bochum, 44801, Germany 
{michael.abramovici,andreas.krebs,thomas.schindler}@itm.rub.de 
Abstract. The use of current complex products is rarely intuitive and self-
explanatory. The approach presented in the paper in hand therefore considers 
support of product users by facilitating ubiquitous and providing easy access to 
product information for different use case scenarios through the use of new in-
formation channels. The approach includes a design method for the integration 
of accessible technical documentation into product development. A special em-
phasis of the proposed product documentation is made on special quick guides 
with instructions for the installations and frequently occurring use case scena-
rios, as well as references to more detailed information. To provide a first vali-
dation of the approach, a case study is introduced towards the end of the paper. 
Keywords: marking technologies, mobile computing, product documentation, 
product life cycle usability, QR-Code, RFID. 
1 
Introduction 
The use of current complex products is rarely intuitive and self-explanatory. Many 
products in everyday life and the industrial environment are hardly or even not at all 
usable due to the lack of information regarding the handling of the product and the 
range of functions. Even the availability of standard documentation may not ensure 
the proper use of complex products. The technical documentation of complex prod-
ucts often comprises several hundred pages, which can pose considerable effort to the 
users. Both the lack of information and too much information can lead to an inappro-
priate use of certain kinds of products. This may even result in damage to users, the 
environment, or the products itself. Particularly affected are products the user is  
unfamiliar with or that are infrequently used. 
The paper in hand introduces a design method for the reduction of usability prob-
lems that is suitable in particular for complex products. The proposed method should 
be integrated into the product development process. To enhance the usability of a 
product, the method suggests the testing of products in different maturity phases in 
interplay with its documentation. The distribution of the developed documentation is 
enabled by the new generation of smart devices and can enable every product life 
cycle party to access the right information in the right place at the right time, after the 

634 
M. Abramovici, A. Krebs, and T. Schindler 
 
point of sale. To render nearly any kind of device or product versatilely applicable, 
the presented approach exploits cloud computing and the Internet of Things based on 
internet-capable mobile devices and marking technologies. The developed method 
and its derived solutions shall lead to a shorter introduction time when handling prod-
ucts. A special emphasis of the proposed product documentation is made on special 
role- and device-specific quick guides with instructions for the installations and  
frequently occurring use case scenarios, as well as references to more detailed  
information. 
The proposed quick guide is intended as a supplement to the common product do-
cumentation and may contain a reference to it. This short documentation is user-
centered and therefore be provided in a multicultural and multilingual version, which 
can be used as a sales argument for the producer. Examples for products which are in 
the focus of the presented approach are highly complicated devices, rental equipment, 
portable devices, corporate devices that are used jointly, many resold devices without 
technical documentation, devices with a potential risk to human health and the envi-
ronment, devices with high operating costs (energy and resources), and industrial 
machines without display. 
The presented approach is suitable for all kinds of products. The paper in hand  
emphasizes the documentation process of quick guides for technical capital goods. 
The approach should be integrated into the development process of new products, but 
is also suitable for products, that are already in serial production. To provide a first 
validation of the approach, two case studies with serial products, a pump and a  
concrete chainsaw, which have been carried out in cooperation with the German  
department of civil defense “Bundesanstalt Technisches Hilfswerk” (THW), are in-
troduced in chapter 4. 
2 
Related Work 
2.1 
Use of Product Identification Technologies with Mobile Devices  
The number of mobile devices such as smartphones and tablet PCs has been increas-
ing significantly over the last years. These devices facilitate mobile computing,  
mobile communications, and mobile internet use, which offer new ways of human–
computer interaction. Many of these devices have embedded cameras, which can be 
used to read visual product identification technologies like barcodes, e.g. the QR-
code. Some later generations of mobile devices even have reader modules for elec-
tromagnetic identification technologies like Near Field Communication (NFC) or 
Radio Frequency Identification (RFID). Product identification technologies offer an 
opportunity for automated identification of marked physical product items without 
manual errors like assignment or typing errors, and allow to store a unique URL or ID 
on the mark and onto a particular product item. These URLs or IDs can be read by 
mobile devices to access and alter related data sets of a product item in databases via 
the internet. 

 
Design for Usability by Ubiquitous Product Documentation 
635 
 
2.2 
Design Approaches 
In addition to traditional approaches for product development as described in norms 
like VDI 2221 [1], in the last decade, new approaches have been developed. One of 
the main reasons for the need of new approaches is the emergence of new require-
ments for the development processes. Apart from mechanical engineering, other dis-
ciplines like electrical engineering and informatics had to be integrated into the  
development process. An example for this kind of approach is the V model de-scribed 
in VDI 2206 [2]. This model has been further extended by aspects like the develop-
ment of adaptronic systems in the so-called W model [3]. The concept of the V model 
corresponds to the design methodology of VDI 2221 but does not contain as many 
detailed process steps. The approach starts with the definition of requirements.  
The next step is the execution of system design for the different disciplines. These are 
mechanical engineering, electrical engineering, and informatics. The next process step is 
domain-specific design and the integration of the different design disciplines into  
an overall solution. These steps are repeated until the serial production phase is  
reached [2]. 
Another group of approaches are the Design for X methods, which support the de-
signer with integrating multiple properties of the different aspects into the product. 
This poses a huge challenge to the designer as desired product properties influence 
each other. As an example, low energy usage can influence the total cost of the prod-
uct [4]. Examples for these methods are design for usability, design for costs, and 
design for sustainability. The design for usability approach already considers the user 
of products during the development process. This can be achieved in various ways. 
There are methods for user-centered design, behavior-based design, practice-oriented 
design, co-creation in terms of using user-generated content, and co-design in terms 
of including co-creation within the design process where users and designers work 
together [5]. The user-centered design approach is also described in ISO 9241-
210:2010 [6]. 
Other approaches like design for sustainability have been gaining significance, due 
to the steady increase of energy-using equipment with the consequence of a signifi-
cant impact on the environment, as well as based on public regulations [5].  
The integration of usability aspects for capital goods into product development will 
lead to cost-efficient, sustainable, and proper use of serial products. 
2.3 
Product Documentation 
According to VDI 4500 [7], technical product documentation can be divided into two 
areas: internal technical documentation and external technical documentation. Internal 
product documentation concentrates on the internal processes of a company e.g. in 
development, production, or quality assurance. External documentation concentrates 
on areas like marketing, the use phase, maintenance, and the disposal of products. 
Such content is technical information intended for distribution, users and consumers. 
It is vital for an external documentation that the quality and comprehensibility is suf-
ficient with regard to an easy understanding for the user. These factors determine 
whether the target group can use functions of the product in a proper way. Results of 

636 
M. Abramovici, A. Krebs, and T. Schindler 
 
the documentation process are e.g. accompanying documents, operating manuals, 
instruction manuals, and technical descriptions. In these instances, contents can be 
basic security instructions and manuals for preparation, assembly, commissioning, 
operation, and maintenance. Additionally, there are instructions for the end-of-life 
considerations of the product [8]. 
Apart from the usual distribution ways with printed manuals, there are new ways of 
distributing documentations. Examples for these kinds of manuals are documents with 
integrated, three dimensional design data, virtual reality, or even augmented reality 
for the integration of real environments with design data. 
3 
Our Approach 
3.1 
Overview 
The presented approach should be integrated into the V model-based development 
process of new products which is described in VDI 2206 [2]. The approach is also 
suitable for products that are already in serial production. To offer a way to create an 
ubiquitous accessible, role- and device-specific, structured product documentation as 
a supplement to common versions, the paper in hand presents a special documentation 
process derived from VDI 4500 [8]. Fig. 1 provides an overview over the integration 
of the proposed documentation process into the development phases of new products. 
 
Fig. 1. Approach for ubiquitous product documentation based on VDI 2206 

 
Design for Usability by Ubiquitous Product Documentation 
637 
 
The proposed documentation process consists of six basic phases: planning, re-
quirement analyses, research, creation, quality assurance, and distribution.  
The planning phase of the documentation process starts with the beginning of the 
development phase. The purpose of the activities in the planning phase is the devel-
opment of a concept that ensures harmonized interaction between activities, assets or 
resources, and the involved roles. 
The requirement phase of the documentation process ranges from the beginning of 
the development to the completion of the pilot series. The requirement analyses are 
based on all external documentation, which is intended for the customer. To provide 
the documentation in the form of a user-centered quick guide, the requirement analy-
sis is focused on use cases and major use scenarios, which are developed in the  
product planning phase of the development process. Most of these use scenarios are 
specified for the different roles that interact with the product like users, professional 
users, and maintenance technicians. 
The research phase ranges from the first creation of technical data to the comple-
tion of the pilot series. The main task in the research phase is to gather norms, stan-
dards, and internal technical documentation (e.g. exploded drawing or virtual three 
dimensional models) and analyze these documents with regard to their relevance for 
the documentation process. Due to the fact that the proposed quick guide is designed 
to be a supplement to the conventional documentation, the focus in this phase of the 
proposed approach lies on internal technical documentation, which is generated in the 
different development phases. 
The creation phase and quality assurance of the documentation process are inte-
grated into the development of the pilot and serial production. The creation phase and 
quality assurance are depicted in the chapter 3.2 and 3.3. The distribution of the quick 
guide, which is described in chapter 3.4, takes place simultaneous to serial  
production. 
3.2 
Creation Phase of the Documentation Process 
The creation phase of the documentation process starts with the development of the 
pilot production as the pilot products should have the same properties and appearance 
and is continued in the serial production process. The proposed quick guide is in-
tended as a supplement to the common product documentation and should include a 
reference to the common documentation. Unlike common documentations, the pro-
posed quick guides shall be centered on the user and not on the technical specification 
of a product. The focus of the quick guides lies on a short presentation of a step by 
step walk through the use cases and major use case scenarios, which can be performed 
by the different user roles like users, professional users, and maintenance technicians. 
Depending on the user role, the quick guide has to be comprehensible without any 
prior knowledge of the product or its functions. 
To make the guides easy to understand and to facilitate a quick usage without han-
dling errors, the operational instructions should be graphically presented e.g. via a se-
quence of photos with a short additional description. The graphic representation of the 
product shows the exact product and not similar versions with different appearances. 

638 
M. Abramovici, A. Krebs, and T. Schindler 
 
Operational steps shall be understood from their graphical representation without reading 
additional text. A major focus of the quick guides must also lie on safety.  
Thus, all operational steps that entail risks for the user or the product have to be 
highlighted by caution marks in the quick guide. To ensure safe operation, the caution 
marks have to be conspicuous and clearly distinguishable from normal additional text.  
To address a broad range of users, the short documentation should be available in a 
multicultural and multilingual version, and should be accessible from different devic-
es. In order to be sufficiently displayable on different devices, the quick guides must 
be formatted only in widely used format types like html or pdf. 
3.3 
Quality Assurance of the Documentation Process 
According to the approach described in Fig. 1, the fifth step of the documentation 
process is effected in the pilot production and the serial production phase. In these 
phases, the quality of the conversion of the requirements according to the complete-
ness of the internal documents, the context of usage, and the involved roles are vali-
dated. Moreover, the use case scenarios that have been developed and described in the 
product planning phase of the development process are examined and, if necessary, 
optimized. The contents of these use case scenarios include e.g. the description of 
installations and frequently occurring situations, as well as references to more detailed 
information. In order to support this step in terms of user-centered design, the quality 
of the document is checked again through tests that involve different user groups. In 
this phase, the quick guide, which is described in chapter 3.4, is developed, and then 
tested in step four of the documentation process. The details of these quality tests are 
lead back to the requirement step. The new information can then be used for the next 
loop in the V model, and serves as feedback to the designers. 
The output of the quality assurance process step is a quality assured product docu-
mentation, in this case a quick guide. This quick guide is ready for integration into the 
distribution process which is described in the following chapter. 
3.4 
Distribution of the Quick Guide via the Internet of Things 
The proposed short documentation can easily be made available via the Internet of 
Things, based on internet-capable mobile devices and marking technologies. The use 
of mobile devices like e.g. tablet computers or smart phones with embedded cameras 
or embedded NFC readers facilitates access to the quick guides of product items that 
are associated with identification marks such as barcodes or NFC tags. Fig 2 shows a 
schematic architecture for the distribution system of the quick guides. The left side of 
Fig 2 shows how a product item is prepared for later use of the quick guide, while the 
right-hand side of Fig 2 illustrates the access to the quick guide. 
To facilitate the later use of the proposed short documentation, the product items 
are labeled with identification marks like 1D barcodes, 2D barcodes, or NFC tags. 
These identification marks store references or URL to a folder with different versions 
of the respective short documentation for different devices, user roles, and languages. 

 
Design for Usability by Ubiquitous Product Documentation 
639 
 
 
Fig. 2. Schematic of the distribution system for the quick guide 
After the producer has prepared the product items, users can access a version of the 
quick guide suitable to them. To access a suitable version of the documentation, the 
users have two basic possibilities: 
The users can either use a general reader application like a widely available QR-
code reader application, to access the folder, which is referenced by the URL (cf. Fig 
2: URL1) to choose a suitable version of the short documentation.  
A considerably more direct and comfortable way to access the short documentation 
is provided if the producer offers a special reader application that transforms the URL 
from the mark (cf. Fig 2: URL1) into another URL (cf. Fig 2: URL2), which directly 
generates a suitable version of the quick guide.  
The producer can store the quick guide in her or his product data management 
(PDM) systems [9] for internal purposes and for access of business customers for 
technical industrial goods, which are produced in small numbers. For products that 
are produced in large quantities, the producer can better use dedicated webserver or 
cloud computing solutions. 
4 
Validation 
As a first validation of the proposed approach, two case studies have been carried out 
with products that are already (in the phase) of serial production. Both case studies 
have been carried out with technical capital goods. In the first case study, a pump with 
a capacity of 5000 liters per minute has been used (cf. Fig 3, right-hand side). The 
second study deals with the use of a concrete chainsaw and a supplying hydraulic 
aggregate (cf. Fig 3, left side). The two studies have been carried out in cooperation 
with the German department of civil defense “Bundesanstalt Technisches Hilfswerk” 
(THW). To provide a quick guide in an easily accessible way, QR codes with stored 
URLs are attached on the product items. These URLs have been used with smart-
phones to access documents formatted as pdf via the internet. Several teams with 
different levels of knowledge about the usage of the related technical capital goods 
have taken part in the case studies. 

640 
M. Abramovici, A. Krebs, and T. Schindler 
 
 
Fig. 3. Case studies using the approach with capital goods 
The majority of the participants in the case studies already had a smartphone with 
an installed QR code reader application. 
The first significant result of the two case studies is the decrease in the number of 
operating errors through the use of the proposed quick guides. 
Another result of the cases studies is that even inexperienced users that are sup-
ported by the quick guides were capable of handling the complicated devices without 
significant errors. A fundamental result of both case studies is that inexperienced 
users with the quick guides scored nearly the same rate of operation errors as expe-
rienced users supported by the guides. The paradox of this ratio of operation errors 
lies in the fact that the experienced users often act intuitively and thus ignore steps 
that are performed only rarely. Inexperienced users strictly perform every operational 
step of the quick guide to the best of their knowledge. The studies further show that 
experienced users with the quick guide have no considerable time advantage. 
The studies` results show that there are many similarities between the two use cas-
es with regard to the handling of the products supported by the quick guide via smart-
phone. Nevertheless, the studies show only few differences about the advantages of 
the quick guides. This can be based on the different dimensions of the considered 
products. The handling of the pump shows that, with the help of the quick guide, both 
experienced and inexperienced users have a strong focus on security aspects such as 
the use of protective clothing, provision of a fire extinguisher, and the use of ear pro-
tection. This leads to a secure handling for complex products like the considered 
pump. The handling of the concrete chain saw supplied by a hydraulic aggregate 
shows that, with the support of the quick guide, fewer errors were made that could 
have led to system damage. 
In conclusion, the use case studies show that, in a normal training situation, the de-
veloped approach can lead to a secure handling of the products with a minimum risk 
of damaging the products themselves. This result applies to both the experienced as 
well as the inexperienced user groups. 
Nevertheless, it is most likely that in stressful situations like disaster operations, 
the experienced user groups will achieve better results. 

 
Design for Usability by Ubiquitous Product Documentation 
641 
 
5 
Outlook and Future Work 
A possible extension of the presented approach can deal with the use of further con-
tents and devices. Examples for especially useful kinds of content are virtual reality, 
animations or videos, which shows a sequence of steps of a use scenario. Examples 
for devices with potential for swift execution of the approach are tablet computers 
with large displays and high resolutions, and glasses with heads-up displays, or em-
bedded monitors. The use of such glasses offers further possibilities like the two-
handed product use. Additional potential lies in the use of augmented reality concepts. 
More benefits that can be achieved with the presented distribution system lie in the 
fields of logistics, anti-counterfeiting [10], and the strengthening of Customer Rela-
tionship Management (CRM). The distribution system can also be used to offer a 
new, fail-proof way of ordering spare parts and follow-up models of products, which 
can be referenced in the quick guides. 
References 
1. VDI-Richtlinie 2221: Methodik zum Entwickeln und Konstruieren technischer Systeme 
und Produkte. Beuth Verlag, Berlin (1993) 
2. VDI-Richtlinie 2206: Entwicklungsmethodik für mechatronische Systeme. Beuth Verlag, 
Berlin (2004) 
3. Nattermann, R., Anderl, R.: Simulation Data Management Approach for Developing 
Adaptronic Systems – The W-Model Methodology. In: Proceedings of the WASET 2011 
International Conference in Software and Data Engineering (ICSDE), Bangkok (2011) 
4. Ehlenspiel, K.: Integrierte Produktentwicklung. Carl Hanser Verlag, München (2009) 
5. Kota, S., Brissaud, D., Zwolinski, P.: Importance of User and Usage for Eco-Design. In: 
22nd CIRP Design, India, ch. 35 (2012)  
6. ISO 9241-210:2010: Ergonomics of human-system interaction - Part 210: Human-centred 
design for interactive systems. Beuth Verlag, Berlin (2010) 
7. VDI-Richtlinie 4500 Blatt 1: Technische Dokumentation – Begriffsdefinitionen und rech-
tliche Grundlagen. Beuth Verlag, Berlin (2006) 
8. VDI-Richtlinie 4500 Blatt 4: Technische Dokumentation: Dokumentationsprozess Planen, 
Gestalten, Erstellen. Beuth Verlag, Berlin (2011) 
9. Abramovici, M., Schulte, S.: PLM - Neue Bezeichnung für alte CIM-Ansätze oder Weite-
rentwicklung von PDM? In: Konstruktion - Zeitschrift für Produktentwicklung und Inge-
nieur-Werkstoffe. Springer, Düsseldorf (2005) 
10. Abramovici, M., Krebs, A.: Anti-counterfeiting based on product authentication with 
prouct-related data. In: Horvath, I., Rusak, Z., Alberts, A., Behrendt, M. (eds.) Proceedings 
of the 9th International Symposium on Tools and methods of Competitive Engineering, 
TMCE 2012, Karlsruhe, Germany, Mai 07-11, vol. 2, pp. 879–892. Delft University of 
Technology (2012) ISBN 978-90-5155-081-8 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 643–652. 
DOI: 10.1007/978-3-642-30817-8_63 
© Springer-Verlag Berlin Heidelberg 2013 
 
Simulation in Human-Centered Design –  
Past, Present and Tomorrow 
Jörg Miehling, Daniel Krüger, and Sandro Wartzack 
Friedrich-Alexander-University Erlangen-Nuremberg, Chair of Engineering Design 
Martensstr. 9, Erlangen, 91058, Germany 
{miehling,krueger,wartzack}@mfk.uni-erlangen.de  
Abstract. Due to the ever rising significance of the products’ usability the con-
sideration of the user in the product development process becomes more and 
more important to stay competitive. This paper firstly outlines the historical 
evolution of how user-product relationships have been investigated and consi-
dered in product design in the past. A literature research has been conducted to 
acquire information about the state of the art in the multidisciplinary field of 
human-centered design as well as to derive which challenges towards a virtual 
assessment of human-related product properties exist. Finally the most recent 
research activities aiming to solve the identified issues as well as future pros-
pects in simulating human-product interactions are presented. 
Keywords: human-centered design, ergonomics, simulation, virtual product 
development, digital human modeling, biomechanics, hybrid mock-up. 
1 
Human-Product Relationship in Design 
Until the early 1960s the dominating view on product design was affected basically 
by technical and economic aspects. Designing a product used to be defined as the 
process of finding a technical perfect, economic reasonable and aesthetic satisfactory 
solution for a given problem [1]. Although the last criterion is in some way human-
related, user-product relationships were not explicitly in the focus of design theory. 
The American industrial designer HENRY DREYFUSS is regarded a pioneer in the 
field of human-centered design as he was one of the first to explicitly incorporate the 
user into design theory: it must be borne in mind that the object being worked on is 
going to be ridden in, sat upon, looked at, talked into, activated, operated, or in some 
way used by people […] If the point of contact between the product and people be-
comes a point of friction, then the designer has failed [2]. His work introduced a new 
scientific view on design and had been an important foundation for the emerging 
fields of ergonomics, anthropometry and human factors. Along with an increasing 
complexity of technical systems (computer science and aeronautical engineering), an 
awareness for the whole system, formed by the product and its user, grew among 
designers. In the following decades several models have been developed that describe 
the complex relationships between user and product from a multidisciplinary point of 
view involving engineering, anthropometry, psychology, sociology and industrial 
design. The model schema depicted in Figure 1 contains all the basic elements  

644 
J. Miehling, D. Krüger, and S. Wartzack 
necessary to describe the relationships between the product and its user: it is assumed 
that the product is described by a set of extensive properties which are influenced by 
decisions made during the design process. The user as a human being on the other 
side is described by demographic and psychographic characteristics according to [3]. 
The interaction between user and product is modeled as a process of perception and 
response. Based on perception and recognition the user is able to assess the properties 
of the product and choose his behavior. Most important the behavior includes all ac-
tivities necessary to operate the product. 
 
 
Fig. 1. Model of human-product relationships based on [3] 
The objective of human-centered design activities is to create a product whose 
properties match the characteristics of the user and the environment in the best possi-
ble way. Therefore it is crucial for the designer to have as much information as possi-
ble on the prospective user and the way the interaction with the product takes place. 
This paper examines the application of simulations used to analyze and predict as-
pects of the interaction between user and product in order to assess design concepts 
with respect to human-related properties. Based on a literature survey early examples 
are given where mock-ups have been used to optimize human-machine interactions. 
Furthermore, examples that illustrate the state of the art are used to reveal which chal-
lenges towards a virtual assessment of human-related product properties exist. In 
addition the authors’ recent research activities to address the identified issues are 
presented. The paper closes with the explanation of future steps to be taken to en-
hance the simulation of human-product interactions. 
2 
Simulation in Human-Centered Design 
2.1 
Approaches towards Simulation of Human-Product Interactions 
According to VDI 3633 [4] the term simulation defines a procedure to emulate a real 
or fictive system including its dynamic processes by means of a model in order to 
conduct experiments which lead to insights that can be transferred back to the real 
Environment
Interaction
Product
technical
economical
human-related
Properties
Human
demographic
and
psychographic
Characteristics
perception
recognition
behaviour/assessment

 
Simulation in Human-Centered Design – Past, Present and Tomorrow 
645 
system. A literature survey has shown that examined approaches towards simulating 
human-product interactions can be classified as shown in Figure 2. 
 
 
Fig. 2. Approaches towards the simulation of human-product interactions 
The most basic idea of a simulation is the physical mock-up. The interaction is ex-
amined empirically by presenting a simplified prototype of the product to real test 
persons. The major drawback of this approach is the expense of time and cost that the 
making of physical prototypes entails. This can partly be overcome by a hybrid mock-
up, a simulation system containing both, physical and virtual components. The idea is 
to emulate the human-machine interface of the product in a multimodal way so that a 
real test person is enabled to interact with a virtual prototype. For this purpose ele-
ments of the product which require haptic interaction are represented in hardware 
whereas the visual appearance, aural feedback as well as the physical behavior of the 
product is reproduced by computer simulations and immersive projection systems. 
Instead of trying to put the product into reality the philosophy of digital human mod-
els is to put the user into virtuality in order to examine the interaction with a virtual 
model of the product. In product design the human aspects of interest and therefore 
relevant modeling domains are mainly biomechanics, anthropometry and percep-
tion/cognition. 
In the following sections examples of the application of simulations in user-
centered design are presented based on a literature survey.  
2.2 
Physical Mock-Ups 
The Link-Trainer (Figure 3) has been developed in the 1930s to accelerate the educa-
tion of pilots including instrument flying. The system uses electric and pneumatic 
actuators and controllers to mimic the physical behavior of an airplane during flight. 
Even though the intention of the Link-Trainer is not directly related to product design, 
it is an early example of a sophisticated physical mock-up used to examine human-
machine interactions. [5] 
Despite the extensive use of computer simulations in product design physical 
mock-ups are still being used especially when focusing on aesthetic or ergonomic 
aspects of a new product. An example is the continuing use of clay models in the 
automotive industry. 
Modeling Domains:
biomechanics
anthropometry
perception / cognition
Digital Human Models
virtual human in 
virtual environment
Emulation of the Human-Machine
Interface (HMI) 

visual

aural

haptic
multimodality
Hybrid Mock-Up
real test person
interacts with virtual
environment
•
physical prototype of the
product
•
simplified to the aspects of
interest
Physical Mock-Up
real test person
interacts with real 
environment

646 
J. Miehling, D. Krüger, and S. Wartzack 
 
Fig. 3. The Link-Trainer as an historic example of a mechanical simulation system [5] 
2.3 
Hybrid Mock-Ups and Immersive Environments 
The core technology of a hybrid mock-up is virtual reality. The objective is to emulate 
the behavior of a virtual prototype in a way that it is accepted as real by the user. The 
effect that the user is no longer able to distinguish between virtuality and reality is 
called immersion. In product design this can be used to examine the interaction of test 
persons with virtual product prototypes without having to produce fully functional 
physical mock-ups. 
In [6] a commercially available haptic device (force feedback device) controlled by 
a dynamic simulation was used to mimic the feel of a manual gearshift typically built 
into passenger cars. In order to validate the degree of realism a usability test has been 
conducted with several test persons. The test persons had to assess the forces neces-
sary to operate the virtual gearbox in comparison to a physical gearbox whose me-
chanical design served as a blueprint for the simulation. Such a system could possibly 
be applied as a supporting tool during the design of gearboxes in order to find out 
about what the mechanical behavior of the gearshift should look like according to the 
customers.  
Another interactive simulation system for mechanical products has been presented 
recently in [7]. The system comprises a stereoscopic projection system, a universal 
haptic device and a real-time simulator. In a case study the usability of a crank driven 
car jack is analyzed. The setup can be seen in Figure 4. It is assumed that the usability 
and user experience is influenced by the interplay of motion and forces imposed on 
the user during operation. In the present use case the haptic device emulates the crank 
of the car jack. As soon as the user moves the crank he will not only see the  
 

 
Simulation in Human-Centered Design – Past, Present and Tomorrow 
647 
mechanism move but also perceive the mechanical load acting back on the crank. It is 
important to notice that the system behavior is computed in real-time in dependence 
of some design variables. This means that the test person can change some characte-
ristics like e.g. the gear ratio and instantaneously perceives the altered product beha-
vior. In this way the users can be incorporated intuitively into design activities. 
 
Fig. 4. Hybrid Mock-Up of a car jack [7] 
2.4 
Digital Human Models 
Digital human models aim at the virtual representation of the human appearance and 
behavior. These usually cover just partial areas of the huge variety of human proper-
ties and abilities. Most models are specifically made for delimited use cases such as 
anthropometric, biomechanical, anatomical or cognitive analyses. [8] 
The three most commonly used man models in industry are Jack (Siemens PLM), 
Human Builder (Dassault Systèmes) and RAMSIS (Human Solutions) which are 
shown in Figure 5. [9] 
Jack for example is mainly applied for the evaluation of the interior design of ve-
hicles. In contrast Human Builder, initially named Safework, was made for workplace 
design but nowadays additionally offers tools for anthropometric and comfort analys-
es. In this regard, comfort analyses evaluate static poses regarding their joint angle 
constellations. RAMSIS, the most popular man model, additionally consolidates ex-
tensive methods for the simulation of the comfort inside vehicle systems such as mir-
ror, seat and belt analyses. [8] 

648 
J. Miehling, D. Krüger, and S. Wartzack 
 
Fig. 5. Important Digital Human Models based on [9] 
Jack, Human Builder as well as RAMSIS, as examples for anthropometric digital 
human models, are available inside predominant CAD systems and are therefore ca-
pable of examining interactions between users and products. They consist of a skele-
ton and a skin model and are mostly scaled using anthropometric databases before 
ergonomic analyses are conducted. Anthropometric models are powerful tools for 
visualizing reach spheres as well as the field of vision. On the other hand the main 
weakness of pure anthropometric models is that these manikins usually provide just 
static and comfort analyses and hence cannot take the physical dynamic effects of 
body movements into account. 
To consider these effects as well as to analyze muscle and joint loads, biomechani-
cal digital human models are being developed. These models still lack wide applica-
tion in industry, not least due to unresolved issues concerning the consideration of 
user-product interactions. Consequently, their biggest field of application is still in 
research. Biomechanical models, also known as musculoskeletal models, commonly 
contain a skeleton, modeled as multi-body system, as well as muscles acting as actua-
tors. The underlying simulation systems are capable of conducting dynamic simula-
tions, whereas muscle forces and activations can be determined. These can be  
regarded as the cause of a given motion behavior. 
RASMUSSEN et al. proposed musculoskeletal modeling as an ergonomic design me-
thod as well and therefore developed the so called AnyBody Modeling System. To 
confirm their hypothesis, they optimized the design of a hand saw handle and ana-
lyzed the forces acting on a human sitting on a chair. [10] 
An example for a freely available biomechanical simulation environment is 
OpenSim (Stanford University) which can be seen in Figure 5. [11] 
Human                Jack               RAMSIS         OpenSim
Builder

 
Simulation in Human-Centered Design – Past, Present and Tomorrow 
649 
MOES and HORVATH are following a completely different approach. They are using 
finite element models of the human body, for example for the optimization of seat 
shapes. [12] 
3 
Need for Research and Recent Activities 
The examples presented in the previous sections provide a good outline on the exist-
ing possibilities to simulate the interaction between users and products. When com-
pared to physical and hybrid mock-ups, digital human models have considerable pros 
with respect to flexibility of application, time and costs. Since the simulation takes 
place entirely “in silicio”, no expensive prototypes or specialized hardware devices 
are necessary. Therefore digital human models are ideal for a continuous application 
throughout the whole product development process starting from early conceptual 
phases. Because it can be expected that results become available in a comparable 
short period of time unavoidable design iterations are accelerated. However digital 
human modeling is a challenging field of research. Even simple human-product  
interactions like the opening of a car door, involve highly complex cognitive and 
sensorimotor processes which up till today can’t be described using deterministic 
mathematical models. State of the art simulation systems therefore are based mostly 
on phenomenological approaches. Biomechanical analyses e.g. like those presented in 
[10] use recorded and therefore empirical motion data in combination with inverse 
dynamics and numerical optimization to determine muscle activation patterns. The 
standard process of gathering those motions is usually associated with costly marker-
based motion capture techniques. Aiming towards a purely virtual simulation proce-
dure this approach has to be considered problematic since the motion which  
determines the interaction with the product is observed rather than predicted. There-
fore further effort has to be put on the improvement of motion prediction procedures. 
One possible approach is to combine forward dynamics computations with numerical 
optimization methods. The major drawback of this approach is the extremely high 
computational load. A simulation of half a human gait cycle [11] required more than 
10,000 CPU hours on a machine featuring 32 cores.  
The authors’ research activities therefore focus on fast, simplified simulation  
approaches that enable product designers to access the power of biomechanical simu-
lations. In [13] the integration of such biomechanical simulations into engineering 
environments is proposed. The underlying framework is depicted in Figure 6. For a  
successful biomechanical analysis the applied digital human model has to match the 
corresponding real person performing a specific task. To achieve this goal a tool for 
the fast acquisition of body measures has been implemented using the inexpensive 
markerless depth imaging device Microsoft Kinect. The gathered data enables the 
calculation of scaling factors for each body segment and eventually enables the scal-
ing of the used OpenSim model. The mentioned tool additionally allows the marker-
less capture of body movements which are being used to extract the underlying  
motion styles which in turn are used as reference in the synthetic generation of realis-
tic motions. The mentioned simplified approach comprises advantages of inverse and 

650 
J. Miehling, D. Krüger, and S. Wartzack 
forward dynamics simulations. The interaction between the simplified kinematical 
dummy and the virtual product model (e.g. CAD geometry) is defined by some geo-
metric motion objectives. These serve as boundary conditions for the inverse kinemat-
ics simulation, which can be solved in real-time. The functioning of the inverse kine-
matics solver is described more detailed in [13]. The result of the inverse kinematics 
step is a motion sequence. This motion has to be calculated taking the motion style of 
reference motions into account to achieve the natural motion behavior of real humans. 
Additionally the external loads acting on the human body while conducting the calcu-
lated motion sequence are to be obtained by some CAE simulations. The calculated 
motion data as well as the external loads are then transferred to the scaled biomechan-
ical digital human model to execute the biomechanical simulation where the desired 
dynamic quantities and eventually muscle activation patterns can be computed. In 
[13] the mentioned motion generation approach is demonstrated by a steering motion, 
whereas the driver’s hand follows the circular trajectory of the steering wheel and the 
torso is fixed to the car’s seat. 
 
 
Fig. 6. Simulation Framework [13] 
4 
Future Prospects 
Biomechanical digital human models appear to be the most promising ones for future 
applications in the field of product development due to the possibility of considering 
dynamic effects of the musculoskeletal system. The main reasons to integrate biome-
chanical man models into CAD/CAE environments are to increase their acceptance 
and consequently their application among product designers as well as to improve the 
motion style
CAD / CAE - environment
biomechanical model
product model
kinematical
dummy
inverse kinematics
user data
(body measures, motion style)
inverse dynamics
dynamic quantities
scaling
motion
external loads
capturing of data

 
Simulation in Human-Centered Design – Past, Present and Tomorrow 
651 
simplicity of their use to enhance their efficiency. The configuration of user-product 
interactions should therefore be easy to model. Thus, the motion generation should 
work without the need for captured motions. A promising attempt to accomplish these 
objectives is already done by using an approach based on inverse dynamics as stated 
above. The mentioned methods still have to be refined to be able to fully describe the 
interactions between the human body and a product or environment, especially the 
synthetic generation of natural motions taking into account external loads. This would 
eliminate the necessity for motion capture techniques. Furthermore tools have to be 
developed to enable product designers to compare and optimize product variants 
without deep knowledge in biomechanics and human factors engineering. Additional-
ly the development of individualized biomechanical digital human models should be 
promoted, which can then be utilized for the evaluation and optimization of products 
regarding individual users or user groups. These models should then be able to take 
into account for example differences in maximum isometric forces and range of mo-
tion extending the currently existing scaling functionalities. [13], [14] 
5 
Summary 
In this paper a general motivation for simulation in human-centered design has been 
given. Due to the virtuality of digital human models as well as that no physical prod-
uct prototypes are necessary for their application they can be used even in the early 
stages of product development. In general they are flexible to use, but still face some 
issues to be addressed in future research. Biomechanical models currently seem to be 
most promising for sophisticated applications in the field of product development. 
The decisive advantage over other model types is their possibility of taking dynamic 
effects of the musculoskeletal system into account. Moreover, these emerging virtual 
methods are to be incorporated into contemporary CAD/CAE environments to further 
increase their acceptance among product designers. Nevertheless, there are still rea-
sons which justify the existence of hybrid and physical mock-ups, especially when 
emotional product properties are to be evaluated like for example the shifting sensa-
tion in virtual gearshift simulations or the haptic perception of consumer products. 
References 
1. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Pahl/Beitz Konstruktionslehre: Grundla-
gen erfolgreicher Produktentwicklung, 7th edn. Methoden und Anwendung. Springer, Hei-
delberg (2006) 
2. Dreyfuss, H.: Designing for People. Allworth Press, New York (2003) 
3. Seeger, H.: Design technischer Produkte, Produktprogramme und -systeme: Industrial De-
sign Engineering, 2nd edn. Springer, Heidelberg (2005) 
4. VDI 3633, Part 1. Simulation of systems in materials handling, logistics and production: 
Fundamentals. Beuth Verlag, Berlin (2010) 
5. Angelo, J.D.: The Link Flight Trainer: A Historic Mechanical Engineering Landmark. 
ASME International, Roberson Museum and Science Center, New York (2000), 
http://files.asme.org/ASMEORG/Communities/History/Landmarks/
5585.pdf 

652 
J. Miehling, D. Krüger, and S. Wartzack 
6. Tideman, M., van der Voort, M.C., van Houten, F.J.A.M.: Design and Evaluation of a Vir-
tual Gearshift Application. In: IEEE Intelligent Vehicles Symposium, Parma, pp. 465–470 
(2004) 
7. Krüger, D., Stockinger, A., Wartzack, S.: A haptic based Hybrid Mock-Up for mechanical 
products supporting human-centered design. In: Proceedings of the 18th International Con-
ference on Engineering Design, Kopenhagen, pp. 331–340 (2011) 
8. Bubb, H., Fritzsche, F.: A Scientific Perspective of Digital Human Models: Past, Present, 
and Future. In: Duffy, V.G. (ed.) Handbook of Digital Human Modeling. Research for Ap-
plied Ergonomics and Human Factors Engineering, pp. 1–26. CRC Press, Boca Raton 
(2009) 
9. Mühlstedt, J., Spanner-Ulmer, B.: Homo Sapiens Digitalis: über den Praxiseinsatz digitaler 
Menschmodelle. In: Lichtenstein, A., Stößel, C., Clemens, C. (eds.) Der Mensch im Mit-
telpunkt Technischer Systeme. 8. Berliner Werkstatt Mensch-Maschine-Systeme, pp. 7–8. 
VDI Verlag, Düsseldorf (2009) 
10. Rasmussen, J., Dahlquist, J., Damsgaard, M., de Zee, M., Christensen, S.T.: Musculoske-
letal modeling as an ergonomic design method. In: International Ergonomics Association 
XVth Triennial Conference, Seoul (2003) 
11. Seth, A., Sherman, M., Reinbolt, J.A., Delp, S.L.: OpenSim: a musculoskeletal modeling 
and simulation framework for in silico investigations and exchange. In: Proceedings of the 
IUTAM Symposium on Human Body Dynamics, Waterloo (2011) 
12. Moes, N.C.C.M., Horvath, I.: Using Finite Elements Model of the Human Body for Shape 
Optimization of Seats: Optimization Material Properties. In: Proceedings of the 7th Inter-
national Design Conference, Dubrovnik, pp. 857–866 (2002) 
13. Krüger, D., Miehling, J., Wartzack, S.: A simplified approach towards integrating biome-
chanical simulations into engineering environments. In: Proceedings of the 9th Nordde-
sign, Aalborg (2012) 
14. Paetzold, K., Wartzack, S., Miehling, J.: Challenges in the Design of Products for Elderly 
People. In: Proceedings of the 9th International Workshop on Integrated Product Devel-
opment, Magdeburg (2012) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 653–662. 
DOI: 10.1007/978-3-642-30817-8_64 
© Springer-Verlag Berlin Heidelberg 2013 
 
Distance Collaboration Support Environment  
Roy Damgrave and Eric Lutters 
Laboratory for Design, Production and Management, University of Twente 
Drienerlolaan 5, Enschede, 7500AE, The Netherlands 
{r.g.j.damgrave,e.lutters}@utwente.nl 
Abstract. This paper explains a new distance collaboration support environ-
ment for use in product development processes. This environment makes it 
possible to organize a meeting with up to eight stakeholders in two locations, 
where there is minimal distraction by the distance. Not only audio and video are 
shared, but also physical products placed on a table and hand movements are 
visible for each participant. This allows for pointing with a finger at objects 
which are only present at one location. All these technological possibilities are 
integrated in one physical setting to minimize start-up times and to ensure that 
all remote locations are comparable. Consequently, so participants know exact-
ly what participants at the remote location see. 
Keywords: virtual reality, multi-stakeholder, distance collaboration, virtual 
presence. 
1 
Introduction 
Working in a group setting implies communication and collaboration with other 
stakeholders to share thoughts and opinions on certain aspects within the development 
process. The most commonly used method to actualize the need for communication 
and collaboration is in the form of a meeting. These meetings are often facilitated by 
communication equipment to enhance the possibilities and methods for collaborating. 
This varies from a simple audio connection by telephone, to a live video supported 
workspace. Even though the availability and possibilities of the equipment is con-
stantly increasing, a noticeable difference remains between local collaboration and 
collaboration over a distance. 
2 
Collaboration in Product Development 
In current product development cycles, the need for fast decision making, incorporat-
ing external expertise and collaborating with other (remote) stakeholders is crucial. In 
order to utilize the expertise of all different stakeholders to its fullest extent, the way 
of mutual interaction should be as little disrupting and distracting as possible. This 
presupposes an effective connection between different stakeholders during the various 
phases of a project. Traditionally, this results in consequential travelling efforts. In 

654 
R. Damgrave and E. Lutters 
 
reducing the inefficiencies involved in travel and doing justice to the relevance of 
collaboration amongst (global) stakeholders, the need for adequate remote collabora-
tion tools is increasing.  
Tradition entails that the most common way of interaction in a project is by means 
of a meeting. They are often organized with a specific goal in mind, based on the 
available participants and the current state of the product development. This implies 
that different meetings require different set-ups, working methods and tools. 
3 
Remote and Local Meetings 
A clear distinction, which is independent of the goal and aim of the meeting, can be 
made between meetings taking place at one location (sharing the same environment) 
and meetings that involve stakeholders at multiple locations (i.e. not physically in the 
same environment). The way in which a meeting is organized and held is often based 
on the location of the participants, the available budget or time and the goal of the 
meeting. The purpose of the meeting has direct consequences for the required types of 
communication; the challenge is to define on beforehand what communication and 
interaction possibilities are necessary and useful. 
Currently, research efforts are directed towards nullifying the differences between 
‘local’ and ‘remote’ meetings. As such, the intent is to address ‘remote’ meetings as if 
they were ‘local’. Technical means are employed to get across the disadvantages of 
not being in the same environment. 
Yet, there still is a substantial difference between local and remote meetings; in 
general, a local meeting (with participants sitting in the same environment, experienc-
ing the same equipment, around the same table) is experienced to be more efficient 
and natural. 
In the future, there might be different means to structure, configure and deploy 
meetings that involve multiple locations. For short and mid-term solutions, focus 
should, however, be on diminishing the dissimilarities between local and remote 
meetings by improving the technical means and working methods that facilitate the 
remote meetings. 
4 
Distance Collaboration  
4.1 
Disadvantages of Current Approaches 
Currently, remote meetings often only use video and audio communication equipment 
to enable visual and aural communication. With these video conferencing systems, a 
combination of camera and microphone on each location is used for recording, while 
playback is done on monitors or televisions. The availability of tools to facilitate this 
form of communication is increasing continuously, where the quality of the video and 
audio increases. Interestingly enough, above a certain threshold, the quality of the 
video and audio signal not necessary increase the quality of the meeting. 

 
Distance Collaboration Support Environment 
655 
 
Communication tools often allow for the integration of additional functionality like 
screen sharing, virtual whiteboarding and file sharing. Such additional functionalities 
enable participants to extend collaboration beyond merely audio and video, thus ad-
dressing for example sharing of information, documents and even haptic feedback. 
The need for such possibilities stems from the wish to present and discuss digital and 
physical content belonging to the project, rather than only having a spoken discussion. 
Nevertheless, no video conferencing tool covers the entire spectrum of possible 
collaboration methods. Moreover, as mentioned, they do not provide the same expe-
rience as sitting together at a table. The reason for this is that in local meetings, much 
more aspects than just seeing someone’s face and hearing his voice are relevant; also 
objects and information on the table or body language provide us (even subconscious-
ly) with information.   
Most videoconferencing systems show a video of the participants as if looking 
through a window, while in actual local meetings the participants are not cooped in a 
rectangular frame. During remote meetings, people often are distracted by back-
ground activities of both the local and the remote locations. This also implies an inter-
esting requirement since people get a distant feeling if differences in weather or time 
are visible during a meeting. 
Moreover, it is usually not clear what every participant can see and observe. This 
results in a lack of feedback from the other participants, causing them to doubt if oth-
ers actually heard and understood what has just been said. Participants often complain 
that a lack of feedback from other participants, either aural or visual, caused much 
repetition and explicit confirmation request [1]. Frequent repetition of statements, and 
frequent meta-interactions “did you hear what I just said” obviously hamper the con-
tent and progress of the meeting. As such, the organization and execution of the meet-
ing often becomes one of the most important topics of the meeting. Therefore, in 
many meeting it is infeasible to separate the means and ends of it. 
Not only the lack of feedback from other participants due to indistinct and distract-
ing technology can cause a feeling of distance, the awareness of the communication 
system itself is likely to create a feeling of distance as well [2]. This awareness can, 
for example, arise if the users experience the system as an explicit interface between 
them, or when dynamics in the meeting (e.g. with respect to the number of partici-
pants) imposes a physical re-arrangement of equipment.  
A meeting with the use of video conferencing equipment is often experienced as 
less personal, because participants felt as if the speaker was addressing them as a 
group, and not as individuals [3]. This mainly occurs because eye contact isn’t possi-
ble, and when the sound direction is independent from the source of the sound. The 
risk of misinterpretation of communication is always present due to misreading body 
language. This can have technical causes, e.g. by lag or hiccups in the connection; 
more often than not the cause is non-technical, as traditional video and audio equip-
ment is simply incapable of transmitting the signals related to body language. This 
can also be caused by receiving an incomplete image of the remote participants, be-
cause a face can express a lot of emotion but it does not reveal nervousness visible by 
shaky hands [2], [4].  
 

656 
R. Damgrave and E. Lutters 
 
Furthermore, reviewing and discussing shared objects is problematic, because it is 
not possible to physically point at (shared) objects, which makes it difficult to discuss 
specific parts of the objects. This issue especially occurs when reviewing digital data 
with more than two persons, or when explaining a specific point on a physical object 
which is available on only one location. In these cases the need for seeing the arms 
and hands of the participants is a first prerequisite. Where a person’s eyes can quickly 
change focus from a presenter’s face, his pose and the object of discussion, this is 
well-nigh impossible to achieve with video conferencing equipment. 
4.2 
Requirement Specification for Support Environment 
Based on the previous paragraph, supporting distance collaboration can be done by 
confining the environment of the meeting, to enable the participants to know what to 
expect. In a support environment the participants should not be seen as remote loca-
tions ‘connected by wire’, but a virtual environment should be created where all 
stakeholder seem to participate in. This confined environment, like a table, can be 
present in all remote location and other stakeholders can virtually ‘join’ all tables. The 
environment should virtually project all stakeholders on each other’s real-world se-
tups; having the same setup enables this.  
In this environment it is essential to not only see the face and facial expression in 
real time, but also physical activity of all persons at the local and the remote table. 
The perspective of this visual feedback of the remote location should closely resemble 
the perspective of the local table. Furthermore the ability to share physical items  
to the meeting will increase the clarity of discussed objects. In addition to visual feed-
back, it is also of great importance to hear the voices of all participants, where any 
participant must be able to locate the origin of the sound. To achieve this, the audio 
playback location should closely resemble the source of the sound. To further enhance 
the cooperation the environment should offer a digital work space that is shared by all 
persons at the local and remote table [5], [6]. This offers the opportunity to see in real 
time what other persons are working on in project documents, from everybody’s own 
perspective.  
5 
Distance Collaboration Support 
An integrated solution for distance collaboration can dispel the disadvantages of cur-
rent communication means and methods. A dedicated distance collaboration envi-
ronment to facilitate meetings is composed to create a physical setting in which  
remote meetings are experienced and executed as similar to local meetings as possi-
ble. In the case study described here, the meeting is based on collaboration between 
two locations with up to 8 participants. This is not doing justice to the more generic 
requirements as stated in section 4 yet it enables to investigate the essence of adequate 
collaboration between two locations. Consequently, users should be able to have a 
meeting with people at the other physical location with minimal distraction by the 
distance. Activities during these meetings consist of for example the planning of  
activities, reviewing data, concept development and decisions making. More commu-
nication technologies and possibilities are added to create a common ground to work 

 
 
on, and to transfer current 
meetings.  
As a rudimentary initial 
lected (figure 1). Four of th
pant, and four of them are th
Fi
5.1 
Audio and Video 
To establish an environme
four) participants as realisti
era located at approximat
processed to remove all th
projected on a half-transpa
(figure 2). The projected im
as the captured image, to ac
in a projection where only t
pants in his own local envi
remote location. Also the fe
no clear boundaries of the p
to see who is looking at wh
Furthermore, the voice o
speaker that approximates t
what sound comes from wh
to see which mouth is movi
this solution will reduce th
that is paramount in tradit
distraction is removed. 
Distance Collaboration Support Environment 
possibilities and use practices of local meetings to rem
solution, an oval shaped table with eight seats has been
hese seats are physical present at the location of the part
he virtual copies of the seats at the other locations. 
ig. 1. Schematic overview of the table 
nt that allows for the virtual presence of the other (up
ically as possible, the participants are viewed using a ca
tely eye level. The image of the virtual participants
he background, leaving only the character. This image
arent fabric which is suspended on one side of the ta
mage of the participants has the same physical dimensi
chieve a real-size view of the remote location. This res
the participants are visible, and the viewer sees the part
ironment. This avoids distraction by the background of 
eeling of looking at a screen is minimized because there 
projection. Because of the fixed configuration, it is possi
o, and to have eye contact (figure 3).  
of each participant is individually recorded and played o
the direction of the source. This makes it easier to disc
hich participant, even without having to look at the scr
ing or to know the voice of every participant. It is clear t
he necessity for exact synchronization of video and au
tional video conferencing. As such, an obvious source
657 
mote 
n se-
tici-
 
p to 
am-
s is 
e is 
able 
ions 
ults 
tici-
the 
 are 
ible 
on a 
cern 
reen 
that 
udio 
e of 

658 
R. Damgrave and E. 
 
F
5.2 
Table 
To add to the realism of the
only an audio and video co
large multi-touch screen, w
tions have the ability to ma
the touch screen, the arm 
above the screen, and proje
possible to point at a virtua
the remote location see you
the users to place physical 
3d objects, and point at the
screen (figure 4, 5). The p
ings, in a real-time editing
process, and highlights the 
data collection. 
 
Lutters 
Fig. 2. Overview of the configuration 
e meeting, the envisaged environment facilitates more t
onnection. The tabletop of the oval shaped table contain
which shows the same content at both locations. Both lo
anipulate content on the surface by touching it. While us
and hand of the user are captured using a camera pla
ected on the tabletop at the remote location. This make
al object projected on the tabletop, while the participant
ur arm and hand moving over the screen. This also allo
objects on the tabletop, varying from paper document
em with a finger or adding comments to it using the tou
possibility to work on shared documents, images or dr
g functionality is an important feature in the collaborat
direct interaction of the multiple participants on a sha
 
than 
ns a 
oca-
sing 
aced 
es it 
ts at 
ows 
s to 
uch-
aw-
tion 
ared 

 
 
Fi
Distance Collaboration Support Environment 
ig. 3. First test with the configuration 
659 
 

660 
R. Damgrave and E. 
 
Fig. 5. P
5.3 
Advantages of an In
All these technological pos
start-up times and to ensur
participants know exactly 
assessment of body langua
mote participant is the same
Even hand gestures on the
Lutters 
Fig. 4. Sketch of the tabletop 
Photo of the tabletop during the test session 
ntegrated Setting  
sibilities are integrated in one physical setting to minim
re that all remote locations are comparable. Consequen
what participants at the remote location see. Moreov
age is better possible, because the viewable area of the
e as if he would be sitting at the table at the local locati
e table can be recognized because they are captured 
 
 
mize 
ntly, 
ver, 
 re-
ion. 
and 

 
Distance Collaboration Support Environment 
661 
 
projected at the remote location. The recognition of these hands also allow for version 
history whereby every action of individual users is captured. This information can be 
processed further to review the design rationale of the stakeholders, or to analyze the 
process of decision making.  
6 
Discussion 
Existing videoconferencing systems have some limitations which could hamper the 
collaboration process. One of the main drawbacks of those systems is the feeling of 
looking through a window, as seen by e.g. the HP Halo, Cisco Telepresence, Polycom 
TPX, MultiView by University of California and FP7 3DPresence [7][8]. Furthermore 
most of the current systems do not visualize the arms and hands of the user if they are 
placed on the table, although the im.point by Fraunhofer and Cisco Telepresence 
show a virtual shared desktop. Another big drawback is the distraction caused by 
showing the background of the remote participants and the lack of an interactive  
tabletop.  
The proposed collaborative environment offers new and expanded opportunities to 
facilitate collaboration. It combines various techniques in a dedicated environment. 
Of course there are also some limitations in this setup. There is a risk that a virtual 
participant will look at an object in his environment that is not available in the local 
environment of other participants, and is out of scope for any camera. This can be 
disruptive for other participants because it is not visible where the distraction comes 
from. In addition, the main objection is that this arrangement is suitable for only two 
locations. This means that the envisaged environment is not suitable yet to replace 
existing video-conferencing systems which offer group discussion with more than two 
locations. It is also of great importance to ensure that all interaction possibilities  
be-tween both locations are synchronized. Any deviation of any of the feedback pos-
sibilities will disrupt the process.  
7 
Concluding Remarks 
The use of a dedicated distance collaboration support environment shows many ad-
vantages seen in terms of ease of use and mutual understanding between stakeholders. 
The main advantage is that the sense of distance is minimized, because compared 
with current video conference systems more information is shared with both loca-
tions. But mainly because the shared information is presented in a more ubiquitous 
and unified setting, the stakeholders are less distracted by the used technologies. Due 
to the use of low cost and off the shelf available technologies the presented configura-
tion can easily be duplicated to more locations and allows for faster espousal.   
7.1 
Future Developments 
Based on the findings of this first version of our proposed collaboration environment 
some directions and challenges for future developments can be determined. First of 

662 
R. Damgrave and E. Lutters 
 
the table should allow for collaboration between more than two locations. In an ideal 
setting every seat should have the option to be a virtual or local seat. This makes the 
setup more flexible, and reduces any noticeable difference between local and remote 
participants.  
Furthermore the integration of a version control and history system is considered. 
This allows changes to be assigned to specific individuals or stages within the 
process. This information can be used to examine which effects can be assigned to 
which choice, but may also provide insight into how the participants deal with the 
method in order to bring improvements. 
References 
1. Ståhl, O.: Meetings for real - Experiences from a series of VR-based project meetings. In: 
VRST 1999 Proceedings of the ACM Symposium on Virtual Reality Software and Tech-
nology (1999) 
2. Knauff, P., Scheer, O.: An immersive 3D video-conferencing system using shared virtual 
team user environments. In: CVE 2002, Bonn, Germany, pp. 105–112 (2002) 
3. Credé, M., Sniezek, J.A.: Group judgment processes and outcomes in video-conferencing 
versus face-to-face groups. International Journal of Human-Computer Studies 59(6), 875–
897 (2003) 
4. Natyavidushi, J.: Importance of body language in effective multicultural communication. 
Annals of the University of Craiova, 109–127 (2011) 
5. Widgor, D., Penn, G., Ryall, K., Esenther, A., Shen, C.: Living with a Tabletop: Analysis 
and Observations of Long Term Office Use of a Multi-Touch Table. In: Second Annual 
IEEE International Workshop on Horizontal Interactive Human-Computer System (2007) 
6. Buisine, S., Besacier, G., Aoussat, A., Vernier, F.: How do interactive tabletop systems in-
fluence collaboration? Computers in Human Behavior 28, 49–59 (2012) 
7. Schreer, O.: Solutions for Immersive 3D Video Communication, presentation. iMinds, 
Ghent, Belgium (2010) 
8. Nguyen, D., Canny, J.: MultiView: spatially faithful group video conferencing. In: Proceed-
ings of the SIGCHI Conference on Human Factors in Computing Systems, Portland, Ore-
gon, USA, pp. 799–808. ACM (2005) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 663–673. 
DOI: 10.1007/978-3-642-30817-8_65 
© Springer-Verlag Berlin Heidelberg 2013 
 
Technology Framework for Product Design 
Michael A. Bitzer1 and Michael Vielhaber2 
1 Accenture, Campus Kronberg, 61476 Kronberg, Germany 
2 Institute of Engineering Design, Saarland University, Saarbrücken, 66123, Germany 
michael.bitzer@accenture.com, vielhaber@lkt.uni-saarland.de 
Abstract. The management of product design relevant information in the  
context of technologies is getting more and more relevant in industries and re-
search. In this paper a framework is introduced to support the management of 
technology in extension to existing PLM concepts. Moreover, the Technology 
Object (TO) as one main building block of this framework is presented. With 
the TO it is possible to represent information which is relevant on technology 
level – instead of on product level. Furthermore, dimensions of information are 
introduced that help to describe each TO; such as requirements towards this 
technology or performance capabilities which a technology is able to fulfill  
towards the respective environment. This paper focusses on concepts and me-
thods in context of the introduced Technology Framework.  
Keywords: Product Design Information, Product Lifecycle Management, 
Technology Management, Technology Framework, Technology Object. 
1 
Introduction 
Global companies working in the manufacturing industry represent a major part of the 
economic power in many parts of the world. The engineering processes and design 
capabilities are the core competences and the heart of those companies. Capturing and 
managing the product design information is a key success factor.  
Looking into the most innovative and successful companies it becomes obvious 
that only capturing product data is not enough, when aiming at constant innovation 
and growth. 35% of interviewed companies stated as relevant to include the most 
cutting-edge technology in their products [1]. This paper will discuss the need of in-
troducing a new layer of product related information to be able to manage technolo-
gies – instead of just products. The introduced framework is aiming to better support a 
holistic view on technologies to benefit cross products and product lines. This paper 
focusses on the elaboration of concepts and methods to manage relevant information. 
It is not aiming to provide IT solutions or tools, which will be subsequent supporting 
tasks.  Moreover, the framework enables designers and engineers to enrich design 
data with cross-product-lifecycle information (e.g. manufacturing, sales or recycling). 
To establish such a framework, the following research questions have to be  
answered. 

664 
M.A. Bitzer and M. Vielhaber 
Research Questions 
• Which information is required to (better) support the management of technologies 
and innovation? 
• How can this information be captured, formalized and incorporated into a technolo-
gy framework? 
• Which building blocks of the framework exist and need further research activities? 
• How would existing PLM systems need to be enhanced to implement such a tech-
nology framework? 
The outline of this paper is as follows. First, it will analyze the state of the art ele-
ments that frame the topic of this paper - starting with Product Lifecycle Management 
and Technology Management. Fig. 1 positions these two concepts in the field of the 
project and management processes in engineering design.  Second, after identifying 
deficits and deriving needs for action, we will introduce the framework and describe 
main building-blocks and focus on the Technology Object as one key element. Final-
ly, the views of research and industry are discussed, and in the conclusion and outlook 
further research steps are elaborated. We understand this paper as a building block in 
a series of contributions in this research area [2], [3]. It will set the frame and focus on 
identifying the needs, and it will propose research and solution approaches. 
 
Fig. 1. Positioning of Technology Management (TM) in relation to Product Data/Lifecycle 
Management (PDM/PLM) and design project processes (example: VDI 2221) 
2 
State of the Art – Industry and Research 
2.1 
Product Lifecycle Management 
Product Lifecycle Management (PLM) is a strategic management concept for manag-
ing products along their lifecycle. Multiple definitions of PLM do exist in literature. 

 
Eigner includes in his defi
topic: management functi
product data management, 
collaboration [4]. Product 
PLM. This is not correct. PL
consequence, PDM in cont
can be a component for the
within a PLM concept or 
Team Data Management (T
Especially in discrete ma
as a key enabler for efficie
PLM often focus just on th
ness processes and method
(PDM) systems. In a broad
of: organization, processes,
be addressed independently
cycle PLM often doesn’t pl
are more penetrated by solu
The PLM concept can b
of view the so called Prod
reflects the typical approac
product. 
Fig. 2.
Beside the more or less w
existing information model
frame a common understand
2.2 
Design Information
Research View 
The research work of this p
the example of two concep
information and knowledg
Properties Modeling [10] o
ledge in terms of character
types of relationships are d
Technology Framework for Product Design 
inition several elements to support a broader view on 
ions, material sourcing, customer needs manageme
manufacturing engineering management and engineer
Data Management (PDM) is often seen as the parent
LM, as seen above, is a concept and not an IT system. A
trast to PLM can be bought as a system; moreover, PD
e realization of a PLM concept. Other typical compone
solution include Computer Aided Design (CAD) tools
TDM) systems. [5] 
anufacturing industry PLM is well-known and already s
nt product information management [6]. Narrow views
he IT dimension as a supportive tool for established bu
ds, then mainly in the form of Product Data Managem
der understanding however, PLM features four dimensi
, methods and IT; all of them being interlinked and no
y [3]. Mainly used in the design phases of the product l
ay an important role in the later phases. These phases of
utions of Enterprise Resource Planning (ERP) [5]. 
e described in multiple ways [4]. From an economic po
duct Lifecycle Curve is used in many articles. This cu
ch of sales figures along the product lifecycle phases 
 
 Product Lifecycle Curve (following [12]) 
well-established PLM/PDM approaches in industry seve
ling frameworks exist. The following sub-chapter help
ding of modeling in the context of this paper. 
n and Knowledge Modeling 
paper can be positioned in the context of related work
pts which set boundaries in terms of modeling of des
e. One well-known research work is the Characterist
of Weber. In his work, Weber focusses on design kno
ristics and properties on product level. Moreover, differ
described that drive the behavior and the order of existe
665 
the 
ent,  
ring 
t of 
As a 
DM 
ents 
s or 
seen 
s on 
usi-
ment 
ions 
ot to 
life-
ften 
oint 
urve 
per 
eral 
s to 
k on 
sign 
tics-
ow-
rent 
ence 

666 
M.A. Bitzer and M. Vielhaber 
of the different elements of the model. Another well-established research work from 
Dankwort is the concept of Engineering Objects in design context [11]. The Engineer-
ing Object is described by properties which are linked to the environment the object is 
used in, such as context and person.  
Both research areas which focus on the product level are well-established ap-
proaches in the modeling of engineering information, data and knowledge. For this 
reason this paper will utilize the results and way of working of those approaches and 
will try to leverage experiences and principles of modeling to the technology level. 
Industry/IT View 
In industry, design information modeling is often left in the responsibility of the IT 
systems (i.e. PLM and/or ERP systems) applied. IT system suppliers provide informa-
tion models underlying the data models of their systems. They generally follow a pure 
product centric view, braking product programs down to the level of single compo-
nents with the respective data. With their database foundation, they may also be able 
to handle technology oriented objects; they do however not provide any concepts or 
methods dedicated to that. [13] 
2.3 
Technology Management 
Technology management describes all activities and methods which are required to 
make a certain technology usable for industries. A technology describes the fulfill-
ment of a requirement by a technique. Both terms are used in literature and business 
context not distinctly. In this context the term technology will be used also as a repre-
sentation of a technique.  
Technology S-Curve 
The Technology S-Curve is a model within the discipline “Management of Technolo-
gy” which describes the technology performance along the R&D effort spent on the 
respective technology, see figure 4.  
This model is used in industry to support strategic business decisions, especially 
regarding innovation investment decisions. This model can be used as a retro perspec-
tive view on a specific technology or used as a forecast method by reference to a  
defined point in time (t0). The Technology S-Curve used as a retro-perspective view 
allows recapturing information on the performance of a certain technology. Based on 
this guidelines and boundaries can be derived and transferred to new or other technol-
ogies. A proactive steering of the investigated technology itself is not possible. To 
utilize the S-Curve in terms of a forecast a challenge is to get reliable data for both 
axis to depicture the curve and by this the expected performance of the technology. 
Beside these spotlights on the Technology S-Curve this model is discussed with its 
pros and cons in literature, showing the boundaries and limitations of this approach.  
Technology Management Process 
The Technology Management Process describes the activities within technology  
management. In industry and research literature several variants of the process are 
presented.  Figure 3 reflects the high level view on this process. 

 
Technology Framework for Product Design 
667 
The first phase of technology screening covers all activities which are related to the 
identification of new technologies. In this context new technologies can belong on the 
one hand to already existing technologies which are transferrable from other industries, 
products or markets. On the other hand new technologies can represent results from 
research and development activities - for technologies which did not exist before. 
 
 
Fig. 3. Technology Management Process 
After the pure identification of technologies these are analyzed and brought into an 
order of priority according to the individual needs of an institution or a company. 
Activities related to this phase are bundled in the technology strategy definition. The 
technology strategy is derived and linked to the business strategy in the industrial 
context. At the same time the technology strategy has an influence on the business 
strategy. For example new identified technologies can have huge impact on existing 
technologies where a company earns its money with.  
The technology operations phase reflects activities in the day-to-day usage of a 
technology in products and operational processes of a company. The Technology 
Framework introduced in this paper can be positioned in this phase of the technology 
management process.  
In the phase of technology controlling all technologies are managed and tracked. 
Methods like the S-curve model can be used to support these activities. 
3 
Technology Framework – Situation  
3.1 
Multiple Product Context 
As described, currently the focus in management of design information is the single 
product or product families and their related information. There is no doubt that this 
information is a key element for companies in the discrete manufacturing. But looking 
into innovative and successful companies the potential of capturing information on a 
more abstract level – cross products and product families – promises a huge impact. 
Looking for example on the European Aerospace Industry where knowledge in the 
area of carbon technologies are existing and companies are preparing to apply these 
technologies in other industries like the automotive industry. Another example is the 
consumer goods industry where a technology (cyclone to spun dust out of the air by 
centrifugal force in sawmill) originating from the agriculture industry was successful-
ly adapted and became one of the most popular household machines in UK and the 
USA [8]. A way to reflect multiple products in the context of a single technology 
lifecycle is depictured in figure 4. [7]. 

668 
M.A. Bitzer and M. Vielhaber 
This curve can be used to reflect on the one hand the usage of a single technology 
cross multiple product releases or product variant. This is used for example for so 
called “face-lifts” in the automotive industry where a specific car release (e.g. the 
Mercedes-Benz A Class) is introduced to the market.  
On the other hand this approach can be used to indicate the usage of a single tech-
nology cross products or product lines which are not derived or dependent on each 
other. As described above the cyclone technology has been used cross products – 
sawmills and vacuum cleaners. 
 
 
Fig. 4. Technology S-Curve with multiple products (following [7]) 
3.2 
Information Management 
The new Technology Framework – introduced in this paper - is an evolution of the 
traditional design information approaches Product Data Management (PDM) and 
Product Lifecycle Management (PLM). The Technology Framework (TF) describes 
an approach to bundle product data, process & production information across prod-
ucts and product families on technology level. Moreover, the TF is evolved and influ-
enced by other modeling approaches and reflects ideas that are currently neither well 
established in research nor implemented in design departments and companies. A 
main building block of the TF approach is the holistic information modeling approach 
[3] – independent of currently existing IT tool boundaries (e.g. PLM systems vs. ERP 
systems) and based on information of related technologies. Currently in research and 
industry the focus is on product level and not on technology level.  
Figure 5 shows a typical information model of PLM concepts, reflecting the prod-
uct from top-down via an end item with a structure of physical parts or items below. 
Moreover, product related documents are attached along the hierarchy. PLM informa-
tion models often allow including also manufacturing information – such as tools and 
materials. 
The intent of this framework is to enlarge this product centric approach and enable 
a holistic view on engineering technologies without product boundaries to enable 
innovative product design and create engineering value. Contemporary Product Da-
ta/Lifecycle Management is thereby taken a step further to better support knowledge, 
technology and thereby innovation management. The framework intends to create a 
so called “Technology Layer” which enables engineers to store and manage product 
design information. Figure 6 indicates the Technology Layer as a new layer above the 
well-established Product Layer – managed by PLM. 
Technology
Performance
Time (R&D Effort)
Product
1
Prod.
2
Prod.
3
t0

 
Fig. 5. Typical PDM/PLM Info
Model 
The intent of the Techn
nologies independent of rea
uct structure by PLM. As a
contains one specific item t
structure various types of en
instrument is needed to be
where they are currently us
on the technology framewo
4 
Technology Fra
4.1 
Building Blocks 
The Technology Framewor
The framework consists of 
Technology Framework for Product Design 
formation 
Fig. 6. Technology Layer vs. Product Layer 
nology Framework is to provide the ability to model te
al physical parts that are reflected in an engineering pr
an example a product is reflected in a product structure 
that represents the engine (in this example). In the prod
ngines are reflected as variants. On the technology layer
e able to reflect these variants independent of the prod
sed in. This starting point is one main driver for the w
rk presented in this paper. 
mework – Building Blocks 
rk is an approach to support product design informati
four main building blocks, see figure 7.  
 
Fig. 7. Technology Framework 
669 
 
ech-
rod-
and 
duct 
r an 
duct 
work 
ion. 

670 
M.A. Bitzer and M. Vielhaber 
• The Technology Object as a core building block of the framework reflects key 
product design information on the technology layer. This paper will focus on this 
building block since this is the base for developing and deriving other building 
blocks of the framework.  
• The building block Methods & Processes utilizes the defined elements of the tech-
nology object and defines the way of using the objects by engineers and end-users. 
Moreover, behavior and rules per object and in structures of objects will be de-
scribed.  
• The Organization building block addresses topics and requirements that are related 
to the business organization where the technology framework will be used. The fo-
cus here will be on a single entity (e.g. a company or institution) – in contrast to the 
next building block. 
• The building block Environment covers the interacting area within a network of 
entities (e.g. companies, supply chains and interaction with customer and market). 
4.2 
Technology Object 
Within the Technology Framework the Technology Object (TO) is described. The TO 
is used to reflect the lowest granularity of the framework; it can be described by four 
dimensions, see figure 8. 
Technology Specification 
The specification of the technology allows the engineer to describe all characteristics 
regarding the technology and the required behavior of the technology or part of tech-
nology. Specifications should allow capturing varieties of information, such as tech-
nical or economic requirements. Industrial examples are patents, guidelines or public 
warranties – such as allowed car emission values.  
Products – Where Used 
This dimension indicates where this technology is used. By this one or several prod-
ucts are associated with the technology and allow cross referencing with product de-
sign data. This element in the framework supports to reflect the information displayed 
in the technology S-curve above.  
Performance of the Technology 
The performance of the technology is the dimension to describe the outcome and the 
impact of a technology on the environment. The environment can be other technolo-
gies or the usage scenario the technology is designed for and working in. An example 
for performance characteristics of the technology “car engine” can be maximum turn-
ing moment or maximum emission heat. 
Interlink – Connecting Technologies 
The interlink dimension allows to reference from one technology to one or more tech-
nologies. For technologies on the same level of granularity this can be used to define 
substitutes or similar approaches. Moreover, variants (configuration options) of tech-
nologies can be reflected. For example: technology “engine” can have variants of 
“diesel” or “petrol”.  

 
Technology Framework for Product Design 
671 
For technologies on different levels of granularity this dimension enables the engi-
neer to reflect technology structures. An example of a technology structure could be that 
the “cyclone technology” [9] and the “ball technology” are located under the technology 
end item “vacuum cleaner”. By this the engineer is able to reflect interdependencies and 
create structures between technologies of different levels of granularity. 
The introduced four dimensions of a Technology Object allow defining an object 
and the context this object is operating in. With this paper the basic boundaries are 
described and first research approaches are defined. 
 
 
Fig. 8. Dimensions of Technology Object 
5 
Discussion and Further Work 
In chapter 2 with the state of the art a gap between the PLM concept and the approach 
to manage technologies was elaborated. With PLM usually the engineers focus on the 
product level. Within the Management of Technology technologies are reflected on a 
high level without direct relations between technologies and sub-technologies. Based 
on this situation, the Technology Framework was introduced in chapter 3. To recap 
and derive a conclusion on the research work presented in this paper the defined re-
search questions are discussed in the following. 
• Which information is required to (better) support the management of technol-
ogies and innovation? 
The framework introduced four main building blocks to reflect the required infor-
mation for a technology. 
• How can this information be captured, formalized and incorporated into a 
technology framework? 

672 
M.A. Bitzer and M. Vielhaber 
With this paper a first element of the framework was introduced – the technology 
object. 
• Which building blocks of the framework exist and need further research activi-
ties? 
Starting with the technology object, methods how to use the object and on the beha-
vior of the object need to be elaborated within the next research activities. 
• How would existing PLM systems need to be enhanced to implement such a 
technology framework? 
At the early phase of research activities, it is not possible to clearly state this. But 
the methods of PLM seem to have a good potential to be leveraged in the next step 
of research on the framework. 
To summarize, this paper described the current situation of PLM and TM in research 
and industry and worked out the need to reflect and manage technology related design 
information as such and on top of the pure product information, as done today. More-
over, existing PLM concepts and methods were discussed to utilize them in the area 
of TM. The Technology Framework was introduced and main buildings block were 
defined. As a first and core main building block the Technology Object was  
presented. Furthermore, dimensions of this object were introduced which helped to 
describe the object itself. 
In order to shape the future work, a first step in the definition of the Technology 
Object was done with this paper on a conceptual level, but further research work is 
needed to drill down the dimensions, to define instances of each dimension and to 
describe their relations (linkages). After this step, the linkage between objects on the 
same level in hierarchies and cross-levels needs further elaboration, to be able to build 
step by step the entire Technology Framework, and hence to allow effective technolo-
gy information management for product development and design. 
References 
1. High performance through product development - Accenture research and insights into 
product development mastery. Market study (2009) 
2. Bitzer, M., Vielhaber, M.: Engineering environment for product innovation. In: Proc. In-
ternational Conference on Engineering Design – ICED 2011, Copenhagen, pp. 40–51 
(2011) 
3. Bitzer, M., Vielhaber, M.: PLM as a lever for innovation. In: International Conference on 
Product Lifecycle Management – PLM 2011, Eindhoven, Paper ID 112 (2011) 
4. Eigner, M., Stelzer, R.: Product Lifecycle Management – Ein Leitfaden für Product Devel-
opment und Life Cycle Management, 2nd edn. Springer, Berlin (2009) 
5. Bitzer, M., et al.: Determination of PLM architectures in the automotive industry. In: 2nd 
Nordic Conference on Product Lifecycle Management – NordPLM 2009, Göteborg (2009) 
6. Prendeville, K., Gupta, A.J.: Product lifecycle management – the innovation enabler goes 
mainstream. Accenture Outlook 3 (2010) 
7. Strebel, H.: Innovations- und Technologiemanagement, 2nd edn. Facultas Verlags - und 
Buchhandels AG, Wien (2007) 
8. Dyson fills a vacuum. @Issue: The Journal of Business and Design 8(1) (2009) 

 
Technology Framework for Product Design 
673 
9. Dyson Website (August 07, 2012), 
http://www.dyson.de/technology/radixcyclone.asp, 
http://www.dyson.de/technology/balltechnology.asp  
10. Conrad, J., et al.: What is design knowledge from the viewpoint of CPM/PDD? In: Interna-
tional Design Conference – DESIGN 2008, Dubrovnik, pp. 745–752 (2008) 
11. Faisst, K.G., Dankwort, C.W.: New extended concept for the usage of engineering objects 
and product properties in the virtual product generation process. In: International Confe-
rence on Engineering Design – ICED 2007, Paris, pp. 689–690 (2007) 
12. Siegwart, H., Senti, R.: Product Lifecycle Management – Die Gestaltung eines integierten 
Produktlebenszyklus. Schäffer-Poeschel Verlag, Stuttgart Germany (1995) 
13. Siemens PLM: Teamcenter 8.1 Handbook - Publication Number PLM00002 D (2009) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 675–684. 
DOI: 10.1007/978-3-642-30817-8_66 
© Springer-Verlag Berlin Heidelberg 2013 
 
Digital Representations of Intelligent Products:  
Product Avatar 2.0 
Thorsten Wuest, Karl Hribernik, and Klaus-Dieter Thoben 
BIBA - Bremer Institut für Produktion und Logistik GmbH, Hochschulring 20,  
28359 Bremen, Germany 
{wue,hri,tho}@biba.uni-bremen.de 
Abstract. Customer expectations towards products are constantly increasing. 
They are not limited to product quality alone but also include the accompanying 
services and information provided. Intelligent Products allow the retrieval and 
communication of large amounts of information from all stages of the product 
lifecycle. Customers have become used to user-centric presentation and custom-
izable information presentation from their experience with the Web 2.0 and  
Social Networks. Implementing Product Avatars as parts of Social Networking 
Services (SNS) as digital representations of physical products allows the pres-
entation of individually customized information in a familiar environment for 
different stakeholders. This can raise the acceptance and the lower the adoption 
threshold for Product Avatars by increasing their availability and usability on 
both stationary and mobile devices. 
Keywords: PLM, Product Avatar, Social Networks, Web 2.0, Digital Repre-
sentation, Information, Leisure Boat. 
1 
Introduction 
In today’s globalized world, customers increasingly expect physical products and 
related information of the highest quality. Current developments such as a higher 
sensibility towards sustainability have moved the whole product lifecycle into focus 
along with the need to actively manage and share product lifecycle information. Si-
multaneously, an increasing amount of products are embedded with computing and 
communication capabilities and are able to interact with each other and with human 
stakeholders, such as users or producers [1]. 
At the same time, Web 2.0, the “social web”, is rapidly changing the way informa-
tion is generated, shared and consumed. These activities have become collaborative 
and user-centric [2]. Studies indicate a quarter of the time spent online by German 
Internet users is with SNS such as Facebook [3]. SNS claim to be designed to support 
users in every aspect of their lives [4] are characterised by evolutionary, user-centric 
innovation and development of functionality and usability. 
Another trend is the convergence of the physical and virtual worlds, with products 
using Web 2.0 services such as Twitter to interact with stakeholders [5]. One ap-
proach to manage communication between Intelligent Products and users is the  

676 
T. Wuest, K. Hribernik, and K.-D. Thoben 
Product Avatar [6]. Since its initial introduction as a technical concept, it has evolved 
into the concept of a customisable “digital representation” of product-related informa-
tion. It enables stakeholders to benefit from value-added services based on product 
lifecycle information generated by Intelligent Products [7]. 
Developments in SNS can contribute notably to the field of Product Avatars. Users 
are familiar with the functionality, design and interaction paradigms of SNS. E.g., 
“pushing” information to both the computers and mobile devices is meanwhile widely 
accepted. Product Avatars could be implemented on SNS in a similar way to human 
users’ profiles, which can also be considered “digital representation” or “avatars”. 
This way, users would instinctively know how to use the Product Avatar to create, 
find or share product-related information. This facilitates a new level of interaction in 
the daily lives of users and creates opportunities for new and innovative services. 
Following, the Product Avatar concept in SNS is introduced, focussing on Web 
2.0, SNS, Product Lifecycle Management (PLM) and the Product Avatar itself. The 
concept is presented on the basis of a real life example: a leisure boat. Stakeholder 
requirements are derived and generalized where possible. Subsequently different SNS 
are investigated w.r.t. their applicability as platforms for Product Avatars. 
2 
Product Avatars in Social Networking Services 
In this section, Intelligent Products are discussed as a prerequisite for acquiring and 
communicating product information throughout the entire lifecycle. After introducing 
Product Avatars and SNS, the concept of Product Avatars in SNS is presented. 
2.1 
Intelligent Products 
Intelligent Products are physical items, which may be transported, processed or used 
and which comprise the ability to act in an intelligent manner. McFarlane et al. [8] 
define the Intelligent Product as “...a physical and information based representation 
of an item [...] which possesses a unique identification, is capable of communicating 
effectively with its environment, can retain or store data about itself, deploys a lan-
guage to display its features, production requirements, etc., and is capable of partici-
pating in or making decisions relevant to its own destiny.” 
The degree of intelligence an intelligent product may exhibit varies from simple 
data processing to complex pro-active behaviour. This is the focus of the definitions 
in [8] and [9]. Three dimensions of characterization of Intelligent Products are sug-
gested by [10]: Level of Intelligence, Location of Intelligence and Aggregation Level 
of Intelligence. The first dimension describes whether the Intelligent Product exhibits 
information handling, problem notification or decision making capabilities. The sec-
ond shows whether the intelligence is built into the object, or whether it is located in 
the network. Finally, the aggregation level describes whether the item itself is intelli-
gent or whether intelligence is aggregated at container level. Intelligent Products have 
been shown to be applicable to various scenarios and business models. For  
instance, Kärkkäinen et al. describe the application of the concept to supply network  
information management problems [9]. Other examples are the application of the 

 
Digital Representations of Intelligent Products: Product Avatar 2.0 
677 
Intelligent Products to supply chain [11], manufacturing control [8], and production, 
distribution, and warehouse management logistics [12]. A comprehensive overview of 
fields of application for Intelligent Products can be found in survey paper by Meyer et 
al [10]. 
Thus, an Intelligent Product is more than just the physical product – it also includes 
the enabling information infrastructure. Up to now, Intelligent Products are not “so-
cially intelligent” [13] in that they could create their own infrastructure to communi-
cate with human users over or store information in. However, Intelligent Products 
could make use of available advanced information infrastructures designed by so-
cially intelligent users, consequently enhancing the quality of information and acces-
sibility for humans who interact with them. 
2.2 
Digital Representation through Product Avatar 
The concept of the Product Avatar describes a distributed, de-centralized and frag-
mented approach to the management of relevant, item-level information throughout a 
product’s lifecycle [6]. At its core lies the idea that each product should have a digital 
counterpart by which it is represented towards the different stakeholders involved in 
its lifecycle. In the case of Intelligent Products, this may also mean the implementa-
tion of digital representations towards other Intelligent Products. Consequently, the 
Avatar concept deals with establishing suitable interfaces towards different types of 
stakeholder. For Intelligent Products, the interfaces required might be, for example 
services, agents or a common messaging interfaces such as QMI. For human stake-
holders, such as the owner, producer or designer, these interfaces may take the shape, 
e.g., of dedicated desktop applications, web pages or mobile “apps” tailored to the 
specific information and interaction needs. This contribution deals with the latter. 
Examples of embryonic Product Avatars are already emerging on the market. For 
example, each new Smart Fortwo electric drive (Smart ED) car has its own web page, 
which shows e.g. maximum range capable with current battery charge [14]. The cur-
rent charge status or the SmartCharging charge configuration can be controlled and 
managed via a web portal from a home computer or with any modern smartphone. In 
the future, several other features will be controlled remotely through a smart drive 
application for the iPhone [15]. 
2.3 
Social Networking Services 
There is currently no widely accepted definition of Web 2.0 [16-17]. Generally speak-
ing, it provides Web technologies which allow users to connect socially, including 
services such as blogs, wikis and video/photo portals [18]. Web 2.0 can be described 
by three characteristics: community, platform/tools and online collaboration [19]. 
A widely accepted definition of social networking sites is “web-based services that 
allow individuals to (1) construct a public or semi-public profile within a bounded 
system, (2) articulate a list of other users with whom they share a connection, and (3) 
view and traverse their list of connections and those made by others within the sys-
tem.” [20] Currently, Facebook, YouTube and similar sites are almost synonymous 
with these services. They are growing quickly and have established themselves in a 
very short time as the de facto standard for online interaction. With Web 2.0, the 

678 
T. Wuest, K. Hribernik, and K.-D. Thoben 
foundation has been laid [21] upon which new SNS can be built, spread and expanded 
among the users new services developed [18]. 
2.4 
Product Avatars of Intelligent Products in Social Networking Services 
A first step in setting up a Product Avatar is to decide how and where the representa-
tion of the Intelligent Product should be made available and for which stakeholders. 
Different stakeholder may prefer specific channels based on their individual require-
ments. For consumers, using a channel they are familiar with and can easily access is 
preferable. SNS like Facebook boast large user bases familiar with their functional-
ities, design and interaction paradigms: the service is an accepted communication 
tool. Their users are used to not just consume but to interact with the “avatars” of 
other users - the step towards interacting with a product through its Product Avatar a 
small one compared to introducing a completely new communication channel. A fur-
ther benefit is the ready-made accessibility from arbitrary devices both stationary and 
mobile. To leverage these benefits, the Product Avatar needs to be designed in accor-
dance with the chosen social networking service’s design and interaction paradigms. 
 
Fig. 1. Digital Representation of a Product through a Product Avatar 
3 
Exemplary Application: A Product Avatar of a Leisure Boat 
This section describes an exemplary application of the Product Avatar for an intelli-
gent leisure boat throughout its lifecycle. First, the rationale for and functionality of 
the intelligent leisure boat are presented. Its required functionality is defined. Suitable 
SNS are identified for its implementation. The section concludes with the presentation 
of a mock-up of the Product Avatar. 
BOL
EOL
MOL
BUILDER
OWNER
REPAIRMAN
PRODUCT AVATAR
PPS
ERP
CRM
Contr.
Warr.
Maint.
Recyc.
!


!
?

GO

 
Digital Representations of Intelligent Products: Product Avatar 2.0 
679 
3.1 
Rationale for Intelligent Leisure Boats 
Up until today, leisure boat builders have focussed solely on the improvement of their 
products’ quality to remain competitive in the marketplace. However, with the recent, 
drastic downturn in the boat market they are increasingly being forced to realise the 
need to additionally emphasize both the after-sales market and their customers’ de-
mands for products that are easy in upkeep, environmentally friendly and which offer 
them added-value services to enhance their boating experience. In order to fulfil these 
requirements, boat builders need to take concepts such as item-level and closed-loop 
PLM, Intelligent Products and Intelligent Maintenance into consideration. Leisure 
boats are complex, high-value consumer products which are often produced in small 
series, designed or made to order and often unique. Maintenance plays a key role in 
safety and ownership costs, the products can be in use for a considerable time, and 
sustainability and green issues are potential key advantages on the marketplace. Inte-
gral to the potential application of intelligent products taken here is a holistic view of 
the entire leisure boat value network: it includes all stakeholders in the boat lifecycle, 
such as the designers, ship yards, charterers, component suppliers, customers, owners, 
marinas, and boat hotels. 
The functionality required by an intelligent leisure boat encompasses all phases of 
the lifecycle. For the beginning-of-life phase (BOL) this includes the enhancement of 
the design process using information from on-board sensors.  In the middle-of-life 
(MOL) phase, concepts of Intelligent Maintenance such as a boat degradation model 
in combination with on-board sensors can improve the boats’ safety, reliability and 
quality. Services for the improvement of the boating experience integrating Web 2.0 
functionality are relevant to the younger generation of boaters. End-of-life (EOL) 
services address the boat builders’ and owners’ needs to for better refurbishing, reuse 
and upgrading of pre-owned craft. Information gathered in the previous lifecycle 
phases can be applied to these processes to increase their cost and time efficiency and 
consequently the sustainability of the overall product. 
3.2 
Required Functionality of an Intelligent Leisure Boats’ Product Avatar 
The potentials of intelligent leisure boats have been discussed in [22]. A result of that 
discussion is the identification of required functionality for intelligent boats by lifecy-
cle phase, involved stakeholder and data source. Table 1 presents the types of func-
tionality required by the corresponding Product Avatar in a social network, derived 
from the functionality by lifecycle phase and stakeholder for the intelligent leisure 
boats identified in [22]. 
3.3 
Selection of Suitable Social Networking Services 
The following section presents a selection of social networks based on a study carried 
out by the authors. Its objective was to identify the most suitable social networks  
currently available to host Product Avatars for leisure boats. The selection includes 
Facebook, as the biggest Social Networking Service with the most monthly active  
 

680 
T. Wuest, K. Hribernik, and K.-D. Thoben 
Table 1. Required Functionality an Intelligent Leisure Boats’ Product Avatar by Lifecycle 
Phase and Stakeholder [22] 
 
 
 
 
 
Required Functionality 
BOL 
MOL 
EOL 
Boat owner 
Designer 
Manufacturer 
Suppliers 
Service provider 
Recycler 
Intelligent service recommendations 
Intelligent upgrade recommendations 
Predictive maintenance information 
Improved customer relationships 
Winter storage monitoring 
Theft monitoring information 
Full boat traceability 
• Service history 
Process optimisation 
Design optimisation 
Sustainable boats/components 
Virtual manual 
Collaborative services 
Social services 
 
users [22], and its main competitors: so.cl, run by Microsoft, and Google+, run by 
Google. Additionally, two smaller SNS, Diaspora and Friendica, were added as they 
represent another, desirable aspect of SNS. They allow the users to create and host 
their own profiles and thus, keep their own information and data secure. 
 
Fig. 2. Comparing selected Social Network Services 
gobal coverage / reach
(number of) users
diversity of users
life cycle representation
open source / decentralization
degree of freedom to individualize
data security (transparency)
usability
dimension
facebook
diaspora
friendica
socl
google+

 
Digital Representations of Intelligent Products: Product Avatar 2.0 
681 
In Fig. 2, the five selected Social Networks are compared according to eight differ-
entiable and descriptive dimensions. The set of dimensions is based on recent expert 
interviews, literature review and previews experience in the field. Each network was 
rated qualitatively and the score is indicated by a more or less filled circle. A fully 
filled circle means the social networking service totally fulfils the dimension’s re-
quirements whilst an empty circle means requirements are not at all met.  
The final selection and a possible ranking depend strongly on the perspective of the 
stakeholders, the individual environment and the product itself. If their individual 
requirements are not yet derived, the selection cannot be utilized satisfactory. An 
example of such requirements is the following: a company decides their data cannot 
be uploaded on another server as it is confidential and has to be protected at all times. 
Therefore, the dimension “data security” becomes automatically a “kill requirement” 
influencing the selection critically.  
3.4 
Product Avatar within a Social Network 
In this chapter a first mock-up of a possible Social Networking Service Product Ava-
tar of a leisure boat is presented. The design and functionality is Avatar based on 
Facebook is illustrated in Fig. 3. Facebook was chosen as it is today the most popular 
Social networking Service and has the highest reach on the relevant consumer focus 
group. As mentioned before, it is also possible to use different channels for Product 
Avatars of the same product depending on the diverse stakeholders needs. This mock-
up is solely based on the requirements of the consumers’ perspective. 
 
Fig. 3. Mock-up of a Product Avatar for an Intelligent Boat on a Social Networking Platform 

682 
T. Wuest, K. Hribernik, and K.-D. Thoben 
The information provided by the Product Avatar of the leisure boat includes cur-
rent and former location (boat traceability), theft monitoring, speed, fuel level etc (see 
Table 1). It can be shared with friends to arrange meeting points or just to inform 
about the ongoing trip. 
A possible scenario could be the following. The boat employs certain sensors to 
analyze the water quality and independently matches the generated data with ideal 
fishing grounds for the preferred prey. The Product Avatar automatically interacts 
with other connected avatars (“friends”) to find the perfect fishing spot in the area 
which is then presented to the user. Instant feedback of the user in case of a catch 
could be communicated to the connected avatars. After successfully catching a large 
Tuna the user wants to get back to the nearest harbour. The marina, incl. gas station 
etc., will be informed and receive relevant information so they can plan their tasks 
(and supplies) after the boat arrives. The local wine dealer can receive information 
about the catch and suggest a corresponding wine for dinner (if that information is 
released by user). In case the fish is big enough, the user can invite friends in certain 
proximity (identifiable by their connected Product Avatars) and celebrate the event. 
4 
Conclusion and Outlook 
The goal of the paper was to present a concept of digital representation of intelligent 
products through the social networking service channel in order to develop new ser-
vices based on product lifecycle data to an interested audience. Based on previous 
work on with a more technical [6] and conceptional [7] background, this paper fo-
cuses on the applicability and requirements towards implementation. Furthermore, the 
analysis of the available and applicable SNS and thus the opportunities and threads 
for professional (e.g. producers) and private (e.g. customers) stakeholders represent 
interesting insights for further developments. This was highlighted through a practical 
example of a Product Avatar for a leisure boat. The types of information to be repre-
sented by the Product Avatar on the social network service were derived from the 
requirements towards intelligent leisure boats identified in prior work. The presenta-
tion of a mock-up of a boat avatar concluded the paper. 
The authors believe that the application of digital representation within widely  
accepted SNS through a Product Avatar presents vast business opportunities for dif-
ferent stakeholders along the whole Product Lifecycle. The next steps include the 
development and implementation of a Product Avatar (or multiple Product Avatars to 
represent different stakeholders’ requirements for that matter) in a pilot case. Fur-
thermore, the evaluation of this pilot case implementation will be executed with real 
users to measure the real life impact. Parallel, application of a Product Avatar in other 
domains, e.g. manufacturing processes (BOL) or recycling (EOL) will be analyzed. 
Acknowledgments. This work has partly been funded by the European Commission 
through the BOMA “Boat Management” project in FP7 SME-2011-1 “Research for 
SMEs”. The authors gratefully acknowledge the support of the Commission and all 
BOMA project partners. The results presented are partly based on a student project at  
 

 
Digital Representations of Intelligent Products: Product Avatar 2.0 
683 
the University of Bremen. The authors would like to thank the participating students 
for their significant contributions: Anika Conrads, Erdem Galipoglu, Rijad Merzic, 
Anna Mursinsky, Britta Pergande, Hanna Selke and Mustafa Severengiz. 
References 
1. Isenberg, M.A., Werthmann, D., Morales-Kluge, E., Scholz-Reiter, B.: The Role of the In-
ternet of Things for Increased Autonomy and Agility in Collaborative Production Envi-
ronments. In: Uckelmann, D., Harrison, M., Michahelles, F. (eds.) Architecting the Inter-
net of Things, pp. 195–228. Springer, Heidelberg (2011) 
2. Gosling, S.D., Augustine, A.A., Vazire, S., Holtzman, N., Gaddis, S.: Manifestations of 
Personality in Online Social Networks: Self-Reported Facebook-Related Behaviors and 
Observable Profile Information. CyberPsychology, Behavior & Social Networking 14(9), 
483–488 (2011) 
3. Arns, T.: Presseinformation - Internetnutzer verbringen die meiste Zeit in Sozialen Netz-
werken. BITKOM e.V.,  
http://www.bitkom.org/de/presse/8477_71209.aspx  
(received February 14, 2012) 
4. Null, C.: How to Live Your Entire Life Through Facebook. PC World 27(6), 88–90 (2009) 
5. Hribernik, K., Ghrairi, Z., Hans, C., Thoben, K.-D.: Co-creating the Internet of Things – 
First experience in the participatory design of Intelligent Products with Arduino. In: 17th 
International Conference on Concurrent Enterprising (ICE), pp. 1–9 (2011) 
6. Hribernik, K.A., Rabe, L., Thoben, K.-D., Schumacher, J.: The product avatar as a prod-
uct-instance-centric information management concept. International Journal of Product Li-
fecycle Management 4(1), 367–379 (2006) 
7. Wuest, T., Hribernik, K., Thoben, K.-D.: Can a Product Have a Facebook? A New Pers-
pective on Product Avatars in Product Lifecycle Management. In: Rivest, L., Bouraz, A., 
Louhichi, B. (eds.) Product Lifecycle Management: Towards Knowledge-Rich Enterprises. 
Proceedings of the 9th International Conference on Product Lifecycle Management, 
Montréal, Canada, July 9-11 (2012) 
8. McFarlane, D., Sarma, S., Chirn, J.L., Wong, C.Y., Ashton, K.: Auto ID systems and intel-
ligent manufacturing control. Eng. Appl. of Artif. Intell. 16, 365–376 (2003) 
9. Kärkkäinen, M., Holmström, J., Främling, K., Artto, K.: Intelligent products - a step to-
wards a more effective project delivery chain. Comput. in Ind. 50, 141–151 (2003) 
10. Meyer, G.G., Främling, K., Holmström, J.: Intelligent Products: A Survey. Comput. in 
Ind. 60, 137–148 (2009) 
11. Ventä, O.: Intelligent and Systems. Technology Theme - Final Report. VTT, Espoo: VTT 
Publications, 304 (2007) 
12. Wong, C., McFarlane, D., Zaharudin, A., Agarwal, V.: The Intelligent Product Driven 
Supply Chain. In: IEEE Int. Conference on Systems, Man and Cybernetics, vol. 4 (2002) 
13. Erickson, T.: Social systems: designing digital systems that support social intelligence. AI 
& Soc. 23, 147–166 (2009) 
14. N.N.: Stromer suchen Anschluss. Die ZEIT, 
http://www.zeit.de/2012/31/Elektroauto-Reichweite-Kosten  
(received September 15, 2012) 
 
 

684 
T. Wuest, K. Hribernik, and K.-D. Thoben 
15. N.N.: Third-generation smart fortwo electric drive to launch worldwide in spring 2012. 
Green Car Congress (2012),  
http://www.greencarcongress.com/2011/08/smart-
20110816.html?cid=6a00d8341c4fbe53ef014e8ab1feeb970d  
(received August 13, 2012) 
16. Berge, S., Buesching, A.: Strategien von Communities im Web 2.0. In: Hass, B.H., Walsh, 
G., Kilian, T. (eds.) Web 2.0 Neue Perspektiven für Marketing und Medien. Springer, Hei-
delberg (2008) 
17. Wilson, D.W., Lin, X., Longstreet, P., Sarker, S.: Web 2.0: A Definition, Literature Re-
view, and Directions for Future Research. In: Proceedings of 17th Americas Conference on 
Information Systems, Detroit, Michigan, August 4-7 (2011) 
18. Cyganski, P., Hass, B.H.: Potenziale sozialer Netzwerke für Unternehmen. In: Hass, B.H., 
Walsh, G., Kilian, T. (eds.) Web 2.0 Neue Perspektiven für Marketing und Medien. Sprin-
ger, Heidelberg (2008) 
19. Knappe, M., Kracklauer, A.: Verkaufschance Web 2.0. Dialoge fördern, Absätze steigern, 
neue Märkte erschließen. Gabler, Wiesbaden (2007) 
20. Ellison, D., Boyd, N.: Social Network Sites: Definition, History, and Scholarship. Journal 
of Computer-Mediated Communication 13, 210–230 (2008) 
21. Pastowsky, M.: Innovationspotenziale und Nutzenaspekte Sozialer Netzwerke für die Per-
sonalarbeit. In: Klaffke, M. (ed.) Personalmanagement von Millennials Konzepte, Instru-
mente und Best-Practice-Ansätze. Gabler, Wiesbaden (2011) 
22. Hribernik, K.A., Cassina, J., Røstad, C.C., Thoben, K.-D., Taisch, M.: Potentials of Item-
level PLM and Servitization in the Leisure Boat Sector. In: Through-life Engineering Ser-
vices Conference, November 4-6 (2012) (to be published) 
23. Sinclaire, J.K., Vogus, C.E.: Adoption of social networking sites: an exploratory adaptive 
structuration perspective for global organizations. Inf. Tech. Manag. 12, 293–314 (2011) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 685–694. 
DOI: 10.1007/978-3-642-30817-8_67 
© Springer-Verlag Berlin Heidelberg 2013 
 
Product Evolution and Optimization Based  
on Gentelligent Components and Product Life Cycle Data 
Roland Lachmayer, Iryna Mozgova, Bastian Sauthoff, and Philipp Gottwald  
Leibniz Universität Hannover, Institut für Produktentwicklung und Gerätebau, 
Welfengarten 1A, 30167 Hanover, Lower Saxony, Germany  
{lachmayer,mozgova,sauthoff,gottwald}@ipeg.uni-hannover.de 
Abstract. To implement a Product Evolution in the product development 
process, intelligent systems are necessary. Gentelligent components combine 
the intelligent system with genetic approaches. In that way, the life cycle infor-
mation has to be prepared and to provide for Design Evolution. Design Evolu-
tion is an optimization strategy which implements evolutionary mechanisms to 
further develop gentelligent components within the scope of adaptive design. A 
gentelligent wheel suspension illustrates its practical application. By this way, it 
is shown how Product Evolution and usage data interact. 
Keywords: life cycle data, evolutionary optimization strategy, design  
evolution. 
1 
Motivation 
Nowadays, the focus of product development shifts towards intelligent systems.  
So mechatronic systems which could arrange self-optimization on their own [1] or 
intelligent sensors which could react to special situations are developed. Steadly en-
hancing energy awareness creates a need for such developments [2]. With these re-
quirements, the complexity of the product development process increases. A lot of  
approaches are taken in using evolutionary mechanisms for the development process 
[3-4]. Also the communication between intelligently determined life cycle informa-
tion and the product development process is a challenge which has to be solved for 
different interfaces [5].  
This feasibility study project investigates the process of Product Evolution includ-
ing an optimization strategy based on gentelligent life cycle information [6]. This 
project is part of collaborate research center 653 (crc) “Gentelligent Components in 
Their Lifecycle” [7]. CRC 653 develops gentelligent components which are capable 
of sensation. So these components could give some information about their loads 
during the product life cycle. First of all, this feasibility study will check the influence 
usage data from highly dynamically loaded mechanical systems has on the develop-
ment process. Thus, the gentelligent, inherent load data is used for optimization dur-
ing the product development process in order to achieve an adaptive design for  
real-life operational demands.  

686 
R. Lachmayer et al. 
2 
Concept of Product Evolution 
The concept of Product Evolution shown in Fig. 1 implies the life cycle of mechanical 
systems. It covers the product creation process, gentelligent components in the field 
and also the data feedback. 
 
Fig. 1. Concept of Product Evolution 
In this concept of Product Evolution, three operating points were detected. 
• The first step is to design the gentelligent component. It expects the new technolo-
gy to be designed in the product development process such that it delivers helpful 
data during the usage phase. 
• In the second working package, the large number of load datasets has to be 
processed by a statistical operator. There some Cluster Analyses are studied.  
• The third point is integration in the product development process. An optimization 
strategy which uses the information from the statistical operator by utilizing evolu-
tionary mechanisms has to be implemented. Therefore the application of the genet-
ic algorithm was investigated. So the optimization creates Product Evolution for an 
adaptive design of gentelligent life cycle information. 
3 
Data Generation 
This chapter uses a gentelligent wheel suspension from a racecar provided by the 
HorsePower Team of Leibniz University of Hanover to illustrate the application, chal-
lenges and approaches of Product Evolution and Optimization based on gentelligent 
life cycle information. 

 
Product Evolution and Optimization Based on Gentelligent Components 
687 
The wheel suspension features kinematic functions of springing and steering 
movements of the car. It also has a major impact on the driving comfort [8]. In a  
racecar's wheel suspension it is possible to equip components with the gentelligent 
technology. One possibility is a wheel carrier of magnetic magnesium. This allows 
collecting load data information during the product life cycle [9]. Another possibility 
is to produce the wishbone or the driving shaft from metastable austenitic steel [10]. 
Figure 2 shows the wheel suspension with the suggested gentelligent components. 
 
Fig. 2. Wheel suspension with gentelligent components 
Implementing gentelligent technology in such a highly dynamically loaded system 
generates a few challenges. First of all, the gentelligent data from the component has 
to be transformed owing to restrictions of adaptive design. The fasteners of a compo-
nent are strictly invariant because changes lead to different load cases and a different 
dynamic response. The fasteners are called hard points or joints. Three key challenges 
ensue: 
1. Where is gentelligent technology positioned at the component? 
2. How do we find the significant load cases for Design Evolution? 
3. In which way is it possible to implement Design Evolution in the product devel-
opment process considering the rules and constraints of adaptive design? 
Design Evolution requires the gentelligent life cycle data to be prepared first. This is 
done in a statistical operator based on the cluster analyses methodology. Before ex-
plaining these methods, the way of generating the life cycle data is discussed. 
Creation of Gentelligent Usage Data. At a first step, the project studies the usage 
data from highly dynamically loaded systems. Since no real field data was available, 
it fell back on the virtual method. So a multi-body simulation model from the racecar 
was created. This is shown in figure 3. 
The figure includes a force curve generated from data of a simulated driving ma-
neuver. With these virtual methods usage data of potential gentelligent components 
could be generated. At the next step, the statistical operator applies statistical methods 
to these large datasets in order to prepare them for Design Evolution. 

688 
R. Lachmayer et al. 
 
Fig. 3. Multi-body simulation model with a force profile 
4 
Statistical Methods 
The scope of this study included the subtask of considering the internal data structure 
in the absence of primary information. Pattern recognition within the scope of this 
study aims at: 
1. Understanding the nature of data by means of grouping. Data segmentation into 
groups of similar objects allows to simplify problems of data processing and to 
make decisions based on the analysis of every cluster; 
2. Organizing and summarizing data to break down sets of objects into homogenous 
groups and, thus, to allow data volume reduction in order to recognize and use 
characteristic points of load values for optimizing the shape and topology later; 
3. Detecting novelties. to ascertain whether or not non-typical objects may be inter-
esting for the modeling of special load cases. 
 
In the first case it makes sense to use small cluster counts. In the second case it is 
important to provide a high strength of association between the objects in every  
cluster.  
There are different approaches to data clustering. The paper [11] presents an over-
view of pattern clustering methods from a statistical pattern recognition perspective. 
The most intuitive and most frequently used criterion in partitional clustering tech-
niques is the squared error. These methods essentially are a probability mixture  
resolving approach to cluster analysis. Most studies in this area have assumed that the 
individual components of the mixture distribution are Gaussian, and in this case the 
parameters of the individual Gaussians are to be estimated by the procedure.  
The traditional approach to this problem involves a maximum likelihood estimate of 
the parameter vectors of the distributions using the EM Algorithm [12]. As an alterna-
tive approach, pattern recognition with fuzzy logic [13] can be used. The C-means is 
the simplest and most commonly used algorithm from this family of methods. 
 

 
Product Evolutio
This study on significan
the Euclidean distance metr
ing a squared error criterio
of the global extremum of 
from the choice of initial 
medoids approach with 
The k-medoids method cou
k-means approach. Both m
ters. Figure 4 depicts an e
centers of clusters can be
Evolution. 
Fig. 4. Example of clusterin
Among the challenges o
probabilistic model of exam
the goal function of the clus
the solution. The statistic
following steps: 
1. Optional primary data an
2. Forming of the system o
3. Defining a distance func
4. Object grouping; 
5. Results presentation/visu
6. Cluster validity analysis 
Possible subsequent researc
using a k-means algorithm
second regards investigatio
known number of clusters [
5 
Design Evolutio
For an adaption of the whee
load cases, a physical produ
on and Optimization Based on Gentelligent Components 
nt load case analysis primarily relies on the k-means w
ric as the simplest and commonly used algorithm empl
on. Among reveal shortcomings of approaches are abse
the goal function of clustering quality, results depende
centers of clusters. Therefore, as an alternative, the
an intragroup dispersion criterion [14] was us
uld be more robust to noise and outliers as compared to 
methods require an a priori knowledge of the count of cl
example of dynamic force data clustering. The geome
e used as the character points of loads for the Des
g: k-means (point view) and k-medoids (triangulation) method
f dynamic load force data recognition are the forming o
minee data, the problem of finding the global extremum 
stering quality criterion, and the possible non-robustnes
al operator used for pattern clustering involves t
nalysis, noise analysis and data filtration; 
f variables; 
tion between pairs of objects; 
ualization; 
[15]. 
ch includes two logical directions. The first will be furt
ms modification and investigating the posterior effect. T
ons of fuzzy logic or probability clustering with an 
[16]. 
n 
el carrier of the suspension linking to the calculated crit
uct model is necessary. In the case of mechanical loads
689 
with 
loy-
ence 
ence 
e k-
sed.  
the 
lus-
etric 
sign  
ds 
of a 
m for 
s of 
the 
ther 
The 
un-
tical 
, an 

690 
R. Lachmayer et al. 
evaluation of the car hub design by a static finite element simulation is feasible. The 
critical loads at the hard points are entered as boundaries in the finite element model. 
In this way, an evaluation considering gentelligent application data is done. [17] 
By the Design Evolution process, the shape and the topology of the wheel carrier is 
varied to find the best-adapted design in relation to the gentelligent information. But 
this process of variation has to be consonant with the gentelligent information feed-
back. By instancing the wheel carrier and the critical loads, a variation of shape and 
topology can be done as long as the position of the hard points is fixed. If varying the 
hard points, the critical load information is no longer invariant to the geometry of the 
wheel carrier. In this case, the geometrical parameters of the wheel suspension are 
changed. A modification of the load situation of the wheel carrier itself is the conse-
quence. A correlation of the different designs does not exist any longer in this case. 
Thus, using critical load case information and a static finite element simulation allows 
design evolution at component design level only [18]. 
Focusing the Design Evolution of a dynamic assembly system such as the wheel 
suspension, a feedback of kinematic information is necessary, the reason being a vari-
ation of mass and stiffness by variation of shape and topology. For a Design Evolu-
tion on this level, an additional aspect has to be considered: A variation of design is 
only feasible for kinematic chain elements because, in the space of a kinematic loop, 
there are always interactions. For a kinematic chain variation without interactions is 
only possible for a section of a free end. Because of these aspects, a Design Evolution 
of a wheel suspension considering kinematic and dynamic characteristics is only feas-
ible by feeding back gentelligent information of wheel excitation and driving charac-
teristics [19]. 
A Design Evolution process always includes an optimization strategy. In the field 
of mechanical structure optimization there are two fundamental approaches: On the 
one hand, there are mesh based methods based on finite element formulations; on the 
other hand, parametric design models coupled with deterministic or stochastic me-
thods are being used. In the field of mesh based methods, topology optimization based 
on homogenization or BESO method are used in industry. These methods are only 
feasible for single-component optimizations considering mechanical properties. Pa-
rametric design models are applied to multidisciplinary optimization considering sev-
eral physical domains e.g. multibody systems as well as stress analysis. For the  
computational solving of such problems, genetic algorithms are widely used. The 
independence of the physical domain and the possibility to integrate arbitrary restric-
tions in the optimization are additional important aspects. Thus, for the Design Evolu-
tion of the wheel carrier, a parametric design model linked to a finite element model 
and a genetic algorithm are used [20]. 
6 
Modeling 
By implementing Design Evolution based on parametric modeling, the geometry of a 
component is enriched with intelligence of its variation. For the development of pa-
rametric design models, there are features in today's commonly used CAD software 

 
Product Evolution and Optimization Based on Gentelligent Components 
691 
packages. These features are suitable for varying an existing geometry, but modeling 
of new elements based on parametric rules is commonly not available. Thus the de-
sign space is restricted and regeneration caused by parameter variation often fails.  
For enhanced parametric modeling, there are certain CAE software packages 
which enable an implementation of extensions by scripting languages. The modeling 
kernel can be thus used for automatical design generation.  
For linking a parametric design model to a simulation environment, a mapping of 
geometrical and functional representation is necessary. Implementing a meta model to 
generate the different domain-specific models is a solution for this aspect. This strate-
gy is commonly used in knowledge-based engineering systems. For Design Evolu-
tion, a meta model of the wheel suspension is developed. The gentelligent application 
information is integrated in this meta model. Transformation into the domain-specific 
models is done by the meta modeling methods [21]. 
Software Evaluation and Concept. Implementation of a meta model for the wheel 
suspension requires a previous software evaluation. To minimize interface problems 
between modeling and simulation domain development, all meta models should be 
preferably based on a single computer-aided engineering software package. Today 
there are CAD software systems with an integrated simulation environment as well as 
finite element software packages with an integrated modeling environment. Because 
of its open API scripting interface, Abaqus, a finite element software package, is used 
for an application study of meta modeling of the wheel carrier. Abaqus provides Py-
thon as scripting language. Its API is integrated as an object-oriented Python module. 
Thus, object-oriented programming is possible as well as integrating the far-reaching 
extension modules provided by Python, e.g. genetic algorithms.  
Table 1. Domain-Specific Software Packages 
software domain 
 
API 
modeling based 
• CreoParametric + CreoSimulate 
• Autodesk Inventor + Autodesk Simulation 
• CATIA Mechanical Design + Analysis 
C++ 
Visual Basic 
Visual Basic, C++ 
simulation based 
• ANSYS Mechanical APDL 
• ANSYS Workbench 
• Abaqus 
Macro lang. APDL 
Python(limited) 
Python 
 
The meta-model consists of an object-oriented approach containing individual 
geometry classes of the wheel carrier. The geometry class itself controls the geometry 
variation process and provides different modification methods e.g. input of a hole. 
The global model parameters are initialized by a parameter class. Interaction of the 
genetic algorithm and the parameter class is controlled by a component control class. 
The gentelligent information consists of different load cases, is implemented as its 
own class and linked as boundaries to functional faces of the component. The compo-
nent class itself manages the simulation routines and calls the corresponding load 
cases. Simulation results are stored by a result class. The objectives are calculated by 

692 
R. Lachmayer et al. 
an objective class and retur
with the genetic algorithm. 
This process of Design E
oriented Python architectur
meta-modeling of mechanic
ules – geometry generation
the modules themselves [22
Fig
7 
Results and Con
First of all, application of th
an inheritance of life cycle 
vided approaches to the thr
the first challenge about th
existent real life cycle infor
the scientific point of the d
sets could directly be placed
The second challenge wa
showed that the large datas
significant load cases of th
lished the basis for Design E
rned to the component class managing the communicat
Evolution is implemented in Abaqus by the above obj
re. An object-oriented architecture is well suited for 
cal components because communication between the m
n, gentelligent information, optimization – is managed
2]. 
g. 5. Architecture of Design Evolution 
nclusion 
he gentelligent wheel suspension shows that the concep
information is possible. Studying the research points p
ree challenges mentioned at the beginning.. With regard
he position of gentelligent technology and in case of n
rmation, a multi-body simulation supplied the data poo
design of gentelligent systems. In that way, the usage d
d at the hard points of the wheel carrier.  
as studied in the statistical operator. The statistical meth
sets could be reduced by cluster analyses. Additionally, 
he wheel carrier could be located. This information est
Evolution. 
tion 
ect-
the 
mod-
d by 
 
pt of 
pro-
d to 
non-
ol at 
ata-
hods 
the 
tab-

 
Product Evolution and Optimization Based on Gentelligent Components 
693 
By generating product-equivalent models of the wheel carrier in combination with 
the usage information, the Design Evolution could be arranged. Considering the rules 
and constraints determined for adaptive design, the wheel carrier was given a new 
shape. Applying a gentelligent wheel suspension helped to show that the Product 
Evolution of individual components, here a gentelligent wheel carrier, is possible. 
Applications for such gentelligent components mainly include innovative and cost-
intensive systems like wind turbines or manufacturing tools. Therefore, the products 
experience different life cycles due to different manufacturing processes caused by 
various components on a machine tool, for example. 
The next challenges are detected when studying the impact large numbers of gen-
telligent components have on the statistical operator and also on Design Evolution. In 
that way, the interfaces between the CAE tools have to transfer the product-equivalent 
models. Industrial integration of this concept will be sought afterwards, possibly fo-
cusing on the automotive or energy branches. 
Acknowledgments. The results presented in this paper were obtained under the um-
brella of Collaborative Research Centre 653 „Gentelligent Components in Their  
Lifecycle“, preliminary inspection project N4. The authors would like to thank the 
German Research Foundation (DFG) and the CRC 653 for its financial and organiza-
tional support. 
References 
1. Gausemeier, J.: Neue Perspektiven für den Maschinen- und Fahrzeugbau durch Selbstop-
timierung. Industrie Management 25, 33–36 (2009) 
2. Gausemeier, J., Kahl, S., Radkowski, R.: Selbstoptimierende Produkte – neue Perspektiven 
zur Steigerung der Energieeffizienz, Energieeffiziente Produkt- und Prozessinnovationen 
in der Produktionstechnik. In: 1. Internationales Kolloquium des Spitzenclusters eniPROD, 
Tagungsband, Technische Universität Chemnitz, Fraunhofer IWU, pp. 641–656 (2010) 
3. Clement, S.: Autogenetische Konstruktionstheorie – Produktentwicklung mit Hilfe der 
Evolution. Konstruktion 5, 77–82 (2008) 
4. Parvan, M., Miedl, F., Lindemann, U.: Nature-inspired Process Model for Concept Selec-
tion and Evaluation in Engineering Design, NordDesign, Aalborg, Denmark (2012) 
5. Neubach, M.: Wissensbasierte Rückführung von Produktnutzungsinformationen in die 
Produktentwicklung im Rahmen einer Product-Lifecycle-Management-(PLM)-Lösung. 
Shaker-Verlag, Aachen (2010) 
6. Lachmayer, R., Sauthoff, B., Gottwald, P.: Product Optimization by Analysis of Gentelli-
gent Life Cycle Information. SysInt. Garbsen, 28–31 (2012) 
7. Denkena, B.; Lorenzen, L.-E.: Genetik und Intelligenz – Neue Wege in der Produktions-
technik. VDI Zeitschrift Konstruktion, IW, pp. 4–5 (2007)  
8. (Ed.) Braess, H.-H., Seiffert, U.: Vieweg Handbuch der Kraftfahrzeugtechnik, 6. Auflage. 
Vieweg + Teubner Verlag (2011) 
9. Bach, F.-W., Schaper, M., Rodmann, M., Bormann, D., Nowak, M.: Magnetische Magne-
siumlegierungen – Entwicklung von Magnesiumlegierungen mit magnetischen Ausschei-
dungen. In: 1. Kolloquium Genetik und Intelligenz – Neue Weg in der Produktionstechnik, 
pp. 7–12 (2007) 

694 
R. Lachmayer et al. 
10. Behrens, B.-A., Weilandt, K., Jocker, J.: Mechanische Belastungssensoren auf Basis der 
Martensitbildung und ihre umformtechnische Herstellung. In: XXVIII. Verformungskun-
dliches Kolloqium Tagungsband, pp. 103–112 (2009) 
11. Jain, A.K., Murty, M.N., Flynn, P.J.: Data Clustering: A Review. ACM Computing Sur-
veys (CSUR) 31(3), 264–323 (1999) 
12. McLachlan, G.J., Krishnan, T.: The EM Algorithm and Extensions. John Wiley & Sons 
(2008) 
13. Bezdek, J.C.: Pattern Recognition With Fuzzy Objective Function Algorithms. Plenum 
Press, NY (1981) 
14. Gowda, K.C., Krishna, G.: Agglomerative clustering using the concept of mutual nearest 
neighborhood. Pattern Recognition 10, 105–112 (1977) 
15. Rand, W.: Objective criteria for the evaluation of clustering methods. Journal of the Amer-
ican Statistical Association 66, 846–850 (1971) 
16. Milligan, G.W., Cooper, M.C.: An examination of procedures for determining the number 
of clusters in a data set. Psychometrika 50, 159–179 (1985) 
17. Lachmayer, R., Sauthoff, B., Gottwald, P.: Technical Evolution Process – An Approach 
for Product Development and Optimization. In: Nord Design Conference, Aalborg (2012) 
18. König, O.: Evolutionary design optimization: tools and applications. Dissertation, EHT 
Zürich (2004) 
19. Bendsoe, M.P., Sigmund, O.: Topology Optimization: Methods and Applications. Springer 
(2003) 
20. Spillers, W.R., MacBain, K.M.: Structural Optimization. Springer (2009) 
21. La Rocca, G.: Knowledge based engineering: Between AI und CAD. Review of a lan-
guage based technology to support engineering design. Advanced Engineering Informat-
ics 26(2), 159–179 (2012) 
22. Simulia: Scripting User’s Manual. Abaqus 6.12 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 695–704. 
DOI: 10.1007/978-3-642-30817-8_68 
© Springer-Verlag Berlin Heidelberg 2013 
 
Early Development  
of Weight-Optimized Mechatronic Products 
Tobias Luedeke and Michael Vielhaber 
Institute of Engineering Design, Saarland University, 66123 Saarbrücken, Germany 
{luedeke,vielhaber}@lkt.uni-saarland.de 
Abstract. Existing process models for the development of mechatronic prod-
ucts are not considering the task of weight optimization – weight reduction and 
weight distribution – in a specific and sufficient way. The weight optimization 
is mostly applied at the end or in the late phases of the development process 
with the consequence that a large number of macro-iterations are necessary 
when design changes regarding the weight have to be done. These points result 
in an increase of development costs and time. In previous work, the authors 
propose a process model which exposes the task of weight optimization as an 
in-process development goal beside the goal of functionality. 
In this paper, a detailed procedure for the design of weight-optimized me-
chatronic products in early phases is proposed. The process is supported by a 
set of different methods based on suitable lightweight strategies. 
Keywords: mechatronic products, weight optimization, early design phases, 
process model, product development. 
1 
Introduction 
1.1 
Problem Statement 
Lightweight design in general – and hence a lower product weight and a better prod-
uct weight distribution – is seen as one possible approach to a resource and energy 
conserving realization of products during their production, usage and recycling life-
cycle phases. In opposite to this, there are some big interdependencies to other prod-
uct properties, e.g. growing safety requirements, power enhancement and customer 
needs regarding comfort and entertainment. These aspects often lead to increasing 
product weights, e.g. shown on the example of automobile. Their realization is mostly 
based on the use of mechatronic concepts. This obvious contradictory trend can be 
disproved by looking at some innovative mechatronic concept solutions, e.g. smart 
materials, x-by-wire concepts or adaptronics. 
However despite these possibilities, the task of weight reduction and optimization 
is mostly insufficiently and unsystematically covered during the development of me-
chanical or mechatronic products. Lightweight measures are often only sporadically 
and locally applied in a few subsystems and during the late phases of the development 
process with the consequence that the whole system is not covered and sufficiently 
optimized. 

696 
T. Luedeke and M. Vielhaber 
Some approaches aim at integrating the task of weight optimization, weight reduc-
tion and weight distribution, into the development process. But a continuous integra-
tion over the whole development process, especially into the early phases due to the 
large influence on future product properties, has not been considered yet. 
1.2 
Framework “Design of Weight-Optimized Mechatronic Products” 
The methodology “Design of Weight-Optimized Mechatronic Products” has been 
introduced by the authors to consider the continuous integration [1,2]. It can be un-
derstood as an integration of two different disciplines, mechatronic and lightweight 
design, into one development process. Mechatronics itself is an interdisciplinary and 
synergetic interaction of the three domains mechanical engineering, electric engineer-
ing and IT/control engineering. Thus, there are two development goals existing: on 
one side the achievement of the functionality by using mechatronic concepts and on 
the other side obtaining the lowest weight and best weight distribution as possible. 
The framework consists of six elements – process model, strategies and methods, 
system understanding, modeling and simulation, organization as well as knowledge 
and communication – which are the basis of the methodology representing two differ-
ent perspectives, the product view and the process view (see Figure 1). 
 
Fig. 1. Methodology for the Development of Weight-Optimized Mechatronic Products 
The product view – the weight optimization by application of mechatronic prod-
ucts – needs an analysis of given mechatronic concepts in terms of their ability or of 
their potential for a weight reduction or an improved weight distribution.  
The process view – the weight optimization of mechatronic products – offers a 
procedure for the development of weight-optimized mechatronic products following 
the VDI guideline 2206 (see Figure 3). 
Organization
Knowledge & 
Communication
Strategies & 
Methods
Process
Model
Modeling & 
Simulation
System 
Understanding
Product
View
Weight Optimization 
through Mechatronic 
Products
Process
View
Weight Optimization 
of Mechatronic 
Products
Methodology “Design of Weight-Optimized Mechatronic Products“

 
Early Development of Weight-Optimized Mechatronic Products 
697 
1.3 
Approach 
In this contribution, the focus lies on the process view. Especially the early phases of 
the design of weight-optimized mechatronic products will be investigated. The impor-
tance of the early phases during product development is widely recognized. 
Based on the macro-level process model (see Figure 3), the analysis gates 0 and 1 
as well as the system design stage are the topic of this paper. The different gates and 
stages are supported by methods and tools of the systemic and the conceptual 
lightweight design strategies which will be illustrated later. 
In analysis gate 0, the desired requirements for the product are investigated in order 
to point out the requirements which are related to the product weight and product 
weight distribution as well as their interdependencies and conflicts of objective. Fur-
thermore, it seems to be practicable to perform a comparison of predecessor or com-
petitive products aiming at deriving weight targets for the new product. 
Within the stage of system design, the overall system solution will be worked out. 
There are possibilities to check influences on the subsequent product weight and 
product weight distribution after different steps are achieved, e.g. functional structure, 
logical structure or physical structure. In this stage, methods will be given to achieve 
these structures in a way aiming at weight and weight distribution optimization. These 
methods are based on the strategies of systemic lightweight design and conceptual 
lightweight design. In analysis gate 1, a weight analysis of the overall cross-domain 
solution concept will be performed. 
2 
Application of Lightweight Strategies in the Early Design 
Phases 
2.1 
Basic Principles of the Systemic and Conceptual Lightweight Design 
Strategy 
Systemic Lightweight Design. This strategy is intended to achieve a holistic weight 
reduction of an overall system. The core objectives of this design measures are the 
optimization of the system weight and system inertia as well as its positioning or ra-
ther its distribution within the system. The systemic lightweight design strategy 
represents a method which is displayed across material and product choice. It is sub-
ject to certain conditions and requirements, e.g. design, manufacturing, quality, safety, 
environment and cost [3]. 
Conceptual Lightweight Design. This lightweight design strategy accomplishes 
weight reduction and optimization through a systematic investigation of certain struc-
ture and modules and their matching to the entire system or subsystem including 
layout and design. Thus, the package and the configuration of the components or the 
design of one single component can immensely influence the weight or the weight 
distribution of the whole system. [3,4] 

698 
T. Luedeke and M. Vielhaber 
2.2 
Classification of Lightweight Strategies to Design Processes 
Figure 2 shows the classification of lightweight design strategies to the design stage 
of mechatronic products demonstrated at the V model of the VDI guideline 2206 [5]. 
Beside the systemic lightweight strategy including the weight optimization for the 
whole system, the conceptual lightweight design strategy plays a very important role. 
The concept in general is significantly responsible for the further design procedure 
and thus for the application of the resting lightweight strategies. 
 
Fig. 2. Classification of Lightweight Strategies to the Mechatronic Design Process [2] 
3 
Process Model with Focus on the Early Design Phases 
The general procedure for the design of weight-optimized, mechatronic products is 
shown in Figure 3. It can be seen first as a top-down development from requirements 
to an overall system concept up to domain solutions and then second as a bottom-up 
design from these domain-specific concepts to an overall system solution and finally 
the real product. 
This paper focuses on the system design stage of mechatronic design between 
analysis gate 0 and 1. In the early phases particularly the strategies of systemic and 
conceptual lightweight design are used. The later phases including domain conception 
and system integration, in which the resting lightweight strategies are applied, will be 
investigated in further research. 
electrical engineering
information technology
mechanical engineering
domain-specific design
modeling and model analysis
requirements
product
assurance of properties
systemic lightweight design
conceptual
lightweight design
structure lightweight
design
conditional lightweight
design
lightweight
material design
manufacturing
lightweight
design

 
Early Development of Weight-Optimized Mechatronic Products 
699 
 
Fig. 3. Process model for the development of weight-optimized products [2] 
3.1 
Importance of the Early Phases 
The early phases of the product development are crucial for the further development 
process and the success of a product. The possibility to improve and to generate innova-
tive solutions is biggest in these phases due to the large number of degrees of freedom. 
The potential for improvement decreases exponentially in contrast to the exponential 
growing of product knowledge and the degree of product concretization. [6] 
 
Fig. 4. Impact on product weight [2,6] 
For weight optimization the early phases carry a special significance. Traditional 
lightweight design strategies used alone are not regarding the product weight and 
product weight distribution at all. The procedure with the strategies and lightweight 
ML
EL
IL
0
1
5
2a
2b
2c
4a
4b
4c
3
MC
EC
IC
requirements
product
assurance of properties
and
MC: Conceptual Design Mechanical Eng.
EC:
Conceptual Design Electrical Eng.
IC:
Conceptual Design IT/Control Eng.
ML: Detail Design Mechanical Engineering
EL:
Detail Design Electrical Engineering
IL:
Detail Design IT/Control Engineering
0
process
progress
impact /
concretization
1
2
3
4
5
SD
DCD
ISI
DDD
FSI
impact on product weight
level of weight concretization/
product knowledge
SD:
System Design
DCD:
Domain Conceptual Design
ISI:
Initial System Integration
DDD:
Domain Detail Design
FSI:
Final System Integration
SysLD:
Systemic Lightweight Design
ConcLD:
Conceptual Lightweight Design
StrLD:
Structural Lightweight Design
CondLD:
Conditional Lightweight Design
MatLD:
Material Lightweight Design
ManLD:
Manufact. Lightweight Design
improvement potential
early
phases
SysLD
ConcLD

700 
T. Luedeke and M. Vielhaber 
design techniques [7] is not appropriate enough because they are primarily only appli-
cable on product shape or design. However, a consideration of weight and weight 
distribution in the planning and the conceptual phase is unavoidable although an esti-
mation of the future product weight is very difficult. But it is possible to detect on one 
side hints for a weight-optimized product and on the other side potential trade-offs 
regarding weight and other product properties. 
3.2 
General Design Procedure 
Figure 5 shows the refined steps – which are based on the early design steps of the 
VDI guideline 2206 [5,8] – of the process model for the development of weight-
optimized mechatronic products. It contains the steps planning and clarifying the task 
and system design which are represented in more detail in the figure. Additionally to 
the design stages, there are several analysis gates: analysis gate 0 for the first step, 
analysis gates a, b and c within the system design step and analysis gate 1 as the 
conclusion of this step. Moreover, the design phases are supported by different light-
weight strategies (systemic lightweight design strategy and conceptual lightweight 
design strategy) and methods (I-VIII) based on these strategies. 
 
Fig. 5. Early Design Phases for the Development of Weight-Optimized, Mechatronic Products 
The following subchapters are describing the methods used in the early design 
phases. 
domain-specific design
system solution concept
planning /clarifying the task
requirements collection
market / patent / product / 
technology analysis
requirements list
conceptual lightweight design strategy
systemic lightweight design strategy
lightweight tasks
I
II
III
IV
V
VII
VIII
system design
abstraction of the problem
functional structure
search for work principles
logical structure
concretization
physical structure
evaluation
VI
0 & 1:
main analysis gates
a, b & c: analysis gates within
system design stage
I – VIII: methods used

 
Early Development of Weight-Optimized Mechatronic Products 
701 
3.3 
Planning and Clarifying the Task 
In this phase, the creation of the requirements list comes along with the systemic 
lightweight design strategy. Methods can be used for the analysis of predecessor and 
competitive products as well as for the requirements list. 
Product Analysis (I). Patent and technology analysis as well as benchmarking of 
predecessor and competitive products are necessary to identify weight goals. For the 
determination of these weight goals the method of “Value Analysis Weight” can be 
very helpful [9]. The methods of the value analysis are transferred from the applica-
tion to reduce cost to the application to reduce weight. However, a procedure of tak-
ing account of weight distribution is not mentioned. Moreover, ABC analysis of 
weight saving potentials or standards and limiting values (for example standards for 
ergonomics) can support this identification. It could be helpful to indicate so-called 
lazy parts which add to a product but have little impact on the performance [10]. It is 
important to get to know possible trade-offs as soon as possible in the definition phase 
of requirements and development goals. 
Requirements List (II). Derived from a collection of requirements and analysis of 
predecessor or competitive products a requirements list is generated. The greatest 
potential of identifying weight optimization options of the final products lies in ana-
lyzing the requirements in this earliest phase of product development with the most 
design freedom. With the aid of a quantitative explicit description of the requirements 
(e.g. weight < 10 kg) and determination to importance factors weight-relevant re-
quirements can be illustrated. Moreover, relations between different properties have 
to be filtered out to illustrate occurrent goal conflicts. Potentially, it can be promising 
to depict the requirements related to the product weight as well as the interdependen-
cies and the effects in a separate refined requirements lists. Furthermore, it is impor-
tant to create the requirements list with the task of weight reduction and distribution 
as a main requirement because lightweight design is only implicitly mentioned in 
conventional lists. A quantification of qualitative tight claims to qualitative final re-
quirements can be helpful if it is done as early and with advanced specificity as possi-
ble (e.g. “weight lower than 15 kg” instead of “easy to carry”) [11]. A method for the 
identification of requirements which are critical to mass reduction is proposed by 
McLellan [12]. After a requirement pre-processing and a mapping of these require-
ments to components, requirements can be identified which are highly mass intensive 
and uncoupled from other requirements.  
Outcome of the requirements list is the clarity of weight goals and the detection 
and observance of goal conflicts, e.g. weight ↔ safety or weight distribution ↔ acce-
leration behavior. 
Analysis Gate 0. This gate serves as a revision of the phase outcome, the require-
ments list. The percentage of weight-related requirements can be measured and com-
pared to the results of later analysis gates. 
3.4 
System Design Stage 
The system design stage is supported by the systemic and the conceptual lightweight 
design strategy. Different structures are built in analogy to the “Münchener Konkreti-
sierungsmodell” [13]: functional structure, logical structure and physical structure. 

702 
T. Luedeke and M. Vielhaber 
Functional Structure (III) and Analysis Gate a. Aim of this step is the identifica-
tion of weight-relevant or weight-critical aspects and of weight saving potentials  
although there is usually no information about the future product weight or weight 
distribution in the functions implemented. With the aid of a functional weight analysis 
in analogy to the target costing [13], the functions can be investigated if they are 
weight-relevant and how much or if they are not relevant. With methods of function 
integration or separation, different functional structures can be built which have to be 
compared in analysis gate a in terms of the percentage of functions regarding weight 
and weight distribution. In principle additional functions tend to result in an increase 
of weight and in a decrease of available space. Different functional structures can 
cause significant distinctions for the further product weight owing to their realization, 
e.g. completely mechanical or mechatronic. The one which completely fulfills the 
functionality with the lowest percentage of weight-influencing functions has to be 
chosen. 
Search for Working Principles (IV). The working principles as the implementation 
of the functions have to be investigated to determine their weight dependency. Here 
can be the starting-off point for the integration of the product view: searching mecha-
tronic working principles in a catalogue in which the principles are classified in terms 
of their weight saving as well as weight distribution improving potential. Mechatronic 
concepts have often the property that they are integrating several functions in one 
working principle. As methods could be in consideration the Theory of Inventive 
Problem Solving with the Matrix of Contradictions where the contradictions have to 
investigated to get a proposal for solution, e.g. "weight of the moving object ↔ 
strength" provides one solution "substitution of the mechanics" [14]. Additionally, a 
comparison with predecessor or competitive products and their working principles 
can lead to an estimation of the weight of different physical working principles and 
can be an assessment criterion for the concept solution. 
Logical Structure (V) and Analysis Gate b. Based on the working principles, the 
logical structure is developed. It can be understood as a combination of the different 
working principles and can show possibly arising problems during the integration of 
the working principles. Similarly to the functional structure, various logical structures 
must be in consideration and compared to each other. Analysis gate b serves as check-
up of the proportion of working principles regarding weight and weight-distribution. 
Concretization of Logical Structure (VI), Physical Structure (VII) and Analysis 
Gate c. The physical structure represents the preliminary stage to the system solution 
concept. The initial structure is built up in terms of geometry based on the combina-
tion of working principles in the logical structure by choosing suitable effects for the 
working principles. The effects stored in design catalogues normally consist of geo-
metric and eventually implicit material information. For the first time, traditional 
lightweight measures are applied, e.g. usage of lightweight design techniques. Thus, 
there is the first possibility to handle explicit weight information which will be ana-
lyzed in analysis gate c. 
Evaluation of Possible Solutions (VIII). For the assessment of the best solution the 
weight has to be in consideration. Thus, the technical-economic evaluation has to be 
extended to a technical-economic-weight evaluation where functionality, costs and 

 
Early Development of Weight-Optimized Mechatronic Products 
703 
weight are represented. Hence, the aspect of weight being a development goal is  
fulfilled. Following weight criteria could be in interest: weight itself, weight distribu-
tion, center of gravity, mass (moment of) inertia, influence on functionality and on 
other important product properties. 
Solution Concept and Analysis Gate 1. The overall system solution concept is the 
conceptual elaboration of the assessed solution. Measures for weight reduction and 
improvement of weight distribution can be applied, for example lightweight design 
principles and techniques (integrated or differentiated design). Moreover, an initial 
decision of materials is taken whereat lightweight criteria are regarded. Difficulties 
can occur by regarding the former identified trade-offs and general conditions (e.g. 
recycling, manufacturing, assembly,…). Analysis Gate 1 gives estimated information 
about the weight and the weight distribution of the entire system solution. 
4 
Conclusions 
The weight and weight distribution and their consequences can significantly influence 
the fulfillment of functionality and other product properties, for example cost, stiff-
ness, safety etc. For that reason, it is relevant to consider the product weight and 
weight distribution as early as possible in the development process. But in today’s 
development processes the task of weight optimization is mostly considered only in 
later phases. The methodology “Development of Weight-optimized Mechatronic 
Products” presented offers a remedy of this drawback. In this paper, the process view 
of the methodology is presented with special focus on refinement of the early phases 
– from the requirements to a system concept. With the help of the systemic and the 
conceptual lightweight design strategy different adapted methods enable a holistic 
view and estimation of lightweight potential. In further research, the refining of the 
later development phases will be performed on the basis of the results presented. Fur-
thermore, the product view of the methodology – weight optimization with the help of 
mechatronic concepts – will be more focused on. The validation of the methodical 
framework will be demonstrated on specific examples in further research. 
References 
1. Luedeke, T., Vielhaber, M.: Lightweight Mechatronics Design – An Integrated Approach. 
In: Proceedings of DESIGN 2012, the 12th International Design Conference, Dubrovnik, 
Croatia, pp. 999–1008 (2012) 
2. Luedeke, T., Vielhaber, M.: Towards a Process Model for the Development of Light, Me-
chatronic Products. In: Proceedings of the 9th NordDesign Conference, Paper ID 47, Aal-
borg, Denmark (2012) 
3. Henning, F., Moeller, E.: Handbuch Leichtbau – Methoden, Werkstoffe, Fertigung. Hans-
er-Verlag, München (2011) 
4. Schmidt, W.: Methodische Entwicklung innovativer Leichtbau-Produkte, Fortschritt-
Berichte VDI Nr. 369. VDI-Verlag, Düsseldorf (2004) 

704 
T. Luedeke and M. Vielhaber 
5. Verein Deutscher Ingenieure: VDI Guideline 2206 – Design methodology for mechatronic 
systems. Beuth-Verlag, Berlin (2004)  
6. Ehrlenspiel, K.: Integrierte Produktentwicklung – Denkabläufe, Methodeneinsatz, Zusam-
menarbeit. Hanser-Verlag, München (2009) 
7. Klein, B.: Leichtbau-Konstruktion. Vieweg + Teubner, Wiesbaden (2009) 
8. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H., Wallace, K., Blessing, L.: Engineering 
Design – A Systematic Approach. Springer, London (2007) 
9. Feyerabend, F.: Wertanalyse Gewicht: Methodische Gewichtsreduzierung, Fortschritt-
Berichte VDI Nr. 201. VDI-Verlag, Düsseldorf (1991) 
10. Namouz, E.: Mass and Assembly Time Reduction for Future Generation Automotive Ve-
hicles Based on Existing Vehicle Model. Master Thesis, Clemson University, South Caro-
lina, USA (2010) 
11. Schmidt, W., Puri, W.: Betrachtungen zur Konzeptphase im Konstruktionsprozess von 
Leichtbauteilen. In: Beiträge zum 12. Symposium “Design for X”, Neukirchen, Germany, 
pp. 21–28 (2001) 
12. McLellan, J.M., Maier, J.R.A., Fadel, G.M., Mocko, G.M.: A Method for Identifying Re-
quirements Critical to Mass Reduction Using DSMs and DMMs. In: Proceedings of the 
11th International Design Structure Matrix Conference, Greenville, South Carolina, USA, 
pp. 197–205 (2009) 
13. Ponn, J., Lindemann, U.: Konzeptentwicklung und Gestaltung technischer Produkte – Sys-
tematisch von Anforderungen zu Konzepten und Gestaltlösungen. Springer, Berlin (2011) 
14. Lindemann, U.: Methodische Entwicklung technischer Produkte – Methoden flexibel und 
situationsgerecht anwenden. Springer, Berlin (2009) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 705–714. 
DOI: 10.1007/978-3-642-30817-8_69 
© Springer-Verlag Berlin Heidelberg 2013 
 
Statistical Tolerance-Cost-Optimization of Systems  
in Motion Taking into Account Different Kinds  
of Deviations  
Michael Walter and Sandro Wartzack 
Friedrich-Alexander-University of Erlangen-Nuremberg, Chair of Engineering Design, 
Martensstr. 9, 91058 Erlangen, Germany 
{walter,wartzack}@mfk.uni-erlangen.de 
Abstract. The time-depending motion behavior of systems in motion is essen-
tially affected by manufacturing-caused as well as operation-depending devia-
tions (e.g., deformations), which appear during the system’s use. Consequently, 
it is almost impossible for the product developer to define the optimal toler-
ances, which both ensure the mechanism’s functionality and cause minimum 
manufacturing costs. This paper presents a methodology for the “statistical to-
lerance-cost-optimization of systems in motion”. Therefore two appropriate ma-
thematical optimization concepts are developed. The practical use of the  
methodology is shown for a non-ideal crank mechanism which is subject to 
manufacturing-caused as well as operation-depending deviations. 
Keywords: tolerance optimization, tolerance-cost-relations, time-depending to-
lerance simulation, systems in motion, statistical tolerance synthesis. 
1 
Introduction 
A product‘s functionality depends largely on the interaction of its components and 
their geometries. Hence, geometric deviations of these components need to be taken 
into account in order to ensure the functionality. These deviations can result e.g., from 
manufacturing discrepancies. But also operation-depending deviations (like deforma-
tions due to inertia forces) are affecting the product’s functionality [1]. 
Usually a tolerance analysis is used to determine, how the appearing deviations are 
affecting a system’s functional key characteristics (FKCs). According to THORNTON 
[2] the FKC is “a parameter of a product, sub-assembly, part, process or resource that 
significantly impacts the final cost, performance, or safety of a product”. Already 
existing approaches enable the product developer to perform statistical tolerance ana-
lyses taking into account many aspects concerning products and processes. How-ever, 
these approaches are not integrating the specific aspects of systems in motion: The 
different kinds of deviations affect the FKCs in two different ways. Random devia-
tions (e.g., manufacturing-caused deviations of dimensions like length or height)  
result in a variation of the FKC, while the deterministic deviations (e.g., operation-
depending deformations) cause a mean shift of the FKC’s distribution. Moreover, the 

706 
M. Walter and S. Wartzack 
time-depending motion behavior and therefore the significantly varying effects of the 
appearing deviations on the FKCs make it almost impossible for the product develop-
er to define the optimal tolerances. Consequently, the question on “the best (and 
cheapest) tolerances of a system in motion” still remains unanswered. 
This paper focuses on answering this question by developing a methodology to sta-
tistical tolerance synthesis of mechanisms taking into account the resulting costs. So 
the product developer will be able to determine the optimal tolerance specification 
which ensures the fulfillment of the functional requirements and causes minimum 
costs. In order to solve this optimization problem with two diverging requirements 
(small tolerances to ensure the functionality vs. large tolerances to minimize the costs) 
two appropriate, but fundamentally different optimization concepts are developed. 
2 
State of the Art 
The tolerance analysis and tolerance synthesis – being the two major objectives of the 
geometric dimensioning and tolerancing – are well known and widely used in today’s 
product development. The large diversity of analyzed products and processes and 
therefore the resulting requirements towards the product developer reflect in research 
dealing with many different tolerance-related aspects of products and processes. 
2.1 
Tolerance Analysis of Systems in Motion 
However, the specific aspects of systems in motion during their use are not integrated 
in existing approaches of the statistical tolerance analysis. In this context, especially 
the time-depending effects of the different kinds of deviations, which appear during 
the product lifecycle, as well as possible interactions between these deviations have to 
be emphasized. This prompts HASENKAMP to say, that in particular the development 
of integrated methods, considering all product lifecycle stages, is both a promising as 
well as a necessary aim of tolerance management and robust design [3]. 
A mechanism’s kinematic behavior is essentially affected by geometric deviations 
of its components, which can be traced back to e.g., manufacturing discrepancies. So 
a time-depending tolerance analysis of the effects of geometrical deviations is re-
quired. Several publications consider manufacturing deviations for mechanism with 
lower [4, 5] and higher kinematic pairs [6]. However, during the product’s use also 
operation-depending deviations appear and affect the system’s FKCs. The displace-
ment of parts due to joint clearance is taken into account in [6] and [7], while [8] and 
[9] consider the deformation of parts due to the forces which result from the system’s 
motion. Further research activities consider both kinds of deviations (manufacturing 
as well as operation-depending) [10]. However, the time-dependence of mechanisms 
and thus of the deviations and interactions between these deviations still remain un-
considered. The approach on the “integrated tolerance analysis of systems in motion” 
is detailed in [1] and [11]. This approach allows the tolerance analysis of mechanisms 
with manufacturing-caused and operation-depending deviations. Moreover, appearing 
interactions between the different kinds of deviations can be taken into account [1]. 

 
Statistical Tolerance-Cost-Optimization of Systems in Motion 
707 
2.2 
Statistical Tolerance Synthesis and Optimization 
Recently CAMPATELLI identifies the tolerance synthesis as “currently one of the most 
proficient ways to reduce the cost of machined parts” [12]. The tolerance synthesis is 
used to distribute the tolerated variation of the functional key characteristic of an as-
sembly among the variations of the product’s components according to an allocation 
scheme. In contrast to a tolerance synthesis, the tolerance optimization uses mathe-
matical optimization methods to determine and apply the optimal allocation scheme 
according to the tolerance optimization’s diverging requirements. 
In the early days of research on the field of “tolerance optimization” as one of the 
first [13] used optimization algorithm (Simulated Annealing) to investigate the single 
part’s tolerances and the corresponding costs. The possibility to reduce the tolerance-
caused manufacturing costs by changing the chosen manufacturing process is taken 
into account in [14]. Hence, the tolerance-cost-relations, which are used as the opti-
mization’s constraints are discontinuous functions. The research of [15] and [16] fo-
cus on the tolerance optimization taking into account both the tolerance range and the 
tolerance’s mean shift. Therefore [15] uses mathematical optimization algorithm, 
while [16] uses the Lambert W function to determine the optimal tolerance specifica-
tion of a non-dynamic (no time-dependence) system. 
The tolerance synthesis and tolerance optimization of time-depending systems 
(e.g., systems in motion) are rarely subject of tolerance-related research. Into the bar-
gain, the effects of different kinds of deviations are hardly considered in these re-
search activities. In [17] an iterative tolerance synthesis process of a non-ideal crank 
mechanism is detailed. However, the time-depending consideration is limited to just 
one motion sequence of the mechanism and the needed functional relationship is not 
time-depend. Moreover, aside of manufacturing-caused deviations no additional devi-
ations (like operation-depending deformations) are taken into account. In contrast to 
[17], the tolerance optimizations in [10] and [18] use a time-depending functional 
relation between the functional key characteristic and the appearing deviations. How-
ever, also these works are limited to manufacturing deviations, since the effects of 
additional operation-depending deviations are not considered. 
3 
Motivation and Identified Problems 
As detailed, a variety of research considers the tolerance synthesis / tolerance optimi-
zation of non-ideal technical systems. However, especially the two main characteris-
tics of systems in motion – the appearance of operation-depending deviations and the 
time-dependence of the system and thus of the functional key characteristics and the 
appearing deviations – are not taken into account yet. 
This results in two major problems the product developer has to face: On the one 
hand, the different kinds of deviations affect the functional key characteristics in two 
different ways: Random deviations (e.g., manufacturing-caused deviations of dimen-
sions like length or height) result in a variation of the functional key characteristic, 
while the deterministic deviations (e.g., the operation-depending deformation due to 
inertia forces) cause a mean shift of the FKC’s distribution (Figure 1). 

708 
M. Walter and S. Wartzack 
 
Fig. 1. Mean shift (MS) of the FKC due to systematic operation-depending deviations 
On the other hand, the time-depending motion behavior and thus the significantly 
varying effects of the deviations make it almost impossible for the product developer 
to define the optimal tolerances. Consequently, the question on “the best (and cheap-
est) tolerances of the parts of a system in motion” remains unanswered. 
4 
Work Methodology 
The paper focuses on the statistical tolerance optimization of systems in motion tak-
ing into account the system’s time-depending motion behavior and the effects result-
ing from systematic operation-depending deviations (mean shifts). So the product 
developer will be able to define the optimal tolerance specification which ensures the 
fulfillment of the system’s functional requirements and causes minimum costs. 
At first, an appropriate mathematical concept must be formulated to solve the con-
sidered optimization problem with its two diverging requirements (small tolerances to 
ensure the functional fulfillment vs. large tolerances to minimize the costs). There-
fore, two fundamentally different optimization concepts are developed and presented 
in section 5. Furthermore, a methodology has to be derived, that supports the product 
developer when performing the “statistical tolerance-cost-optimization of a system in 
motion” (section 6). In order to show the concepts’ practical use, a tolerance-cost-
optimization of a non-ideal crank mechanism inside a 4-stroke combustion engine 
will be performed in section 7. The crank mechanism’s components are subject to 
both manufacturing-caused as well as operation-depending deviations. The paper 
closes with a comparison of both concepts concerning the determined tolerance speci-
fications, the resulting manufacturing costs as well as the numerical expense. 
5 
Mathematical Concepts 
In order to develop an appropriate mathematical concept for the needed tolerance-
cost-optimization, the verbal formulation of the considered optimization problem is 
useful. Therefore, the objective as well as the corresponding constraints and restric-
tions of the optimization problem have to be identified. These can be derived from the 
challenge the product developer has to face during his/her every day work: The over-
all objective of a successful product development is to ensure the product’s functio-
nality which goes hand in hand with the ambition to realize products of high quality. 
This should be achieved in less time, causing low manufacturing costs. 
FKCideal
Functional key
characteristic
variation
variation and mean shift of the
distribution
Frequency
mean shift MS

 
Statistical Tolerance-Cost-Optimization of Systems in Motion 
709 
Concerning the tolerance specification of non-ideal systems in motion, two differ-
ent points of view can be derived, based on the verbal optimization problem: 
• The deviation of the system’s functional key characteristic should be as small as 
possible at any point in time of the motion sequence, as long as the corresponding 
manufacturing costs do not exceed a given limit. 
• The manufacturing costs which result from the tolerance specification should be as 
small as possible, as long as the functional key characteristic does not exceed the 
given upper and lower specification limits at any point in time of the motion. 
Consequently, multiple concepts can be developed in order to solve these optimiza-
tion problems. The following remarks of the subsections 5.1 and 5.2 detail two prom-
ising concepts to solve the two different optimization problems. 
5.1 
Concept 1 
The objective of the first concept is to minimize the deviation of the functional key 
characteristic (FKC) from its ideal value (FKCideal) at any point in time t of the motion 
sequence. Therefore, the distribution and the ±3σ quantiles of the FKC are determined 
statistically, based on the functional relation and weighted by w(t) which corresponds 
to the FKC’s mean shift at any point in time. The optimization constraint is the max-
imization of the component’s manufacturing tolerances Ti within a given cost limit 
Kmax. Therefore the lower limits of the tolerances can be defined e.g., by the precision 
of the used manufacturing processes. This concept corresponds to TAGUCHI’s philos-
ophy of quality – saying that a product’s deviations should be as small as possible 
since otherwise the corresponding quality loss increases [19]. 
 
݉݅݊∑
|ݓሺݐሻ· ሾܨܭܥሺݐሻെܨܭܥ௜ௗ௘௔௟ሺݐሻሿ|
்
௧ୀ଴
 
(1) 
 
ݓሺݐሻ= ܧሾܨܭܥሺݐሻሿെܨܭܥ௜ௗ௘௔௟ሺݐሻ 
(2) 
subject to constraints: 
ܭ௧௢௧௔௟൑ܭ௠௔௫ 
(3) 
 
ܶ௜൐0 
(4) 
5.2 
Concept 2 
In contrast to the first concept, the manufacturing costs (resulting from the tolerance 
specification) are the objective of the second concept. These manufacturing costs Ktotal 
have to be minimized, based on the corresponding tolerance-cost-relations of each 
tolerance Ti. However, the ±3σ quantiles of the functional key characteristic should 
not exceed its given time-depended lower and upper specification limits LSL(t) and 
USL(t). Similar to concept 1, also the manufacturing-caused tolerances should be 
maximized. 
 
minሺܭ௧௢௧௔௟ሻ 
(5) 
subject to constraints: 
ܮܵܮሺݐሻ൑ܨܭܥሺݐሻ൑ܷܵܮሺݐሻ 
(6) 
 
ܶ௜൐0 
(7) 

710 
M. Walter and S. Wartzack 
5.3 
Considered Parameters of Both Concepts 
In addition to the range of the manufacturing tolerances Ti, also a mean shift of these 
tolerances MS(Ti) will be considered during the tolerance-cost-optimization. This is 
due to the fact, that the appearing mean shift of a functional key characteristic can 
only be compensated by an appropriate mean shift of the manufacturing tolerances. 
Hence, the tolerance-cost-optimization considers both possibilities the product devel-
oper has to affect the deviation of a functional key characteristic: Changing the range 
Ti as well as the mean shift MS(Ti) of the appearing manufacturing tolerances. Con-
sequently, both each tolerance’s range Ti and mean shift MS(Ti) must be included in 
the concept’s relation of the total manufacturing costs Ktotal (objective and/or con-
straints) to be considered during the optimization according to: 
 
ܭ௧௢௧௔௟= ∑ሾܭሺܶ௜ሻ+ 0 · ܯܵሺܶ௜ሻሿ
௜
 
(8) 
However, since the manufacturing tolerances’ mean shifts MS(Ti) do not cause addi-
tional manufacturing costs these are multiplied by the factor zero in equation (8). 
6 
Methodology 
In order to support the product developer to perform a statistical tolerance-cost-
optimization of a system in motion, an appropriate methodology is derived (Figure 2). 
 
Fig. 2. Methodology: “Integrated statistical tolerance-cost-optimization of systems in motion” 
Analogous to the “integrated tolerance analysis” [1], at first the functional relation 
between the functional key characteristic and the deviations which appear in the sys-
tem’s lifecycle has to be formulated. Therefore closed vector-chains can be used. 
Based on the functional relation and the corresponding tolerance-cost-relations of 
each manufacturing-caused deviation, the tolerance optimization problem has to be 
formulated. Consequently, as detailed in section 5, also the mathematical concept as 
well as the objective, constraints and restrictions of the statistical tolerance-cost-
optimization are defined thereby. The mathematical optimal solution, which could 
e.g., mean “the cheapest, but still full functional tolerance specification”, is deter-
mined using appropriate optimization algorithms. Since the kinematic behavior of 
systems in motion usually leads to non-linear functional relations, appropriate algo-
rithms must be used to solve the considered optimization problem. 
objective
Technical 
system
Tolerance-
cost relations
Functional 
relation
Application 
of 
optimization 
algorithm
Represen-
tation of 
the results
Deviations
constraints
restrictions
Formalization 
of the tolerance 
optimization 
problem and 
choice of 
concept

 
Statistical Tolerance-Cost-Optimization of Systems in Motion 
711 
The final step is similar to the closing step of the “integrated tolerance analysis” - 
the result representation. This includes the tolerances Ti and mean shifts MS(Ti) of the 
manufacturing deviations as well as the resulting manufacturing costs Ktotal. 
7 
Demonstrator: Crank Mechanism 
The demonstrator, used to show the tolerance-cost-optimizations’ practical use, is a 
crank mechanism inside a 4-stroke combustion engine. The appearing deviations are 
essentially affecting the precise motion of the crank mechanism’s components. These 
deviations can cause collisions of the mechanism’s components among themselves 
(e.g., piston and crank shaft) or collisions with additional parts of the engine (e.g., 
piston and valves). Furthermore, the engine’s performance (e.g., engine’s combustion 
ratio) is affected due to the varying position of the piston during the motion sequence. 
Consequently, the functional key characteristic of the crank mechanism is the position 
of the piston along the X-axis of the global coordinate system (see Figure 3). 
 
Fig. 3. Crank mechanism with manufacturing-caused deviations and coordinate system [1] 
The piston underlies a time-depending combustion pressure with a maximum of 
180 bar, which results in a rotation of the crank shaft with 3000 rpm [11]. A motion 
sequence consists of two rotations of the crank shaft with a total crank angle of 
φ = 720°. The considered deviations are: 
• Manufacturing-caused deviation of the crank radius r = 45 ± 0.02 mm (distribution: 
triangle) and the con rod link length l = 138 ± 0.05 mm (distribution: trapeze) 
• Operation-depending deformation of the crank shaft and displacement of the con 
rod due to joint clearance s = 0.06 mm in the lower con rod bearing 
Xglob
Yglob
crank shaft
con rod
piston
piston position (FKC)

712 
M. Walter and S. Wartzack 
8 
Tolerance-Cost-Optimization of the Crank Mechanism 
In order to ensure the crank mechanism’s functionality, the deviation of the piston 
from its ideal value is limited to the specification limits of ± 0.2 mm. At the beginning 
of the crank mechanism’s motion sequence (φ = 10°), the lower 3σ-quantile of the 
deviation of the piston DevPiston exceeds its lower specification limit (LSL = -0.2 mm) 
and consequently causes defective crank mechanisms, which do not fulfill the func-
tional requirements. In order to reduce the number of defective mechanism, a redesign 
of the tolerances of the crank mechanism’s components should be done by the product 
developer. Hence, the presented methodology of the “statistical tolerance-cost-
optimization of a system in motion” will be used. 
Therefore the so-called reciprocal power cost-tolerance relations are used, accord-
ing to [20]. This tolerance-cost-model considers the component’s fixed costs Ai, 
which include e.g., the needed tools. The costs Bi represent the individual manufactur-
ing costs of the considered tolerance Ti. 
 
ܭሺܶ௜ሻ= ܣ௜+ ܤ௜· ሺܶ௜ሻିଵ 
(9) 
Consequently, the needed tolerance-cost-relations can be formulated for the crank 
radius r (Ar = 25€, Br = 1.50€) and the con rod length l (Al = 30€, Bl = 2€). Since the 
initial tolerance specification (r: ± 0.02, l: ± 0.05) causes manufacturing costs of 
112.50€, the limit of the first concept’s manufacturing costs Kmax should be 100€. 
Consequently, the optimization of concept 1 should lead to a fully functional toler-
ance specification at even lower costs. The optimization runs are done for 100 (con-
cept 1) and 5000 (concept 2) Monte-Carlo-based samples. Table 1 details the results. 
Table 1. Results of the tolerance-cost-optimizations of the crank mechanism 
 
Time   
in h 
MS(Tr) 
in mm 
Tr        
in mm 
MS(Tl) 
in mm 
Tl        
in mm 
Costs  
in € 
Defects 
Concept 1 
4.9 
0.010 
0.149 
0.077 
0.180 
76.18 
Yes 
Concept 2 
29.0 
-0.008 
0.083 
0.089 
0.081 
97.76 
No 
 
It can be noticed that the tolerances of the first optimization concept result in lower 
manufacturing costs, but still lead to defective mechanisms, exceeding the upper spe-
cification limit. However, concept 2 results in a tolerance specification, which ensures 
the product’s functionality and even reduces the corresponding manufacturing costs. 
The numerical expense of both concepts, but in particular of concept 1 (~5 hours for 
100 Samples), is immense. Consequently, the second concept should be the product 
developer’s first choice to determine the optimal tolerance specification. The time-
depending deviation of the piston DevPiston as well as its upper and lower specification 
limits are shown in figure 4. 

 
Statistical Tolerance-Cost-Optimization of Systems in Motion 
713 
 
Fig. 4. Deviation of the Piston during one motion sequence of the crank mechanism 
9 
Conclusion and Outlook 
This paper focused on the tolerance-cost-optimization of a system in motion taking 
into account the two main characteristics of systems in motion during its use – the 
appearance of different kinds of deviations and the time-dependence of the system 
and thus of the functional key characteristics and the appearing deviations. Therefore 
both each tolerances’ range Ti as well as the corresponding mean shifts MS(Ti) must 
be considered – since those two parameters have to be defined by the product devel-
oper during the product development process. 
A methodology was presented which supports the product developer to systemati-
cally determine a mechanism’s optimal tolerance specification. Therefore, two differ-
ent mathematical optimization concepts were derived and implemented. The practical 
use of the statistical tolerance-cost-optimization was shown for a crank mechanism, 
which is subject to both manufacturing as well as operation-depending deviations. 
Basically, the tolerance-cost-optimization of systems in motion (using concept 2) 
enables the product developer to determine the tolerance specification, which ensures 
the time-depending system’s functionality during the system’s use, while causing low 
manufacturing costs. However, the possibility of choosing alternative manufacturing 
processes and the effects on the tolerance-cost-relations is an important aspect that is 
not considered yet. Furthermore, possible interactions between the appearing devia-
tions are currently not taken into account. Consequently, the presented tolerance-cost-
optimization has to be improved towards more complex systems. 
Acknowledgment. The authors thank the German Research Foundation (DFG) for 
supporting the research project ME1029/16-1 “Functional product validation and 
optimization of technical systems in motion as a part of product lifecycle oriented 
tolerance management”. 
-0.25
-0.20
-0.15
-0.10
-0.05
0.00
0.05
0.10
0.15
0.20
0.25
0
90
180
270
360
450
540
630
720
Deviation of pistson from its ideal position
DevPiston in mm
crank angle ĳ in °
Max (concept 1)
Max (concept 2)
Min (concept 1)
Min (concept 2)
upper specification limit USL = + 0.2 mm 
lower specification limit LSL = - 0.2 mm

714 
M. Walter and S. Wartzack 
References 
1. Walter, M., Wartzack, S.: Analysis of the effects of manufacturing-caused deviations and 
varying operation parameters on operation-depending deviations of systems in motion. In: 
12th CIRP Conference on Computer Aided Tolerancing, Huddersfield (2012) 
2. Thornton, A.C.: Variation Risk Management Using Modeling and Simulation. J. Mech. 
Design 2, 297–304 (1999) 
3. Hasenkamp, T., Arvidsson, M., Gremyr, I.: A review of practices for robust design metho-
dology. J. Eng. Design 6, 645–657 (2009) 
4. Adabi, M.E., Farkhondeh, S., Farkhondeh, F.: Tolerance Analysis of Mechanisms by Us-
ing a Fictitious Slider. Eur. J. Sci. Res. 3, 385–398 (2010) 
5. Bruyère, J., Dantan, J.Y., Bigot, R., et al.: Statistical tolerance analysis of bevel gear by 
tooth contact analysis and Monte Carlo simulation. Mech. Mach. Theory 42, 1326–1351 
(2007) 
6. Sacks, E., Joskowicz, L.: Parametric kinematic tolerance analysis of general planar sys-
tems. Compu.-Aided Des. 30, 707–714 (1998) 
7. Muvengei, O., Kihiu, J., Ikua, B.: Dynamic Analysis of Multi-Body Mechanical Systems 
with Imperfect Kinematic Joints: A Literature Survey and Review. Sustainable Research 
and Innovations Proceedings 3 (2011) 
8. Dupac, M., Beale, D.G.: Dynamic analysis of a flexible linkage mechanism with cracks 
and clearance. Mech. Mach. Theory 45, 1909–1923 (2010) 
9. Imani, B.M., Pour, M.: Tolerance analysis of flexible kinematic mechanism using DLM 
method. Mech. Mach. Theory 44, 445–456 (2009) 
10. Hanzaki, A.R., Rao, P.V.M., Saha, S.K.: Kinematic and sensitivity analysis and optimiza-
tion of planar rack-on-pinion steering linkages. Mech. Mach. Theory 44, 42–56 (2009) 
11. Stuppy, J., Meerkamm, H.: Tolerance analysis of a crank mechanism by taking into ac-
count different kinds of deviations. In: 11th CIRP International Seminar on Computer 
Aided Tolerancing, Annecy (2009) 
12. Campatelli, G.: Tolerance Synthesis Using Axiomatic Design. In: Proceedings of the Sixth 
International Conference on Axiomatic Design, Daejeon, pp. 152–157 (2011) 
13. Zhang, C., Wang, H.-P.: Integrated Tolerance Optimisation with Simulated Annealing. Int. 
J. Adv. Manuf. Tech. 3, 167–174 (1993) 
14. Sivakumar, K., Balamurugan, C., Ramabalan, S.: Concurrent multi-objective tolerance al-
location of mechanical assemblies considering alternative manufacturing process selection. 
Int. J. Adv. Manuf. Tech. 5-8, 711–732 (2010) 
15. Zhang, J., Li, S.P., et al.: A robust design approach to determination of tolerances of me-
chanical products. CIRP Ann. Manuf. Technol. 1, 195–198 (2010) 
16. Shin, S., Kongsuwon, P., Cho, B.R.: Development of the parametric tolerance modeling 
and optimization schemes and cost-effective solutions. Eur. J. Oper. Res. 3, 1728–1741 
(2010) 
17. Krishnaswami, P., Kelkar, A.G.: Optimal design of controlled multi-body dynamic sys-
tems for performance, robustness and tolerancing. Eng. Comput. 1, 26–34 (2003) 
18. Goethals, P.L., Cho, B.R.: The Development of a Robust Design Methodology for Time-
oriented Dynamic Quality Characteristics with a Target Profile. Qual. Reliab. Eng. Int. 4, 
403–414 (2011) 
19. Taguchi, G., Chowdhury, S., Wu, Y.: Taguchi’s Quality Engineering Handbook. John Wi-
ley & Sons, New Jersey (2004) 
20. Sutherland, G.H., Roth, B.: Mechanism Design: Accounting for Manufacturing Tolerances 
and Costs in Function Generating Problems. J. Eng. Ind.-T ASME 97, 283–286 (1975) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 715–724. 
DOI: 10.1007/978-3-642-30817-8_70 
© Springer-Verlag Berlin Heidelberg 2013 
 
Forecasting Environmental Profiles in the Early Stages  
of Product Development by Using  
an Ontological Approach 
Hesam Ostad-Ahmad-Ghorabi 1, Touba Rahmani2, and Detlef Gerhard2 
1 Magna Steyr Engineering Germany (MSED), Kolumbusstraße 1,  
Sindelfingen, 71063, Germany 
2 Institute of Engineering Design and Logistics Engineering, Vienna University of Technology, 
Getreidemarkt 9, Vienna, 1060, Austria 
hesamedin@ostad.at, 
{touba.rahmani,detlef.gerhard}@tuwien.ac.at 
Abstract. Considering environmental aspects in early product development 
stages is a complex endeavor. Product life cycle data are fuzzy and subject to 
changes. Additional workload due to data handling is a common reason why it 
is withdrawn by engineering designers. Some studies suggest parameterization 
of products in order to gain a limited set of parameters to handle, some others 
suggest integration of Life Cycle Assessment into CAD or Product Data Man-
agement systems. However, the handling of heterogeneous data from multiple 
sources is not paid much attention. This paper suggests an ontological approach 
that allows considering data from multiple sources to set up an environmental 
profile of the product and allow for adaptations in the product concept. 
Keywords: ecodesign, life cycle assessment (LCA), sustainability. 
1 
Introduction 
The integration of Life Cycle Assessment (LCA) into early stages of product devel-
opment is an important and efficient way to strive for environmental benign products. 
However, conducting an LCA in the early design stages is a difficult and complex 
endeavor: in these early stages, the product is subject to many changes which affect 
the life cycle of the product and its environmental performance. Later in the develop-
ment process, more specific data may be available, but the possibility to influence the 
product decreases; a phenomenon known as the design paradox [1]. 
Recently, some studies have suggested comparing the environmental performance 
of a new product being developed with similar products in the market [2]. The so-
called Life Cycle Comparison Family (LCP-family) includes products that have simi-
lar functional units. Product similarity can therefore be understood as similarity in 
regard of similar functionalities or similar structures (e.g. similar hierarchy of parts 
and components, similar platforms, etc…). In this paper, products that fulfill the same 
or similar requirements are taken to infer the environmental performance of a new 
product that shares similar requirements.  

716 
H. Ostad-Ahmad-Ghorabi, T. Rahmani, and D. Gerhard 
Design process is already complex, and engineering designers are not necessarily 
environmental experts. Any approach to integrate LCA into early product develop-
ment and design stages will fail if it adds to workload or complexity to the process. 
Engineering designers usually have a good understanding of the product they are 
developing as well as of the benchmarks and competitors. To implement a methodol-
ogy for environmental assessment at this phase of data availability, Ostad-Ahmad-
Ghorabi and Collado-Ruiz [3] have proposed a methodology that uses only  
information available and commonly known by engineering designers in the early 
product development stages; this information is called primary parameters. The me-
thodology asks for technical data, which is usually defined in the list of requirements 
of the product. This data is then linked to life cycle inventory data to infer environ-
mental performance. This can be compared to similar products, as described above. 
However, this methodology has been developed for a specific product type, namely 
cranes. A systematic approach to derive primary parameters is missing. 
In this paper, authors are proposing an ontological approach to set up primary pa-
rameters systematically for particular product categories. The aim of using an onto-
logical approach is to enable the management and use of heterogeneous data along the 
product development process [4] [5]. In fact, data defined in the proposition of re-
quirements are taken and processed to set up primary parameters suitable for a specif-
ic product category. Further, an ontology can be used to establish a pool of proper 
products serving as references to compare the environmental performance of the 
product with as well as to position it within the benchmark.  
A case study of hydraulic machines is presented. A proper ontology is developed 
and suitable primary parameters are derived by considering the requirements of hy-
draulic machines. A specific product among hydraulic machines serves as case study 
and withthe help of the primary parameters, a forecast for the environmental perfor-
mance of this product is derived and compared with a suitable benchmark. The paper 
demonstrates how data from various sources, i.e. data found in the definition of re-
quirements, data considering the functional structure or data from the process struc-
ture can be efficiently handled by using the developed ontologapproach that links this 
data with environmental life cycle data and allows for the forecast of an environmen-
tal profile and a comparison with similar products. The main goal of the methodology 
proposed in this paper is to bring together all relevant data for environmental assess-
ment into the early product development stages. At the same time, it is of highest 
priority to avoid confronting any user with all the detail information and flood of data 
needed to proceed with a full LCA. However, the assessment shall be representative 
enough to allow for strategic decisions regarding the preliminary development of the 
product; may it be the conceptualization of parts and components, material composi-
tion, realization of functionalities or else.  
2 
State of the Art 
The start of the product development process is characterized by giving answer what 
requirements the product has to fulfill. Two types of requirements can be distin-
guished: general requirement, valid for a product category, and specific requirements, 
valid for specific products within a category. An example for general requirements is 

 
Forecasting Environmental Profiles in the Early Stages of Product Development 
717 
the minimum safety requirements a car has to fulfill in accordance with standards and 
regulations, e.g. requirements to be fulfilled for frontal-impact test. Specific safety 
requirements can be defined by a car manufacturer and constitute all additional fea-
tures (e.g. realization of tiredness sensor for the driver or realization of vehicle-
interval radar) which add to safety, but are not demanded by any regulation. 
Once requirements are defined, the next step is to think about how the require-
ments can be realized, which functions have to be realized and what parts and compo-
nents are needed . Usually, the first approach would be to gather as much information 
as possible from previous or similar products  or to take a deeper look into products 
from the benchmark.In fact, there are four main source of information that are availa-
ble and can therefore be handled in the early product development stages:   
1. Information that are used to set up requirements 
2. Information regarding the realization of functions and processes 
3. Information from previous product concepts or variants 
4. Information from similar products or benchmarks 
Most of the available information is either of general nature or, specific to a product 
but fuzzy and subject to changes and adaptations.  
The aim of conducting an environmental assessment of a product in early product 
development stages may therefore suffer from insufficient data quality. However, 
Ostad-Ahmad-Ghorabi and Collado-Ruiz have shown that it is possible to conclude to 
a reliable environmental profile and assessment results by developing a parametric 
model of the product. Furthermore it is possible to benchmark the environmental pro-
file by setting up a proper family [2,3,6]. However, some existing shortcomes with-
draw the concepts to be practicable for daily use. Firstly, the parametric model was 
derived for a specific product and no automated process was used to derive the model. 
Secondly, the concepts of a parametric model and benchmark family are not linked.   
What can be taken as an essential module for the development of an ontological 
approach is the concept of primary parameters, defined as “…the most important 
design parameters that are defined in the very early conceptual design stages”. Also 
the concept of secondary parameters, through which data in the LCA inventory can be 
described, is of important need for the methodology being developed in this paper. [3]  
3 
Method and Concept 
To be able to consider all sources of information, an ontology [7] is developed to 
enable information handling during early product development stages. In this paper, 
the method of Noy and McGuinness is used to develop the ontology [8]. Their method 
premises that the development of an ontology is an iterative approach [9]. Ontology 
concepts shall consider objects and relations of the domain. Objects are described by 
nouns and relations by verbs. 
According to Noy and McGuiness, seven steps have to be followed to create an  
ontology:  

718 
H. Ostad-Ahmad-Ghorabi, T. Rahmani, and D. Gerhard 
1. Determining the domain of the ontology: This is done by giving answer to the  
following questions:  What is the ontology going to be used for? What types of 
questions shall be given answer to by the use of the information contained in the 
ontology? Who will use and maintain the ontology? 
2. Reusing existing ontologies: Existing ontologies may be adapted for the particular 
domain. Many available ontologies can be accessed through different libraries in 
the internet, e.g. Ontolingua ontology library [10]. 
3. Specifying the important terms and listing them: The aim is to create a comprehen-
sive list of terms, without worrying about overlap thoughts. Giving answer to 
“What are the terms the ontology should talk about?” and “What properties do 
those terms have?” can help in this endeavor.  
4. Defining and creating classes and class hierarchies: They are described using for-
mal (mathematical) descriptions that state precisely the requirements for member-
ship of the class. For example, the class Product would contain all the individuals 
that are Products in our domain of interest. Classes may be organized into a super-
class-subclass hierarchy, which is also known as taxonomy. 
5. Determining properties of the classes: To answer the questions defined in step 1, 
more information is needed, in particular the relation between different items of the 
class. Formulation such as “has a” or “is part of” can be used. Subclasses inherit all 
slots from superclasses. 
6. Determining the facets of the slots: a slot can have different facets describing: val-
ue type, cardinality or permissible values (domain and range). Common value 
types are: string or number. The slot cardinality defines how many instances a slot 
can have 
7. Creating individual instances in the hierarchy. The approach is as follows: First a 
class is chosen, second an individual instance for that class is created, and third, the 
slot values are defined. 
 
 
Fig. 1. Top Level Ontology: R: Requirement, F: Function, P: Process, PS: Product structure, 
LCA: Life cycle Assessment, PP: Primary Parameter, SP: Secondary Parameter 

 
Forecasting Environmental Profiles in the Early Stages of Product Development 
719 
The ontology serves as a semantic network, which represents and provides infor-
mation in a structured way [11]. Product functions, product structures and processes 
are determined by requirements. Product structure refers to information that can be 
extracted from pervious products and product variants as well as from benchmark 
products. It implies information about the hierarchical structure of parts and compo-
nents. Process implies all activities needed to develop a product; including design 
processes or manufacturing processes. Information processed from the requirements 
is used to set up primary parameters. Secondary parameters are derived from the LCA 
model, which can be set up once the product structure, functions and processes are 
known. LCA inventory data can be assigned to secondary parameters.    
4 
Case Study and Results 
The ontology in this paper is developed for hydraulic machines. This product category 
contains products such as hydraulic cranes, loaders, pumps or similar. The example of 
knuckle-boom cranes is further detailed. The ontology shall help to retrieve an envi-
ronmental profile of the product in early design stages.  
To fill the ontology sketched in Fig.1, a list of requirements of hydraulic machines 
was consulted first. For the example of knuckle-boom cranes, the most important 
requirement is to lift a certain load over a certain length. At the same time, the dead 
weight of the crane has to be minimized in order to allow for maximum lifting load. 
To implement the requirements, certain functions have to be realized. To lift a load, 
cylinders have to be moved; in fact hydraulic oil has to be pumped into the cylinders. 
This function on the other hand accounts for further requirements, e.g. those regarding 
the flow rate of the oil pump. Considering product development processes, more in-
terrelations between the parameters occur. Table 1 shows an excerpt of the list of 
requirements, the functions and processes and how they interlink. 
Table 1. Requirements, functions and processes for crane  
Requirement 
ID 
Interlinks with  
Total weight of crane 
R-1 
F-1 
Maximum lifting moment 
R-2 
F-1, F-2 
Operating time of crane over lifetime 
R-3 
F-1, f-2 
 
 
 
Function 
 
 
Lift load 
F-1 
 
Pump oil 
F-2 
 
Rotate main boom 
F-3 
 
 
 
 
Process 
 
 
Determine output power 
P-1 
F-2 
Determine specific fuel consumption 
P-2 
F-1 
Determine necessary oil volume 
P-3 
F-1 

720 
H. Ostad-Ahmad-Ghorabi, T. Rahmani, and D. Gerhard 
The information above shows what is available in the very early design stages. On 
the one hand, this information can be taken to generate primary and secondary para-
meters. The result is taken into account when aiming at conducting an environmental 
evaluation.   
For the aforementioned main requirement of the crane, the primary parameter that 
maps this requirement is Maximum lifting moment, its unit in meter tons (mt). Accor-
dingly, the parameter that is able to map the requirement of minimum dead weight is 
Maximum weight of crane, its unit in tons (t). The complete list of primary parameters 
for the requirements listed in Table 1 is retrieved according to the method in de-
scribed in [3]. An excerpt is listed in Table 2. 
Table 2. Primary parameters for a crane 
Parameter 
Unit 
Maximum lifting moment 
meter ton 
Total weight of crane 
Ton 
Estimated weight distribution of each  component 
Ton 
Manufacturing site 
- 
Weight of packaging 
Ton 
Flow rate of oil 
dm3/sec 
Etc… 
 
 
Secondary parameters are derived out of information needed for LCA. This re-
quires the handling of inventory data, usually available through LCA databases.  
Primary parameters and secondary parameters are linked through either guidelines, 
physical independencies or statistical data. The latter refers to information from pre-
vious product concept and variants or benchmark information.    
Information at hand is different in quality and heterogeneous in their sources.  To 
be able to handle the information properly, an ontology is developed by following the 
seven step approach discussed earlier in the paper:   
• Step 1: Domain: Life Cycle Assessment in early stages of product development. 
• Step 2: Does not apply to this case study, since there is no ontology available that 
can be reused, adapted or extended. 
• Step 3: Important terms: LCA, Part, Primary Parameters, Environmental Perfor-
mance, Requirements, Engineering 
• Step 4: Classes and class hierarchies: Classes: Function, Condition, Life Cycle 
Assessment, Primary Parameter, Process, Product, Requirement, Result, Second-
ary Parameter; Class hierarchies: Relationen: e.g. Condition to Process, Function 
to Product, Process to Product, Requirement to Function etc.   
• Step 5: Properties of the product classes: has amount, has ID-number, has Materi-
al, is Part of, has Weight, has Version etc.  
• Step 6: Determining the facets of the slots: Value types for Life Cycle Assessment 
class: Potential for global warming indicator, expressed in g-CO2-eq, for all life 
cycle stages (Raw materials, manufacturing, distribution, use and end of life)     
• Step 7: Instances: Product: Crane_20LM  
The result is partly shown in Fig. 2. 

 
Forecasting Environmental Profiles in the Early Stages of Product Development 
721 
 
Fig. 2. Protégé [12] screenshot of the ontology) with Protegé Jambalaja Plugin [13] 
To demonstrate the usage of the ontology, first a reference crane is considered. The 
reference crane has a maximum lifting moment of 20mt. Since maximum lifting mo-
ment is the most important design parameter for a crane, it is assumed that for a new 
model of the crane, this parameter is changed from 20mt to 78mt. This results in a 
bigger crane that has to fulfill different requirements, may have additional functionali-
ties or asks for different processes in the development process. All these changes 
influence the environmental performance of the product.   
With the help of the ontology, it is now possible to track how the change in the pa-
rameter maximum lifting moment will finally influence the environmental evaluation 
results. Using SPARQL [14] as ontology querying language, all information inter-
linked with the parameter maximum lifting moment will be listed. The query indicates 
that the primary parameter maximum lifting moment is interlinked with the functions 
lift_load, move_cylinder, move out crane, pick_load, transmit_torque, pump_oil or 
operate_hydraulich_pump. These functions on the other hand, are linked to processes 
such as testing cylinder with load, determine oil volume, or determine output power of 
oil pump. Some of these processes are already linked to LCA inventory data, for ex-
ample oil volume is linked to LCA inventory data of oil, and CO2 value for oil can 
directly be linked to its volume.  

722 
H. Ostad-Ahmad-Ghorabi, T. Rahmani, and D. Gerhard 
 
 
Fig. 3. SPARQL query [15] and results 
The results further indicate that 89% of the total environmental impacts of the 
crane occur in its use phase. The impact sin the use stage are dominated by the fuel 
consumption for crane operation. Improvements can be achieved by installing affec-
tive oil pumping systems (e.g. variable displacement pumps rather than fixed ones) or 
thinking of alternative power supply for the crane (e.g. electric power supply on con-
struction sites and whenever possible). The impacts in the other life cycle stages are 
negligible compared to the use phase (materials 6%, manufacturing 7%, distribution 
negligible, end of life -2%, due to recycling processes).  

 
Forecasting Environmental Profiles in the Early Stages of Product Development 
723 
5 
Summary 
In the early stages of product development a mixture of qualitative and quantitative 
data is available. This data can be taken to forecast the environmental profile of the 
product. To avoid additional workload for engineering designers for handling data 
and providing relevant data at any step of the product development process, it is im-
portant to extract as much information automatically as possible. The interrelations of 
information from different sources are an important aspect in this process. The ontol-
ogy described in this paper shows how different data found in the list of requirements, 
in the qualitative description of functions and processes can be interlinked. The 
SPARQL query provides a platform where the query asks for the relevant information 
and where the user can input as much information as is known at a particular time. 
The ontology links the data with primary and secondary parameters; the latter itself is 
linked to LCA inventory data. The LCA inventory data can then be used to set up o 
first environmental profile of the product.  
In future research steps, more ontologies will be developed for the same purpose 
for different product types. The aim is to provide suitable ontology elements for as 
many product types and categories as possible.    
 
Acknowledgments. The project in this paper has been funded by the Austrian Federal 
Ministry of Science and Research in the scope of the Sparkling Science research  
program. 
References 
1. Lindahl, M.: Engineering designers’ requirements on design for environment methods and 
tools. PhD thesis, Kungliga Tekniska Högskolan (KTH), Stockholm (2005) 
2. Collado-Ruiz, D., Ostad-Ahmad-Ghorabi, H.: Comparing LCA Results out of Competing 
Products - Developing Reference Ranges from a Product Family Approach. Journal of 
Cleaner Production 18(4), 355–364 (2010) 
3. Ostad-Ahmad-Ghorabi, H., Collado-Ruiz, D.: Tool for the Environmental Assessment of 
Cranes Based on Parameterization. International Journal of Life Cycle Assessment 16(5), 
392–400 (2011) 
4. Sudarsan, R., Fenves, S.J., Sriram, R.D., Wang, F.: A product information modeling 
framework for product lifecycle management (NIST information modeling framework). 
Computer-Aided Design 37, 1399–1411(2005); Manufacturing Systems Integration Divi-
sion, Manufacturing Engineering Laboratory, National Institute of Standards and Technol-
ogy, Gaithersburg, MD 20899, USA (accepted February 2, 2005) 
5. Chen, Y.-J., Chen, Y.-M., Hui-Chuan, C.: Development of a mechanism for ontology-
based product lifecycle knowledge integration Department of Accounting and Information 
Systems, National Kaohsiung First University of Science and Technology, Kaohsiung, 
Taiwan, ROC, Institute of Manufacturing Engineering National Cheng Kung University 
Tainan, Taiwan, ROC Department of Special Education, National University of Tainan, 
Tainan ROC  
6. Collado-Ruiz, D., Ostad-Ahmad-Ghorabi, H.: Fuon Theory: Standardizing Functional 
Units for Product Design. Resources, Conservation and Recycling 54, 683–691 (2010) 

724 
H. Ostad-Ahmad-Ghorabi, T. Rahmani, and D. Gerhard 
7. Mecke, K.P.: Ontologiebasierte Repräsentation und Verarbeitung von Information 
bezüglich Unsicherheit in der virtuellen Produktentwicklung. Technische Universitäte 
Darmstadt (November 2011) 
8. Natalya, N., Deborah, M.: Ontology Development 101: A Guide to Creating Your First 
Ontology. Stanford University, Stanford 
9. Sprenger, A., Haydn, M., Ondoua, S., Mosch, L., Anderl, R.: Ontology-based system for 
supporting the uncertailnly in the product lifecycle. In: Proceedings of TMCE Symposium 
2010, Ancona, Italien, pp. S:1259–S:1274 (April 2010) 
10. Ontolingua website, 
http://www.ksl.stanford.edu/software/ontolingua/ (retrieved May 31, 
2012) 
11. Thel, M.: Wissensstrukturierung und –repräsentation im Produktentwicklungsprozess, 
Technische Universitäte Darmstadt (April 2007) 
12. Protegé website, http://protege.stanford.edu/ (retrieved May 31, 2012)  
13. Jambalaya webpage, 
http://thechiselgroup.org/2004/07/06/jambalaya/ (retrieved May 31, 
2012) 
14. Miller, L., Seaborne, A., Reggiori, A.: Three Implementations of SquishQL, a Simple RDF 
Query Language. In: Horrocks, I., Hendler, J. (eds.) ISWC 2002. LNCS, vol. 2342, pp. 
423–435. Springer, Heidelberg (2002) 
15. Schmidt, M. Foundations of SPARQL Query Optimization, Albert-Ludwigs-Universität 
Freiburg (December 2009) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 725–734. 
DOI: 10.1007/978-3-642-30817-8_71 
© Springer-Verlag Berlin Heidelberg 2013 
 
Leveraging Product Development  
for a Sustainable Future: Energy and Resource Efficiency 
in Lifecycle Analysis  
Martin Eigner, Patrick D. Schäfer, and Hristo Apostolov 
Institute for Virtual Product Engineering (VPE), University of Kaiserslautern, 
Gottlieb-Daimler Straße 44, 67663 Kaiserslautern, Germany 
{eigner,schaefer,apostolov}@mv.uni-kl.de 
Abstract. Life Cycle Assessment (LCA) and eco-efficiency analysis are power-
ful tools to support the lifecycle engineering and to become a part of Product 
Lifecycle Management (PLM). This paper presents an approach of the imple-
mentation of LCA tools as fully integrated within the PLM supporting the  
design engineer in terms of sharing relevant product data across modules and li-
fecycle phases, thereby reducing the overall amount of data management related 
work performed by engineers and increasing data availability. In a case study 
the capabilities of an existing LCA solution are examined and presented at the 
example of a wheeled excavator. 
Keywords: Sustainable Development, Life Cycle Thinking, Product Lifecycle 
Management, Life Cycle Assessment, Energy and Resource Efficiency. 
1 
Introduction 
Sustainable Development, conceptually founded on the three dimensions economical, 
ecological and social sustainability is the main paradigm for the future improvement 
of humankind in the 21st century. One of the guiding principles for engineering design 
is to develop products that conform to the sustainability paradigm. Already in the 
early phases of the product lifecycle, the engineer defines key properties of a product, 
implicitly including the definition of the resulting lifecycle costs as well as environ-
mental and social effects. The process of creating more a clearly sustainable product 
needs to be monitored and managed, whether a new product is to be designed or an 
existing one is to be improved. To address an engineering design process and the 
product development, appropriate methods and tools are required. [1] 
This paper considers Product Lifecycle Management (PLM) [2] as one key concept 
for the establishment of sustainable engineering design processes. 
2 
Sustainable Development 
The awareness of limited resources availability, environmental problems and  
pollution, the increasing demand for goods, energy and materials from the already 

726 
M. Eigner, P.D. Schäfer, and H. Apostolov 
developed and the new developing countries, as well as the increase of costs of scarce 
resources, all are calling for a new paradigm of life, overcoming the obsolete consu-
merist model of modern societies [3]. Sustainability became even more than a buzz-
word these days. The visionary paradigm of Sustainable Development was introduced 
in 1987 by the World Commission on Environment and Development of the United 
Nations in a very basic term within the Brundtland Report. Five years later the vision 
of Sustainable Development was globally accepted at the United Nations Conference 
for Environment and Development. Once again ten years later the United Nations 
World Summit on Sustainable Development stipulates Lifecycle Thinking as one self-
contained principle within the sustainability paradigm. [4] 
However, in current literature, sustainability denotes a development which meets 
the needs of the present generation without compromising the ability of future genera-
tions to meet their own needs [5]. It identifies three dimensions – economic, environ-
mental and social issues – which, in regard to the characteristic of the model of  
sustainability, are considered equivalent and standing side by side, or in other words, 
harmony has to be brought between them [3].   
 
Fig. 1. Monitoring of Sustainability performance within the Integrated Sustainability Triangle 
(adapted from [4], [6], [7]) 
The Integrated Sustainability Triangle (IST) (see Figure 1) is a promising new in-
strument for systemizing the sustainability performance of a company, currently used 
for monitoring the product development process regarding sustainability management 
[8]. In the case of products total lifecycle approach is necessary for a sustainable 
management. 

 
Leveraging Product Development for a Sustainable Future 
727 
2.1 
Lifecycle Thinking 
The economic system as well as science and politics are claimed to enable a sustaina-
ble change. For that a tremendous improvement of current technologies is also  
required. In such a setting, sustainable manufacturing will become one of the most 
relevant topics in future engineering. Within this rethinking, the product concept with 
all participating and involved processes has to be reshaped, especially taking into 
account the lifecycle view [9]. This rethinking causes new requirements related to 
sustainability assessment of a product and all lifecycle phases, ranging from raw ma-
terial extraction, across production, to use and recycling or waste disposal. This  
ecological view of the product lifecycle refers to the material flow. To organize the 
product development, a consideration of the information flow is of central meaning. 
Already in the early lifecycle phases, the engineer defines key properties of the prod-
uct. Here up to 80% of the resulting lifecycle costs as well as about the same percen-
tage of environmental effects are defined [10-11]. Assuming a similar rate of social 
effects a sustainable improvement of the product development process is appointed in 
this early step.   
2.2 
Product Development Process 
Enterprises today have to deal with new arising challenges such as globalization. As a 
result they have to collaborate more directly with others like their suppliers and cus-
tomers. Furthermore the rising demand for product innovation, product reliability and 
product liability have caused new challenges in the product management and process 
management. The information about a product throughout its complete lifecycle - 
from the early phases up to the recycling or disposal - is often distributed across a 
global network of data handling systems supporting different lifecycle phases and 
different engineering disciplines [12].  
 
Fig. 2. Sustainability influencing the Product Development Process  

728 
M. Eigner, P.D. Schäfer, and H. Apostolov 
A holistic approach, almost in the sense of a "product biography", has to begin in a 
phase when the physical product does not yet exist and end when it can no longer 
fulfill its original purpose. The criticism of the classical - partly economically, partly 
ecologically oriented - product lifecycle models stipulates for rethinking in terms of 
an integrated approach. The sustainable economical activities of a company should be 
understood as an ongoing search and learning process with the objective to conquer 
new markets through sustainable products. The sustainable product itself should be 
seen as environmentally friendly, economically viable and socially acceptable (see 
Figure 2). Therefore an entire view of the product is necessary. Central here is the 
potentially sustainable, intellectual product, which is accompanied transparently from 
the initial idea, through all lifecycle phases, to the recycling and reuse. Furthermore a 
consideration of the three dimensions – economy, ecology and social – is required. 
Bearing in mind that a lifecycle product model is a multidimensional network of 
requirements, functions, behaviors, development, test and production structures across 
all disciplines involved with the product, across all internal and external development 
and manufacturing sites, all resources used for manufacturing and assembly of the 
product, as well as across all relevant documentation, the existing manual and  
separate handling techniques fail. 
3 
Product Lifecycle Management 
In order to manage an engineering design process and handle the product complexity 
the engineer has to administrate all kinds of data and information about the product. 
This would not be possible without using modern IT tools. The use of IT tools for 
support of the integration and federation of distributed product data and their related 
processes, as well as the administration of high number of product variants have al-
ready been recognized as a driving factors for success, survival and competitiveness 
[12]. Product Lifecycle Management is the administrative and managing backbone for 
all these tasks in product engineering [2] [13].  
3.1 
Product Data Management 
One benefit of computer aided engineering design is that it facilitates the development 
of sustainable products by a better availability of relevant information. Even as early 
as today, a computer aided lifecycle assessment with an automatic calculation and 
monitoring of energy and material flows caused by the processes of the product  
and the linkage with conventional lifecycle databases could be customized. Therefore, 
the product structure - available in a Product Data Management (PDM) system - has 
to be extended with lifecycle processes like production, transport, use and end-of-life 
processes [1]. Product Data Management systems offer first answers to these  
problems by linkage to tools for Life Cycle Assessment (LCA).   
 

 
Leveraging Product Development for a Sustainable Future 
729 
3.2 
Life Cycle Assessment 
Product related Life Cycle Assessment (LCA) with an emphasis on energy, resource 
and waste was started back in the 1970s, the time of the famous report ´The Limits to 
Growth` addressed to the Club of Rome and the first worldwide oil crisis, which 
brought awareness of the finiteness of oil and revealed the vulnerability of the global 
economic system. In the 1990s the LCA was developed as a method, mainly driven 
by the Society of Environmental Toxicology and Chemistry (SETAC), and standar-
dized by the International Organization for Standardization (ISO). Later on, in 2006 
the international standards have been slightly revised to their current version of ISO 
14040 and 14044 [14]. The basic principles of any Life Cycle Assessment are the 
“cradle to grave” analysis and the use of functional unit. All mass and energy flows, 
resource and land use, and even the potential impacts and probable interventions are 
set in relation to the functional unit as quantitative measures of the benefit. A main 
characteristic of the current ISO standard is the clear structure which consists in the 
four components: “Goal and Scope Definition”, “Inventory analysis”, “Impact Analy-
sis” and “Interpretation” [15]. The LCA is an essential comparative method to  
estimate the environmental aspects of a product system. It explores environmental 
aspects and potential environmental impacts across the life cycle, from raw material 
extraction, across production, to reuse of recyclates and waste disposal. A Life Cycle 
Assessment can assist engineers and decision-makers in the industry by identifying 
opportunities to improve the environmental performance of product systems and pro-
viding them with a platform for observing the environmental product declarations and 
compliance, thus support the design of more eco-efficient products [16-17]. 
3.3 
Eco-efficiency Analysis 
A first step towards an integrated sustainability assessment provides the link between 
economic and ecological issues and is represented in the new international standard 
ISO 14045 - Eco-efficiency assessment of product systems: principles, requirements 
and guidelines, adopted in 2012 [18]. 
The principles of eco-efficiency are combined with the lifecycle thinking and 
translated into certain goals [2]: 
 
- 
Minimize energy intensity 
- 
Minimize the material intensity of goods and services  
- 
Maximize the use of renewable resources 
- 
Minimize toxic dispersion 
- 
Extend product durability 
- 
Increase product efficiency  
- 
Promote recycling 
 
 
Fig. 3. Goals of eco-efficiency analysis 

730 
M. Eigner, P.D. Schäfer, and H. Apostolov 
The objective of the eco-efficiency analysis is to support the evaluation of different 
optimization solutions by providing an overall life cycle view. The concept shown in 
Figure 4 refers to such kind of eco-efficiency analysis and is linked to PLM [19-20]. 
It extends the product model as well as the process model by technical-economical 
attention of ecological parameters, thus it allows an aggregated evaluation of energy 
and resource efficiency of the product over the product lifecycle. Energetically im-
proved technical products will assert themselves in enterprise practice only if they are 
advantageous in economic regard. Hence, the deliberate design of eco-efficiency 
products is as important as both economic balance and life cycle assessment. The 
concept deals with both and is outlined in a way, that it corresponds to a multidimen-
sional, interdisciplinary and federated lifecycle management. 
 
 
Fig. 4. Concept of an eco-efficiency analysis as part of PLM (adapted from [19]) 
The main topic of the research project underlying the following case study is to  
develop energy-efficient concepts and technologies for mobile working machines. 
Major focus is put on the powertrain, energy management, friction management and 
lifecycle management. [20] 
4 
Case Study 
In the sector of construction equipment the importance of LCA and eco-efficiency 
analysis is very high. Similar to passenger cars sector, where the political and technic-
al requirements are already very high, the sector of construction equipment is current-
ly in change. The scarcity of fossil resources and raw materials, as well as the rising 
energy costs in the last years have brought industry to a massive rethinking. The re-
duction of energy consumption and the successive use of renewable energy are one of 

 
Leveraging Product Development for a Sustainable Future 
731 
the most important innovation topics in this industry branch. In order to fulfill the 
high energy requirements in the near future, extensive concepts, new structures and 
innovative technical approaches for increasing the total energy efficiency of the ma-
chines are needed. Looking at the lifecycle of a wheeled excavator for example, LCA 
helps to identify environmental key factors and cost drivers within the use phase 
where CO2 emissions (resulting from fuel consumption) are still very high [20]. Fur-
thermore the approach helps to estimate lifecycle based efficiency for future concepts 
of the excavator. For this purpose, an overall numerical full simulation model of the 
machine is designed. The developed simulation tool is used to calculate the energy 
consumption of the machine with respect to specific user profiles. Based on this, it 
will become possible to analyze and optimize the current wheeled excavator focusing 
on new and innovative mechanical, hydraulic and electric/electronic components and 
systems (see Figure 5). Product Lifecycle Management guides the whole analysis and 
optimization process.  
 
Fig. 5. Interdisciplinary Optimization supported by Lifecycle based Eco-Efficiency Analysis  
Aim of the analysis made with the PLM solution is to measure multiple dimensions 
of the product performance in two of the three sustainability areas – ecological and 
economical (see Figure 6). The modules share common, scalable infrastructure and 
support multiple methods for supplier data acquisition. Bi-directional integrations 
with multiple enterprise systems such as authoring systems and PDM is also sup-
ported [21], it is however not functioning to the desired extend. 

732 
M. Eigner, P.D. Schäfer, and H. Apostolov 
Product Analytics: The Product Analytics module uses streamlined Life Cycle As-
sessment approach for qualifying the environmental impact over the life of a product. 
This means that relies primarily on secondary (existing) life cycle impact data and, 
but not necessarily, third party data such as the “ecoinvent” eco-balance data base, to 
produce quick, relatively robust information. It has the capabilities to model and  
analyze embodied environmental impacts throughout the lifecycle utilizing supplier 
material disclosures for that, it can identify highest impact areas among products, 
parts, material and suppliers. It provides relevant eco-efficiency data in report dash-
boards, which are of help for design engineers in the evaluation of improvement  
opportunities. 
Environmental Compliance: The Environmental Compliance module is used to ana-
lyze and report compliance of company products to multiple standardized regulations 
and requirements such as RoHS, JIG, REACH using supplier material declarations as 
data source. It helps to measure and manage compliance risk early in the product  
development phase. 
Lifecycle Costing: The Cost Module enables users to easily estimate and display cost 
information for company or supplier parts or products based on different estimation 
models and maintain them as dynamical database. It supports cost breakdowns and 
cost confidence levels and gives an overview of the cost history of a part or product. 
 
 
Fig. 6. Boom of a wheeled excavator – Extended Product Structure in a PLM solution  
5 
Conclusion 
Product Lifecycle Management, as the overall engineering concept is based on the 
idea of connecting knowledge and seeks to provide the right information at the right 

 
Leveraging Product Development for a Sustainable Future 
733 
time in the right context and extent. It offers a solution to systematization of the vari-
ous operational tasks in design and production so that processes are rationalized and 
optimized for a Smart Product Engineering. Internationally standardized tools for the 
assessment of environmental-economical sustainability of a product do already exist – 
Life Cycle Assessment and eco-efficiency analysis. They are powerful tools for envi-
ronmental management whose use, together with the incorporation of lifecycle think-
ing in the company policy, is a necessary requirement for successful lifecycle  
engineering. With the increasing importance of Design for Environment, due to the 
scarcity of resources and stricter requirements on the products, a close collaboration 
between design and environmental engineers is needed. Therefore it makes complete 
sense LCA tools to be fully integrated in the PLM solution, which means that relevant 
product data should be shared across all modules of the PLM and authoring systems 
infrastructure and limitation should come only from user related permission restric-
tions. With the current tools for Life Cycle Assessment very complex products with 
huge variety of different material and complex value chain can be assessed with a 
reasonable work demand providing satisfactory results and presenting them in neat 
dashboards. However an improvement potential can be seen in the area of data shar-
ing between modules of the PLM and authoring systems in order to achieve the state, 
in which no work for entering data defining a particular product or part property 
needs to be repeated at different nodes of the infrastructure. 
 
Acknowledgments. The research project ERMA is funded by the Foundation for 
Innovation of the German state Rhineland-Palatinate. We extend our sincere thanks to 
all partners at the product design group at the Center of Commercial Vehicle Tech-
nology at the University of Kaiserslautern who contribute in preparing the project. 
References 
1. Eigner, M., von Hauff, M., Schaefer, P.: Sustainable Product Lifecycle Management - A 
Lifecycle based Conception of Monitoring a Sustainable Product Development. In: Hes-
selbach, J., Herrmann, C. (eds.) Glocalized Solutions for Sustainability in Manufacturing, 
pp. 501–506. Springer, Heidelberg (2011) 
2. Eigner, M., Stelzer, R.: Product Lifecycle Management - Ein Leitfaden für Product Devel-
opment und Life Cycle Management, 2nd edn. Springer, Heidelberg (2009) 
3. Blank, J.: Sustainable Development. In: Schulz, W., Burschel, C., Weigert, M. (eds.) Lex-
ikon Nachhaltiges Wirtschaften, pp. 374–385. Oldenbourg, München (2001) 
4. von Hauff, M., Kleine, A.: Sustainability-Driven Implementation of Corporate Social Re-
sponsibility – Application of the Integrative Sustainability Triangle. Springer, Heidelberg 
(2009) 
5. Hauff, V. (ed.): Unsere gemeinsame Zukunft - der Brundtland Bericht der Weltkommis-
sion für Umwelt und Entwicklung. Eggenkamp, Greven (1987) 
6. Kleine, A.: Operationalisierung einer Nachhaltigkeitsstrategie - Oekologie, Oekonomie 
und Soziales integrieren. Gabler, Wiesbaden (2009) 
7. von Hauff, M., Kleine, A.: Nachhaltige Entwicklung – Grundlagen und Umsetzung. Ol-
denbourg, München (2009) 

734 
M. Eigner, P.D. Schäfer, and H. Apostolov 
8. von Hauff, M., Wilderer, P.: Industrial Ecology: engineered representation of sustainabili-
ty. Sustainability Science 3(1), 103–115 (2008) 
9. Ciceri, N.D., Garetti, M., Terzi, S.: Product Lifecycle Management Approach for Sustai-
nability. In: Proceedings of the 19th CIRP Design Conference, pp. 147–154. Cranfield 
University (2009) 
10. Posch, A., Perl, E.: Regionale Verwertungsnetze und industrielle Symbiose. In: Isenmann, 
R., von Hauff, M. (eds.) Industrial Ecology - Mit Oekologie Zukunftsorientiert Wirtschaf-
ten, pp. 265–276. Elsevier, München (2007) 
11. Eigner, M., Schaefer, P.: Nachhaltig mit geringem Aufwand. Digital Engineering Maga-
zin 15(4), 56–58 (2012) 
12. Sendler, U.: Das PLM Kompendium – Referenzbuch des Produkt Lebenszyklus Manage-
ments. Springer, Heidelberg (2009) 
13. Stark, J.: Product Lifecycle Management – 21st Century Paradigm for Product Realisation. 
Springer, London (2011) 
14. Kloepffer, W., Grahl, B.: Oekobilanz (LCA) – Ein Leitfaden für Ausbildung und Beruf. 
Wiley-Vch, Weinheim (2009) 
15. ISO 14040: Environmental management - Life cycle assessment - Principles and frame-
work. Beuth, Berlin (2006) 
16. Kloepffer, W.: Life Cycle Sustainability of Products. International Journal of Life Cycle 
Assessment 13(2), 89–95 (2008) 
17. Kloepffer, W.: Life Cycle based methods for sustainable product development. Interna-
tional Journal of Life Cycle Assessment 8(3), 157–159 (2003) 
18. ISO 14045: Environmental management - Eco-efficiency assessment of product systems - 
Principles, requirements and guidelines. Beuth, Berlin (2012) 
19. Feikert, S.: Oekologisches Product Lifecycle Management - Ein Integrationskonzept der 
oekologischen Produktbilanzierung in betrieblichen ERP-Systeme. Shaker, Aachen (2007) 
20. Eigner, M., Mueller, S., et al.: Lifecycle based Evaluation of new Energy- and Resource-
Efficient Concepts for Mobile Working Machines. In: Berns, K., Schindler, et al. (eds.) 
Proceedings of the 2nd Commercial Vehicle Technology Symposium, pp. 319–328. Shak-
er, Aachen (2012) 
21. Vajna, S., Weber, C., Bley, H., Zeman, K.: CAx für Ingenieure, 2nd edn. Springer, Hei-
delberg (2009) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 735–744. 
DOI: 10.1007/978-3-642-30817-8_72 
© Springer-Verlag Berlin Heidelberg 2013 
 
Exploring Opportunities to Improve Life Cycle 
Environmental Performance of a Complex Product  
Minjung Kwak and Harrison Kim* 
Department of Industrial and Enterprise Systems Engineering,  
University of Illinois at Urbana-Champaign, 104 S Mathews Ave., Urbana, IL 61801, USA 
{kwak14,hmkim}@illinois.edu 
Abstract. Life cycle assessment (LCA) is an essential tool for achieving design 
for life cycle. An LCA examines all stages of a product’s life cycle and gives a 
quantitative assessment of its potential environmental impact. The results of the 
LCA help identify priority areas for improvement and ways to reduce environ-
mental impacts. This paper presents a comprehensive LCA approach to quantit-
ative assessment of environmental impact for industrial off-road equipment. 
This paper describes how LCA can be applied to off-road equipment and how it 
can be used to improve the environmental performance of a product. Six poten-
tial opportunities for reducing the environmental impact of off-road equipment 
are explored, and sensitivity analyses are presented as a tool for evaluating the 
effectiveness of those solutions. The LCA approach is demonstrated with the 
example of a construction machine. 
Keywords: Product life cycle, life cycle assessment (LCA), design for life 
cycle, design for environment. 
1 
Introduction 
Life cycle stages of a product – design, manufacturing, usage, and end-of-life treat-
ment – are closely linked from the environmental perspective. The link is particularly 
important when the environmental impact from the usage phase is dominant, which is 
shown in products such as automotive vehicles and heavy-duty off-road equipment. 
This type of products – products with major environmental impact during usage phase 
– must be designed carefully so that the entire life cycle of a product should be more 
sustainable with less adverse environmental impact.  
Life cycle assessment (LCA) is an essential tool for achieving design for life cycle. 
LCA evaluates the potential environmental impact associated with a product, consi-
dering its entire life cycle. An effective LCA can demonstrate how much environmen-
tal impact is caused by a product and how different life cycle phases and/or product 
subsystems contribute to the total impact. With such LCA, a company can identify 
priority areas for improvement and become proactive in its sustainability initiatives. 
                                                           
*  Corresponding author. 

736 
M. Kwak and H. Kim 
This paper presents a comprehensive LCA study of industrial off-road equipment. 
Off-road equipment is a complex product characterized by a large number of consti-
tuent parts and high fuel consumption throughout the long life cycle. Although LCA 
has gained popularity in various industries, only few studies have examined off-road 
equipment due to the complexity of the product. This paper describes how LCA can 
be applied to such a complex machine and how it can help improve the environmental 
performance of the product. More specifically, six potential opportunities for reducing 
the environmental impacts are discussed; four of the opportunities relate to product 
design (i.e., reduced fuel consumption rate, reduced emission rate, improved durabili-
ty, and improved productivity) while the other two relate to usage behavior (i.e., re-
duced idling during operation and increased use of biodiesel). Sensitivity analyses are 
used to evaluate the effectiveness of those solutions, and the results highlight oppor-
tunities with the greatest potential for reducing the environmental impact.  
The remainder of the paper is organized as follows. Section 2 describes the model-
ing for an LCA of off-road equipment. Section 3 presents an illustrative LCA study 
with the example of a construction machine. Section 4 explores the potential oppor-
tunities for reducing the environmental impacts of off-road equipment. Section 5 pro-
vides a summary of the study and concludes the paper.  
2 
Life Cycle Assessment of Off-Road Equipment 
2.1 
Goal and Scope Definition 
This section describes how an LCA can be applied to an off-road machine. An LCA 
considers the entire life cycle of a machine, which consists of three phases: manufac-
turing, usage, and end-of-life treatment. The manufacturing phase includes the extrac-
tion of raw materials and the production of the entire machine. The usage phase  
encompasses the operation and maintenance of the machine, and fuel consumption 
and emissions from machine operation and the replacement of spare parts, filters, oils, 
and fluids are all taken into account. The end-of-life phase involves recycling and 
disposal activities. It incorporates processing of the used machine and all the waste 
from the maintenance (i.e., used spare parts and refills).  
Equation (1) computes the total life-cycle impact of a machine, where Itotal, Imfg, 
Iusage, and Ieol denote the impacts of manufacturing, usage, and end-of-life treatment, 
respectively. In the remainder of the section, it is discussed how to assess the impact 
of each life cycle stage. 
total
mfg
usage
eol
I
I
I
I
=
+
+
 
(1)
2.2 
Modeling the Life Cycle of a Machine 
Manufacturing. The impact of manufacturing is determined by the design of a ma-
chine. More precisely, it is defined by the material composition and manufacturing 
processes of the machine. Transportation is also included. In general, three types of 
transportation occur during the machine manufacturing: (1) the transportation of raw 
materials to parts manufacturers, (2) the transportation between part manufacturers 

 
Exploring Opportunities to Improve Life Cycle Environmental Performance 
737 
and the assembly factory, and (3) the delivery of the finished machine to the customer 
(or dealership). The mass, travel distance, and transportation mode (by truck, train, 
and/or oceanic freight shipping) determine the impact of transportation.  
mfg
matl
mproc
tproc
i
i
p
p
q
q
i I
p P
q Q
I
e
x
e
x
e
x
∈
∈
∈
=
⋅
+
⋅
+
⋅



 
(2)
Equation (2) computes the impact of manufacturing, where 
,
,  and 
matl
mproc
tproc
i
p
q
e
e
e
de-
note the per-unit impacts of material i (
),
i
I
∈
manufacturing process p (
),
p
P
∈
and 
transportation mode q (
),
q
Q
∈
respectively; xi, xp, and xq denote the total number of 
units of material i, manufacturing process p, and transportation mode q, respectively, 
that are used for the manufacturing of the machine.  
Usage. The impact of usage is originated from two sources, i.e., machine operation 
and maintenance. The impact of machine operation can be further divided into the 
impact of diesel fuel consumption and the impact of emissions. The former includes 
the impact from extracting, producing, and delivering fuel. The latter focuses on the 
emissions from diesel fuel combustion. The impact of usage is affected by machine 
lifetime (i.e., the total number of hours the machine is used over its life cycle), and 
how the machine is utilized over the lifetime. 
Impact of Fuel Consumption. Equation (3) formulates the environmental impact of 
fuel consumption, where efuel, FR, and TH denote the per-unit impact of diesel fuel, 
the average fuel consumption rate (in kg/hr), and the machine lifetime, respectively.  
fuel
fuel
I
e
FR TH
=
⋅
⋅
 
(3)
(
)
(
)
(1
)
(
)
idle
idle
nonidle
nonidle
FR
EP
y
BSFC y
y
BSFC y
α
α
=
⋅
⋅
⋅
+
−
⋅
⋅
 
(4)
Assuming that the machine operates in two modes, i.e., idle and nonidle operations, 
the average fuel consumption rate FR is given by Equation (4), where α denotes the 
ratio of idle operation over the machine’s lifetime, and yidle and ynonidle denote the en-
gine load factors. For an operation, the fuel consumption rate is determined by three 
factors: the rated power of the engine (i.e., EP in kW), the engine load factor for the 
specific operation (i.e., yidle or ynonidle), and the engine’s brake-specific fuel consump-
tion rate at the given engine load factor (i.e., BSFC(y) in kg/kWh). The rated power 
EP refers to the maximum power (in kW) that an engine is capable of producing at its 
rated speed. Machines typically operate at a variety of speeds and workloads, depend-
ing on the severity of the application and the operator’s skill. It is rare for a machine 
to operate at full load using the engine’s rated power. To take into account partial load 
operations, Equation (4) uses the concept of engine load factor y, which indicates the 
fraction of the rated power used [1]. The brake-specific fuel consumption rate is an 
engine-specific characteristic that indicates the engine’s fuel efficiency. It represents 
the amount of diesel fuel needed to produce a kilowatt hour of energy. Since it  
varies at different engine load factors, it is represented as a function of engine load 
factor.  

738 
M. Kwak and H. Kim 
Impact of Emissions. Equation (5) formulates the impact of emissions, where 
emission
je
 
and ERj denote the per-kilogram environmental impact and the average emission rate 
(in kg/hr) of emission j, respectively. In this study, six types of emissions are consi-
dered, i.e., carbon dioxide (CO2), sulfur dioxide (SO2), hydrocarbons (HC), nitrogen 
oxides (NOx), carbon monoxide (CO), and particulate matter (PM).   
(
)
emission
emission
j
j
j J
I
e
ER
TH
∈
=
⋅
⋅

 
(5)
(6)
The emission rate ERj is given by Equation (6). Similar to the fuel consumption rate 
in Equation (4), the emission rate of an operation is determined by three factors: the 
engine’s rated power, the engine load factor, and the emission factors of the engine 
for a specific engine load factor (i.e., EF(y)). The emission factors (in kg/kWh) 
represent the amounts of emissions that are emitted per unit of energy produced. They 
are determined by the engine and after-treatment performance. 
The emission factors for HC, NOx, CO, and PM can be obtained from machine 
emission testing. For CO2 and SO2, the emission factors are calculated based on the 
brake-specific fuel consumption rate, as shown in Equations (7) and (8) [2, 3]: 
2
CO ( )
 (
( )
HC) 0.87
(44 /12)
EF
y
BSFC y
=
−
×
×
 
(7)
2
SO ( )
(
( )
(1
)
HC) 0.01
2
EF
y
BSFC y
soxcnv
soxdsl
=
×
−
−
×
×
×
 
(8)
where HC is the hydrocarbon emission factor (in kg/kWh), 0.87 is the average carbon 
fraction of the diesel fuel, 44/12 is the ratio of CO2 mass to carbon mass, soxcnv is the 
fraction of fuel sulfur converted to direct PM (assumed to be 0.3 in this study), 0.01 is 
the conversion factor from weight percent to weight fraction, soxdsl is the weight 
percent of sulfur in the diesel fuel (assumed to be 0.0015 in this study), and the num-
ber 2 is the grams of SO2 formed from a gram of sulfur.  
Impact of Maintenance. Maintenance activities, including parts replacements and oil 
and filter changes, are another factor that affects the environmental impact of the 
machine usage. To capture the impact of maintenance, one should know the number 
of replacements during the life cycle and the impact of each replacement.  
max(0,
)
maint
maint
maint
k
k
k
k
k
k
k K
k K
k
TH
I
RN
e
e
λ
μ
μ
λ
∈
∈


−
=
⋅
⋅
=
⋅
⋅






 
(9)
Equation (9) computes the impact of maintenance, where μk denotes the number of 
units of part k in the machine, RNik denotes the number of replacements of part k over 
the machine lifetime TH, 
maint
ke
denotes the per-unit impact of a replacement part k 
(i.e., the impact of producing, transporting, and replacing a part k), and λk denotes the 
replacement cycle of part k in hours. In Equation (9), the first replacement cycle is 
(
(
)
(1
)
(
)) 
j
idle
j
idle
nonidle
j
nonidle
ER
EP
y
EF
y
y
EF
y
α
α
=
⋅
⋅
⋅
+
−
⋅
⋅

 
Exploring Opportunities to Improve Life Cycle Environmental Performance 
739 
subtracted from TH, since the first part is included in a new machine at the manufac-
turing stage and does not constitute a replacement.  
End-of-Life Treatment. The impact of end-of-life treatment (i.e., recycling, landfill, 
and/or incineration) includes both the impacts of processing the end-of-life machine 
and processing all replacement parts and fluids consumed over the machine lifetime. 
The impact is modeled as Equation (10), where 
and 
eol
eol
prod
k
e
e
denote the per-unit im-
pacts of end-of-life treatment for a machine and for part k, respectively. The per-unit 
impact includes the impact of transporting the end-of-life unit to the treatment facility.  
max(0,
)
eol
eol
eol
k
prod
k
k
k K
k
TH
I
e
e
λ
μ
λ
∈


−
=
+
⋅
⋅





 
(10)
3 
Case Illustration: LCA of a Construction Machine 
This section presents an illustrative LCA study of a typical piece of off-road equip-
ment (Fig. 1). The machine is typically used in construction settings to lift and move 
heavy material around a worksite. In this study, the machine lifetime is assumed to be 
20,000 hours of operation including 30% idle operation. The load factors during the 
idle and nonidle operations were assumed to be 0.1 and 0.5, respectively.  
 
 
Fig. 1. Target machine (picture courtesy of Deere.com) 
For the impact assessment, this study used the LCA software SimaPro (version 
7.3). Most of the life cycle inventory data were taken from the Ecoinvent database. 
Among various impact-assessment methods, Eco-Indicator 99 (H/A) was used [4]. 
3.1 
Data Collection and Model Construction 
Manufacturing. To examine the weight and material composition of the target ma-
chine, this study analyzed the bill-of-materials obtained from the manufacturer. The 
total weight of the target machine is approximately 18,000 kg. Table 1 shows the 
material composition of the target machine collectively by material type.  

740 
M. Kwak and H. Kim 
Table 1. Assumptions on material composition of the target machine 
Material type 
Weight percent (%) 
Steel 
71.49 
Cast iron 
17.59 
Rubber & Plastics 
7.37 
Aluminum 
1.12 
Lubricating oil 
0.89 
Glass 
0.89 
Others 
0.65 
 
In this study, manufacturing processes were taken into account as follows. When 
process data was not directly available from the manufacturer, “General Manufactur-
ing” data from the Ecoinvent database was used. For manufacturing of metal parts 
that accompanies scrap, a scrap rate was defined as 0.227 kg per kilogram of finished 
part. The welding rate was defined to be 0.146 m of welding per kilogram of steel in 
the machine. Finally, it should be noted that transportation for parts manufacturing 
was not included in this analysis due to lack of data.  
Usage. Equation (11) shows the brake-specific fuel consumption rate assumed for the 
target machine; based on engine testing data, it was modeled as an exponentially-
decreasing function of engine load factor. Using the equation, the average fuel  
consumption rate was calculated as 18.52 kg/hr. Table 2 shows the emission rates 
assumed for the target machine. Note that all rates are represented in grams per hour.  
Table 3 summarizes the amount of various parts, fluids, and filters that were as-
sumed to be consumed throughout the machine lifetime. The maintenance activities 
were assumed based on the maintenance schedule recommended by the manufacturer.  
 
(11)
Table 2. Assumptions on emission rates (g/hr) 
Rate 
Idle 
Nonidle 
Total operation 
Carbon dioxide (CO2) 
36372.28 
68824.29 
59088.69 
Sulfur dioxide (SO2) 
0.24 
0.45 
0.39 
Hydrocarbon (HC) 
0.42 
2.08 
1.58 
Nitrogen oxides (NOx) 
28.55 
142.76 
108.50 
Carbon monoxide (CO) 
1.83 
9.13 
6.94 
Particulates (PM) 
0.13 
0.66 
0.50 
Table 3. Assumptions on maintenance 
Maintenance type 
Total amount 
Filters 
311 Filters 
Fluid (coolant and oils) 
2282.9 Liters 
Replacement parts 
12 Tires / 34 Parts 
(
)
( )
850.9757 exp(
/ 0.1692)
215.6509 /1000
BSFC y
y
=
⋅
−
+

 
Exploring Opportunities to Improve Life Cycle Environmental Performance 
741 
End-of-Life Treatment. The machine and all replacement parts and refills follow the 
same end-of-life scenario. It was assumed that 90% of steel and iron is recycled while 
the rest 10% is discarded by landfill. The other materials are assumed to be discarded 
either by landfill (80%) or incineration (20%). For end-of-life processing, the “cut-off 
approach” was used for allocation. In other words, the environmental impacts or ben-
efits from recycling were not allocated to the current life cycle [5, 6]. 
3.2 
Life Cycle Assessment Results 
Assuming the baseline usage scenario (i.e., 30% idle operation; 0.1 and 0.5 engine 
load factors for idle and nonidle operations, respectively; 20,000 hours of machine 
lifetime), Figure 2 and Table 4 provide Eco-indicator 99 scores associated with the 
target machine’s life cycle. The total environmental impact results in 120,600 points. 
The manufacturing phase accounts for approximately 8%, usage approximately 91%, 
and end-of-life phase accounts for less than 1% of the total impact. Table 4 gives 
more detailed results clustered by impact category. Among 11 impact categories, 
fossil fuels, respiratory inorganics, and climate change are three main impact catego-
ries; they together cause over 93% of the total environmental impact.  
End-of-life
0.73%
Emissions
17.68%
Fuel consumption
70.12%
Maintenance
3.25%
Manufacturing
8.22%
 
Fig. 2. LCA results (Method: Eco-indicator 99 H/A) 
Table 4. LCA results: Eco-indicator 99 scores (Method: Eco-indicator 99 H/A) 
Impact category 
Manufacturing
Usage 
End-of-life 
Total 
Carcinogens 
1655.92 
1399.34 
223.55 
3278.80 
Respiratory organics 
2.99 
35.24 
0.39 
38.63 
Respiratory inorganics
3727.53 
19188.88 
219.05 
23135.47 
Climate change 
704.41 
13405.98 
89.63 
14200.02 
Radiation 
18.55 
32.03 
0.43 
51.01 
Ozone layer 
0.30 
8.53 
0.03 
8.86 
Eco-toxicity 
735.28 
458.61 
89.49 
1283.38 
Acidification 
89.76 
1282.94 
14.85 
1387.55 
Land use 
84.14 
993.12 
5.22 
1082.48 
Minerals 
658.13 
155.19 
2.20 
815.52 
Fossil fuels 
2233.91 
72842.64 
235.43 
75311.98 
Total score (Pt) 
9910.93 
109802.50 
880.27 
120593.70 

742 
M. Kwak and H. Kim 
4 
Exploring Opportunities for Reducing Environmental Impact 
4.1 
Potential Opportunities 
This section explores potential opportunities for reducing the environmental impacts 
of the target machine. Assume that there are six potential solutions for reducing the 
environmental impact of the target machine. Four of the solutions relate to product 
design, i.e., reduced fuel consumption rate, reduced emission rates, improved dura-
bility, and improved productivity. The other two solutions relate to usage behavior, 
i.e., reduced idling during operation and increased use of biodiesel. Different solu-
tions have different environmental implications, as indicated below:  
• Reduced fuel consumption rate: A machine that has improved energy efficiency 
requires less fuel to finish the same amount of work. The decrease in fuel con-
sumption can contribute to reducing the impact of usage, more precisely, the im-
pact of machine operation. 
• Reduced emission rates: A machine that has lower emission factors produces less 
gaseous emissions during operation, thereby lowering the impact of usage.  
• Improved durability: A more durable machine requires less frequent maintenance, 
thereby lowering the environmental impact of maintenance. Reduced maintenance 
also provides an additional benefit in the end-of-life stage, i.e., since less parts are 
consumed over the machine’s lifetime, less waste is generated, which, in turn, re-
duces the impact of end-of-life treatment.    
• Improved machine productivity: A machine with better productivity can accom-
plish more amount of production in the same period of time. Accordingly, the total 
impact remains the same, but the impact per unit amount of production (e.g., im-
pact per ton of material) decreases as the machine’s productivity increases.   
• Reduced idling: Reduced idling indicates that more time is spent on non-idling 
operation during an hour. Since non-idling operation consumes more fuel and ge-
nerates more emissions than idling operation, the total impact per hour may  
increase as the idle-time ratio increases. However, an increased amount of produc-
tion can offset the downside, when the impact per unit of production is considered.   
• Use of biodiesel: Using biodiesel, such as B20 (20% biodiesel), is a potential solu-
tion for reducing the impact of machine usage. Biodiesel is known to have both 
pros and cons. With biological recycling of carbon, biodiesel can reduce the emis-
sion of CO2, which is a major greenhouse gas. Biodiesel also reduces the emission 
of SO2; no SO2 is emitted from biodiesel. However, Biodiesel may require more 
energy and resources for fuel production. In addition, its energy content is less than 
that of conventional diesel, so, as the percent of biodiesel increases, more fuel is 
required to accomplish the same amount of work. To assess the effect of biodiesel, 
fuel consumption and emission rates were assumed based on Ref. [7, 8]. Taking in-
to account differences in energy and carbon contents and fuel density, the average 
fuel consumption rate of biodiesel was assumed to be 112.5% of the conventional 
diesel fuel’s average rate. Table 5 shows how the emission rates of HC, NOx, CO, 
and PM would change as biodiesel is blended with conventional diesel fuel.  

 
Exploring Opportunities to Improve Life Cycle Environmental Performance 
743 
Table 5. Percent change in emission rates (%) (calculated based on Ref. [7, 8]) 
B5 
B10 
B20 
B100 
HC 
-5.44 
-10.59 
-20.06 
-67.36 
NOx 
0.49 
0.98 
1.98 
10.29 
CO 
-3.23 
-6.35 
-12.30 
-48.11 
PM 
-3.14 
-6.18 
-11.99 
-47.19 
 
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
90000
100000
110000
120000
130000
140000
150000
EI-99 score [Pt]
Improvement rate
 Fuel consumption rate 
 Emission rate
 Maintenance cycle
 Productivity
 Idling rate
 Biodiesel
(a) Total life-cycle impact
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
80%
85%
90%
95%
100%
105%
110%
115%
120%
125%
130%
EI-99 score [%]
Improvement rate
 Fuel consumption rate
 Emission Rate
 Maintenance cycle
 Productivity
 Idling rate
 Biodiesel
(b) Impact per unit amount of production 
 
Fig. 3. Improvement analysis 
4.2 
Sensitivity Analysis 
LCA can help assess the effect of each solution quantitatively and provides a means 
for prioritizing the solutions. In this study, a sensitivity analysis was conducted for 
each of the six solutions by varying the improvement rate from 0% to 20%. For ex-
ample, the lifecycle impact of the target machine was calculated when the fuel con-
sumption rates were assumed to be reduced by 0% to 20%. The total impact for zero 
percent improvement is the same as shown in Section 3.2. For biodiesel, a 20% im-
provement rate indicates that biodiesel B20 was used.  
Figure 3 illustrates the results of the sensitivity analyses. When the total impact is 
considered, reducing the rate of fuel consumption is shown as the most effective solu-
tion. If the impact per unit amount of production is considered, improving the ma-
chine’s productivity is identified as the most effective solution. Using biodiesel shows 
an interesting result; the increasing trend of the EI-99 score implies that using biodie-
sel may not be environmentally sustainable, even though it can reduce a significant 
amount of greenhouse gas emissions.  
5 
Conclusion 
This paper presented an LCA study for a typical piece of off-road equipment. Due to 
the maintenance and machine operation, the usage phase accounts for most of the 
total environmental impact, over 90%. Specifically, 70% of the total environmental 
impact comes from fuel consumption at the usage stage. This implies that improving 

744 
M. Kwak and H. Kim 
the usage stage should have a higher priority than any other consideration when at-
tempting to improve the environmental performance of off-road equipment. 
LCA can help improve the environmental performance of the product. In this 
study, LCAs helped examine six potential opportunities for reducing the environmen-
tal impacts (i.e., reduced fuel consumption rate, reduced emission rate, improved 
durability, improved productivity, reduced idling during operation, and increased use 
of biodiesel); sensitivity analyses were conducted to evaluate the effectiveness of 
those solutions from the environmental perspective. The results highlight reducing 
fuel consumption rate and improving machine productivity as the two opportunities 
with the greatest potential for reducing the environmental impact.  
Acknowledgments. This material is based upon the work supported by Deere and 
Company and the National Science Foundation under Award No. 0953021. Any opi-
nions, ﬁndings, and conclusions or recommendations expressed in this publication are 
those of the authors and do not necessarily reﬂect the views of Deere and Company 
and the National Science Foundation. 
References 
1. Assessment and Standards Division: Median Life, Annual Activity, and Load Factor Values 
for Nonroad Engine Emissions Modeling. EPA-420-R-10-016, Office of Transportation and 
Air Quality, U.S. Environmental Protection Agency (2010) 
2. Sihabuddin, S.S., Ariaratnam, S.T.: Methodology for Estimating Emissions in Underground 
Utility Construction Operations. Journal of Engineering, Design and Technology 7, 37–64 
(2009) 
3. Assessment and Standards Division: Exhaust and Crankcase Emission Factors for Nonroad 
Engine Modeling - Compression-Ignition. EPA-420-R-10-018, Office of Transportation and 
Air Quality, U.S. Environmental Protection Agency (2010)  
4. Goedkoop, M., Spriensma, S.: The Eco-Indicator 99: A Damage Oriented Method for Life 
Cycle Impact Assessment. PRe Consultants B.V., Amersfoort, The Netherlands (2000) 
5. Vogtländer, J.G., Brezet, H.C., Hendriks, C.F.: Allocation in Recycling Systems: An Inte-
grated Model for the Analyses of Environmental Impact and Market Value. International 
Journal of Life Cycle Assessment 6, 344–355 (2001) 
6. Frischknecht, R.: LCI Modelling Approaches Applied on Recycling of Materials in View of 
Environmental Sustainability, Risk Perception and Eco-Efficiency. International Journal of 
Life Cycle Assessment 15, 666–671 (2010) 
7. Assessment and Standards Division: A Comprehensive Analysis of Biodiesel Impacts on 
Exhaust Emissions. EPA420-P-02-001, Office of Transportation and Air Quality, U.S. En-
vironmental Protection Agency (2002) 
8. Sheehan, J., Camobreco, V., Duffield, J., Graboski, M., Shapouri, H.: Life Cycle Inventory 
of Biodiesel and Petroleum Diesel for Use in an Urban Bus. NREL/SR-580-24089, U.S. 
Department of Agriculture, U.S. Department of Energy (1998) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 745–754. 
DOI: 10.1007/978-3-642-30817-8_73 
© Springer-Verlag Berlin Heidelberg 2013 
 
Cost-Effects of Product Modularity – An Approach to 
Describe Manufacturing Costs as a Function of 
Modularity  
Thomas Hohnen, Judith Pollmanns, and Jörg Feldhusen 
RWTH Aachen University, Chair and Institute for Engineering Design, Aachen, Germany 
hohnen@ikt.rwth-aachen.de 
Abstract. In recent years modularization has been used to solve the trade-off 
between the diversity of variety and the effort to optimize product complexity 
or product standardization. In the context of the implementation of an optimal 
product strategy and the conduction or extension of a modularization concept, it 
is required to evaluate the costs to support engineering decisions. The outcome 
of this paper is an approach to support modularization decisions of an engineer. 
With it, a structured procedure is introduced to identify factors which influence 
the costs and which depend on the product modularity. 
Keywords: modularity, modularization, cost, development cost, product. 
1 
Introduction 
Modularization of products is a generally accepted method for reducing the total ex-
penditure of a company (e. g. [1], [2], [3], [4]). Although the financial success of a 
company is affected by product modularity [5], a higher modularity is not necessarily 
more cost-effective than a lower modularity [6]. 
In the context of product development, two pieces of information are needed to 
achieve a cost-optimum product modularity. On the one hand, a method is required to 
analyze and quantify product modularity. On the other hand, it is necessary to know 
about the relation between modularity and product costs. This relation depends on 
many, partly company-specific factors (cf. [5]). Hence, the knowledge of these factors 
and the quantitative specification is crucial to determine modularity concerning its 
cost-optimum [6]. 
In general, the economies of a higher modularity are mainly caused by economies 
of scale and scope (cf. [7], [8]). On closer inspection, it becomes apparent, that this 
simplified view is insufficient. For example, a division of one part into two identical 
parts could double the number of parts, this separation causes additional assembly and 
test steps. The cost reduction of the single part by an increasing lot size is accompa-
nied by additional assembly and test costs. So it is possible, that the overall costs in-
crease due to modularization. Hence, the relation of product modularity and product 
costs should be analyzed more detailed to identify the cost-optimum. As the direct 

746 
T. Hohnen, J. Pollmanns, and J. Feldhusen 
relation between cost and modularity is not concretely describable yet, an approach is 
presented in this paper.  
Before the approach is presented in section 3, in section 2 the general relation of 
cost and modularity is introduced. Afterwards in sections 4 and 5 several qualitative 
relations are displayed based on a literature research. Here, this paper focuses on the 
relation of modularity and manufacturing costs.  
2 
State of Art 
2.1 
Terms and Definitions 
The terms modul, modularization and modularity are well known in industry and 
science [4]. Despite of that, those terms are not exactly defined in literature [9], so 
that there can be found different interpretations [4, 10]. 
VDI-guideline 2221 demands structuring the general solution into modules. Here, 
the term modul describes, in contrast to the function structure of a product, the classi-
fication of the solution in real sub-systems, system elements and their interfaces. [11] 
This article therefore defines the term modul corresponding to Baumgart [12] as a 
functionally and physically describable entity, which is almost independent from oth-
er product modules. The transformational structure between the functional structure of 
a product and its physical building structure is called, according to Pahl [1], product 
architecture. The gradual character of product architecture in terms of advantageous 
structuring and thereby a possible description of product architecture is according to 
Pahl modularity [1].  
A product is called ideal-modular at the point of its highest modularity. That means 
(among other things) that it is possible to assign function and components on a  
one-to-one basis. If a product is ideal-integral, then all functions are met by one  
component. [13] 
Modularization aims at product structuring by increasing product modularity. So, 
existing product architecture is optimized in regard to product requirements or to 
achieve economy of scale in products' formation phases. (acc. to [1] and [12]) 
According to Gershenson [10] there are no existing standardized measurement me-
thods for quantifying product modularity. Literature presents different methods; see 
for example approaches of Ericsson et al. [4], Sosa et al. [14], Stryker [15] or Hohnen 
et al. [13]. With the help of these methods, different aspects of product architecture 
are rated. It is for example possible to quantify dependency and cross-linking of func-
tions and corresponding construction components with just one operating figure 
called functional modularity index (FMI) (cf. [13]). 
2.2 
Cost Functions of Product Modularity 
A study shows that the financial success of a company depends on product modularity 
[5]. However, the effects of modularity like increasing complexity or costs influence 
are still unknown, [16], [17]. A general description of the relation of product  

 
Cost-Effects of Product Modularity 
747 
modularity and product costs could neither be identified in a detailed literature re-
search. Hence, there is no system to support an engineer in optimizing modularity 
regarding costs. 
Literature just gives relative statements about how a higher product modularity can 
influence product costs. Within these literature, the possible economical savings of a 
higher modularity are on the one hand justified by the achievable scale effects and 
learning curve effects as a result of reuse (e. g. [7], [8] or [18]). On the other hand, 
literature also refers to increased costs due to a higher modularity, which can arise for 
example by specification and design of modules and their interfaces (e. g. [7], [19]). 
A qualitative figure of a modularity function could be abstracted from these state-
ments, cf. Fig. 1. 
 
Fig. 1. Qualitative modularity influence on overall costs (cf.[6]) 
The knowledge of the financial negative and positive effects are necessary in order 
identifying the cost optimum regarding modularity. 
It is assumed that the curve progression is highly effected by company specific fac-
tors like the product itself, manufacturing resources or overhead costs. Consequently, 
it is crucial to identify and consider these specific factors by developing a method to 
describe the functional relationship of costs and modularity.  
3 
Introduction of Approach  
The superior objective of modularization is cost optimization. A determination of the 
absolute product costs is therefore not necessary in the context of modularity. Hence, 
product costs which are independent of product modularity are neglected in this  
approach. 
The main aspect of this approach is to determine the relationship of modularity and 
cost indirectly. Several factors are rather analyzed to determine modularity costs. In 
the following these factors will be called properties. In this approach it is required that 
on the one hand the properties’ values depend on modularity. On the other hand the 
properties’ values influence the product costs. Fig. 2 shows the basic steps of this 
approach. 
Overall 
cost
Optimum
100%
0%
integral
modular
financial negative effect
financial positive effect
Modularity

748 
T. Hohnen, J. Pollmanns, and J. Feldhusen 
As mentioned before, the properties must be identified in the first step. Afterwards 
the cost function f cost (property i) and modularity function f modularity (property i) of each 
property must be determined in steps two and three. Here the interactions of the prop-
erties are neglected. A change of the properties’ value influences different divisions 
like development or manufacturing. Because of that, a separate consideration of these 
divisions is suggested. Combining the functions (2) and (3) using nomograms, the 
direct relation of costs and modularity is derived in step 4. Last the modularity costs 
of each property can be accumulated. 
 
 
Fig. 2. Basic method to determine the modularity costs 
cost i
property i
property i
integral
modular
modularity
property i
property i
cost i = f 1,i (property i)
cost i = f 1,i x f 2,i (modularity)
property i = f 2,i (modulariy)
1)
2)
3)
Identifiation of properties with influence on modularity
5)
Accumulation of modularity costs of properties
4)
Determination of modularity costs of properties
Determine the property as 
a function of modularity
Determine the cost as 
a function of the property
cost i
property i
integral
modular
modularity

 
Cost-Effects of Product Modularity 
749 
4 
Description of Manufacturing Costs as a Function of 
Modularity  
In this chapter the introduced approach is implemented using literature data. Addi-
tionally, hypotheses were added to expand this showcase. 
4.1 
Identification of Properties with Influence on Modularity (Step 1) and 
Determination of Properties as a Function of Modularity (Step 2) 
As outlined above, properties which depend on modularity and influence the products 
costs are identified. Following properties were identified based on an in-depth litera-
ture research and interviews with experts. Following properties show a context to the 
costs of manufacturing. 
Number of Units of a Component 
The number of units of an existing component is generally independent of a changing 
modularity. But additional modules can be defined by an increasing modularity. This 
functional and physical separation will increase the chance of reuse of these new 
modules in other products or orders. At the best, the batch size of these components 
will rise, too. In literature the relation of modularity and the number of units has not 
been analyzed in detail yet. So the authors assume a linear progression between  
modularity and the number of units, cf. Fig. 3.1. Based on the same reason, all other 
progressions are assumed as linear, too. 
 
Fig. 3. Number of units of a component as a function of modularity 
Size of a Component 
The size of a component basically depends on the function, which has to be fulfilled, 
and the material. The more functions should be met by a component, the higher the 
requirements to the material are. This usually means, that more material is needed. If 
you follow the above mentioned definition (cf. section 2), then the modularity of a 
modularity
0%
100%
number of units of a 
component
1)
2)
3)
4)
5)
modularity
0%
100%
size of  a 
component
modularity
0%
100%
number of working 
surfaces of a component
modularity
0%
100%
number of components
of a product
modularity
0%
100%
number of components
of a company

750 
T. Hohnen, J. Pollmanns, and J. Feldhusen 
component is getting lower, the more functions it should meet. This leads to the  
conclusion that with an increasing modularity, the size of a component decreases, cf. 
Fig. 3.2.  
Number of Working Surfaces of a Component 
Each component has got several surfaces. Some of these surfaces are so called  
working surfaces, which are needed to execute a function. The more functions a com-
ponent fulfills, the more working surfaces are needed. As defined, an ideal modul 
executes a single function, cf. [13]. The higher the modularity, the less the number of 
working surfaces of a component is, cf. Fig. 3.3. 
Number of Components of a Product 
It is assumed that the number of functions, which have to be fulfilled by a product, is 
independent from modularity. The above mentioned definition shows that with a ris-
ing modularity more modules are necessary to break up the relation of functions and 
components. So, the number of a product's components rises theoretically with an 
increasing modularity, cf. Fig. 3.4. This fact could be proved by comparison of simi-
lar products with a different modularity. For example the number of components of a 
modular desktop phone is higher than an integral cell phone (cf. [20]). Obviously this 
comparison is simplified, because different requirements cause a different number of 
product functions. Based on the mentioned definition, the modularity of these prod-
ucts is different. 
Number of Components of a Company 
While the number of a product's components rises according to an increasing mod-
ularity, the possibility to reuse the newly defined modules (see property ‘number of 
units of a component’) rises as well. As a consequence the number of a company’s 
components decreases because it is no longer necessary to develop specific modules 
for each of the company’s products, cf. Fig. 3.5. 
4.2 
Determination of Costs as a Function of Properties (Step 3) 
This paper focuses on the identification of the manufacturing costs as a function of 
modularity. So, in this chapter the identified cost-functions are described. Based on a 
detailed literature research Fig. 4 illustrates all identified relations between costs and 
properties. To expand this approach, several hypothesis were added.  
Number of Units of a Component 
Three financial influences could be identified, which dependent on the number  
of units of a component. The higher the number of units, the smaller the influence of 
setup costs is [21]. From this, a declining progression of the cost can be deducted, cf. 
Fig. 4.1. Furthermore, DIN ISO 2859-1 [22] suggests that the number of parts, which 
should be tested, show decrease declining, if the lot size increases. As the testing time 
is proportioanl to the number of units to be tested, testing costs will decrease decli-
ningly, too, cf. Fig. 4.2. Moreover, an influence of manufacturing technologies to unit 

 
Cost-Effects of Product Modularity 
751 
costs could be identified. With a growing lot size, alternative manufacturing technol-
ogies (e. g. welding, casting) might be cheaper and this could create cost progression 
like shown in Fig. 4.3 [21], [23].  
Size of a Component 
The size of components influences, among others, the manufacturing time. With an 
increasing size, manufacturing costs rise on a progressive scale [24], cf. Fig. 4.4. Ad-
ditionally, it is assumed, that with an increasing size of components bigger and more 
expensive manufacturing machines are needed. Each machine can cover a certain 
range and this results in escalating costs, cf. Fig. 4.5. 
 
Fig. 4. Cost functions influencing manufacturing  
Number of Working Surfaces of a Company 
Working surfaces usually have higher tolerance requirements than non-working  
surfaces. Because of that, it is assumed that with an increasing number of working 
surfaces the number of manufacturing steps for components also increase. So it can be 
deducted, that costs also increase. A declining process is assumed because, although 
complexity rises, efforts of additional manufacturing steps presumably decrease, cf. 
Fig. 4.6. 
Number of Components of a Product 
Additional assembly steps are necessary when more components are part of a product. 
According to Miese [25] the assembly costs increase progressively with rising number 
of components of a product, cf. Fig. 4.7. Furthermore additional components and 
 
9) testing cost
1) cost by set-up time
cost by manufacturing time
cost by assambly steps
4)
6)
7)
costs
 per unit
costs
 per order
tiotal
 costs
costs
 per unit
costs
 per unit
1
number of units
of a component
2) testing cost
costs
 per unit
1
number of units
of a component
3) change of technologie
costs
 per unit
1
number of units
of a component
size of a component
invest for tools
number of manuf. steps
5)
costs
 per unit
size of a component
number of comp.
of a product
1
testing cost
8)
costs
 per order
number of comp.
of a product
1
number of comp
of a company
1
number of working
surfaces of a comp.
2

752 
T. Hohnen, J. Pollmanns, and J. Feldhusen 
additional assembly steps require additional tests. As the number of components and 
assembly steps increase progressive, the number of tests and so the costs increase 
progressively, too (cf. Fig. 4.8.) 
Number of Components of a Company 
As the number of components of a product, the number of components of a company 
has got the same influence on costs, cf. Fig. 4.9. 
4.3 
Determination of Modularity Costs of Properties 
After introducing the dependence of properties and modularity, and costs and proper-
ties, in this chapter the relation of costs and modularity of each property will be  
identified using nomograms, a graphical calculating device. 
Fig. 5 illustrates all identified relations with a positive financial impact of an  
increasing modularity. 
 
Fig. 5. Properties with positive financial impact on modularity  
Following relations could be identified with a negative financial impact of an in-
creasing modularity, see Fig. 6. 
number of working surfaces ...
(number of manuf. steps)
modularity
0%
100%
1)
costs
per unit
number of units of a component
(cost by set-up time)
modularity
0%
100%
2)
costs
per unit
number of units of a component
(testing cost)
modularity
0%
100%
3)
costs
per unit
number of units of a component
(change of technologie)
5)
modularity
0%
100%
size of  a  components
(invest for tools)
costs
per unit
4)
modularity
0%
100%
size of  a  components
(cost by manufacturing time)
costs
per unit
modularity
0%
100%
6)
costs
per unit
number of components of a company
(testing costs)
7)
modularity
0%
100%
total
costs

 
Cost-Effects of Product Modularity 
753 
 
Fig. 6. Properties with negative financial impact on modularity costs  
5 
Summary and Further Steps 
The engineer requires the knowledge about company specific cost factors to deter-
mine a cost-optimum product modularity. Therefore, the outcome of this paper is an 
approach to support modularization decisions of an engineer. With it, a structured 
procedure is introduced to identify factors which influence costs and which depend on 
product modularity. 
Based on literature, the influence of modularity on manufacturing costs is pre-
sented in this paper as a case study. 
Several limitations of this approach have to be mentioned. On the one hand the  
literature data just points out relative cost progressions. To use this data, company 
specific progressions needs to be quantified. On the other hand interactions of the 
properties were not considered in this approach, because company requirements will 
influence these interactions. So it is also necessary to identify and quantify the efforts 
of the interactions in a company. 
In further research, the influence of modularity on the other departments (devel-
opment, operations scheduling or procurement) should be analyzed. Additional to a 
literature research, several companies should be interviewed to identify these influ-
ences and cost-functions in detail. 
References 
1. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Pahl/Beitz Konstruktionslehre: Grundla-
gen erfolgreicher Produktentwicklung. Methoden und Anwendung, 7th edn. Springer, Ber-
lin (2007) 
2. Schuh, G.: Produktkomplexität managen: Strategien - Methoden - Tools, 2nd edn. Hanser, 
München (2005) 
3. Pine, B.J.: Mass customization: The new frontier in business competition. Harvard Busi-
ness School, Boston (1999) 
4. Ericsson, A., Erixon, G.: Controlling design variants: Modular product platforms. Society 
of Manufacturing Engineers, Dearborn (1999) 
5. Eitelwein, O., Weber, J.: Unternehmenserfolg durch Modularisierung von Produkten, Pro-
zessen und Supply Chains: Benchmarking-Studie Modularisierung, Norderstedt (2008) 
6. Hohnen, T.: Ein Ansatz zur kostenoptimalen Modularisierung. In: Feldhusen, J. (ed.) Aa-
chener Konstruktionstechnik. Mitteilungen, p. 16. Shaker (2010) 
number of components  of a product
(cost by assambly steps)
1)
modularity
0%
100%
total
costs
number of components of a product
(testing cost)
2)
modularity
0%
100%
total
costs

754 
T. Hohnen, J. Pollmanns, and J. Feldhusen 
7. Göpfert, J.: Modulare Produktentwicklung: Zur gemeinsamen Gestaltung von Technik und 
Organisation. Dt. Univ.-Vlg., Wiesbaden (1998) 
8. Müller, M.: Modularisierung von Produkten: Entwicklungszeiten und -kosten reduzieren. 
Hanser, München (2000) 
9. Arnoscht, J.: Beherrschung von Komplexität bei der Gestaltung von Baukastensystemen, 
1st edn. Apprimus-Verl., Aachen (2011) 
10. Gershenson, J.K., Prasad, G.J., Zhang, Y.: Product modularity: definitions and benefits. 
Journal of Engineering Design 14(3), 295–313 (2003) 
11. VDI-Gesellschaft Entwicklung Konstruktion Vertrieb: Methodik zum Entwickeln und 
Konstruieren technischer Systeme und Produkte. Beuth-Verlag (2221) (2012) (accessed 
September 3, 2012) 
12. Baumgart, I.M.: Modularisierung von Produkten im Anlagenbau, Mainz, Aachen (2005) 
13. Hohnen, T., Schliefer, I., Gneist, C., Feldhusen, J.: Methode zur kennwertgestützten Mod-
ularisierung: Retrospektive Untersuchung der Produktmodularität. In: EEE 2012, pp. 677–
690 (2012) 
14. Sosa, M.E., Eppinger, S.D., Rowles, C.M.: A network approach to define modularity of 
components in complex products. Journal of Mechanical Design 129(11), 1118–1129 
(2007) 
15. Stryker, A.C.: Development of Measures to assess Product Modularity and Reconfigurabil-
ty. Air Force Insitute of Technology, Ohio (2010) 
16. Westphal, J.R.: Komplextitätsmanagement in der Produktlogistik: ein Ansatz zur flusso-
rientierten Gestaltung und Lenkung heterogener Produktsysteme. Deutscher Univer-
sitätsverlag, Wiesbaden (2001) 
17. Hungenberg, H.: Komplexitätskosten. Kosten-Controlling, 539–552 
18. Kersten, W., Koppenhagen, F.: Systematische Ableitung modularer Produktarchitekturen: 
Komplexitätsreduzierung in der Konzeptphase. PPS Management 7(1), 9–13 (2002) 
19. Ulrich, K.: The role of product architecture in the manufacturing firm. Research Policy 24, 
419–440 (1995) 
20. Höltta-Otto, K., de Weck, O.: Degree of Modularity in Engineering Systems and Products 
with Technical and Business Constraints. Concurrent Engineering 15(2), 113–126 (2007) 
21. Ehrlenspiel, K., Kiewert, A., Lindemann, U.: Kostengünstig entwickeln und konstruieren: 
Kostenmanagement bei der integrierten Produktentwicklung; mit 143 Tabellen, 6th edn. 
Springer, Heidelberg (2007) 
22. Normenausschuss Qualtitätsmanagement, Statistik und Zertifizierungsgrundlagen: An-
nahmestichprobenprüfung anhand der Anzahl fehlerhafter Einheiten oder Fehler (Attri-
butprüfung). Beuth-Verlag (2859-1) (2004) 
23. Hafner, J.: Entscheidungshilfen für das kostengünstige Konstruieren von Schweiß- und 
Gußgehäusen, München TU, Diss. (1987) 
24. Fischer, D.: Kostenanalyse von Stirnzahnrädern: Erarbeitung und Vergleich von Hilfsmit-
teln zur Kostenfrüherkennung, München (1983) 
25. Miese, M.: Systematische Montageplanung in Unternehmen mit Kleinserienproduktion, 
Essen (1976) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 755–764. 
DOI: 10.1007/978-3-642-30817-8_74 
© Springer-Verlag Berlin Heidelberg 2013 
 
Proposal of a Research Methodology to Increase the 
Robustness of the Conjoint Trends Analysis Method 
through Its Formalization 
Angela Cadavid and Jorge Maya 
Product Design Engineering, Universidad EAFIT, Cr 49 # 7 Sur - 50, Medellin, Colombia 
{acadavi3,jmayacas}@eafit.edu.co 
Abstract. Nowadays, the product user experience (UX) is essential in the de-
sign of innovative products. Several methods assist in defining this UX. Some 
help to define the aesthetic appearance of a product (which conveys the desired 
UX). They are very precise but are also complex and expensive to use. Others 
are easy and inexpensive to use but imprecise. The Conjoint Trends Analysis 
Method (CTAM) lies between these two extremes. However, several CTAM’s 
instructions can be biased by the subjectivity of the CTAM user. Therefore, this 
research seeks to increase the CTAM robustness by formalizing instructions 
and making its concepts more explicit, aiming to increase its accuracy. Six ex-
periments divided in four studies are proposed to respond to different research 
questions. Finally, there is a discussion on how the results can provide a basis 
from which to extract more robust guidelines for each step of the CTAM. 
Keywords: Conjoint Trends Analysis Method, product aesthetics, user expe-
rience, product embodiment, product appearance. 
1 
Introduction 
Some of the systematic product design engineering methods most commonly used 
today are Pahl et al. [1], Ulrich et al. [2] and Otto et al. [3]. However, none of these 
methods allow the definition of the product´s aesthetic appearance. Such aesthetic 
appearance here is understood as the set of values that must present the product in all 
properties that define its aesthetics [4]: psycho-physical, organizational, semantic, 
cultural-individual and universal. According to Cagan et al. [5] this aesthetic appear-
ance conveys emotions and values necessary for the product to generate the desired 
user experience (UX). Hekkert et al. [4] defines UX as the self-awareness, subjective 
and affective, of what we feel when interacting with the product, at three levels: aes-
thetic-perceptual (given by the appearance of the product), emotional (emotions and 
feelings surrounding the product) and semantic (meaning and values of the product). 
Therefore depending on the interaction of these levels, in different proportions, and 
on the person, the product and the context, the UX if formed. Nowadays the UX can 
be fundamental to obtain innovative products, due to the high level of user  

756 
A. Cadavid and J. Maya 
satisfaction with the functionality of the products, including basic range products [5]. 
Among the authorities in ergonomics [6] and quality (KANO) [7] there is a consensus 
in this respect. Schifferstein et al. [4] argues that the aesthetic elements of the product 
must be coherent with the desired UX.  
In general terms, the research problem of this project is how to directly link the UX 
with product-adapted aesthetic attributes and characteristics in a direct and structured 
manner. In order to define the aesthetic appearance of a product, the literature reports 
many methods. Following Pahl et al.[1], 11 of the most known methods can be classi-
fied into 4 categories: intuitive, analytical, evaluative and global (discursive). Below 
the methods of each category and their advantages and disadvantages are discussed 
[8]. Intuitive (Loewy’s sixth sense, seeing magazines, Internet browsing): short and 
quick but not very precise; Analytic (Kansei Engineering 3 and 4): precise, respects 
UX’s complexity but long, expensive and requires special statistics knowledge; Eva-
luative (laddering, semantic differential): relatively precise but a model or prototype 
is required, it is also expensive, long and risky; Global (Repertory Grid Technique,  
Moodboards, ZMET, Conjoint Trends Analysis Method): relatively precise but some 
of them need special knowledge in psychology and may be expensive. Most of the 
above methods do not adjust to the industrial context of SMEs, especially in emerging 
countries, such as Colombia, where 99% of companies are SMEs [9]. Usually in these 
companies resources are limited. However, the Conjoint Trends Analysis Method 
(CTAM) can adapt well to these contexts, as it does not require special knowledge, 
specialized professionals or expensive massive user studies for its implementation 
Moreover, CTAM can be used by product designers themselves [8].  
CTAM is a semi-structured method that seeks to increase the accuracy in defining 
the product´s aesthetic attributes and in defining the UX desired for it [8]. Such a 
trend is considered a popular emergent style in product design in emerging countries 
[10]. CTAM has its origins in the fashion design industry. It has been developed, 
structured and used for over 10 years [10][11][12] in the car industry in France [11], 
among others. The result of applying the CTAM is a board (a composition of images) 
through which not only the desired aesthetic attributes of the product are conveyed (a 
style), but also the emotional and semantic attributes (values) desired for the UX 
[10][13] (see Figure 1). The CTAM then communicates the desired UX through the 
product. In addition to that, it is well known that visual aids encourage creativity in 
design [10][12]. The steps to build a TB are as follows (figure 1): 1) Establish the 
sectors of influence surrounding the designed product. An influence sector is any 
sector of human or natural activity that is related to the kind of product to be designed 
in any way and to the natural or artificial objects that exist in the sector. These objects 
have characteristic forms (geometry, colors, materials, textures, volumes) that can be 
integrated into the product to be designed and serve as a reference or inspiration for 
the product design embodiment phase. Magazines and websites that have images on 
these sectors must be collected. 2) Identify whether an image will be inspirational or 
not in the design process; inspirational ones should be selected until reaching a  
saturation point where no more inspirational images are found. 3) Classify images 
according to aesthetical, semantic or emotional criteria. 4) Debug the groups of im-
ages separating those that seem less coherent with the criteria. 5) Select only the most 

 
Proposal of a Research Methodology to Increase the Robustness of the CTAM 
757 
coherent and richer groups of images. A TB will be constructed from those groups. 6) 
Propose a harmonic composition of the images. Samples of colors and textures must 
be extracted in order to compose a palette to place alongside the composition. 7) 
Terms of the main features of the UX conveyed through the TB are identified and 
placed on the TB. The TB´s name must be defined as well. This name should be 
evocative so as to fulfill the inspiring function of the TB. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. Steps to follow to build a TB [10] 
Castano et al. [8] assessed the use of CTAM in the design of a home plastic orga-
nizer and conclude that “this (evaluative) work reveals points where CTAM is less 
structured and where the subjectivity of those who conceived the TB can occur (for 
example, when deciding whether an image is inspiring or not)”. This is due to the 
poor nature of some instructions. Castano et al. [8] concluded that CTAM is a promis-
ing method but can be improved in many areas. According to Castano et al. [8], the 
questions corresponding to the implementation and evaluation areas of the CTAM 
are: How to define influence sectors for a TB? These sectors have only been identi-
fied for the automotive sector. Which are the criteria for choosing an image as inspir-
ing? Which criteria allows to group images and to debug groups of similar images? 
How to make the composition of a TB? How to assign the UX terms and name? Most 
of these steps are done intuitively. Other problems identified by Castano et al. [8], i.e. 
concerning construction and ecological validity will be addressed in an upcoming 
research. This publication is placed consequently within a broader study, which seeks 
to increase the robustness of CTAM by creating a better definition of its instructions 
and making its concepts more explicit. That study seeks a CTAM less sensitive to 
external disturbances (e.i. mood changes of the person doing the TB, or changes in the 
aesthetic training required). The final aim of the project is to consolidate the CTAM 
to increase the accuracy of its results. The remainder of the paper is organized as  
follows: Section 2 describes the set of experiments used to answer the research  

758 
A. Cadavid and J. Maya 
questions, Section 3 presents the results obtained until now and Sections 4 and 5 
present the discussion and conclusions respectively. 
2 
Proposal of a Research Methodology 
In order to achieve the research objective, four studies are proposed and can be  
observed in table 1: influence sectors, inspiration, composition and validation of 
CTAM with increased robustness. Each study consists of experiments that correspond 
to each research question. The following paragraphs then demonstrate the systemati-
cal development of different alternatives for each experiment, its choice, and the final 
development. 
Table 1. Studies to increase the robustness of the CTAM 
Study 
Experiment 
Questions 
Objective 
1 
Influence sectors 
1 
How to define influence 
sectors for a TB? 
Define the influence sector in different fields 
from automotive design 
2 
Inspiration 
2 
Which are the criteria for 
choosing an image as inspir-
ing? 
Define criteria for choosing an image as 
inspiring 
3 
Understand the process of inspiration when 
designing a particular product 
3 
CTAM composi-
tion 
4 
Which criteria allow forming 
and debugging groups of 
similar images? 
Define criteria to form and debug groups of 
images 
5 
How to make the composi-
tion of a TB? 
Define criteria to make the composition, to 
assign the terms and to name a TB 
4 
Validation. 
CTAM with 
increased robust-
ness 
6 
How to verify an increase of 
CTAM robustness? 
Verify an increase in the robustness of the 
CTAM 
2.1 
STUDY 1: Influence Sectors Experiment  
Experiment 1 should contribute to defining the influence sectors for a TB. Two  
alternatives were proposed: a) Proposal of boards with visual stimuli from which 
participants (product designers) are asked to make a list of influence sectors, thus 
obtaining a general list. b) Make a general list of influence sectors from magazine 
images from the different design sectors. The list is delivered to some designers in 
order to select and rank the sectors that have influenced their designs. The selected 
alternative was the second because it allowed to cover different sectors of products 
frequently designed (i.e. furniture, tableware and home appliances), and would take 
less time than the first proposal. 
In order to carry out the experiment an exhaustive and general list of influence  
sectors related to the type of products mentioned before should be made. This list is 
useful in building a TB to design common product categories. The initial list will 
begin with the sectors found in widely used design magazines. Within one image 
multiple objects are classified in different influence sectors. In this case, all the sec-
tors found will be on the list. A saturation point must be reached, at which point the 

 
Proposal of a Research Methodology to Increase the Robustness of the CTAM 
759 
new sectors found are already part of the list and new sectors no longer appear. After 
this, a brainstorming of the possible influence sectors that influence the design of 
certain products is made (i.e. tableware, furniture and home appliances), in order to be 
exhaustive with the already created list. Then, the existence of characteristic objects 
for each sector is inquired, and whether there is general agreement on the characteris-
tic objects pertaining to the sector (e.g. tableware sector´s characteristic objects: dish-
es, cutlery, etc...). This is done because TBs function visually. If the result of these 
two is positive, then the activity is defined as an influence sector, otherwise, it is not. 
From the general list of influence sectors, product designers from the industry are 
asked to choose and classify the sectors they considered for their last designed prod-
uct, and rank them from the most influential to the least influential.  
2.2 
STUDY 2: Experiments about Inspiration 
Experiment 2 should contribute in defining what the criteria for cataloguing an im-
age as inspiring are. The alternatives proposed were: a) Ask participants (i.e. product 
designers) to leaf through a design magazine, and select inspiring images for a given 
simulated design situation. A protocol analysis method will be used to know the crite-
ria for the selection of these kinds of images, and the process will be recorded. This 
method allows participants to verbalize their thoughts while performing a task, thus 
not altering the sequence nor the content of these (Cf.[14]). b) Select and place sever-
al groups of images of products, natural objects and people performing actions on 
sheets. Ask participants to select one of the images that they consider inspiring for a 
particular simulated design situation. During this activity the protocol analysis method 
will be used. The first proposal was selected because the magazine contains images of 
several influence sectors that provide richer data for the experiment. First, the images 
that are in a previously selected magazine are numerated, and a simulated product 
design task is proposed. Each participant is asked to leaf through the magazine and 
ask himself or herself in each image: is this image inspiring to me to design the task’s 
product? So the individual images that are inspiring to a specific product design are 
identified. This activity is carried out until the end of the mag. This experiment con-
sists of two groups of product designers: experienced and novice, a between subjects 
experiment [15]). 
Experiment 3 should contribute to complement the results from experiment 2 with 
information obtained from interviewing product designers about their lives, expe-
riences or situations as expressed in their own words [15]. It is considered that the 
inspiration process is continuous through life, and that the early stages of any creative 
process are individual [16]. According to the statements above, three in-depth inter-
views with expert designers are realized and the data is analyzed with the results of 
Experiment 2. Experts selected must fulfill the following criteria: first, they must have 
worked in product design and teaching; second, they must have designed constantly 
throughout their professional lives and third they have to be familiarized with the 
design language. The interview questions are first, to characterize the designers;  
 

760 
A. Cadavid and J. Maya 
secondly, to allow them to recognize inspiration from their personal experiences; 
thirdly, to have insights on the sources of inspiration and fourth, to gain insights into 
the images as part of the inspiration. Each interview is recorded and transcribed. The 
results allow the recollection of data based on the criteria to choose an image as in-
spiring from the experience of experts. 
2.3 
STUDY 3: CTAM Composition Experiments 
Experiment 4 should contribute to know which criteria allow to form and debug 
groups of similar images. The options considered were: a) Ask the participants (e.g. 
product designers) to form groups of similar images and discard the dissimilar ones. 
These groups belong to a larger set of images. In that set there are similar and dissimi-
lar images defined a priori. A protocol analysis method is used to perform the task. b) 
Give a magazine to each participant and ask them to cut images to form groups of 
similar images. The first option was selected because it allowed the use of a limited 
number of pre-defined images chosen by the participants and consequently limited the 
number of images of similar criteria. During the activity the protocol analysis method 
is used [14] complemented with an eye tracker. This equipment allows to have infor-
mation of exactly where and how long participants looked (and did not look) [17] 
when viewing the images. The subjects are then asked to form groups of similar im-
ages (according to a single criteria: emotional, semantic or aesthetical, according to 
the definition of UX [4]). Then the subjects are asked to remove the dissimilar images 
of each group (dissonant images), and ask themselves about each one: is this image 
highly similar with this group? If the answer is "more or less" or "little", the image 
must be discarded. Anyway, the discarded image can pass to another group of images. 
This is done to debug the groups and to obtain criteria used to do it. 
Experiment 5 should contribute to the insights on how to compose a TB. The alterna-
tives were: a) Show each participant (i.e. product designers) a TB already created 
themselves. Then ask them to remember how they made the composition. b) Ask the 
participants to make a TB for a simulated design situation, from a similar group of 
images, according to some formal-aesthetic, semantic or emotional elements they 
have in common (UX definition [4]). The protocol analysis method must be used in 
order to obtain insights of this process. Then a TB composition is done using image 
editing software. The second option was chosen because this experiment is to define 
criteria for the composition of a TB, which can be obtained from protocol analysis 
and would not be affected by haphazard memories as in a). The same set of images is 
given to each subject on a digital file. A laptop with sufficient graphical performance 
must be used. Each subject is asked to produce one TB from these images by dia-
gramming the images in the software to form a harmonious and coherent composition. 
At the end of the layout, the subject is asked to write down emotions, meanings and 
values suggested by the TB, according to the UX desired to convey with the TB. Then 
the participant is asked to give an evocative name to the TB. 

 
Proposal of a Research Methodology to Increase the Robustness of the CTAM 
761 
2.4 
STUDY 4: More Robust CTAM Validation Experiment 
Experiment 6 should verify whether there is an increase in the robustness of the new 
CTAM method based on the criteria defined from experiments 1 to 5. It should be 
defined as follows: two groups of people doing the same activity but, one with the 
CTAM already presented in the introduction of this paper, and the other with the new 
CTAM based on the new criteria found. A sample consisting of six Product Design 
Engineering Students from EAFIT University who have already taken the product 
aesthetics course and know the original CTAM could be proposed. They should be 
skilled in an image editing software. All other people variables are kept as homoge-
neous as possible. The six people are split into two equal groups: original and new 
CTAM. The original CTAM group receives the instructions given in the introduction, 
in figure 1. The magazines used to obtain the images could be the same used in expe-
riment 2. As a result from 4 to 7 TBs are obtained. The “new CTAM “ group must 
follow the same scheme described in Figure 1 for the same design situation with these 
changes: for influence sector definition, criteria defined from the experiments 1 must 
be used; for analysis and selection of images, the criteria defined from the experi-
ments 2–3; for images clustering and debugging, the criteria defined from the experi-
ment 4; for composition of a TB, name and terms, the criteria defined from the  
experiment 5. The quality of the TBs obtained by both groups, in terms of the UX 
conveyed by each TB, is evaluated through a semantic differential. This is a quantita-
tive technique used to obtain the connotative meaning of an object. It analyzes to 
which extent product semantic oppositions directly concern the UX´s conveyed in 
each TB [8]. 
3 
Results and Discussion 
The first 5 experiments were performed as described above. Therefore the type of 
results obtained in each experiment and whether those results allow obtaining an an-
swer to the research questions are described below. Experiment 6 has not been per-
formed yet because it depends on the analysis of the results of experiments 1 to 5. 
Consequently, this last experiment will be discussed differently.  
3.1 
STUDY 1: Sectors of Influence 
The initial list obtained began with the influence sectors found in the widely used 
magazine Domus (Italy), from February 2012 (latest volume available to the comple-
tion date of the activity): the magazine contains images of the influence sectors for 
tableware, furniture and electrical appliances design. As results of experiment 1, 2 
lists of 15 influence sectors each one were obtained for design of home appliances 
and furniture. The first 5 influence sectors of furniture were in order of importance: 
architecture, furniture, fashion, work in office and construction. For electrical ap-
pliances the order was: electrical appliances, automotive design, technology, objects 
for the kitchen and objects for the table. Comparing the results from the three  

762 
A. Cadavid and J. Maya 
designers could evidence if there is a specific order in the list of influence sectors and 
consequently to propose a general list adaptable to any product design project. It is 
known that sectors of influence can be gradually distant from the target, based on the 
proportion of shared properties with the target object [18].  
3.2 
STUDY 2: Inspiration 
In experiment 2 the same magazine “Domus” used in experiment 1 was used. The 
sample selected for this experiment consisted of groups a and b: a) One industrial 
designer (design chief) and two product design engineers (design manager 1 and 2), in 
charge of the embodiment in product design in a company of home appliances. Only 
one of the design managers knew the CTAM, the other persons do not. This group 
had people with more than 4 years of experience. b) Three students of third semester 
of the Product Design Engineering program at EAFIT University who were taking the 
Product Aesthetics course in which CTAM is taught. This group had novice design-
ers. The results from experiment 2 were 6 transcripts of talk-aloud protocols, 3 from 
the home appliances designers (140 minutes, 43 pages) and 3 from the product design 
engineering students (101 minutes, 22 pages). First the transcripts of each participant 
will be analyzed, and then a comparison will be made between these two results to 
obtain information about the criteria for choosing a picture as inspirational. From the 
transcript results it is expected that experienced designers be “able to take advantage 
of various sources, semantically near or far from the target object, and to adopt and 
integrate different points of views about the suggested sources” [19]. In experiment 3 
for the selection of images it is expected that designers use experience and knowledge 
gained before the project to conduct direct gathering of information [16]. The novel 
designers can consider an image inspiring when they find mainly functional similari-
ties with the target object, whereas the experts also relate the images based on the 
structural aspects [19]. For experiment 3, three people were selected: first, two teach-
ers from the EAFIT University Product Design Engineering program. They have 
worked the topic of inspiration and creativity and have a extensive experience in 
product design and in product design teaching (27 and 10 years respectively). Finally, 
a product design engineer with 3 years of experience (running his own product design 
company) was selected. The first two teachers knew the CTAM but they had not ap-
plied it; the latter did not know the CTAM. In this experiment 3 transcriptions of the 
experts’ interviews were obtained (116 minutes, 35 pages) to have insight about  
the inspiration process in product design to complement results from experiment 2. 
The analysis of these results would allow identifying what kind of images (domain 
sources) are most inspiring to experienced or novel designers. It would help to define 
criteria on how to know whether an image is inspiring or not, as well. 
3.3 
STUDY 3: TB Composition 
The participants in experiment 5 were three students of third semester Engineering 
Product Design at EAFIT University, who were taking the Product Aesthetics course 
at the time of the realization of the experiment, and already knew the CTAM. They 

 
Proposal of a Research Methodology to Increase the Robustness of the CTAM 
763 
had the highest grades using CTAM in that course. 30 images were selected for the 
experiment, as follows: 25 similar and 5 dissimilar, similarity was defined according 
to colors and kinds of materials. The image selection was made by one of the re-
searchers. The results for this experiment were 3 transcripts (15 pages) of talk-aloud 
protocol, one for each participant and 3 videos and data obtained from the eye tracker. 
Then, as a result of the experiment is expected that elements of the cognitive psychol-
ogy categorization models [20] were used to define the criteria to form and debug the 
groups of images. For experiment 5, three students from Product Design Engineering 
(from EAFIT University) were chosen. They were from fifth semester (two female, 
20 and 21, and a male, 19), and they had already studied and used the CTAM. They 
had medium level knowledge in image editing software. Although participants knew 
already the CTAM, it is initially explained to them and some TBs examples were 
shown. As results of experiment 5, 3 transcripts were obtained of the talk-aloud pro-
tocols (99 minutes, 16 pages) and 3 digital files (34.1 MB), each containing the TB 
realized by each participant. From the results of this experiment it would be possible 
to obtain clear criteria to, first make an organized and harmonious visual composition 
and second, to know the keywords and TB´s name, were chosen to complete the UX 
conveyed by the images.  
3.4 
STUDY 4: Validation 
Experiment 6 has not been performed yet. It depends on the results obtained in all the 
above experiments. It is expected that when including the criteria defined in each one 
of the experiments 1-5 to the corresponding phase of the CTAM, the possibility of 
introducing subjective bias by participants in the decisions to be taken during the 
development of CTAM is reduced. With the criteria defined through the experiments, 
it would be possible to have TBs that allow designers to design products that convey 
more precisely the desired UX for the user. 
4 
Conclusion 
Product design should be benefited with the results, in that it would allow defining the 
aesthetic appearance of a product with a qualitative method, in a relatively accurate 
manner. The design product activity is to formalize the CTAM´s instructions, 4  
studies and 6 experiments were defined. For experiments 1 to 5 two options were 
proposed. The option that best met the research objectives was chosen. The results of 
the protocol analysis method data should help to reveal what the content of peoples’ 
minds are while they are using CTAM. Eye tracking results should reveal the connec-
tion between the type of visual content and the amount of attention paid to that con-
tent. In summary all the results should prevent noise generation from external and 
subjective aspects in CTAM thus avoiding imprecise results. It should be considered 
that these experiments are qualitative: they allow to obtain criteria for guidance only, 
not to take crucial decisions in the design process. If even more robust CTAM´s in-
structions are required, quantitative experiments should be designed (i.e. quantitative 
correlational and significance analysis of this data) [21]. 

764 
A. Cadavid and J. Maya 
References 
1. Pahl, G., Beitz, W., Wallace, K.: Engineering design: a systematic approach. Springer 
(1996) 
2. Ulrich, K.T., Eppinger, S.D.: Product design and development. McGraw-Hill (2011) 
3. Otto, K.N., Wood, K.L.: Product design. Prentice Hall, Englewood Cliffs (2000) 
4. Hekkert, P., Schifferstein, H.: Introducing product experience. In: Product Experience, pp. 
1–8 (2008) 
5. Cagan, J., Vogel, C.M.: Creating breakthrough products: Innovation from product plan-
ning to program approval. FT Press (2002) 
6. Jordan, P.W.: Pleasure with products: Human factors for body, mind and soul. In: Human 
Factors in Product Design: Current Practice and Future Trends, pp. 206–217 (1999) 
7. Matzler, K., Hinterhuber, H.H.: How to make product development projects more success-
ful by integrating Kano’s model of customer satisfaction into quality function deployment. 
Technovation 18, 25–38 (1998) 
8. Castano, M., Hernan, J., Arenas, M., Velez, M.: Implementation and Assessment of the 
Trend Boards Method in a Product Design Engineering Program. In: Proceedings of the 
13th International Conference on Engineering and Product Design Education E&PDE 
2011, pp. 541–546 (2011) 
9. Rodríguez, A.G.: The reality of Colombian SMEs: a challenge for development. Program 
Improvement Corporate Environmental Conditions. FUNDES Colombia (2003) (in Span-
ish) 
10. Bouchard, C., Christofol, H., Roussel, B., Aoussat, A.: Identification and integration of 
product design trends. In: International Conference on Engineering Design, Munich (1999) 
11. Bereciartua, A., Bouchard, C., Ferecatu, M., Logerot, G., Rigouste, L., Vitale, C.: Meta 
Deliverable 1-state of the art (2007) 
12. Mougenot, C.: Modélisation de la phase d’exploration du processus de conception de pro-
duits, pour une créativité augmentée. Design Studies 27, 587–613 (2008) 
13. Eckert, C., Stacey, M., Clarkson, P.: Algorithms and inspirations: creative reuse of design 
experience. In: Proceedings of Greenwich 2000: The International Symposium on Digital 
Creativity, pp. 1–10 (2000) 
14. Ericsson, K.A.: Protocol analysis and expert thought: Concurrent verbalizations of think-
ing during experts’ performance on representative tasks. In: The Cambridge Handbook of 
Expertise and Expert Performance, pp. 223–241 (2006) 
15. Graziano, A.M., Raulin, M.L.: Research methods: A process of inquiry. HarperCollins 
College Publishers (1993) 
16. Mougenot, C., Bouchard, C., Aoussat, A., Westerman, S.: Inspiration, images and design: 
an investigation of designers’ information gathering strategies. Journal of Design Re-
search 7, 331–351 (2008) 
17. Hanington, B., Martin, B.: Universal Methods of Design: 100 Ways to Research Complex 
Problems, Develop Innovative Ideas, and Design Effective Solutions. Rockport Publishers 
(2012) 
18. Mougenot, C., Watanabe, K., Bouchard, C., Aoussat, A.: Visual materials and designers’ 
cognitive activity: Towards in-depth investigations of design cognition. International As-
sociation of Societies of Design Research, Seoul (2009) 
19. Bonnardel, N., Marmèche, E.: Towards supporting evocation processes in creative design: 
A cognitive approach. International Journal of Human-Computer Studies 63, 422–435 
(2005) 
20. Sternberg, R.J., Mio, J.: Cognitive psychology. Wadsworth Pub. Co. (2008) 
21. Pearsall, J., Hanks, P.: The new Oxford dictionary of English. Clarendon Press (1998) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 765–775. 
DOI: 10.1007/978-3-642-30817-8_75      © Springer-Verlag Berlin Heidelberg 2013 
Analyzing the Deviation of Product Value Judgment  
Payam Amini and Robert Schmitt 
Laboratory for Machine Tools and Production Engineering (WZL),  
RWTH Aachen University, Aachen, Germany 
P.Amini@wzl.rwth-aachen.de, R.Schmitt@wzl.rwth.aachen.de 
Abstract. Consumers perceive the value of durable goods at two points of time: 
first, they form their opinion about the value immediately before purchase de-
termining their choice. Second, they form their opinion during the utilization 
phase, which determines their repurchase behavior. Both the expected pre-
purchase value and perceived value during the utilization phase are affected by 
the product quality. Functionality and accuracy of production maintain no long-
er the only criteria for product differentiation. Ultimately, the perceived product 
quality, design and usability have a significant impact on the customer per-
ceived product value. This paper presents the developed CPV construct and a 
three phased approach to provide the evidence that the pre-purchase differs sig-
nificantly from the post-purchase value judgments. Empirical data of pre-
studies are presented in this paper. 
Keywords: perceived quality, product value judgment, perceived value. 
1 
Customer Perceived Value 
The customer perceived value (CPV) [1] is defined as “personal perception of advan-
tage arising out of a customer’s association with an organization’s offering, and can 
occur as reduction in sacrifice; presence of benefit (perceived as either attributes or 
outcomes); the resultant of any weighed combination of sacrifice and benefit; or an 
aggregation, over time, of any or all of these.” [2] The customer’s perception is the 
core issue, because CPV is not an objectively measurable value. Rather, the percep-
tion and attribution of value is different from individual to individual and from situa-
tion to situation (depending on time, place and environment). “Not only does each of 
us value the same things differently, we individually value different things, and at 
different times in different ways.” [2] In particular, the dependence of the value 
judgment on the time of assessment is essential to this paper. 
The expected value of a product is determined by the consumer in a cognitive, sub-
jective comparison process. He trades the expected benefits off against the expected 
costs and risks [3–6] .The result of this process is the value, which can be compared 
with the values of alternative products. 
The pre-purchase phase describes the stage, in which the customer gathers first in-
formation and makes his choice about the product. The point of purchase is imminent. 

766 
P. Amini and R. Schmitt 
Pre-purchase product value determines the initial purchase. Whereas the post-
purchase phase describes the stage, in which the customer uses the product in his 
everyday life and has an appropriate time to get used to it. Post-purchase product 
value has an impact on the probability of repurchase [7]. For both decisions, initial 
purchase and repurchase, the perceived product quality is essential. Especially in the 
higher price segment products are preferred acquired, whose quality is assessed by the 
customer as superior to competing products [8]. This impact can manifest itself both 
directly (product quality as a critical purchase reason) and indirectly (product quality 
increases satisfaction, which in turn leads to repurchase) [9]. For companies, the 
knowledge about the formation of the judgments about the expected and perceived 
product value is important. The customer lifetime value research shows, that regular 
customer are more profitable than new customers, which emphasizes the relevance of 
the post-purchase judgment. A high level of customer loyalty leads to repurchases of 
products, cross-buying and recommendation of the supplier. One Euro, which a dura-
ble goods producer spends to hold a regular customer, has a higher return than one 
Euro he spends to obtain a new customer [10]. The post-purchase value judgment will 
decide on the probability of repurchase and is based in most instances on the expe-
rience qualities of the product. The product perception changes over the consumption 
process. It is essential to see whether, where and how the perceived pre-purchase 
product value changes to the post-purchase value. The subconscious assessment and 
the changing review of priorities of customers are important aspects that must be con-
sidered when looking at the CPV over time. During the utilization phase, some ele-
ments of the CPV will appear more important – so the customer’s judgment will be 
predicated on a different base. The hierarchical-model by Woodruff and Gardial also 
highlights the fact that customers estimate the product value at different times in vary-
ing contexts. Thus, the pre-purchase judgment is especially characterized by the de-
sired product attributes. However, the consequences of the product utilization have a 
major impact on the CPV during and after the utilization phase [11]. Therefore, con-
sumer perceived value is a dynamic construct [3], [12]. The focus of judgment shifts 
between the generating dimensions, depending on the time of observation [13–14]. 
Huber et al. postulate that the benefits and costs are defined in terms of consumer 
perceptions in the activities of acquisition, consumption, maintenance, as well as con-
sumer expectations of personal value-satisfaction before purchase [15].  
2 
Justification of Need for Research 
So far, most research is related to the declaration of the expected product value. It is 
often implicitly assumed, that the expected and perceived product value are deter-
mined by the same quality attributes. If this assumption is valid, companies might 
restrict their market research activities to the purchase situation. It would be sufficient 
to involve the voice of the customer at this point to develop customer-focused prod-
ucts and build up satisfaction and loyalty to the company.  

 
Analyzing the Deviation of Product Value Judgment 
767 
However, evidences from several lines of research indicate that there are not the 
same quality attributes affecting the value judgment before the purchase and during 
the utilization phase: 
• Pre-purchase value judgment is based rather on product attributes, whereas the 
post-purchase value judgment is based more on the consequences of the usage. 
Different levels of means-end-chains are dominant in these two phases. [14] 
• Before purchase attributes are important, by which consumers can differentiate 
between alternatives. Customers frequently express rational considered thoughts. 
After purchase, attributes have a stronger impact on value judgment, which are re-
levant for satisfaction of need. Customers speak more often about emotions caused 
by product-usage. [14], [16–17] 
• The approach of quality uncertainty from information economy distinguishes be-
tween search-, experience- and credence qualities of products. A product’s quality 
that has mainly search qualities can be determined by inspection before purchase. 
For experience goods, this is only possible after purchase by using it. Therefore, 
pre-purchase value judgment can only be based on search qualities, experience 
with the product category and information taken from external sources, while in-
formation about experience qualities can be involved into post-purchase value 
judgment. [18–21] 
So far, a detailed examination of the product value in two separate measurements 
does not occur in the current research approaches. The known publications are exclu-
sively about surveys that attempt to measure product value at any time during the 
utilization phase. This is a serious methodological deficiency, as they imply an ad-
justment of pre-purchase and post-purchase value judgments in the perception of the 
subjects over time. 
In summary, the literature seems to hold three gaps: First, in most publications ei-
ther only pre-purchase or post-purchase product value is considered. Second, in the 
few cases, in which an attempt is made to collect both product values, there is no 
quantitative measure instrument to collect both values and thus to compare them with 
each other. Third, the measurement of the product values does not take place at two 
temporally separate times. For this purpose, it is essential to investigate on which 
level of abstraction and why the perceived pre-purchase product value differs signifi-
cantly from the perceived post-purchase product value. 
3 
The CPV Construct 
To understand and to be able to predict consumers’ choices, it must be known, how 
they perceive products. One stated hypothesis is that consumers perceive products as 
bundles of quality attributes with connected attribute performances [3]. On the one 
hand, the benefits of an attribute result from the degree to which it is assessed as use-
ful for the satisfaction of needs. On the other hand, whether the product-specific, per-
ceived attribute performance has a perceptible difference in benefits compared to 
alternative products [22–24]. Therefore, from a business point of view, not the  

768 
P. Amini and R. Schmitt 
objective product quality is relevant. Instead, the perceived value created at the inter-
face of the product and the user [25], compared to his expectations, affects the con-
sumers’ choice, deciding for or against the purchase [26]. The product value can be 
increased by two mechanisms: either by reducing the cost to the consumer or just by 
an increase in product deliverables, so the benefits [27]. However, it must be ensured 
that this value is also perceived as such by the consumer [27], which means, the man-
ufacturer must have an idea of the product value from a customer’s perspective.  
Sanchez-Fernandez et al. suggest using a multidimensional construct for the con-
ceptualization of the dimensions of the customer’s perceived product value. Accord-
ing to this view, the product value is related to an aggregation of elements [28]. To 
design a general instrument for measuring the value of the product before and after 
the purchase, the trade-off between benefits and costs is the conceptual framework for 
the scale as shown in Figure 1 [4], [6], [29]. In accordance with the literature, this 
cognitive-rational approach should be extended to emotional, so-called hedonic  
dimensions [30–32]. 
 
Fig. 1. Trade-off between benefits and costs as the conceptual framework 
Furthermore, it is necessary to specify the dimension of benefits and costs with 
corresponding elements. Treacy and Wiersema emphasize the key question, which 
must be taken into account for the creation of a measuring instrument: “What are the 
dimensions of value that customers care about?” [33]. The objective is to create a 
universally applicable item catalog. Consequently, a weighted additive combination 
of the elements seems suitable. This methodology ensures that no important aspects of 
the CPV will be ignored and follows the multidimensional view of the construct. Fur-
thermore, the additive model takes into account the cognitive balancing trade-off 
between benefits and cost [34] and is preferred to a multiplicative model in the form 
of a ratio of benefits to costs [35]. First, all influencing benefit and cost elements must 
be examined and identified in order to deliver such an equation.  
Sinha et al. point out: “Perceived Value is clearly a multidimensional construct de-
rived from perceptions of price, quality, quantity, benefits, and sacrifice, and whose 
dimensionality must be investigated and established for a given product category.” 
[36] If there is no certain product category, the elements of the construct should be 
investigated as comprehensive as possible including all items. 
Furthermore, the aspect of consequences of the usage has to be added, because it is 
closely linked to perceived or desired characteristics of a product [3], [11]. The 
means-end-chain theory delivers statements to the question of how product characte-
ristics establish benefits for the individual. Its central assumption is, that individuals 
aim for the achievement of valued states of being, also called ends, e.g. joy, security, 
the feeling of having a good performance [37]. Individuals choose those alternatives 
whose consequences contribute to achieving their goals. Hence, the consumption of 
the product equals the means to achieve the esteemed states of being (ends).  
benefits
costs
CPV

 
Analyzing the Deviation of Product Value Judgment 
769 
Therefore, the benefit of the product depends on the perceived contribution of the 
achievement [38]. 
In the following, item sets are developed that query the identified elements of the 
CPV individually [39]. The outcome is a multiple-item scale, which contains the CPV 
construct comprehensively. Thus, it is possible to determine the elements, in which 
the product value perceptions have changed, by comparing the results of the pre-
purchase to the post-purchase judgment. Figure 2 shows the derived cost-benefit 
model, which sets the framework for this item generation.  
 
Fig. 2. The CPV construct with the abstraction levels as concept for the measuring instrument 
The importance of individual product attributes for the overall value judgment 
changes after the purchase. The individual dimensions are considered unequally  
relevant and effect the final judgment unequally weighted, depending on the product 
category and individual. Interactions between the elements are considered as well. 
[39-40]  
In most cases, the customer has difficulties to estimate the comprehensive product 
value prior to purchase. In accordance with the selected CPV model the described 
logic is used. Equation (1) is the calculation instruction, where n stands for the num-
ber of benefit elements and m represents the number of cost elements. The coefficient 
x is used to weight the corresponding element relating to the identified customer’s 
relevance. According to the item rating scale, CPVs in the range between 6 and -6 are 
possible. 
 
ܥܸܲ= 
ଵ
௡∑
ݔ௜כ ܾ݂݁݊݁݅ݐ ݈݁݁݉݁݊ݐ௜
௡
௜ୀଵ
 – 
ଵ
௠∑
ݔ௝כ ܿ݋ݏݐ ݈݁݁݉݁݊ݐ௝
௠
௝ୀଵ
 
(1) 
The formation of the arithmetic average seems suitable to generate exemplary results 
from the questionnaire.  
4 
Approach to the Analysis of Deviation of Product Value 
Judgment  
The objective is to provide the evidence that the pre-purchase value judgment differs 
systematically from the post-purchase value judgment. Based on that, the causes of 
benefits
costs
CPV
psychological 
costs
perceived 
risks
economical 
costs
purchase price
fulfilled 
product 
attributes
desired 
consequences
service
perceived 
quality
pragmatic quality
hedonic quality
brand
monetarily costs of 
usage
time

770 
P. Amini and R. Schmitt 
the deviations have to be identified. First, it is necessary to investigate, in which way 
the CPV can be queried systematically based on its elements to determine deviations 
over the course of time. Subsequently, it must be specified at which two different 
measuring points the CPV can be evaluated. Finally, it must be investigated, how the 
results can be interrelated analytically. In terms of answering the research questions, it 
is necessary to go through three specific phases: 
 
Phase 1: Scientific derivation and creation of a measuring instrument 
 
It is pursued to investigate the pre-purchase and post-purchase product value with a 
measuring instrument. Based on the knowledge about the CPV construct, a measuring 
instrument (questionnaire) is developed. Afterwards, a durable good is selected to 
serve as a sample product in the following surveys. Initially, a list of criteria is devel-
oped for selection of this product based on literature on durable goods and practical 
considerations. The product should meet at least the following criteria: It should be 
known and used by many potential subjects, be applied in various usage situations 
and part of every day life to ensure usage. Subsequently, its relevant attributes have to 
be identified. Attributes are relevant, if the user derives any benefits through them. 
The identification of the attributes covers several steps. Considerations resulted in the 
following minimum activities: Test magazines, Internet forums and similar publica-
tions are investigated by comparing attributes within the media research. During the 
dealer survey the (specialty) retailer is interviewed about the attributes that customers 
care about. The lead user survey includes a half-day focus group conducted with 6-10 
volunteers of the target group. Within this group interview, attributes and potential 
use situations are developed on the basis of existing physical exemplars of various 
brands. Based on determined attributes, the questionnaire is adjusted to the sample 
product and subject to a pre-test, which checks the different quality criteria. 
The result of phase 1 is a validated questionnaire that measures the pre-purchase 
and post-purchase product value. It is adjusted with a set of customer-relevant 
attributes of the product sample. 
 
Phase 2: Application of the questionnaire and inquiry of data at two measuring points 
 
It requires a survey at two separate measuring points in order to determine the devia-
tion of the CPV over time. The deviation of pre-purchase and post-purchase product 
value or during the utilization phase is of particular interest. At the beginning of the 
second phase, subjects are identified that are about to purchase a product in the rele-
vant category. Therefore, they are in the first stage of the buying process (search for 
alternatives, selection of an alternative). The sample product is presented to the sub-
jects, so that they have time to test it. Subsequently, the subjects fill out the question-
naire (survey of pre-purchase product value) and purchase the same product to ensure 
comparable data sets. The product is not delivered immediately as the purchase at the 
counter is an important part of the consumption process. Therefore, it should not be 
omitted in order to create a realistic situation.  

 
Analyzing the Deviation of Product Value Judgment 
771 
After purchasing the product, the subjects use it during a specified period in their 
everyday lives. The period is set to three months, as it is long enough to use the prod-
uct in different usage situations and get an idea of the product. After that  
period they fill out the questionnaire a second time (survey of post-purchase product 
value). 
Phase 2 delivers answers about the pre-purchase and post-purchase product values 
through the questionnaires that can be transformed into analyzable data sets. 
 
Phase 3: Analysis of the deviations 
The evaluation of the collected data in phase 2 includes a preparation of the demo-
graphic data, re-examination of the quality criteria and inferential exploration for 
investigating the deviation of pre-purchase and post-purchase value judgments. 
The different perspectives of the questionnaires allow a distinct analysis of the ex-
pected deviations at various abstraction levels (product value in general, benefit and 
cost dimensions of the product value, included elements). For example, it is possible 
to investigate whether deviations found in the evaluation of individual elements also 
affect the product value in general.  
The design of the questionnaire allows to express the CPV using a concrete numer-
ical value of each subject, both for the pre-purchase as well as the post-purchase sur-
vey. A numerical value is not only easier to compare, but also allows more precise 
statements about the perceived quality of a product. Furthermore, a measurable value 
of CPV allows the direct comparison of two time points of the survey, which is in 
terms of the present investigation, since the deviation of the CPV can be determined 
directly over time for each subject. For purpose of statistical analysis, values must be 
assigned corresponding to the response levels of the measuring instrument. The rating 
direction is set for each item. Based on the developed equation (1) to calculate the 
CPV, a graphical representation is provided in order to illustrate findings from the 
questionnaires. The radar chart shown in Figure 3 (a) is an appropriate compromise 
between a detailed level and an overall picture. Results of pre-purchase and post-
purchase survey can be plotted in one diagram in order to allow direct comparison. 
Striking deviations between the perceptions at both times can be notified directly.  
The objective is to identify the reasons for the deviation of pre-purchase and post-
purchase value judgments. Results are both statistical statements about the deviation 
of pre-purchase and post-purchase value judgments and the results of the scale valida-
tion. Subsequently, a qualitative research approach is chosen to identify the reasons 
for the uncovered differences. Methodologically, the implementation of semi-
structured interviews seems promising. Depending on whether significant deviations 
are found at all or only some abstraction levels, the configuration of the interviews 
changes. Subjects from phase 2, where deviations have occurred on the relevant ab-
straction levels, will be interviewed about the causes behind this alteration.  
The result of phase 3 is a list of possible reasons for deviations from the pre-
purchase to post-purchase product value. 
 
 

772 
P. Amini and R. Schmitt 
Application and validation of the approach 
 
A pre-study has been conducted to validate the generated CPV construct with the 
developed questionnaire containing 115 items. A tablet-PC has been chosen as sample 
product as it is matching the developed list of criteria of phase 1. With 19 subjects, 
individual and summarized CPVs have been evaluated. Figure 3 (a) shows exemplary 
the results of an evaluated individual pre-purchase CPV with an overall outcome of 
2,01 and a possible fictive post-purchase CPV with an overall outcome of 2,31. It 
shows the deviation for each dimension over time. As shown in Figure 3 (b), it is also 
possible to evaluate a summarized CPV for all individuals with an overall outcome of 
0,89 with the corresponding confidence intervals of all subjects for the product, to 
either compare different products of one category or different product categories with 
each other.  
 
Fig. 3. Radar chart of the CPV elements and the deviation over time 
5 
Conclusion and Discussion 
This paper discusses the need for research of analyzing the deviation of product pre-
purchase and post-purchase value judgment. So far, the research was limited only to 
one point of time. In this approach, the research is extended to two separate measur-
ing points to have comparable value judgments of each subject. Furthermore, this 
paper describes the suggested methodology to identify and discover both the deviation 
and the reasons for the different perception of the consumer. A measuring instrument 
was developed in form of a questionnaire, which makes it possible to analyze statisti-
cally data of the product value, both before the purchase and after the utilization phase 
and to compare the results. The approach to the evaluation has been applied in pre-
studies with 19 individuals to assess each individual CPV and a summarized CPV. It 
must be pointed out, that the amount of subjects is not enough to make a valid state-
ment. Therefore, more subjects have to be enlisted in further researches. 
Subsequently, a qualitative research approach is proposed in order to uncover the 
causes for the deviations, so as to express recommendations to companies for future 
Benefit
elements
Cost
elements
Benefit
elements
Cost
elements
0
1
2
3
4
5
6
brand
purchase
price
monetarily costs
of usage
psychological
costs
time
perceived risks
desired
consequences
pragmatic
quality
hedonic
quality
fulfilled product
attributes
service
0
1
2
3
4
5
6
brand
purchase
price
monetarily costs
of usage
psychological
costs
time
perceived risks
desired
consequences
pragmatic
quality
hedonic
quality
fulfilled product
attributes
service
individual pre-purchase
product value
possible individual post-
purchase product value
summarized pre-purchase product value with confidence
intervalls
(a)
(b)

 
Analyzing the Deviation of Product Value Judgment 
773 
product development. Current literature provides various approaches to measure the 
product value. The CPV construct introduced in this paper was chosen to expose the 
different elements on a sufficiently detailed level in order to analyze the deviations 
over time. The quantification of the CPV at two separate measuring points will be 
conducted by means of the methodical approach. Within the context of this investiga-
tion, two measuring points are sufficient to analyze the causes for deviation of prod-
uct value judgment over time. For further researches, it can be considered to extend 
the investigation to more than two measuring points. This allows identifying the de-
veloping customer product values through different stages of the product lifetime. 
Acknowledgments. This paper results from the research project PWuse of the Labor-
atory for Machine Tools and Product Engineering (WZL), RWTH Aachen University, 
Germany. The research project has been funded by the German National Science 
Foundation (Deutsche Forschungsgemeinschaft e.V. DFG). The authors would like to 
express their gratitude to all parties involved. 
References 
1. Graf, A., Maas, P.: Customer value from a customer perspective: A comprehensive review. 
Journal für Betriebswirtschaft 58(1), 1–20 (2008) 
2. Woodall, T.: Conceptualising ’value for the customer’: an attributional, structural and dis-
positional analysis. Academy of Marketing Science Review 12 (2003) 
3. Woodruff, R.B.: Customer Value: The Next Source for Competitive Advantage. Journal of 
the Academy of Marketing Science 25(2), 139–153 (1997) 
4. Zeithaml, V.: Consumer Perceptions of Price, Quality, and Value: A Means-End Model 
and Synthesis of Evidence. Journal of Marketing 52(2-22) (1988)  
5. Eggert, A., Ulaga, W.: Customer perceived value: a substitute for satisfaction in business 
markets? Journal of Business & Industrial Marketing 17(2/3), 107–118 (2002) 
6. Dodds, W.B., Monroe, K.B., Grewal, D.: Effects of Price, Brand, and Store Information on 
Buyers Product Evaluations. Journal of Marketing Research 28(3), 307–319 (1991) 
7. Tam, J.L.: Customer Satisfaction, Service Quality and Perceived Value: An Integrative 
Model. Journal of Marketing Management 20(7-8), 897–917 (2004) 
8. Ramesh Kumar, D.: Consumer behaviour and branding. Concepts, readings and cases. 
Pearson Education, New Delhi (2009) 
9. Tsiotsou, R.: Perceived Quality Levels and their Relation to Involvement, Satisfaction, and 
Purchase Intentions. Marketing Bulletin 16 (2005) 
10. Bruhn, M.: Marketing. Grundlagen für Studium und Praxis, 10th edn. Gabler Verlag / 
Springer Fachmedien Wiesbaden GmbH Wiesbaden, Wiesbaden (2010) 
11. Woodruff, R.B., Gardial, S.: Know your customer. New approaches to understanding cus-
tomer value and satisfaction, 1st edn. Blackwell, Malden (1998) 
12. Sanchez-Fernandez, R., Iniesta-Bonillo, M.A.: The concept of perceived value: a systemat-
ic review of the research. Marketing Theory 7(4), 427–451 (2007) 
13. Wuestefeld, T., Hennigs, N., Schmidt, S., Wiedmann, K.-P.: The impact of brand heritage 
on customer perceived value. der markt – International Journal of Marketing 51(2-3), 51–
61 (2012) 

774 
P. Amini and R. Schmitt 
14. Gardial, S., Clemons, D.S., Woodruff Robert, B., Schumann, D.W., Burns, M.J.: Compar-
ing Consumers’ Recall of Prepurchase and Postpurchase Product Evaluation Experiences. 
Journal of Consumer Research 20(4), 548–560 (1994) 
15. Huber, F., Herrmann, A., Morgan, R.E.: Gaining competitive advantage through customer 
value oriented management. Journal of Consumer Marketing 18(1), 41–53 (2001) 
16. Woodruff, R.B., Cadotte, E.R., Jenkins, R.L.: Modeling consumer satisfaction processes 
using experience-based norms. Journal of Marketing Research 20(3), 296–304 (1983) 
17. Mittal, V., Ross, W.T., Baldasare, P.M.: The asymmetric impact of negative and positive 
attribute-level performance on overall satisfaction and repurchase intentions. Journal of 
Marketing 62(1), 33–47 (1998) 
18. Nelson, P.: Information and Consumer Behaviour. Journal of Political Economy 78(2), 
311–329 (1970) 
19. Darby, M.R., Karni, E.: Free Competition and the Optimal Amount of Fraud. Journal of 
Law and Economics 16(1), 76–88 (1973) 
20. Ford, G.T., Smith, D.B., Swasy, J.L.: Consumer Skepticism of Advertising Claims: Test-
ing Hypotheses from Economics of Information. Journal of Consumer Research 16(4), 
433–441 (1990) 
21. Weiber, R., Adler, J.: Informationsökonomische begründete Typologisierung von Kauf-
prozessen. Zeitschrift für betriebswirtschaftliche Forschung 47(1), 43–65 (1995) 
22. Tversky, A., Simonson, I.: Context-dependent Preferences. Management Science 39(10), 
1179–1189 (1993) 
23. Myers, J.H., Alpert, M.I.: Semantic Confusion in Attitude Research: Salience vs. Impor-
tance vs. Determinance. Advances in Consumer Research 4, 106–110 (1977) 
24. Steiner, M.: Nachfragerorientierte Präferenzmessung, 1st edn. DUV Deutscher Univer-
sitäts-Verlag (2007) 
25. Boztepe, S.: User Value: Competing Theories and Models. International Journal of De-
sign 1(2), 55–63 (2007) 
26. Mitra, D., Goldner, P.N.: How Does Objective Quality Affect Perceived Quality? Short-
Term Effects, Long-Term Effects, and Asymmetries. Marketing Science 25(3), 230–247 
(2006) 
27. Porter, M.E.: Competitive advantage. Creating and sustaining superior performance. Free 
Press, New York (1985) 
28. Ulaga, W., Chacour, S.: Measuring Customer-Perceived Value in Business Markets: A 
Prerequisite for Marketing Strategy Development and Implementation. Industrial Market-
ing Management 30(6), 525–540 (2001) 
29. Lapierre, J.: Customer-perceived value in industrial contexts. Journal of Business & Indus-
trial Marketing 15(2/3), 122–145 (2000) 
30. Hassenzahl, M., Monk, A.: The Inference of Perceived Usability From Beauty. Human-
Comp. Interaction 25(3), 235–260 (2010) 
31. Sánchez, J., Callarisa, L., Rodríguez, R., Moliner, M.: Perceived Value of the purchase of 
a tourism product. Tourism Management 27(3), 394–409 (2006) 
32. Hirschman, E., Holbrook, M.: Hedonic Consumption: Emerging Concepts, Methods and 
Propositions. Journal of Marketing 46(3), 92–101 (1982) 
33. Treacy, M., Wiersema, F.: The discipline of market leaders. Choose your customers, nar-
row your focus, dominate your market. HarperCollins, London (1995) 
34. Cronin, J.J., Brady, M.K., Brand, R.R., Hightower Jr., R., Shemwell, D.J.: A cross-
sectional test of the effect and conceptualization of service value. Journal of Services Mar-
keting 11(6), 375–391 (1997) 

 
Analyzing the Deviation of Product Value Judgment 
775 
35. DeSarbo, W.S., Jedidi, K., Sinha, I.: Customer value analysis in a heterogeneous market. 
Strategic Management Journal 22(9), 845–857 (2001) 
36. Sinha, I., DeSarbo, W.S.: An Integrated Approach toward the Spatial Modeling of Per-
ceived Customer Value. Journal of Marketing Research 35(2), 236–249 (1998) 
37. Gutman, J.: A Means-End Chain Model Based on Consumer Categorization Processes. 
Journal of Marketing 46(2), 60–72 (1982) 
38. Steenkamp, J.-B.E.: Conceptual model of the quality perception process. Journal of Busi-
ness Research 21(4), 309–333 (1990) 
39. Sweeney, J.C., Soutar, G.N.: Consumer perceived value: The development of a multiple 
item scale. Journal of Retailing 77(2), 203–220 (2001) 
40. Sheth, J.N., Newman, B.I., Gross, B.L.: Why We Buy What We Buy: A Theory of Con-
sumption Values. Journal of Business Research 22(2), 159–170 (1991) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 777–786. 
DOI: 10.1007/978-3-642-30817-8_76 
© Springer-Verlag Berlin Heidelberg 2013 
 
Complexity Connectivity Metrics – Predicting Assembly 
Times with Low Fidelity Assembly CAD Models 
Essam Z. Namouz1 and Joshua D. Summers2 
1 Industrial Engineering, Clemson University, Clemson, SC, USA 
2 Mechanical Engineering, Clemson University, Clemson, SC, USA 
{enamouz,jsummer}@clemson.edu 
Abstract. Expanding on previous work to predict assembly times from detailed 
assembly models, low fidelity part models are used in a series of predictive per-
formance experiments.  Results reveal that this tool can predict the assembly 
time of a product to within 40% of the target “as built” time using a high fideli-
ty neural network and a low fidelity CAD model.  The tool is based on struc-
tural complexity, representing the assembly graph as complexity vector of 29 
metrics.  The graphs are automatically compiled from examining part proximi-
ty (interference checks) regardless of the choice of mating constraints used in 
the modeling.  A neural network is then used to build a relationship between 
the complexity vector (input) and the assembly time (output).  Low-fidelity 
models can be used to predict assembly times, thereby supporting earlier inclu-
sion of design for assembly methods in the design process. 
Keywords: Design for Assembly, Assembly Time Estimation, DFA. 
1 
Motivation for Time Estimation 
Design for Assembly (DFA) is a design method used in industry to improve the as-
semblability of a product with the ultimate goal of reducing manufacturing costs.  
With increasing manufacturing costs, an interest in DFA has emerged due to the  
assembly phase in product development accounting for approximately 50% of the 
manufacturing time and 20% of the manufacturing cost [1–8]. Furthermore, approx-
imately 70% of the total product cost is determined during the early stages of design, 
motivating the need for DFA tools that can support product development throughout 
the design process [9, 10].  
Assembly time estimation tools, within the larger design for assembly method, 
have been developed for predicting the assembly time of a product [5, 8, 11]. Time 
estimate tools do not support assembly time estimation in the early stages of design as 
detailed information about the parts, assembly sequence, and assembly structure are 
required. This information is often not determined until the embodiment or detail 
design phase of the design process. The majority of these assembly time estimation 
tools are used primarily to estimate the assembly benefit of a design change to an 
existing product. This paper focuses on the development of an extended complexity 

778 
E.Z. Namouz and J.D. Summers 
connectivity assembly time estimation [12–14] method based on information retrieved 
from low fidelity CAD models in the conceptual design phase.   
1.1 
Connectivity Complexity Method 
The term complexity is used in many disciplines all with different interpretations of 
the definition [15–17]. For this research, the term complexity will be used to describe 
amount of information required to describe a system comprised of more than one 
component [15, 18]. Previous research developed a set of complexity metrics to cap-
ture the connectedness of parts within a system [12, 14, 19]. The connectivity method 
uses the complexity metrics as the input vector to a historical-based prediction model 
to estimate assembly times of a product [12–14]. The complexity connectivity method 
uses 29 graph-based complexity metrics of an assembly[14, 19]. 
2 
Low Fidelity CAD Model Assembly Time Estimate: 
The Experiment 
Previous work has focused on estimating assembly times from detailed component 
and assembly models. This work evaluates the potential of using components 
represented at lower levels of detail (conceptual models or low-fidelity models).  
While the exact dimensions and features of the components are not know, the general 
system architecture and layout is captured [20]. The form of the individual compo-
nents are developed throughout the design process to create a completed CAD model 
with working drawings in the detailed design stage [20]. For clarity, low-fidelity 
models are those that are found in conceptual design and high-fidelity models are 
found in detailed design phases. 
This experiment explores the use a modified complexity connectivity method to es-
timate the assembly time of models in the conceptual design phase. The estimated 
assembly time of the conceptual models is compared to the estimated assembly time 
of the complete models using the same modified complexity connectivity method. 
The following research questions are answered: 
• What is the predictive power of ANN trained on detailed models to predict detailed 
models? 
• What is the predictive power of ANN trained on detailed models to predict low 
fidelity models? 
• What is the predictive power of ANN trained on low fidelity models to predict low 
fidelity models? 
• What is the predictive power of ANN trained on low fidelity to predict high  
fidelity? 
2.1 
Set of Models 
The experiment used a total of thirteen products (Table 1) to compare the estimated 
assembly time of high-fidelity models and low-fidelity models. The models were used 

 
in previous work and were
neering existing products o
last three models are withhe
Table 
Common Name 
Stapler 
Flash Light 
Ink Pen 
Pencil Compass 
Indoor Electric Grill 
Solar Yard Light 
Table Vise 
Drill 
Shift Frame 
Vegetable Chopper 
Computer Mouse 
Piston Assembly 
3 Hole Punch 
2.2 
Reducing Model Fi
Low-fidelity CAD models 
the designer before they are
work, the high-fidelity mod
els in the conceptual design
To do this, each part incl
feature. In SolidWorks the 
order in which those feature
ty of the parts, the feature
It should be noted that if
Complexity Connectivity Metrics 
 created by multiple designers by physically reverse en
or downloading models from the public domain [13]. T
eld for testing purposes. 
1. Products Used in Training and Testingt 
Training/Testing 
CAD Model Image 
Testing 
Not included for brevity 
Testing 
 
Testing 
Not included for brevity 
Training 
 
Training 
See Fig. 1 
Training 
Not included for brevity 
Training 
 
Training 
Not included for brevity 
Training 
Not included for brevity 
Training 
Not included for brevity 
Training 
Not included for brevity 
Training 
Not included for brevity 
Training 
Not included for brevity 
idelity  
are difficult to define and are often not distinctly saved
e evolved to more detailed higher fidelity models. For 
dels were reduced in fidelity to represent low-fidelity m
n phase.  
luded in an assembly model was reduced to its lowest le
feature tree stores the features used to create a part and 
es were created. To decrease bias in the reduction of fid
e tree was reduced to the top level feature for each p
f a multiple designers create the same part, a differ
779 
ngi-
The 
d by 
this 
mod-
evel 
the 
deli-
part.  
rent  

780 
E.Z. Namouz and J.D
conceptual model may resu
reserved for future work.  
As an example, the first
Extrude1). Next, a swept ex
shaft of the bolt. An additio
and then an extruded cut (C
head. Starting from the bott
followed by Boss-Extrude2
of a conceptual model for a
Table 2. Reduction of Fide
 
Cut-Extrude1 
Bo
 
This removes detail from
of the product simulating 
process. The indoor electric
an assembly of the low-fid
transformation, precluding 
a mate-independent method
interference checks. 
 
Fig. 1. Transformation of E
2.3 
Artificial Neural Ne
The artificial neural networ
agation network [12, 13]. T
D. Summers 
ult. This uncertainty is not the focus of this research an
t feature used to create a bolt is an extruded shaft (Bo
xtrusion (Sweep1) is used to create the threads around 
onal extrude (Boss-Extrude2) is used to create the bolt h
Cut-Extrude1) is used to cut the hex in the top of the b
tom of the feature design tree, the Cut-Extrude1 is delet
2 and Sweep1 leaving only the initial extrude as an exam
a bolt (see Table 2). 
elity of a Bolt Complete Model to Create a Low Fidelity Mode
oss-Extrude2 
Sweep1 
Boss-Extrude1 
m the parts in the CAD model, leaving a low-fidelity mo
a model created in the conceptual phase of the des
c grill (Fig. 1) is similarly reduced from a detailed mode
elity part models. Mating relationships may be lost in 
the use of previous graph generation tools [21]. Therefo
d for generating the connectivity graphs is used based
Electric Grill from High Fidelity Model to Low Fidelity Model
etwork Generation 
rk (ANN) used for this research is a supervised back pr
The ANN is trained by providing a set of input vectors 
d is 
oss-
the 
head 
bolt 
ted, 
mple 
el 
 
odel 
sign 
el to 
this 
ore, 
d on 
 
l 
rop-
and 

 
Complexity Connectivity Metrics 
781 
a set of target values. The ANN then creates a relationship between the input values 
and the target value. In this case, the complexity vector of 29 metrics is the input vec-
tor and the assembly time of the product will be used as the output. Once an ANN is 
trained, a new complexity metric is input and the ANN provides an assembly time. 
2.4 
Experimental Sets 
Two separate neural networks are created and compared. The first ANN uses the 
complexity vector of the high-fidelity models as input and assembly times as the tar-
gets. The second ANN uses the complexity vectors of the low-fidelity models as the 
training inputs and the same assembly times as target times. This approach is used to 
test the ability to train a neural network to find a relationship between low-fidelity 
complexity vectors and product assembly times. Each ANN is used to predict the 
assembly time of a test data set (three products) using the high-fidelity and low-
fidelity models. The experimental sets are summarized in Table 3. 
Table 3. Experiment Design Sets 
Set Number 
ANN Trained on: 
Test Set Type: 
1 
High Fidelity Models Vectors 
High Fidelity Model Test Vector 
2 
High Fidelity Models Vectors 
Low Fidelity Model Test Vectors 
3 
Low Fidelity Model Vectors 
High Fidelity Model Test Vector 
4 
Low Fidelity Model Vectors 
Low Fidelity Model Test Vectors 
3 
Conceptual Model Time Estimate Results 
After the two ANN are trained, the input vectors are passed back in to the neural net-
work to gain a qualitative assessment of ANN fit to the training set. One shortcoming 
with ANNs is the potential for overtraining, limiting the ability of the ANN to extra-
polate to new data sets [22–24]. The percent error is calculated as the normalized 
difference from the target time (see Eqn. 1). A positive percent error indicates that the 
predicted time was greater than the target time, and a negative percent error indicates 
that the predicted time is less than the target time. 
 
Percent Error = (Predicted Time - Target Time)/Target Time 
(1) 
The ANNs are able to estimate the training set assembly times within 70% of the 
target time, but visually do not appear to be over fit to the training set data (see Fig. 
2). Previous research offers techniques to prevent ANN over fit and improve perfor-
mance of ANN by varying ANN parameters. As the focus of this paper is to demon-
strate the potential to use ANN to predict assembly times of low-fidelity models, the 
improvement in design of the ANN itself is reserved for future work. 
To test the performance of the two ANNs in predicting the assembly times, com-
plexity vectors of three products (stapler, flash light, and ink pen) not used in the 
training are used for testing. For each of the test products the high fidelity and low 
fidelity graph complexity vectors were calculated and used as the input to both ANNs 
trained (high fidelity and low fidelity).  

782 
E.Z. Namouz and J.D. Summers 
 
Fig. 2. Training Set Percent Error from Target Time 
The target time, the predicted time, and the percent error for each of the three test 
cases are presented in Table 4. Each ANN predicted an assembly time greater than the 
target time for the test cases except for the high-fidelity ANN for the stapler. The test 
products varied in target assembly times from 34 seconds to 123 seconds. Additional 
test cases with a larger range of assembly times are needed to determine if the ANN 
time estimate accuracy is dependent on the assembly time or the complexity of the 
product being studied, but this is reserved for future work. 
Table 4. Test Products Results Summary 
Fidelity Levels 
Predicted Time [s] (Percent Error) 
ANN 
Test Assembly 
Stapler 
Flash Light 
Ink Pen 
High 
High 
115.84 (-6%) 
107.65 (43%) 
54.78 (59%) 
High 
Low 
119.43 (-3%) 
91.79 (22%) 
46.41 (35%) 
Low 
High 
157.19 (27%) 
109.89 (46%) 
72.36 (110%) 
Low 
Low 
198.30 (61%) 
95.19 (26%) 
51.65 (50%) 
Target Time [s] 
123.51 
75.40 
34.40 
 

 
Complexity Connectivity Metrics 
783 
The percent error from the target time was calculated for each of the outcomes (see 
Fig. 3, Fig. 4, and Fig. 5). 
 
Fig. 3. Test Case Results for Stapler 
 
Fig. 4. Test Case Results for Flash Light 

784 
E.Z. Namouz and J.D. Summers 
 
Fig. 5. Test Case Results for Ink Pen 
The results from the analysis of the test cases indicate that both of the ANNs (high 
fidelity and low fidelity trained) can predict an assembly time to within 120% inde-
pendent of the type of input vector used.  However, the low fidelity ANN was the 
generally the worst at predicting assembly time when presented with a high fidelity 
input vector.  The best combination of ANN and input vectors, based on the lowest 
percent error for all three test cases is the high fidelity ANN being provide low fideli-
ty input vectors.  The focus of this research is if an ANN can predict the assembly 
time of a low fidelity model.  Both the high fidelity ANN and the low fidelity ANN 
were able to predict the assembly time of the conceptual model to within 120% of the 
target time.  There was not sufficient evidence in this study to determine if there is a 
significant difference in assembly time estimation between the high fidelity and low 
fidelity ANN when using the low fidelity input vectors. The training sets and the test 
cases were limited in number and could potentially influence the results.  The results 
of this study serve as motivation that there is potential to use an ANN to estimate the 
assembly time of models early in the design process. 
4 
Conclusions and Future Work 
The ability of a neural network to create a relationship between input vectors and 
output vectors depends on the training set provided.  The larger the training set (to a 
degree to avoid over fitting), the better the neural network is at predicting the output.  
While the input vectors used to train the neural network in this research are limited to 
ten training products, future work includes increasing the training set to determine if 
the assembly time estimation can be further improved. The number of test products 

 
Complexity Connectivity Metrics 
785 
will also be increased to ensure the trends in this limited population are valid.  This 
paper presents the preliminary findings that must be extended with more validation. 
The findings of this study suggest that the high fidelity assembly model based 
neural networks provide good prediction tools for estimating assembly time for both 
high fidelity and low fidelity conceptual models. There was not significant evidence 
to suggest that the high fidelity neural network or the low fidelity neural network can 
better predict assembly time. It is clear however that a neural network trained on low 
fidelity models should not be used to predict the assembly time of high fidelity mod-
els.  Ultimately, this tool shows promise for providing engineers in conceptual stages 
of product development with useful information about production costs early in the 
design process.  The accuracy of these predicted times are sufficient to provide justi-
fication for alternative engineering selection decisions at early stages. 
References 
1. Lit, P.D., Delchambre, A., Henrioud, J.-M.: An integrated approach for product family and 
assembly system design (2003) 
2. Khan, Z., Boothroyd, G., Knight, W.: Design for Assembly. Assembly Automation 28, 
200–206 (2008) 
3. Barnes, C., Dalgleish, G., Jared, G.: Assembly sequence structures in design for assembly. 
Assembly and Task, 164–169 (1997) 
4. Zha, X.F., Lim, S.Y.E., Fok, S.C.: Integrated intelligent design and assembly planning: a 
survey. The International Journal of Advanced Manufacturing Technology, 664–685 
(1998) 
5. Boothroyd, G., Dewhurst, P., Knight, W.A.: Product Design for Manufacture and Assem-
bly. CRC Press, Boca Raton (2011) 
6. Pandit, A., Siddique, Z.: A tool to integrate design for assembly during product platform 
design. Presented at the (2004) 
7. Tavakoli, M.S., Mariappan, J., Huang, J.: Design for Assembly Versus Design for Disas-
sembly: A Comparison of Guidelines. Presented at the (2003) 
8. Maynard, H., Stegemerten, G.J., Schwab, J.L.: Methods Time Measurement. McGraw-
Hill, New York (1948) 
9. Kalpakjian, S., Schmid, S.R.: Manufacturing Processes for Engineering Materials. Pearson 
Education (2008) 
10. Zha, X.F., Lim, S.Y.E., Fok, S.C.: Integrated intelligent design and assembly planning: A 
survey. The International Journal of Advanced Manufacturing Technology 14, 664–685 
11. Ohashi, T.: Extended Assemblability Evaluation Method (AEM). JSME International 
Journal: Series C - Mechanical Systems, Machine Elements, and Manufacturing 45, 567 
(2002) 
12. Miller, M., Mathieson, J., Summers, J.D., Mocko, G.M.: Representation: Structural Com-
plexity of Assemblies to Create Neural Network Based Assembly Time Estimation Mod-
els. In: International Design Engineering Technical Conferences and Computers and In-
formation in Engineering Conference, Chicago, IL, pp. DETC2012–71337 (2012)  
13. Owensby, E., Namouz, E.Z., Shanthakumar, A., Summers, J.D.: Representation: Extract-
ing Mate Complexity from Assembly Models to Automatically Predict Assembly Times. 
In: International Design Engineering Technical Conferences and Computers and informa-
tion in Engineering Conference, Chicago (2012) 

786 
E.Z. Namouz and J.D. Summers 
14. Mathieson, J.L., Wallace, B.A., Summers, J.D.: Assembly Time Modeling Through Con-
nective Complexity Metrics. In: 2010 International Conference on Manufacturing Automa-
tion (ICMA), pp. 16–23 (2010) 
15. Ameri, F., Summers, J.D., Mocko, G.M., Porter, M.: Engineering design complexity: an 
investigation of methods and measures. Research in Engineering Design 19, 161–179 
(2008) 
16. Lindemann, U., Maurer, M., Braun, T.: Structural Complexity Management: An Approach 
for the Field of Product Design. Springer, Berlin (2009) 
17. Suh, N.: Complexity: Theory and Applications. Oxford University Press, New York 
(2005)  
18. Braha, D., Maimon, O.: The Measurement of a Design Structural and Functional Complex-
ity. IEEE Transactions on Systems, Man, and Cybernetics (Part A - Systems and Hu-
mans) 28, 527–535 (1998) 
19. Mathieson, J.L., Summers, J.D.: Complexity Metrics for Directional Node-Link System 
Representations: Theory and Applications. In: Proceedings of the ASME IDETC/CIE 2010 
(2010)  
20. Pahl, G., Beitz, W., Wallace, K.: Lucienne Blessing: Engineering Design: A Systematic 
Approach. Springer-Verlag London Limited, London (2007) 
21. Owensby, J.E., Namouz, E., Shanthakumar, A., Summers, J.D.: Representation: Extracting 
Mate Complexity from Assembly Models to Automatically Predict Assembly Times. In: 
International Design Engineering Technical Conferences and Computers and Information 
in Engineering Conference, Chicago, IL, pp. DETC2012–70995 (2012) 
22. Blackard, J.A., Dean, D.J.: Comparative accuracies of artificial neural networks and dis-
criminant analysis in predicting forest cover types from cartographic variables. Computers 
and Electronics in Agriculture 24, 131–151 (1999) 
23. Sethi, I.K.: Entropy nets: from decision trees to neural networks. Proceedings of the 
IEEE 78, 1605–1613 (1990) 
24. Francis, L.: Neural Networks Demystified. Casualty Actuarial Society Forum, 253–320 
(2001) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 787–794. 
DOI: 10.1007/978-3-642-30817-8_77 
© Springer-Verlag Berlin Heidelberg 2013 
 
The Virtual Reality Lab as a Synthetic Environment: 
From Strategic Approach to Practical Implement  
Roy Damgrave, Eric Lutters, and Fred J.A.M. van Houten  
Laboratory for Design, Production and Management, University of Twente 
Drienerlolaan 5, Enschede, 7500AE, The Netherlands 
{r.g.j.damgrave,e.lutters,f.j.a.m.vanhouten}@utwente.nl 
Abstract. The Virtual Reality laboratory (VR-Lab) at the University of Twente 
facilitates multi-stakeholder decision making processes. Using Synthetic Envi-
ronments (SE) to facilitate collaboration and to visualize consequences and de-
pendencies of choices, the lab stimulates optimal use of available expertise. The 
VR-lab embodies a flexible set of VR tools, software and working methods; 
therefore adequate facilitation of preparation and configuration of use is essen-
tial. For this purpose, a roadmap facilitates the attuning of the intents of (poten-
tial) user and the capabilities of the provider of the SE. This publication outlines 
the use of the VR-lab as a Synthetic Environment, as well as the preparations 
that a required to make that usage purposeful and efficient. 
Keywords: synthetic environments, virtual reality, multi-stakeholder, decision 
making. 
1 
Introduction 
Product development can be seen as a process of constantly making choices, often in 
consultation with multiple stakeholders from different fields of expertise. Within this 
team setting, developers have a need for support in communicating and for visualizing 
their problems and solutions, especially for gaining insight in the consequences and 
dependencies of choices.  
This publication provides an overview of the approach, development, use and out-
look of the Virtual Reality laboratory (VR-Lab) at the University of Twente [1]. This 
multi-stakeholder decision making laboratory facilitates the collaboration and deci-
sion making process in product development projects. 
1.1 
Scope 
Within the field of product development, the need for support and facilitation of the 
various product development phases is obvious. In general, product developers also 
show enthusiasm for approaches that offer adequate support. Within the VR lab of the 
University of Twente, product developers are encouraged to address and optimize the 
development processes they employ in daily practice. The scope and emphasis is on 
supporting the decision-making processes that involve multiple stakeholders who aim 

788 
R. Damgrave, E. Lut
to understand the implicati
these processes, many cho
This immediately leads to 
consequences thereof. Thes
involved. The VR-lab aims
of all decisions, and to pre
Furthermore, for the design
quickly, with minimal adju
and schedule. This implies
easy to adapt to different sit
The VR-lab gives desig
when it comes to cooperat
stages of the development
stakeholders to define a co
plicit) problems that togeth
the solution generation pha
scenarios using many type
insight into the consequenc
use situation and compositi
(large) group of stakeholde
and when they approach an
expertise. In this manner, th
data are immediately releva
sess the collaboratively co
this, it is essential that ever
for him; therefore it must b
that cannot be processed. 
capabilities of the VR-lab 
efficiently addressing inform
the overall solution (path). 
Fig. 
tters, and F.J.A.M. van Houten 
ions of the many decisions that have to be made. Wit
oices have to be made that involve multiple stakehold
complex dependencies between the choices made and 
se consequences are not always clear for every stakehol
s to give insight in the relations between and dependenc
esent them in the most purposeful and adequate mann
n process, it is important that such support can be delive
ustment to the development process, the project plann
s that the configuration of support should be flexible 
tuations. 
gners the ability to rely on support in the design proc
tion (figure 1). Such support is available from the earl
t process. In these stages, it is desirable for the vari
ommon vision and understanding of the (explicit and 
her constitute the basis of the development project. Dur
ase, the lab offers the possibility to clarify ideas for fut
s of visualisations and interactions. This also gives dir
es of possible choices in areas such as safety, constructi
ion. The lab adds the most added value to a process whe
ers is interacting with one and the same set of informati
nd handle this set in a way that best suits their own perso
he interactions and adjustments of other stakeholders to 
ant and can be made visible. It is therefore possible to 
omposed solution (path) based on individual expertise
ry stakeholder perceives the information that is meaning
be possible to omit redundant information and informat
In other words, the working methods, together with 
should enable individual stakeholders in effectively 
mation that is relevant for his perspective, in the contex
1. Impression of projects in the VR-lab 
thin 
ders. 
the 
lder 
cies 
ner. 
ered 
ning 
and 
cess 
liest 
ious 
im-
ring 
ture 
rect 
ion, 
en a 
ion, 
onal 
the 
as-
. In 
gful 
tion 
the 
and 
xt of 
 

 
2 
Methodology 
A product development pro
problems and solutions. To
stakeholders share an align
actual design problem, ren
holders. With this, everyon
same way by all participant
ent backgrounds and exper
even end-user can be involv
possibility to directly and e
of misinterpretations is min
cess by being provided with
input, the interests and bias
be depicted, in which this 
Based on all supplied inp
definition, solution generati
The methodology and w
low for structured, transpar
uct development lifecycles
and serious games to what-
compared to a workshop, i
addressed as utensils in a to
the following aspects shoul
composite approach: 
• Hardware 
• Software 
• User 
• Environment 
• Information 
• Knowledge 
• Methodology 
• Resources 
• Working methods 
F
The Virtual Reality Lab as a Synthetic Environment 
ocess can be considered to be a constant trade-off betw
o support and facilitate this trade-off, it is important that
ned vision on both. This starts with the definition of 
dering a shared understanding between the various sta
ne can assume that the defined problem is interpreted in 
ts. This synchronized start creates a situation where dif
rtise of the various participants come together and wh
ved in the process. By providing each stakeholder with 
explicitly communicate in his or her preferred way, the r
nimized. Each stakeholder should be involved in the p
h an optimized method to contribute. Based on the suppl
s of each stakeholder within each stage of the process 
preference is strongly related to the actual circumstanc
ut purposeful and continuous iteration between probl
ion and the assessment is feasible.  
working methods that constitute the basis for the VR-lab
rent and straightforward use of different tools during pr
. Usage of these tools ranges from scenario developm
if design and information management. The VR-lab can
in which the available (VR) tools are exposed and can
oolbox (figure 2). In order to make optimal use of the l
ld be taken into account, and should also be reflected in 
Fig. 2. Selection of multiple VR tools 
789 
ween 
t all 
the 
ake-
the 
ffer-
here 
the 
risk 
pro-
lied 
can 
ces. 
lem 
b al-
rod-
ment 
n be 
n be 
lab, 
the 
 

790 
R. Damgrave, E. Lutters, and F.J.A.M. van Houten 
The combination of the abovementioned methodology, the Virtual Reality lab and the 
set of stakeholders together constitute a so-called Synthetic Environment (SE). This 
artificial environment represents an alternative reality, which acts as commensurable 
to a real environment as required. To allow for natural behavior, enabling intuitive 
use, the interaction possibilities provided in the synthetic environment correspond to 
real world interactions as adequate as is appropriate. This alternative reality uses both 
virtual and augmented reality techniques to allow the various stakeholders to interact 
with it (e.g. make adjustments to it) in a way that is easier, more transparent, more 
purposeful and more controllable than in reality, while requiring less effort. This 
makes it possible to quickly evaluate multiple configurations, and also to review the 
consequences of possible choices. A SE can be adjusted while it is in use in real time 
and it therefore allows stakeholders to deal with design information in an interactive 
way. Therefore, it is easier to evaluate features and experiences under a wide variety 
of circumstances. As a result, the stakeholders become more conscious of their deci-
sions and the related interdependencies. This is mainly because the information is 
presented to every stakeholder in an understandable format/way that is independent 
from the stakeholder’s background or expertise.   
These SE’s prove their usefulness best in development processes of new or com-
plex products, involving many stakeholders with various backgrounds. They can be 
used in nearly all phases of development processes; especially in the early stages SE’s 
may well help to describe current and future scenarios.  
The main characteristics and objectives of SE’s are to ensure validity of decisions 
by using realistic interactions, to simulate effects in a familiar context in order to 
achieve a realistic image of the future environment, and to present corresponding 
images of future situations to facilitate negotiations about consequences of their char-
acteristics [2]. 
In summary, Synthetic Environments can be depicted as well-considered composi-
tions of possible future environments, used to give more insight into the consequences 
of the choices in such compositions. To enable stakeholders to experience this envi-
ronment as veracious as possible, extensive use is made of virtual and augmented 
reality technologies. Bringing such tools to industrial practice becomes increasingly 
relevant, as the cost of hardware and software tend to decrease. Therefore, the ability 
to use SE’s in smaller companies increases continually. 
3 
The VR-Lab as a Synthetic Environment 
As with every workshop, the end result that can be achieved depends on the crafts-
manship and expertise of the people using it; therefore, it is essential to adequately 
align the users’ requirements, functional specifications and capabilities with the ap-
propriate configuration of the lab and its tools. In other words, no fixed configuration 
can cater for all approaches, but a flexible and modular set of equipment can allow for 
quick creation of new settings.   
The basic approach, as mentioned in the previous section, leads to an atmosphere 
in which initially a conjoint view of the problem, its setting as well as a better under-
standing of dependencies between all stakeholders is realized. From that systematic 
beginning, solution paths can be explored effectively and efficiently by employing 

 
working methods and tools
realization of a Synthetic E
fectively provide substantia
between designers who wan
the environment. 
3.1 
Approach  
The flexible layout and com
variety of projects. Howev
ples as concerns the compo
workshop, it is important 
elements that together will 
tion generator, but it can b
expertise of all attendees. 
three categories of aspects t
• Techniques; new techniq
• Tools: techniques can be
applicable in a specific s
• Solutions: in combining 
Fig. 3. Rel
The Virtual Reality Lab as a Synthetic Environment 
s in the lab. It is without doubt that the development 
Environment requires extensive preparation in order to
al and useful results. This preparation often is a cooperat
nt to use a SE, and the host of the facility that can prov
mposition of the VR-lab provides capabilities within a w
ver, this flexibility of the configuration needs clear prin
osition of attributes. Given the comparison of the lab wit
to adequately address the preparation in the selection
constitute an SE. In itself, the lab is not an automatic so
be a facilitator to make effective and efficient use of 
In this respect, it is important to distinguish between 
that together constitute the SE [3], as illustrated in figure
ques allow for new possibilities to acquire or present dat
e used and combined in virtual reality tools to make th
cenario. 
multiple tools, a virtual reality facility is created. 
 
lation between techniques, tools and solutions 
791 
and 
 ef-
tion 
vide 
wide 
nci-
th a 
n of 
olu-
the 
the 
e 3: 
ta. 
hem 

792 
R. Damgrave, E. Lut
In order to properly prepar
beforehand what the condit
the technology used. Furthe
cies of the different stakeho
To be able to make optim
cate the characteristics of s
tive of the facilitator, it is im
but that information can on
value a SE can have for the
a way that the appropriate
structured way so that the d
environment. This is essent
the desired goal.  
In order to prepare the a
and explain the possibilitie
nication and library tool [4
ess between the facilitator 
information is needed to ex
should be, interrelated (figu
how to structure all the inf
provides an overview of the
Fig. 4
tters, and F.J.A.M. van Houten 
re for the usage of a SE, it is necessary to determine
tions are as concerns the desired goal, but also as conce
ermore, the initiation of the desired use and the depend
olders should be available on beforehand. 
mal use of the VR-lab as a SE, it is essential to commu
such an environment with future users. From the persp
mportant to get the right information from the future us
nly be given by those users if they understand what ad
em. This preparation should therefore be organized in s
e information is gathered from the prospective users i
data can directly be used by the facilitator to assemble 
tial to make it possible to combine the right tools based
assembly of a Synthetic Environment, and to communic
s to potential future users, a roadmap is used as a comm
], [5]. The roadmap is a guideline for the consulting pr
and the potential user; it guides both in determining w
xecute the process and defines how the information is
ure 4). Furthermore, the roadmap provides a blueprint
formation generated in the start-up phase. Additionally
e current state of preparation. 
. Blueprint for developing and using a SE 
e on 
erns 
den-
uni-
pec-
ers, 
ded 
uch 
in a 
the 
d on 
cate 
mu-
roc-
what 
, or 
t on 
y, it 
 

 
The Virtual Reality Lab as a Synthetic Environment 
793 
4 
Projects 
Over the years, the VR lab has served as facilitator of Synthetic Environments for a 
large variety of projects in several phases of development. It is in the diversity of the 
projects that the flexibility of the use of the lab is evident. The diversification includes 
the number of stakeholders involved, the phase of the project, the equipment and the 
time required. But especially the content and objective of such projects is never the 
same, and does not allow for a one-to-one copy to other projects. Although the focus 
of the VR-lab is mainly on product development processes, it often shows that the 
methodology and approach is also applicable to decision-making processes in general. 
The core value mainly lies in the support of multiple stakeholders in creating and 
balancing multiple choices/decisions. 
The researchers of the VR lab perceive clear similarities in the way of approach for 
the different projects. Because different devices and techniques are used in multiple 
ways, the VR-lab is a good example of a facilitator for SE’s. Since the lab contains a 
selection of commercially available and custom made hardware and software, combi-
nations of hardware and software that are frequently used together gradually emerge 
as a kind of higher-level building blocks. Every combination of VR-lab equipment 
and intended use in a SE allows the researchers to assess how successful the combina-
tion is or if the combination proposes any challenges. In any case, the gained experi-
ence provides input for use in future projects. In addition, during project execution, 
demands for adjustments and additions to the SE can be addressed. For example, ad-
ditional support may be necessary in the process, which provides directions for new 
equipment or methods that need to be developed or purchased in the near future.  
Not only the results of composing a SE often shows parallels for the researchers of 
the VR-lab, also the similarities in the underlying activities in establishing the SE 
provide ample guidance. To optimize stakeholder involvement in this, it is important 
to make good use of dedicated roadmaps. This makes it easier for each stakeholder to 
indicate and clarify the goal of the process, and makes it easier for the facilitator to 
come up with the most appropriate configuration of equipment, software and working 
methods. In addition, it gives all parties a sense of suitability and therefore brings 
more confidence in the choices of support and feedback methods in the environment. 
The roadmap simultaneously is the checklist and the guideline for the arrangement of 
a SE. 
Future projects of the VR lab focus more on integrating different types of da-
ta/information sources. This not only results in increased integration of different types 
of documents, images and simulations, but mainly in using multiple types of 3D mod-
els in the environment. In addition to static virtual 3D models, the use of dynamic 3D 
models is explored and exploited, even in combination with e.g. tactile and haptic 
models. In addition, the possibilities of integrating multiple research disciplines dur-
ing the exploitation of an SE are expanded. During the development and assessment 
of a product or environment, more information can be obtained about the potential 
risks, the expected maintenance costs, life cycle analysis or guidelines for suggested 
improvements. 

794 
R. Damgrave, E. Lutters, and F.J.A.M. van Houten 
In addition, the integration of 3D projected images, such as holograms, will be ex-
panded further. The introduction of such new techniques that make it possible to view 
voluminous 3D models with multiple stakeholders at the same time from different 
angles provides easier understanding of future products and environments. As such, 
the transition from virtual to real, and vice versa, will gradually dissolve. 
5 
Conclusion 
Using the VR-lab as a flexible toolbox has the inherent consequence that there is a 
continuous need and possibility to add new or improved tools. On the one hand this 
may seem to hamper the trouble-free employment of SE’s in everyday practice, but 
more important, it also provides insight in what is expected in future tools. After 
every single project in the VR-lab, the design and implementation of the preparation 
roadmap is extended and improved, and is better aligned with the experience gained. 
Also, the range of available (VR) tools is constantly increasing, whereas the price of 
hardware and software is decreasing. This ensures that the use of Synthetic Environ-
ments for smaller project groups increasingly comes within reach. Simultaneously, it 
allows the VR-lab to be applicable and useful for a wider audience. The experiences 
of previous projects in the VR-lab also challenge the researchers to define more gen-
eral implementable tools; at the same time it offers more opportunities to customize 
and personalize them before or even during use. 
References 
1. Virtual Reality Laboratory Twente, http://vrlab.ctw.utwente.nl 
2. Dankers, W., Lutters, E.: A Tool for Preparing Trans-National Access to High Level Visua-
lisation Facilities. In: International Conference on Competitive Manufacturing (2013) 
3. Damgrave, R.G.J., Lutters, D., Thalen, J.: Selecting Virtual Reality Tools in relation with 
their use context. In: Enabling Manufacturing Competitiveness and Economic Sustainabili-
ty, pp. 269–274. Springer, Heidelberg (2012) 
4. Miedema, J.: Synthetic Environments in Design Processes. PhD. Thesis, University of 
Twente, Enschede, The Netherlands (2010) 
5. Schutte, C.S.L., Du Preez, N.D.: A comparative study about the formal design life cycle of 
the integrated knowledge network to support innovation. In: COMA 2010 International 
Conference on Competitive Manufacturing, Wallenberg Research Centre @ STIAS, pp. 
327–334. Stellenbosch University, Stellenbosch(2010) 
 
 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 795–804. 
DOI: 10.1007/978-3-642-30817-8_78 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Tool Proposition to Support Multidisciplinary 
Convergence in Immersive Virtual Environment: 
Virtusketches  
Ahmad Al Khatib, Morad Mahdjoub, Jean-Bernard Bluntzer,  
and Jean-Claude Sagot 
Université de technologie Belfort-Montbéliard, Laboratoire SeT,  
Rue Thierry Mieg, Belfort, 90010, France 
{ahmad.al-khatib,morad.mahdjoub,jean-bernard.bluntzer, 
jean-claude.sagot}@utbm.fr 
Abstract. Companies need to improve product use value thanks to ergonomics 
integration into product design process. This could be achieved by collaboration 
between ergonomists and mechanical designers. However, these actors have 
different methods, representations and tools making difficult to carry out the 
convergence between them. Thus, the aim of this paper is to introduce a new 
tool, Virtusketches, to improve communication between ergonomists and me-
chanical designers during convergence phases of a human-centered design 
process. Virtusketches is based on linking virtual reality (VR) technologies with 
2D sketching and annotation as a tool that could be used by all the actors in-
volved in design process regardless of their specificity. A case study is pre-
sented to test the tool during a convergence step of an automotive design 
project.  
Keywords: Product Design Process, Collaborative Engineering, Ergonomics 
Integration, Intermediary Objects, Virtual Reality, 2D Sketching.  
1 
Introduction 
Nowadays, innovation is a key driver to improve competitiveness and business perfor-
mance in industrial companies. This innovation could be achieved by considering use 
value into product design process [1].  Indeed, use value is a component of the product 
global value. Recently, it became a product differentiation factor in the market.  
Ergonomics is one science dealing with use value. It applies information about the 
human behavior, skills and limitations in the product design. Hence, ergonomics inte-
gration into product design process is one way to improve product’s use value and 
consequently product’s competitiveness [2]. In this context, Broberg has defined three 
approaches to integrate ergonomics into product design process [3].  In the first ap-
proach, ergonomics integration could be done by transferring ergonomics knowledge 
and skills to engineers. To this end, several solutions could be found such as the ergo-
nomics experts systems [4].  The second approach focuses on organizational factors. 

796 
A. Al Khatib et al. 
 
For instance, in this approach, the role of the ergonomist as a member of the design 
team during design process can be enhanced; an ergonomist can take the role of an 
expert, a facilitator [5], a political agent [6] or a political reflective navigator [7]. The 
third approach focuses on the extra-organizational factors. These are factors outside 
the organization as the regulatory aspects (e.g. European Union’s directive on ma-
chines).  
In accordance with the second approach of Broberg, we focus our work on the col-
laboration between ergonomists and mechanical designers for integrating ergonomics 
into product design process. This collaborative work is mainly achieved during suc-
cessive iterative convergent phases of the design process. Indeed, convergence is 
considered as an activity that relies on exchanges between different actors concerning 
several design alternatives. However, ergonomists and mechanical designers have 
different intentions, backgrounds and circumstances making it difficult to carry out 
the convergence between them [8].  
Thus, the aim of this paper is to introduce a new tool, Virtusketches, to improve 
communication between ergonomists and mechanical designers during convergence 
phases of a human-centered design process. This tool is based in the fact of linking 
VR technologies with 2D sketching and annotation as a tool that could be used by all 
the actors involved in design process regardless of their specificity.  
2 
Multidisciplinary Convergence  
The overall design process is convergent, but it contains phases of both divergence 
and convergence [9]. While the objective of divergent phases is to generate several 
ideas and solutions to solve the design problem, the objective of convergent phases is 
to reduce the number of proposed solutions and to lead to one detailed solution.  
Convergent phases are considered as phases of exchanges and argumentation be-
tween different actors concerning several design alternatives in order to reach a satis-
fying joint decision. During these phases, the actors transmit, communicate, propose, 
criticize and share ideas like they are in a kind of debate. Each actor has his/her own 
viewpoint based on his constraints, objectives and experience. These viewpoints have 
a dynamic nature. They evolve during convergence phases through social interactions 
and communications between actors who are trying to reach a joint decision [10]. 
Three levels of convergence can be defined. The first level is the convergence about 
the objectives. In this level, formulating the problem of the design is necessary to 
subsequent ideas generation into design process. A well-defined problem is half 
solved. A good strategy to define a problem is to consider it from multiple viewpoints 
that may reflect the interests of different actors [11]. The second level is the conver-
gence about design alternatives. The different alternatives need to converge on one 
choice of design that satisfies the different actors. Additionally, we propose a third 
level of convergence which concerns the evaluation of the design. It is clear that con-
vergence in the higher levels is more important and has more impact on the product 
design than lower levels (fig.1).    

 
A Tool Proposition to Support Multidisciplinary Convergence 
797 
 
This paper focuses more precisely on convergence between ergonomists and me-
chanical designers. However, these actors have different methods, representations and 
tools making difficult to carry out the convergence between them [8]. Moreover, eve-
ryone is attached to specific profession and, consequently, attached to a specific lexi-
cal field and vocabulary. One word could be very precise and have a specific and one 
meaning for one expert but will be totally confuse and imprecise for another expert 
working in another field [12]. This fact sometimes leads to a lack of information and 
to language barrier between these actors. It can also lead to some bad decisions and 
mistakes during design process. 
 
Fig. 1. Three levels of the convergence into product design process adapted from [10] 
Thus, ergonomists and mechanical designers need to support the communication 
and, consequently, convergence between them. In this context, a lot of researches 
highlight the importance of Intermediary Objects (IOs) into product design process 
and more specifically in the development of collaborative work. 
3 
Intermediary Objects (IOs) 
A key dimension of collaborative design is the use of IOs. They include all artifacts, 
whether physical (mock-ups, sketches, etc.) or virtual (CAD models, calculation re-
sults, etc.) produced by the actors during design process. In other words, they cover 
all kinds of externalization and they circulate between actors involved in the product 
design process [8]. These artifacts represent a part or the whole characteristics of 
product’s identity (functions, form, materials, constraints, etc.). Furthermore, they 
constitute the traces of design activities carried out by the actors. IOs contribute to the 
construction of a compromise and the sharing of common knowledge between the 
actors. Moreover, they ease shifting their viewpoints during convergence phases. That 
is to say, IOs act as mediators into product design [13]. 
 

798 
A. Al Khatib et al. 
 
More interestingly, Star and Griesemer [14] introduce the notion of Boundary Ob-
jects (BOs). These objects are located at the intersection between different heteroge-
neous social worlds. They maintain coherence across these worlds. In the context of 
participatory ergonomics, Broberg identifies eight characteristics of boundary objects 
[15]. For example, they have to be objects- in-the making and built-in affordances as 
will explained later. 
However, the notion of IOs is often confused with the notion of BOs. Vinck [13] 
indicates that IOs don’t necessarily have the characteristics of BOs. In some cases, 
IOs contribute to the articulation between heterogeneous social worlds. In fact, IOs 
becomes BOs when they are equipped with a common structure between these differ-
ent worlds [14]. This structure can be found by integrating BOs’ characteristics pro-
posed by Broberg in the development of IOs into product design process.  
4 
Virtusketches: A Tool to Support Multidisciplinary 
Convergence  
4.1 
Problematic and Objective  
Virtual reality (VR) is a recent technology which is used into product design process. 
It allows immersion in virtual environment (VE) [16]. So, actors can visualize and 
manipulate VR mock-up in relief and in an intuitive way. Moreover, VR allows actors 
to interact with VR mock-up in real time. Actors can also test some scenarios of using 
the product. Considering all these advantages of VR, this paper focuses on the fact of 
using VR mock-up as BO to support collaboration between ergonomists and mechan-
ical designers during convergence phases of product design process. In this context, 
project reviews are conducted around VR mock-up in VE throughout convergence 
phases [2]. The primary objective is typically to ensure the design is in conformance 
with its requirements with a secondary objective of highlighting potential deficiencies 
in the design as viewed by the various actors [17].  Face to face communication be-
tween actors is conducted during this virtual design review to reach a compromise and 
joint decision concerning some alternatives of design. However, despite all the possi-
bilities of immersion and interaction offered by VR mock-up, collaboration and 
communication between actors, especially between ergonomists and mechanical de-
signers, remains difficult. More precisely, these actors need an intuitive support of 
action on VR mock-up to express their intentions and to explain their ideas and argu-
mentations to other actors [18].  Furthermore, the need to obtain traces of actors’ 
activities during project review is highlighted to support the next design phases. These 
needs could theoretically be justified by BO’s characteristics proposed by Broberg 
[15]. Indeed, Broberg indicates that BOs are objects- in-the making and built-in affor-
dances. They are not ready; they need to be created by the actions of the actors during 
the project review. In other words, all actors should have the possibility to act on the 
BOs. This clearly is linked to the notions of open objects and closed objects [19]. 
Closed objects only transfer some information from one actor to another actor.  For 
example, CAD drawing may transfer some information from the designer to the ergo-
nomist; but the ergonomist is not able to modify it. On the opposite, open objects 

 
A Tool Proposition to Support Multidisciplinary Convergence 
799 
 
offer the possibility of action to all actors. This possibility will help to increase the 
power of their ideas and make them clearly understandable by everyone involved in 
the design process.  
Thus, our objective is to offer to actors the possibility of action on VR mock-up 
during design review in VE. This should improve the communication and hopefully 
the collaboration between these actors. 
4.2 
Linking VR Technology and 2D Sketching 
Sketching is a powerful means of interpersonal communication [20]. It consists of the 
production of quick and messy intuitive drawing of actors’ idea. It provides not only a 
mean for representing mental images of actor’s ideas but also a way to facilitate the 
actual generation of such mental images [21]. That is to say, the activity of sketching 
stimulates creativity in design thinking. Moreover, self-made sketches also support 
the limited human memory capacity and mental processing for a detailed problem 
analysis [22]. In addition, annotations play a major role in design coordination and 
knowledge elicitation in asynchronous phases, and an important cognitive synchroni-
zation role during synchronous phases [23]. They can foster knowledge creation and 
participate to the development of shared understanding among the design team [24]. 
Virtusketches is a tool based on the fact of linking VR technology to 2D sketching 
and annotations. More precisely, we propose to complete VR techniques with 2D 
sketching and annotations as a tool that could be used by all the actors involved in the 
design process regardless of their specificity. This will transform the VR mock-up 
into open object on which every actor can act.   
4.3 
Tool Description  
As already mentioned, the objective of this work is to provide the actors with a design 
supporting tool which can be used in VE during convergence phases into product 
design process. The goal is to integrate the benefits of 2D sketching and annotations 
into the benefits of VR technology. The conjoint use of Virtusketches should enable 
the actors 1) to act on the product design and 2) to ease ideas confrontation. This tool 
should ease exchange ideas between actors through sketches over recently captured 
photos of the VR mock-up during project review in VE.  
To implement Virtusketches, we use a virtual reality platform composed of 3 ac-
tive stereoscopic screens (2.10m * 2.80m) (fig.2-a). To manipulate VR mock-up and 
to capture actor’s viewpoint in VE during project review, Wii (Nintendo®) Remote 
Controller is used. An Ethernet network is implemented between virtual reality sys-
tem and a sketching pen tablet (fig.2-b). A remote monoscopic screen of the im-
mersed actor’s viewpoints is used. This remote view allows non-immersed actors who 
are out of the VR platform to obtain a non-distorted view of the immersed actor’s 
viewpoint (Fig.2-c). VR software (Virtools) is used to develop VR application on the 
VR platform. To sketch on the captured photos by the pen tablet, Autodesk  
SketchBook Designer software is used as user interface. 

800 
A. Al Khatib et al. 
 
One typical use case of our tool would take place during project reviews. In this 
scenario, one of the actors visualizes, manipulates and interacts with the virtual mock-
up in VE. If this actor finds improvements, faults or ideas to express and communi-
cate with other actors, he or she can capture his or her VE viewpoint like a snapshot 
which will be directly transferred on the sketching pen tablet. After that, this actor can 
express and communicate his/her own intention to the other actors by sketching and 
annotating his/her captured viewpoint thanks to the pen tablet. The other actors may 
also participate in sketching and annotating the captured viewpoints. Then, the 
sketched and annotated captured viewpoints can be displayed and hidden on VR plat-
form during the project review. This process is iterative until reaching convergence 
between the actors. 
 
Fig. 2. Virtusketches a support tool of collaborative work: (A) VR platform. (B) Sketching pen 
tablet. (C) Monoscopic screen. 
5 
Case Study: Ergonomics-Mechanical Design Convergence 
5.1 
Context of the Study  
In order to test Virtusketches as a tool that supports the convergence, it was used dur-
ing a convergence phase of an automotive design project. MobyPost is an ongoing 
project that aims at developing a whole system combining a carbon neutral vehicle 
with a novel technology based on a solar hydrogen fuel cell system. In this context, a 
hydrogen vehicle for postmen is being designed. The project is conducted by nine 
partners; one of them is the SeT laboratory (laboratoire Système et Transport). The 
objective of SeT laboratory is to develop a vehicle adapted to some specific needs of 
the postmen (i.e. considering ergonomics aspects) with the respect to technical func-
tions of the vehicle (i.e. considering mechanical aspects).  
This paper presents a project review which was conducted using Virtusketches in 
VE.  The actors who participate in this project review were one ergonomist, 2 me-
chanical designers and one industrial designer. The objective of this project review 
was to achieve a consensus about powertrain accessibility for maintenance and the 
design of the vehicle’s cockpit. Indeed, ergonomics constraints play a critical role in 
the current vehicle design. For instance, one of the major constraints concerns the fact 

 
A Tool Proposition to Support Multidisciplinary Convergence 
801 
 
that postmen get in and get out of their vehicle about 350 times per day. Ergonomics, 
thus, want to consider this in the design of the vehicle’s cockpit.  
5.2 
Procedure  
Some actors involved in this study had no previous experience with the VR project 
review and the manipulation of the VR mock-up. So, a familiarization phase was 
conducted. By the end of this phase every actor has to know how to: 1) turn the VR 
mock-up and change its scale to check some details; 2) turn around the VR mock-up 
and even enter it; 3) use a ray to indicate or to select some parts of the vehicle; 4) hide 
and display a previous mock-up of the vehicle to compare it with the new mock-up. 5) 
Hide and display a virtual human in the vehicle, the reach zones and the vision zones; 
6) take a photo of their viewpoint in the VE; 7) sketching and annotating their cap-
tured viewpoint on the sketching pen tablet; 8) hide and display sketched and anno-
tated viewpoint in the VE.  
Each actor’s contribution was colored differently (using a specific color on the pen 
tablet). This enabled to follow the traces done by every actor. Moreover, 2 roles were 
defined for actors in the VE: 
─ The first role is the role of immersed actor. In this role, the actor leads the interac-
tion with VR mock-up in the VE and he can visualize VR mock-up in relief. He 
can also capture his own viewpoint in the VE and then sketch and annotate his/her 
intentions on the pen tablet.  
─ The second role is the role of non-immersed actor. In this role, the actor discusses 
and communicates with the immersed actor and with the other non-immersed ac-
tors. Sometimes, he can be near the immersed actor on VR platform (with quite 
distorted viewpoint) or he can also visualize the immersed actor’s viewpoint on the 
monoscopic screen. He can participate in sketching and annotating the captured 
viewpoint.  
The change between the roles of these actors has to take place in a dynamic way. 
Every actor can take the role of immersed actor when he wants.  The documentation 
provided to actors was the PDS document (Product Design Specifications) which 
provide information such as functions and design constraints. However, the actors 
during this project review were focusing on the cockpit’s specification and the power-
train’s specifications. Actors were provided also with an assembly drawing that 
showed the principle dimensions of the vehicle. The project review was recorded by 
two video recorders; one was oriented towards the VR platform and the other was 
oriented towards the pen tablet. The participants in the study were then interviewed 
about the used tool after the project review.   
5.3 
Study Results 
Following the project review, semi-structured interviews were conducted with the 
various actors who participate in the study. These interviews permitted to gather a 
qualitative feedback from the different actors concerning the use of Virtusketches 

802 
A. Al Khatib et al. 
 
during the project review. The experience feedback from using Virtusketches was 
generally positive. All the actors appreciated the possibility to act on vehicle design 
by sketching and annotating on their own captured viewpoint.  
Virtusketches allowed the ergonomist to visualize the vehicle on the real scale. He 
had the possibility to validate the dimensions of the different parts of the cockpit by 
displaying the virtual human, the reach zones and the vision zones. He could also take 
the place of the driver on a physical seat in VE for testing the ease of ingress and 
egress out of the vehicle. It was possible to expect the use of the future vehicle by 
experiencing the gestures and postures of the driver on VR platform. Virtusketches 
enabled the ergonomist to express his ideas by sketching and annotating on his cap-
tured viewpoints. He felt more confident to suggest ideas and to express his intention 
without using technical terms normally used by mechanical designers. However, the 
sketches provided by ergonomist were simple and it was sometimes necessary to de-
velop it by the industrial or mechanical designers who had experience in advanced 
sketching. 
   
 
Fig. 3. The ergonomist visualizes the vehicle with the virtual human (left). The ergonomist 
takes a snapshot to his viewpoint (right). 
Virtusketches allowed the mechanical designers to validate some mechanical spe-
cifications (e.g. the possibility of fabrication of some parts of the vehicle). It helped 
them to argue and to clarify their design choices to the other actors. By sketching and 
annotating the captured viewpoint, they could illustrate some ideas to other actors. 
However, mechanicals designers highlighted the need for some CAD functionalities 
in VE (e.g. the need to hide some parts to see the interior parts and the need to inter-
secting planes).  
The industrial designer was very satisfied with the access to his traditional tech-
niques which is 2D sketching. He also appreciated the possibility to capture his own 
viewpoint in VE. As the ergonomist, VR provided him the perception of the real di-
mensions of the vehicle. 
 
The immersed actor could access to the sketched and annotated photos in the VE. 
All the actors think that Virtusketches tool helped them to facilitate the communica-
tion and consequentially convergence between them. However, due to some technical 
limits of VR platform, only one immersed actor could be on the VR platform. The 
actors wanted to discuss together on VR platform around the VR mock-up.   

 
A Tool Proposition to Support Multidisciplinary Convergence 
803 
 
    
 
Fig. 4. The ergonomist explains his idea by sketching on the captured viewpoint on the pen 
tablet 
6 
Conclusion and Future Works  
Collaboration between ergonomist and mechanical designers is one approach for er-
gonomics integration. In a collaborative work, it is necessary for these actors to com-
municate during convergence phases such as during project reviews. However, this 
communication is difficult due to the difference in their backgrounds, methods, and 
even in their vocabulary. Thus, relying on a literature review about Intermediary Ob-
jects (IOs) and Boundary Objects (BOs), this work presented Virtusketches as a sup-
port tool to ease communication between actors during convergence phases in VE. 
Virtusketches is based on linking VR technology with 2D sketching and annotating.  
It grants the actors the possibility to act on the product design turning VR mock-up 
into an open object. In order to test Virtusketches, a case study was presented during a 
project review of an automotive design project. Semi-structured interviews with the 
actors showed that the feedback concerning the use of Virtusketches was generally 
positive. The ergonomist was more confident and could more easily express his ideas 
to the other actors. Virtusketches also helped mechanical designers to clarify their 
design choices. The industrial designer was satisfied with the access to his traditional 
techniques of work which is 2D sketching.  
These first results are essentially qualitative. Our future work will consist on con-
ducting a quantitative evaluation of the proposed tool. Moreover, the impact of  
Virtusketches on the work of industrial designer will be more studied.  
References  
1. Brangier, E., Barcenilla, J.: Concevoir un produit facile à utiliser: adapter les technologies 
à l’homme, Editions d’Organisation (2003) 
2. Mahdjoub, M., Monticolo, D., Gomes, S., Sagot, J.C.: A collaborative Design for Usability 
approach supported by Virtual Reality and a Multi-Agent System embedded in a PLM en-
vironment. Computer-Aided Design 42, 402–413 (2010) 
3. Broberg, O.: Integrating ergonomics into engineering: Empirical evidence and implications 
for the ergonomists. Human Factors and Ergonomics in Manufacturing & Service Indus-
tries 17, 353–366 (2007) 

804 
A. Al Khatib et al. 
 
4. Kaljun, J., Dolšak, B.: Ergonomic design knowledge built in the intelligent decision sup-
port system. International Journal of Industrial Ergonomics 42, 162–117 (2012) 
5. Broberg, O.: Integrating ergonomics into the product development process. International 
Journal of Industrial Ergonomics 19, 317–332 (1997) 
6. Jensen, P.L.: Human factors and ergonomics in the planning of production. International 
Journal of Industrial Ergonomics 29, 121–131 (2002) 
7. Broberg, O., Hermund, I.: The OHS consultant as a political reflective navigator in tech-
nological change processes. International Journal of Industrial Ergonomics 33, 315–326 
(2004) 
8. Boujut, J.F., Laureillard, P.: A co-operation framework for product–process integration in 
engineering design. Design Studies 23, 497–513 (2002) 
9. Cross, N.: Engineering Design Methods: Strategies for Product Design, 4th edn. (2008) 
10. Lu, S.Y., Elmaraghy, W., Schuh, G., Wilhelm, R.: A scientific foundation of collaborative 
engineering. CIRP Annals - Manufacturing Technology 56, 605–634 (2007) 
11. Zeng, L., Proctor, R., Salvendy, G.: Fostering creativity in service development: Facilitat-
ing service innovation by the creative cognition approach. Service Science 1, 142–153 
(2009) 
12. Luz, T., Loup-Escande, E., Christofol, H., Richir, S.: The Collaborative Product Design 
and Help to Decision Making: Interactive Mind-Mapping. In: Global Product Develop-
ment, pp. 237–244. Springer, Heidelberg (2011) 
13. Vinck, D.: De l’objet intermediare à l’objet frontière Revue d’anthropologie des connais-
sances. In: SAC, vol. 3, pp. 51–72 (2009) 
14. Star, S., Griesemer, J.: Institutional ecology, translations’ and boundary objects: Amateurs 
and professionals in Berkeley’s Museum of Vertebrate Zoology, 1907-39. Social Studies 
of Science 19, 387–420 (1989) 
15. Broberg, O., Andersen, V., Seim, R.: Participatory ergonomics in design processes: The 
role of boundary objects. Applied Ergonomics 42, 464–472 (2011) 
16. Fuchs, F., Moreau, G., Guitton, P.: Virtual Reality: Concepts and Technologies. CRC 
Press (2011) 
17. Ostergaard, K., Wetmore III, W., Divekar, A., Vitali, H., Summers, J.: An experimental 
methodology for investigating communication in collaborative design review meetings. 
Co-Design 1, 169–185 (2005) 
18. Meyrueis, V.: Modification interactive de formes en Réalité Virtuelle: Application à la 
conception d’un produit. Phd thesis, ParisTech (2011) 
19. Vinck, D., Jeantet, A.: Mediating and Commissioning Objects in the Sociotechnical 
Process of Product Design: A Conceptual Approach. In: MacLean, D., Saviotti, P., Vinck, 
D. (eds.) Management and New Technology: Design, Networks and Strategy, Bruxelles. 
COST Social Science Series (1995) 
20. Kenneth, D., Forbus, R., Usher, J.: Towards a computational model of sketching. In: Pro-
ceedings of Intelligent User Interfaces, pp. 77–83 (2001) 
21. Van Der Lugt, R.: Brainsketching and how it differs from brainstorming. Creativity and 
Innovation Management 11, 43–54 (2002) 
22. Schutze, M., Sachse, P., Romer, A.: Support value of sketching in the design process. Re-
search in Engineering Design 14, 89–97 (2003) 
23. Hisarciklilar, O., Boujut, J.F.: An annotation model to reduce ambiguity in design commu-
nication. Research in Engineering Design 20, 171–184 (2009) 
24. Boujut, J.F.: User-defined annotations: artefacts for co-ordination and shared understand-
ing in design teams. Journal of Engineering Design 14, 409–419 (2003) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 805–814. 
DOI: 10.1007/978-3-642-30817-8_79 
© Springer-Verlag Berlin Heidelberg 2013 
 
A Visual Language for the Collaborative Visualization  
of Integrated Conceptual Models  
in Product Development Scenarios 
Johannes Herter1, Ross Brown2, and Jivka Ovtcharova3  
1 UNITY AG, Wankelstrasse 3, 70563 Stuttgart, 
2 Faculty of Science and Engineering, Queensland University of Technology, 128 Margret St., 
Brisbane, QLD 4000, Australia 
3 Institute for Information Management in Engineering, Karlsruhe Institute of Technology, 
Zirkel 2, Karlsruhe, 76131, Germany 
johannes.herter@unity.de, ross.brown@qut.edu.au, 
jivka.ovtcharova@kit.edu 
Abstract. In various industrial and scientific fields, conceptual models are  
derived from real world problem spaces to understand and communicate con-
taining entities and coherencies. Abstracted models mirror the common under-
standing and information demand of engineers, who apply conceptual models 
for performing their daily tasks. However, most standardized models in Process 
Management, Product Lifecycle Management and Enterprise Resource Plan-
ning lack of a scientific foundation for their notation. In collaboration scenarios 
with stakeholders from several disciplines, tailored conceptual models compli-
cate communication processes, as a common understanding is not shared or im-
plemented in specific models. To support direct communication between  
experts from several disciplines, a visual language is developed which allows a 
common visualization of discipline-specific conceptual models. For visual dis-
crimination and to overcome visual complexity issues, conceptual models are 
arranged in a three-dimensional space. The visual language introduced here fol-
lows and extends established principles of Visual Language science. 
Keywords: Visual Languages, Product Lifecycle Management, Collaborative 
Engineering. 
1 
Introduction 
With the division of work into specializations, collaborative scenarios have gained 
increased relevance in many disciplines and are indispensible in modern product 
manufacturing scenarios. Achieving consensus between local (inner domain issues) 
and global (cross domain coordination) are considered a major challenge [1].  
Commonly, conceptual models (CM) are created to pervade and understand prob-
lem domains for analysis, optimization and communication tasks. This has led to 
standardized or de-facto standardized CMs for singular disciplines like BPM (BPMN, 
BPEL), ERP and product Construction (STEP) which are used to formalize and  

806 
J. Herter, R. Brown, and J. Ovtcharova 
exchange information based on standardized definitions. Much effort has been con-
ducted on the modeling process in order to derive a consistent and complete abstrac-
tion; however, less regard has been given to a sound visual representation of CMs. 
Most discipline-specific CMs do not provide a distinct, specified notation for entities 
and relationships. Those which provide designated notations like BPMN are criticized 
regarding their usability [2], [3]. Visual Language Theory proposes generic principles 
and guidelines for sound and understandable notations on the basis of information 
visualization and cognitive science.  
Established models in science and industry are optimized for capturing knowledge 
of a singular discipline; however there is a trend to apply more holistic approaches in 
information management. Systems Engineering and Product Lifecycle Management 
address cross-cutting concerns with holistic information management strategies. Yet, 
notational aspects are not focused by these disciplines, although Systems Engineering 
provides a UML-derivative, namely SysML [4] for notation. Notably, Visual lan-
guage science-based critiques of the notation paradigms in UML indicate that re-
quirements for usability with UML are not fully met [5], [6].   
The targeted field of application of the work introduced here is a common visuali-
zation of conceptual models to support direct communication processes between ex-
perts from several domains. With a common and integrated visualization, overlapping 
concerns and dependencies between CMs should be made transparent for users. For 
this purpose, a visual language definition is developed to display integrated CMs from 
several disciplines within one common display. The definition follows rules and prin-
ciples from the discipline of visual language theory. To overcome the visual complex-
ity arising from the number of discipline-specific CMs displayed together, we intro-
duce space as a visual variable to display relationships between CMs. 
This paper is structured as follows: firstly, the context and proposed application of 
this concept is introduced by presenting concrete use-cases. On this basis, related 
scientific and industrial concepts for CM integration and Visual Language develop-
ment are analyzed and explained. In the following a visual language definition is in-
troduced which is capable to support a common visualization of discipline-specific 
models.  
The work introduced here is developed in scope of a research framework of the  
Institute for Information Management in Engineering at Karlsruhe Institute of  
Technology.  
Context and Environment of Application Scenario 
The application scenario of this research is characterized by decision making 
processes which impact several disciplines. Various tasks in product development 
require participation and coordination of several disciplines. Product design decisions 
would most certainly influence related manufacturing processes and related resource 
management. Alternations of a manufacturing process might require adaptations of 
the product design. Following these examples, coordination and collaboration issues 
between disciplines must be resolved to achieve optimal design solutions with refer-
ence to all related fields. In direct communication, processes experts in their respec-
tive fields have to negotiate agreements to cross-discipline issues. Therefore they 
must communicate their discipline-specific knowledge, formalized in their conceptual  
 

A Visual Language for the Collaborative Visualization of Integrated Conceptual Models 
807 
models to colleagues with a different background. The differing backgrounds of 
stakeholders, manifested in nomenclature and semantic constructs of their models, has 
to be resolved during the communication process. In order to support and expedite 
communication, a common visualization of the conceptual models is proposed. With a 
visualization of the major elements from each discipline and a link to corresponding 
entities in other disciplines, communication should be eased as dependencies are vi-
sually transparent. In addition, the work presented further develops a conceptual 
framework addressing collaborative scenarios for manufacturing process optimization 
[7], [8].  
 
Fig. 1. (left) Vision of common visualization of conceptual models, showing an immersive 
cylindrical visualization of multiple models; (right) Mock-up of a Petri-net based Process vi-
sualization within the framework.  
Common use-cases for the targeted platform are characterized as decision making 
with impact on several disciplines. Typical examples are Make-or-Buy decisions dur-
ing manufacturing process planning. Action alternatives for procurement or in-house 
production would impact on product design and construction, the manufacturing 
process as well as resource management. The procurement of components might  
require alternation of the product structure and design for integrating purchased com-
ponents. The manufacturing process would require adoptions for assembling the  
components. This would further on impact resource application.  
A concrete example considers a decision making process in the branch of solar 
manufacturing devices. The decision considers procurement or self manufacturing of 
the mounting. The decision alternatives influence the product design as the connector 
between solar panel and mounting device might need adaption. The manufacturing 
process is influenced by varying process sequences for the manufacturing and conse-
quently a different application of resources.  
To identify an overall optimal solution, communication and compliance between 
all concerned disciplines is required. A common visualization of integrated concep-
tual models could support the communication process, because the dependencies and 
impacts of each alternative would be transparent. Each discipline expert would have a 
specialized model for conducting the issue in his CM, whereas the dependencies to 
the other disciplines are visible. This should avoid misunderstandings as participants 
have visual support about the artifacts in the models affected by the decision. 

808 
J. Herter, R. Brown, and J. Ovtcharova 
2 
State of the Art und Related Work 
The activity of conceptual modeling results in a formalization of the aspects of a 
problem domain with an explicit description of entities of interest, their properties and 
relationships with the purpose of communication support and to provide a thorough 
understanding of the problem domain [9].  
Referring to General Model Theory by Stachoviak [10], Conceptual Models (CMs) 
are derived from reality following the paradigms of “Reduction” (consider relevant 
entities and properties), “Pragmatism” (specific purpose of the model, factor for the 
selection of entities) and “Projection” (mapping of real world concepts and relations 
to the model). In industrial production scenarios there are standardized and de facto-
standard conceptual models for capturing and exchanging knowledge of disciplines. A 
common standard on which most proprietary and open PLM systems are oriented is 
STEP, catalogued as ISO norm 10303 [11]. Process Management applies a conceptual 
model for activities and arrangements of activities. Standardized models in process 
management differ in executability, power of expression and visual representation. 
Enterprise Resource Management (ERP) is a concept for effectively managing hu-
man, financial and production related resources of an enterprise. In opposite to stan-
dardization of BPM and PLM, ERP information is formalized in de facto-standard 
models, commonly following major system developers. The awareness of a need for 
discipline-spanning approaches is evident in implemented methods and systems in 
PLM and ERP to support cross domain collaboration with holistic information man-
agement strategies; however integral visualization is not proposed by neither of them. 
Visualization of product data and process information is implemented in separate 
windows of an application or within a sectioned common diagram. Approaches lack 
of visual means for showing interconnection properties.  
A widely followed approach in Visual Languages (VL) design research is to create 
an analogy to spoken or theoretical languages. Visual syntax and sentences function 
as generic elements from which Visual Languages are composed. Graphic primitives 
are considered as language terminals which are used to form visual sentences. Lan-
guage terminals or variables are the most primitive visual means used to express and 
distinguish semantic entities in the underlying model. A catalog of basic notations, 
enumerated as visual variables is described by Bertin [12]. 
  
 
Fig. 2. Visual Syntax elements defined by [12], [13] 

A Visual Language for the Collaborative Visualization of Integrated Conceptual Models 
809 
A general but very detailed description of notation concepts is presented as the 
“Physics of Notation” by Daniel Moody [13], [2]. Cited approximately 1401 times 
within the last three years it continues to receive increasing recognition. The “Physics 
of Notation” (PoN) summarizes and integrates criteria and principles for developing 
visual languages representing conceptual models. In his research, he critically engag-
es with current notations in technical areas like software engineering or process man-
agement and points out the demand for a scientifically rigorous analysis of VLs. In 
VL research, there are a number of approaches to define criteria for CM visualization 
focusing mainly on cognitive aspects of perception and understanding. Although the 
PoN principles are not empirically validated, they are scientifically and theoretically 
well constituted. The basis of the principles in line with acknowledged approaches of 
visual language research [2], [14],  explicitly the Cognitive Dimensions Framework 
[15], [16]. The principles are introduced based on standardized visual languages like 
UML [5] and BPMN [3]. For the visual language proposed here, the PoN principles 
are applied because of their scientific and theoretical validity. There is no distinct 
method for measuring the extent to which the criteria are fulfilled. The principles are 
formulated to be both principles and evaluation guidelines and should not be inter-
preted as absolute laws but guideline to the VL definition process. The principles 
consider both visual (perceptual) and cognitive aspects regarding understanding.  
PoN principles are applied to the interconnections between entities of different 
CMs and are not fundamentally different as the general demand for understandability 
and distinctness is of great importance in the cross-domain representation, too. The 
aggregation of CMs does not introduce fundamentally different constructs, except for 
the higher complexity. This demands a more thorough and balanced application of 
PoN principles to both CM specific visualizations and CM-spanning elements. 
3 
A Visual Language for Integrated Conceptual Models 
In order to develop a visual language definition to display a set of related conceptual 
models, methods must be applied to establish the connections between the concepts. 
The general issue of information integration is widely extensively researched and 
although not yet fully solved, there are numerous promising industrial and scientific 
approaches to solve this issue [17], [18]. Scientific approaches differ in structural and 
semantic information model heterogeneity. In order to provide sound and correct 
integrated models for the use case of decision making, the semantic integration as the 
highest level of information integration is pursued [19]. Therefore established ap-
proaches from the field of knowledge management namely ontological based  
approaches are applied, namely an ontology based integration based on Bunge- 
Wand- Weber ontologies [20]. In scope of this work, a semantic based integration 
between entities of regarded information models is assumed. This results in direct 
interconnections between entities of several conceptual models. The relationships are 
assigned a type which expresses the characteristics of the connection.  
                                                           
1 According to Google Scholar,  http://scholar.google.de/  

810 
J. Herter, R. Brown, and J. Ovtcharova 
3.1 
Visual Language Principles  
Previously described Physics of Notation principles are proposed for notations of 
singular CMs.  The principals are applied to the common visualization of related 
conceptual models. This demands an extension of the visual variable set used for 
singular models. The repertoire of visual variables is used in singular models. Al-
though not all variables are commonly applied, there are no reserved variables, as the 
regarded models do not provide a distinct notation though their notation is (de-facto 
standardized). Therefore it must be assumed that the full set of visual might be ap-
plied in any of the models regarded. The semantic concepts which are displayed with 
the visual variables used in the individual models demand however an obvious dis-
tinction for users.  
To stick to the common notation and support with familiar notation, the visual syn-
tax element “Shape” should not be used for distinguishing CMs, as this would result 
in a “symbol overload” [2] as the same visual discriminator would be used to distinct 
inner CM concepts and CM as a whole. Constraints for VL development are summa-
rized as follows: 
• The discipline identity must be retained and visually transparent  
• A high recognition capability for discipline specific models must be given 
• The visual complexity must be balanced against the need for completeness of visu-
alization.  
• Discrimination of discipline-specific models 
Visual discrimination of differing discipline models is essential for users. The as-
signment of concepts to their CM has to be distinct and visual. In given constraints 
(reserved visual variables) the discipline models are be color-coded. Colors are not 
applied in standardized notations and can therefore be used for discriminating models. 
Furthermore color coding for distinction is a powerful visual means [21].  Each dis-
cipline is assigned a designated color to be used for all other visual variables of the 
model. The principle of Semiotic Clarity with a distinct mapping between color and 
discipline is thus fulfilled by a single discipline color allocation. In the aggregated 
overall visualization, the CMs are arranged, visually separated, without overlapping, 
for distinction and recognition value. The combination of the visual variables follows 
the PoN principle of Dual Coding [2], which recommends applying several visual 
variables for the same semantic concept to improve distinction. 
3.2 
Discipline-Specific Encoding 
Orientation-related visual variables are used expressing hierarchical (vertical ar-
rangement) and sequential (horizontal arrangement) properties of the structures to 
support recognition value. The distinction between different types of concepts is 
commonly implemented with geometric shapes (eg. BPMN) or pictorial icons. To 
support the recognition value for singular CMs, both 2D shape and orientation are 
considered reserved and will not be applied for indicating cross-domain disciplines.  

A Visual Language for the Collaborative Visualization of Integrated Conceptual Models 
811 
Applying the orientation and shape variables to inner CM artifacts fulfills the prin-
ciple of Cognitive Fit in PoN. When sticking to native and common notations in the 
CMs, the most commonly used shape would be a box. To provide Semiotic Clarity, 
the geometry would have to be altered which would contradict with Cognitive Fit 
principles. To avoid confusion, the color coding is complemented with a spatial dis-
tribution of individual models. The models are spatially separated in the visualization; 
there is no overlap of concepts from several conceptual models. When showing mod-
els in isolated areas of the visualization canvas, the affiliation of concepts to their 
models is transparent. A distinction by shapes is not essential for discipline specific 
CM discrimination.  
3.3 
Instance Descriptions 
Common discipline-specific CM notations widely apply textual annotations to encode 
artifact properties and information. Especially in CMs with a comprehensible number 
of different concepts, textual information is of high relevance in order to discriminate 
singular entities. Text is commonly used to identify and mark individuals (instances) 
of the concepts. This paradigm is followed in the VL introduced here. Textual repre-
sentations are reserved for instance discrimination. Textual instance discrimination is 
used for all discipline-specific CMs and follows a standardized notation and nomen-
clature to support Cognitive Fit. Instances are both concrete concepts and relation-
ships. 
3.4 
Cross Discipline Relationship Visualization 
Relationships between concepts of several disciplines have different characteristics 
compared to the inner discipline relationships. They are not integral components of 
the CMs, like relationships between concepts in a discipline. Inner discipline relation-
ships express the structure of model artifacts in a discipline, whereas cross discipline 
relationships indicate how discipline-specific CMs are connected. These fundamental-
ly different characteristics must be regarded in visualization for avoiding confusion 
and misleading users.  
The visual variable applied should not visually emphasize the cross-domain rela-
tionships, as they are not introduced to be in focus of the consideration. The introduc-
tion of the cross-discipline relationships is a major difference to common approaches 
both in conceptual modeling and visual language development. This demands a dis-
tinct and unique visual syntax to indicate the special character of the relationship to 
users. To provide a unique visual variable for cross discipline relationships, a spatial 
arrangement of CMs in the third dimension is introduced to visualize the cross discip-
line relationships in depth. The discipline-specific CMs remain in a 2D planar dia-
gram; while the collection of diagrams as a whole are arranged in the volumetric 
space. The spatial visualization is only applied to discriminate conceptual models and 
relationships. 
 

812 
J. Herter, R. Brown, and J. Ovtcharova 
 
Fig. 3. Schema of spatially arranged conceptual models in layers and cylindrical arrangement 
This visualization approach demands consideration of the position and orientation 
of users. Depending on the point of view, singular CMs are visible in the foreground 
when looking straight on a CM diagram. When moving in the information space, 
established from arranged and linked CMs, cross-discipline relationships become 
visible. The orientation and viewport of users are means of interaction and navigation 
with the visualization. With including users’ perspective to navigation, new means for 
visual complexity management are introduced: The pruning technique for handling 
visual complexity with reduction of entities displayed is implemented with navigation 
paradigms [22], [2]. Perceptive complexity is addressed with interaction paradigms. 
With reference to modeling theory the abstraction and mapping process results in a 
specific view on the real problem domain. With this metaphor, the contradiction be-
tween visual complexity [29] and a holistic view (showing multiple CM diagrams in 
one view) is overcome with means of interaction.  
This 3D approach makes the different kinds of relationships visually distinguisha-
ble by separating the dimensions of the visualization. This reduces the visual com-
plexity, as elements which are not of primary interest are put to background without 
losing a view on the big picture. The spatial arrangement is consistent with principles 
for complexity management. Parts of the CM complex which are not the focus of 
consideration are kept in the background, which follows pruning principles of non-
relevant information for complexity management [2]. As the entire artifact of linked 
CMs is available the pruning mechanism is implanted with visualization and interac-
tion metaphors.  
4 
Conclusion 
Discipline specific conceptual models are commonly visualized with the set of visual 
variables described in [5]. In a common visualization of several CMs the arising visu-
al complexity is addressed by introducing the visual variable of “spatial distribution”. 
Considering CMs as images of the real world created from the point of view (formal 
“Pragmatism” [10]) of a spatial arrangement of CMs is self-evident as the viewpoint 
of disciplines is just mapped to the information visualization artifact. With the third 
dimension used for the introduced relationships, a visual distance is established to the 
inner discipline relationships. This results in handling the issue of complexity not only 
with visual language elements but additionally with interaction metaphors like  

A Visual Language for the Collaborative Visualization of Integrated Conceptual Models 
813 
position and point of view of users. Depending on the point of view of users, only 
fragments of the CM complex are visible, either singular CMs or relationships be-
tween a few CMs. However, the relationships to relevant CMs are visible in the same 
display and therefore part of the scenery. This bridges the gap between a task-based 
application of the platform and a visualization of the context of the task within the 
problem domain. 
Complexity is reduced, as non-relevant CMs are taken out of primary focus of the 
user within performing a task. Empirical studies with 3D visualization have shown 
that the visual complexity can be reduced by using a combination of stereoscopic 
visualization and movement tracking, with the complexity reduced by a factor of three 
[14]. The possibility of visualizing the context of tasks without changing the visuali-
zation environment indicates benefits in the understanding of cross discipline issues 
without visually overburden users.  
The concept of a spatial visualization of CMs following the principles of PoN is 
currently in development. The target platform is an immersive Virtual Reality envi-
ronment at the Lifecycle Engineering Solutions Center in Karlsruhe, Germany. The 
facility offers a passive stereoscopic 3-side projection system with head-tracking. An 
empirical evaluation of a use-case for business process planning scenarios is expected 
in late 2012. 
Acknowledgments. The work introduced here was developed in scope of a research 
visit supported by the Karlsruhe House of Young Scientists. 
References 
1. Mahl, A., Semenenko, A., Ovtcharova, J.: An approach for building intercultural and 
cross- domain virtual organizations. In: CARV 2007 (2007) 
2. Moody, D.: The “Physics” of Notations: Toward a Scientific Basis for Constructing Visual 
Notations in Software Engineering. IEEE Transactions on Software Engineering 35 (2009) 
3. Genon, N., Heymans, P., Amyot, D.: Analysing the Cognitive Effectiveness 
of the BPMN 2.0 Visual Notation. In: Malloy, B., Staab, S., van den Brand, M. (eds.) SLE 
2010. LNCS, vol. 6563, pp. 377–396. Springer, Heidelberg (2011) 
4. OMG systems modeling language (OMG SysML TM). OMG Available Specification 
(2007) 
5. Moody, D., Hillegersberg, J.V.: Evaluating the Visual Syntax of UML: An Analysis of the 
Cognitive Effectiveness of the UML Family (2009)  
6. Britton, C., Jones, S., Kutar, M., Loomes, M., Robinson, B.: Evaluating the intelligibility 
of diagrammatic languages used in the specification of software. In: Anderson, M., Cheng, 
P., Haarslev, V. (eds.) Diagrams 2000. LNCS (LNAI), vol. 1889, pp. 376–391. Springer, 
Heidelberg (2000) 
7. Eichhorn, D., Herter, J., Oberweis, A.: An Approach for a Domain-spanning Collaboration 
Platform for Decision Support Using Immersive Visualization Techniques in Product 
Manufacturing.In: 1st International Workshop on Collaborative Usage and Development 
of Models and Visualizations, CollabViz (2011)  
8. Brown, R., Herter, J.: Virtual World Process Perspective Visualization. In: Conference on 
Information, Process and Konwledge Management (2012) 

814 
J. Herter, R. Brown, and J. Ovtcharova 
9. Mylopoulos, J., Loucopoulos, P., Zicari, R.: Conceptual Modelling and Telos. Information 
Systems Journal (1992) 
10. Stachowiak, H.: Allgemeine Modeltheorie, Wien (1973) 
11. ISO/IEC ISO 10303-214 DIS -Part 214 2nd Edition, ISO - International Organisation for 
Standardization (2003)  
12. Bertin, J.: Semiology of Graphics. Esri Press (2010) 
13. Moody, D.L.: The “Physics” of Notations: Toward a Scientific Basis for Constructing Vis-
ual Notations in Software Engineering (2009) 
14. Ware, C.: Information visualization: perception for design. Morgan Kaufmann (2004) 
15. Moody, D.: Theory Development in Visual Language Research: Beyond the Cognitive 
Dimensions of Notations 2. Theoretical Basis: What is a (Scientific). In: IEEE Symposium 
on Visual Languages and Human-Centric Computing, VL/HCC (2009) 
16. Green, T., Blandford, A., Church, L., Roast, C., Clarke, S.: Cognitive dimensions: 
Achievements, new directions, and open questions. Journal of Visual Languages & Com-
puting 17 (2006) 
17. Dreiling, A.: Business Process Management and Semantic Interoperability. In: Handbook 
on Business Process Management, vol. 1 (2010) 
18. Tursi, A., Panetto, H., Morel, G., Dassisti, M.: Ontological approach for products-centric 
information system interoperability in networked manufacturing enterprises. Annual Re-
views in Control 33 (2009) 
19. Obrst, L.: Ontologies for semantically interoperable systems. In: Proceedings of the 
Twelfth International Conference on Information and Knowledge Management - CIKM 
2003 (2003) 
20. Wand, Y., Weber, R.: An Ontological Model of an Information System. IEEE Transac-
tions on Software Engineering (1990) 
21. Keller, T., Gerjets, P., Scheiter, K., Garsoffky, B.: Information visualizations for know-
ledge acquisition: The impact of dimensionality and color coding. Computers in Human 
Behavior (2006) 
22. Kim, J., Hahn, J.: Reasoning with multiple diagrams: focusing on the cognitive integration 
process. In: The Nineteenth Annual Conference of Cognitive Science Society (1997) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 815–824. 
DOI: 10.1007/978-3-642-30817-8_80 
© Springer-Verlag Berlin Heidelberg 2013 
 
The Right Knowledge Management Strategy  
for Engineering Analysis SME: A Case Study 
Christophe Chevalier1,2, Franck Pourroy1, François Villeneuve1,  
and Alex Du Pasquier2 
1 G-SCOP Laboratory, Polytechnic Institute of Grenoble, 38000, Grenoble, France 
2 ANTECIM, 10 Impasse du Pré de l'Orme, 38760, Varces-Allières-et-Risset, France 
{christophe.chevalier,franck.pourroy, 
francois.villeneuve}@g-scop.inp.fr 
Abstract. The aims were firstly to identify major problems within a calculation 
and analysis SME and then to determine the adequate Knowledge Management 
Strategy. A participant observation was performed for a tem month period with-
in an engineering analysis SME, making a census of usual and unusual prob-
lems met by engineers during their daily activities. A keyword characterization 
grid for engineer's problems was performed. Through census prioritization and 
anecdote characterization three families of problems were identified: perpetual 
training, electronic-paper-document-mail classification and engineering activi-
ties support. The results justify a Knowledge Management implementation, 
highlight the importance of the turnover factor, and lead to recommending a co-
dification strategy. 
Keywords: Knowledge Management Strategy, Codification, Personalization, 
High Turnover, SME, Characterization, Engineer Problems. 
1 
Introduction 
Computational mechanics generally includes Finite-Element Analysis specificities 
and complexity. Intensive knowledge acquisition occurs through the highly cognitive 
tasks involved in the analysis process. Such engineering analysis activity is often 
outsourced to small specialized companies. These SMEs have to manage projects of 
various sizes from different industrial sectors with a wide range of problems. There 
are knowledge intensive companies, where the knowledge management could be a 
way to improve the efficiency and the reliability of process. 
Still from nowadays, a few works have been done describing how Small and Me-
dium Enterprises (SME) manage their competitive performance through a well chosen 
Knowledge Management System [1]. The managers and the companies have perfectly 
seen the potential benefits of managing the knowledge of their employees. This man-
agement must be done at all levels and particularly in technical departments. The 
improvement of the knowledge transfer within a team contributes significantly in 
increasing the final product quality; it can be also timesaving on the long term and 

816 
C. Chevalier et al. 
 
could leverage the satisfaction of employee through a feeling of efficacy and perfor-
mance. But the implementing cost of such a KM system is a severe issue for SME 
especially since the implementation and maintenance are costly, and the success is 
unsure with a failure rate about 70% [2]. The question of the necessity of the know-
ledge management is a great issue and the right Knowledge Management that suits to 
the SME is not obvious.  
Through a case study, in this paper we investigate on the relevance of a knowledge 
management in engineering analysis SMEs, and on the choice of an appropriate strat-
egy in the deployment of the KM approach. The following section exposes the state of 
art and the research question. The third section describes the Case Study approach, 
and the research method to address the question. The fourth section explains how the 
samples were collected and characterized. These samples are then analysed in sec-
tion 5 in order to answer the research question. The sixth part concludes the paper. 
2 
Literature Review 
As summarized by Wong and Aspinwall [3], businesses that can efficiently capture 
the knowledge embedded in their organization and deploy it into their operations, 
productions and services will have an edge over their competitors. And it is obvious 
that SME have to always keep an edge over multinationals in order to survive. More-
over, as Zanjani rightly states "Besides large organizations, the success of small busi-
ness or an SME can be linked to how well they manage their knowledge" [4]. Our 
research team shares the knowledge definition of Wilson [5], which clearly distin-
guishes the information from the knowledge. He defines the particularity of the know-
ledge by existing only in the human cerebral activity within his experience and his 
interaction with his environment. On the contrary, the information is only an associa-
tion of several data that could make sense in the context, like a message, a schema, or 
a graph. But in spite of the inexistence of knowledge outside of our mind, we accept 
the human capability to create highly organized information from his personal know-
ledge, through adequate "knowledge transfer spaces" called by Nonaka as "Ba"[6]. 
These "Ba" are supporting knowledge transfers.  
Referring to engineering analysis activity, a knowledge support typology was pro-
posed by Baizet [7] in order to characterize the knowledge created and handled by 
professionals. Knowledge supports for analysis teams is broken down into four cate-
gories: References, Tools, Methodology, and Modeling. References refer to actors' 
knowledge about the localization of know-how, information or knowledge: like expert 
person, seek document/norm/procedure or right server/computer. Tools refer to actors' 
knowledge about the potentials of his analysis tools, including its functionalities and 
limitations like known bugs, modeling limitations or machine capacity exceeding. 
Methodology refers to actors' knowledge of internal procedures which organized the 
enterprise and the processing of affairs, including the process of construction and 
validation of a new knowledge. Modeling refers to all technical know-how, like mod-
eling tricks, modeling simplifications and equivalences of real system behavior. 

 
The Right Knowledge Management Strategy for Engineering Analysis SME 
817 
 
Beylier et al. consider each piece of technical knowledge as linked with specific in-
formation that they call Support Data [8].   
We have widened our vision with the awareness of crucial knowledge identifica-
tion [9], as it is obvious that all knowledge within an organization do not have the 
same weight and the same criticality. The company needs to support the creation, 
storage and spread of the highly organized information generated. That's why the 
knowledge management could be a solution if well aligned with the enterprise policy 
and working employee needs. 
According to Hansen [10], it is necessary to have a clear strategy to implement a 
functional and sustainable knowledge management. Currently there are two major 
strategies widely accepted: the codification strategy (system oriented strategy) and the 
personalization strategy (human oriented strategy); both shall be present, but the 
whole system will be long-lasting only if one of them is chosen as the major strategy. 
The choice depends on several factors like the company size, its core business, its 
employee culture, its geographic dispersion, its policy) called by Wong [3] as Critical 
Success Factors (CSF). As reported by Wong, many companies that are attempting to 
initiate KM are unsure of the best approach to adopt.  
In the same objective, Gourova [11] has summed up not less than 9 papers on KM 
strategies and practices, augmented by 5 other papers intending to help SME (includ-
ing Wong's one) to choose the most appropriate strategy for them. The challenge  
results are based on a huge study of 199 SMEs from 7 countries of EU. The major 
challenge found is the lack of a KM Champion to lead the KM implementation; in 
second position are cultural and organizational barriers; followed by management 
resistance, the lack of experience in the senior management and lack of financial re-
sources. Among the barriers for KM introduction, the most relevant to the SMEs are 
as followed: Time and Priority of managers, Lack of management commitment, Fear 
to share the knowledge, Apathy of sharing knowledge, Lack of confidence and trust in 
consultant companies of KM expertise. Not included in Gourova's paper, in 2004, 
Meroño et al. [2] made an interesting experiment to test their theory of a relationship 
between KM strategy and business strategy. They proposed an alignment of a perso-
nalization KM strategy with a differentiation business strategy versus an alignment of 
a codification KM strategy with a cost business strategy. The experiment over 4 diffe-
rently aligned SMEs, two supposed as good and two supposed as bad, has proved this 
relationship but has also shown the consequent influence of the CSF which seems to 
assure the success of a SME not well aligned strategically. More recently and partially 
based on the alignment work of Meroño, Zanjani [4] has proposed an alignment tak-
ing into account three binary levels. The choice of the KM approach could be driven 
by the SME specific characteristics leading to choose successively the KM strategy, 
the KM tactic and the KM tool. The decisional tree leads to eight classes of KM  
approach. Recently, Hussain has made a census in developing countries of SME cha-
racteristics and has proposed several recommendations. One of these states the perso-
nalization strategy as the unique possible strategy for SME [12].  
Hence, in the light of this literature review, it comes that due to the specific nature 
of knowledge, managing it could be costly and the result is often uncertain. In addi-
tion, SMEs have specific characteristics which highlight some critical success factors. 

818 
C. Chevalier et al. 
 
The engineering analysis activity proved to hardly rely on different categories of 
knowledge, which could be worth being managed. In this paper we address the fol-
lowing questions: is it sensible to deploy a Knowledge Management approach to sup-
port engineer activities in an engineering analysis SME, and if so, which KM strategy 
best fits the context of such accompanies. A case study approach was used to investi-
gate on these questions. 
3 
Our Case Study Approach 
The study was conducted within a mechanical engineering SME that provides its 
customers with engineering analysis services. Within this 28 people company, 15 
engineers are in provision and 7-8 engineers are part of a parent house team dedicated 
to this activity. The customers can be either principal contactors or other SMEs, work-
ing in various industrial sectors (transportation, energy, leisure…). Projects ordered 
are spread out over half a day to 4 months. The work is generally done by a single 
analysis engineer but several knowledge exchanges are done within the team to boost, 
help or check the evolution of the project. Projects can be either totally new with an 
occasional customer or at the contrary very recurrent with a regular customer and it is 
not rare to have recurrent affairs spaced out of three years like dam gates analysis. 
Engineers in charge are encouraged to ask to the reference, for instance the expert. 
The research method used to answer our research questions consists in five steps. 
Firstly, a participant observation has been performed for a year within the SME. 
The aim was to make a census of the usual and unusual problems met by engineers in 
their daily activity. As problems generally occurred in a set of successive events, the 
problems set was collected as descriptive anecdotes. In a second step, the anecdotes 
have been prioritized and characterized. The aim was to identify the major problems 
of the SME to focus on, and to define common descriptors for all the anecdotes. In the 
third step of our research method, the most critical anecdotes have been analyzed in 
order to determine whether or not deploying a Knowledge Management solution 
could be relevant for the SME (our first research question). The forth step is using the 
previous literature revue to identify the recommended Knowledge Management strat-
egy and options in our specific case. Finally, the aim of the last step is to test the rec-
ommended strategy from the literature to the most critical anecdotes, in order to check 
if such a strategy could solve these major problems regarding the specific SME  
context. 
4 
Sample Creation and Characterization 
Identification and recording of the daily problems  
Problems were detected through several indicators: 
• 
an allocated hours exceeding for the affair 
• 
an unconformity detected at the validation procedure 
• 
a client return at the After Sales Service 

 
The Right Knowledge Management Strategy for Engineering Analysis SME 
819 
 
• 
an unwilling infringement of the internal procedures of an affair process 
• 
an informal discussion between engineers 
• 
a problem directly confronted to, as an engineer of the company. 
Once detected, the anecdotes were formalized thanks to informal interviews with the 
stakeholders involved in the anecdote. Over a ten months period, a total of 51 anec-
dotes were captured.  
Prioritization. A meeting involving a former expert of the enterprise helped the priori-
tization. Severe anecdotes were selected based on impact analysis while others have 
been eliminated. The impact was assessed according to three criteria: the lost time, the 
extra cost and the customer satisfaction decline. The impact has been classified ac-
cording to three levels of anecdote treatment priority: Critical, Priority and Comfort. 
This prioritization had two aims. The first one was to purify the census by eliminating 
the anecdotes which were not an issue. The second one was to prioritize the critical 
set of problems that have a major negative impact on the enterprise business. It should 
be noticed that the prioritization have considered the gravity of the consequences 
taking into account the occurrences. Therefore it is no longer necessary to treat the 
occurrences of anecdotes; it is included in the priority level. By prioritization of the 
51 anecdotes, 12 were classified as critical, 14 as priority, 14 as comfort and 11 were 
eliminated, or 22% of the anecdotes, like sum up in Table 1.  
Table 1. Prioritization of the anecdotes 
Priority 
Number of Anecdote 
Total 
Critical 
12 
 
51 (100%) 
Priority 
14 
40 (78%) 
Comfort 
14 
 
Eliminated 
11 
11 (22%) 
Characterization grid. The selected anecdotes were characterized according to the 
actors' experience and seniority, and to the nature of the problem.  
Table 2. Characterization grid for job experience and seniority levels 
Job Experience levels 
Expert 
Confirmed 
Initiate 
Beginner 
Seniority levels 
Former (+3 years) Intermediate (1 to 3 years) 
Recruit (- 1yearr) 
 
Table 2 shows the characterization grid related to the job experience level, and to 
the seniority level. We considered 4 levels of experience: the beginner, who has never 
used a Finite Element Software; the initiate, who is a worker seeking a lot of know-
ledge; the confirmed, who is an experienced and nearly knowledge independent 
worker; the expert, who is a reference. Regarding the seniority, 3 levels were defined.  

820 
C. Chevalier et al. 
 
Table 3 shows the different categories (first row) and sub-categories (last four 
rows) of the nature characterization. The description of this classification is out of the 
scope of the paper, but the four main categories are intended to make it possible to 
distinguish between the truly knowledge related anecdotes and some possible interfe-
rences. Document refers to a misused/a misunderstanding of a document or an un-
found document. Tool refers to a material failure, an unwanted crash, an expert file 
break or an obsolete method; and Processes refers to the willingly infringement of a 
known procedure or a lack/a flaw in a procedure. The Knowledge category is adapted 
from the literature [7-8] in order to have a more precise identification of the know-
ledge related with each of the anecdotes.  
Table 3. Characterization grid for the nature of the anecdotes 
DOCUMENT 
TOOL 
PROCESSES 
KNOWLEDGE 
Creation 
Software 
Commercial Offer 
Practice 
Retrieval 
Material 
Realize 
Project 
Modification 
Method 
Verify 
Customer 
Reused 
 
Validate 
Activity Communication 
 
The 40 anecdotes selected by the prioritization have been subdivided into 113 ele-
mentary anecdotes prioritized by inheritance. Among them, the rest of the paper is 
focused on the 27 anecdotes (88 elementary one) concerning engineering analysts’ 
activities.  
5 
Anecdotes Analysis 
Relevance of a KM approach 
The keyword characterization grid is used to identify major problems met by our en-
gineering analysis SME, with a special focus on knowledge transfers. 
 
Fig. 1. Number of elementary anecdotes according to each category and priority 
Figure 1 shows a representation of the number of elementary anecdotes by catego-
ries and by priorities. A large dot denotes a high number of anecdotes. It comes from 
this figure that half of the anecdotes are concentrated in the Knowledge category, 
Knowledge   Document     Processes      Tool 
Critical 
Priority 
Comfort 

 
The Right Knowledge Management Strategy for Engineering Analysis SME 
821 
 
making of this category the most important point to focus on, whatever the considered 
priority level. 
This observation is strengthened by Figure 2 and Figure 3, which show the relative 
level of each of the previous categories in relation with job experience (figure 2) and 
seniority (figure 3). 
 
 
 
Fig. 2. Number of elementary anecdotes per calculators according to experience 
 
 
 
Fig. 3. Number of elementary anecdotes per calculators according to seniority 
It is to be noticed that both graphs display the number of elementary anecdotes per 
employee of the same job experience or seniority category. According to job expe-
rience, statistics show there are three times more anecdotes for beginners, as there are 
many of them, but as shown in figure 2, the job experience per employee has no sig-
nificant problem decrease. According to seniority, statistics show clearly there is a 
sharp decrease of the number of anecdotes per employee between a new recruit and 
an intermediate. It means that the 2 first years are the worst for project quality and 
SME profitability. 
Evolution with seniority  

822 
C. Chevalier et al. 
 
In addition, it comes from a more detailed analysis of the content of the anecdotes 
that a high number of anecdotes prioritized as Critical or Priority are linked to a par-
ticular know-how seeking left unsatisfied, (knowledge, document, and even process 
categories). Among others, the following elementary anecdotes: the time wasted try-
ing to use a specific macro, the difficulty of understanding a too complex macro, the 
absence of a expert file notice, the vacation or departure of a norm expert to clarify 
parameters or analysis steps, show the lack of expert people. Analyzing the human 
resource information of the company shows that the parent house team is nearly com-
pletely renewed each 3 to 5 years. The matter is not only the tediousness of recurrent 
affairs, the analysis job has no possibility neither of evolution nor of promotion  
excepting becoming a project manager and managing, therefore analysis engineers 
rarely stay in their profession and a specialized SME cannot keep them in another 
department or promote everyone. 
Hence, the knowledge nature of most of the anecdotes, the large amount of anec-
dotes involving recruits, and the high level of turnover, inherent to the engineering 
analysis profession, show the relevance a knowledge management intention for engi-
neering analysis SMEs. More in detail, the critical anecdotes combined with a high 
turnover effect may lead to a continuous training waste and a perpetual low quality 
fatal for the SME. The observation backs this conclusion because new recruits and 
beginners disturb continually former and more experienced workers who form them 
knowing they won't stay, as direct consequence, the motivation of formers can only 
go down by usury until they left too. 
The Document criticality shows a major problem related to classification, because 
of huge occurrence of the "retrieval" keywords. It means a key issue about knowing 
where the information is. Accordingly, the observation has shown the systematic 
emergence of a personal electronic library of useful information built by each engi-
neer. This personal document is often be used and transferred instead of official ones; 
but when the owner leaves the enterprise, the personal library stops being dynamic 
because it can no longer be known, referenced and spread by the "librarian". The 
knowledge linked to this unstructured and rarely well tidy information is lost. The 
Processes occurrences show a consequent carelessness of employees to respect proce-
dures. The combination of the "not respected" sub-keyword from Processes category 
and the "not aware" sub-keyword from Reference Knowledge category about organi-
zation procedures shows the urge of well integrate employees relatively to internal 
organization described by procedures. The recruit autodidact or the practice integra-
tion is not enough; as the turnover is severe, there is a quick degeneration of the inte-
gration by personalization. The association of: the lack of document management; the 
number of keywords in Tool (tool awareness) from Practice Knowledge category; 
with the need of spreading procedures, shows that in spite of a law number of anec-
dotes in the Tools category (tool failures) a job supporting tool is needed. 
Choice of an appropriate KM strategy 
If KM is a potential solution then according to Hussain the personalization strategy is 
the right strategy for SME. Logically this conclusion should be back by the decisional 
tree of Zanjani. The foreseen approach according to this decisional tree of Zanjani is:  

 
The Right Knowledge Management Strategy for Engineering Analysis SME 
823 
 
• a KM strategy of Personalization: The SME has as mainly innovative and know-
ledge intensive tasks in spite of a lot of routine and very specific knowledge tasks. 
• a KM tactic of Individualization: The SME is clearly a small company. 
• a KM tool should not be an IT-based tool: The SME is geographically oriented 
because engineer in provision does not participate to knowledge transfers. 
The literature seems coherent and recommends a Personalization Strategy in the case 
of our SME. Nevertheless, regarding the turnover characteristic previously identified, 
this recommendation is worth to be questioned. Indeed, anecdotes could hardly be 
solved sustainably by personalization as the majority of actors are no longer present 
and current intermediaries have had short overlap with formers. When testing the 
principle of a personalization strategy against the anecdotes, it comes that only nine of 
them may be solved. The personalization approach requires direct interactions be-
tween people which are often quite impossible in such a turnover context, and being 
inherent to the engineering analysis activity, even a radical change in the management 
strategy would not have a significant effect.   
Moreover the presence of tedious recurrent tasks to be long term supported and 
well transmitted to new recruits tend to the choice of a codification approach for at 
least supporting these tasks. The observation in the company has shown that each 
engineer has built a personal useful electronic data library, for recurrent and more 
innovative affairs, knowing perfectly where personal information is. It means unstruc-
tured information are already written and in use for all kind of affairs. This is more 
especially true for intermediate and former engineers, which could partially explain 
the observations form figure 3. The analysis of complete anecdotes shows that 25 
anecdotes have good prospects to be sustainably solved by an adapted codification 
strategy. The turnover, factor generally ignored in literature, is here a heavy decision-
al factor and led to consider the turnover as a Critical Success Factor which influences 
the choice of the KM strategy. So, as a conclusion of the results, in spite of several 
coherent recommendations, the codification strategy looks a better choice for an engi-
neering analysis SME with a high turnover. 
6 
Conclusion 
A participant observation within an engineering analysis SME has permitted to de-
termine some major problems met by the engineers in their daily activities, particular-
ly in knowledge transfers. 88 elementary anecdotes characterized for the study have 
been analyzed in order to answer to 2 research questions about the necessity of a KM 
approach and, if necessary, the right KM strategy. 
The analysis of the anecdotes has permitted to justify a KM solution targeting on 
new recruits, independently of their experience. In addition, the necessity of clear 
procedures of data storage for certain document has emerged. 
Even if, according to the actual academic theories and results, a personalization 
strategy should be recommended, it has been shown that a codification strategy seems 
to better suit this engineering analysis SME. The main reason is the high turnover in 
this kind of SME that particularly affects knowledge sharing, or even document  

824 
C. Chevalier et al. 
 
sharing and retrieval due to time constraints and lack of former employees. The turn-
over implies also that a lot of energy is wasted in perpetual training even on highly 
recurrent projects. The heavy financial implementation cost of a codification strategy 
could be profitable as it mainly supports recurrent affairs. 
Limitations of this work may be related the few number of elementary anecdotes, 
the restriction to a department of the SME, and the cloud of the minimal turnover 
justifying a codification strategy. Further investigations will focus on deploying this 
strategy while taking care to diminish the cost and ensuring compatibility with high 
turnover. 
Acknowledgment. The authors are grateful to ANTECIM for their support. 
References 
1. Desouza, K.C., Awazu, Y.: Knowledge management at SMEs: five peculiarities. Journal 
of Knowledge Management 10(1), 32–43 (2006) 
2. Meroño, A., López, C., Sabater, R.: KM strategy and instruments alignment: helping SME 
to choose, OKLC, Innsbruck, Austria, pp. 1–25 (2004) 
3. Wong, K.Y., Aspinwall, E.: An empirical study of the important factors for knowledge-
management adoption in the SME sector. Journal of Knowledge Management 9(3), 64–82 
(2005) 
4. Zanjani, M.S., Mehrasa, S., Modiri, M.: Organizational dimensions as determinant factors 
of KM approaches in SMEs. In: 45th World Academy of Science, Engineering and Tech-
nology, pp. 389–394 (2008) 
5. Wilson, T.D.: The nonsense of ’knowledge management’. Information Research 8(1), pa-
per no. 144, 1–25 (2002) 
6. Nonaka, I., Konno, N.: The concept of “Ba": Building a foundation for knowledge crea-
tion. California Management Review 40(3), 40–54 (1998) 
7. Baizet, Y., Pourroy, F., Allera, R.: Toward an identification and a formalization of know-
ledge in computational mechanics. In: Gogu, G., Coutellier, D., Chedmail, P., Ray, P. 
(eds.) Recent Advances in Integrated Design and Manufacturing in Mechanical Engineer-
ing, pp. 391–400. Kluwer Academic Publisher (2006) 
8. Beylier, C., Pourroy, F., Villeneuve, F., Mille, A.: A collaboration-centred approach to 
manage engineering knowledge: a case study of an engineering SME. Journal of Engineer-
ing Design 20(6), 523–542 (2009) 
9. Grundstein, M.: GAMETH®: a constructivist and learning approach to identify and locate 
crucial knowledge. Int. Journal of Knowledge and Learning 5(3/4), 289–305 (2009) 
10. Hansen, M.T., Nohria, N., Tierney, T.: What’s your strategy for knowledge management? 
Havard Business Review, 1–11 (March-April 1999) 
11. Gourova, E.: Knowledge Management strategy for Small and Medium Enterprises. In: 
Proceedings of the International Conference on Applied Computer Science, pp. 639–648 
(2010) 
12. Hussain, I., Si, S., Ahmed, A.: Knowledge Management for SMEs in developing countries. 
Journal of Knowledge Management Practice 11(2) (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 825–834. 
DOI: 10.1007/978-3-642-30817-8_81 
© Springer-Verlag Berlin Heidelberg 2013 
 
An Ontological Approach to Integrated Product  
and Process Knowledge Modeling  
for Intelligent Design Repositories 
Farhad Ameri1 and Stephen Allen2 
1 Department of Engineering Technology, Texas State University, 601 University Dr.  
San Marcos, TX 78666, U.S.A 
2 Department of Computer Science, Texas State University, 601 University Dr.  
San Marcos, TX 78666, U.S.A 
{ameri,ssa15}@txstate.edu 
Abstract. Formal representation of design and manufacturing knowledge in 
distributed design repositories is a key enabler of autonomous design-to-
fabrication. This paper presents an ontological approach to formal representa-
tion of product and process knowledge in order to enhance the intelligence of 
design repositories. The product and process ontology developed in this work is 
based on the Web Ontology Language (OWL). A methodology is proposed for 
part characterization, clustering, and similarity measurement based on the as-
serted and inferred properties of the part instances in the ontology. Part families 
are formed according their semantic similarities from the perspective of geome-
tric and non-geometric attributes. Ontology population is conducted  
semi-automatically assisted by automated feature recognition techniques.  
Keywords: Product ontology, part family, design repository, similarity  
measurement, semantic web. 
1 
Introduction 
Engineering design process oftentimes involves alteration of the existing designs in 
order to arrive at design variants based on the new requirements. About 80% of de-
sign artifacts created in a typical manufacturing firm are modifications of the designs 
already available either internally or externally [1]. Design repositories are among the 
key components of the decision-support systems that enable the designers to efficient-
ly reuse the existing design knowledge in the form of design artifacts or engineering 
best practices. A design repository archives design artifact and design processes 
knowledge and typically provides some means of knowledge search and navigation. 
Despite significant advances in development of design information models and search 
and retrieval tools and technologies such as commercial PDM and PLM solutions, the 
existing design repositories still lack the required level of intelligence for enabling 
effective knowledge reuse. Designers in average spend 60% of their time searching 
for the right information or verifying the validity and relevance of the retrieved in-
formation in a given context [2]. To effectively support knowledge management 

826  
F. Ameri and S. Allen 
throughout product life cycle, design repositories need to move beyond serving as 
passive and static data silos and enhance their intelligence such that they can dynami-
cally provide every design task with the relevant design knowledge. Also, an intelli-
gent design repository lends itself better to semantic search and retrieval. 
To create more intelligent design repositories, it is first necessary to enrich their 
underlying information models in terms of formal semantics. In presence of formal 
semantics, machine agents can actively participate in design retrieval and synthesis 
process. Given the fact that design practice has increasingly become collaborative 
conducted by virtual teams distributed globally, a unifying semantic model is essen-
tial for eliminating or reducing the semantic ambiguities that are typically present in 
heterogeneous environments. For more than three decades, researchers have been 
working on developing better ways of capturing, representing, exchanging, and reus-
ing engineering design knowledge in a robust and flexible fashion. Numerous know-
ledge representations schemas have been developed based on different formalisms 
including pictorial, symbolic, linguistic, virtual, and algorithmic [1]. With the advent 
of Semantic Web technologies, symbolic knowledge representation through develop-
ment of formal ontologies has gained a new momentum and researchers started adopt-
ing the new knowledge representation standards provided by the Semantic Web  
technology suite, such as the Web Ontology Language 1 (OWL), for modeling differ-
ent types of knowledge with varying levels of formality and rigor. OWL is recom-
mended by World Wide Web Consortium (W3C) as the ontology language of the 
Semantic Web. It uses XML/RDF as the syntax, hence it has enough portability, flex-
ibility, and extensibility for web-scale applications. Description Logic [3] (DL) is the 
knowledge representation formalism used in OWL-based ontologies. DL provides 
formal syntax and semantics for developing knowledge models within a domain of 
interest in terms of concepts, relationships between concepts, and logical constraints 
that concepts must satisfy. Additionally, it enables automated reasoning services such 
as concept classification and consistency checking.  Therefore, due to its formality, 
expressive power, and its web-native format, OWL provides a suitable framework for 
knowledge representation in distributed design repositories.  
This paper presents an integrated ontology for product and process knowledge 
modeling. The presented ontology is axiomatic and uses logical restrictions for formal 
definition of the concepts used in the ontology. One notable aspect of the proposed 
ontology is providing a direct connection between product knowledge and manufac-
turing process knowledge. This connection is built through relating the manufacturing 
features of the parts to the manufacturing services that can fulfill the requirements of 
each feature. The proposed ontology not only encodes the asserted properties of the 
design artifacts available in the CAD models, but also accounts for the inferred prop-
erties that can be extracted from the asserted ones. In this way, design knowledge can 
be expanded and enriched systematically, thus improving the performance of the 
search engines that operate based on the ontological content. To demonstrate how the 
proposed ontology can support intelligent design retrieval, a methodology for part 
characterization, classification and clustering is proposed in this paper.  
                                                           
1  http://www.w3.org/TR/owl2-overview/ 

 
An Ontological Approach to Integrated Product and Process Knowledge Modeling 
827 
2 
Related Work 
Several ontologies have been developed in engineering design with the objective of 
providing a richer conceptualization of a complex domain and providing a shared 
vocabulary for information exchange among product stakeholders [1]. Core Product 
Model (CPM) [4] is one of the earliest ontologies developed for design representation 
that supports form, function, and behavioral aspects of the product based on the model 
developed by Gorti et al. [5].  Two key concepts in CPM are Artifact (i.e., a distinct 
entity such as part or assembly) and Feature (i.e., a portion of artifact’s form). The 
Open Assembly Model (OSM) is an extension of CPM that provides the standard 
means for representing assembly and tolerance propagation models [6]. Both CPM 
and OSM use object-oriented formalism and do not provide adequate formal seman-
tics. Some researchers have used the core concepts of CPM and developed more for-
mal ontologies accordingly such as the work reported in [7]. In the area of product 
family knowledge molding, Nanda et al. [8] proposed a methodology for capturing 
component design information through adopting a graph-based formalism represented 
in OWL.  There are also multiple information models and ontologies that are specifi-
cally developed for knowledge representation in design repositories [9].  
The literature review revealed that most of the existing ontologies for design repre-
sentation do not provide sufficient formality for concept definition. In other words, 
the axiomatic aspect of knowledge modeling and representation are not adequately 
addressed in the existing models. Also, a clear disconnect between design knowledge 
and manufacturing knowledge in the existing models undermines their ability in creat-
ing a cohesive body of knowledge that supports various phases of product realization 
process.  
3 
Product and Process Ontology 
Products can be described from different viewpoints including structure, function, and 
behavior. The proposed ontology mainly describes the structural aspects of a product. 
The functional aspects are addressed indirectly through referring to the category to 
which the products belongs. Fig. 1 shows the concept diagram for the three core 
classes that describe a design artifact, namely, Product, Assembly, and Part. Product 
class has a set of data-type properties that describe the physical attributes of the prod-
uct such as the dimensions of the bounding box and the weight of the product. Also, 
through a set of Boolean properties, it can be specified whether the product belongs to 
the mechanical, electrical, or electromechanical categories. A more detailed categori-
zation is provided through assigning the Central Product Classification2 (CPC) code 
to each instance of the product class. CPC is a comprehensive classification of prod-
ucts based on the physical characteristics of products and is aimed at providing a 
framework for standardization of product classifications and promoting harmoniza-
tion of various types of statistics dealing with goods and services. Since taxonomies 
provide useful semantics by virtue of representing parent-child relationships, CPC and 
                                                           
2  http://unstats.un.org/unsd/cr/registry/cpc-2.asp 

828  
F. Ameri and S. Allen 
other similar taxonomies are used extensively in the product ontology. Also, the in-
dustries related to a given product instance can be specified through hasRelatedIn-
dustry property which has the Industry class in its range. The instances of the  
Industry class are categorized based on North American Industry Classification 
System (NAICS) taxonomy.   
The BOM-related information is described through the relationships between 
Product, Assembly, and Part classes. An instance of the product class can have one or 
more parts or sub-assemblies. A product can also be a single part such as a gear or 
shaft. In this situation, the physical attributes of the product such as material and di-
mensions are directly derived from the properties of the single part connected to the 
product through hasComponent property.  The properties of the Assembly class are 
very much similar to those of the Product class. However, an instance of the Assem-
bly class should have at least two assemblies or parts attached to it in order to be re-
garded as a valid instance of the Assembly class. This condition is formally specified 
in the ontology through a constraint, or axiom, as shown in Fig. 1.  
 
Fig. 1. The concept diagram of the product class 
The Part class has more detailed generic attributes that specify the overall shape, 
manufacturing features, tolerances, surface finish, material, and coating type of the 
part along with the dimensions of the bounding box. These properties can be used for 
inferring the manufacturing requirements of the parts and their upper-level assem-
blies. Furthermore, each part can have a set of specific attributes pertaining to the 
particular category the part belongs to. For instance, an ejection pin can be further 
described through some attributes such as shoulder length, head diameter, and head 
thickness. These attributes are classified as sub-classes of the hasPartAttribute 
property. All the properties discussed so far are categorized as asserted properties, the 
properties that are directly specified by the user or available in the CAD model.  
However, capitalizing on the reasoning services enabled by a formal DL-based  

 
An Ontological Approach to Integrated Product and Process Knowledge Modeling 
829 
ontology, one can infer new properties not explicitly specified by the user or the  
geometric model. These properties are referred to as inferred properties. Some exam-
ples of inferred properties are complexity level, machinability, and fixturing difficul-
ty. The inferred properties are not part of the core properties of the product ontology 
and the intention is to allow users to introduce their own desirable properties depend-
ing on their particular search scenario. More details on how to infer part properties are 
provided in the next section. 
4 
Part Clustering and Characterization 
This section describes a methodology for part clustering and characterization based on 
the product ontology presented in this work. The objective of part clustering and  
characterization is to provide a unified framework for quantitative comparison of 
different parts based on their asserted and inferred properties. The scope of this me-
thodology is limited to individual machined parts with varying levels of geometric 
complexities. Fig. 2 shows a representative set of parts that are included in the scope 
of the part characterization methodology. However, the underlying techniques pre-
sented here can be applied to other processes as well. The ability to effectively quanti-
fy the similarities of parts within a design repository is one of the key requirements 
for accurate formation of part families. A methodology for measuring part similarity 
is proposed Mun et al. [10]  which mainly focuses on numeric and textual properties 
of the attributes part instances but it does not take into account the information con-
tained in the taxonomic structure of the ontology.  
 
 
Fig. 2. Sample parts that are included in the part clustering methodology 
The proposed methodology combines schema-based approach (by utilizing the 
taxonomic structure and axiomatic definition of the concepts) and instance-based 
approaches (by utilizing the property values given for product instances) for part  
classification and similarity analysis. The main steps of part clustering and characteri-
zation process are described below. 
Step 1: Part Classification 
In the first step of the proposed methodology, parts are classified based on a given 
array of geometric and non-geometric criteria such as part material, required 
processes, types of machining features, overall shape and dimensions, main function, 
and the associated industry.  The purpose of part classification step is to construct an 
initial set (S) of parts (p) that have enough commonalities to render a meaningful 
comparison among them. The output of this step is the set S = {p1, p2, … pn} with size 
n whose members are instances of the Part class. The set S narrows down the search 
space into a smaller set of similar parts. The part instances in the set S are selected 

830  
F. Ameri and S. Allen 
either explicitly through picking one or more existing named part classes from the 
ontology, such as PumpPart or AerospacePart, or implicitly through building a tem-
porary class of parts that possess a set of desired properties. The former class specifi-
cation approach is referred to as schema-based approach, whereas the latter is called 
instance-based approach in this work. 
Schema-based Classification:  For schema-based classification, DL reasoners such as 
Pellet 3 or HermiT4 are employed to automatically classify the available sub-classes 
of the Part class represented as named classes in the ontology. These classes are 
defined formally through a set of logical constraints, thus lending themselves to au-
tomated classification. If a certain class is selected by the user for similarity analysis, 
then the instances of the selected class, along with the instances of the associated sub-
classes, will be added to the set S for further analysis.  
Instance-based Classification: Semantic Web Rule Language (SWRL) rules are used 
in order to create temporary classes that define the search space S. The following 
SWRL rule, for example, represents a class of parts, called DesiredPart, that are 
made of aluminum, belong to the hardware manufacturing industry, and have a cylin-
drical shape with a diameter less than 2 inches. The class, DesiredPart will be  
temporary added to the ontology by the reasoner on the fly and then all instances 
available in the ABox, that conform to the specified requirements, are retuned as in-
stances of the DesiredPart to be included in the comparison set S.  
 
Part(?p) ^ isMadeOf(?p,?m) ^ Aluminum(?m) ^ hasRelatedIndustry 
(?p,?i) ^ HardwareManufacturing(?i) ^ hasShape(?p,?s) ^ Cylind-
er(?s) ^ hasDiameter(?p, ?d) ^ swrlb:LessThan(?d, 2) 
 DesiredPart(?p) 
 
Step 2: Comparison Factor Selection 
In this step, the factors (F) for part characterization and clustering are identified. 
Some examples for comparison factors are level of precision, level of complexity, 
machine setup difficulty, and manufacturability. A pairwise arrangement of the fac-
tors results in creation of one or more two-dimensional comparison planes. It should 
be noted that the value of a comparison factor for a given part might not be directly 
obtainable from the ontology and extra reasoning and inference processes might be-
come necessary to arrive at the factor value or level for each part. For example, the 
level of complexity, as a possible comparison factor, has no explicit definition man-
dated by the ontology and it can be interpreted differently for different part families. 
The factor pair (Fx, Fy) represents the axes of the 2D comparison plane. Each part 
instance serves as a data point to be positioned in the comparison plane based on its 
coordinates along each axis.  
 
Step 3: Normalization 
Depending on the choice of comparison factor, each axis of comparison might have 
different scales and units. Therefore, in order to provide unified metrics for comparison, 
                                                           
3 http://clarkparsia.com/pellet/ 
4 http://hermit-reasoner.com/ 

 
An Ontological Approach to Integrated Product and Process Knowledge Modeling 
831 
it is first necessary to normalize the min and max values on each axis on a [0,1] scale. 
To this end, each comparison factor undergoes a normalization process based on the 
minimum and maximum values available in the entire population.    
)
(
)
(
)
(
j
j
j
p
F
p
F
F
Min
F
Max
F
Min
S
S
i
j
i
j
−
−
=
′
   
 (1)
Where  
i
j
p
F
S
 is the score of the ith part along the jth factor, Min(Fj)  and Max(Fj)   
are the minimum and maximum value respectively for the jth factor based on the 
given set S, and  
i
j
p
F
S′
 is the normalized score of the ith part in the set along the jth 
factor. For example, if the machinability rating is selected as the jth factor of compar-
ison, and given the min and max values of machinability to be 75 and 124 respective-
ly across the entire comparison set, then the normalized score of the ith part with the 
machinability rating of 95 is calculated as: 
 
i
j
p
F
S′
=(95-75)/(124-75)= 0.4 
(2)
The normalized score, in itself, does not convey much information and it should al-
ways be treated as a relative measure.  
Step 4: Clustering  
Once the data points (i.e., part instances) are positioned in the comparison plane, they 
can be clustered for characterization and similarly analysis. Different approaches can 
be adopted for part clustering: 
Four-Quadrant approach ( Fig. 3-a):  In this approach, the comparison plane is di-
vided into four identical quadrants through splitting each axis. This approach is de-
vised when the attributes represented by both axes are more qualitative or discrete in 
nature and a high-low scale can sufficiently describe the attribute without significant 
lose of information.   The count of manufacturing features on a part or the number of 
machined faces are examples of such attributes. Also, one may choose to treat more 
quantitative and continuous attributes such as size or weight as high-low attributes 
just to perform a rough comparison based on these attributes.  
Rule-based approach ( Fig. 3-b):  In this approach, SWRL rules are used to create 
part clusters or bubbles that satisfy the requirements set by the rules. For example, a 
rule can be generated for identifying all part instances that belong to automotive in-
dustry and need 5-axis machining. In fact, both instance-based technique and schema-
based techniques used for creating the initial set of part instances in step 1 can be 
applied here to create more specific sub-sets for a more refined comparison.   
k-nearest neighbor approach ( Fig. 3-c): In this approach, part groups are formed 
based on their Euclidian distance form a given reference part. The objective is to iden-
tify the top k parts that are closer to the reference part in the two-dimensional compar-
ison space.   
Closeness radius approach (Fig. 3-d): In this approach, the part instances that are 
within a given proximity threshold from the reference part form a cluster.  
 

832  
F. Ameri and S. Allen 
 
Fig. 3. Different approaches for creating part clusters (a) four-quadrant approach (b) rule-based 
approach (c) k-nearest neighbor approach (d) closeness radius approach 
5 
Implementation 
This section describes a proof-of-concept implementation of the proposed methodology. 
There are two main implementation areas is this work, namely, the ontology and the part 
characterization and clustering programs. The ontology is developed in Protégé 4.1 and 
ontology manipulation is conducted using OWL API for OWL 2.0. Ontology population 
is carried on semi-automatically; meaning that the property values are partially extracted 
from the CAD file in an automated fashion. The information items that are obtained from 
the CAD file are mainly geometric in nature and include values such as the overall di-
mensions, removal volume, mass, and type of manufacturing features and their corres-
ponding attributes. Also, the required manufacturing services, such as milling, turning, 
and drilling are automatically identified through using a pre-defined feature-to-process 
mapping incorporated in the system. More detailed information on automated feature 
recognition and service identification techniques used in this work  can be found in [11].  
Other non-geometric information such as material, coating type, product type, and related 
industries are entered manually through the Java interface shown in Fig. 4. Using this 
interface, all the asserted properties of the parts are collected and saved as an 
OWL/XMLfile. The OWL/XML files are used as the input for the part characterization 
and clustering programs. The lower right corner of Fig. 4 shows the user interface of the 
part characterization program that infers various characteristics, or inferred properties, of 
the individual parts based on their asserted properties. Some of the inferred properties of 
the parts that are used in this implementation include complexity, machinability, and 
fixturing requirements. Both asserted and inferred properties can be used for the  
purpose of part characterization and clustering. However, the inferred properties usually 
provide more in-depth understanding of various part features and characteristics.  

 
An Ontological Approach to Integrated Product and Process Knowledge Modeling 
833 
 
Fig. 4. The user interface for entering the explicit properties of the part directly and indirectly 
(through the CAD file) 
Fig. 5 depicts an example scenario for comparison of three parts based on fixturing 
difficulty and machinability factors. In this example, Part B is closer to Part A com-
pared to part C with respect to the Euclidian distance in a 2D plane built by fixturing 
complexity and machinability as the comparison factors.   
 
 
Fig. 5. Three sample parts compared based on their machinability and fixturing complexity 
6 
Conclusion 
This paper introduced an ontology for integrated representation of process and prod-
uct knowledge. The proposed ontology uses OWL as the ontology language. The 
purpose of the product and process ontology in this work is to enhance the intelli-
gence of the design repositories through annotating their contents with formal seman-
tics. A part clustering and characterization methodology was introduced to support 

834  
F. Ameri and S. Allen 
semantic search and retrieval process in engineering design. The proposed methodol-
ogy adopts both schema-based and instance-based approaches. Future work in this 
area include validation of the ontology and the clustering methodology based on a 
larger sample of parts through comparison with human expert judgment and also en-
hancement of the axiomatic definitions of product categories. Additionally, automated 
ontological learning and cognition in design repositories is another promising re-
search avenue that calls for further investigation.  
References 
[1] 
Chandrasegaran, S.K., Ramani, K., Sriram, R.D., Horváth, I., Bernard, A., Harik, R.F., 
Gao, W.: The evolution, challenges, and future of knowledge representation in product 
design systems. Computer Aided Design (2012) 
[2] 
Ameri, F., Dutta, D.: Product lifecycle management: Closing the knowledge loops. 
Computer-Aided Design and Applications 2(5), 577–590 (2005) 
[3] 
Badder, F., Calvanese, D., McGuinness, D.L., Nardi, D., Patel-Schneider, P.F.: The de-
scription logic handbook: Theory, implementation and applications. Cambridge Univer-
sity Press (2010) 
[4] 
Fenves, S.J.: A Core Product Model For Representing Design Information No. NISTIR 
6736 National Institute of Standards and Technology Gaithersburg, MD (2001) 
[5] 
Gorti, S.R., Gupta, A., Kim, G.J., Sriram, R.D., Wong, A.: An object oriented represen-
tation for product and design processes. Computer Aided Design 30(7), 489–501 (1998) 
[6] 
Rachuri, S., Baysal, M., Roy, U., Foufou, S., Bock, C., Fenves, S.J., Subrahmanian, E., 
Lyons, K., Sriram, R.D.: Information models for product representation: core and as-
sembly models. International Journal of Product Development 2(3), 207–235 (2005) 
[7] 
Lee, J., Huh, S., Fenves, S.J., Sudarsan, R., Fiorentini, X., Sriram, R.D.: A semantic 
product modeling framework and language for behavior evaluatio. National Institute of 
Standards and Technology, Gaithersburg (2010) 
[8] 
Nanda, J., Thevenot, H.J., Simpson, T.W.: Product family design knowledge representa-
tion, integration, and reuse. In: Proc. International Conference on Information Reuse and 
Integration, pp. 32–37. IEEE (2005) 
[9] 
Bohm, M.R., Stone, R.B., Szykman, S.: Enhancing virtual product representations for 
advanced design repository systems. Journal of Computing and Information Science in 
Engineering 5(4), 360–372 (2005) 
[10] 
Mun, D., Cho, J., Ramani, K.: A method for measuring part similarity using ontology 
and a multi-criteria decision making method. In: Proceedings of the ASME Design En-
gineering Technical Conference, pp. 419–430 (2009) 
[11] 
Ameri, F., McArthur, C., Asiabanpour, B., Hayasi, M.: A web-based framework for se-
mantic supplier discovery for discrete part manufacturing. In: Proc. NAMRC/SME, So-
ciety of Manufacturing Engineers, pp. 99–107 (2011) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 835–844. 
DOI: 10.1007/978-3-642-30817-8_82 
© Springer-Verlag Berlin Heidelberg 2013 
 
Know-How Identification, Scoring, and Classifying  
in Product Development Processes 
Mathias Lojewski, Christoph Bärmann, and Frank Mantwill 
Strategie und Prozessplanung, Volkswagen AG, Berliner Ring 2, Wolfsburg, 38440, Germany 
Helmut-Schmidt-Universität, Holstenhofweg 85, Hamburg, 22043, Germany 
MRP, Helmut-Schmidt-Universität, Holstenhofweg 85, Hamburg, 22043, Germany 
mathias.lojewski@volkswagen.de, 
{chris.baermann,frank.mantwill}@hsu-hh.de 
Abstract. The following paper describes the development of a method for iden-
tifying, scoring and classifying captive process know-how in German enterpris-
es of the automobile industry. Object of study will be the product development 
process. An initial point is described, in order to illustrate the results of an em-
pirical analysis that lead to the necessity of protecting captive process know-
how methodology.  
After decomposing the selected product development processes (PDP) into 
rea-sonable steps, a special scoring system is used. Subsequently, the process 
de-pendencies between each other are being analyzed using multidimensional 
scal-ing. Within these steps the criticality of each process part concerning 
know-how protection is being scored. Using the method of scenario planning it 
is feasible to draw different possibilities of global cooperative partnerships in 
the future. These scenarios are the basis for classifying the identified process 
know-how for utilization in future cooperations. 
Keywords: know-how protection, international cooperation, product develop-
ment process. 
1 
Introduction 
The German automobile industry is facing new big challenges. The continuous dete-
rioration of the global economy and the persistent importance of the Asian market for 
the international automobile industry conspicuously illustrate that situation [5]. The 
changing preferences of German-Chinese cooperation towards increasing of corporate 
development projects lead to the necessity of reconsidering the cooperation models of 
German original equipment manufacturers (OEM).  
Furthermore the Chinese Government imposes strict conditions on western enter-
prises, which try to join the Chinese market. This attitude, also commonly known as 
Chinese protectionism, has the purpose to selectively open their market and,  
also, protect it from unwanted outer impact [17]. Especially key branches, like the 
 

836 
M. Lojewski, C. Bärmann, and F. Mantwill 
automobile industry, are rigorously controlled. For instance, the market entry for  
foreign enterprises is bound on acquiring in a Joint Venture (JV) with a Chinese OEM 
[4] [8] [22] [23].   
Within these collaborations, Chinese OEM effectively learn how to produce high 
quality and their aspirations concerning collaborative job splits has changed. The 
evolution of the collaboration processes between German and Chinese OEMs has 
being observed the past few years. At the beginning these collaborations where  
characterized by developing the products in Germany and locating the production in 
China. Today a large percentage of product development is now placed in China [9] 
[24]. Against this background, an empirical analysis had been conducted in order to 
point out the necessity of reconsidering German OEMs collaboration processes with 
Chinese JVs. The results of that analysis constitute the initial point of developing the 
methodology, which is presented in this paper. 
2 
Results of an Analysis 
As already mentioned above, it was the analysis’ intention to point out the necessity 
of reconsidering the collaboration processes in German-Chinese corporation projects. 
Furthermore optimization potential should be exposed.  Therefore, a representative 
German-Chinese cooperation project was selected for analysis. Using a semi-
structured questionnaire, eighty interview partners were questioned. Each interview 
was recorded and evaluated based on Mayrings procedures of interpreting content 
analysis [15].  
The implications of this research are manifold. The results of the evaluation point 
out the necessity not only of reconsidering the collaboration models in German-
Chinese cooperation projects, but also of reconstructing them.  
The data would seem to suggest, that one topic is of great importance. One of the 
main differences between the German and Chinese interviewees is the understanding 
of the contractual based know-how transfer. Whereas the Chinese members claim to 
need more transfer of process know-how than given by the German partners, German 
members are afraid of losing core competence while rendering its captive process 
know-how. In consequence of this an unrestricted sharing of captive process know-
how by the German partners could indeed entail losses of core competence. Otherwise 
a strict rejection of the Chinese requirements could disrupt the collaborative projects 
progressing [13]. 
Finally, the results lead to the conclusion that only the accurate knowledge about 
their own captive process know-how could protect the German OEM from losing its 
core competence.  Therefore the captive process know-how ensconced in the various 
processes need to be identified. To sum up, it is necessary to find a method which 
allows identification, scoring, and classifying of the enterprises captive process know-
how, in order to handle it properly in cooperation with a potential competitor.   

Know-How Identification, Scoring, and Classifying in Product Development Processes 
837 
3 
Methodology for Identifying, Scoring and Classifying Process 
Know-How 
Before starting to develop the above named method, a literature research exposed that 
there are numerous amounts of approaches for scoring know-how in enterprises. For 
example the know-how protection portfolio [25], the Knowledge-Asset-Management-
System [18] [19] [20], the APUD-model [21] [26], or the Competitive Advantage 
Valuation Method [21] [3] can be named. Unfortunately, most of these approaches 
deal with the focused topic on a very abstract level. Although giving valid theoretical 
background, the implementation into practice mostly fails [1]. Thus, the necessity of a 
method to be developed, which is of practical suitability, is implicated.   
The following executions explain a practically suitable method for identification, 
scoring, and classifying of captive process know-how in automobile planning 
processes.  
3.1 
Decomposing Assigned Processes 
Decomposing the advertised processes into reasonable parts is the first step of the 
method. Therefore, the different sub-processes of the product development process 
(PDP) are divided into process-parts, which are subdivided again into process-steps. 
This differentiation has to be realized by an expert, which at the best is located in a 
section where the example sub-process is situated primarily. Figure 1 shows the diffe-
rentiation using the layout planning process as example.  
3.2 
Scoring Single Processes 
The second step of the method, which is characterized by scoring the sub-processes 
individually, is divided into two parts. Using disqualification criteria, the process-
parts have to be rated concerning know-how protection criticality. Following [10] 
[11] [25] and based on patent system principles, five questions were defined which 
label each process-part into high, middle, or low criticality of losing captive know-
how. Following is the list of the mentioned questions: 
 
1. Is the respective process a key process for the whole project, or for any other 
enterprises project? 
2. Does the process require the necessity to unfold captive know-how or tech-
niques from other processes? 
3. Is the achievement of the project objectives essential for the project’s success 
in general? 
4. Is the process highly responsible for projects return on investment? 
5. Is there a high risk of copying the process by competitors? 
 
If these questions can be answered positively, a high criticality of losing captive 
process know-how is given. As a consequence that process should hardly be shared 

838 
M. Lojewski, C. Bärmann, and F. Mantwill 
with a potential competitor. If these questions can be answered negatively, there is no 
risk to share the process know-how with any partner. But if these questions cannot be 
answered either with yes or no, the second part of the step comes into play.  
Within that step, twelve criteria have to be scored for every process-step in the re-
spective process-part. Every criterion, also based on patent system principles and 
according to [14] [26] [16], has to be appraised in a Likert-related scale. Following 
the twelve criteria are listed: 
 
1. The respective process-step is innovative. 
2. The process-step contains a lot of the enterprises captive innovations. 
3. The process-step is unique compared with other branch competitors.  
4. A fast modification of the processes functionality could not yet be foreseen. 
5. The necessary level of education for this process is high and expensive.   
6. The process-step allows synergies with other processes of the enterprise. 
7. Time expenditure and financial effort for creating the process-step techniques are 
high. 
8. The 
process-step 
generates 
a 
great 
cost 
advantage 
towards 
branch  
competitors 
9. The process-step generates a great profit advantage towards branch  
competitors 
10. The costs for external creation of the process-steps outputs would be high. 
11. Outsourcing of that process-step means loss of image. 
12. It is impossible to keep the process-bound know-how or techniques secret.  
 
The results of appraising the questions are cumulated percentages for every process-
step, which are the base for classifying the process-parts and, finally, the whole sub-
process at a later stage. Firstly, the process dependencies have to be analyzed in step 
three. Figure 1 shows an example for single scoring using layout planning as  
sub-process. 
3.3 
Scoring Process Dependencies 
After scoring single sub-processes it is necessary to analyze process dependencies.  
Step one ensures that a lowly rated single sub-process, according to know-how pro-
tection, does not have any exchange of captive process know-how with a highly 
scored single sub-process. If this is the case, the scoring of the low rated sub-process 
has to be reconsidered. Otherwise loss of captive process know-how is highly  
probable.  
To achieve a valid result the use of multidimensional scaling has been proven as a 
reasonable method. On account of the fact that there are different techniques of multi-
dimensional scaling, it should be specified the selected technique for each step in 
accordance to [2]. Initially, a process-structure matrix, to be completed by an expert, 
is needed. This matrix contains the pairwise assessment of each sub-process with any 
other sub-process it is cross-linked to. It would be advisable to analyze both the  
quantity and the quality of the process-dependencies. The connection between the 

Know-How Identification, Scoring, and Classifying in Product Development Processes 
839 
advised sub-processes are based on input and output of information. The information 
that is to be rated as captive process know-how are bound on different know-how-
owners, which can be of personal, material or quasi-material source [12]. For rating 
the process dependencies, it is necessary to consider the amount and quality of the 
know-how owners bound information, which are to be transferred.  
 
 
 
Fig. 1. Example of single scored sub-process  
Figure 2 shows an example of a quality-related process-structure matrix on the ba-
sis of a pairwise comparison combined with a 9-point Likert scale. 
 
 
 
Fig. 2. Example of a quality-related process-structure matrix  
process-part
rating
 
0-5
concept layout
1
process-step 1: examine the tenders
63%
2
process-step 2: …
58%
3
…
…
…
…
…
…
layout workshop
1
process-step 1: presenting the premises
K.O.
2
process-step 2: …
K.O.
3
…
K.O.
…
…
K.O.
…
…
1
process-step 1: …
32%
2
process-step 2: …
41%
M3
3
…
…
4
…
…
…
…
1
process-step 1: …
…
2
process-step 2: …
…
3
…
…
…
…
…
process-steps
scoring
sub-
process
timeline in 
month
layout planning
high criticallity of losing captive know-how
low criticallity of losing captive know-how
middle criticallity of losing captive know-how
1
2
3
4
5
6
7
8
9
10
11
…
1
Layout Planning
2
Materials Handling
3
3
Logistic Planning
3
2
4
…
1
8
8
5
…
2
8
3
9
6
…
2
8
4
8
9
7
…
2
8
1
9
9
8
8
…
6
1
3
9
3
2
2
9
…
9
4
8
5
8
9
8
4
10
…
9
4
8
5
8
9
8
4
2
11
…
9
6
6
6
8
9
9
4
7
9
…
…
9
4
8
5
8
9
8
4
2
5
3
Sub-Process

840 
M. Lojewski, C. Bärmann, and F. Mantwill 
To ensure a useful interpretation the process-structure matrix data has to be trans-
formed into an easily comprehensible presentation. Using the method of multidimen-
sional scaling, it is possible to transparently decrypt the data. Figure 3 illustrates an 
example that is based on the Euclidean Distance Model. The advantage of this model 
is its simple indication of the sub-processes connections. The distance between each 
point represents the strength of the researched subject. In the present example it is the 
quality of the know-how related process-dependencies. A short distance means that 
there is critical mass of captive process know-how exchange. A long distance, on the 
other hand, stands for less exchange.  
After single scoring of every sub-process the PEP includes, it is possible to mark 
every process in the design-structure matrix with its individual scoring result. Thus 
the possibility of elucidating highly critical process-dependencies in the matrix is 
given. 
In Figure 3 an example for the layout planning process is presented. Obviously 
there are great dependencies to process 6 and 11. The matrix also shows that these 
processes have a high criticality concerning loss of captive process know-how. Hence 
a sharing of the layout planning process with a potential competitor could mean to 
loose captive process know-how hidden in process 6 and 11 as well.  
 
 
Fig. 3. Example of design-structure matrix using the rating method   
As a conclusion the single scored criticality should be reconsidered. Every single 
scored sub-process that appears negatively in the process-dependency scoring is 
marked relating to a 3-point marking-system, listed below: 
 

Know-How Identification, Sco
1. M1: quantity and quali
2. M2: high quantity and 
3. M3: quantity and quali
 
For example in Figure 1 th
pointed out, that using the p
ure process-dependencies. A
that is based on Quality Fu
[27]. The QFD-matrix is us
(HoQ) in which the matrix r
3.4 
Classifying Process
The foundation for classify
enabled to score every sub-
Also the process-dependenc
fy the identified processes,
process know-how.  
Because the criticality o
models, it is necessary to 
Therefore a useful method f
Without going into detail, 
opportunity to develop di
OEMs. Normally a worst c
using the scenario planning
on loss of captive process k
 
Fig. 4. Exa
On account of the fact 
know-how handling, only a
system, as it is illustrated in
oring, and Classifying in Product Development Processes 
ty of process-dependency is low 
low quality of process-dependency 
ty of process-dependency is high 
e layout planning process is marked with M3. It should
process structure matrix is not the only possibility to me
Another option would be the use of the correlation-ma
unction Development (QFD), created by Shigeru Mizu
sed to sum up the results by showing the House of Qua
represents the house’s roof.  
ses Based on Scenario Planning  
ying each sub-process is laid out. Up to this point one
-process, process-part, and process-step of the PEP sing
cies can be considered. Last step of the method is to cla
, which have a high criticality concerning loss of capt
of losing know-how is dependent on future collaborat
consider different models that are possible in the futu
for prognostication is the scenario planning method [6] 
the scenario planning method for this example gives 
ifferent future scenarios for collaboration with Chin
ase, a best case, and a trend scenario are developed wit
g method. Furthermore, every scenario has a unique imp
know-how.  
ample of scenario-based classification system   
that every process has its own characteristics concern
an experienced expert should accomplish the classificat
n Figure 4.  
841 
d be 
eas-
atrix 
uno 
ality 
e is 
gly. 
assi-
tive 
tion 
ure. 
[7]. 
the 
nese 
thin 
pact 
 
ning 
tion 

842 
M. Lojewski, C. Bärmann, and F. Mantwill 
The expert has to estimate the scenario-related circumstances impact onto the risk 
of know-how loss. Within that estimation, the expert sets the limits for criticality, 
marks and percentages of every single scored sub-process, process-part, or process-
step. 
For its appearance as a decision-making assistant tool the presented method can 
only give guidance for handling captive process know-how in collaboration projects 
with potential competitors.  
The application of the presented method shall be explained at the example of sce-
nario 1 of Figure 4 in which the risk of know-how loss is lowly rated. 
It starts with considering the scored criticality for the proposed sub-process. If this 
is rated middle or lower, or the process-dependency is marked with M3 or lower, 
there is no need to go into detail. The whole sub-process can be shared with any part-
ner. If the criticality is rated high, or the process-dependency is marked with M2 or 
higher, the whole sub-process should be kept secret and not be shared with any part-
ner. Every result that can be spaced between these extreme values needs to be indivi-
dually considered. Next step would be to take a look upon the process-parts, where 
only the scored criticality of know-how loss is determined. A middle or lower scored 
criticality leads to the decision to share the whole process-part with any partner. A 
high rated criticality entails the denying of process-part access for every partner. In-
termediate rating again has to be considered individually by precisely looking onto 
the process-steps. Now the percentages found in step 2, by single scoring the process-
steps, come into play. A scoring of 25% or lower enables the process-step for being 
shared with a partner. A scoring of 80% or higher blocks any partners access. The 
sharing or access-denying of an intermediate rated process-step needs to be indivi-
dually decided by an expert.  
4 
Conclusion 
Although it is very complex and elaborative, the method presented enables the user to 
identify, score and classify any sub-process of an automotive product development 
process concerning captive process know-how. Certainly the method contains some 
deficiencies concerning the continuously understanding of scoring. Thus it does not 
seem to be possible to ensure an absolutely unique consistency in the overall scoring, 
because of the high diversity of individual rating influences. In order to keep the defi-
ciencies to a minimum, one department should control the implementation of this 
methodology, and an expert team should moderate all data-collection. It could be 
useful to establish a standardized business process, which will enable consistent use 
of the above procedures.   
As a reminder, this method is to be understood as a decision-making assistant tool 
for handling captive process know-how in collaboration projects with potential com-
petitors.   
In addition, this methodology offers the possibility not only to understand it as a 
tool for protecting captive know-how, but also to use it in order to identify qualifica-
tion weaknesses in the project partner’s processes, in order to give advice on where 

Know-How Identification, Scoring, and Classifying in Product Development Processes 
843 
and how to optimize their qualification skills. The identified processes, which contain 
enterprises core competences in the form of captive process know-how, are unique. 
Potentially, this knowledge could improve every project partner’s process efficiency. 
References 
1. Binner, H.F.: Pragmatisches prozessorientiertes Wissensmanagement. In: Arbeitgeber-
verbände, B.D. (Hrsg.) Leistung und Lohn - Zeitschrift für Arbeitswirtschaft, 
450/451/452/453. Heider, Bergisch Gladbach (2008) 
2. Borg, I., Groenen, P.J.F.: Modern Multidimensinal Scaling: Theory and Applications, 2nd 
edn. Springer, New York (2005) 
3. Carmichael, D.R., Graham, L., Whittington, O.R.: Accountants’ Handbook. 2008 Supple-
ment, 11th edn. John Wiley & Sons, Inc. Hoboken, New Jersey (2008) 
4. Fischer, D.: Chinas sozialistische Marktwirtschaft. Informationen zur politischen Bildung: 
Volksrepublik China, Nr. 289/2005, pp. S.9–S.14 (2005) 
5. Flaig, M.: Die Automobilbranche in China und die Folgen der Finanz- und Wirt-
schaftskrise. Diplomica Verlag, Hamburg (2009) 
6. Gausemeier, J., Plass, C., Wenzelmann, C.: Zukuntsorientierte Unternehmensgestaltung: 
Strategien, Geschäftsprozesse und IT-Systeme für die Produktion von morgen. Hanser, 
München (2009) 
7. Grienitz, V., Ley, S., Schmidt, A.M.: Scenariobased Complexity Management by adapting 
the Methods of Social Network Analysis. In: The International Multi-Conference on Com-
plexity, Informatics and Cybernetics (IMCIC), Orlando, Florida, USA (2010) 
8. Hellmann, A.: Methodik zur Vorbereitung eines Markteintritts in China. Ein Leitfaden für 
Unternehmen. Diplomica Verlag, Hamburg (2007) 
9. Holtbrügge, D., Puck, J.F.: Geschäftserfolg in China. Strategien für den größten Markt der 
Welt. 2. Auflage. Springer, Heidelberg (2008) 
10. Huber, B.: Bedeutung von Know-how in der Wirtschaft. In: Ann, C., Loschelder, M., 
Grosch, M. (Hrsg.) Praxishandbuch Know-how-Schutz, pp. S.62–S.108. Carl Heymanns 
Verlag, Köln (2010) 
11. Huber, B.: Faktischer Know-how-Schutz in der Unternehmenspraxis. In: Ann, C., Lo-
schelder, M., Grosch, M. (Hrsg.) Praxishandbuch Know-how-Schutz, pp. S.591–S.620. 
Carl Heymanns Verlag, Köln (2010) 
12. Landwehr, S.: Know-how-Management bei der Gründung innovativer Unternehmen. 
Deutscher Universitätsverlag (2005) 
13. Lojewski, M., Mantwill, F.: Mit der Konkurrenz aus Fernost kooperieren. In: Wissensma-
nagement. Das Magazin für Führungskräfte, Heft 4/2012, Frankfurt, pp. 38–40 (2012) 
14. Matschiner, B.: Patente und andere gewerbliche Schutzrechte in Forschung und Entwick-
lung. Verlag Dashöfer GmbH, Hamburg (2010) 
15. Mayring, P.: Qualitative Inhaltsanalyse. Grundlagen und Techniken. Beltz Verlag, Wein-
heim und Basel (2003) 
16. Munari, F., Oriani, R.: The Economic Valuation of Patens. Methods and Applications. 
Edward Elgar Publishing Inc., Northampton (2011) 
17. Murray, G.: China: The Next Superpower. St. Martin‘s Press, New York (1998) 
18. Nagel, C., Mohr, C.: Die Wissensbilanz als Teilaspekt eines Knowledge-Measurement-
Systems. In: Mertins, K., Alwert, K., Heisig, P. (Hrsg.) Wissensbilanzen: Intellektuelles 
Kapital Erfolgreich Nutzen und Entwickeln, pp. S.121–S.138. Springer, Berlin (2005) 

844 
M. Lojewski, C. Bärmann, and F. Mantwill 
19. Nagel, C.: Von der Wissensbilanz zur Wissensbewertung: Das Knowledge-Asset-
Measurement-System. In: Streich, D., Wahl, D. (Hrsg.) Moderne Dienstleistungen: Im-
pulse für Innovation, Wachstum und Beschäftigung, pp. S.477–S.484. Campus Verlag 
GmbH, Frankfurt/Main (2006) 
20. Ribiere, V., Worasinchai, L.: Proceedings of the 8th International Conference on Intellec-
tual Capital, Knowledge Management & Organisational Learning. The Institute for Know-
ledge and Innovation Southeast Asia (IKI-SEA) of Bangkok University, Bangkok Thail-
and, October 27-28, vol. 1 (2011) 
21. Schwingenschlögl, T., Gotwald, A.: Wirtschaftliche Bewertungsmethoden für Patente: Pa-
tentbewertung für die Praxis. Linde Verlag, Wien (2008) 
22. Sieren, F.: Business Know-How China. So wird ihre Geschäftsreise zum Erfolg. Redline 
Wirtschaft, Heidelberg (2007) 
23. Staude, T.F.A., Theisen, C.: Mergers & Acquisitions in der Volksrepublik China. Ein Leit-
faden für Direktinvestitionen mit einer speziellen Einführung in die Problematik der Due 
Diligence. 1. Auflage. Düsseldorfer Wirtschaftsstudien, Hamburg (2000) 
24. Tao, H.: Globalisierung der Chinesischen Automobilindustrie. Eine empirische Untersu-
chung unter Berücksichtigung der chinesisch-deutschen Kooperation. Dissertation. Freie 
Universität Berlin, Berlin (2003) 
25. Wildemann, H.: Konzeptwettbewerb und Know-how-Schutz in der Automobil- und Zulie-
ferindustrie. TCW Transfer-Centrum-Verlag, München (2004) 
26. Wurzer, A., Reinhardt, D.: Handbuch der Patentbewertung. Carl Heymanns Verlag, Köln 
(2010) 
27. Yoji, A.: Quality Function Development. Integrating Customer Requirements into. Produc-
tivity Press, New York (2004) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 845–854.  
DOI: 10.1007/978-3-642-30817-8_83          © Springer-Verlag Berlin Heidelberg 2013 
How the Integration of Environmental Concerns 
Modifies the Integrated Design Process  
Maud Dufrene, Peggy Zwolinski, and Daniel Brissaud 
G-SCOP Laboratory, 46 Avenue Felix Viallet, 38031 Grenoble Cedex 1, France 
{maud.dufrene,peggy.zwolinski, 
daniel.brissaud}@g-scop.grenoble-inp.fr 
Abstract. Regarding the economical and environmental pressure from consum-
ers, legislation, and competitors companies need to be proactive and to propose 
new products in shorter times. For this, they need to change and adapt their de-
sign methods. To be effective, the design process must now address the entire 
life cycle of a product, from its definition to its end of life. Thus, integrated de-
sign is one solution to better design a product while considering and integrating 
all its aspects all along its life cycle. However, the environmental aspect is not 
so easy to integrate than another technical constraint. This paper aims at show-
ing the changes induced by the integration of environmental concerns into the 
integrated design process. It involves notably new data, new tools, new actors 
or still new strategies. 
Keywords: ecodesign, integrated design, environmental concerns, design 
process, design team. 
1 
Introduction 
Regarding the economical and environmental pressure from consumers, legislation, 
and competitors, companies need to be proactive and to propose new products in 
shorter times. For this, they need to change and adapt their design methods. The de-
sign phase is an important phase of the product life because it determines the success 
or failure of the commercial offer.  
The design process should not only be considered as an activity to solve problems, 
but as a complex activity to answer technical, social, strategic and economic concerns. 
Tools supporting the plurality of the domains and parameters to consider in a design 
process exist. Design for X tools is an example of this evolution. Those tools, issued 
from design for X methods, recommend a very early consideration of parameters that 
are usually considered very late in the design process (and so, often too constrained to 
be well considered). The « X » represents the Assembly (Design For Assembly), the 
Manufacturing (Design For Manufacturing, Design for Machining), the Recycling 
(Design For Recyclability), etc. They introduce a new point of view during the design 
process to describe not only the technological requirements but also all the future life 
cycle aspects of the product. 

846 
M. Dufrene, P. Zwolinski, and D. Brissaud 
 
With these new approaches considering the product life cycle, the way the organi-
zation supports design also has to be reconsidered. Concurrent engineering and then 
integrated design and simultaneous design are used to answer the common goal: to 
better design a product while considering all its aspects all along its life cycle.  
However, the environmental aspect is not so easy to integrate than another aspect or 
technical constraint. So, design approaches have to be developed to consider envi-
ronmental constraints early during the design process. This paper aims at showing the 
changes induced by the integration of environmental concerns into the integrated 
design process. It involves notably new data, new tools, new actors and moreover new 
strategies. Necessary elements and their relationships are identified for a good envi-
ronment integration. 
Section 2 presents a brief review of research background. The changes induced by 
ecodesign in companies are presented in Section 3. Section 4 described the required 
elements to support the integration of ecodesign in a context of integrated design. 
Section 5 draws the conclusions.  
2 
Background 
This section aims at presenting on one hand ecodesign and on the other hand concur-
rent engineering. The review of these two concepts is required because they are the 
founding principles for the issue of this paper.  
2.1 
Environmental Concerns in the Product Development  
Currently, mechanical designers provide technical solutions to meet companies’ and 
customers’ needs as the function to perform, the cost of the product, or its ability to be 
mass produced. However, increasing importance of the environmental issues forces 
product designers to consider certain environmental criteria in the design process [1]. 
Thus, ecodesign focuses on the integration of environmental considerations into prod-
uct development [2].  
Ecodesign covers any design activity which aims at improving the environmental 
performance of a product [3]. According to Bovea and Pérez-Belis [4], three key factors 
are required to optimize the design process in term of environmental performance:  
• The integration of the environmental aspects in the early stages of the product  
design process. 
• The consideration of a life cycle approach.  
• The consideration of a multi-criteria approach. 
Thus ecodesign, through these three features, helps the design team to improve the 
product environmental performance minimizing its environmental impacts during its 
whole life cycle. Different motivations lead a company to ecodesign. Gurauskiene and 
Varzinskas [5] present some drivers for eco-design: legislative pressure, cost savings, 
and emerging green markets. Van Hemel and Cramer [6] summarize the most influen-
tial external and internal stimuli (table 1).  

How the Integration of Environmental Concerns Modifies the Integrated Design Process 
847 
 
Table 1. The most influential external and internal stimuli for ecodesign [6] 
The most influential external stimuli 
The most influential internal stimuli 
Customer demands 
Innovational opportunities 
Government regulation 
Increase of product quality 
Industrial sector initiatives 
New market opportunities 
 
These drivers enable to better understand the ecodesign choices of companies and 
their associated changes according to the priorities given for each driver.   
2.2 
Concurrent Engineering  
In accordance with Poveda [7] we will present in this part the concurrent engineering. 
In order to develop a product a designer or more often a design team look for infor-
mation to generate and assess solutions that satisfy both the requirements and the 
constraints [8]. The product design process consists in a set of actions realized by 
different actors. Each actor has his own tasks but the design team work together. To 
facilitate the works, companies are now working with new organizations “project 
oriented”. 
Those organizations do not modify the structure of companies that is still based on 
divisions that can be design office, production methods, purchasing department, etc. 
Nevertheless, those new “project oriented” approaches create a group of persons that 
have to meet together to discuss and to establish compromises and design choices on 
a particular project. This organization is necessary to avoid a viewpoint not to be con-
sidered or to be aware too late that the solution could be better. 
In this organisation, the project manager is needed for conflict resolution. He man-
ages the market study and defines the economical and technical objectives. He is a 
direct link between experts who have their own knowledge in their field of expertise. 
Those experts have the ability to assess the project with their viewpoints and to pro-
vide the information to the project manager and to the other experts. 
Numerous propositions are done to model the design activity and to try to have this 
cooperation between the different actors that have to be considered during the life 
cycle. The simultaneous design concept is described as a design process where the 
design of the product, the design of the processes, the design of the manufacturing, 
the design of the assembly process, the distribution, the logistic for the use by users, 
and finally all the product life cycle characteristics are simultaneously considered. 
This can be done with two approaches described in the two following paragraphs. 
A Design Can Be Parallel. One speaks about parallel design when the tasks are 
shared among designers and that those design tasks are realised in parallel with the 
others. This parallelism is necessary to decrease development time.  
A Design Can Be Integrated. The integrated design aims at integrating early during 
the design process the constraints from different expertises to allow CAD tools to help 
the design and not only to assess too late the proposed solutions.  

848 
M. Dufrene, P. Zwolinski, and D. Brissaud 
 
The objective is to integrate the life cycle actors during the design process and to 
provide them all the necessary data to think about the solution and to allow them to 
act on the product definition. It is not only a problem of knowledge formalisation. The 
problem is to create new tools to favour cooperation between the different actors hav-
ing different viewpoints on the product during its definition. 
From the Willingness to Integrate to the Need for Cooperation. The integrated 
design aims at allowing all disciplines, all experts concerned by the product under 
development to intervene on its design, and take into account the different experts 
viewpoints to make decisions all along the design process. But, either if those objec-
tives are clear, it is not easy to implement the approach. A real difficulty consists in 
the translation of those recommendations and constraints into product requirements. 
And a more difficult question is how the different viewpoints can coexist for different 
steps of the product life cycle. There are often antagonisms (i.e. manufacturing con-
straint and aesthetics). 
So, in this process starting from the expert viewpoint to a translation of the re-
quirements, is there any general rule to assume those translations? Is there any prior-
ity between the different viewpoints? How to establish the best compromise for a 
product optimise regarding all the viewpoints? 
That is the issue for integrated design and those questions are addressed to all the 
actors of the design process and new relations between them have to be established. 
During a project, they construct specific knowledge and know-how related to their 
own expertise. To really cooperate, they need to change the way they were used to 
work, considering only their field of expertise. They have to go toward cooperative 
works. In that case, they have to modify a little bit their goals and usual references. 
This modifies radically the design process. The goal is not only to share data but to 
share and modify in-depth their own expert logic. 
3 
Changes Induced by Ecodesign in Companies 
The integration of environmental concerns induces changes in the company to be 
effective. Indeed, to integrate environmental goals into a design process is not like the 
integration of another technical constraint and organizational aspects have to be con-
sidered. In accordance with Le Pochat [9], we will show how this new viewpoint, 
considering a multi criteria approach during the assessment and the improvement of 
the product involves important changes in a company. 
3.1 
Data Fluxes  
Numerous data need to be compiled to be able to realize environmental assessment 
and environmental improvement of the products. The data can be technical, organiza-
tional but also sociological. Moreover, those data have to be identified inside and 
outside the boundaries of the company, from the raw material extraction phase to the 
end-of life phase. This inventory shows that beyond the classical designers’ teams, 

How the Integration of Environmental Concerns Modifies the Integrated Design Process 
849 
 
eco-design projects need the involvement of all the divisions of the company. Berto-
luci et al. [10] showed on an industrial example that the informational fluxes were 
transformed and that a real transverse approach is necessary inside and outside the 
company. Sarkis [11] showed that when strategic decisions have to be made at  
the strategic level of the company, they have to modify their internal organization and 
the relations with the customers and the supply chain. 
As mentioned by Gondran [12], environmental data are necessary to manage envi-
ronmental impacts for a company and the network of data is really important to  
integrate environmental aspects during the design process. The more a company con-
structs relations with others, the better environmental aspects are integrated. 
Two points have to be underlined: 
• On one side, the necessary environmental data are outside the boundaries of the 
company and are spread on numerous suppliers, subcontractors, customers, recyc-
lers, etc.  
• On the other side, those data are not directly available. Indeed, the need for those 
data appears gradually with the eco design emergence in companies. Those needs 
were not relevant before, so the data were not collected. 
This show the necessity to create those environmental data fluxes to complete the 
existing data fluxes. Many companies are now working on those questions. Indeed, 
this modification is not trivial for the company, because: 
• The data networks are not usual. This leads the company to modify the habits and 
relations with the life cycle partners. 
• The data are rare and spread, that generate difficulties in the collection inducing 
time consuming processes and additional costs. 
3.2 
Partnerships  
One of the barriers for the eco-design approach in companies is the separation be-
tween the « environment division » and the classical structures (the « green wall »). 
And yet the representation of the fluxes linked to environmental concerns in the com-
pany and toward external partners shows that all the actors of the company are con-
cerned by eco-design. Beyond the actors of the company, industrial partners from the 
supply chain have to be implicated in the eco-design processes. This implies a net-
work involving internal and external partners and changes in their works.  
3.3 
The Design Process 
Sherwin and Bhamra [13] state that ecodesign implies a concurrent engineering 
process. But the integration of environmental aspect during the design process is also 
depending on the use of new tools, on new design process and new knowledge [14]. 
Tonnelier [15] also underlines that technical aspects have to be considered for eco-
design, but also management aspects. 

850 
M. Dufrene, P. Zwolinski, and D. Brissaud 
 
So, the new organisation for eco-design, based on concurrent engineering, should 
consider the following transformations during the design process: 
• The use of new tools (eco-design tools). 
• The creation of new indicators to be able to assess the product under design from 
an environmental point of view. 
• The use of new data. 
• The implementation of new procedures to allow the definition and validation to 
take into account environmental constraints into the product requirements. 
3.4 
Companies Strategies 
Integrating eco-design involves changes within the corporate strategy [11]: 
• At the level of its policy. 
• At the level of the strategic approach of the product development, i.e. for the defi-
nition of the specifications. 
Policy: The Environment as a Value. The company has to define the environment 
as a value in order to explain its involvement among the workforce of the company. 
This involvement is necessary [15] to ensure the success of the integration.    
Thus, the integration of this environmental constraint changes the hierarchy of 
usual values of the company (performance, quality, cost, etc.). This hierarchy has then 
to be redefined. Millet [14] mentioned a paradigm shift in the business. 
This change in the corporate strategy will contribute to modify the communication 
system of the company, both internally (information, involvement and motivation of 
staff) and externally (marketing, CSR, etc.). 
Definition of the Specifications. Defining the product specifications is difficult in the 
evolving context of the company. The integration of eco-design, changing the influ-
ence of each constraint to each other, will force the company to change its business 
strategy to enable the team to prioritize constraints, and define the specifications. 
3.5 
Knowledge and Skills 
The eco-design integration, through the integration of a new and complex constraint, 
modifies necessarily the required knowledge. All the modifications within the  
company presented in this section require knowledge and skills which need to be 
created because they do not culturally exist in the company. They will enable to de-
fine the strategy, use the eco-design tools and manage the environmental data of the 
product. 
Jacqueson [17] declares that this environmental knowledge is the driver of the eco-
design integration.  

How the Integration of Environmental Concerns Modifies the Integrated Design Process 
851 
 
4 
Elements to Support Changes 
To support these changes, we propose different elements to facilitate the integration 
of ecodesign in an integrated design. This section describes these different elements 
and their connections. 
4.1 
Actors 
As described in a previous part, a design team in constituted to develop a product. 
This team consists obviously of designers from the design office but it also includes 
other actors from different departments, like research and development, methods, 
production, purchasing department, etc.  The design team is managed by a steering 
team, usually by an only person called the project manager.  In a context of integrated 
design, this project manager has a multidisciplinary role. He ensures the coordination 
between the different actors and their points of view in order to meet all the  
constraints.  
Moreover, considering ecodesign, an environment expert is strongly required in 
order to manage eco-design and the environmental issues in the product design proc-
ess. This expert is called in this paper the environmental supervisor. Indeed, the pro-
ject manager needs to be assisted by this environmental expert because he usually 
does not have the skills to understand the environmental data, the environmental indi-
cators and thus he cannot take informed decisions. The environment supervisor can be 
an environment expert of the company or if there is no environmental expert, it can be 
a consultant. In the same way, an environmental expertise can be required occasion-
ally within the design process to manage environmental issue in a particular field. The 
project manager can learn the environmental skills through experience over time. 
Thus the environmental supervisor is no longer necessary. 
Environmental data coming from the suppliers are required to ecodesign a product. 
The suppliers are therefore requested to share information about their products, com-
ponents, materials, factories or other. The shared data will be useful to realize the 
environmental and cost analysis of the designed product. These close relationships 
between the supplier and the design team are quite new. 
4.2 
Different Requirements 
This part is dedicated to the description of elements that should be required to carry 
out the development of an eco-product in integrated design. These elements should 
help all the actors to communicate, to share their work and to have a global view of 
the design process. The objective of these elements is to help the project manager in 
the decision-making process.  
The Sustainability Calculation Module. The sustainability calculation module con-
sists of three modules: a simplified life cycle assessment (S-LCA), a life cycle cost 
module (LCC) and a specific calculation module for ad-hoc indicators. This module is 

852 
M. Dufrene, P. Zwolinski, and D. Brissaud 
 
managed by the environmental supervisor and returns indicators. Data to carry out the 
calculations come from the suppliers and from the designer tools. 
Indicators. Indicators are useful to represent the product and guide the design. 
Different types of indicators can be used: environmental indicators coming directly or 
indirectly from a life cycle analysis (LCA), cost indicators, indicators usually used by 
the designers, e.g. the energy efficiency of electrical motors, and other ad hoc 
indicators, e.g. recyclability rate.  
Dashboard. The dash board consists of a panel of suitable indicators to represent the 
product and then guide the design towards the objectives. The project manager in 
collaboration with the environmental supervisor chooses indicators which meet the 
main objectives. Thus the project manager has to prioritize because it is not possible 
to manage all indicators. This set of relevant indicators changes according to the 
company but also according to the project. These indicators will be visible by all the 
actors involved in the design process. Most indicators are generated in the sustainabil-
ity calculation module but some of them come directly from the designer tools. This 
dashboard enables to have an overview of the most relevant indicators and thus make 
easier the decision-making. 
Reports. Two kinds of reports are generated from the calculation model with the 
assistance of the environmental supervisor towards different goals. The first kind of 
reports is dedicated to the project manager and consists of an environmental report 
and a cost report. These reports associated to the dashboard aim at having a compre-
hensive vision of the product and its weaknesses. The second type of reports is  
dedicated to the designers. It could be report from different viewpoints: component 
viewpoint, product point of view (assembly, etc.) or other viewpoints, such as materi-
al, distribution, etc. 
According to the results described in the report, specific rules and guidelines are 
suggested to the designers in order to improve the critical points of the product hig-
hlighted by the reports.   
Rules/Guidelines. Rules and guidelines for ecodesign are used to help designers to 
improve the environmental performance of the product. 
4.3 
Interaction between the Different Elements 
Fig. 1 represents the main elements described in this section and their relationships. 
To sum up, the project manager is in charge on one hand of the product model and on 
the other hand of the dashboard including indicators. The environmental supervisor 
works on the sustainability calculation module to get indicators and reports. Obvious-
ly, the project manager and the environmental supervisor work together to manage the 
project and to make the decisions. The designers use their tools to realize their tasks 
and satisfy the requirements and the constraints 
 

How the Integration of Environmental Concerns Modifies the Integrated Design Process 
853 
 
 
Fig. 1. Elements described in this section and their relations 
The different tools and indicators are already developed but what is important is 
the elements dynamics and in particular the relationships between tools managed by 
the environmental supervisor and classical design tools managed by designers. The 
good choice of tools and methods is also essential and it must be done depending on 
the activity fields, the design process and the eco-design objectives of the company.      
5 
Conclusion  
This paper showed the consequence of integrating environmental concerns into a 
design process and more precisely in the context of integrating design. Some charac-
teristic stay constant whereas others have to evolve. 
This integration of environment does not modify the functioning of the design 
process organization. The objective stays as in classical integrated design to integrate 
the life cycle actors and their different viewpoints during the design process. Moreo-
ver the need of cooperation between the different expert and also with the project 
manager is not changing. 
However the integration of environmental issues involves changes and evolutions 
for a number of things. Thus, we show the needs to create environmental data fluxes, 
to extend the relationships with the suppliers, to have an environmental expert, to 
develop rules, guidelines and indicators for ecodesign, or still the need of involvement 
from the company. All these criteria contribute to the success of integration. 
Different required elements were identified in order to support these changes into 
the design process. Then, these elements will be useful to describe an ecodesign me-
thodology developed within the G.EN.ESI project. The methodology will enable to 
help designers in their environmental design choices.  
Acknowledgments. This research is carried out in the framework on the G.EN.ESI 
project founded by the European Union within the seventh framework. This European 
project involves 8 partners from Academia, Research and Industry.  
 
Indicators
Indicators
Indicators
Sustainability 
calculation module
Designer tools 
Dashboard & 
Environmental and cost 
reports
Product 
model 
Environmental supervisor 
Project manager 
Designers 

854 
M. Dufrene, P. Zwolinski, and D. Brissaud 
 
References 
1. Ilgin, M.A., Gupta, S.M.: Environmentally conscious manufacturing and product recovery 
(ECMPRO): A review of the state of the art. Journal of Environmental Management 91(3), 
563–591 (2010) 
2. Karlsson, R., Luttropp, C.: EcoDesign: what’s happening? An overview of the subject area 
of EcoDesign and of the papers in this special issue. Journal of Cleaner Production 14(15), 
1291–1298 (2006) 
3. Hauschild, M.Z., Jeswiet, J., Alting, L.: Design for Environment — Do We Get the Focus 
Right? CIRP Annals - Manufacturing Technology 53(1), 1–4 (2004) 
4. Bovea, M.D., Pérez-Belis, V.: A taxonomy of ecodesign tools for integrating environmen-
tal requirements into the product design process. Journal of Cleaner Production 20(1), 61–
71 (2012) 
5. Gurauskienė, I., Varžinskas, V.: Eco-design methodology for electrical and electronic 
equipment industry. Environmental Research, Engineering and Management. Aplinkos Ty-
rimai, InžInerija ir Vadyba 37(3), 43–51 (2006) 
6. van Hemel, C., Cramer, J.: Barriers and stimuli for ecodesign in SMEs. Journal of Cleaner 
Production 10(5), 439–453 (2002) 
7. Poveda, O.: Technical management of concurrent engineering projects, process modeling, 
analysis and instrumentation. PhD thesis of Institut National Polytechnique de Grenoble 
(2001) (French) 
8. Janthong, N., Brissaud, D., Butdee, S.: Combining axiomatic design and case-based rea-
soning in an innovative design methodology of mechatronics products. CIRP Journal of 
Manufacturing Science and Technology 2(4), 226–239 (2010) 
9. Le Pochat, S.: Ecodesign integration in SMEs – proposal for a knowhow appropriation me-
thod for environmental product design. PhD thesis of Ecole Nationale Supérieure d’Arts et 
Métiers, Centre de Paris (2005) (French) 
10. Bertoluci, G., Le Pochat, S., Le Coq, M.: Integrating ecodesign: what are the implications 
for companies? In: 6ème Congrès International de Génie Industriel, Besançon (2005) 
(French) 
11. Sarkis, J.: A strategic decision framework for green supply chain management. Journal of 
Cleaner Production 11, 397–409 (2003) 
12. Gondran, N.: System of information dissemination to encourage SMEs to improve their 
environmental performance. PhD thesis of INSA Lyon and Ecole des Mines Saint Etienne 
(2001) (French) 
13. Sherwin, C., Bhamra, T.: Early ecodesign integration experiences from a single case. The 
Journal of Design Research 1(2) (2001) 
14. Millet, D.: Integration of environment in design - Companies and sustainable development. 
Science Publications, Lavoisier (2003) 
15. Tonnelier, P.: Proposing an approach to integrate a new design constraint: case of the re-
covery of end of life vehicles in PSA Peugeot Citroën. PhD thesis of Ecole Nationale 
Supérieure d’Arts et Métiers, Centre de Paris (2002) (French) 
16. Argyris, C.: Knowing for acting. Overcoming barriers to organizational learning. Dunod, 
Paris (2000) (French) 
17. Jacqueson, L.: Integration of environment in company: Proposing a tool to manage the 
process of environmental knowledge creation. PhD thesis of Ecole Nationale Supérieure 
d’Arts et Métiers, Centre de Paris (2002) (French) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 855–864. 
DOI: 10.1007/978-3-642-30817-8_84 
© Springer-Verlag Berlin Heidelberg 2013 
 
Towards Integrating Sustainability in the Development  
of Product/Packaging Combinations 
Jos de Lange, Ellen Oude Luttikhuis, Roland ten Klooster, and Eric Lutters 
Laboratory of Design, Production and Management, Faculty of Engineering Technology  
University of Twente, Enschede, The Netherlands 
{j.delange,e.j.oudeluttikhuis}@utwente.nl 
Abstract. An auspicious approach to increase sustainability of a prod-
uct/packaging cycle is to investigate the dependencies between the different life 
cycle phases and corresponding actors. For any actor, the perspective on the 
global cycle initiates from his own position; therefore, it is required to make the 
entire life cycle more transparent by showing and interpreting the dependencies 
as well as the correlations and sensitivities of the different phases in the life cy-
cle. The emerging actor network can serve as a fundament for an approach that 
allows for the meaningful comparison of various aspects of sustainability dur-
ing the development life cycles of product/packaging combinations. 
Keywords: packaging, life cycle assessment, integration, development cycle.  
1 
Introduction 
Sustainability is an important topic in the field of packaging, which is reflected by the 
embodiment of the topic in mission statements of many of the large organizations in 
the field. Economic plans are complemented with specific ecologic goals and social 
protocols, adhering to Brundtland’s original definition of sustainable development. 
Within development trajectories organizations are also more aware of the need and 
use for sustainability, doing justice to people, planet and profit. However, external 
influences put these developments under pressure.  For instance, the rapid growth of 
economies and accompanying increase in prosperityseverely threatens the availability 
and accessibility of resources. Together with an ever-increasing population it is a 
challenge for organizations to develop product/packaging combinations that advocate 
a livable world for next generations. 
Packaging, especially those used for fast moving consumer goods (FMCG) have an 
indirect but essential role in fulfilling the concept of sustainability, as its contribution 
to health is indispensable; without packaging it would be impossible to conserve and 
transport FMCG in the existing supply chain. Yet, these important aspects sometimes 
seem to be used as a permit for the use of scarce resources, over-specified packaging, 
or using the package as billboard. However, in adhering to sustainable development, it 
is inevitable for organizations to take ecologic responsibility for their activities. Si-
multaneously, economics are the driver of current society and aspects as profitability 
and solvability are indispensable for organizations. Thus while social aspects form the 

856     J. de Lange et al. 
underlying ratio, the balance between economic and ecologic aspects is decisive in 
integrating sustainable development in the field of packaging for FMCG. 
While the need for sustainable development is recognized by the industry and 
moves towards a standard prerequisite in development trajectories, the translation of 
mission statements, goals and protocols in adequate working methods for everyday 
practice is not obvious. Many tools and techniques for assessing sustainability exist, 
yet none is tailored to the development of product/packaging combinations; usually 
they do not account for the importance and particularities of the conjoint (develop-
ment) life cycles of the product and the packaging.  
This publication describes an approach that allows for the purposeful comparison 
of analysis methods for development life cycles of product/packaging combinations. 
In this, the corresponding working methods are elaborated on as well. Moreover, first 
insights on integrating LCA methodology and sensitivity analyses in earlier phases of 
product/packaging development cycles are depicted. 
2 
Sustainable Packaging 
For fast moving consumer goods, the packaging often is an inherent and indispensable 
prerequisite to be able to transport, sell and use the product. Consequently, the life 
span of FMCG packaging generally depends on the life span of the content. Given the 
resources and energy spent in producing, transporting and using both the product and 
the packaging, the influence of the packaging on the ‘degree of sustainability’ of the 
combination becomes significant. 
In fulfilling its functions the packaging is mostly subservient to the content. This is 
all the more true for FMCGs in which packaging have a larger influence on the entire-
ty. Without its packaging, sprinkles for instance would be well-nigh impossible to 
transport or use. In serving its intended purpose, the product is thus largely dependent 
on the packaging. Moreover, the packaging becomes an intrinsic part of the product. 
Therefore the definition for packaging that is used in this research is: ‘Packaging is a 
(set of) physical artifact(s) that temporarily or unremittingly assumes the functions 
preserving, protecting, enabling use & handling and conveying formal & informal 
information of the related product.’ 
The relation of packaging with sustainability is two-sided. By containing, protect-
ing and preserving its content, packaging contributes to important social aspects of 
sustainability like health and need for food. Especially for FMCG, packaging foster 
the transport and use of food and drinks. On the other hand, packaging often only 
partly fulfill the intrinsic expectations that are embedded in the concept of sustainabil-
ity. For instance, packaging materials are often made from scarce resources that are 
disposed when the packaging has fulfilled its functions. Furthermore, the energy 
needed to produce the packaging is not always in balance with its relative short life 
span. Therefore, it is obvious that mission statements of large organizations in the 
field aim at reducing use of packaging materials. However, important prerequisites 
such as fulfilling the packaging functions and preventing product failure, are lacking. 
These preconditions are important because in many cases, the product or content 
has a higher environmental impact than the packaging [1]. If the packaging fulfills its 
functions it has a role of leverage; its potential impact on the environment is  less 
than the potential impact of the content. The packaging can make the entirety more 

   Integrating Sustainability in the Development of Product/Packaging Combinations  
857 
sustainable. In other words, an investment in packaging can prevent a product from 
having an environmental impact that exceeds the investment in the packaging by far. 
3 
Existing Tools and Techniques 
For applying sustainability in the various development processes of prod-
uct/packaging combinations, it is important to investigate the existing  tools involved 
in developing product/packaging combinations. Presumably, in many cases, the de-
gree of sustainability is determined in hindsight. Consequently, sustainability is often 
applied as an a posteriori check, instead of as an aspect that is inherently integrated in 
the development cycle. It would obviously be beneficial to use a life cycle assessment 
for making decisions based on environmental impact during the development phase. 
However, this is often unfeasible, as such an assessment requires much and detailed 
data that is simply uncertain or not available during the (earlier phases of the) devel-
opment cycle.  
3.1 
Tools Evaluated 
Numerous tools have been developed for assessing sustainability in the development 
processes of products and packaging [2, 3]. When these tools are grouped on both the 
required maturity of the project and a global/ specific axis, three main groups can be 
distinguished: principles, guidelines, checklists and scorecards; quality function dep-
loyment (QFD); and analytical tools (figure 1). 
 
Fig. 1. Overview of tools and their applicability 
Principles, Guidelines, Scorecards and Checklists. These tools offer a qualitative 
way of evaluating different concepts or aspects of sustainability and are amongst the 
most applied tools in industry. Within these tools, a huge amount of potential useful 
knowledge is enclosed, usually via guidelines. However, in themselves guidelines 
cannot offer an indication of its own applicability or scope. This is all the more true 

858     J. de Lange et al. 
when conflicting guidelines arise. Guidelines thus provide an easy to use tool, while 
in reality it requires huge amount of knowledge and experience to actually adequately 
utilize them. In the hands of less experienced users, the guidelines can thus be easily 
misinterpreted or misused. 
Sustainable Quality Function Deployment. Several tools based on Quality Function 
Deployment (QFD) are specifically aimed at sustainable development. As with cus-
tomer demands and wishes, these tools try to translate sustainable demands and wish-
es into requirements for a product or packaging. While the translation into more  
specific requirements is useful, the input is often largely dependent on existing guide-
lines and principles. This makes QFD aimed at sustainable development subjective to 
the same disadvantages. However, the technique does have a unique potential in anc-
horing the basic functions of packaging in a sound requirement specification and thus 
applying the sustainable principle described in paragraph 2. 
Analytical Tools. Analytical tools are almost always based on the standardized LCA 
methodology, in which the product/packaging life cycles are modeled and the degree 
of sustainability is calculated using an impact assessment method. Life cycle assess-
ments are a reliable and adequate technique for determining the degree of sustainabili-
ty, provided they are executed in the appropriate and intended manner. However, 
reliable data is needed and the decisions that are made about for example used data-
bases, impact assessment method and weighing factors, have to be transparent to ap-
propriately interpret the results [4]. Tools based on LCA thus heavily rely on detailed 
information and data sheets, which are often simply not known or available yet in the 
earlier design phases. Furthermore, the proper use of a LCA methodology asks for 
expert knowledge and software, which is not an expertise that is embodied in normal 
businesses within the field.  
3.2 
Conclusion Available Tools 
Existing tools offer a lot of guidance and support in evaluating the environmental 
impact. However, none of the available tools offers the quantitative and specific re-
sults while being applicable for all development phases. The research described in this 
paper is aimed at filling this gap by using the functional basis offered by tools based 
on QFD and incorporating life cycle thinking and best practices from the standardized 
LCA methodology as a basis. Making it applicable in the earlier development phases, 
the described problems with data and expertise are challenges to be faced. 
4 
Life Cycles of Product/Packaging Combinations 
4.1 
Global Life Cycle 
A common way of depicting the global life cycle of a product is from cradle to grave, 
consisting of the following phases: raw material, production, distribution, use and 
disposal. Both product and packaging pass though such a life cycle. Because of their 
inherent dependencies, the life cycles of product/packaging combinations are more 
complex than the individual life cycles for products. Reason for this is the partial 
concurrence of the two life cycles. Figure 2 shows the overall product/packaging chain. 

   Integrating Sustainability in the Development of Product/Packaging Combinations  
859 
In the overlapping phases, the product and packaging form an interacting entity in 
which the packaging fulfills its functions. Since these functions are strongly related to 
the product, it is inevitable to take the product into account. 
 
 
Fig. 2. Global product/packaging life cycle 
4.2 
Actor Network 
For each product/packaging combination, the overall chain is more or less similar. 
While this depiction serves as a basis for understanding of the life cycles for e.g. leg-
islation, the representation is not yet complete. In reality many more life cycles are 
incorporated in the product/packaging chain. Examples are life cycles from machi-
nery, transport packaging, labels, ink, etc..  
The life cycle of product/packaging combinations is thus more complex than 
shown in figure 2. Zoom in and the global life cycle transforms in an amalgamation of 
actors. For every actor, the local chain looks similar;  inputs are coming from the 
preceding actors in the chain, the involved actors converge the inputs and the result-
ing output is sent to the next actors in the chain. Connecting all these links renders a 
network with all actors that are involved in the development of a product/packaging 
combination. An example of such an actor network is shown in figure 3.  
In theory, an actor network is infinite, in using the network as visualization of the 
product/packaging life cycle, the network accommodates according to the focus point 
and case. This makes it possible to structure the network from every point of focus. 
Using an actor network, the product/packaging life cycle becomes more clear and 
accurate for the  various involved actors. It gives a more detailed insight into the 
actors that are involved in the life cycles of product/packaging combinations.    
4.3 
Influences and Dependencies 
Mutual dependencies and influences can exists within the network, resulting in differ-
ent types of relations between actors [5]. In determining these influences and their 
bandwidth, the so-called Customer Order Decoupling Point (CODP) can be used. This 
theory originated from the field of production engineering [6]. It defines the point(s)  
 

860     J. de Lange et al. 
 
Fig. 3. Actor network 
in a production process that separates the entire process in two  parts: one outside the 
reach of external actors and a part that is organized to act and respond to the influence 
of these actors. In a similar manner the CODP can be used for determining the various 
relations that exist in the entire network. Making the points relative to the actor and 
changing the customer into subsequent actors, the theory renders a better assessment 
of the inherent dependencies as well as the influences of  the various actors. Due to 
this influence and dependencies certain parts of the network can be considered varia-
ble or static depending on the perspective of the actor. 
4.4 
Conclusion Product/Packaging Life Cycles 
The emerging actor network does justice to industrial practice and makes the underly-
ing differences visible. It incorporates the daily practice of each stakeholder involved, 
while simultaneously focusing on the entire chain. It models the level of aggregation 
on which sustainable visions, missions and goals need to be incorporated in develop-
ment cycles. The network forms a good foundation for a new approach which investi-
gate different aspects like sustainability. 
5 
Envisaged Approach 
5.1 
Aim 
The network described in section 4 provides insight in the various relations between 
the stakeholders involved and the information about stakeholders and their products. 

   Integrating Sustainability in the Development of Product/Packaging Combinations  
861 
This network, and the information it contains, can underpin the analysis of different 
aspects - like sustainability - throughout the entire development life cycle of prod-
uct/packaging combinations. In order to facilitate developers of product/packaging 
combinations to adequately employ such analyses, a structured and transparent ap-
proach is required that allows for the effective and efficient manipulation of the in-
formation in the network. To make this approach instrumental in product/packaging 
development, the development of a facilitating tool is the appropriate way. The result-
ing tool can be used to make deliberate and well-considered decisions and allows for 
unequivocal communication between stakeholders and external parties.  
The communication that is enabled between the stakeholders does not only depend 
on the quantitative data that substantiates life cycle analyses. Especially the different 
perspectives of the different stakeholders render the ability to provide stakeholder-
specific decision support based on the network and corresponding, available and 
evolving information. In this, stakeholders can address their own focus and selection 
of aspects and environment, while simultaneously mapping out the overlaps and inter-
faces in the network. In doing this, the relative importance of individual aspects can 
be balanced against the context of the overall development cycle.  
5.2 
Scope  
The tool will be developed to analyze product/packaging combinations. In this field of 
expertise, it is not uncommon for stakeholders to have overview of just a specific 
section of the entire network involved. As a consequence, probably not all stakehold-
ers know each other. Even more, in theory the stakeholder network is infinite, having 
many unexpected, implicit connections and links stemming from different contexts. It 
is therefore impossible to take all the processes into account when analyzing the net-
work. The goal definition and the stakeholders that are involved determine which 
processes are in scope during the assessment. 
The advantage of the network is that it renders a dynamic structure that can trans-
form depending on the point of view. As mentioned, the network and corresponding 
information are addressed differently by every stakeholder that is involved. With the 
tool , for example, the material processor can use the network to investigate the con-
sequences on the network regarding to for example the environmental impact. Simul-
taneously, for the waste processor it is possible to determine the consequences on the 
life cycle when changing for example the location of waste processing. Even the con-
current attention for varying levels of aggregation is possible. 
5.3 
Scenarios of Use 
Different scenarios from an industrial background are described to demonstrate the 
relevance of using an actor network for analyzing a variety of aspects such as sustai-
nability and implementing it into different phases of development trajectories. The 
three scenarios prospect the achievements made possible by the network principle and 
outline the high-level requirements for the tool based on the stakeholder network and 
the accompanying working methods.   

862     J. de Lange et al. 
Scenario 1. With goals addressing recycling percentages achieved while reaching the 
limitations of material reduction, a large manufacturer decides to shift its focus to-
wards the entire network of the product/packaging combination. As the environmental 
impact of the product is significantly higher than the packaging, focus is on the avoid-
ance of food spoilage. Aiming at a more sustainable combination, a new balance is 
sought between the product and the packaging. With the tool, this balance between 
the environmental impact of the product and the packaging can be explored and the 
ecologic consequences for the product/packaging chain when using more material for 
the packaging can be investigated.   
Scenario 2. In FMCGs there are countless smaller companies that have expert know-
ledge on one aspect of the product/packaging chain and outsource all other issues 
while aiming to maintain coordination and control. The decisions made in selecting a 
kind of packaging or logistics are made without always knowing the consequences for 
the entire life cycle and the degree of sustainability. The use of the stakeholder net-
work gives such organizations more insight and control over the process. For in-
stance, the tool can offer guidance in whether to fill wine bottles at the winegrower or 
close to the point of sale at a dedicated location. The possibility arises to give compa-
nies with restricted resources and budgets a meaningful comparison between various 
chain configurations on costs and ecologic impacts. 
Scenario 3. For a packaging manufacturer, the replacement of machinery is a far-
reaching decision that asks for careful consideration and trade-offs between e.g. effi-
ciency and costs. Complementing these more or less standard procedures, the tool 
addresses life-cycle aspects that are more difficult to quantify. For making a thought-
out decision, the tool is needed to analyze machine configurations with respect to 
different aspects. A first important aspect that should be investigated to assess the 
environmental impact is matching the supply with demand, thus whether the new 
capacity that assumingly creates the supply, fits the demand of the customers and the 
intended end users [7]. Furthermore, the desired configuration and needed flexibility 
to produce different packaging on a machine can be taken into account, translating 
future demand into requirements for the machine. The replacement of the machine 
could have consequences for many product/packaging life cycles involving different 
stakeholder that might have opposing interest. These possible conflict are always a 
problem in a life cycle were many stakeholders are involved. With the network these 
possible conflicts can be assessed. 
5.4 
Requirements 
The scenarios in section 5.3 are examples of the broad spectrum of scenarios that can 
be brought together to depict the actors, environments and aspects that together define 
the modus operandi of the tool [8]. In other words, the scenarios depict the possible 
future situations in which the tool may be employed. The scenarios need to be trans-
lated into functional specifications and subsequently into technical specifications that 
ultimately guide the actual implementation and realization of the tool. 

   Integrating Sustainability in the Development of Product/Packaging Combinations  
863 
From a practical viewpoint, the envisaged tool must make it possible to examine 
the consequences for different sections of different life cycles. The result of the tool 
can be an overview containing the consequences on the life cycles regarding for ex-
ample costs, energy use and match of supply & demand. As a basis for multi-variable, 
multi-stakeholders decision making, this overview renders a well-structured, transpa-
rent and objective representation that allows for mutual comparison of aspects be-
tween actors. Therefore, every actor can make a conscious decision based on the  
outcome of the assessment.  
A specific requirement that needs to be addressed in the development of the tool is 
related to the availability, certainty and reliability of the information that is required 
or used in the development cycle. As mentioned, information can be lacking, or in-
complete, which may directly influence the result of any assessment as well as the 
reliability of that result. Often, quantifiable uncertainties propagate throughout the 
assessment, but more often sheer assumptions implicitly influence the result. As a 
consequence, the sheer values used as inputs might even interfere with the applicabili-
ty of the result. The tool thus needs to be capable of not only including the sheer 
quantification of the inputs, but also of including the sensitivity related to these in-
puts. Even more, if information on sensitivities is available and the actual value of 
inputs is lacking or uncertain, the tool may be able to give a more accurate result than 
when only using inadequate input values. Therefore, next to performing ‘traditional’ 
ways of impact assessment, the tool needs to be able to incorporate more advanced 
and elaborate ways of determining impacts while taking into account uncertain or 
lacking information.  
6 
Concluding Remarks 
In the field of the development of product/packaging combinations, there is a clear 
need for more transparent and comparable assessment strategies as concerns e.g. sus-
tainability, but also cost, safety etc. Up to now, there are too many approaches that are 
all based on valid, yet dissimilar inputs, rendering outcomes that have the hint of sub-
jectivity – to say the least. In deriving a set of requirement specifications that can 
drive the development of an assessment approach in a top down manner, it is envi-
saged that a tool can be established that can determine and compare impact assess-
ment information in the wide field of expertise involved in the development of  
product/packaging combinations. For this purpose, it is important to take into account 
the inherent complexity of the actor network that underlies the product/packaging life 
cycle. 
While exploring this actor network and the processes and aspects that play a role in 
the development cycles involved, the first contours of a requirement specification for 
a tool and its architecture are uncovered, giving a first guideline for developing a tool 
that will enable to gather more experience and expertise in the field, thus giving the 
ability to adjust, refine and contribute to the requirement specification. 
By validating the first drafts by means of case studies and scenarios from industry, 
the underlying principles as well as the applicability in daily practice can be assessed, 

864     J. de Lange et al. 
from which further (subsets of) specifications and working methods can be deter-
mined. In the context of that architecture, the tool will be able to adequately support 
developers of product/packaging combinations independent of the position in the 
chain, independent of the phase of the development cycle and independent of the 
specific field of expertise of the actor.  
Acknowledgement. The authors acknowledge the support of the Dutch ministry of 
Economic Affairs, Agriculture and Innovation. 
References 
1. Williams, H., Wikström, F.: Environmental impact of packaging and food losses in a life 
cycle perspective: a comparative analysis of five food items. J. of Cleaner Production 19, 
43–48 (2010) 
2. Ramani, K., Ramanujan, D., Bernstein, W.Z., Zhao, F., Sutherland, J., Hand-werker, C., 
Choi, J.-K., Kim, H., Thurston, D.: Integrated Sustainable Life Cycle Design: A Review. J. 
Mech. Design 132(9) (2010) 
3. Byggeth, S., Hochschorner, E.: Handling trade-offs in Ecodesign tools for sustainable prod-
uct development and procurement. J. of Cleaner Production, 14, 1420–1430 (2006) 
4. Lewis, H., Verghese, K., Fitzpatrick, L.: Evaluating the sustainability impacts of packaging: 
the plastic carry bag dilemma. Packag. Technol. Sci. 23, 145–160 (2010) 
5. Boons, F.: Greening products: a framework for product chain management. J. of Cleaner 
Production 10, 495–505 (2002) 
6. Olhager, J.: The role of the customer order decoupling point in production and supply chain 
management. Computers in Industry 61(9), 863–868 (2010) 
7. Kooijman, J.M.: Environmental Assessment of Packaging: Sense and Sensibility. Environ-
mental Management 17(5), 575–586 (1993) 
8. Lutters, D., Ten Klooster, R.: Functional requirement specification in the packaging devel-
opment chain. CIRP Annals - Manufacturing Technology 57(1), 145–148 (2008) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 865–874. 
DOI: 10.1007/978-3-642-30817-8_85 
© Springer-Verlag Berlin Heidelberg 2013 
 
Tolerance Specification Optimization  
for Economic and Ecological Sustainability  
Steven Hoffenson, Andreas Dagman, and Rikard Söderberg 
Department of Product and Production Development,  
Chalmers University of Technology, Gothenburg, Sweden 
{stevenh,andreas.dagman,rikard.soderberg}@chalmers.se 
Abstract. In the final stages of product development, dimensional tolerances 
are specified by designers to ensure high functionality at low costs. A tradition-
al approach to this decision-making process is to minimize economic losses to 
the manufacturer and the consumer. This paper presents a new approach for to-
lerance allocation optimization that considers sustainability not only from  
economic costs but also ecological costs. The framework is formulated as a 
multi-objective optimization problem and explored with a case study on the de-
sign of an automotive body panel. Results of the case study include Pareto fron-
tiers of non-dominated optimal solutions along with a parametric study to  
explore the influence of material choice on the results. 
Keywords: tolerance allocation, variation propagation, sustainability, cost  
minimization, multi-objective optimization. 
1 
Introduction 
The quality of products is partially determined when designers select tolerances to 
specify in engineering drawings. Tolerance selection is a necessary step in product 
design since no product dimension can be manufactured with perfect accuracy, and a 
tradeoff exists because tighter tolerances increase manufacturing costs and looser 
tolerances often inhibit the assemblability and functionality of the product. Poor as-
semblability can result in a high number of defective parts and products being dis-
carded as well as time wasted. Poor functionality, which is typically associated with 
consumer perception of quality, can result in products being discarded and replaced 
early in the expected life cycle along with customer dissatisfaction and lowered brand 
reputation. Discarding parts and products is not only an economic concern for manu-
facturers and consumers, but it is also an ecological issue, since resource and energy 
usage in the production and disposal of replacement parts can be significant. 
This article presents a multi-objective modeling and optimization framework for 
considering the economic and ecological ramifications of tolerance selection in the 
final stages of product design. The design approach is demonstrated through a case 
study that considers the tolerance specifications for an automotive body panel, where 
specified tolerances influence a critical dimension that affects the product’s assem-
bled functionality. Results include Pareto frontiers as well as a parametric study to 
introduce the influence of material choice on the optimization results. 

866 
S. Hoffenson, A. Dagman, and R. Söderberg 
The ensuing section discusses previous literature on tolerance analysis and varia-
tion propagation, previous efforts in tolerance-cost minimization, and techniques for 
environmental impact assessment. Section 3 presents the framework, methodology, 
and models used in the case study. Optimization results for the tolerances in the au-
tomotive panel are delivered in Section 4 and discussed in Section 5. The final section 
offers conclusions and discusses the broader implications of the work. 
2 
Background 
Dimensional tolerance allocation is a subject currently addressed at several confe-
rences and journals, and there is abundant literature discussing years of investigation 
on variation propagation and cost optimization in the domain of tolerance specifica-
tion. Sections 2.1 and 2.2 discuss the state of the art in these two areas, respectively. 
The novelty of the present approach is the addition of ecological sustainability as an 
objective for tolerance allocation, and Section 2.3 provides a brief background of 
research on ecological sustainability and how it is managed in general applications. 
2.1 
Tolerance Allocation and Variation Propagation 
Tolerances of product dimensions must be chosen carefully due to the aforementioned 
tradeoff between quality and cost, and they are often chosen based on the ways that 
each dimension influences the functional requirements of the part or product. Va-
riance in one measurement of a part or product is said to propagate and influence the 
resulting variance in other measurements. Techniques to calculate the relationships 
and propagation between specified tolerances and critical dimension variations are 
often referred to as tolerance analysis and synthesis [1].  
Designers typically set tolerances based on either worst-case design scenarios or 
statistical analyses of tolerance probability distributions and the associated critical 
dimension variations. This is most simply calculated using linear or linearized toler-
ance accumulation models, where the tolerances of components assembled in series 
are summed or root-sum-squared to predict the variation across the length of the 
whole [2]. A more common method is statistical tolerancing, where researchers and 
practitioners typically assume normal distributions for prescribed tolerances and cal-
culate the distribution of the critical dimension [3]. When this calculation is imprac-
tical due to complex geometries, Monte Carlo methods are employed, in which a 
number of randomly-selected tolerances are generated as inputs to measure the result-
ing output variations [4]. 
All of these methods of analyzing and synthesizing tolerances have been imple-
mented in a variety of commercial and proprietary computer software packages to aid 
in tolerance allocation and, in some cases, optimization [5,6]. 
2.2 
Tolerance-Cost Minimization 
Typical tolerance optimization is conducted with the objective of minimizing manu-
facturing costs, seeking to improve the manufacturer’s economic sustainability while 
preserving product functionality. The two challenges in doing so are in understanding 

 
Tolerance Specification Optimization for Economic and Ecological Sustainability 
867 
the relationships between specified tolerances and critical dimension variations, as 
discussed in the previous section, and in modeling the relationships between specified 
tolerances and costs. Data linking manufacturing costs and tolerances depend on a 
number of environmental factors and are typically proprietary, so researchers often 
use simple mathematical functions to describe these relationships for various manu-
facturing processes [7]. In many cases, the production of a component has more than 
one eligible manufacturing process, and piecewise functions or discrete tolerance-cost 
points can then be used for optimization [8,9]. Curves are generally fit to a set of cost-
tolerance data when available, but it is common for researchers to present methods 
using assumed or generic cost-tolerance curves due to either an absence of data or 
unwillingness to publish proprietary data. One popular function is the reciprocal func-
tion, shown in Equation (1) where c is cost, t is tolerance, and a and b are parameters 
fit to match actual or estimated costs [10-12]. 
c = a + b/t 
(1)
Some previous work in this area treats costs as losses to the manufacturer, where the 
loss is the difference between the cost to manufacture a certain tolerance and the min-
imum possible manufacturing cost, which is typically associated with loose tolerances 
[13-15]. This removes fixed costs from the equation and allows comparison of finan-
cial costs with functionality losses due to loose tolerances as described by Taguchi et. 
al [16]. The process capability index is commonly used in this work to measure 
process performance, normalized to three standard deviations from the mean of a 
tolerance distribution [17,18]. Söderberg [13] extends the loss function technique with 
an additional objective representing loss to the customer, as parts with looser toler-
ances are more likely to fail early during the use phase of a product. 
2.3 
Ecological Sustainability Metrics and Assessment Tools 
In addition to influencing manufacturing costs, product quality for the manufacturer, 
and quality to the customer, tolerance selection affects the ecological impact of a 
product. Factors including choices of manufacturing processes, time and electricity 
requirements, material usage, and rates of defective parts produced link tolerances 
with ecological sustainability. 
Ecological, or environmental, sustainability has been increasingly studied and de-
bated in recent decades, particularly in the context of Life Cycle Assessment (LCA) 
or Life Cycle Engineering (LCE) techniques, which specifically target the cradle-to-
grave impact of products and processes [19]. A number of environmental impact da-
tabases, standards, and software packages have been proposed to aid in designing for 
the health of the planet. These measurement tools include extensive environmental 
impact databases that associate ecological impacts with various human actions, and 
many of them include user interfaces to aid in identifying inputs and analyzing out-
puts [20,21]. Since the outputs typically fall under different categories of impact, such 
as resource depletion, greenhouse gas emissions, air pollution, water pollution, and 
landfill use, the tools use different techniques for presenting the results in a managea-
ble format. These include equating ecological impacts with monetary values [21] or 
normalizing them against an average consumer’s annual impact [22].  

868 
S. Hoffenson, A. Dagman, and R. Söderberg 
3 
Methodology 
The present study combines tools and techniques from the literature to present a 
framework for analysis and optimization of tolerances to minimize costs and ecologi-
cal impacts. The approach is illustrated in Figure 1. 
 
 
Fig. 1. Framework of product tolerance-cost-environmental-impact relationships 
Here, the late-stage design problem is considered where the product function, ar-
chitecture, and geometry have been decided upon, and batch size, locator positions, 
materials and manufacturing processes are parameters. Parameters are not allowed to 
vary during optimization, but they may be modified between optimizations to demon-
strate how they influence the solution; this is referred to as a parametric study, and 
Section 4.2 shows the results of such a study on material choice. The ensuing subsec-
tions describe models that link the input parameters and variables to the outcomes in 
Figure 1, and with them a bi-objective optimization problem is formulated. 
3.1 
Modeling 
The following sub-sections describe the models used to calculate the links in Figure 1 
between tolerance specifications and quality, manufacturing parameters and cost, and 
manufacturing parameters and environmental impact. The models are built around the 
case study of a D-pillar in an automotive vehicle body, which is comprised of two 
stamped sheet metal components. 
 
Variation Propagation. To model the propagation of variation from prescribed toler-
ances to critical functional dimensions, the software package RD&T is used [13]. This 
program is designed specifically for the purpose of analyzing variation propagation in 
complex geometries and includes a graphical user interface for creating models and 
visualizing results. The D-pillar used in this paper is shown in Figure 2, and it is sub-
ject to manufacturing variations in places where the parts are supported by other 
frame components and at the mating surface of the two parts. The functional require-
ment is that the three-dimensional position of the upper-rear corner, denoted with 
black squares in Figure 2, be located near the nominally designed coordinates.  

 
Tolerance Specification O
The allowed variation fo
extent on the judgment of t
case, the variation in all thre
but parametric studies are r
this decision. 
Cost Modeling. As previo
not widely available or pu
reciprocal manufacturing co
in Ding, the equation param
Ecological Impact Modeli
deoffs among costs and eco
a monetary-based rating sy
mental Priority Strategies i
sustainability, and it contain
with environmental impac
(ELU) [21]. In this rating sy
cost of one Euro (€). 
Environmental impacts f
the production phase, the 
impacts for creating variou
well as end-of-life impacts
combustion (for combustib
volves disassembling and 
ELUs, since reusing a mate
raw minerals. This study re
strating the benefits of desig
Optimization for Economic and Ecological Sustainability 
or these critical dimensions is unclear and depends to so
the designer and the design of the connecting parts. In 
ee coordinate dimensions is allowed within one millime
recommended to understand the sensitivity of the result
 
Fig. 2. D-pillar model in RD&T 
ously discussed, cost data on manufacturing processes 
ublishable. In this study, like in much of the literature
ost function is assumed with the form of Equation (1).
meters are set at a = 0, b = 1 [11]. 
ing. Since the objective of this article is to discuss the 
ological impacts when making late-stage design decisio
ystem is used to present environmental impacts. Envir
in product design (EPS) is one such metric for ecolog
ns an extensive database that links materials and proces
ts, quantified in terms of the Environmental Load U
ystem, one ELU is equivalent to an environmental dam
for a component such as the D-pillar of a vehicle come
use phase, and the end-of-life. EPS provides product
us materials and manufacturing with different processes
s for different scenarios, including disposal in a land
ble materials), and reuse. The reuse scenario, which 
recycling the components, is typically assigned negat
erial reduces future needs to produce usable materials fr
eports results for the landfill and reuse scenarios, dem
gning with disassembly and reuse in mind. 
869 
ome 
this 
eter, 
ts to 
are 
e, a 
 As 
tra-
ons, 
ron-
gical 
sses 
Unit 
mage 
e in 
tion 
s, as 
dfill, 
in-
tive 
rom 
mon-

870 
S. Hoffenson, A. Dagman, and R. Söderberg 
The use phase of an automotive part is almost entirely dependent on mass, as high-
er-mass parts will require more fuel combustion over the life of the vehicle. Studies 
consistently indicate that a 10% mass reduction on a vehicle results in a 7% reduction 
in fuel consumption [23]. This is extrapolated and scaled for every percentage of mass 
change in this study. Using the assumptions that the full vehicle mass is 1800 kilo-
grams, the baseline fuel consumption is 10 liters per 100 kilometers, and the baseline 
calculation that the D-pillar weighs 4.78 kilograms, the impact of the D-pillar on fuel 
consumption is calculated. The use phase impact is then calculated using the assump-
tion that an average vehicle drives 300,000 kilometers over its lifetime and the EPS 
specification that unleaded petroleum costs the environment 1.12 ELU per kilogram. 
The environmental impact of the use phase is thus calculated by the amount of fuel 
required by the weight of the D-pillar over the life of the vehicle, compared to how 
much fuel would be consumed if the vehicle had no right-side D-pillar. 
3.2 
Optimization Framework 
A multi-objective optimization formulation has been developed around the use of 
these three modeling tools. To simplify the optimization problem, the response sur-
face method (RSM) is used on the computationally-intensive model of variation prop-
agation in RD&T, where each simulation uses the Monte Carlo method with 5000 
points. Based on 3600 complete simulations with full-factorial sampling of the two 
input tolerances on a range of 0.05 to 3.00 millimeters, a polynomial response surface 
was fit using linear regression to generate a closed-form mathematical function for the 
probability of unacceptable critical dimensions in the part as a function of the input 
tolerances. This surrogate model is given as Equation (2), where ߮ is the percen-
tage of unacceptable parts and t1 and t2 are the input tolerances for the mating 
surfaces and support points, respectively. The model fits the data with a 0.9985 
coefficient of determination, suggesting that the structure of the model is acceptable. 
߮= ሺ0.71ݐଵെ7.96ݐଶ+ 29.51ݐଶ
ଶെ6.32ݐଶ
ଷെ0.96ሻ/100 
(2)
Using this surrogate model, a cost function is formulated with respect to tolerances 
and the percentage of discarded parts, shown in Equation (3) where C is the economic 
cost in Euros and Cmat is the cost of the materials in Euros, taken from [24]. The final 
term on the right side of the equation ሺ1 + ߮ሻ represents a multiplier to account for 
discarded products, as a larger percentage of discarded products raises the effective 
cost of producing acceptable products. 
ܥ= ቀܥ௠௔௧+ 1 ݐଵ
ൗ
+ 1 ݐଶ
ൗቁሺ1 + ߮ሻ 
(3)
An additional model for ecological impact is formulated using the percentage of dis-
carded parts and each of the factors from EPS. This is given as Equation (4), where E 
is ecological impact in ELUs and is a function of mass m and ecological factors Ei 
relating to material production, manufacturing process, the use phase, and end-of-life. 
ܧ= ݉൫ܧ௠௔௧+ ܧ௣௥௢௖+ ܧ௘௢௟൯ሺ1 + ߮ሻ+ ܧ௨௦௘ 
(4)

 
Tolerance Specification Optimization for Economic and Ecological Sustainability 
871 
Combining these models, a bi-objective optimization problem can be solved, formu-
lated as Equation (5). 
 
min௧భ,௧మݓ௖ܥ+ ݓ௘ܧ 
(5) 
Here, the two components of the objective function are economic cost C measured in 
Euros, and ecological impact E measured in ELUs, each with its associated weighting 
parameter w. Optimization is performed with respect to the tolerance input variables t1 
and t2. 
4 
Results 
Multi-objective optimization results are commonly presented as Pareto frontiers, 
where each point on a curve represents a Pareto-optimal design that cannot be im-
proved for one objective without a sacrifice to the other. Results for the D-pillar case 
study are presented in this section, starting with the baseline scenario and extending 
with a parametric study on material choice. 
4.1 
D-pillar Design Costs and Ecological Impact 
The baseline D-pillar is constructed with mild steel sheet metal and an allowance of 
one millimeter of variance in all three critical dimensions. The Pareto frontier demon-
strating the tradeoff between cost and environmental impact is presented in Figure 3. 
 
Fig. 3. Pareto frontier for baseline D-pillar, end-of-life landfill disposal 
Each Pareto-optimal solution is represented in Figure 3 by a box, the dimensions of 
which indicate the optimizing tolerances. The horizontal width of the box represents t1 
(a wider box indicates a wider tolerance), and the vertical height represents t2. Here, 
the upper-left solution corresponds with a purely cost-minimizing objective, and the 
lower-right solution minimizes environmental impact. The results shown here corres-
pond with end-of-life disposal in a landfill; when reusing the materials is possible, the 
optimal economic costs stay the same while the environmental impacts are lowered 
by 8%. Combustion of mild steel is not feasible. 

872 
S. Hoffenson, A. Dagman, and R. Söderberg 
4.2 
D-pillar Design with Parametric Variation of Material Choice 
The choice of material in this case study is important to the optimization solutions, as 
it affects outcomes such as part mass, and therefore the use phase ecological impact, 
as well as ecological sustainability impacts of extraction, production, and end-of-life 
disposal. Further, the material thickness is adjusted for each material based on the 
yield strength to ensure that the part can withstand the same compressive forces as a 
steel component (e.g., for a rollover/roof-crush test). Data on the strength, density, 
cost, and ecological impacts of five common automotive body materials were found 
in [24] and [21], and the resulting Pareto frontiers from optimizing the D-pillar using 
these materials are given in Figure 4, assuming end-of-life disposal in a landfill. 
 
Fig. 4. Pareto frontiers of D-pillar varying material 
Stainless steel has a much higher ecological impact of production than the other 
materials, and so the entire curve is off the visible chart in an environmental impact 
range of 340 to 380 ELUs. Like in Figure 3, Figure 4 presents data points as boxes 
where the widths and heights represent the corresponding optimal t1 and t2 values. 
Changing the end-of-life behavior from landfill disposal to reuse reduces the ecologi-
cal impact by 14%, 63%, 27%, and 29% for the latter four materials, respectively. 
5 
Discussion 
The convexity of the bi-objective optimization results for the D-pillar tolerances  
demonstrates a clear tradeoff between economic costs to the manufacturer and ecolog-
ical impact to society. The lower-right corner of Figure 3 shows that at high manufac-
turing costs, the tightest tolerances are associated with the lowest environmental  
impact. As the weighting of the objective function shifts towards economic costs, t1 is 
observed to increase, followed by an increase in t2. Even when the environmental 
impact does not affect the objective function, t2 never reaches its maximum allowed 

 
Tolerance Specification Optimization for Economic and Ecological Sustainability 
873 
value, shown by the box in the upper-left corner of the plot. This is attributed to the 
economic cost of discarding a large number of faulty parts when t2 is high. Designing 
parts in which the materials can be reused rather than discarded is also beneficial, as it 
reduces the environmental impact by up to 63%. 
Parametrically varying the material choice shows that, for cost-minimizing firms, 
mild steel is the best option for the D-pillar. In cases where the environmental impact 
is more important to the manufacturer and costs are more flexible, magnesium be-
comes a better choice with tighter tolerances. In a scenario where the use phase be-
comes more important, e.g., if the number of kilometers driven increases substantially 
from the assumption of 300,000, lighter materials such as aluminum and magnesium 
may become more attractive than the mild steel. Likewise, in cases where corrosion of 
the part is a concern, galvanized or stainless steels may become better choices. When 
the design allows for reuse of the material, magnesium becomes the best choice. 
6 
Conclusions 
While the specific results presented in this paper rely on assumptions built into the 
models, the framework provides new and important insights into how late-stage de-
sign choices should be considered with respect to internal manufacturing costs and 
external ecological costs. The automotive panel case shows a substantial tradeoff 
between economic and ecological costs resulting from tolerance and material choices, 
and further research is planned to reveal the impacts of additional sustainability deci-
sions by designers and policymakers. As legislation and rising consumer interests in 
ecological sustainability continue to affect the market, this design approach will be-
come more common for firms seeking to maximize profits in a competitive market. 
Acknowledgments. The authors acknowledge valuable intellectual and modeling 
contributions from Kristina Wärmefjord and Lars Lindkvist of the Department of 
Product and Production Development at Chalmers. This work, carried out at the 
Wingquist Laboratory VINN Excellence Centre within the Area of Advance – Pro-
duction at the Chalmers University of Technology, in Gothenburg, Sweden, was sup-
ported by the Swedish Governmental Agency for Innovation Systems (VINNOVA). 
That support is gratefully acknowledged. 
References 
1. Hong, Y.S., Chang, T.C.: A Comprehensive Review of Tolerancing Research. Int. J. Prod. 
Res. 40, 2425–2459 (2002) 
2. Fortini, E.T.: Dimensioning for Interchangeable Manufacture. Industrial Press, New York 
(1967) 
3. Bjørke, O.: Computer-Aided Tolerancing. ASME Press, New York (1989) 
4. Turner, J.U., Wozny, M.J.: Tolerances in Computer-Aided Geometric Design. The Visual 
Computer 3, 214–226 (1987) 
5. Shah, J.J., Ameta, G., Shen, Z., Davidson, J.: Navigating the Tolerance Analysis Maze. 
CAD and App. 4, 705–718 (2007) 

874 
S. Hoffenson, A. Dagman, and R. Söderberg 
6. Söderberg, R., Lindkvist, L.: Computer Aided Assembly Robustness Evaluation. J. Eng. 
Des. 10, 165–181 (1999) 
7. Chase, K.W., Parkinson, A.R.: A Survey of Research in the Application of Tolerance 
Analysis to the Design of Mechanical Assemblies. Res. in Eng. Des. 3, 23–37 (1991) 
8. Lööf, J., Hermansson, T., Söderberg, R.: An Efficient Solution to the Discrete Least-Cost 
Tolerance Allocation Problem with General Loss Functions. In: Davidson, J.K. (ed.) Mod-
els for Computer Aided Tolerancing in Design and Manufacturing, pp. 1148–1158. Sprin-
ger, Dordrecht (2007) 
9. Ostwalt, P.F., Huang, J.: A Method for Optimal Tolerance Selection. J. Eng. for Indus-
try 99, 558–565 (1977) 
10. Choi, H.G.R., Park, M.H., Salisbury, E.: Optimal Tolerance Allocation with Loss Func-
tions. J. Man. Sci. Eng. 122, 529–535 (2000) 
11. Ding, Y., Jin, J., Ceglarek, D., Shi, J.: Process-Oriented Tolerance Synthesis for Multistage 
Manufacturing Systems. In: ASME International Mechanical Engineering Congress and 
Exposition (2000) 
12. Li, Z., Izquierdo, L.E., Kokkolaras, M., Hu, S.J., Papalambros, P.Y.: Multiobjective Opti-
mization for Integrated Tolerance Allocation and Fixture Layout Design in Multistation 
Assembly. J. Man. Sci. Eng. 130, 0445011–0445016 (2008) 
13. Soderberg, R.: Tolerance Allocation Considering Customer and Manufacturer Objectives. 
In: Gilmore, B.J. (ed.) Advances in Design Automation, DE-vol. 65-2, pp. 149–157. 
ASME, Albuquerque (1993) 
14. Cheng, B.W., Maghsoodloo, S.: Optimization of Mechanical Assembly Tolerances by In-
corporating Taguchi’s Quality Loss Function. J. Man. Sys. 14, 264–276 (1995) 
15. Jeang, A.: An Approach of Tolerance Design for Quality Improvement and Cost Reduc-
tion. Int. J. of Prod. Res. 35, 1193–1211 (1997) 
16. Taguchi, G., Elsayed, E.A., Hsiang, T.: Quality Engineering in Production Systems. 
McGraw-Hill, Columbus (1989) 
17. Evans, D.H.: Statistical Tolerancing: The State of the Art, Part I: Background. J. Qual. 
Tech. 6, 188–195 (1974) 
18. Kane, V.E.: Process Capability Indices. J. Qual. Tech. 18, 41–52 (1986) 
19. Vigon, B.W., Tolle, D.A., Cornaby, B.W., Latham, H.C.: Life-Cycle Assessment: Invento-
ry Guidelines and Principles. Technical Report, Environmental Protection Agency, Cin-
cinnati (1993) 
20. Taghizadeh, A., Dagman, A., Almefelt, L.: Evaluation of Four Tools for Environmental 
Impact Life Cycle Assessment in Sustainable Product Development. In: 17th CIRP Inter-
national Conference on Life Cycle Engineering. University of Technology Press (2010) 
21. Steen, B.: A Systematic Approach to Environmental Priority Strategies in Product Devel-
opment (EPS). Version 2000 – General System Characteristics. Technical report, Chalmers 
University of Technology, Technical Environmental Planning (1999) 
22. The Netherlands Ministry of Housing, Spatial Planning and the Environment: Eco-
indicator 99 Manual for Designers. Manual (2000) 
23. Lutsey, N.: Review of Technical Literature and Trends Related to Automobile Mass-
Reduction Technology. Technical report, California Air Resources Board (2010) 
24. Ullman, D.G.: The Mechanical Design Process, 3rd edn. McGraw Hill, Boston (2003) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 875–883. 
DOI: 10.1007/978-3-642-30817-8_86 
© Springer-Verlag Berlin Heidelberg 2013 
 
Material Selection for Eco-design  
Julie Rockizki and Zwolinski Peggy 
G-SCOP Laboratory, Grenoble University, 46 avenue Félix Viallet,  
38031 Grenoble Cedex 01, France 
julie.rockizki@g-scop.inpg.fr 
Abstract. Product performances are directly influenced by both classical and 
non technical properties of the materials used. That is why Material Selection 
Process (MSP) is an important part of the design process. The objective of this 
paper is to show that existing material selection approaches including mechani-
cal and environmental criteria are not enough complete to make optimal materi-
al choice in preliminary design phase. Indeed, in this design phase, it is  
necessary to do the best choice in order to optimize technical and economical 
requirements of a component while reducing the product Environmental Impact 
(EI) in the whole Life Cycle (LC). Another LC approach is proposed here, in-
cluding material in several life cycles and influencing the material selection 
process. 
Keywords: material selection, eco-design, environmental indicators, “cradle to 
cradle” life cycle.  
1 
Introduction 
Product performance is influenced by both classical properties (density, Young’s 
modulus…) and non “technical” properties (environmental, sensorial, costs…) of the 
materials used. A lot of tools and methods help designers to optimize the material 
selection based on technical, mechanical and economical criteria during the product 
design process.  
With green consumer pressure, and to support industry regarding new environmen-
tal legislations and competitiveness, designers are constrained to reduce environmen-
tal impacts over the whole product LC. New material selection tools based on existing 
methodologies are developed and include environmental indicators to take into ac-
count the environmental impact during the product LC. But those approaches are still 
focused on classical criteria and the relative importance of the environmental perfor-
mance during the design phase is low. Also, these eco-design methodologies do not 
optimize technical and economical requirements of a component while reducing the 
product environmental impact in the whole LC.  
The objective of the approach presented in this paper is to optimize as much as 
possible the use of material and component including them in a closed loop life cycle 
model. A discussion is proposed to identify what kind of characteristics designers will 
require to optimize material selection. 

876 
J. Rockizki and Z. Peggy 
2 
Evolution of Material Consideration in Design Process 
The main element which makes that a product is different than another one, more 
resistant, shinier, lighter or rougher is the materials from which it is made. That is 
why it is important to consider materials in design process to satisfy design require-
ments. This next part is an overview of the evolution of material consideration in 
design process. 
2.1 
Evolution of the Design Process 
To success an industrial project, designers answer to a market request, a need, and in 
more technical terms, to a functional requirement. To gain time, designers and engi-
neers make a design plan in order to move from a conceptual idea to a physical prod-
uct. This design plan is a very important part of the success of a project, especially the 
preliminary step of design process where everything is planned. Product and engi-
neering designers follow systematic approach for product design. The most known 
and used is the Pahl and Beitz [1] approach with the following steps: 
• 
Step 1: need, market identification, new design 
• 
Step 2: idea Offer: Concept 
• 
Step 3: idea development: embodiment 
• 
Step 4: technical design, (preliminary sketch, dimension): Detail 
• 
Step 5: production 
But for several years, product design process has evolved from a “technical design” 
based on engineering and mechanical functions to a “product design process” were 
designers try to take into account the product all over its LC and then several aspects 
as technical performance and cost [2] (Fig. 1). This evolution of the design process 
influences the consideration of material selection in the product design process and 
constrains designers to take into account a set of criteria to choose an optimal  
material. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. The requirement pyramid 
 
Functionality  
economic /function - Cost 
Satisfaction 
Value 
Product design 
Industrial design
Technical design 
Use (easy to use) 
Cost

 
Material Selection for Eco-design 
877 
2.2 
Material Consideration in Product Design  
For a long time, many researchers and designers were focused on an analytic ap-
proach toward MSP based on mechanical engineering aspects (density, weight, 
Young’s modulus). According to Pahl & Beitz, material is considered like a technical 
input which must be defined on quantitative, qualitative and cost terms. But, in their 
first version, material selection during conceptual phase, both technical and economi-
cal criteria should be specified as early as possible. For most of the product design, 
designers defined product requirements with classical criteria and cost and compared 
them with properties of existing materials in database for a first selection. Then poten-
tial materials were tested. With this analytic approach, materials are only a way to 
move from a conceptual idea to a physical object. 
Several researchers in material selection fields as M. Ashby from the University of 
Cambridge changed the importance of material during the product design process.  
According to Ashby [3], product design is an interaction among Materi-
al/Process/Shape. The start of a design process is the function that specified the shape 
and the material which had an influence on process. It was still a mechanical vision of 
the design process but he introduced the importance of the material selection in this 
process. This element allows going from an immaterial design (CAO) to a physical 
design. Only material and process associated could provide with their properties the 
final characteristic of the new product during this transition.  
A lot of researchers (Table 1) [4] made a list of different aspects to be taken into 
account in the design process, in order to meet design requirement and market de-
mands. Also, the vision of the product evolves with Ashby and Johnson. For them, the 
product supplies a functional use and a new attraction, a “personality”. Material has 
two roles: functional (technical function) and product personality (aesthetic aspect, 
sensorial).  
With this new considerations on material, product characteristics become a mix 
among function, use and “personality”. The functional quality, aestheticism, appear-
ance of a product depend on the material it is made from. The evolution of material 
consideration makes material selection more and more difficult due to the set of crite-
ria and information to take into account to make an optimal choice at the earliest stage 
of the design process. As said by Dobranski (2001), material selection is more than fit 
together all the required functions, but it is a compromise among several constraints 
that could be completely different and opposite. It is a multi criteria selection. 
To sum up, material consideration during the design process has evolved in few 
years. In 70’s year, material selection was considering as a mechanical and engineer-
ing approach, based on technical and economical criteria during the early design 
phase of product design process. But society evolves, consumer needs change and the 
industry competitiveness is still stronger. In order to meet functional requirement and 
market demands, researchers in material field consider that is necessary to introduce 
new criteria as important as “classical criteria” during the product design phase. 
 

878 
J. Rockizki and Z. Peggy 
For few years, with the increasing awareness of negatives product effects on the 
environment (resources depletion, climate warming, …) and green consumers pres-
sures, several researchers focused on including an environmental criteria in the ma-
terial selection [5].  These new external drivers highlighted the necessity to take in 
consideration environmental criteria in MSP as well as mechanical and cost criteria in 
conceptual phase of the design process. 
Table 1. Review of different sources defining the effective material aspects for materials 
selection process 
 
3 
Environmental Consideration 
Today, easy extraction fields of some essential resources as petrol, gold and copper 
are depleting. Their extraction becomes more difficult and consumes resources itself, 
as water and combustible. This has repercussion on manufacturing processes and on 
the final cost of the product. Also, the consumption of important resources like miner-
als, combustibles (fossil and biomass) is still increasing. The UNEP (United Nations 
Environment Program) [6] states that the consumption volume of these resources 
could reach 140 billion tons per year in 2050, three times more than today. 
In the same way, the world population explosion increases also the natural re-
sources consumption. Developing countries as China, Brazil, India, have to answer to 
the increasing demands of their populations. And the current consumer society is such 
as human creates always new needs. So this economical development increases the 
consumption of consumer goods, raw materials and energy and makes pressure on 
natural resources. 
Other external drivers affect the material consideration during the product design 
phase: the law. New international laws such as legislation (REACH, WEEE, ErP,  
 

 
Material Selection for Eco-design 
879 
ROHS, ROHS2,ELV)1 and environmental standards lead designers to reduce Envi-
ronmental Impact (EI) of the product all along its Life Cycle (LC). Of course these 
environmental legislations are important to integrate eco design in industry, but in 
material field, they only allow the substitution of toxic materials by others less dan-
gerous. Indeed, it is possible to transfer negative environmental impacts across the 
product life cycle.  
Despite the governmental or green consumer’s pressures, material selection is still 
a compromise between technical performance and costs. The relative importance of 
the environmental performance during the design phase is low compared to the other 
“classical” criteria. Regarding to the difficulties to take into account several criteria in 
material selection process, it exists some tools and methods in order to help designer 
during the product design process summarized in the following chapter. 
4 
Material Selection – Tools and Methods 
As said previously, product performance is directly influenced by the properties of 
materials used such as, weight, corrosion resistance, manufacturing processes, and 
emissions involved in production phase, etc. MSP is an important step in product 
design process. Several tools included different kind of criteria are developed in order 
to help designers, to make decision on materials, to structure all the information 
needed, to deal with multi–criteria selection and to do material hyper-choice. 
4.1 
Classical Tools 
A lot of methods, strategies [7]and tools [8][9][10] are developed  like bills of ma-
terial, material databases, “material library”, guidelines and softwares. The objective 
of material selection is to convert a set of inputs (technical, non-technical design re-
quirements) to output (potential materials). But, in industry no particular strategy or 
systematic methodologies are used by designers in order to select material during the 
design process [9][4]. 
The most known material selection method is from Ashby. He has developed a set 
of tables which represent relationship among material properties/functional require-
ments and material/process. Currently, this method is the most complete to choose 
material during the product design process. Based on the development of performance 
indicators it is an iterative methodology which doesn’t exclude any material solution 
and evolves during the product design [3]. Most of materials selection tools, as the 
                                                           
1 REACH : Registration, Evaluation, Authorization and Restriction of Chemicals, Regulation 
n°1907/2006 
WEEE : Waste Electrical and Electronic Equipment directive (2002/96/EC) 
ErP : Energy related Product directive (2009/125/EC) 
ROHS : Restriction of the use of certain Hazardous Substances in electrical and electronic 
equipment 
ELV : End-of –Life Vehicle directive (2000/53/EC). 

880 
J. Rockizki and Z. Peggy 
well-known software CES Selector, use this multi-criteria methodology developed by 
M.F Ashby. 
4.2 
Environmental Criteria in Material Selection Process  
To support industry regarding environmental legislations and standards, and to sup-
port their competitiveness, new methodologies and tools are developed to take into 
account the EI during the product over it all LC, Life Cycle Design (LCD) and Design 
for Environment (DFE) [11][12]. At various stages of product life cycle, natural re-
sources, combustible, chemicals, etc are required and interact with the environment 
(water, ground, air). As well as classical material properties, environmental material 
performance impacts on the environmental product performance directly or indirectly. 
From this stand point, new material selection tools, based on existing methodologies 
mentioned previously, include environmental indicators able to measure “environ-
mental performance” of a material [13]. 
But, some problems are observed with these different approaches: 
• 
Material selection with Life Cycle Assessment (LCA) or Simplified LCA 
(SLCA). This method take into account all the life cycle phases of the product, 
from the extraction to end of life. This approach is not usable during the prelimi-
nary design phase, because a lot of material and product data are required, but 
unavailable at this design stage. Indeed, a set of material selection tools based on 
LCA approach are developed depending to the industries. Designers use different 
environmental profile databases for materials and it becomes difficult to compare 
them. A problem of Environmental Impact (EI) standardization is highlighted. 
For instance, if the Inventory Life Cycle (ICV) EI’95 and EDIP are applied, the 
plastic PVC has a bad environmental behavior, but according to EI’99 and EPS 
methods PVC environmental behavior is higher [14]. Often, the eco-indicator 
method is used to calculate EI. This method aggregates all the EI into a single 
score [15]. It is easier to understand but there is a risk to lose a set of information. 
Finally, designers should be train with this type of LCA software which can be 
complicated and take more time. 
• 
Other approach is to translate EI in economical terms [16]. This MSP is a com-
promise among cost, production and EI. But it is difficult to quantify the EI, that 
is why this kind of method is most of the time focused on quantifiable data such 
as energy consumption during the manufacturing phase and End-of life phase 
when designers obtain the information.  
• 
In his works, Ashby extends his material selection method, known in mechanical 
field, by integrating environmental notion (energy consumption, recyclable frac-
tion) [5][17].This approach is interesting for designers. Most of the time design-
ers are not expert in eco-design, here MSP is still simple and easy and designers 
consider Environmental criteria exactly like another one. But this approach does 
not calculate EI all over the Product LC and there is a risk to make pollution 
transfer. 

 
Material Selection for Eco-design 
881 
All these approach are not sufficient to assess the environmental performance of a 
material. Firstly, these methods are focused on first phases of the product LC, extrac-
tion of raw materials, production and manufacturing phases. Designers meet some 
problems to calculate EI in use and EoL phases due to a lack of data in early design 
phase. MSP in eco-design is a multi-criteria and multi-phase method. If designer con-
sider only the first LC phases the risk of pollution transfer increases. Moreover, these 
methods use only quantifiable environmental parameters such as energy consumption, 
toxicity, substances (CO2, NOx, SO2 emission). Those methodologies help designers 
to make material substitution, or to assess environmental impact of an existed prod-
uct, but at the End-of-Life stage the product is still a waste unusable, it is a classical 
“Cradle to grave” life cycle assessment.  
To summarize, those approaches included either classical or environmental criteria 
or both are not enough complete to make optimal choice in preliminary design phase 
in order to optimize technical and economical requirement of a component while 
reducing the product EI in the whole LC. In the next paragraph, a new LC approach is 
proposed which allows to take into consideration several end of life scenario and to 
reduce environmental impact of product in the whole LC. 
5 
New Life-Cycle Approach 
For a few years, the notion of sustainable product has changed product LC moving 
from “cradle-to-grave”(C to G) approach to the “cradle-to cradle”(C to C) one, which 
is a closed loop life cycles’ model. In material field, one of the most important eco-
design focus of research is to optimize, as much as possible the use of material and 
component including them in a closed loop life cycle model. When a product be-
comes obsolete it is possible to improve the product lifespan using different product 
End-of-Life (EoL) strategies, named “3R strategy” [18].  
Each element of this product could have several new LC by recovered and Reused, 
Remanufactured or Recycled. Determining the most appropriate EoL scenarios for 
each component should reduce the product EI. This new LC approach is a way to 
complete existing methodologies to minimize EI over the whole product LC. 
But designers have to determinate this 3R strategy during the early design stage 
and none of existing material selection methods and tools seen previously take into 
account that materials and components could have several usage cycles.  
Several researchers highlighted environmental benefits of components remanufac-
turing [19][20]. According to them, remanufacturing could reduce energy consump-
tion over several usage cycles. Indeed, materials recycling allow consuming less raw 
materials, water, substances and energy during the extraction and preliminary trans-
formation phases. 
This LC approach could really improve product environmental performance in the 
early design stage. It is a solution to take into account all the stages of the product life 
cycle. Of course, designers should identify new aspects for material selection process; 
the best EoL strategy depending to materials which compose the product in order to 
avoid pollution transfer interaction among those materials, their environment in usage 
phase, existing recycling process. Designers need new material characteristics, as 
environmental characteristics, in addition to the classical one (recyclability rate, CO2 
emissions…): 

882 
J. Rockizki and Z. Peggy 
• 
Compatibility among several materials 
• 
Material capacity for recycling 
• 
Material assembly 
• 
Material quality 
• 
Material lifespan 
• 
Life cycle options 
• 
Etc. 
6 
Conclusion 
In brief, material selection is an important step in the conceptual design phase of the 
product design process. However, there is not a systematic method to select material 
during this phase, and designers have to deal with an hyper choice of material and 
material aspects. When considering technical and economical criteria, it is still diffi-
cult to identify which parameters are suitable in order to optimize the material selec-
tion at the beginning of a design process. But it is even more difficult when designers 
considering environmental and no quantifiable material aspects. 
The objective of MSP is to assess the performance of a material in early design 
stage depending to material classes, material function, use of material and end of life 
strategies in order to optimize the use of material and component. To select material 
during the conceptual phase of product design process, designers have to make a re-
quirement list. For evaluating this entire requirement during this phase, technical and 
economical characteristics are important but also environmental and ‘Life cycle’ cha-
racteristics should be considered as early as possible. All these criteria are linked to 
each other. Designers cannot separate these materials characteristics if the objectives 
of the product design are to optimize technical and economical requirements of a 
component while reducing the product EI in the whole LC.  
To propose a comprehensive material selection method by taking into account 
those new LC approach and new parameters, it is necessary to understand the com-
plexity of material usage over the whole value chain of a product. This will be the 
next step of this work. Based on existing value chains of materials in France, the loss 
of the material value through the whole value chain of a product will then be drawn. 
This loss will be assessed by taking into account economical and environmental crite-
ria, in order to identify which strategy should be chosen to optimize material value. 
With this model, the actors involved in the VC should be identified and the best VC 
strategy should be chosen depending to criteria highlighted previously and while re-
ducing EI on the whole material value chain, by example: 
- 
Insource the material inside its initial VC the material as “bottle to bottle” model.   
- 
Outsource for another usage as initial and develop a new value chain  
The aim of this model is not to propose new indicator or a new method to MSP, but 
help designers to understand the material value chain in order to influence their choic-
es in material selection and design.  

 
Material Selection for Eco-design 
883 
References 
1. Beit, W., Pahl, G.: Engineering Design. A systematic Approach. Springer, London (2007) 
2. Ashby, M.F., Johnson, K.: The art of materials selection. Materials today, 24–35 (2003) 
3. Ashby, M.F.: Material selection in mechanical design. Butterworth-Heinemann, Oxford 
(1992) 
4. Karana, E., Hekkert, P.: Material considerations in product design: A survey on crucial 
material aspects used by product designers. Materials & Design 29(6), 1081–1089 (2008) 
5. Weaver, P.M., Ashby, M.F.: Selection of materials to reduce environmental impact: a case 
study on refrigerator insulation. Materials & Design 17(1), 11–17 (1996) 
6. United Nations Environment Program (UNEP), http://www.unep.org 
7. Ashby, M.F., Johnson, K.: Materials and Design — the Art and Science of Materials Se-
lection in Product Design. Butterworth Heinemann, Oxford (2002) 
8. Bréchet, Y., Ashby, M.F., Dupeux, M.: Choix et usage des matériaux. Techniques de 
l’ingénieur 33 (2012) 
9. Van Kesteren, I.E.H.: Product designers’ information needs in materials selection. Mate-
rials & Design 29(1), 133–145 (2008) 
10. Ashby, M.F., Bréchet, Y., Salvo, L.: Sélection des matériaux et des procédés de mise en 
œuvre. PPUR presses polytechniques (2001) 
11. Keoleian, G.A., Menerey, D., Arbor, A.: Sustainable Development by Design. Review of 
Life Cycle Design and Related Approaches (1994) 
12. Lindahl, M.: Engineering designers’ experience of design for environment methods and 
tools – Requirement definitions from an interview study. Journal of Cleaner Produc-
tion 14(5), 487–496 (2006) 
13. Ashby, M.F.: Matériaux et environnement: choix eco-responsable en conception. Dunod, 
Paris (2011) 
14. Bovea, M.D., Gallardo, A.: The influence of impact assessment methods on materials se-
lection for eco-design. Materials & Design 27(3), 209–215 (2006) 
15. Giudice, F., La Rosa, G., Risitano, A.: Materials selection in the Life-Cycle Design 
process: a method to integrate mechanical and environmental performances in optimal 
choice. Materials & Design 26(1), 9–20 (2005) 
16. Chen, R.W., Navin-Chandra, D., Kurfess, T.: A systematic methodology of material selec-
tion with environmental considerations, pp. 252–257 (1994) 
17. Ashby, M.F., Miller, A., Rutter, F., Seymour, C., Wegst, U.: The CES Eco Selector – 
Background Reading, vol. 1, pp. 1–23 (2009) 
18. Gehin, A., Zwolinski, P., Brissaud, D.: Integrated design of product lifecycles—The fridge 
case study. CIRP Journal of Manufacturing Science and Technology 1(4), 214–220 (2009) 
19. Sutherland, J.W., Adler, D.P., Haapala, K.R., Kumar, V.: A comparison of manufacturing 
and remanufacturing energy intensities with application to diesel engine production. CIRP 
Annals - Manufacturing Technology 57(1), 5–8 (2008) 
20. Amaya, J., Zwolinski, P., Brissaud, D.: Environmental benefits of parts remanufacturing: 
The truck injector case. In: 17th CIRP International Conference on Life Cycle Engineer-
ing, China (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 885–894. 
DOI: 10.1007/978-3-642-30817-8_87     © Springer-Verlag Berlin Heidelberg 2013 
An Information Model of the Design Process  
for the Estimation of Product Development Effort 
Judith Pollmanns, Thomas Hohnen, and Jörg Feldhusen  
RWTH Aachen University, Chair and Institute for Engineering Design, Aachen, Germany 
pollmanns@ikt.rwth-aachen.de 
Abstract. Especially for small batch series or customer specific development 
orders, development costs are a noteworthy part of the overall costs and cannot 
be balanced by optimized production processes and lower production costs as in 
the case of mass production. Even though they might not be state of the art in 
industrial application, there are numerous methods for the estimation of costs 
that are determined in the course of product development. However, the costs 
caused by the actual development processes are hardly considered. This gap is 
supposed to be filled by an estimation of development costs as a direct conse-
quence of development (and with it information handling) effort. For this an in-
formation model of the design process and the information handling steps 
which aregenerating the design effort are presented. 
Keywords: process model, information model, development costs. 
1 
Introduction 
Especially for small batch series or customer specific development orders, develop-
ment costs are a noteworthy part of the overall costs [1] and cannot be balanced by 
optimized production processes and lower production costs as in the case of mass 
production. For the definition of a realistic quotation, the expectable costs must be 
identified as an insufficient quotation either inhibits the possible order, leads to a non-
lucrative order or costs have to be changed during the project, displeasing the custom-
er. This cost information has to be at hand before the development takes place (for 
quotations) or in in the very early phases (for go/no-go decisions). 
As development costs depend mainly on labor costs [2], design effort is the crucial 
driver for design costs. Depending on a defined cost rate, costs can therefore be calcu-
lated based on the design effort. Another important benefit of effort estimation is the 
enabled project scheduling and definition of required resources. Third, effort planning 
enables project controlling as controlling needs a reliable planning of costs and  
resources. [3]  
This is why this paper proposes an approach to estimate the effort of a design 
project as a function of product requirements and project-, process- and environment 
related factors prior to the actual development. 

886 
J. Pollmanns, T. Hohnen, and J. Feldhusen 
2 
Estimation of Design Effort Based on the Standard Design 
Process 
For the estimation of design effort, a standardized process is to be defined which is 
generally valid and easily adoptable to a special use case. Widely accepted process 
models as VDI 2221 [4] are unsuitable here, as they are too generic concerning the 
actual process steps. It is, as a consequence, not possible to estimate the effort based 
on this generally valid, but hence not very detailed process. They also differ from 
“real” design processes, as they are not limited by industrial and organizational con-
straints, are less dynamic, not affected by personal characteristics and usually aim at 
an ideal solution instead of a satisfying one [5]. Neither theoretic reflection nor the 
empirical exploration of the design process could give a detailed, generally valid 
process model [6]. In contrast to the generic models, a detailed process model describ-
ing each single step would be suitable for an accurate estimation but would cause high 
effort for process definition and, of course, lose its general applicability, as design 
processes highly depend on the product, which is designed [7]. Hence, the definition 
of process steps cannot fulfill the requirements of both a suitable level of detail and 
general applicability. 
2.1 
Abstracting the Process 
For the reasons explained above, a design process model is not a reasonable basis for 
an estimation of design effort. Consequently, the design process has to be abstracted 
in a different manner, not regarding the individual process steps. 
For this purpose, the design is, in a first step, abstracted to the simple model of  
input-black box-output (cf. Figure 1). 
 
 
Fig. 1. Input-black box-output model of the design process 
In this context, for the starting point of the design process (problem, customer or-
der etc.) the only information considered are the new product’s requirements. The 
process output is not the product but the information which is necessary for the manu-
facturing of the product, hence the 2-dimensional production drawings and bills of 
materials (cf. [8]). So, the design effort is the effort for the transition of the “require-
ments” to the production drawings and the bills of material.  
 
 
 
design process
requirements
drawings, 
bills of material

 
An Information Model of the Design Process 
887 
Following this reasoning, the design process itself is abstracted to the generation of 
documents. These are drawings as the final result but also different documents in the 
course of the process, such as sketches, block diagrams, descriptions, 2- or  
3-dimensional models (cf. [9], [10]).  
Abstracting the process to the generated documents, the estimation of design effort 
is based on the following assumptions: 
• In each design process there are similar kinds of documents that are generated. 
• The number of documents depends on different influencing factors. Factors like the 
innovativeness of the product influences the effort as new products require more 
calculations, certification documents etc. 
• The more documents have to be generated, the longer it takes. 
• The generation of different kinds of documents requires different amounts of time. 
This reasoning follows the research of the 1970s (cf. [11], [12]), assuming, that all 
generated documents can be normalized to a standard document (DIN A4 drawings in 
the example of [11] and [12]). For these documents the generation effort can be esti-
mated using data of the analysis of working time recording [11].  
But design documents in actual processes differ considerably compared to the do-
cumentation in the 1970s. Even though today drawing are still the legally relevant 
documents, relevant data is mainly stored in computerized models and processes are 
computer-aided. There are even attempts to integrate the entire geometric and calcula-
tion data in just one file [13]. This leads to the conclusion, that the consideration of 
design documentation is not suitable for the estimation of design effort anymore. It is 
much more the generation of information that is contained in the documentation, 
which has to be considered.  
Neither the actual process steps nor the information container define the informa-
tion, but it is determined by the design task and influencing factors, which will be 
explained in detail in chapter 4. Hence, abstracting the process to the information, it is 
possible to estimate the design effort on a basis, which is generally valid and easily 
adaptable to a special use case. 
3 
Information Model of the Design Process 
3.1 
Approach 
A three step approach is executed (cf. Figure 2). First relevant design information is 
collected and clustered. Then the operation action (i. e. handlings steps, cf. chapter 
3.4) for the information items are defined. In a third step the factors influencing both 
operation action and amount of information are analyzed. These will be the basis for 
the quantified estimation of the development effort in future research. 

888 
J. Pollmanns, T. Hohnen, and J. Feldhusen 
 
Fig. 2. Research approach 
3.2 
Relevant Data 
First of all, process information has to be collected and structured according to the 
relevant literature (e. g. [8-10, 13–19] etc.). 
Here documents as well as single information were taken into account and consoli-
dated into one table of information. The information was clustered to “presumptive 
documents” for a clearer arrangement even though these documents themselves may 
never occur during design. For a specific process, information could be clustered to 
different groups resulting in completely different documents. It is also possible, that 
information only exists as tacit knowledge and is never displayed. Nevertheless, the 
information has to be at hand. 
Table 1. List of information (exerpt) 
Information cluster
Information 
 
Information cluster
Information 
List of requirements, 
technical specifica-
tion 
 
 
Concept, principle 
solution 
 
 
Requirements 
 
 
Rough calculations 
 
Wishes 
 
 
Sketches 
 
Operation area 
 
 
Function defined tolerances 
 
Aim/purpose 
 
 
Function defined fittings 
Function structure 
 
 
 
Function defined surface 
conditions 
 
Function 
 
 
Rough kinematics 
 
Interrelation of functions 
 
 
 
Definition of relevant information
Assessment 
of operation 
action
Assessment 
of 
influencing 
factors
Quantifying, estimation of effort
In this
paper
Further
research

 
An Information Model of the Design Process 
889 
3.3 
The Information Model 
The relevant development information, the interrelationship within the information 
and the information cluster (cf. Table 1) are, in a second step, graphically represented 
in the information model (cf. Fig. 3). 
The small white boxes represent the pieces of information that are connected with 
lines, representing the nondirectional relations. Dashed lines border the information 
clusters. In the left upper corner, requirements, wishes, operation area and purpose of 
the new product represent the process input, whereas production drawing, bills of 
material etc. represent the output in the bottom right corner. The lines of interrelation 
show that there is no strict and consistent process flow, but that information from the 
very beginning of the process are needed for all following design steps. There are no 
technically relevant information (i. e. information considered for the information 
model) items that have neither input nor output, but all information are linked. 
 
Fig. 3. Information model (full size view: www.ikt.rwth-aachen.de/information_model) 
The most densely connected area can be seen in the middle of the model, where in-
formation is concentrated in the preliminary design. (As mentioned before, the term 
“preliminary design” is used for the clustered information and is not a specific docu-
ment resulting from a specific process.) Here, all concept information is concentrated 
and used as a basis for calculations, simulations and the following embodiment de-
sign. In the end, most information is linked to the CAD model and processed here.  
3.4 
Information Operating  
After identifying and clustering the relevant data, it has to be assessed in regard to 
handling steps each item causes. For each piece of information, there are several  

890 
J. Pollmanns, T. Hohnen, and J. Feldhusen 
handling options (operation action in the following) and the authors assume that each 
one causes a different amount of effort.  
Generation 
The information considered must be generated completely up to the actually neces-
sary degree of detail. If a piece of information is considered repeatedly in the informa-
tion model, the completed definition is only executed once. For the intermediate steps 
only partly generation can be assumed. 
Example: For the dimensions, generation would be the definition of the required 
dimension and the value. 
Processing 
The information exists in a form that has to be revised. It has to be handled to be 
available in an adapted/changed way. This can either be the adaption, the check or the 
combination of already given values/information.  
Example: For dimensions, processing would be the definition of the value based on 
a known dimensioning item. It could also be the combination of given val-
ues/properties in a known calculation formula. 
Display 
The information is known, either from predecessor projects/products or because it 
has been generated or processed before, and is documented in this step. The action of 
display is to be considered in addition to the generation and cannot simply be a part of 
it for numerous reasons. For example even given information has to be graphical-
ly/textually displayed to fulfill legal or customer requirements causing additional 
effort. There is also a difference in documentation frequency and the ratio of informa-
tion display and tacit information depending on special influencing factors (cf.  
chapter 4) 
Example: For dimensions, display would be the notation of a dimensional value  
into the drawing (or the CAD-Model). 
For a first assessment of the operation action for each piece of information a  
standard scenario has to be defined as most of the handling depends on special in-
fluencing factors (cf. chapter 4): 
The product to be developed is represented by a set of standard requirements, 
which is typical for the array of products of the company. The company is expe-
rienced concerning the type of product. The actual requirements are predefined but 
the values/characteristics depend on the specific use case. For this scenario, the cha-
racteristics are close to the current average (cf. chapter 4), i. e. the new product is 
rather an incremental than a radical innovation. There is a predecessor product availa-
ble within the company, which has to be changed. New components have to be added 
but both product structures have a substantial similarity. 
4 
Influencing Factors 
There are factors influencing the product development process and hence the design 
effort coming from several different fields [20]. As the process itself is not considered 

 
An Information Model of the Design Process 
891 
in this estimation approach, the influencing factors considered here affect the number 
of one information item (“How many dimensions does the part have?”) and the opera-
tion action (“Do I have to generate the dimension or do I only have do display given 
information?”). 
Several factors could be identified in a literature review [3, 20–34]: 
• Innovation 
• Product size 
• Employee experience 
• Multi-site development and development outsourcing 
• Difficulty of the design task 
• Team aspects and working environment 
• Criticality of the designed product 
• Educational level of the employees 
Exemplarily the factors “innovation – product newness” and “product size – complex-
ity” shall be described in the following. 
Innovation –product newness 
There is evidence for the direct link of a product’s innovativeness (deviation of the 
standard requirements’ values from the current average) to its development costs [35] 
and its newness (deviation of a product from previous generations) to the develop-
ment cycle time [31]. As for this approach, innovation is only considered regarding 
the developing company (“new to the company”), both terms can be regarded equally 
here. As development cost and cycle time both depend directly on the development 
effort, it can be concluded here, that there is an impact of company internal innova-
tion on the product development effort. 
For the detailed analysis of this impact the assessment of the operation actions 
concerning the development information is expanded in two ways. First, the operation 
action is redefined concerning an increased innovativeness (“For an innovative prod-
uct, the concept has to be newly generated and cannot be adapted, i. e. processed”). In 
a second step, a possibly increased amount of information (“Innovative concepts need 
more simulation for feasibility studies”) is assessed. 
Table 2. Impact of the influencing factor “innovation – product newness” (excerpt) 
Information 
Cluster 
Information 
Operation action 
Change of opera-
tion action 
Change in informa-
tion quantity 
Product structure 
 
 
 
 
 
Planned assemblies 
Process 
Generate 
No impact 
 
Planned parts 
Process 
Generate 
No impact 
 
Interrelations 
Process 
Generate 
No impact 
 
Interfaces 
Process 
Generate 
No impact 
 

892 
J. Pollmanns, T. Hohnen, and J. Feldhusen 
Product size – complexity 
For software development an accepted method for estimating the development effort 
is based on the product size, i. e. the lines of source code [36]. It is obvious and prov-
en that also for the development of physical products, the size has a reasonable impact 
[31], but the number of parts of the product is a measure that is usually not available 
at the point of time, when development effort and costs are to be estimated. In early 
phases the product size is therefore defined as the number of functions, a product has 
to fulfill [3], [23], [31]. In literature often the term “complexity” is used instead of 
size. 
For the detailed analysis of the impact on development effort, the assessment of the 
list of information is again expanded. As the mere existence of more functions and 
accordingly more product parts does not affect the operation action, only the effect on 
the amount of information has to be reconsidered. 
Table 3. Impact of the influencing factor “product size –complexity” (excerpt) 
Information Clus-
ter 
Information 
Operation action 
Change of opera-
tion action 
Change in infor-
mation quantity 
Product structure 
 
 
 
 
 
Planned assemblies 
Process 
N/A 
Strong increase 
 
Planned parts 
Process 
N/A 
Strong increase 
 
Interrelations 
Process 
N/A 
Strong increase 
 
interfaces 
Process 
N/A 
Strong increase 
5 
Summary and Further Outlook 
For the estimation of product development effort, the examination of the design 
process itself is not likely to deliver results as the process model can either be general-
ly valid but in a consequence too generic for direct applicability or very detailed but 
only suitable for one special use case. To handle this contradiction an information 
model of the design process was introduced. For the evaluation of the design effort, 
the information was assessed regarding their operation actions. In a second step, in-
fluencing factors on design processes were analyzed concerning their impact on both 
the amount of information and the operation action. 
For the estimation of the product development effort, standard values for the opera-
tion actions have to be summed up for the design information. The effect, influencing 
factors have on these values, has to be taken into account as relative changes, too. 
References 
1. Schnauffer, P.: Multidisziplinärer Datenfluss im Entwicklungsprozess der Flugzeugbaus 
am Beispiel eines Senkrechtstarters. Dissertation, Universität Stuttgart 
2. Ehrlenspiel, K., Kiewert, A., Lindemann, U.: Kostengünstig Entwickeln und Konstruieren: 
Kostenmanagement bei der integrierten Produktentwicklung. VDI-Buch. Springer, Heidel-
berg (2005) 

 
An Information Model of the Design Process 
893 
3. Bashir, H.A., Thomson, V.: Models for estimating design effort and time. Design Stu-
dies 22(2), 141–155 (2001) 
4. VDI: Methodik zum Entwicklung und Konstruieren technischer Systeme und Produkte 
(2221) (1993)  
5. Bender, B.: Erfolgreiche individuelle Vorgehensstrategien in frühen Phasen der Produk-
tentwicklung. VDI-Verl., Düsseldorf (2004) 
6. Heymann, M.: "Kunst" und Wissenschaft in der Technik des 20. Jahrhunderts: Zur Ge-
schichte der Konstruktionswissenschaft. Chronos, Zürich (2005)  
7. Reich, Y.: My method is better! Res. Eng. Design 21(3), 137–142 (2010) 
8. Ehrlenspiel, K.: Integrierte Produktentwicklung: Denkabläufe, Methodeneinsatz, Zusam-
menarbeit, 4th edn. Hanser, München [u.a.] (2009) 
9. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Pahl/Beitz Konstruktionslehre: Grundla-
gen erfolgreicher Produktentwicklung, 7th edn. Methoden und Anwendung. Springer, Ber-
lin (2007) 
10. Breiing, A., Flemming, M.: Theorie und Methoden des Konstruierens: Mit 3 Tabellen. 
Springer, Berlin (1993) 
11. Hichert, R.: Praktische Ansätze zur Termin-, Kapazitäts- und Kostenplanung in Entwick-
lung und Konstruktion. In: Moll, H.H. (ed.) RKW-Handbuch Forschung, Entwicklung, 
Konstruktion (F+E); ergänzbares Handbuch für Wissenschaft, Technik, Wirtschaft und 
Verwaltung, vol. 3. Schmidt, Berlin (1976) 
12. Paul, J.: Planung, Steuerung und strategische Ausrichtung der Produktentwicklung. In: 
Moll, H.H. (ed.) RKW-Handbuch Forschung, Entwicklung, Konstruktion. (F+E); 
ergänzbares Handbuch für Wissenschaft, Technik, Wirtschaft und Verwaltung. Schmidt, 
Berlin (1976)  
13. Amft, M.: Phasenübergreifende bidirektionale Integration von Gestaltung und Berechnung. 
Dissertation, Technische Universität München (2002) 
14. Lauer, W.: Integrative Dokumenten- und Prozessbeschreibung in dynamischen Produk-
tentwicklungsprozessen. Disseration, Technische Universität München (2010) 
15. Zahay, D., Griffin, A., Fredericks, E.: Sources, uses, and forms of data in the new product 
development process. Industrial Marketing Management 33(7), 657–666 (2004) 
16. Müller, J.: Arbeitsmethoden der Technikwissenschaften: Systematik, Heuristik, Kreati-
vität. Springer, Berlin (1990) 
17. DIN: Dokumentenmanagement Teil 1: Prinzipien und Methoden (82045-1:2002-11)  
18. DIN: Technische Produktdokumentation Lebenszyklusmodell und Zuordnung von Doku-
menten (15226) (1999) 
19. Ulrich, K.T., Eppinger, S.D.: Product design and development, 3rd edn. McGraw-
Hill/Irwin, Boston (2004) 
20. Frankenberger, E., Badke-Schaub, P., Birkhofer, H. (eds.): Designers: The key to success-
ful product development, Darmstadt. Springer, London (Dezember 1997, 1998)  
21. Stöckert, H.: Fehlervermeidung an Schnittstellen-Prozessen der verteilten Produktentwick-
lung. Dissertation, Technische Universität Berlin (2011) 
22. Kessler, E.H.: Tightening the belt: methods for reducing development costs associated 
with new product innovation. J. Eng. Technol. Management 17, 59–92 (2000) 
23. Griffin, A.: Metrics for Measuring Product Development Cycle Time. Journal of Produc-
tion Innovation Management 10(2), 112–125 (1993) 
24. Cooper, R.G., Kleinschmidt, E.J.: Determinants of timeliness in Product Development. 
Journal of Production Innovation Management 11, 381–396 (1994) 
 

894 
J. Pollmanns, T. Hohnen, and J. Feldhusen 
25. Frankenberger, E., Badke-Schaub, P.: Integration of Groups, Individual and External In-
fluences in the Design Process. In: Frankenberger, E., Badke-Schaub, P., Birkhofer, H. 
(eds.) Designers. The key to Successful Product Development, Darmstadt, pp. 149–164. 
Springer, London (Dezember 1997, 1998) 
26. Duffy, A.H.B., O’Donnell, F.J.: A Model of Product Development Performance. In: Fran-
kenberger, E., Badke-Schaub, P., Birkhofer, H. (eds.) Designers. The Key to Successful 
Product Development, Darmstadt, pp. 269–283. Springer, London (Dezember 1997, 1998)  
27. Uhlig, V.: Informationstechnische Unterstützung von Entwicklungskooperationen. PZH, 
Produktionstechn. Zentrum, Hannover (2002) 
28. Badke-Schaub, P., Frankenberger, E.: Management kritischer Situationen: Produktent-
wicklung erfolgreich gestalten. Springer, Berlin (2004 [erschienen 2003])  
29. Frankenberger, E.: Arbeitsteilige Produktentwicklung: Empirische Untersuchung und 
Empfehlungen zur Gruppenarbeit in der Konstruktion, vol. 291. VDI-Verl., Düsseldorf 
(1997) 
30. Hacker, W., Sachse, P., Schroda, F.: Design Thinking – Possible Ways to successful Solu-
tions in Product Development. In: Frankenberger, E., Badke-Schaub, P., Birkhofer, H. 
(eds.) Designers. The Key to Successful Product Development, Darmstadt, pp. 205–216. 
Springer, London (Dezember 1997, 1998) 
31. Griffin, A.: The Effect of Project and Process Characteristics on Product Development 
Cycle Time. Journal of Marketing Research (34), 24–35 (1997)  
32. Müller, A.: Iterative Zielklärung und Handlungsplanung als Faktoren erfolgreichen Grup-
penhandelns bei der Lösung komplexer Probleme: Eine handlungstheoretische Betrach-
tung des Konstruierens in Gruppen. Dissertation, Technische Universität München (2007) 
33. Christiaans, H.C., van Andel, J.: Information Processing and Storage during the Design 
Process: The Use of a Flexible Information System. In: Frankenberger, E., Badke-Schaub, 
P., Birkhofer, H. (eds.) Designers. The Key to Successful Product Development, 
Darmstadt, pp. 233–248. Springer, London (Dezember 1997, 1998)  
34. Hondorf, T.: Evaluierung von Methoden zur Entwicklung sicherheitskritischer Software-
systeme. Informatikprojekt, Hochschule Ravensburg-Weingarten (2010) 
35. Heller, J.E., Pollmanns, J., Feldhusen, J.: Bestimmung des Produktentwicklungsaufwands 
basierend auf Kennzahlen am Beispiel der Luftfahrzeugentwicklung. In: Stelzer, R., Grote, 
K.-H., Brökel, K., Rieg, F., Feldhusen, J. (eds.) Entwerfen Entwickeln Erleben: Methoden 
und Werkzeuge in der Produktentwicklung (EEE 2012). 10. Gemeinsames Kolloquium 
Konstruktionstechnik, Residenzschloss Dresden, Juni 14-15, pp. 565–580. TUDpress, 
Dresden (2012) 
36. Boehm, B.W.: Software engineering economics. Prentice-Hall advances in computing 
science and technology series. Prentice-Hall, Englewood Cliffs (1981) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 895–904. 
DOI: 10.1007/978-3-642-30817-8_88     © Springer-Verlag Berlin Heidelberg 2013 
Assessing the Relationship between New Product 
Development Practices and Performance in the 
Norwegian Manufacturing Industry 
Torgeir Welo1, Silje H. Aschehoug2, and Geir Ringen2 
1 Department of Engineering Design and Materials, Norwegian University of Science and 
Technology (NTNU), N-7491Trondheim, Norway 
2 Sintef Raufoss Manufacturing, N-2831 Raufoss, Norway 
Torgeir.Welo@ntnu.no, 
{Silje.Aschehoug,Geir.Ringen}@sintef.no 
Abstract. This article reports on a survey for determining the current status of 
lean product development (LPD) practices among Norwegian manufacturing 
companies using a generic LPD model as basis. The model includes six main 
dimensions and was developed based on a systematic review of existing mod-
els, practices and interpretations, including the Toyota Product Development 
Systems amongst others. Using non-probabilistic design, a sample of 258 res-
ponses out of 356 invited subjects, demonstrates the relation between product 
development performance and the following elements: clear project prioritiza-
tion based on portfolio thinking and balancing the workload; loyalty to the 
agreed upon design strategy; project dynamics cause project members to conti-
nuously update themselves on critical product characteristics; and the use of 
simple and visual communication modes is deployed in the organization.  
Keywords: lean product development, survey, company performance. 
1 
Introduction 
During the past several decades, lean principles and techniques have become the 
benchmark for Western manufacturing companies competing in the global market 
place. Nearly all significant market players nowadays have introduced lean principles 
in one way or the other, strongly founded in the success of the Toyota Production 
System (TPS). For the companies involved the ultimate goal is to sustain competi-
tiveness in an increasingly hostile global business environment.  
Recently, a common strategy to secure competitiveness has been associated with 
sourcing production to low-cost countries, which may provide short term benefits as 
immediate cost reductions and expanded marked presence. However, this does not 
necessarily guarantee long-term competitiveness; for example, many automakers have 
been struggling financially for a long time despite outsourcing strategies, cuts in labor 
and development costs, and open e-bidding for sourcing parts. A major concern in this 
respect is how manufacturing companies can position themselves better in the global 

896 
T. Welo, S.H. Aschehoug, and G. Ringen 
market place, especially if the companies primarily operate in high-cost countries in, 
say, Northern Europe? One answer may be to dramatically improve the companies’ 
capabilities to invent, develop, and produce new products, and at the same time in-
crease customer value. Hence, companies in high-cost countries must improve their 
focus on innovation and product development activities to develop more attractive 
products, ones that satisfy user (customer) requirements, needs and desires. These 
improved products must research the market place earlier than the ones provided by 
the competitors, and before new and improved technologies are available for other 
market players or the market changes [1].   
This article aims to address this outermost important challenge by drawing on 
available extant scientific and management literature on lean and integrated product 
development. By assessing and understanding current lean product development prac-
tices in the Norwegian manufacturing industry, companies can establish strategies for 
closing the gap between current performance and best-practice in areas critical to their 
performance. Hence, the overall objectives of the present article are as follows: 
• To present the theoretical background of a lean product development (LPD) model. 
To present and explain the LPD model which is believed suitable for western-style 
climate, culture, and management, and for companies (not limited to large corpora-
tions) with a strategic focus on development and manufacturing of innovative 
products for the global markets.  
• To present the new results of a survey in Norwegian manufacturing companies 
using the LPD model; that is, to identify current practices relative to several com-
ponents related to lean practices in new product development. 
In the following, when referring to (new) product development, this process is herein 
defined as: “the collective activities, or systems, that a company uses to convert its 
technology and ideas into a stream of products that meet the needs of customers and 
the strategic goals of the company”. Moreover, lean product development is referred 
to as “a company-wide product development system aimed at maximizing customer or 
user value, within the constraints of value of other stakeholders” [2]. Lean product 
development is in this article used in a broader term than presently described by, for 
example, Liker and Morgan in Toyota's Product Development System (TPDS) [3]. 
2 
Theoretical Background 
2.1 
Lean Product Development 
Lean in a historical perspective has its roots back to the Japanese industrial success 
after World War II, mainly as a result of their knowledge of modern manufacturing 
techniques and principles utilized in a ‘zeitgeist’ of product and technology driven 
transformations. As a paradox, many of these techniques and principles originated 
from USA and Europe, but were adapted, customized and developed to fit the Japa-
nese culture and context. The Toyota Production System (TPS) being the most suc-
cessful example in this context [4]. Today, lean manufacturing is mainly an  

Assessing the Relationship between New Product Development Practices and Performance 
897 
operational management strategy derived from TPS in the early 1980’s, [5],[6],[7] 
and [8]. Later, lean manufacturing principles have been further developed from a 
manufacturing context, to organization and leadership, healthcare, military and also 
public organizations [3].  
As relevant elements concerning LPD was scattered across fields, a new model for 
LPD was developed grounded in a review of scientific and management literature and 
the authors’ interpretations based on personal experiences from industrial companies.   
2.2 
The LPD Model 
The LPD model used is based on six core components with different sub-
characteristics derived from lean thinking applied to new product development: 1) 
(lean) culture, 2) stabilization, 3) standardization, 4) knowledge, 5) customer value, 
and 6) continuous improvement [1]. The model shown in Figure 1 is based on various 
interpretations of the TPDS found in management and research literature, in addition 
to new thinking, views and practices captured from various sources. As the model is 
meant for LPD practices in Norwegian manufacturing companies with strategic focus 
on value-added products, the characteristics of the model are to some extent adapted 
to the climate, culture, organization and management (style) believed typical in Nor-
wegian and other Scandinavian companies. In the following, each core component, or 
category, is briefly described. 
 
Fig. 1. The lean product development model 
Customer Value (CV) 
In lean, a generally accepted definition of value generation is when a specific opera-
tion meets all three of the following requirements [9]: 

898 
T. Welo, S.H. Aschehoug, and G. Ringen 
• 
The customer is willing to pay for the activity. 
• 
It transforms the physical shape of the object or product. 
• 
It is done correctly first time. 
Waste, on the other hand, occurs when an operation fails to meet just one of these 
criteria. However, separating value from waste is more complicated in product devel-
opment than in manufacturing since there is no physical object, the process is  
iterative, and the cycle time is months and years, not seconds or minutes. Product 
development typically involves problem-solving, information transformation [10] and 
knowledge creation [11], in which the work product is information and knowledge 
aimed at reducing the risks of taking a new product to market [2].  
Mascitelli [12] claims that the values in PD are embodied in the essential delive-
rables needed to launch a new product: “any activity or task that transforms a new 
product design (or the essential deliverables needed to produce it) in such a way that 
the customer is both aware of it and willing to pay for it”. Value starts with the cus-
tomer, i.e., the user, consumer or ultimate customer, and the perception of value based 
on his or her needs, wants (spoken and unspoken), and meanings of a product [13]. 
Customer value then represents all the benefits that a customer, explicitly or implicit-
ly, acknowledges with a product relative to its price.  
 
Culture (C) 
Lean thinking represents an important, but often neglected element of a company’s 
culture [14]. Hence, LPD is an integral part of the business system, at all organiza-
tional levels and functional areas. Important in lean culture is trust, respect and re-
sponsibility.  Everyone’s opinion is respected, valued and considered. Responsibility 
is delegated to the lowest possible level, the one closest to the problem, and decisions 
are fact-based. Other typical elements in lean culture involve a desire for learning and 
use of knowledge to solve problems at the root cause. Lean culture also involves ex-
perimentation and outside-the-box thinking as seeds to innovation. Finally, visual 
communication to create understanding, involvement and commitment of people is 
integral part of lean culture. 
 
Stabilization (S) 
A PD system (infrastructure, organization, management and process) must provide a 
fundament for continuous improvements (CI), quality work and organizational  
performance; that is, there has to be an organizational infrastructure that facilitates 
strategic deployment and long term commitment to build excellence in PD. Browning 
[2] states that LPD is commonly applied with a system perspective. LPD includes 
defining a technology and product strategy, product leadership, portfolio manage-
ment, and a design reuse strategy. In all organizational levels, management focuses on 
deliverables⎯rather than processes, activities and tasks⎯as most of the value is em-
bodied in the deliverables. To secure predictable conditions in product development, 
resource and workload planning are important. In addition, integration of manufactur-
ing early in PD is a key to prevent waste, i.e. design loopbacks, resource squeezes and 
overruns [7]. Finally, defining core and strategic products, along with the suppliers’ 
strategic roles in delivering value, are important for establishing a design strategy 
founded in lean principles. 

Assessing the Relationship between New Product Development Practices and Performance 
899 
Standardization (Std) 
Standardization is important for allowing more experimentation and innovation, ra-
ther than providing a means for enforcing discipline. True customer value is believed 
to be best and consistently delivered from multidisciplinary work based on the same 
knowledge standard. A standardized product development process is not only vital for 
variability testing, but also for making room for creativity and entrepreneurship. The 
purpose of standardization is generally to reduce waste, development time, risk, er-
rors, and output variability from product development [7],[15]. Primary focus should 
be on standardizing output deliverables⎯and not on enforcing a rigid structure of 
activities, say, between phase gates in a rigid business governance process.  
 
Knowledge (K) 
Knowledge is important as a value stream and competitive factor for lean companies; 
in fact, Kennedy [16] refers to LPD as a “world of knowledge, rather than a world of 
tasks”. Companies that lack systems, processes and culture for generating, capturing 
and standardizing knowledge for later re-use will suffer from dilution of market value 
when losing people (downsizing)⎯and not the opposite in an instant perspective, as 
commonly seen in the stock market. Collective knowledge generation and ability for 
learning are the only permanent advantages as markets, technologies and competitors 
change over time.  
 
Continuous Improvement (CI) 
Continuous improvement (CI) is one of the core components of lean. CI is perhaps the 
lean principle with the most obvious transparency between manufacturing and new 
product development. CI involves high involvement of people and incremental 
changes in products or processes for enhanced business performance [17]. CI may 
also be looked upon as a learning process in which a deliberate effort is made to man-
age and accelerate learning by as many people in improvement tasks as possible [18]. 
CI work is a systematic exercise and practice over time and not quick fixes, most 
commonly involving the use of productivity measures (performance indicators). Fi-
nally, the understanding of relationships between lead time, product performance, 
development cost and product cost, and business performance (profits) is required to 
prioritize improvement work. 
3 
Research Methodology 
3.1 
Research Design 
A wide set of Norwegian manufacturing companies were chosen to participate in a 
descriptive survey to gain preliminary insight into the status of LPD. The main pur-
pose of the study was, more importantly than theory development in itself, to describe 
facts that may be used for theory building or theory refinement [19]. The population 
frame (i.e. the list of all elements in the population in the sample is drawn from [19]), 
was based on company size (minimum 50 employees in company), having in-house 
product development department, manufacturing of physical non-commodity products 

900 
T. Welo, S.H. Asche
(not services), at least 30%
specific or engineered produ
Non-probabilistic sampl
tion relevant to and availab
product development and d
ment and design engineers,
managers and functional m
characteristics as well as s
of firms by industry is show
 
Fig. 2. Num
3.2 
Data Collection Me
The survey was conducted 
was based on a questionna
swers. The survey was distr
ly and was made open to th
was pretested within the aca
The questionnaire was b
to measure subjective mean
the field of social science,
companies’ current perform
point scale was used as pre
pondents prefer a five-poin
and that it is easy to use [
opportunity of indicating ne
response as the easy way ou
Furniture, 12
Defense, 3
Tooling, 23
Marine,
ehoug, and G. Ringen 
% value added in the manufacturing process, and custom
ucts.  
e design was used as it was important to obtain inform
le from only certain groups [19] (i.e. personnel involved
design). The subjects in the sample were product devel
 quality engineers, process development engineers, proj
managers. The sample size in each firm was based on f
ize of product development departments. The distribut
wn in Figure 2 below. 
mber of respondents per industry sector (N=258) 
ethod 
in the period between September 2011 and April 2012
aire with 24 questions with predefined categories for 
ributed on e-mail to all subjects and managed electronic
he respondents for approximately three weeks. The sur
ademic faculty before distribution to industrial compani
based on a Likert scale. The Likert scale is widely appl
nings, preferences, opinions, emotions and attitudes wit
, and was therefore chosen as appropriate for measur
mance with respect to LPD. In the current research, a f
esented in Table 1. McDonald found that most survey r
nt rating scale because the number of options is adequ
20]. The adopted scale provides the respondents with 
eutrality, even though the respondents may see the mid
ut [21]. 
Automotive, 68
Protective
Clothing, 
Water, Gas & 
Distribution
Systems, 28
Healthcare, 11
Other 
Component 
Industry, 22
Offshore, 25
6
 9
Food and 
Beverages, 20
mer 
ma-
d in 
lop-
ject 
firm 
tion 
  
2. It 
an-
cal-
rvey 
ies. 
lied 
thin 
ring 
five 
res-
uate 
the 
ddle 
e 
4
Air 
n 
8

Assessing the Relationship between New Product Development Practices and Performance 
901 
Table 1. The five point Likert scale used in the survey 
Ranking 
Explanation 
Level 1 
Strongly Disagree – There exist no evidence of this specific component 
Level 2 
Mostly Disagree – Awareness has begun and top-down implementation is initiated 
Level 3 
Neutral – Component established and ownership in place 
Level 4 
Mostly Agree – Component undergoing refinement and technology introduced where 
appropriate 
Level 5 
Strongly Agree – Culture change clearly evident, world-class performance achieved 
3.3 
Data Analysis 
For each of the categories Customer value, Culture, Stabilization, Standardization, 
Continuous improvement and Knowledge (described above), a set of statements was 
outlined. The results from 24 statements, see Table 3, were compared against an indi-
cator describing new product development performance, which was developed based 
on the six statements listed in Table 2. These were indexed through a factor analysis 
giving a Cronbach's alpha value of 0.71. Cronbach’s alpha is used to calculate how 
well each individual item in the scale correlates with the sum of the remaining items. 
When Cronbach’s alpha is sufficiently high for a group of items, it is reasonable to 
treat them as an indicator instead of in terms of individual items. The threshold-value 
for a reliable measure is widely discussed; for example both Kline [22] and Halvorsen 
[23] argued that alpha-values of 0.7 or higher are acceptable, and that 0.8 or higher 
indicates good reliability.  
Table 2. Statements defining product development performance index 
S1 
Customers are generally satisfied with the true value realized in our new products 
S2 
Product development projects are launched on time 
S3 
Product development projects are launched at budget 
S4 
During the last three years, new product introductions have met profitability targets 
S5 
During the last three years our product portfolio have been extended by introducing (new to us) 
type of products in the marketplace 
S6 
During the last three years new product introductions have contributed as expected to our sales 
objectives 
 
In total 258 respondents from 35 companies answered the survey. Finally, a mul-
tiple linear regression analysis was conducted to define the relationship between the 
dependent variable, denoted the NPD performance indicator, and the 24 exploratory 
variables representing the six lean product development dimensions. Support criteria 
are defined at p<0.05.  

902 
T. Welo, S.H. Aschehoug, and G. Ringen 
4 
Results and Discussions 
This study shows that four out of twenty-four statements are significantly related to 
the product development indicator. These four are: (1) “Every project team member 
knows critical product characteristics at any time, the ones that influence perceived 
customer value”; (2) “Selection criteria for projects are based on defined metrics, and 
primarily driven by our ability to mitigate risks while satisfying customer value”; (3) 
“Our design engineers follow the design strategy and see standardization as a means 
to create products of high value to customers”; and (4) “Use of simple and visual 
communication is strongly anchored in the company’s culture”. The first statement 
(1) is defined within the category “Customer Value” and may be perceived as a fun-
damental and practical way of fulfilling customer requirements.  
Statement (2) is interlinked with the customer value dimension but has a more stra-
tegic perspective, which may be seen as mandatory or as more distant from everyday 
 
Table 3. Results multiple regression analysis (* = significant at p < 0.05) 
LPD 
factor 
Statements (abbreviations) 
Mean Std. 
Dev 
Pr > |t| 
CV 
Customer value in company's mission/vision statement 
4.1 
0.91 
0.820 
CV 
Customer value drives strategy and activities in PD 
4.1 
0.77 
0.358 
CV 
Customer is integrated in PD activities 
3.7 
0.99 
0.167 
CV 
Team member knows product characteristics related to customer value 
3.4 
0.89 
0.035* 
K 
Knowledge gaps in PD are identified 
2.8 
0.93 
0.749 
K 
The company always develops multiple design concepts early in PD 
3.1 
1.06 
0.435 
K 
Insight and new information is discovered by physical testing 
4.1 
0.85 
0.787 
K 
Information and knowledge is sought actively from outside company 
3.6 
0.88 
0.141 
CI 
CI in PD is part of company strategy 
3.9 
0.92 
0.206 
CI 
Responsibility and roles in PD are clearly defined  
3.4 
0.90 
0.677 
CI 
Value added work and waste in PD are clearly defined 
2.9 
1.00 
0.683 
CI 
Plan-Do-Check-Act problem solving cycle is used in PD 
2.7 
0.90 
0.388 
S 
Holistic approach for project selection and portfolio planning is used 
3.0 
0.93 
0.600 
S 
Risks and satisfying customer value drives project selection 
3.1 
0.86 
0.049* 
S 
Resource planning is used in PD 
2.9 
0.92 
0.357 
S 
Design for Manufacturing (DFM) is used in PD 
2.9 
1.09 
0.079 
St 
A formal product realization process is followed 
3.3 
0.96 
0.138 
St 
Cross training is used to increase resource (management) flexibility 
2.7 
0.89 
0.223 
St 
The roles of reuse, modularization and customization are defined in 
design strategy 
3.1 
0.92 
0.956 
St 
Design strategies are followed and standardization is sought in PD 
3.4 
0.92 
0.023* 
C 
Opinions and views are equally respected of all employees 
3.5 
0.99 
0.401 
C 
Responsibility is delegated to the level closest to the problem 
3.7 
0.83 
0.656 
C 
Decisions are based on a process of involvement 
3.6 
0.82 
0.928 
C 
Simple and visual communication is part of company culture 
3.4 
0.89 
0.042* 

Assessing the Relationship between New Product Development Practices and Performance 
903 
work of satisfying customers. Secondly, stabilization of the product development 
environment by selecting the projects where the company has the higher probability 
of mitigating risks and solve problems is rated high in relation to financial success. In 
lean this phenomena refers to ‘balancing the line’⎯which is as important in product 
development to avoid overburden and unnecessary interruptions by defining a project 
portfolio that balances risk and reward. Here an issue may be that too much focus on 
stabilization and low risk projects can harm the company’s ability to come up with 
breakthrough innovations. 
The significant statement (3) belongs to the category standardization, highlighting 
the importance of following the agreed-upon design strategy. Deploying a design 
strategy means that people know what to make and do, why they should do it and how 
to achieve the defined objectives. This statement is highly interwoven with dimension 
‘Knowledge’ since a defined design strategy also expresses what knowledge is impor-
tant to the company, including actions in place to capture, generalize and reuse it.  
The significant statement (4) is primarily related to the ‘Culture’ dimension, indi-
cating the importance of simple and visual communication in the product develop-
ment environments. The non-linear dynamics of product development processes, with 
information being the work product, calls for shared communication practices to 
overcome this over-complexity between different functions, competencies, countries 
and even time zones in order to maximize value⎯i.e., make the right information 
available to the right person when needed. 
5 
Conclusion 
To dramatically improve the capabilities to invent, develop and produce new prod-
ucts, while increasing customer value, is key to sustain and increase competitiveness 
of Western companies. Grounded in lean theory, a model for LPD has been developed 
based on a literature review and interpretations and the authors’ own industrial  
experience.  
Based on survey results from Norwegian manufacturing companies, current prac-
tices relative to key LPD components have been identified. To generalize from these 
initial findings, and to give advice about practical application, there seems to be a 
pattern that PD team members emphasize clear project prioritization based on portfo-
lio thinking with basis in the capabilities of the company. To enforce standardization 
and level the work load, the agreed upon design strategy is to be followed. In addi-
tion, a successful outcome in PD presumes that every project member continuously 
update themselves on critical product characteristics. Finally, the use of simple and 
visual communication modes is essential to overcome the complex dynamics and 
invisible nature of PD as a ‘process’ within the organization. Overall, this study can-
not tell that these elements should be prioritized in any case; however, in context of 
LPD in Norwegian manufacturing companies these four characteristics seem to be 
highly correlated to both company and new product development performances.  
For further research, the authors will increase the population to strengthen the data 
set and to conduct comparative analyses across borders and industries.                    

904 
T. Welo, S.H. Aschehoug, and G. Ringen 
Acknowledgments. This research has been funded by Centre for Research Based 
Innovation - Norwegian Manufacturing Future (NORMAN). 
References 
1. Welo, T.: On the Application of Lean Principles in Product Development: a Commentary 
on Models and Practices. Int. Journal of Product Development 13, 316–343 (2011) 
2. Browning, T.R.: On Customer Value and Improvements in Product Development 
Processes. Systems Engineering 6, 49–61 (2006) 
3. Liker, J.K., Morgan, J.M.: The Toyota Way in Services: the Case of Lean Product Devel-
opment. Academy of Management Perspectives 20, 5–20 (2006) 
4. Liker, J.K.: The Toyota Way – 14 Management Principles form the World’s Greatest 
Manufacturer. McGraw-Hill, New York (2003) 
5. Womack, J.P., Jones, D.T., Roos, D.: The Machine that Changed the World: the Story of 
Lean Production. Harper Perennial, New York (1990) 
6. Karlsson, C., Åhlström, P.: The Difficult Path to Lean Product Development. Journal of 
Product Innovation Management 13, 283–294 (1996) 
7. Womack, J.P., Jones, D.T.: Lean Thinking: Banish Waste and Create Wealth in Your Cor-
poration. Free Press, New York (1996) 
8. Baines, T., Lightfoot, H., Williams, G.M., Greenough, R.: State-of-the-art in Lean Design 
Engineering: a Literature Review on White Collar Lean. Journal of Engineering Manufac-
ture 220(Pt. B), 1539–1547 (2006) 
9. Fiore, C.: Accelerated Product Development, Combining Lean and Six Sigma for Peak 
Performance. Productivity Press, New York (2005) 
10. Hubka, V., Andersen, M.M., Eder, W.E.: Practical Studies in Systematic Design. Butter-
worth & Co., London (1988) 
11. Hicks, B.J., Culley, S.J., Allen, R.D., Mullineux, G.: A Framework for the Requirements 
of Capturing, Storing and Reusing Information and Knowledge in Engineering Design. Int. 
Journal of Information Management 22, 263–280 (2002) 
12. Mascitelli, R.: The Lean Development Guidebook. Technology Perspective (2007) 
13. Verganti, R.: Desig Dtriven Innovation – Changing the Rules of Cometition by Radically 
Innovating what Things Mean. Harvard Business Press, Boston (2009) 
14. Yoshimur, T.: Lean Product Development from Inside Toyota & How Toyota Attains Ex-
cellent Quality. In: JMAC Scandinavia Conference, Gothenburg, Sweden (2009) 
15. Reinertsen, D.G.: Managing the Design Factory. Free Press, New York (1997) 
16. Kennedy, M.N., Harmon, K., Minnock, E.: Ready, Set, Dominate: Implement Toyota’s Set 
Based Learning for Product Development. The Oaklea Press, Richmond (2008) 
17. Ljungström, M., Klefsjö, B.: Implementation Obstacles for a Work development-oriented 
TQM Strategy. Total Quality Management 13, 621–634 (2002) 
18. Imai, M.: Kaizen. Random House, New York (1986) 
19. Karlsson, C.: Researching Operations Management. Taylor & Francis, New York (2009) 
20. McDonald, J.L.: The Optimal Number of Categories for Numerical Rating Scales. Univer-
sity of Denver, Denver, PhD (2004) 
21. Krosnick, J.A.: Response Strategies for Coping with the Cognitive Demands of Attitude 
Measures in Surveys. Applied Cognitive Psychology 5, 213–236 (1991) 
22. Kline, P.: The Handbook of Psychological Testing. Routledge, London (1999) 
23. Halvorsen, K.: Å Forske på Samfunnet – En Innføring i Samfunnsvitenskapelig Metode. 
Cappelen Forlag, Oslo (2008) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 905–914. 
DOI: 10.1007/978-3-642-30817-8_89 
© Springer-Verlag Berlin Heidelberg 2013 
 
An Indicator-Based Process Monitoring Cockpit  
for Controlling and Enhancing Product Development 
Processes – An Industrial Case Study 
Thomas Luft, Simon Smoll, and Sandro Wartzack 
Friedrich-Alexander-University of Erlangen-Nuremberg, Chair of Engineering Design,  
Martensstrasse 9, 91058 Erlangen, Germany 
{luft,wartzack}@mfk.uni-erlangen.de 
Abstract. The need for shorter development times, lower costs, and higher 
quality requires improving the performance of the product development process 
(PDP) continuously. Therefore the evaluation and monitoring of the product 
development performance is of great interest for companies wishing to increase 
their competitiveness. However, there are very few approaches for the perfor-
mance measurement of the PDP. For this purpose, a process monitoring cockpit 
with the overall goal to enhance the PDP regarding time, costs, quality is pre-
sented. This approach for measuring and continuously improving the current 
state of a company’s PDP has been validated in an industrial case study. 
Keywords: product development process, evaluation methodology, knowledge 
management, decision making, continuous improvement, industrial case study. 
1 
Introduction 
Today’s challenges facing product developers are in particular shorter product life 
cycles and increasing product complexity and customized products. Therefore product 
development processes (PDP) have to be thoroughly planned, adapted to a specific 
project or enterprise and constantly improved in terms of certain target criteria. This 
however, requires a consistent, indicator-based monitoring of each step of the product 
development process in order to measure its performance [1]. But, one of the biggest 
problems in practice is the difficulty of process improvements, as the process perfor-
mance cannot be measured easily and adequately enough [1], [2]. 
For this problem, literature contains a variety of possible process management ad-
vices; for example, that changes in the actual process can increase the efficiency of 
the product development process [1], [3-5]. Moreover, there are also lots of industrial 
projects that seek to introduce a suitable product development process or to improve 
it. However, an appropriate generic process evaluation tool for the considered enter-
prise could not be found because existing evaluation tools are particularly not suffi-
ciently detailed or objective or systematic [2], [6], [7]. 

906 
T. Luft, S. Smoll, and S. Wartzack 
This paper introduces an indicator-based evaluation methodology for monitoring 
and enhancing product development processes in terms of quality, costs and time. The 
aim is to optimize the holistic product development processes with respect to its 
process design and organization as well as to its execution. To validate the developed 
process monitoring cockpit, a product development process has been introduced in an 
international civil engineering company with the intention to improve the processes 
during the development of new products in terms of these three target dimensions. 
2 
Background and Related Work 
In the following section some fundamentals and important approaches which are the 
scientific background of this paper will be delineated.  
2.1 
Product Development Process 
In times where challenges for companies are growing more complex, product cycle 
times are decreasing and more individual demands on products have to be considered, 
the development, production and first operation of new products require an efficient, 
transparent and detailed product creation process to enable the cooperation of the 
involved employees from different departments [8]. In order to monitor and optimize 
the single process steps as well as their interdependencies, new approaches are 
oriented particularly to a comprehensive product life cycle [9]. Thereby, all phases – 
starting from a placing of order (or market research or product idea) to the final dis-
posal of the product – have to be considered [9], [10]. The focus of this paper is the 
product development process, which is part of the product life cycle, and includes, 
among others, the phases of product planning, definition, development, design, simu-
lation [10].  
Essential preconditions for the structured evaluation and monitoring are the consis-
tency, completeness and logical feasibility of the product development process [11]. 
Once these have been checked and revised (if necessary), all steps of the product de-
velopment process with their respective relations have to be displayed with the help of 
process modeling method (e.g. ARIS, BPMN) in form of a flowchart in order to de-
scribe and visualize each process step in detail. 
2.2 
Development of the Monitoring Cockpit by Existing Methods 
The approach to the development of a monitoring cockpit described below can be 
found in [12]. In order to design an efficient and effective performance measurement 
system to evaluate a product development process, the company-specific require-
ments have to be analyzed first of all. This leads to the following requirement specifi-
cation which has to be fulfilled by the performance measurement system. Thereby the 
main requirements are: the monitoring of the target dimensions quality, cost, time 
(A); the definition of individual target figures (B); the applicability to the process 
design (C) and to the process execution (D); the graphical presentation of the  

 An Indicator-Based Process Monitoring Cockpit for Controlling and Enhancing PDP 
907 
evaluation results (E); the indicator-based results (F); the low expenditure of time for 
evaluation (G); simplified feasibility of a repeated evaluation (H); the highly objectiv-
ity (I). Subsequently, a literature research has to be conducted. The following five 
methods were identified and served as a basis for the developed and in this paper 
outlined conceptual design of such an indicator-based monitoring cockpit: 
• 
Requirements oriented weighted evaluation by Breiing [13]: Multi-criteria evalu-
ation methodology in which especially customer requirements are taken into ac-
count. Determination of overall weights by multiplication of weights and values 
as a basis for decision making between different alternatives. 
• 
Process audit of the VDA [14]: A process audit is used to evaluate the quality 
capability for specific products or product groups and their processes with the 
help of a detailed questionnaire. 
• 
Technical-financial evaluation by VDI 2225 [15]: Subdivision of the criteria for 
technical and financial aspects and the overall evaluation results in a graphical 
diagram. In this two-dimensional diagram the position of the technical and finan-
cial value of one variant is shown. 
• 
Cost utility analysis by Zangemeister [16]: Target criteria are displayed in a tree 
structure, in which any clusters can be formed. After the preparation and weight-
ing of the target system, the target dimension matrix and subsequently the target 
value matrix have to be set up before the utility matrix can be determined. 
• 
Performance measurement system of the ZVEI [17]: Evaluation of corporate 
success with the help of business indicators and thereof developed business per-
formance measurement system. 
These five methods described have to be compared with the requirement specification 
of the cockpit – see Figure 1 – in order to identify the respective methodological 
components that meet all the requirements (marked with a tick). These components 
are used in the following chapter to develop a methodological approach for an indica-
tor-based process monitoring cockpit for controlling and enhancing product develop-
ment processes. 
 
Fig. 1. Overview of the fulfillment the of requirements by methods 
Evaluation methods
Requirements oriented 
weighted evaluation
Process audit
Technical-financial 
evaluation
Cost utility analysis
Performance 
measurement system
A
B
C
D
E
F
Respective components
G
H
I
Requirements

908 
T. Luft, S. Smoll, and S. Wartzack 
3 
Methodology for the Monitoring Cockpit 
As a result of this preliminary work, an indicator-based process monitoring cockpit 
regarding the mentioned requirements is developed. This cockpit consists of three 
levels "process step", "target criteria" and "evaluation criteria". For the evaluation of 
the product development process, the three target dimensions “quality” and “cost” and 
“time” of the target criteria have to be broken down to the evaluation criteria level for 
each process step (see Figure 2). The company-specific evaluation criteria themselves 
can be optionally split again [11]. To reflect the importance of each single criterion 
for the fulfillment of the target dimensions, a weighting is necessary. The weighting is 
based on the mentioned cost utility analysis by Zangemeister [16] and has to be done 
for each process step and target criterion and evaluation criterion. This results in mul-
tiple target systems that include the entirety of all evaluation criteria at the finest dif-
ferentiation level. Furthermore the consistency of all criteria and the same direction in 
terms of their target fulfillment has to be ensured. 
 
Fig. 2. Simplified overview of the indicator-based process monitoring cockpit 
Fulfillment of 
the target criteria regarding
Rating of 
the criteria regarding
Weighted fulfillment of
the target criteria regarding
Fulfillment of 
the target criteria regarding
Weighted  fulfillment of 
the three target dimensions
Measure of the total target 
performance for each process step
Measure of the potential for 
improvement of a
process step
process step 
level 
target criteria 
level
evaluation criteria 
level
Measure of the potential for 
improvement of a
target criterion regarding 
Measure of the potential for 
improvement of an 
evaluation criterion regarding
time
time
time
time
time
cost
quality
cost
quality
cost
quality
cost
quality
cost
quality
time
cost
quality
time
cost
quality
Calculation paths of the evaluation
Calculation paths of the potentials
Valuation numbers and degrees of fulfillment
Potentials for improvement
Analysis paths of the potentials

 An Indicator-Based Process Monitoring Cockpit for Controlling and Enhancing PDP 
909 
For each level, potentials for improvement of the three target dimensions can be 
calculated by using appropriate characteristic values (calculation paths of the poten-
tials). Moreover, it is also possible that the potential for improvement (analysis paths 
of the potentials) of a process step (e.g. project kickoff) can be broken down into the 
target criteria level (e.g. personnel costs) and the evaluation criteria level (e.g. hourly 
rate). It will be searched, for example, for the maximum potential (see Figure 4) and 
then selected in the level below the corresponding target values and criteria. 
Generally, it is possible to weight evaluation criteria as well as target criteria and 
process steps. For this weighting two different perspectives of the indicator-based 
process monitoring cockpit are introduced. 
The first perspective is the target dimension view that allows a view on the PDP 
concentrating on the total fulfillment of the three target dimensions. Therefore a 
weighting at the target criteria level is necessary, which determines the importance of 
the process steps for the fulfillment of the target criteria considering the whole PDP. 
Because of the great number of process steps, a weighting following the principles of 
Zangemeister is not the ideal solution. In contrast, the requirements oriented weighted 
evaluation by Breiing allows to proportionate objects compared with only one refer-
ence object [13]. Afterwards the proportions between the other objects can be  
calculated. For this purpose consistent decision matrices are used. This results in per-
centage weightings of the target dimensions for all process steps. 
The second perspective on the PDP is the process step view, which focuses on the 
importance of the target dimensions for the fulfillment of the different process steps. 
Therefore a process step internal weighting of the target dimensions is conducted. 
Because of the recurring weighting objects, a graphical weighting procedure which 
requires only the determination of a so called balanced weighting point is reasonable 
to be used (see Figure 3). 
This is done by a weighting triangle, which is stretched between the three target 
dimensions process quality, process cost and process time. From any point within the 
weighting triangle, lines can be drawn parallel to the three sides of the triangle to the 
outside in order to read off from the respective scale the target dimensions weightings 
for the considered process step. Placing the balanced weighting point inside of the 
triangle and following the lines parallel to the sides of the triangle the intersection 
points with the scales positioned at the triangle’s borders show the process step inter-
nal weighting (e.g. process quality 30%, process cost 40%, process time 30%), which 
is already normalized to 100%. 
Since each process step is associated with a specific department and because for 
each department characteristic minimum and maximum weightings can be derived 
from their importance for the PDP, a standardized restriction field can be defined. In 
Figure 3 a grey colored area is marked in the middle of the triangle which represents 
the part of the triangle that is possible for setting of the balanced weighting points for 
all process steps which belong, for example, to the product development department. 
These restrictions are set up for each department depending on their specific cha-
racteristics and objectives. The restriction, which applies to all departments, is the so 
called ten-percent-rule, which limits the lowest weighting for all target dimensions at 
ten percent in order to ensure a minimum in terms of balanced weighting. 

910 
T. Luft, S. Smoll, and S. Wartzack 
 
Fig. 3. Weighting triangle with an example 
For instance, the weighting of the target dimension quality depends on the position 
of the process step in the PDP. Is it in an early phase of the PDP and the results are 
only first estimations which includes that there will be an overworking process step, 
the quality can be depressed down to 20 %. In the opposite case of a process step 
occurring in a late phase of the PDP the weighting for the process quality consequent-
ly has to appointed higher. So the upper limit in this example is placed at 40 %. The 
process costs of the process steps affecting the product development department de-
pend on, for example, the used IT-infrastructure which can be very expensive in the 
case of finite elements method software or other highly specialized software solutions 
partially necessary in this department. So the upper limit is expanded up to 50 %. But, 
there are also process steps that require almost no IT-infrastructure and lower paid 
employees, and therefore the lower limit can be set to 10 %. In Figure 3 has the third 
target dimension a very great spread from the lower limit of 10 % and the upper limit 
of 60 %. 
For the evaluation of the single criteria (e. g. expenditure of time on searching doc-
uments, time required for installation) have to be linked to target values (T) (e.g. eight 
hours) at the “evaluation criteria”-level. Using the actual values (V), the degrees of 
fulfillment (F) can be calculated. The specific degrees of fulfillment are calculated by 
 
10 %
20 %
40 %
50 %
60 %
70 %
80 %
90 %
Quality 100 %
30 %
0 %

 An Indicator-Based Process Monitoring Cockpit for Controlling and Enhancing PDP 
911 
dividing the actual performance value by the target value like it is shown in formula 
(1). The addition of the (weighted) degrees of fulfillment of the criteria leads to the 
degrees of fulfillment regarding the three target dimensions. By their process step 
internal weighting and addition, the degree of fulfillment of a process step can be 
obtained. 
 
F = 
V
T 
(1) 
The potential for improvement (P) is calculated of the difference between the degree 
of fulfillment and the corresponding maximum value of one hundred percent which 
represents the gap to the top. To ensure, that this potential also indicates the value as 
well as the importance of this gap for the process performance (e.g. time), the gap has 
to be multiplied by the respective weighting (w). The correspondent equation is 
represented by formula (2). 
 
P = w × (1 - F) 
(2) 
The weighting is depending on the perspective of the PDP. For the target dimension 
view the target dimension weighting over the whole PDP is used and consequently the 
calculated potentials are called target dimension potentials. In contrast the potentials 
in the process step view are denominated step potentials and are calculated by multip-
lying the fulfillment gaps by the step internal weightings. For a structured analysis of 
the potentials in the process step view, an additional potential concerning all target 
dimensions is necessary. It is positioned on the process step level, calculated by the 
process step internal weighted sum of the degrees of fulfillment and multiplied by the 
process step effectively. This is a new weighting over all process steps, which re-
spects the portion of the PDP output which is generated by the single process steps. 
Its calculation is elaborated analogical to the target dimension weightings with the 
consistent decision matrices of Breiing [13]. 
In order to realize the identified and calculated potentials regarding the desired tar-
get dimension, as it is shown in Figure 4, it is important to analyze the maximum 
potentials at an appropriate level. For instance, if the target dimension quality has to 
be improved, it is recommended, to concentrate on the quality potentials in the target 
dimension view. After locating the process step with the highest potential, the corres-
ponding criteria potentials have to be analyzed in detail. However, there are not only 
positive but also negative potentials – as can be seen from the below figure 4. The 
time targets, for example, in the fourth step of the process were exceeded. In this case, 
resources or efforts can be reduced and used for the optimization of other process 
steps or target dimensions. 
If there is otherwise no preferred target dimension, the effectively potentials serve 
as a starting point for concrete improvements. After finding the maximum, a process 
step for the optimization is determined. So its step potentials have to be investigated, 
to identify the most effective target dimension, which leads again to the correspond-
ing criteria potentials. 

912 
T. Luft, S. Smoll, and S. Wartzack 
 
Fig. 4. Simplified example of an evaluation report of the potentials of the target dimensions 
The entire execution of the product development process is monitored by the 
process monitoring cockpit at the mentioned levels. The graphic representation of all 
relevant information provides a comprehensive, objective and key-performance-
indicator based evaluation for decision making in quality gate meetings. In addition, 
various projects can be compared with each other. Moreover, the managers get in-
depth analyses about what benefits can be achieved at which process step or in respect 
of which target dimension through what effort. As a result, the potentials with the 
most effective leverage (usually corresponds to the highest positive potential) can be 
identified very effectively with the help of the diagram in figure 4 (see also the ap-
proach by [7]). To realize these potentials, appropriate approaches and methods for 
optimizing the process steps or target dimensions will be proposed automatically [6]. 
Negative potentials caused by the overfilling of targets indicate over-engineering or a 
waste of resources. In step four, for example, resources are freed up by the reducing 
the target fulfillment of the target dimension time. These freed resources can be used 
to improve the achievement of the target dimensional quality. 
Beyond that, this process monitoring cockpit can be applied not only for process 
design but also analogously for adjusting and improving the process execution of the 
product development process (e.g. to avoid unnecessary iterations [18]). Therefore the 
differences between the execution potentials and the design potentials have to be cal-
culated to make sure that the high execution potential does not have its source in a 
high design potential. This is indicated by a potential difference close to zero. In this 
case it would be recommended to improve first of all the process design and not the 
process execution. 
0,7
0,6
0,5
0,4
0,3
0,2
0,1
0
-0,1
-0,2
-0,3
1                  2                 3                 4                5                  6                 7 
Process step
Potential of the target dimensions
4
3
7
Quality
Cost
Time
Negative potential
if targets are
exceeded
Positive potential
if targets are not 
met

 An Indicator-Based Process Monitoring Cockpit for Controlling and Enhancing PDP 
913 
4 
Applying and Evaluation of the Monitoring Cockpit 
The assessment of the indicator-based valuation methodology for monitoring and 
optimizing the product development process is done by its application in an industrial 
case study. Thereby strengths and weaknesses become apparent. Practice has shown 
that in particular the measurability of key criteria in the selected process steps during 
the product development process is crucial to the overall decision making process as 
well as to identify and realize optimization potentials. However, some criteria are 
difficult to measure and sometimes the setting of target values is difficult due to lack 
of information and experience. The key findings resulting from the application of the 
developed methodology are used for its continuous improvement. 
Furthermore the application of the weightings in the industrial case study has re-
vealed another challenge. As the process steps are executed by different departments, 
the knowledge about the importance of the steps to each other regarding the different 
target dimensions and the importance of the target values for the process steps itself 
have only a couple of persons. Consequently the head of each department has to de-
fine the weightings concerning the process steps under his responsibility and control. 
To ensure the objectivity, the weightings have to be reviewed by at least one of the 
executive directors. Besides this pure analysis and optimization of the product devel-
opment process, the methodology can also be used for monitoring of concrete projects 
and for the motivation of employees through the comprehensive communication of 
performance charts. 
5 
Conclusions and Further Work 
The goal of this paper was to present an indicator-based process monitoring cockpit 
for controlling and enhancing the product development process as well as to apply 
and evaluate it in an industrial case study. Practice has been shown that the developed 
methodology supports optimizing product development processes in their design and 
in their execution by connecting both aspects without losing the focus on the target 
dimensions quality, cost and time. This aspect-link even goes along with very useful 
synergy effects like the revealing of positive or harmful deviations of the execution in 
contrast to the process design.  
In order to realize the potential benefits in future work, it is necessary that the me-
thodology is implemented in all development processes and carried out by qualified 
employees. This can be ensured only when all employees are convinced of the useful-
ness of the process monitoring cockpit. 
Acknowledgment. The authors would like to thank sincerely designers and managers 
from our industrial partner, the Bauer Maschinen GmbH, for their great help and con-
siderate comments on the project. 

914 
T. Luft, S. Smoll, and S. Wartzack 
References 
1. Maranzana, N., Dubois, S., Gartiser, N., Caillaud, E.: Proposal of a System of Indicators to 
Measure Performance of Problem Solving Processes in Design. In: 10th DESIGN Interna-
tional Design Conference, pp. 1175–1180. DESIGN, Dubrovnik (2008) 
2. Wright, A.L., Ullman, D.G.: Introduction to Developing Measurements of the Process. In: 
12th ICED International Conference on Engineering Design, pp. 781–786. ICED, Munich 
(1999) 
3. Beskow, C., Norell, M.: Dialogue Conferences as a Means to Improve Co-operation in In-
dustrial Product Development. In: 11th ICED International Conference on Engineering 
Design, pp. 81–86. ICED, Tampere (1997) 
4. Eriksson, J., Johnsson, S., Olsson, R.: Modelling Decision-Making in Complex Product 
Development. In: 10th DESIGN International Design Conference, pp. 1129–1138. 
DESIGN, Dubrovnik (2008) 
5. Ritzén, S., Beskow, C., Norell, M.: Continuous Improvement of the Product Development 
Process. In: 12th ICED International Conference on Engineering Design, pp. 793–798. 
ICED, Munich (1999) 
6. Kreimeyer, M., Daniilidis, C., Lindemann, U.: A Framework to Classify Process Im-
provement Projects. In: 10th DESIGN International Design Conference, pp. 951–958. 
DESIGN, Dubrovnik (2008) 
7. O’Donnell, F., Duffy, A.: Modelling Product Development Performance. In: 12th ICED 
International Conference on Engineering Design, pp. 417–420. ICED, Munich (1999) 
8. Kreppenhofer, D., Langer, T.: Wissensmanagement in Prozessen des Maschinenbaus. 
ZWF. Jahrgang 100(4), 169–174 (2005) 
9. Denkena, B.: Wissensmanagement im integrierten Produktlebenszyklus. ZWF. Jahr-
gang 97(9), 428–431 (2002) 
10. Ehrlenspiel, K.: Integrierte Produktentwicklung. Denkabläufe, Methodeneinsatz, Zusam-
menarbeit. 4., aktualisierte Auflage. Carl Hanser Verlag, München (2009) 
11. Lindemann, U., Bichlmaier, C., Stetter, R., Viertlböck, M.: Enhancing the Transfer of In-
tegrated Product Development in Industry. In: 12th ICED International Conference on En-
gineering Design, pp. 373–376. ICED, Munich (1999) 
12. Saretz, B.: Entwicklung einer Methodik zur Parallelisierung von Planungsabläufen. Ein 
Beitrag zur Reduzierung von Produktentwicklungszeiten in der Serienproduktion. RWTH 
Aachen, Dissertation (1993) 
13. Breiing, A., Knosala, R.: Bewerten technischer Systeme. Theoretische und methodische 
Grundlagen bewertungstechnischer Entscheidungshilfen. Springer, Berlin (1997) 
14. Verband der Automobilindustrie e.V. (VDA): VDA 6 Qualitätsmanagement in der Auto-
mobilindustrie. Teil 3 Prozessaudit. Frankfurt am Main (1997) 
15. VDI 2225, Blatt 3: Technisch-wirtschaftliche Bewertung. Hrsg. Verein Deutscher Inge-
nieure (1998) 
16. Zangemeister, C.: Nutzwertanalyse in der Systemtechnik. Eine Methodik zur multidimen-
sionalen Bewertung und Auswahl von Projektalternativen. 4. Auflage. Zippel-Druck, Ber-
lin (1976) 
17. Zentralverband Elektrotechnik- und Elektroindustrie e. V. (ZVEI): ZVEI-Kennzahlen-
system. 4. Auflage. Verlag W. Sachon, Frankfurt am Main (1989) 
18. Krehmer, H., Meerkamm, H., Wartzack, S.: Avoidance of Unnecessary Design Iterations 
by Monitoring the Product’s Degree of Maturity. In: 11th DESIGN International Design 
Conference, pp. 285–294. DESIGN, Dubrovnik (2010) 

 
M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 915–925. 
DOI: 10.1007/978-3-642-30817-8_90     © Springer-Verlag Berlin Heidelberg 2013 
A Method to Design a Smart Home Interface  
Silvia Ceccacci, Michele Germani, and Maura Mengoni 
Department of Industrial Engineering and Mathematical Sciences 
Università Politecnica delle Marche, Ancona, Italy 
{s.ceccacci,m.germani,m.mengoni}@univpm.it 
Abstract. Smart home grids require that their control devices are both usable 
and acceptable. The assessment of device usability and acceptability is often 
neglected due to the cost of prototyping solutions to be submitted to end-user 
during the different stages of the design process. In this context, the present pa-
per describes a structured User-Centered Design (UCD) approach to develop 
usable control devices. It exploits advanced Tangible Augmented Reality 
(TAR) technique to represent the achieved design solution and perform usabili-
ty testing without increasing development time and costs. Experimental results 
prove that such technology sensibility increases timesaving compared with tra-
ditional prototyping approaches and demonstrate its reliability to detect usabili-
ty problems. 
Keywords: Smart Home, Virtual Prototyping, Tangible Augmented Reality, 
Human-Computer Interaction. 
1 
Introduction 
A smart home environment can be defined as "a dwelling incorporating a communica-
tion network which connects the key electrical appliances and services, and allows 
them to be remotely controlled, monitored or accessed" [1]. In the context of energy 
efficiency, emerging smart grid technologies have been applied to reduce the energy 
consumption of electric devices installed at home, to seek out the lowest rates, and 
contribute to the smooth and efficient functioning of the electric grid. Although most 
of them are commercially available, the limited service scalability, the complexity of 
configuration and the low usability prevent their mass adoption [2]. Rashidi and Cook 
[3] have demonstrated that many of these technologies are brittle and do not adapt to 
the user's explicit and implicit wishes. It has been demonstrated that the success of a 
management system for home energy efficiency is mostly determined by its ability to 
motivate users to adopt it in everyday life [4]. As motivation passes through the usa-
bility and acceptability of home automation control devices, they both become key 
requirements in device design. A computer-based interface is then necessary to prop-
erly display information and manage the system.  
The increased need of accessible and usable human-computer interfaces has trig-
gered research to develop structured methods and related tools to evaluate their utility 

916 
S. Ceccacci, M. Germani, and M. Mengoni 
 
and usability. Two main approaches are currently used: empirical approaches (i.e. test 
methods) and analytical approaches (i.e. inspection methods) [5]. In analytical evalua-
tion, the testing process only involves expert analytics performing the assessment 
with the help of some well-known theoretical methods (e.g. heuristic evaluation, cog-
nitive walkthrough, predictive methods, etc.). Contrariwise, empirical approaches are 
user-focused. This means that a set of representative sample users is directly involved 
for examining and comparing alternative design solutions while experts conduct both 
qualitative and qualitative evaluations. According to ISO 13407 standard [6], User-
Centered Design (UCD) approach can be used for robust HMI development. In fact, 
involving users in the design process has been demonstrated to lead to more usable 
satisfying designs [7]. Notwithstanding these benefits, the problems of UCD imple-
mentation in real product design still remain costs, development time consumption 
and complexity of managing multidisciplinary teams to perform comprehensive anal-
ysis of user behaviors. In addition, the construction of high-fidelity interactive proto-
types to conduct empirical testing at the preliminary design stage is difficult to 
achieve in short time and at low cost. 
Over the last ten years Virtual Reality (VR)-based technologies have been intro-
duced to replace physical mock-ups with virtual ones to achieve time saving and  
reduced development cost. Some studies demonstrate how VR can be useful for usa-
bility testing [8]. Although traditional VR-based mock-ups provide a good visual 
fidelity, they lack in behavioral simulation and natural interaction. Consequently, 
Mixed Reality (MR) environments have drawn a lot of attention in the field of UCD 
as they combine real and virtual worlds in various proportions and present them as a 
unified whole. Within the MR framework, the Augmented Reality (AR) technique is 
one of the most adopted one due to the low cost of the technologies and to its ability 
to enhance the real scene with computer graphics and emerging tactile and sound 
rendering displays [9-10]. While there has been substantial research on the underlying 
technology, user experience and interaction techniques are poorly explored.  
In this context, the present paper describes a structured UCD approach to design 
highly usable control devices dedicated to manage the functionalities of smart grid 
platforms for home automation. The focus of the study is on the development of the 
graphical user interface (GUI) dedicated to desktop PCs, smart phones and other per-
sonal digital assistant devices generally used as preferred tools for software platform 
access and control. To improve the efficiency of the proposed UCD approach, Tangi-
ble Augmented Reality (TAR) techniques are exploited to virtually prototype the 
conceived design solutions and carry out usability testing with sample users. Experi-
mental results show that designers do not require extra work to build TAR prototypes 
or modify solutions to meet users' explicit and implicit needs. TAR sensibility in-
creases timesaving compared with traditional prototyping approaches.  
The proposed approach and TAR prototyping technique are adopted in a particular 
case study that is an innovative domestic smart grid platform, called Homeline, to 
monitor and manage energy at home consumptions. The research partner is Indesit 
Company S.p.a, World leader manufacturer in household appliances, that developed 
the Homeline platform.  

 
A Method to Design a Smart Home Interface 
917 
 
2 
The Method 
A UCD approach is adopted to design the Homeline interface for a large range of 
users with medium-level skill and expertise in using web-based applications.  
After design requirements are set, the design stage starts with the definition of the 
system workflow and the implementing layout. Software designers, graphic designers, 
psychologists, marketing experts and usability experts are involved in a multidiscipli-
nary team for brainstorming. Storyboarding is used to realize and assess various inter-
face layout options. Once ended Homeline ideation, interface design starts. At this 
stage two different prototyping techniques are used to support design evaluation and 
performance comparison is made possible. 
Firstly, a traditional technique based on the use of throwaway low-fidelity proto-
types (built in Adobe Flash CS5) that allow experts to assess alternative design  
solutions at the early stages and a high-fidelity prototype to subsequently perform 
usability testing. In this case the high-fidelity prototype is developed in the same pro-
gramming language of the final software (Silverlight 4 and C#). In both stages the 
interface is visualized on a desktop computer. The visualization on alternative dis-
plays requires additional software development efforts; 
Secondly, an innovative technique that exploits MR technology to build low-cost 
disposable prototypes able to simulate the functionality of the interface in the same 
way as high-fidelity prototypes do and to simultaneously enable users to manipulate 
real world objects where the graphic interface is displayed (e.g. desktop computer, 
personal digital assistant device, portable tablet). The use of TAR prototypes allows 
the easily representation of system logics and graphics in an intuitive and interactive 
way.  
Five experts in HCI are involved in preliminary testing to assess the usability of the 
Homeline interface. Two additional experts support test moderation and data collec-
tion. Tests are carried out both on the low-fidelity prototype and on the first TAR 
prototype. Experts assess usability according to Nielsen and Mohlic's heuristics [11] 
and then define a set of guidelines to improve the GUI. They do not find significant 
differences in interaction quality in case of traditional and virtual prototypes. Rec-
ommendations are followed both to build the high-fidelity prototype and to modify 
the preliminary TAR prototype to provide reliable prototypes. 
Finally, usability testing is conducted by involving end-users. Tests are performed 
both on high-fidelity prototype and TAR prototype. Two samples of users, each  
composed by 8 typical users of the Homeline system, are involved: this number was 
chosen because Nielsen (1993) suggests that least eight users have to be involved to 
detect usability problems. Both samples presented homogeneous features:  aged 30-
35, 3 males and 5 females, computer and internet skilled.  
The first sample is asked to carry out the usability test with the high-fidelity proto-
type, whereas the other one is asked to assess the usability of the TAR interface. In 
both cases, users have to perform a series of tasks and think aloud while a moderator 
takes note of any difficulties they encounter. Two experts in HCI are involved in the 
test, one to moderate and one to note problems.  

918 
S. Ceccacci, M. Germani, and M. Mengoni 
 
Table 1 shows the experimental protocol used. It is based on ISO 9241-11 guide-
lines. Different observation techniques are used to collect data during testing: Video 
Interaction Analysis (VIA), direct observation and questionnaires. VIA allows hu-
man-computer interaction to be captured by recording user behaviour, words, gestures 
and facial expressions. Objective data such as task completion time, number of re-
quests of assistance and clarification and errors is registered. The eye-tracking system 
is used to analyse eye movements and identify the GUI areas where user attention is 
focused during task performance. User satisfaction is explored through a question-
naire. Users are asked to express a judgement according to the 5-point Likert scale to 
a pre-defined set of questions. 
At the end of the design process the Homeline GUI interface is developed both for 
desktop and handheld devices. 
Table 1. The experimental protocol for usability test 
Usability 
dimensions 
Evaluation metrics 
Units 
Investigation  
techniques 
Effectiveness 
Completion rate without assistance 
% 
VIA, 
direct observation 
Completion rate with assistance 
% 
Assistances occurrence 
number 
Error number  
number 
Efficiency 
Time to complete Task  
time (s) 
Satisfaction 
Ease to use 
1-5 
judgment 
Questionnaire 
Understandability 
Utility 
Sense of order 
Pleasantness of the graphics 
Global satisfaction 
3 
TAR Prototype to Support User Testing 
The Virtual Prototyping (VP) technique supporting both analytical and empirical 
study aims to involve multiple sensory channels, mainly touch and to simultaneously 
provide an interactive GUI without the effort of developing the final software before 
design has finished. It exploits both Tangible User Interfaces (TUIs) and AR tech-
niques to allow users to interact with digital contents by manipulating real objects 
representing the physical mean of interaction. 
TUIs describe physical objects able to translate user actions into input events in the 
computer interface. They make virtual objects accessible to the user through physical 
proxies by implementing what Shniderman [12] called “direct manipulation”, which 
can be seen as taxonomy of TUI. To fill the gap between virtual and real, AR tech-
nique is used. The final result is a Tangible Augmented Reality (TAR) prototype. It 
provides an immediate relationship between the virtual model, representing the GUI 
and the aesthetic appearance of the device where the GUI is displayed, and the physi-
cal object (a mock-up realized by rapid prototyping techniques without any surface 
finishing or aesthetic features) on which the first is projected.  

 
A Method to Design a Smart Home Interface 
919 
 
To be interactive and usable TAR prototype (Figure 1) requires the realization of 
three key elements [13]: the physical elements of the system, the visual display ele-
ments and the interaction metaphor mapping the user-object interaction in the real 
world to virtual object manipulation.  
The physical elements of the system comprehend: 
• the product interface physical prototype that is equipped with a set of removable 
supports both for an AR id-encoded marker to calculate the camera position and 
orientation in real time, and for reflective markers to detect the object position dur-
ing user manipulation. These components are first modeled by a CAD system and 
then prototyped by a 3D printer (ZPrinter 450 by Z Corporation). 
• a wearable glove with four reflective markers to detect the hand position in real 
time (Figure 2). The fourth marker is mounted on a special ring slipped onto the 
user's forefinger. The glove is made of latex to ensure maximum fit. It is created to 
keep all user fingers uncovered to enable the sense of touch. This allows the 
achievement of a comfortable barrier free glove and an increasing naturalness of 
manipulation to minimize the user mental load during interaction. 
• a Head-Mounted Display (HMD) (i.e. Wrap920AR eyewear by Vuzix), which has 
an optic display in front of each eye. The real world is captured by its stereo cam-
era system and displayed in standard 2D or 3D in the eyewear. Computer-
generated data is overlaid to the real image thanks to the use of AR markers.  
The visual display elements concern the type of GUI representation and the way to 
realize the logical connections implementing the whole system workflow. In the de-
veloped application, the ARToolKitPlus plug-in for Virtools is used to show a virtual 
screen model overlaid on a user view of the physical prototype. The custom software 
updates the current screen in real time while the user finger interacts with the physical 
display selecting a specific area corresponding to a GUI control. Screens are realized 
by changing the virtual display “material” that consists of the interface graphics and 
the virtual prototype rendering. The graphics are created in Adobe Photoshop CS5.  
The interaction technique is based on the metaphor of a real touch screen. The user 
accesses a new site page or interactive menus by “tapping” on these areas. Touch is 
simulated by the user’s forefinger staying on a sensitive area of the display prototype 
for 1 second. This temporal interval allows the software engine to discern between a 
fingertip shift without pressing and a functionality selection and represent a compro-
mise between the prototype intuitiveness and its functionality. The interaction is com-
puted by calculating the mutual positions between the user finger and the display area 
detected by it.  
An optical tracking system (Optitrack by Natural Point, equipped with 6 IR cam-
eras V110:R2 and the Tracking Tools software granting 6DoF rigid object tracking) is 
used to capture relative positions. The “touch sensitive” areas for each simulation 
screen are defined by using virtual layers.  
The software system used to manage the software application is 3DVIA Virtools 
by Dessault Systems, a development platform for VR applications. 
TAR actually allows true spatial registration and presentation of both 3D virtual 
objects and contents in the physical environment. The involvement of a haptic  

920 
S. Ceccacci, M. Germani, and M. Mengoni 
 
feedback and the creation of continuity between the interaction space and the display 
space lower the mental load due to the fact that the user is able to directly experience 
the effects of their actions on the object. A Head-Mounted Display (HMD) is adopted 
to keep the user viewpoint focused on the GUI projected on the object they are hold-
ing in their hands. 
 
Fig. 1. How to build TAR prototype (a). The glove and the IR-marker holder ring (b). 
4 
The Experimental Case Study  
Usability tests are first carried out on the traditional high fidelity prototype. They 
involve 8 sample users (5 women and 3 men), aged between 35 and 40 years. They 
currently use computer applications and navigate Internet sites. The laboratory is 
equipped with an HW workstation with Internet connection and a VCR camera and 
microphones for VIA. Product usability is evaluated by task analysis (Table 2) fol-
lowing the above-mentioned experimental protocol (Table1).  
Table 2. Task description 
 
Task description 
1 
Imagine that this is the situation in your home at this time. What is the total consumption in your 
home? 
2 
What appliances are running at the moment and what they are consuming? 
3 
Check out the consumption of your home last week.  
4 
Check the trend of consumption in the last hours of your oven.  
5 
Check how much it costs to start a wash at this time and how much it would cost at 23:00. The data 
set: wash "Cotton White", temperature 60°C, centrifuge at 800 rpm and extra rinse option.  
6 
Find tips on how to prepare the chicken. 
7 
You would like to have the laundry washed and chicken cooked  by tonight at 20:30. You have to 
simulate various solutions time to run appliances and to examine how the costs vary. Set data:  
OVEN: traditional cooking at 180°C for 60 min., WASHING MACHINE: coloured cotton at 30°C 
with 1000 rpm spin. 
8 
Your washing machine has problems in the centrifuge. Ask for technical assistance. 

 
A Method to Design a Smart Home Interface 
921 
 
Before starting the test, all users are informed of the testing purpose. Then, the 
Homeline home page is shown to the users and they are asked to answer a question-
naire to assess their first impression (i.e. before use satisfaction) (Table 3).  
Table 3. The satisfaction questionnaire 
 
Evaluation metrics 
Questions (1-5 Likert judgement) 
A 
Ease to use 
The website is very ease to use 
B 
Understandability 
Information provided by the website are very ease to understand 
C 
Utility 
The information provided by the website are very useful 
D 
Sense of order 
The information inside the website are very well organized 
E 
Pleasantness of the graphics 
You find very beautiful the look and feel of the website  
F 
Global satisfaction 
Overall, you are satisfied with the website 
 
At the end of the test, the same questionnaire is newly submitted to the users to as-
sess the quality in use (i.e. satisfaction in use). For each task performance data about 
percentage of task completion, completion time, number of errors and clarification 
requests are collected. The achieved results show that the Homeline interface has 
serious usability problems. In particular, task success is low for tasks 3, 6 and 7  
(Figure 2).  
 
Fig. 2. Usability result with high-fidelity prototype: judgment about the average user satisfac-
tion metrics before and after use (a); task success (b) 
The percentage is higher in the case of very simple tasks. The measured low usa-
bility strongly influences users' opinions about the system. Satisfaction ratings pre and 
after-use are low (Figure 2). In particular, the average judgment expressed after use 
regarding ease of use, sense of order and understandability, is significantly lower than 
that expressed before use.  
Nevertheless, the average rating for system usefulness after use is higher than be-
fore use. This demonstrates that the system is able to motivate users and communicate 
the benefits offered by provided services. It was not possible to make an objective 
assessment of the efficiency of the system SW, based on a comparison of average 
time taken by users and the expert user time, because of large differences in page 
loading times between the various users. 
Once the TAR set-up has been arranged, usability testing is carried out with end-
users. The same protocol of Table 1 is adopted without any variation with respect to 
the traditional approach. The laboratory where the tests are carried out is equipped 

922 
S. Ceccacci, M. Germani, and M. Mengoni 
 
with: an eye-tracking system (Optitrack V110:R2 by Natural Point, equipped  with 6 
IR cameras and Tracking Tools for tracking rigid bodies, which allows 6DoF object 
tracking), a PC with the Optitrack Software and Virtools 4.0 by Dassault Systems and 
the developed plug-in applications, two gloves in different sizes (small and large) and 
a resizable ring with reflective markers, an head-mounted display iWear VR920 
HMD, and, finally, a digital camera and microphones for VIA. 
A second group of users with the same characteristics as the ones involved in the 
tests on the traditional high-fidelity prototype is involved. Before starting with the 
test, all users are informed of the testing purpose. A calibration stage is required to 
ensure that the optical tracking system captures the position of the user's hand and 
forefinger. Users are asked to wear the two gloves in succession to identify the one, 
which fits their hand size better. The position of this ring is continuously varied along 
the forefinger to set the right position of the virtual pointer. 
The experimental procedure is the same as the test with the traditional prototype. 
Both efficiency and effectiveness metrics are measured as well as satisfaction before 
and after use. Performance and satisfaction tables are fulfilled for each task and in-
volved user. The results of task success (Figure 3) reveal that the interface is not able 
to assist users in performing tasks 3, 6 and 7. This strongly influences the user's 
judgments after-use (i.e. ease to use and sense of order metrics). However, the global 
satisfaction is partially high due to the enthusiasm deriving from the use of an innova-
tive technology and the iPad-like interface. 
 
 
Fig. 3. Usability result with TAR prototype: judgment about the average user satisfaction me-
trics before and after use (a); task success (b) 
By analyzing the results, it is possible to infer that TAR interactivity guarantees the 
simulation of the final software behaviour still conceptual design. Comparison of 
Fig.2b and Fig.3b highlights that the technological set-up does not affect task success 
and satisfaction perceived before use. Although TAR enhances satisfaction in use it 
does not affect the trend of satisfaction perceived after use. Accordingly, results point 
out TAR effectiveness to create interactive prototypes for usability testing.   
In order to prove the efficiency of TAR techniques to carry out usability assess-
ment in place of high fidelity prototypes an additional evaluation is performed. It 
consists in the analysis of the effort to virtually prototype alternative solutions and to 
set the proper experimental environment.  

 
A Method to Design a Smart Home Interface 
923 
 
Table 4 reports the time spent for design the system by following both approaches. 
This achievement is fundamental to increase the use of these technologies in industry. 
Total time comparison shows that TAR sensibly reduces prototyping time. 
Table 4. Comparison between traditional UCD and TAR based approaches 
 
An additional analysis is carried out to evaluate the perceived interactivity of TAR. 
Users are asked to answer a post-hoc questionnaire by expressing a judgement ac-
cording to the 1-5 Likert scale of 1-5 (Figure 4).  
 
Fig. 4. Post-hoc questionnaire and results 
Each question is correlated to evaluation metrics which is significant to assess the 
exploited technology performance. By analyzing post-hoc questionnaire results  
(Figure 4) it is possible to infer that users find the interaction with the TAR prototype 
intuitive (average intuitiveness judgement = 4.2). The exploited technology enables 
users to interact with the prototype in a realistic way and natural way (average judge-
ment for interactivity, natural involvement and sense of realism respectively are: 3.9, 
4.7 and 4.2). However, most users need to customize the display mode. In fact, even if 
 
Traditional UCD approach 
TAR based approach 
5 days 
Brainstorming and storyboarding 
5 days 
30 days 
Construction of a low fidelity  proto-
type using Adobe Flash CS5 
TAR prototype construction 
10 days 
2 days 
Tests with experts 
2 days 
40 days 
Construction of a high fidelity proto-
type using Silverlight 4 and C# ac-
cording to new design solution 
Edit the TAR prototype accord-
ing to new design solutions 
2 days 
5 days 
Task analysis with users 
5 days 
15 days 
Development of final SW  
Development of final SW 
40 days 
97 days 
TOTAL TIME 
64 days 

924 
S. Ceccacci, M. Germani, and M. Mengoni 
 
the focus point of the lens video cameras mounted on head-mounted display can be 
changed, this does not overcome the need for lenses in case of high dioptres. Free 
movement is still an open issue. 
5 
Conclusion 
The proposed TAR interaction approach to control Smart Home energy consumptions 
actually represents an important challenge in UCD. TUIs are combined with AR dis-
play methods to create a very intuitive and natural application, which allows users to 
physically interact with a product and explore virtual contents in real time. Its value 
has been investigated through rigorous user studies, comparing ease of use with this 
style of interaction to other traditional methods. For this purpose, numerous analyses 
are carried out to demonstrate the efficiency and effectiveness of the proposed TAR 
prototype. The proven main benefits regard achieved timesaving (-34%) due to the 
ease to carry out design modifications without a significant HW/SW development 
effort. Future work will be focused on the collection of additional case studies in or-
der to support the experimental findings. Additional applications should be imple-
mented and tested. 
References 
1. Jiang, L., Liu, D.A., Yang, B.: Smart home research. In: Proceedings of the Third Interna-
tional Conference on Machine Learning and Cybernetics, Shanghai, August 26-29 (2004) 
2. Papadopoulos, N., Meliones, A., Economou, D., Karras, I., Liverezas, I.: A connected 
Home Platform and Development frame work for smart home control applications. In: 
Proceedings of the 7th IEEE Conference on Industrial Informatics, Cardiff, Wales, pp. 
402–409 (2009) 
3. Rashidi, P., Cook, D.J.: Keeping the resident in the loop: adapting the smart home to the 
user. IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Hu-
mans 39(5), 949–959 (2009) 
4. Wood, G., Newborough, M.: Energy-use information transfer for intelligent homes: Enabl-
ing energy conservation with central and local displays. Energy and Buildings 39, 495–503 
(2007) 
5. Chikhaoui, B., Pigot, H.: Towards analytical evaluation of human machine interfaces de-
veloped in the context of smart homes. Interacting with Computers 22, 449–464 (2010) 
6. ISO 13407:1999 - Human-centred design processes for interactive systems 
7. Abras, C., Maloney-Krichmar, D., Preece, J.: User-centered design. In: Bainbridge, W.S. 
(ed.) Berkshire Encyclopedia of Human-Computer Interaction. Berkshire Publishing 
Group, Great Barrington (2004) 
8. Kuutti, K., Battarbee, K., Sade, S., Mattelmaki, T., Keinonen, T., Teirikko, T., Tornberg, 
A.M.: Virtual prototypes in usability testing. In: Proceedings of the 34th Hawaii Interna-
tional Conference on System Sciences (2001) 
9. Seichter, H., Kvan, T.: Tangible interfaces in design computing. Virtual Environment 2 
(2004) 
10. Park, H., Moon, H.C., Lee, J.Y.: Tangible augmented prototyping of digital handheld 
products. Computers in Industry 60, 114–125 (2009) 

 
A Method to Design a Smart Home Interface 
925 
 
11. Nielsen, J., Molich, R.: Heuristic evaluation of user interfaces. In: Proceedings of the 
SIGCHI Conference on Human Factors in Computing Systems, pp. 249–256. ACM, New 
York (1993) 
12. Shneiderman, B.: The future of interactive systems and the emergence of direct manipula-
tion. University of Marylan, College Park (1982) 
13. Billinghurst, M., Grasset, R., Seichter, H.: Tangible Interfaces for Ambient Augmented 
Reality Applications. In: Aghajan, H., Augusto, J.C., Delgado, R. (eds.) Human-Centric 
Interfaces for Ambient Intelligence. Elsevier, New York (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 927–936. 
DOI: 10.1007/978-3-642-30817-8_91 
© Springer-Verlag Berlin Heidelberg 2013 
 
Virtual Reality Coupled with Adapted Physical Interface 
for a Better Evaluation  
of the Innovative Surgical Instrument 
Duy Minh Phan Nguyen1, Jérôme Tonetti2, and Guillaume Thomann1 
1 G-SCOP Laboratory, Grenoble Institute of Technology, France  
2 Orthopaedic and Traumatology Centre, Michallon Hospital, Grenoble, France  
Nguyen-Duy-Minh.Phan@gi-etu.grenoble-inp.fr, 
guillaume.thomann@g-scop.inpg.fr, 
JTonetti@chu-grenoble.fr 
Abstract. In the domain of designing innovative products in the medical field, 
investigations are often oriented towards communication between actors and 
needs comprehension. In the DESTIN (DEsign of Surgical/Technological  
INnovation) project, User Centered Design methodology with concrete experi-
ments is applied. Researchers propose emulations in an operating room for  
co-evaluation of innovative products and new adapted surgical procedures. In 
this paper, they intend to evaluate the usage of the product in a virtual environ-
ment using a 3D haptic feedback system. Researchers not only propose a better 
ergonomic situation of the physician in front of the operating screen, but also 
increase the performance of the simulator in order to allow the manipulation of 
the innovative surgical instrument developed. We used virtual reality environ-
ment and the manufactured prototype with the aim to validate the new surgical 
procedure and the innovative designed surgical instrument. 
Keywords: Virtual Reality, 3D-Haptic feedback system, Prototyping, User 
Centered Design (UCD), Minimally Invasive Surgery. 
1 
Introduction 
The development of new technologies in medicine can significantly improve the ef-
fectiveness. On the contrary, the use of more complex systems tends to make the 
practice of medicine more difficult. In particular, this complexity reinforces the im-
portance of preoperative planning and postoperative monitoring. New technologies in 
informatics and virtual reality allow physicians to better interpret the enormous 
amount of information that is provided by the imaging systems or therapy systems 
[1]. Specifically, virtual reality allows better understanding, better planning and better 
work through visualization of three-dimensional images of anatomy and pathology. In 
addition, virtual reality supports the practitioner through the stages of diagnosis, ther-
apy, and postoperative monitoring. 
The main aim of DESTIN project (Design of Surgical-Technological INnovation) 
is to propose a new methodology focused on this specific context: How to create a 

928 
D.M.P. Nguyen, J. Tonetti, and G. Thomann 
new operative surgical procedure and innovative surgical instrument when a new 
medical approach is imagined? 
The specific surgical application we are working on addresses thoraco-lumbar frac-
ture. The current "classical" procedure is carried out with the patient in the prone 
position under general anesthesia. The surgeon performs a posterior open approach 
through a 15cm large incision. The posterior vertebral arch is exposed. Pedicle screw 
entry points are chosen by direct visual control and they are fixed to the vertebrae. 
Rods are placed to connect the pedicle screws together. Prone placement added with 
rod-screw connection provides reduction of the trauma deformity and durable stability 
Thus, vertebrae are preventing from moving while bone healing and graft fusion takes 
place. 
The new surgical procedure proposed by the surgeon consists in inserted the rod 
inside the pedicular screw in MIS (Minimally-Invasive Surgery). Thus, new little 
incisions should allow the insertion of the rod in the three pedicle screws.  
Currently, researchers, designers and the medical staff regularly work in the real 
operating room. This work was very effective but necessitates heavy organization and 
management (mainly in the hospital), the creation of mannequins, the manufacturing 
of many prototypes, etc. 
To facilitate this procedure, we create a CATIA CAD model of the virtual operat-
ing room. It integrates patient, medical equipment and surgical instruments. In this 
virtual environment, the surgeon has to manipulate the virtual surgical instrument on 
the virtual patient's spine (the spine has been modeled in a compatible format as the 
CATIA environment and integrated in a mannequin placed on the operative table). 
The goal of this exercise is to provide information to the designers for the validation 
of the innovative surgical instruments during the design process. At the same time, it 
also allows surgeons to perform the operative procedures with haptic feedback as in 
the real operative case. 
The difficulty in our research concerns the ability to sufficiently represent the vir-
tual environment for the co-validation of the medical procedure and the innovative 
surgical instrument.  
Knowing that surgeons can already manipulate the virtual innovative surgical in-
strument using a 3D-Haption© haptic system in the virtual operating room, the re-
search questions can be summarized as follow: 
- 
How to modify the configuration of the virtual reality room and the physical in-
terface for a better integration of the physician in the virtual environment? 
- 
Which are the optimal dimensions of the virtual surgical instruments for a best 
representation and manipulation of the real surgical intervention using the 3D-
Haption© haptic system? 
To answer these questions, this research methodology is proposed:  
- 
To research some ergonomic references in the surgery domain and compare them 
to our virtual reality room organization, 
- 
Secondly to design and to link a new physical interface to the arm of the 3D-
Haption© haptic system, 
- 
To modify the virtual model and to compare the surgical intervention feasibility 
with the real one. 

 
Virtual Reality Coupled with Adapted Physical Interface 
929 
In this article, we firstly present the User Centered Design methodology we use dur-
ing our study. Then we focus on the virtual reality and ergonomic applications and 
research in the surgical field. This first step allows us to analyze the general situation 
in the world. From this work, we propose modifications and adaptations of our current 
virtual operating room and 3D-Haption© haptic system user interface. 
Next, we present the results of the manipulation in virtual environment and conclu-
sions concerning its efficiency related to the situation in real situation. 
2 
User Centered Design 
User Centred Design (UCD) is considered as one of the cornerstones theories about 
user involvement. UCD, as a design approach, was first time introduced in NF EN 
ISO 9241-210: Human-Centred Design Processes for Interactive Systems [2]. The 
main issue is how to involve, integrate and consider the end-user and its requirements 
throughout the product design process. This ISO 13407 model proposes technical 
points the project must encompass to be considered as human centred: 1 – a certain 
knowledge of the end-users: their tasks and of their environment – 2 – an active par-
ticipation of these end-users, the clear understanding of their needs and the require-
ments linked with the tasks – 3 – an appropriate distribution of the end-
users/technological functions – 4 – an iterative design solution – 5 – the intervention 
of a multidisciplinary designing team. This is necessary to better interpret the end-
user, its knowledge and how-know: human factors, information architecture, design, 
quality, marketing, etc. 
The UCD cycle is decomposed into six main steps (Figure 1). It is an iterative 
cycle (step 2 to 5) which ends when the system answers the end-user requirement 
(step 6). 
 
 
Fig. 1. The six steps of the UCD cycle 

930 
D.M.P. Nguyen, J. Tonetti, and G. Thomann 
To better understand this UCD design steps, Jokela et al. propose another interpre-
tation of this NF EN ISO 9241-210 UCD Process. They explain more concretely how 
it can be applied on a project and suggest a new UCD process model [3]. Another 
important issue in UCD is how identifying and selecting relevant end-users in the 
development work. In practice it is commonly possible to involve only a limited 
number of users, and therefore it is very important to define criteria in order to select 
the most “representative users” to centre the design on their requirements and  
expectations. 
3 
Virtual Reality, Ergonomics and Their Application  
in Surgical Field 
3.1 
Virtual Reality and Application to the Surgical Field 
Virtual Reality (VR) is an interactive immersive data-processing simulation in real or 
imaginary environments. Currently, the technology of RV was applied in many dif-
ferent fields such as: formation by simulator (driving vehicles, aerospace), design of 
products, the simulation of surgery, meteorology… 
In the surgical field, the laparoscopy is a new procedure which requires surgeons to 
observe the operations on a monitor and requires acquisitions of new competences. 
This Minimally Invasive Surgery (MIS) differ from the opened surgery by the fact 
that the surgeon operates through small incisions and uses specific instruments as 
scalpel, grips, nets, etc. [4]. In spite of its many advantages (faster recovery of the 
patients, less damage with healthy tissues and smaller scars, less pain and less need 
for drugs), the MIS requires a long time training eyes-hands coordination. 
Researches developing the haptic control feedback device can be found in [5-7]. 
To follow the user intentional movements, by interaction between hand and device, 
high powerful haptic devices must be able to produce force feedback. Consequently, 
it is essential to closely examine the human touched and the constraints of application 
during the construction of these devices. A haptic interface with 4 degrees of freedom 
was designed by Guiatni et al. to compare it with devices commercially available [4]. 
This device has the capacity to offer force feedback in all the degrees of freedom 
available during the MIS procedure. 
In our case, researchers, designers and physicians work together on the develop-
ment of a virtual environment to simulate a MIS operation on the spinal column. The 
goal is to create an integral virtual surgical environment with surgical instruments and 
haptic feebdack in a model of the operating room. 
3.2 
Ergonomics in the Surgical Field 
Ergonomics is a relatively new science. It is based on the models in the design of 
machines and tools that optimize the performance of users. In order to improve the 
simulation conditions in virtual reality surgery environment  that create the better 
immersion for surgeons who will eventually be the primary user of this innovative 
surgical instrument, we had to first study some articles to  find the factors that influ-
enced the comfort of surgeons during operations in the operating room. The optimum 

 
Virtual Reality Coupled with Adapted Physical Interface 
931 
ergonomic position of the monitor was defined according to various sources in the 
literature [8], [9], [10], [11]. The monitor was at a distance of 0.6 m apart from the 
subjects’ eyes. The monitor height (from the middle of the screen to the ground) was 
between the operating surface and eyelevel height, and the monitor was inclined (to a 
maximum of 15°) as by the subjects.  
The optimal operating surface height was 80% of the elbow height and the table 
was positioned in 20° tilt. [12]. 
In an article submitted by Gurvinder Kaur [13], he conducted a test to find the 
height of the ergonomics table in the minimally invasive surgery. This study aims to 
propose an ergonomic table height required for the surgeon height so they can per-
form comfortably in the operation. In this study, the height of the table has an effect 
on the upper joint movements of the shoulders, arms and wrist during laparoscopy. 
Table height should vary from 65 to 90 cm from the floor. The surgeon should be able 
to adjust the table corresponding to his/her height in order to bring upper joint move-
ments to the minimum position with the resultant less discomfort in the shoulder, 
back elbow and the wrist. After analyzing the ratio between the surgeon’s height with 
the height of the operating table, it was assumed that the height of the operating table 
should be calculated as follows:  
Table Height = Surgeon’s Height X 0.49 
The ideal posture for the MIS is supposed in the literature [14] and [15]. The arms are 
slightly removed, retroversion, and turned inward at the level of the shoulder (abduc-
tion <30 °). The elbows are bent at about 90-120° of flexion. This position leads to the 
maximum force to be applied for a maximum duration. The head is slightly bent with 
an angle of between 15 and 45°. 
Through this study literature, we find that the virtual reality technology plays an 
important role in many areas. In particular, the applications of VR technique in sur-
gical simulation have been developed to provide better and better ergonomic solutions 
which satisfy users. Through these studies, we can better consider the virtual reality 
room and design the components that give a better immersion for the surgeons. Thus, 
we can improve the ergonomics in surgical simulation by changing haptic interface, 
the position of the surgeon and his posture. 
4 
Related Works 
4.1 
The Surgeon Posture 
To perform their operations with haptic sensation as in real surgical environment, we 
adapted and modified the haptic interfaces as well as the position of surgeon: 
- 
Setting the table height corresponding to the surgeon’s height. We chose table 
height is equal to 0.49 of surgeon‘s height. [14]. Notice that it is now possible to 
adjust at real time the position of table? 
- 
Adjusting the distance between the screen and the surgeon’s position. Normally, 
this distance is 0.60 m, but with the giant screen in Virtual Reality room at our la-
boratory; we chose the distance of 1.5 m. 
- 
Changing position as well as the posture of the surgeon: the surgery is always in 
front of the screen, the direction of movement of the tool should be parallel with 
the spine. 

932 
D.M.P. Nguyen, J. Tonetti, and G. Thomann 
4.2 
The New Human Machine Interface 
The practitioner manipulated the haptic arm using the 3D-prining machine handle 
(figure 2). The position of the physician was not comparable to the real operating 
room environment and the conditions of experimentation not ideal: 
- 
The surgeon was not in front of the screen and the posture position not comforta-
ble. 
- 
The 3D-printing handle material was different than the final product’s one. 
Moreover, the idea is to use the same surgical instrument on mannequin in operating 
room and during the simulation. 
 
 
Fig. 2. Picture of the 3D-prining machine handle at the end of the haptic arm 
Through ergonomics studies, we can better consider the virtual reality room and 
design the components that can make a better immersion of the practitioner. Thus, we 
can improve ergonomics in surgery simulation by changing haptic interfaces to adapt 
with the position and posture of the surgeon. In order that the surgeon can use the 
haptic arm in the operating simulation as in reality, we thought to create an interme-
diate mechanical piece to hang the surgical instrument prototype (Protige) at the end 
of the haptic arm. The objectives of this adaptation are to give the surgeon a real sen-
sation when holding the real instrument Protige and then to carry it in a direction pa-
rallel to the spine’s main axis. 
Before the mechanic piece was fabricated, we carried out a numerical simulation to 
ensure the strength, deformation and constraints of the piece to work properly when it 
tightened the surgical instrument .We divided the simulation into two cases: 
- 
Test the strength of the piece under the Protige’s effort when the simulator is run-
ning maximum the instrument. 
- 
Test the tightness of the piece under the load of the screws so it could tight well 
Protige. 
The intermediate piece of aluminum has been made at our workshop by the Numeri-
cal Control of Machine Tools (figure 3). We observe the 90° modification orientation 
compared to the previous 3D-prining machine handle (figure 2) 

 
Virtual Reality Coupled with Adapted Physical Interface 
933 
 
Fig. 3. The instrument tightened by the piece is tied to the haptic arm 
4.3 
Testing the Haptic Sensation in the Insertion of the Rod with Screw Holes 
Concerning the simulations, the previous virtual surgical instruments dimensions 
never allows the insertion of the rod inside the pedicle screws head. The main objec-
tive of this test is to find optimal dimensions of the virtual surgical instruments and 
verify the friction sensation when inserting the rod into the holes of the pedicle screws 
head. Using a simplified virtual model, we test different manipulation situations (fig-
ure 4): changing screws holes diameters and rod diameters. We modified the diameter 
of the pedicle screw hole from 6 to 9mm. We also used different rod diameters: from 
4 to 6mm. 
To simplify the simulation with many different cases, we proposed a simple model 
of vertebral column and screws. Of course, the positional parameters between the 
screws and spine are respected as the MIS procedure. The new versions of models 
vertebral column and the screws are created using CATIA software. 
 
Fig. 4. The translation of Protige parallel with the spine position 
 

934 
D.M.P. Nguyen, J. Tonetti, and G. Thomann 
We ran several simulations with different views ("multiple views" on IFC CATIA 
software). So we set up a kind of viewpoints allowing the surgeon to possess more 
isometric views, each vignette characterizing a different spatial view. 
User moves the virtual instrument over the spine and we set the test duration up to 
one minute to validate the feasibility of the procedure phase. Five trials are conducted 
for each case. The experimental duration for each case were taken in order to deter-
mine the levels of difficulty in inserting the rod into the holes. A test was considered 
successful if the positioning time of the rod through the holes did not exceed one 
minute. 
We got the test results with different cases (see an extraction of the results in table 
1). Simulation is recorded by video to analyze the results as well as confirmation of 
results. 
Table 1. Results of the trials (in duration time)  
Rod diameter
(mm) 
Duration of the trials (s) 
Screws’ hole: 8 mm 
Mean 
(s) 
Duration of the trials (s), 
Screws’ hole: 9 mm 
Mean 
(s) 
4 
8 
7 
6 
4 
10 
7 
12 
8 
7 
4 
7 
7,6 
4,5 
60 
50 
120 
90 
60 
76 
5 
7 
8 
8 
9 
7,4 
5 
 
 
 
 
 
 
5 
8 
9 
8 
7 
7,4 
5,5 
 
 
 
 
 
 
10 
8 
8 
11 
9 
9,2 
 
We have tested a maximum of the possible experimental conditions. Depending of 
the trauma cases, the physician has to use two or more screws inside the body. We 
asked the user to test the virtual insertion of the rod in 1, 2 and 3 screws. An extrac-
tion of the complete results is presented in table 2. 
Table 2. Conclusion for one specific configuration: extraction of the complete table. Hole of 
the screws: 8mm / diameter of the rod 4 and 4,5 mm 
Rod diameter 
(mm) 
Number of screws 
Duration of the trials (s) 
1 
2 
3 
1 
2 
3 
4 
5 
Mean (s) 
4 
OK 
OK 
OK 
8 
7 
6 
4 
10 
7 
4.5 
diff. 
imp. 
imp. 
60 
90 
120 
50 
60 
76 
 
“OK” means that the corresponding experiment is working well. For example, in-
serting a 4 mm diameter rod through 3 pedicle screws’ holes of 8mm takes less than 1 
minute. Inserting a 4.5mm diameter rod through 1 pedicle screws’ hole of 8mm takes 
around 1 minute. We qualified this situation as difficult (diff.). Finally, it is impossi-
ble for the user to insert the 4.5mm diameter rod through 2 or more pedicle screw 
holes of 8mm. 
 

 
Virtual Reality Coupled with Adapted Physical Interface 
935 
The complete experiment shows that the 9mm pedicle screws’ holes always allow 
the insertion of the rod from 4mm to 5.5mm. For the 8mm pedicle screws’ holes, they 
are compatible with the 4mm rod diameter (insertion through 3 screws) and 4.5mm 
rod diameter (insertion through 1 screw). One of the reasons that prevent this inser-
tion is the precision of the collision detection between parts using the IFC CATIA 
software coupled with the haptic device. It doesn’t allow the relative movements be-
tween rod and holes even if the rod’s diameter is smaller than screws’ holes. Moveov-
er, the durations of the trials depend of the user’s experience. 
5 
Conclusion 
In this study, we not only propose a better ergonomic situation of the physician in 
front of the operating screen, but also increase the performance of the simulator in 
order to allow the manipulation of the real innovative surgical instrument developed. 
We used virtual reality environment and the manufactured prototype with the aim 
to validate the new surgical procedure and the innovative designed surgical instru-
ment. For that, an adaptation piece has been designed, manufactured and manipulated. 
This adaptation has really increased the real sensation of the user in front of the vir-
tual reality screen. 
Moreover, the disposition of the experimental room and the user has evolved. The 
modification of the model and the different trials with different users allow research-
ers to find parameters which influence the quality of physical sensation. This activity 
will allow  
- 
Designers to propose tools and models more realistic for effective simulations 
during the design process. In consequences, design choices can be more precise 
- 
Physicians to quickly evaluate and validate an adapted operative procedure. 
These experiments with users and researchers give us some qualitative results. The 
next step will be the evaluation of the complete virtual environment (with different 
dimensional models) with numerous expert surgeons to: 
- 
validate the design of the surgical instrument 
- 
quantify the sensations of the experts 
The surgical instruments developed are generally composed of multiple mobile parts. 
One of the future objectives will be to work on the possibility to manipulate all the 
parts of the product in virtual reality. This objective imposes the integration of mul-
tiple cameras and markers in the experimental room. 
References 
1. Melton, G.B.: Biomedical and health informatics for surgery. Advanced Surgery 44, 117–
130 (2010) 
2. NF EN ISO 9241-210: ‘Human-centred Design Processes for Interactive Systems’ Genève, 
Switzerland, International Organization for Standardization (January 2011) 

936 
D.M.P. Nguyen, J. Tonetti, and G. Thomann 
3. Jokela, T.: Making user-centred design common sense: striving for an unambiguous and 
communicative UCD process model. In: Proceedings of the Second Nordic Conference on 
Human Computer Interaction, Aarhus, Denmark. ACM Press (2002) 
4. Guiatni, M., Riboulet, V., Kheddar, A.: Design and Evaluation of a Haptic Interface for In-
teractive Simulation of Minimally-Invasive Surgeries. In: IEEE/ASME International Con-
ference on Advanced Intelligent Mechatronics, Suntec Convention and Exhibition Center, 
Singapore, July 14-17 (2009) 
5. Saupin, G., Duriez, C., Cotin, S.: Contact Model for Haptic Medical Simulations. In: Bel-
lo, F., Edwards, E. (eds.) ISBMS 2008. LNCS, vol. 5104, pp. 157–165. Springer, Heidel-
berg (2008) 
6. Zarrad, W., Poignet, P., Cortesão, R., Company, O.: Stability and Transparency Analysis 
of a Haptic Feedback Controller for Medical Applications. In: Proceedings of the 46th 
IEEE, Conference on Decision and Control, New Orleans, LA, USA, December 12-14 
(2007) 
7. Mizokamit, R., Abet, N., Kinoshitatt, Y., He, S.: Simulation of ICSI Procedure Using Vir-
tual Haptic Feedback Model. In: 2007 IEEE/ICME International Conference on Complex 
Medical Engineering (2007) 
8. Matern, U., Waller, P., Giebmeyer, C., Rückauer, K.D., Farthmann, E.H.: Ergonomics: re-
quirements for adjusting the height of laparoscopic operating tables. JSLS: Journal of the 
Society of Laparoendoscopic Surgeons 5, 7–12 (2001) 
9. Burgess-Limerick, R., Mon-Williams, M., Coppard, V.L.: Visual display height. Human 
Factors: The Journal of the Human Factors and Ergonomics Society 42(1), 140–150 (2000) 
10. Jaschinski, W., Heuer, H., Kylian, H.: Preferred position of visual displays relative to the 
eyes: a field study of visual strain and individual differences. Ergonomics 41(7), 1034–
1049 (1998) 
11. Turville, K.L., Psihogios, J.P., Ulmer, T.R., Mirka, G.A.: The effects of video display ter-
minal height on the operator: a comparison of the 15°and 40° recommendations. Applied 
Ergonomics 29(4), 239–246 (1998) 
12. Van Veelen, M.A., Kazemier, G., Koopman, J., Goossens, R.H., Miejer, D.W.: Assess-
ment of the ergonomically optimal operating surface height for laparoscopic surgery. Jour-
nal of Laparoendoscopic & Advanced Surgical Techniques 12(1), 47–52 (2002) 
13. Kaur, G.: Role of OT Table Height on the Task Performance of Minimal Access Surgery 
World. Journal of Laparoscopic Surgery 1(1), 49–55 (2008) 
14. Van Veelen, M.A.: Human-Product Interaction in Minimally Invasive Surgery: A Design 
Vision for Innovative Products, pp. 92–97. Delft University of Technology, Delft (2003) 
15. Matern, U., Waller, P.: Instruments for minimally invasive surgery. Surgical Endosco-
py 13(2), 174–182 (1999) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 937–945. 
DOI: 10.1007/978-3-642-30817-8_92 
© Springer-Verlag Berlin Heidelberg 2013 
 
Application of AR Technologies to Sheet Metal Forming 
in Shipbuilding 
Kohei Matsuo1, Uwe Rothenburg2, and Rainer Stark2,3 
1 Structure Engineering Department, National Maritime Research Institute, Japan 
2 Division of Virtual Production Creation, Fraunhofer IPK Berlin, Germany 
3 Chair of Industrial Information Technology, TU Berlin, Germany 
kohei@nmri.go.jp 
Abstract. This paper describes the application of AR (Augmented Reality) 
technologies to shipbuilding. General overview about AR technologies is intro-
duced at first, and its potential of utilization especially given in shipbuilding is 
discussed in this section. The paper introduces actual development of an AR 
system which is applied to sheet metal forming works in shipyards. Workers 
can get quantitative information about the geometry of the curved plate or the 
work procedure intuitively by looking the plate through a tablet PC to which 
this AR system is installed. The paper also covers technical problems and solu-
tions to put this AR system into practical use in a shipyard. Finally, the paper 
introduces some possible applications of AR technologies for an entire ship-
building process. 
Keywords: augmented reality, shipbuilding, sheet metal forming. 
1 
Introduction 
Manufacturing Enterprises are highly forced to improve their development and pro-
duction processes regarding time and quality. Digital supported methods are the sub-
stantial enabler to achieve improvements in this area [1]. Also Digital Manufacturing 
is getting to be familiar in a research field and a practical use in a shipbuilding indus-
try which is characterized as a small batch production, hand working-dependent and 
to be done under special condition that a ship not fully designated and planned before 
starting its production. Although various approaches can be considered to digital 
manufacturing for an entire shipbuilding process, key approach should be an effective 
connection to a 3D CAD model which is generated in a design stage. Some previous 
studies introduce examples to apply the 3D CAD model to digital manufacturing in a 
shipyard, and those are mainly related to a 3D model viewing in a shop floor, a con-
trol of NC machines or various robots, and production simulation [2]. Virtual reality 
technologies are a new way especially for production simulation by feeling in an im-
mersive environment [3-4]. 
On the other hand, even there exist above mentioned proposals to use 3D model to 
digital manufacturing, new demands are emerging about a further utilization of the 3D 
CAD models in the shop floor or a building yard. AR (Augmented reality) technolo-
gies would answer this need. By introducing this technology, there would be  

938 
K. Matsuo, U. Rothenburg, and R. Stark 
expectations not only to watch the 3D view directly on the real object, but also to new 
paradigms which change the way to use information around the shipyard. Examples 
of those new paradigms are concepts for 3D drawing sheet with AR or an interactive 
information exchange between the shop floor and the design division. These aspects 
are discussed later in this paper. 
Application of AR technologies to shipbuilding is not popular at present, and only 
few reports which are almost written by European researchers are found in interna-
tional journals or conference papers [5-6]. This study introduces the application of AR 
technologies to sheet metal forming works in shipbuilding, and this application of AR 
technologies to a hull fabrication would be a first attempt. Although implementation 
of applying 3D CAD to ship production has been slow in the industry mainly due to 
the large cost associated with developing the models, this new technology would 
promote ideal utilization of 3D information in shipbuilding. 
2 
Development of AR System for Sheet Metal Forming  
in Shipbuilding 
2.1 
Sheet Metal Forming in Shipyard and Proposed AR Application 
Surface of a ship consists of many pieces of a steel plate which has thickness of near-
ly 10 – 30 mm. Fore and aft part of the ship are usually dominated by elliptical sur-
face and hyperbolical surface, and those curved plates are formed to the designated 
shape by pressing and gas heating. This sheet metal forming can be regarded as a 
typical example of a skilled work operation in shipbuilding because of the following 
reasons: difficulty in imagining an accurate 3D form; difficulty in judging the geo-
metric forming procedure; difficulty in processing to the objective surface, especially 
in the gas heating process; and difficulty in confirming the shape by traditional way, 
as illustrated in Figure 1. 
To facilitate these complex tasks as described before, AR technologies would be ef-
ficient to support workers by providing intuitive information in real time and directly. 
Following AR application are proposed to get imagination and quantitative informa-
tion directly and intuitively: 
• Imagination of a shape and a work procedure; 
• Indication of the status of work procedure while execution; 
• Confirmation of the resulting shape without templates; 
To give an appropriate support to these applications several requirements have to be 
addressed and considered: 
• Provide information on large scale parts; 
• Provide effective visualization of context oriented information with situation 
awareness; 
• Maintain accuracy in terms of correctness of deformation of steel plate; 
• Provide a practical system by considering a particular circumstance of work condi-
tion in shipyards; 

 
Application of AR Technologies to Sheet Metal Forming in Shipbuilding 
939 
 
Fig. 1. Sheet metal forming in shipbuilding and its difficulties 
2.2 
Development of the AR Application for Sheet Metal Forming 
The main characteristic of these application scenarios using AR is to provide capabili-
ty to see information directly on the plate in real world through a mobile device such 
as a tablet PC or a smart phone. For satisfying this, AR applications are composed of 
a series of functions which are developed specially for this study. 
Figure 2 shows the conceptual image and the flow diagram of the AR system. Once 
a camera which is installed on a mobile device captures video of the real world, AR 
applications identify the plate under sheet metal forming work. Identification is con-
ducted by using markers which are put on the plate, and a number of markers are 
arranged on the plate as they compose a lattice pattern to cover the entire area of the 
plate. Because position (X,Y,Z) of each marker is estimated from the position of a 
single marker which is arbitrarily selected from among all markers, identification can 
be done about location and orientation of the plate in space and the entire shape of the 
plate, Figure 3. 
AR applications prepare information which is displayed on the captured scene in 
the next step. This information is one which is concerning to the work of sheet metal 
forming, and information includes design data such as a target shape of the plate, 
work process data such as information for next work procedure or geometric data of 
the present shape. This information is not only generated and imported by a separate 
system such as a CAD system or a special program which analyzes sheet metal form-
ing, but also generated directly by analyzing the shape of the plate which is identified 
by using the series of markers. It is important and distinguishing characteristic of the 
AR application to use markers as this way, because this enable a good correlation 
with a general way to represent ship’s surface, and be able to provide static and pre-
pared data set by utilizing an existing software. As is shown in Figure 3, once the 
Flat plate before forming
There is no guidance about work procedure.
Press work
Worker decides work procedure by himself.
Shape confirming while pressing
Gas heating work
Shape confirming by templates
Resulting shape
Worker uses a template for shape confirming.
Work procedure varies each time. 
It is impossible to confirm quantitatively.

940 
K. Matsuo, U. Rothenburg, and R. Stark 
lattice pattern which covers the entire plate is generated, geometric analysis is con-
ducted by mathematical representation of the surface. Work procedure such as a next 
work step is also obtained by importing from the sheet metal forming program. 
A coordinate system in the captured scene is generated by using a single marker on 
the plate. 3D objects which are generated from information described above are cor-
rectly transferred and superposed on the plate in real world according to relation be-
tween the coordinate system and the plate. 
The AR application is developed by utilizing “ARToolkit” which is a software li-
brary for building AR applications [7]. “ARToolKit” is able to perform camera track-
ing in real time, objects recognizing by looking a marker, and ensuring that a virtual 
object al-ways appear overlaid on the tracking markers. 
 
 
Fig. 2. Conceptual picture and the flow diagram of the AR system 
 
Fig. 3. Identification of the plate by using markers 
2.3 
Demonstration of AR Application and Improvement for Practical Use 
An experimental demonstration was conducted to verify how to utilize the AR appli-
cation to sheet metal forming work and which kinds of benefits to be expected. In this 
demonstration, an actual-size model is prepared to simulate sheet metal forming in 
shipbuilding. 
3D object
Camera
Monitor
plate
marker
Capture scene of the real world by video
Identify the plate into the scene
(position, orientation and shape of the plate)
Prepare information which is displayed in AR system
1. Import from external system
2. Calculate directly from information obtained by scene
Transferred and superpose this prepared information on 
Define coordinate system in the AR system by using a 
single marker
Generate 3D objects (3D CG)
Render 3D objects in the scene
Function
representation
x
y
z
x
y
z
plate
marker
R=f(x, y, z)
x
y
z
Superposition of 
Contour plot
I(i+2)
I(i+1)
I(i)
I(i+3)

 
Application of AR
The actual-size model wh
the Figure 4. This plate is 
size 1.7m length and 1.5m h
the plate to recognize the s
to represent mathematical s
Demonstration is carried
metal forming, and AR app
• Getting an image of the t
• Confirming the work pro
• Confirming the present s
Figure 5 are snapshots of th
 
(a) 
Fig. 4. Location
(a) 
Fig. 5. S
R Technologies to Sheet Metal Forming in Shipbuilding 
hich is equivalent to a ship’s plate is prepared as shown
one which composes a stern part of actual ship, and ha
height. In this demonstration, a series of markers are put
hape, and the lattice pattern is generated by those mark
urface. 
d out along workflow of actual work procedure of sh
plication is used to confirm following functions. 
target shape before beginning of the work; 
ocedure before beginning of the work; 
shape with its target shape while or after the working; 
he demonstration which is held in a laboratory. 
(b) 
n in a ship and marker arrangement of the test piece 
(b) 
Snapshot of the experimental demonstration 
941 
n in 
as a 
t on 
kers 
heet 
 
 

942 
K. Matsuo, U. Rothe
(c) 
(e) 
2.4 
Consideration thro
Application 
Following remarks includi
arise from the demonstratio
─ It is easy to get a visual 
from arbitrary eyesight. 
─ A confirmation of the sh
is possible. This remove
save time and money to u
─ Some shortcomings are 
unstable when the camer
Size of a plate used in s
from a worker to an obj
tained in high level. A 
worker wants to watch fr
 
enburg, and R. Stark 
(d) 
(f) 
Fig. 5. (continued) 
ough the Demonstration and Improvement of the 
ing technical problems toward practical use in shipya
on. 
imagination about a 3D shape by watching the 3D obj
hape of the entire area of the plate with quantitative analy
es constraint to use traditional templates, and contribute
use templates. 
found in the AR application. Tracking markers becom
ra is put at the point from which it is hard to look mark
shipyards is usually huge length about 20m. Even it is
bjective plate, performance of perception should be ma
stable tracking is required to the huge plate, even w
rom both near side and far side. 
 
 
AR 
ards 
bject 
ysis 
s to 
mes 
ker. 
 far 
ain-
when 
 

 
Application of AR Technologies to Sheet Metal Forming in Shipbuilding 
943 
─ For practical use of sheet metal forming it is not reasonable to put several markers 
on the plate. There also exist other problems to rely a marker such as when the 
plate is turned upside down while working. 
─ Accuracy is not stable to recognize the shape of a plate. Most important factor to 
realize practical use is to maintain enough accuracy to recognize the shape of a 
plate. Considering quality control in Japanese shipyards, accuracy concerning the 
position needs less 10mm. On the other hand, a high-speed response is not required 
in this case, because a plate usually deforms slowly. 
─ It is hard to handle tablet PC while working. 
─ Typically the factory environment dark, vibrating and dusty. Thus, a robust and 
stable system is required to use in such environment. 
Following solutions are proposed to solve above technical problems: 
─ To recognize the shape of a curved surface with high accuracy, a laser scanning 
system is connected to the AR application. Figure 6 (a) shows point data around 
the plate in our factory which is scanned by the “FARO Laser scanner Focus3D” 
with accuracy of 2mm. By utilizing this point data for identifying the shape of a 
plate, a series of quantitative amount of gap between the present shape and the tar-
get shape is acquired with high accuracy (Figure 6 (b)).  
─ Effective GUI should be developed to understand more friendly and instinctively. 
New concept about new GUI is conceived from the demonstration, and this con-
cept is one which duplicates traditional templates in AR system as shown in Figure 
7. This GUI connects new technologies to traditional ones, and helps workers to 
access to new technologies. 
─ Some hardware should be required to support a stress-free usage of the AR system 
in the factory. Especially, some kinds of a mounting device to fix the tablet PC 
near the worker would be required to let the worker be free to use his hand. Also, a 
projection based AR system would be raised as another approach to this solution. 
 
 
(a) 
(b) 
Fig. 6. Connecting to the laser system to the AR system 

944 
K. Matsuo, U. Rothenburg, and R. Stark 
 
Fig. 7. New concept of GUI of AR application 
3 
Consideration about Application of AR Technologies to 
Various Shipbuilding Work Process 
In case of labor-intensive work which it is inevitable to depend on individual skills 
such as shipbuilding, the new technology such as AR plays important role to com-
municate information effectively. AR technologies realize the following advantages: 
─ Process related information, which are otherwise not visible are available; 
─ Transfer design information (e.g., CAD information) directly as the natural 3D 
object on the surface. This means realization of a 3D drawing sheet with a tablet 
PC or an electronic paper in future, and 3D information is transferred as the 3D in-
formation without passing the 2D drawing. 
─ Worker are be able communicate interactively; 
Besides the presented application of AR many other tasks within the entire ship-
building work process which would benefit from introduction of AR-technologies: 
─ Provision of information such as for factory management (e.g., part number, pro-
cessing status, etc.) or physical properties (weight, temperature, etc.) 
─ Provision directions to and supports workers with an instinctive viewing (e.g., for 
sheet metal forming, block assembly, pipe fitting, maintenance, etc) 
─ Realization of interactive communication to design data and floor shops (e.g., 
drawing at a floor shop, control of NC machines or welding robots) 
─ Inexperienced worker to be instructed are assisted by convenient methods 
(educational purpose) 
─ Platform of a measurement instrument (e.g., template in sheet metal forming, etc.) 
4 
Conclusion 
In this paper a developed AR application which addresses specific requirements of 
sheet metal forming is presented. A comprehensive demonstration is conducted for 
verifying the AR application. General overview is described about future utilization 
of AR system for an entire shipbuilding in last section. In the following result of this 
study are summarized: 
Idea of the AR template
9.1mm
5.3mm
Next Step
Traditional way to confirm the shape

 
Application of AR Technologies to Sheet Metal Forming in Shipbuilding 
945 
─ AR application improves understanding about the work by direct and instinctive 
expression of information. The application will contributes improvement of sheet 
metal forming works in shipbuilding. 
─ Some defects are found about the accuracy and stability. Effective GUI is also 
necessary for practical use in shipyard. 
─ Connecting a laser scanning system to the AR system improves the accuracy to 
recognize the shape of the plate. It does not lose the functionality in the practical 
use for sheet metal forming in shipyard, even it lose split-second operation. 
─ There are many proposed AR applications for various purposes in shipbuilding. 
Acknowledgments. The results presented in this paper are originated by a research 
visit of Mr. Matsuo at the Fraunhofer IPK in 2011/2012. The authors would like to 
express grateful thanks to the Fraunhofer IPK, TU Berlin, and the NMRI having sup-
ported the successful exchange. 
References 
1. Stark, R., Kim, M., Rothenburg, U.: Vom Virtuellen Produkt zur Digitalen Fabrik: Poten-
tiale und Herausforderungen (From Virtual Product to Digital Factory: Potencials and Chal-
lenges. In: XIII. Internationales Produktionstechnisches Kolloquium. Produktionstechnik - 
Motor aus der Krise, Oktober 4-5, pp. 87–98. PTK, Berlin (2010) 
2. Okumoto, Y., Hiyoku, K., Uesugi, N.: Simulation –Based Ship Production Using Three-
Dimensional CAD. Journal of Ship Production 22(3), 155–159 (2006) 
3. Nedess, C., Friedewald, A., Schaefer, C., Schleusener, S.: A Procedure Model to Tap the 
Full Potential of Virtual Reality in the Maritime Industry. In: Int. Conference on Computer 
Applications in Shipbuilding, ICCAS 2009 (2009) 
4. Loedding, H., Friedewald, A., Heinig, M., Schleusener, S.: Virtual Reality Supported As-
sembly Planning in the Shipbuilding Industry. Journal of Ship Production and Design 27(3), 
146–152 (2011) 
5. Broas, P., Salonen, T., Saaski, J.: Simulations, Virtual and Augmented Reality Technologies 
for Ship Life-cycle Engineering. In: 8th Int. Conf. Computer and IT Applications in the Ma-
ritime Industries, Budapest, pp. 486–497 (2009) 
6. Fellner, D., Behr, J., Bockholt, U.: Istantreality - A Framework for Industrial Augmented 
and Virtual Reality Applications. In: 2nd Sino-German Workshop Virtual Reality & Aug-
mented Reality in Industry, pp. 78–83 (2009) 
7. ARToolKit, http://www.hitl.washington.edu/artoolkit/ 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 947–956. 
DOI: 10.1007/978-3-642-30817-8_93     © Springer-Verlag Berlin Heidelberg 2013 
Are Smart Products Foiling Automated Design?  
Patrick Klein, Johannes Lützenberger, and Klaus-Dieter Thoben 
IKAP, BIBA – Bremer Institut für Produktion und Logistik GmbH, Hochschulring 20,  
Bremen, 28259 Bremen, Germany 
{klp,lue,tho}@biba.uni-bremen.de 
Abstract. Design Automation (DA) implements the idea of deriving the physi-
cal design of a product automatically from within codified engineering knowl-
edge. If a product is no longer limited to be a physical device, it should be  
analysed if the idea of DA can be enhanced or if DA becomes obsolete for 
smart products. The authors believe that DA can even play a major role for the 
smart products development. Thus this paper additionally aims to provide a 
concept for an enhancement of DA. Instead of case based and locally imple-
mented solutions, the concept relies on a central knowledge-based system in  
order to process the smart layer on top of the geometrical design. The proposed 
system should be grounded upon an ontology in order to represent the physical 
and the virtual domain at once. This way different kinds of product develop-
ment applications can rely on one central knowledge-base. 
Keywords: Smart Products, Design Automation, KBE, User-Centered Design, 
Collaborative Design, Ontologies. 
1 
Introduction 
So called smart or intelligent products offer the potential for consumer goods to be-
come more intelligent and better suited to the requirements of the users. The possibil-
ity to adjust functionalities during the usage phase and customize the product on item 
level offers an added value to the user and provides new business opportunities to 
manufacturers and service providers [1]. 
Expanding on the idea of smart products, current research proposes a user-centred 
and collaborative design. Such design would focus on the identification of interac-
tions and services to fully reflect user requirements and preferences from early devel-
opment stages, rather than improvements driven by technology [1], [2]. 
On the other hand, Design Automation, which is an area of Knowledge Based En-
gineering (KBE), has significantly improved product development especially in  
the domain of repetitive, routine tasks. According to [3] about 80% of the time during 
the product development phase is dedicated to repetitive tasks. That means, that the 
enormous potential of successfully implemented DA solutions has already been vali-
dated by several research and development projects [4]–[7].  
 

948 
P. Klein, J. Lützenberger, and K.-D. Thoben 
Design Automation and its underlying domain KBE, demands for codified knowl-
edge. In other words a KBE solution relies on rules, formulas, constraints and other 
codified knowledge to “autonomously” derive the physical design of the product from 
the knowledge base (e.g. a product shape derived from aero dynamical constraints). 
The deterministic approach of KBE seems to be in opposition to the user cen-
tred/collaborative design. Even if KBE has already proven its validity some important 
questions remain to be answered: Is it possible to develop smart products if the design 
is processed by knowledge bases? Is knowledge-based engineering contradicting the 
paradigm of user-centred design?  
Of course it can be argued that Design Automation is still of importance on com-
ponent or sub-component level (e.g. casings for a device), but the authors believe that 
the approach of KBE can even play a major role for the overall development of smart 
products in the near future.  
2 
Background 
According to e.g. Mühlhäuser [8], Smart Products can be defined as entities designed 
and made for self-organized embedding into different environments in the course of 
its lifecycle, providing improved simplicity and openness through improved product 
to user (p2u) and product to product (p2p) interaction. The capabilities to interact can 
rely on context-awareness, semantic self-description, proactive behaviour, multimodal 
natural interfaces, Artificial Intelligence planning, and machine learning [8]. A smart 
product is embedded within an environment that provides the intelligence to 
download, process and store information on individual users, their prior interactions 
with products and the ability to create a context to p2u interaction. 
In compliance with the definition above Maass and Varshney characterize smart 
products by several dimensions [9]:  
1. Situatedness: recognition of situational and community contexts  
2. Personalization: in terms of tailoring the product according to buyer’s and con-
sumer’s needs and affects 
3. Adaptiveness: the ability to change product behaviour according to buyer’s and 
user’s responses and tasks   
4. Pro-activity: anticipation of user’s plans and intentions  
5. Business-awareness: consideration of business and legal constraints 
6. Network capability: the ability to communicate and bundle with other products 
Herewith different classes of smart products can become possible ranging from cus-
tomer to products communication in order to help selecting (and buying) a perfectly 
suited product up to the ability to create pleasant experiences along the usage phase 
(e.g. game apps running on a mobile phone and this way enhancing the experience to 
use the mobile).  
Different types of smart products require different enabling technologies, which 
are directly influencing the product development process itself (e.g. weather-
conditions can be recognized by on-board sensors of a device or alternatively  

 
Are Smart Products Foiling Automated Design? 
949 
provided by an internet service). As mentioned above, the design has to focus on the 
identification and interpretation of interactions and services to fully reflect user  
requirements and wishes from early development stages, rather than improvements 
driven by technology.  
Product development teams become not only responsible for the definition of a 
digital representation of the product, which enables adaptation to situations and  
consumers and its respective environment. But they have to identify the interplay 
between the physical and the virtual world. The existence or absence of a physical 
button on contemporary mobile devices can serve as a prominent example. The re-
spective design decisions are neither technology/feature driven nor assembly or 
manufacturing related. Moreover, user centred design has become the driving force 
[1], [2]. 
However, to pick up the introductory question, user-centred design is by far not in 
contradiction to Design Automation. Nowadays DA is usually performed by KBE 
solutions. KBE (DA respectively) is implemented on many levels in different indus-
tries: From simple templates in CAD software to extensive stand-alone software  
solutions with integration towards other CAx systems, there are many ways of im-
plementing codified knowledge through rule-sets on product design. Within a KBE 
solution, engineering knowledge is represented in a formal manner and enables the 
system to automate specific development tasks (thus it is called Design Automation). 
Each KBE system provides on the one hand an interface to capture the knowledge in 
terms of logical rules, algorithms, or constraints, and on the other hand an output 
module to trigger adjacent CAx systems or/and visualize results [10].  
In this sense, KBE can be seen as the process of gathering, managing, and using 
engineering knowledge to automate the design process by usage of a KBE  
system [11]. The meaning of “automate” even covers analysis tasks in terms of vali-
dation or quality checking, such as compliance to required safety parameters, or ISO 
standards. Next to time savings a KBE solution can enable a broader variety of de-
tailed design studies of a given master-concept by usage of a rule-based approach for 
an automated detailing and examination of design variants and in consequence exten-
sively support the optimization of a given (mechanical) design against defined con-
straints and requirements. KBE as an enabler for easy and fast examination of design 
variants can be of high value for user centred design, because it is an established idea 
in this context to provide users with different kinds of virtual or physical mock-ups 
(e.g.[12]).  
However, current KBE is not an one-solution-fits-all approach. By an analysis of 
more than 500 scientific publications in the area of KBE, major limitation of contem-
porary KBE approaches have been identified [13].  
Currently most KBE-solutions are still very much case based and not grounded in 
structural frameworks or methodologies [14]. In addition many product developers 
seem to improvise a KBE solution based upon a customized development process and 
an unstructured problem analysis[13]. 
This kind of unstructured approach is followed by contemporary CAD systems. 
Leading CAD applications provide add-on modules (e.g. [15]) for KBE related features. 
 

950 
P. Klein, J. Lützenberger, and K.-D. Thoben 
In such modules the KBE intelligence (e.g. a design rule) directly remains inside a 
CAD-model and is directly stored within the CAD file. Based on a parameterized 
CAD model, they provide functions like formulas (to create dependencies  
between parameters), rules (such as If. . . then. . .) and user defined features, allowing 
to partly reuse design procedures [15].  
Herewith an enormous disadvantage appears with respect to collaborative engi-
neering, neither an intelligent usage of the companies gathered codified knowledge 
nor an ability to combine such KBE-models into an overall solution is easily possible. 
Even if it would be possible to break up this encapsulation, which is given by the 
proprietary structure of such files, a sharing of the implemented design knowledge 
would fail, due to a lack of standardization of at least these items:  
• Namespaces: different naming of parameters (e.g. “surface”  “shape”) 
• Relations: dependencies between parameters (e.g. “if … then …”  “if ….else”) 
• Operators & Rules: (e.g. “if … then …”  “if ….else”) 
In addition to the encapsulation, [13] criticizes that many KBE-Solutions store and 
represent codified knowledge decoupled from its original context. An adequate 
documentation is missing and formulas or equations remain unexplained [5]. The 
cause is often grounded in an unstructured knowledge acquisition process. Without a 
documentation of the problem in terms of objectives, constraints etc. the traceability 
of the implemented solution becomes impossible. Along with the insufficiency of a 
structured knowledge codification, a lack of knowledge reuse has been identified. Due 
to missing knowledge - e.g. neglected alternatives for a desired solution – KBE solu-
tions are too often limited to their origin context and thus the reuse of knowledge will 
be hindered or impossible [13]. 
All of those KBE limitations (lack of openness, lack of documentation, lack of 
knowledge reuse, etc.) may not to be seen as super critical for contemporary solutions 
in context of Design Automation. But due to the interdisciplinary nature of smart prod-
ucts development (mechanical engineering, informatics, etc.) black box approaches or 
unstructured codification may become a key hurdle for Design Automation.  
3 
Enabling KBE for Smart Products – An Approach 
The authors believe that to achieve a support for smart products, KBE-systems must 
be enhanced and adapted, particularly in the sense of paying special attention to the 
potential interaction of products with different sorts of information and content.  
This is of course by no means possible, if the KBE intelligence (e.g. a design  
rule) directly remains inside a CAD-model and is solely stored within the CAD file 
(pls. see above). Further, to represent the semantics of intelligent functions, the sys-
tems have to capture product as well as content knowledge. This means that a struc-
tured knowledge acquisition and codification becomes a precondition. The knowledge 
 
 

 
Are Smart Products Foiling Automated Design? 
951 
of different domains has not only to be captured, but merged into one integrated 
model.  
The underlying IT-layer, which has to be set up for this purpose, already exists: 
formal ontologies expressed in a formal ontology language (e.g. Web Ontology Lan-
guage – OWL [16]) at the level of so-called description-logics: 
In context of KBE and DA respectively, ontologies have been successfully imple-
mented [3], [17]. Own research activities already show the potential of ontologies to 
process rules and constraints for KBE [18].  
Even more common is the usage of ontologies to add machine-readable meaning to 
(web-) content. The latter is for instance covered by the so-called semantic web. 
Herewith the idea is to provide the content of the WWW not only on behalf of hu-
mans but also according to software. 
With respect to the ability to merge knowledge from different domains, ontologies 
are capable to enhance current Design Automation solutions. The basic idea is there-
fore to provide a central knowledge based system on basis of description logics. The 
proposed ontology will comprehend a semantic representation of both worlds:  
1. Smart layer: representing context, user and usage scenarios interactions etc. 
2. Physical layer: product related dependencies, physical constraints, etc.  
Herewith smart product related knowledge will rely on both layers and the product 
design is no longer limited to the physical layer. 
In this context it should be mentioned, that the “standard” ontology notation is very 
limited with respect to typical requirements of codified engineering knowledge:  
Especially features are needed to compare values or parameters and enable simple 
calculations respectively. Hence it has become common for ontology related KBE 
approaches to rely on enhancements such as SWRL [19] or RuleML [20]. In conse-
quence the use of those enhancements makes sense also for the “smart product ontol-
ogy” approach. 
Against this background the authors believe that the ongoing research and stan-
dardization activities for ontologies and formal logic languages are going to have 
huge impact on KBE and especially on possibilities to enable the above mentioned 
two layer approach for KBE. The already noted demand for a “move from black-box 
applications (proprietary software) to applications with user-friendly knowledge 
bases” [14] can only be grounded on those activities.  
Next to the knowledge base itself, it remains a critical issue to define a user-
friendly interface to such a KBE-System. GUIs have to reflect that the target-group is 
no longer limited to mechanical engineering. To define for instance software features 
and GUI elements demands for specific expertise from the field of informatics. An 
ideal solution should be to provide a middleware layer with respective application 
programming interfaces (APIs) and thus use existing product- and software develop-
ment applications as a front-end access to the envisaged system (Fig. 1): 

952 
P. Klein, J. Lützenberger, and K.-D. Thoben 
 
Fig. 1. Reduction of geometry related parameters to a core set of parameters  
Even if KBE systems for smart products can not rely on encapsulated engineering 
knowledge inside a CAD-model, it does not mean that a central storage of all design 
parameters is the best approach.  
Product shapes can be referred to a minor set of core parameters by usage of the 
on-board features of contemporary CAD systems (refer to Fig. 2). For the example 
illustrated below only the core parameter „buttonsize“ has to be delivered by a knowl-
edge based system. This way - if the knowledge repository is not blown up with over-
loads of geometrical concepts – consequently it becomes much easier to handle the 
envisaged “smart product ontology” (and the knowledge based system respectively). 
 
 
Fig. 2. Reduction of geometry related parameters to a core set of parameters  
4 
Practical Benefits Drafted in Two Sample Scenarios 
To visualize the potential of the approach, a scenario is provided upon the envisaged 
approach consisting of two parts:  

 
Are Smart Products Foiling Automated Design? 
953 
• a decision support, capable to recommend, if user interfaces (e.g. a knob or button) 
are to be placed virtually on a touch screen or physical as part of the casing. 
• content/scenario related definition of the products shape: an electronic device for 
home office leads to product shape A, an electronic device in a mobile environ-
ment leads to casing B. 
 
 
Fig. 3. Simplified ontology representing the User-Interface element button  
Figure 3 illustrates a simplified ontology addressing both layers (smart and physical 
layer) for such a button development. In the centre you will find two concepts repre-
senting both layers (physical and smart). Further a switch on/switch off functionality is 
implemented by a concept called button. As illustrated, a button can belong to both 
concepts since the respective user interface (UI) element can be either realized virtual 
(e.g. as a slider in GUI) or physical (e.g. as a push-button integrated in the casing). 
Based upon this model, simple engineering rules can be set for the scenario:  
 
if environment.home = true then casing.lenth = 300 
if casing.length > 250 then button.diameter = 30  
if casing.length > 250 then button.layertype = physical  
 
Not even product-related entities can be modelled in such an ontology, but also other 
concepts: The sample ontology (Fig. 3) includes on the top-right corner an environ-
ment concept together with two status attributes. This expands the degree of freedom 

954 
P. Klein, J. Lützenberger, and K.-D. Thoben 
for modelling rules within the domain. By choosing enviroment.home the button be-
comes physical and has a diameter of 30. This can be further processed by the CAD 
system, in terms of automated sizing of the geometrical representation of a parameter. 
The value of the button.diameter may serve as an input for the associated CAD 
model, such as illustrated above (Fig. 2).  
Carrying the thought a bit further, modelling of complex semantics become possi-
ble: the button could be a component of a keypad with additional core parameters 
(such as numberOfbuttons) and other dependencies (such as spaceBetweenbuttons or 
positionInkeypad).  
Further, the existence or absence of a physical button leads to different casings and 
product layouts, rules such as below may complete the scenario:  
 
 
If button.layertype = virtual  
then casing.length = casing.length – button.diameter  
 
With respect to the above mentioned easy and fast examination of design variants this 
rule based approach enables to change the design and appearance of a product quickly 
just by switching attribute values. In fact, the underlying description logics of ontol-
ogy languages enables, that nearly all dimensions of smart products can be considered 
for a DA of a smart product: 
1. Situatedness: concepts and rules representing and reflecting the situational and 
community context 
2. Personalization: in terms of modelling personal needs and affects, e.g. rules such 
as: if person.impairment = true then button.layertype = physical  
3. Adaptiveness: the ability to process rules in order to change product behaviour ac-
cording to concepts representing user’s responses and tasks 
4. Pro-activity: not applicable directly (but not in contradiction to the approach) 
5. Business-awareness: consideration of business and legal constraints by respective 
ontology enhancement  
6. Network capability: not applicable directly (but not in contradiction to the ap-
proach) 
The six dimensions may not only be represented by a single concept such as the  
environment concept, but can cover other - even already existing - ontologies (name-
spaces). The semantic sensor network ontology can be such an example. Its descrip-
tion supports not only the physical structure of a device, but also the processing  
structure of the sensors. The sensor itself is not limit to a physical object, but can be 
seen as anything that can estimate or calculate the value of a phenomenon, so a device 
or computational process or combination could play the role of a sensor [21]. 
5 
Conclusions 
According to the analysis in this paper, there is no contradiction between Design 
Automation and product development of smart products. Even more, the development 

 
Are Smart Products Foiling Automated Design? 
955 
of smart products can directly benefit from knowledge based engineering, if the latter 
is realized under consideration of specific constraints. However in some contempo-
rary solutions initial hurdles do exist, such as encapsulation of engineering knowledge 
in proprietary files. But nevertheless it is possible to provide a support to the product 
development process of smart products by Design Automation, if these hurdles can be 
solved. 
To achieve this, the authors propose an architecture, where smart product related 
knowledge is stored in a central knowledge repository and managed by a knowledge-
based system. An ontology is proposed to become the core of the underlying  
IT-infrastructure. As drafted above, ontologies provide the required flexibility to rep-
resent classical engineering knowledge and at the same time the “smart layer” do-
main. Even better the possibility to use existing (fully elaborated) ontologies as an 
integral part becomes possible. Consequently, there is no need to reinvent domain 
specific knowledge. 
Such a central knowledge-based system may become an integrative part of PLM, 
thus being implemented as services (as already proposed by [22]). Grounded on this 
approach product development applications can not only use stored information (such 
as parameters and functions) in order to control the mechanical design, but in addition 
can gain benefits for further knowledge acquisition by reasoning and data mining. If 
,for instance semantic reasoning can be set upon a “smart products” ontology (which 
covers all kind of information from the smart products life cycle), the product quality 
itself may benefit from the proposed approach as well. Findings gained from a central 
knowledge-based system can directly push a smart product to become smarter. 
Acknowledgments. Part of this research has been funded under the EC 7th Frame-
work Programme, in the context of the LinkedDesign project (www.linkeddesign.eu). 
References 
[1] 
Buurman, R.D.: User-centred design of smart products. Ergonomics 40(10), S.1159–
S.1169 (1997) 
[2] 
Hazenberg, W., Huisman, M.: Meta Products: Building the Internet of Things. Bis Pub-
lishers (2012) 
[3] 
Skarka, W.: Application of MOKA methodology in generative model creation using 
CATIA. Engineering Applications of Artificial Intelligence 20(5), S.677–S.690 (2007) 
[4] 
Nacsa, J., Bueno, R., Alzaga, A., Kovács, G. L.: Knowledge management support for 
machine tool designers using expert enablers. International Journal of Computer Inte-
grated Manufacturing 18(7), S.561–S.571 (2005) 
[5] 
Kulon, J., Broomhead, P., Mynors, D.: Applying knowledge-based engineering to tradi-
tional manufacturing design. The International Journal of Advanced Manufacturing 
Technology 30(9), S.945–S.951 (2006) 
[6] 
Danjou, S., Lupa, N., Koehler, P.: Approach for Automated Product Modeling Using 
Knowledge-Based Design Features. Computer-Aided Design & Applications 5(5), 
S.622–S.629 (2008) 

956 
P. Klein, J. Lützenberger, and K.-D. Thoben 
[7] 
Penoyer, J.A., Burnett, G., Fawcett, D.J., Liou, S. Y.: Knowledge based product life 
cycle systems: principles of integration of KBE and C3P. Computer-Aided Design 32(5-
6), S.311–S.320 (2000) 
[8] 
Mühlhäuser, M.: Smart products: An introduction. Constructing Ambient Intelligence, 
S.158–S.164 (2008) 
[9] 
Maass, W., Varshney, U.: Preface to the Focus Theme Section:’Smart Products’. Elec-
tronic Markets 18(3), S.211–S.215 (2008) 
[10] 
Milton, N.: Knowledge technologies. Polimetrica, Monza (2008) 
[11] 
Prasad, B.: What Distinguishes KBE from Automation (2005), 
http://legacy.coe.org/newsnet/Jun05/knowledge.cfm  
(accessed: Dezember 02, 2009) 
[12] 
Bevan, N., Curson, I.: Planning and implementing user-centred design. In: CHI 1999 
Extended Abstracts on Human Factors in Computing Systems, pp. S.137–S.138 (1999) 
[13] 
Verhagen, W.J.C., Bermell-Garcia, P., van Dijk, R.E.C., Curran, R.: A critical review of 
Knowledge-Based Engineering: An identification of research challenges. Advanced En-
gineering Informatics 26(1), S.5–S.15 (2012) 
[14] 
Verhagen, W.J.C., Curran, R.: Knowledge-Based Engineering Review: Conceptual 
Foundations and Research Issues. New World Situation: New Directions in Concurrent 
Engineering, S.267–S.276 (2010) 
[15] 
C. IBM, IBM - CATIA Product Synthesis Discipline - All products in this discipline 
(2009),  
http://www-01.ibm.com/software/applications/plm/catiav5/ 
disciplines/prodsynth/products.html (accessed: Dezember 08, 2009)  
[16] 
McGuinness, D.L., Van Harmelen, F.: OWL web ontology language overview. W3C 
Recommendation 10(2004-03), S.10 (2004) 
[17] 
Ansaldi, S., Bragatto, P., Camossi, E., Giannini, F., Monti, M., Pittiglio, P.: A know-
ledge-based tool for risk prevention on pressure equipments. Computer-Aided Design & 
Applications 3(1-4), S.99–S.108 (2006) 
[18] 
Franke, M., Klein, P., Schröder, L., Thoben, K.D.: Ontological semantics of standards 
and plm repositories in the product development phase. In: Proc. 20th CIRP Design 
Conference, pp. S.473–S.482 (2010) 
[19] 
Horrocks, I., Patel-Schneider, P.F., Boley, H., Tabet, S., Grosof, B., Dean, M.: SWRL: 
A semantic web rule language combining OWL and RuleML. W3C Member Submis-
sion 21, S.79 (2004) 
[20] 
RuleML Homepage, http://ruleml.org/ (accessed: November 28, 2012)  
[21] 
W3C, Report Work on the SSN ontology - Semantic Sensor Network Incubator Group, 
http://www.w3.org/2005/Incubator/ssn/wiki/Report_Work_on_ 
the_SSN_ontology (accessed: November 23, 2012)  
[22] 
Fan, I.S., Bermell-Garcia, P.: International standard development for knowledge based 
engineering services for product lifecycle management. Concurrent Engineering 16(4), 
S.271–S.277 (2008) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 957–967. 
DOI: 10.1007/978-3-642-30817-8_94 
© Springer-Verlag Berlin Heidelberg 2013 
 
Management of Cost Knowledge in Product Design – 
Integration of Upstream and Downstream Life Cycle 
Phases 
Susann Köhler, Annett Bierer, and Uwe Götze 
Chair of Management Accounting and Control, Chemnitz University of Technology,  
Chemnitz, Saxony 09126, Germany 
{susann.koehler,annett.bierer, 
uwe.goetze}@wirtschaft.tu-chemnitz.de 
Abstract. To design products that meet the customer and market requirements 
as well as given cost limits, knowledge from the whole life cycle is essential. 
On the one hand, important parts of this knowledge are generated before and af-
ter design decisions. On the other hand, knowledge has to be distributed among 
various internal and external knowledge carriers. Thus, there is a need of me-
thods supporting the integration of cost (related) knowledge from upstream and 
downstream phases in design processes. The paper presents such methods as 
part of a holistic cost knowledge management approach for product design. 
Keywords: cost knowledge, design-oriented knowledge management, know-
ledge integration, product life cycle. 
1 
Introduction 
In today’s industrial markets products should fulfill functional, ecological and cost-
related requirements concerning their manufacturing, use, maintenance, disassembly, 
and recycling/disposal, respectively. Product characteristics and, thus, the fulfillment 
of these requirements are largely determined by product design decisions, since they 
limit the scope of action in later steps of the product creation process and not least the 
following life cycle phases. As a result, on the one hand, design decisions have a great 
influence on products’ successes. On the other hand, a lot of information from market 
research and (strategic) product planning (as upstream phases of product life cycle) as 
well as manufacturing/purchase, sales/marketing, use, maintenance, and end-of-life 
(as downstream phases) has to be gathered and processed in order to enable design 
decisions leading to products that meet the requirements mentioned above [1]. 
This constitutes a great challenge. Especially in the early phases of design 
processes, only little solid information about the prospective cost and other effects of 
design alternatives is available. Different departments as well as external actors (sup-
pliers, customers) have to participate and, thus, communication barriers between these 
participants have to be managed. Additionally, design-relevant knowledge from the 

958 
S. Köhler, A. Bierer, and U. Götze 
whole product life cycle has to be handled and the lag of time between decision mak-
ing and knowledge generation about its effects has to be closed. Concluding, it is 
essential to systematically identify, acquire, develop, share/distribute, use and pre-
serve the knowledge that is useful for design. This especially applies to cost know-
ledge as an important part of the knowledge needed. But until now only little attention 
has been paid to a systematical cost knowledge management focused on design. And 
particularly the integration of cost knowledge from the upstream and downstream 
phases and its methodological support has been neglected so far. This motivates to 
present methods for the handling of cost knowledge from these life cycle phases as 
part of a holistic cost knowledge management approach for product design. The fol-
lowing presentation of methods is based on literature review and analyti-
cal/conceptual considerations. An application and validation in corporate practice has 
not been conducted yet. 
The remainder of this paper is structured as follows: In section 2, the relevant 
knowledge from upstream and downstream life cycle phases is structured. Section 3 
provides an overview of useful approaches and a holistic concept for the design-
oriented management of cost knowledge. Based on this, section 4 presents methods 
for the integration of cost knowledge from upstream and downstream phases.  
2 
Cost Knowledge in Upstream and Downstream Phases 
The relevant cost knowledge from upstream and downstream phases comprises fac-
tual (declarative) knowledge, i. e. objective knowledge about processes, products and 
their cost, as well as methodological (procedural) knowledge, i. e. knowledge about 
operations and sequences to process this factual knowledge. This knowledge can be 
explicit (i. e. conscious knowledge that can be articulated and stored in the form of 
data) or tacit/implicit (i. e. personal, partially unconscious knowledge that is devel-
oped through practice and cannot or can only incompletely be articulated) [2], [3]. 
The first basic category of knowledge is the factual cost knowledge, which is either 
specific, i. e. related to a specific product, component or process, or generalized from 
several existing objects [3], [4]. Design-relevant factual cost knowledge from up-
stream phases includes specific market-/customer-based requirements and their future 
trends as well as the customer’s willingness to pay. This knowledge is a prerequisite 
to design cost-efficient products that meet the customer needs. With respect to down-
stream phases, this knowledge category comprises: 
• cost relevant requirements from different downstream phases, e. g., resulting from 
available production technologies and utilities (inside the company as well as at the 
supplier), concepts for assembly, use or disposal, existing legal regulations as well 
as from sales/marketing information, 
• cost drivers and their effects in different downstream phases, e. g., the used materi-
al determining the production technologies and the options for recycling/disposal, 
or the manner of connecting different parts with its influence on (dis-)assembly, 
• the post calculation of existing products and their components, differentiated by 
cost categories (like material and labor cost, depreciation), by periods and by  

 
Management of Cost Knowledge in Product Design 
959 
different phases and processes (especially manufacturing or purchasing, use, main-
tenance, disassembly, and recycling or disposal – partially dependent on customer 
specific conditions like maintenance intervals or frequency of use) (see Figure 2). 
This cost knowledge has to flow back into the design stage for usage in future design 
processes. In order to make it applicable for life cycle-related cost prediction, it has to 
be converted into design specific knowledge (e. g., by using post calculations to  
deduce rules for low-cost design or cost functions based on specific product characte-
ristics) [5].  
The second basic category is the methodological cost knowledge, which allows the 
use and generation of factual knowledge in every step of a specific design process as 
well as cost analyses in other life cycle phases. This category refers to cost calculation 
and the estimation of the life cycle cost impacts of different function or embodiment 
variants as well as experience regarding cost estimation, cost impacts in downstream 
phases, and the accordance of estimated and realized cost. Especially in design  
phases, various development-concurrent cost estimation methods, e. g., similarity 
calculation, calculation with relative cost, and calculation formulas [6], life cycle cost 
calculations or simulations of process times and cost are applicable. 
Summarizing, Figure 1 shows the flows of cost knowledge from, within and to 
product design. On the one hand, existing factual and methodological cost knowledge 
from upstream and downstream phases is used to design cost-efficient products. On 
the other hand, by applying existing methods and knowledge new factual and metho-
dological cost knowledge is generated. At the end of design specifications of products 
are fixed and, therewith, the cost in the downstream phases is largely determined. 
 
Fig. 1. Flows of Cost Knowledge in the Product Life Cycle (based on [3], p. 97) 
market 
research
product 
planning
downstream phases
upstream phases
flow of cost knowledge, e. g. post calculations
flow of cost relevant knowledge
flow of cost relevant specifications
needed/applied cost knowledge (examples)
(customer) 
requirements
definition of 
target cost
function cost
estimation of
life cycle impacts
division of 
target cost
relative cost
cost of 
purchased parts
estimation of 
life cycle impacts
factual 
knowledge
methodological 
knowledge
requirements 
definition
conceptual 
design
embodiment 
design
detail design
generated cost knowledge (examples)
target costs
definition of 
target cost
specific 
function cost
draft based
cost estimation
function based 
cost estimation
specific embo-
diment cost
specific
product cost
detailed
cost forecast
factual 
knowledge
methodological 
knowledge
product
design
manufactur-
ing/purchase
use/manu-
facturing
end-of-life
sales/ 
marketing

960 
S. Köhler, A. Bierer, and U. Götze 
3 
Cost Knowledge Management for Product Design – Useful 
Approaches and Holistic Concept 
First, it has to be noted that (life cycle) engineering literature and engineer standards are 
full of cost related knowledge items that can be used in design processes. Within this 
literature, different lines exist. Several engineer standards and codes of practice (e. g., 
VDI 2884 [7], VDMA 34160 [8]) define elements of life cycle cost and cost drivers and 
describe procedure models for forecasting life cycle cost. But they are mostly focused on 
cost from the user’s point of view (i. e. purchasing, using and recycling of capital goods). 
In addition, the effects of cost drivers on life cycle cost are often not considered. Besides, 
methods of development-concurrent cost calculation are discussed intensively (e. g., in 
[6]). As a basis for this, some life cycle-oriented approaches are focused on life cycle 
process models and the estimation of process parameters (i. e. times for manufacturing 
and assembly) (e. g., [9]). Indeed, methods of development-concurrent cost calculation 
become more and more life cycle-oriented, but in many cases the calculation either deals 
with simple, standardized tasks/objects (e. g., [1]) or refers to a very abstract level (e. g., 
[10]). Existing knowledge-based concepts for (life cycle-oriented) product design are 
often not focused on cost knowledge. Furthermore, in most cases the optimization of a 
product is only limited to one specific stage of the life cycle, e. g., manufacturing or 
maintenance (“Design for X”). The emphasis is put on the modeling of knowledge (e. g., 
in form of rules), the structure of IT-systems (like, among others, PLM-systems and  
collaboration systems) or other technical aspects (e. g., [11]). Generally, the existing 
approaches do not mention any ideas to manage cost knowledge in a systematic and 
holistic way. 
Motivated by the need to successfully handle the various and complex (life cycle-
related) cost knowledge required in product design, on the one hand, and the corres-
ponding deficits in literature on the other hand, a concept for the management of this 
knowledge was developed by KÖHLER [3]. This concept is composed of a theoretical 
framework with different knowledge management building blocks and recommenda-
tions for their elaboration. The definition of the building blocks bases on the general 
(not design-specific) knowledge management approach by PROBST/RAUB/ROMHARDT 
([2], p. 34). This approach includes “operational” building blocks as core processes of 
knowledge management: knowledge identification, acquisition, development, sharing/ 
distribution, utilization, and preservation. The blocks are interrelated, since “interven-
tions … in single core processes … will inevitably affect others” ([2], p. 30). In addi-
tion, the activities in the operational blocks need a direction. As this depends on the 
goals a company pursues with a cost knowledge management, the framework is en-
hanced by two “strategic” building blocks, knowledge goal definition and knowledge 
assessment. Together with these blocks a superordinate management cycle is consti-
tuted which is similar to the Plan-Do-Check-Act-cycle by DEMING. 
As mentioned, KÖHLER formulated recommendations for the elaboration of the 
building blocks. However, these recommendations are not focused on the specifics of 
managing knowledge from upstream and downstream phases. Thus, this point is taken 
up here by presenting methods for the integration of cost knowledge from these phas-
es in product design on the basis of the holistic approach outlined above. 

 
Management of Cost Knowledge in Product Design 
961 
4 
Design-Oriented Management of Cost Knowledge from 
Upstream and Downstream Life Cycle Phases  
4.1 
Overview 
The integration of cost knowledge from upstream and downstream phases in product 
design processes requires intensive use of a variety of methods that support the ga-
thering and processing of cost knowledge. Thus, based on literature and referring to 
mechanical engineering, Table 1 gives an overview of relevant methods for all  
knowledge management building blocks (some phases are summarized, since the 
distinction between methods for these phases does not seem to be useful). 
Table 1. Methods for the Integration of Cost Knowledge from Upstream and Downstream 
Phases in Design Processes – Examples from Mechanical Engineering 
life cycle
phases
building 
blocks 
upstream 
downstream 
manufacturing/ 
purchase 
use/maintenance and 
sales/marketing 
end-of-life 
knowledge goal 
definition 
methods of the goal setting process, like creativity techniques, portfolio techniques, 
analysis of the information demand, SWOT analysis 
knowledge 
identification 
questioning of or 
direct discussion 
with customers, 
analysis of items on 
internet platforms 
group discussion 
between staff mem-
bers, analysis of own 
and suppliers’ calcula-
tions 
questioning of custo-
mers/users/mainte-
nance staff members, 
analysis of (custom-
ers’) cost documents 
questioning of cus-
tomers, recyclers or 
staff members, analy-
sis of (recyclers’) 
calculations 
knowledge 
development/ 
acquisition 
market research 
methods (e. g., 
conjoint analysis), 
product clinics, 
quality function 
deployment, 
target costing 
LCC/TCO, cost accounting and cost benchmarking, creation of process 
models, development-concurrent calculation with the application of 
generated and prepared cost knowledge from downstream phases 
analysis of manufac-
turing and assembly 
data, open book 
accounting 
analysis of machine 
use and maintenance 
data, cooperation with 
customers 
analysis of disassem-
bly times, recycling/ 
disposal data, cooper-
ation with recyclers 
knowledge  
sharing/ 
distribution 
open book account-
ing, product clinics 
collaboration with 
suppliers for access to 
production cost 
collaboration with 
customers, providing 
incentives 
collaboration with 
recycling/disposal 
companies 
establishing suitable conditions to stimulate sharing/distribution of cost knowledge (between 
design and other departments as well as external knowledge carriers), e. g., direct communica-
tion across organizations and life cycle phases, integrated IT-systems 
knowledge 
utilization 
establishing suitable conditions to stimulate the usage of cost knowledge, 
e. g., preparation of cost knowledge for reuse; (semi-)automatic application of 
IT-based cost models for cost estimation 
knowledge 
preservation 
storage of require-
ments in catalogues 
and specifications 
storage of cost knowledge in relative cost catalogues, in cost data 
bases, PLM-systems, cost models, in terms of calculation rules; 
regular verification and actualization with new generated data 
knowledge 
assessment 
methods for the collection and evaluation of key figures, 
e. g., questioning knowledge carriers or analyzing IT-systems with stored cost knowledge 
 
 

962 
S. Köhler, A. Bierer, and U. Götze 
Results from the strategic building blocks “knowledge goal definition” (for me-
thods see [2], [12]) and “knowledge assessment” (see [2], [13]) are starting points of 
activities in operational blocks. A selection of methods supporting these activities 
with respect to knowledge integration is presented in the next sections. 
4.2 
Upstream Phases 
As mentioned in section 2, cost relevant knowledge from upstream phases comprises 
the customer’s willingness to pay as well as specific market-/customer-based require-
ments. This knowledge is used both in the first stage of the design process (require-
ments definition) and as a framework in following design stages. In order to make 
such knowledge available for product designers, several methods for distributing/ 
sharing existing or developing/acquiring missing knowledge can be used. 
With the application of market research methods, like different forms of conjoint 
analysis or testing products with specific properties [14], new cost relevant know-
ledge about requirements is generated or acquired by the company. For example, the 
conjoint analysis is used to detect customers’ preferences regarding specific characte-
ristics of requirements. For this, customers have to evaluate different product profiles 
[15], [16]. If the product price is included as a product characteristic, the preferences 
regarding this item indicate the customers’ willingness to pay [17]. The method is 
applicable in B2C-markets as well as in B2B-markets. However, companies acting in 
B2B-markets should observe changes in demands of end-customers in addition to 
direct customers’ requirements, to be aware of market trends at an early stage. 
In order to evaluate different product concepts, these can be presented to and tested 
by key customers or lead users (both real and virtually) in so-called “product clinics”. 
The advantage of this method is the direct contact that can both decrease communica-
tion and motivate customers to share their cost relevant knowledge in terms of re-
quirements and experience in product use [18], [19]. 
The derived preferences provide the basis for target costing, a “cost management 
tool for reducing the overall cost of a product over its entire life cycle with the help of 
the production, engineering, R&D, marketing, and accounting departments” ([20], 
p. 41). With this method, the products’ target cost can be derived from pricing prefe-
rences and the designer obtains indications for decomposing this cost to specific 
product properties, functions, and components, which support cost-oriented design 
[21]. 
To provide design engineers with generated cost knowledge, it can be aggregated, 
structured, and stored – in terms of data – in generally applicable requirements cata-
logues or in functional/performance specifications for a specific design task [22]. 
4.3 
Downstream Phases 
Figure 2 shows a methodological concept for the integration of cost knowledge from 
downstream phases in product design. The core of this concept is built by cost models 
with cost items differentiated according to products and their components, processes, 
cost categories, and periods. The cost models are linked with product models 

 
Management of Cost Knowledge in Product Design 
963 
representing the product structure and process models for the product-related 
processes and activities accomplished during downstream phases (together with their 
cost relevant characteristics). Additionally, further cost drivers are included. Data 
bases store the huge amount of cost (related) data and make it available for design 
processes. 
 
Fig. 2. Concept for the Integration of Cost Knowledge from Downstream Phases 
For cost knowledge generation during downstream phases a variety of methods ex-
ist; some important methods are briefly introduced in the following: 
• To evaluate total life cycle cost, approaches of Life Cycle Costing (LCC) or – with 
regard to customers that purchase the specific product – of Total Cost of Owner-
ship (TCO) can be used [23], [24]. In order to collect the life cycle cost, differen-
tiated according to Figure 2, particular methods of (standard) cost accounting are 
needed [21]. In some cases, these methods have to be refined or extended to grasp 
the monetary effects of design alternatives in the downstream phases (e. g., the 
maintenance cost of a machine or the cost of an end-customer using a car). Based 
on this, new factual knowledge about cost drivers as well as the amount of cost ac-
cording to specific conditions (e. g., use cost in several utilization profiles) can be 
generated – depending on the cost incurrence either by the product designing com-
pany itself or by suppliers, direct and indirect customers or service companies.  
cost data base
LCC/TCO and
cost accounting 
methods
condition 
monitoring data, 
process data
measuring 
conditions and 
process times
standardized
product/process 
models, use cases
generalization
analysis of data and trans-
formation in cost knowledge
product 
models
process 
models
further
cost drivers
cost models
cost 
categories
phases/periods in the life cycle
com-
ponents
processes
product design
upstream 
phases
requirements, 
use cases
methods for
cost estimation

964 
S. Köhler, A. Bierer, and U. Götze 
This external cost knowledge is necessary for both estimating customers’ require-
ments and willingness to pay and considering TCO-commitments in customer con-
tracts. Together with experience from the practical performance of activities, new 
methodological knowledge about cost-efficient design is developed that can be 
edited in terms of cost calculation formulas, rules or relative cost [6]. 
• As a basis for a life cycle cost estimation, process models comprising the product-
related processes and activities of the downstream phases are useful [25]. For  
example, disassembly processes can be segmented in basic motions [26] or – much 
more aggregated – logistic activities may be described by the SCOR model [27]. 
Based on this, new factual cost knowledge is generated by measuring and (statisti-
cally) analyzing times and cost of different processes. Therewith, new methodolog-
ical knowledge (time and cost calculation formulas [6]) as well as factual know-
ledge (standardized models, use cases and corresponding cost [24]) can be derived. 
• Furthermore, knowledge generation in the use phase of machines is supported by 
the application of condition monitoring, i. e. the regular or continuous acquisition 
of current machine conditions. Based on this data, use- and maintenance-relevant 
knowledge about failures and their sources as well as the frequency of scheduled or 
unscheduled maintenance activities can be generated [28], [29]. 
• In the last phase of the already mentioned target costing, the calculated standard 
cost of the product and its components is compared to the target cost to control the 
accordance with the given cost limits. In case of a negative deviation, options for 
cost reduction have to be found [20]. Hence, new cost knowledge about the quality 
of cost estimations as well as cost drivers and their effects is generated. 
• In addition to the upstream phases, customer requirements as cost relevant know-
ledge can be identified and generated by analyzing customers’ inquiries, com-
plaints, negotiations, conversations as part of sales/marketing activities [30]. 
The cost knowledge of internal experts can be elicited by interviews or monitoring 
their activities, whereas the knowledge of external knowledge carriers is less accessi-
ble. Firstly, physical and temporal distances impede direct communication to share 
knowledge [31], especially tacit experience. Secondly, since knowledge sharing takes 
place beyond corporate boundaries, communication barriers resulting from differenc-
es in previous knowledge and knowledge articulation as well as from fear of losing 
knowledge are high [2], [32]. The cooperation with different downstream knowledge 
carriers could diminish these problems. By collaboration with (important) suppliers, 
easier access to manufacturing knowledge, durability data for specific components as 
well as information about recyclability is conceivable. Within open book accounting, 
i. e. the exchange of cost data and calculation-related additional information, suppli-
ers’ cost information can be shared. Necessary prerequisites for the application of this 
method are “the expectation of cooperative behavior from the supply-chain partner 
and trust between those companies” ([33], p. 231). Selected customers can deliver 
condition-specific machine data and cost data from the use phase, whereas  
recycling/disposal companies provide knowledge about prices or the recyclability of 
materials. In order to stimulate the customers’ and recycling/disposal companies’ 
motivation for collaboration, different incentives (e. g., cost reduced service, integra-
tion in development, contracting) seem to be adequate [34]. 

 
Management of Cost Knowledge in Product Design 
965 
For knowledge application in design processes, the cost knowledge generated in 
downstream phases has to be prepared and – as far as practicable – stored, e. g., in 
(relative) cost data bases and calculation formulas [6] as well as in PLM-systems [35]. 
Otherwise the utilization of the factual and methodological knowledge could fail. 
5 
Concluding Remarks 
On the basis of a holistic cost knowledge management concept, methods for suppor-
ting the integration of cost knowledge from upstream and downstream phases into 
design decision-making are presented. Due to the restricted scope of the paper, the 
methods and their usage could not be deeply elaborated and discussed, respectively. 
Further work should concentrate on the refinement and validation of the presented 
and other methods. Besides, specific methodological issues should be raised, concer-
ning, e. g., the collaboration between different companies and the management of 
knowledge (from upstream and downstream phases) about revenues. Especially with 
regard to service processes, the interdependencies between a customer’s cost and the 
company’s revenues ask for an integrated cost and revenue knowledge management. 
References 
1. Sandberg, M., Boart, P., Larsson, T.: Functional product life-cycle simulation model for 
cost estimation in conceptual design of jet engine components. Concurrent Engineer-
ing 13(4), 331–342 (2005) 
2. Probst, G., Raub, S., Romhardt, K.: Managing knowledge: building blocks for success. 
John Wiley & Sons, Chichester (2002) 
3. Köhler, S.: Kostenorientiertes Wissensmanagement in den Konstruktionsprozessen des 
Maschinenbaus. GUC, Chemnitz (2012) 
4. Christiaans, H., Venselaar, K.: Creativity in design engineering and the role of knowledge: 
modelling the expert. Int. J. Tech. Des. Educ. 15(3), 217–236 (2005) 
5. Roy, S., Sivakumar, K.: Innovation generation in upstream and downstream business rela-
tionships. J. Bus. Res. 63(12), 1356–1363 (2010) 
6. Ehrlenspiel, K., Kiewert, A., Lindemann, U., Hundal, M.S. (eds.): Cost-efficient design. 
Springer, Heidelberg (2010) 
7. VDI Guideline 2884: Purchase, operating and maintenance of production equipment using 
Life Cycle Costing (LCC), VDI, Düsseldorf (2005) 
8. VDMA 34160: Forecasting model for lifecycle costs of machines and plants, VDMA, 
Frankfurt/M. (2006) 
9. Sandberg, M.: Knowledge enabled engineering design tools for manufacturability evalua-
tion of jet engine components. Luleå University of Technology (2005) 
10. Trender, L.: Entwicklungsintegrierte Kalkulation von Produktlebenszykluskosten auf Basis 
der ressourcenorientierten Prozeßkostenrechnung. Karlsruhe Institute of Technology 
(2000) 
11. Zdrahal, Z., Mulholland, P., Domingue, J., Hatala, M.: Sharing engineering design know-
ledge in a distributed environment. Behav. Inform. Tech. 19(3), 189–200 (2000) 

966 
S. Köhler, A. Bierer, and U. Götze 
12. Schorcht, H., Nissen, V., Petsch, M.: Knowledge goals as an essential component of know-
ledge management, Working Paper No. 2011-06. Illmenau University of Technology 
(2011) 
13. Lettice, F., Roth, N., Forstenlechner, I.: Measuring knowledge in the new product devel-
opment process. Int. J. Prod. Perform. Manag. 55(3), 217–241 (2006) 
14. Mooi, E., Sarstedt, M.: A concise guide to market research. The process, data, and me-
thods using IBM SPSS statistics. Springer, Heidelberg (2011) 
15. Götze, U., Schmidt, A., Weber, T.: Ansätze zur Einbeziehung von Kundenanforderungen 
und Erlösen in die Ermittlung und Steuerung von Werkstofferfolgen. Materialwissenschaft 
und Werkstofftechnik 41(9), 784–794 (2010) 
16. Orme, B.K.: Getting started with conjoint analysis: strategies for product design and pric-
ing research. Research Pub. Llc., Chicago (2009) 
17. Breidert, C.: Estimation of willingness-to-pay. Theory, measurement, application. DUV, 
Wiesbaden (2006) 
18. Tegel, O.: Information and communication technologies to support cooperation in the 
product development process. In: Jürgens, U. (ed.) New Product Development and Produc-
tion Networks: Global Industrial Experience, pp. 389–406. Springer, Berlin (2000) 
19. Schulte, S.: Integration of customer feedback into product development – an approach for 
a digital product validation. In: Blecker, T., Edwards, K., Friedrich, G., Hvan, L., Salva-
dor, F. (eds.) Innovative Processes and Products for Mass Customization, Hamburg, Ber-
lin, pp. 355–363 (2007) 
20. Sakurai, M.: Target Costing and how to use it. J. Cost Manag. 3(2), 39–50 (1989) 
21. Götze, U.: Kostenrechnung und Kostenmanagement, 5th edn. Springer, Berlin (2010) 
22. Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H., Wallace, K., Blessing, L.: Engineering de-
sign: a systematic approach. Springer, London (2007) 
23. Götze, U., Weber, T.: ZP-Stichwort: Total Cost of Ownership. Zeitschrift für Pla-
nung 19(2), 249–257 (2008) 
24. Götze, U., Koriath, H.-J., Kolesnikov, A., Lindner, R., Paetzold, J.: Integrated methodolo-
gy for the evaluation of the energy- and cost-effectiveness of machine tools. CIRP Journal 
of Manufacturing Science and Technology 3(5), 151–163 (2012) 
25. Denkena, B., Harms, A., Jacobsen, J., Möhring, H.-C., Jungk, A., Noske, H.: Life-cycle 
oriented development of machine tools. In: LCE 2006, 13th CIRP International Confe-
rence on Life Cycle Engineering, Leuven, pp. 693–698 (2006) 
26. Hartel, M.: Kennzahlenbasiertes Bewertungssystem zur Beurteilung der Demontage- und 
Recyclingeignung von Produkten: Beitrag zur wirtschaftlichen Gestaltung des Lebenszyk-
lus in der Produktentwicklung. Karlsruhe Institute of Technology (1997) 
27. Supply Chain-Council: Supply-Chain Operations Reference (SCOR®) model. Overview of 
Version 10.0, Pittsburgh (2010)  
28. Weir, B.: Intelligent knowledge based systems in condition monitoring. In: McDonald, 
J.R., Burt, G.M., Zielinski, J.S., McArthur, S.D.J. (eds.) Intelligent Knowledge Based Sys-
tems in Electrical Power Engineering, pp. 99–118. Chapman & Hall, London (1997) 
29. Bufardi, A., Kiritsis, D., Xirouchakis, P.: Generation of design knowledge from product 
life cycle data. In: Bernard, A., Tichkiewitch, S. (eds.) Methods and Tools for Effective 
Knowledge Life-Cycle-Management, pp. 375–389. Springer, Berlin (2008) 
30. Zogaj, S., Bretschneider, U.: Customer integration in new product development: a litera-
ture review concerning the appropriateness of different customer integration methods to at-
tain customer knowledge. In: Proceedings of the 20th ECIS, Barcelona (2012) 
31. Revilla, E., Cury, T.: Antecedents and consequences of knowledge integration in product 
development: an empirical evidence. In: Proceedings of OLKC 2007, pp. 767–782 (2007) 

 
Management of Cost Knowledge in Product Design 
967 
32. Tichkiewitch, S.: Method and tools for the effective knowledge management in product 
life cycle. In: Bernard, A. (ed.) Proceedings of 20th CIRP Design Conference on Global 
Product Development, pp. 19–25. Springer, Berlin (2011) 
33. Hoffjan, A., Lührs, S., Kolburg, A.: Cost Transparency in supply chains: demystification 
of the cooperation tenet. Schmalenbach Business Review 63(3), 230–251 (2011) 
34. Müller, R.M., Spiliopoulou, M., Lenz, H.-J.: The influence of incentives and culture on 
knowledge sharing. In: Proceedings of the 38th Hawaii International Conference on Sys-
tem Sciences (2005) 
35. Ameri, F., Dutta, D.: Product lifecycle management: closing the knowledge loops. Com-
puter-Aided Design & Applications 2(5), 577–590 (2005) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 969–978. 
DOI: 10.1007/978-3-642-30817-8_95 
© Springer-Verlag Berlin Heidelberg 2013 
 
Functional Shape Elements Integrating Design  
and Manufacturing Knowledge  
Volkmar Wenzel1, Alexander Christ1, Daniel Strang1, Jan Tim Jagenberg1,  
Reiner Anderl1, and Thomas Bornkessel2 
1 Department of Computer Integrated Design, Technical University Darmstadt,  
Petersenstraße 30, 64297 Darmstadt, Germany 
2 Rolls Royce Deutschland Ltd & Co KG, Eschenweg 11,  
15827 Blankenfelde-Mahlow, Germany 
{wenzel,christ,strang,jagenberg,anderl}@dik.tu-darmstadt.de, 
Thomas.Bornkessel@Rolls-Royce.com 
Abstract. In the aviation industry, engine components of different families, va-
riants and versions are characterized by a high level of geometrical, functional 
and procedural similarities and offer a high potential for rationalization. The 
reuse of existing knowledge is enabled in product development by part families 
and features libraries. The classification, administration and retrieval of these 
parts and features are mainly based on geometric properties, while functional 
aspects are not considered sufficiently. Nevertheless, in engine design the func-
tional attribute is crucial: Two geometry elements with a nearly identical shape 
may be applied for completely different functions. A flange serves for transmit-
ting torque as well as for sealing against hot air. In this paper functional shape 
elements integrating knowledge from both design and manufacturing and the 
method for their definition are introduced and their application is presented  
exemplarily. 
Keywords: functional shape, design reuse, knowledge integration, knowledge 
based engineering, CAD, PDM, PLM. 
1 
Introduction 
To reuse valuable knowledge, new products are typically not designed from scratch 
but by modification of existing solutions. While in the area of mechanical engineering 
more than 80% of developments are based on reuse [13], for jet engines this number 
can be assumed to be even higher. In doing so, developments are very time-
consuming due to multiple iterations between design and manufacturing. On the one 
hand components, their function and shape are well-known from past developments. 
On the other hand, the knowledge that engineers needed and created to develop  
optimal solutions is not sufficiently documented and stored. 
To enable the reuse of knowledge for future developments, harmonization and  
definition of a template is necessary. Standard parts, standardized part families and 

970 
V. Wenzel et al. 
customized part libraries are common and facilitate the reuse of frequently used geo-
metry elements on a component level [6]. Feature Templates are also common and 
standard functionality of most professional CAD systems. They allow defining reusa-
ble geometry elements on a feature level, below the component or part level. 
All these technologies have in common, that they are mainly geometry-based ap-
proaches: Classification, retrieval and reuse follows primarily geometric properties 
(e.g. standard screw). Although functional commonality is known to be the key for 
design reusability for a long time [4], additional information about the function of 
reusable elements is usually not provided. In the field of jet engines, in particular this 
attribute is crucial. Two geometry elements with a nearly identical shape may be ap-
plied for completely different functions: A flange serves for transmitting torque as 
well as for sealing against hot air. This example shows, that a classification of reusa-
ble elements based on only geometric properties is deficient. An extension by func-
tional information is necessary to ensure the distinct classification, retrieval and reuse. 
In this paper Functional Shape Elements (FSE) will be introduced, which can be 
interpreted as a unit of shape and intended function of the geometric element. Intro-
ductory, recent approaches and related work from science and industrial application 
will be listed in chapter 2. The method to define a FSE and developed tools support-
ing the method will be described in chapter 3. The concept of FSE has been success-
fully proofed by adapting to three different components and will be exemplarily  
presented in chapter 4. An outlook and a conclusion finish the paper in chapter 5. 
2 
Related Work 
The approach described in this paper belongs to the wide field of knowledge-based 
engineering (KBE) [18]. KBE systems capture and reuse engineering knowledge 
about products and processes to solve design problems. The main goal of KBE - re-
ducing costs and saving time in product development - can be achieved by automation 
of recurring tasks and the integration of different domains [2]. Further advantages of 
KBE are a transparent communication and information sharing. The storage of  
knowledge in a common database such as a Product Data Management (PDM) system 
allows reusability [17]. 
In this chapter a brief overview about related topics that have influenced the devel-
opment of FSE from within KBE will be given: Part libraries, feature templates,  
design reuse solutions and knowledge integration. 
2.1 
Part Libraries and Feature Templates 
Part libraries of standard parts and reusable feature templates are common know-
ledge-based engineering approaches. Part libraries are well-established in science and 
industry. They can contain predefined standard parts (like screws, bolts, etc.) and 
incorporate national standards as well as individually modeled geometry based on 
company standards. Developing methods for modeling and classifying these parts is 
still a matter of research [11] [16]. 

 
Functional Shape Elements Integrating Design and Manufacturing Knowledge 
971 
While part libraries offer reusing geometry on a component level, feature template 
technologies enable the definition of reusable geometry elements below the compo-
nent level. Feature Templates are also known as User Defined Features (UDF) and 
can be defined as a set of attributes and constraints, which specify the overall shape. 
Feature templates are an aggregation of predefined CAD features and can be modeled 
with every established 3D-CAD system. They enable users to generate complex geo-
metric elements and offer a favorable design environment. Besides shape information, 
features templates can also contain additional information, e. g. tolerances [5]. 
2.2 
Design Reuse and Knowledge Integration 
The integration of KBE and Product Lifecycle Management (PLM) enables the reuse 
of design solutions, sharing of documents and central storage of CAD files in a repo-
sitory. But even with KBE methods it is difficult to get relevant knowledge for feature 
based modeling out of a PDM system [12]. Information about a models design para-
meters often cannot be viewed directly in the PDM system and updates of those pa-
rameters have to be done manually in the CAD environment [9]. Representation in 
PDM systems is mostly limited to geometry, although non-geometric information e.g. 
bills of materials or product requirements are available [15]. Pugliese, Colombo and 
Spurio claim that there is a strong demand for methods to retrieve knowledge from a 
PDM system and prepare it properly for KBE application [12]. 
Today several KBE methodologies are available: One of the most known is 
MOKA (Methodology and tools Oriented to Knowledge-based engineering Applica-
tions) [10] [14]. Other KBE methodologies are KOMPRESSA [8] and KNOMAD [3]. 
For more information about these approaches, the reader is referred to the sources 
above.  
3 
Design Approach 
FSE are characterized by a defined shape and a unique function and integrate know-
ledge from both design and manufacturing. They can be interpreted as a geometric 
area of a component that fulfills a specified function of the entire part. In this paper, 
the geometric area is called a design zone and is, in terms of CAD technology, 
represented by an aggregation of features and design operations.  
In the following, the method to define a FSE is described: Seven subsequent steps 
are necessary to ensure uniqueness of the intended function, to analyze, compare and 
merge different design approaches into one shape and to integrate knowledge: Com-
ponent Variants and Versions; Function Analysis; FSE Identification; Capturing  
Design Rationale; Parameterization; Naming Convention; Modeling and Storing. 
Supplementary, tools developed to adapt the method to component families and to 
define a FSE will also be explained. The tools and the seven steps of the method are 
shown in figure 1 and will be explained in detail in the following paragraphs. 

972 
V. Wenzel et al. 
F
3.1 
Component Varian
The first step is to choose th
it can be any component o
Before using the method, t
the design alternatives of th
be applied have to be colle
veloped to collect the infor
templates the different vari
matrix. 
F
Fig. 1. Method and developed tools 
nts and Versions 
he component family which should be analyzed. In gene
of an assembly containing function oriented design zon
the design zone has to be defined and analyzed. Theref
he selected component family on which the method sho
cted and compared to each other. A template has been 
rmation about variants and versions (see figure 2). In th
iants and versions for each component are mapped ont
 
Fig. 2. Variants and versions template 
 
eral 
nes. 
fore 
ould 
de-
hese 
to a 

 
Functional Shape Elements Integrating Design and Manufacturing Knowledge 
973 
This first step of the method is important for getting an overview of all components 
of the engines that fulfill same functions. Similarities or variations in engine design 
alternatives are pointed out and possibilities for rationalization are identified. 
3.2 
Function Analysis 
In function analysis a detailed understanding of all variants and versions for the com-
ponent family is necessary. Thereby it is possible to detect areas with similar design 
and similar function as well as areas with differing design and similar function. As the 
idea of FSE is to reuse knowledge and design rationale in further developments,  
understanding the functions of design zones and the rationale behind these is funda-
mental for their identification. To provide an overview of all the functions of the 
component, they have to be collected in a table. As in the previous step, a template for 
the function analysis has been developed. In this template it is checked whether a 
component implements a certain function or not. 
 
Fig. 3. Function analysis template 
If it represents the function, the function is marked with a 1, if not with a 0. With 
this procedure it is possible to get a clear understanding of the functional variation 
and development within one component family. This understanding provides an indi-
cation about which geometric areas are relevant for the definition of FSE. 
3.3 
Functional Shape Element Identification 
The next step is the identification of possible FSE. Therefore a schematic sketch is 
created from component designs that were collected in the template for variants and 
versions. By labeling design zones that fulfill important functions, those which  
appeared in function analysis can be assigned to geometry. The example of a high 
pressure compressor casing is illustrated in figure 4. All components typically provide 
at least one particular function which is important for the entire engine function. To 
define functional importance of an element and to assign it to a design zone, it is ne-
cessary to involve design and manufacturing engineers of the original component. 

974 
V. Wenzel et al. 
3.4 
Design Rationale – 
After defining the FSE, all
analyzed to identify simila
documentation of all decisi
sions have been drawn. In 
collected from variants and
differences in the design a
are highlighted and discuss
solutions are categorized in
riant exists or not. All desig
to make sure, that all of the
FSE and the design rational
Fig. 5. Captu
For capturing the design
identified knowledge needs
grating knowledge into a F
transformed into a docume
 
Fig. 4. FSE identification 
Gathering knowledge 
l related components collected in the first step have to
arities and differences. Capturing design rationale [1] i
ions made during the design process and why these de
 this step traditionally technical drawings of the FSE 
d versions template and compared to each other. If so
alternatives of different variants and versions appear, t
sed with responsible designers of this design zone. Des
nto reasonable or not, whether a valid rationale for the 
gn variants that are necessary should be represented in F
ese can be reused. Comparing all versions and variants o
l behind is extensive work, but essential for the method.
uring knowledge from design and manufacturing 
n rationale an interview method has been developed. 
s to be represented and stored digitally and accessible, in
FSE is only recommendable if it exists explicitly or can
ent. The interview method is based on the template sho
o be 
is a 
eci-
are 
ome 
they 
sign 
va-
FSE 
of a 
 
 
As 
nte-
n be 
own 

 
Functional Shape Elements Integrating Design and Manufacturing Knowledge 
975 
in the upper left corner of figure 5. Starting with issues in past development projects, 
engineers from both design and manufacturing are being interviewed about, which 
information would have been useful to avoid similar future issues and how this infor-
mation should be represented. For each interview, both documents and their relations 
among each other are documented graphically as a network. To highlight the relations 
between all relevant documents of the design and the manufacturing department, all 
networks are merged into one “knowledge map”. An analysis of the identified  
documents and relations of this knowledge map provides a pool of documents, which 
contain knowledge required and which are already used by multiple engineers. The 
knowledge within this pool of documents is represented appropriately to be integrated 
into FSE. 
3.5 
Parameterization 
As described in steps before, for a complete parameterization a close information 
exchange with designers of engine components is indispensable. In order to simplify 
the implementation of FSE, a robust and possibly established parameterization should 
be used. The designers must analyze all components of the component family which 
will be represented in the FSE including their parameterization. If the parameteriza-
tion is not unique, a decision has to be made, which type of parameterization is more 
robust and universal. Parallel to establishing parameterization, working on the naming 
convention is required, because the labels which describe the parameters are a funda-
mental part of the convention. 
3.6 
Naming Convention 
The naming convention covers all elements of the FSE: The considered component, 
the particular features and the labeling of parameters. To start, a naming for the FSE 
has to be defined. Therefore, names and labels designers used in the traditional design 
process should be taken into consideration. This allows an easier implementation of 
the new method in existing product development workflows, because familiar ele-
ments are included. For naming elements, a method was developed that supports a 
standard labeling for all parameters. The method for the parameter labeling is shown 
in figure 6.  
 
Fig. 6. Naming convention 

976 
V. Wenzel et al. 
The first part of the naming defines the location of the element. This element of the 
name consists of a rough definition, which is the naming of the FSE and a specifica-
tion, which part of the feature is described. The second element characterizes the type 
of element, e.g. an edge. The labeling of the parameters is also based on this method 
and enlarged by a description of the dimension, e.g. FlangeFrontEdgeLength. 
3.7 
Modeling and Storing 
Until step 7, the method was completely independent of any software. To complete 
the method and to model a FSE, User Defined Features (UDF) as part of the design 
software Siemens NX 7.5 can be used. To create a robust and sustainable library, best 
practice for UDF creation need to be defined. This was already described in an earlier 
publication on knowledge-driven design features [7]. 
FSE are stored in a PDM/PLM system such as Siemens Teamcenter 8.3. The hie-
rarchical structure of the CAD reuse library allows a functional classification of FSE. 
Associating FSE to geometric elements of the product structure is enabled by the 
PDM structure Manager. While the CAD system offers only a view on the concrete 
used documents within the assembly structure, the PDM systems provides an inde-
pendent and structured overview of all existing documents related to the product 
structure. The storing and classification structure of a FSE library in CAD and PDM 
can be seen in Figure 7. 
 
Fig. 7. FSE storing concept 
4 
Proof of Concept 
To proof the concept of FSE, the methods to define FSE and the developed tools have 
been successfully adapted within showcases in the industry. Three different jet engine 
component families have been analyzed for adaption: Compressor and turbine disks, 
compressor casings, and combustor parts. Among these, high levels of similarities 
between part variants, versions and other parts could be identified. Expert knowledge 
has also been externalized and associated to the newly defined Functional Shape Ele-
ments. During the first six steps of the method, only standardized document types 
were generated and no specific software was required. For modeling the Functional 
 

 
Functional Shape Elements Integrating Design and Manufacturing Knowledge 
977 
 
Fig. 8. Snapshots of an exemplary FSE definition 
Shape Elements in step seven, NX 7.5 and Teamcenter 8.3, both software systems 
from Siemens PLM, were used. 
In figure 8 two work results of the FSE definition of a flange for transmitting tor-
que are shown: On the left side, a result from step 1 is illustrated. Different variants 
and versions of flanges with the same function (transmitting torque) but different 
shapes are listed. On the right side, the complete result of the definition process after 
step 7 is visualized. An exemplary disk modeled by using FSE as user defined fea-
tures with NX 7.5 is demonstrated. Several flanges are used, while each represents a 
particular design zone. The model in the figure consists only of FSE. This shows that 
the entire modeling of a disk with several functions can be achieved by using only 
FSE. 
5 
Conclusion and Outlook 
In this paper an approach for defining Functional Shape Elements integrating know-
ledge from design and manufacturing has been introduced. In common part and fea-
ture libraries, classification, retrieval and reuse of reusable library elements are mostly 
based on geometric properties. The here described Functional Shape Elements 
represent a reusable element integrating knowledge from design and manufacturing 
and provide not just geometric shape, but also functional information. The definition 
process for Functional Shape Elements consists of seven subsequent steps and is the 
main core of this paper. The method has been successfully adapted to exemplary 
component families in industry. 
The approach is of a conceptual nature and until step 6 independent from any 
CAD/PDM software. This generic character of the method leads to a possible applica-
tion in different industries and IT infrastructures. Nevertheless, extensive work is 
necessary to implement the method, in particular during the analysis phase. 
Future research will concentrate on further integration of design and manufacturing 
knowledge. Furthermore, the applicability of the method to other engineering sectors 
with different requirements such as automotive is planned. 

978 
V. Wenzel et al. 
References 
1. Bracewell, R., Wallace, K., Moss, M., Knott, D.: Capturing design rationale. Computer 
Aided Design 41(3), 173–186 (2009) 
2. Cooper, D., LaRocca, G.: Knowledge-based Techniques for Developing Engineering Ap-
plications in the 21st Century. In: Proceedings of the 7th AIAA Aviation Technology, Inte-
gration, and Operations Conference (ATIO), Belfast (2007) 
3. Curran, R., Verhagen, W.J.C., et al.: A multidisciplinary implementation methodology for 
knowledge based engineering: KNOMAD. Expert Systems with Applications 37(11), 
7336–7350 (2010) 
4. Girczyc, E., Carlson, S.: Increasing Design Quality and Engineering Productivity through 
Design Reuse. In: 30th ACM/IEEE Design Automation Conference, pp. 48–53 (1993) 
5. Hoffmann, C.M., Arinyo, J.: On user defined features. Computer Aided Design 30(5), 
321–332 (1998) 
6. ISO 13584-001:2000(E): International Standard for the computer-interpretable representa-
tion and exchange of parts library data: 8 
7. Jagenberg, J.T., Gilsdorf, E.A., Anderl, R., Bornkessel, T.: Knowledge driven design fea-
tures for the product life cycle of engine parts. In: Proceedings of the ASME 2009 Interna-
tional Design Engineering Technical Conferences & Computers and Information in Engi-
neering Conferences, no. 2009, American Society of Mechanical Engineers (2009) 
8. Lovett, P., Bancroft, C.: Knowledge transfer for knowledge based engineering. In: Pro-
ceedings of Technology Transfer and Innovation Conference (TTI), London (2000)  
9. Noel, F., Brissaud, D.: Dynamic Data Sharing in a Collaborative Design Environment. In-
ternational Journal of Computer Integrated Manufacturing 16(7-8), 546–556 (2003)  
10. Oldham K. et al.: MOKA - A Methodology and tools Oriented to Knowledge-based engi-
neering Applications. Shaping the ICT-solutions for the Next Century, 198–207 (1998)  
11. Pernot, J.-P., Giannini, F., Falcidieno, B., Léon, J.-C.: Parameterised free-form feature 
templates. In: IEEE International Conference on Shape Modeling and Applications (SMI), 
pp. 140–147 (2009) 
12. Pugliese, D., Colombo, G., Spurio, M.S.: About the Integration between KBE and PLM. 
In: Proceedings of the 14th CIRP Conference on Life Cycle Engineering, Advances in Life 
Cycle Engineering for Sustainable Manufacturing Business, Tokyo, pp. 131–136 (2007)  
13. Rezayat, M.: Knowledge-based product development using XML and KCs. Computer 
Aided Design 32, 299–309 (2000) 
14. Sainter, P., et al.: Product Knowledge Management within Knowledge-based Engineering 
Systems. In: Proceedings of the Design Engineering Technical Conference, ASME Design 
Automation Conference, Baltimore (2000) 
15. Szykman, S., Sriram, R.D., Regli, W.C.: The Role of Knowledge in Next-generation Prod-
uct Development Systems. Journal of Computing and Information Science in Engineering, 
ASME 1, 3–11 (2001) 
16. Tong, X., Wang, D., Wang, H.: Research and Realization of Standard Part Library for 3D 
Parametric and Autonomic Modeling. In: Global Design to Gain Competitive Edge, ch. 2, 
pp. 293–301 (2008)  
17. Van der Laan, A.H.: Knowledge-based Engineering Support for Aircraft Component De-
sign, Delft (2008) 
18. Verhagen, W.J.C., Curran, R.: Knowledge-Based Engineering Review: Conceptual Foun-
dations and Research Issues. In: New World Situation: New Directions in Concurrent En-
gineering, London, pp. 239–248 (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 979–988. 
DOI: 10.1007/978-3-642-30817-8_96 
© Springer-Verlag Berlin Heidelberg 2013 
 
Decision-Making Support  
for Sustainable Product Development 
Kai Lindow1, Oliver Heimann1, Sebastian Adolphy2, Haygazun Hayka2,  
and Rainer Stark1,2 
1 Chair Industrial Information Technology, Technische Universität Berlin, Germany 
2 Division Virtual Product Creation, Fraunhofer Institute Production Systems and  
Design Technology, Berlin, Germany 
{Kai.Lindow,Rainer.Stark,Oliver.Heimann}@tu-berlin.de, 
{Sebastian.Adolphy,Haygazun.Hayka}@ipk.fraunhofer.de 
Abstract. This research focuses on the conceptual design of a Knowledge-
based Engineering (KBE) System. A concept for the KBE System and its re-
quirements are described and an outlook of the KBE structure is provided. The 
KBE System supports design engineers to better understand the impacts of their 
design decisions on the entire product/system lifecycle. The following points 
are addressed: Clarification of the evaluation criteria for sustainability assess-
ment, concept development for the KBE System and establishment of a com-
prehensive knowledge base. 
Keywords: sustainability, environmental assessment, decision making, sustain-
able product design, knowledge-based systems. 
1 
Introduction 
Addressing the problem of global distributed and limited resources, engineers have to 
develop solutions which consider the entire lifecycle of a product and make reference 
to the environmental and social impacts. Products and services which ensure a high 
standard of living are tied to the use of natural resources which are limited available 
and unevenly distributed [1]. In order to cope with the challenges of an overload of 
sustainability and lifecycle information the support of KBE Systems during the design 
phase seems to be necessary. KBE systems should combine the collective knowledge 
of engineers and designers as well as sociologists and ecologists. That way, product 
design could contribute significantly to minimize the negative impact on the environ-
ment and the society. This paper proposes the aspects of sustainability context areas 
which support the designer in making decisions based on either: 
• Economical Sustainability (e.g. market success and investment efficiency), 
• Environmental Sustainability (e.g. ecological footprint) or 
• Social Sustainability (e.g. Customer use value and societal value). 
The engagements between the context areas are discussed against the background of 
the decision-making process in the conceptual design stage. Design decisions and 
their impact on the lifecycle could be shown. Additionally, the paper presents an  

980 
K. Lindow et al. 
ontology approach following the CommonKADS methodology [2]. The paper 
presents the current work on the development of a KBE System that supports sustain-
able product development. The system is designed to make statements about the sus-
tainability of design decisions. The following points are addressed: 
• Clarification of the evaluation criteria for the sustainability assessment 
• Concept development for the knowledge-based system 
• Establishment of a comprehensive knowledge base 
2 
Problem Statement 
To assess a product in terms of sustainability the question arises how the issue of 
sustainability can be made convenient and controllable for the designer. "Clearly, 
technology designers need more concrete targets then just sustainability. What do you 
do when you are working on ceramics, aircraft, communication protocols, skyscrapers 
and you want to contribute to sustainable development?" [3] In the course of time a 
variety of environmental factors have emerged which can be quantified. The most 
prominent examples are the assessment of CO2 emissions and water pollution. Based 
on these two factors other factors can be derived which have a direct impact on the 
sustainability of a product/system, i.e. the consumption of fresh water during produc-
tion can be measured, the pollutants emitted during the use phase and the fuel con-
sumption for any transportation have an impact on product’s sustainability. However, 
extensive sustainable assessment like Life Cycle Assessment (LCA) is not suited for 
industrial practice of product creation and hence does not lead to more sustainable 
products. For this widely acknowledged problem three major drivers can be identi-
fied: 
• LCA is too extensive to conduct thus it cannot be done comprehensively for a large 
number of different products. Therefore most products will never be covered. 
• The value of LCA is questionable due to massive uncertainty of the analysis, in 
particular regarding the impact assessment. The aforementioned effort has to be 
seen in combination with weak results in an unfavorable cost-value-ratio. 
• The amount and specificity of information generated by an LCA cannot be used by 
the designer, i.e. applied in his daily decisions. Hence even well conducted LCA 
rarely influence design decisions and therewith the product’s characteristics. 
To cope with the challenge, certain governmental and non-governmental organiza-
tions have published databases which list different sustainability indicators of differ-
ent states, e.g. the Japanese National Institute for Environmental Research [4]. Based 
on these data a team of German and Japanese scientists has elaborated 32 factors for 
sustainability which are directly related to the product lifecycle (Table 1) and can be 
influenced by the designer [5]. Most of the criteria only state formulations like "the 
less / the more better" since not all factors are determined to an absolute minimum or 
maximum. The sustainability of products can therefore not be assessed absolute com-
pared to an LCA approach. At this point the lifecycle’s stakeholders have to define 
the sustainability goal in terms of the sustainability context areas. 

 
Decision-Making Support for Sustainable Product Development 
981 
Table 1. Selection of officially accepted sustainability indicators [5] 
Category 
Subcategory 
Indicator 
Country 
Year 
Ecological 
Recycling 
Amount of recycled and 
reused wastes 
Mexico 
2000 
Ecological 
Chemicals 
Hazardous chemicals, quan-
tity 
Sweden 
2006 
Economical 
Material Use 
Energy and raw materials 
productivity 
Germany 
2002 
Economical 
Transportation 
Traffic related emissions 
Austria 
2002 
Social 
Family 
Frequency 
of 
interaction 
with family and friends 
New Zeal-
and 
2002 
… 
… 
… 
… 
… 
 
In order to allow an assessment in accordance with the indicators it is necessary to 
provide additional knowledge about a number of topics in the form of databases on 
the system. These are addressed in Table 2. 
Table 2. Knowledge areas for sustainability assessment 
Topic 
Brief description 
Example 
Environmental 
influences 
Impact of processes and mate-
rials on the environment 
Linking of CO2 emissions and 
global warming 
Manufacturing 
method 
Contains the knowledge about 
the amount of work involved, 
the process as well as the used 
tooling of the single manufac-
turing methods 
Mill consumes energy, leave 
filings and coolants 
Working con-
ditions 
Different terms of working 
conditions and their effect on 
the environment 
Monotonous, heavy work is 
harmful to health 
Utilization 
model 
Contains 
knowledge 
about 
different application purposes 
of products 
An engine is pursued with 
petrol to drive a vehicle 
Method 
for 
recycling 
Types of recycling 
Waste glass is melted under 
energy application and re-
cycled 
Local 
know-
ledge 
Knowledge 
about 
working 
conditions and infrastructure 
Transport by rail in Germany 
Knowledge of 
material 
Knowledge of various material 
properties 
Carbon can be completely 
recycled 
 
Since the stakeholders of each lifecycle phase have certain expectation, needs or re-
quirements the main question for the designer is how to design sustainable prod-
ucts/systems if the impacts on the lifecycle remain uncertain (Figure 1).  

982 
K. Lindow et al. 
 
Fig. 1. Relation between Sustainability demands and Sustainability performance 
3 
Goal and Scope 
To improve the described situation, an effective support of the designer is required. 
He has to be enabled to make decisions towards more sustainable products/systems 
within his given working environment under certain constraints (systems, resources, 
knowledge). The goal is the establishment of an assessment method which has the 
potential to have an impact on products/systems towards less environmental harmful 
characteristics. While aiming on this aspect a reduction of accuracy of the assessment 
has to be accepted. Given the required input data and manpower, the method can be 
adopted and used for other products as well. 
4 
Other Approaches (Examples) 
The complexity, and hence lavishness, of (product) assessment procedures is a com-
mon problem, in particular if the information is required in industrial product devel-
opment processes as temporal resources are increasingly scarce due to accelerated 
innovation cycles. Various methods attempt to address the aforementioned problem 
by reducing the extent of analysis of the environmental impact of systems. This re-
duction of complexity is achieved by different approaches, which can be allocated 
into the following categories (examples given): 
• Reduction of scope (i.e. the considered lifecycle phases) in full LCA approaches:  
─ Cradle-to-grave: from resource extraction (cradle) via primary use to disposal 
(grave), which excludes recycling or remanufacturing and additional use phases 
─ Cradle-to-gate: from resource extraction to the factory gate 
─ Gate-to-gate: production phase within the factory 
• Reduction of detail in full LCA approaches 
• Reduction to a single factor: 
System
Assembly
Part
Part
Part
Sub-
System
Part
Part
Use
Economic: low fuel consumption
Environmental: low CO2 emissions
Social: Warning for driver fatigue 
Manufacturing
Design
Service and 
Maintenance
End of Life 
Treatment
Economic: low production costs
Environmental: energy efficiency
Social: ergonomic friendly assembly
…
…

 
Decision-Making Support for Sustainable Product Development 
983 
─ Energy: 
o LCEA: Life-Cycle Energy Analysis 
o KEA/KEV: 
Cumulated 
energy 
consumption 
(Kumulierter 
Energie 
Aufwand/Verbrauch) [6] 
─ Material: 
o MIPS: Material Input Per Service unit [7] 
o Carbon Footprint CO2 or hazardous substances 
• Specification on single industries or product classes: 
─ GREET Model: Greenhouse gases, Regulated Emissions, and Energy use in 
Transportation 
─ Well-to-Wheel: Assessment of energy efficiency in vehicles 
• Simplified environmental evaluation [6] 
─ MET-Matrix: Material, Energy and Toxic substances in- and output 
─ Eco-Compass by DOW Europe 
─ Eco-Estimator by Philips 
5 
Knowledge-Based Approach 
5.1 
Goal and Concept 
The major function of the KBE System is the classification of the various properties 
and characteristics of a product (which can be a either a single part or a system) with 
respect to its sustainability. The environmental and social impacts over the entire 
lifecycle are considered. For this purpose, the system should ideally access to data-
bases to gather information about the sustainability context areas, i.e. manufacturing 
processes and equipment and the re-use purposes. Furthermore, the system evaluates 
both the designer’s intention and the stakeholder’s needs and requirements. It is clear 
that the necessary databases achieve a volume which must inevitably lead to a restric-
tion of the evaluation space. It is likely that computing power can be saved if the user 
of the system already provides information about the manufacturing processes, the 
purpose of use and the envisaged type of recycling/re-use. Additionally, the same 
product/system can have different effect on its sustainability due to different methods 
of processing and assembly in different countries. The KBE System’s inputs and out-
puts are the following: 
• Input 
─ CAD model (properties) and material characteristics 
─ Production site/location, location of use and recycling  
─ Design intention (sustainability context areas) 
─ Databases (knowledge areas for sustainability assessment, cp. Table 2) 
 
• Output 
─ Sustainability assessment of manufacturing, use phase and recycling 
─ Combined sustainability assessment 

984 
K. Lindow et al. 
Figure 2 provides an overview of the major functions of the KBE System and its in-
puts and outputs. The system is designed modular. There exists a modular inference 
machine for each assessment (manufacturing, use, recycling) and one for the com-
bined assessment. Modules can be expanded if necessary and replaced. Furthermore, 
the modules are designed to deal with any amount of information on each assessment. 
Therefore, the system repeats the evaluation steps for an arbitrary number of informa-
tion. Subsequently, the results of all assessments are allocated. 
 
Fig. 2. Major function of the Knowledge-based System 
5.2 
Task-Level Definition 
The highest stage of a KBE System is represented by the so-called task level. In this 
discipline-unspecific level a general statement about the task of the system is taken. It 
can be distinguished from six areas of responsibility of a system [2]. In addition, there 
are a number of solution-generating task types. These serve, i.e. to perform planning 
tasks or to develop models. The concept of knowledge engineering affords that the 
task definitions are done independently. Since the KBE System evaluates an object 
with respect to a superior goal implementation the following three steps have to be 
succeeded (see also Figure 3): 
1. The system assigns properties of the object to corresponding instances of classes 
2. The object is evaluated against the assigned instances and its inherent properties 
3. The object is evaluated in terms of individual assessment results 
To complete the first step the application of the task "classifying" is repeated until all 
classes of an object are assigned to an instance. The task of the second step consists of 
evaluating the object on the basis of data from the knowledge-base (Knowledge areas 
for sustainability assessment, Table 2). Within the third step the results of step two are 
allocated and an overall result is presented. 
Database input
• Environmental influences
• Manufacturing method
• Working conditions
• Utilization model
• Method for recycling
• Local knowledge
• Knowledge and material
Specification input
• Production site
• Location of use
• Location of recycling
• Design intention
Knowledge-based System
• Product/system breakdown into single parts
• Restrictions due to user specifications
• Link to knowledge databases (e.g. GABI)
• Application of restrictions and knowledge to parts
• Provision of a product/system review
Evaluation output
• Manufacturing 
assessment
• Use phase assessment
• Recycling assessment
• Combined assessment
Product input
• CAD model
• Material characteristics

 
Decision-Making Support for Sustainable Product Development 
985 
 
Fig. 3. Task definition and performance (three step approach) 
The CAD model and the other inputs (cp. Figure 2) are entered into the KBE Sys-
tem. Based on that, the system assigns an instance per class to the model. Figure 4 
provides an example for the classes and its instances. The modular design of the sys-
tem allows adding any other classes. 
 
Fig. 4. Example of the classification principle for the KBE System 
5.3 
Knowledge-Base 
The knowledge base is the basic element of every KBE System. Once the overall 
objectives of the system are defined the knowledge structure has to be defined in a 
way it can be processed in a KBE System. Initially, an abstract object contains all 
relevant and available information which is required to assess the object. The presen-
tation of the proposed concept which includes the relationships between the different 
objects as well is based on the notation of CommonKADS [2]. Figure 5 provides an 
overview of the necessary environment (relevant knowledge areas) for sustainability 
assessment. 
The state of the art provides the evaluation criteria (sustainability indicators) based 
on environmental impact models and health effects. Additionally, the ideal values of 
the evaluation criteria have to be defined. The data from the CAD model provide all 
necessary parameters to simulate the corresponding processes. Another important 
issue for sustainability evaluation is the country of origin. The production site  
 
Evaluation of an 
object
Classification by 
exclusion procedure
Assessment by using 
a knowledge base
Encryption of single 
assessments
Method
Task
Sub-task
1. Classification
2. Assessment of 
single classes
3. Overall 
assessment
Production type
Intended use
Recycling
Milling
Forging
Molding
Vehicle part
Gadget
Packaging
Composting
Reconditioning
Remanufacturing
Object
Classes
Instance
characteristic

986 
K. Lindow et al. 
 
Fig. 5. Environment (relevant knowledge areas) for sustainability assessment 
determines the available technology and defines its processes. Furthermore, the pro-
duction site is crucial for the applied legislation. Legal obligations regarding working 
conditions, i.e. maximum lifting loads, as well as environmental protection play a 
major role. That way, the same process has unlike characteristics in different coun-
tries, e.g. an air filter protection is mandatory in country A whereas the costs for 
emissions are lower in country B. Both the legislation and the state of technology 
limit the processes for production, use and recycling. The KBE System can, in turn, 
assign process models to production, use and recycling. That way, the process models 
can be simulated and evaluated against the pre-defined ideal concept. 
The state of art delivers comparative data to the system thus the system is enabled 
to assess the impact of individual process parameters. The description of this know-
ledge is a challenge due to the fact that there are no defined guidelines to make  
various health parameters, environmental impacts and other important indicators for 
sustainability comparable. Combined LCSA (Life-Cycle Sustainability Assessment,  
State of art
Parameters
Object
Description
Production site
Name
Legislation
Parameter 
Specifications
State of 
technology
Parameter 
Specifications
CAD model
Process models
Ideal concept
Environmental impact models
Medical knowledge
Material
Name
Density
Geom. structure
Properties
Tolerances
Process model
Parameter
Process model
Parameter
Process model
Parameter
Simulation rule
Calculation rules
Simulation rule
Calculation rules
Simulation rule
Calculation rules
Production type
Intended use
Recycling
limits
provides
information
subjected
to decision

 
Decision-Making Support for Sustainable Product Development 
987 
consisting of Life-Cycle Assessment, Life-Cycle Costing and Social Life-Cycle As-
sessment) approaches are already in focus of other research [8]. In fact, it can be  
assumed that there exists another KBE System or database which provides informa-
tion about the relationships between the different indicators. 
The KBE System of this research could request to the state of art "10kg of CO2 are 
emitted" and would receive a corresponding value of sustainability from the database 
in response. At the same time, the state of art has to be given an ideal value for each 
parameter to allow the formation of a decision. The database has knowledge about the 
effect for each parameter in terms of a sustainability value. This effect is related to a 
denomination which indicates the amount the sustainability value draws. To take up 
the example of the CO2 emissions, the following information has to be stored in the 
database: CO2 impact is 4.2 per 1.0kg. According to the state of knowledge ideal 
values for all parameters have to be stored. Moreover, it is necessary to provide a 
possibility to break down the individual impacts of resources to each of the parame-
ters. Since the average energy mix in different countries varies widely it is necessary 
to define this knowledge in conjunction with the country. The system can resolve a 
query of the type "3kg iron are processed" into "40kWh energy are consumed" and it 
concludes at the same time " 25kg of CO2 are produced + 0.0002g nuclear waste have 
to be discharged". These two parameters can be evaluated by the system and a state-
ment about the sustainability of iron production can be made according to the current 
CAD model, either if it is a single part or an assembly (system). 
6 
Conclusions and Outlook 
This research presents the current progress on the development of a KBE System that 
supports decision-making for sustainable product development. The following points 
are addressed within the paper: 
• Clarification of the evaluation criteria for the sustainability assessment 
• Concept development for the knowledge-based system 
• Establishment of a comprehensive knowledge base 
The next steps are: 
• Establishment of the inference levels 
• Implementation of the KBE System 
• Integration of the KBE System into a PLM (Product-Lifecycle Management) Sys-
tem 
• Integration of existing databases (sustainability indicators and data) 
• Application and testing within industry 
One of the important steps is to define the inference levels. At this level the tasks of 
the KBE System are broken down to the individual logical units of the solution 
process. That way, the inference structure represents the determined linking of know-
ledge with each task level at the knowledge-base. Based on this research an applica-
tion for a PLM system will be developed. PLM systems are a core component of the 

988 
K. Lindow et al. 
system landscape in product development environments and represent a standard for 
industrial applications such as in the automotive or aerospace industry. The applica-
tion includes the development of a Virtual Database approach, which is extended by 
the KBE System ontology as a data model. This allows the modeling of logical state-
ments (e.g. rules) and the use of logical query languages (e.g. SPARQL). With the 
help of the application design engineers will be assisted to develop sustainable and 
energy-efficient products/services. 
The engineer’s primary task in industry is still not to design sustainable prod-
ucts/systems. For now his dominant assignment is designing products which ensure a 
high return on investment. Thus no matter how well the support for the designer will 
be, his priorities still are not in favor of sustainable products. In order to give to the 
sustainability dimension an appropriate, i.e. manageable place and weight, engineer-
ing IT systems like PLM systems should be adopted to support to the multidimen-
sional nature of engineering decisions. 
Acknowledgments. This research is supported by the German Research Foundation, 
SFB 1026. We extend our thanks to all who contributed in preparing and executing 
the project. Sincere thanks to Marzia Traverso and Erwin Schau from the Department 
of Sustainable Engineering for the lively exchange of knowledge. 
References 
1. Li, W., Winter, M., Kara, S., Herrmann, C.: Eco-efficiency of Manufacturing Processes. 
CIRP Annals - Manufacturing Technology 61(1) (2012) 
2. Schreiber, G., Akkermans, J.M., Anjewierden, A.A., de Hoog, R., Shadbolt, N.R., Van de 
Velde, W., Wielinga, B.J.: Knowledge Engineering and Management: The Common-KADS 
Methodology. MIT Press, Cambridge (1999) 
3. Mulder, K., Ferrer, D., Van Lente, H.: What is sustainable technology. Greenleaf Publish-
ing Limited, Sheffield (2011) 
4. National Institute for Environmental Research. Sustainable Development Indicator (SDI) 
Database (2012), http://www.nies.go.jp/sdi-db/search_e.php 
5. Inoue, M., Lindow, K., Stark, R., Ishikawa, H.: Application of Sustainable Aspects to the 
Set-based Design Method. In: Proceedings of the 11th International Design Conference, 
DESIGN 2010, Dubrovnik, Croatia (2010) 
6. Tischner, U.: Was ist EcoDesign? Ein Handbuch für ökologische und ökonomische Gestal-
tung. Verl. form (Praxis), Frankfurt am Main (2000) 
7. Schmidt-Bleek, F., Bierter, W.: Das MIPS-Konzept. Weniger Naturverbrauch - mehr Le-
bensqualität durch Faktor 10. Droemer Knaur, München (2000) 
8. Finkbeiner, M., Schau, E.M., Lehmann, A., Traverso, M.: Towards Life Cycle Sustainabili-
ty Assessment. Sustainability 2(10) (2010) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 989–997. 
DOI: 10.1007/978-3-642-30817-8_97 
© Springer-Verlag Berlin Heidelberg 2013 
 
Industry Requirements for an Assistant System 
Supporting Energy-Efficient Product Development  
in the Automotive Supply Industry 
Michael Abramovici, Akamitl Quezada, and Thomas Schindler 
Chair of Information Technology in Mechanical Engineering (ITM), Ruhr-Universität Bochum, 
Universitätsstraße 150, Bochum, D-44801, Germany 
{michael.abramovici,akamitl.quezada,thomas.schindler}@itm.rub.de 
Abstract. Today the development of energy-efficient products throughout their 
entire lifecycle is more than a major issue for environmental and economic con-
cerns, it also represents a high business potential for companies. 
Unfortunately, a complete integration of energy-efficiency aspects into in-
dustrial product development practice has not been achieved yet. This is partly 
due to a lack of simple methods and tools for the evaluation of energy-
efficiency and costs of design alternatives, as well as checking their compliance 
with regulatory constraints. A possible solution to this problem is an assistant 
system adapted to the designer’s working environment. This paper describes the 
main requirements in the automotive supply industry for such an IT support. 
The requirements list was first obtained from an analysis of industrial and 
research publications and the most relevant regulations related to the energy-
efficiency of products. This list was then weighted, extended, and validated 
through interviews with design experts from the industry. 
Keywords: Assistant system, product development, energy-efficiency. 
1 
Introduction 
In economics, efficiency is defined as achieving maximum output from a given level 
of resources (input) used to carry out an activity [1]. In this context, energy is consi-
dered a resource, contrary to the concept used in physics: energy can be “created” or 
“lost”. The term energy-efficiency comprises the attainment of maximum useful out-
put from a given energy input, or the use of minimum energy input to obtain a given 
useful output. A social view of energy-efficiency may consider energy savings to be 
an efficiency gain, e.g. when taking the stairs instead of using an elevator. A more 
technical view may consider savings as conservation rather than efficiency improve-
ment. Today the development of energy-efficient products throughout their entire 
lifecycle is an important issue for environmental and economic concerns. This is 
mainly motivated by the following drivers: 
 

990 
M. Abramovici, A. Quezada, and T. Schindler 
Ecological Drivers. Nowadays, climate change is the most important environmental 
problem, where greenhouse emissions take the most crucial role [2]. Energy-related 
emissions are responsible for an important part of these emissions [3]. 80% of future 
environmental impacts of a product are defined in the product development lifecycle 
phase [4]. According to the International Energy Agency (IEA), the increase of ener-
gy-efficiency will play an equivalent role for climate protection as the sum of renew-
able energies and other environmentally friendly measures [3]. 
 
Economic Drivers. Especially over the last ten years, energy prices have been in-
creasing considerably [3]. According to the European Commission (EC), energy-
efficiency is the most cost-effective and fastest way to increase security of the energy 
supply needed for economic growth in the European Union [5]. Furthermore, energy-
efficient products represent a high business potential for companies. On the one hand, 
companies can achieve important savings by improving the energy-efficiency of their 
products in the different product lifecycle stages. According to a study in German 
companies, 80% of product costs are defined in the product development stage [6]. 
On the other hand, companies achieve a greener image. Energy-efficiency and savings 
benefit the economy as a whole, the public sector, businesses, and private individuals 
[5] [7]. 
 
Legislative Drivers. As the result of environmentally friendly governmental policies, 
an important number of standards (norms, labels, guidelines, etc.) related to the ener-
gy-efficiency of products have been developed or are under development all over the 
world [4] [5] [8]. 
Unfortunately, a complete integration of energy-efficiency aspects into industrial 
product development practice has not been achieved yet (cf. chap. 2.1). Figure 1 out-
lines the main stages for the development of an assistant system supporting energy-
efficient product development. The assistant system first and foremost focuses on the 
requirements of the automotive supply industry. 
 
 
Fig. 1. First steps towards an assistant system supporting energy-efficient product development 
in the automotive supply industry 
The requirements for the assistant system have been defined in three main steps. 
First a detailed literature analysis has been carried out (cf. chap. 2.1). Based on the 
analysis, a list of main preliminary requirements has been defined (cf. chap. 2.2). The 
main preliminary requirements have then been validated through a survey in the  
automotive supply industry (cf. chap. 3). 

 
Industry Requirements for an Assistant System 
991 
2 
Preliminary Requirements 
2.1 
Literature Analysis 
This chapter summarizes the results of an analysis of industrial and research publica-
tions. The detailed analysis has been presented by the authors in a previous publica-
tion [9]. The goal of the analysis was to determinate a list of preliminary requirements 
for IT support environmentally friendly product development with special regard to 
energy efficiency.  
Environmentally friendly engineering IT support proposes nowadays the major so-
lutions for the development of energy-efficient products. This type of IT support can 
be divided into two main categories [9]: Eco design specific IT tools and environmen-
tally friendly modules in general purpose software such as specific modules in Enter-
prise Resource Planning (ERP) systems, Computer Aided Design (CAD) software, or 
Product Lifecycle Management (PLM) software. Specific eco design IT support 
presents one or more facets of which the most important are: the definition of strate-
gies, the analysis of product alternatives, the management of specific data, the com-
pliance with standards, and e-learning.  
Approaches for strategy definition consider energy efficiency and are interesting 
for early design stages [7] [10] [11]. Unfortunately, IT support in this area does not 
sufficiently exchange information with other systems [9]. The amount of time and 
efforts needed for collecting information is considerable. 
In spite of important progress in the field of IT support for environmental assess-
ment, there is a lack of solutions that support designers in finding and substantiating 
solutions after the assessment [9]. Life Cycle Assessment (LCA) software today 
represents the most comprehensive support for energy-efficient product development, 
even if it is not specific for design [9]. 
It is important for a reliable energy efficiency assessment to access and collect 
quality data from appropriate sources [12]. Unfortunately, there is currently a lack of 
solutions for the integration of different systems such as CAD, ERP, PLM etc. [9]. 
Furthermore there is a need for solutions for automated and permanent updating of 
data [12] [9].  
With regard to compliance checking of products with regulatory constraints, current 
IT solutions are focused on integrating norms, labels etc. into databases. The most ad-
vanced solutions are proposed by LCA software [13]. Furthermore, some current re-
search projects deal with the facilitation of the use of standards [8] [14]. Unfortunately, 
support in verifying compliance of the design alternatives is still poor [9]. 
IT solutions that support eco design learning (e-learning) are rare and very limited. 
The most advanced support in this area is LCA academic software [9].  
The most used IT support in product development practice is commercial general 
purpose engineering software (CAD, ERP, PLM etc.). It thus generates a high poten-
tial support for the development of energy-efficient products. At present, however, it 
does not sufficiently deal with energy efficiency aspects [9] [15].  
To enable the designer to consider energy efficiency aspects throughout the entire 
lifecycle of a product, CAD software is capable of providing useful information aside 
from material, volume, and geometric data [16]. Currently, some CAD modules pro-
vide relatively simple proposals e.g. concerning material choice [17]. Other solutions 

992 
M. Abramovici, A. Quezada, and T. Schindler 
comprise giving advice about different design alternatives in various scenarios via 
dashboards [18].  
Since data of items and manufacturing processes is often handled in industrial ERP 
appliances, it represents an important source of product energy-efficiency data. In the 
early 1990s, first ideas came up to integrate LCA software into ERP software. Today, 
this is carried out only partially as a result of research projects [9].  
Due to the fact that PLM solutions facilitate the collection of information with the 
support of authoring systems (CAD, CAE, CAM and office systems) [19], it 
represents a high potential support for the development of energy-efficient products 
[9]. Unfortunately, the management of energy data is currently not sufficiently inte-
grated into PLM environments. Furthermore, like other IT solutions, most of the data 
has to be integrated manually and the quality of the results depends on the accuracy of 
the database [9]. 
2.2 
Preliminary Requirements Definition 
Based on the literature analysis in chap. 2.1, a list of preliminary requirements for an 
assistant system supporting the development of energy-efficient products has been 
defined. The most important functional requirements are: 
• Automatically collect information from different systems (e.g. CAD, PDM/PLM, 
ERP, etc.). 
• Constantly and automatically update relevant databases for energy and cost as-
sessment, as well as for compliance checking. 
• Support simple visualization of the results (e.g. markup of critical elements in 3D 
models). 
• Allow a simple comparison of the states of the design alternatives before and after 
changes. 
• Automatically search and recommend useful alternatives. 
• Include an e-learning module. 
• Perform compliance checking of the design alternatives with regulatory  
constraints. 
In addition to the above functional requirements, the following main general require-
ments are included in the list: 
• Consider the entire product lifecycle of a product. 
• Always evaluate the energy-efficiency of design alternatives in relation to costs. 
• Extend the assistant system to cover other design aspects like safety. 
• The system should be user-friendly. 
• Integration into an existing IT tool (e.g. CAD, PDM/PLM, ERP, etc.). 
In order to verify whether the above-mentioned main functional and general prelimi-
nary requirements coincide with the real needs in current industrial product develop-
ment practice, a survey has been conducted in the automotive supply industry. The 
central questions were: 
 

 
Industry Requirements for an Assistant System 
993 
- 
How important are the requirements to the designers? 
- 
What are the particular requirements regarding an assistant system to support 
energy-efficient product development in the automotive supply industry? 
3 
Requirements Validation  
3.1 
Validation Approach 
The approach used for the validation of the preliminary requirements (cf. chap. 2.2) 
has been carried out in two main stages:  
Development of a Survey Concept. In order to answer the central questions men-
tioned in chap. 2.2, a survey based on a list of more detailed questions (questionnaire) 
for design experts from the automotive supply industry has been chosen as the most 
suitable way to validate the requirements. The two main advantages of this type of 
survey are: it allows a statistical analysis of the results, and it can be carried out both 
by personal interviews and online. The questionnaire was first drawn up in coopera-
tion with a selected group of design experts from the automotive supply industry. The 
questions dealt among others with the following topics: the weighting of the prelimi-
nary requirements, consideration of regulations related to energy-efficiency (including 
labels and methods from norms and guidelines), the consideration of the lifecycle, and 
the consideration of specific product features and characteristics with regard to ener-
gy-efficiency aspects and costs. 
Execution of the Survey. The survey was carried out through personal in-depth and 
online interviews. In the second instance, in order to optimize its distribution and 
evaluation, the questionnaire was transferred to a PHP Surveyor browser-based  
survey tool. The completed questionnaires were evaluated via SPSS software. The 
survey was carried out with a large multinational company at several sites in Germa-
ny. The company is one of the major global players in the automotive supply industry. 
This supplier was chosen because of her large product range, presence in the global 
market, and her environmentally friendly policies (including energy-efficiency poli-
cies). For these reasons, the current results of the survey can be considered representa-
tive for the automotive supply industry. A total of 138 experts from the different 
company sites on product development weighted, extended, and commented the pre-
liminary requirements list. About one fifth of them submitted their opinions and 
comments via personal interviews, and the rest participated in the online survey.  
Approximately 70% of the experts were experienced designers, 30% of which top 
managers. The results of the survey will be described and analyzed in the following 
chapters. 
3.2 
Main General Findings 
One of the most important topics of the survey was the question: Into which kind of 
software should the assistant system be integrated? The results confirm with the con-
sent of approx. one third of the experts that the integration into CAD software is the 
most favored option. Nevertheless, PLM/PDM software is almost at the same level of 

994 
M. Abramovici, A. Quezada, and T. Schindler 
desirability. An unexpected result was that around 25% of the interviewees prefer an 
assistant as independent software. Another interesting result was that energy-
efficiency claims the sixth priority position among the aspects to be considered during 
product development, just after function assurance, costs, quality, manufacturability, 
and safety, and before transportation, maintenance, recycling, and others. 
Depending on their characteristics, products may present important differences, 
even within the same company. It is a very ambitious project to develop an assistant 
system compatible with existing general purpose software (CAD, PLM, etc.) and 
capable of automatically collecting real and virtual data for energy-efficiency and cost 
assessment, and congenial to all types of products. The assistant system should first 
and foremost serve as a calculation tool. The assessment should consider at least the 
perspectives of the company and those of her customers. 
Compliance checking of design alternatives with regulatory constraints (laws, 
standards, labels, etc.) is an important requirement but difficult to implement. In part 
this is due to the same reasons mentioned in the previous paragraph, and also as regu-
latory constraints are specific to each individual country, company (e.g. internal  
standards), and product. 
To solve the problems mentioned in the last two paragraphs, prior to its first utili-
zation, the assistant system would require configuration according to the specific 
processes of the company and the specific characteristics and constraints of the prod-
uct or product family to be analyzed. This can be accomplished by developing ade-
quate IT solutions to create models of products and product-related processes for the 
calculations. The models should be based on real and virtual data which require  
permanent updating. 
3.3 
Final Weighted Requirements List 
One of the most important survey topics was the weighting of the requirements. The 
industrial experts on product development were asked to classify the preliminary  
requirements according to their degree of importance. For this particular item, the 
survey concept has been developed to allow experts to add requirements and give 
feedback. The most common comments included: 
User friendliness is the most important requirement. The system should include 
e.g. a simple visualization of the results such as by color marking the critical parts of 
a product with different components within a CAD model.  
The system should always evaluate the energy-efficiency of design alternatives in re-
lation to costs, first and foremost, from a company perspective, then from a customer 
perspective. These two perspectives are particularly important for the automotive supply 
industry since their primary focus is on the energy-efficiency of the final vehicle. 
With regard to the consideration of the different lifecycle phases, the production 
and the use phases have almost the same relevance, and they are the most important 
stages to be considered. The disposal/recycle phase has less relevance. An unexpected 
result of the survey was that the last place of importance was attributed to the pro-
curement phase. In conclusion, the below table summarizes the weighting, opinions 
and comments made by industrial experts:  

 
Industry Requirements for an Assistant System 
995 
Table 1. Final weighted requirements list 
Requirements 
Priority according to 
Experienced 
designers 
Design 
managers 
functional 
Automatically collect information from different 
systems (e.g. CAD, PDM/PLM, ERP, etc.). 
2 
3 
Constantly and automatically update relevant 
databases for energy and cost assessment, as 
well as for a compliance checking. 
2 
2 
Support simple visualization of the results (e.g. 
markup of critical elements in 3D models). 
2 
2 
Allow a simple comparison of the states of the 
design alternatives before and after changes. 
2 
1 
Automatically search and recommend useful 
alternatives. 
2 
2 
Include an e-learning module. 
3 
2 
Perform compliance checking of the design 
alternatives with regulatory constraints. 
2 
2 
general 
Consider the “procurement” product lifecycle 
phase. 
4 
2 
Consider the “production” product lifecycle 
phase. 
2 
1 
Consider the “use” product lifecycle phase. 
2 
1 
Consider the “disposal/recycle” product life-
cycle phase. 
2 
3 
Always evaluate the energy-efficiency of design 
alternatives in relation to costs. 
1 
1 
Extend the assistant system to cover other de-
sign aspects like safety. 
4 
4 
The system should be user-friendly 
1 
1 
Integration into an existing IT tool (e.g. CAD, 
PLM/PDM, ERP)  
3 
2 
(1: highest priority, 4: lowest priority) 
4 
Outlook 
The results presented in the paper in hand concern the automotive supply industry 
exclusively. In order to extend the number of opinions provided by industrial experts 
from other branches, the interviews will be continued in the context of a study, the 
focus of which will be the evaluation of the state of integration of energy-efficiency 
aspects into product development practice. 
Considering the final requirements list and its weighting (cf. chap. 3), the devel-
opment of a prototype for the assistant system has started. It will be focused on a 

996 
M. Abramovici, A. Quezada, and T. Schindler 
rough assessment for early product development stages of mass products. The general 
concept consists of the following modules: 
Data Integration Platform for Energy and Cost Assessment. The platform collects 
and process information from different software such as CAD, ERP, and PLM. This 
concerns real and virtual lifecycle data of previous product generations (including 
processes), similar or alternative products and alternative materials, or product-related 
processes, as well as information about regulatory constraints. The data will be stored 
in a common database and permanently updated. 
Assessment of Design Alternatives. The main tasks of this module are energy-
efficiency and cost assessment of the current design alternatives. This will be  
supported by methods such as Cumulative Energy Demand (CED) and Life Cycle 
Costing (LCC). Its second task is a compliance checking of the design solutions with 
regulatory constraints. 
Frontend. The current idea is to present the assessment results in a CAD environ-
ment. Critical elements will be marked in 3D models with different colors. Detailed 
information based on adapted indicators will be available in special windows.  
Figure 2 below provides a general overview of the concept for the assistant system. 
 
Fig. 2. General overview of the assistant system concept 
Acknowledgements. We would like to thank the Graduate School of Energy Efficient 
Production and Logistics (Ruhr-Universität Bochum and TU Dortmund) for their 
support. 

 
Industry Requirements for an Assistant System 
997 
References 
1. Organisation for Economic Co-operation and Development (OECD) - Glossary of statis-
tical terms, http://stats.oecd.org/glossary/index.htm 
2. UNEP: Annual report 2010. UNEP, Nairobi (2011) 
3. Pehnt, M.: Energieeffizienz - Ein Lehr- und Handbuch. Springer, Heidelberg (2010) 
4. Großmann, J.: Die EuP-Richtlinie und ihre Potenziale für die Entwicklung umweltgerech-
ter Produkte. VDI Verlag GmbH, Düsseldorf (2009) 
5. European Commission (EC): Proposal for a Directive COM/2011/370 on energy efficien-
cy. EC, Brussels (2011) 
6. Energieeffizienz in der Produktion – Untersuchung zum Handlungs- und Forschungsbe-
darf, Fraunhofer Gesellschaft, Germany (2008) 
7. Wimmer, W., Lee, K., Quella, F., Polak, J.: Ecodesign - The Competitive Advantage. 
Springer, Heidelberg (2010) 
8. Rath, K., Röder, B., Birkhofer, H., Bohn, A.: Standardized requirement acquisition 
through clustering: a tool for energy efficient products. In: Proceedings of the DESIGN 
Conference 2012, Dubrovnik, pp. 1081–1090 (2012) 
9. Abramovici, M., Quezada, A., Schindler, T.: IT support for eco design - state of the art and 
research requirements. In: Proceedings of the GCSM Istanbul 2012. Springer (2012) 
10. Wimmer, W., Züst, R., Lee, K.: Ecodesign Implementation. Springer, Heidelberg (2004) 
11. UNEP, TU Delft, Design for Sustainability - A step-by-step approach, UNEP, Paris, 
France (2009) 
12. Netzwerk Lebenszyklusdaten: Projektbericht – Studien zum AK Produktentwicklung / 
IPP. Forschungszentrum Karlsruhe GmbH, Karlsruhe (2007) 
13. Wagner, H.-J.: Energieaufwendungen, Ökobilanzierung und Umwelt. In: Lecture Notes 
Chair Energy Systems and Energy Economics. Ruhr-Universität Bochum, Bochum (2011) 
14. Vezzoli, C., Manzini, E.: Design for Environmental Sustainability. Springer, London 
(2008) 
15. Reichel, T., Rünger, G., Steger, D., Xu, H.: IT-Unterstützung zur energiesensitiven Pro-
duktentwicklung. TU Chemnitz, Chemnitz (2010) 
16. Russo, D., Matina, D.: Structural optimization and LCA for a Computer Aided sustainable 
design. In: Proceedings of the ASME Houston 2012 (2012) 
17. Russo, D.: A Computer Aided Strategy for More Sustainable Products. In: Cavallucci, D., 
de Guio, R., Cascini, G. (eds.) Building Innovation Pipelines through Computer-Aided In-
novation. IFIP AICT, vol. 355, pp. 149–162. Springer, Heidelberg (2011) 
18. Lindow, K., Woll, R., Inoue, M., Ishikawa, H., Stark, R.: Approaches for sustainability as-
sessment in the conceptual design phase. In: Proceedings of the 22nd CIRP Design Confe-
rence, ch. 36 (2012) 
19. Eigner, M., Stelzer, R.: Ein Leitfaden für Product Development und Life Cycle Manage-
ment. Springer, Berlin (2009) 

M. Abramovici and R. Stark (Eds.): Smart Product Engineering, LNPE, pp. 999–1008. 
DOI: 10.1007/978-3-642-30817-8_98 
© Springer-Verlag Berlin Heidelberg 2013 
 
Early System Simulation to Support 
EcoDesign of Vehicle Concepts 
Fabio Dohr, Pascal Stoffels, and Michael Vielhaber 
Institute of Engineering Design, Saarland University, Saarbrücken, 66123, Germany 
stoffels.pascal@googlemail.com, 
{dohr,vielhaber}@lkt.uni-saarland.de 
Abstract. Environmental restrictions request new – potentially revolutionary – 
mobility concepts which simultaneously address both user requirements and ef-
ficiency aspects. Early system simulation within the development process is 
able to support new development projects on the conceptual level. In an overall 
effort to support ecodesign as early as possible in the development process, this 
paper introduces a process model for ecodesign-oriented product development 
which focuses on these specific aspects. Special focus is set on the use of mod-
eling and simulation in the conceptual design phase to evaluate and compare the 
energy efficiency and lifecycle assessment on the example of vehicle concepts. 
Keywords: ecodesign, system simulation, vehicle concepts, simulation-based 
ecodesign. 
1 
Introduction 
Today environmental restrictions have become a very important aspect in product 
development – especially in the vehicle industry. Hence there are a lot of activities 
regarding environment-oriented design (ecodesign) in general and new mobility and 
drive concepts in particular, leading to completely new designs for automobiles and 
other vehicles. 
As a consequence the traditional evolutionary development process from one car 
model to the next is not sufficient anymore to create new revolutionary concepts and 
to evaluate these early in the process. It is well known, however, that – especially in 
projects with new development character – the early phases are of outstanding impor-
tance for the product’s success regarding cost, quality and environmental impact. The 
use of modeling and simulation has proven to be an appropriate tool for early evalua-
tion since it allows system optimization long before any hardware is detailed or even 
produced. 
Today, a lot of effort is spent on the energetic optimization of vehicle concepts. 
There are various aspects to be considered – e.g. finding the optimal size and opera-
tional strategy for an electric motor of a hybrid drivetrain. 
In an overall effort to support ecodesign as early as possible in the design process, 
this paper introduces a process model for ecodesign-oriented product development 

1000 
F. Dohr, P. Stoffels, 
which focusses on the spec
use of modeling and simula
energy efficiency and lifecy
This paper is structured 
simulation-based product de
other hand and derives need
chapter 3 introduces a con
cases for its application. A
sented approach. After a d
outlook on future work. 
2 
Background 
Simulation-based product d
work presented in this pape
discussed. Thus integration 
2.1 
Simulation-Based P
Product development gene
company-specific or gener
production-ready product d
[1] and [2], modeling and s
regarding the huge potentia
[3]. Thus a new process mo
eling and simulation in the 
the product creation process
Fig. 1. Mode
and M. Vielhaber 
cific aspects mentioned. Special focus has been set on 
ation in the conceptual design to evaluate and compare 
ycle assessment. 
as follows. Chapter 2 describes the background regard
evelopment on the one hand and ecodesign concepts on 
ds for action regarding the integration of both fields. Th
ncept for simulation-based ecodesign, providing also 
A prototypical realization illustrates the power of the p
discussion, chapter 4 draws conclusions and provides
development and ecodesign provide the background of 
er. In both areas, new approaches are being developed 
of both fields requires further attention. 
Product Development 
erally follows process frameworks which guide throu
ric process steps and milestones from the first ideas t
design. Especially in academic process frameworks such
simulation does not play the significant role that it deser
als it offers for efficient and effective product developm
odel has been developed by the authors that positions m
core of the engineering environment of the innovation 
s, see Figure 1. 
 
eling and simulation within product development 
the 
the 
ding 
the 
hen, 
use 
pre-
s an 
the 
and 
ugh 
to a 
h as 
rves 
ment 
mod-
and 

 
Early System Simulation to Support EcoDesign of Vehicle Concepts 
1001 
Especially in new development type projects, early system simulation can play a 
predominant role to accelerate the knowledge buildup and to increase early concept 
maturity. 
Typically simulations are kept on system level in this early stage of the process. 
The main purpose is to gain information about the overall system behavior. And since 
design parameters are still very vague during conceptual design, simulation results 
cannot be as exact as during detailed design. Hence it is mainly used to compare dif-
ferent concepts in order to find the best solution. Those simulations mainly concen-
trate on the compatibility of different partial solutions and on the generation and  
refinement of dimensioning criteria. Suitable tools for that purpose are domain-
spanning system level environments like Modelica [4] based programs. The use of 
Modelica is further elaborated on in section 3.3. 
Ecological criteria and especially lifecycle aspects are difficult to be considered 
systematically during the choice of a vehicle concept. Only in later phases when the 
basic concept is clear, simulations are used to estimate the energy consumption. For 
this purpose modeling is done on a high level of detail. Hence respective tools are 
highly specialized and as a consequence require a lot of effort for modeling of the 
system. An example of such a tool is AVL Cruise [5]. 
For the purpose of very early simulation with an ecodesign focus, it is essential to 
cover a broad range of different vehicle concepts – i.e. from battery electric cars  
to combustion cars and the corresponding hybrid variants. The properties of interest in 
this case might be the best compromise between the vehicle size – corresponding to 
the weight – and the required power or expected range. In order to get information 
about the entire lifecycle and the holistic environmental impact, it is not sufficient to 
merely simulate the energy consumption during the utilization. Instead it is important 
to also consider production and recycling of the entire vehicle. 
2.2 
EcoDesign 
With environmental aspects exponentially gaining in importance, ecodesign has come 
up as a prominent subdiscipline of engineering design. Having now become interna-
tionally standardized, ecodesign is defined as the “integration of environmental  
aspects into product design and development, with the aim of reducing adverse envi-
ronmental impacts throughout the lifecycle” [6]. Ecodesign research and activities are 
underway in multiple directions – see Figure 2. Besides laying the strategic grounds, 
methods are being developed to support transparency regarding environmental ef-
fects, environment-oriented process models are being proposed, methods and tools set 
up, e.g. [7], and implementation concepts for ecodesign approaches developed, e.g. 
[8], [9]. In the context of this paper, especially transparency methods, process models 
and supporting tools are relevant. 
Transparency Methods. A multitude of transparency methods has been developed to 
support the analysis and assessment of environmental effects of products. Prominent 
examples in this context are the cumulative energy demand analysis (KEA) [10], 
which focusses on calculating the energy consumption along the entire lifecycle, the  
 

1002 
F. Dohr, P. Stoffels, 
global warming potential (
house gas emissions, and th
holistic analysis and assess
cation purposes – both eff
reduced to the utilization ph
Process Models. Process m
of environment oriented me
guidance on which method 
ment methods playing an i
ronmental assessment alrea
based on the analysis of a fe
Supporting Tools. With a
assessment – requiring com
been developed to support t
later design or lifecycle pha
simulation tools offer comp
velopment process, see sect
AVL Cruise offer modelin
energy consumption estima
plexity, they are mainly ap
prehensive vehicle models. 
the analysis and improveme
related guidance. 
2.3 
Needs for Action 
Although diverse approach
tion or environment-cons
simulation-based ecodesign
oriented simulation support
the European Union sets b
and M. Vielhaber 
 
Fig. 2. Dimensions of ecodesign 
(GWP) assessment [11], which addresses lifecycle gre
he lifecycle assessment (LCA) [12], which focusses on 
sment of all environmental aspects. However, for simpl
fort- and marketing-driven – practical application is of
hase of the lifecycle. 
models such as ISO/TR 14062 [13] focus on the integrat
ethods into the product development process. They prov
and approach to apply in which process step, with asse
important role. Birkhofer [14] e.g. proposes detailed en
ady in the specification and early conceptual phase, thou
easible reference product. 
analysis and assessment methods – such as the lifecy
mprehensive data and complex calculations, IT tools h
this process. Generally, these tools are applied reactively
ases and used as a basis for design iterations. Modeling 
prehensive and powerful techniques along the product 
tion 2.1. Specialized tools for vehicle development such
ng and simulation functions for driving performance 
ations for the use phase of the lifecycle. Due to their co
pplied for detailed drivetrain optimizations based on co
More hands-on tools such as Ecodesign Pilot [15] supp
ent of existing products by providing generic environme
hes and tools exist to support either modeling and simu
scious design, no integrated concept is available 
n, which requires early system development and lifecy
t on a high conceptual level. On the example of mobil
broad goals of 60-80% energy savings over the next f
een-
the 
lifi-
ften 
tion 
vide 
ess-
nvi-
ugh 
ycle 
have 
y in 
and 
de-
h as 
and 
om-
om-
port 
ent-
ula-
for  
ycle 
lity, 
few 

 
Early System
decades [16], leading to th
traditional evolutionary dev
able to cope with these ne
high importance for projec
mine further process steps 
modeling and simulation o
tion about the detailed desig
Thus, in the following c
design will be presented. I
and simulation techniques,
very early development stag
3 
Concept 
The early phases of product
cations and concepts and im
crease the maturity as fast 
For this, modeling and sim
applied to the example of v
powered bicycles to light el
3.1 
Simulation-Based E
Modeling and simulating e
tionary solutions – requires
until sound information is g
emplary functional structur
the whole variety from hum
tives and utilization modes
phase, it has to be holistic a
Fig. 3. Concept development 
depicted in figure 1) 
m Simulation to Support EcoDesign of Vehicle Concepts 
1
he demand for revolutionary new vehicle concepts. T
velopment process from one car model to the next is 
ew circumstances anymore. Especially early phases are
cts with new development character since those prede
and design decisions. In order to choose the right conc
offer great potential to compare concepts without inform
gn. 
chapter, a conceptual framework for simulation-based e
It has to be supported by efficient and effective model
 which allow concept assessments on a lifecycle basis
ges. 
t development are characterized by uncertainties in spec
mmaturities in solution principles. The main goal is to
as possible based on fast but sound concept evaluatio
mulation offer the techniques which will subsequently
vehicle concepts – in a broad understanding from hum
lectric vehicles to traditional combustion engine cars. 
EcoDesign 
early product concepts – especially when targeting revo
s keeping the decision space as wide and open as possi
gained to discard inferior concepts. Figure 3 shows an 
re for mobility products with solution principles includ
man power to fuel and electric power, of car body alter
s. If modeling and simulation shall be used in this ea
and open enough to cover this variety of options. 
and simulation of vehicle concepts (detailed excerpt of proc
1003 
The 
not 
e of 
eter-
ept, 
ma-
eco-
ling 
s in 
cifi-
 in-
ons. 
y be 
man-
olu-
ible 
ex-
ding 
rna-
arly 
 
cess 

1004 
F. Dohr, P. Stoffels, and M. Vielhaber 
In a first step the – generally still rough – requirements from the specification are 
mapped to a functional structure. Open question in the specification might be to 
which extent recuperation should be applied in electric drives or what the tradeoff 
should be between material selection and lightweight requirements. Then, depending 
on the requirements, valid solution principles are selected for further investigation. 
Already at this level, a model can be built up to run first simulations. The simula-
tion depth depends on the desired information – shall only the use phase be optimized 
or the entire lifecycle, and shall energy be in focus (KEA) or a holistic view on envi-
ronmental impacts (LCA), see figure 3. This modeling and simulation step has to be 
efficient to enable multiple iterations, so that the specification becomes more and 
more solid and the solution principles more and more sound, until a certain level of 
maturity is reached to allow for a sound concept decision. 
It becomes obvious that methods and tools which are currently available for  
lifecycle analysis and system simulation are too complex to allow for this process. 
Therefore those are not consistently applied in product development, especially not 
proactively and not in small or medium-sized companies.  
Hence, in order to realize the presented concept, a prototypical tool has been set up 
to cover the requirements mentioned on a high enough level to be fast and efficient in 
its application, and on a deep enough level to allow assessments with sufficient breath 
of scope and exactness of results. Exemplary use cases and the functionality of the 
tool will be described in the following chapter. 
3.2 
Use Cases 
The possible applications of the concept presented here and the modeling and simula-
tion tool are manifold. In the following, examples will be given to demonstrate the 
power of early system simulation if applied to vehicle concepts. 
Simulation of energy consumption during the utilization phase can support engi-
neers in finding the best combination for a hybrid drive – e.g. finding the best ratio of 
human and electric power in a human power hybrid vehicle (e. g. e-bike) under con-
sideration of different utility conditions and driving cycles, or determining the best-
sized electric motor for hybridizing a combustion drivetrain based on the resulting 
total efficiency. Also, the model can support determining the best strategy for recupe-
ration and driving strategies, e.g. for automatic transmission. 
Simulation of energy consumption over the lifecycle can help judge the use of 
lightweight materials, assuming energy being the major environmental impact of their 
application. Trading off the potentially far higher production energy of, e.g., CFRP or 
aluminum designs against the higher energy consumption during the use phase of 
higher-weight steel designs may lead to “energetic mileage break-even points” for the 
use of alternative materials.  
If environmental effects other than energy cannot be neglected, the described in-
clusion of simplified LCA estimations will empower the simulation even more to-
wards a holistic environmental assessment on early conceptual level. An example 
could be the evaluation of different battery types also based on the emissions and 
toxicity during their production and recycling. 

 
Early System Simulation to Support EcoDesign of Vehicle Concepts 
1005 
3.3 
Prototypical Realization 
In the specific case of vehicle concepts, a physical model has been developed for this 
purpose based on the object-oriented modeling language Modelica. It is suitable to 
describe the interdisciplinary interaction between electrical and mechanical compo-
nents. Furthermore the individual modeled objects can be easily connected, replaced 
or extended. In this specific case SimulationX [17] has been used because it offers a 
comprehensive library with standardized models for most of the components used in a 
vehicle. 
The model is built up modularly and has a separate graphical user interface so that 
the user can choose the mobility and drive concept and can easily parameterize the 
model without going into detail of the model itself. If some parameters are still un-
known at this time, the user can select preset values from literature or similar prod-
ucts. Currently the model contains all aspects of the utilization phase. Hence the user 
can choose any vehicle concept from a bicycle up to a full-size car as well as different 
types of drive from human powered to electric drives to combustion engines and nu-
merous hybrid combinations. Furthermore, the various energy sources like human 
power, battery, petrol, fuel cell or solar panel are implemented, too. The simulation 
can be started directly from the user interface based on the driving resistance which 
depends, among other things, on the driving cycle – which can be individually defined 
by the user. The results – e.g. energy consumption or the recuperated energy – are 
also illustrated in the user interface and can be stored in a separate file, e.g. for use in 
Microsoft Excel. Until now the model has been validated with data provided by sev-
eral automobile manufactures. The deviation of the results is below five percent 
which is very satisfactory considering the simplifications which had to be done to 
make the model applicable in early phases. 
As already mentioned, the model currently only contains the energy needs during 
utilization. Two system boundaries exist in here: tank-to-wheel (TTW) considers the 
energy just within the vehicle itself whereas well-to-wheel (WTW) includes the ener-
gy flow from the primary energy source, too. Presently the simulation is being ex-
tended in order to describe the complete product lifecycle of a vehicle, beginning with 
the production up to the recycling. Therefore for the materials used in an automobile, 
an average value for the energy needed in the production and recycling phase is pre-
defined. Combined with the lifetime of the vehicle, the total energy need for the entire 
lifecycle can be simulated.  
Figure 4 illustrates the structure of a model for a parallel hybrid vehicle as an ex-
ample. The different modules are connected via signals. The vehicle is built up start-
ing from the driving resistance to the wheel and the energy converters to the chemical 
and electrical energy storage. Each module in this energy flow reduces the efficiency 
of the overall system. 
The comparison of the calculated efficiencies and CO2 emissions between different 
vehicle concepts is shown in Figure 5, as an example for the results from the simula-
tion. The two system boundaries TTW and WTW are listed separately. Electric  
vehicles emit no carbon dioxide during driving. However in the energy mix the tradi-
tional power production causes CO2 emissions. The high mean energy effort for food 
production results in an efficiency factor of about 24 percent [18], based on data of a 
European country. Hence the carbon dioxide emissions for the bicycle are relatively 

1006 
F. Dohr, P. Stoffels, and M. Vielhaber 
high considering the WTW view. Therefore the usage of human muscles leads to 
higher energy consumption than electric drive systems. A conventional car with an 
internal combustion engine produces the highest emissions, which are not solely 
caused by the higher mass. The engine often works in the part-load range where the 
efficiency is very low. In contrast, electric drive systems are not as sensitive concern-
ing the load factor. 
 
Fig. 4. Structure of the parallel hybrid model 
 
Fig. 5. Comparison of the CO2 emissions for different types of vehicles (human power estima-
tions based on European nutrition averages) 

 
Early System Simulation to Support EcoDesign of Vehicle Concepts 
1007 
4 
Discussion and Conclusion 
The concept presented here proposes a process model that features a deep integration 
of modeling and simulation techniques into the early conceptual phases of new devel-
opment style projects. Postulating that environment-conscious design demands un-
conventional, potentially revolutionary solution principles and solutions based on a 
wide solution space, early system simulation can offer important support for concep-
tual decisions – if set up sufficiently open and comprehensive to allow for sound as-
sessments, yet simple enough to allow for fast assessments. 
The tool presented here offers simple but powerful support for environment-
oriented analysis and assessment on the example of vehicle concepts and thereby 
fulfills the criteria mentioned. It is obvious that conceptual models as the one pre-
sented here are limited regarding the product range they are able to cover. The exam-
ple, however, shows that it is possible to set up a model to run evaluations across the 
entire spectrum of mobility products with reasonable effort, but results keep accurate 
enough at this stage of the development process. 
Thus, concept and tool together enable the process model for simulation-based 
ecodesign as laid out in figures 1 and 3. 
With the approach presented in this paper focusing on early conceptual design, its 
applicability may be limited for detailed simulation and optimization tasks in later 
phases. Here, more detailed models and commercial software for both energy simula-
tion and lifecycle analysis may deliver more accurate results. The applicability may 
also be limited for other than new development type projects, where solution prin-
ciples and potentially even physical realizations are already set from the early phases. 
Further work will extend the model and tool as described in chapter 3.2 and 3.3. 
With the concept already existing, it will be possible to balance the environmental 
impacts in the utilization phase with the entire lifecycle and to trade off energy 
against other environmental effects before important conceptual courses are set, and 
long before any physical prototype hardware is created. In another ongoing activity, 
the model will be integrated into a driving simulator which is currently being devel-
oped by a partner institute. In this way it will be possible to optimize driving strate-
gies and to train drivers in a virtual environment without physical prototypes. 
To summarize, ecodesign is a field that calls for innovative concepts which have to 
be thoroughly analyzed and assessed already in early, conceptually immature process 
phases. A simulation-based process as presented in this paper, supported by pragmatic 
simulation solutions, can offer highly valuable support to reach early concept maturity 
as a basis for sound concept decisions. 
References 
1. Verein Deutscher Ingenieure: VDI guideline 2221 – Systematic approach to the develop-
ment and design of technical systems and products. VDI-Verlag, Düsseldorf (1993) 
2. Verein Deutscher Ingenieure: VDI guideline 2206 – Design methodology for mechatronic 
systems. VDI-Verlag, Düsseldorf (2004) 

1008 
F. Dohr, P. Stoffels, and M. Vielhaber 
3. Dohr, F., Vielhaber, M.: Toward simulation-based mechatronic design. In: Proceedings of 
DESIGN 2012, the 12th International Design Conference, Dubrovnik, Croatia, pp. 411–
420. Faculty of Mechanical Engineering and Naval Architecture, University of Zagreb, 
The Design Society, Glasgow (2012) 
4. http://www.modelica.org (last visit: September 20, 2012) 
5. http://www.avl.com (last visit: September 20, 2012) 
6. ISO 14006:2011 – Environmental management systems - guidelines for incorporating eco-
design, International standard, ISO (2011) 
7. Abele, E., et al.: EcoDesign – Von der Theorie in die Praxis. Springer, Berlin (2008) 
8. McAloone, T., Bey, N.: Environmental improvement through product development. DTU 
Technical University of Denmark, Copenhagen (2009) 
9. Wimmer, W., et al.: Ecodesign implementation. Springer, Berlin (2004) 
10. VDI 4600 – Cumulative Energy Demand - terms, definitions, methods of calculation. VDI 
Verlag, Düsseldorf (1997) 
11. PAS 2050:2011 – Specification for the assessment of the lifecycle greenhouse gas emis-
sions of goods and services. Publicly available specification, BSI (2011) 
12. ISO 14040:2006 – Environmental management - life cycle assessment - principles and 
framework, International standard, ISO (2006) 
13. ISO TR 14062:2002 – Environmental management - integrating environmental aspects in-
to product design and development. Technical report, ISO (2002) 
14. Birkhofer, H., Rath, K., Zhao, S.: Umweltgerechtes Konstruieren. In: Rieg, F., Steinhilper, 
R. (eds.) Handbuch Konstruktion. Hanser, München (2012) 
15. Wimmer, W., Züst, R.: Ecodesign Pilot: product investigation, learning and optimization 
tool for sustainable product development. Kluwer Academic Publishers, Dordrecht (2003) 
16. European Environment Agency: Laying the foundations for greener transport 
(TERM 2011). EEA report No 7/2011, EEA, Copenhagen (2011) 
17. http://www.itisim.com (last visit: September 24, 2012) 
18. Jungbluth, N.: Umweltfolgen des Nahrungsmittelkonsums, dissertation. de, Berlin (2000) 

Author Index
Abramovici, Michael
357, 633, 989
Adolphy, Sebastian
979
Adragna, Pierre-Antoine
137, 493
Albers, Albert
83, 181, 525
Albrecht, Katharina
1
AlGeddawy, Tarek
535
Ali, Salam
137
Allen, Stephen
825
Al-Zaher, Abdo
253
Ameri, Farhad
825
Amigo, Carolina Rom´an
169
Amini, Payam
765
Anderl, Reiner
1, 221, 231, 399, 969
Andolfatto, Lo¨ıc
315
Antoine, Jean-Francois
147
Apostolov, Hristo
725
Aschehoug, Silje H.
895
B¨ar, Thomas
293
B¨armann, Christoph
835
Bauer, Frank
273
Bauer, J¨org
593
Bawachkar, Deodatt
211
Becker, Juan Manuel Jauregui
33
Berglehner, Sophia
243
Bierer, Annett
957
Binz, Hansgeorg
103
Bittner, Martin
463
Bitzer, Michael A.
663
Blank, Thomas
283
Blessing, Lucienne
563
Bluntzer, Jean-Bernard
795
B¨onig, Jochen
463
Bornkessel, Thomas
969
Brad, Stelian
159
Braun, Andreas
181
Brinkmeier, Bernd
211
Brissaud, Daniel
845
Br¨okelmann, Jan
23
Brossog, Matthias
463
Brown, Ross
805
Burchardt, Carsten
61
Bursac, Nikola
525
Butt, Sajid Ullah
147
Cadavid, Angela
755
Campean, Felician
113, 191
Ceccacci, Silvia
915
Chevalier, Christophe
815
Christ, Alexander
969
Cicconi, Paolo
431
Dagman, Andreas
865
Damerau, Thomas
51
Damgrave, Roy
653, 787
Dantan, Jean-Yves
563
Denger, Andrea
71
Deuse, Jochen
303
Dim Nguyen, Von
493
Djaloeis, Raymond
515, 623
Dohr, Fabio
999
Douilly, Marc
315
Duckwitz, S¨onke
623
Duﬂou, Joost R.
505
Dufrene, Maud
845
Du Pasquier, Alex
815
Durupt, Alexandre
137
Earl, Chris F.
201
Ebel, Bj¨orn
181

1010
Author Index
Eckert, Claudia M.
201
Eigner, Martin
303, 725
Eilmus, Sandra
543
Eisenbart, Boris
563
ElMaraghy, Hoda
253, 535, 553
ElMaraghy, Waguih
253
Elverum, Christer W.
441
Ernst, Joscha
303
Erohin, Olga
303
Feldhusen, J¨org
515, 623, 745, 885
Fellner, Dieter W.
421
Fischer, Anath
345
Fischer, Christian
463
Fischer, J¨org W.
43, 211
Fleischer, J¨urgen
593
Frank, Gernot
613
Franke, J¨org
463
Frouin, Marc
335
Fuchs, Markus
463
Gausemeier, J¨urgen
23, 273
Gerhard, Detlef
71, 715
Germani, Michele
431, 915
G¨ohlich, Dietmar
377
G¨ohner, Peter
125
Gottwald, Philipp
685
G¨otze, Uwe
957
Haefner, Benjamin
283
Hanafy, Mohmmad
553
Haße, Armin
43
Hayka, Haygazun
51, 979
Hedlind, Mikael
483
Heimann, Oliver
979
Heller, Jan Erik
515
Henshall, Edwin
113
Herter, Johannes
805
Hinsch, Malte
515, 623
Hoﬀenson, Steven
865
Hohnen, Thomas
745, 885
Hosenfeld, Dirk
211
Hribernik, Karl
675
Iritani, Diego Rodrigues
169
Izaguirre, Alberto
451
Jagenberg, Jan Tim
969
Jazdi, Nasser
125
Jonas, Henry
543
Joshi, Shraddha
583
Kaiser, Christian
71
Kernschmidt, Konstantin
125
Khatib, Ahmad Al
795
Kim, Harrison
735
Kjellberg, Torsten
483
Klein, Patrick
947
Klein, Philipp
125
Kleiner, Sven
93
Klooster, Roland ten
855
K¨ochling, Daniel
273
K¨ohler, Susann
957
Kramer, Christoph
93
Krause, Dieter
543
Krebs, Andreas
357, 633
Kr¨uger, Daniel
643
Kwak, Minjung
735
Lachenmaier, Lena
243
Lachmayer, Roland
685
Lafon, Pascal
493
Lammel, Bernhard
211
Landi, Daniele
431
Lange, Jos de
855
Lanza, Gisela
283
Lartigue, Claire
315
Lenord, Matthias
613
Li, Yujiang
483
Lindkvist, Lars
473
Lindner, Andreas
357
Lindow, Kai
979
Lojewski, Mathias
835
Luedeke, Tobias
695
Luft, Thomas
905
Lutters, Eric
653, 787, 855
Luttikhuis, Ellen Oude
855
L¨utzenberger, Johannes
947
Mahdjoub, Morad
795
Malmiry, Roozbeh Babaeizadeh
411
Manns, Martin
325
Mantwill, Frank
835
Mart´ın, N´estor Andr´es Arteaga
293,
325
Martin, Patrick
147
Matsuo, Kohei
937
Maul, Ludwig
525
Maya, Jorge
755
Meboldt, Mirko
263
Mengoni, Maura
915

Author Index
1011
Michielsen, Cees
43
Miehling, J¨org
643
Moura, Antonio ´Alvaro de Assis
231
Mozgova, Iryna
685
Murar, Mircea
159
Namouz, Essam Z.
777
Nattermann, Roland
221
Nguyen, Duy Minh Phan
927
Oesters¨otebier, Felix
273
Ometto, Aldo
169
Ostad-Ahmad-Ghorabi, Hesam
715
Ovtcharova, Jivka
805
Pajo, Sanjin
505
Pasek, Z.J.
253
Peggy, Zwolinski
875
Perry, Nicolas
411
Picard, Andr´e
1, 231
Piccini, Luiz
243
Pollmanns, Judith
745, 885
Pourroy, Franck
815
Quezada, Akamitl
989
Qureshi, A.J.
563
Rahmani, Touba
715
Rebel, Martin
43
Ringen, Geir
895
Rockizki, Julie
875
Roth, Daniel
103
Rothenburg, Uwe
937
Roubanov, Daniil
303
Rozenfeld, Henrique
169
Rusitschka, Fabian
103
Sagot, Jean-Claude
795
Sauthoﬀ, Bastian
685
Sch¨afer, Patrick D.
725
Schallow, Julian
303
Schindler, Thomas
633, 989
Schleich, Benjamin
573
Schlick, Christopher M.
515, 623
Schl¨ogl, Wolfgang
613
Schmitt, Robert
387, 765
Schm¨udderrich, Tanja
23
Sch¨ule, Anselm
399
Sch¨utzer, Klaus
231
Serna, Sebastian Pena
421
Sivard, Gunilla
483
Smoll, Simon
905
S¨oderberg, Rikard
473, 865
Spangenberg, Felix
377
Stark, Rainer
51, 293, 937, 979
Steden, Frank
603
Stoﬀels, Pascal
999
Stork, Andre
421
Strang, Daniel
969
St¨ucheli, Marius
263
Summers, Joshua D.
583, 777
Tahera, Khadija
201
Tansky, Dmitry
345
Thi´ebaut, Fran¸cois
315
Thoben, Klaus-Dieter
675, 947
Thomann, Guillaume
367, 927
Tonetti, J´erˆome
927
Tr¨achtler, Ansgar
23
Uddin, Amad
113
Ugarte, Done
451
Ulonska, S¨oren
11
Vandevenne, Dennis
505
van Houten, Fred J.A.M.
787
Verhaegen, Paul-Armand
505
Veytizou, Julien
367
Vielhaber, Michael
663, 695, 999
Villeneuve, Fran¸cois
815
Vogel-Heuser, Birgit
125
Walter, Michael
705
W¨armefjord, Kristina
473
Wartzack, Sandro
573, 643, 705, 905
Weckend, Holger
463
Welo, Torgeir
11, 441, 895
Wenzel, Volkmar
969
Westk¨amper, Engelbert
613
Weyrich, Michael
125, 603
Williams, Huw
113
Wits, Wessel W.
33
Worinkeng, Emily
583
Wuest, Thorsten
675
Xuereb, Hugo
367
Yildirim, Unal
113, 191
Zafeiriou, Efstratia
103
Zancul, Eduardo
243
Zentis, Thomas
387
Zingel, Christian
83
Zwolinski, Peggy
845


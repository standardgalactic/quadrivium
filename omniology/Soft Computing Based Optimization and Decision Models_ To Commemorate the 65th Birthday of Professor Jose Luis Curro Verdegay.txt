Studies in Fuzziness and Soft Computing
David A. Pelta
Carlos Cruz Corona    Editors 
Soft Computing
Based
Optimization and
Decision Models
To Commemorate the 65th Birthday of 
Professor José Luis "Curro" Verdegay

Studies in Fuzziness and Soft Computing
Volume 360
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Studies in Fuzziness and Soft Computing” contains publications on
various topics in the area of soft computing, which include fuzzy sets, rough sets,
neural networks, evolutionary computation, probabilistic and evidential reasoning,
multi-valued logic, and related ﬁelds. The publications within “Studies in Fuzziness
and Soft Computing” are primarily monographs and edited volumes. They cover
signiﬁcant recent developments in the ﬁeld, both of a foundational and applicable
character. An important feature of the series is its short publication time and
world-wide distribution. This permits a rapid and broad dissemination of research
results.
More information about this series at http://www.springer.com/series/2941

David A. Pelta
• Carlos Cruz Corona
Editors
Soft Computing Based
Optimization and Decision
Models
To Commemorate the 65th Birthday
of Professor José Luis “Curro” Verdegay
123

Editors
David A. Pelta
Department of Computer Science and A.I
Higher Technical School of Information
Technology and Telecommunications
Engineering
University of Granada
Granada
Spain
Carlos Cruz Corona
Department of Computer Science and A.I
Higher Technical School of Information
Technology and Telecommunications
Engineering
University of Granada
Granada
Spain
ISSN 1434-9922
ISSN 1860-0808
(electronic)
Studies in Fuzziness and Soft Computing
ISBN 978-3-319-64285-7
ISBN 978-3-319-64286-4
(eBook)
DOI 10.1007/978-3-319-64286-4
Library of Congress Control Number: 2017947475
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Foreword
Jose Luis Verdegay, “Curro” for his friends, has greatly inﬂuenced our academic
lives in the University of Granada, as much for his special dedication to our
Department as Director, his successful management of the research, and especially
as supervisor of our doctoral thesis.
He introduced us to the Fuzzy Sets Theory in the late 80’s, beginning of the 90’s
and its application in optimization and decision making. He taught us to be com-
mitted to our University and scientiﬁc rigor. Under his supervision, we have learned
to carry out serious research work. He has been a demanding advisor with the work
that we have developed under his supervision, but he has always motivated us to
focus on new challenges and, in such a way, we have grown scientiﬁcally and
humanly. He has taught us to work as a team and, on the other hand, he has
encouraged us to achieve scientiﬁc leadership in our academic lives.
As a result, we can say that we have got many scientiﬁc achievements and
international recognition. Today we can assure that the Department of Computer
Science and Artiﬁcial Intelligence is a reference team in our scientiﬁc community.
Special mention is made for his excellent activity in Latin America. He has been
able to connect with many researchers from different universities in countries such
as Cuba, Peru, Ecuador, etc., helping them in the development of their scientiﬁc
careers.
His dedication and recognition for his initiatives at the many institutional
positions he occupied, such as the most recent Rector’s Delegate of the University
of Granada for Information and Communication Technologies, is very well known.
He is a national and international leader in our scientiﬁc community. He has the
role of “Invited Professor” in three Cuban institutions (Instituto Superior
Politécnico “José Antonio Echeverría” from La Habana, Universidad Central
“Marta Abreu” from Las Villas and Universidad “Oscar Lucero Moya”, from
Holguín). Also, he is a fellow of the International Fuzzy Systems Association
(IFSA), he is recognized as an IEEE Senior Member, and he is a Honorary Member
of the Mathematics and Computation Academy from Cuba.
Today more than ever, our friend Curro is still working actively in our
Department and in our University.
v

For us, he represents the ﬁgure of our scientiﬁc and academic father with whom
we share our scientiﬁc and academic achievements. It is always a pleasure to enjoy
a good meal and cup of wine with him in one of our favorite restaurants “Chikito”
or “Antonio Pérez”.
We want to use these brief words to thank you for everything you have done for
us and to acknowledge your laudable dedication to the Department of Computer
Science and Artiﬁcial Intelligence and the University of Granada.
Thank you very much CURRO!!!
Francisco “Paco” Herrera
Enrique Herrera-Viedma
Department of Computer Science and Artiﬁcial Intelligence
University of Granada
Spain
vi
Foreword

Preface
Decision and optimization problems are ubiquitous in the current social and tech-
nological context. Our societies face several challenges (in health, transportation,
energy, climate, etc.) which are clearly framed either as decision or optimization
problems. Moreover, research on Soft Computing is a key aspect in paving the way
for better models and tools to solve the corresponding problems.
This commemorative book titled Soft Computing based Optimization and
Decision Models. To commemorate the 65th birthday of Professor José Luis
“Curro” Verdegay, contains 18 guest chapters addressing hot topics on soft
computing based decision and optimization models and tools. They are written by
key leading experts in the ﬁeld from the USA, Brazil, UK, France, Cuba, Finland,
Italy, Spain, etc., in which the reader will ﬁnd short surveys, theoretical research
and practical applications on the latest advances in the ﬁeld.
The book is organized into three parts.
The ﬁrst one comprises ﬁve chapters that review the main applications of Soft
Computing in different ﬁelds.
In Chapter “A Review of Soft Computing Techniques in Maritime Logistics and
Its Related Fields”, Expósito-Izquierdo et al. highlight the role and relevance of
maritime logistics and associated problems, and review the applications of soft
computing techniques in the ﬁeld. Opportunities for further developments are also
explored.
J. Cadenas and M. Garrido, in Chapter “Intelligent Data Analysis, Soft
Computing and Imperfect Data”, analyze different hybridization approaches
between soft computing and intelligent data analysis, focusing on the data pre-
processing and data mining stages. They mainly focus on evaluating whether the
elements of soft computing are incorporated in the design of the method/model, or
whether they are also used to deal with imperfect information.
Chapter “Soft Computing Methods in Transport and Logistics” by J. Brito et al.
begins by providing an overview of transport and logistic problems and their
models focusing on the management of uncertainty by means of fuzzy optimization
and metaheuristics methods. Then, and given the promising results, some emerging
areas are presented and described.
vii

Masegosa et al. contribute with the Chapter “Applications of Soft Computing in
Intelligent Transportation Systems”. Intelligent transportation systems combine
electronic, communication and information technologies with trafﬁc engineering to
obtain more efﬁcient, reliable and safer transportation systems. The chapter gathers
and discusses some of the most relevant and recent advances in the application of
soft computing techniques in relevant areas of intelligent transportation systems,
namely autonomous driving, trafﬁc state prediction, vehicle route planning and
vehicular ad hoc networks.
Finally, in Chapter “Fuzzy Cognitive Maps Based Models for Pattern
Classiﬁcation: Advances and Challenges”, G. Napoles et al. focus on Fuzzy
Cognitive Maps (FCMs), a sort of recurrent neural networks that include elements
of fuzzy logic during the knowledge engineering phase. The authors observe that
many studies show how this soft computing technique (FCM) is able to model
complex and dynamic systems, but here, they explore a new approach: the use of
FCMs in solving pattern classiﬁcation problems.
The second part of the book contains six contributions.
The ﬁrst one is Chapter “A Proposal of On-Line Detection of New Faults and
Automatic Learning in Fault Diagnosis”, by A. Rodríguez Ramos et al. The authors
present a new approach to automatic learning for a fault diagnosis system. The
proposal includes an off-line learning stage, fuzzy clustering techniques and a
metaheuristic (differential evolution). Then a novel fault detection algorithm is
applied. This algorithm is able to determine whether an observation may constitute
a new class, probably representative of a new fault or whether it is noise. The
approach is validated using an illustrative example.
Then, two chapters deal with the portfolio selection problem. In the ﬁrst one,
Chapter “Fuzzy Portfolio Selection Models for Dealing with Investor’s Preferences”,
C. Calvo et al. recall their previous works and propose a fuzzy model for dealing with
the vagueness of investor preferences on the expected return and the assumed risk,
and then consider several modiﬁcations to include additional constraints and goals. In
the second one, Chapter “On Fuzzy Convex Optimization to Portfolio Selection
Problem”, R. Coelho departs from the fact that the portfolio selection problems can be
classiﬁed as convex programming problems. Then, he presents a fuzzy set based
method that solves a class of convex programming problems with vagueness costs in
the objective functions and/or order relation in the set of constraints. The solution
approach transforms a convex programming problem under fuzzy environment into a
parametric convex multi-objective programming problem. The method is applied to a
portfolio selection problem using the data of some Brazilian securities.
C. Carlsson, in Chapter “Digital Coaching for Real Options Support”, claims
that classical management science is making the transition to analytics and that
there is a growing interest in replacing the classical net present value (NPV) with
real options theory, especially for strategic issues and uncertain, dynamic envi-
ronments. Both factors motivate the use of soft computing. As real options theory
requires rather advanced levels of analytics, the author suggests that digital
coaching is a way to guide and support users in giving them better chances for
effective and productive use of real options methods. A real-world example on the
viii
Preface

development and use of fuzzy real options models for the case of closing (or not
closing or closing later) an old paper mill in the UK is shown.
Chapter “An Analysis of Decision Criteria for the Selection of Military Training
Aircrafts”, by J. Sanchez Lozano et al. also ﬁts in the context of decision making. The
authors describe the process by which the relevance of technical criteria in determining
the quality of a military training aircraft is obtained. Experts provided the criteria
information and both qualitative and quantitative criteria are considered. A fuzzy AHP
(Analytic Hierarchy Process) methodology is proposed to extract the knowledge from
the group of experts and ﬁnally obtain a unique set of weights for the criteria.
Y. Liu and F. Gomide, in Chapter “Participatory Search in Evolutionary Fuzzy
Modeling”, focus on one of the key elements of soft computing, namely meta-
heuristics.
They
introduce
the
so-called
participatory
search,
a
class
of
population-based search algorithms constructed upon the participatory learning
paradigm. To illustrate the potential of the proposal, they resort to the problem of
obtaining fuzzy rule-based models from actual data and provide comparisons with a
state-of-the-art genetic fuzzy system.
The third part contains seven chapters exploring theoretical aspects of soft
computing.
With the Chapter titled “But, What is It Actually a Fuzzy Set?”, E. Trillas states
that the idea of a fuzzy set is not yet clear enough and discusses the concept of fuzzy
set as a quantity in whatever universe of discourse, and its possible use in the
context of ‘Computing with Words’.
D. Dubois and H. Prade contributed with the Chapter “Gradual Numbers and
Fuzzy Solutions to Fuzzy Optimization Problems”. The authors start with the idea
of fuzzy elements in a fuzzy set, that is, entities that assign elements to membership
values, in contrast with fuzzy sets that assign membership values to elements. Then,
establishing a clear connection with the work of J.L. Verdegay, they observe that
the fuzzy solution to a fuzzy optimization problem is a very early example of a
fuzzy element in (or a gradual subset of) the fuzzy constraint set.
R. Yager, in Chapter “Using Fuzzy Measures to Construct Multi-criteria Decision
Functions” explores the formulation of multi-criteria decision functions based on the
use of a measure over the space of criteria, where the relationship among the criteria
is expressed using a fuzzy measure. Such a fuzzy measure is used within the Choquet
integral to construct decision functions and several speciﬁc cases are outlined.
Chapter “A Modal Account of Preference in a Fuzzy Setting”, by F. Esteva et al.
considers the problem of extending fuzzy preference relations on a set, to fuzzy
preferences on subsets, and characterize different possibilities. They then propose
several two-tiered graded modal logics to reason about the corresponding different
notions of fuzzy preferences.
R. Fuller and I.Á. Harmati, in Chapter “On Possibilistic Dependencies: A Short
Survey of Recent Developments”, present a survey of the latest work on the
extensions and developments of the notions of possibilistic mean value and vari-
ance of fuzzy numbers, possibilistic covariance, correlation ratio and correlation
coefﬁcient and the informational coefﬁcient of correlation.
In Chapter “Penalty Function in Optimization Problems: A Review of Recent
Developments”, H. Bustince et al. highlight the role and relevance of penalty
Preface
ix

functions as a tool for information fusion. They review the ideas of penalty and
penalty-based functions and discuss how such notions can be extended to deal with
data in Cartesian products of lattices.
Finally, S. Bortot et al. in Chapter “The Single Parameter Family of Gini
Bonferroni
Welfare
Functions
and
the
Binomial
Decomposition,
Transfer
Sensitivity and Positional Transfer Sensitivity” analyze the so-called generalized
Gini welfare functions and consider their binomial decomposition. They introduce
measures of transfer sensitivity and positional transfer sensitivity and illustrate the
behaviour of the binomial welfare function with respect to these measures.
As Editors, we should highlight that it was both a challenge and a great pleasure
for us in compiling this book.
On the one hand, it was a challenge because Prof. Verdegay has many friends
and colleagues worldwide and we needed to select some of them as potential
collaborators. The task was difﬁcult but, in the end, we have an excellent set of
topics written by top researchers, who collaborate or have collaborated with Curro.
Here we thank the researchers who immediately accepted to join this editorial
project. Readers of this book will appreciate these high-quality contributions.
In addition, we thank Profs. Enrique Herrera and Francisco Herrera for writing
the foreword of the book.
On the other hand, it was a pleasure because we have known Curro since a long
time ago. We started to work with him in 1998 (D. Pelta) and 2002 (C. Cruz) when
we began our Ph.D. studies under his direction. From that time, we have had the
opportunity to share many discussions, talks and personal situations with Curro that
make us consider him a true friend. During these years, we have come to know all of
Curro’s facets. His scientiﬁc and academic merits are very well known, but we
would like to mention here also his kindness, availability and true support for the
academics and friends mainly from developing countries (especially Latin America).
Since his work in the University Government, we have observed the huge number of
visits he receives at his ofﬁce asking for guidance or suggestions. His experience as a
researcher, professor and manager (in several positions at the University
Government) is invaluable and we are lucky to have him available every day.
Thank you Curro!
Acknowledgements D. Pelta and C. Cruz acknowledge the support of projects TIN2014-55024-P
(Spanish Ministry of Economy and Competitiveness) and P11-TIC-8001 (Consejería de Economía,
Innovación y Ciencia, Junta de Andalucía). Both projects include FEDER funds from the European
Union.
Granada, Spain
David A. Pelta
May, 2017
Carlos Cruz Corona
x
Preface

Contents
A Review of Soft Computing Techniques in Maritime Logistics
and Its Related Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Christopher Expósito-Izquierdo, Belén Melián-Batista
and J. Marcos Moreno-Vega
Intelligent Data Analysis, Soft Computing and Imperfect Data. . . . . . . .
25
Jose M. Cadenas and M. Carmen Garrido
Soft Computing Methods in Transport and Logistics. . . . . . . . . . . . . . . .
45
Julio Brito, Dagoberto Castellanos-Nieves, Airam Expósito
and José. A. Moreno
Applications of Soft Computing in Intelligent
Transportation Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
Antonio D. Masegosa, Enrique Onieva, Pedro Lopez-Garcia
and Eneko Osaba
Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation:
Advances and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
Gonzalo Nápoles, Maikel Leon Espinosa, Isel Grau, Koen Vanhoof
and Rafael Bello
A Proposal of On-Line Detection of New Faults and Automatic
Learning in Fault Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
Adrián Rodríguez Ramos, Alberto Prieto Moreno, Antônio José da Silva
Neto and Orestes Llanes-Santiago
Fuzzy Portfolio Selection Models for Dealing with Investor’s
Preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
Clara Calvo, Carlos Ivorra and Vicente Liern
On Fuzzy Convex Optimization to Portfolio Selection Problem . . . . . . .
137
Ricardo Coelho
xi

Digital Coaching for Real Options Support . . . . . . . . . . . . . . . . . . . . . . .
153
Christer Carlsson
An Analysis of Decision Criteria for the Selection
of Military Training Aircrafts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
Juan M. Sánchez-Lozano, M.A. Socorro García-Cascales
and María T. Lamata
Participatory Search in Evolutionary Fuzzy Modeling. . . . . . . . . . . . . . .
191
Yi Ling Liu and Fernando Gomide
But, What Is It Actually a Fuzzy Set?. . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
Enric Trillas
Gradual Numbers and Fuzzy Solutions to Fuzzy Optimization
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
Didier Dubois and Henri Prade
Using Fuzzy Measures to Construct Multi-criteria
Decision Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
Ronald R. Yager
A Modal Account of Preference in a Fuzzy Setting . . . . . . . . . . . . . . . . .
241
Francesc Esteva, Lluís Godo and Amanda Vidal
On Possibilistic Dependencies: A Short Survey of Recent
Developments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
Robert Fullér and István Á. Harmati
Penalty Function in Optimization Problems: A Review of Recent
Developments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
Humberto Bustince, Javier Fernandez and Pedro Burillo
The Single Parameter Family of Gini Bonferroni Welfare Functions
and the Binomial Decomposition, Transfer Sensitivity and Positional
Transfer Sensitivity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
Silvia Bortot, Mario Fedrizzi , Ricardo Alberto Marques Pereira
and Anastasia Stamatopoulou
xii
Contents

Contributors
Rafael Bello Central University of Las Villas, Santa Clara, Cuba
Silvia Bortot Department of Economics and Management, University of Trento,
Trento, Italy
Julio
Brito Grupo
de
Computación Inteligente,
Instituto
Universitario
de
Desarrollo Regional, Universidad de La Laguna, La Laguna, Spain
Pedro Burillo Departamento de Automatica y Computacion, Universidad Publica
de Navarra, Pamplona, Spain
Humberto Bustince Departamento de Automatica y Computacion, Universidad
Publica de Navarra, Pamplona, Spain
Jose M. Cadenas Department of Information and Communications Engineering,
University of Murcia, Murcia, Spain
Clara Calvo Universidad de Valencia, Valencia, Spain
Christer Carlsson IAMSR/Abo Akademi University, Turku, Finland
Dagoberto Castellanos-Nieves Grupo de Computación Inteligente, Instituto
Universitario de Desarrollo Regional, Universidad de La Laguna, La Laguna, Spain
Ricardo Coelho Departamento de Estatística e Matemática Aplicada, Centro de
Ciências, Universidade Federal do Ceará, Fortaleza, Brazil
Didier Dubois CNRS-IRIT, Université Paul, Toulouse, Cedex 9, France
Francesc Esteva IIIA - CSIC, Bellaterra, Spain
Christopher Expósito-Izquierdo Universidad de La Laguna, Santa Cruz de
Tenerife, Spain
Airam Expósito Grupo de Computación Inteligente, Instituto Universitario de
Desarrollo Regional, Universidad de La Laguna, La Laguna, Spain
xiii

Mario Fedrizzi Department of Industrial Engineering, University of Trento,
Trento, Italy
Javier Fernandez Departamento de Automatica y Computacion, Universidad
Publica de Navarra, Pamplona, Spain
Robert Fullér Department of Informatics, Széchenyi István University, Győr,
Hungary
M.A.
Socorro
García-Cascales Dpto.
de
Electrónica,
Tecnología
de
Computadoras y Proyectos, Universidad Politécnica de Cartagena, Murcia, Spain
M.
Carmen
Garrido Department
of
Information
and
Communications
Engineering, University of Murcia, Murcia, Spain
Lluís Godo IIIA - CSIC, Bellaterra, Spain
Fernando Gomide School of Electrical and Computer Engineering, University of
Campinas, Sao Paulo, Brazil
Isel Grau Central University of Las Villas, Santa Clara, Cuba
István Á. Harmati Department of Mathematics and Computational Sciences,
Széchenyi István University, Győr, Hungary
Carlos Ivorra Universidad de Valencia, Valencia, Spain
María T. Lamata Dpto. Ciencias de la Computación e Inteligencia Artiﬁcial,
Universidad de Granada, Granada, Spain
Maikel Leon Espinosa University of Miami, Miami, FL, USA
Pedro Lopez-Garcia Faculty of Engineering, University of Deusto, Bilbao, Spain;
DeustoTech-Fundacion Deusto, Deusto Foundation, Bilbao, Spain
Vicente Liern Universidad de Valencia, Valencia, Spain
Yi Ling Liu School of Electrical and Computer Engineering, University of
Campinas, Sao Paulo, Brazil
Orestes
Llanes-Santiago Departamento
de
Automática
y
Computación,
Universidad Tecnológica de la Habana José Antonio Echeverría, CUJAE, La
Habana, Cuba
Ricardo Alberto Marques Pereira Department of Economics and Management,
University of Trento, Trento, Italy
Antonio D. Masegosa Faculty of Engineering, University of Deusto, Bilbao,
Spain;
DeustoTech-Fundacion
Deusto,
Deusto
Foundation,
Bilbao,
Spain;
IKERBASQUE, Basque Foundation for Science, Bilbao, Spain
Belén Melián-Batista Universidad de La Laguna, Santa Cruz de Tenerife, Spain
xiv
Contributors

J. Marcos Moreno-Vega Universidad de La Laguna, Santa Cruz de Tenerife,
Spain
José. A. Moreno Grupo de Computación Inteligente, Instituto Universitario de
Desarrollo Regional, Universidad de La Laguna, La Laguna, Spain
Enrique Onieva Faculty of Engineering, University of Deusto, Bilbao, Spain;
DeustoTech-Fundacion Deusto, Deusto Foundation, Bilbao, Spain
Eneko Osaba Faculty of Engineering, University of Deusto, Bilbao, Spain;
DeustoTech-Fundacion Deusto, Deusto Foundation, Bilbao, Spain
Gonzalo Nápoles Hasselt Universiteit, Diepenbeek, Belgium
Henri Prade CNRS-IRIT, Université Paul, Toulouse, Cedex 9, France
Alberto
Prieto
Moreno Departamento
de
Automática
y
Computación,
Universidad Tecnológica de la Habana José Antonio Echeverría, CUJAE, La
Habana, Cuba
Adrián
Rodríguez
Ramos Departamento
de
Automática
y
Computación,
Universidad Tecnológica de la Habana José Antonio Echeverría, CUJAE, La
Habana, Cuba
Anastasia
Stamatopoulou Department
of
Economics
and
Management,
University of Trento, Trento, Italy
Juan M. Sánchez-Lozano Centro Universitario de la Defensa. Academia General
del Aire. Universidad Politécnica de Cartagena, Murcia, Spain
Enric Trillas University of Oviedo, Asturias, Spain
Koen Vanhoof Hasselt Universiteit, Diepenbeek, Belgium
Amanda Vidal ICS, Czech Academy of Sciences, Prague, Czech Republic
Ronald R. Yager Machine Intelligence Institute, Iona College, New Rochelle,
NY, USA
Antônio José da Silva Neto Instituto Politécnico da Universidade Do Estado Do
Rio de Janeiro (IPRJ/UERJ), Nova Friburgo, Brazil
Contributors
xv

A Review of Soft Computing Techniques
in Maritime Logistics and Its Related Fields
Christopher Expósito-Izquierdo, Belén Melián-Batista
and J. Marcos Moreno-Vega
Abstract The incessant increase in the world seaborne trade over the last few
decades has encouraged maritime logistics has become a very attractive area of study
for applying the general frameworks of soft computing. In this environment, there
is a signiﬁcant lack of eﬃcient approaches aimed at obtaining exact solutions of a
wide variety of optimization problems arisen in this ﬁeld and which are classiﬁed
as hard from the perspective of the complexity theory. These optimization problems
demand increasingly new computational approaches able to report inexact solutions
by exploiting extensively uncertainty, tolerance for imprecision, and partial truth to
achieve tractability, among others. In the chapter at hand, we provide a review of
the most highlighted soft computing techniques implemented in maritime logistics
and its related ﬁelds and identify some opportunities to go further into depth on
knowledge.
Keywords
Optimization ⋅Maritime container terminal ⋅Logistics
1
Introduction
Over the last few decades, maritime container terminals have become outstand-
ing infrastructures in global supply chains [35]. They are usually situated within
the boundaries of ports located in strategic regions with the aim of acting as eco-
nomic engines. The main purpose of these infrastructures is to carry out the eﬃcient
exchange of freights among heterogeneous means of transportation. Traditionally,
these means have diﬀerent operational and technical characteristics. Firstly, we can
C. Expósito-Izquierdo (✉) ⋅B. Melián-Batista ⋅J.M. Moreno-Vega
Universidad de La Laguna, Santa Cruz de Tenerife, Spain
e-mail: cexposit@ull.edu.es
B. Melián-Batista
e-mail: mbmelian@ull.edu.es
J.M. Moreno-Vega
e-mail: jmmoreno@ull.edu.es
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_1
1

2
C. Expósito-Izquierdo et al.
ﬁnd container vessels, which are aimed at moving containers among distant terminals
along a predeﬁned maritime route within container transportation networks. Also,
land transportation means are brought together in a conventional maritime container
terminal. In most of the cases, trucks and trains arrive at a maritime container ter-
minal with the aim of picking-up and deliverying containers.
Broadly speaking, the freights are fundamentally moved along global supply
chains through the use of standard-sized metal boxes: containers. The appearence
of the container as international vehicle for the freights has constituted a true indus-
trial revolution due to the fact that it encourages to reduce transportation costs by
exploiting the beneﬁts provided by the economies of scale, prevents cargo dam-
ages, and minimizes shipping costs. They are designed in accordance with predeﬁned
dimensions with the aim of enabling their exchange within multi-modal transporta-
tion networks in which at least vessels, trucks, and trains are brought together. It is
worth mentioning that the standard capacity unit of measure for containers is the
Twenty-foot Equivalent Unit, which measures about 6 m long.
The relevance of maritime container terminals has progressively increased from
the time of the introduction of the containerization in the international trade trough
the present time. Nowadays, maritime container terminals are facing with the increas-
ing growth in the world merchandise trade and seaborne shipments. It must be
pointed out that the seaborne shipments and the merchandise trade have increased in
tandem over the last years. As indicative data, more than 10 billion tons of freights
have been moved around the world by shipping business during 2015, according to
the Review of Maritime Transport 2016 published by the United Nations Conference
on Trade And Development (UNCTAD).1 This constitutes a growth of 2.5% in the
world seaborne trade. However, projections indicate that this volume of containers
is going to be largely exceeded in the course of the coming years after the ﬁnancial
and economic crisis.
As previously indicated, in order to manage such a great volume of containers,
large maritime container terminals are required. The maritime container terminals
are multi-modal logistic interfaces dedicated to connect maritime and hinterland
means of transportation. In this context, the layout of a conventional maritime con-
tainer terminal consists of the following functional areas [40]:
∙Sea-side. It is the part of the terminal in which the incoming container vessels are
berthed. The main goal of this functional area is to carry out the transshipment
of containers included into the corresponding stowage plan eﬃciently. For this
purpose, a set of quay cranes available at the terminal is deployed. These cranes
are aimed at unloading the containers whose destination is the terminal in which
they are available. At the same time, they are also dedicated to load containers
into the incoming vessels to be transported to other maritime container terminals.
The book [71] provides a rigurous analysis of the main logistic operations in the
sea-side.
1http://unctad.org.

A Review of Soft Computing Techniques ...
3
Sea-side
Yard
Land-side
Fig. 1
Functional areas of a conventional maritime container terminal
∙Yard. It is a large open-air surface aimed at storing containers temporarely until
their later retrieval. The containers are distributed among homogeneous blocks,
which are three-dimensional storages. Each block is divided into a set of parallel
bays, which are composed of a set of contiguous stacks with a maximum stacking
limit [9].
∙Land-side. It is the interface dedicated to exchange freights between the maritime
container terminal and the hinterland means of transportation. With this goal in
mind, a set of external gates are available to monitor and control freights entering
and leaving the infrastructure [10].
These functional areas of a maritime container terminal are illustrated from left to
right in Fig. 1.
The technological advancement has given rise to the maritime container termi-
nals have become complex infrastructures to manage due to the fact that its operat-
ing eﬃciency must be achieved through improved productivity and performance of
analysts and transportation agents. However, the large and diverse number of logis-
tic activities brought together is a major obstacle to the eﬀective decision-making
processes. In most of the cases, the logistic processes in this environment belong
to the N P-hard class of problems. Furthermore, practitioners must deal with the
imprecision and uncertainty regarding the arrival and retrieval of containers, chang-
ing information, etc. The existence of uncertainty, imprecision, and partial truth in
logistic processes must be explicitly taken into account as much as possible. Unfor-
tunately, the narrow scope in practice of hard computing approaches to handle these
issues in an appropriate form encourages the study and implementation of soft com-
puting proposals.
The present chapter is aimed at reviewing the main logistic processes arisen in
the context of maritime container terminals and its related ﬁelds. Speciﬁcally, we
present and illustrate the most outstanding optimization problems studied by the sci-
entiﬁc community over the last few decades and discuss the main ﬁndings associated
with the existing research of soft computing methodologies. In this regard, our main

4
C. Expósito-Izquierdo et al.
objective is to provide a general overview of the most relevant solving approaches
related to the logistic processes from the perspective of soft computing and identify
some opportunities to go further into depth on knowledge.
The remainder of this chapter is organized as follows. Firstly, Sect. 2 describes the
main logistic processes associated with the movement of freights at maritime con-
tainer terminals. Afterwards, Sect. 3 overviews the major contributions made by the
soft computing to maritime logistics. Finally, this chapter ends by a brief summary
and indicates several promising lines for further research in maritime logistics and
its related ﬁelds.
2
Logistic Processes
In spite of the fact that all the means of transportation can be present, maritime con-
tainer terminals play a relevant role within global supply chains due to the fact that
they are able to connect the maritime and hinterland means of transportation eﬃ-
ciently. Their primary goal is to enable the exchange of freights between producers
and consumers with no repacking. The global supply chains in which the impli-
cated maritime container terminals are inherently complex to manage due to the
large amount of containers, technical characteristics of the loading and unloading
equipment, existing staﬀ, and legal aspects, among others.
From a general perspective, a global supply chain is composed of multiple ﬂows of
standardized containers in which the freights are packaged into. Before loaded with
freights, the containers are manufactured by some metal factory from weathering
steel, powder coated a color, and with the dimensions imposed by the requirements
of the carriers. The empty containers are temporarily stored on a depot until the pro-
ducers of the freights to transport request them. Once required, an empty container is
transported by road to the producer of the freights, or some infrastructure dedicated
to this goal, to move toward their consumers. The containers stuﬀed with the cargo
to transport are moved to some maritime container terminal to be shipped overseas
by some container vessel. In the simplest case, one truck moves each container to
the terminal. However, multiple hinterland transportation means and container ter-
minals can be involved in this process. The containers are stored on the yard of the
terminal until the target vessels that are actually at the terminal are ready to load
them onto. Similarly as in the hinterland case, the oversea transportation can involve
multiple vessel journeys. Some minor container terminals, termed hubs, are usually
used as intermediate storage infrastructures when moving containers over long dis-
tances. The target maritime container terminal is considered as the departure point of
the hinterland transportation toward the customers of the freights. Lastly, the freights
are stripped and the empty containers are stored again in a depot until another trans-
portation is requested.

A Review of Soft Computing Techniques ...
5
The eﬃcient exchange of freights between heterogeneous means of transportation
in a maritime container terminal depends on having a suitable integrated planning,
coordination, execution, and control of container ﬂows. In this scenario, the source
and destination of each container deﬁne its type. Speciﬁcally, an import container
is that unloaded from an incoming container vessel or delivered by a truck or train
and must be stored on the yard of the terminal. Similarly, an export container is that
stored on the yard of the terminal and must be included into the stowage plan of some
incoming container vessel or loaded onto a truck or train. Lastly, a transshipment
container is that unloaded from an incoming container vessel, stored on a block of
yard, and transferred to another incoming container vessel to be moved toward its
destination port.
The maritime container terminals are large engines for the economic develop-
ment because they connect production sources and end consumers. In this regard,
a maritime container terminal is responsible for serving eﬃciently the means of
transportation brought together. However, the high complexity associated with the
management of the container ﬂows arisen in these infrastructures poses a continu-
ing challenge that has to be recognised and addressed by practitioners. In particular,
multitude of logistic planning problems known as N P-hard are identiﬁed in a con-
ventional maritime container terminal.
Several general classiﬁcations have been already proposed in the scientiﬁc liter-
ature to organize the logistic planning problems according to the functional area in
which they take place, the means of transportation involved in them, or the planning
horizon considered, among others. In this chapter, we use the classiﬁcation of the
logistic problems proposed in [43]. In this case, the logistic problems are split into
four categories on the basis of the functional areas. These categories are termed ship
to shore, transfer, storage, and delivery. The most highlighted optimization problems
included into the previous categories are summarized in the following:
∙Ship to shore. It involves those optimization problems associated with the load-
ing and unloading operations of containers included into the stowage plan of the
incoming vessels:
– Ship routing [54]. It seeks to determine the best journeys of a ﬂeet of container
vessel to exchange containers among a given set of maritime container termi-
nals.
– Stowage planning [74]. Its objective is to identify the positions assigned to the
containers to carry by a vessel while taking into account the features of its ship-
ping route.
– Berth allocation [7]. It seeks to determine the berthing position and berthing
time of the incoming container vessels arrived to the maritime container termi-
nal to minimize their service times.
– Quay crane assignment [72]. This optimization problem is aimed at determining
the subset of quay cranes to assign to the incoming container vessels over a given
planning horizon to provide the best possible service.

6
C. Expósito-Izquierdo et al.
– Quay crane scheduling [70]. Its goal is to obtain a schedule of loading and
unloading operations of containers to perform by the quay cranes assigned to a
particular container vessel in such a way that its waiting time is minimized.
∙Storage. It involves those optimization problems arisen from the movement and
warehousing of containers on the yard and its related facilities to be ready for
loading onto the incoming means of transportation:
– Yard crane scheduling [36]. It is aimed at determining an eﬃcient schedule
of the storage, retrieval, and relocation of containers around the yard of the
terminal.
– Container storage [12]. It is a family of optimization problems aimed at max-
imizing the performance of the stacking cranes on the yard when storing and
retriving containers.
∙Transfer and delivery. It comprises those optimization problems arisen from the
movement of containers between the functional areas of the terminal:
– Vehicle dispatching [3]. It is aimed at managing the internal delivery vehicles
of the terminal to maximize their performance.
– Gate operations planning [17]. It seeks to optimize the access of the trucks and
trains to the terminal to fulﬁll access and capacity restrictions.
In the remainder of this chapter, the focus is put on the application of soft com-
puting methodologies in maritime logistics and its related ﬁelds.
3
Soft Computing in Maritime Logistics and Related Fields
Most of the authors who have addressed planning problems in maritime container
terminals and its related ﬁelds over the last decades have assumed a very optimistic
standpoint, in which uncertainty is completely ruled out. However, despite of the
fact that the scientiﬁc literature considers the planning problems arisen in maritime
container terminals as completely static in most of the cases, multitude of uncer-
tainty sources are present in real-life scenarios. In particular, several general types
of uncertainty sources can be distinguished. The former is mainly composed of that
uncertainty derived from those elements in which an accident could happen in the
infrastructure, information resources, communication systems, machinery, staﬀ, or
environment, among others. This is the case of a traﬃc accident in which a stacking
crane is directly involved. Also, a second type of uncertainty source is composed of
those elements that give rise to a change of the freight requests. Illustrative examples
of this type of uncertainty are the potential wide-ranging ﬂuctuations in the arrival or
departure of container vessels, late retrieval of containers by external trucks, among
others.

A Review of Soft Computing Techniques ...
7
The existence of such a complex and dynamic environment within global sup-
ply chains forces terminal managers to be provided with eﬃcient tools to manage it.
The reason is found in that considering uncertainties in the scenario appropriately
allows to mitigate the impact of potential disruptions, as well as the infeasibility of
the working operations over a given planning horizon. In this regard, pro-active and
reactive approaches can be considered. Obtaining robust solutions of the planning
problems under analysis is the main goal of pro-active approaches, whereas reactive
approaches are aimed at recovering the complete infrastructure when some unfore-
seen incident has already happened. A classic example of pro-active approaches is
the insertion of slack time when scheduling container requests in such a way that
the arrival of new requests can be easily assumed by the terminal with the lowest
possible impact on the overall service quality. The major drawback associated with
these approaches is the time that is not spent in eﬀective work due to the existence
of slack time in the schedules.
Since the introduction of the worldwide accepted deﬁnition of soft computing by
Zadeh [101] in 1994, the use of soft computing methodologies has consolidated as
a relevant branch of science to be applied in maritime logistics and its related ﬁelds.
In this regard, soft computing has gained in importance due to the fact that it has
the ability to tackle optimization problems classiﬁed as complex according to the
complexity theory and in which partial truth, imprecision, and uncertainty appear
and with the aim of achieving low cost solutions, tractability, and truth. For this
reason, successful soft computing applications have attracted increased attention by
the research and practitioner communities over the last decades. However, the large
volume of research published so far encourages to make available classiﬁcations and
reviews of the most highlighted works to draw the main contributions and identify
promising lines for further research.
According to the previous discussion, in the remainder of this section we review
the main applications of soft computing methodologies in maritime container ter-
minals and its related ﬁelds. This review is organized on the basis of the main
components of the soft computing, described in [93]. These components are brieﬂy
described in the following:
∙Probabilistic models. It handles stochastic uncertainty. That is, the uncertainty
derived from the potential occurrence of a particular event is quantiﬁed by a certain
degree of probability. The most extended methods in this ﬁeld are based on the
Bayesian calculus, which allows to consider probability statements.
∙Fuzzy logic. It is a mathematical tool that allows to capture the tolerance to errors
and is aimed at dealing with approximate reasoning in which fuzzy truth-values
are used as adapter elements applied to fuzzy statements.
∙Artiﬁcial neural networks. They are systems integrated by multiple simple process-
ing components that work in parallel with the goal of exhibiting some brain-like
behavior.
∙Metaheuristic techniques. They are computational techniques designed to provide
approximate solutions of a given optimization problem and which largely fulﬁll
the requirements of the decision maker.

8
C. Expósito-Izquierdo et al.
3.1
Approximate Reasoning
3.1.1
Probabilistic Models
The probabilistic models result from the need of tackling jointly vagueness and prob-
ability associated with real-life problems. In the context of maritime container ter-
minals and its related ﬁelds, its is usual to address planning problems in which it is
required to reason in presence of uncertain information under partial knowledge. Ter-
minal manager demand to count on intelligent tools to support in the identiﬁcation
of those unforeseen events that could happen and to assess their potential impact
and consequences. The probabilistic models are essential ingredients in eﬀective
approaches aimed at minimizing their occurrence.
The resilience of a maritime container terminal is quantiﬁed through a Bayesian
network in [45]. As discussed by the authors, Bayesian networks are able to draw
relationships among diﬀerent variables involved in the performance of the whole
infrastructure. Also, the container operator eﬃciency is assessed from the perspec-
tive of soft computing in [99]. In this case, the authors present an empirical model
that measures eﬃciency changes and which is estimated by means of a Bayesian
approach supported by a Markov chain Monte Carlo simulation to make inference
of the unknown parameters.
Another representative example of a probabilistic model in the scientiﬁc literature
applied to maritime logistics is presented in [1]. This paper introduces a model to
apply a failure mode and eﬀects analysis when evaluating the performance of safety
measures integrated into the operational system of a container terminal. Speciﬁcally,
this model combines a fuzzy rule-based Bayesian network with evidential reasoning.
Their goals are to describe input failure information to identify hazardous events
and to aggregate these events while enabling dynamic risk-based decision support,
respectively. Furthermore, the container throughput forecasting in maritime termi-
nals has been a topic which has traditionally attracted the attention of the research
community. In this regard, [98] discusses the applicability of three hybrid approaches
based on least squares support vector regression model for this goal. The proposed
approaches are compared to each other and to benchmark proposals. The computa-
tional experiments indicate clearly that seasonal decomposition of the series is an
eﬀective approach to obtain a good container throughput forecast.
Moreover, a lateness probability of the containers when moving around the diﬀer-
ent functional areas of a terminal is studied in [88]. In this case, the authors propose
a Bayesian network designed to exploit the information recorded by the information
processing systems of the terminal in the form of event logs. This network is built
by considering the causal execution and co-occurrence between events to predict
lateness probabilities.

A Review of Soft Computing Techniques ...
9
3.1.2
Fuzzy Logic
Fuzzy logic is a mathematical formal multi-valued logic concept which is able to
tackle imprecision, uncertainty, lack of information, and partial truth by imitating
complex perception processes. This analytical tool has deﬁnitely played a leading
role in maritime logistics over the last years when addressing planning problems
for which hard computing methodologies have not proved successful. However, the
implementation of approaches based upon fuzzy logic has been particularly remark-
able when controlling driving-related tasks associated with technical machinery.
Some examples are the guidance system and steering control of internal delivery
vehicles, the real-time illumination stability system of stacking cranes, among oth-
ers. Input data are expressed by means of linguistic variables in control applications,
and taken together with if-then statements are used to formulate the conditional cases
and eventually to produce representations of human knowledge. In addition, the opti-
mization criteria of the planning problems arisen in maritime logistics are usually
diﬃcult to be deﬁned accurately due to the fact that there is a certain inherent degree
of imprecision in the way the preferences, constraints, and priorities of stakeholders
are expressed.
One of the applications of fuzzy logic with the highest impact on maritime logis-
tics is related to the control of quay and gantry cranes. The larger the cranes, the
greater the needs of controlling them while satisfying strict speciﬁcations about
payload position and swing angles, among others. A fuzzy logic-based controller
is presented in [86] to avoid payload oscillations. Other examples of fuzzy logic in
maritime logistics are related to the competitiveness of the terminals. The compet-
itiveness of any maritime container terminal is highly inﬂuenced by its capability
to attract shipping lines and retain those it is serving so far. In this regard, a large
amount of factors are behind the choice of a port by a maritime operator that must be
appropriately quantiﬁed. Some of these are the connectivity, eﬃciency, port charges,
and range of port services, among others. The paper [100] presents a fuzzy evidential
reasoning method to choice ports under uncertain environments from the perspec-
tive of shipping lines. The evidential reasoning is here used to values associated with
the factors involved in the port alternatives. In fact, objective and subjective data are
computed as fuzzy grades through linguistic terms based upon certain degrees of
belief. These are later combined by using evidential reasoning to obtain the assess-
ment of the alternatives. The computational experiments carried out in the paper
under analysis indicate that the proposal is able to ease the exhaustive assessment of
ports. The work [32] introduces fuzzy mathematical models aimed at determining
the berthing time of incoming container vessels in a terminal and scheduling their
transshipment operations. The arrival time of the containers and the processing time
of the transhipment operations are considered as fuzzy. Another interesting appli-
cation of fuzzy logic in maritime logistics is discussed in [39]. The authors present
a probabilistic-fuzzy method that allows to determine quantitatively the probability
of dangerous situation occurrence of a vessel manoeuvring in waterways by taking
into account scenarios in which navigational safety is threatened. Furthermore, [86]
presents a fuzzy logic-based robust feedback anti-sway control system which can be

10
C. Expósito-Izquierdo et al.
used either with or without a sensor of sway angle of a payload. As described by
the authors, unlike other fuzzy approaches based on linguistic rule-based strategies
and tuning of membership functions, the cited paper considers an interval analysis of
closed-loop control system characteristic polynomial coeﬃcients to solve the fuzzy
interpolation control scheme design.
3.2
Functional Approximation and Optimization Methods
3.2.1
Artiﬁcial Neural Networks
Since the introduction of the seminal computational model inspired by the human
brain in 1943 [69], the study of artiﬁcial neural networks has consolidated itself as an
essential element in the ﬁeld of soft computing. Artiﬁcial neural networks are known
to be highly eﬃcient to simulate the learning processes of human brains by miming
the biological neurons in a nervous system. The group of interconnected artiﬁcial
neurons in a neural network is in this case considered as a combination of simple
processing elements that allows to provide advanced reasoning.
The application of artiﬁcial neural networks in the context of maritime container
terminals has expanded dramatically in recent years due to their ability for reporting
approximation to analytical functions, describing the behavior of carriers, contain-
ers, staﬀ, among others, as well as predicting recurring phenomena derived from time
series. This ability is especially interesting in maritime logistics because mathemat-
ical descriptions of the planning problems are not always available to be addressed
and complex relations among variables are usually present. Instead, a large amount
of data is usually reported by the stakeholders, from which neural networks can learn
the underlying model and the interdependencies among parameters to support deci-
sion making processes.
Artiﬁcial neural networks have been successfully applied in maritime logistics
and its related ﬁelds in manifold ways. Some of these are container demand fore-
casting, freight control, assessment of transportation parameters, maintenance of
logistic infrastructures, among others. As an example, an artiﬁcial neural network
model is proposed in [66] to determine the wave agitation in Spanish ports in order
to provide a suitable anchorage for the incoming vessels. In particular, multilayer
feed-forward back-propagation neural networks are considered due to the general-
ization capabilities to estimate wave heights. The Levenberg-Marquardt algorithm
is here used to train the network, whereas a Bayesian regularisation is integrated
to avoid over-ﬁtting. The proposed model uses time series of deep-water wave buoy
observations alone to obtain new knowledge and overcome the drawbacks associated
with previous physical and numerical models used traditionally for this purpose. The
computational experiments demonstrate that, unlike classic approaches, the proposal
is more simple, does not require a large amount of data, and has a more eﬃcient per-
formance. Another application of artiﬁcial neural network to the reliability of coastal
structures is described in [47]. In this case the authors propose a model based on an
artiﬁcial neural network to estimate the armor damage sustained by a rubble-mound

A Review of Soft Computing Techniques ...
11
breakwater under wave action. A similar approach is presented in [50]. Speciﬁcally,
a combination of artiﬁcial neural network and a Monte Carlo simulation is used to
estimate damage of breakwater armor blocks. Other interesting examples of artiﬁcial
neural network in wind direction forecasting [89], wave forecasting [25], long-wave
prediction inside the port [67], or port tranquility [65], have been published in the
scientiﬁc literature.
3.2.2
Metaheuristic Techniques
Since the term metaheuristic was coined by Fred Glover in 1986, a wide variety
of deﬁnitions have been provided in the scientiﬁc literature. The paper [87] deﬁnes
“a metaheuristic as a high-level problem-independent algorithmic framework that
provides a set of guidelines or strategies to develop heuristic optimization algo-
rithms. The term is also used to refer to a problem-speciﬁc implementation of a
heuristic optimization algorithm according to the guidelines expressed in such a
framework.”
Most of the optimization problems that take place in maritime container termi-
nals belong to the N P-hard class of problems. Even in small-size scenarios of
some of these optimization problems, eﬃcient exact approaches do not exist. There-
fore, metaheuristic algorithms provide high-quality solutions within short computa-
tional times. Taking into account the classiﬁcation of these problems given in [43]
and described in Sect. 2, in the following, the application of metaheuristics to some
of the most highlighted optimization problems within the categories Ship-to-shore
and Storage are brieﬂy summarized. Genetic Algorithms (GA) [44], GRASP [33],
Tabu Search (TS) [38], Variable Neighborhood Search (VNS) [73], Large Neighbor-
hood Search (LNS) [85], Adaptive Large Neighborhood Search (ALNS) [83], and
Simulated Annealing (SA) [53] are among the most eﬀective algorithms to solve the
considered optimization problems. However, due to the very large volume of publica-
tions, the following review is limited to indicate those publications appeared over the
last years and considered as representative in the view of the authors of this chapter.
∙Ship-to-shore.
– Ship routing and scheduling.
The goal of this class of optimization problems is to determine the best journeys
of a ﬂeet of container vessel to exchange containers among a given set of mar-
itime container terminals. Depending on the operation mode, three kinds of ship
routing problems can be distinguished: liner, industrial, and tramp shipping.
Liners operate according to an agreed itinerary and schedule similar to a bus
line. In industrial shipping, the cargo owner or shipper controls the ships. Indus-
trial operators strive to minimize the costs of shipping their cargoes. Tramp
ﬂeets engage in contracts to transport speciﬁed (usually large) volumes of cargo
between two ports within a period of time. They engage in contracts to make
one or several trips, each trip having speciﬁed origin and destination ports
and time windows for picking and delivering the cargo. Tramp is usually the

12
C. Expósito-Izquierdo et al.
operation mode selected to transport liquid and dry commodities, or cargo
involving a large number of units.
The fast growth of the containership ﬂeet has resulted in a large number of
research papers about liner network design and related topics published in the
last decades. Recent reviews about ship routing problems can be found in works
[19, 20], where literature contributions are classiﬁed. Most papers about ship
routing and scheduling problems focus on the development of Mixed Integer
Programming (MIP) models or heuristic/metaheuristic methods to solve them.
Given the fact that the works by Christiansen et al. provide a comprehensive
survey about ship routing and scheduling problems, this review is limited to
some of the most recent literature works that present metaheuristic approaches
for solving this kind of problems.
The work [6] proposes an Adaptive Large Neighborhood Search heuristic for a
ship routing and scheduling problem with voyage separation requirements. [75]
proposes a Genetic Algorithm with Local Search to solve a ship routing prob-
lem. [84] combines a Simulated Annealing with a Genetic Algorithm, and [54]
uses a Tabu Search algorithm which allows infeasible solutions with respect to
ship capacity and time. Other works in the literature introduce speciﬁc concepts
in ship routing problems. [82] proposes a GRASP and discusses aspects related
to data gathering and updating, which are particularly diﬃcult in the context
of ship routing. Lastly, [55] considers a cost function that depends on the wind
speed and its direction, as well as on the wave height and its direction, and solves
the problem using a Simulated Annealing algorithm.
– Stowage planning. The goal of this class of problems is to determine the position
to be occupied by each container into a vessel taking into account the shipping
route of that vessel. Notice that once a vessel arrives at a port, shifts are not
desired. Shifts are the movements that correspond to the temporarily unloading
and re-loading of containers in order to retrieve other containers that have to be
unloaded at that port. In addition, any stowage plan has to lead to a seaworthy
vessel, whose static stability is correct and all stress forces are within some
limits as stated in [18]. [78] states that there are two main approaches to solve the
ship stowage planning problem (SSPP): single-phase approaches, which tackle
the SPP as a whole, and two-phases approaches, which consider a hierarchical
decomposition of the problem - master planning, that assigns the containers to
locations of the vessel, and slot planning, that determines the exact position of
a container within a location.
Since the publication of the paper [5] in 1993, in which exact and heuristic solu-
tions were proposed for solving the SSPP in order to minimize the number of
shiftings without considering stability constraints, many variants of the stowage
planning problem using either a single-phase or a two-phase approach have been
solved by means of metaheuristics. Genetic algorithms, GRASP, Tabu Search,
and Simulated Annealing are among the most eﬀective algorithms for the SSPP.
The work [102] presents a Genetic Algorithm based on NSGA−III combined
with a local search component to solve a multiobjective SSPP with the goals
of optimizing the ship stability and the number of re-handles. [103] presents

A Review of Soft Computing Techniques ...
13
a Genetic Algorithm to tackle the SSPP for 40-feet outbound containers. [80]
presents a GRASP algorithm to solve a generalization of the Slot Planning Prob-
lem, in which the explicit handling of rolled out containers and separations rules
for dangerous cargo are introduced. [4] proposes the use of the hybrid method
Pareto Clustering Search, which combines metaheuristics based on Simulated
Annealing and local searches to solve the 3D Container ship Loading Plan Prob-
lem. [74] proposes a two-step heuristic for solving the SSPP, which are both
based on the Tabu Search metaheuristic. [23] proposes a combination of meta-
heuristics, including Genetic Algorithm and Simulated Annealing, to solve the
3D container ship loading planning problem. [2] presents a three-step heuris-
tic for the Master Bay Plan Problem, making use of tabu search to look for the
global ship stability of the overall stowage plan. [49] proposes a Genetic Algo-
rithm to solve the simultaneous stowage and load planning for a container ship
with container rehandle in yard stacks with two objectives, ship stability and the
minimum number of shifts. [26] also proposes a Genetic Algorithm for solving
the SSPP with the goal of minimizing the number of container movements. The
work [97] also presents a Tabu Search algorithm to solve the SSPP.
– Berth allocation and Quay crane assignment.
Given a berth layout and a set of vessels to be served, the aim of the Berth
Allocation Problem (BAP) is to determine a berthing time and a berthing posi-
tion for each vessel in order to optimize a given objective function. The sci-
entiﬁc literature is replete with variants of this problem, which depend mainly
on the berth layout (i.e., discrete, continuous, and hybrid), the arrival times of
the vessels (i.e., static and dynamic) and the optimization level (i.e., strategic,
tactical, and operational, which is the most extended in the literature). Taking
into account the berth layout, a discrete quay is divided into several berths, in
which a single vessel at a time can be served. In a continuous quay, the vessels
can be assigned to any position as far as no space-time overlaps appear. In the
hybrid case, the quay is also divided into berths, but the vessels can share them.
Moreover, depending on the arrival times of the vessels, in the static case, all
the vessels are in port before the planning horizon, while in the dynamic case,
they can arrive at any time during the planning horizon. Finally, the optimiza-
tion level can be either operational, when it covers decisions ranging from one
up to several days, tactical, when the decisions cover operations ranging from
one week up to several months, and strategic, when the decisions range from
one up to some years. In addition, several objective functions have been con-
sidered in the literature. The main goal is to optimize the delays and waiting
times of container vessels at the operational level. At this level, some of the
goals are to optimize the transshipment ﬂows among vessels, cycling visiting
of the vessels, fulﬁllment of contracts among shipping companies and terminal
managers, route design, etc. At the strategic level, the problem seeks to estab-
lish speciﬁc and dedicated berths, strategic cooperation agreements between
terminal and shipping companies, etc. Note that the Quay Crane Assignment
Problem, whose aim is to assign a set of quay cranes to a vessel to perform
the loading and unloading operations, is usually solved together with the Berth

14
C. Expósito-Izquierdo et al.
Allocation Problem. Therefore, it is integrated in the variants that are related in
the literature.
If attention is focused on journal papers, it can be checked that, since the pub-
lication of the paper [48], less than three papers per year related to variants
of berth allocation were published until 2006. This number rapidly increased
to reach more than ﬁfteen papers published in 2016. Such as it is reported in
the survey [8], a large number of meta-heuristics have been eﬀective to solve
this kind of optimization problem. Genetic Algorithms, Tabu Search, Simulated
Annealing, and GRASP are among the most widely used techniques.
Given the fact that the work [8] provides a comprehensive review about BAP,
the following is limited to cite some of the papers by the authors of this book
chapter related to diﬀerent variants of BAP. The work [56] proposes a coopera-
tive search to solve the discrete dynamic BAP. In the work [59], the authors con-
sider an aditional constraint that appear in real situations, water depth and tidal
constraints. It is proposed a POPMUSIC approach (Partial Optimization Meta-
heuristic Under Special Intensiﬁcation Conditions) that includes the resolution
of an appropriate mathematical programming formulation as an embedded pro-
cedure. The paper [57] proposes a Biased Random Key Genetic Algorithm for
solving the tactical Berth Allocation Problem. Finally, the work [58] presents a
hybrid Tabu Search—Path Relinking to solve the dynamic BAP.
– Quay crane scheduling.
It is an optimization problem arisen to deal with the transshipment operations
associated with each incoming container vessel arrived at a maritime container
terminal. In particular, the stowage plan of a vessel indicates the individual con-
tainers to be loaded and unloaded onto/from it after its berthing. These con-
tainers must be handled by a subset of the quay cranes available at the ter-
minal. Because crane operations are highly expensive, terminal practitioners
must determine a suitable schedule in order to deliver high quality service while
reduce operative costs.
The Quay Crane Scheduling Problem, in short QCSP, has attracted a great deal
of interest of the soft computing community due to the fact that it introduces
a set of novel and challenging constraints in comparison with other scheduling
problems tackled to date. This is the case of, for example, the well-known Job
Shop Scheduling Problem. On one hand, the movements of the quay cranes used
to perform the transshipment operations are physically restricted because these
are mounted on a system of rails. This means that the cranes can only move
horizontally along the berthing line. Also, the quay cranes cannot cross to each
other, which gives rise to their initial relative order is kept over the planning
horizon. Lastly, the cranes have to keep a safety distance between them in order
to prevent potential collisions.
Multitude of proposals belonging to the ﬁeld of soft computing have appeared in
the scientiﬁc literature to address the QCSP. These approaches can be classiﬁed
on the basis of diﬀerent criteria. Some of these are the level of aggregation of
the containers into the stowage plan to handle, the technichal characteristics of
the machinery and their crane drivers, level of potential interferences, and per-

A Review of Soft Computing Techniques ...
15
formance measure. Firstly, the containers into the stowage plan of a vessel are
usually arranged according to their destination port or physical characteristics,
among others. This way, containers with similar characteristics can be easily
loaded/unloaded in a row at a given bay of the vessel. The level of aggregation
indicates the volume of containers to handle. At the lowest level of aggregation,
individual containers are handled by the quay cranes, whereas all the contain-
ers in a bay are handled at the highest level. Representative metaheuristics for
this variants can be found in the works [41, 90], respectively. Furthermore, one
of the most complex constraints of the QCSP is associated with the operative
chracteristics of the quay cranes. In this context, some metaheuristics have been
designed to address the impact of the temporal availability of the cranes on the
overall performance of the transshipment operations. This is the case of the
work [62]. However, other authors have also proposed eﬃcient metaheuristics
in which the movement of the quay cranes is considered as non-negligible, as
stated in the work [68]. In addition, the potential interference derived from the
presence of a rail system has been an attractive focus of research. A safety dis-
tance is considered by some metaheuristics to avoid collisions between quay
cranes. For example, this is considered in [21, 27]. Finally, a wide range of
performance measures have been already addressed in the scientiﬁc literature
so far. In particular, some high-eﬃcient metaheuristics have been designed to
minimize the completion time of the operations [64], the ﬁnishing times of the
quay cranes [22], the movement of the quay cranes [96], and maximize the crane
utilization rate [95], among others. It is worth pointing out that a comprehen-
sive review of the proposals aimed at solving the QCSP can be found in the
survey [8].
∙Storage.
– Yard crane scheduling
Given the fact that the shipping lines are the customers of a container terminal
and their payment depends on the time spent at the terminal, the main goal of
the berth allocation problem usually considers minimizing the berthing time of
the vessels. In order to minimize this time, the loading and unloading operations
have to be eﬃciently performed taking into account the available resources and
their particular eﬃciencies. Quay cranes, internal vehicles, and yard cranes play
then a crucial role. Although the speed of quay cranes has improved substan-
tially over the last years, their performance depends on the performance of yard
cranes, whose speed is approximately one-third of a modern quay crane. There-
fore, in order to fully utilize quay cranes, containers to be loaded or unloaded by
them are distributed over several blocks. Moreover, while retrieving and stack-
ing containers of ships, each yard crane has also to serve the landside. Then, the
goal of the Yard Crane Scheduling Problem is to determine an eﬃcient schedule
to carry out a set of container storage, retrieval and relocation requests around
the yard of the terminal as indicated in [36].
In the following, a reduced number of the recent references related to the Yard
Crane Scheduling Problem. [42] proposes the combination of a Genetic Algo-

16
C. Expósito-Izquierdo et al.
rithm with Particle Swarm Optimization to solve the Yard Crane Scheduling
Problem with the aim of improving the eﬃciency of the terminal and mini-
mizing energy-consumption. The authors of [94] solve the problem with two
cooperating automated stacking cranes in a single block using a mathematical
model and a Simulated Annealing based heuristic. [63] tackles the Yard Crane
Scheduling Problem with inter-crane interference, ﬁxed yard crane separation
distances, and simultaneous container storage/retrievals by means of heuris-
tics. The problem of scheduling multiple yard cranes by means of a dynamic
programming-based heuristic and of an algorithm to ﬁnd lower bounds is solved
in [77]. [76] ﬁrst carries out a theoretical investigation of the problem and then
proposes a branch-and-bound based enumerative method and heuristics for solv-
ing it. Most of the works that tackle the problem at hand propose mathematical
formulations to either solve it to optimality when it is possible or to obtain lower
bounds or heuristics/metaheuristics to provide high quality solutions in reason-
able computational times.
– Container storage
Container storage is a three-level problem that arises at maritime container
terminals, which involves deﬁning the yard layout and selecting the handling
machinery to use at a strategic level, determining the storage capacity of the
yard and the handling machinery deployment at a tactical level, and moving the
containers on the yard in a short-term at an operational level. At this last level,
during a certain planning horizon, containers arrive and leave the yard at certain
arrival and retrieval times, respectively. The objective of the yard cranes is to
perform feasible movements to store and retrieve the containers on the basis of
the intrinsic Last In First Out (LIFO) structure of the stacks. Therefore, a stor-
age movement involves the placement of the next incoming container at the top
of a stack with at least one empty slot, a retrieval movement involves taking out
the next container to retrieve from its bay whenever it is currently placed at the
top of a stack, and a relocation movement involves the movement of a container
from the top of its assigned stack to the top of another one with at least one
empty slot.
In the state-of-art of container storage, the incoming and outgoing containers
in a bay give rise to the deﬁnition of the following closely-related N P-hard
optimization problems:
· Stacking Problem [28]. It is aimed at determining the shortest sequence of
movements to be performed by the crane in order to store and retrieve the con-
tainers in/from the bay. The works [24, 51] estimate the number of relocation
movements that are required to retrieve a random container from its current
location. Also, [52] proposes a decision rule to determine the storage loca-
tions of relocated containers and to determine the containers to be retrieved
among multiple containers with similar retrieval times. [79] derives formulas
to estimate the number of relocation movements to retrieve a container from
a stack using diﬀerent stacking methods. [81] proposes several semi-greedy
construction heuristics that are used in conjunction with a discrete-event sim-

A Review of Soft Computing Techniques ...
17
ulation model to build feasible solutions for the stacking problem. Note that
these works propose heuristics for solving the problem at hand.
· Container Relocation Problem [31]. It seeks to determine the shortest
sequence of relocation movements to retrieve a subset of containers. It is
assumed that all the containers are already stored in the bay and no new
incoming containers arrive. [30] propose a heuristic algorithm aimed at solv-
ing the unrestricted blocks relocation problem considering the minimization
of the number of relocation operations. [13] presents a complete study on
the blocks relocation problem and propose an eﬀective heuristic algorithm.
[34] provides a general classiﬁcation concerning all the feasible container
relocation movements that can be performed on a given incumbent bay con-
ﬁguration. A tree search procedure is developed and a lower bound of the
minimum number of relocation movements required to retrieve all the con-
tainers from the bay is considered to prune some branches of the tree. [16]
addresses an extension of this problem in a storage area of the container
yard where incoming and outgoing containers are arranged by a straddle car-
rier. They propose several constructive algorithms and three nature inspired
metaheuristics are studied for improving the initial solutions reported by the
heuristics. [15] presents a recursive formulation and a dynamic programming
algorithm for the restricted blocks relocation problem and a corridor method.
[61] presents a heuristic method composed of three general stages executed
one after the other to retrieve the containers from a bay and move them toward
a vessel. [11] proposes a smart binary encoding for the problem that allows to
develop optimization methods without having in-depth knowledge concern-
ing the current problem features. They design an algorithm based on the pilot
metaheuristic, in which simple heuristics are included in order to compute
the suitability of neighbor stacking conﬁgurations. [52] proposes two opti-
mization methods to be applied when a container pickup sequence is given
and use a heuristic rule to determine the number of expected future container
relocation movements.
· Pre-Marshalling Problem [29]. Its objective is to ﬁnd the shortest sequence
of movements to arrange the containers within a given bay, such that any
container is placed above other container with earlier retrieval time in the
same stack. In this case, neither incoming containers nor outgoing containers
are considered. [91] solves the pre-marshaling problem to optimality using
A* and IDA*. [46] designes a biased random-key genetic algorithm. [37]
proposes a variable chromosome length genetic algorithm. [92] proposes two
metaheuristics, a Pilot method and a Max-Min Ant System, to solve the two-
dimensional pre-marshalling problem. [29] proposes a heuristic algorithm
and an instances generator for the pre-marshalling problem. [14] proposes a
metaheuristic approach based on the paradigm of the Corridor Method. [60]
designes an algorithm composed of a neighborhood search process, an integer
programming model, and three minor subroutines.

18
C. Expósito-Izquierdo et al.
4
Summary and Further Research
Container terminals are huge complex infrastructures located within the boundaries
of maritime ports. The complexity arises from the large number of heterogeneous
processes and multitude of stakeholders with conﬂicting goals that coexist in this
context. For this reason, terminal managers demand nowadays to count with eﬃ-
cient operative strategies due to the fact that these allow to achieve the established
performance and service objectives.
Up to now, most of the computational proposals aimed at improving the overall
performance of maritime container terminals assume data is not inﬂuenced by uncer-
tainty, inconsistency, nor noise. However, for example, multitude of uncertainty
sources appear in a realistic environment. For this reason, intelligent approaches
belonging to the ﬁeld of soft computing have gained much popularity over the last
decades when tackling problems in maritime logistics and its related ﬁelds. In this
regard, the chapter at hand provides a general review of the most representative con-
tributions of soft computing in this context. In particular, a brief overview of the
main applications of soft computing methodologies is here presented. This appli-
cations are organized on the basis of the main components of the soft computing:
probabilistic models, fuzzy logic, artiﬁcial neural networks, and metaheuristic tech-
niques.
Finally, it is worth mentioning that despite the eﬀorts done so far, there is still a
large number of open promising lines for further research. One of these open lines
involves the combination of both optimization techniques with online learning. This
type of approaches are very useful to predict optimized process parameters associ-
ated with the movement of containers around the terminal. Also, hybrid proposals
have demonstrated to be very eﬀective when tackling complex optimization prob-
lems in which analytical techniques cannot be applied. Some of these proposals are
genetic fuzzy systems and neural networks combined with evolutionary techniques,
among others. The former are usually designed to obtain a suitable accuracy tradeoﬀ
in optimization cycles, whereas the last are able to solve multi-objective approxima-
tion.
Acknowledgements This work has been partially funded by the Spanish Ministry of Economy and
Competitiveness (project TIN2015-70226-R) with FEDER funds.
References
1. Alyami, H., Yang, Z., Riahi, R., Bonsall, S., Wang, J.: Advanced uncertainty modelling for
container port risk analysis. Accid. Anal. Prev. (2016). doi:10.1016/j.aap.2016.08.007
2. Ambrosino, D., Anghinolﬁ, D., Paolucci, M., Sciomachen, A.: A new three-step heuristic for
the master bay plan problem. Marit. Econ. Logist. 11(1), 98–120 (2009)
3. Angeloudis, P., Bell, M.G.H.: An uncertainty-aware AGV assignment algorithm for auto-
mated container terminals. Transp. Res. Part E: Logist. Transp. Rev. 46(3), 354–366 (2010)

A Review of Soft Computing Techniques ...
19
4. Araújo, E.J., Chaves, A.A., de Salles, L.L.: Neto, and A.T. de Azevedo. Pareto clustering
search applied for 3D container ship loading plan problem. Expert Syst. Appl. 44, 50–57
(2016)
5. Avriel, M., Penn, M.: Exact and approximate solutions of the container ship stowage problem.
Comput. Ind. Eng. 25(1–4), 271–274 (1993)
6. Bakkehaug, R., Rakke, J.G., Fagerholt, K., Laporte, G.: An adaptive large neighborhood
search heuristic for ﬂeet deployment problems with voyage separation requirements. Transp.
Res. Part C: Emerg. Technol. 70, 129–141 (2016)
7. Bierwirth, C., Meisel, F.: A survey of berth allocation and quay crane scheduling problems
in container terminals. Eur. J. Oper. Res. 202(3), 615–627 (2010)
8. Bierwirth, C., Meisel, F.: A follow-up survey of berth allocation and quay crane scheduling
problems in container terminals. Eur. J. Oper. Res. 244(3), 675–689 (2015)
9. Carlo, H.J., Vis, I.F.A., Roodbergen, K.J.: Storage yard operations in container terminals: Lit-
erature overview, trends, and research directions. Eur. J. Oper. Res. 235(2), 412–430 (2014)
(Maritime Logistics)
10. Carlo, H.J., Vis, I.F.A., Roodbergen, K.J.: Transport operations in container terminals: Lit-
erature overview, trends, research directions and classiﬁcation scheme. Eur. J. Oper. Res.
236(1), 1–13 (2014)
11. Caserta, M., Schwarze, S., Vo𝛽, S.: A new binary description of the blocks relocation prob-
lem and beneﬁts in a look ahead heuristic. In: Cotta, C., Cowling, P. (eds.) Evolutionary
Computation in Combinatorial Optimization. Lecture Notes in Computer Science, vol. 5482,
pp. 37–48. Springer, Berlin (2009)
12. Caserta, M., Schwarze, S., Vo𝛽, S.: Container rehandling at maritime container terminals.
In: Böse, J.W., Sharda, R., Vo𝛽, S. (eds.) Handbook of Terminal Planning. Volume 49 of
Operations Research/Computer Science Interfaces Series, pp. 247–269. Springer, New York
(2011)
13. Caserta, M., Schwarze, S.: Vo𝛽, S.: A mathematical formulation and complexity considera-
tions for the blocks relocation problem. Eur. J. Oper. Res. 219(1), 96–104 (2012)
14. Caserta, M., Vo𝛽, S.: A corridor method-based algorithm for the pre-marshalling problem.
In: Giacobini, M., Brabazon, A., Cagnoni, S., Di Caro, G.A., Ekárt, A., Esparcia-Alcázar,
A.I., Farooq, M., Fink, A., Machado, P. (eds.) Applications of Evolutionary Computing. Vol-
ume 5484 of Lecture Notes in Computer Science, pp. 788–797. Springer, Berlin (2009)
15. Caserta, M.: Vo𝛽, S., Sniedovich, M.: Applying the corridor method to a blocks relocation
problem. OR Spectr. 33(4), 915–929 (2011)
16. Casey, B., Kozan, E.: Optimising container storage processes at multimodal terminals. J.
Oper. Res. Soc. 63, 1126–1142 (2012)
17. Chen, G., Govindan, K., Yang, Z.: Managing truck arrivals with time windows to alleviate
gate congestion at container terminals. Int. J. Prod. Econ. 141(1), 179–188 (2013)
18. Christensen, J., Pacino, D.: A matheuristic for the cargo mix problem with block stowage.
Transp. Res. Part E: Logist. Transp. Rev. 97, 151–171 (2017)
19. Christiansen, M., Fagerholt, K., Nygreen, B., Ronen, D.: Ship routing and scheduling in the
new millennium. Eur. J. Oper. Res. 228(3), 467–483 (2013)
20. Christiansen, M., Fagerholt, K., Ronen, D.: Ship routing and scheduling: status and perspec-
tives. Transp. Sci. 38(1), 1–18 (2004)
21. Chung, S.H., Chan, F.T.S.: A workload balancing genetic algorithm for the quay crane
scheduling problem. Int. J. Prod. Res. 51(16), 4820–4834 (2013)
22. Chung, S.H., Choy, K.L.: A modiﬁed genetic algorithm for quay crane scheduling operations.
Expert Syst. Appl. 39(4), 4213–4221 (2012)
23. de Azevedo, A.T., Ribeiro, C.M., de Sena, G.J., Chaves, A.A., Neto, L.L.S., Moretti, A.C.:
Solving the 3d container ship loading planning problem by representation by rules and meta-
heuristics. Int. J. Data Anal. Tech. Strateg. 6(3), 228–260 (2014)
24. de Castillo, B., Daganzo, C.F.: Handling strategies for import containers at marine terminals.
Transp. Res. Part B: Methodol. 27(2), 151–166 (1993)

20
C. Expósito-Izquierdo et al.
25. Deo, M.C., Jha, A., Chaphekar, A.S., Ravikant, K.: Neural networks for wave forecasting.
Ocean Eng. 28(7), 889–898 (2001)
26. Dubrovsky, O., Levitin, G., Penn, M.: A genetic algorithm with a compact solution encoding
for the container ship stowage problem. J. Heuristics 8(6), 585–599 (2002)
27. Expósito-Izquierdo, C., González-Velarde, J.L., Melián-Batista, B., Moreno-Vega, J.M.:
Hybrid estimation of distribution algorithm for the quay crane scheduling problem. Appl.
Soft Comput. 13(10), 4063–4076 (2013)
28. Expósito-Izquierdo, C., Lalla-Ruiz, E., de Armas, J., Melián-Batista, B., Moreno-Vega, J.M.:
A heuristic algorithm based on an improvement strategy to exploit idle time periods for the
stacking problem. Comput. Ind. Eng. 87, 410–424 (2015)
29. Expósito-Izquierdo, C., Melián-Batista, B., Moreno-Vega, J.M.: Pre-marshalling problem:
heuristic solution method and instances generator. Expert Syst. Appl. 39(9), 8337–8349
(2012)
30. Expósito-Izquierdo, C., Melián-Batista, B., Moreno-Vega, J.M.: A domain-speciﬁc
knowledge-based heuristic for the blocks relocation problem. Adv. Eng. Inf. 28(4), 327–343
(2014)
31. Expósito-Izquierdo, C., Melián-Batista, B., Moreno-Vega, J.M.: An exact approach for the
blocks relocation problem. Expert Syst. Appl. 42(1718), 6408–6422 (2015)
32. Expósito-Izquiero, C., Lalla-Ruiz, E., Lamata, T., Melián-Batista, B., Moreno-Vega, J.M.:
Fuzzy optimization models for seaside port logistics: berthing and quay crane scheduling.
In: Computational Intelligence, pp. 323–343. Springer (2016)
33. Feo, T.A., Resende, M.G.C.: A probabilistic heuristic for a computationally diﬃcult set cov-
ering problem. Oper. Res. Lett. 8(2), 67–71 (1989)
34. Forster, F., Bortfeldt, A.: A tree search procedure for the container relocation problem. Com-
put. Oper. Res. 39(2), 299–309 (2012)
35. Fransoo, J.C., Lee, C.Y.: The critical role of ocean container transport in global supply chain
performance. Prod. Oper. Manag. 22(2), 253–268 (2013)
36. Gharehgozli, A.H., Yu, Y., de Koster, R., Udding, J.T.: An exact method for scheduling a
yard crane. Eur. J. Oper. Res. 235(2), 431–447 (2014) (Maritime Logistics)
37. Gheith, M., Eltawil, A.B., Harraz, N.A.: Solving the container pre-marshalling problem using
variable length genetic algorithms. Eng. Optim. 48(4), 687–705 (2016)
38. Glover, F.: Future paths for integer programming and links to artiﬁcial intelligence. Comput.
Oper. Res. 13(5), 533–549 (1986)
39. Gucma, L., Pietrzykowski, Z.: Ship manoeuvring in restricted areas: an attempt to quantify
dangerous situations using a probabilistic-fuzzy method. J. Navig. 59(02), 251–262 (2006)
40. Gunter, H.O., Kim, K.H.: Container terminals and terminal operations. OR Spectr. 28, 437–
445 (2006)
41. Hakam, M.H., Solvang, W.D., Hammervoll, T.: A genetic algorithm approach for quay crane
scheduling with non-interference constraints at narvik container terminal. Int. J. Logist. Res.
Appl. 15(4), 269–281 (2012)
42. He, J., Huang, Y., Yan, W.: Yard crane scheduling in a container terminal for the trade-oﬀ
between eﬃciency and energy consumption. Adv. Eng. Inf. 29(1), 59–75 (2015)
43. Henesey, L.: Overview of transshipment operations and simulation. In: MedTrade Confer-
ence, Malta, pp. 6–7 (2006)
44. Holland, J.H.: Adaptation in natural and artiﬁcial systems: an introductory analysis with
applications to biology, control, and artiﬁcial intelligence. University of Michigan Press
(1975)
45. Hosseini, S., Barker, K.: Modeling infrastructure resilience using bayesian networks: a case
study of inland waterway ports. Comput. Ind. Eng. 93, 252–266 (2016)
46. Hottung, A., Tierney, K.: A biased random-key genetic algorithm for the container pre-
marshalling problem. Comput. Oper. Res. 75, 83–102 (2016)
47. Iglesias, G., Rabuñal, J., Losada, M.A., Pachón, H., Castro, A., Carballo, R.: A virtual lab-
oratory for stability tests of rubble-mound breakwaters. Ocean Eng. 35(1112), 1113–1120
(2008)

A Review of Soft Computing Techniques ...
21
48. Imai, A., Nagaiwa, K., Tat, C.W.: Eﬃcient planning of berth allocation for container termi-
nals in Asia. J. Adv. Transp. 31(1), 75–94 (1997)
49. Imai, A., Sasaki, K., Nishimura, E., Papadimitriou, S.: Multi-objective simultaneous stowage
and load planning for a container ship with container rehandle in yard stacks. Eur. J. Oper.
Res. 171(2), 373–389 (2006)
50. Kim, D.H., Kim, Y.J., Hur, D.S.: Artiﬁcial neural network based breakwater damage estima-
tion considering tidal level variation. Ocean Eng. 87, 185–190 (2014)
51. Kim, K.H.: Evaluation of the number of rehandles in container yards. Comput. Ind. Eng.
32(4), 701–711 (1997)
52. Kim, K.H., Hong, G.P.: A heuristic rule for relocating blocks. Comput. Oper. Res. 33(4),
940–954 (2006)
53. Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P.: Optimization by simulated annealing. Science
220, 671–680 (1983)
54. Korsvik, J.E., Fagerholt, K.: A tabu search heuristic for ship routing and scheduling with
ﬂexible cargo quantities. J. Heuristics 16(2), 117–137 (2010)
55. Kosmas, O.T., Vlachos, D.S.: Simulated annealing for optimal ship routing. Comput. Oper.
Res. 39(3), 576–581 (2012)
56. Lalla-Ruiz, E., Expósito-Izquierdo, C., Melián-Batista, B., Moreno-Vega, J.M.: A set-
partitioning-based model for the berth allocation problem under time-dependent limitations.
Eur. J. Oper. Res. 250(3), 1001–1012 (2016)
57. Lalla-Ruiz, E., González-Velarde, J.L., Melián-Batista, B., Moreno-Vega, J.M.: Biased ran-
dom key genetic algorithm for the tactical berth allocation problem. Appl. Soft Comput. 22,
60–76 (2014)
58. Lalla-Ruiz, E., Melián-Batista, B., Moreno-Vega, J.M.: Artiﬁcial intelligence hybrid heuris-
tic based on tabu search for the dynamic berth allocation problem. Eng. Appl. Artif. Intell.
25(6), 1132–1141 (2012)
59. Lalla-Ruiz, E., Voß, S., Expósito-Izquierdo, C., Melián-Batista, B., Moreno-Vega, J.M.: A
popmusic-based approach for the berth allocation problem under time-dependent limitations.
Ann. Oper. Res. 1–27 (2015)
60. Lee, Y., Chao, S.L.: A neighborhood search heuristic for pre-marshalling export containers.
Eur. J. Oper. Res. 196(2), 468–475 (2009)
61. Lee, Y., Lee, Y.J.: A heuristic for retrieving containers from a yard. Comput. Oper. Res. 37(6),
1139–1147 (2010)
62. Legato, P., Trunﬁo, R., Meisel, F.: Modeling and solving rich quay crane scheduling prob-
lems. Comput. Oper. Res. 39(9), 2063–2078 (2012)
63. Li, W., Wu, Y., Petering, M.E.H., Goh, M., de Souza, R.: Discrete time model and algorithms
for container yard crane scheduling. Eur. J. Oper. Res. 198(1), 165–172 (2009)
64. Liu, M., Zheng, F., Li, J.: Scheduling small number of quay cranes with non-interference
constraint. Optim. Lett. 9(2), 403–412 (2015)
65. Londhe, S.N., Deo, M.C.: Wave tranquility studies using neural networks. Marine Struct.
16(6), 419–436 (2003)
66. López, I., López, M., Iglesias, G.: Artiﬁcial neural networks applied to port operability
assessment. Ocean Eng. 109, 298–308 (2015)
67. López, M., Iglesias, G.: Artiﬁcial intelligence for estimating infragravity energy in a harbour.
Ocean Eng. 57, 56–63 (2013)
68. Lu, Z., Han, X., Xi, L., Erera, A.L.: A heuristic for the quay crane scheduling problem based
on contiguous bay crane operations. Comput. Oper. Res. 39(12), 2915–2928 (2012)
69. McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in nervous activity.
Bull. Math. Biophys. 5(4), 115–133 (1943)
70. Meisel, F.: Operational planning problems. Seaside Operations Planning in Container Ter-
minals. Contributions to Management Science, pp. 17–30. Physica-Verlag, HD (2009)
71. Meisel, F.: Seaside Operations Planning in Container Terminals. Springer (2009)
72. Meisel, F., Bierwirth, C.: A framework for integrated berth allocation and crane operations
planning in seaport container terminals. Transp. Sci. 47(2), 131–147 (2013)

22
C. Expósito-Izquierdo et al.
73. Mladenović, N., Hansen, P.: Variable neighborhood search. Comput. Oper. Res. 24(11),
1097–1100 (1997)
74. Monaco, M.F., Sammarra, M., Sorrentino, G.: The terminal-oriented ship stowage planning
problem. Eur. J. Oper. Res. 239(1), 256–265 (2014)
75. Moon, I.K., Qiu, Z.B., Wang, J.H.: A combined tramp ship routing, ﬂeet deployment, and
network design problem. Marit. Policy Manag. 42(1), 68–91 (2015)
76. Narasimhan, A., Palekar, U.S.: Analysis and algorithms for the transtainer routing problem
in container port operations. Transp. Sci. 36(1), 63–78 (2002)
77. Ng, W.C.: Crane scheduling in container yards with inter-crane interference. European Jour-
nal of Operational Research 164(1), 64–78 (2005)
78. Pacino, D., Delgado, A., Jensen, R.M., Bebbington, T.: Fast generation of near-optimal plans
for eco-eﬃcient stowage of large container vessels. In: International Conference on Compu-
tational Logistics, pp. 286–301. Springer (2011)
79. Park, T.K., Kim, K.H.: Comparing handling and space costs for various types of stacking
methods. Comput. Ind. Eng. 58(3), 501–508 (2010)
80. Parreño, F., Pacino, D., Álvarez-Valdés, R.: A grasp algorithm for the container stowage slot
planning problem. Transp. Res. Part E: Logist. Transp. Rev. 94, 141–157 (2016) (cited By 0)
81. Rei, R.J., Pedroso, J.P.: Heuristic search for the stacking problem. Int. Trans. Oper. Res. 19(3),
379–395 (2012)
82. Romero, G., Durán, G., Marenco, J., Weintraub, A.: An approach for eﬃcient ship routing.
Int. Trans. Oper. Res. 20(6), 767–794 (2013)
83. Ropke, S., Pisinger, D.: An adaptive large neighborhood search heuristic for the pickup and
delivery problem with time windows. Transp. Sci. 40(4), 455–472 (2006)
84. Santhanakrishnan, S., Narendran, T.T., Ganesh, K., Anbuudayasankar, S.P.: Comparison of
meta-heuristics for container ship routing problem. Int. J. Serv. Oper. Manag. 12(3), 348–367
(2012)
85. Shaw, P.: Using constraint programming and local search methods to solve vehicle routing
problems. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁ-
cial Intelligence and Lecture Notes in Bioinformatics) 1520, 417–431 (1998)
86. Smoczek, J.: Fuzzy crane control with sensorless payload deﬂection feedback for vibration
reduction. Mech. Syst. Signal Process. 46(1), 70–81 (2014)
87. Sörensen, K., Glover, F.: Metaheuristics. In: Gass, S., Fu, M. (eds.) Encyclopedia of Opera-
tions Research and Management Science, 3rd edn., pp. 960–970. Springer Science & Busi-
ness Media (2013)
88. Sutrisnowati, R.A., Bae, H., Song, M.: Bayesian network construction from event log for
lateness analysis in port logistics. Comput. Ind. Eng. 89, 53–66 (2015) (Maritime logistics
and transportation intelligence)
89. Tagliaferri, F., Viola, I.M., Flay, R.G.J.: Wind direction forecasting with artiﬁcial neural
networks and support vector machines. Ocean Engineering 97, 65–73 (2015)
90. Tang, L., Zhao, J., Liu, J.: Modeling and solution of the joint quay crane and truck scheduling
problem. Eur. J. Oper. Res. 236(3), 978–990 (2014)
91. Tierney, K., Pacino, D., Vo𝛽, S.: Solving the pre-marshalling problem to optimality with a∗
and ida∗. Flex. Serv. Manuf. J. 1–37 (2016)
92. Tus, A., Rendl, A., Raidl, G.R.: Metaheuristics for the two-dimensional container pre-
marshalling problem. Lecture Notes in Computer Science (including subseries Lecture Notes
in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) 8994, 186–201 (2015)
93. Verdegay, J.L., Yager, R.R., Bonissone, P.P.: On heuristics as a fundamental constituent of
soft computing. Fuzzy Sets Syst. 159(7), 846–855 (2008)
94. Vis, I.F.A., Carlo, H.J.: Sequencing two cooperating automated stacking cranes in a container
terminal. Transp. Sci. 44(2), 169–182 (2010)
95. Vis, I.F.A., van Anholt, R.G.: Performance analysis of berth conﬁgurations at container ter-
minals. OR Spectr. 32(3), 453–476 (2010)
96. Wang, S., Zheng, J., Zheng, K., Guo, J., Liu, X.: Multi resource scheduling problem based
on an improved discrete particle swarm optimization. Phys. Proc. 25, 576–582 (2012)

A Review of Soft Computing Techniques ...
23
97. Wilson, I.D., Roach, P.A.: Principles of combinatorial optimization applied to container-ship
stowage planning. J. Heuristics 5(4), 403–418 (1999)
98. Xie, G., Wang, S., Zhao, Y., Lai, K.K.: Hybrid approaches based on LSSVR model for con-
tainer throughput forecasting: a comparative study. Appl. Soft Comput. 13(5), 2232–2241
(2013)
99. Yan, J., Sun, X., Liu, J.J.: Assessing container operator eﬃciency with heterogeneous and
time-varying production frontiers. Transp. Res. Part B: Methodol. 43(1), 172–185 (2009)
100. Yeo, G.T., Ng, A.K.Y., Lee, P.T.W., Yang, Z.: Modelling port choice in an uncertain envi-
ronment. Marit. Policy Manag. 41(3), 251–267 (2014)
101. Zadeh, L.A.: Fuzzy logic (abstract): Issues, contentions and perspectives. In: Proceedings of
the 22nd Annual ACM Computer Science Conference on Scaling Up: Meeting the Challenge
of Complexity in Real-world Computing Applications: Meeting the Challenge of Complexity
in Real-world Computing Applications, CSC ’94, New York, NY, USA, p. 407. ACM (1994)
102. Zhang, Z., Lee, C.Y.: Multiobjective approaches for the ship stowage planning problem con-
sidering ship stability and container rehandles. IEEE Trans. Syst. Man Cybern.: Syst. 46(10),
1374–1389 (2016)
103. Zhao, N., Shen, Y., Xia, M., Jin, J.: A novel strategy for stowage planning of 40 feet containers
in container terminals. J. Marine Sci. Technol. (Taiwan) 24(1), 61–74 (2016)

Intelligent Data Analysis, Soft Computing
and Imperfect Data
Jose M. Cadenas and M. Carmen Garrido
Abstract In diﬀerent real problems the available information is not as precise or
as accurate as we would like. Due to possible imperfection in the data (understand-
ing that these contain data where not all the attributes are precisely known, such
as missing, imprecise, uncertain, ambiguous, etc. values), tools provided by Soft
Computing are quite adequate, and the hybridization of these tools with the Intelli-
gent Data Analysis is a ﬁeld that is gaining more importance. In this paper, ﬁrst we
present a brief overview of the diﬀerent stages of Intelligent Data Analysis, focus-
ing on two core stages: data preprocessing and data mining. Second, we perform an
analysis of diﬀerent hybridization approaches of the Intelligent Data Analysis with
the Soft Computing for these two stages. The analysis is performed from two levels:
If elements of Soft Computing are incorporated in the design of the method/model,
or if they are also incorporated to be able to deal with imperfect information. Finally,
in a third section, we present in more detail several methods which allow the use of
imperfect data both for their learning phase and for the prediction.
1
A Brief Overview of Intelligent Data Analysis
Intelligent data analysis (IDA) or knowledge discovery in databases is deﬁned in
[23] as the “non-trivial process of identifying valid, novel, potentially useful and
understandable (if not immediately, with some kind of further processing) patterns
from the data”. As it follows from this deﬁnition, in the IDA process, the data are the
most important part of the discipline [23] and it is a complex process that includes
the obtaining of the models and also other stages.
J.M. Cadenas (✉) ⋅M.C. Garrido
Department of Information and Communications Engineering,
University of Murcia, Murcia, Spain
e-mail: jcadenas@um.es
M.C. Garrido
e-mail: carmengarrido@um.es
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_2
25

26
J.M. Cadenas and M.C. Garrido
IDA is divided into the following stages, [23]:
∙The “Data Integration and Collection” (selection) stage.
∙The “Data Preprocessing” stage, related to the treatment of the data and the strate-
gies that would be used to handle the available information.
∙The “Data Mining” stage, related to the selection and application of the appropri-
ate methods for the modeling from the available data and the obtaining of under-
standable models and high accuracy.
∙The “Evaluation (interpretation) and Diﬀusion” stage.
Although all stages are fundamental to the development of the IDA process, its
core is in the data preprocessing and data mining stages.
1.1
Data Preprocessing Stage
Data preprocessing can have a great impact on the performance of the data mining
methods, [27]. One of the problems that must be faced in this stage is to understand
and analyze the nature of the data avoiding the loss of useful information during
the process. This stage includes, among others, the cleaning of data (such as the
elimination of inconsistent data, treatment of missing values, etc.), data integration
(multiple sources), data transformation (discretization, etc.) and reduction of data
(attribute/instance selection) [27].
Speciﬁcally, the “discretization of continuous attributes” plays a critical role in
IDA and has been studied in depth. Discretization consists in dividing the values
of a numerical (continuous) attribute into a set of intervals. By means of the dis-
cretization, a numerical attribute can be more concise and easier to understand. In
the general description of the discretization process, we can do the following taxon-
omy (there are other taxonomies for the diﬀerent discretization methods such as that
presented in [51]):
∙Top-down methods: The attribute domains are progressively cut to construct a set
of intervals.
∙Bottom-up methods: They start with the individual values in the dataset that are
fused progressively until constructing a set of intervals.
Among the top-down methods we ﬁnd the ones proposed in [15, 33, 34, 46,
81]. Besides, the decision trees construction methods, such as, ID3 [71] and C4.5
[72], can be interpreted as top-down discretization methods. Among the bottom-up
methods we ﬁnd methods such as those proposed in [9, 44, 52]. All these methods
generate classical discretization, i.e., crisp intervals.
Also, the “attribute selection” plays an important role in the IDA process and
more speciﬁcally in the classiﬁcation task. On the one hand the computational cost
is reduced and on the other hand, a model is constructed from the simpliﬁed data and
this improves the general abilities of classiﬁers. The ﬁrst motivation is clear, since
the computation time to build models is lower with a smaller number of attributes.

Intelligent Data Analysis, Soft Computing and Imperfect Data
27
The second reason indicates that when the dimension is small, the risk of “overﬁt-
ting” is reduced. Removing insigniﬁcant attributes of datasets can make the model
more transparent and more comprehensible providing a better explanation of the sys-
tem [53]. Therefore, the attribute selection addresses the problem of reducing dataset
dimensionality by identifying an available attributes subset. Researchers have stud-
ied various aspects of attribute selection. One of the key aspects is to measure the
goodness of an attribute subset determining an optimal one. Depending on evalua-
tion criteria, attribute selection methods can be divided into the following categories,
[29, 75]:
∙Filter methods: These methods select subsets of attributes as a preprocessing step,
independently of the chosen classiﬁer.
∙Wrapper methods: These methods use a method of data mining as a black-box to
score attribute subsets according to their predictive power.
∙Embedded methods: These methods select attributes in the training process and
are usually speciﬁc to the given modeling method.
∙Hybrid methods: These methods are a combination of ﬁlter and wrapper methods.
Hybrid methods use the ranking information obtained using ﬁlter methods to guide
the search in the optimization algorithms used by wrapper methods.
In literature we can ﬁnd a variety of methods to carry out attribute selection, such
as the proposed in [3, 42, 52, 75].
1.2
Data Mining Stage
The data mining (DM) stage is the more characteristic stage in the IDA process.
The purpose of DM is the construction of models based on the data to produce
new knowledge that can be used by the user. The model is a description of patterns
and relationships in the data, which can be used to make predictions in a particu-
lar area, better understand the domain, improve performance or explain past situ-
ations. In practice, there are two types of models: Predictive (identify patterns to
estimate future values using predictor attributes) and Descriptive (identify patterns
that explain the data). In addition, diﬀerent types of tasks are distinguished in DM.
Each task has its own requirements and obtains a type of knowledge diﬀerent from
the obtained one by other tasks. Among the aimed tasks that obtain predictive mod-
els, we can ﬁnd both the classiﬁcation and the regression tasks; while clustering and
association are tasks aimed at obtaining descriptive models. This stage includes the
choice of the most appropriate task for the problem, the choice of the DM method,
and ﬁnally the use and adaptation to the problem of the selected method, [27, 85].
We group these methods according to the type of model obtained. Without being
exhaustive, we ﬁnd models represented by discriminant functions, decision trees,
neural networks, based on rules or based on instances.

28
J.M. Cadenas and M.C. Garrido
∙One of the most useful ways of representing a model is through a set of discrimi-
nant functions. The model in this case can be seen as a machine which computes
c discriminant functions gi(x) and which selects for x the class 𝜔i with the highest
value for the discriminant function [22, 26]. In this way the model is expressed
as gi(x) = P(𝜔i∕x), such that the maximum discriminant function is the maximum
a posteriori probability. When the discriminant functions are linear functions we
ﬁnd methods such as descending gradient, Newton’s algorithm, the Perceptron cri-
terion [32, 36]. When the discriminant functions are complex density functions,
these can be approximated by a mixture of simpler density functions.
∙The models based on instances approximate an unknown density function using
an averaged version of the density based on the probability of a speciﬁc vector’s
falling within a certain region of the attribute space [22]. The methods based on
these models have no learning phase since the model is formed by the dataset
instances. There are two common methods based on these models: Parzen method
and k neighbors method [21, 31, 58].
∙The methods which model the problem through decision trees are useful for ﬁnd-
ing structures in high dimensionality spaces or when the conditional densities of
the classes are unknown or are multimodal. Some basic and well-known methods
to generate decision/regression trees are ID3 [71], C4.5 [72] and CART [10].
∙Rules based methods model a system through a base of rules (if-then) constructed
from the instances. Some methods for obtaining rules (association rules) are based
on the concept of frequent items sets and use counting and minimum support
methods [79]. Other methods obtain rules covering the instances (cover meth-
ods) such as those based on CN2 [17] and AQ algorithms [57]. Genetic algo-
rithms/programming [51, 86] have also been used to generate rules.
∙Other type of model is the neural network. Neural networks are a very power-
ful computation paradigm allowing complex problems with possible non linear
interactions between the attributes. Among the most important neural networks
we can ﬁnd the multilayer Perceptron which generates more than one boundary of
separating in the attributes space [32, 36, 74].
∙There is a further group of methods whose aim is to generate groupings of data
and these are known as clustering. The aim of cluster analysis is to ﬁnd a valid and
convenient organization for the data and an underlying structure. Within these
methods we can include Kohonen’s self-organizing maps [45], those based on
the K-means algorithms which obtain partitional cluster [62], in contrast to the
hierarchical methods which do not establish a priori the groups number [25].
2
Intelligent Data Analysis and Soft Computing
In [56] several paradigms introduced with the data analysis are identiﬁed. Among
them, the management and processing of data respecting the true nature of them
(imperfect data) are included. Therefore, by focusing on the data, and before applying
any stage of the IDA process, we must take into account the nature of these data

Intelligent Data Analysis, Soft Computing and Imperfect Data
29
to ensure the success of the process. This means that depending on the nature and
precision of these data, we must apply diﬀerent methods depending on their degree
of tolerance to them. A clear example to illustrate the problem of the diﬀerent nature
of the data and the importance of tolerance to diﬀerent types of imperfect data is the
problem of parking a car [91], where most of the population is able to do it easily.
Therefore, we need methods that can extract knowledge and handle imperfect data,
in order to provide quality information and generate useful knowledge.
Generally, the IDA process uses and combines diﬀerent methods and tools from a
variety of disciplines [5]. Due to possible imperfection in the data, tools provided by
the Fuzzy Sets theory [90] and, in general, Soft Computing (SC) [7, 82, 91] are quite
adequate. In this way, the hybridization of the SC methods with IDA is a ﬁeld that is
gaining more importance. The methods proposed by SC and their applications have
been very important in recent years, and in particular, the advances in the hybridiza-
tion of SC with IDA are aimed at obtaining more ﬂexible methods with results more
eﬃcient compared to the classical methods [30, 61]. In this framework, we comment
on diﬀerent methods proposed from two levels: In a ﬁrst level, if the SC elements
are incorporated in the design of methods/models; and, in a second level, if they are
incorporated for the treatment of imperfect information, additionally.
2.1
Data Preprocessing in Soft Computing Framework
In the data preprocessing stage, SC has generally been applied to the design of ﬂex-
ible methods for the diﬀerent tasks of this stage. Although most of them use SC
in their development, to our knowledge, the methods that allow and management
imperfect data are seldom studied.
In particular, in the discretization of numerical attribute we ﬁnd methods that
allow the use of membership degree to intervals (denoted by fuzzy discretization
methods). These methods are grouped according to the used algorithm.
∙Decision tree based methods: In [40, 43, 63] diﬀerent approaches for the fuzzy
discretization of numerical attributes are proposed. All of them use a fuzzy deci-
sion tree combined with some basic strategy.
∙Clustering based methods: These methods are based on dividing a numerical
attribute domain into fuzzy partitions by using fuzzy clustering. In particular, sev-
eral methods using the fuzzy c-means method are proposed in [59, 64, 80].
∙Genetic algorithm based methods: The genetic algorithms (GA) are combined
with existing specialized methods to create hybrid algorithm that improve the over-
all results. In particular, we can ﬁnd several methods, [16, 18], using strategies of
classical/fuzzy discretization together a genetic algorithm to optimize the number
of partitions, interval limits and the degree of overlaps of these limits.
∙Hybrid methods: In the literature we can also ﬁnd methods based on combinations
of two or more methods. In [88] a cluster and a neural network (NN) are used. In

30
J.M. Cadenas and M.C. Garrido
[76] the combination of the FCM clustering algorithm and a GA are used, and in
[48, 73, 84] a kd-tree and a minimum spanning tree are used.
In attribute selection, there are a lot of methods using SC in their development
but they perform the selection from crisp data.
∙Attribute selection methods using SC for their design can be ﬁnd in [3, 16, 42]
where a neural network, a GA or an ant colony (AC) are used, respectively. There
are other methods that also use elements of the fuzzy set theory as in [53, 83]
where a fuzzy criteria or fuzzy entropy are used, or in [87] where the attribute
selection is performed using the fuzzy evidence theory.
∙To perform the attribute selection from imperfect data we can ﬁnd several pro-
posals: in [41] a method taking into account the uncertainty in the data through
fuzzy-rough sets is presented. This method employs fuzzy-rough sets to provide
a means by which discrete or real-valued noisy data (or a mixture of both) can be
eﬀectively reduced without the need for user-supplied information. In [77, 78] a
fuzzy mutual information measure between two fuzziﬁed numerical attributes to
handle imprecise data is used (they deﬁne a new extended version of Battiti’s ﬁlter
attribute selection method). This measure is used in combination with a genetic
optimization to deﬁne the method proposed.
Table 1 shows the summary of papers discussed.
Table 1
Hybridization of data preprocessing with Soft Computing: summary of papers
Method based on ...
SC at method
level
SC at minable view level
Allowed data
Fuzzy
discretization
Fuzzy decision trees
[40, 43, 63]
–
–
Fuzzy clustering
[59, 64, 80]
–
–
GA to optimize
[16, 18]
–
–
kd tree—spanning tree
[48, 73, 84]
–
–
Cluster—GA
[76]
–
–
Cluster—NN
[88]
–
–
Attribute
selection
NN, GA, AC
[3, 16, 42]
[77, 78]
Fuzzy sets
Fuzzy criteria/entropy
[53, 83]
–
–
Fuzzy evidence theory
[87]
–
–
Fuzzy-rough metric
–
[41]
Fuzzy-rough
sets

Intelligent Data Analysis, Soft Computing and Imperfect Data
31
2.2
Data Mining in Soft Computing Framework
SC has also been applied in the DM stage, and, to our knowledge, the methods that
allow and management imperfect data are seldom studied. From this, we can consider
the DM methods hybridized with SC in two levels:
∙At the level of generated models: Methods that generate models described in the
framework of SC. These models are more interpretable and we can ﬁnd elements
of SC in rule-based systems, methods based on k-nearest neighbors, decision trees,
clustering and support vector machines.
In 1971 Zadeh proposed the design of rules if-then using linguistic variables
that can be provided by a group of experts or obtained through DM methods.
So, among others, in [4] a set of fuzzy rules is obtained using a method based
on genetic programming, in [24] a set of fuzzy rules is obtained in unbalanced
problems using a genetic selection process of rules, in [37] diﬀerent weights are
assigned to a set of fuzzy rules using heuristic methods, and, in [65] an initial set
of fuzzy rules is constructed by clustering and then are optimized using a neuro-
fuzzy learning algorithm.
Among the fuzzy versions of the k-nearest neighbors rule we can highlight works
that assign memberships degree of each instance to each class, use fuzzy distance
measures, use diﬀerent ways of combining the votes of neighbors, etc. A complete
review of these methods is carried out in [20].
Also, fuzzy decision trees have been designed as the proposed in [66] that obtains
the best fuzzy partition of the best attribute in each node to split. Using fuzzy
decision trees, fuzzy ensembles are proposed as in [19] where an ensemble is con-
structed from a non-fuzzy tree construction algorithm that subsequently is trans-
formed to fuzzy.
With the aim to construct data partitions that allow an instance belongs to more
than one partition, fuzzy clustering algorithms have been developed such as the
fuzzy C-means proposed in [6]. Diﬀerent versions of this algorithm are found
in [35] to extend it to nominal data, in [49] to deal with missing values through
intervals or in [80] to deal with fuzzy values.
Also, fuzzy versions of support vector machines have been designed. So, in [50] a
membership degree to each class is assigned to each instance, allowing that each
one contributes in a diﬀerent way in the learning of the decision surface. In [1]
a method for multilabel classiﬁcation is generalized. For each multilabel class,
a region with the associated membership function is deﬁned and an instance is
classiﬁed into a multilabel class whose membership function is the largest.
∙At minable view level: Methods that besides incorporating the SC elements, sup-
port input imperfect data. In this case, the methods allow us that the data are com-
posed of attributes described by imperfect values. This generates the following
advantages: (1) methods can interpret the imprecision/uncertainty expressed in the
data and generate robust models to these types of information without transform-
ing the true nature of them; (2) data preprocessing is simpliﬁed by not carrying out
these transformations (replacement, deleting data, ...); and (3) the minable view

32
J.M. Cadenas and M.C. Garrido
contains a greater number of instances because the imprecise and uncertain data
are not discarded. In general, signiﬁcant eﬀorts are being carried out to incorpo-
rate the treatment of imperfect data into DM methods using SC.
Thus we can ﬁnd works that incorporate the treatment of fuzzy values. There are
fuzzy decision trees based on a fuzzy partition of numerical attributes. This par-
tition is used in the test of nodes as in [38, 47]. Fuzzy partitions of numerical
attributes are also use in the construction of fuzzy ensembles to incorporate fuzzy
values. This approach is used in [39] where to select the test of each node, the set of
the best attributes for partitioning that node is used or in [55] where a fuzzy ensem-
ble for each class value of the problem is constructed. In [60] a fuzzy version of
multilayer perceptron is presented which performs the learning from fuzzy values.
In [68] a genetic classiﬁer based on fuzzy rules is obtained from data described
with fuzzy values. In [69, 70] Adaboost and FURIA algorithms are extended in
order to obtain fuzzy rules from this type of values. In [67] an algorithm to obtain
a set of fuzzy association rules from a fuzzy partition is proposed. As particular
cases of fuzzy values, some works deal with values expressed by intervals as in
[47, 67–70].
On the other hand, the set of methods that allow the existence of missing values is
considerable. We highlight only a few that allow the treatment of some other type
of imperfect information as [38, 39] or as in [47], where missing values are only
allowed in the classiﬁcation phase.
Finally, there is a considerable set of methods that have considered the possibility
that an instance has more than one associated class value (multi-valued class),
but few extend this possibility to other nominal attributes of a problem (multi-
valued attributes). So, among the ﬁrst we can ﬁnd works as [68] where class may
be deﬁned by a crisp set, or [89] where a fuzzy k-nearest neighbor method is used
to allow that an instance can belong to more than one class with several degrees.
In [54] we can ﬁnd a comparison of this kind of methods.
Table 2 shows the summary of papers discussed.
Table 2
Hybridization of data mining with Soft Computing: summary of papers
Method based on ...
SC at method
level
SC at minable view level
Allowed data
Fuzzy rules
[4, 24, 37, 65]
[67–70]
Fuzzy sets, intervals
–
[68]
Fuzzy sets, intervals, multivalued
class
k-nearest neighbors
[20]
[89]
Multivalued class
Fuzzy decision trees
[19, 66]
[55]
Fuzzy sets
[38, 39]
Fuzzy sets, missing
[47]
Fuzzy sets, intervals, missing
Fuzzy clustering
[6, 35, 49, 80]
[35, 49, 80]
Nominal, fuzzy sets, intervals
Support vector m
[1, 50]
[1]
Multivalued class
Neural network
–
[60]
Fuzzy sets

Intelligent Data Analysis, Soft Computing and Imperfect Data
33
3
Hybridization on the Two Level of Soft Computing
and Data Preprocessing/Mining Methods
In this section we describe the characteristic elements of two methods in the data
preprocessing stage and three methods in DM stage that use SC in the two levels
commented: at model/technique level and at minable view level. Due to the high
ﬂexibility in the design of these methods, they can easily be extended to support
new types of imperfect data.
A more detailed analysis of these methods can be found in papers [11, 13] for the
preprocessing methods and papers [8, 12, 14, 28] for the DM ones.
3.1
Notation, Types and Representation of Imperfect Values
Let us consider a set of instances E, where each instance 𝐱is characterized by n
attributes in a vector (x1, x2, … , xn) (the n-th attribute represents the class). The
domains of each attribute, 𝛺x1, 𝛺x2, … , 𝛺xn−1, can be numerical or nominal, while
the domain of the class 𝛺xn (nominal attribute) can take the values {𝜔1, 𝜔2, … , 𝜔I}.
The numerical attributes are represented by fuzzy sets with a trapezoidal fuzzy
membership function [2] 𝜇(x) deﬁned by a quadruple (a, b, c, d):
𝜇(x) =
⎧
⎪
⎪
⎨
⎪
⎪⎩
0
x < a or x ≥d
x−a
b−a
a ≤x < b
1
b ≤x < c
d−x
d−c
c ≤x < d
With this representation, the methods use the following values:
∙Crisp values are represented by the quadruple (a, a, a, a).
∙Interval values [a, b] are represented by the quadruple (a, a, b, b).
∙Fuzzy values are represented by trapezoidal fuzzy membership functions.
∙Missing values include pieces of information that are unknown. These values are
represented by the quadruple (mini, mini, maxi, maxi), where mini and maxi are,
respectively, the minimum and maximum values of 𝛺xi included in the dataset.
The nominal attributes (including the class attribute) are represented by fuzzy
subsets {𝜇(h1)∕h1, … , 𝜇(hs)∕hs}, where hj is a value into attribute domain and ∃hk ∈
𝛺i ∶𝜇(hk) = 1. With this representation, the methods use the following values:
∙Crisp values are represented by the fuzzy subset {1∕hj}.
∙Crisp subset values consider more than a possible nominal value. They are repre-
sented as {1∕h1, … , 1∕hs}.

34
J.M. Cadenas and M.C. Garrido
∙Fuzzy subset values consider more than one nominal value with a membership
value 𝜇∈[0, 1]. They are represented using the notation introduced above.
∙Missing nominal values are represented using a fuzzy subset that contains all pos-
sible values with membership degree equals to 1.
3.2
OFP_CLASS: A Hybrid Method for Attribute
Discretization
In [13], OFP_CLASS method is proposed to data preprocessing. It is a hybrid method
for discretizing numerical (continuous) attributes by means of fuzzy sets, which con-
stitute a fuzzy partition of the domains of these attributes. The aim of this method
is to ﬁnd an attribute partition so that the fuzzy classiﬁcation methods obtain better
results. The OFP_CLASS method can deal with datasets with imperfect values and
it is labeled as supervised, local, top down, and incremental, using the entropy as
measure to obtain the partition.
The OFP_CLASS method is composed of two stages (Fig. 1): (a) In the ﬁrst stage,
crisp intervals are deﬁned for each attribute using a fuzzy decision tree (FDT); and
(b) in the second stage, these intervals are used as the starting point to form an opti-
mal fuzzy partition for classiﬁcation. In this second stage, a genetic algorithm is used
to determine the cardinality and fuzzy boundary of these intervals.
The partition obtained for each attribute guarantees:
∙Completeness (no point in the domain is outside the fuzzy partition), and
∙Strong fuzzy partition (it veriﬁes that ∀x ∈𝛺i, ∑Fi
f=1 𝜇Bf (x) = 1, where B1, … , BFi
are the Fi fuzzy sets for the partition corresponding to the i-th numerical attribute
with 𝛺i domain).
The FDT used in the ﬁrst stage allows the dealing of imperfect data, and for this,
uses a speciﬁc information gain, Gi, for each attribute i in order to choose the best
attribute to divide a node. Function Gi uses the standard information associated with
the node (taking into account the membership degree of an instance to the node and
P1
*
P1
v11={v11,..,v1u}
Pi
*
Pi
vi={vi1,,..,vin}
DataSet
At1
Ati
P1={p11,..,p1u}
Pi ={pi1,..,pin}
…        …
…        …
if P1≠Ø
Ω = [0,1] i
First Stage
if Pi≠Ø
Second Stage
…        …
…        …
pij
pij–vi1
pij+vi1
0
…
p1k–v11
p1k+v11
0
…
p1k
…
…
Fig. 1
Scheme for the discretization of numerical attributes using the OFP_CLASS method

Intelligent Data Analysis, Soft Computing and Imperfect Data
35
the membership degree of example ej to each class) and a factor which represents
the standard information obtained by dividing the node using attribute i adjusted
to the existence of missing values. We must highlight that, in the second stage, the
ﬁtness function of the genetic algorithm is deﬁned by
∑n
i=1 Ii
∑n
i=1 Hi where Ii and Hi are the
information gain and entropy of attribute i respectively, taking into account the crisp
intervals obtained in the ﬁrst stage.
OFP_CLASS method is an eﬀective strategy and it obtains very good results when
is compared with other methods of the literature. These results have been validated
by applying statistical techniques to analyze the behavior of diﬀerent methods in
each experiment.
3.3
FRF_fs: A Filter-Wrapper Method for Attribute Selection
In [11] is proposed the FRF_fs method of attribute selection to data preprocessing
which can handle imperfect data. This method is based on a Fuzzy Random For-
est ensemble (a method that supports imperfect data, [8, 12]) and is classiﬁed as a
Filter-Wrapper method with sequential forward selection on the subset of attributes
obtained by the Filter method and using a ranking obtained with these attributes.
This method consists of the following main steps (Fig. 2): (1) Scaling and discretiza-
tion process of the attribute set; and attribute pre-selection using the discretization
process (Filter); (2) Ranking process of the attribute pre-selection; and (3) Wrapper
attribute selection based on cross-validation.
Note that in each step the approach obtains information useful to the user (pre-
selected attribute subset, pre-selected attribute subset ranking and optimal attribute
subset). Some details of these steps are discussed below.
∙Filter method for attribute pre-selection
Initially, the method carry out a scaling and discretization (in [13], a hybrid method
for the fuzzy discretization of numerical attributes is presented), and as in the
discretization process some attributes may be discretized into a single interval,
Data preprocess
Dataset
Attribute set
Attribute ranking 
process
Attribute subsets
Obtaining subset
of attributes
Pre-selection and Ranking of the
subset of pre-selected attributes
Optimal attribute
subset
Fig. 2
Framework for the attribute selection using the FRF_fs method

36
J.M. Cadenas and M.C. Garrido
these latter attributes can be removed. Thus, the method obtain a pre-selection of
the attribute set.
∙Attribute importance (Ranking process)
From the pre-selected attribute subset and through a fuzzy random forest ensem-
ble, the method obtains a vector RANK ordered, in descending order, of this
attribute subset. This vector is obtained from the value of each attribute xi as
RANK = ∑T
t=1 W ⋅IMPt, where the information provided by the T trees of the
fuzzy random forest ensemble is aggregated using an OWA operator. Values IMPt
are obtained from the information gain of nodes in the FDT t to each attribute xi,
and from the accuracy of FDT t classifying the OOB dataset.
∙Wrapper for attribute ﬁnal selection
Once the ranking of the pre-selected attribute subset, RANK, is obtained, the
method ﬁnd an optimal subset of attributes. The process adds a single attribute
at a time following the RANK vector. The several attribute subsets obtained by
this process are evaluated by a method that supports imperfect data using a cross-
validation. In particular, and using a fuzzy random forest ensemble, an ascending
sequence of fuzzy random forest models is constructed, by invoking and testing
the stepwise attributes.
The eﬃciency and eﬀectiveness of the FRF_fs method is proved through sev-
eral experiments using both high dimensional and imperfect datasets. The method
shows a good performance (not only classiﬁcation accuracy, but also with respect
to the number of selected attributes) and good behavior both with high dimensional
datasets and with imperfect datasets.
3.4
EMFGN: A Method Based on Gaussian Mixture Models
Extended Mixture of Factorized Generalized Normal (EMFGN) method [28] is a
predictive DM method for performing learning and inference from imperfect data.
The method obtains an explicit expression of the model-observation joint function
of the attributes, where both the model expression and the input information are
interpreted and represented in a common framework. The Dempster-Shafer Evidence
Theory (DSET) is the framework that allows its interpretation as mass functions
deﬁned on the domains of the single attributes.
In Fig. 3, the general scheme of the process followed by EMFGN method is
shown. From the dataset with imperfect information, the method provides a model
reﬂecting the joint dependence of the attributes by means of a mixture of factor-
ized normals. This model and the input available information are interpreted and
represented in the DSET in order to combine them (using the Dempster-Shafer’s
combination rule). The model provided by EMFGN method is the following:
p(z) =
∑
ir
P{Ci}𝜋r
n
∏
j=1
Firj(mrj(𝛩j) ⊕mij(𝛩j))

Intelligent Data Analysis, Soft Computing and Imperfect Data
37
Fig. 3
A general scheme of
the EMFGN method
Dataset with
imperfect values
EMFGN inference
Framework 
DSET
FEEM
(extended EM)
learned
model
where:
∙𝛩j ∈P(𝛺zj) and P(𝛺zj) is the set of parts of 𝛺zj.
∙mrj(𝛩j) is the likelihood function of the r-th component of the input information
expressed through a mass function.
∙mij(𝛩j) is the mass function corresponding to the i-th component of the model.
∙Firj is a necessary normalization factor in the combination of two mass functions.
∙P{Ci}𝜋r is the product of the likelihood function of input information in its r-th
component and the expression of the model in the i-th component.
In this framework, the EMFGN method uses the FEEM algorithm in the learn-
ing phase. This algorithm is an extended EM algorithm to allow both the imperfect
information and the model represented in DSET.
From the learned model, EMFGN method can infer both nominal and numerical
attributes. To numerical attributes, the method infers the value zj = ∑
ir 𝛼ir ̄mirj, and
to nominal attributes, the method infers the value zj = argmaxw
∑
ir 𝛼irmirj(𝜔), with
𝜔∈𝛺zj. The value 𝛼ir indicates the likelihood of the r-th component of the input
information having been generated by the i-th component of the mixture. mirj(⋅) is a
mass function combining the input information and the model to the attribute j, and
the value ̄mirj is the average value of mirj(𝛩j).
The results obtained are very satisfactory with the advantage of having a global
model to be able to perform inference on any attribute of an instance.
3.5
FRF: A Method Based on an Ensemble of Fuzzy
Decision Trees
Fuzzy Random Forest (FRF) method [8, 12] is a multiple classiﬁer system (ensem-
ble) to DM. FRF is a predictive method for classiﬁcation and show us its ability to
handle imperfect data both in the model learning and in the inference process.
In Fig. 4, the general scheme of the process followed by FRF method is shown.
FRF obtains a model with the structure of an ensemble based on FDTs. The learning

38
J.M. Cadenas and M.C. Garrido
Fig. 4
A general scheme of
the FRF method
FDT 2
FDT 1
… …
FRF inference
FDT t
Combining the information from the FDTs
Strategies 1 and 2
Dataset with
imperfect values
learned
model
phase generates FDTs with the following characteristics: (a) each FDT is constructed
from a dataset obtained by bagging, (b) the FDTs are constructed without consider-
ing all the attributes to split the nodes (a random subset of the set of attributes avail-
able at each node is selected), (c) the numerical attributes are discretized by fuzzy
partitions, (d) each FDT is constructed to the maximum size and without pruning,
(e) a function (𝜒t,N(⋅)) is used to indicate the degree with which an instance sat-
isﬁes the conditions that lead to node N of tree t, and (f) FDTs support instances
with imperfect values (a function 𝜇simil(⋅) is used to measure the membership degree
of these types of values to the fuzzy sets forming the partition of the numerical
attributes).
From the obtained model, FRF method uses two strategies to combine the infor-
mation of several FDTs and to obtain the ﬁnal decision for a target instance. Strategy
1 combines the information from the diﬀerent leaves reached in each FDT to obtain
the decision of each individual FDT and then applying the same or another combina-
tion method to generate the global decision of the FRF model. Strategy 2 combines
the information from all reached leaves from all FDTs to generate the global decision
of the FRF model.
The method assigns class 𝜔M to a new instance such that 𝜔M = argmaxi{D_FRFi}
where D_FRF is a vector with size I that indicates the conﬁdence assigned by the
method to each class i. The vector elements are obtained from the support for each
class in the leaves reached when applying the several strategies and combination
methods.
The results obtained by FRF method are promising concluding that by using
imperfect values instead of crisp, we capture better the nature of the underlying
information.
3.6
KNNimp: A Method Based on Instances
The kNNimp method [14] is a k-nearest neighbors classiﬁer from datasets with imper-
fect values to DM. Figure 5 shows the general scheme followed by kNNimp method.
This method belongs to the methods with lazy learning, that is, the method does not

Intelligent Data Analysis, Soft Computing and Imperfect Data
39
Fig. 5
A general scheme of
the kNNimp method
Dataset with
imperfect values
Combining the information
from the k-nearest instances
kNNimp inference
Distance/Similarity
Imperfection degree
Control:
Maximum imperfection
Similar classes
local model
(k-nearest instances)
need of an explicit learning phase. Therefore, this method requires that all dataset
instances are stored.
To classify a instance, the kNNimp method computes its “k” nearest instances
and generates a class value from them (a local model dependent on the new instance
has been constructed). By containing imperfect values the dataset, the importance of
each instance (neighbor) in the output decision is based on relative distance/similarity
dimp(⋅, ⋅) (distance/similarity measures that support imperfect data) and its degree of
imperfection. Speciﬁcally, for each instance, two weights are calculated depending
on its degree of imperfection p(⋅) and its distance/similarity q(⋅).
Furthermore, the overall degree of imperfection in “k” nearest instances is mea-
sured, if it is too high, the classiﬁcation is not performed. To establish the maximum
degree of imperfection, kNNimp method uses the parameter UI.
Once the local model is obtained (k nearest instances), kNNimp method combines
the information provided for each neighbor instance (weights p(⋅) and q(⋅)) to obtain
the set of possible weighted classes. The class with the highest score is chosen as
output, together with other classes whose score is similar to the highest. To assess if
a class should be included in the ﬁnal output, this method uses the threshold UD.
The method obtains a fuzzy subset {𝜇(𝜔i)∕𝜔i} as possible values to the class
attribute of the new instance where 𝜇(𝜔i) =
∑k
j 𝜇j(𝜔i)p(xj)q(xj)
∑k
j
∑
i 𝜇j(𝜔i)p(xj)q(xj) and 𝜇j(𝜔i) is the mem-
bership degree of the j-th neighbor to the class value 𝜔i. Therefore, the method
assigns to the new instance the class 𝜔M = argmaxi{𝜇(𝜔i)} or the fuzzy subset
{𝜔M, 𝜔t}, with 𝜔M−𝜔t
𝜔M
> UD.
The kNNimp classiﬁer is robust when working with imperfect data and maintains
a good performance when is compared with other methods in the literature, applied
to datasets with or without imperfection.
4
Conclusions
In data-driven application domains, the suitable use of available information is very
important. Because of this, it becomes increasingly necessary to design methods that
support diﬀerent types of information (imperfect or not) and obtain more ﬂexible

40
J.M. Cadenas and M.C. Garrido
models with an appropriate behavior. In this framework, the hybridization of the
tools provided by Soft Computing and Intelligent Data Analysis methods is a ﬁeld
that is gaining more importance. In this work, some proposals that carry out this
hybridization obtaining quite satisfactory results are commented and analyzed. For
this reason we consider that it is a ﬁeld in which new proposals must be made with
the objective of approaching the Intelligent Data Analysis process from datasets that
express the true nature of the information.
Acknowledgements Supported by the projects TIN2014-52099-R (EDISON) and TIN2014-
56381-REDT (LODISCO) granted by the Ministry of Economy and Competitiveness of Spain
(including ERDF support).
References
1. Abe, S.: Fuzzy support vector machines for multilabel classiﬁcation. Pattern Recogn. 48(6),
2110–2117 (2015)
2. Barua, A., Mudunuri, L.S., Kosheleva, O.: Why trapezoidal and triangular membership func-
tions work so well: Towards a theoretical explanation. J. Uncertain Syst. 8(3), 164–168 (2014)
3. Battiti, R.: Using mutual information for selection features in supervised neural net learning.
IEEE Trans. Neural Netw. 5, 537–550 (1994)
4. Berlanga, F.J., Rivera, A., del Jesús, M.J., Herrera, F.: GP-COACH: genetic programming-
based learning of compact and accurate fuzzy rule-based classiﬁcation systems for high-
dimensional problems. Inf. Sci. 180(8), 1183–1200 (2010)
5. Berthold, M.R., Borgelt, C., Höppner, F., Klawonn, F.: Guide to Intelligent Data Analysis:
How to Intelligently Make Sense of Real Data. Springer (2010)
6. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press
(1981)
7. Bonissone, P.: Soft computing: the convergence of emerging reasoning technologies. Soft
Comput. 1(1), 6–18 (1997)
8. Bonissone, P., Cadenas, J.M., Garrido, M.C., Díaz-Valladares, R.A.: A fuzzy random forest.
Int. J. Approximate Reasoning 51(7), 729–747 (2010)
9. Boulle, M.: Khiops: a statistical discretization method of continuous attributes. Mach. Learn.
55(1), 53–69 (2004)
10. Breiman, L., Friedman, J., Stone, C.J., Olshen, R.A.: Classiﬁcation and Regression Trees.
CRC Press (1984)
11. Cadenas, J.M., Garrido, M.C., Martínez, R.: Feature subset selection ﬁlter-wrapper based on
low quality data. Expert Syst. Appl. 40(16), 6241–6252 (2013)
12. Cadenas, J.M., Garrido, M.C., Martínez, R., Bonissone, P.P.: Extending information process-
ing in a fuzzy random forest ensemble. Soft Comput. 16(5), 845–861 (2012)
13. Cadenas, J.M., Garrido, M.C., Martínez, R., Bonissone, P.P.: OFP_CLASS: a hybrid method
to generate optimized fuzzy partitions for classiﬁcation. Soft Comput. 16(4), 667–682 (2012)
14. Cadenas, J.M., Garrido, M.C., Martínez, R., Muñoz, E., Bonissone, P.P.: A fuzzy k-nearest
neighbor classiﬁer to deal with imperfect data. Soft Comput. (2017). doi:10.1007/s00500-
017-2567-x
15. Chan, C.C., Batur, C., Srinivasan, A.: Determination of quantization intervals in rule based
model for dynamic systems. In: Proceedings of the International Conference on Systems, Man,
and Cybernetics, pp. 1719–1723 (1991)
16. Choi, Y.S., Moon, B.R.: Feature selection in genetic fuzzy discretization for the pattern clas-
siﬁcation problems. IEICE Trans. Inf. Syst. 90(7), 1047–1054 (2007)

Intelligent Data Analysis, Soft Computing and Imperfect Data
41
17. Clark, P., Boswell, R.: Rule induction with CN2: Some recent improvements. In: Y. Kodratoﬀ
(ed.) Lecture Notes in Artiﬁcial Intelligence. Machine Learning EWSL-91, pp. 151–163.
Springer (1991)
18. Cox, E., Taber, R., O’Hagen, M.: The Fuzzy Systems Handbook, 2nd edn. Academic Press
(1998)
19. Crockett, K., Bandar, Z., Mclean, D.: Growing a fuzzy decision forest. In: Proceedings of the
10th IEEE International Conference on Fuzzy Systems, vol. 2, pp. 614–617 (2001)
20. Derrac, J., García, S., Herrera, F.: Fuzzy nearest neighbor algorithms: taxonomy, experimental
analysis and prospects. Inf. Sci. 260, 98–119 (2014)
21. Djouadi, A., Bouktache, E.: A fast algorithm for the nearest-neighbor classiﬁer. IEEE Trans.
Pattern Anal. Mach. Intell. 19(3), 277–282 (1997)
22. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classiﬁcation, 2nd edn. Wiley (2001)
23. Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, P.: From data mining to knowledge discovery: an
overview. In: U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, R. Uthurusamy (eds.) Advances in
Knowledge Discovery and Data Mining, pp. 1–34. American Association for Artiﬁcial Intel-
ligence (1996)
24. Fernández, A., del Jesus, M.J., Herrera, F.: Hierarchical fuzzy rule based classiﬁcation systems
with genetic rule selection for imbalanced data-sets. Int. J. Approximate Reasoning 50(3),
561–577 (2009)
25. Fisher, D.H.: Knowledge acquisition via incremental conceptual clustering. Mach. Learn. 2(2),
139–172 (1987)
26. Fukunaga, K.: Introduction to statistical pattern recognition. Academic Press (1990)
27. García, S., Luengo, J., Herrera, F.: Data Preprocessing in Data Mining. Springer (2015)
28. Garrido, M.C., Cadenas, J.M., Bonissone, P.P.: A classiﬁcation and regression technique to
handle heterogeneous and imperfect information. Soft Comput. 14(11), 1165–1185 (2010)
29. George, G., Raj, V.C.: Review on feature selection techniques and the impact of svm for cancer
classiﬁcation using gene expression proﬁle. Int. J. Comput. Sci. Eng. Surv. 2(3), 16–27 (2011)
30. Ghosh, A., Pal, S.K.: Soft computing approach to pattern recognition and image processing.
World Scientiﬁc (2002)
31. Hamamoto, Y., Uchimura, S., Tomita, S.: A bootstrap technique for nearest neighbor classiﬁer
design. IEEE Trans. Pattern Anal. Mach. Intell. 19(1), 73–79 (1997)
32. Hecht-Nielsen, R.: Neurocomputing. Addison-Wesley (1990)
33. Ho, K.M., Scott, P.D.: Zeta: a global method for discretization of cotitinuous variables. In:
Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining,
pp. 191–194 (1997)
34. Holte, R.C.: Very simple classiﬁcation rules perform well on most on most commonly used
datasets. Mach. Learn. 11, 63–90 (1993)
35. Huang, Z., Ng, M.K.: A fuzzy k-modes algorithm for clustering categorical data. IEEE Trans.
Fuzzy Syst. 7(4), 446–452 (1999)
36. Isasi, P., Galván, I.: Las redes neuronales artiﬁciales - Un enfoque práctico. Pearson - Prentice
Hall (2004)
37. Ishibuchi, H., Yamamoto, T.: Rule weight speciﬁcation in fuzzy rule-based classiﬁcation sys-
tems. IEEE Trans. Fuzzy Syst. 13(4), 428–435 (2005)
38. Janikow, C.Z.: Fuzzy decision trees: issues and methods. IEEE Trans. Syst. Man Cybern. Part
B (Cybern.) 28(1), 1–14 (1998)
39. Janikow, C.Z.: Fuzzy decision forest. In: Proceedings of the 22nd International Conference of
the North American Fuzzy Information Processing Society, pp. 480–483 (2003)
40. Janikow, C.Z., Fajfer, M.: Fuzzy partitioning with ﬁd3. 1. In: Proceedings of the 18th Interna-
tional Conference of the North American Fuzzy Information Processing Society, pp. 467–471
(1999)
41. Jensen, R., Shen, Q.: Fuzzy-rough sets assisted attribute selection. IEEE Trans. Fuzzy Syst.
15(1), 73–89 (2007)
42. Kabir, M., Shahjahan, M., Murase, K.: A new hybrid ant colony optimization algorithm for
feature selection. Expert Syst. Appl. 39, 3747–3763 (2012)

42
J.M. Cadenas and M.C. Garrido
43. Kbir, M.A., Benkirane, H., Maalmi, K., Benslimane, R.: Hierarchical fuzzy partition for pat-
tern classiﬁcation with fuzzy if-then rules. Pattern Recogn. Lett. 21(6), 503–509 (2000)
44. Kerber, R.: ChiMerge: discretization of numeric attributes. In: Proceedings of the Tenth
National Conference on Artiﬁcial Intelligence, pp. 123–128 (1992)
45. Kohonen, T.: Self-Organising Maps, 3rd edn. Springer (2001)
46. Kurgan, L.A., Cios, K.J.: Caim discretization algorithm. IEEE Trans. Knowl. Data. Eng. 16(2),
145–153 (2004)
47. Lee, K.M., Lee, K.M., Lee, J.H., Lee-Kwang, H.: A fuzzy decision tree induction method for
fuzzy data. Proceedings of the IEEE International Conference on Fuzzy Systems 1, 16–21
(1999)
48. Li, C.: A combination scheme for fuzzy partitions based on fuzzy majority voting rule. In:
Proc. of the International Conference on Networks Security, Wireless Communications and
Trusted Computing, vol. 2, pp. 675–678 (2009)
49. Li, D., Gu, H., Zhang, L.: A fuzzy c-means clustering algorithm based on nearest-neighbor
intervals for incomplete data. Expert Syst. Appl. 37(10), 6942–6947 (2010)
50. Lin, C.F., Wang, S.D.: Fuzzy support vector machines. IEEE Trans. Neural Netw. 13(2), 464–
471 (2002)
51. Liu, H., Hussain, F., Tan, C.L., Dash, M.: Discretization: an enabling technique. Data Min.
Knowl. Discov. 6(4), 393–423 (2002)
52. Liu, H., Setiono, R.: Feature selection via discretization. IEEE Trans. Knowl. Data. Eng. 9(4),
642–645 (1997)
53. Luukka, P.: Feature selection using fuzzy entropy measures with similarity classiﬁer. Expert
Syst. Appl. 38(4), 4600–4607 (2011)
54. Madjarov, G., Kocev, D., Gjorgjevikj, D., Džeroski, S.: An extensive experimental comparison
of methods for multi-label learning. Pattern Recogn. 45(9), 3084–3104 (2012)
55. Marsala, C.: Data mining with ensembles of fuzzy decision trees. In: Proceedings of the IEEE
Symposium on Computational Intelligence and Data Mining, pp. 348–354 (2009)
56. Mayer-Schönberger, V., Cukier, K.: Big Data: A revolution that will transform how we live,
work, and think. Houghton Miﬄin Harcourt (2013)
57. Michalski, R.S., Mozetic, I., Hong, J., Lavrac, N.: The AQ15 inductive learning system:
an overview and experiments. In: Proceedings of the International Meeting on Advances in
Learning (1986)
58. Micó, M.L., Oncina, J., Vidal, E.: A new version of the nearest-neighbour approximating
and eliminating search algorithm with linear preprocessing time and memory requirements.
Pattern Recogn. Lett. 15(1), 9–17 (1994)
59. Mirkin, B.: Mathematical Classiﬁcation and Clustering. Kluwer Academic Publishers (1996)
60. Mitra, S., Pal, S.K.: Fuzzy multi-layer perceptron, inferencing and rule generation. IEEE
Trans. Neural Netw. 6(1), 51–63 (1995)
61. Mitra, S., Pal, S.K., Mitra, P.: Data mining in soft computing framework: a survey. IEEE Trans.
Neural Netw. 13(1), 3–14 (2002)
62. Moody, J., Darken, C.J.: Fast learning in networks of locally-tuned processing units. Neural
Comput. 1(2), 281–294 (1989)
63. Myles, A.J., Brown, S.D.: Induction of decision trees using fuzzy partitions. J. Chemom.
17(10), 531–536 (2003)
64. Nascimento, S., Mirkin, B., Moura-Pires, F.: A fuzzy clustering model of data and fuzzy c-
means. Proceedings of the Ninth IEEE International Conference on Fuzzy Systems 1, 302–307
(2000)
65. Nauck, D., Kruse, R.: A neuro-fuzzy method to learn fuzzy classiﬁcation rules from data.
Fuzzy Sets Syst. 89(3), 277–288 (1997)
66. Olaru, C., Wehenkel, L.: A complete fuzzy decision tree technique. Fuzzy Sets Syst. 138(2),
221–254 (2003)
67. Palacios, A.M., Palacios, J.L., Sánchez, L., Alcalá-Fdez, J.: Genetic learning of the member-
ship functions for mining fuzzy association rules from low quality data. Inf. Sci. 295, 358–378
(2015)

Intelligent Data Analysis, Soft Computing and Imperfect Data
43
68. Palacios, A.M., Sánchez, L., Couso, I.: Extending a simple genetic cooperative-competitive
learning fuzzy classiﬁer to low quality datasets. Evol. Intell. 2(1–2), 73–84 (2009)
69. Palacios, A.M., Sanchez, L., Couso, I.: Boosting of fuzzy rules with low quality data. Multiple-
Valued Logic Soft Comput. 19(5–6), 591–619 (2012)
70. Palacios, A.M., Sanchez, L., Couso, I.: An extension of the FURIA classiﬁcation algorithm to
low quality data. In: Proceedings of the International Conference on Hybrid Artiﬁcial Intelli-
gence Systems, pp. 679–688 (2013)
71. Quinlan, J.R.: Induction of decision trees. Mach. Learn. 1, 81–106 (1986)
72. Quinlan, J.R.: C4.5 Programs for Machine Learning. Morgan Kaufmann (1993)
73. Redmond, S.J., Heneghan, C.: A method for initialising the k-means clustering algorithm using
kd-trees. Pattern Recogn. Lett. 28(8), 965–973 (2007)
74. Ruck, D.W., Rogers, S.K., Kabrisky, M., Oxley, M.E., Suter, B.W.: The multilayer perceptron
as an approximation to a bayes optimal discriminant function. IEEE Trans. Neural Netw. 1(4),
296–298 (1990)
75. Saeys, Y., Inza, I., Larrañaga, P.: A review of feature selection techniques in bioinformatics.
Bioinformatics 23(19), 2507–2517 (2007)
76. Saha, S., Bandyopadhyay, S.: A fuzzy genetic clustering technique using a new symmetry
based distance for automatic evolution of clusters. In: Proceedings of the International Con-
ference on Computing: Theory and Applications, pp. 309–314 (2007)
77. Sánchez, L., Suárez, M.R., Villar, J.R., Couso, I.: Mutual information-based feature selec-
tion and partition design in fuzzy rule-based classiﬁers from vague data. Int. J. Approximate
Reasoning 49, 607–622 (2008)
78. Suárez, M.R., Villar, J.R., Grande, J.: A feature selection method using a fuzzy mutual infor-
mation measure. Int. J. Reasoning-Based Intell. Syst. 2, 133–141 (2010)
79. Toivonen, H.: Sampling large databases for association rules. In: Proceedings of the 22nd
Conference on Very Large Databases, vol. 96, pp. 134–145 (1996)
80. Torra, V.: Fuzzy c-means for fuzzy hierarchical clustering. In: Proceedings of the 14th IEEE
International Conference on Fuzzy Systems, pp. 646–651 (2005)
81. Tsai, C.J., Lee, C.I., Yang, W.P.: A discretization algorithm based on class-attribute contin-
gency coeﬃcient. Inf. Sci. 178, 714–731 (2008)
82. Verdegay, J.L., Yager, R.R., Bonissone, P.: On heuristic as a fundamental constituent of soft
computing. Fuzzy Sets Syst. 159(7), 846–855 (2008)
83. Vieira, S.M., Sousa, J.M.C., Kaymak, U.: Fuzzy criteria for feature selection. Fuzzy Set Syst.
189, 1–18 (2012)
84. Wang, Y., Li, C., Zuo, Y.: A selection model for optimal fuzzy clustering algorithm and num-
ber of clusters based on competitive comprehensive fuzzy evaluation. IEEE Trans. Fuzzy Syst.
17(3), 568–577 (2009)
85. Witten, I.H., Frank, E., Hall, M.: Data Mining—Practical Machine Learning Tools and Tech-
niques. Morgan Kaufmann (2011)
86. Wong, M.L., Leung, K.S.: Data Mining Using Grammar Based Genetic Programming and
Applications. Kluwer Academic Publishers (2002)
87. Yan-Qing, Y., Ju-Sheng, M., Zhou-Jun, L.: Attribute reduction based on generalized fuzzy
evidence theory in fuzzy decision systems. Fuzzy Sets Syst. 170, 64–75 (2011)
88. Yang, Y., Jia, Z., Chang, C., Qin, X., Li, T., Wang, H., Zhao, J.: An eﬃcient fuzzy kohonen
clustering network algorithm. Proceedings of the Fifth International Conference on Fuzzy
Systems and Knowledge Discovery 1, 510–513 (2008)
89. Younes, Z., Abdallah, F., Denœux, T.: Fuzzy multi-label learning under veristic variables. In:
Proceedings of the IEEE International Conference on Fuzzy Systems, pp. 1–8 (2010)
90. Zadeh, L.A.: Fuzzy sets. Inf. Control 8(3), 338–353 (1965)
91. Zadeh, L.A.: Soft computing and fuzzy logic. IEEE Softw. 11(6), 48–56 (1994)

Soft Computing Methods in Transport
and Logistics
Julio Brito, Dagoberto Castellanos-Nieves, Airam Expósito and José. A.
Moreno
Abstract The current economic context generates in supply chain management
greater demands for ﬂexibility and dynamism. In addition, there is an increase in
uncertainty that adds more complexity to the problems associated with planning and
management. Soft Computing oﬀers a set of methodologies capable of responding
to these challenges. This work provides an overview of transport and logistics prob-
lems, as well as the most representative combinatorial optimization models. Speciﬁ-
cally, it focuses on the treatment of uncertainty through fuzzy optimization and meta-
heuristics methodologies. Promising results from the use of this approach suggest
emerging areas of application, which are presented and described.
1
Introduction
Logistical, transport and distribution planning have adapted to the evolution of new
business organization models. Distribution strategies and transportation decisions
are some of the main subjects in supply chain management and play an important
role in its success because they improve service quality, reduce costs and optimize
resources [20]. Supply chain management (SCM) involves all activities related to
integration, planning and control of product and information ﬂows that are gener-
ated between suppliers and clients. The supply chain can be broken down into three
J. Brito (✉) ⋅D. Castellanos-Nieves ⋅A. Expósito ⋅J.A. Moreno
Grupo de Computación Inteligente, Instituto Universitario de Desarrollo Regional,
Universidad de La Laguna, La Laguna, Spain
e-mail: jbrito@ull.edu.es
D. Castellanos-Nieves
e-mail: dcastell@ull.edu.es
A. Expósito
e-mail: aexposim@ull.edu.es
J.A. Moreno
e-mail: jamoreno@ull.edu.es
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_3
45

46
J. Brito et al.
Fig. 1
Cycles of the supply chain procurement, production and distribution
cycles, procurement, production and distribution (Fig. 1). The distribution cycle
refers to the activities and processes associated with storage and distribution [21].
One of the basic activities in the integral management of the supply chain is
implied planning and decision making. Three levels of planning are apparent: strate-
gic, tactical and operational [33]. Each one of these three levels is associated with
diﬀerent time horizons and creates a distinct set of important problems which usu-
ally correspond to optimization problems. Thus long term strategic problems include
decisions on the number, size, location and capacity of the storage units and trans-
port; tactical decisions in the medium term are the design of the distribution network,
location and assignment of distribution zones/areas and the supply rate; and ﬁnally,
operational aspects, studied in the short term, such as the establishment of pickup
and delivery routes, and/or the organization of vehicle loading/unloading [67].
Methods/methodologies found in Soft Computing oﬀer a useful alternative to
solve problems with this type of complexity [6, 58]. The design of Intelligent Sys-
tems to aid in decision making in real settings, such as transport and logistics plan-
ning, needs to take advantage of Soft Computing methodologies [37]. The design of
Intelligent Systems to aid in decision making in real settings, such as transport and
logistics planning, needs to take advantage of Soft Computing methodologies. The
quality of information is the most common scenario, especially in real-world appli-
cations, and this incomplete or imprecise information is reﬂected in the parameters
and variables. Fuzzy set theory oﬀers an appropriate methodological framework to
approach this class of uncertainty, which is not the product of absence of informa-
tion, nor of a random nature, but instead of the imprecise nature of the expression.
Some solution techniques employ exact methods but in real-world problems, these
methods do not guarantee that an optimal solution will be found. Heuristic and meta-
heuristics techniques are important tools constituent of Soft Computing [81] to tackle
complex optimization problems. They are capable of evaluating possible alternatives
and determine the preferred solution in eﬃcient time, by means of strategies that
integrate problem knowledge.
Our discussion is centered on problems and application of the levels of tactical
and operational transportation planning. Transportation planning concerns the short-
term planning of the distribution operations and mostly deals with the planning of

Soft Computing Methods in Transport and Logistics
47
deliveries to diﬀerent customers. Typical considerations at this decision level are the
details of delivery routes: that is, at what exact times, by which vehicle, and in what
sequence customers will get their products delivered. In addition, location decision
problems may have to be made on these levels.
The aim of this chapter is to analyze some relevant and emerging problems and
application in transport and distribution. The application of fuzzy optimization in
this ﬁeld can be signiﬁcant. In addition to providing an overview of transport and
distribution problems and their models, the purpose is to give an overview of the
fuzzy optimization and metaheuristic approach for the treatment of uncertainty in
these models, to review their use and to propose new areas of application in real
practical problems.
The remainder of this chapter is organized as follows. Section 2 introduces the
Soft Computing based approach. Section 3 then presents a review of some problems
that we have considered emerging, which are of interest for the application of Soft
Computing methodologies, fuzzy optimization and metaheuristics. The chapter ends
with some conclusions in Sect. 4.
2
Soft Computing Based Solution Approach
This section describes the use of an approximation that integrates speciﬁc techniques
from Soft Computing, such as fuzzy optimization and metaheuristics. An outline of
how these techniques are applied in the resolution of classic transport and logistics
problems is also given.
2.1
Fuzzy Optimization
Fuzzy sets and systems are used to build computing systems to solve decision and
optimization problems whose modeling is diﬃcult to deﬁne accurately, managing
the uncertainty and the imprecision of the available information, as well as of the
formulation of preferences, restrictions and objectives expressed by decision mak-
ers. If there is imprecision in some of the formulation components of the optimiza-
tion problem and we can express it with fuzzy terms then we are faced with fuzzy
optimization problem. Discussions concerning solutions do not focus on their feasi-
bility, nor if they are optimal solutions. We, in turn, have chosen to discuss the degree
of feasibility and optimality of the solution. Bellman and Zadeh [10] are the authors
who introduced the fundamentals for fuzzy optimization problems, where objectives
and constraints can be deﬁned in an imprecise way and characterized using member-
ship function such as fuzzy sets. This approximation requires that the formulation
and problem solutions be dealt with adequately by making use of fuzzy number rep-
resentations and their operations.

48
J. Brito et al.
An optimization problem is described as the search for the value of speciﬁc deci-
sion variables x so that identiﬁed objective functions f(x) attain their optimum values.
The value of the variables is subject to stated constraints g(x). In these problems the
objective functions are deﬁned on a set of solutions that we will denote by X. Thus, an
optimization problem can be represented by: max{ f(x) ∶g(x) ≥0}, ∀x ∈X. When
some of its components are considered fuzzy, we are facing a problem of fuzzy opti-
mization. Among all optimization problems, Linear Programming are those whose
objective function and constraints are linear. The general model of Linear Program-
ming is formulated as max{ f(xj, cj) ∶g(xj, aij) ≥bi, i = 1, ..., m, j = 1, ..., n, xj ≥0}.
In the formulation aij ∈ℜm×n is the technological matrix, bi ∈ℜm the resources,
cj ∈ℜn the costs and xj ∈ℜn the variables. Fuzzy Linear Programming (FLP) con-
stitutes the basis for solving fuzzy optimization problems and their solution methods
have been the subject of many studies in the fuzzy context. Diﬀerent FLP models can
be considered according to the fuzzy components [80]. These models can be solved
in a direct and simple way, obtaining solutions that are coherent with their fuzzy
nature.
(a) Models with fuzzy constraints. In this case there is a certain tolerance in the
fulﬁlment constraints and consequently the feasible region can be deﬁned as a fuzzy
set. This is max{ f(xj, cj) ∶g(xj, aij) ≥f bi, i = 1, ..., m, j = 1, ..., n, xj ≥0}. In partic-
ular, Verdegay [79], using the representation theorem for fuzzy sets, proves a solu-
tion which can be obtained from the auxiliary model: max{ f(xj, cj) ∶g(xj, aij) ≥f
bi + 𝜏i(1 −𝛼), i = 1, ..., m, j = 1, ..., n, xj ≥0}, where 𝛼∈[0, 1] and 𝜏= (𝜏1, ..., 𝜏m)
is referred to as a violation tolerance level.
(b) Models with fuzzy cost. In this case the model is represented by max{ f(xj, cf
j ) ∶
g(xj, aij) ≥bi, i = 1, ..., m, j = 1, ...n, xj ≥0}, where cf
j is a fuzzy number described
by its corresponding membership function 𝜇j(x). [23] prove that the solution can be
obtained with the multi-objective auxiliary model. We can also used a simple method
considering fuzzy solutions that are solved with the application of an ordered func-
tion h for the constraints [35], i.e. max{ f(xj, h(cj)) ∶g(xj, aij) ≥bi, i = 1, ..., m, j =
1, ...n, xj ≥0}.
(c) Models with fuzzy coeﬃcients in constraints. This model considers a prob-
lem of the type max{ f(xj, cj) ∶g(xj, af
ij) ≥f bf
i , i = 1, ..., m, j = 1, ...n, xj ≥0} where
the values of the technological matrix and the coeﬃcients are fuzzy numbers and
are described by its corresponding membership function. Delgado et al. [22] also
include imprecision in the constraints. They propose considering fuzzy solutions that
are solved with the application of an ordered function h for the constraints. The new
formulation is expressed by the auxiliary problem: max{ f(xj, cj) ∶g(xj, af
ij) ≥h bf
i +
𝜏f
i (1 −𝛼), i = 1, ..., m, j = 1, ...n, xj ≥0} where the symbol ≥h stands for a compar-
ison relation between fuzzy numbers, 𝛼∈[0, 1] and 𝜏f
i is a tolerance of fuzzy nature
set by the decision maker.

Soft Computing Methods in Transport and Logistics
49
2.2
Evolutionary Heuristic and Metaheuristics
In Artiﬁcial Intelligence (AI), the qualiﬁer Heuristic is usually applied to all those
aspects related with the use of knowledge in the dynamic realization of tasks. Heuris-
tics are used to refer to any intelligent technique, method or procedure of performing
a task that is not the product of a rigorous formal analysis, but of expert knowledge
about the task. In particular, the term heuristic is used to refer to a procedure that tries
to provide solutions of a problem with a good performance, as regards the quality of
the solutions and the resources used. Successful heuristic procedures have emerged
in solving speciﬁc problems or performing diﬃcult tasks. It has been tried to extract
from them what was essential in their success in order to apply it to other problems or
tasks, or in larger contexts. As has clearly occurred in various ﬁelds of AI, especially
with expert systems, this line of research has contributed to the scientiﬁc develop-
ment of the ﬁeld of heuristics and to extend the application of its results. In this
way, both speciﬁc computational techniques and resources have been obtained, as
well as general design strategies for problem resolution heuristic procedures. These
general strategies for constructing algorithms, which go beyond heuristics, and go
further, are called Metaheuristics. Speciﬁc and elaborated heuristics for solving a
simple problem in a narrow context have usually better performance than any algo-
rithm based on metaheuristics. However, the metaheuristics tries to exploits other
kind of advantages. They can improve while are used, are ﬂexible and adaptable.
They get good performance with low level of knowledge. Some recent reviews and
survey show the relevance of the methods [7, 11, 68, 90].
2.3
Fuzzy Optimization in Transport and Logistics Problems
Vehicle routing, scheduling, locations and relations between them are the most
important processes and decisions in transport and logistics. From a mathematical
point of view, they can usually be modeled as a combinatorial optimization problem.
The numerous applications of these problems include, among others, movement of
goods, public transport, fresh and perishable food distributions, courier services,
solid waste collection and caterers. Each problem has its own objectives, associated
with cost, available resources or quality of service which in general is expressed in
terms of time or distance. They also have their own speciﬁc constraints that may
reﬂect vehicle capacity, ﬂeet size, warehouse number and capacity, collection and/or
delivery, points of loading/unloading, distances traveled and speciﬁc time windows.
Most of the transportation and logistics models reviewed in this work are based on
well-known problems.
Routing problems have been widely addressed in the literature and among them
the Vehicle Routing Problem (VRP) is one of the most widely studied [75]. VRP is
commonly deﬁned as the problem of designing optimal delivery or collection routes
for a vehicle ﬂeet from one or several depots to a set of geographically scattered

50
J. Brito et al.
demand points, under an extensive variety of conditions. Some real-life examples,
which are variants of the classic VRP with diﬀerent constraints, are presented in [1].
Location problems (LP) deal with the optimal choice of a set of points for establish-
ing certain facilities that take into account diﬀerent criteria and constraints. Several
models have been proposed to address these problems such as that of the P-median,
P-center or covering [56]. A literature review of facility location models in the con-
text of supply chain management is given in [49]. The Location routing problem
(LRP) models and solves the facility location problem by taking into account simul-
taneous route planning which implies an integrated solution. Two recent surveys
have been presented that describe models and variants investigated [25, 61].
In practical real-world problems decision makers use subjective knowledge or lin-
guistic information when making decisions, measuring parameters, objectives and
constraints and even when modeling the problem. In this context, a Soft Computing
approach, speciﬁcally fuzzy optimization and metaheuristics, is useful for solving
routing and location problems because they are ﬂexible enough to deal with com-
plex systems, provide acceptable approximate solutions and therefore add value. Our
speciﬁc interest is in problems where various components are imprecise, treated as
fuzzy and addressed with fuzzy optimization and metaheuristics.
In the literature we can ﬁnd several approaches, models and solutions of the VRP
considering some fuzzy component. The most widely discussed models are the ones
that have imprecision demand, time travel and time services. The fuzzy optimiza-
tion approach described in previous subsection are applied to VRP and some of its
variants [13, 14, 74].
Several authors formulate and solve fuzzy LP, on networks [51], on networks
using p-center [53], using p-median [15] or with covering [34]. Models of the fuzzy
LRP problem have also been presented such as the capacitated model with fuzzy
demands in [48] or a realistic version with more fuzzy components [31].
3
Applications and Emergent Problems
This section analyzes some relevant and emergent problems and application in trans-
port and logistics, in which the application of fuzzy optimization can be signiﬁcant.
In addition to providing a synthesis of the existing literature on the subject, we give
an overview of the problem in the context of the ﬁeld to help researchers better
understand the practical motivations where to apply methodologies. Thus it iden-
tiﬁes the challenges and the direction in which future researches could be conducted
in this ﬁeld. The relevant and emergent problems and applications selected to realize
an overview in the remainder of this section are Disaster Emergency Humanitarian
Logistics, City Logistics, Green Logistics, and Tourist Trip Logistics.

Soft Computing Methods in Transport and Logistics
51
3.1
Disaster Emergency Humanitarian Logistics
The International Federation of Red Cross and Red Crescent Societies deﬁnes a dis-
aster as a sudden, calamitous event that seriously disrupts the functioning of a com-
munity or society and causes human, material, and economic or environmental losses
that exceed the community‘s or society‘s ability to cope using its own resources.
Though often caused by nature, disasters can have human origins. Disaster impacts
more than 200 million people and produce around 75,000 fatalities every year [87].
The authors in [64] deﬁne emergency logistics as a process of planning, manag-
ing and controlling the eﬃcient ﬂows of relief, information, and services from the
points of origin to the points of destination to meet the urgent needs of the aﬀected
people under emergency conditions. In this context, there are many challenges in
logistics that diﬀer from those encountered in commercial supply chains. Some of
these challenges that introduce complexity and uncertainty are listed by [9] as:
∙High level of uncertainty of demands in terms of capacities, travel times, locations,
etc.
∙Limited resources in large scale disasters, lack of supplies, people, technology,
transportation capacity and money.
∙Sudden changes in terms of demand, large resource demands in short time frames.
∙Diﬃculties in achieving eﬃcient and timely delivery.
In [17], an analysis of the literature is presented, showing the main optimization
models used in emergency logistics. Facility location, location-evacuation, location
with relief distribution and pre-positioning, relief distribution and casualty trans-
portation, resource allocation, commodity ﬂow, resource allocation and commodity
ﬂow, and other models are others just some of the cited models.
A survey of recent advances in bio-inspired meta-heuristics, including genetic
algorithms, particle swarm optimization, ant colony optimization, etc., for solving
emergency transportation problems is also presented in [92].
Optimization problems of emergency logistic involve complex systems introduc-
ing fuzzy sets and systems. In the literature there are several papers related to fuzzy
location and routing problem, for instance, in [66], a hybrid fuzzy optimization
methodology to solve the large-scale disaster relief distribution problem is presented.
The solution approach is made up of three steps. In the ﬁrst step fuzzy clustering tech-
niques are used to classify the damaged areas, while the second and third steps use
FLP to deal with lack of resources. The authors in [65] provide a hybrid fuzzy cluster-
ing optimization approach to the operation of an emergency logistics co-distribution
center responding to the urgent relief demands in the crucial rescue period.
The fuzzy LRP is a research area with several papers in emergency logistics. In
[84] a model considering fuzzy demands of relief materials, timeliness and limited
resources is proposed. The objective function of the model minimizes the total cost
and the relief time of the system.

52
J. Brito et al.
Other papers focus on emergency logistics transportation path optimization using
fuzzy approaches. In [89] a multi-objective optimization model for emergency logis-
tics transportation path is proposed. Factors such as transportation time, safety factor
and transportation costs are described using trapezoidal fuzzy numbers in order to
deal with the uncertainty of these factors.
3.2
City Logistics
Urban mobility plays a key role in the promotion of the sustainable urban develop-
ment of a city. In particular, an eﬃcient freight transport system is required as it plays
a signiﬁcant role in its competitiveness and represents an important element for the
local economy regarding the employment and income that it generates [63]. Logis-
tics activities and operations, especially transportation and distribution of goods,
now receive a speciﬁc treatment which is known as urban logistics or city logistics.
This concept has its origins in the 1980s, it has become in the last decades more
relevant by the development of cities, the growing demands for supply that is more
eﬃcient and concern about the negative impacts of it. Taniguchi et al. [73] deﬁned
city logistics as the process for totally optimizing the logistics and transport activities
by private companies with support of advanced information systems in urban areas
considering the traﬃc environment, the traﬃc congestion, the traﬃc safety and the
energy savings within the framework of a market economy [73].
Urban distribution and transport of goods is an important part of urban logistics.
In cities complicated problems arise related to urban freight transport: demand of
higher levels of service in terms of time, need for better services with lower costs
for customers, use of fewer vehicles, better utilization of vehicle capacity, reduc-
tion of negative environmental impacts, lower energy consumption, reduction of
noise, contribution to traﬃc congestion, the use of alternative energies and improv-
ing safety [26]. In the next coming years, changes from increased e-commerce and
home delivery will be apparent. This growth reinforces the general trend in logistics
towards smaller consignments, single orders and thus higher delivery frequency and
an increase in vehicle movements within cities [83].
In urban areas, goods distribution services are the most important transportation
and logistics activities and are usually called the last mile. Clear examples of these
are courier express and parcel services food delivery, perishable products, milk and
newspaper, urban solid waste collection and emergency transport. Surveys about
these problems are available in [16, 41].
Important areas in urban freight routes are concerned with reduction of fuel con-
sumption and emissions [70], and the use of night delivery schemes [18]. In contem-
porary living the travel speed between locations varies throughout the day according
to traﬃc conditions, especially in urban areas. Therefore, it is necessary to adapt the
models with time-dependence for routing planning [27]. The eﬀects of e-commerce
on the urban freight transport using vehicle routing and scheduling problem model
are studied in [36, 72].

Soft Computing Methods in Transport and Logistics
53
The design and development of city logistics systems requires the availability
of the models for the location of the logistics centers. The location of warehouse,
distribution centers and/or consolidation centers should be appropriately determined
for optimal operations [54]. On the other hand, depot location and vehicle routing are
two interdependent decisions and there are many authors who consider the integrated
problem of locating distribution centers in urban areas and the corresponding freight
distribution [52]. Recently, the authors in [38] investigate the combined impact of
depot location, ﬂeet composition and routing decisions on vehicle emissions in city
logistics. When real-time information is available, dynamic models can support the
developed systems. For example, in [29] on-line re-optimization based on current
traﬃc information and soft time constraints are proposed.
All of these problems are subject to the uncertainty of the environment. In some
situations logistic problems cannot exactly specify attributes as either deterministic
numbers or probabilistic random variables and it is natural and realistic to express
vagueness and ambiguity in fuzzy terms and to solve the problem with a fuzzy
approach [88]. In the literature, Soft Computing methodologies have been considered
in routing, scheduling and location. But integrating such methods into city logistics
has not been fully considered and needs to be addressed. Only a few references are
found in the literature. In [45] a postal delivery in agglomerations with a large num-
ber of customers modelled as a street routing problem is solved using fuzzy clus-
tering. Customer clustering is also used for VRP and is solved in the framework of
urban freight transport [85]. Another paper studies the dynamic operational envi-
ronment of courier service with fuzzy time windows [44]. Location models [86] and
speciﬁcally the design of e-commerce distribution systems [40] complete the papers
found in the literature.
3.3
Green Logistics
For decades, the main goal of logistics has been to improve its objectives from a
purely economic point of view. Through this period logistics professionals have per-
petuated this economic and commercial paradigm by allowing organization and man-
agement logistics to focus on maximizing economic proﬁtability. Economic perfor-
mance did not include costs such as environmental and social impact, instead it only
considered operational economic costs.
Nowadays there is a growing demand for multi-objective metrics associated with
logistical processes, where reduced operational costs and the negative impact of the
environment are some of the most common. As regards the last objective, logistic
stakeholders are pressured from public administrations and government to reduce
the environmental impact of their logistics operations. The environmental impact of
logistics companies can be measured in diﬀerent ways such as generation of noise
and vibrations, air quality and the contribution to global warming.

54
J. Brito et al.
Mckinnon et al. [47] present a classiﬁcation of the main pillars that compose what
we call green logistics:
∙Reducing Freight transport externalities. The largest part of the research on the
environmental impact of logistics is due to growth of freight traﬃc volumes. As
a consequence, the search for ways of reducing freight-related externalities is a
priority.
∙City Logistics. The study of transport eﬃciency and environmental impact of
logistics in urban areas. This topic will be further developed in later sections.
∙Reverse Logistics. The logistic associated with waste product and packaging for
reuse, is a promising area of green logistics. The logistics of waste management
help to increase the proportion of waste material that is recycled and reused.
∙Corporate environmental strategies. Companies have adopted and improved strate-
gies to reduce environmental impact and manage a balanced relationship with
nature. These strategies generate a wide range of actions on logistics operations
of the company.
∙Green Supply Chain Management. This pillar can be deﬁned as the alignment and
integration of environmental management within supply chain management. This
is based on that part of the environmental impact that is extended beyond their
own structure.
It is possible to ﬁnd several surveys and literature reviews related to green logis-
tics. In [93] a review of combinatorial optimization problems related with green
logistics and meta-heuristics in the swarm intelligence ﬁeld is proposed. A review
of Green Logistic Vehicle Routing Problems (GVRP) and a discussion of the next
wave of research into GVRP is presented in [43].
In general, the complex infrastructure of logistics and working in a dynamic busi-
ness environment imposes a high degree of uncertainty in the logistics process that
aﬀects its overall performance. More speciﬁcally, there is a complex system present
in green logistics with highly imprecise parameters and environmental factors with
a complexity that requires fuzzy sets to be represented.
In the literature, it has been proven that green logistics belong to a wide range
of practical application areas. In most cases, these optimization problems involve
complex systems introducing fuzzy sets and systems. A literature review and a dis-
cussion of the applications of fuzzy green logistics focus on fuzzy optimization in
green transport are provided in the remaining part of this section.
There are several papers in literature related with green logistics and fuzzy
approaches, speciﬁcally in green routing problems. A transport spatial decision sup-
port model for the optimization of green routes for city logistics centers is presented
in [57], where the solution integrates a multi-criteria method of Weighted Linear
Combination and the modiﬁed Dijkstra algorithm with a geographic information sys-
tem for processing spatial data in order to minimize the environmental impact of the
routes. In freight transportation activities, [28] puts forward the use of green logis-
tics in order to reduce the negative impact on the environment considering demand
and travel time uncertainty.

Soft Computing Methods in Transport and Logistics
55
Other papers focus on aspects such as reverse logistics and green scheduling. In
[62], a bi-objective mathematical model in the distribution of perishable products
with speciﬁed expiration date in inventory routing problem is presented. In order to
solve the model the Torabi-Hassini method based on a fuzzy approach is applied. A
green train scheduling model with a fuzzy multi-objective optimization algorithm is
presented in [42]. In order to solve the problem and obtain non-dominated solutions
which has an equally satisfactory degree on both objectives, a fuzzy mathematical
programming approach is proposed.
3.4
Tourist Trip Logistics
The Tourist Trip Design Problem, (TTDP) [78] arises when a tourist visiting a tourist
area, for one or more days, and is interested in visiting a number of points of interest
(POI). The problem more speciﬁcally deals with the choice of the POIÂt’s of the
trip and the order to follow each day. Each one of these POIs is associated with sev-
eral features that are taken into account in the selection. The main features are the
minimum time required for the visit, the days and hours of operation, the cost of the
activities within the visit and some indicators of proﬁt or degree of satisfaction that
could be perceived by the tourist for the visit. Information regarding distance, travel
time and cost between the POIs, and between these points and the hotel or stay resi-
dence of the tourist (start and end point of the trips), must also be taken into account.
This information together with some information about the tourist, such as his/her
preferences, budget and time limitations must be used to decide the trip selected for
each day of the stay at destination. The corresponding optimization problems have
received increasing interest in the tourist management and service in order to be
incorporated to recommenders, tourist planning tools and electronic guides. Since
most of the features that have to be used are subjective or subject to some level of
imprecision and vagueness, the fuzzy techniques and points of view have been used.
The problems may be complicated and made more realistic by considering addi-
tional features and constraints. Some of them are maximum budget (daily or com-
plete stay at destination), speciﬁc requirements on the minimum and/or maximum
number of days that the tourist visits to the POIs within a certain category (restau-
rants, beaches, historic sites, nature facilities, etc.), or on the number of visits to POIs
of a category some days. Travel times that depend on traﬃc congestion, weather
conditions, or the time of the day when traveling. Other realistic variants arise when
some of the POIs have time window constraints and the time used to visit them have
to be taken into account in the cost or proﬁt of the visit. Finally, in realistic cases
evaluation of the proﬁts or interest of the POIs depend on the already visited POIs
or some additional information [5].
The literature ﬁrstly distinguishes between problems with only one tour and with
several tours. Most research considers two opposed features or criteria: the proﬁt and
the cost. In the single tour problem, the main variants are the Proﬁtable Tour Problem
(PTP) where the objective is to maximize the diﬀerence between the proﬁt and the

56
J. Brito et al.
total cost that has been considered in [24], the Prize Collecting Travelling Salesman
Problem (PCTSP) where the objective is to minimize the travel cost subject to a
minimum proﬁt [2], and the Orienteering Problem (OP) where the objective is to
maximize the total proﬁt subject to a limit on the travel costs that was introduced by
[77].
Problems with multiple tours are usually known as Vehicle Routing Problems
with Proﬁt (VRPP), and the simplest version is the well-known Team Orienteering
Problem (TOP), which is an extension of the OP in which there is a ﬁxed number m
of routes and m also corresponds to the number of days available to tourists.
There are other problems using diﬀerent objective functions, such as the Prize-
Collecting VRP (PCVRP) [71] and the Capacitated Proﬁtable Tour Problem (CPTP)
[3]. In PCVRP, the objective function is given by the combination of three diﬀerent
objectives: minimizing the travel distance, minimizing the number of vehicles used
and maximizing proﬁts collected, while in CPTP the goal is to minimize the diﬀer-
ence between the total harvest proﬁt and the total trip cost.
The Team Orienteering Problem has been extensively studied in the literature [4]
and several versions, such as the Team Orienteering Problem with Time Windows
(TOPTW) [76] and Time Dependent Team Orienteering Problem with Time Win-
dows [30].
The use of metaheuristics and fuzzy approaches are quite common in wide range
of recent TTDP and other routing problems. The early work [46] considers a fuzzy
routing problem for sightseeing. A Genetic Algorithm for the VRPTW with fuzzy
demand is applied in [91]. In [55] a supplier selection model using fuzzy logic is
developed. Several multi-objective metaheuristics are used in [69] to solve VRP with
fuzzy demands. Several authors [19, 59, 60] apply a Particle Swarm Optimization for
a VRP with fuzzy demands and [8] use Genetic Algorithm. The paper [94] deal with
the CVRPTW with fuzzy travel time and demand using a hybrid between Ant Colony
and Genetic Algorithm. A fuzzy capacitated location routing problem is solved in
[32] by applying a Simulating Annealing with a mutation operator. Recently [82]
apply fuzzy optimization to the orienteering problem, [39] apply a fuzzy Ant Colony
system to solve the dynamic vehicle routing problem with uncertain service time,
and [50] uses fuzzy number comparisons to deal with VRPTW with fuzzy scores.
Finally, [12] apply a GRASP for solving the TOP with fuzzy scores and constraints.
4
Conclusions
This review has focused on two major areas of Soft Computing applied to optimiza-
tion: metaheuristics and fuzzy optimization. Works in metaheuristics are numerous
and often applied in emerging logistics areas. Relevant works with metaheuristics
outperform other methods, such as exact algorithms. Fuzzy optimization methods
are less frequent in the literature, providing additional research opportunities for Soft
Computing in a very open and promising ﬁeld.

Soft Computing Methods in Transport and Logistics
57
Acknowledgements This work has been partially funded by the Spanish Ministry of Economy
and Competitiveness with FEDER funds (TIN2015-70226-R) and supported by Fundación Caja-
canarias research funds (project 2016TUR19). Contributions from A. Expósito is supported by the
ACIISI of the Gobierno de Canarias and FSE.
References
1. Anbuudayasankar, S.P., Ganesh, K., Mohapatra, S.: Models for Practical Routing Problems in
Logistics. Springer International Publishing (2014)
2. Archetti, C., Bianchessi, N., Speranza, M.G.: Optimal solutions for routing problems with prof-
its. Discrete Appl. Math. 161(4), 547–557 (2013)
3. Archetti, C., Feillet, D., Hertz, A., Speranza, M.G.: The capacitated team orienteering and
proﬁtable tour problems. J. Oper. Res. Soc. 60(6), 831–842 (2009)
4. Archetti, C., Hertz, A., Speranza, M.G.: Metaheuristics for the team orienteering problem. J.
Heuristics 13(1), 49–76 (2007)
5. Archetti, C., Speranza, M.G., Vigo, D.: Vehicle routing: problems, methods, and applications,
vol. 18, chapter Vehicle Routing Problem with Proﬁts, pp. 273. SIAM (2014)
6. Avineri, E.: Soft computing applications in traﬃc and transport systems: a review. In: Hoﬀ-
mann, F., Kappen, M., Klawonn, F., Roy, R. (eds.) Soft Computing: Methodologies and Appli-
cations. Advances in Soft Computing, vol. 32, pp. 17–25. Springer, Berlin (2005)
7. Baghel, M., Agrawal, S., Silakari, S.: Survey of metaheuristic algorithms for combinatorial
optimization. Int J Comput Appl 58(19) (2012)
8. Bahri, O., Amor, N.B., El-Ghazali, T.: Optimization algorithms for multi-objective problems
with fuzzy data. In: 2014 IEEE Symposium on Computational Intelligence in Multi-Criteria
Decision-Making (MCDM), pp. 194–201. IEEE (2014)
9. Balcik, B., Beamon, B.M.: Facility location in humanitarian relief. Int. J. Logistics Res. Appl.
11(2), 101–121 (2008)
10. Bellman, R.E., Zadeh, L.A.: Decision-making in a fuzzy environment. Manage. Sci. 17(4),
B–141 (1970)
11. BoussaïD, I., Lepagnot, J., Siarry, P.: A survey on optimization metaheuristics. Inf. Sci. 237,
82–117 (2013)
12. Brito, J., Expósito, A., Moreno, J.A.: Solving the team orienteering problem with fuzzy scores
and constraints. In: 2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pp.
1614–1620. IEEE (2016)
13. Brito, J., Moreno, J.A., Verdegay, J.L.: Fuzzy optimization in vehicle routing problems. In:
IFSA/EUSFLAT Conference, pp. 1547–1552 (2009)
14. Brito, J., Moreno, J.A., Verdegay, J.L.: Transport route planning models based on fuzzy
approach. Iran. J. Fuzzy Syst. 9(1), 141–158 (2012)
15. Cadenas, J., Canas, M., Garrido, M., Ivorra, C., Liern, V.: Soft-computing based heuristics for
location on networks: the p-median problem. Appl. Soft Comput. 11(2), 1540–1547 (2011).
The Impact of Soft Computing for the Progress of Artiﬁcial Intelligence
16. Cattaruzza, D., Absi, N., Feillet, D., González-Feliu, J.: Vehicle routing problems for city logis-
tics. EURO J. Transp. Logistics 1–29 (2015)
17. Caunhye, A.M., Nie, X., Pokharel, S.: Optimization models in emergency logistics: a litera-
ture review. Socio-Econ. Plann. Sci. 46(1), 4–13 (2012). Special Issue: Disaster Planning and
Logistics: Part 1
18. Cavar, I., Kavran, Z., Jolic, N.: Intelligent transportation system and night delivery schemes
for city logistics. Comput. Technol. Appl. 2(9), 782–787 (2011)
19. Chen, J.-Q., Li, W.-L., Murata, T.: Particle swarm optimization for vehicle routing problem
with uncertain demand. In: 2013 4th IEEE International Conference on Software Engineering
and Service Science (ICSESS), pp. 857–860. IEEE (2013)

58
J. Brito et al.
20. Cooper, M.C., Lambert, D.M., Pagh, J.D.: Supply chain management: more than a new name
for logistics. Int. J. Logistics Manag. 8(1), 1–14 (1997)
21. Croom, S., Romano, P., Giannakis, M.: Supply chain management: an analytical framework
for critical literature review. Eur. J. Purchasing Supply Manag. 6(1), 67–83 (2000)
22. Delgado, M., Verdegay, J., Vila, M.: A general model for fuzzy linear programming. Fuzzy
Sets Syst. 29, 21–29 (1989)
23. Delgado, M., Verdegay, J.L., Vila, M.A.: Imprecise costs in mathematical programming prob-
lems. Control Cybernet 16(2), 113–121 (1987)
24. Dell’Amico, M., Maﬃoli, F., Värbrand, P.: On prize-collecting tours and the asymmetric trav-
elling salesman problem. Int. Trans. Oper. Res. 2(3), 297–308 (1995)
25. Drexl, M., Schneider, M.: A survey of variants and extensions of the location-routing problem.
Eur. J. Oper. Res. 241(2), 283–308 (2015)
26. Ehmke, J.: Integration of information and optimization models for routing in city logistics, vol.
177. Springer Science & Business Media (2012)
27. Ehmke, J.F., Steinert, A., Mattfeld, D.C.: Advanced routing for city logistics service providers
based on time-dependent travel times. J. Comput. Sci. 3(4), 193–205 (2012). City Logistics
28. Eshtehadi, R., Fathian, M., Demir, E.: Robust solutions to the pollution-routing problem with
demand and travel time uncertainty. Transp. Res. Part D: Transp. Environ. 51, 351–363 (2017)
29. Flamini, M., Nigro, M., Pacciarelli, D.: The value of real-time traﬃc information in urban
freight distribution. J. Intell. Transp. Syst. (2017). In press
30. Gavalas, D., Konstantopoulos, C., Mastakas, K., Pantziou, G., Vathis, N.: Heuristics for the
time dependent team orienteering problem: application to tourist route planning. Comput.
Oper. Res. 62, 36–50 (2015)
31. Ghaﬀari-Nasab, N., Ahari, S.G., Ghazanfari, M.: A hybrid simulated annealing based heuristic
for solving the location-routing problem with fuzzy demands. Scientia Iranica 20(3), 919–930
(2013)
32. Golozari, F., Jafari, A., Amiri, M.: Application of a hybrid simulated annealing-mutation oper-
ator to solve fuzzy capacitated location-routing problem. Int. J. Adv. Manuf. Technol. 67(5–8),
1791–1807 (2013)
33. Gunasekaran, A., Kobu, B.: Performance measures and metrics in logistics and supply chain
management: a review of recent literature (1995–2004) for research and applications. Int. J.
Prod. Res. 45(12), 2819–2840 (2007)
34. Guzmán, V.C., Masegosa, A.D., Pelta, D.A., Verdegay, J.L.: Fuzzy models and resolution
methods for covering location problems: an annotated bibliography. Int. J. Uncertainty Fuzzi-
ness Knowl. Based Syst. 24(04), 561–591 (2016)
35. Herrera, F., Verdegay, J.: Three models of fuzzy integer linear programming. Eur. J. Oper. Res.
83(3), 581–593 (1995)
36. Hong, X., Jingjing, Q., Xingli, T.: B2c e-commerce vehicle delivery model and simulation.
Inf. Technol. J. 12(20), 5891 (2013)
37. Ko, M., Tiwari, A., Mehnen, J.: A review of soft computing applications in supply chain man-
agement. Appl. Soft Comput. 10(3), 661–674 (2010)
38. Koc, C., Bekta, T., Jabali, O., Laporte, G.: The impact of depot location, ﬂeet composition and
routing on emissions in city logistics. Transp. Res. Part B: Methodol. 84, 81–102 (2016)
39. Kuo, R., Wibowo, B., Zulvia, F.: Application of a fuzzy ant colony system to solve the dynamic
vehicle routing problem with uncertain service time. Appl. Math. Modell. 40(23), 9990–10001
(2016)
40. Lau, H.C.W., Jiang, Z.Z., Ip, W.H., Wang, D.W.: A credibility-based fuzzy location model with
hurwicz criteria for the design of distribution systems in b2c e-commerce. Comput. Ind. Eng.
59(4), 873–886 (2010)
41. Lewczuk, K., Żak, J., Pyza, D., Jacyna-Gołda, I.: Vehicle routing in an urban area: environ-
mental and technological determinants. WIT Trans. Built Environ. 130, 373–384 (2013)
42. Li, X., Wang, D., Li, K., Gao, Z.: A green train scheduling model and fuzzy multi-objective
optimization algorithm. Appl. Math. Modell. 37(4), 2063–2073 (2013)

Soft Computing Methods in Transport and Logistics
59
43. Lin, C., Choy, K., Ho, G., Chung, S., Lam, H.: Survey of green vehicle routing problem: past
and future trends. Expert Syst. Appl. 41(4, Part 1), 1118–1138 (2014)
44. Lin, C., Choy, K., Ho, G., Lam, H., Pang, G.K., Chin, K.: A decision support system for opti-
mizing dynamic courier routing operations. Expert Syst. Appl. 41(15), 6917–6933 (2014)
45. Matis, P., Koháni, M.: Very large street routing problem with mixed transportation mode.
CEJOR 19(3), 359–369 (2011)
46. Matsuda, Y., Nakamura, M., Kang, D., Miyagi, H.: A fuzzy optimal routing problem for sight-
seeing. IEEJ Trans. Electron. Inf. Syst. 125, 1350–1357 (2005)
47. McKinnon, P., Cullinane, S., Whiteing, A., Browne, P.: Green Logistics: Improving the Envi-
ronmental Sustainability of Logistics. Kogan Page (2010)
48. Mehrjerdi, Y.Z., Nadizadeh, A.: Using greedy clustering method to solve capacitated location-
routing problem with fuzzy demands. Eur. J. Oper. Res. 229(1), 75–84 (2013)
49. Melo, M., Nickel, S., da Gama, F.S.: Facility location and supply chain management: a review.
Eur. J. Oper. Res. 196(2), 401–412 (2009)
50. Mendez, C.E.C.: Team Orienteering Problem with Time Windows and Fuzzy Scores. PhD
thesis, National Taiwan University of Science and Technology (2016)
51. Moreno, J.A., Vega, J.M., Verdegay, J.L.: Fuzzy location problems on networks. Fuzzy Sets
Syst. 142(3), 393–405 (2004)
52. Muñoz-Villamizar, A., Montoya-Torres, J.R., Juan, A.A., Cáceres-Cruz, J.: A simulation-based
algorithm for the integrated location and routing problem in urban logistics. In: 2013 Winter
Simulations Conference (WSC), pp. 2032–2041 (2013)
53. Nayeem, S.M.A., Pal, M.: The p-center problem on fuzzy networks and reduction of cost. Iran.
J. Fuzzy Syst. 5(1), 1–26 (2008)
54. Olsson, J., Woxenius, J.: Localisation of freight consolidation centres serving small road
hauliers in a wider urban area: barriers for more eﬃcient freight deliveries in gothenburg.
J. Transp. Geogr. 34, 25–33 (2014)
55. Ordoobadi, S.M.: Development of a supplier selection model using fuzzy logic. Supply Chain
Manage. Int. J. 14(4), 314–327 (2009)
56. Owen, S.H., Daskin, M.S.: Strategic facility location: a review. Eur. J. Oper. Res. 111(3), 423–
447 (1998)
57. Pamučar, D., Gigović, L., Ćirović, G., Regodić, M.: Transport spatial model for the deﬁnition
of green routes for city logistics centers. Environ. Impact Assess. Rev. 56, 72–87 (2016)
58. Peidro, D., Mula, J., Poler, R., Verdegay, J.-L.: Fuzzy optimization for supply chain planning
under supply, demand and process uncertainties. Fuzzy Sets Syst. 160(18), 2640–2657 (2009)
59. Peng, Y., Chen, J.: Vehicle routing problem with fuzzy demands and the particle swarm opti-
mization solution. In: 2010 International Conference on Management and Service Science
(MASS), pp. 1–4. IEEE (2010)
60. Peng, Y., Qian, Y.-M.: A particle swarm optimization to vehicle routing problem with fuzzy
demands. J. Convergence Inf. Technol. 5(6), 112–119 (2010)
61. Prodhon, C., Prins, C.: A survey of recent research on location-routing problems. Eur. J. Oper.
Res. 238(1), 1–17 (2014)
62. Rahimi, M., Baboli, A., Rekik, Y.: Sustainable inventory routing problem for perishable prod-
ucts by considering reverse logistic. IFAC-PapersOnLine 49(12), 949–954 (2016)
63. Russo, F., Comi, A.: A classiﬁcation of city logistics measures and connected impacts.
Procedia—Soc. Behav. Sci. 2(3), 6355–6365 (2010)
64. Sheu, J.-B.: Challenges of emergency logistics management. In: Transportation Research Part
E: Logistics and Transportation Review, vol. 43, no. 6, pp. 655–659 (2007). Challenges of
Emergency Logistics Management
65. Sheu, J.-B.: An emergency logistics distribution approach for quick response to urgent relief
demand in disasters. In: Transportation Research Part E: Logistics and Transportation Review,
vol. 43, no. 6, pp. 687–709 (2007). Challenges of Emergency Logistics Management
66. Sheu, J.-B., Chen, Y.-H., Lan, L.W., et al.: A novel model for quick response to disaster relief
distribution. Proc. Eastern Asia Soc. Transp. Stud. 5, 2454–2462 (2005)

60
J. Brito et al.
67. Simchi-Levi, D., Chen, X., Bramel, J.: The logic of logistics. Algorithms, and Applications for
Logistics and Supply Chain Management, Theory (2005)
68. Sörensen, K.: Metaheuristics-the metaphor exposed. Int. Trans. Oper. Res. 22(1), 3–18 (2015)
69. Sulieman, D., Jourdan, L., Talbi, E.-G.: Using multiobjective metaheuristics to solve vrp with
uncertain demands. In: 2010 IEEE Congress on Evolutionary Computation (CEC), pp. 1–8.
IEEE (2010)
70. Suzuki, Y.: A new truck-routing approach for reducing fuel consumption and pollutants emis-
sion. Transp. Res. Part D: Transp. Environ. 16(1), 73–77 (2011)
71. Tang, L., Wang, X.: Iterated local search algorithm based on very large-scale neighborhood
for prize-collecting vehicle routing problem. Int. J. Adv. Manuf. Technol. 29(11), 1246–1258
(2006)
72. Taniguchi, E., Kakimoto, Y.: Modelling eﬀects of e-commerce on urban freight transport,
chapter Chapter 10, pp. 135–146. emeraldinsight (2004)
73. Taniguchi, E., Thompson, R., Yamada, T., Duin, R.V.: City Logistics: Network Modelling and
Intelligent Transport Systems. Pergamon (2001)
74. Torres, I., Cruz, C., Verdegay, J.L.: Solving the truck and trailer routing problem with fuzzy
constraints. Int. J. Comput. Intell. Syst. 8(4), 713–724 (2015)
75. Toth, P., Vigo, D.: Vehicle Routing. Society for Industrial and Applied Mathematics. Philadel-
phia, PA (2014)
76. Tricoire, F., Romauch, M., Doerner, K.F., Hartl, R.F.: Heuristics for the multi-period orien-
teering problem with multiple time windows. Comput. Oper. Res. 37(2), 351–367 (2010)
77. Tsiligirides, T.: Heuristic methods applied to orienteering. J. Oper. Res. Soc. 797–809 (1984)
78. Vansteenwegen, P., Oudheusden, D.V.: The mobile tourist guide: an or opportunity. OR Insight
20(3), 21–27 (2007)
79. Verdegay, J.: Fuzzy Information and Decision Processes, chapter Fuzzy mathematical pro-
gramming. North-Holland (1982)
80. Verdegay, J.L.: Fuzzy optimization: models, methods and perspectives. In: In proceeding 6th
IFSA-95 World Congress, pp. 39–71 (1995)
81. Verdegay, J.L., Yager, R.R., Bonissone, P.P.: On heuristics as a fundamental constituent of soft
computing. Fuzzy Sets Syst. 159, 846–855 (2008)
82. Verma, M., Shukla, K.K.: Application of fuzzy optimization to the orienteering problem. Adv.
Fuzzy Syst. 2015, 8 (2015)
83. Visser, J., Nemoto, T., Browne, M.: Home delivery and the impacts on urban freight transport:
a review. Procedia—Soc. Behav. Sci. 125, 15–27 (2014)
84. Wang, S., Ma, Z., Zhuang, B.: Fuzzy location-routing problem for emergency logistics systems.
Comput. Modell. New Technol. 18(2), 265–273 (2014)
85. Wang, Y., Ma, X., Xu, M., Wang, Y., Liu, Y.: Vehicle routing problem based on a fuzzy cus-
tomer clustering approach for logistics network optimization. J. Intell. Fuzzy Syst. 29, 1427–
1442 (2015)
86. Wang, Y., Ma, X.L., Wang, Y.H., Mao, H.J., Zhang, Y.: Location optimization of multiple
distribution centers under fuzzy environment. J. Zhejiang Univ. Sci. A 13(10), 782–798 (2012)
87. Wassenhove, L.N.V.: Humanitarian aid logistics: supply chain management in high gear. J.
Oper. Res. Soc. (2006)
88. Wong, B.K., Lai, V.S.: A survey of the application of fuzzy set theory in production and oper-
ations management: 1998–2009. Int. J. Prod. Econ. 129(1), 157–168 (2011)
89. Xiao, S.C., Wu, J.F., He, H., Yang, Z.D., Shen, X.: An emergency logistics transportation
path optimization model by using trapezoidal fuzzy. In: 2014 11th International Conference
on Fuzzy Systems and Knowledge Discovery (FSKD), pp. 199–203 (2014)
90. Xiong, N., Molina, D., Ortiz, M.L., Herrera, F.: A walk into metaheuristics for engineering
optimization: principles, methods and recent trends. Int. J. Comput. Intell. Syst. 8(4), 606–636
(2015)
91. Xu, J., Goncalves, G., Hsu, T.: Genetic algorithm for the vehicle routing problem with time win-
dows and fuzzy demand. In: Evolutionary Computation, 2008, pp. 4125–4129. IEEE (2008)

Soft Computing Methods in Transport and Logistics
61
92. Zhang, M.-X., Zhang, B., Zheng, Y.-J.: Bio-inspired meta-heuristics for emergency transporta-
tion problems. Algorithms 7(1), 15–31 (2014)
93. Zhang, S., Lee, C., Chan, H., Choy, K., Wu, Z.: Swarm intelligence applied in green logistics:
a literature review. Eng. Appl. Artif. Intell. 37, 154–169 (2015)
94. Zulvia, F.E., Kuo, R., Hu, T.-L.: Solving cvrp with time window, fuzzy travel time and demand
via a hybrid ant colony optimization and genetic algortihm. In: 2012 IEEE Congress on Evo-
lutionary Computation (CEC), pp. 1–8. IEEE (2012)

Applications of Soft Computing in Intelligent
Transportation Systems
Antonio D. Masegosa, Enrique Onieva, Pedro Lopez-Garcia
and Eneko Osaba
Abstract Intelligent Transportation Systems emerged to meet the increasing
demand for more eﬃcient, reliable and safer transportation systems. These systems
combine electronic, communication and information technologies with traﬃc engi-
neering to respond to the former challenges. The beneﬁts of Intelligent Transporta-
tion Systems have been extensively proved in many diﬀerent facets of transport and
Soft Computing has played a major role in achieving these successful results. This
book chapter aims at gathering and discussing some of the most relevant and recent
advances of the application of Soft Computing in four important areas of Intelligent
Transportation Systems as autonomous driving, traﬃc state prediction, vehicle route
planning and vehicular ad hoc networks.
1
Introduction
New trends in business, commerce or leisure have increased the demand for more
eﬃcient, reliable and safer transportation systems. This fact is claimed by diﬀer-
ent national and international institutions, such as the OECD [49] or the European
Commission [20], just to name but a few. Some of the reasons behind the increasing
A.D. Masegosa (✉) ⋅E. Onieva ⋅P. Lopez-Garcia ⋅E. Osaba
Faculty of Engineering, University of Deusto, 48007 Bilbao, Spain
e-mail: ad.masegosa@deusto.es
E. Onieva
e-mail: enrique.onieva@deusto.es
P. Lopez-Garcia
e-mail: pedro.lpzgrc@deusto.es
E. Osaba
e-mail: e.osaba.es@deusto.es
A.D. Masegosa ⋅E. Onieva ⋅P. Lopez-Garcia ⋅E. Osaba
DeustoTech-Fundacion Deusto, Deusto Foundation, 48007 Bilbao, Spain
A.D. Masegosa
IKERBASQUE, Basque Foundation for Science, 48011 Bilbao, Spain
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_4
63

64
A.D. Masegosa et al.
importance of improving transportation systems are the oﬀshore outsourcing of pro-
duction, the adoption of just-in-time distribution systems, the tight scheduling of per-
sonal and freight activities, the broadening of international trade, the large number
of people living in cities or the big amount of Green House Gases emission caused
by transport.
The development of Intelligent Transportation Systems (ITS) [18] is one of the
major areas of research that work in addressing these issues nowadays. ITS is a dis-
cipline that combines telecommunication, electronics, information technologies and
traﬃc engineering methodologies to provide innovative services associated to dif-
ferent modes of transport as well as traﬃc management in order to oﬀer the users
more information and safety and to allow a more eﬃcient and eﬀective use of trans-
port networks. The beneﬁts of these systems have been successfully proved in many
diﬀerent transport environments [33].
Since their beginning around 1930 with the ﬁrst electric traﬃc signals, ITS have
coped virtually all facets of transport given rise to diﬀerent types of systems to
respond to the diﬀerent problems that appear in each of those facets. Some of these
types of ITS are [23]:
∙Advanced Traﬃc Management Systems (ATMSs) that aim at improving traﬃc ser-
vice quality by collecting data, supporting decision making to operators and con-
trol traﬃc in real-time with diﬀerent systems.
∙Advanced Travelers Information Systems (ATISs) which are design to help travel-
ers in the diﬀerent stages of their trips by providing them information in real-time
about the best route to their destiny, the most appropriate schedule or transport
media, etc.
∙Commercial Vehicles Operation Systems whose objective is to increase safety and
eﬃciency in commercial vehicles ﬂeets, by combining diﬀerent ITS technologies
with the intention of improving the management and control of vehicles as well
as the information available for drivers and decision makers.
∙Advanced Public Transportations Systems (APTSs) that pursue the enhancement
of the operation of public transportation media (subway, tram, bus, etc.) with the
joint use of technologies from ATISs and ATMSs.
∙Advanced Vehicles Control Systems (AVCSs) aims at assisting, alerting and taking
the whole or part of vehicle driving with the aid of several in-vehicle sensors,
computers and/or communication networks.
Soft Computing (SC) [75] has played a major role in the success of ITS in recent
years, especially because of the bigger amount of data provided and collected from
several sources by the diﬀerent stakeholders involved in these systems as govern-
ments, industry and citizens [92]. ITS are an environment very appropriated to
applied SC techniques because the information handle present most of the features
for which SC was designed for. For example, the sensors usually present impreci-
sion in their measures; traﬃc is strongly aﬀected by factors with a high uncertainty
as weather; and the decision making should take into account drivers’ or other users’
preferences which subject to a high vagueness and subjectivity. For these reasons,

Applications of Soft Computing in Intelligent Transportation Systems
65
techniques such as fuzzy sets, neural networks, metaheuristics or probabilistic rea-
soning have been widely used by the research community in ITS.
This book chapter aims at gathering some of the most relevant and recent advances
of the application of SC in ITS in order to serve as a guide for student, researchers and
practitioners interested in this ﬁeld. Concretely, we focus on four important areas of
ITS as the AVCSs for autonomous driving, the ATISs for the prediction of the future
state of the traﬃc, the CVOSs for the planning of routes of ﬂeets of vehicles, as well
as one of the most important key enable technologies of future ITS, vehicular ad hoc
networks.
The manuscript is organized as follows. The next four sections are devoted to
review the application of SC in each of the ITS areas aforementioned in order of
appearance. After that, in the last section of the chapter, we discuss the main con-
clusions drawn from the works reviewed.
2
Soft Computing in Autonomous Driving
Autonomous driving has been one of the most beneﬁted ﬁelds in ITS of the appli-
cation of SC, since, until recently, autonomous driving remained like one of those
problems that humans were able to manage better than machines [13].
The European Union has an ambitious road safety target for this decade: halving
the number of road deaths between 2010 and 2020. In 2014, almost 25,700 road
fatalities were reported in the EU, this is around 1% fewer deaths than reported in
2013 and 18% fewer than in 2010 [1].
Automation, and in particular digitalization of driving will change road transport
in a way which will be viewed as a revolution in the ﬁeld of mobility. As human error
is the main reason for road traﬃc accidents, controlling the driving by a computer
is expected to make future road transport safer and more secure. A fully automated
mobility of vehicles in roads will have incredible potential impact in the society as
known until now. Beneﬁts from such total automation of the vehicles will derive in
evident proﬁts for society lowering costs and increasing safety, but also will provide
deep changes in the ways people and goods move around cities.
Car sharing [8] and car pooling [82] are two examples of emerging paradigms of
mobility that can deeply impact the mobility, if come accompanied by autonomous
driving of vehicles involved in the business models. Some authors study social ten-
dency of population to owning a vehicle in property, and question about the eﬀect
that autonomous vehicles will cause in social perception of owning a car [76]. Philo-
sophical and ethical sciences are also inﬂuenced by the emergence of autonomous
vehicle applications, and researchers try to answer if humans are ready for utilitarian
autonomous vehicles [11]. As can be seen, not only technological but also societal
and business, among other ﬁelds of research, are involved in the study of the automa-
tion of vehicles.
In one hand, both the upgrade and lowering of the sensor and equipment neces-
sary for the processing of information needed by an autonomous vehicle to take a

66
A.D. Masegosa et al.
decision have made this ﬁeld to receive interest from research groups from the entire
world. In the other, methods and algorithms able to deal in each one of the stages
involved in the driving of a vehicle (e.g. signal processing, decision making, control,
communication or planning) are performing really well nowadays. Both equipment
and methods are making nearer the day in which a fully autonomous driving is avail-
able for society.
It is known that the techniques under the topic of Soft Computing have a strong
capability of learning and cognition, as well as a good tolerance to uncertainty and
imprecision. Due to these properties they can be applied successfully to problems
derived by the driving of a vehicle. Methods associated to the ﬁeld of Soft Computing
have been naturally used by researchers to take steps through the development of
autonomous vehicles.
In the remainder part of this section, some of the most relevant applications of Soft
Computing to the ﬁeld of autonomous vehicles are discussed. These applications
are structured according to the main Soft Computing technique involved, concretely,
fuzzy logic, genetic algorithms and neural networks.
2.1
Fuzzy Logic in Autonomous Driving
Due to the ability of representing expert knowledge in the form of simple and legible
rules, fuzzy logic has been broadly used in the ﬁeld of autonomous vehicles, mainly
in the development of control algorithms. In this case, control of a complex non-
linear system can be expressed in the form of a set of simple fuzzy rules (e.g. if
speed is too high, then press brake). Fuzzy based methods are specially indicated
when we try to emulate human control actions, such as human car driving [59].
Examples of the use of fuzzy rules for the control of the elements of an
autonomous vehicle can be found in the recent literature in large amount of exam-
ples. In [64], Rodriguez-Castao et al. used a Takagi-Sugeno-Kang fuzzy system for
GPS based autonomous navigation of heavy vehicles at high speed. Other example
can be found in [22], where Faddel et al. used a fuzzy system to manage the con-
troller for electric vehicle charging. The steering control of autonomous vehicles has
received important attention from researchers in fuzzy logic, examples of such inter-
est are [5], where the authors proposed an autonomous platooning system for trucks,
including steering control, in order to increase in traﬃc capacity. Motion planning
has been another topic where fuzzy logic has been applied. For instance, in [37], Kala
and Warwick implemented decision making over autonomous vehicle maneuvering.
Pedals control by fuzzy logic has been also tackled in several works, as [51] or [34].
2.2
Metaheuristics in Autonomous Driving
Metaheuristics, specially genetic and evolutionary algorithms, have been exten-
sively used, as optimization methods in the ﬁeld of autonomous driving. Their main

Applications of Soft Computing in Intelligent Transportation Systems
67
objective has been the tuning of control systems, decision making and improvement
of the eﬃciency of the overall traﬃc. In conjunction with fuzzy logic, as the way of
representing knowledge, these algorithms have been extensively used for optimizing
the distribution of membership functions, rule base or both, under the paradigm of
the genetic fuzzy systems.
In [19], Du et al. used a genetic algorithm to optimize a model predictive con-
troller for the simultaneous control of the steering and pedals of an autonomous
vehicle, taking the comfort of the passengers as an objective to optimize. A path
planning and scheduling method for ﬂeets of autonomous vehicles was proposed in
[79]; in this work, Xidias et al. used a genetic algorithm to obtain a near optimum
solution for a problem resulting of combining both (planning and scheduling) ones. A
multi-objective genetic algorithm was used by Onieva et al. in [52] in order to gener-
ate speed proﬁles for autonomous vehicles to follow in order to cross an intersection
where no cooperation among vehicles is possible. Finally, parking trajectories have
also been candidates to be optimized by evolutionary algorithms [91].
2.3
Neural Networks in Autonomous Driving
Neural networks (NNs) provide a set of qualities that makes them extremely precise
for the representation of complex non-linear systems. They have been used in the
ﬁeld of autonomous vehicles, in one hand, for the control of the actuators of the
vehicle, but also for the processing of the high amount of data received by the vehicle,
in particular under the ﬁeld of computer vision.
Recently, with the explosion of deep learning paradigm [66], researchers have
deﬁned a new framework where all the information available is used to train models,
which are increasing in accuracy as new elements are fed into the NN [15]. They have
been applied by a large number of researchers to the processing of visual information
captured by autonomous vehicles. In [36], Jia et al. used deep neural networks to
provide precise obstacle detection in front of an autonomous vehicle, as well as to
segment obstacles and infer their depths. A convolutional NN is used in [88] by Yang
et al. to classify roads signs in a hierarchical way, obtaining both the subclasses
within each superclass exposed in a picture. In combination with fuzzy logic, the
work by Barman et al. [7] presented a fuzzy-NN to guide an unmanned vehicle for
maintaining traﬃc rules to reach its goal and avoid obstacles. Examples of control
of actuators at a low level by means of NNs can be found in [17] for lateral, and [58]
for the longitudinal control of autonomous vehicles.
3
Soft Computing in Traﬃc State Prediction
According to the Eurobarometer 2014 about the quality of the transportation, the
preferred mode of transport in a typical day is the car, well above from urban public
transport. This issue, added to the fact that vehicles per capita have been increased in

68
A.D. Masegosa et al.
the last 10 years, has raised the eﬃciency of the transport to the level of fundamental
condition, especially in big cities. For these reasons, road trips are a key point inside
ITS, due to the importance in daily life not only for people but also for transportation
companies. Inside this subject, one ﬁeld where diﬀerent techniques are being used
during the last years with a high impact and reliable performance is the prediction
of the traﬃc state in freeway and urban scenarios. One of the principal challenges
in this ﬁeld is to predict, with a certain level of conﬁdence, possible traﬃc jams in
a short-term horizon. The principal advantage of the successful prediction of traﬃc
jams is the adaptation of decision making in the exact moment when diﬀerent events
that may aﬀect traﬃc, as for example accidents, occur. Another advantage is the
capability of calculating not only the travel time but also of planning the route to
follow before its beginning. If the user knows the probability of ﬁnding a traﬃc
jam in its route, he/she can avoid it by changing the route before or even during the
journey. In a general way, the successful prediction of traﬃc jams can lead to the
decrease of travel time, the reduction of CO2 emissions as well as fuel consumption,
or the decrease of acoustic contamination in urban and freeway environments.
Following the same guidelines of the previous section, some relevant applications
of Soft Computing in traﬃc state prediction are reviewed. Concretely, the application
of the three components of Soft Computing most commonly used in this topic as
neural networks, fuzzy logic and probabilistic reasoning.
3.1
Neural Networks in Traﬃc State Prediction
In the last years, traﬃc congestion prediction is one of the ﬁelds where NNs have
been widely used, as can be seen in literature. For example, in [42], Kumar et al.
applied a NN to predict traﬃc congestion using historical traﬃc data. Volume, speed,
density, and both time and day of the week were used as input variables. The model
was validated using rural highway traﬃc. Another case was presented in [50] by Oh
et al., where Gaussian mixture model clustering is combined with a NN to create an
urban traﬃc ﬂow prediction system. The system forecasts traﬃc ﬂow by combining
road geographical and environmental factors with traﬃc ﬂow properties obtained by
the use of detectors. Another type of NNs, called Back Propagation NN (BPNN) is
used to forecast campus traﬃc congestion level in [90]. The results are compared
with a Markov model, and the BPNN achieved higher accuracy and more stable
performance.
In [41], Koesdwiady et al. used a deep belief networks to enhance prediction
accuracy using weather conditions. The study had two objectives: to investigate a
correlation between weather parameters and traﬃc ﬂow, and to improve traﬃc ﬂow
prediction accuracy. The data used for this paper was originated from San Francisco
Bay area of California. A Big Data-based framework was adopted in [68] by Soua et
al. to address the problem of short-term traﬃc ﬂow prediction. Deep belief networks
are used to independently predict traﬃc ﬂow using historical traﬃc ﬂow and weather
data, and event-based data.

Applications of Soft Computing in Intelligent Transportation Systems
69
3.2
Probabilistic Reasoning in Traﬃc State Prediction
Authors of [4] presented an hybrid approach of parametric and nonparametric meth-
ods such as an ensemble of Kalman Filter and NNs in order to improve the travel
time prediction of journeys that starts from 15 to 30 min in the future. Kalman Filter
was combined with ARIMA in [80], where the ARIMA model is built using histor-
ical traﬃc data. After that, the model is integrated with a Kalman Filter to construct
a road traﬃc state prediction algorithm. Four road segments in Beijing were adopted
for the case studies accomplished.
In [24], Fusco et al. hybridized Bayesian networks and NN to create short-term
prediction models using as data the link speeds recorded on the metropolitan area
of Rome during 7 months. Other example where Bayesian networks are applied to
short-term traﬃc prediction was presented in [81]. In the mentioned paper, traﬃc
ﬂow is predicted using a Bayesian multivariate adaptive-regression splines model.
Data is collected from a series of observation stations along the freeway Interstate
205 in Portland, USA, and used to evaluate the performance of the model. Results
were compared with diﬀerent methods, as ARIMA, seasonal ARIMA, and a Kernel
method Support Vector Regression.
3.3
Fuzzy Logic in Traﬃc State Prediction
As mentioned above, fuzzy logic allows to process imprecise information using IF-
THEN rules, which helps to the interpretation of the ﬁnal model. One of the most
used and known types of fuzzy systems are Fuzzy Rule Based Systems (FRBS),
which can be divided into Mamdani and Takagi-Sugeno-Kang (TSK) systems.
Besides, another kind of systems, based in the previous ones, are called Hierarchi-
cal FRBS (HFRBS). This class of systems counts with several FRBSs, which are
joined in a way that the output of one of them is connected to the input of another
one. Depending of the structure of the hierarchy, those systems can be divided into
parallel, serial, and hybrid [9].
In traﬃc congestion prediction, those systems have been used in [93, 94] to
develop a congestion prediction system employing a large number of input vari-
ables. In these papers, a Steady-State GA is applied to tune the diﬀerent parts of the
FRBSs. A related work is presented in [45], where Lopez-Garcia et al. used a hybrid
algorithm that combines GA and Cross Entropy method to tune a HFRBS in order to
predict congestion in a freeway in California with time horizons of 5, 15, and 30 min.
An extension of that work is presented in [46], where state-of-the-art techniques are
compared with the results obtained by the tuned HFRBSs in diﬀerent traﬃc con-
gestion datasets. Finally, another interested paper in this topic was presented in [53]
Onieva et al. where the authors compare the performance of several Evolutionary
Fuzzy and Crisp Rule Based methods for traﬃc congestion prediction.

70
A.D. Masegosa et al.
4
Soft Computing in Vehicle Route Planning
Other ﬁeld in which soft computing techniques have demonstrated an outstanding
performance is vehicle route planning or vehicle routing problems. Nowadays, route
planning is a widely studied ﬁeld in which the most used and well-known problems
are the Traveling Salesman Problem (TSP) [43], and the Vehicle Routing Problem
(VRP) [44], being the focus of a big amount of studies in the literature.
The reasons for the importance and popularity of this kind of problems are both
scientiﬁc and social. On the one hand, most of the problems arising in this ﬁeld
have a great complexity since they belong to NP-Hard class. For this reason, their
resolution supposes a major challenge for the scientiﬁc community. On the other
hand, routing problems are usually built to address real world situations related to
logistics, transportation, electronics, robotics, etc.
The ﬁrst part of this section is focused on metaheuristics, whose application in the
resolution of these optimization problems has been very successful. The second part
revolves around the use of fuzzy logic in routing problems, describing some relevant
works published in the last years.
4.1
Metaheuristics in Vehicle Route Planning
Metaheuristics have been widely used for the solving of routing problems in the
last decades, becoming the state-of-the-art in the resolution of many of the vari-
ants of these problems. One of the ﬁrst metaheuristics applied in this context was
Simulated Annealing (SA) [74]. For example, in [47], Malek et al. presented a ser-
ial and parallel SA for solving the TSP. Other example of the application of this
technique for route planning is the work published by Chiang and Rusell [16], in
which the VRP with Time Windows is solved using a SA. More recently, Baos et al.
developed a parallel variant of SA, called Multiple Temperature Pareto SA in [6], to
also solve the VRP with Time Windows with very successful results. Another well-
known stochastic local search, Tabu Search (TS), has been also frequently used for
solving route planning problems. A recent work on this topic is the one presented by
Escobar et al. in 2014 [21], in which they proposed a hybrid granular TS for tackling
the challenging Multi-Depot VRP. Brieﬂy explained, the proposed method considers
diﬀerent neighborhoods and diversiﬁcation strategies, with the aim of improving the
initial solution obtained by a hybrid procedure. The Variable Neighborhood Search
(VNS) has also demonstrated its eﬃciency in this area. An interesting example is
the work presented in [14], in which Carrabs et al. proposed a VNS for solving a
multi-attribute version of the TSP: a Pickup and Delivery TSP with LIFO Loading.
More concretely, the authors of this paper introduce three new local search opera-
tors, which are then embedded within a VNS. In a more recent publication, Sarasola
et al. [65] developed a VNS for facing a stochastic and dynamic VRP. This version
of the VRP contemplates two diﬀerent features. The ﬁrst one is stochastic demand,

Applications of Soft Computing in Intelligent Transportation Systems
71
which is only revealed when the vehicle arrives at the customer location. The second
feature is the dynamic request, meaning that new orders from previously unknown
customers can be received and scheduled over time.
Furthermore, evolutionary methods have also shown a great performance for this
sort of problems, being Genetic Algorithms (GA) one of the most successful ones.
The work presented by Vidal et al. in 2013 is an example of this fact [77]. In this
research, a hybrid genetic algorithm with adaptive diversity management is imple-
mented for tackling the VRP with time windows. Another example is the survey
paper published by Karakatič and Podgorelec in 2015 [39], which collects some of
the most important works focused on the application of the GA to the multi-depot
VRP.
Additionally, since the appearance of GA in the early 1970s, a wide variety
of nature-inspired metaheuristics have also appeared in literature. Some of these
recently proposed methods are the Fireﬂy Algorithm (FA) and the Bat Algorithm
(BA). The FA was proposed by Yang in 2008 [84]. This meta-heuristic has been
applied to a wide range of optimization ﬁelds and problems since its proposal [87],
and it has also shown a promising performance for routing problems. In [35], for
example, Jati and Suyanto presented the ﬁrst application of the FA for solving the
TSP. In order to do that, authors adapt the FA, which was ﬁrstly proposed for tack-
ling continuous problems, providing it with an evolutionary and discrete behavior.
Another interesting example of application is the one presented in [3] by Alinaghian
and Naderipour, in which a hybrid version of the FA is proposed to solve a time-
dependent VRP with multi-alternative graph, in order to reduce the fuel consump-
tion. The developed hybrid version of the FA is a Gaussian Fireﬂy Algorithm. The
most interesting part of this paper is the real-world use case that authors presented,
focused on a distribution company, established in Esfahan, Iran. Additionally, in [56]
Osaba et al. also shown that the FA is able to face complex routing problem, such as
the asymmetric and clustered VRP with simultaneous pickup and deliveries, variable
costs and forbidden paths. Finally, in [54], the same authors presented a evolution-
ary discrete FA with a novel operator to deal with VRP with time windows with
successful results.
Regarding the other nature-inspired method mentioned above, the BA, it was pro-
posed by Yang in 2010 [85]. As can be read in several surveys [86], the BA has been
successfully applied to diﬀerent optimization ﬁelds and problems since its proposal.
Focusing in routing problems, several recent papers have shown that the BA is a
promising technique in vehicle route planning. For example, in [70], Taha et al. pre-
sented an adapted version of this algorithm for solving the well-known Capacitated
VRP. The Adapted BA developed in that study allows a large diversity of the popu-
lation and a balance between global and local search. Zhou et al. addressed the same
problem in [95]. In that paper a hybrid BA with path relinking is described. This
approach is constructed based on the framework of the continuous BA, in which
the greedy randomized adaptive search procedure and path relinking are eﬀectively
integrated. Additionally, with the aim of improving the performance of the tech-
nique, the random subsequences and single-point local search are operated with
certain probability. In [55], Osaba et al. presented an improved adaptation of the

72
A.D. Masegosa et al.
BA for addressing both symmetric and asymmetric TSP. The results shown that the
improved version of BA could obtain promising results, in comparison with some
reference techniques, such as an evolutionary simulated annealing, a genetic algo-
rithm, a distributed genetic algorithm or an imperialist competitive algorithm.
We want to highlight that the meta-heuristics referenced in this section form a
small part of all diﬀerent approaches that can be found in current literature. We are
aware that many other interesting and eﬃcient techniques are available in the scien-
tiﬁc community, such as the Harmony Search [27], or Gravitational Search [61, 62],
which also show a good performance when they are applied to routing problems.
Additionally, many additional classic methods have also shown a great performance
for this kind of problems, such as the Particle Swarm Optimization [89], the Ant
Colony Optimization [63] or Large Neighborhood Search [60].
4.2
Fuzzy Logic in Vehicle Route Planning
The use of fuzzy systems is also an important topic in the ﬁeld of vehicle routing
problems. In real situations, these problems are susceptible to suﬀer imprecision
or uncertainty in their data. Many works in literature show that one of the most
successful ways to tackle with this uncertainty and imprecision in the information
available when solving vehicle route planning problems, it is the use of fuzzy logic.
In this way, we can ﬁnd interesting studies in the literature, such as the one pre-
sented in [29], in which Ghannadpour et al. proposed a multi-objective dynamic
vehicle routing problem with fuzzy time windows. In this research, authors not only
describe the problem, but also the main real-world applications that it could have.
The constraints related with travel times and user satisfaction are some of the most
subject to uncertainty and imprecision. Apart from the previous work, this type of
imprecision is modelled in other studies such as the one presented by Tang et al.
in [71], in which a VRP with fuzzy time windows is proposed and solved using a
two-stage algorithm which decomposes the problem into two subproblems. An addi-
tional example of this trend is the work proposed in [28], in which a multi-objective
dynamic VRP with fuzzy travel times and customers’ satisfaction level is presented.
Speciﬁcally, the customers’ satisfaction level is considered in the route planning of
vehicles by using the concept of fuzzy time windows. Additionally, the dynamic
solving strategy proposed is based on a genetic algorithm, and its performance is
evaluated on various test problems generalized from a set of static instances in the
literature. Other interesting application of fuzzy logic for vehicle routing problems
are the works presented in [12], where Brito et al. proposed a variant of the close-
open VRP with fuzzy time windows and fuzzy vehicle capacity, and [72], where
Torres et al. solved a variant of the Truck and Trailer Routing Problem where the
imprecision in the capacity of the truck and the trailers is modeled by fuzzy logic.

Applications of Soft Computing in Intelligent Transportation Systems
73
5
Soft Computing in Vehicular Ad-Hoc Networks
Vehicular Ad-Hoc Networks (VANETs) are communication networks in which the
nodes are vehicles [32]. This ﬁeld has attracted the attention of the scientiﬁc com-
munity, automobile industry and institutions worldwide because of the huge number
of innovative applications they can enable [57]. Among the areas of application,
some of the most relevant are: security (warnings about emergency break, collision
at intersection, line shift, etc.); leisure and entertainment (multimedia content down-
load, nearby points of interest, etc.); traﬃc management (virtual traﬃc lights, limited
access zones, electronic tolls, etc.); and driver assistance (remote diagnosis; eﬃ-
cient and eco-driving; etc.). The communications that take place within VANETs
can be classiﬁed in Infrastructure-to-Infrastructure (I2I), Infrastructure-to-Vehicle
(I2V), Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) depending on
which agent is the transmitter and the receptor.
VANETs are an especially complex environment because of the high dynamism
in the movement of the vehicles, communication failures, diﬀerent driver proﬁles,
high variability of nodes and interconnections, etc. For this reason, data handled
in VANETs is subject to imprecision, uncertainty and vagueness which make it
an excellent ﬁeld for the application of Soft Computing techniques. This section is
devoted to review some of these applications. Concretely, the next subsections will
show, in this order, applications in VANETs of three components of Soft Computing
as metaheuristics, fuzzy sets and neural networks.
5.1
Metaheuristics in Vehicular Ad-Hoc Networks
In the context of VANETs, several complex optimization problems appear, in which
metaheuristics have shown to be an excellent tool to solve them. One of the ﬁrst
works in which metaheuristics were applied to VANETs can be found in [26]. Here,
Garcia-Nieto and co-authors proposed the used of these techniques to optimize the
File Transfer Protocol Conﬁguration. Concretely, they optimized the parameters of
the Vehicular Data Transfer Protocol, which operate over the transport layer protocol
of VANETs, in order to allow end-to-end communications. To address this problem,
they test ﬁve diﬀerent metaheuristics over two scenarios that simulated urban and
highway environments. The authors concluded that the metaheuristics reduced the
transmission time in a 19 and 25.43% in urban and highway scenarios, respectively,
when they were compared to the conﬁguration provided by a human expert. A similar
approach was followed by the same authors in [73]. In this case, the ﬁve metaheuris-
tics mentioned before were employed to ﬁnd the optimal conﬁguration of the Open
Link State Routing protocol for VANETs. The results showed again a signiﬁcant
improvement in terms of Packet Delivery Ratio (PDR), network routing load and
End-to-End Delay (E2ED) in comparison to standard and expert conﬁgurations.

74
A.D. Masegosa et al.
Another optimization problem from VANETs that has been addressed with meta-
heuristics is multi-cast routing. Souza et al. presented in [69] a tree based multi-
cast routing protocol called MAV-AODV. Here, the Ant Colony Optimization’s
pheromone mechanism is used to establish a quality measure of the stability of the
routes. The new method was tested over a simulated Manhattan scenario and com-
pared with the MAODV protocol. MAV-AODV obtained a better performance than
MAODV in terms of E2ED, overhead and PDR. Other example of routing protocol
inspired in metaheuristics was presented in [10]. This unicast and multipath pro-
tocol, called HyBR, used two types of routing procedures: a topology-based and
a geographic-based routing procedure for high and low density scenarios, respec-
tively. The ﬁrst one was inspired in the working of bee swarm optimization whereas
the second one used a genetic algorithm to optimize the route between the origin
and the destiny. The experimentation was done over high density and a low density
scenarios and the performance measures considered were the average E2ED, PDR
and normalized overhead load. HyBR outperformed AODV and geography-based
routing protocol (GPSR) in the ﬁrst two measures but not in the last one.
A more recent application of metaheuristics in VANETs is given by Masegosa et
al. in [48]. This work is focused on information dissemination from a central server to
vehicles by means of Virtual Infrastructures (VIs). The selection of the nodes of the
VI is modelled as a covering location problem and it is solved by means of a genetic
algorithm. The main challenge for metaheuristics in this environment was the short
response time imposed by the latency requirements of some VANET’s applications.
The experimentation over a real scenario with 45 vehicles indicated that the proposal
outperforms another state-of-art method based on a deterministic greedy strategy,
called NAVI.
5.2
Fuzzy Logic in Vehicular Ad-Hoc Networks
Fuzzy set theory has been also applied in diﬀerent areas of VANETs. For example,
in [30], Abdel Hafeez et al. presented a Cluster Head (CH) selection mechanism that
made use of a fuzzy inference system. CH selection is one of the main challenges of
cluster-based medium access control protocols, whose aim is to improve the access
and capacity of the network among other aspects. The previous mechanism elected
the CHs dynamically and taking into account a stability criteria. The fuzzy inference
system was used to predict the future position and speed of all cluster members using
as input the inter-vehicle distance and speed. The procedure proposed outperforms
CMCP [31] and APROVE [67] protocols.
Another fuzzy inference system was presented in [78] to design a multi-hop
broadcast protocol, named FUZZYBR. In a more speciﬁc way, the fuzzy inference
system was employed to select the relay nodes considering variables with a high
degree of imprecision and uncertainty as the inter-vehicle distance, mobility and
signal strength. The evaluation of the methods was done over simulated freeway and
street scenarios, and the proposal was compared with other broadcast protocols as

Applications of Soft Computing in Intelligent Transportation Systems
75
Flooding, Weighted persistence, MPR and Enhance MPR. The results conﬁrmed a
signiﬁcant performance advantage of FUZZYBR over the mentioned protocols in
terms of PDR, E2ED and number of messages per data package. In [25], Galaviz-
Mosqueda et al. utilized FUZZYBR and another multi-hop broadcast protocol based
on fuzzy inference systems, and called RLMB, to test the use of genetic fuzzy sys-
tems. The motivation of the authors was to adjust the membership functions of the
fuzzy rules of the two protocols by means of a genetic algorithm, in order to obtain
a better performance than the one with expert tuning. The results conﬁrmed their
hypothesis, and the two versions of FUZZYBR and RLMB automatically tuned with
the genetic algorithm signiﬁcantly improved the performance of the counterparts
heuristically conﬁgured by humans.
Fuzzy control was also applied in VANETs to adapt beaconing rate to the changes
in traﬃc density that usually occur along time. This mechanism, called ABR, was
proposed in [96] where the authors developed a method in which a rule-based system
adapted the frequency of beacon broadcasting taking into account the percentage of
vehicles traveling in the same direction and the emergency status of vehicles. The
simulations showed how this method reduced the beaconing load at the expense of
cooperative awareness between vehicles.
5.3
Neural Networks in VANETs
One of the ﬁrst works that suggested the application of NNs to VANETs can be found
in [40]. In this paper, the authors aimed at demonstrating the beneﬁts of VANETs
on traﬃc safety, and concretely for designing an Accident Prevention Application
(APA). To this end, they proposed, in a ﬁrst stage, the use of a Markov Reward
Process to estimate the expected time until an accident will happen, taking into
account the traﬃc states observed so far; and in a second stage, the use of NNs to
make these estimations when there are unobserved traﬃcs situation. To this end, the
NNs should have been trained with known pairs (state, expected time). The authors
claimed that, in this way, they provided the basis for the analysis of VANETs and
their impact on traﬃc safety.
In a more recent paper [83], Yang et al. combined ANN and VANETs to develop
a short-term average-speed forecast and adjustment approach to improve gas con-
sumption, decrease CO2 emissions and reduce travel time. In the proposed method, a
Traﬃc Information Center (TIC) collected average speed from vehicles and road side
sensors through VANETs. Then, the TIC trained a NN with average speed, weather
information and traﬃc ﬂow to predict the average speed. The predicted speed was
then sent to the CH that adjusted the prediction according to the observed speed. The
simulations done showed an important improvement in the accuracy of the average-
speed predictions when the system was compared versus a hybrid approach.
Another important issue in VANETs handled with NNs is security and vulner-
ability, given that VANETs are even more exposed than other similar networks. A
good example of this application can be found in [2], where the authors employed

76
A.D. Masegosa et al.
NNs to build an Intrusion Detection System (IDS) to prevent Denial of Services
(DoS) attacks. Concretely, the aim of the NN was the real-time detection of mali-
cious vehicles in order to isolate them from the network. With this purpose in mind,
the authors generated data for normal and malicious vehicles through simulations.
From this data, they extracted relevant features and a pre-processed dataset that it
was used to train the NN. The experimentation showed that the system obtained an
error rate of 2.05%, conﬁrming its eﬀectiveness. Other example of the application
of NNs for security in VANETs has been recently presented in [38]. The authors of
this work developed a Deep NN (DNN) for an IDS to secure in-vehicular networks
that use the CAN protocol. Concretely, the proposed IDS considered a scenario in
which malicious data-package are injected into the in-vehicle CAN bus. The DNN
was trained with labeled (i.e. normal or attack) and preprocessed CAN packets to
extract features that model the statistical behavior of the network. In the detection
phase, each CAN packet was pre-processed and passed to the trained DNN to make
the binary decision. The experimentation demonstrated that the approach obtained
a 98% detection ratio in real-time response to attacks.
6
Discussions
In this book chapter we have presented an overview of application of SC to four
important areas of ITS: autonomous driving, traﬃc state prediction, vehicle route
planning and VANETs. Our overview has shown that SC techniques are an eﬀective
and eﬃcient framework to deal with many of the problems that arise in those areas
and therefore, to develop better performing ITS.
The main reasons behind the success of SC in the four ITS areas aforementioned
is associated with the recent trend in ITS to follow a data-driven approach; and the
inherent tolerance of SC techniques to deal with the imprecision, uncertainty and
vagueness, omnipresent in the information handled in this complex environments,
and their ability to provide cost-eﬀective solutions.
To ﬁnish, we would like to point out that the emergence of new ITS technologies
such as autonomous cars, electric vehicles, more advance VANETs or Unmanned
Aerial Vehicles will probably boost a shift paradigm, along the next decade, in the
way in which goods and persons are transported nowadays. SC plays and will play a
major role in this shift so we augur a great future for the application and development
of SC techniques in this ﬁeld.
Acknowledgements This work has been supported by the research projects TEC2013-45585-C2-
2-R and TIN2014-56042-JIN from the Spanish Ministry of Economy and Competitiveness, and
TIMON project which received funding from the European Union’s Horizon 2020 research and
innovation programme under grant agreement No 636220.

Applications of Soft Computing in Intelligent Transportation Systems
77
References
1. Road safety in the European union trends, statistics and main challenges. Technical report,
European Commission: General Mobility and Transport, Unit C.4 Road Safety (2015)
2. Alheeti, K.M.A., Gruebler, A., McDonald-Maier, K.D.: An intrusion detection system against
malicious attacks on the communication network of driverless cars. In: 2015 12th Annual IEEE
Consumer Communications and Networking Conference (CCNC), pp. 916–921 (2015)
3. Alinaghian, M., Naderipour, M.: A novel comprehensive macroscopic model for time-
dependent vehicle routing problem with multi-alternative graph to reduce fuel consumption: a
case study. Comput. Ind. Eng. 99, 210–222 (2016)
4. Allström, A., Ekström, J., Gundlegård, D., Ringdahl, R., Rydergren, C., Bayen, A., Patire, A.:
Hybrid approach for short-term traﬃc state and travel time prediction on highways. Trans. Res.
Rec. 2554, 60–68 (2016)
5. Ario, T., Sugimachi, T., Fukao, T., Kawashima, H.: Evaluation of fuzzy inference-based self-
tuning of steering control gains for heavy-duty trucks. Int. J. Intell. Trans. Syst. Res. 14(2),
92–100 (2016)
6. Baños, R., Ortega, J., Gil, C., Fernández, A., de Toro, F.: A simulated annealing-based parallel
multi-objective approach to vehicle routing problems with time windows. Expert Syst. Appl.
40(5), 1696–1707 (2013)
7. Barman, B., Kanjilal, R., Mukhopadhyay, A.: Neuro-fuzzy controller design to navigate
unmanned vehicle with construction of traﬃc rules to avoid obstacles. Int. J. Uncertain. Fuzzi-
ness Knowl. Based Syst. 24(3), 433–449 (2016)
8. Belk, R.: You are what you can access: sharing and collaborative consumption online. J. Bus.
Res. 67(8), 1595–1600 (2014)
9. Bentez, A., Casillas, J.: Multi-objective genetic learning of serial hierarchical fuzzy systems
for large-scale problems. Soft Comput. 17(1), 165–194 (2013)
10. Bitam, S., Mellouk, A., Zeadally, S.: HyBR: A hybrid bio-inspired bee swarm routing protocol
for safety applications in vehicular ad hoc networks (VANETs). J. Syst. Archit. 59(10), 953–
967 (2013)
11. Bonnefon, J.F., Shariﬀ, A., Rahwan, I.: Autonomous vehicles need experimental ethics: are we
ready for utilitarian cars? arXiv preprint. arXiv:1510.03346 (2015)
12. Brito, J., Martínez, F.J., Moreno, J.A., Verdegay, J.L.: An ACO hybrid metaheuristic for close-
open vehicle routing problems with time windows and fuzzy constraints. Appl. Soft Comput.
32, 154–163 (2015)
13. Brynjolfsson, E., McAfee, A.: Race Against the Machine. Digital Frontier, Lexington, MA
(2011)
14. Carrabs, F., Cordeau, J.F., Laporte, G.: Variable neighborhood search for the pickup and deliv-
ery traveling salesman problem with lifo loading. Inf. J. Comput. 19(4), 618–632 (2007)
15. Chen, C., Seﬀ, A., Kornhauser, A., Xiao, J.: Deepdriving: learning aﬀordance for direct percep-
tion in autonomous driving. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 2722–2730 (2015)
16. Chiang, W.C., Russell, R.A.: Simulated annealing metaheuristics for the vehicle routing prob-
lem with time windows. Ann. Oper. Res. 63(1), 3–27 (1996)
17. Chouraqui, S., Selma, B.: Unmanned vehicle trajectory tracking by neural networks. Int. Arab
J. Inf. Technol. 13(3), 272–275 (2016)
18. Dimitrakopoulos, G., Demestichas, P.: Intelligent transportation systems. IEEE Veh. Technol.
Mag. 5(1), 77–84 (2010)
19. Du, X., Htet, K., Tan, K.: Development of a genetic-algorithm-based nonlinear model predic-
tive control scheme on velocity and steering of autonomous vehicles. IEEE Trans. Ind. Elec-
tron. 63(11), 6970–6977 (2016)
20. ERTRAC: Multi-annual implementation plan for Horizon 2020. Tech. rep. (2013). http://www.
ertrac.org/uploads/documentsearch/id20/ertrac-map-h2020_67.pdf
21. Escobar, J.W., Linfati, R., Toth, P., Baldoquin, M.G.: A hybrid granular tabu search algorithm
for the multi-depot vehicle routing problem. J. Heuristics 20(5), 483–509 (2014)

78
A.D. Masegosa et al.
22. Faddel, S., Mohamed, A., Mohammed, O.: Fuzzy logic-based autonomous controller for elec-
tric vehicles charging under diﬀerent conditions in residential distribution systems. Electr.
Power Syst. Res. 148, 48–58 (2017)
23. Figueiredo, L., Jesus, I., Machado, J., Ferreira, J., Martins de Carvalho, J.: Towards the devel-
opment of intelligent transportation systems. In: Proceedings of the 2001 IEEE Intelligent
Transportation Systems, pp. 1206–1211 (2001)
24. Fusco, G., Colombaroni, C., Isaenko, N.: Comparative analysis of implicit models for real-time
short-term traﬃc predictions. IET Intell. Trans. Syst. 10(4), 270–278 (2016)
25. Galaviz-Mosqueda, A., Villarreal-Reyes, S., Galeana-Zapien, H., Rubio-Loyola, J., Rivera-
Rodriguez, R.: Genetic tuning of fuzzy rule-based systems for multi-hop broadcast protocols
for VANETs. Telecommun. Syst. 63(3), 399–420 (2016)
26. García-Nieto, J., Toutouh, J., Alba, E.: Automatic tuning of communication protocols for vehic-
ular ad hoc networks using metaheuristics. Eng. Appl. Artif. Intell. 23(5), 795–805 (2010)
27. Geem, Z.W., Kim, J.H., Loganathan, G.: A new heuristic optimization algorithm: harmony
search. Simulation 76(2), 60–68 (2001)
28. Ghannadpour, S.F., Noori, S., Tavakkoli-Moghaddam, R.: Multiobjective dynamic vehicle
routing problem with fuzzy travel times and customers satisfaction in supply chain manage-
ment. IEEE Trans. Eng. Manag. 60(4), 777–790 (2013)
29. Ghannadpour, S.F., Noori, S., Tavakkoli-Moghaddam, R., Ghoseiri, K.: A multi-objective
dynamic vehicle routing problem with fuzzy time windows: model, solution and application.
Appl. Soft Comput. 14, 504–527 (2014)
30. Hafeez, K.A., Zhao, L., Liao, Z., Ma, B.N.W.: A fuzzy-logic-based cluster head selection algo-
rithm in VANETs. In: 2012 IEEE International Conference on Communications (ICC), pp.
203–207 (2012)
31. Hang, Su, Zhang, Xi: Clustering-based multichannel MAC protocols for QoS provisionings
over vehicular ad hoc networks. IEEE Trans. Veh. Technol. 56(6), 3309–3323 (2007)
32. Hartenstein, H., Laberteaux, K.P.: A tutorial survey on vehicular ad hoc networks. IEEE Com-
mun. Mag. 46(6), 164–171 (2008)
33. Hatcher, G., Burnier, C., Greer, E., Hardesty, D., Hicks, D., Jacobi, A., Lowrance, C., Mercer,
M.: Intelligent transportation systems beneﬁts, costs, and lessons learned: 2014 update report.
In: Technical report, U.S. Department of Transportation, ITS Joint Program Oﬃce (2014).
https://trid.trb.org/view.aspx?id=1334694
34. Huang, W., Wu, Q., Ma, Y.L., Chu, X.M.: Design of speed controller of small intelligent vehicle
based on visual navigation. J. Wuhan Univ. Technol. 32(6), 103–106 (2010)
35. Jati, G.K., Suyanto: Evolutionary discrete ﬁreﬂy algorithm for travelling salesman problem.
In: Proceedings of the Adaptive and Intelligent Systems: Second International Conference, pp.
393–403 (2011)
36. Jia, B., Feng, W., Zhu, M.: Obstacle detection in single images with deep neural networks.
Signal, Image Video Process. 10(6), 1033–1040 (2016)
37. Kala, R., Warwick, K.: Reactive planning of autonomous vehicles for traﬃc scenarios. Elec-
tronics 4(4), 739–762 (2015)
38. Kang, M.J., Kang, J.W., Wang, X., Larochelle, H., Vincent, P., Bengio, S.: Intrusion detection
system using deep neural network for in-vehicle network security. PLOS ONE 11(6), e0155,
781 (2016)
39. Karakatič, S., Podgorelec, V.: A survey of genetic algorithms for solving multi depot vehicle
routing problem. Appl. Soft Comput. 27, 519–532 (2015)
40. Killat, M., Hartenstein, H.: Vehicular ad hoc networks: how to show the impact on traﬃc
safety? In: 2007 IEEE 65th Vehicular Technology Conference—VTC2007, Spring, pp. 659–
663 (2007)
41. Koesdwiady, A., Soua, R., Karray, F.: Improving traﬃc ﬂow prediction with weather informa-
tion in connected cars: a deep learning approach. IEEE Trans. Veh. Technol. 65(12), 9508–
9517 (2016)
42. Kumar, K., Parida, M., Katiyar, V.: Short term traﬃc ﬂow prediction in heterogeneous condi-
tion using artiﬁcial neural network. Transport 30(4), 397–405 (2015)

Applications of Soft Computing in Intelligent Transportation Systems
79
43. Laporte, G.: The traveling salesman problem: an overview of exact and approximate algo-
rithms. Eur. J. Oper. Res. 59(2), 231–247 (1992)
44. Laporte, G.: The vehicle routing problem: an overview of exact and approximate algorithms.
Eur. J. Oper. Res. 59(3), 345–358 (1992)
45. Lopez-Garcia, P., Onieva, E., Osaba, E., Masegosa, A.D., Perallos, A.: A hybrid method for
short-term traﬃc congestion forecasting using genetic algorithms and cross entropy. IEEE
Trans. Intell. Trans. Syst. 17(2), 557–569 (2016)
46. Lopez-Garcia, P., Osaba, E., Onieva, E., Masegosa, A.D., Perallos, A.: Short-term traﬃc con-
gestion forecasting using hybrid metaheuristics and rule-based methods: a comparative study.
In: Luaces, O., Gámez, J.A., Barrenechea, E., Troncoso, A., Galar, M., Quintián, H., Corchado,
E. (eds.) Advances in Artiﬁcial Intelligence: 17th Conference of the Spanish Association for
Artiﬁcial Intelligence, CAEPIA 2016, pp. 290–299. Springer International Publishing (2016)
47. Malek, M., Guruswamy, M., Pandya, M., Owens, H.: Serial and parallel simulated annealing
and tabu search algorithms for the traveling salesman problem. Ann. Oper. Res. 21(1), 59–84
(1989)
48. Masegosa, A.D., de la Iglesia, I., Hernandez-Jayo, U., Diez, L.E., Bahillo, A., Onieva, E.: Solu-
tions based on soft computing for the sustainability and climate change. In: A New Approach
for Information Dissemination in VANETs Based on Covering Location and Metaheuristics,
pp. 1–24. Springer International Publishing (2017). In press
49. OECD: Improving Reliability on Surface Transport Networks. OECD Publishing (2010). URL
http://www.oecd-ilibrary.org/transport/improving-reliability-on-surface-transport-networks_
9789282102428-en
50. Oh, S.D., Kim, Y.J., Hong, J.S.: Urban traﬃc ﬂow prediction system using a multifactor pattern
recognition model. IEEE Trans. Intell. Trans. Syst. 16(5), 2744–2755 (2015)
51. Onieva, E., Godoy, J., Villagrá, J., Milanés, V., Pérez, J.: On-line learning of a fuzzy controller
for a precise vehicle cruise control system. Expert Syst. Appl. 40(4), 1046–1053 (2013)
52. Onieva, E., Hernandez-Jayo, U., Osaba, E., Perallos, A., Zhang, X.: A multi-objective evo-
lutionary algorithm for the tuning of fuzzy rule bases for uncoordinated intersections in
autonomous driving. Inf. Sci. 321, 14–30 (2015)
53. Onieva, E., Lopez-Garcia, P., Masegosa, A.D., Osaba, E., Perallos, A.: A comparative study
on the performance of evolutionary fuzzy and crisp rule based classiﬁcation methods in con-
gestion prediction. Trans. Res. Proced. 14, 4458–4467 (2016)
54. Osaba, E., Carballedo, R., Yang, X.S., Diaz, F.: An Evolutionary Discrete Fireﬂy Algorithm
with Novel Operators for Solving the Vehicle Routing Problem with Time Windows, pp. 21–
41. Springer International Publishing, Cham (2016)
55. Osaba, E., Yang, X.S., Diaz, F., Lopez-Garcia, P., Carballedo, R.: An improved discrete bat
algorithm for symmetric and asymmetric traveling salesman problems. Eng. Appl. Artif. Intell.
48, 59–71 (2016)
56. Osaba, E., Yang, X.S., Diaz, F., Onieva, E., Masegosa, A.D., Perallos, A.: A discrete ﬁreﬂy
algorithm to solve a rich vehicle routing problem modelling a newspaper distribution system
with recycling policy. Soft Comput. 1–14 (2016)
57. Papadimitratos, P., La Fortelle, A., Evenssen, K., Brignolo, R., Cosenza, S.: Vehicular commu-
nication systems: enabling technologies, applications, and future outlook on intelligent trans-
portation. IEEE Commun. Mag. 47(11), 84–95 (2009)
58. Pérez, J., Gajate, A., Milanés, V., Onieva, E., Santos, M.: Design and implementation of a
neuro-fuzzy system for longitudinal control of autonomous vehicles. In: International Confer-
ence on Fuzzy Systems, pp. 1–6 (2010)
59. Pérez, J., Milanés, V., Godoy, J., Villagrá, J., Onieva, E.: Cooperative controllers for highways
based on human experience. Expert Syst. Appl. 40(4), 1024–1033 (2013)
60. Pisinger, D., Ropke, S.: Large neighborhood search In: Handbook of Metaheuristics, pp. 399–
419 (2010)
61. Precup, R.E., David, R.C., Petriu, E.M., Preitl, S., Rădac, M.B.: Fuzzy logic-based adaptive
gravitational search algorithm for optimal tuning of fuzzy-controlled servo systems. IET Con-
trol Theory Appl. 7(1), 99–107 (2013)

80
A.D. Masegosa et al.
62. Precup, R.E., David, R.C., Petriu, E.M., Radac, M.B., Preitl, S.: Adaptive GSA-based opti-
mal tuning of PI controlled servo systems with reduced process parametric sensitivity, robust
stability and controller robustness. IEEE Trans. Cybern. 44(11), 1997–2009 (2014)
63. Reed, M., Yiannakou, A., Evering, R.: An ant colony algorithm for the multi-compartment
vehicle routing problem. Appl. Soft Comput. 15, 169–176 (2014)
64. Rodriguez-Castao, A., Heredia, G., Ollero, A.: High-speed autonomous navigation system for
heavy vehicles. Appl. Soft Comput. J. 43, 572–582 (2016)
65. Sarasola, B., Doerner, K.F., Schmid, V., Alba, E.: Variable neighborhood search for the sto-
chastic and dynamic vehicle routing problem. Ann. Oper. Res. 236(2), 425–461 (2016)
66. Schmidhuber, J.: Deep learning in neural networks: an overview. Neural Netw. 61, 85–117
(2015)
67. Shea, C., Hassanabadi, B., Valaee, S.: Mobility-based clustering in VANETs using aﬃnity
propagation. In: GLOBECOM 2009—2009 IEEE Global Telecommunications Conference,
pp. 1–6 (2009)
68. Soua, R., Koesdwiady, A., Karray, F.: Big-data-generated traﬃc ﬂow prediction using deep
learning and dempster-shafer theory. In: 2016 International Joint Conference on Neural Net-
works (IJCNN), pp. 3195–3202 (2016)
69. Souza, A.B., Celestino, J., Xavier, F.A., Oliveira, F.D., Patel, A., Latiﬁ, M.: Stable multicast
trees based on ant colony optimization for vehicular ad hoc networks. In: The International
Conference on Information Networking 2013 (ICOIN), pp. 101–106 (2013)
70. Taha, A., Hachimi, M., Moudden, A.: Adapted bat algorithm for capacitated vehicle routing
problem. Int. Rev. Comput. Softw. 10(6), 610–619 (2015)
71. Tang, J., Pan, Z., Fung, R.Y., Lau, H.: Vehicle routing problem with fuzzy time windows. Fuzzy
Sets and Syst. 160(5), 683–695 (2009)
72. Torres, I., Cruz, C., Verdegay, J.L.: Solving the truck and trailer routing problem with fuzzy
constraints. Int. J. Comput. Intell. Syst. 8(4), 713–724 (2015)
73. Toutouh, J., Garcia-Nieto, J., Alba, E.: Intelligent OLSR routing protocol optimization for
VANETs. IEEE Trans. Veh. Technol. 61(4), 1884–1894 (2012)
74. Van Laarhoven, P.J.M., Aarts, E.H.L.: Simulated annealing. In: Simulated Annealing: Theory
and Applications, pp. 7–15 (1987)
75. Verdegay, J.L., Yager, R.R., Bonissone, P.P.: On heuristics as a fundamental constituent of soft
computing. Fuzzy Sets and Syst. 159(7), 846–855 (2008)
76. Verma, M., Manoj, M., Verma, A.: Analysis of aspiration for owning a car among youths in a
city of a developing country, India. Trans. Dev. Econ. 3(1), 7 (2017)
77. Vidal, T., Crainic, T.G., Gendreau, M., Prins, C.: A hybrid genetic algorithm with adaptive
diversity management for a large class of vehicle routing problems with time-windows. Com-
put. Oper. Res. 40(1), 475–489 (2013)
78. Wu, C., Ohzahata, S., Kato, T.: VANET broadcast protocol based on fuzzy logic and light-
weight retransmission mechanism. IEICE Trans. Commun. E95-B(2), 415–425 (2012)
79. Xidias, E., Zacharia, P., Nearchou, A.: Path planning and scheduling for a ﬂeet of autonomous
vehicles. Robotica 34(10), 2257–2273 (2016)
80. Xu, D.W., Wang, Y.D., Jia, L.M., Qin, Y., Dong, H.H.: Real-time road traﬃc state prediction
based on arima and kalman ﬁlter. Front. Inf. Technol. Electron. Eng. 18(2), 287–302 (2017)
81. Xu, Y., Kong, Q.J., Klette, R., Liu, Y.: Accurate and interpretable bayesian mars for traﬃc ﬂow
prediction. IEEE Trans. Intell. Trans. Syst. 15(6), 2457–2469 (2014)
82. Yan, S., Chen, C.Y., Chang, S.C.: A car pooling model and solution method with stochastic
vehicle travel times. IEEE Trans. Intell. Trans. Syst. 15(1), 47–61 (2014)
83. Yang, J.Y., Chou, L.D., Tung, C.F., Huang, S.M., Wang, T.W.: Average-speed forecast and
adjustment via VANETs. IEEE Trans. Veh. Technol. 62(9), 4318–4327 (2013)
84. Yang, X.S.: Nature-inspired metaheuristic algorithms. Luniver press (2008)
85. Yang, X.S.: A new metaheuristic bat-inspired algorithm. In: Nature Inspired Cooperative
Strategies for Optimization (NICSO 2010), pp. 65–74 (2010)
86. Yang, X.S., He, X.: Bat algorithm: literature review and applications. Int. J. Bio-Inspir. Com-
put. 5(3), 141–149 (2013)

Applications of Soft Computing in Intelligent Transportation Systems
81
87. Yang, X.S., He, X.: Fireﬂy algorithm: recent advances and applications. Int. J. Swarm Intell.
1(1), 36–50 (2013)
88. Yang, Y., Luo, H., Xu, H., Wu, F.: Towards real-time traﬃc sign detection and classiﬁcation.
IEEE Trans. Intell. Trans. Syst. 17(7), 2022–2031 (2016)
89. Yao, B., Yu, B., Hu, P., Gao, J., Zhang, M.: An improved particle swarm optimization for
carton heterogeneous vehicle routing problem with a collection depot. Ann. Oper. Res. 242(2),
303–320 (2016)
90. Yu, X., Xiong, S., He, Y., Wong, W., Zhao, Y.: Research on campus traﬃc congestion detection
using BP neural network and Markov model. J. Inf. Secur. Appl. 31, 54–60 (2016)
91. Zhang, D., Li, S., Yang, Q., Liu, L.: Optimization based trajectory planning of parallel parking
with multiple constraints. SAE Int. J. Passeng. Cars Electron. Electr. Syst. 8(2), 413–418 (2015)
92. Zhang, J., Wang, F.Y., Wang, K., Lin, W.H., Xu, X., Chen, C.: Data-driven intelligent trans-
portation systems: a survey. IEEE Trans. Intell. Trans. Syst. 12(4), 1624–1639 (2011)
93. Zhang, X., Onieva, E., Lee, V., Liu, K.: Congestion prediction by means of fuzzy logic and
genetic algorithms. In: Intelligent Transport Systems: Technologies and Applications, pp. 189–
205 (2015)
94. Zhang, X., Onieva, E., Perallos, A., Osaba, E., Lee, V.: Hierarchical fuzzy rule-based system
optimized with genetic algorithms for short term traﬃc congestion prediction. Trans. Res. Part
C: Emerg. Technol. 43, 127–142 (2014)
95. Zhou, Y., Luo, Q., Xie, J., Zheng, H.: A hybrid bat algorithm with path relinking for the capac-
itated vehicle routing problem. In: Yang, X.S., Bekdaş, G., Nigdeli, S.M. (eds.) Metaheuristics
and Optimization in Civil Engineering, pp. 255–276. Springer International Publishing, Cham
(2016)
96. Zrar Ghafoor, K., AbuBakar, K., van Eenennaam, M., Khokhar, R.H., Gonzalez, A.J.: A fuzzy
logic approach to beaconing for vehicular ad hoc networks. Telecommun. Syst. 52(1), 139–149
(2013)

Fuzzy Cognitive Maps Based Models
for Pattern Classiﬁcation: Advances
and Challenges
Gonzalo Nápoles, Maikel Leon Espinosa, Isel Grau,
Koen Vanhoof and Rafael Bello
Abstract Fuzzy Cognitive Maps (FCMs) have proven to be a suitable methodology
for the design of knowledge-based systems. By combining both uncertainty depic-
tion and cognitive mapping, this technique represents the knowledge of systems that
are characterized by ambiguity and complexity. In short, FCMs can be deﬁned as
recurrent neural networks that include elements of fuzzy logic during the knowledge
engineering phase. While the literature contains many studies claiming how this Soft
Computing technique is able to model complex and dynamical systems, we explore
another promising research ﬁeld: the use of FCMs in solving pattern classiﬁcation
problems. This is motivated by the transparency of the decision model attached to
these cognitive, neural networks. In this chapter, we revise some prominent advances
in the area of FCM-based classiﬁers and open challenges to be confronted.
1
Introduction
In the last years, Fuzzy Cognitive Maps (FCMs) [12] have notably increased their
popularity within the scientiﬁc community. They constitute a suitable tool for the
designing of knowledge-based systems, where one of the most relevant character-
istics is the interpretability of the network topology. Not many computer science
techniques can claim this valuable feature.
From the structural perspective, an FCM can be deﬁned as a fuzzy digraph that
describes the underlying behavior of an intelligent system in terms of concepts
G. Nápoles (✉) ⋅K. Vanhoof
Hasselt Universiteit, Agoralaan gebouw D, Diepenbeek, Belgium
e-mail: gonzalo.napoles@uhasselt.be
M. Leon Espinosa
University of Miami, 5250 University Dr, Miami, FL, USA
e-mail: mleon@bus.miami.edu
I. Grau ⋅R. Bello
Central University of Las Villas, Carretera Camajuaní km 5.5, Santa Clara, Cuba
e-mail: rbellop@uclv.edu.cu
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_5
83

84
G. Nápoles et al.
(i.e., objects, states, variables or entities). Such concepts comprise a precise mean-
ing for the problem domain under analysis and they are connected by signed and
weighted edges that denote causal relationships.
The sign and intensity of causal relations involve the quantiﬁcation of a fuzzy
linguistic variable that can be assigned by experts during a knowledge acquisition
phase [13]. These elements recurrently interact when updating the activation value
of each concept (or simply neuron). In point of fact, an FCM exploits an activation
(state) vector by using a rule similar to the standard McCulloch-Pitts scheme [15].
Therefore, the activation value of each neuron is given by the value of the trans-
formed weighted sum that this processing unit receives from connected neurons on
the causal network. This activation value actually comprises an interpretable feature
for the physical system under investigation. More explicitly, the higher the activation
value of a neuron, the stronger its inﬂuence (positive or negative) over the connected
neural entities. Of course, this inﬂuence also depends on the intensity of the causal
relations connecting the actual neuron with the other neural processing entities.
FCM-based models can be understood as a kind of recurrent neural networks
that support backward connections that sometimes form cycles in the causal graph.
These backward relations (called feedback) enable the network to handle memory
to compute the outputs of the current state and maintain a sort of recurrence to the
past processing [6]. During the inference phase, the updating rule is repeated until
the system converges to a ﬁxed-point attractor or a maximal number of iterations is
reached. The former implies that a hidden pattern was discovered [12] while the latter
suggests that the outputs are cyclic or completely chaotic. Whichever the observed
behavior, the recurrent network will produce a response (i.e., state vector) at each
discrete-time step, which comprises the activation degree of all neurons of the model.
Although FCMs inherited many aspects from other neural models (i.e., the
reasoning rule), there are some important diﬀerences regarding to other types of
Artiﬁcial Neural Network (ANNs). Classical ANN models regularly perform like
black-boxes, where both the neurons and the connections do not have a clear mean-
ing for the problem itself, or results cannot easily be explained by the same predicting
model. However, all neurons in an FCM have a precise meaning for the physical sys-
tem being modeled and correspond to speciﬁc variables that form part of the solution.
It should be highlighted that an FCM does not comprise hidden neurons since these
entities could not be interpreted nor help at explaining why a solution is suitable for a
given problem. If this were the case, the model becomes unfriendly for many further
phases.
In the last years, FCMs have been widely studied due to its advantageous char-
acteristics for handling complex systems. Less attention has been given to the
development of FCM-based classiﬁers. Pattern classiﬁcation [4] is one of the most
ubiquitous real-world problems and certainly one at which humans really excel. It
consists of identifying the right category (among those in a predeﬁned set) to which
an observed pattern belongs. These patterns are often described by a set of predic-
tive attributes of numerical and/or nominal nature called features. Some successful
classiﬁers include: artiﬁcial neural networks [7], support vector machines [8] or ran-
dom forest [2]. Regrettably none of these black-box classiﬁers provides an inherent

Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation ...
85
introspection into the reasoning process associated to the decision model. However,
in some areas where machine learning models are applied, the transparency in their
predictions is crucial.
Aiming at developing a novel classiﬁcation model, Papakostas et al. [31, 32]
introduced the notion of FCM-based classiﬁer. The most prominent challenge to
be confronted when constructing an FCM-based classiﬁer relies on the approach to
connect input and output neurons. It should be remarked that the topology of an
FCM-based classiﬁer must comprise a coherent and precise meaning for the physi-
cal system under investigation. This suggests that the intervention of human experts
to deﬁne the network topology is usually required.
The development of accurate learning algorithms for computing the required
parameters is another issue that deserves attention. In the literature, several unsu-
pervised and supervised learning methods have been recently proposed [29]. These
algorithms are mostly focused on computing the weight matrix that deﬁne the seman-
tic of the whole cognitive system. However, the prediction capability of an FCM-
based classiﬁer does not exclusively depend on the weight set. Other aspects such as
the network’s capability the represent the problem domain or the convergence issues
are equally important.
In this chapter, we focus on main advances on FCM-based classiﬁcation and chal-
lenges that remain open problems for the scientiﬁc community. The rest of the manu-
script is structured as follows. Section 2 brieﬂy surveys theoretical aspects related to
FCMs. Section 3 discusses about the transparency and usability of models for under-
standing the decision process. Section 4 describes the use of FCMs in the context of
pattern classiﬁcation. Section 5 describes the FCM-based models where input neu-
rons denote information granules rather low-level features. To conclude, Sects. 6 and
7 will wrap-up the paper and highlight the main points of view of this proposal.
2
Fuzzy Cognitive Maps
FCMs can be seen as recurrent neural networks with interpretability features that
have been widely used in modeling tasks [11]. They consist of a set of neural process-
ing entities called concepts (neurons) and their causal relations. The activation value
of such neurons regularly takes values in the [0, 1] interval, so the stronger the acti-
vation value of a neuron, the greater its impact on the network. Of course, connected
weights are also relevant in this scheme. The strength of the causal relation between
two neurons Ci and Cj is quantiﬁed by a numerical weight wij ∈[−1, 1] and denoted
via a causal edge from Ci to Cj.
There are three possible types of causal relationships between neural processing
units in an FCM-based network that express the type of inﬂuence from one neuron
to the other, which are detailed as follows:

86
G. Nápoles et al.
∙If wij > 0 then an increase (decrement) in the cause Ci produces an increment
(decrement) of the eﬀect Cj with intensity |wij|.
∙If wij < 0 then an increase (decrement) in the cause Ci produces an decrement
(increment) of the neuron Cj with intensity |wij|.
∙If wij = 0 then there is no causal relation between Ci and Cj.
Equation 1 shows the Kosko’s activation rule, with A(0) being the initial state. This
rule is iteratively repeated until a stopping condition is met. A new activation vector
is calculated at each step t and after a ﬁxed number of iterations, the FCM will be
at one of the following states: (i) equilibrium point, (ii) limited cycle or (iii) chaotic
behavior [12]. The FCM is said to have converged if it reaches a ﬁxed-point attractor,
otherwise the updating process terminates after a maximum number of iterations T
is reached.
A(t+1)
i
= f
( M
∑
j=1
wjiA(t)
j
)
, i ≠j
(1)
The function f(⋅) in Eq. 1 denotes a monotonically non-decreasing nonlinear func-
tion used to clamp the activation value of each neuron to the allowed interval. Exam-
ples of such functions are the bivalent function, the trivalent function, and the sig-
moid variants [37].
We put emphasis in the sigmoid function since it has exhibited superior prediction
capabilities [3]. Equation 2 formalizes the non-linear transfer function used in our
conducted researches, where 𝜆is the sigmoid slope and h denotes the oﬀset. Several
studies reported at [1, 10, 14, 17, 27] have shown that such parameters are closely
related with the network convergence.
f(Ai) =
1
1 + e−𝜆(Ai−h)
(2)
Equation 1 shows an inference rule widely used in many FCM-based applications,
but it is not the only one. Stylios and Groumpos [36] proposed a modiﬁed inference
rule, found at Eq. 3, where neurons take into account its own past value. This rule is
preferred when updating the activation value of neurons that are not inﬂuenced by
other neural processing entities.
A(t+1)
i
= f
( M
∑
j=1
wjiA(t)
j + A(t)
i
)
, i ≠j
(3)
Another modiﬁed updating rule was proposed in [28] to avoid the conﬂicts emerg-
ing in the case of non-active neurons. Being more explicit, the rescaled inference
depicted in Eq. 4 allows dealing with the scenarios where there is not information
about an initial neuron-state and helps preventing the saturation problem. The reader
can notice that we can obtain a similar eﬀect by using a shifted sigmoid function with
the adequate slope.

Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation ...
87
A(t+1)
i
= f
( M
∑
j=1
wji(2A(t)
j −1) + (2A(t)
i −1)
)
, i ≠j
(4)
If the cognitive network is able to converge, then the system will produce the same
output towards the end, and therefore the activation degree of neurons will remain
unaltered (or subject to inﬁnitesimal changes). On the other hand, a cyclic FCM
produces dissimilar responses with the exception of a few states that are periodically
produced. The last possible scenario is related to chaotic conﬁgurations in which the
network yields diﬀerent state vectors. Formally, such situations are mathematically
deﬁned as follows:
∙Fixed-point (∃t𝛼∈{1, 2, … , (T −1)} ∶A(t+1) = A(t), ∀t ≥t𝛼): the FCM produces
the same state vector after the t𝛼-th iteration-step. This suggests that A(t𝛼) =
A(t𝛼+1) = A(t𝛼+2) = · · · = A(t).
∙Limit cycle (∃t𝛼, P ∈{1, 2, … , (T −1)} ∶A(t+P) = A(t), ∀t ≥t𝛼): the FCM peri-
odically produces the same state vector after the t𝛼-th iteration-step. This sug-
gests that A(t𝛼) = A(t𝛼+P) = A(t𝛼+2P) = · · · = A(t𝛼+jP) where t𝛼+ jP ≤T, such that
j ∈{1, 2, … , (T −1)}.
∙Chaos: the FCM continues producing diﬀerent state vectors for successive cycles,
thus being impossible to make suitable decisions.
If the FCM is unable to converge, then the model will produce confusing responses
and thus a pattern cannot be derived [26], thus being impossible to arrive at suitable
conclusions. In presence of chaos or cyclic situations, the reasoning rule stops once
a maximal number of iterations T is reached. In classiﬁcation scenarios, the decision
class is then calculated from the last cycle, but this output is partially unreliable due
to the lack of convergence.
3
The Reasoning Process and Its Explainability
The classiﬁcation problem [4] is about building a mapping f ∶→that assigns
to each instance x ∈described by the attribute set 𝛷= {𝜙1, … , 𝜙M} a decision
class D from the N possible ones in = {D1, … , DN}. The mapping is often learned
in a supervised fashion, i.e., by relying on an existing set of previously labeled exam-
ples used to train the model. The learning process is regularly driven by the mini-
mization of a cost/error function.
Researchers in Machine Learning are primarily focused on prediction rates.
Regrettably, most accurate classiﬁers do not provide any mechanism to explain how
they arrived at a particular conclusion and therefore behave like a “black-box”. Some
classiﬁers like Artiﬁcial Neural Networks, Support Vector Machines, Ensemble tech-
niques or Random Forests are well-known to be the most likely successful algorithms
for addressing classiﬁcation problems in terms of prediction rates. However, their
lack of transparency negatively eﬀects their usability in scenarios where understand-
ing the decision process is required.

88
G. Nápoles et al.
For example, neural computation is a widely studied research ﬁeld within Arti-
ﬁcial Intelligence. The main limitation of Artiﬁcial Neural Networks is their lack
of transparency, which means that the network cannot justify its complex reasoning
process. As a result, these models do not allow interpreting the semantic behind the
physical system under investigation since the transparency is a necessary condition
to build interpretable classiﬁers.
Aiming at elucidating the hidden reasoning process of black-boxes, several
post-hoc procedures have been proposed. For example, one of these explanatory
techniques used explicit IF-THEN rules for extracting knowledge from black-box
classiﬁers while more recent procedures use symbolic approaches to approximate
the model [9]. But whether such explanation is truly comprehensive and meaningful
in the case of large trees or rule sets is questionable.
The transparency inherent to FCMs and their underlying neural foundations have
motivated researchers to build interpretable FCM-based classiﬁers. In these models,
the interpretability may be achieved through causal relations between neural enti-
ties deﬁning the modeled system. Regrettably, building accurate, truly interpretable
FCM-based classiﬁers involves diﬃcult challenges.
4
Low-Level FCM-Based Classiﬁers
As already mentioned, FCMs have been widely studied due to their appealing proper-
ties for handling complex and dynamic systems, but the development of FCM-based
classiﬁers has received less attention.
One of the ﬁrsts attempt to use FCMs in the context of pattern classiﬁcation was
implemented in [31, 32]. In these references, the authors deﬁned the notion of FCM-
based classiﬁers and proposed some generic architectures. The most prominent chal-
lenge to be faced when constructing an FCM-based classiﬁer lies on how to connect
input and output neurons.
It should be remarked that an FCM classiﬁer’s topology (i.e., concepts and causal
relations) must comprise a coherent and precise meaning for the physical system
being modeled. If the input neurons represent features of the classiﬁcation problem,
then we are in presence of a low-level cognitive classiﬁer where neural processing
units can be categorized as shown below:
Deﬁnition 1 We say that a neural processing entity Ci is an independent input neu-
ron if its activation value does not depend on the other input neurons.
Deﬁnition 2 We say that a neural processing entity Ci is a dependent input neuron
if its activation value is inﬂuenced by other connected neurons.
Deﬁnition 3 We say that a neural processing entity Ci in an FCM-based classiﬁer
is an output neuron if we can predict a decision class from its ﬁnal activation value,
which only depends on the connected input neurons.

Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation ...
89
Typically, independent and dependent input neurons are used to activate the cog-
nitive networks since they often denote problem features. Output neurons, on the
other hand, are used to compute the decision class for an initial activation vector.
In the case of independent input neurons, they can propagate their initial activation
vector and they are not inﬂuenced by any other input neurons, therefore their acti-
vation values remain static. Notice that the expert must ideally determine the role of
each neurons and the way that input neurons are connected each other. In spite of
this fact, Papakostas et al. [30] investigated three generic architectures for mapping
the decision classes:
∙Class-per-output architecture. Each decision class is mapped to an output neu-
rons. Therefore, the predicted decision class corresponds to the label of the output
neuron having the highest activation value.
∙Single-output architecture. Each decision class is enclosed into the activation
space of a single output neuron.
1. Using a clustering approach. Each class is associated with a cluster center. In the
testing phase, the center having the closest distance to the projected activation
value is assigned to the input instance.
2. Using a thresholding approach. Each decision class is associated with a pair of
thresholds. In the testing phase, the interval comprising the projected activation
value is then assigned to the input instance.
In these architectures, the human intervention is required during the construction
stage, and even so, the supervised learning methods will probably fail in producing
authentic causal relations since they just ﬁt the model to the existing data. There-
fore, we are losing the interpretability features attached to the network, although the
decision process remains transparent.
On the other hand, the absence of hidden neural entities in these recurrent neural
networks may probably lead to poor prediction rates. Aiming at boosting the pre-
diction capability of FCM-based classiﬁer, in [32] the authors put forth two hybrid
typologies. Figures 1 and 2 show these typologies that include a black-box classiﬁer
to improve the overall prediction rates.
In the ﬁrst model, the black-box produces a conﬁdence degree per decision class.
Sequentially, this vector is used as initial conﬁguration for the FCM model that cor-
rects the responses produced by the black-box. In the second model, the input neu-
rons are also connected to output ones, so the predictions computed by the black-box
classiﬁer can be understood as a bias.
These hybrid models completely destroy the transparency attached to the cogni-
tive network. If this happens, then, there is no real reason to use FCMs in classi-
ﬁcation scenarios, instead we may adopt black-box models such as Support Vector
Machines, Neural Networks or Random Forests.
Another key element towards designing a low-level FCM-based classiﬁer relies
on the learning algorithm. The chief objective behind FCM learning has been to

90
G. Nápoles et al.
Fig. 1
Hybrid FCM-based classiﬁer type-1
Fig. 2
Hybrid FCM-based classiﬁer type-2

Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation ...
91
derive the weight matrix W(M×M) that minimizes the prediction error based on expert
intervention, available historical data or both. According to the their classiﬁcation
scheme, existing learning algorithms can be roughly gathered into two large groups:
unsupervised and supervised.
4.1
Unsupervised Learning Algorithms
Hebbian-based learning methods are unsupervised procedures that do not require
a set of labeled historical data, i.e., data in which the value of the decision fea-
ture(s) are previously known. The aim of learning FCMs by using adaptive Hebbian-
based methods is to yield weight matrices on the basis of experts’ knowledge and to
improve the accuracy of previously set weights.
Papakostas et al. [30] thoroughly tested the performance of several Hebbian-type
algorithms in classiﬁcation scenarios, and concluded that these learning procedures
regularly produce very poor classiﬁcation rates.
More explicitly, Hebbian-type methods are convenient to ﬁne-tune the weight
set with a small deviation from the initial conﬁguration. As a result, the adjusted
causal relations partially preserve their physical meaning, which cannot be guar-
anteed when using a heuristic-based learning method. Of course, the requirement
of experts’ knowledge is a serious drawback. The ﬂexibility on data requirements
of these algorithms is the key aspect behind their poor generalization capability.
This makes Hebbian-type algorithms unﬁt for solving pattern classiﬁcation prob-
lems where multiple classes must be predicted.
4.2
Supervised Learning Algorithms
As an alternative to Hebbian-based methods, we can learn the network structure from
data using heuristic-based algorithms [29] in a supervised fashion. Heuristic learn-
ing approaches aim at generating weight matrices that minimize an error function,
viz. the diﬀerence between the expected responses and the map-inferred outputs.
These methods are more expensive optimization techniques given that they regu-
larly explore multiple candidate solutions. Besides, they require the deﬁnition of the
objective function to be optimized, which is the core of these learning procedure,
rather than the adopted search method.
Equation 5 formalizes an error function for pattern classiﬁcation scenarios, where
X denotes the weight matrix, K is the number of training instances, 𝜓(.) is the
decision model to be used for determining the class label, while Sk represents the
expected decision class for the kth training instance. In the case of the single-output
architecture, the class is computed from the activation value of the decision neuron
at the last iteration-step.

92
G. Nápoles et al.
E(X) = 1
k
K
∑
k=1
{𝛾k, 𝜓(A(T)
Mk) = Sk
1, 𝜓(A(T)
Mk) ≠Sk
(5)
Aiming at reducing the convergence error of the FCM-based classiﬁer, the error
function depicted in Eq. 5 uses a penalization factor 𝛾k for those instances that have
been correctly classiﬁed. In short, the convergence error can be understood as the
overall dissimilarity between the system response at each iteration, and the activation
value at the last iteration-step.
Nápoles et al. [16, 17, 27] investigated the convergence of FCM-based classiﬁers
and proposed a learning method to improve the system convergence, without alter-
ing the causal weights. More recently, they introduced an extended learning algo-
rithm [26] where weights are estimated taking into account both accuracy and con-
vergence. Based on these results, we propose a generalized measure to compute the
convergence error of an FCM-based classiﬁer.
Equation 6 shows the convergence error for the kth instance, where 𝜔t = t∕T is
the relevance of each iteration, M is the number of neurons, N < M is the number of
input-type ones, whereas A(t)
ik denotes the current activation value for the ith neuron.
Moreover, 𝜋k represents the centroid (ideal) point of the decision label associated to
the kth training instance.
𝛾k =
T
∑
t=1
2𝜔t
M(T + 1)
⎛
⎜
⎜⎝
N
∑
i=1
(A(t)
ik −A(T)
ik )
2
N
+
M−N
∑
i=1
(A(t)
ik −𝜋k)
2
M −N
⎞
⎟
⎟⎠
(6)
Let us assume an FCM-based classiﬁer using a single-output architecture and a
thresholding approach, where the kth instance is associated with jth decision class.
Equation 7 computes the centroid, where Lk
j and Uk
j denote the lower and upper deci-
sion thresholds, respectively.
𝜋k =
⎧
⎪
⎨
⎪⎩
Lk
j ,
Lk
j = 0
Uk
j ,
Uk
j = 1
Lk
j +Uk
j
2
, Lk
j ≠0, Uk
j ≠1
(7)
This approach introduces two key contributions in regard to the algorithm pro-
posed in [26]. First, we remove the required parameters by measuring the conver-
gence error if the target instance is correctly classiﬁed. This suggests that the system
accuracy will always be favored. Second, we compute the converge error of sigmoid
neurons according to their role in the network. The convergence error of input-type
neurons is measured as the overall dissimilarity between the system response at each
cycle, and the activation value at the last iteration. However, in the case of output-
type neurons, we calculate the overall dissimilarity between each response, and the
corresponding centroid.

Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation ...
93
Preliminary simulations using a Bioinformatic problem [21] have shown that this
algorithms is capable of producing a suitable trade-oﬀbetween convergence and
accuracy. Ensuring the convergence helps in preventing the misclassiﬁcations of
boundary instances, otherwise the model becomes fragile to perturbations. How-
ever, this algorithm cannot be generalized to other domains where the experts are
unable to deﬁne the network topology.
5
High-Level Cognitive Classiﬁers
Cognitive mapping allows modeling diﬀerent levels of interpretability, which depend
on the abstraction degree. Neurons denoting entities with high abstraction level (i.e.,
information granules or prototypes) lead to high-level interpretable networks. If the
level of abstraction is too high, then the physical system under investigation is diﬃ-
cult to analyze, so we are losing interpretability. On the other hand, deﬁning attribute-
level entities allow interpreting the system behavior at a low-level. However, some-
times the domain experts are unable to deﬁne precise, authentic causal relations with
such speciﬁcity level.
High-level cognitive classiﬁer refer to FCM-based models where input neurons
denote information granules rather than low-level features. For example, Nápoles et
al. [23, 24] introduced the notion of rough cognitive mapping in the context of pat-
tern classiﬁcation. The new classiﬁcation model transforms the feature space into a
granular one that is exploited using the neural inference rule present in FCM-based
models. In these so-called Rough Cognitive Networks (RCNs), the weight matrix is
automatically computed on the basis of the three-way decision rules [38] that con-
struct three rough regions [33] to perform the classiﬁcation process. The RCN model
achieved competitive performance with respect to state-of-the-art methods in a real-
world classiﬁcation problems [23] as well as in a network intrusion detection sce-
nario [22].
Figure 3 shows an RCN to solve any classiﬁcation problem with two decision
classes, where Pk, Nk and Bk are input neurons denoting the positive, negative and
boundary regions related to the kth decision class.
More recently, two improved RCN models were introduced: Rough Cognitive
Ensembles [20] and Fuzzy-Rough Cognitive Networks [25]. The purpose of these
algorithms is to deal with the parametric requirement of rough cognitive classiﬁers
while preserving their global prediction capabilities. The former is a granular ensem-
ble model where each base RCN operates at a diﬀerent granularity degree, whereas
the latter replaced the crisp-rough constructs with fuzzy-rough ones. Numerical
results have shown that both approaches are capable of outperforming the RCN algo-
rithm. These modiﬁed algorithms perform comparably, thus we can achieve the same
prediction rates using an ensemble composed of several networks that using a single
fuzzy-rough classiﬁer!

94
G. Nápoles et al.
Fig. 3
RCN-based classiﬁer for binary problems
Inspired on the RCN semantics and the approaches discussed in [34, 35], Nápoles
et al. [19] proposed a partitive granular cognitive map to solve graded multi-label
classiﬁcation problems. In these machine learning problems, the goal is to predict the
degree to which each instance relates to each available decision class. Three diﬀerent
FCM topologies were studied and several convergence features were included into
the supervised learning methodology. Numerical experiments conﬁrmed the ability
of these granular classiﬁers to accurately estimate the degree of association between
an object and each label.
It is worth highlighting the transparency on the decision model attached to Rough
Cognitive Networks. In these models, we can interpret the physical system at a high-
level by relying on the semantics behind the information constructs. However, a low-
level reasoning is not possible, even when the classiﬁer’s decision process remains
transparent and comprehensible.
6
Remaining Challenges
The development of accurate, truly interpretable FCM-based classiﬁers involves
three main challenges, that still remain open:
∙Construction issues. FCMs are knowledge-based techniques that regularly require
the intervention of experts to deﬁne the network topology, i.e., the neurons and
causal relations connecting them. Alternatively, we can learn the network struc-
ture from data using heuristic-based algorithms in a supervised fashion. However,
these methods cannot produce authentic causal relations describing the system
under analysis since they are oriented to ﬁt the network to the historical data,

Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation ...
95
without considering the system semantics. This implies that we cannot interpret
the problem domain from such models, even if the FCM inference process is still
transparent. Some authors attempt overcoming this drawback using correlation
measures, which fail in capturing the underlying semantics behind causal rela-
tions. Being more explicit, it is well-known that causality does surely imply the
existence of correlation, but the opposite does not necessarily hold.
∙Accuracy issues. Generally speaking, the prediction rates of FCM-based clas-
siﬁers are poor when compared with standard black-box models, mainly due to
their limitations to represent the problem domain and the absence of theoretically
sound learning algorithms. Papakostas et al. [30] concluded that Hebbian-based
algorithms are not suitable in pattern classiﬁcation environments, while the per-
formance of heuristic-based learning methods quickly deteriorates when the num-
ber of neurons scales up. Froelich [5] proposed a promising post-optimization
method to improve the prediction rates of FCM-based classiﬁers using a single-
output architecture. Notice however that the overall prediction rates achieved by
this method will heavily depend on the learning algorithm used to estimate the
weight set.
∙Convergence issues. FCM-based networks are recurrent cognitive systems that
produce an output vector at each discrete-time step. This procedure is repeated
until either the map converges or a maximal number of iterations is reached. With-
out ensuring the convergence, the model becomes unreliable and decision making
becomes impossible. Regrettably, heuristic-based methods cannot ensure the FCM
convergence, which implies that the resultant models are no longer interpretable
and therefore, there is no reason to use cognitive mapping in pattern classiﬁca-
tion environments. More recently, Nápoles and his collaborators [16, 17, 26, 27]
obtained promising results toward improving the convergence of FCM-based mod-
els without modifying the causal relations. However, analytical results reported in
[18] have shown that establishing a suitable balance between convergence and
accuracy cannot always be achieved without altering the weights.
It should be observed that the accuracy and convergence issues are mathematical
challenges that can be present in other Machine Learning approaches. After all, the
main purpose of traditional classiﬁers is to achieve the best possible prediction rates.
The construction issues are, however, more delicate. Deﬁning authentic causal rela-
tions between neural entities is the key aspect towards designing truly interpretable
FCM-based systems. Otherwise, the model will produce misleading results when
performing WHAT-IF simulations.
As far as we know, there is no learning method able to discover authentic causal
structures from historical records due to the lack of well-established statistical tests
for measuring causality. Even some authors aﬃrm that the term “causality” is a
philosophic concept that cannot possibly be measured in a numerical way without
performing controlled experiments.

96
G. Nápoles et al.
7
Conclusions
The use of FCMs for modeling real-life problems by recreating virtual scenarios have
been demonstrated and reported in literature. These knowledge-based networks have
been used as a modeling tool to analyze the behavior of complex systems, where it
is very diﬃcult to describe the entire system by a precise mathematical model. Con-
sequently, it is easier and more practical to represent the decision-making process in
a graphical way.
This paper explored the development of FCM-based classiﬁers and focused on the
wide research avenues it provides. In spite of the detected shortcomings and chal-
lenges, the transparency inherent to cognitive mapping keeps motivating researchers
to build interpretable FCM-based classiﬁers. In these models, the interpretability is
achieved through causal relations between neurons deﬁning the system under analy-
sis. FCM-based models also provide other set of attractive characteristics: they are
able to discover hidden patterns, are ﬂexible, dynamic, combinable and tunable from
diﬀerent perspectives.
The FCM-based modeling approach allows building the network in presence of
incomplete, conﬂicting or subjective information. Moreover, the inherent neural fea-
tures of cognitive mapping provide a promising research avenue towards improving
their accuracy in prediction scenarios. This suggests that FCM-based models could
be as eﬃcient as black-box models while retaining their ability to elucidate the sys-
tem behavior through causal relations. Precisely, this conjecture, among other fac-
tors, keeps this research subject as a challenge open to the scientiﬁc community and
a lively ﬁeld of research.
Acknowledgements This work was partially supported by the Research Council of Hasselt Uni-
versity. Moreover, Gonzalo Nápoles would like to thank Frank Vanhoenshoven, Hasselt University,
for his critical remarks and valuable suggestions.
References
1. Boutalis, Y., Kottas, T.L., Christodoulou, M.: Adaptive estimation of fuzzy cognitive maps
with proven stability and parameter convergence. IEEE Trans. Fuzzy Syst. 17(4), 874–889
(2009)
2. Breiman, L.: Random forests. Mach. Learn. 45(1), 5–32 (2001)
3. Bueno, S., Salmeron, J.L.: Benchmarking main activation functions in fuzzy cognitive maps.
Expert Syst. Appl. 36(3), 5221–5229 (2009)
4. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classiﬁcation, 2nd edn. Wiley (2012)
5. Froelich, W.: Towards improving the eﬃciency of the fuzzy cognitive map classiﬁer. Neuro-
computing 232, 83–93 (2017)
6. Grau, I., Nápoles, G., Bonet, I., Garcia, M.M.: Backpropagation through time algorithm for
training recurrent neural networks using variable length instances. Computación y Sistemas
17(1), 15–24 (2013)
7. Haykin, S.: Neural Networks: A Comprehensive Foundation, 2nd edn. Prentice Hall PTR,
Upper Saddle River (1998)

Fuzzy Cognitive Maps Based Models for Pattern Classiﬁcation ...
97
8. Hearst, M.A., Dumais, S.T., Osman, E., Platt, J., Scholkopf, B.: Support vector machines. IEEE
Intell. Syst. Appl. 13(4), 18–28 (1998)
9. Jacobsson, H.: Rule extraction from recurrent neural networks: a taxonomy and review. Neural
Comput. 17(6), 1223–1263 (2005)
10. Knight, C.J., Lloyd, D.J., Penn, A.S.: Linear and sigmoidal fuzzy cognitive maps: an analysis
of ﬁxed points. Appl. Soft Comput. 15, 193–202 (2014)
11. Kosko, B.: Fuzzy cognitive maps. Int. J. Man-Mach. Stud. 24(1), 65–75 (1986)
12. Kosko, B.: Hidden patterns in combined and adaptive knowledge networks. Int. J. Approx.
Reason. 2(4), 377–393 (1988)
13. Kosko, B.: Fuzzy Engineering. Prentice Hall (1997)
14. Kottas, T.L., Boutalis, Y.S., Christodoulou, M.A.: Fuzzy cognitive networks: adaptive net-
work estimation and control paradigms. In: Glykas, M. (ed.) Fuzzy Cognitive Maps: Advances
in Theory, Methodologies, Tools and Applications, pp. 89–134. Springer, Berlin, Heidelberg
(2010)
15. McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in nervous activity. In:
Anderson, J.A., Rosenfeld, E. (eds.) Neurocomputing: Foundations of Research, pp. 15–27.
MIT Press, Cambridge (1988)
16. Nápoles, G., Bello, R., Vanhoof, K.: Learning Stability Features on Sigmoid Fuzzy Cogni-
tive Maps through a Swarm Intelligence Approach, pp. 270–277. Springer, Berlin, Heidelberg
(2013)
17. Nápoles, G., Bello, R., Vanhoof, K.: How to improve the convergence on sigmoid fuzzy cog-
nitive maps? Intell. Data Anal. 18(6S), S77–S88 (2014)
18. Nápoles, G., Concepción, L., Falcon, R., Bello, R., Vanhoof, K.: On the accuracy-convergence
trade-oﬀin sigmoid fuzzy cognitive maps. IEEE Trans. Fuzzy Syst. (submitted) (2017)
19. Nápoles, G., Falcon, R., Papageorgiou, E., Bello, R., Vanhoof, K.: Partitive granular cognitive
maps to graded multilabel classiﬁcation. In: 2016 IEEE International Conference on Fuzzy
Systems (FUZZ-IEEE), pp. 1363–1370 (2016)
20. Nápoles, G., Falcon, R., Papageorgiou, E., Bello, R., Vanhoof, K.: Rough cognitive ensembles.
Int. J. Approx. Reason. 85, 79–96 (2017)
21. Nápoles, G., Grau, I., Bello, R., Grau, R.: Two-steps learning of fuzzy cognitive maps for
prediction and knowledge discovery on the HIV-1 drug resistance. Expert Syst. Appl. 41(3),
821–830 (2014)
22. Nápoles, G., Grau, I., Falcon, R., Bello, R., Vanhoof, K.: A granular intrusion detection system
using rough cognitive networks. In: Abielmona, R., Falcon, R., Zincir-Heywood, N., Abbass,
H. (eds.) Recent Advances in Computational Intelligence in Defense and Security, chapter 7.
Springer (2016)
23. Nápoles, G., Grau, I., Papageorgiou, E., Bello, R., Vanhoof, K.: Rough cognitive networks.
Knowl.-Based Syst. 91, 46–61 (2016)
24. Nápoles, G., Grau, I., Vanhoof, K., Bello, R.: Hybrid model based on rough sets theory and
fuzzy cognitive maps for decision-making. In: International Conference on Rough Sets and
Intelligent Systems Paradigms, pp. 169–178. Springer (2014)
25. Nápoles, G., Mosquera, C., Falcon, R., Grau, I., Bello, R., Vanhoof, K.: Fuzzy-rough cognitive
networks. Neural Netw. (2017)
26. Nápoles, G., Papageorgiou, E., Bello, R., Vanhoof, K.: Learning and convergence of fuzzy
cognitive maps used in pattern recognition. Neural Process. Lett. 1–14 (2016)
27. Nápoles, G., Papageorgiou, E., Bello, R., Vanhoof, K.: On the convergence of sigmoid fuzzy
cognitive maps. Inf. Sci. 349–350, 154–171 (2016)
28. Papageorgiou, E.I.: A new methodology for decisions in medical informatics using fuzzy cog-
nitive maps based on fuzzy rule-extraction techniques. Appl. Soft Comput. 11(1), 500–513
(2011)
29. Papageorgiou, E.I.: Learning algorithms for fuzzy cognitive maps—a review study. IEEE
Trans. Syst. Man Cybern. Part C (Appl. Rev.) 42(2), 150–163 (2012)
30. Papakostas, G., Koulouriotis, D., Polydoros, A., Tourassis, V.: Towards hebbian learning of
fuzzy cognitive maps in pattern classiﬁcation problems. Expert Syst. Appl. 39(12), 10620–
10629 (2012)

98
G. Nápoles et al.
31. Papakostas, G.A., Boutalis, Y.S., Koulouriotis, D.E., Mertzios, B.G.: Fuzzy cognitive maps for
pattern recognition applications. Int. J. Pattern Recogn. Artif. Intell. 22(8), 1461–1486 (2008)
32. Papakostas, G.A., Koulouriotis, D.E.: Classifying patterns using fuzzy cognitive maps. In:
Glykas, M. (ed.) Fuzzy Cognitive Maps: Advances in Theory, Methodologies, Tools and Appli-
cations, pp. 291–306. Springer, Berlin, Heidelberg (2010)
33. Pawlak, Z.: Rough sets. Int. J. Comput. Inf. Sci. 11(5), 341–356 (1982)
34. Pedrycz, W.: The design of cognitive maps: a study in synergy of granular computing and
evolutionary optimization. Expert Syst. Appl. 37(10), 7288–7294 (2010)
35. Pedrycz, W., Homenda, W.: From fuzzy cognitive maps to granular cognitive maps. IEEE
Trans. Fuzzy Syst. 22(4), 859–869 (2014)
36. Stylios, C.D., Groumpos, P.P.: Modeling complex systems using fuzzy cognitive maps. IEEE
Trans. Syst. Man Cybern.—Part A: Syst. Hum. 34(1), 155–162 (2004)
37. Tsadiras, A.K.: Comparing the inference capabilities of binary, trivalent and sigmoid fuzzy
cognitive maps. Inf. Sci. 178(20), 3880–3894 (2008)
38. Yao, Y.: Three-way decisions with probabilistic rough sets. Inf. Sci. 180(3), 341–353 (2010)

A Proposal of On-Line Detection of New
Faults and Automatic Learning in Fault
Diagnosis
Adrián Rodríguez Ramos, Alberto Prieto Moreno,
Antônio José da Silva Neto and Orestes Llanes-Santiago
Abstract In this paper a new approach of automatic learning for a fault diagnosis
system using fuzzy clustering techniques is presented. The proposal presents an oﬀ-
line learning stage, for training the classiﬁer to diagnose the initial known faults
and the normal operation state. In this stage, the data are ﬁrstly pre-processed to
eliminate outliers and reducing the confusion in the classiﬁcation process by using
the Density Objective Fuzzy C-Means (DOFCM) algorithm. Later on, the Kernel
Fuzzy C-Means (KFCM) algorithm is used to achieve greater separability among the
classes, and reducing the classiﬁcation errors. Finally, a step is developed to optimize
the two parameters used in the algorithms in the training stage using the Diﬀerential
Evolution algorithm. After the training, the classiﬁer is used on-line (recognition
stage) in order to process every new sample taken from the process. In this stage, a
novel fault detection algorithm is applied. The algorithm analyzes the observations
which are not classiﬁed in the known classes and belonging to a window of time to
determine if they constitute a new class, probably representative of a new fault or if
they are noise. If a new class is identiﬁed, a procedure is developed to incorporate
it to the known classes by the classiﬁer. The approach proposed was validated using
an illustrative example. The results obtained indicate the feasibility of the proposal.
A. Rodríguez Ramos ⋅A. Prieto Moreno ⋅O. Llanes-Santiago (✉)
Departamento de Automática y Computación, Universidad Tecnológica
de la Habana José Antonio Echeverría, CUJAE, CUJAE Calle 114, No. 11901,
10390 La Habana, Cuba
e-mail: orestes@tesla.cujae.edu.cu
A. Rodríguez Ramos
e-mail: adrian.rr@automatica.cujae.edu.cu
A. Prieto Moreno
e-mail: albprieto@automatica.cujae.edu.cu
A.J. da Silva Neto
Instituto Politécnico da Universidade Do Estado Do Rio de Janeiro (IPRJ/UERJ),
Nova Friburgo, Brazil
e-mail: ajsneto@iprj.uerj.br
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_6
99

100
A. Rodríguez Ramos et al.
1
Introduction
In current industries, there is a marked necessity to improve the processes eﬃciency
in order to produce with higher quality besides attending the environmental and
industrial safety regulations [18, 39]. In the industries, the faults in equipments can
have an unfavorable impact in the availability of the systems, the environment and
the safety of operators. For such reasons, the faults need to be detected and isolated,
being these tasks associated to the fault diagnosis systems [34].
Within the fault diagnosis methods there are those based on models [7, 8, 13, 32,
39, 40] and those based on the process historical data [2, 3, 15, 31, 37]. In the ﬁrst
approach, the tools use models that describe the functioning of the processes. These
tools are based on the residue generation obtained from the diﬀerence between the
measurable signals from the real process and the values obtained from the model.
This entails an elevated knowledge about the characteristics of the processes, their
parameters, and operation zones. However, it is usually very diﬃcult to achieve due
to the complexity of the industrial processes. On the other hand, the approaches based
in historical data do not need a mathematical model, and they do not require much
prior knowledge of the process parameters [41]. These characteristics constitute an
advantage for complex systems, where relationships among variables are nonlinear,
not totally known, and therefore, it is very diﬃcult to obtain an analytical model that
describes eﬃciently the dynamics of the process.
The fault diagnosis systems based on historical data are trained to be able to clas-
sify the process states known by the experts. However, with the decrease of the useful
life of the automation technical devices such as sensors, actuators and pumps among
others, the probability of occurrence of new faults increases. In this situation, the
diagnostic systems will not be able to correctly classify the new faults, which will
cause an erroneous decision making. For this reason, the topic related with the auto-
matic identiﬁcation of new patterns has gained a great importance in the area of fault
diagnosis, [12, 17], where the fault diagnosis systems are needed to detect the new
faults and incorporate them in their knowledge base. In this way, the fault diagnosis
systems will have an automatic learning mechanism to update their knowledge base.
By performing an analysis of the diﬀerent techniques developed in the recent
years for control and fault diagnosis tasks, it is signiﬁcative the increment in the use
of the fuzzy clustering methods [1, 5, 19, 33, 36].
Fuzzy clustering techniques are very important tools of unsupervised data clas-
siﬁcation [16], that can be used to organize data into groups based on similarities
among the individual data. Fuzzy clustering deals with the uncertainty and vague-
ness that can be found in a wide variety of applications, such as: image processing,
pattern recognition, object recognition, modeling and identiﬁcation [20, 23, 25, 35,
38, 42, 44]. The main focus of all fuzzy clustering techniques is to improve the
clustering by avoiding the inﬂuence of the noise and outlier data.
The Fuzzy C-Means (FCM) algorithm [4], is one of the most widely used algo-
rithm for clustering due to its robust results for overlapped data. Unlike k-means
algorithm, data points in the FCM algorithm may belong to more than one cluster.

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
101
FCM algorithm obtains very good results with noise free data but are highly sensitive
to noisy data and outliers [16].
Other similar techniques such as, Possibilistic C-Means (PCM) [24] and Possi-
bilistic Fuzzy C-Means (PFCM) [30] interpret clustering as a possibilistic partition
and work better in presence of noise in comparison with FCM. However, PCM fails
to ﬁnd optimal clusters in the presence of noise [16] and PFCM does not yield satis-
factory results when data set consists of two clusters which are highly unlike in size
and outliers are present [16, 22].
Noise Clustering (NC) [10, 11], Credibility Fuzzy C-Means (CFCM) [9], and
Density Oriented Fuzzy C-Means (DOFCM) [21] algorithms were proposed specif-
ically to work eﬃciently with noisy data.
The clustering output depends upon various parameters such as distribution of
data points inside and outside the cluster, shape of the cluster and linear or non-linear
separability. The eﬀectiveness of the clustering method strongly relies on the choice
of the metric distance adopted. FCM uses Euclidean distance as the distance mea-
sure, and therefore, it can only be able to detect hyper spherical clusters. Researchers
have proposed other distance measures such as, Mahalanobis distance measure, and
Kernel based distance measure in data space and in high dimensional feature space,
such that non-hyper spherical/non-linear clusters can be detected [45, 46].
Another problem usually present in fuzzy clustering methods is that their perfor-
mance depend signiﬁcantly on the initialization of their parameters. In many occa-
sions, it is necessary to make multiple runs of the algorithm in order to obtain good
results which is time consuming, and not always the obtention of the best solution is
guaranteed.
In order to overcome these problems in this paper a new approach to automatic
learning and on-line detection of new faults using fuzzy clustering techniques is
proposed. The methodology presents an oﬀ-line training stage and an on-line recog-
nition stage. In the training stage the historical data of the process are used to train
a fuzzy classiﬁer and the center of each one of known classes are calculated. In a
ﬁrst step, the data are pre-processed to eliminate outliers and reducing the confusion
in the classiﬁcation process. To accomplish this objective, the Density Objective
Fuzzy C-Means (DOFCM) algorithm is used. Later on, the Kernel Fuzzy C-Means
(KFCM) algorithm is used to achieve greater separability among the classes, and
reducing the classiﬁcation errors. Finally, an step is used to optimize the parameters
m (factor that regulates the fuzziness of the resulting partition) and 𝜎(bandwidth
and indicates the degree of smoothness of the Gaussian kernel function) of the algo-
rithms used in this stage, applying DE algorithm. After the training, the classiﬁer
is used on-line (recognition) in order to process every new sample taken from the
process. In this stage, a new fault detection algorithm is applied. The algorithm ana-
lyzes the observations which are not classiﬁed in the known classes and belonging
to a window of time to determine if they constitute a new class, probably represen-
tative of a new fault or if they are noise. If a new class is identiﬁed, a procedure is
developed to incorporate it to the knowledge base of the classiﬁer.
The principal contribution of this chapter is the obtaining of a methodology that
adequately combines fuzzy clustering algorithms to solve the drawbacks of this type

102
A. Rodríguez Ramos et al.
of techniques when the data is aﬀected by noise and outliers, to improve the classiﬁ-
cation results by using kernel tools and to incorporate a new on-line fault detection
algorithm as part of a mechanism of automatic learning.
The organization of the chapter is as follows: in Sect. 2 are presented the general
characteristics of the tools used in the proposed methodology. In Sect. 3, a descrip-
tion of the new classiﬁcation methodology using fuzzy clustering techniques is pre-
sented. The Sect. 4 presents an illustrative example used to validate the methodology
proposed. Finally, the conclusions are presented.
2
General Description of the Computational Tools
In this section, a general description of the Density Oriented Fuzzy C-Mean algo-
rithm and Kernel FCM is presented. In addition, the DE algorithm used to opti-
mize the parameters of the DOFCM and KFCM algorithms in the training stage is
described too.
2.1
Density Oriented Fuzzy C-Means (DOFCM)
The algorithm attempts to decrease the noise sensitivity in fuzzy clustering by iden-
tifying outliers before the clustering process. The DOFCM algorithm creates c + 1
clusters with c good clusters and one cluster of noise. This algorithm identiﬁes out-
liers before the construction of the clusters, based on the density of data set, as is
shown in Fig. 1.
The neighborhood of a given radius of each point in a data set has to contain at
least a minimum number of other points. DOFCM deﬁnes a density factor, called the
neighborhood membership, which express the measure density of an object in rela-
tion to its neighborhood. The neighborhood membership of a point i in X is deﬁned
as:
Mi
neighborhood =
𝜂i
neighborhood
𝜂max
(1)
Fig. 1
Identiﬁcation of
outliers with the DOFCM
algorithm

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
103
where 𝜂i
neighborhood is the number of points in the point neighborhood i; 𝜂max is the
maximum number of points in the neighborhood of any point in the data set.
If the point q is in the point neighborhood of the point i, q will satisfy:
q ∈X|dist(i, q) ≤rneighborhood
(2)
where rneighborhood is the radius of neighborhood, and dist(i, q) is the distance between
points i and q. Calculation of neighborhood radius is done as per [14]. Neighborhood
membership of each point in the data set X is calculated using (1). The threshold
value 𝛼is selected from the complete range of neighborhood membership values,
depending on the density of points in the data set. The point will be considered as
an outlier if its neighborhood membership is less than 𝛼. Let i be a point in the data
set X, then
{Mi
neighborhood < 𝛼then i outlier
Mi
neighborhood ≥𝛼then i non −outlier
(3)
𝛼can be selected from the range of Mi
neighborhood values after observing the density of
points in the data set and it should be close to zero. Ideally, a point will be outlier only
if no other point is present in its neighborhood i.e. when neighborhood membership
is zero or threshold value 𝛼= 0. However, in this scheme, a point is considered as
an outlier when its neighborhood membership is less than 𝛼, where 𝛼is a critical
parameter to identify outlier. Its value depends upon the nature of data set, i.e., taking
into account the density of the data set, then, its value will vary for diﬀerent data sets.
After identifying the outliers, the process of clustering begins. DOFCM reformu-
lates FCM objective function as:
JDOFCM (X; U, v) =
c+1
∑
i=1
N
∑
k=1
(𝜇ik
)m (dik
)2
(4)
where, the distances are deﬁned by,
d2
ik = (𝐱k −𝐯i
)T 𝐀i
(𝐱k −𝐯i
) , ∀k, i = 1, … , c
(5)
Membership function 𝜇ik is modiﬁed as:
𝜇ik =
{
1
∑c
j=1(dik∕djk)
2∕(m−1) if non −outlier
0
if outlier
(6)
To update the centroid, DOFCM algorithm uses Eq. (7) as FCM algorithm. For
the constraint on fuzzy membership, DOFCM algorithm uses Eq. (8). The DOFCM
algorithm is presented in Algorithm 1.

104
A. Rodríguez Ramos et al.
𝐯i =
∑N
k=1
(𝜇m
ik𝐱k
)
∑N
k=1 𝜇m
ik
(7)
0 ≤
c
∑
i=1
𝜇ik ≤1, k = 1, 2, … , N
(8)
Algorithm 1 Density Oriented Fuzzy C-Means (DOFCM)
Input: data with outliers 𝐗, c, 𝜖> 0, m > 1, 𝛼, number of iterations
Output: data without outliers 𝐗𝐩, fuzzy partition 𝐔, class centers 𝐕.
Identiﬁcation of the outliers:
Calculate neighborhood radius.
Calculate 𝜂i
neighborhood according to (2).
Select 𝜂max.
Calculate Mi
neighborhood according to (1).
With the value of 𝛼, identify outliers according to (3).
Clustering process:
Initialize 𝐔to random fuzzy partition.
for l = 1 to l = Itr_max do
Update classes centers according to (7).
Calculate the distance dik according to (5).
Update 𝐔according to (6).
Verify stopping criterion.
end for
2.2
Kernel Fuzzy C-Means (KFCM)
KFCM represents the kernel version of FCM. This algorithm uses a kernel function
for mapping the data points from the input space to a high dimensional space, as it
is shown in Fig. 2.
KFCM algorithm modiﬁes the objective function of FCM using the mapping
𝚽as:
JKFCM =
c
∑
i=1
N
∑
k=1
(𝜇ik
)m ‖‖𝚽(𝐱𝐤) −𝚽(𝐯𝐢)‖‖
2
(9)
Fig. 2
KFCM feature space
and kernel space

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
105
subject to:
c
∑
i=1
𝜇ik = 1, k = 1, 2, … , N
(10)
where ‖‖𝚽(𝐱𝐤) −𝚽(𝐯𝐢)‖‖
2 is the square of the distance between 𝚽(𝐱𝐤) and 𝚽(𝐯𝐢). The
distance in the feature space is calculated through the kernel in the input space as
follows:
‖‖𝚽(𝐱𝐤) −𝚽(𝐯𝐢)‖‖
2 = 𝐊(𝐱𝐤, 𝐱𝐤) −𝟐𝐊(𝐱𝐤, 𝐯𝐢)
+ 𝐊(𝐯𝐢, 𝐯𝐢)
(11)
If the Gaussian kernel is used, then 𝐊(𝐱, 𝐱) = 𝟏and ‖‖𝚽(𝐱𝐤) −𝚽(𝐯𝐢)‖‖
2 =
𝟐(𝟏−𝐊(𝐱𝐤, 𝐯𝐢)). Thus (4) can be written as:
JKFCM = 2
c
∑
i=1
N
∑
k=1
(𝜇ik
)m ‖‖1 −𝐊(𝐱𝐤, 𝐯𝐢)‖‖
2
(12)
where,
𝐊(𝐱𝐤, 𝐯𝐢) = e−‖𝐱k−𝐯i‖
2∕𝜎2
(13)
Minimizing (9) under the constraint shown in Eq. (10), yields:
𝜇ik =
1
∑c
j=1
(
1−𝐊(𝐱𝐤,𝐯𝐢)
1−𝐊(𝐱𝐤,𝐯𝐣)
)1∕(m−1)
(14)
𝐯i =
∑N
k=1
(𝜇m
ik𝐊(𝐱𝐤, 𝐯𝐢)𝐱𝐤
)
∑N
k=1 𝜇m
ik𝐊(𝐱𝐤, 𝐯𝐢)
(15)
The KFCM algorithm is presented in Algorithm 2.
Algorithm 2 Kernel Fuzzy C-Means (KFCM)
Input: data 𝐗𝐩, c, 𝜖> 0, m > 1, 𝜎, number of iterations.
Output: fuzzy partition 𝐔, class centers 𝐕.
Initialize 𝐔to random fuzzy partition.
for l = 1 to l = Itr_max do
Update classes centers according to (15) for Gaussian kernels.
Calculate the distances according to (11).
Update 𝐔according to (14).
Verify stopping criterion.
end for

106
A. Rodríguez Ramos et al.
2.3
Diﬀerential Evolution (DE)
DE is an evolutionary algorithm based on populations, that uses methods derived
from Biology like inheritance, mutation, natural selection and crossover. The idea
behind DE is to generate a population of new feasible solutions based on perturbed
solutions belonging to the previous population of solutions obtained up to a given
time. This generation scheme is based on three operators: Mutation, Crossover
and Selection. The conﬁguration of DE can be summarized using the following
notation:
DE∕𝕏j∕𝛾∕𝜆∗
where 𝕏j denotes the solution to be perturbed in the j-th iteration, 𝛾indicates the
number of pairs of solutions to be used for perturbations of the current solution 𝕏j,
and 𝜆∗indicates the distribution function that will be used during the crossover. In the
present work has been considered the conﬁguration DE∕𝕏j(best)∕1∕Bin, where 𝕏j(best)
indicates the best individual of the population, Z, and Bin the Binomial Distribution
function. This mutation operator is expressed in the following way:
𝕏j+1 = 𝕏j(best) + FS(𝕏j(𝛼) −𝕏j(𝛽))
(16)
where 𝕏j+1, 𝕏j(best), 𝕏j(𝛼), 𝕏j(𝛽) ∈ℝn, 𝕏j(𝛼) and 𝕏j(𝛽) are elements of the Z popula-
tion and FS is the escalation factor. For complementing the mutation operator, the
crossover operator is deﬁned for each component 𝕏n of the solution vector:
𝕏j+1
n
=
{
𝕏j+1
n , if R < CR
𝕏j(best)
n
, otherwise
(17)
where 0 ≤CR ≤1, is the crossover constant that is another control parameter in DE,
and R is a random number generated by the distribution 𝜆∗that in this case it is the
binomial distribution.
Finally, the selection operator results as follows:
𝕏j+1 =
{
𝕏j+1, if F (𝕏j+1) ≤F (𝕏j(best))
𝕏j(best), otherwise
(18)
The DE algorithm is presented in Algorithm 3.

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
107
Algorithm 3 Diﬀerential Evolution (DE)
Input: Z, FS, CR, number of iterations.
Output: 𝕏best (F( ̂𝜃)=(m, 𝜎))
Generate initial population of Z solutions.
Select better solution 𝕏best.
for l = 1 to l = Itr_max do
Apply Mutation according to (16)
Apply Crossover according to (17)
Apply Selection according to (18)
Update 𝕏best
Verify Stopping criterion
end for
3
A New Classiﬁcation Methodology with Automatic
Learning Using Fuzzy Clustering
The new classiﬁcation scheme with automatic learning proposed in this paper is
shown in Fig. 3. It presents an oﬀ-line learning stage and an on-line recognition
stage. In the training stage the historical data of the process are used to train (mod-
eling the functional stages through the clusters) a fuzzy classiﬁer. After the train-
ing, the classiﬁer is used on-line (recognition) in order to process every new sample
taken from the process. In this stage, the observations which are not classiﬁed in the
known classes and belonging to a window of time are analyzed to determine if they
Fig. 3
Classiﬁcation scheme using fuzzy clustering

108
A. Rodríguez Ramos et al.
constitute a new class, probably representative of a new fault or if they are noise. If
a new class is identiﬁed, the experts characterize the new fault and it is incorporated
to the knowledge base of the known faults and the classiﬁer is trained again.
The clustering methods group the data in diﬀerent classes based on a measure
of similitude. In the processes, the data are acquired by means of a SCADA system
(Supervisory Control and Data Acquisition), and the classes can be associated to
functional states. In the case of statistical classiﬁers, each sample is compared with
the center of each class by means of a measure of similitude to determine at which
class the sample belongs. In the case of the fuzzy classiﬁers, the comparison is made
to determine the membership degree of the sample to each class. In general, the
higher membership degree determines the class which the sample is assigned, as it
is showed in (19).
Ci = {i ∶max {𝜇ik
} , ∀i, k}
(19)
3.1
Oﬀ-Line Training
In the ﬁrst step, the center of the known classes 𝐯= 𝐯𝟏, 𝐯𝟐, … , 𝐯𝐍is determined
by using a historical data set representative of the diﬀerent operation states of the
process. In this step, a set of N observations (data points) 𝐗= [𝐱1, 𝐱2, … , 𝐱N] are
classiﬁed into c + 1 groups or classes using the DOFCM algorithm. The c classes
represent the normal operation conditions (NOC) of the process, and the faults to
be diagnosed. They contain the information to be used in the next step. The other
remaining class contains the data points identiﬁed as outliers by the DOFCM algo-
rithm, and they are not used in the next step.
In the second step, the KFCM algorithm receives the set of observations classiﬁed
by the DOFCM algorithm in the c classes. The KFCM algorithm maps these obser-
vations into a higher dimensional space in which the classiﬁcation process obtains
better results of satisfactory classiﬁcations. This step is shown in Algorithm (2). The
Fig. 4 shows the procedure described in steps 1 and 2.
Finally, a step to optimize the parameters of the algorithms used in steps 1 and
2 is implemented. In this step, the parameters m and 𝜎are estimated to optimize a
validity index using an optimization algorithm. This will allow to obtain an improved
U partition matrix, and therefore, a better position of the centers of the classes that
characterize the diﬀerent operation states of the system. Later, the estimated values
of m in Eqs. (4), (12) and 𝜎in Eq. (13) will be used during the on-line recognition,
and it will contribute to improve the classiﬁcation of the samples obtained by the
data acquisition system from the system.
The validity measures are indexes to evaluate quantitatively the result of a clus-
tering method and comparing its behavior when its parameters vary. Some indexes
evaluate the resulting U matrix, while others are focused on the geometric resulting
structure. The partition coeﬃcient (PC) [26, 29, 43], which measures the fuzziness

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
109
Fig. 4
Procedure performed by the DOFCM and KFCM algorithms
degree of the partition U, is used as validity measure in this case. Its expression is
shown in the Eq. (20).
PC = 1
N
c
∑
i=1
N
∑
k=1
(𝜇ik
)2
(20)
If the fuzziness degree of the partition U is high, the clustering process is better.
Being analyzed in a diﬀerent way, it allows to measure the degree of overlapping
among the classes. In this case, the optimum comes up when PC is maximized, i.e.,
when each pattern belongs to only one group. Likewise, minimum comes up when
each pattern belongs to each group.
Therefore, the optimization problem is deﬁned as:
max {PC} = 1
N
c
∑
i=1
N
∑
k=1
(𝜇ik
)2
subject to:
mmin < m ≤mmax
𝜎min ≤𝜎≤𝜎max
Then, a range of values of m and 𝜎should be deﬁned. It is known that 1 < m < ∞,
but from the practical point of view in many applications the value of m does not
exceed of two [23, 25, 38, 44], therefore in this case: 1 < m ≤2. The parameter 𝜎
is called bandwidth, and it indicates the degree of smoothness of the function. In
the case of 𝜎if it is overestimated, the function tends to show a linear behavior and

110
A. Rodríguez Ramos et al.
its projection in high-dimensional space loses its ability to separate non-linear data.
Meanwhile, if 𝜎is underestimated, the result will be highly sensitive to the noise
presents in the data. Then, the search space of the algorithm must be large, so that
during the exploration small and large values will be considered. In this chapter, a
group of experiments were developed and it was found that an appropriate range
was: 0.25 ≤𝜎≤20.
In many scientiﬁc areas, and in particular in the fault diagnosis ﬁeld, bio-inspired
algorithms have been widely used with excellent results [6, 27, 28] to solve optimiza-
tion problems. They can eﬃciently locate the neighborhood of the global optimum
in most occasions with an acceptable computational time. There is a large number of
bio-inspired algorithms, in their original and improved versions. Some examples are
Genetic Algorithm (GA), Diﬀerential Evolution (DE), Particle Swarm Optimization
(PSO) and Ant Colony Optimization (ACO) among others. In this proposal, the DE
algorithm, as described in Sect. 2.3 will be used to obtain the optimum values of the
parameters m and 𝜎due to its easy implementation and excellent outcomes.
3.2
On-Line Recognition
In this stage, a window of time with k observations and the parameter Th are estab-
lished by the experts in this stage. The value of k is related with the characteristics of
process. Th represents the percent of observations established by the experts to ana-
lyze if the observations classiﬁed as noise constitute a new class. If the observation
is classiﬁed as a good sample the KFCM algorithm identiﬁes to which of the known
classes Ci belongs the observation. However, if the observation is classiﬁed as noise
it is stored and a counter of noise observations (NO) is incremented. This procedure
is repeated until the windows of time of k observations is completed.
After the k observations were classiﬁed, the percent of them classiﬁed as noise
(NOP) is determined. If NOP < Th the noise observations are not considered and
the NO counter is restarted to begin a new cycle. If NOP > Th the NO observations
are analyzed to determine if their constitute a new class, probably representative of
a new fault or they are outliers.
To analyze the noise observations, the DOFCM algorithm is used. It is based on
the fact that the outliers are dispersed data with low density and do not form a cluster.
However, when a new fault occurs the data will be concentrated (high density) by
forming a cluster which characterize a new state.
The DOFCM algorithm is applied to the noise observations to determine based
on the density of the data if they are outliers or represent the pattern of a new class.
If the noise observations constitute a new class, the experts should identify whether
the pattern corresponds to a single fault or the pattern is the result of several single
faults by acting simultaneously (multiple fault). After identifying the pattern, it will
be stored, if correspond, in the historical database used in the training stage. Later
on, the classiﬁer should be trained again and the procedure of online recognition will
be repeated systematically.

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
111
The procedure explained for the on line stage represents a mechanism of online
detection of novel faults with automatic learning for a fault diagnosis system. It is
described in Algorithm 4.
Algorithm 4 Recognition
Input: data Xk, class centers 𝐕, rneighborhood, nmax, 𝛼, m, 𝜎.
Output: Current State.
Select k
Select Th
Initialize O counter = 0
Initialize NO counter = 0
for j = 1 to j = k do
O counter = O counter + 1
Calculate 𝜂i
neighborhood according to (2).
Calculate Mi
neighborhood according to (1).
if k ∉Coutlier then
Calculate the distances from the observation k to class centers according to (11).
Calculate the membership degree of the observation k to the c good classes according to
(14).
Determine to which class belongs the observation k using (19).
else
Store observation k in Cnoise
NO counter = NO counter + 1
end if
end for
Calculate NOP = NO counter
O counter
if NOP > Th then
Apply DOFCM algorithm for Cnoise considering only classes CNF and Coutlier
if Cnoise ∉Coutlier then
Create a new fault: CNF
Store in the historical database.
else
Delete Cnoise
NO counter = 0
end if
else
Delete Cnoise
NO counter = 0
end if
4
Illustrative Examples with the Diamond Data Set
The diamond data set is used to present three illustrative examples of the proposed
new fault diagnosis system with automatic learning using fuzzy clustering tools. This
data set presents two classes with 5 observations each one, and each class has two
variables: x1 and x2 [22, 30]. Figure 5 shows the diamond data set.

112
A. Rodríguez Ramos et al.
Fig. 5
Diamond data set
X1
-6
-4
-2
0
2
4
6
8
X2
-3
-2
-1
0
1
2
3
Fig. 6
Diamond data set
modiﬁed for the training
stage
X1
-10
-5
0
5
10
X2
-8
-6
-4
-2
0
2
4
6
8
4.1
Oﬀ-Line Training
Four new observations were added to the original data in order to represent the pos-
sible outliers. Figure 6 shows the diamond data set modiﬁed for the training stage.
To estimate the parameters mentioned in Sect. 3.1, DE algorithm was used due
to its simple structure, and robustness [6]. The control parameters in DE are the size
of the population Z, the crossover constant CR and the scaling factor FS. The values
of the parameters for the DE algorithm considering a search space 1 < m ≤2 and
0.25 ≤𝜎≤20 were: CR = 0.5, FS = 0.1 and Z = 10.
For the implementation of the DE algorithm the following stopping criteria are
considered:
∙Criterion 1: Maximum number of iterations.
∙Criterion 2: Value of the objective function.
The behavior of the objective function (PC) is shown in Fig. 7 where it can be
seen how its the value converges rapidly to one. Since the iteration number 4 the
best parameters were obtained: m = 1.0150 and 𝜎= 15.3556.

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
113
Fig. 7
Value of the
objective function (PC)
Iterations
1
2
3
4
5
6
Value of the objective function (PC)
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 8
Results of the
training stage
X1
-10
-5
0
5
10
X2
-8
-6
-4
-2
0
2
4
6
8
Results obtained by the DOFCM algorithm
class 1
class 2
outliers
class centers
X1
-6
-4
-2
0
2
4
6
8
X2
-3
-2
-1
0
1
2
3
Results obtained by the KFCM algorithm
class 1
class 2
class centers
Figure 8 shows the result of the clustering performed by the Algorithms 1 and
2 in the training stage. The values of the parameters used in these algorithms were:
Number of iterations = 100, 𝜖= 10−5 and 𝛼= 0.05.

114
A. Rodríguez Ramos et al.
4.2
On-Line Recognition
Three examples were analyzed in order to test the recognition stage proposed in this
chapter. Five observations were considered in all examples to simulate the observa-
tions sequentially obtained on-line. The numbers assigned to the added observations
indicate the order of arrival of them. In the three examples, the value of the window
of time of observations was k = 5, and a decision threshold of Th = 50% was used.
In Example 1, 5 observations are used, 4 belonging to class 1 and the other is an
outlier. The objective is to evaluate the ability of a correct classiﬁcation.
In Example 2, 2 observations belonging to class 2 and 3 outliers that did not form
a class are used. The objective is to evaluate the correct classiﬁcation and the ability
to detect that the outliers do not conform a new class.
In Example 3, 5 observations belonging to a new class are used. The objective
is to evaluate the correct classiﬁcation of outliers and the ability to detect that these
outliers conformed a new class.
Example 1 Four observations were classiﬁed in class 1 (in blue) and the other one
was classiﬁed as outlier (in black) as it is shown in Fig. 9a. In this case k = 5, NO = 1,
and NOP = 20%, then, the analyzes of the noise observations was not done. The ﬁnal
result of the on-line recognition is shown in Fig. 9b.
Example 2 Two observations were classiﬁed in class 2 (in red) and three observa-
tions were classiﬁed as outliers (in black) as is shown in Fig. 10a. In this case k = 5,
NO = 3, and NOP = 60%, then, the analyzes of the noise observations was done.
After the analysis of the three observations classiﬁed as outliers the diagnosis sys-
tem decided that they did not represent a new class. The ﬁnal result of the on-line
recognition is shown in Fig. 10b.
X1
-6
-4
-2
0
2
4
6
X2
-2
0
2
4
6
8
2
4
3
1
5
(a) Results of the on-line 
classiﬁcation after applying 
DOFCM algorithm in the 
example 1.
X1
-6
-4
-2
0
2
4
6
X2
-2
0
2
4
6
8
(b) Results after applying 
KFCM algorithm in the 
example 1.
Fig. 9
Results of the recognition stage (Example 1)

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
115
X1
-6
-4
-2
0
2
4
6
8
X2
-4
-2
0
2
4
6
8
6
8
9
7
10
(a) Results of the on-line
classiﬁcation after applying 
DOFCM algorithm in the 
example 2.
X1
-6
-4
-2
0
2
4
6
8
X2
-4
-2
0
2
4
6
8
(b) Results after applying 
KFCM algorithm and to 
analyze the outliers in the 
example 2.
Fig. 10
Results of the recognition stage (Example 2)
X1
-6
-4
-2
0
2
4
6
X2
-2
0
2
4
6
8
10
15
11
12
13
14
(a) Results after applying
DOFCM algorithm in the
example 3
X1
-6
-4
-2
0
2
4
6
X2
-2
0
2
4
6
8
10
(b) Results after applying KFCM
algorithm and to analyze the 
outliers in the example 3.
Fig. 11
Results of the recognition stage (Example 3)
Example 3 The ﬁve observations were classiﬁed as outliers (in black) as is shown in
Fig. 11a. In this case k = 5, NO = 5, and NOP = 100%, then, the analysis of the noise
observations was done. After the analyzes four observations were classiﬁed in a new
class (in purple) and the other was classiﬁed as outlier and it was not considered.
The ﬁnal result of the on-line recognition is shown in Fig. 11b.
5
Conclusions
In the present chapter a new classiﬁcation scheme for on-line detection of new
faults and automatic learning using fuzzy clustering techniques is proposed. In the
proposal, the DOFCM algorithm is used in the ﬁrst step of the training stage for

116
A. Rodríguez Ramos et al.
preprocessing the data with the objective to remove the outliers. The KFCM
algorithm is used in the second step of the training step for the data classiﬁcation
to make use of the advantages introduced by the kernel function in the separability
of the classes, in order to obtain better classiﬁcation outcomes. The algorithm DE is
used to optimize the parameters of the DOFCM and KFCM algorithms. These para-
meters are used in the on-line recognition stage, where the classiﬁer incorporates
a new fault detection algorithm. In the on-line recognition stage, the proposed new
algorithm analyzes the observations belonging to a window of time which are not
classiﬁed in the known classes and determines if they constitute a new class, prob-
ably representative of a new fault or if they are noise. If a new class is identiﬁed,
a procedure is developed to incorporate it to the base of knowledge of the classi-
ﬁer. The excellent results obtained show the feasibility of the proposal. For future
research, an interesting idea is to design a fault diagnosis system with the ability to
detect multiple new faults in the on-line recognition stage.
Acknowledgements The authors acknowledge the ﬁnancial support provided by FAPERJ, Fun-
dacão Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro; CNPq, Consehlo
Nacional de Desenvolvimento Cientíﬁco e Tecnológico; CAPES, Coordenação de Aperfeiçoamento
de Pessoal de Nível Superior, research supporting agencies from Brazil; UERJ, Universidade do
Estado do Rio de Janeiro and CUJAE, Universidad Tecnológica de La Habana José Antonio Echev-
erría.
References
1. Bedoya, C., Uribe, C., Isaza, C.: Unsupervised feature selection based on fuzzy clustering for
fault detection of the tennessee eastman process. Adv. Artif. Intell. 7637, 350–360 (2012).
Springer LNAI
2. Bernal de Lázaro, J.M., Llanes Santiago, O., Prieto Moreno, A., Knupp, D.C., Silva-Neto, A.J.:
Enhanced dynamic approach to improve the detection of small-magnitude faults. Chem. Eng.
Sci. 146, 166–179 (2016)
3. Bernal de Lázaro, J.M., Prieto Moreno, A., Llanes Santiago, O., Silva Neto, A.J.: Optimizing
kernel methods to reduce dimensionality in fault diagnosis of industrial systems. Comput. Ind.
Eng. 87, 140–149 (2015)
4. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Springer,
Plenum, New York (1981)
5. Botia, J., Isaza, C., Kempowsky, T., LeLann, M.V., Aguilar-Martín, J.: Automation based on
fuzzy clustering methods for monitoring industrial processes. Eng. Appl. Artif. Intell. 26,
1211–1220 (2013)
6. Camps Echevarría, L., Llanes Santiago, O., Silva Neto, A.J.: An approach for fault diagnosis
based on bio-inspired strategies. Stud. Comput. Intell. 284, 53–63 (2010)
7. Camps Echevarría, L., Llanes Santiago, O., Hernández Fajardo, J.A., Silva Neto, A.J., Jímenez
Sánchez, D.: A variant of the particle swarm optimization for the improvement of fault diag-
nosis in industrial systems via faults estimation. Eng. Appl. Artif. Intell. 28, 36–51 (2014)
8. Camps Echeverría, L., de Campos Velho, H.F., Becceneri, J.C., Silva Neto, A.J., Llanes San-
tiago, O.: The fault diagnosis inverse problem with ant colony optimization and ant colony
optimization with dispersion. Appl. Math. Comput. 227, 687–700 (2014)
9. Chintalapudi, K.K., Kam, M.: A noise resistant fuzzy c-means algorithm for clustering. In:
IEEE Conference Fuzzy Systems Proceedings, vol. 2, pp. 1458–1463 (1998)

A Proposal of On-Line Detection of New Faults and Automatic Learning ...
117
10. Dave, R.N.: Characterization and detection of noise in clustering. Pattern Recogn. Lett. 12,
657–664 (1991)
11. Dave, R.N., Krishnapuram, R.: Robust clustering methods: a uniﬁed view. IEEE Trans. Fuzzy
Syst. 5, 270–293 (1997)
12. Detroja, K., Gudi, R., Patwardhan, S.: A possibilistic clustering approach to novel fault detec-
tion and isolation. J. Process Control 16, 1055–1073 (2006)
13. Ding., S.X.: Model-Based Fault Diagnosis Techniques. Springer (2008)
14. Ester, M., Kriegel, H., Sander, J., Xu, X.: A density-based algorithm for discovering clusters
in large spatial databases with noise. In: Proceedings of the 2nd ACM SIGKDD pp. 226–231,
Portland, Oregon (1996)
15. Fan, J., Wang, Y.: Fault detection and diagnosis of non-linear non-gaussian dynamic processes
using kernel dynamic independent component analysis. Inf. Sci. 259, 369–379 (2014)
16. Gosain, A., Dahika, S.: Performance analysis of various fuzzy clustering algorithms: a review.
In: 7th International Conference on Communication, Computing and Virtualization 2016, vol.
79, 100–111 (2016)
17. Hu, D., Sarosh, A., Dong, Y.F.: A novel kfcm based fault diagnosis method for unknown faults
in satellite reaction wheels. ISA Trans. 51, 309–316 (2012)
18. Hwang, I., Kim, S., Kim, Y., Seah, C.: A survey of fault detection, isolation, and reconﬁguration
methods. IEEE Trans. Control Syst. Technol. 18, 636–656 (2010)
19. Jahromi, A.T., Er, M.J., Li, X., Lim, B.S.: Sequential fuzzy clustering based dynamic fuzzy
neural network for fault diagnosis and prognosis. Neurocomputing 196, 31–41 (2016)
20. Jiang, X.L., Wang, Q., He, B., Chen, S.J., Li, B.L.: Robust level set image segmentation algo-
rithm using local correntropy-based fuzzy c-means clustering with spatial constraints. Neuro-
computing 207, 22–35 (2016)
21. Kaur, P.: A density oriented fuzzy c-means clustering algorithm for recognising original cluster
shapes from noisy data. Int. J. Innovative Comput. Appl. 3, 77–87 (2011)
22. Kaur, P., Soni, A., Gosain, A.: Robust kernelized approach to clustering by incorporating new
distance measure. Eng. Appl. Artif. Intell. 26, 833–847 (2013)
23. Kesemen, O., Tezel, O., Ozkul, E.: Fuzzy c-means clustering algorithm for directional data
(fcm4dd). Expert Syst. Appl. 58, 76–82 (2016)
24. Krishnapuram, R., Keller, J.M.: A possibilistic approach to clustering. IEEE Trans. Fuzzy Syst.
1, 98–110 (1993)
25. Leski, J.M.: Fuzzy c-ordered-means clustering. Fuzzy Sets Syst. 286, 114–133 (2016)
26. Li, C., Zhou, J., Kou, P., Xiao, J.: A novel chaotic particle swarm optimization based fuzzy
clustering algorithm. Neurocomputing 83, 98–109 (2012)
27. Liu, Q., Lv, W.: The study of fault diagnosis based on particle swarm optimization algorithm.
Comput. Inf. Sci. 2, 87–91 (2009)
28. Lobato, F., Steﬀen Jr., F., Silva Neto, A.: Solution of inverse radiative transfer problems in
two-layer participating media with diﬀerential evolution. Inverse Prob. Sci. Eng. 18, 183–195
(2009)
29. Pakhira, M., Bandyopadhyay, S., Maulik, S.: Validity index for crisp and fuzzy clusters. Pattern
Recogn. 37, 487–501 (2004)
30. Pal, N.R., Pal, K., Keller, J.M., Bezdek, J.C.: A possibilistic fuzzy c-means clustering algo-
rithm. IEEE Trans. Fuzzy Syst. 13, 517–530 (2005)
31. Pang, Y.Y., Zhu, H.P., Liu, F.M.: Fault diagnosis method based on kpca and selective neural
network ensemble. Adv. Mater. Res. 915, 1272–1276 (2014)
32. Patan, K.: Artiﬁcial Neural Networks for the Modelling and Fault Diagnosis of Technical
Processes. Springer (2008)
33. Qin, S.J.: Survey on data-driven industrial process monitoring and diagnosis. Annu. Rev. Con-
trol 36(2), 220–234 (2012)
34. Ramos, A.R., Acosta, C.D., Torres, P.R., Mercado, E.S., Baez, G.B., Rifón, L., Llanes-
Santiago, O.: An approach to multiple fault diagnosis using fuzzy logic. J. Intell. Manuf. 1–11
(2016). doi:10.1007/s10845-016-1256-4

118
A. Rodríguez Ramos et al.
35. Saltos, R., Weber, R.: A rough-fuzzy approach for support vector clustering. Inf. Sci. 339,
353–368 (2016)
36. Seera, M., Lim, C.P., Loo, C.K., Singh, H.: A modiﬁed fuzzy min-max neural network for
data clustering and its application to power quality monitoring. Appl. Soft Comput. 28, 19–29
(2015)
37. Sina, S., Sadough, Z.N., Khorasani, K.: Dynamic neural network-based fault diagnosis of gas
turbine engines. Neurocomputing 125, 153–165 (2014)
38. Thong, P.H., Son, L.H.: Picture fuzzy clustering for complex data. Eng. Appl. Artif. Intell. 56,
121–130 (2016)
39. Venkatasubramanian, V., Rengaswamy, R., Kavuri, S.N.: A review of process fault detection
and diagnosis, part 1: quantitative model-based methods. Comput. Chem. Eng. 27, 293–311
(2003)
40. Venkatasubramanian, V., Rengaswamy, R., Kavuri, S.N.: A review of process fault detection
and diagnosis, part 2: qualitative models and search strategies. Comput. Chem. Eng. 27, 313–
326 (2003)
41. Wang, J., Hu, H.: Vibration-based fault diagnosis of pump using fuzzy technique. Measurement
39, 176–185 (2009)
42. Wong, K.I., Vonga, C.M., Wong, P.K.: Simultaneous-fault detection based on qualitative symp-
tom descriptions for automotive engine diagnosis. Appl. Soft Comput. 22, 238–248 (2014)
43. Wu, K., Yang, M.: A cluster validity index for fuzzy clustering. Pattern Recogn. 26, 1275–1291
(2005)
44. Zhang, L., Lu, W., Liu, X., Pedrycz, W., Zhong, C.: Fuzzy c-means clustering of incomplete
data based on probabilistic information granules of missing values. Knowl.-Based Syst. 99,
51–70 (2016)
45. Zhang, D.Q., Chen, S.C.: Clustering incomplete data using kernel based fuzzy c-means algo-
rithm. Neural Process Lett. 18, 155–162 (2003)
46. Zhang, D.Q., Chen, S.C.: A novel kernelized fuzzy c-means algorithm with application in
medical image segmentation. Artif. Intell. Med. 32, 37–50 (2004)

Fuzzy Portfolio Selection Models
for Dealing with Investor’s Preferences
Clara Calvo, Carlos Ivorra and Vicente Liern
Abstract This chapter provides an overview of the authors’ previous work about
dealing with investor’s preferences in the portfolio selection problem. We propose a
fuzzy model for dealing with the vagueness of investor preferences on the expected
return and the assumed risk, and then we consider several modiﬁcations to include
additional constraints and goals.
1
Introduction
H.M. Markowitz won the 1990 Nobel Price for his work in the foundation of modern
portfolio theory (MPT) [27, 28], which has become a main tool in portfolio manage-
ment as well as in other economic theories, such as asset pricing [33]. MPT is a deep
theory which can hardly be described in a few words (see [11] for a comprehensive
account), but, roughly speaking, it aims to determine the best portfolio we can form
from a given set of possible assets on the basis of two characteristics. The ﬁrst one is
the expected return. In order to measure it, the return of each asset is considered as
a random variable and the expected return is often measured by its mean, which in
practice is estimated by the arithmetical mean of the historical returns. The expected
return of a portfolio is deﬁned as the weighted sum of the expected returns of its
assets.
Here we should face the critical question: to what extent can we trust that the
future return of a portfolio will be similar to the expected return calculated from
its past returns? This leads to the second characteristic to be considered in order to
select a portfolio to invest in: the risk. It tries to estimate the diﬀerence between the
expected return and the real future return of a portfolio.
C. Calvo ⋅C. Ivorra ⋅V. Liern (✉)
Universidad de Valencia, Valencia, Spain
e-mail: vicente.liern@uv.es
C. Calvo
e-mail: clara.calvo@uv.es
C. Ivorra
e-mail: carlos.ivorra@uv.es
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_7
119

120
C. Calvo et al.
Whereas the theoretical relevance of MPT is not questioned, several criticisms
about its real world applicability have arisen [38]. However there are also renowned
specialists supporting it [18, 30, 33]. Assuming that the expected return and an
adequate measure of the risk are reliable, MPT establishes that a rational investor
should select an eﬃcient portfolio, i.e., a portfolio providing the least possible risk
for a given expected return or, what is essentially the same, providing the greatest
expected return for a maximum allowable risk.
The original (classical) Markowitz model is also called the mean-variance model
since it takes as indicators of the expected return and the risk of a portfolio the mean
and the quadratic form associated to the variance-covariance matrix of the returns of
the assets, respectively, which in practice are estimated from the historical data by
standard statistical techniques assuming that they are normally distributed.
However, several alternative ways for measuring the risk of a portfolio have been
proposed. Value at Risk (VaR) is widely used (see [36] for a discussion of this con-
cept, or also [16]). Konno and Yamazaki [21] propose a linear model which dra-
matically simpliﬁes the computational aspects of the portfolio selection problem.
Some other possibilities arose from the fact that many risk measures become high
when there is a high probability that the return will be far from the expected return,
but they do not distinguish whether the diﬀerence is positive (higher return than
expected) or negative (less return than expected). Since aversion to having more
beneﬁts than expected is questionable, some asymmetric measures for the so-called
downside risk that take into account only the risk of having less return than expected,
have been proposed. The ﬁrst downside risk measure appeared in Roy’s “safety-ﬁrst”
model [31, 32]. See also [8, 9, 25, 26, 35]. Other asymmetric measures of risk take
into account higher statistical moments: skewness, kurtosis [15, 17, 20]. For more
advanced models taking into account the dynamics of the variance see for instance
[10].
2
The Classical Portfolio Selection Problem
Thus, the original Markowitz Portfolio Selection Model is formulated as
Min. 𝐱𝐭𝐕𝐱
s.a 𝐞t𝐱≥r
𝟏t𝐱= 1
𝐱≥0
(1)
where the vector 𝐱contains the weights of the assets in the portfolio (i.e. the propor-
tion of each asset in the total invested fund), 𝐞is the vector of expected returns, mea-
sured by the means of the historical data and 𝐕is the variance-covariance matrix of
such data, so that R estimates the risk of the portfolio and r is the minimum expected
return speciﬁed by the investor.

Fuzzy Portfolio Selection Models for Dealing ...
121
Alternatively, a dual form of the problem consists of maximizing the expected
return and imposing a maximum admissible risk:
Max. 𝐞t𝐱
s.a 𝐱t𝐕𝐱≤R
𝟏t𝐱= 1
𝐱≥0
(2)
In fact, the portfolio selection problem is better understood as a bi-objective prob-
lem aiming both maximizing the expected return and minimizing the risk. The min-
imizing formulation is the most widely used in the literature, mainly because by
being a quadratic problem it is more easily handled from a mathematical point of
view. However, we will also deal with (2) since it is more realistic to ask an investor
what risk he considers acceptable rather than forcing him to ﬁx a minimum return
without having any reference about the risk it carries. In fact, it is the usual practice
for small investors [see for instance (http://www.santander.com)].
The original Markowitz Portfolio Selection Model included just linear constraints,
mainly because computers could not handle more diﬃcult instances. However,
nowadays the available computational power is much greater and hence more sophis-
ticated models can be dealt with, looking for eﬃcient portfolios satisfying also
additional constraints. There are many contexts in which such constraints become
necessary. Some of them are related to the mutual fund management. Fund man-
agers must comply contractual requirements determined by the prospectus as well
as legal requirements, such as the 5–10–40-constraint imposed by the §60(1) of the
German investment law [2], which establishes that securities of the same issuer are
allowed to amount to up to 5% of the net asset value of the mutual fund, but they are
allowed to 10% if the total of all of these assets is less than 40% of the net asset value.
It is also usual to include buy-in thresholds to reduce transaction costs. This means
not allowing the stocks of a mutual fund in a given asset to be less than a certain
amount. A third typical example is that managers often impose upper bounds to the
total number of assets in a mutual fund also to reduce transaction costs, as well as
lower bounds in order to diversiﬁcate the investment. See [13] for the computational
aspects associated to these additional constraints. This leads to the model
Min. 𝐱t𝐕𝐱
s.t. 𝐞t𝐱≥r
𝟏t𝐱= 1
𝐥≤𝐱≤𝐮,
(3)
where 𝐥and 𝐮are the vectors of lower and upper bounds for each weight. However,
in many cases the investor does not really wish to force each asset to have a min-
imum weight in the portfolio but, in order to avoid an artiﬁcially imposed excess
of diversiﬁcation in the optimal portfolio, he may just wish to require a minimum

122
C. Calvo et al.
weight only for those assets actually appearing in it. This leads to the incorporation
of semicontinuous variable constraints into the model, which means that each vari-
able xi is allowed either to be 0 or to vary in the rank [li, ui]. Such constraints can
be expressed with the help of binary variables yi taking the value 1 if the i-th asset
appears in the portfolio and 0 otherwise. The resulting model is:
Min. R = 𝐱t𝐕𝐱
s.t. 𝐞t𝐱≥r
𝟏t𝐱= 1
liyi ≤xi ≤uiyi, 1 ≤i ≤n,
yi ∈{0, 1}.
(4)
In any portfolio selection problem, the set of optimal pairs (r, R) consisting of the
minimal risk R providing a given expected return r, or, equivalently, the maximum
expected return provided by a portfolio that does not exceed a maximum level of
risk R, is known as the eﬃcient frontier of the problem. In the simplest case where
even the sign constraints are removed, it consists just of a branch of parabola [6, 11].
However, in the last decades, computation techniques have been developed to solve
large and more sophisticated instances of the portfolio selection problem including
many diﬀerent kinds of constraints, making it more realistic (see [12, 37]).
3
A Fuzzy Formulation of the Portfolio Selection
Problem
Obviously, the portfolio selection, like most ﬁnancial problems, is related with uncer-
tainty because it consists of taking a decision about future events. Moreover, it is
not easy to model the investor’s preferences. After the seminal work by Markowitz,
attention has been given to the study of alternative models [22, 25] which try to deal
more eﬃciently with the uncertainty of the data. Most of these models are based on
probability distributions, which are used to characterize risk and return. However,
another way of dealing with uncertainty is to work with models based on soft com-
puting. Watada [41] solves this problem by using imprecise aspiration levels for an
expected biobjective approach, where the membership functions of the goals are of
a logistic-type. In 2000, Tanaka et. al. [39] propose using possibility distributions
to model uncertainty on the expected returns and to incorporate the knowledge of
ﬁnancial experts by means of a possibility degree of similarity between the future
state of ﬁnancial markets and the state in previous periods [14]. Multiobjective pro-
gramming has also been used to design fuzzy models of portfolio selection, either
for compromise solutions [29] or by introducing multi-indices [1]. Speciﬁc meth-
ods have even been proposed for dealing with the unfeasibility provoked by conﬂict
between the expected return and the investor’s diversiﬁcation requirements [23, 24].

Fuzzy Portfolio Selection Models for Dealing ...
123
However, in this section we will consider a very diﬀerent class of vagueness
related to the portfolio problem, namely the vagueness of the invertor’s criteria for
selecting a satisfactory trade oﬀbetween the risk he considers acceptable and the
return he wishes to obtain. In other words, the investor must choose a point at the
eﬃcient frontier of the problem. From a theoretical point of view, the investor’s pref-
erences are usually formalized by means of utility functions, so that the ﬁnal choice
is that eﬃcient portfolio maximizing a given utility function, but when we try to
reﬂect the preferences of a real speciﬁc investor we must ask him directly for a point
in the eﬃcient frontier. Nevertheless, it is obvious that the investor’s preferences are
essentially vague, so that it is unnatural to force him to choose a speciﬁc point. In
practice, he could only determine a zone or a fuzzy point on it.
This leads to the fuzzy model proposed by the authors in [5]. The main idea is to
consider partially feasible solutions involving slightly greater risk than that ﬁxed by
the decision-maker, and to study the possibilities that they oﬀer in order to improve
the expected return.
A fuzzy set ̃S, of partially feasible solutions, is deﬁned so that the membership
degree of a given portfolio depends on how much its risk exceeds the risk R0 ﬁxed by
the investor. On the other hand, a second fuzzy set ̃G is deﬁned, whose membership
function reﬂects the improvement on the return provided by a partially feasible solu-
tion with respect to the optimal crisp return z∗. In practice, we consider piecewise
linear membership functions
𝜇̃S(x) =
⎧
⎪
⎨
⎪⎩
1
if r ≤R0,
1 −r−R0
pf
if R0 < r < R0 + pf ,
0
if r ≥R0 + pf ,
𝜇̃G(x) =
⎧
⎪
⎨
⎪⎩
0
if z ≤z∗,
z−z∗
pg
if z∗< z < z∗+ pg,
1
if z ≥z∗+ pg,
where r and z are the risk and the return provided by the portfolio x (which is assumed
to satisfy the constraints of (MV), except the second one); the parameter pf is the
maximum increment in the risk that the decision-maker can accept, and pg is the
increment on the return that the decision-maker would consider completely satisfac-
tory. From this, we can deﬁne a global degree of satisfaction
𝜆(x) = min{𝜇̃G(x), 𝜇̃S(x)},
which is the membership degree for the fuzzy intersection of ̃S ∩̃G. The fuzzy port-
folio model becomes

124
C. Calvo et al.
Table 1
Returns on ﬁve assets
Year
AmT
ATT
USS
GM
ATS
1937
−0.305
−0.173
−0.318
−0.477
−0.457
1938
0.513
0.098
0.285
0.714
0.107
1940
0.055
0.2
−0.047
0.165
−0.424
1941
−0.126
0.03
0.104
−0.043
−0.189
1942
−0.003
0.067
−0.039
0.476
0.865
1943
0.428
0.3
0.149
0.225
0.313
1944
0.192
0.103
0.26
0.29
0.637
1945
0.446
0.216
0.419
0.216
0.373
1946
−0.088
−0.046
−0.078
−0.272
−0.037
(FMV) Max. 𝜆(x)
s.t. x ∈̃S.
(5)
In [3] exact and heuristic procedures for solving this problem are described. In
order to illustrate the main idea on which the model is based we consider ﬁve assets
from the historical data introduced by Markowitz [28]. Table 1 shows the returns
of American Tobacco, AT&T, United States Steel, General Motors and Atcheson &
Topeka & Santa Fe.
We have ﬁxed a risk level R = 0.03. The optimal crisp portfolio is formed by
assets AmT, ATT, GM, ATS and provides an optimal return z∗= 0.103926. For the
fuzzy model, we have ﬁxed pf = 0.02, pg = 0.02. By explicitly solving the Kuhn-
Tucker conditions associated to the model, we can calculate the optimal return for a
given risk R, which happens to be
F(R) = −0.02355 + 52.6832R + 2.6136
√
−0.77841 + 52.6832R
9.09494 × 10−13R + 33.84051
√
−0.77841 + 52.6832R
Computations are valid for risks in the interval I = [0.025826, 0.083341]. The
functions 𝜇f (R) and 𝜇g(R) are shown in Fig. 1. They intersect at R∗= 0.041381, cor-
responding to 𝜆= 0.430977. The return on the fuzzy portfolio is 0.112545, whereas
the crisp return was 0.103926.
We observe that the global degree of satisfaction is low. This means that the risk
is increased much more than the return of the asset. The higher value of 𝜆, the more
preferable the alternative fuzzy portfolio is. High fuzzy satisfaction levels are more
usual when additional constraints are considered making the eﬃcient frontier more
irregular, as we will see in the next section.

Fuzzy Portfolio Selection Models for Dealing ...
125
Fig. 1
Membership
functions for the degree
feasibility and degree of
improvement on the
expected return
0.2
0.4
0.6
0.8
1
RISK
SATISFACTION LEVEL
0.02
0.04
0.06
0.08
µf (R)
µg(R)
Crisp risk
4
Portfolio Selection with Semi-continuous Variable
and Cardinality Constraints
As we have already noticed, real world investments require incorporating many addi-
tional constraints into the portfolio selection model, many of which can be expressed
as mathematically simple linear constraints, but some others are more complex from
a mathematical point of view since they transform the model into a mixed integer
one. To illustrate this fact we will consider semicontinuous variable and cardinality
constraints, although any set of linear constraints could be considered in addition.
Hence, our starting point is now the model (4).
Let us call X the set deﬁned by the constraints imposed on the problem when the
minimum return constraint is relaxed, which will be handled separately. The con-
straint and goal set will be fuzzy subsets of the (crisp) universe set X. The fuzzy
constraint set ̃C must be such that the value 𝜇̃C(𝐱) is high when the expected return
on the portfolio 𝐱∈X is not much less than r0 and the risk is not much greater than
R0. This means that ̃C can be deﬁned as the fuzzy intersection of two fuzzy sets
̃Cr and ̃CR, such that the degree of membership of each portfolio 𝐱∈X is given by
𝜇̃Cr(𝐱) ∶= f1(r(𝐱)) and 𝜇̃CR(𝐱) ∶= g1(R(𝐱)), where r(𝐱) and R(𝐱) are the expected
return and risk of the portfolio 𝐱, f1 ∶IR ⟶[0, 1] is a non-decreasing function
such that f1(r0) = 1 and g1 ∶IR ⟶[0, 1] is a non-increasing function such that
g1(R0) = 1. The speciﬁc choice of f1 and g1 will depend on the available information
about the investor’s preferences regarding risk and return. Hence the membership
function to the fuzzy feasible set ̃C ∶= ̃Cr ∩̃CR is given by the membership function
𝜇̃C(𝐱) ∶= min{𝜇̃Cr(𝐱), 𝜇̃CR(𝐱)}, which is of the form 𝜇̃C(𝐱) = h1(r(𝐱), R(𝐱)), where
h1(r, R) ∶= min{f1(r), g1(R)} generally has the shape shown in Fig. 2a.
On the other hand, the degree of membership of the goal set ̃G of the fuzzy
problem must be high for portfolios whose expected return is much greater than
r0 or the risk is much less than R0. Hence, ̃G is the fuzzy union of the fuzzy sets
̃Gr and ̃GR whose membership functions are of the form 𝜇̃Gr(𝐱) ∶= f2(r(𝐱)) and
𝜇̃GR(𝐱) ∶= g2(R(𝐱)), where f2 ∶IR ⟶[0, 1] is a non-decreasing function such that

126
C. Calvo et al.
R
R
r0
R0
R0
0
r
1
0
1
(b)
(a)
Fig. 2
Membership functions to ̃C and ̃G
Fig. 3
Degree of global
satisfaction and the eﬃcient
frontier. The surface
represents the degree of
global satisfaction for each
pair (R, r) of expected risk
and desired return. The
dotted line is the eﬃcient
frontier and the continuous
line is its lifting to the graph
R0
R
r
(r0,R0)
f2(r0) = 0 and g2 ∶IR ⟶[0, 1] is a non-increasing function such that g2(R0) = 0.
Notice that in this case ̃G is a fuzzy union and not a fuzzy intersection, since improv-
ing the crisp optimal portfolio means improving the risk or improving the expected
return, but both cases cannot occur simultaneously. The fuzzy intersection would be
the empty set.
Thus, the membership function of the fuzzy goal set ̃G = ̃Gr ∪̃GR (which can be
called the degree of improvement of the goal) is given by
𝜇̃G(𝐱) ∶= max{𝜇̃Gr(𝐱), 𝜇̃GR(𝐱)} = h2(r(𝐱), R(𝐱)),
where h2(r, R) ∶= max{f2(r), g2(R)} has the shape shown in Fig. 2b.
Now, following Delgado et al. [7], we consider the fuzzy decision set of our prob-
lem, deﬁned as the fuzzy intersection ̃D ∶= ̃C ∩̃G, which has the shape shown in
Fig. 3. The degree of membership of a portfolio 𝐱to ̃D is called its degree of global
satisfaction: 𝜆(𝐱) ∶= min{𝜇̃C(𝐱), 𝜇̃G(𝐱)}. In Fig. 3 we have represented a possible
(simpliﬁed) eﬃcient frontier of the crisp problem (SCP) and the pair (r0, R0) chosen
by the investor as the starting point for the fuzzy model. We see that its degree of

Fuzzy Portfolio Selection Models for Dealing ...
127
feasibility is 1 but its degree of improvement of the goal is 0, and so the degree of
global satisfaction is 0. We can also see the lifting of the eﬃcient frontier to the graph
of the degree of global satisfaction. In order to choose a speciﬁc solution from the
fuzzy decision set ̃D we maximize its degree of global satisfaction, i.e. we ﬁnd the
optimal fuzzy portfolio by solving the program:
(FSC) Max. 𝜆(𝐱)
s.t. 𝐱∈X
In Fig. 3 we can see that the degree of global satisfaction has two local maxima
on the eﬃcient frontier, the best of which is the optimal solution of the fuzzy model
we are introducing.
The problem of choosing membership functions suitable for modelling a real
uncertain situation is a very subtle issue in fuzzy set theory. Here, in the absence
of speciﬁc preferences, we will consider the simplest case. Notice that we intend
to compare possible variations of the expected return with possible variations of
the risk, and what is really comparable with a variation of the expected return is
not a variation of its variance but a variation of its typical deviation. The diﬀerence
between the variance and the typical deviation is just a square root, which is irrel-
evant when minimizing the risk, but it must be incorporated into our membership
functions. In other words, the natural way to express the investor’s preferences on
the trade-oﬀbetween variations in the expected return and variations in the risk is in
terms of the mean and the typical deviation instead of the mean and the variance. In
the absence of more speciﬁc criteria, we will assume a piecewise linear dependence
on r and
√
R; namely, we take
f1(r) ∶=
⎧
⎪
⎨
⎪⎩
0
if r < r0 −pf1,
1 −r0−r
pf1
if r0 −pf1 ≤r ≤r0,
1
if r0 < r,
g1(R) ∶=
⎧
⎪
⎨
⎪⎩
1
if
√
R<
√
R0,
1 −
√
R−
√
R0
pg1
if
√
R0≤
√
R≤
√
R0+pg1 ,
0
if
√
R>
√
R0+pg1 ,
f2(r) ∶=
⎧
⎪
⎨
⎪⎩
0
if r < r0,
r−r0
pf2
if r0 ≤r ≤r0 + pf2,
1
if r0 + pf2 < r,
g2(R) ∶=
⎧
⎪
⎨
⎪⎩
1
if
√
R<
√
R0−pg2 ,
√
R0−
√
R
pg2
if
√
R0−pg2 ≤
√
R≤
√
R0,
0
if
√
R0<
√
R.
In [5] we show how to handle this fuzzy model. As an illustration, we con-
sider the same data set considered in the previous section, but now we incorpo-
rate semicontinuous variables with vectors of bounds 𝐥= (0.2, 0.3, 0.2, 0.3, 0.2) and
𝐮= (0.6, 0.6, 0.6, 0.6, 0.6), as well as a cardinality constraint with m = 2 and M = 5.
Let us consider an investor that has chosen an expected return r0 = 0.125, whose
corresponding risk is R0 = 0.0742. In order to interpret this variance, we will calcu-
late the standard deviation
√
R0 = 0.272. We can consider the later as a quite high
risk, and so we assume that the investor would be interested in reducing it. In this
sense, a reduction of pg2 = 0.06 would be considered as totally satisfactory. On the
other hand, an increment greater than pg1 = 0.01 would not be acceptable in any
case. We assume that the investor would accept variations on the expected return
with tolerances pf1 = pf2 = 0.02.

128
C. Calvo et al.
0.11
0.12
0.13
0.14
0.2
0.4
0.6
0.8
1.0
0.22
0.24
0.26
0.28
0.11
0.12
0.13
0.14
r
r
r0
R
R0
Fig. 4
Left Membership function to the feasible set and to the goal set as functions of the expected
return. Right The eﬃcient frontier around the crisp expected return
Table 2
Comparison between the crisp and the fuzzy solutions of Example 1
x1
x2
x3
x4
x5
r
√
R
Crisp
0.31486
0
0.2
0.486
0
0.125
0.273
FzL
0.244
0.354
0
0.40
0
0.1192
0.23
FzR
0.5
0
0
0.3
0.2
0.1313
0.281
𝜆
𝛥r
𝛥R
Fuzzy left
0.71
-0.0058
-0.013
Fuzzy right
0.12
0.0063
0.008
Figure 4 left shows the degrees of membership to the feasible and the goal sets
as functions of the expected return on a given eﬃcient portfolio. We can compare it
with the piece of the eﬃcient frontier around r0 within the tolerance levels, which is
shown at the right.
There we can see that near r0 there are two horizontal jumps below and a vertical
one above. In Fig. 4 we have highlighted the two local maxima of the degree of
global satisfaction. Speciﬁcally, they correspond to the eﬃcient portfolios described
in Table 2, which also includes the crisp eﬃcient portfolio. Both in the ﬁgure and in
the table we can see that the fuzzy optimal solution is the left-hand one with a degree
of global satisfaction 𝜆= 0.71. Notice that the three portfolios shown in Table 2 have
diﬀerent compositions.

Fuzzy Portfolio Selection Models for Dealing ...
129
In Table 2 we can also appreciate the interest of the fuzzy alternative: by changing
from the crisp portfolio to the fuzzy one, we reduce the risk of the investment by a
bit more than 1% at the cost of reducing the expected return by just 0.0058. The sig-
niﬁcantly lower degree of global satisfaction of the right fuzzy solution is reasonable
since the increment on the expected return is far less than the increment on the risk.
5
Portfolio Selection with Non-ﬁnancial Goals
The Social Investment Forum in its new 2012 Trends Report in US [40] ﬁnds that
11.23% of all assets under professional management in the United States at the end
of 2011 applied various environmental social, governance and ethical criteria in
their investment analysis. Investors practicing Socially Responsible Investment (SRI)
strategies held $3.74 trillion out of $33.3 trillion of investment assets. This represents
an increase of 22% since 2009 and reﬂects the “growing investors’ interest in con-
sidering environmental, community, other societal or corporate governance (ESG)
issues to reﬁne how they make decisions as they select and manage their portfolios
or raise their voices as shareholders” [40].
This growth of SRI strategies all around the world has stimulated in turn the rise of
many entities working in the rating of assets with regard to their social responsibility.
This poses two mathematical problems: how to evaluate assets’ social responsibility
which is by its nature a vague an imprecise concept and how to aggregate in a ﬁnal
rating a great amount of relevant but imprecise information about ﬁrms and/or funds.
Nevertheless, and although investors could be provided with highly processed
non-ﬁnancial information from the experts, in order to select a portfolio, they must
elicit their preferences. The simplest way would be to restrict the feasible set of
investments to those being “acceptable” for the investor from a SRI point of view.
However this would mean to completely subordinate the ﬁnancial goals to the non-
ﬁnancial ones and in fact, in practice most of the SRI assets ﬁrst apply ﬁnancial
screens and then social screens. This clearly reﬂects that actually most of socially
responsible investors consider SRI as a secondary goal with regard to maximizing
the ﬁnancial return and minimizing the ﬁnancial risk.
In this section a fuzzy portfolio selection problem is proposed in which a sec-
ondary goal besides the ﬁnancial ones is considered in such a way that no poten-
tially interesting solution with regard to the risk and the return is discarded by the
constraints. Speciﬁcally, the constraints of the model do not mention the secondary
goal, which appears just in the objective function, in such a way that all the feasi-
ble portfolios within given ranges of risk and return are taken into consideration.
Thus, the investor can be aware of what is being exactly missed as a result of the
improvement of the additional non-ﬁnancial goal.
Our starting point is again the model (4), and hence each portfolio is determined
by a pair (𝐱, 𝐲) of weights and binary variables. We measure the Social Responsibility
of a portfolio as the degree of membership of a fuzzy set ̃S, say 𝜇̃S(𝐱, 𝐲). See [4] for
a way of deﬁning such a fuzzy set.

130
C. Calvo et al.
Now we take as fuzzy feasible set the fuzzy subset of the set of all portfo-
lios satisfying the hard constraints of (4) (i.e., all but the ﬁrst one), deﬁned as
̃C = ̃Cr ∩̃CR, where the membership functions of the fuzzy sets ̃Cr and ̃CR are given
by:
𝜇̃Cr(x, y) =
⎧
⎪
⎨
⎪⎩
1
if r ≥r0,
r−r0+sr
sr
if r0 −sr < r < r0,
0
if r ≤r0 −sr,
𝜇̃CR(x, y) =
⎧
⎪
⎨
⎪⎩
1
if R ≤R0,
R0+sR−R
sR
if R0 < R < R0 + sR,
0
if R ≥R0 + sR,
where r y R are respectively the expected return and the risk of the portfolio (x, y)
and the values r0, R0, sr and sR are determined from the investor’s preferences. This
means that r0 and R0 are an expected return and a risk that the investor considers
as completely acceptable, but he would accept worse values until reaching the toler-
ances sr and sR if this provides better results for the secondary goal.
Next we deﬁne a fuzzy goal set ̃G from two auxiliary fuzzy sets ̃E and ̃S, the ﬁrst
one deﬁning the “eﬃcient enough” portfolios and the second one deﬁning the “good
enough” ones with regard to the secondary goal (always according to the investor’s
preferences). The set ̃E will express what we are loosing by accepting a non-eﬃcient
portfolio, and so eﬃcient portfolios will be now the totally eﬃcient portfolios, i.e.
those having degree of membership of ̃E equal to 1.
First we deﬁne eﬃciency with regard to the expected return and then, the eﬃ-
ciency with regard to the risk by means of two fuzzy sets ̃Er and ̃ER. The membership
of ̃Er is:
𝜇̃Er(x, y) =
{
1 −ref(R)−r
tr
if r ≥ref (R) −tr,
0
otherwise,
where tr is a tolerance determined from the investor’s preferences and ref(R) is the
eﬃcient expected return corresponding to the risk R of the portfolio (x, y). This
means that the degree of eﬃciency with regard to the expected return reaches the
value 0 when the diﬀerence between the expected return r of the portfolio and ref(R)
exceeds a tolerance ﬁxed by the investor.
Analogously, we deﬁne the membership function of ̃ER as
𝜇̃ER(x, y) =
{
1 −R−Ref(r)
tR
if R ≤Ref (r) + tR,
0
otherwise,
which means that the degree of eﬃciency of a portfolio with regard to the risk is 1 for
eﬃcient portfolios and reaches the value 0 when the diﬀerence between the risk R of
the portfolio and the eﬃcient risk Ref(r) for its return r exceeds a given tolerance tR.

Fuzzy Portfolio Selection Models for Dealing ...
131
Now we deﬁne ̃E = ̃Er ∩̃ER, where the membership function of the fuzzy inter-
section is deﬁned as the minimum of the previously deﬁned membership functions.
Hence the set ̃E allows us to speak about partially eﬃcient portfolios in such a way
that eﬃcient portfolios in the usual sense are now the totally eﬃcient ones, but a
portfolio close enough to the eﬃcient frontier is considered as “almost eﬃcient” in
the fuzzy sense.
Finally, we deﬁne our fuzzy goal set ̃G by means of the membership function as
a weighted sum
𝜇̃G(x, y) = w𝜇̃S(x, y) + (1 −w)𝜇̃E(x, y),
where the weight w expresses the importance of the secondary goal for the investor
with regard to eﬃciency. So, a high value for w means that the investor is willing
to go relatively far from the eﬃcient frontier in order to obtain higher values of 𝜇̃S,
whereas a small value of w means that the investor wishes to stay near the eﬃcient
frontier. In any case, recall we have deﬁned the feasible set in such a way that only
good enough solutions with regard to the ﬁnancial goals are under consideration, and
so the ﬁnancial goals are always the main goals of the problem. More speciﬁcally,
a large value for w means that, among the acceptable solutions with regard to the
ﬁnancial goals, those best with regard to ̃S are preferred, and only for similar values
with regard to ̃S the degree of eﬃciency becomes relevant.
All in all, the degree of membership of the decision set is given by
𝜇̃D(x, y) = min{𝜇̃C(x, y), w𝜇̃S(x, y) + (1 −w)𝜇̃E(x, y)}
and the fuzzy problem (6) is the problem determined by this decision set, whose
optimal solutions are those with maximum degree of membership of ̃D:
Max.min{𝜇̃C(x, y), w𝜇̃S(x, y) + (1 −w)𝜇̃E(x, y)}
s.t.1x = 1
m ≤
∑
i yi ≤M
liyi ≤xi ≤uiyi,
i = 1, … , n
xi ≥0, yi ∈{0, 1}
i = 1, … , n
(6)
In order to illustrate this model, we consider the 10 mutual funds listed in Table 3.
The ﬁrst ﬁve have positive SRI degree, whereas the last ﬁve are conventional funds
with null SRI degree. The variance-covariance matrix and the vector of expected
returns are calculated from the weekly data from 31-12-2006 to 31-12-2007 provided
by Morningstar Ltd. Assume we wish to select a portfolio consisting of a minimum
of 3 and of a maximum of 6 funds in such a way that each non-zero weight is at
least 0.05. As upper bounds for the weights, we ﬁx 0.25 for the ﬁrst ﬁve (the socially
responsible ones) and 0.15 for the conventional ones. These weights allow up to a
75% of conventional funds and up to a 100% of socially responsible funds in each
feasible portfolio.

132
C. Calvo et al.
Table 3
Selected funds
#
Name
#
Name
F1
Calvert Large Cap
Growth A
F6
BlackRock Index
Equity Inv A
F2
Calvert Social
Investment Equity A
F7
Dreyfus Appreciation
F3
Domini Social Equity
Inv
F8
JPMorgan Equity
Index Select
F4
Green Century Equity
F9
Legg Mason Cap
Mgmt All Cap B
F5
MMA Praxis Core
Stock A
F10
MFS Blended Res.
Core Equity A
By observing the eﬃcient frontier, the investor can choose the zone of the plane
risk-return he is interested in. Formally, this means to determine the fuzzy set ̃C. For
this, we ﬁx (r0, R0) = (0.26, 1.98) with tolerances (sr, sR) = (0.01, 0.02).
To determine an instance of the problem (6), we need to ﬁx the weight w for the
social responsibility degree in the goal function. Let us set a quite high value, namely
w = 0.8 to favor those portfolios being quite far from the eﬃcient frontier if they are
good with regard to SRI.
The optimal solution of (6) is the portfolio N1 in Table 4, whose degree of mem-
bership of the decision set is 0.6262. With this solution, the investor gets an expected
return r = 0.258, with a risk R = 1.98 and a social responsibility degree s = 0.3808.
It is interesting to compare this optimal solution with other alternatives, and therefore
Table 4 contains the six best portfolios that are optimal with regard to the portfolios
with the same composition. Notice that this does not mean that portfolio N2 is the
second best solution of (FP), since there are inﬁnitely many portfolios near to N1
that are better than N2. What we can say is that, if we look for a portfolio with a
composition diﬀerent from that of N1, the best possibility is N2, and so on.
Figure 5 shows the position of the portfolios appearing in Table 4 in the risk-return
plane. We see that N2 is completely eﬃcient. When compared to N1, it has a similar
expected return, a substantially better risk, but a signiﬁcantly lower social responsi-
bility degree. By contrast, portfolio N3 is again a good solution with regard to social
responsibility (it has the second best SRD), but it is worse than N1 because of its
SRD, and worse than N2 because of its signiﬁcantly lower degree of eﬃciency.
In general, when applying a heuristical procedure for solving a larger instance
of (6), it is useful to save not only the best portfolio along the search process, but
the best portfolio found for each composition. Hence, in the end we can present the
investor not only the optimal portfolio, but a list of alternatives for diﬀerent compo-
sitions. These alternatives are ordered a priori according to his own preferences. In
this way the investor is given a last chance to decide which portfolio suits better his
preferences with regard to the trade oﬀbetween risk, return and social responsibility.

Fuzzy Portfolio Selection Models for Dealing ...
133
Table 4
The six best solutions for diﬀerent portfolio compositions
Portfolio
𝜇̃D
SRD
Return
Risk
N1
0.23187
0.25
0.218134
0
0
0
0
0
0.1
0.15
0.6262
0.3808
0.268
1.98
N2
0.20585
0.25
0
0.25
0
0
0
0
0.14415
0.15
0.6250
0.2922
0.264
1.931
N3
0.25
0.25
0.15
0.05
0
0
0
0
0.15
0.15
0.6185
0.3476
0.272
1.973
N4
0.16669
0.25
0
0.23331
0
0.05
0
0
0.15
0.15
0.5913
0.2690
0.26
1.924
N5
0.16803
0.25
0
0.23197
0
0
0.05
0
0.15
0.15
0.5818
0.2688
0.26
1.926
N6
0.25
0.25
0.15
0
0
0.05
0
0
0.15
0.15
0.5803
0.3207
0.273
1.978

134
C. Calvo et al.
1
2
3
45
6
1.92
1.94
1.96
1.98
2.00
Risk
0.255
0.260
0.265
0.270
0.275
Return
Fig. 5
Location on the plane risk-return of the best solutions
With this proposal, the investor knows exactly what he is missing with respect to
the ﬁnancial goals by accepting the solution of (6), and if he considers the ﬁnancial
cost excessive, he has the possibility of choosing a more conservative alternative
among the proposed list or even solving again (FP) with a lower value for the weight
of the social responsibility degree.
6
Conclusion
In this chapter we have seen how fuzzy techniques can be applied to the portfolio
selection problem in order to deal with diﬀerent issues related to the subjectivity
of the investor’s preferences: on one hand, the integrality constraints considered in
Sect. 4 make the problem very sensitive to small changes of the risk and return pref-
erences, and our proposed model look for the best solution taking into account that
those preferences are soft ones and, hence, the investor will accept slight variations
if they provide a reasonable improvement of the solution. On the other hand, when
considering non-ﬁnancial goals as in Sect. 5, our model provides a precise way of
prioritizing the ﬁnancial behavior of the selected portfolio without disregarding its
non-ﬁnancial properties. Of course, it would not be reasonable to expect that a single
model would be suitable for reﬂecting the preferences of every investor (even if it
has some adjustable parameters to this end), and hence any other investor’s proﬁles
will need essentially diﬀerent models involving new ideas, and this leaves a rich ﬁeld
for future research.
References
1. Arenas Parra, M., Bilbao Terol, A., Rodríguez Uría, M.V.: A fuzzy goal programming approach
to portfolio selection. Eur. J. Oper. Res. 133, 287–297 (2001)
2. Bundesgesetzblatt: Teil I Nr 62 (2003)

Fuzzy Portfolio Selection Models for Dealing ...
135
3. Cadenas, J.M., Carrillo, J.V., Garrido, M.C., Ivorra, C., Liern, V.: Exact and heuristic proce-
dures for solving the fuzzy portfolio selection problem. Fuzzy Optim. Decis. Making 11(1),
29–46 (2012)
4. Calvo, C., Ivorra, C., Liern, V.: Fuzzy portfolio selection with non-ﬁnancial goals: exploring
the eﬃcient frontier. Ann. Oper. Res. (2014). doi:10.1007/s10479-014-1561-2
5. Calvo, C., Ivorra, C., Liern, V.: Fuzzy portfolio selection including cardinality constraints and
integer conditions. J. Optim. Theory Appl. 170(1), 343–355 (2016)
6. Chifu, H., Litzenberg, R.H.: Foundations for Financial Economics. North Holland, Amsterdam
(1988)
7. Delgado, M., Verdegay, J.L., Vila, M.A.: Fuzzy linear programming: from classical methods
to new applications. In: Delgado, M., Kacprzyk, J., Verdegay, J.L., Vila, M.A. (eds.) Fuzzy
Optimization: Recent Advances. Physica Verlag, Heidelberg (1994)
8. Estrada, J.: The cost of equity in emerging markets: a downside risk approach. Emerg. Mark.
Q. 19–30 (2000)
9. Estrada, J.: Mean-semivariance behavior: downside risk and capital asset pricing. Int. Rev.
Econ. Finance 16(2), 169–185 (2007)
10. Francis, J.C., Kim, D.: Modern Portfolio Theory: Foundations, Analysis and New Develop-
ments. Wiley, New Jersey (2013)
11. Grinold, R., Kahn, R.: Active Portfolio Management: A Quantitative Approach for Producing
Superior Returns and Controling Risk. McGraw Hill, New York (2000)
12. Hallerbach, W., Ning, H., Soppe, A., Spronk, J.: A framework for managing a portfolio of
socially responsible investments. Eur. J. Oper. Res. 153, 517–529 (2004)
13. Horniman, M.D., Jobst, N.J., Lucas, C.A., Mitra, G.: Computational aspects of alternative port-
folio selection models in the presence of discrete asset choice constraints. Quant. Finance 1,
489–501 (2001)
14. Inuiguchi, M., Tanino, T.: Portfolio selection under independent possibilistic information.
Fuzzy Sets Syst. 115, 83–92 (2000)
15. Jondeau, E., Rockinger, M.: Optimal portfolio allocation under higher moments. Eur. Financ.
Manage. 12, 29–55 (2006)
16. Jorion, P.: Value at Risk: The New Benchmark for Managing Financial Risk, 3rd edn. McGraw-
Hill, New York (2006)
17. Joro, T., Na, P.: Portfolio performance evaluation in mean-variance-skewness framework. Eur.
J. Oper. Res. 175(1), 446–461 (2006)
18. Kaplan, P.D., Siegel, B.: Portfolio theory is alive and well. J. Invest. 3(3), 18–23 (1994)
19. Getting Started With KLD STATS, KLD Research & Analytics, Inc. (2008)
20. Konno, H., Suzuki, K.: A mean-variance-skewness portfolio selection model. J. Oper. Res.
Soc. Jpn. 38, 173–187 (1995)
21. Konno, H., Yamazaki, H.: Mean-absolute deviation portfolio optimization model and its appli-
cations to Tokyo Stock Market. Manage. Sci. 37, 519–531 (1991)
22. Lai, Y.J., Hwang, C.L.: Fuzzy Mathematical Programming: Theory and Applications. Springer,
Berlin (1992)
23. León, T., Liern, V., Vercher, E.: Viability of infeasible portfolio selection problems: a fuzzy
approach. Eur. J. Oper. Res. 139, 178–189 (2002)
24. León, T., Liern, V., Vercher, E.: Fuzzy Mathematical Programming for Portfolio Management,
Financial Modelling. Physica-Verlag, Heidelberg (2000)
25. León, T., Liern, V., Marco, P., Segura, J.V., Vercher, E.: A downside risk approach for the
portfolio selection problem with fuzzy returns. Fuzzy Econ. Rev. 9(1), 61–77 (2004)
26. Lien, D., Tse, Y.K.: Hedging downside risk: futures vs. options. Int. Rev. Econ. Finance 10,
159–169 (2001)
27. Markowitz, H.M.: Portfolio selection. J. Finance 7, 79–91 (1952)
28. Markowitz, H.M.: Portfolio Selection: Eﬃcient Diversiﬁcation of Investments. Wiley, New
York (1959)
29. Ortí, F.J., Sáez, J., Terceño, A.: On the treatment of uncertainty in portfolio selection. Fuzzy
Econ. Rev. 7, 59–80 (2002)

136
C. Calvo et al.
30. Pﬂeiderer, P.: Is Modern Portfolio Theory Dead? Come On. http://techcrunch.com/2012/08/
11/is-modern-portfolio-theory-dead-come-on/ (2012)
31. Roy, A.D.: Safety ﬁrst and the holding of assets. Econometrica 20(3), 431–449 (1952)
32. Roy, A.D.: Risk and rank or safety ﬁrst generalised. Economica 23(91), 214–228 (1956)
33. Sharpe, W.F.: Capital asset prices: a theory of market equilibrium under conditions of risk. J.
Finance 19(3), 425442 (1964)
34. Sharpe, W.F.: The sharpe ratio. J. Portfolio Manage. 21(1), 49–58 (1994)
35. Sortino, F.A., Price, L.N.: Performance measurement in a downside risk framework. J. Invest.
3(3), 59–64 (1994)
36. Stambaugh, F.: Risk and value at risk. Eur. Manage. J. 14(6), 612–621 (1996)
37. Steuer, R., Qi, Y., Hirschberger, M.: Suitable-portfolio investors, nondominated frontier sen-
sitivity, and the eﬀect of multiple objectives on standard portfolio selection. Ann. Oper. Res.
(2007). doi:10.1007/s10479-006-0137-1
38. Taleb, N.N.: The Black Swan: The Impact of the Highly Improbable. Random House (2007)
39. Tanaka, H., Guo, P., Türksen, I.B.: Portfolio selection based on fuzzy probabilities and possi-
bility distributions. Fuzzy Sets Syst. 3, 387–397 (2000)
40. SIF: 2012 Report on Socially Responsible Investing Trends in the United States. Social Invest-
ment Forum, Washington, DC (2012)
41. Watada, J.: Fuzzy portfolio selection and its applications to decision making. Tatra Mount.
Math. Publ. 13, 219–248 (1997)

On Fuzzy Convex Optimization to Portfolio
Selection Problem
Ricardo Coelho
Abstract The goal of an investor is to maximize the required return in an investment
by minimizing its risk. With this in mind, a set of securities are chosen according to
the experience and knowledge of the investor, which subjective evaluations. Select-
ing these securities is deﬁned as the portfolio selection problem and it can be classi-
ﬁed as convex programming problems. These problems are of utmost importance in
a variety of relevant practical ﬁelds. In addition, since ambiguity and vagueness are
natural and ever-present in real-life situations requiring solutions, it makes perfect
sense to attempt to address them using fuzzy convex programming technique. This
work presents a fuzzy set based method that solves a class of convex programming
problems with vagueness costs in the objective functions and/or order relation in the
set of constraints. This method transforms a convex programming problem under
fuzzy environment into a parametric convex multi-objective programming problem.
The obtained eﬃcient solutions to the transformed problem by satisfying an aspira-
tion level deﬁned by a decision maker. This proposed method is applied in a port-
folio selection numerical example by using BM&FBOVESPA data of some Brazilian
securities.
1
Introduction
Due to desire of maximizing the expected income over a given time horizon, the
investors make portfolio and investment decisions. These decisions are supported by
the subjective evaluations of income expectations over the chosen time horizon and
the risk preferences of the proﬁle of each investor. In this context, we can identify two
important problems in achieving the desired objective. The ﬁrst is the uncertainty in
R. Coelho (✉)
Departamento de Estatística e Matemática Aplicada, Centro de Ciências,
Universidade Federal do Ceará, Av. Mister Hull, s/n, Campus do Pici, Bloco 910,
Fortaleza 60440-900, Brazil
e-mail: rcoelhos@dema.ufc.br
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_8
137

138
R. Coelho
the subjective evaluation for each scenario because there exist a number of decisions
according to the expert’s knowledge. The second is the actual decision-making on
feasible solutions.
The two problems described in the previous paragraph involve to develop an inter-
face between two close areas, Fuzzy Sets and Systems and Decision Support Sys-
tems. On the one hand, in the early sixties, based on the fact that classical logic does
not reﬂect, to the extent that it should, the omnipresent imprecision in the real world,
L.A. Zadeh proposed the Theory of Fuzzy Sets and Fuzzy Logic. Nowadays Fuzzy
Logic, or rather Soft Computing, is employed with great success in the conception,
design, construction and utilization of a wide range of products and systems whose
functioning is directly based on the human beings reason ways. On the other hand,
the term Decision Support System (DSS) was coined at the beginning of the 70’s to
feature the computer programs that could support a user in making decisions when
facing ill-structured problems. Nowadays, software for supporting decision-making
is available for almost any management and optimization problem, that involve mini-
mization (or maximization) of one or various objective functions in a domain that can
be constrained or not. If all the functions are linear, we obviously have a linear pro-
gram. Otherwise, the problem is called a nonlinear program. Nowadays we can use
highly eﬃcient and robust algorithms and software for linear programming, which
are important tools for solving problems in diverse ﬁelds. However, many realistic
problems cannot be enough represented or approximated as a linear program owing
to the nature of the non-linearity of the objective function and/or the non-linearity
of any of the constraints.
As it is well known, convex programming represents a special class of nonlinear
programming in which the objective is a convex function or are various convex func-
tions over a convex feasible set. Thus, it is clear that convex programming encom-
passes all linear problems, including applications in scheduling, planning and ﬂow
computations, and they may be used to solve some interesting combinatorial opti-
mization problems. There are several classes of problems that are naturally expressed
as convex problems. Examples of such problems can be found in game theory, engi-
neering modeling, design and control, problems involving economies of scale, facil-
ity allocation and location problems, problems in microeconomics among others.
Several applications and test problems for quadratic programming can be found in
[10, 16]. For instance, the risk investment analysis, ﬁrst introduced by Markowitz
[15], is an important research ﬁeld in the modern ﬁnance by modeling this problem
as a convex optimization problem.
The paper is organized as follows: Sect. 2 introduces the proposed method to solve
convex programming problems under a fuzzy environment. This fuzzy environment
can be in the costs of the objective function(s) and/or in the set of constraints. The
approach described in this work uses two phases: the ﬁrst one transforms the prob-
lem to be optimized into a parametric convex multi-objective programming problem
while in the second part the parametric problem is solved for a satisfaction level given

On Fuzzy Convex Optimization to Portfolio Selection Problem
139
by means of classical optimization techniques. To illustrate the approach Sect. 3
oﬀers a general portfolio selection problem formulated as a fuzzy convex multi-
objective programming. Section 4 presents numerical simulations and an analysis
of the results obtained. Finally, in Sect. 5, some conclusions are pointed out.
2
Parametric Convex Programming Approach
There are several approaches that solve fuzzy mathematical programming problems,
see [1] and [13], which use some defuzziﬁcation index, represent the fuzzy coeﬃ-
cients by intervals or transform this fuzzy problem into a parametric mathematical
programming problem. The main goal is to transform this imprecise problem into
a classical problem and using classical techniques to solve the equivalent problem.
In this work, we will focus on the parametric approach in order to transform fuzzy
problems into many classical problems with a parameter representing the satisfac-
tion level which belongs to the interval [0,1]. Another way would be deﬁning a new
parameter as a new decision variable and to ﬁnd out the optimal satisfaction level.
However, due to space limitations we will not consider this approach.
The parametric approach is divided into two parts: to transform a fuzzy prob-
lem into a classical parametric problem; a mathematical formulation of the classical
parametric problem which is equivalent to the original fuzzy problem.
2.1
Parametric Approach to Solve Convex Programming
Problems Under Fuzzy Environment
The imprecise costs in the objective function(s) of programming problems are rep-
resented by fuzzy numbers. These imprecise costs can be deﬁned by the decision
maker and they permit some violations that it is not possible in the classical case.
Thus, a programming problem with fuzzy costs in the objective function(s) can be
formulated in the following way:
̃
min F(̃𝐜; 𝐱)
s.t. 𝐱∈𝛺
(1)
where F = (f1, … , fm)(m ≥1) is an m vector of objective function(s), ̃𝐜∈F
(Rn×m) represents a vector of fuzzy costs, and 𝛺⊂Rn.
A set of membership functions deﬁnes the satisfaction level of a feasible solution
𝐱∈Rn. These membership functions can be formulated as follows:
𝜇i ∶R →(0, 1], i ∈I

140
R. Coelho
where 𝜇is a membership function and I is the set that contains all imprecise costs.
A fuzzy number can be represented by a classical one in several ways by using
some techniques as the Yager’s index [2], the mass center, among many others which
are indexes of defuzziﬁcation.
In this work, the fuzzy number is transformed into an interval that is deﬁned by
lower and upper bounds of the fuzzy number for a determined 𝛼-cut level.
Therefore, Problem (1) can be rewritten as:
̃
min {F([𝐜𝐋, 𝐜𝐔]𝛼; 𝐱) | 𝐱∈𝛺}
Thus, the fuzzy feasible solution to the original fuzzy problem is a set of optimal
solutions for each 𝛼∈(0, 1]. It can be deﬁned in the following way:
S(𝛼) = min {F(𝐜𝛼; 𝐱) | 𝐱∈𝛺}
(2)
where c𝛼∈[cL, cU]𝛼is a real value obtained by a linear combination of bounds of
the interval formed by the 𝛼-cut level with 𝛼∈(0, 1].
This fuzzy optimal solution can be found, 𝛼-cut by 𝛼-cut, by solving the equivalent
parametric multi-objective programming problem.
Hence, the fuzzy multi-objective programming problem was parametrized in the
end of ﬁrst part. In the second one the parametric problem is solved for each one of
diﬀerent 𝛼values by using conventional multi-objective programming techniques.
Thus, the optimal solution achieved by each 𝛼of the parametric problem satisﬁes
the Karush-Kuhn-Tucker suﬃcient optimality conditions to the multi-objective case
[4]. This point is described as an eﬃcient solution of original fuzzy problem.
The results for each 𝛼value generate a set of satisfaction solutions S(𝛼) and, there-
fore, according to Representation Theorem to fuzzy numbers, it can be deﬁned as
̃S =
⋃
𝛼
(1 −𝛼)S(𝛼),
where it can be used to associate all these 𝛼-optimal solutions. Hence, it is shown
that the feasible solutions are attained by the parametric approach. It determines a
fuzzy optimal solution ̃S which solves the original fuzzy multi-objective program-
ming problem.
2.2
Mathematical Formulation of Parametric Approach to
Fuzzy Costs in the Objective Function
A multi-objective approach that solves a fuzzy linear programming problem with
imprecise costs in the objective functions is presented in [8, 9]. This approach can
be extended to solve nonlinear programming problems with one or several objec-
tives. In [11], another multi-objective approach is developed that solves nonlinear

On Fuzzy Convex Optimization to Portfolio Selection Problem
141
programming problems with only one objective with imprecise coeﬃcients in the
objective function and in the set of constraints. But it can be extended to solve fuzzy
multi-objective problems too.
In this work, multi-objective programming problems with fuzzy costs in the
objective functions are formulated in the following way:
min [f1(̃𝐜1; 𝐱), … , fm(̃𝐜m; 𝐱)]
s.t. 𝐱∈𝛺
(3)
where 𝐱is an n vector of real numbers, ̃𝐜i is a vector of fuzzy numbers with pi compo-
nents, i = {1, … , m}. The fuzzy numbers are characterized by the membership func-
tions that are deﬁned by the decision maker. The membership functions are deﬁned
as
𝜇j ∶R →[0, 1], j ∈J = {1, 2, … , m}
In particular, these membership functions are described by:
𝜇j(y) =
⎧
⎪
⎨
⎪⎩
0
if cU
j < y or y < cL
j
Lj(y) if cL
j ≤y ≤c1
j
j ∈J
Rj(y) if c2
j ≤y ≤cU
j
(4)
where 𝐋(⋅) and 𝐑(⋅) are strictly increasing and decreasing continuous functions,
respectively, Lj(c1
j ) = Rj(c2
j ) = 1, j ∈J.
The problem considered in Verdegay [8] presents trapezoidal membership func-
tions to describe the fuzzy numbers. In this work, we consider it as deﬁned in (4).
By considering the (1 −𝛼)-cut level of all the costs, for 𝛼∈[0, 1], ∀x ∈R
𝜇j(x) ≥1 −𝛼⇔L−1
j (1 −𝛼) ≤x ≤R−1
j (1 −𝛼), j ∈J
Thus, according to the parametric transformations shown above, a fuzzy solution
to (1) can be obtained from a parametric solution of a equivalent parametric multi-
objective programming problem which is formulated as:
min [f1(𝐜𝟏
𝟏; 𝐱), f1(𝐜𝟐
𝟏; 𝐱), … , f1(𝐜𝟐𝐩𝟏
𝟏; 𝐱), … ,
fm(𝐜𝟏
𝐦; 𝐱), fm(𝐜𝟐
𝐦; 𝐱), … , fm(𝐜𝟐𝐩𝐦
𝐦; 𝐱)]
s.t. 𝐱∈𝛺, 𝐜𝐤∈E(1 −𝛼),
𝛼∈[0, 1], k = 1, 2, … , 2pj,
(5)
where E(1 −𝛼), for each 𝛼∈[0, 1], is the set of vectors in Rpj, such that pj informs
the amount of fuzzy numbers that represent the imprecise costs in each objective
function, for j ∈{1, 2, … , m}.

142
R. Coelho
Each (1 −𝛼)-cut level element of this set is in the lower bound, L−1
j (1 −𝛼), or in
the upper bound, R−1
j (1 −𝛼), i.e., ∀k = 1, 2, … , 2pj,
𝐜𝐤= (ck
1, ck
2, … , ck
m) ∈E(1 −𝛼)
where ck
j is equal to L−1
j (1 −𝛼) or R−1
j (1 −𝛼), for all j = 1, … , m
It is clear that a parametric optimal solution to (5) is part of the fuzzy optimal solu-
tion to (3). This approach was developed to the convex case with one only objective,
as described in [5, 18, 20], and here is extended to the convex case with several
objectives. The parametric optimal solutions can be obtained by using any optimiza-
tion method to solve classical multi-objective programming problems.
2.3
Mathematical Formulation of Parametric Approach to
Fuzzy Order Relation in the Set of Constraints
As in [6, 17, 19], the constraints of a convex problem are deﬁned as having a fuzzy
nature, that is, some violations in the accomplishment of such restrictions are per-
mitted. Thus, the convex problem can be addressed as follows
min F([𝐜𝐋, 𝐜𝐔]𝛼; 𝐱)
s.t. gi(𝐱) ≤f bi, i = 1, … , m
xj ≥0, j = 1, … , n
(6)
where gi, for each i = 1, … , m, is a convex function by building a convex constraint
set. Besides, the membership functions
𝜇i ∶Rn →[0, 1], i = 1, … , m
on the fuzzy constraints are to be determined by the decision maker. It is clear
that each membership function will give the membership (satisfaction) degree such
that any x ∈Rn accomplishes the corresponding fuzzy constraint upon which it is
deﬁned. These membership functions can be formulated as follows
𝜇i(𝐱(𝜆)) =
⎧
⎪
⎨
⎪⎩
1
gi(𝐱) ≤bi
1 −gi(𝐱) −bi
di
bi ≤gi(𝐱) ≤bi + di
0
gi(𝐱) > bi + di
with i = 1, … , m. In order to solve this problem in a two-phase method, ﬁrst let us
deﬁne for each fuzzy constraint,
Xi = {𝐱∈Rn | gi(𝐱) ≤f bi, 𝐱≥0} .

On Fuzzy Convex Optimization to Portfolio Selection Problem
143
If 𝐗= ⋂Xi(i = 1, … , m) then the former fuzzy quadratic problem can be addressed
in a compact form as
min {F([𝐜𝐋, 𝐜𝐔]𝛼; 𝐱) | 𝐱∈𝐗} .
It is clear that ∀𝜆∈(0, 1], an 𝜆-cut of the fuzzy constraint set will be the classical
set
X(𝜆) = {𝐱∈Rn | 𝜇X(𝐱) ≥𝜆}
where ∀𝐱∈Rn,
𝜇X(𝐱) = min 𝜇i(𝐱(𝜆)), i = 1, … , m
Hence an 𝜆-cut of the i-th constraint will be denoted by Xi(𝜆). Therefore, if ∀𝜆∈
(0, 1],
S(𝜆) = {𝐱∈Rn | F([𝐜𝐋, 𝐜𝐔]𝛼; 𝐱) = min F([𝐜𝐋, 𝐜𝐔]𝛼; 𝐲), 𝐲∈X(𝜆)}
the fuzzy solution to the problem will therefore be the fuzzy set deﬁned by the fol-
lowing membership function
S(𝐱) =
{
sup{𝜆∶𝐱∈S(𝜆)} 𝐱∈⋃
𝜆S(𝜆)
0
otherwise.
Provided that ∀𝜆∈(0, 1],
X(𝜆) =
⋂
i=1,…,m
{𝐱∈Rn | gi(𝐱) ≤ri(𝜆), 𝐱≥0, 𝐱∈Rn}
with ri(𝜆) = bi + di(1 −𝜆). The operative solution to the former problem can be
found, 𝜆-cut by 𝜆-cut, by means of the following auxiliary parametric convex pro-
gramming model,
min F([𝐜𝐋, 𝐜𝐔]𝛼; 𝐱)
s.t. gi(𝐱) ≤bi + di(1 −𝜆), i = 1, … , m
xj ≥0, j = 1, … , n, 𝜆∈[0, 1].
(7)
Thus, the fuzzy convex programming problem was parameterized at the end of
the ﬁrst phase.
In the second phase the parametric quadratic programming problem is solved for
each of the diﬀerent 𝜆values using the technique described in previous subsection.

144
R. Coelho
3
Portfolio Selection Problem
In order to illustrate the above described parametric method to solve fuzzy convex
programming problems, we are going to focus on general portfolio problems. It is
important to emphasize that, at the present time, we do not try to improve other solu-
tion methods for this kind of important problems, but only to show how our solution
approach performs. In [15], a description of a classical portfolio selection problem
is given that was formulated by Markowitz as a convex programming problem.
Markowitz’s model combines probability and optimization techniques to model
the behavior of investment under uncertainty. The investors are assumed to strike a
balance between minimizing the risk and maximizing the return of their investment.
The risk is characterized by the variance, and the return is quantiﬁed by the mean,
of a portfolio of assets. The two objectives of an investor are thus to minimize the
variance of a portfolio and to maximize the expected value of return.
Markowitz model for portfolio selection can be formulated mathematically in two
ways: minimizing risk when a level of return is given and maximizing return when
a level of risk is given. Hence, assume that there are n securities denoted by Sj (j =
1, … , n), the former problem is formulated on the following way:
min 𝐱𝐭𝐐𝐱
s.t. 𝐱𝐭𝐄(𝐑) ≥𝜌
𝟏𝐱= 1
𝐱≥𝟎
(8)
where 𝐱is an n vector that represents the percentage of money invested in asset,
i.e., the proportion of total investment funds devoted to each security; 𝐄(𝐑) is the
average vector of returns over m periods because 𝐑= [rij] is an m × n matrix that
represents the random variables of the returns of asset varying in m discrete times;
𝜌is a parameter representing the minimal rate of return required by an investor;
and 𝐐= [𝜎2
ij] is a covariance n × n matrix between returns of asset which can be
written as
𝜎2
ij =
m
∑
k=1
(rki −E(ri)) (rkj −E(rj))
m −1
.
(9)
On the other hand, the latter problem is formulated as
min 𝐱𝐭𝐄(𝐑)
s.t. 𝐱𝐭𝐐𝐱≤𝛾
𝟏𝐱= 1
𝐱≥𝟎.
(10)
where 𝛾is the maximum risk level the investor would bear.

On Fuzzy Convex Optimization to Portfolio Selection Problem
145
The expected return rate, 𝜌, and the maximum risk level, 𝛾, are decision maker’s
values that represent an expert’s knowledge. These two formulations of portfolio
selection problems can be mixed and formulated as a bi-objective convex program-
ming problem. Moreover, uncertainty and multiple objectives are the important fac-
tors in decision making. From a practical viewpoint, it is usually diﬃcult to deter-
mine exactly the coeﬃcients in mathematical programming problems due to various
kinds of uncertainties. However, it is sometimes possible to estimate the perturba-
tions of coeﬃcients by intervals, fuzzy numbers or possibilistic distributions.
The portfolio problem is a typical decision making problem under uncertainty
which has received considerably attention in the literature recently. This problem
addresses the dilemma that each investor faces the conﬂicting objectives of high
proﬁt versus low risk. In this work, the uncertainties in the objective functions are
represented by fuzzy costs and the order relation in the set of constraints is fuzzy
too. So, portfolio selection problem can be formulated in the following way
min 𝐱𝐭𝐄( ̃𝐑)
s.t. 𝐱𝐭𝐐𝐱≤f 𝛾
𝟏𝐱= 1
𝐱≥𝟎.
(11)
There are many formulations to describe a portfolio selection problem that are
more realistic than one was presented in this work. One of them is to put another
objective function called “Value at Risk”, as described in [12]. It is deﬁned as a
threshold value, which is a given probability level of the worse loss on the portfolio
over the given time horizon. However, we choose this convex formulation to show
the eﬃciency of our approach, and we will extend it to apply in other formulations
in the next step.
4
Numerical Experiments
In this section, a portfolio selection problem with fuzzy costs and fuzzy order relation
in the set of constraints are analyzed. In Sect. 4.1 we will show the used data to
formulate the fuzzy portfolio problems. Then in Sect. 4.2 the computational results
and a comparative analysis of the classic and parametric approaches responses will
be presented.
The tests were all performed on a PC with 2.7 GHZ IntelⓇCoreTM i7, 16 GB RAM
running MacOS Sierra operational system. All the problems presented in this work
were resolved using fmincon function to solve constraint programming problems
of ToolBox Optimization of MATLABⓇR2015a program. The evolutionary algo-
rithm parameter are 100 generations and 100 individuals in the population, while
the crossover and mutation index are 0.6 and 0.1, respectively.

146
R. Coelho
4.1
Formulation of the Numerical Examples
In order to show the performance of our method, we used the set of historical data
shown in Table 1 took by BM&FBOVESPA which is an important market for Brazilian
securities. It was chosen ten Brazilian securities and the columns 2–11 represent
Cemig, Cesp, Copel, Eletrobras, Embraer, Light, Petrobras, Unipar, Usiminas, and
Vale securities data, respectively. The returns on the ten securities, during the period
of 1994 up to 2016, are presented in Table 1.
The vagueness was inserted into the set of constraints in the form around 40%
variation in the modal value of each constraint function. Besides, each component
of the vector of imprecise costs is a fuzzy number and they are transformed into a
interval.
This example will consider performance of portfolios with respect to “return” and
“risk”. This assumes that a euro of realized or unrealized capital gains is exactly
equivalent to an euro of dividends, no better and no worse. This assumption is
appropriate for certain investors, for example, some types of tax-free institutions.
Other ways of handling capital gains and dividends, which are appropriate for other
investors, can be viewed in [15].
4.2
Results and Analysis
Here we show two results obtained for the portfolio selection problem with uncer-
tainties by the fuzzy convex programming method introduced in Sect. 2. The prob-
lems described in this work were solved by using the equivalent parametric multi-
objective problem as presented by Problems (5) and (7). The data from BM&FBOVESPA
shown in Table 1 was used on two ways of the Problem (11): (i) only order relation in
the set of constraints is uncertain; and (ii) the linear objective function which max-
imizes the return has imprecise costs and the order relation in the set of constraints
is uncertain.
Table 2 presents the solutions of the parametric portfolio selection problem with
ten securities and this same problem with imprecise order relation in the set of con-
straints. It is clear to see that the investor should choose only four securities of them
for any satisfaction level. With this choice, the expected value of return of this invest-
ment is between 22% up to 26%. To highlight, the maximum risk allowance is 18%
and the maximum tolerance is 7%.
Figure 1 presents the solutions of the parametric portfolio selection problem with
ten securities and this same problem with imprecise order relation in the set of con-
straints. The line represents the interpolation of the obtained points for each satis-
faction level.
Figure 2 presents the solutions of the parametric multi-objective portfolio selec-
tion problem with ten securities and this same problem with imprecise costs in the
linear objective function. Besides, the order relation in the set of constraints is impre-

On Fuzzy Convex Optimization to Portfolio Selection Problem
147
Table 1
BM&FBOVESP data from 1995 to 2016
#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
Periods
Cemig
Cesp
Copel
Eletrobras
Embraer
Light
Petrobras
Unipar
Uniminas
Vale
1994
0.7651
0.0000
0.5017
0.4375
0.6667
0.3600
0.1207
0.4875
0.0993
1.8763
1995
−0.7856
−0.9800
−0.2342
−0.1204
−0.9050
0.0163
−0.3846
−0.8386
−0.4839
−0.1147
1996
1.0674
1.0545
0.5942
0.4144
1.1053
0.1865
2.1250
−0.3582
0.3125
-0.8907
1997
−0.0226
0.1945
0.1364
-0.8508
1.3505
0.2601
0.4800
−0.3721
8.0476
−0.2185
1998
−0.6282
−0.6499
−0.4480
−0.6252
−0.7788
−0.6839
−0.6000
0.1111
−0.5789
−0.3697
1999
0.6903
−0.5608
0.6826
0.8995
−0.1346
0.3537
4.4050
3.3333
0.5000
2.1579
2000
−0.0420
−0.1084
0.1189
−0.0876
0.3722
0.1256
−0.8787
−0.1308
−0.0500
0.1071
2001
0.3416
0.7838
0.2163
−0.0763
0.0130
−0.4531
0.0823
−0.0265
−0.0877
0.118
2002
−0.2540
−0.5114
−0.4937
−0.2913
−0.0120
−0.6177
0.0059
0.2455
0.2212
0.9615
2003
0.4489
0.7674
0.0813
0.8538
0.5372
0.7784
0.5928
0.5328
2.9370
0.5906
2004
0.6705
−0.1053
0.1329
−0.1200
−0.1684
−0.2480
0.2663
1.0905
0.9996
−0.5346
2005
0.4065
0.0245
0.5255
−0.0130
0.1392
−0.7549
−0.6122
−0.4579
0.0262
0.2649
2006
0.1400
0.8660
0.4381
0.3445
0.2250
0.0853
0.3194
−0.0966
0.8713
−0.3330
2007
−0.6295
0.8103
0.3721
−0.5316
−0.0862
0.7197
0.9270
0.3814
−0.1354
−0.0689
2008
−0.2587
−0.6941
−0.2542
0.0819
−0.5628
−0.2370
−0.7382
−0.6633
−0.6886
−0.5331
2009
−0.0259
0.7963
0.6591
0.4036
0.0795
0.1885
0.5151
1.2000
0.9377
0.7876
2010
−0.1496
0.1289
0.0411
−0.3880
0.2408
−0.0212
−0.2665
−0.6818
−0.5738
0.1178
2011
0.3157
0.2466
−0.1311
−0.1978
−0.0034
0.1325
−0.2471
−0.3857
−0.1967
−0.2870
2012
−0.1978
−0.3960
−0.2368
−0.6452
0.2287
−0.2250
−0.1500
0.0698
−0.2029
0.0717
2013
−0.3516
0.2068
−0.1151
−0.0727
0.3073
−0.0090
−0.1821
0.3043
−0.0929
−0.1554
2014
0.5775
0.1256
0.1166
−0.0119
0.2938
−0.2306
−0.4003
−0.1167
−0.0081
−0.3864
2015
−0.7125
−0.5446
−0.3574
−0.0069
0.2353
−0.4183
−0.1064
0.1132
−0.6732
−0.4053
2016
0.2189
0.2255
0.1925
2.9601
−0.4700
0.7535
0.9767
0.2733
1.0547
0.9708

148
R. Coelho
Table 2
Solutions of the parametric portfolio selection problem
𝜆
#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
FunObj
0.0
0.0000
0.0000
0.5893
0.0000
0.0000
0.0000
0.0000
0.1475
0.1636
0.0996
0.2581
0.1
0.0000
0.0000
0.5981
0.0000
0.0000
0.0000
0.0000
0.1446
0.1594
0.0979
0.2548
0.2
0.0000
0.0000
0.6070
0.0000
0.0000
0.0000
0.0000
0.1416
0.1552
0.0962
0.2514
0.3
0.0000
0.0000
0.6163
0.0000
0.0000
0.0000
0.0000
0.1385
0.1508
0.0944
0.2479
0.4
0.0000
0.0000
0.6258
0.0000
0.0000
0.0000
0.0000
0.1353
0.1463
0.0926
0.2444
0.5
0.0000
0.0000
0.6356
0.0000
0.0000
0.0000
0.0000
0.1321
0.1416
0.0907
0.2407
0.6
0.0000
0.0000
0.6457
0.0000
0.0000
0.0000
0.0000
0.1287
0.1368
0.0888
0.2368
0.7
0.0000
0.0000
0.6531
0.0000
0.0000
0.0000
0.0000
0.1254
0.1321
0.0868
0.2328
0.8
0.0000
0.0000
0.6421
0.0000
0.0000
0.0000
0.0000
0.1231
0.1298
0.0853
0.2287
0.9
0.0000
0.0000
0.6303
0.0000
0.0000
0.0000
0.0000
0.1209
0.1274
0.0837
0.2246
1.0
0.0000
0.0000
0.6185
0.0000
0.0000
0.0000
0.0000
0.1186
0.1250
0.0821
0.2203

On Fuzzy Convex Optimization to Portfolio Selection Problem
149
Expected Value of Return
0.22
0.225
0.23
0.235
0.24
0.245
0.25
0.255
0.26
Satisfaction Level
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Objective Function
Fig. 1
The fuzzy solution applied in the linear objective function
Expected Value of Return
0
0.05
0.1
0.15
0.2
0.25
0.3
Satisfaction Level
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Objective Function
Fig. 2
The fuzzy solution applied in the linear objective function with fuzzy costs

150
R. Coelho
cise too. In this case it is clear to identify the belt that represents the fuzzy eﬃcient
solutions and the classical eﬃcient solutions are inside of this belt.
5
Conclusion
Convex Programming problems are very important in a variety of both theoreti-
cal and practical areas. When real-world applications are considered, the vagueness
appears in a natural way, and hence it makes perfect sense to think in Fuzzy Convex
Programming problems. In contrast to what happens with Fuzzy Linear Program-
ming problems, unfortunately until now no solution method has been found for this
important class of problems. In this context this paper has presented an operative
and novel method for solving Fuzzy Convex Multi-Objective Programming prob-
lems which is carried out by performing two phases which ﬁnally provide the user
with a fuzzy solution. The method has been validated by solving a portfolio selec-
tion problem. The obtained solutions allow the author to follow along this research
line trying to solve real problems in practice, in such a way that oriented Decision
Support Systems involving Fuzzy Convex Programming problems can be built.
An evolutionary algorithm called NSGA-II was used and it produces a sequence
of points according to a prescribed set of instructions, together with a termination
criterion. Usually we look for a sequence that converges to a set of eﬃcient solutions,
but in many cases however we have to be satisﬁed with less favorable solutions. Then
the procedure may stop either (1) if a point belonging to a preﬁxed set (the solution
set) is reached, or (2) if some preﬁxed condition for satisfaction is veriﬁed. In any
case, assuming that a solution set is preﬁxed, the algorithm would stop if a point
in that solution set is reached. Frequently, however, the convergence to a point in
the solution set is not easy because, for example, of the existence of local optimum
points. Hence we must redeﬁne some rules to ﬁnish the iterative procedure.
Hence the control rules of the algorithms solving convex programming problems
could be associated to the solution set, and to the criteria for terminating the algo-
rithm. As it is clear, fuzziness could be introduced in both points, not assuming it as
inherent in the problem, but as help for obtaining, in a more eﬀective way, some solu-
tion for satisfying the decision-maker’s wishes. This mean that the decision maker
might be more comfortable obtaining a solution expressed in terms of satisfaction
instead of optimization, as it is the case when fuzzy control rules are applied to the
processes.
Acknowledgements The authors want to thank the support provided by the Brazilian agency CNPq
with process number 4849002/2013-0.

On Fuzzy Convex Optimization to Portfolio Selection Problem
151
References
1. Bector, C.R., Chandra, S.: Fuzzy Mathematical Programming and Fuzzy Matrix Games. Stud-
ies in Fuzziness and Soft Computing, vol. 169. Springer, Berlin (2005)
2. Bortolan, G., Degani, R.: A review of some methods for ranking fuzzy subsets. Fuzzy Sets
Syst. 15, 1–19 (1985)
3. Carlsson, C., Fullér, R., Majlender, P.: A possibilistic approach to selecting portfolio with high-
est utility score. Fuzzy Sets Syst. 131, 13–21 (2002)
4. Chankong, V., Haimes, Y.Y.: Multiobjective Decision Making: Theory and Methodology.
North Hollando Series in System Science and Engineering, vol. 8. North Holland, New York,
USA (1983)
5. Cruz, C., Silva, R.C., Verdegay, J.L.: Fuzzy costs in quadratic programming problems. Fuzzy
Optim. Decis. Making 12, 231–248 (2013)
6. Cruz, C., Silva, R.C., Verdegay, J.L.: Extending and relating diﬀerent approaches for solving
fuzzy quadratic problems. Fuzzy Optim. Decis. Making 10, 193–210 (2011)
7. Deb, K.: Multi-objective Optimization Using Evolutionary Algorithms. Wiley, Chichester, UK
(2001)
8. Delgado, M., Verdegay, J.L., Vila, M.: Imprecise costs in mathematical programming prob-
lems. Control Cybern. 16(2), 113–121 (1987)
9. Delgado, M., Verdegay, J.L., Vila, M.: Relating diﬀerent approaches to solve linear program-
ming problems with imprecise costs. Fuzzy Sets Syst. 37, 33–42 (1990)
10. Floudas, C.A., Pardalos, P.M., Adjiman, C., Esposito, W.R., Gümüs, Z.H., Harding, S.T.,
Klepeis, J.L., Meyer, C.A., Schweiger, C.A.: Handbook of Test Problems in Local and Global
Optimization. Nonconvex Optimization and Its Applications, vol. 33. Kluwer Academic Pub-
lishers, Dordrecht (1999)
11. Jiménez, F., Cadenas, J., Sánchez, G., Gómez-Skarmeta, A., Verdegay, J.L.: Multi-objective
evolutionary computation and fuzzy optimization. Int. J. Approx. Reason. 43(1), 59–75 (2006)
12. Jorion, P.: Value at Risk: The New Benchmark for Managing Financial Risk, 3rd edn. McGraw-
Hill (2006)
13. Lai, Y.J., Hwang, C.L.: Fuzzy Mathematical Programming: Methods and Applications. Lecture
Notes in Economics and Mathematical Systems, vol. 394. Springer, Berlin (1992)
14. León, T., Liern, V., Vercher, E.: Viability of infeasible portfolio selection problems: a fuzzy
approach. Eur. J. Oper. Res. 139, 178–189 (2002)
15. Markowitz, H.M.: Portfolio Selection: Eﬃcient Diversiﬁcation of Investments, 2nd edn. Black-
well Publisher, Massachusetts, USA (1991)
16. Schittkowski, K.: More Test Examples for Nonlinear Programming Codes. Spring (1987)
17. Silva, R.C., Cruz, C., Verdegay, J.L., Yamakami, A.: A survey of fuzzy convex programming
models. In: Lodwick, W.A., Kacprzyk, J. (eds.) Fuzzy Optimization: Recent Advances and
Applications. Studies in Fuzziness and Soft Computing, vol. 254, pp. 127–143. Springer, Berlin
(2010)
18. Silva, R.C., Cruz, C., Yamakami, A.: A parametric method to solve quadratic programming
problems with fuzzy costs. In: IFSA/EUSFLAT 2009. Lisbon, Portugal, July 2009
19. Silva, R.C., Verdegay, J.L., Yamakami, A.: Two-phase method to solve fuzzy quadratic pro-
gramming problems. In: 2007 IEEE International Fuzzy Systems Conference, London, UK,
July 2007
20. Silva, R.C., Verdegay, J.L. Yamakami, A.: A parametric convex programming approach applied
in portfolio pelection problem with fuzzy costs. In: 2010 IEEE International Fuzzy Systems
Conference, Barcelona, Spain, July 2010
21. Tanaka, H., Guo, P., Türksen, B.: Portfolio selection based on fuzzy probabilities and possibity
distributions. Fuzzy Sets Syst. 111, 387–397 (2000)

Digital Coaching for Real Options Support
Christer Carlsson
Abstract Classical management science is making the transition to analytics,
which has the same agenda to support managerial planning, problem solving and
decision making in industrial and business contexts but is combining the classical
models and algorithms with modern, advanced technology for handling data,
information and knowledge. In work with managers in the forest industry, we found
out that there is a growing interest to replace the classical net present value
(NPV) with real options theory, especially for strategic issues and uncertain,
dynamic environments. Uncertainty and dynamics motivate the use of soft com-
puting, i.e. versions of the real options methods that use fuzzy numbers (intervals),
macro heuristics, approximate reasoning and evolutionary algorithms. In general,
managers can follow the logic of the real options theory but the methods require
rather advanced levels of analytics; when the methods are implemented, they will
be used by growing numbers of people with more of a business than analytics
background. They ﬁnd themselves in trouble pretty quickly as they need to master
methods, they do not fully understand and details of which they forget from time to
time. We propose that digital coaching is a way to guide and support users to give
them better chances for effective and productive use of real options methods.
Keywords Digital coaching ⋅Analytics ⋅Fuzzy real options
1
Introduction
This chapter has a history and a reason that bridges the past, the present and the
future. The history is a paper I wrote called On the Relevance of Fuzzy Sets in
Management Science Methodology in 1984 [4]. This was a time when we tried to
make the case for fuzzy sets in management science and as a support theory for
managers who plan the future, and solve problems and make decisions in their daily
C. Carlsson (✉)
IAMSR/Abo Akademi University, Turku, Finland
e-mail: christer.carlsson@abo.ﬁ
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_9
153

activities. This is an activity where Professor Jose Luis “Curro” Verdegay has made
signiﬁcant contributions over many years and where he is a much sought-after
collaborator for many researchers.
If we continue the history a bit, a ﬁrst version of the paper had been presented
and discussed at the 11th meeting of the EURO Working Group on Fuzzy Sets (in
which Curro was an active collaborator) at the European Institute for Advanced
Studies in Management in Brussels on February 19–20, 1981. The EIASM is the
centre for serious research on management in Europe and getting an invitation to
run a workshop on fuzzy sets took some negotiation; I was chairing the EURO WG
in that period and had to do the negotiating.
Management science methodology—and especially operations research that
applied the same methodology for engineering problems and theory development—
had already in 1981 been under attack for more than a decade for failing to deal
with the real world problems managers have to tackle, for oversimplifying problems
and for spending too much time with mathematically interesting but practically
irrelevant problems and solutions. The message was simply that management sci-
ence methodology produced theory and methods that were irrelevant for handling
actual management problems. The paper in 1984 argued that fuzzy sets when
properly worked into management science methodology would make the models,
the algorithms and the theory more relevant and better suited to deal with man-
agement problems in practice.
Now, more than 30 years later, we have to admit that we were not successful in
bringing it about, that fuzzy sets remained a marginal development in management
science and that we have been able to get fuzzy sets based methods accepted only
for some limited applications, such as multiple criteria optimisation, real options
valuation, logistics optimisation, etc. for which there have been algorithmic beneﬁts
of allowing the use of fuzzy numbers.
Management science and operations research have also changed over the dec-
ades; two major organisations in the ﬁeld—TIMS and ORSA—merged and became
INFORMS to combine the applications oriented research (TIMS) with the algo-
rithms and theory oriented research (ORSA); now the annual INFORMS confer-
ences collect 2–3000 participants; in Europe the EURO Association is a sister
organisation to INFORMS and the annual EURO conferences also collect 2–3000
participants. Both organisations run major, well-established journals with high
impact factors and there are dozens of journals publishing material produced under
guidance of management science methodology. The ﬁeld is alive and well and
promotes lively research that activates thousands of researchers. The context is
there; what is needed for fuzzy sets to be relevant (again) for the research that is
carried out—a question and a goal we have had in the International Fuzzy Systems
Association (IFSA) for the last decade? Or to be more focused and speciﬁc—is
there any management science problem area where fuzzy sets theory could be vital
for breakthrough research (cf. [15])?
154
C. Carlsson

1.1
Analytics
Operations Research and Management Science are now in the process of being
transformed by (Business) Analytics which is getting the attention of major cor-
porations and senior management. On our part, in our work with complex, difﬁcult
problems for large industrial corporations, we have for a number of years been
promoting Soft Computing to the same audience instead of trying to explain fuzzy
sets theory and fuzzy logic in the way it was originally done. The experience we
have—summarized in a few words—is that analytics methods which implement
soft computing theory and algorithms are turning out to be very effective and useful
for planning, problem solving and decision making in “big data” environments; the
“big data” is one of the challenges of the modern digital economy and for which we
propose that fuzzy sets would offer instruments for breakthrough research [15].
Analytics adds value to management; it promotes data-driven and analytical
decision making, which was somehow “reinvented” as being important and useful for
management that had relied on other schools of thought for a couple of decades.
Analytics builds on recent software improvements in information systems that has
made data, information and knowledge available in real time in ways that were not
possible for managers only a few years ago [18]. INFORMS gradually found out that
the new movement represents both “potential opportunities” and “challenges” to
management science and operations research professionals [25]. The methods and the
application cases worked out in the Davenport-Harris book [18] are very close to
traditional management science methodology, actually so close that a manager
probably fails to see any differences, which is why INFORMS ﬁnds “challenges”.
Soft Computing (introduced by LotﬁZadeh in 1991) builds on fuzzy sets theory
[31], fuzzy logic, optimisation, neural nets, evolutionary algorithms, macro heuristics
and approximate reasoning. Soft Computing is focused on the design of intelligent
systems to process uncertain, imprecise and incomplete information; soft computing
methods applied to real-world problems offer more robust, tractable and less costly
solutions than those obtained by more conventional mathematical techniques.
Liberatore and Luo [25] list four factors that drive the analytics movement:
(i) availability of data (ii) improved analytical software (iii) the adoption of a
process orientation by organisations, and (iv) managers and executives who are
skilled users of information and communication technology. Compared to the
experience of the 1980s the last factor is probably the most important driver—there
is a new generation of managers and executives in charge of the corporations that
are using information technology as part of their daily routines. They work with
data, information and knowledge on a real time basis and they continuously hunt for
improved analytical tools to help give them competitive advantages. They do not
necessarily recognize the analytical tools as classical management science algo-
rithms; analytical software (cf. (ii)) has become user-friendly with graphical user
interfaces and visualisation of results; users typically do not have the mathematical
background to get into details with the algorithms. Information technology has
made data available on a real time basis which allows online planning, problem
Digital Coaching for Real Options Support
155

solving and decision making. Maybe “allow” is not the right verb as online man-
agement work in real time is more of a necessity to keep up with the competition.
The same driver also explains the adoption of a process orientation (cf. (iii)) as
management work typically is group—and teamwork online and in real time.
Davenport and Harris [18] describe analytics as “the extensive use of data, statis-
tical and quantitative analysis, explanatory and predictive models and fact-based
management to drive decisions and actions”. Liberatore and Luo [25] identify three
levels of modelling—descriptive, predictive and prescriptive—and state that man-
agement science and operations research typically would focus on advanced ana-
lytics, i.e. prescriptive modelling. They also point out that analytics would focus on
the transformation of data into actions through analysis and insight, which in their
discussion contributes to the application cases of management science.
The modern movement of analytics appears to offer interesting possibilities and
opportunities for soft computing; the movement is data-driven which will require
tools for handling imprecision; the movement is focused on managers who need to
deal with real world problems, for which available data, information and knowledge
are incomplete, imprecise and uncertain and should allow for fast, often intuitive
conclusions; the movement builds on improved analytical software that offers
platforms for a multitude of algorithms, intelligent technologies, soft computing,
computational intelligence, etc. Modern analytics offers platforms and environments
for digital coaching of managers in planning, problem solving and decision making.
There are beneﬁts of having worked with management science for a few decades
(like myself and my contemporaries)—there has been hundreds of innovative ideas
and some successful solutions from which it has been possible to extract working
principles and growing understanding of how good science can guide and con-
tribute to successful planning, problem solving and decision making. In the context
of the digital economy these processes—not surprisingly—offer new challenges:
real-time management challenged by “big data” and relying on fast processing by
advanced analytics methods would best be carried out by postdoc-qualiﬁed man-
agers—these are rather scarce and would most often be ﬁltered out by corporate
career qualifying processes much before they reach senior management positions.
Thus there will be a need to reinstate “coaching” functions with the advanced
analytics methods to tell/explain to the users what can/should be done, how it
should be carried out, what the results are and what they mean, and how they
should be applied (with explanations of what could/should not be done).
My storyline is worked out in the context of analytics and soft computing and the
history that has formed that context over the last few decades. I will work with fuzzy
real options modelling, that is one of the more advanced analytics methods with a
theory that is not easily introduced to managerial decision makers but which shows
results that represent insight that can offer sustainable competitive advantages. The
fuzzy real options modelling is introduced in Sect. 2 and the principles and the state
of the art of digital coaching in Sect. 3; then we will use the methods in Sect. 4 to ﬁnd
out how users of real options models could be given coaching support for strategic
planning and effective decision making. The chapter will ﬁnish with a summary of the
main points and with some linking to insight developed by Kahneman [24].
156
C. Carlsson

2
Fuzzy Real Options Modelling
Black and Scholes (cf. [17]) introduced the options theory to decide the price for
options on ﬁnancial assets that would be effective in the ﬁnancial market; Merton
(cf. [17]) then proved that similar options modelling also could be applied to real
assets and used to work out the pricing of investment alternatives based on future
cash ﬂows for the alternatives. Merton’s method—named real options analysis—
became popular among professionals as it offered more ﬂexibility than the classical
NPV methods and the options represented alternative foresight scenarios for
development that could not be worked out with the NPV methods. At IAMSR the
real options analysis has been applied to (i) so-called giga investments which are
large enough to change the dynamics of the markets on which the investment object
operates [17]; (ii) mergers and acquisitions where the strategic positions of the new,
merged company offer new and different options [1]; (iii) portfolios of R&D pro-
jects where combinations of projects (some successful, others not) offer options to
proceed with investments for own production, investments combined with licensing
of technology or production capacity, licensing of patents, disinvestment and sales
of patents, or with discontinuing the project as unsuccessful (Heikkilä [22]);
(iv) portfolios of patents with options to apply/not apply for patents (national, EU,
USA or global), to continue/discontinue the patents, to exclusively use the patents,
to license the patents, to sell the rights to the patents and collect royalty, to sell the
rights outright and to discontinue the patents; all these options have different pro-
ﬁles of expected revenue, risk and cash ﬂows (Wang [30]).
In the following, we will use experience from a real world case—the strategic
decision on the closing/not closing of a paper mill in the UK owned by a Finnish
forest corporation. We worked with the management team during an 18 month
period and followed the processes they went through and tried to support them with
good analytical tools as best we could. In this way we gained a fairly good
understanding of how management works with analytics tools, how they can for-
mulate their insight with the elements of real options models, what compromises
and simpliﬁcations they are ready to accept and how they understand and apply the
results (cf. [21, 22]). This is now more than digital coaching for the managers, it is
actually practical real world coaching.
2.1
A Paper Mill Case
The paper mill has had an unsatisfactory proﬁtability development for a number of
reasons: (i) ﬁne paper prices have been going down for 6 years (ii) costs are going
up (raw material, energy, chemicals) (iii) demand is either declining or growing
slowly depending on the markets (iv) production capacity cannot be used optimally,
and (v) the £/USD exchange rate is unfavourable (sales invoiced in USD, costs paid
in £). The standard solution for most forest industry corporations is to try to close
Digital Coaching for Real Options Support
157

any old, small and not cost-effective production plants (like the UK plant); the
common wisdom is that modern, large production plants will always be more
proﬁtable. The UK paper mill is producing ﬁne paper products, it is rather aged, the
paper machines were built a while ago, the raw material is not available close by,
energy costs are reasonable but are increasing in the near future, key domestic
markets are close by and export markets (with better sales prices) will require
investments in the logistics network.
The intuitive conclusion, based on the facts, is—a sunset case and senior
management should close the plant. On the other hand we have the UK trade
unions, which are strong, and we have pension funds commitments for many more
years which are very strict, and we have long-term power contracts which are
expensive to get out of. Finally, by closing the plant we will invite competitors to
ﬁght us in the UK markets we have served for more than 50 years and which we
cannot serve from other plants at any reasonable cost.
It is clear that the decision problem is more complex than standard routine
formulations and that a number of factors that will decide the outcome will not be
easily handled with algorithms.
There were a number of conditions which were more or less predeﬁned. The ﬁrst
one was that no capital could/should be invested as the plant was regarded as a
sunset plant. The second condition was that we should in fact consider ﬁve sce-
narios: the current production setup with only maintenance of current resources and
four options to switch to setups that save costs and have an effect on production
capacity used. The third condition is that the plant together with another unit should
carry sizeable administrative costs of the sales organization which should be cov-
ered in some way (but not clear how) if the plant is closed. The fourth condition is
the pension scheme that needs to be ﬁnanced for several more years. The ﬁfth
condition is given by the power contracts that are also running for several more
years. These speciﬁc conditions have consequences on the cost structure and the
risks that various scenarios involve. It is not known if the conditions are truly
non-negotiable. The management team should decide if the plant will (i) be closed
as soon as possible (ii) not be closed, or (iii) be closed at some later point of time
(and then at what point of time).
The ﬁrst step to decide on the best decision is typically to carry out a proﬁtability
analysis. Modern proﬁtability analysis works with methods that originate in neo-
classical ﬁnance theory. The models are by nature normative and offer general
decision support for the long run but may not be helpful for real life decisions in a
real industry setting where data is neither well-structured nor complete. In prof-
itability planning a good enough solution is many times both efﬁcient, in the sense
of smooth management processes, and effective, in the sense of ﬁnding the best way
to act, as compared to theoretically optimal outcomes.
Economic feasibility and proﬁtability are key factors but more issues are at stake.
Management decisions will be scrutinized and questioned regardless of what the
close/not close decision is going to be. The shareholders will react negatively if they
ﬁnd out that share value will decrease (closing a proﬁtable plant, closing a plant
158
C. Carlsson

which may turn proﬁtable, or not closing a plant which is not proﬁtable, or which
may turn unproﬁtable) and the trade unions, local and regional politicians, the press,
etc. will always react negatively to a decision to close a plant almost regardless of
the reasons.
Only very few decisions are of the type now-or-never—often it is possible to
postpone, modify or split up a complex decision in strategic components, which can
generate important learning effects and therefore essentially reduce uncertainty. If
we close a plant we lose all alternative development paths which could be possible
under changing conditions. These aspects are widely known—they are part of
managerial common wisdom—but they are hard to work out unless we have the
analytical tools to work them out and unless we have the necessary skills to work
with these tools.
The intention here is to demonstrate that in industrial cases the focus of the
standard NPV based methods is too narrow; the net present value of estimated
future revenues and costs gives an over-simplistic view and comparison of the
decision alternatives; nevertheless, this approach is used as better tools are not
readily known.
2.2
Real Options Modelling
We chose to work with real options models as our analytical tools for the paper
mill. The rule we worked out, is that we should only close the plant now if the net
present value of this action is high enough to compensate for giving up the value of
the option to wait. Because the value of the option to wait vanishes right after we
decide to close the plant, this loss in value is actually the opportunity cost of our
decision (cf. Alcaraz [1], Borgonovo and Peccati [3], Carlsson and Fullér [6, 9],
Heikkilä [22]). This is a principle based in theory but it turned out that it was well
understood by the management team and the managers we worked with were
interested in learning to use the real options methods. We worked out the rather
advanced mathematics in a series of workshops in which we also introduced and
demonstrated the software (actually Excel models) we were using—the key turned
out to be that we used the management team’s own data to explain the models step
by step. They could identify the numbers and ﬁt them to their own understanding of
the close/no close alternatives and their consequences and the possible problem
solving paths shown by the real options models. This is a good example of how
coaching can be built to work in practice—managers have lots of experience, ideas,
advice from various sources and inﬂuence from stakeholders that all could inter-
vene, sometimes unintentionally, to make the decisions for them; systematic
modelling will not get time and space even if it is found out that the results may be
vital for the actual decision making.
Digital Coaching for Real Options Support
159

The value of a real option is computed by (cf. Carlsson et al. [9, 10], Collan et al.
[16], Collan [17]),
ROV = S0e −δTNðd1Þ −Xe −rTNðd2Þ,
where
d1 = ln S0 ̸X
ð
Þ + r −δ + σ2 ̸2
ð
ÞT
σ
ﬃﬃﬃﬃ
T
p
,
d2 = d1 −σ
ﬃﬃﬃﬃ
T
p
Here, S0 denotes the present value of the expected cash ﬂows, X stands for the
nominal value of the ﬁxed costs, r is the annualized continuously compounded rate
on a safe asset, δ is the value lost over the duration of the option, σ denotes the
uncertainty of the expected cash ﬂows, and T is the time to maturity of the option
(in years). The interpretation is that we have the difference between two streams of
cash ﬂow: the S0 is the revenue ﬂow from the plant and the X is the cost generated
by the plant; both streams are continuously discounted with a chosen period of time
T and the streams are assumed to show random variations, which is why we use
normal distributions N. In the ﬁrst stream we are uncertain about how much value δ
we will lose if we postpone the decision and in the second stream we have
uncertainty on the costs σ.
The function N (d) gives the probability that a random draw from a standard
normal distribution will be less than d, i.e. we want to ﬁx the normal distribution,
NðdÞ =
1ﬃﬃﬃﬃﬃ
2π
p
Z d
−∞
e −x2 ̸2dx.
Facing a deferrable decision, the main question that a manager primarily needs
to answer is the following: how long should we postpone the decision—up to T time
periods—before (if at all) making it? In a managerial context that is normally not
decided by any algorithm but by experience, advice from people—or (in the present
context) from impressions gained in negotiations.
With the model for real option valuation we can ﬁnd an answer and develop the
following natural decision rule for an optimal decision strategy (cf. Carlsson and
Fullér [5, 8–10]).
Let us assume that we have a deferrable decision opportunity P of length L years
with expected cash ﬂows {cf0, cf1, …, cfL}, where cfi is the cash inﬂows that the
plant is expected to generate in year i (i = 0, …, L). We note that cfi is the
anticipated net income (revenue—costs) of decision P year i. In these circum-
stances, if the maximum deferral time is T, we can make the decision to postpone
for t′ periods (which is to exercise the option at time t′, 0 < t′ < T) for which the
value of the option, ROVt′ is positive and gets its maximum value; namely (cf.
Carlsson and Fullér [9] for details),
160
C. Carlsson

ROVt0 =
max
t = 0, 1, ..., T ROVt =
max
t = 0, 1, ..., T Vte −δTNðd1Þ −Xe −rTNðd2Þ > 0,
If we make the decision now without waiting, then we will have
ROV0 = V0 −X = ∑
L
i = 0
cfi
ð1 + βPÞi −X.
That is, this decision rule also incorporates the net present valuation of the
assumed cash ﬂows; βP is the risk-adjusted discount rate of the decision. This is the
rule for how long we can postpone the decision; this is anchored in solid economic
theory which is a rational motivation for the decision. The real option model
actually gives a value for the deferral which makes it possible to ﬁnd the optimal
deferral time. The management team gets an instrument for the decision to be made.
2.3
Real Options and Real World Decisions
Having got this far we will have to face another problem: the difference between
management science modelling and what is possible in the real world case. Real
options theory requires rather rich data with a good level of precision on the
expected future cash ﬂows. This is possible for ﬁnancial options and the stock
market (following the effective market hypothesis) for which we can use models
that build on stochastic processes and which have well known mathematical
properties. The data we could collect on expected future cash ﬂows were not precise
and were incomplete; the management team was rather reluctant to offer any ﬁrm
estimates (for very understandable reasons, these estimates can be severely ques-
tioned with the beneﬁt of hindsight). It turns out that we could work out the real
options valuation also with imprecise and incomplete data.
Let us now work out the case that expected cash ﬂows of the close/not close
decision cannot be characterized with single numbers. With the help of possibility
theory (cf. Carlsson and Fullér [7, 9] for details) we can estimate the expected
incoming cash ﬂows at each year of the project by using a trapezoidal possibility
distribution of the form
Vi = ðsL
i , sR
i , αi, βiÞ,
i = 0, 1, . . . , L,
that is, the most possible values of the expected incoming cash ﬂows lie in the
interval [si
L, si
R] (which is the core of the trapezoidal fuzzy number describing the
cash ﬂows at year i of the paper mill); (si
R + βt) is the upward potential and (si
L – αt)
is the downward potential at year i, (i = 0, 1, …, L). In a similar manner, we can
Digital Coaching for Real Options Support
161

estimate the expected costs by using a trapezoidal possibility distribution of the
form
X = ðxL, xR, α′, β′Þ,
i.e. the most possible values of the costs lie in the interval [xL, xR]; (xR + β′) is the
upward potential and (xL −α′) is the downward potential (there should actually be
different costs for each year, but the management team stated that they do not
change much and that the trouble of estimating them does not have a good trade-off
with the accuracy of the model).
By using possibility distributions we can extend the classical probabilistic
decision rules for an optimal decision strategy to the case with imprecise data.
Let P be a deferrable decision opportunity with incoming cash ﬂows and costs
that are characterized by the trapezoidal possibility distributions given above.
Furthermore, let us assume that the maximum deferral time of the decision is T, and
the required rate of return on this project is βP. In these circumstances, we should
make the decision (exercise the real option) at time t′, 0 < t′ < T, for which the
value of the option, Ct′ is positive and reaches its maximum value. That is,
FROVt0 =
max
t = 0, 1, ..., T FROVt =
max
t = 0, 1, ..., T Vte −δtNðdðtÞ
1 Þ −Xe −rtNðdðtÞ
2 Þ > 0,
where
dðtÞ
1 = ln EðVtÞ ̸EðXÞ


+ r −δ + σ2 ̸2
ð
Þt
σ
ﬃﬃt
p
,
dðtÞ
2 = dðtÞ
1 −σ
ﬃﬃ
t
p
= ln EðVtÞ ̸EðXÞ


+ r −δ −σ2 ̸2
ð
Þt
σ
ﬃﬃt
p
.
Here, E denotes the possibilistic mean value operator and
σ = σðVtÞ ̸EðVtÞ
shows the annualized possibilistic variance of the aggregate expected cash ﬂows
relative to its possibilistic mean. Furthermore,
Vt = PV(cf0, cf1, . . . , cfL; βPÞ −PV(cf0, cf1, . . . , cft −1; βPÞ = PV(cft, . . . , cfL; βPÞ = ∑
L
i = t
cfi
ð1 + βPÞi
computes the present value of the aggregate (fuzzy) cash ﬂows of the project if this
has been postponed t years before being undertaken.
162
C. Carlsson

To ﬁnd a maximizing element from the set
FROV0, FROV1, . . . , FROVT


we need a method for the ordering of trapezoidal fuzzy numbers. This is one of the
partially unsolved problems for fuzzy numbers as we do not have any complete
model for ranking intervals (cf. Carlsson and Fullér [7, 9] for details), which is why
we have to resort to various ad hoc methods to ﬁnd a ranking. Basically, we can
simply apply some value function to order fuzzy real option values of trapezoidal
forms
FROVt = ðcL
t , cR
t , α
0
t, β
0
tÞ,
t = 0, 1, . . . , T.
νðFROVtÞ = cL
t + cR
t
2
+ rA ⋅β
0
t −α
0
t
6
,
where rA ∃0 denotes the degree of risk aversion. If rA = 1 then trapezoidal fuzzy
numbers are compared by their pure possibilistic means (cf. Carlsson and Fullér
[6]). Furthermore, in the case rA = 0, we are risk neutral and fuzzy real option
values are compared by the centre of their cores.
Thus we can work out the best time for making a close/not close decision on the
paper mill also with imprecise and incomplete data, that is—if we can work out the
mathematics. If this is not the case and our knowledge and expertise is to be found
in business operations (the mathematics is basically known but details are distant
memories) we would beneﬁt from coaching. We have worked with managers in a
number of real option cases (cf. [8]) and have found out that they could beneﬁt from
the following types of coaches, which now are at the analytics level and in the
domain of models and algorithms:
• Coach 1: collects and guides estimates of imprecise incoming cash ﬂows and
costs, shows FROV with guides to meaning, and shows the T periods for
optimal postponement of the decision with guides to explanation (net present
value of action vs. option value of postponement)
• Coach 2: collects, guides market estimates for r (compound rate), δ (value lost
over the option) and σ (uncertainty of expected cash ﬂows); guides estimates of
α′ and β′, the downward and upward potentials
• Coach 3: guides to explanations of FROV, possibilistic mean, possibilistic
variance, present value of aggregate fuzzy cash ﬂows
• Coach 4: guides to explanations of ranking of FROV (ranking of fuzzy
numbers)
• Coach 5: works out trade off variations between net present value of action vs.
option value of postponement with variations of parameters
The immediate reaction is of course that this coaching could be carried out by a
senior expert with knowledge and experience of real options and the key factors in
decisions to close an old paper mill.
Digital Coaching for Real Options Support
163

Experience from real life cases shows that this is both expensive and impractical
and that online digital coaching would be both less expensive and more practical;
the remaining problems are to ﬁnd some effective methods to build and implement
digital coaches. In the actual case we were able to use Excel to implement the
FROV models; Coach 1 would guide estimates of the cash ﬂows for the Excel
model and explain the optimal solutions; Coaches 2–4 would guide parameter
estimates and explain the real options and fuzzy theory parts of the model; Coach 5
would guide simulations with the models and offer explanations of the results.
In the next section we will ﬁnd out how digital coaches could be constructed to
offer the described functionality without supervision of personal coaches.
3
Digital Coaching
The digital coaching systems got started a few years ago as an answer to the
demand on human operators to master advanced automated systems that are used to
monitor and control often complex and very large industrial process systems.
Digital coaching will work on and with data, information and knowledge that is
collected from digital devices, instruments, tools, monitoring systems, sensor sys-
tems, software systems, data and knowledge bases, data warehouses, etc. and then
processed to be usable for the digital systems that will guide and support users. The
processing is done with digital fusion which operates in three phases: data, infor-
mation and knowledge fusion.
Data fusion is the ﬁrst step; a function that combines multiple tuples into one is
called a fusion function and the standard, rather simple operation is a fusion of data
attributes. The traditional way is to deﬁne some ordering relation a priori and then
to keep it updated for continuous use; maintenance is challenging for big, fast data
which is why we want automatic support in modern industrial applications. A better
than the traditional way is to construct order relations automatically when data
attributes to be fused are inspected.
Data fusion builds data sets (or families of data sets). The next step is to extract
process information from the (often big) data sets with analytics methods such as
data mining, statistical analysis, machine learning, computational intelligence,
visualization, etc. The process is continued with analytical techniques to fuse sets of
information to more meaningful summary information—information fusion (cf.
Carlsson et al. [11, 12, 14, 15]). Some early research results show that information
fusion reduces the uncertainty in (social) big data by extracting key (valid, relevant)
factors, cleaning out outliers, high-lighting illogical assumptions, etc. (cf.
Morente-Molinera et al. [28, 29]).
Knowledge fusion builds on ﬁrst data fusion, then information fusion. Knowl-
edge fusion applies taxonomies or ontologies—in the D2I program (funded by
Tekes 340/12) fuzzy ontology was developed and used to detect, identify and deal
with recurring problems in pump valve packages (Carlsson et al. [13–15]). Auto-
mated knowledge builds on natural (or near-natural) language processing,
164
C. Carlsson

information extraction with analytics tools, information integration (or federation),
computational intelligence (soft computing, evolutionary computation, swarm
intelligence, intelligent agents, etc.) (cf. Morente-Molinera et al. [28, 29]).
With the proposed basis in digital fusion we can then look for theory and
technology constructs that could be used for the digital coaches.
One of the approaches to virtual coaches builds on the emerging technology of
embodied conversational agents (ECA’s). ECA’s are animated virtual characters,
displayed on a computer or a mobile device screen. ECA’s play the roles of
teachers, mentors, advisors, social companions, and, increasingly, of virtual coaches
(cf. Hudlicka [23]). The ECA’s engage in natural interaction with humans through
dialogue and non-verbal expression which requires minimal or no training; we will
probably not be aided by animated virtual characters but the core agent constructs
may be useful for the digital coaching when working on data and information
fusion material. With some quick sketching we could work with the following setup
and functionality: Coachi will (i) collect incoming cash ﬂows (data fusion);
(ii) carry out estimates of incoming cash ﬂows (information fusion), and (iii) build
explanations of NPV of action versus option value of postponement (knowledge
fusion).
The virtual trainer systems are becoming popular as supporting services to
ﬁtness and wellness applications; they are typically identiﬁed as three classes
(i) smart phone applications (ii) sensor devices, and (iii) image processing devices.
Sensor data and images are collected and processed through data fusion, which can
be a low level implementation that builds on fast, efﬁcient process monitoring. In
our present context this could be used for working out the parameters needed for the
Markov processes. The “trainer” gives feedback on the progress of the exercise and
offers summary post exercise data for learning and for motivation to keep up the
exercises. This basic functionality appears to be generic and we should bear it in
mind for the digital coaches.
3.1
Coaching with Markov Decision Processes
Fern et al. [19] work out a theory base for personalised AI systems that work as
personal assistants to support human users with tasks they do not fully know how to
carry out. This type of technology has gained much attention in the last 10 years
because of the growing use of automated systems with intelligent functions. Fern
et al. [19] work out a model where the assistant (an AI system) observes the user
(represented as a goal-oriented agent) and must select assistive actions from a
closed set of actions in order to best help the user achieve his goals.
The context studied is a physical environment in which the assistant helps a user
to navigate, follows up on the progress, adjusts the behaviour towards some chosen
goal and continues with sequential adjustments until the user is satisﬁed with the
goal attainment. The functionality is close to the one we have in mind for the
Coaches 1–5. The interesting thing is that this functionality builds on a
Digital Coaching for Real Options Support
165

decision-theoretic model, which is worked out with partially observable Markov
decision processes (POMDPs). Fern et al. [19] work out variations of these Markov
processes to get a formal basis for designing intelligent assistants. A speciﬁc case is
the hidden goal Markov decision processes (HGMDP) that are designed to cover
the application environment and the user’s policy and hidden goals. The HGMDP is
a tuple <S, G, A, A′, T, R, π, IS, IG> where S is a set of states, G is a ﬁnite set of
possible user goals, A is a set of user actions, A′ is the corresponding set of assistant
actions, T is a transition function that decides the transition from s to s′ (element of
S) after the user takes action a (element of A) towards a goal g (element of G); R is
a reward function for both the user and the assistant, π is the user’s (optimal) policy
mapping to the context, and IS the initial and IG the goal states. Markov processes
are generic constructs that can be used to describe complex processes in a fairly
compact form. The digital coaching, we want to build, appears not to be neither a
POMDP nor an HGMDP because (A, A′) are not stochastic but decided by the
ROV model, the transition function T is a function of the ROV and the R could be a
(fuzzy) distance function.
Nevertheless, there are some useful constructs that build on the Fern et al. [19]
theoretical framework. The ﬁrst is a special case where the assistant’s policy is
deterministic for each speciﬁc goal. This opens up for the use of an optimal tra-
jectory tree (OTT) where the nodes represent the states of the MDP reached by the
preﬁxes of optimal action sequences for different goals starting from the initial state.
Each node in the tree represents a state and a set of goals for which it is on the
optimal path from the initial state. The size of the optimal trajectory tree, which we
need to be reasonably compact for computational purposes, is bounded by the
number of goals times the maximum length of any trajectory, which is at most the
size of the state space in deterministic domains. This gives some hints at what
constructs to look for when trying to work out the Coaches 1–2 that decide the
parameter values for the ROV model.
Another interesting result is the approach to solve the problem of selecting an
assistive action. For an HGMDP Fern et al. [19] work out a combination of
bounded look-ahead search and myopic heuristic computations (selecting an action
that has the highest probability of being accepted). By increasing the amount of
look-ahead search the actions returned will be closer to optimal at the cost of more
computation; for many HGMDPs the useful assistant actions can be computed with
relatively little or no search.
We will need some constructs to learn (e.g.) an HGMDP while interacting with
the context, i.e. the assistant should follow how the user interacts with the context
and learn the user’s policy and goal distributions. These constructs would be useful
when the assistant is called upon many times for the same construct (quite often at
irregular intervals) by the same user; a further extension would be for the assistant
to start from basic constructs obtained with one user and then to learn another user’s
policy and goal distributions. The classical approach is to use Maximum Likelihood
estimates of the user’s policy distributions from continuous follow-ups and com-
bine that with estimates of the goal attainment (e.g. as fuzzy distances). Another
approach that Fern et al. [19] propose is to use an MDP model of the context and
166
C. Carlsson

bootstrap the learning of the user policy. This would be useful if the user is near
optimal in his policy choices and will likely select actions that are near-optimal for a
selected goal and an actual context.
3.2
Coaching with Virtual Environments
Fricoteaux et al. [20] work out the use of virtual environments for ﬂuvial naviga-
tion; these environments offer training in easily modiﬁable environmental condi-
tions (wind, current, etc.), which have impacts on the behaviour of a ship; ﬂuvial
navigation would in our context be applied to rapidly and randomly changing
market conditions. The main difﬁculty in ﬂuvial navigation is to anticipate
manoeuvres and the variability of the conditions of the environment. It is interesting
to note that the formal framework to represent and support the decision-making
system builds on classical Dempster-Shafer theory in order to take account of
uncertainty. Unlike the theory of probability, the DS-theory allows for explicit
modelling of ignorance. This can be combined with directed graphs to represent
inﬂuences between variables; if the inference is probabilistic, Bayesian networks
(BNs) can be used; with belief functions there are evidential networks with con-
ditional belief functions (ENCs). Then in turn ENCs have been generalised by
evidential networks with conditional belief functions (DEVNs), etc.—thus there
appears to be constructs available to apply to the building of digital coaches. The
remaining challenge appears to get it done.
Bloksmal and Struik [2] work out a program for coaching farmers using human
health as a metaphor for farm health, which helps both them and the farmers to gain
an understanding of the issues that are crucial for improving the processes and the
productivity of a farm. The coach and the farmer together work out the course of
life of the farm, they learn from what has happened in the history of the farm and
translate images of possible futures into the current state of the farm. The
shorter-term issues, making work more rewarding, improving work efﬁciency and
effectiveness, farm productivity and proﬁt are worked out in this context and
longer-term issues such as organisational and spatial redesign of the farm are
worked on against this background. The coach operates like a physician—“alter-
natingly observing the diseased part and the whole being of an ill person”—and by
referring to this metaphor, opens up similar mechanisms for what may be wrong
with the farm. If we do this skilfully, it will show to what extent the farm resembles
a living and healthy entity and the farmer will get new ideas on how to improve “the
living farm organism”; the use of metaphors takes out the blame from the narrative,
the farmer will not feel that the coach blames him for having done something
wrong.
Bloksmal and Struik [2] show that the process to ﬁnd and describe the identity
and key processes of a farm is not easy; they use a narrative method—the coach
listens to discover the drama behind the facts. This approach could work for our
digital coaching of real options modelling and planning—i.e. the real options
Digital Coaching for Real Options Support
167

models could be good representations of the “drama behind the facts” and would
show the key relations to decide the optimal interval for the postponement of a
decision with explanations that a manger could understand and make use of.
4
Real Options Support for Decisions on a Paper Mill
The fuzzy real options models were used to make a decision on the old paper mill in
the United Kingdom. The decision had two parts (i) to decide if the paper mill
should be closed or not, and if (i) is positive, then (ii) the optimal point in time
when to close the mill (actually the optimal number of periods to postpone the
closing from present time, cf. [21, 22] for details).
The implementation of the real options theory for a practical case entails sim-
pliﬁcations, mostly due to lack of either data, information or knowledge, or in order
to be able to build the actual real options model. This is a typical trade-off we have
to make in order to use a theory, but if we simplify too much we will stray too far
from the core of the theory which makes the results questionable (cf. [26, 27]).
We built scenarios in order to outline possible developments with the infor-
mation and knowledge we could get about the plant, the product lines, the cus-
tomers, the competition, the markets and the country context offered by the UK.
This produced data for us to work on but data came from different sources and
related to different periods, which gave somewhat inconsistent and heterogeneous
scenarios. As we could not be very precise, the managers argued that simpliﬁcations
would not add much to the uncertainty of the views of the future worlds.
Each scenario assumes a match between sales and production, which is a sim-
pliﬁcation; in reality there are signiﬁcant, stochastic variations in sales which cannot
be matched by the production. Since the planning assumed no capital investments,
there will be no costs in switching between the scenarios (which is another sim-
pliﬁcation). We worked out some exercises on the possibilities to switch in the
future as (real) options for senior management; the opportunity to switch to another
scenario is a call option. The option values build on the estimates of future cash
ﬂows, which are the basis for the upward/downward potentials.
Senior management (reluctantly) adopted the view that options can exist and that
there is a not-to-decide-today possibility for the close/not-close decision. The
motives to include options into the decision process built on the following logic:
• New information changes the decision situation (“good or bad news”)
• Consequently, new information has a value and it increases the ﬂexibility of the
management decisions
• The value of the new information can be analysed to enable the management to
make better informed decisions
In the discussion, we were able to show that companies fail to invest in valuable
projects because planners overlook and leave out the options embedded in a project
168
C. Carlsson

from the proﬁtability analysis. The real options approach shows the importance of
timing as the real option value is the opportunity cost of the decision to wait in
contrast with the decision to act immediately.
We were then able to give the following practical description of how to form the
option value:
Option value = Discounted cash flow
× Value of uncertainty usually standard deviation
ð
Þ −Investment
× Risk free interest
If we compare this sketch with the decision to close/not-close the production
plant with the FROV models we introduced we cannot avoid the conclusion that
things are much simpliﬁed. The substance of the decision is not the same, which we
could not describe very well without getting into the stochastic processes at the core
of the real options theory—and this was beyond what we could reasonably expect
the managers to follow. Then we run into another, more serious problem—senior
management will distrust results of an analysis they cannot evaluate and verify with
numbers they recognize or can verify as “about right”.
We found out—somewhat unexpectedly—that we could build the fuzzy real
options models using Excel; this solved part of the problem, as the managers we
worked with were semi-professional Excel users and could ﬁgure out how the
models work by experimenting with Excel. Another part of the solution was that we
could include actual numbers from the plant in the scenarios and the managers
could judge from the outcome that they were “about right”, “reasonable” and
“veriﬁable”. Then we (of course) added NPV to the Excel models to allow the
managers to test their intuitive understanding with veriﬁcations carried out through
familiar NPV calculations.
We found out that the added functionality of Coach 1 and Coach 2 would have
simpliﬁed the implementation and use of the real options models; senior decision
makers want to know how the key factors work and interact, what the outcome is
going to be and then what consequences can be expected from the outcome (Fig. 1).
In the same way, the combined use of Coach 1 and Coach 5 will be helpful to
work out and explain the scenario combinations over time; Coach 5 will show the
trade-off between the net present value of action now (closing the paper mill now)
and the option value of postponing the decision (option value = 0 if action now)
(Fig. 2).
The detailed reports are for the managers so that they could check how rea-
sonable and veriﬁable the outcome could be (the numbers are modiﬁed and the
timeframe is altered as the data was highly conﬁdential). Coach 3 explains what the
possibilistic (fuzzy) values are, explains the FROV and shows and explains
the assumptions underlying the NPV of cash ﬂows (which are fuzzy numbers).
Digital Coaching for Real Options Support
169

Coach 4 works out and explains the ranking of the FROV values, which actually is
a ranking of fuzzy numbers (cf. [12, 13]). The functionality of the last two coaches
show issues that were questioned and tested in order to get at the core of the models
and the results (Fig. 3).
5
Summary and Future Scenarios
The theory framework built by and for the classical Operations Research and
Management Science since the early 1950s is now in the process of being trans-
formed by (Business) Analytics, which is getting the attention of major corporations
and senior management. The research groups working at IAMSR have been part of
this process and have specialised in working out and using analytics methods,
which implement soft computing theory and algorithms (cf. [17, 22, 26, 27]). This
has turned out to be very effective and useful for planning, problem solving and
decision making in “big data” environments; the “big data” is one of the challenges
Fig. 1 Scenario alternatives included estimated cash ﬂows, compound rate, option value of a
postponement, uncertainty of incoming cash ﬂows, calculated FROV and optimal postponement T
170
C. Carlsson

of the modern digital economy and for which we propose that fuzzy sets would
offer instruments for breakthrough research.
Analytics adds value to management; it promotes data-driven and analytical
decision-making and “reinvented” fact-driven management. Analytics builds on
recent software improvements in information systems that has made data, infor-
mation and knowledge available in real time in ways that were not possible for
managers only a few years ago (Davenport and Harris, [18]).
In the context of the digital economy common wisdom ﬁnds that real-time
management is a necessity as operations should be planned and carried out in a fast
changing and complex environment where careful and thoughtful management will
be bypassed by fast, innovative approaches (which may turn out to be of inferior
quality, but have then already established sustainable competitive positions).
Real-time management is challenged by “big data” and the necessity for fast pro-
cessing using advanced analytics methods. The advanced analytics would require
postdoc-qualiﬁed managers—these are rather scarce in senior management posi-
tions. Thus there will be a need to reinstate “coaching” functions with the advanced
analytics methods to tell/explain to the users what can/should be done, how it
Fig. 2 Outcome of scenario alternatives worked out as FROV (ﬂexibility), NPV and a binomial
process value
Digital Coaching for Real Options Support
171

should be carried out, what the results are and what they mean, and how they
should be applied (with explanations of what could/should not be done).
We have worked out a context and a scenario for digital coaching with the help
of real options modelling. Black and Scholes introduced the options theory to
decide the price for options on ﬁnancial assets; Merton proved that similar options
modelling could be applied to real assets and used to work out the pricing of
investment alternatives based on future cash ﬂows for the alternatives. Merton’s
method—named real options analysis—became popular among professionals as it
offered more ﬂexibility than the classical NPV methods.
We illustrated the development and use of fuzzy real options models with the
case of closing (or not closing or closing later) an old paper mill in the UK. The rule
we worked out was that we should only close the plant now if the net present value
of this action is high enough to compensate for giving up the value of the option to
wait. Because the value of the option to wait vanishes right after we decide to close
the plant, this loss in value is actually the opportunity cost of our decision. This is a
principle based in theory but it turned out that the management team could well
accept the principle—and then wanted to ﬁnd out how to use it.
We discussed the need for coaching, but have found through experience from
real life cases that it would be both expensive and impractical to try to ﬁnd and use
experienced human coaches. The alternative is online digital coaching and we only
need to ﬁnd some effective methods to build and implement digital coaches. We
found out that there are not many useful approaches offered in the literature, the
closest we could get was an elaborate framework built around Markov decision
processes, which was not actually up to the task. In order to outline what the
Fig. 3 Numerical outcome reports on the scenario alternatives
172
C. Carlsson

coaches should do we worked out the processes in terms of the models we
developed for the old paper mill in the UK. We were able to use Excel to implement
the FROV models in the actual case, which was a bit of a surprise. We could then
describe that Coach 1 would guide estimates of the cash ﬂows for the Excel model
and explain the optimal solutions; Coaches 2–4 would guide parameter estimates
and explain the real options and fuzzy theory parts of the model; Coach 5 would
guide simulations with the models and explain the results. We will work out some
possible theoretical frameworks for the digital coaches in a coming series of papers.
We will conclude with some arguments for why it will make sense to work with
analytical models even in a context, which is not recognized as a domain for
analytics. Kahneman [24] relates the case of Orley Ashenfelter, a Princeton econ-
omist and wine lover, who wanted to ﬁnd a way to predict the future value of ﬁne
Bordeaux wines from information available in the year they are made (cf. [24]). He
was of course well aware that he was stepping on the sensitive toes of
world-renowned experts who claimed that they could predict the value development
for individual wines over years to come and also in which year the wines will reach
their peak quality and highest price. The experts built their judgement on tasting the
wines and decades of experience of and insight in the wine markets; Ashenfelter
built his predictions on regression analysis and an effective use of statistics tools—
he had no possibility to actually taste the wines. Ashenfelter collected statistics on
London auction prices for select mature red Bordeaux wines 1990–91 (sold in lots
of a dozen bottles); mature red Bordeaux were deﬁned as vintages 1960–1969 and
the wines selected came from six Bordeaux chateaux which are large producers
with a reputation to have produced high quality wine for decades—or centuries in
some cases. Ashenfelter found out that the quality of the Bourdeaux wines is
decided by (i) the age of the vintage (ii) the average temperature over the growing
season (April–September) (iii) the amount of rain in September and August (less
rain gives better wine), and (iv) the amount of rain preceding the vintage (October–
March). These four factors are all measurable and built on published and easily
veriﬁable facts; Aschenfelter collected data on the vintages 1952–1980 and built a
regression model with the four factors which turned out to explain about 80% of the
variation in the average price of Bordeaux wine vintages. His point is that the future
quality of Bordeaux wines can be worked out without tasting the wines or intro-
ducing any kind of subjective judgements. He used his models to predict the price
development for new vintages of Bordeaux (the correlation between prediction and
actual prices is above 0.90) which he has shared with a crowd of followers that are
investing in promising, good vintages. Through his models he has also found a few
very good vintages that are under-priced in the market and which he and his friends
have bought and much enjoyed.
First we can notice that Aschenfelter made sure that he had observations on large
selections of wine over 10 years from six large chateaux—but only in Bordeaux in
order to reduce the number of external factors that inﬂuence the wine production
but are not relevant for the key issues of his study. Second, his models forecast
future prices (years, and even decades into the future) more accurately than the
current market prices of young wines do that build on expert estimates;
Digital Coaching for Real Options Support
173

this challenges economics theory that claims that market prices (in effective mar-
kets) will reﬂect all information on the products. Third, experts make judgements
that are inferior to algorithms; Kahneman (cf. [24] p. 224) argues that some reasons
for this is that experts try to be clever, to think outside the box and to work with
(too) complex combinations of features to make their predictions. Complexity may
work in speciﬁc cases but will reduce validity in most cases. There is a second point
to be made—analytics, when the proper methods are developed and used, will give
insight that intuition and experience will not be able to produce. This is a lesson
learned for the digital economy where it is claimed that the dynamics of the market
and the need to make (almost) real time decisions in order to stay competitive
makes it necessary to forego analytics and rely on the intuition and experience of
visionary managers and executives.
References
1. Alcaraz Garcia, F.: Real options, default risk and soft applications, 82 pp. TUCS Dissertations
(2006)
2. Bloksmal, J.R., Struik, P.C.: Coaching the process of designing a farm: using the healthy
human as a metaphor for farm health. NJAS 54(4), 413–429 (2007)
3. Borgonovo, E., Peccati, L.: Sensitivity analysis in investment project evaluation. Int. J. Prod.
Econ. 90, 17–25 (2004)
4. Carlsson, C.: On the relevance of fuzzy sets in management science methodology. In:
Zimmermann, H.J., Zadeh, L.A., Gaines, B.R. (eds.) Fuzzy Sets and Decision Analysis, TIMS
Studies in Management Sciences, vol. 20, pp 11–28. North-Holland, Amsterdam (1984)
5. Carlsson, C., Fullér, R.: Capital budgeting problems with fuzzy cash ﬂows. Mathw. Soft
Comput. 6, 81–89 (1999)
6. Carlsson, C., Fullér, R.: On optimal investment timing with fuzzy real options. In:
Proceedings of the EUROFUSE 2001 Workshop on Preference Modeling and Applications,
pp. 235–239 (2001)
7. Carlsson, C., Fullér, R.: On possibilistic mean value and variance of fuzzy numbers. Fuzzy
Sets Syst. 122, 315–326 (2001)
8. Carlsson, C., Fullér, R.: Fuzzy Reasoning in Decision Making and Optimization. Springer,
Berlin, Heidelberg (2002)
9. Carlsson, C., Fullér, R.: A fuzzy approach to real option valuation. Fuzzy Sets Syst. 139, 297–
312 (2003)
10. Carlsson, C., Fullér, R., Majlender, P.: A fuzzy real options model for R&D project
evaluation. In: Proceedings of the 11th IFSA World Congress (2005)
11. Carlsson, C.: Soft computing in analytics: handling imprecision and uncertainty in strategic
decisions. Fuzzy Econ. Rev. XVII(2), 3–21 (2012)
12. Carlsson, C., Heikkilä, M., Mezei, J.: Possibilistic bayes modelling for predictive analytics.
In: Proceedings of 15th IEEE International Symposium on Computational Intelligence and
Informatics, Budapest, pp. 15–20 (2014)
13. Carlsson, C., Mezei, J., Wikström, R.: Aggregating linguistic expert knowledge in type-2
fuzzy ontologies. Appl. Soft Comput. 35, 911–920 (2015)
14. Carlsson, C., Heikkilä, M., Mezei, J.: Fuzzy entropy used for predictive analytics. In:
Kahraman, C. (ed.) Fuzzy Sets in its 50th year. New Developments, Directions and
Challenges, Studies in Fuzziness, vol. 341, pp. 23. Springer (2016)
174
C. Carlsson

15. Carlsson, C.: Imprecision and uncertainty in management—the possibilities of fuzzy sets and
soft computing. NOEMA XV Rom. Acad. Sci. 89–114 (2016)
16. Collan, M., Carlsson, C., Majlender, P.: Fuzzy black and scholes real options pricing.
J. Decis. Syst. 12, 391–416 (2003)
17. Collan, M: Giga-investments: modeling the valuation of very large industrial real investments,
57 pp. TUCS Dissertations, Turku (2004)
18. Davenport, T.H., Harris, J.G.: Competing on Analytics: The New Science of Winning.
Harvard Business School Press, Boston (2007)
19. Fern, A., Natarajan, S., Judah, K., Tadepalli, P.: A decision-theoretic model of assistance.
J. Artif. Intell. Res. 49, 71–104 (2014)
20. Fricoteaux, l., Thouvenin, I., Mestre, D.: GULLIVER: A decision-making system based on
user observation for an adaptive training in informed virtual environments. Eng. Appl. Artif.
Intell. 33, 47–57 (2014)
21. Heikkilä, M., Carlsson, C.: A fuzzy real options model for (not) closing a production plant: an
application to forest industry in ﬁnland. In: Proceedings of the 12th Annual International
Conference on Real Options, Rio de Janeiro (2008)
22. Heikkilä, M.: R&D Investment Decisions with Real Options—Proﬁtability and Decision
Support, Åbo Akademi University Press, Åbo (2009)
23. Hudlicka, E.: Virtual training and coaching of health behavior: example from mindfulness
meditation training. Patient Educ. Couns. 92(2013), 160–166 (2013)
24. Kahneman, D.: Thinking Fast and Slow. Penguin Books, London (2011)
25. Liberatore, M., Luo, W.: INFORMS and the analytics movement: the view of the
membership. Interfaces 41(6), 578–589 (2011)
26. Majlender, P.: A normative approach to possibility theory and soft decision support, 54
pp. TUCS Dissertations, Turku (2004)
27. Mezei, J.: A quantitative view on fuzzy numbers, 142 pp. TUCS Dissertations (2011). Turku
28. Morente-Molinera, J.A., Wikström, R., Carlsson, C., Viedma-Herrera, E.: A linguistic mobile
decision support system based on fuzzy ontology to facilitate knowledge mobilization. Decis.
Support Syst. 81 (2016)
29. Morente-Molinera, J.A., Mezei, J., Carlsson, C., Viedma-Herrera, E.: Improving supervised
learning classiﬁcation methods using multi-granular linguistic modelling and fuzzy entropy.
Trans. Fuzzy Syst. (2016)
30. Wang, X.: Fuzzy real options analysis in patent related decision making and patent valuation.
Åbo Akademi Dissertations, Åbo (2015)
31. Zadeh, L.: Fuzzy sets. Inf. Control 8(3), 338–353 (1965)
Digital Coaching for Real Options Support
175

An Analysis of Decision Criteria
for the Selection of Military Training
Aircrafts
Juan M. Sánchez-Lozano, M.A. Socorro García-Cascales
and María T. Lamata
Abstract The Spanish Minister of Defense needs to replace the current military
training aircrafts by other models to meet current training needs in the Spanish Air
Force Academy. In order to know the main features that the candidate aircrafts
should have, there is a need to take into account the knowledge and experience of
experts in this speciﬁc ﬁeld, such as trained test pilots and ﬂight instructors. In this
way, it will be possible to recognize the main technical criteria to consider. This
study shows a case study that allowed obtaining not only the preferences of an
expert’s group, but also the importance of the considered criteria. Given that the
criteria information provided by the experts has different nature, with qualitative
criteria (human factors, ﬂying and handling qualities, etc.) coexisting with quan-
titative criteria (service ceiling, stalling speed, endurance, etc.), the joint use of
linguistic labels and numerical values is needed. Therefore, a survey focused on the
fuzzy AHP (Analytic Hierarchy Process) methodology is proposed to extract the
knowledge from the experts group and ﬁnally obtain a unique set of weights for
the criteria.
J.M. Sánchez-Lozano (✉)
Centro Universitario de la Defensa. Academia General del Aire.
Universidad Politécnica de Cartagena, Murcia, Spain
e-mail: juanmi.sanchez@cud.upct.es
M.A.S. García-Cascales
Dpto. de Electrónica, Tecnología de Computadoras y Proyectos,
Universidad Politécnica de Cartagena, Murcia, Spain
e-mail: socorro.garcia@upct.es
M.T. Lamata
Dpto. Ciencias de la Computación e Inteligencia Artiﬁcial,
Universidad de Granada, Granada, Spain
e-mail: mtl@decsai.ugr.es
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_10
177

1
Introduction
Nowadays, the Air Force Academy uses two training aircrafts: the model
ENAER T.35C Tamiz for elementary basic education and the model CASA C-101
Aviojet for advanced basic education. These aircraft have been operating from the
1980s so that, as a result of the continuous advancement of aviation technology and
the high number of ﬂight hours that they have seen, in the near future it will be
necessary to replace them by other models to meet current training needs.
From the point of view of training aircrafts and, as occurs in the subsequent
stages of design [1, 2], decision-making is an intellectual activity which is neces-
sary and essential to face. Before taking any decisions, facts, knowledge and
experience should be gathered to assess the context of the problem. In this type of
decision-making process, a large number of essential criteria is involved. To resolve
them, it is therefore advisable to employ tools such as Multi-Criteria Decision
Making (MCDM), a process whose use is widespread today, not only in the military
ﬁeld [3–5], but also in many research ﬁelds [6–8].
In addition, when selecting the best training aircraft a number of criteria of
different nature should be taken into account, such as quantitative criteria (service
ceiling, stall speed, fuel range, etc.), and qualitative criteria (cockpit ergonomics,
feelings of instructor, etc.). In order to model and evaluate the latter type of criteria,
fuzzy logic techniques [9–13] are a good alternative, not only to operate in an
isolated way but also combined with pseudo-Delphi techniques and MCDM
methods (AHP [14]; TOPSIS [15]; ELECTRE [16], etc.). Although some of the
aforementioned multi-criteria methods are able to apply fuzzy logic and evaluate
the potential alternatives, the AHP methodology also allows obtaining the weight of
the criteria. That is the main reason why in this study case a pseudo-Delphi tech-
nique has been combined with fuzzy AHP methodology.
From the point of view of the Spanish Air Force, the Air Staff and the Logistics
Support Command of this force are the main decision-makers. Nevertheless, it is
advisable to make a preliminary assessment taking into account the most signiﬁcant
technical criteria which also reﬂect the experience of important expert groups such
as trained test pilots and ﬂight instructors of the Spanish Air Force.
Therefore, our aim here is to determine the relative importance of the main
technical criteria and then, to transform such importance into weights that should be
later used in a MCDM scheme. The problem will be solved using the AHP
methodology to obtain the weights of the criteria that inﬂuence the decision. Fur-
thermore, given that the criteria are both qualitative and quantitative, both methods
will be combined with fuzzy logic through the design and development of a survey
to experts in the ﬁeld of military training aircraft.
This chapter is divided into four sections: Sect. 2 will deﬁne the criteria that
inﬂuence the decision-making, in Sect. 3 the fundamentals of fuzzy sets and the
AHP methodology will be described. Section 4 will explain the way in which the
weights of the considered criteria are obtained and the results. The ﬁnal section will
detail the conclusions of this study.
178
J.M. Sánchez-Lozano et al.

2
Decision Criteria for Evaluating Military Training
Aircraft
The mission of the Spanish Air Force Academy is to train future ofﬁcers of the
Spanish Air Force by providing them with academic, military and aeronautical
teaching. Although today there are many aircraft, they are usually classiﬁed
according to their use [17]. In the case of the Spanish Air Force Academy, these
aircrafts should have speciﬁc features that allow the future ofﬁcers to carry out their
basic and advanced education [18]. Due to that, it is highly advisable to identify the
main technical criteria that inﬂuence the decision; these data have been obtained
from [17, 19–22] and an advisory group composed by instructors and ﬂight per-
sonnel of the Air Force Academy. The chosen criteria are the following:
• C1: Service ceiling (ft), the highest operating altitude at which the maximum
achievable rate of climb is 100 ft/min and the aircraft can bear the atmosphere
and operate efﬁciently.
• C2: Cruising speed (kt), the constant and uniform speed in which an aircraft is
able to ﬂy with normal conditions of pressure and temperature.
• C3: Stalling speed (kt), the minimum speed in which the wings maintain lift at
ﬂameout.
• C4: Endurance (minutes), the maximum time in which an aircraft can remain in
the air until all fuel has expired.
• C5: Positive Limit Load Factor (+ G), the maximum value of positive accel-
eration forces which can withstand the airframe.
• C6: Negative Limit Load Factor (−G), the minimum value of positive accel-
eration forces which it can withstand the airframe.
• C7: Take-off distance (ft), the minimum distance required by the aircraft to
accelerate along the runway until it reaches a speed at which it can generate
sufﬁcient aerodynamic lift to overcome its weight (in standard sea level
conditions).
• C8: Landing distance (ft), the minimum distance required by the airplane to land
(in standard sea level conditions).
• C9: Human factors: the comfort conditions inside the cockpit (beginner pilot and
instructor positions)
• C10: Flying and handling qualities, conﬁdence that the instructor or beginner
pilot on the plane to perform complex training exercises.
• C11: Security systems, devices of the aircraft for responding face with setbacks
or unexpected situations (ejection systems, sensors, etc.)
• C12: Maneuvering Capability, software tools capable of being conﬁgured and
adapted to several models of education (elementary and advanced stage)
In order to determine the relative importance of these technical criteria, we have
access to a group of experts (trained test pilots and ﬂight instructors of the Spanish
Air Force) who will answer a survey based on the application of the methodology
described.
An Analysis of Decision Criteria for the Selection …
179

3
Methodology
Fuzzy Sets
The fuzzy set theory, introduced by Zadeh [9] to deal with vague, imprecise and
uncertain problems has been used as a modelling tool for complex systems that can
be controlled by humans but are hard to deﬁne precisely. Examples of fuzzy sets are
classes of objects (entities) characterized by such adjectives as large, small, serious,
simple, approximate, etc. The main reason for this is that in a real world, there are
not crisp or real boundaries which separate those objects which belong to the
classes in question from those which do not [23]. A collection of objects (universe
of discourse) X has a fuzzy set A described by a membership function fA with
values in the interval [0,1] [24].
In this chapter, we only make reference to the operations on a triangular
membership function through the fuzzy number sets that will be used in the study
case. The basic theory regarding Triangular Fuzzy Numbers (TFN) is described in
detail in [9]. Herein, we only make reference to the operations on fuzzy sets that we
will use in the application.
Deﬁnition 1.- A1 and A2 are two TFN deﬁned by the triplets (a1, b1, c1) and (a2,
b2, c2), respectively. For this case, the necessary arithmetic operations with positive
fuzzy numbers are:
(a) Addition:
A1⊕A2 = a1 + a2, b1 + b2, c1 + c2
½

ð1Þ
(b) Subtraction:
A1ΘA2 = A1 + −A2
ð
Þ = a1 −c2, b1 −b2, c1 −a2
½

ð2Þ
(c) Multiplication:
A1⊗A2 = a1 × a2, b1 × b2, c1 × c2
½

ð3Þ
(d) Division:
A1ϕA2 =
a1, b1, c1
ð
Þ ⋅1 ̸c2, 1 ̸b2, 1 ̸a2
ð
Þ
½

ð4Þ
180
J.M. Sánchez-Lozano et al.

When 0 ≠a2, b2, c2
½

(e) Scalar Multiplication:
k◦T1 = k◦a1, k◦b1, k◦c1
ð
Þ
ð5Þ
(f) Root:
T1 ̸2
1
= a1 ̸2
1 , b1 ̸2
1 , b1 ̸2
1


ð6Þ
Analytic Hierarchy Process (AHP)
The AHP methodology, proposed by Saaty [14], has been accepted by the
international scientiﬁc community as a robust and ﬂexible MCDM tool to deal with
complex decision problems. Basically, AHP has three underlying concepts:
• Structuring the complex decision as a hierarchy of goal, criteria and alternatives.
• Pair-wise comparison of elements at each level of the hierarchy with respect to
each criterion on the preceding level, and ﬁnally
• Vertically synthesizing the judgements over the different levels of the hierarchy.
AHP attempts to estimate the impact of each one of the alternatives on the
overall objective of the hierarchy. In this case, we shall only apply the method to
obtain the criteria weights.
We assume that the quantiﬁed judgements provided by the decision-maker on
pairs of criteria (Ci, Cj) are contained in an n x n matrix as follows:
C =
C1
C2
⋮
Cm
C1
C2
⋯
Cn
c11
c12
⋯
c1n
c21
c22
⋯
c2n
⋮
⋮
⋱
⋮
cm1
cm2
⋯
cmn
0
B
B
@
1
C
C
A
For instance, the c12 value represents an approximation of the relative impor-
tance of C1 to C2, i.e., c12 ≈(w1/w2). This can be generalized and the statements
below can be concluded:
∙cij≈wi ̸wj


, i, j = 1, 2, . . . , n
∙cii≈wi ̸wi
ð
Þ = 1, i = 1, 2, . . . , n
• If cij = α, α ≠0, then cji = 1 ̸α, i = 1, 2, . . . , n
• If Ci is more important than Cj, then cij ≅wi ̸wj


> 1
An Analysis of Decision Criteria for the Selection …
181

This implies that the matrix C should be a positive and reciprocal matrix with 1’s
on the main diagonal. Hence, the decision maker only needs to provide value
judgments in the upper triangle of the matrix. The values assigned to cij according
to the Saaty scale usually lie in the interval of 1-9 or their reciprocals.
It can be shown that the number of judgments (L) needed in the upper triangle of
the matrix is:
L = n n −1
ð
Þ ̸2
ð7Þ
where n is the size of the matrix C.
As the reader can observe, there are both qualitative and quantitative criteria, so
it is necessary to transform the Saaty’s scale to fuzzy numbers. Therefore, Table 1
presents the decision-maker’s linguistic preferences in the fuzzy pairwise com-
parison process.
The vector of weights is the eigenvector corresponding to the maximum
eigenvalue “λmax” of the matrix C. The traditional eigenvector method of estimating
weights in AHP yields a way of measuring the consistency of the referee’s pref-
erences arranged in the comparison matrix.
In AHP problems, where the values are fuzzy not crisp, instead of λ using the
eigenvector as an estimator of the weight, we will use the geometric normalized
average, expressed by the following expression:
wi =
∏n
j = 1 aij, bij, cij



1 ̸n
∑m
i = 1
∏n
j = 1 aij, bij, cij



1 ̸n
ð8Þ
where, (aij, bij, cij) is a fuzzy number.
Additionally, to obtain the weight vector, the normalizing operation must be
used; this will be achieved through expression (9).
wcia, wcib, wcic
ð
Þ =
cia
∑n
i = 1 cic
,
cib
∑n
i = 1 cib
,
cic
∑n
i = 1 cia

	
ð9Þ
Table 1 Fuzzy Scale of valuation in the pair-wise comparison process [25]
Labels
Verbal judgments of preferences between
criterion i and criterion j
Triangular fuzzy scale and
reciprocals
(II)
Ci and Cj are equally important
(1, 1, 1)/(1,1,1)
(M + I)
Ci is slightly more important than Cj
(2, 3, 4)/(1/4,1/3,1/2)
(+I)
Ci is strongly more important than Cj
(4, 5, 6)/(1/6,1/5,1/4)
(Mu + I)
Ci is very strongly more important than Cj
(6, 7, 8)/(1/8,1/7,1/6)
(Ex + I)
Ci is extremely more important than Cj
(8, 9, 9)/(1/9,1/9,1/8)
182
J.M. Sánchez-Lozano et al.

4
Determining the Criteria Importance
Not all the criteria which have inﬂuence in this kind of decision problems have the
same importance. Besides, although there are decision problems that could be
similar, the selection of the criteria depend of the speciﬁc necessities of each
country. Therefore, not only it is important to carry out an appropriate selection of
criteria, but also to choose the way of obtaining their weights. For instance, pre-
vious studies [22, 26] have determined the weights of criteria via direct assignment.
However, in this study, the way of obtaining these weights has been through
preferences of an experts group.
The group of experts involved in the decision process consisted of six experts
specialized in this speciﬁc ﬁeld (three trained test pilots and three ﬂight instructors
of the Spanish Air Force).
According to expression (7), 66 questions should be answered by each one of the
experts. Despite the huge amount of work needed, it is possible that some incon-
sistent matrices can be generated. In order to decrease the inconsistency in this
speciﬁc case study and to reduce the amount of work required for each expert, we
reduced the number of questions in such a way that no loss of relevant information
is produced [27, 28]. Therefore, we propose an alternative method, which only
requires making (n −1) comparisons. For that purpose, a questionnaire similar to
[29] was carried out. This questionnaire also allow us to reduce the uncertainty and
imprecision in the proposed problem.
4.1
Fuzzy-Delphi-AHP Survey
The methodology used for the extraction of the experts’ knowledge is a
pseudo-Delphi technique, since the members who are part of the decision-making
do not interact at any time. In order to do this, a series of questionnaires were
distributed among the six participants in this process so that they could choose the
answers they considered most appropriate, in order to reduce the uncertainty and
vagueness involved with the problem presented.
The questionnaire designed has two clearly different parts. The ﬁrst one consists
of the presentation of the decision problem where the variables employed and the
work methods to carry out are detailed. The experts were asked if the approach
made to solve the problem was suitable and if they agreed with it. The six experts
gave an afﬁrmative answer and therefore it was possible to carry on with the survey
(second part of the questionnaire).
It is known that if one criterion is more important than another, it should be
considered that said criterion has a greater weight than the other. Therefore, the rest
of the survey was focused on the following group of questions:
An Analysis of Decision Criteria for the Selection …
183

a. Do you think that the twelve criteria considered have the same importance?
If the answer is afﬁrmative, the weight associated with criterion Cj is wj = 1/m,
j = 1,2,…,m. If on the contrary, experts consider that not all the criteria have the
same importance, then it is appropriate to proceed to the next question of the
survey.
The next step will be to ﬁnd the extent to which one criterion is more important
than another, this degree of importance will be analyzed to be able to assign a
weight to each criterion. For example, when indicating that a particular criterion has
a higher weight than the rest of the criteria, it is declared that this is the most
important criterion. Forthwith, the weights of the criteria will be used to quantify
their importance.
The six experts have considered that certain criteria should have a greater weight
than others. Therefore, those weights need to be determined.
b. Write the order of importance among the twelve criteria (Table 2).
As can be seen in Table 2, the six experts believe the importance of the criteria
to be different, although they differ in the order of importance of the criteria.
Analyzing the above table, experts indicate that criteria C9, C10 and C11 are the
most important criteria. Due to that, these criteria will have larger weights.
Once the expert has indicated the order of importance, the next question would
be considered:
c. Compare the criterion chosen in ﬁrst place with respect to that considered
secondly and successively, using the following labels, {(II), (M +), (+I),
(Mu + I), (Ex + I)} according to the meanings in Table 1.
To determine the weights of the criteria, as has been discussed, a pair-wise
comparison has been made. Using Expert 1 as an example, in Fig. 1 his appreci-
ation by pair-wise comparison is shown.
The meaning is as follows: criterion C11 is extremely more important than C4, C5
and C6, with respect to C2, C3, C7 and C8 it is very strongly more important,
Table 2 Order of importance of criteria for each of the experts
E1
C11 = C10 > C9 > C12 = C1 > C2 = C3 = C7 = C8 > C4 = C5 = C6
E2
C9 = C10 = C11 > C3 = C7 = C8 > C4 = C1 = C2 = C5 = C6 > C12
E3
C11 > C10 = C9 = C12 > C4 > C3 = C5 = C2 = C7 > C6 = C1 = C8
E4
C11 > C12 = C7 = C9 > C10 = C8 = C2 > C1 = C3 = C4 > C5 = C6
E5
C11 > C9 = C10 > C1 = C2 = C3 = C4 = C5 = C6 = C7 = C8 = C12
E6
C11 > C9 = C10 = C12 = C2 = C3 > C7 = C8 = C5 > C6 = C4 = C1
Fig. 1 Valuations given by E1
184
J.M. Sánchez-Lozano et al.

with respect to C12 and C1 it is strongly more important, with respect to C9 it is
slightly more important and with respect to C10 is equally important.
This, translated to the fuzzy numbers according to Table 1, gives the results
shown in Fig. 2.
Taking into account [30] and operation (9), the weights of the considered criteria
are obtained (Fig. 3).
The information detailed above for E1 would also be carried out for the other
experts. The normalized weights associated with the corresponding criterion Cj,
j = 1,2,…,12 given by each of the experts can be seen in Table 3.
Analyzing the above table, criterion C11 (security systems) has the maximum
score for each of the experts; this criterion is equally important to criterion C10
(ﬂying and handling qualities) for expert 1, and equally important to criteria C10 and
C9 (human factors) for expert 2. This expert also considers as the second most
important criteria C3 (stalling speed), C7 (take-off distance) and C8 (landing dis-
tance). Conversely, the least important criterion for this expert is C12 (tactical
capability).
The weights of the criteria for expert 3 are similar. According to this expert,
criterion C11 is also the highest rated, with the second most important criteria being
C9, C10 and C12. The least important criteria are C1 (service ceiling), C6 (negative
limit load factor) and C8.
Fig. 2 Matrix of decision making for E1
Fig. 3 Criteria weight for E1
An Analysis of Decision Criteria for the Selection …
185

Apart from criteria C9 and C12, expert 4 also considers C7 as the second most
important criterion. The least important criteria for this expert are C5 and C6
(positive and negative limit load factors).
According to expert 5, criteria C9 and C10 are the second most important criteria,
while for that expert the remaining criteria have the same importance.
Expert 6 estimates that there is a criteria group consisting of C2 (cruising speed),
C3, C9, C10 and C12 which have the following score after that of the highest
criterion (C11). The least important criteria are C1, C4 (endurance) and C6.
In order to unify the weights of the obtained criteria and to stablish a speciﬁc
weight for each one of the criteria, a homogeneous aggregation will be carried out.
i.e., all experts are equally important in the decision, as a measure of aggregation
the arithmetic average will be used (expression 10).
Table 3 Weights of criteria for the six experts (heterogeneous aggregations)
Normalized (Expert 1)
Normalized (Expert 2)
Normalized (Expert 3)
C1
[0.041
0.055
0.073]
[0.023
0.030
0.037]
[0.029
0.036
0.045]
C2
[0.031
0.039
0.049]
[0.023
0.030
0.037]
[0.033
0.046
0.061]
C3
[0.031
0.039
0.049]
[0.046
0.069
0.111]
[0.033
0.046
0.061]
C4
[0.027
0.031
0.037]
[0.023
0.030
0.037]
[0.044
0.064
0.091]
C5
[0.027
0.031
0.037]
[0.023
0.030
0.037]
[0.033
0.046
0.061]
C6
[0.027
0.031
0.037]
[0.023
0.030
0.037]
[0.029
0.036
0.045]
C7
[0.031
0.039
0.049]
[0.046
0.069
0.111]
[0.033
0.046
0.061]
C8
[0.031
0.039
0.049]
[0.046
0.069
0.111]
[0.029
0.036
0.045]
C9
[0.062
0.092
0.146]
[0.183
0.207
0.223]
[0.066
0.107
0.182]
C10
[0.247
0.275
0.293]
[0.183
0.207
0.223]
[0.066
0.107
0.182]
C11
[0.247
0.275
0.293]
[0.183
0.207
0.223]
[0.264
0.322
0.364]
C12
[0.041
0.055
0.073]
[0.020
0.023
0.028]
[0.066
0.107
0.182]
Normalized (Expert 4)
Normalized (Expert 5)
Normalized (Expert 6)
C1
[0.031
0.044
0.059]
[0.039
0.058
0.083]
[0.036
0.040
0.049]
C2
[0.042
0.062
0.088]
[0.039
0.058
0.083]
[0.053
0.072
0.098]
C3
[0.031
0.044
0.059]
[0.039
0.058
0.083]
[0.053
0.072
0.098]
C4
[0.031
0.044
0.059]
[0.039
0.058
0.083]
[0.036
0.040
0.049]
C5
[0.028
0.034
0.044]
[0.039
0.058
0.083]
[0.040
0.052
0.066]
C6
[0.028
0.034
0.044]
[0.039
0.058
0.083]
[0.036
0.040
0.049]
C7
[0.063
0.103
0.176]
[0.039
0.058
0.083]
[0.040
0.052
0.066]
C8
[0.042
0.062
0.088]
[0.039
0.058
0.083]
[0.040
0.052
0.066]
C9
[0.063
0.103
0.176]
[0.059
0.096
0.167]
[0.053
0.072
0.098]
C10
[0.042
0.062
0.088]
[0.059
0.096
0.167]
[0.053
0.072
0.098]
C11
[0.250
0.308
0.351]
[0.235
0.288
0.333]
[0.320
0.362
0.393]
C12
[0.063
0.103
0.176]
[0.039
0.058
0.083]
[0.053
0.072
0.098]
186
J.M. Sánchez-Lozano et al.

X̄ia, X̄ib, X̄ic
ð
Þ = ∑n
i = 1 Xia
n
, ∑n
i = 1 Xib
n
, ∑n
i = 1 Xic
n

	
ð10Þ
By the homogeneous aggregations indicated, the weights of the criteria will be
obtained, taking into account the entire decision-making group. Therefore, the
values obtained for the selection problem of the best military training aircraft are
those indicated in Table 4 and Fig. 4.
Table 4 Weights of criteria
through experts’
homogeneous aggregation
Experts’ homogeneous aggregation
C1
[0.0332
0.0437
0.0578]
C2
[0.0368
0.0511
0.0693]
C3
[0.0389
0.0547
0.0768]
C4
[0.0334
0.0444
0.0593]
C5
[0.0317
0.0416
0.0545]
C6
[0.0304
0.0380
0.0493]
C7
[0.0419
0.0611
0.0909]
C8
[0.0378
0.0525
0.0737]
C9
[0.0809
0.1129
0.1653]
C10
[0.1084
0.1366
0.1750]
C11
[0.2499
0.2937
0.3262]
C12
[0.0471
0.0697
0.1067]
Fig. 4 Graphic representation (experts’ homogeneous aggregation)
An Analysis of Decision Criteria for the Selection …
187

Through homogeneous aggregation it is observed that the most important criteria
are C11 (security systems), C10 (ﬂying and handling qualities) and C9 (human
factors). According to experts 1, 2, 3, 5 and 6 these criteria are also the most
important criteria. The only expert who lightly differs of the rest of experts is expert
4. This expert indicates as the second criteria in importance order the criteria C7
(take-off distance), C9 and C12 (tactical capability) while criterion C10 is moved to
the third position.
The following criteria group in importance is comprised of two criteria; C7
(take-off distance), and C12 (tactical capability) which are the criteria that expert 4
located in the second position. Whereas the least important criteria are C5 and C6
(positive and negative limit load factors).
5
Conclusions
With respect to the applied methodology, it is worth highlighting that, carrying out
the extraction of knowledge from an experts group in this speciﬁc ﬁeld (trained test
pilots and ﬂight instructors of the Spanish Air Force) has allowed to combine the
Delphi method and the fuzzy logic techniques with a well-known decision making
tool like AHP methodology.
Furthermore, it has not only been possible to select and deﬁne a list of criteria
which inﬂuence the selection problem, but also to obtain their coefﬁcients of
importance through the AHP methodology.
Through the homogeneous aggregation, it is observed that the most important
criteria when selecting the best military training aircraft are C11 (security systems),
C10 (ﬂying and handling qualities) and C9 (human factors).
Finally, it should be emphasized that the aforementioned criteria constitute the
group of relevant criteria which should be taken into account in order to preserve
the security or decrease of risk during the training, to extend this work, a further
study regarding additional relevant criteria, such as economic aspects or even
institutional factors, should be carried out.
Acknowledgements The authors acknowledge support through the projects TIN2014-55024-P
from the Spanish Ministry of Economy and Competitiveness, and P11-TIC-8001 from the Con-
sejería de Economía, Innovación y Ciencia of Junta de Andalucía (both including FEDER funds
from the EU).
References
1. Oroumieh, M.A.A., Malaek, S.M.B., Ashraﬁzaadeh, M., Taheri, S.M.: Aircraft design cycle
time reduction using artiﬁcial intelligence. Aerosp. Sci. Technol. 26, 244–258 (2013)
188
J.M. Sánchez-Lozano et al.

2. Wei, S.D., Xing, G.P. Sun, D.X., Gao, K., Liu, Y.W.: Research on SPA-based approaches
and
application
of
the
evaluation
for
maintenance
quality
of
military
aircraft.
978-1-4577-1232-6/11. IEEE. New York (2011)
3. Cheng, C.H., Yang, K.L., Hwang, C.L.: Evaluating attack helicopters by AHP based on
linguistic variable weight. Eur. J. Oper. Res. 116, 423–435 (1999)
4. Israeli, A.A., Mehrez, A., Bochen, D., Hakim, D.: Justiﬁcation of global positioning systems
purchase using the analytic hierarchical process—The case of the Israeli Defense Force.
Technovation 18, 409–424 (1998)
5. Crary, M., Nozick, L.K., Whitaker, L.R.: Sizing the US destroyer ﬂeet. Eur. J. Oper. Res. 136:
680–695 (2002)
6. Ullah, R., Zhou, D.Q., Zhou, P., Hussain, M., Sohail, M.A.: An approach for space launch
vehicle conceptual design and multi-attribute evaluation. Aerosp. Sci. Technol. 25, 65–74
(2013)
7. Sánchez-Lozano, J.M., Teruel-Solano, J., Soto-Elvira, P.L., García-Cascales, M.S.: Geo-
graphical information systems (GIS) and multi-criteria decision making (MCDM) methods for
the evaluation of solar farms locations: case study in south-eastern Spain. Renew. Sustain.
Energy Rev. 24, 544–556 (2013)
8. Gómez-López, M.D., Bayo, J., García-Cascales, M.S., Angosto, J.M.: Decision support in
disinfection technologies for treated wastewater reuse. J. Clean. Prod. 17, 1504–1511 (2009)
9. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
10. Klir, G.J., Yuan, B.: Fuzzy Sets and Fuzzy Logic: Theory and Applications. Prentice Hall,
New Jersey (1995)
11. Hajela, P.: Soft computing in multidisciplinary aerospace design-new directions for research.
Prog. Aerosp. Sci. 38, 1–21 (2002)
12. Cheng, C.H.: Evaluating weapon systems using ranking fuzzy numbers. Fuzzy Sets Syst. 107,
25–35 (1999)
13. Hossain, A., Rahman, A., Hossen, J., Iqbal, A.K.M.P., Zahirul, M.I.: Prediction of
aerodynamic characteristics of an aircraft model with and without winglet using fuzzy logic
technique. Aerosp. Sci. Technol. 15, 595–605 (2011)
14. Saaty, T.L.: The Analytic Hierarchy Process. McGraw Hill, New-York (1980)
15. Hwang, C.L., Yoon, K.: Multiple Attribute Decision Methods and Applications. Springer,
Berlin (1981)
16. Roy, B.: Classement et choix en presence de points de vue multiples (la method ELECTRE).
Revue informatique et recherché opérationnelle 8, 57–75 (1968)
17. Winchester, J.: Modern Military Aircraft (Aviation Factﬁle. The). Amber Books Ltd., London
(2004)
18. Aeronaves operativas del ejército del aire español. Ministerio de defensa. http://www.
ejercitodelaire.mde.es/ea/. Accessed 3 June 2014
19. Noor, A.K., Venneri, S.L.: Future Aeronautical and Space Systems. Progress in Astronautics
and Aeronautics. 72. American Institute of Aeronautics and Astronautics. Virginia (1997)
20. Eshelby, M.E.: Aircraft performance—Theory and practice. Elsevier, London (2000)
21. Newberry, C.F.: The conceptual design of deck-launched waverider-conﬁgured aircraft. Aircr.
Des. 1, 159–191 (1998)
22. Wang, T.C., Chang, T.H.: Application of TOPSIS in evaluating initial training aircraft under a
fuzzy environment. Expert Syst. Appl. 33, 870–880 (2007)
23. Bellman, R.E., Zadeh, L.A.: Decision-Making in a fuzzy environment. Washington. D.C.
National Aeronautics and Space Administration (NASA CR-1594) (1970)
24. García-Cascales, M.S., Lamata, M.T.: Multi-criteria analysis for a maintenance management
problem in an engine factory: rational choice. J. Intell. Manuf. 22, 779–788 (2011)
25. Saaty, T.L.: Group decision making and the AHP. Springer, New York (1989)
An Analysis of Decision Criteria for the Selection …
189

26. Wang, J., Fan, K., Su, Y., Liang, S., Wang, W.: Air combat effectiveness assessment of
military aircraft using a fuzzy AHP and TOPSIS methodology. Asia Simulation Conference—
7th International Conference on System Simulation and Scientiﬁc Computing (2008)
27. Sánchez-Lozano, J.M., García-Cascales, M.S., Lamata, M.T., Sierra, C.: Decision Criteria for
Optimal Location of Wind Farms, Exploring Innovative and Successful Applications of Soft
Computing. IGI Global Ed (2013)
28. Sánchez-Lozano, J.M., García-Cascales, M.S., Lamata, M.T.: Identiﬁcation and selection of
potential sites for onshore wind farms development in Region of Murcia, Spain. Energy 2014;
73, 311–324 (2014)
29. Garcia-Cascales, M.S., Lamata, M.T., Sanchez-Lozano, J.M.: Evaluation of photovoltaic cells
in a multi-criteria decision making process. Ann. Oper. Res. 199, 373–391 (2012)
30. Zadeh, L.A., Kacprycz, J.: Computing with Words in Information/Intelligent Systems. Part 1.
Physica-Verlag (Springer-Verlag), Heidelberg and New York (1999)
190
J.M. Sánchez-Lozano et al.

Participatory Search in Evolutionary Fuzzy
Modeling
Yi Ling Liu and Fernando Gomide
Abstract Search is one of the most useful procedures employed in numerous
situations such as optimization, machine learning, information processing and
retrieval. This chapter introduces participatory search, a class of population-based
search algorithms constructed upon the participatory learning paradigm. Participa-
tory search relies on search mechanisms that progress forming pools of compatible
individuals. The individual that is the most compatible with the best individual is
always kept in the current population. Random immigrants are added to complete
the population at each algorithm step. Diﬀerent types of recombination are possible.
The ﬁrst is a convex combination, arithmetic-like recombination modulated by the
compatibility between individuals. The second is a recombination mechanism based
on selective transfer. Mutation is an instance of diﬀerential variation modulated by
compatibility between selected and recombined individuals. Applications concern-
ing development of fuzzy rule-based models from actual data illustrate the potential
of the algorithms. The performance of the models produced by participatory search
algorithms are compared with a state of the art genetic fuzzy system. Experimental
results show that the participatory search algorithm with arithmetic-like recombina-
tion performs better than the remaining ones.
1
Introduction
The interest in evolutionary procedures to develop fuzzy systems from data has
gained considerable attention in the last decade. Evolutionary fuzzy systems are
fuzzy systems with added evolutionary components. An important instance of evolu-
tionary fuzzy systems is genetic fuzzy systems (GFS). GFS combine fuzzy systems
Y.L. Liu (✉) ⋅F. Gomide
School of Electrical and Computer Engineering, University of Campinas,
Sao Paulo, Brazil
e-mail: yiling155@gmail.com
F. Gomide
e-mail: gomide@dca.fee.unicamp.br
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_11
191

192
Y.L. Liu and F. Gomide
with genetic algorithms [1] to solve complex classiﬁcation, approximation, nonlinear
modeling and control problems.
As it is well known, genetic algorithm (GA) is a population-based stochastic
search procedure whose idea is to evolve a population of individuals using selec-
tion, recombination, and mutation operations working in sequence during several
steps called generations [2]. A ﬁtness function distinguishes the ability of an indi-
vidual to remain in the next population. The better the value of the ﬁtness function
achieved by an individual, the higher is its chance to survive. This is the survival of
the ﬁttest saga. Individuals, candidate solutions of a problem, are points in the search
space. Diﬀerently from GA, diﬀerential evolution [3] creates new candidate solutions
combining the existing ones via mutation, recombination, and selection working in
sequence during several generations. DE keeps whichever candidate solution that
achieves the highest performance.
In [4] we read the following: In actual survival of the ﬁttest saga, there appears
to be additional processes going on. In particular, the objective function in addition
to be determined by some external requirement is often aﬀected by the population
itself.
An approach that has been devised mimic the eﬀect that a population itself has
in its evolution is participatory learning [5]. The key idea of participatory learning
is to account for compatibility between observations and current state of the learner.
As it will be shown late, selection and variation operators such as recombination
and mutation can designed to account for the compatibility between the individuals
of a population. Compatibility and similarity have been shown to be eﬀective in
evolutionary computation [6–9].
This chapter addresses a new class of population-based search algorithms based
on participatory learning. In common with other types of evolutionary algorithms,
participatory search operates with a population of solutions, rather than with a single
solution at a step, and employs procedures to combine these solutions to create new
ones. Participatory search algorithms are novel instances of evolutionary algorithms
because they do not need to assume that evolutionary approaches must necessarily
be based on randomization [10, 11] though they are compatible with randomized
implementations. Participatory search algorithms embody principles that are still
not used by other evolutionary approaches, and that prove advantageous to solve a
variety of complex optimization and design tasks.
The performance of the participatory algorithms is evaluated using actual data
and compared with a state of the art genetic fuzzy system approach developed
in [1]. Computational results show that the participatory search algorithm with
arithmetical-like recombination performs better than the GFS approach.
After this introduction the chapter proceeds as follows. Section 2 brieﬂy reviews
genetic fuzzy systems. Section 3 reminds the concept of participatory learning.
Section 4 introduces the participatory search operators: selection, selective transfer,
arithmetic-like recombination and mutation operators. The search algorithms sum-
marized in Sect. 5. Section 6 evaluates the performance of the participatory search
algorithms against state of the art genetic fuzzy systems approaches. Section 7 con-
cludes the chapter and list issues that deserve further development.

Participatory Search in Evolutionary Fuzzy Modeling
193
2
Genetic Fuzzy Systems
This section gives a brief overview of genetic fuzzy systems (GFS) and their appli-
cations. The focus is on genetic fuzzy rule-based systems (GFRBS), one of the most
important types of GFS. The structure of GFRBS is summarized in Fig. 1.
GFRBS is a fuzzy rule-based system enhanced by genetic algorithms. A fuzzy
rule-based system (FRBS) is composed by a knowledge base (KB) that encodes the
knowledge of a target model. The KB contains two main components, a data base
and a fuzzy rule base. The data base (DB) stores data that characterize the linguistic
variables used by the fuzzy rules, the membership functions that deﬁne the semantics
of the linguistic labels, and the parameters of the model. The fuzzy rule base (RB)
is a collection of fuzzy if-then rules. Other three components complete fuzzy rule-
based models. The ﬁrst is a fuzziﬁcation module to serve as an input interface with
the fuzzy reasoning process. The second is an inference engine that performs fuzzy
reasoning. The third is a defuzziﬁcation output interface module to convert a fuzzy
output into a representative pointwise output. An eﬀective approach to construct the
KB of an FRBS is to simultaneously develop the DB and the RB within the same
process, but in two steps such as in embedded GFRBS learning. Embedded GFRBS
is a scheme to learn the DB using simultaneously a simple method to derive a RB
for each DB.
Embedded GFRBS does not necessarily provide simple, transparent, and com-
petitive models in terms of the generalization capability. They may not scale well in
terms of processing time and memory, two essential requirements especially in high-
dimensional, large-scale, and complex problem solving. These issues are addressed
in [1] where a way to reduce the search space in an embedded genetic DB learn-
ing framework is suggested. Lateral displacement of fuzzy partitions using a unique
parameter for all membership functions of each linguistic variable is one of the mech-
anisms adopted to reduce search space complexity. The idea is to prescreen promis-
ing partitions to avoid overﬁtting and to maintain coverage and semantic soundness
Fig. 1
Genetic fuzzy rule-based system

194
Y.L. Liu and F. Gomide
of the fuzzy partitions. The evolutionary algorithm also includes incest prevention,
restarting, and rule-cropping in the RB generation process to improve convergence.
Despite the use of mechanisms to manage dimensionality, the algorithm does not
scale up on the number of data in datasets. A way to deal with scalability is to avoid
large percentage of samples, and to estimate errors using a reduced subset. A post-
processing step may further reﬁne the algorithm.
Application examples of GFS are many. For example, [12] addresses a multi-
objective optimization in which a fuzzy controller regulates the selection procedure
and ﬁtness function of genetic algorithms. Optimization is used to develop timeta-
bles of railway networks aiming at reducing passenger waiting time when switching
trains, while at the same time, minimizing the cost of new investments to improve
the necessary infrastructure. The result of the genetic optimization is a cost-beneﬁt
curve that shows the eﬀect of investments on the accumulated passenger waiting
time and trade-oﬀs between both criteria. In [13] the aim is to optimize trip time
and energy consumption of a high-speed railway with fuzzy c-means clustering and
genetic algorithm. The method is used to develop a control strategy for a high-speed
train line. An economical train runs with a trip time margin of less than 7% and an
energy saving of 5% is reported. A model to relate the total length of low voltage
line installed in a rural town with the number of people in the town and the mean of
the distances from the center of the town to three furthest clients is discussed in [14].
The authors compare the training and test set error achieved by diﬀerent modeling
techniques for low line value estimation.
3
Participatory Learning
Participatory learning appeared in [5] as a process of learning that depends on what
has already been learned. A central issue in the idea of participatory learning is
that data have the greatest impact in causing learning or knowledge revision when
they are compatible with the current knowledge. Learning occurs in an environ-
ment in which the current knowledge participates in the process of learning about
itself. Clearly, a fundamental factor of participatory learning is the compatibility
degree between input data and current knowledge. The current knowledge, denoted
by v(t), in addition to provide a standard against which input data z(t) is compared
with, directly aﬀects the learning process. This is the participatory nature of learning
process. High compatibility between the current knowledge and current input data
opens the system for learning. In PL, this enhancement is expressed by the compati-
bility degree. A facility is provided to measure the conﬁdence in the current knowl-
edge structure. If a long sequence of input data have low compatibility with current
knowledge, it may be the case that what has been learned so far is mistaken, not
the data. This is seen as a form of stimulation called arousal. Participatory learning
includes an arousal mechanism to monitor the performance of the learning process
by watching at the values of the compatibility degrees of the current knowledge with

Participatory Search in Evolutionary Fuzzy Modeling
195
inputs. Monitoring information is fed back in terms of an arousal index that subse-
quently aﬀects the learning process.
The instance of participatory learning we explore in this chapter uses the compati-
bility degree between current knowledge and current input data to update knowledge
employing the following procedure [5, 15]:
v(t + 1) = v(t) + 𝛼𝜌t(z(t) −v(t))
(1)
where v(t) and z(t) are n-dimensional vectors that denote the current knowledge and
current input data, respectively. Assume, without loss of generality, that v(t), z(t) ∈
[0, 1]n. The parameter 𝛼∈[0, 1] is the basic learning rate and 𝜌t ∈[0, 1] is the com-
patibility degree between v(t) and z(t) at step t. The product of the basic learning rate
by the compatibility degree produces the actual learning rate. If an input is far from
the current knowledge, then the value of the corresponding compatibility degree is
small and the input is ﬁltered. The actual learning rate is lowered by the compatibility
degree. This means that if input data are too conﬂicting with the current knowledge,
then they are discounted [5]. Lower values of actual learning rates avoid ﬂuctuations
due to values of input data which do not agree with current knowledge. As it will
be shown shortly, (1) induces one of the recombination operators of participatory
search algorithms.
The mechanism to monitor compatibility degrees during learning is the arousal
index. The arousal index enters in the basic PL update formula (1) as follows
v(t + 1) = v(t) + 𝛼𝜌1−at
t
(z(t) −v(t))
(2)
where at ∈[0, 1] is the arousal index at t.
One way to compute the compatibility degree 𝜌at step t is
𝜌t = 1 −1
n
n
∑
k=1
|zk(t) −vk(t)|.
(3)
In (3) 𝜌t is the complement of the average absolute diﬀerence between input infor-
mation z(t) and current knowledge v(t). In a more general sense, 𝜌t may be seen to be
a measure of similarity between z(t) and v(t). If 𝜌t = 0, then v(t + 1) = v(t) and the
current input z(t) is completely incompatible with the current knowledge v(t). This
condition means that the system is not open to any learning from the current informa-
tion. On the other hand, if 𝜌t = 1, then v(t + 1) = z(t). In this case input information
is in complete agreement with the current knowledge and the system is fully open to
learn.
Arousal can be seen as the complement of the conﬁdence in the current knowl-
edge. A simple procedure is to update the arousal index a at step t is
at+1 = (1 −𝛽)at + 𝛽(1 −𝜌t+1)
(4)

196
Y.L. Liu and F. Gomide
where 𝛽∈[0, 1] controls the rate of change of arousal. The higher at, the less conﬁ-
dent is the learning system in current knowledge. If 𝜌t+1 = 1, then we have a highly
compatible input and the arousal index decreases. On the other hand, if 𝜌t+1 = 0,
then input information compatibility is low and the arousal index increases.
The notion of compatibility degree enters in participatory search algorithms dur-
ing the formation of pools of individuals for selection, recombination, and mutation.
The pools are assembled from two populations St and St′. The individuals of St′ are
those of St which are the most compatibles, one to one. Selection uses compatibility
to choose those individuals from the pool that are closer to current best individ-
ual. Recombination is done pairwise between individuals of the mating pool, mod-
ulated by their compatibility degrees and arousal indexes. Mutation adds a variation
to the current best individual proportional to the diﬀerence between the selected and
recombined individuals modulated by the corresponding compatibility degrees. The
eﬀect of compatibility is to encourage selection and recombination of similar mates
from which good oﬀspring are likely to be produced, as indicated in [9].
4
Participatory Search Operators
The main construct elements of search algorithms are the representation, search
operators, ﬁtness function, and initial solution. These elements are relevant for all
types of population-based algorithms. The remaining element is the search strat-
egy. Representation concerns encoding mechanisms that maps problems solutions
to strings. Representations allow deﬁnitions of search operators and of the search
space. The search strategy deﬁnes types of intensiﬁcation and diversiﬁcation mech-
anisms.
In what follows we assume that a populations is a ﬁnite set of strings.
4.1
Selection
Let S be a set of N strings of ﬁxed length n, and s, s
′ ∈S be two individuals, s
′ distinct
of s, such that
s′ = argmaxr∈S(𝜌(s, r))
(5)
where
𝜌(s, r) = 1 −1
n
n
∑
k=1
|sk −rk|,
(6)
and s = (s1, s2, .., sN) and r = (r1, r2, .., rN). Expression (5) means that s
′ is the indi-
vidual of S whose compatibility degree with s is the largest. This procedure is
repeated in sequence for each individual s of S to assemble a corresponding pool

Participatory Search in Evolutionary Fuzzy Modeling
197
Fig. 2
A population and its
pool of N individuals
S
′ with N individuals. Notice that construction of the pool is biased by the compati-
bility degrees between the individuals of S. Figure 2 illustrates how the populations
S and S′ are assembled.
In participatory search algorithms, selection is done by computing the compat-
ibility degrees between s ∈S and the corresponding s
′ ∈S
′ with the current best
individual best = s∗, and picking the one that is the most compatible to assemble
a population L of selected individuals, that is, the ones that are the closest to the
current best individual. Formally,
s∗= argmins∈Sf(s),
(7)
where f is the objective function.
More speciﬁcally, selection computes the compatibility degrees 𝜌s(s, s∗) and
𝜌s
′(s
′, s∗) using
𝜌s = 1 −1
n
n
∑
k=1
|sk −s∗
k|
(8)
and
𝜌s
′
= 1 −1
n
n
∑
k=1
|s
′
k −s∗
k|,
(9)
and the individual whose compatibility degree is the largest, denoted by pselected, is
selected. That is, participatory selection proceeds according to the following rule
if
𝜌s ≥𝜌s
′
then
pselected = s
else
pselected = s
′.
(10)
Fig. 3 illustrates the process of selection.
Selection depends on the objective function f(s), which identiﬁes current best s∗,
and on 𝜌s(s, s∗) and 𝜌s
′(s
′, s∗) which measure the compatibility between s∗and the
corresponding pair of individuals s and s
′ of the current pool. Jointly, f, 𝜌s and 𝜌s′
decide if an individual will be selected or not.

198
Y.L. Liu and F. Gomide
Fig. 3
Selection
4.2
Selective Transfer
During the last few years, we have witnessed a growing interest to use economic
principles and models of learning in genetic algorithms. For instance, evolutionary
processes have been used to model the adaptive behavior of a population of economic
agents [16]. Here agents develop models of ﬁtness to their environment in conjunc-
tion with the corresponding economic activities. Economists believe that behavior
acquired through individual experience can be transmitted to future generations, and
that learning changes the way to search the space in which evolution operates. This
is an argument in favor of the interaction between the processes of evolution and
learning. Since technical knowledge is distributed across the economic population,
technological change can be viewed as a process of distributed learning. Here, the
term learning is used in a broad sense, that is, there is no distinction between learn-
ing as propagation of knowledge through the population and the process of inno-
vation, creation, and discovery. The distributed learning perspective helps to under-
stand technological change and focus on the population suggests that an evolutionary
perspective may be appropriate.
Birchenhall and Lin [16] claim that our knowledge and technology are modu-
lar, i.e., they can be decomposed into several components or modules. From the
evolutionary computation point of view, they suggest that the crossover operator of
genetic algorithms could be seen as a representative of modular imitation. To bring

Participatory Search in Evolutionary Fuzzy Modeling
199
Fig. 4
Selective transfer
these ideas together, they advocate an algorithm that replaces selection and crossover
operators by an operator based on selective transfer. Essentially, selective transfer is
a ﬁltered replacement of substrings from one string to another, without excluding
the possibility that the entire sequence is copied [17]. Clearly, the selective transfer
is similar to Holland crossover, but it is one-way transfer of strings, not on exchange
of strings. The behavior selective transfer is likely to be very diﬀerent from the com-
bination of selection and crossover.
Assume that an individual pselected is selected using the objective function and
compatibility. Two positions h ≤k in the pselected string are chosen randomly, and
a fair coin is tossed. If the coin turns head, then the substrings from pselected(h) to
pselected(k) of pselected is replaced by the corresponding substrings from s∗(h) to s∗(k)
of s∗. If the coin turns up tail, then the substrings from pselected(1) to pselected(h −1)
and from pselected(k + 1) to pselected(n) are replaced by the corresponding substrings
of s∗. These steps are repeated for all individuals of L. Figure 4 illustrates the idea of
selective transfer.
Despite similarity with crossover of the standard genetic algorithms, there are
some diﬀerences. The most important one is that selective transfer uses one-way
relocation of substrings, from the best individual to the one selected, and hence it is
not a crossover. This is important because selective transfer is much more schemata
destructive than the standard crossover [17].
4.3
Arithmetic Recombination
Arithmetic recombination emerges from the participatory learning update formula
(2). To see this, notice that (2) can be rewritten as
v(t + 1) = v(t) + 𝛼𝜌(1−at)
t
(z(t) −v(t))
= (1 −𝛼𝜌(1−at)
t
)v(t) + 𝛼𝜌(1−at)
t
z(t).
(11)

200
Y.L. Liu and F. Gomide
Fig. 5
Recombination
Let 𝛾= 𝛼𝜌(1−at)
t
. Thus (11) becomes
v(t + 1) = (1 −𝛾)v(t) + 𝛾z(t).
(12)
Expression (12) is of the following type
sv(t + 1) = (1 −𝛿)sv(t) + 𝛿sz(t)
(13)
where 𝛿∈[0, 1]. Notice that (13) is a convex combination of sv(t) and sz(t) whose
result is the oﬀspring sv(t + 1). Interestingly (12) is similar to (13) and hence (12)
is an arithmetic-like recombination. While parameter 𝛿of (13) is either a constant
or variable, depending on the age of population, the value 𝛾of (12) is variable and
modulated by compatibility and arousal.
Participatory recombination proceeds as in (12) to produce oﬀspring pr from indi-
viduals s and s
′ of pools S and S
′, respectively, as follows
pr = (1 −𝛼𝜌(1−a)
r
)s + 𝛼𝜌(1−a)
r
s
′.
(14)
Figure 5 illustrates the process of participatory recombination. Sums in the ﬁgure
are done on an individual basis, and should be understood from the point of view of
the operation (14).
4.4
Mutation
There are many ways to do mutation in search algorithms. For example, consider a
population of N individuals represented by n-dimensional vectors denoted by sri,t at
generation t. Diﬀerential evolution, for instance, produces new individuals by adding
the weighted diﬀerences between distinct vectors to a third vector [3]. For each vector
sri,t, i = 1, 2, ..., N, a mutated vector is generated using

Participatory Search in Evolutionary Fuzzy Modeling
201
Fig. 6
Mutation
si,t+1 = sr1,t + 𝜙⋅(sr2,t −sr3,t)
(15)
where r1, r2, r3 ∈{1, 2, ..., N} are random indexes, and 𝜙> 0 is a parameter which
controls the amount of the diﬀerential variation (sr2,t −sr3,t).
Mutation in participatory search is similar to diﬀerential evolution mutation. It
produces a mutated individual pm as follows
pm = best + 𝜌1−a
m (pselected −pr).
(16)
Fig. 6 illustrates the process of mutation.
In participatory mutation, the amount of the variation of the best individual
best = s∗is controlled by compatibility between the selected and recombined indi-
viduals, and the arousal index.
5
Participatory Search Algorithms
Let St be the set of N with strings of length n at step t. The participatory search algo-
rithms (PSA) start with a population St at t = 0 with N randomly chosen individuals
and, for each individual of St, the most compatible individual amongst the remaining
ones is chosen to assemble the population St′ with N individuals. St and St′ form the
mating pool. Next, the best individual s∗in the current population St, denoted by
best, is chosen. For instance, for minimization problems best is such that
best = argmins∈Stf(s).
(17)
Selection chooses, by looking at each individual of St and the corresponding mate
in St′, the one which is the closest to best. Recombination is done pairwise between
the individuals of the mating pool, weighted by their values of compatibility and
arousal. Mutation uses the selected and recombined individuals to produce variations
whose amount is weighted by compatibility and arousal as well. If a oﬀspring is
better than the current best individual, then it replaces the current best. Otherwise,
if a mutated individual is better than current best individual, then it replaces the

202
Y.L. Liu and F. Gomide
Fig. 7
Participatory search algorithms
current best. A new iteration starts with a new population St+1 composed by the
current best individual, with the remaining (N −1) individuals chosen randomly.
We should remark that participatory search algorithms are elitist: the best individual
encountered is always kept in a population. The directive last(St) ←best means that
the best individual found up to generation t, denoted by best, is kept at the position
that corresponds to the last individual of the population at step t + 1.
There are four instances of PSA, respectively, participatory search with selec-
tive transfer (PSST), participatory search with arithmetic recombination (PSAR),
diﬀerential participatory search with selective transfer (DPST), and diﬀerential par-
ticipatory search with arithmetic recombination (DPSA). They are distinguished by
the nature of the recombination, and the order in which the operations of selection,
recombination, and mutation are processed in each generation. They also diﬀer from
similar evolutionary approaches developed in [6, 7, 18] in the way the mating pool
is constructed to produce the new population. A class of participatory search algo-
rithms that incorporates participatory learning is shown in Fig. 7.
PSST is similar to the algorithm discussed in [6] in the sense that both algorithms
use participatory selective transfer and mutation. PSAR uses participatory arithmetic
recombination and mutation, processed in a diﬀerent order than PSST. DPST is sim-
ilar to the algorithm of [7] because it also uses selective transfer and participatory

Participatory Search in Evolutionary Fuzzy Modeling
203
mutation. Likewise, DPSA is similar to the algorithm of [18] and uses participatory
arithmetic recombination and mutation. DPSA proceeds similarly as DPST except
that it uses arithmetic recombination instead of selective transfer. PSST, PSAR,
DPST and DPSA diﬀer from all previous approaches because selection is done indi-
vidually for each of the N individuals of the current population. Participatory recom-
bination and mutation are performed likewise. Recall that PSST, PSAR, DPST and
DPSA are all elitist: the best individual is always kept in the current population.
As an illustration, the procedure PSAR is detailed below. The remaining algorithms,
except for their nature, have similar format. A in-depth description, characterization,
and convergence analysis of the PSA can found in [19].
1: procedure PSAR
2:
f an objective function
3:
s ∈St and s
′ ∈St′
4:
set best randomly
5:
set a0 ←0; t ←0
6:
while t ≤tmax do
7:
generate population St randomly
8:
last(St) ←best
9:
St′ ←s′ = argmaxr∈St(𝜌(s, r))
10:
ﬁnd best in St
11: Selection:
12:
compute 𝜌s(s, best) and 𝜌s
′ (s
′, best)
13:
if 𝜌s ≥𝜌s
′ then
14:
pselected ←s
15:
else
16:
pselected ←s
′
17:
end if
18: Recombination:
19:
choose 𝛼, 𝛽∈[0, 1] randomly
20:
compute 𝜌r = 𝜌(s, s
′)
21:
compute at+1 = at + 𝛽((1 −𝜌r) −at)
22:
pr = (1 −𝛼𝜌1−at
r
)s + 𝛼𝜌1−at
r
s
′
23: Mutation:
24:
compute 𝜌m = 𝜌(pselected, pr)
25:
pm = best + 𝜌1−at+1
m
(pselected −pr)
26:
if f(pr) better than f(best) then
27:
best ←pr
28:
end if
29:
if f(pm) better than f(best) then
30:
best ←pm
31:
end if
32:
t ←t + 1
33:
end while
34:
return best
35: end procedure

204
Y.L. Liu and F. Gomide
6
Participatory Search Algorithms in Fuzzy Modeling
This section concerns the use of participatory search algorithms in fuzzy rule-based
system modeling. The aim is to illustrate potential applications of PSA and to eval-
uate and compare the performance of PSST, PSAR, DPST and DPSA algorithms
using actual data and results reported in the literature.
The problem of interest here is to develop linguistic fuzzy models using actual
data sets available in KEEL (http://www.keel.es/). The KEEL (Knowledge Extrac-
tion based on Evolutionary Learning) is a software tool to assess evolutionary algo-
rithms for data mining problems including regression, classiﬁcation, clustering, and
pattern mining. KEEL provides a complete set of statistical procedures for multiple
comparisons. The features of the data sets are summarized in Table 1. These data are
the same used in [1], a state of the art representative GFS reported in the literature
[20]. The representation and encoding schemes of PSAR are also the same of the
one adopted in [1]. They are as follows:
1. Database encoding: (C = C1, C2) a double-encoding scheme.
First, equidistant strong fuzzy partitions are identiﬁed considering the granularity
(labels) speciﬁed in C1. Second, the membership functions of each variable are
uniformly rearranged to a new position considering lateral displacement values
speciﬁed in C2.
∙Number of labels C1: this is a vector of integers of size n representing the
number of linguistic variables.
C1 = (L1, ..., Ln).
(18)
Gene Li is the number of labels of the ith linguistic variable, Li ∈{2, ..., 7}.
∙Lateral displacements C2: this is a vector of real numbers of size n that encodes
displacements 𝛼i of the diﬀerent variables, 𝛼i ∈[−0.1, 0.1]. A detailed
description of the linguistic 2-tuple representation is given in [21, 22].
Table 1
Summary of the datasets
Problem
Abbr.
Variables
Samples
Electrical maintenance ELE
4
1056
Auto MPG6
MPG6
5
398
Analact
ANA
7
4052
Abalone
ABA
8
4177
Stock prices
STK
9
950
Forest ﬁres
FOR
12
517
Treasury
TRE
15
1049
Baseball salaries
BAS
16
337

Participatory Search in Evolutionary Fuzzy Modeling
205
Fig. 8
A double-encoding
scheme C1 and C2
Fig. 9
Lateral displacement
of the linguistic variable V
values V1, V2, and V3
C2 = (𝛼1, ..., 𝛼n).
(19)
An example of the encoding scheme is given in Fig. 8.
Figure 9 illustrates the lateral displacement of V for 𝛼= −0.05.
2. Rule base: constructed using the Wang and Mendel algorithm (WM) [23, 24] as
follows:
a. granulate the input and output spaces;
b. generate fuzzy rules using the given data;
c. assign a certainty degree to each rule generated to resolve conﬂicts;
d. create a fuzzy rule base combining the rules generated and rules provided
by experts (if available);
e. determine the input-output mapping using the combined fuzzy rule base and
a defuzziﬁcation procedure.
An example of a fuzzy rule-base developed for ELE is shown in Fig. 10.
Example of rules of the rule base of Fig. 10 include:
rule 1:
IF X1 is 1 and X2 is 1 and x3 is 1 and x4 is 1 THEN Y is 1
rule 2:
IF X1 is 2 and X2 is 1 and x3 is 1 and x4 is 2 THEN Y is 3
rule 3:
IF X1 is 3 and X2 is 3 and x3 is 2 and x4 is 3 THEN Y is 4
3. Objective function: the mean-squared error (MSE)
MSE =
1
2|D|
|D|
∑
l=1
(F(xl) −yl)2
(20)

206
Y.L. Liu and F. Gomide
Fig. 10
Rule base
constructed using WM
algorithm
where |D| is the size of the dataset, F(x) is the output of the FRBS model, and y
the actual value of the output. Fuzzy inference uses the max-min procedure with
center of gravity deﬀuziﬁcation.
4. Initial population: each chromosome has the same number of linguistic labels,
from two to seven labels for each input variable. For each label of the inputs, all
possible combinations are assigned to the respective rules consequents. More-
over, for each combination, two copies are added with diﬀerent values in the C2
part. The ﬁrst has values randomly chosen in [−0.1, 0] and the second random
values chosen in [0, 0.1].
5. Recombination: pr ←ﬂoor(pr) for C1.
If a gene g of pr in C1 is lower than 2, then Lg = 2, else if a gene g is higher than
7, then Lg = 7.
6. Mutation: pm ←ﬂoor(pm) for C1.
If a gene g of pm in C1 is lower than 2, then Lg = 2, else if a gene g is higher than
7, then Lg = 7.
The electric maintenance model has four input variables and one output variable.
The ELE dataset contains electrical maintenance data and has 1056 samples. This is
an instance in which we expect learning methods to develop large number of rules.
ELE modeling involves a large search space [1]. The MPG data concerns city-cycle
fuel consumption in miles per gallon (mpg), to be predicted in terms of one multival-
ued discrete and ﬁve continuous attributes. The MPG6 dataset has 398 samples. The
categorical data (ANA) is one of the data sets used in the book Analyzing Categor-
ical Data by Jeﬀrey S. Simonoﬀ. It contains information about the decisions taken
by a supreme court. The ANA dataset concerns seven variables and 4052 samples.
Abalone age data come from physical measurements. The abalone model has eight
input variables and one output variable. The abalone dataset (ABA) contains 4177
samples. The STK data provided are daily stock prices from January 1988 through
October 1991, for ten aerospace companies. The task is to approximate the price

Participatory Search in Evolutionary Fuzzy Modeling
207
Table 2
Methods considered by the computational experiments [1]
Method
Type of learning
WM(3)
Rule base produced by WM, 3 linguistic labels for each variable
WM(5)
Rule base produced by WM, 5 labels for each variable
WM(7)
Rule base produced by WM, 7 labels for each variable
FSMOGFS
Gr. Lateral partition parameters, and rule base produced by WM
FSMOGFS+TUN
FSMOGFS + Tuning of MF parameters and rule selection by SPEA2
FSMOGFSe+TUNe
FSMOGFS+TUN including fast error estimation
of the 10th company given the prices of the rest. The STK has nine input variables
and 950 samples. The FOR dataset has 12 variables and 517 samples. The aim is to
predict the burned area of forest ﬁres, in the northeast region of Portugal. The TRE
contains the economic data information of USA and has 15 variables input and 1049
samples. The goal is to predict 1-Month Rate. The BAS contains the salaries of the
set of Major League Baseball players and has 16 variables input and 337 samples.
The task is to approximate the salary of each player. The datasets are available at
http://sci2s.ugr.es/keel/index.php. The methods considered in [1] are summarized in
Table 2. The method of Wang and Mendel (WM) is also a reference because all PSA
and the GFS use it as a rule generation procedure during evolution. The participatory
search algorithms were run using the datasets to compare their results with the ones
produced by PSAR and results reported in the literature [1]. The processing times of
the diﬀerent methods in [1] were obtained using an Intel Core 2 Quad Q9550 2.83-
GHz, 8 GB RAM. The processing times of participatory search algorithms reported
here were obtained using an Intel Core 2 Quad Q8400 2.66GHz, 4 GB RAM.
The input parameters used by participatory search algorithms in the experiments
reported in this section are: population size of 60, and maximum number of function
evaluations of 1000. Data sets were randomly split into ﬁve folds, each partition
containing 20% of the dataset. Four of these partitions are used for training and the
remaining one is used for testing. The algorithms are run six times for each data
partition using six distinct seeds.
The results show that the average mean-squared error for the test data achieved
by the fuzzy models developed by PSAR, Table 6, is lower than the average mean-
squared error of test data achieved by the FSMOGFSe+TUNe, except for ANA data.
Also, the average mean-squared error for the test data achieved by DPSA is lower
than the FSMOGFSe+TUNe. For the test data of ANA, FSMOGFSe+TUNe achieves
the lowest MSE value. Considering the test data PSAR, with WM using diﬀerent
number of labels for each linguistic variable, is more accurate than when the num-
ber of linguistic labels for each linguistic variable is kept ﬁxed, WM(3),WM(5) and
WM(7), respectively. Thus, PSAR performs better than FSMOGFSe+TUNe from
the point of view of the test data of MSE. Also, the standard deviation (SD) of test
data for PSAR and FSMOGFSe+TUNe is better than WM(3),WM(5) and WM(7).

208
Y.L. Liu and F. Gomide
Table 3
Average rank of the algorithms
Algorithm
Friedman rank
p-value
H0
WM(3)
7.3125
WM(5)
6.25
WM(7)
6
FSMOGFSe+TUNe
3.75
1.38E-7
Rejected
PSAR
2.125
PSST
4
DPSA
2.3125
DPST
4.25
Table 4
Holm’s Post-Hoc for 𝜀= 0.05.
Control algorithm: PSAR
i
Algorithm
z value
p-value
𝜀∕i
H0
7
WM(3)
4.2355
2.3E-5
0.0071
Rejected
6
WM(5)
3.368
0.0007
0.0083
Rejected
5
WM(7)
3.1639
0.0015
0.01
Rejected
4
DPST
1.735
0.08273
0.0125
Rejected
3
PSST
1.5309
0.1257
0.0166
Rejected
2
FSMOGFSe+TUNe
1.3268
0.184573
0.025
Accepted
1
DPSA
0.153
0.8783
0.05
Accepted
Further analysis is pursued as suggested in [25, 26] to verify if there exist statisti-
cal diﬀerences among the performance of the algorithms. Recall that the conﬁdence
level is 𝜀= 0.05. Table 3 shows how PSAR and GFS are ranked. PSAR achieves the
highest rank with 1.375. Also, recall that the null hypothesis H0 is that PSAR and
GFS algorithms are equivalent, that is, H0 means that the rank of all algorithms are
equal. If the hypothesis is rejected, then we conclude that the algorithms perform
diﬀerently.
Iman-Davenport’s test suggests that there are signiﬁcant diﬀerences among the
algorithms in all datasets once the null hypothesis is rejected (p-value = 1.38 E−7).
Thus the Holm post-hoc test is conducted with PSAR as the control algorithm.
Table 4 shows that the Holm post-hoc test rejects the hypothesis concerning WM(3),
WM(5), WM(7), DPST and PSST, but do not reject FSMOGFSe+TUNe and DPSA.
Therefore, PSAR outperforms WM(3), WM(5), WM(7), DPST and PSST because
the rank pf PSAR is the highest and rejects the hypothesis in the Holm test. We
notice that the diﬀerence of the performance of FSMOGFSe+TUNe and DPSA is
not statistically relevant because the null hypothesis is accepted.
Table 5 highlights, that for each dataset, the average processing time of
FSMOGFSe+TUNe and PSAR in minutes and seconds. We notice the diﬀerent
complexity of the solutions generated during the evolutionary process. The com-

Participatory Search in Evolutionary Fuzzy Modeling
209
Table 5
Average runtime of the algorithms (minutes:seconds M:S)
Dataset
FSMOGFSe+TUNe
PSAR
ELE
00:42
00:45
MPG6
1:00
00:53
ANA
5:17
5:05
ABA
3:54
4:25
STK
1:31
1:12
FOR
1:07
00:40
TRE
00:46
1:02
BAS
00:58
1:01
Fig. 11
MSE performance of the algorithms versus the number of rules: MPG6 data
putational cost of the ﬁtness evaluation depends of the number of rules and con-
ditions in rules antecedents. In the case of ANA, the PSAR needs less time than
FSMOGFSe+TUNe because the number of rules is small. On the other hand, it is
higher than 3 min in ANA and ABA because the large number of samples.
In sum, the performance of PSAR in developing fuzzy rule-based models with
actual data illustrates its potential to solve complex problems. Overall, the results
suggest that PSAR performs better than current state of the art genetic fuzzy system
approaches from the point of view of the average mean square error with test data.
Figure 11 summarizes the MSE performance of the algorithms versus the number
of rules for MPG6 data set. More importantly, participatory search algorithms are

210
Y.L. Liu and F. Gomide
Table 6
Average MSE of PSA and GFS Algorithms
WM(3)
WM(5)
WM(7)
FSMOGFSe+TUNe
PSAR
PSST
DPSA
DPST
Dataset
Tra
Tst
Tra
Tst
Tra
Tst
Tra
Tst
Tra
Tst
Tra
Tst
Tra
Tst
Tra
Tst
ELE
Rule
27
65
103
8
28
27
23
23
Mean
192241
192647
56135
56359
53092
55495
9665
10548
9502
10480
11409
10560
9586
10434
11250
10544
SD
9658
14436
1498
4685
1955
9452
823
1150
3951.94
3986.8
3874.74
3906.8
3914.4058
3753.2432
3020.37
3260.55
MPG6
Rule
43
115
194
20
57
77
85
63
Mean
13.552
14.649
4.136
6.096
2.642
6.382
2.86
4.562
1.6132
1.5205
3.424
3.1699
1.8901
1.5151
2.0641
2.467
SD
1.239
3.204
0.317
2.416
0.11
2.126
0.218
0.714
1.3712
1.2983
1.3093
1.2639
1.1779
1.2563
1.1532
1.299
ANA
Rule
72
124
171
10
6
4
7
5
Mean
0.187
0.189
0.027
0.03
0.012
0.017
0.003
0.003
0.0256
0.0856
0.0292
0.0682
0.0274
0.0507
0.0292
0.0795
SD
0.001
0.005
0
0.002
0
0.003
0
0.001
0.0833
0.0574
0.0875
0.0485
0.0913
0.0866
0.0836
0.0373
ABA
Rule
68
199
368
8
34
23
34
30
Mean
8.407
8.422
3.341
3.474
3.057
3.268
2.445
2.509
0.0016
0.0047
0.0018
0.005
0.0018
0.0047
0.0055
0.0048
SD
0.443
0.545
0.13
0.247
0.084
0.185
0.114
0.184
0.0002
0.0002
0.0001
0.0002
0.0002
0.0002
0.0005
0.0002
STK
Rule
123
265
378
23
58
73
66
78
Mean
8.852
8.951
1.576
1.624
0.611
1.488
0.764
0.912
0.7342
0.9002
1.1022
0.9064
0.7784
0.8139
1.6701
1.8139
SD
0.508
1.193
0.09
0.09
0.029
1.634
0.139
0.181
0.0479
0.1001
0.0687
0.1333
0.0912
0.0597
0.0738
0.0987
FOR
Rule
246
375
401
10
23
31
8
43
Mean
2030
3793
1435
34235
340
1.00E+05
1418
2628
2115.76
1619.13
2231.0333
2.35E+03
2652.8666
2.04E+03
1600.9333
1.74E+03
SD
531
2340
505
4356
147
4708
539
2108
381.98
756.86
169.3108
450.3585
207.1127
461.7959
194.4706
524.3924
TRE
Rule
75
196
261
9
26
28
30
25
Mean
1.636
1.631
0.401
0.405
0.17
0.176
0.034
0.044
0.0529
0.0372
0.1168
0.1707
0.0655
0.0392
0.1492
0.128
SD
0.121
0.181
0.014
0.055
0.009
0.017
0.003
0.015
0.0059
0.0045
0.0059
0.019
0.0067
0.008
0.0099
0.0205
BAS
Rule
181
253
264
17
50
52
47
65
Mean
1.9E+05
3.6E+05
7.8E+04
6.2E+05
3.1E+04
1.0E+06
1.4E+05
2.6E+05
1.5E+05
1.3E+05
1.0E+05
1.5E+05
1.1E+05
3.6E+05
1.0E+05
3.9E+05
SD
1.0E+04
7.3E+04
4.7E+03
6.8E+04
6.0E+02
1.3E+05
1.9E+04
5.8E+04
7753.67
124393
5.9E+03
3.1E+04
7.6E+03
2.6E+05
7.2E+03
3.0E+05

Participatory Search in Evolutionary Fuzzy Modeling
211
simpler, have high computational performance, and require few parameters to run. In
particular, PSAR is a highly competitive population-based search approach (Table 6).
7
Conclusion
Participatory search is a population-based instance of the participatory learning
paradigm. Compatibility degrees and arousal indexes account for the eﬀect of the
population individuals during search. Recombination arises from an instance of
participatory learning formula. The participatory search algorithms are elitist and
employ compatibility and arousal information in selection, recombination and muta-
tion. Applications concerned the use of participatory search algorithms to develop
fuzzy linguistic models of actual data. The performance of the models produced by
the participatory search algorithms were evaluated and compared with a state of the
art genetic fuzzy system approach. The results suggest that the participatory search
algorithm with arithmetical-like recombination performs best.
Acknowledgements The second author is grateful to CNPq, the Brazilian National Council for
Scientiﬁc and Technological Development (CNPq), for grant 305906/2014-3.
References
1. Alcalá, R., Gacto, M., Herrera, F.: A fast and scalable multiobjective genetic fuzzy system for
linguistic fuzzy modeling in high-dimensional regression problems. IEEE Trans. Fuzzy Syst.
19(4), 666–681 (2011)
2. Herrera, F., Verdegay, J.: Genetic Algorithms and Soft Computing. Physica-Verlag, Heidel-
berg, Germany (1996)
3. Storn, R., Price, K.: Diﬀerential evolution—a simple and eﬃcient heuristic for global opti-
mization over continuous spaces. J. Glob. Optim. 11, 341–359 (1997)
4. Yager, R.: Participatory Genetic Algorithms. BISC Group List, message posted on 29 Aug
2000
5. Yager, R.: A model of participatory learning. IEEE Trans. Syst. Man Cybern. 20(5), 1229–
1234 (1990)
6. Liu, Y.L., Gomide, F.: Participatory genetic learning in fuzzy system modeling. In: Proceedings
of IEEE SSCI 2013, Singapore (2013)
7. Liu, Y.L., Gomide, F.: Evolutionary participatory learning in fuzzy system modeling. In: Pro-
ceedings of IEEE International Conference on Fuzzy System, p. 2013, Hyderabad, India (2013)
8. Liu, Y.L., Gomide, F.: Participatory search algorithms in fuzzy modeling.In: Proceedings of
the World Conference in Soft Computing, Berkeley, USA (2016)
9. Ishibuchi, H., Narukawa, K., Tsukamoto, N., Nojima, Y.: An empirical study on similarity-
based mating for evolutionary multiobjective combinatorial optimization. Eur. J. Oper. Res.
188, 57–75 (2008)
10. Fogel, D.: An Introduction to Evolutionary Computation, Chapter 1 of Evolutionary Compu-
tation: The Fossil Record. Wiley-IEEE Press, New York, USA (1998)
11. Glover, F., Marti, R.: Fundamentals of scatter search and path relinking. Control Cybern. 29(3),
653–684 (2000)

212
Y.L. Liu and F. Gomide
12. Voget, S., Kolonko, M.: Multidimensional optimization with a fuzzy genetic algorithm. J.
Heuristics 4(3), 221–244 (1998)
13. Hwang, H.-S.: Control strategy for optimal compromise between trip time and energy con-
sumption in a high-speed railway. IEEE Trans. Syst. 28(6), 791–802 (1998)
14. Cordón, Ó.: Genetic fuzzy systems: evolutionary tuning and learning of fuzzy knowledge
bases. In: Advances in Fuzzy Systems. World Scientiﬁc Publishing, Singapore (2001)
15. Brown, R.: Moothing, Forecasting and Prediction of Discrete Time Series. Prentice-Hall, New
Jersey, USA (2004)
16. Birchenhall, C., Lin, J.-s.: Learning and adaptive artiﬁcial agents: Analysis of an evolu-
tionary economic model. In: Computing in Economics and Finance, vol. 327. University of
Manchester, United Kingdom (2000)
17. Hwang, H.-S.: Genetic algorithms in evolutionary modelling. J. Evolut. Econ. 7(4), 375–393
(1997)
18. Liu, Y.L., Gomide, F.: Fuzzy systems modeling with participatory evolution. In: IFSA World
Congress and NAFIPS Annual Meeting (IFSA/NAFIPS). Edmonton, AB, Canada (2013)
19. Liu, Y.L.: Participatory search algorithms and applications. Ph.D. Thesis, School of Electrical
and Computer Engineering, University of Campinas, Campinas, Sao Paulo, Brazil (2016)
20. Fazzolari, M., Alcala, R., Nojima, Y., Ishibuchi, H., Herrera, F.: A review of the application of
multiobjective evolutionary fuzzy systems: current status and further directions. IEEE Trans.
Fuzzy Syst. 21, 45–65 (2013)
21. Herrera, F., Martínez, L.: A 2-tuple fuzzy linguistic representation model for computing with
words. IEEE Trans. Fuzzy Syst. 8(6), 746–752 (2000)
22. Alcalá, R., Alcalá-Fdez, J., Herrera, F., Otero, J.: Genetic learning of accurate and compact
fuzzy rule based systems based on the 2-tuples linguistic representation. Int. J. Approx. Reason.
44(1), 45–64 (2007)
23. Wang, L., Mendel, J.: Generation fuzzy rules by learning from examples. IEEE Trans. Syst.
Man Cybern. 22(6), 1414–1427 (1992)
24. Wang, L.: Adaptive Fuzzy Systems and Control: Design and Stability Analysis. Prentice-Hall,
Upper Saddle River, USA (1994)
25. Derrac, J., García, S., Molina, D., Herrera, F.: A practical tutorial on the use of nonparametric
statistical tests as a methodology for comparing evolutionary and swarm intelligence algo-
rithms. Swarm Evolut. Comput. 1, 3–18 (2011)
26. Antonelli, M., Ducange, P., Marcelloni, F.: An eﬃcient multi-objective evolutionary fuzzy
system for regression problems. Int. J. Approx. Reason. 54, 1434–1451 (2013)

But, What Is It Actually a Fuzzy Set?
Enric Trillas
Abstract Supported in a new view of meaning as a quantity in whatever universe
of discourse, and for its possible use concerning plain language and ordinary rea-
soning in ‘Computing with Words’, the paper deals with the basic concept of a
fuzzy set. That is, not only with the collective a linguistic label generates in lan-
guage, but also with what membership functions reﬂect on it once ideally seen as
measures of meaning.
1
Introduction
More than ﬁfty years after its introduction [1], the idea of a fuzzy set is not yet clear
enough, and although no ‘if and only if’ deﬁnition of it exists, too often fuzzy sets
are seen as if they were mathematical entities in themselves instead of linguistic
entities.
The identiﬁcation of a fuzzy set with a single one of its possible membership
functions is something very bizarre, since the question that should be immediately
posed is, ‘But, which one of them?’. Paraphrasing Quine’s words [2], it does not
seem possible to describe an entity without some criteria of identity; and the cor-
respondence between fuzzy sets and membership functions is one-to-many.
In addition, identifying the fuzzy sets in a universe of discourse X with all the
functions in [0, 1]X is still another oddity, since no general criteria are known for
assigning to each one of these this enormous amount of functions a linguistic label
of which it can be a membership function. In front of the unknown multiplicity of
predicative words acting on X, it is the aleph-one cardinality of the functions in
[0, 1]X. Furthermore, an important characteristic usually required of membership
To Professor José Luis Verdegay, with deep affection for his academic work. Curro, thanks for
the large friendship we jointly kept from so long ago!
E. Trillas (✉)
University of Oviedo, Asturias, Spain
e-mail: etrillasetrillas@gmail.com
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_12
213

functions is their continuity, for which a topology in X is necessary; when X can be
seen as a subset of the real line such topology is automatically given, but, in general
its existence is unknown. In plain language, words do act in whatsoever universe of
discourse.
Nevertheless, most books on fuzzy set theory and fuzzy logic usually begin
under these presumptions that, although sometimes not explicit but implicitly
present in them, not even can be sustained as simple ‘working hypothesis’. Hence
both theories usually appear as based on moving grounds; trying to ﬁnd, at least, a
more solid and suitable basement for the idea of fuzzy set is the only goal of this
paper.
2
Language and Fuzzy Sets
2.1 What does not seem to be debatable is that a fuzzy set in a universe of discourse
X comes from a linguistic label, that is, a predicative word P whose behavior in X is
manifested through the elemental statements ′x is P′, and generates the ‘fuzzy set
labeled P′. Each one of these statements reﬂects that a property p, with name P, not
only holds up to some extent for the elements in X, but is exhibited by them and is
externally recognized.
For instance, in the universe of the London inhabitants, the word ‘young’ is so
well anchored in plain language that it allows to speak on the ‘young Londoners’;
something that is understood by everybody. In the universe of the positive integers,
the word ‘odd’ is also so well anchored in the language of Arithmetic, that it allows
to speak on the ‘odd numbers’. In a universe of trains, the word ‘large’ is so well
anchored in plain language that it allows to speak on the ‘large trains’. At each part
of language, its speakers perfectly understand these statements. Often, in plain
language, recognizing p in the elements in X has an empirical character.
There are predicative words generating ‘linguistic collectives’, that is, words that
‘collectivize’ in language [3]. When such language is an artiﬁcial one, like it is that
of Arithmetic, those words whose behavior is deﬁned by ‘if and only if’ conditions,
are the precisely used words that, of course, also exist in plain language. For
instance, and in a universe of discourse constituted by two people, John and Sarah,
the word ‘young’ can be undoubtedly applied to Sarah provided it is known that she
is 18 years old, but John is 67. Notwithstanding, the same word ‘young’ does not
admit a precise use in a universe with a very big number of elements like it is that of
London inhabitants; for instance, provided it were accepted that ′x is young’ if and
only if x is no more than thirty years old, is that a Londoner of thirty years and a
few days old, should cease to be qualiﬁed as young? Obviously, it is not in plain
language where its speakers see linguistic collectives as a single entity.
It can be said that the use of precisely used words is rigid, but that of the not
precisely used is ﬂexible. Hence, the ﬁrst are those words that collectivize in just a
subset of the universe of discourse; they perfectly classify the universe in those
elements fulﬁlling the property p, and those not fulﬁlling p. It just corresponds to
214
E. Trillas

the ‘axiom of speciﬁcation’ in the Naïve Set Theory [4], stating that a bi-valued
property p generates in X a perfect classiﬁcation of its elements in two subsets, that
with those x fully verifying p, and that with those not verifying p at all. A repre-
sentation of a precisely used word P in X, is done by the set P stated by the axiom
of speciﬁcation; its use is rigid, and the Boolean algebra of the subsets in X is the
non debatable structure for representing these linguistic labels; it is a mathematical
model perfectly mimicking how precise words are used when all the necessary
information on their use is, at least, potentially known.
The words not precisely used, and as it is shown by a ‘Sorites’ type argument [5]
applied to each one of them, cannot collectivize in a subset. The, well anchored in
plain language, linguistic collectives they generate are like gaseous, or cloudy,
entities; but, anyway, gas volumes and clouds do actually exist, and are scientiﬁ-
cally studied. Indeed, linguistic collectives are rooted in plain language, and
founding a mathematical model for them is imperative; at least for symbolically
representing the ordinary reasoning that is greatly permeated by imprecision.
2.2 It seems that the study of the collectives generated by not precisely used
words deserve to be approached by not considering them as pure mathematical
entities characterized by an ‘if and only if’ deﬁnition, but in a scientiﬁc-like style.
Such entities should be seen in a different way than sets, but, in any case, they can,
and will, be identiﬁed with the fuzzy sets; that is, naïvely renaming them as fuzzy
sets. The fuzzy set in X labeled P is just the linguistic collective P generates in X,
the collective named ‘the Ps of X′, and it allows to see a fuzzy set as a single,
although not precise, entity. In this way, the linguistic collective generated by a
precisely used word is just a subset of X; subsets represent but degenerate,
bi-valued, collectives; a non-degenerated collective is a purely linguistic concept.
In this line of thought, the fuzzy set labeled P can be seen as a single, although
cloudy, linguistic entity rooted in X. But it still lacks to answer the question: How a
fuzzy set can be speciﬁed in such a way that the axiom of speciﬁcation for the
precisely used words can follow from it? A possible answer for this question lies in
‘meaning’; it is a semantic topic.
3
Words with Measurable Meaning
3.1 The meaning of a word is not independent of the context of its use; for instance,
‘odd’ does not mean the same in a universe of positive integer numbers than in one
of people. Sometimes the meaning of words can even change depending on the
purpose for its use, as it is with ‘odd’ when used to qualify people either with a
joking, or a hilarious, or an insulting purpose. It sufﬁces to look for words in a
dictionary to check all this by seeing how their uses are described.
Hence, no realistic theory of meaning can assign to words a single and universal
meaning, since it depends on the universe of discourse, and on the particular
context of its use; the meaning of words is context-dependent and purpose-driven.
Thus, and in addition, for arriving at a theory of meaning it should be considered
But, What Is It Actually a Fuzzy Set?
215

how P ‘behaves’, or ‘acts’ in X. Language is not static, but dynamic; almost always,
time intervenes in language.
Once a pair (X, P) is given, how the ‘behavior’ of the word P in the universe X
can be described? It only can come from considering the action of P for the x in X;
that is, from the elemental statements ′x is P′, that constitute a different entity X [P]
than X; the second can be physical or virtual, but the ﬁrst is always virtual. Nev-
ertheless, to capture the ‘behavior’ of P in X, it should be recognized not only the
action of P on the elements x, but the context on which the statements ′x is P′ are
used. To capture how P behaves, or acts, in X, it not sufﬁces to statically capture
what indicate the statements ′x is P′, it is also necessary to know how such action
varies along X; its internal dynamism. That is, recognizing the linguistic relation-
ship [6],
0x is less P than y0, equivalent to 0y is more P than x';
in other words, that x veriﬁes p, less than y does. Such recognition is, in the case of
plain language, often of an empirical character not immediately allowing the
assignation of a degree to the veriﬁcation of p.
Let’s, symbolically, denote it by x < P y, and by x = P y the case in which both
x < P y and y < P x, or x < P
−1 y, hold; obviously, = P = < P ∩< P
−1.
To avoid the possibility < P = Ø, let’s state that
x < P x holds for all x in X, that < P is a reflexive relation,
although no reason for stating other properties like symmetry, or transitivity, etc.,
can really, and generally, exist. In general, < P is not a partial order in X; less again
it is a total, or linear, relation since usually non comparable elements will exist, that
is, pairs x, y for which it is neither x < P y, nor y < P x.
In the case the word P is precisely used in X, the relation < P collapses in the
relation = P; that is, < P = < P ∩< P
−1 ⟺< P = < P
−1. For instance, there is no
way to directly stating that ‘7 is less odd than 517′; all odd numbers are equally
odd; analogously, no way of directly establishing that ‘7 is less prime than 17′
exists, all prime numbers are equally prime. The word ‘directly’ refers to doing it
without a previous new deﬁnition of ‘less than’ for the corresponding word, but
only under the old deﬁnitions of ‘odd’ and ‘prime’. Numbers are perfectly classiﬁed
in ‘odd’ and ‘not odd’, ‘prime’ and ‘not prime’, etc.
Once the relation < P is known, the graph (X, < P) reﬂects the ‘semantic
organization’ the use of P introduces in X; the graph is a qualitative meaning of P in
X. It should be pointed out that the universe X cannot be always supposed to be
mathematically structured; in plain language, words act in universes whatsoever.
For instance, the Kolmogorov’s theory of probability concerns ‘probable’ events in
a Boolean algebra, but, in plain language, the same word ‘probable’ is not only
applied to such kind of rigid events. A theory of linguistic meaning cannot presume
that the universe of discourse is directly endowed with a particular mathematical
structure.
216
E. Trillas

Notice that such a deﬁnition remembers the intuitive idea that, when (intelli-
gently) talking on some subject, some ‘ordering’ between the statements that are
uttered, gestured, or written at the respect and for the corresponding reasoning’s
argumentation, is tried to be introduced among them. Additionally, the former
deﬁnition also seems to be in agreement with the famous Ludwig Wittgenstein’s
statement [7], ‘The meaning of a word is its use in language’.
3.2. Once a graph (X, < P) is known, the possibility of measuring to which
extent x veriﬁes p is open. A measure on such graph is a mapping mP: X →[0, 1],
such that [8]:
(1) If x < P y, then mP (x) ≤mP (y),
(2) If x is maximal in the graph, that is, no other y verifying x < P y does exist,
then mP (x) = 1,
(3) If y is minimal in the graph, that is, no other x verifying x < P y does exist, then
mP (x) = 0.
Notice that the closed unit interval could be substituted by any closed interval
[a, b] in the real line, by just a playing the character of 0, and b that of 1 in the
former deﬁnition. Notice also that no additive law is presumed for mP; its deﬁnition
is free from considering ‘and’, ‘or’, and either the concepts of incompatibility or
contradiction that are only indistinguishable in the framework of Boolean algebras.
The additive law is deeply involved with a ‘rigid form’ of classifying elements,
and, in plain language contradiction is independent of incompatibility, contrarily,
for instance to the case of Ortho-lattices where the ﬁrst implies the second (p ≤q
′ => p ⋅q = 0), with the reciprocal only holding provided the Ortho-lattice is a
Boolean algebra, that is, it is distributive and consequently veriﬁes the law of
‘perfect repartition’, p = p ⋅q + p ⋅q′, for all pair p, q. In this case, p ⋅q = 0
implies p = p ⋅q′ ⇔p ≤q′, and the reciprocal also holds. A lot of structural laws
is necessary for the equivalence between contradiction and incompatibility;
something that cannot be presumed in plain language.
The former general deﬁnition of a measure is inspired on that of a ‘fuzzy
measure’ introduced by Michio Sugeno [9], but liberated from the constraints
imposed by just measuring subsets; it is free from any mathematical structure in X
further than that of graph. An antecedent of it can be found in the concept of a
‘fuzzy entropy’, introduced by Aldo de Luca, and Settimo Termini [10], where
P = fuzzy and < fuzzy coincides with the so called ‘sharpened order’ between
functions in [0, 1]X.
Like, for instance, in the case with probabilities, with Sugeno’s λ-measures, or
with de Luca-Termini fuzzy entropies, all of them measures, the three axioms of a
measure are not sufﬁcient for individuating a single mP, and additional suppositions
should be added for each one of them. Anyway, each triplet (X, < P, mP) facilitates
a quantity reﬂecting a quantitative meaning of P in X, and, in this way, each ‘full
meaning’ can be seen as a quantity.
But, What Is It Actually a Fuzzy Set?
217

By paraphrasing Lord Kelvin’s words [11], ‘If you cannot measure it, it is not
Science’, viewing meaning as a quantity can open the door towards a scientiﬁc-like
study of collectives. Let’s show a toy-example to illustrate what has been said.
Consider X = [0, 10], and P = big, generating the linguistic collective, or fuzzy
set ‘big numbers between 0 and 10′. This collective is not a set, provided ‘big’ is
not rigidly but ﬂexibly used; for instance, provided it can be stated that ‘8 is big’,
also ‘7.99 is big’ can be stated. A ‘Sorites’ argument [5] shows that, not being the
‘collective big’ empty since 10 is always considered big, it is not a subset of [0, 10].
The qualitative use of big in [0, 10] can be described by:
x < bigy ⇔x ≤y,
that is, by the qualitative meaning ([0, 10], ≤), a graph that is but a linearly ordered
interval with maximum 10 (the only maximal), and minimum 0 (the only minimal).
Hence, the measures of big in [0, 10] are the mappings mbig: [0, 10] →[0, 1], such
that,
(1) x ≤y = > mbig x
ð Þ ≤mbig y
ð Þ
(2) mbig 10
ð
Þ = 1
(3) mbig 0
ð Þ = 0,
to which, adding the condition of ‘usual ﬂexibility’,
(4) If x can be qualiﬁed as big, it exists ε (x) > 0 such that those y in the interval
(x – ε (x), x] can be also qualiﬁed as big,
that could be translated into mbig, as
(4*) mbig is continuous in [0, 10].
Hence, the measures of big are the mappings between [0, 10] and [0, 1] that are
strictly non-decreasing, and verify the border conditions (2) and (3). There is an
enormous amount of them.
Consequently, to specify a measure for the meaning of big, it is necessary to add
some additional information on the behavior of big, like it can be on its shape. For
instance, provided it is known, or can be reasonably presumed, that the measure
should be linear, mbig (x) = ax + b, it follows that the only possible linear measure
is x/10, but provided the information on its shape were that it is quadratic, mbig
(x) = ax2 + bx + c, several possibilities for the values of a and b are possible,
since by (3) it follows c = 0, and from (2) that 100 a + 10 b = 1; for instance, a
quadratic measure is x2/100 (with a = 1/100 and b = 0), but, obviously, it is not the
only quadratic measure that is possible for ‘big’.
In conclusion, the graphs ([0, 10], ≤, mbig), with mbig verifying the axioms
(1)–(4), plus some additional information or additional reasonable hypotheses, are
the quantities that specify a full meaning of ‘big’ in [0, 10]. These quantities require
the ‘design’ of the corresponding measures.
Notice that axioms (4) and (4*) exclude to interpret the use of big as a precise
one, since it will be speciﬁed by a subset of [0, 10] whose measure cannot be
continuous but with jumps; notwithstanding, such rigid interpretation is possible by
218
E. Trillas

avoiding (4), by renouncing to ﬂexibility. That is by ‘making precise’ the meaning
of ‘big’; something that means, indeed, changing the ordinary and often usual, use
of ‘big’ in plain language.
In some cases, the designer should add to the axioms, some hypotheses, rea-
sonable for the current situation he is faced with, and like it is the former linear
hypothesis. In this way, the linguistic collective P generates is qualitatively
described by the graph (X, < P) once it is known, and shows different ‘informa-
tional states’ each one given once a quantity (X, < P, mP) is speciﬁed.
3.3. The former interpretation of meaning as a quantity, actually preserves what
has been said for the precisely used words, and it can be proven as follows.
If P is precisely used in X, the graph is (X, = P), and then if x = P y ⇔x < P y &
y < P x. It implies mP (x) ≤mP (y) & mP (y) ≤mP (x), or mP (x) = mP (y). Thus,
only the values 0 and 1 can be taken by the measure, since there are no other
elements than the maximal (those verifying P), and the minimal (those not verifying
P). Hence, the subset mP
−1(1) consists in the maximal (or prototypical) elements, and
is the set P speciﬁed by P in X; obviously, it is mP
−1(0) = Pc that contains the
minimal or anti-prototypical elements.
4
Membership Functions
Is there any difference between the former measures of the linguistic label ‘big’, and
the membership functions assigned to it in any book on Fuzzy Logic? There is no
one. Thus, it can be stated that a membership function is, ideally, but a measure of
the qualitative meaning of its linguistic label; is a quantitative ‘informational state’
of the fuzzy set.
Nevertheless, as the word ‘ideally’ tries to remark, this statement should be
submitted to caution since a membership function is designed with the information
available to its designer; an information not always consisting in all the relation
‘less than’, nor in knowing all its maximal and minimal elements but only some of
them. There is some similarity with what happens when saying that the probability
of obtaining each one of the six faces in throwing a die is 1/6, but without knowing
if the die has some imperfection, or it is a tricky one, or the landing surface is not
perfectly plane. An ‘ideal’ die is supposed to have a probability of 1/6 for each one
of its faces; but often, ideal dice, those who throw them, and the landing surfaces,
are not perfect. To say nothing when the die is tricky, for instance, having inside
and in front of a face, a very small piece of plumb, or when it is thrown into sand.
Throwing a die is a real situation, and often real situations are not ‘ideal’; analo-
gously, the design of a membership function departs from some real situation in
plain language. No doubt that the measures are membership functions, but, are
membership functions always measures?
In addition, once the designer arrives at a membership function μP, a new
relation is automatically established in X:
But, What Is It Actually a Fuzzy Set?
219

x ≤μ y ⇔μP x
ð Þ ≤μP y
ð Þ,
that is a linear one and, consequently, not always coincidental with < P. Thus,
provided ≤μ substitutes < P, the new ‘qualitative meaning after design’ is not
already the original one. Provided μP were a measure, since
x < P y = > μP x
ð Þ ≤μP y
ð Þ⇔x ≤μ y, or < P ⊆≤μ,
the design enlarges the qualitative meaning. Hence, the original qualitative meaning
could be changed by the larger one coming from design; design could modify
meaning. This is certainly risky, since practitioners usually look at the behavior of P
in X after counting with a membership function and just through its shape; with it,
they can easily appreciate a larger qualitative meaning than the real one.
In conclusion, in most practical cases, the membership function cannot be
exactly a measure, but an often unknown approximation to one of them. Thus, to
well representing the meaning of a linguistic label, it is important to know when a
membership function can be seen as a good enough approximation to a measure.
Working with a membership function not well reﬂecting the meaning of its lin-
guistic label, can conduct to wrong results coming, for instance, from the fact that
the membership function actually represents a different linguistic label.
5
Searching for a Deﬁnition and a Theorem
Fuzzy Control counts with a theorem stating on which conditions the computed
output universally approaches the real one [12], but it lacks a similar theorem for
assuming that a membership function truly approaches a measure. That is, a the-
orem from whose antecedent it can follow that, given a measure mP of a qualitative
meaning (X, < P) of P in X, a designed membership function μP is ‘good enough’
provided it exists a measure mP such that either
(1) For all ε > 0 is ImP x
ð Þ −μP x
ð ÞI ≤ε, for all x in X,
or
(2) It is minimized the function Supx ∈X ImP (x) – μP (x)I.
Instead of a system of rules and a defuzziﬁcation’s method, what here is initially
known are the relation < P, or a part of it (both can be seen as a system of rules),
and the axioms mP should verify.
Both (1) and (2) could be considered as suitable deﬁnitions for stating that μP
approaches mP. Nevertheless, to prove them as a theorem’s conclusion, some
reasonable hypotheses, or some additional contextual information on the behavior
of P in X, should be taken into account. But it depends on each particular case, and,
for example, on how the designer did build up the membership function, that is, on
220
E. Trillas

the information available to s/he on the behavior of P in X. It remains an open
question that, perhaps, should be posed from a different point of view as it is, for
instance, beginning by modifying up to some limits the relation < P, or the mea-
sure, or by just considering some type of measures.
I mention such possibility under the conviction that a generally accepted deﬁ-
nition concerning the relation a designed membership function should keep with
measures, as well as to proving on which conditions it can hold, is an important
topic. Of course, were μP a measure, both the deﬁnition and the theorem are
unnecessary. Anyway, and at least, some sufﬁcient condition for knowing if the
membership function approaches a measure will be interesting. Up to when
something similar will be found, the design of linguistically described systems will
continue being done in a not standardized and blind form.
6
Conclusion
This paper is just a ﬁrst trial to penetrate on what the idea of a fuzzy set is, and on
what its description by membership functions means. That the topic is just open, but
not fully achieved, is manifested by the ﬁnal call towards clarifying which mem-
bership functions can be actually considered as a good enough approximation to the
fuzzy sets informational states.
What does not seem dubious is that fuzzy sets are linguistic, not mathematical,
entities, that rooted in plain language belong to its domain, and that their mem-
bership functions should come from a process of design. In themselves, fuzzy sets
seem to need a scientiﬁc-like study instead of a purely mathematical one. In
addition, it yet lacks to count with a standardization of the corresponding design’s
processes for what concerns the approximate character of membership functions.
Since plain language is full of imprecise words, to mathematically mimicking
ordinary reasoning, that is, to establish mathematical models for the not fully
deductive types of reasoning, it is strictly necessary to consider imprecision, and,
hence managing fuzzy sets for its symbolic representation. Thus, clarifying the idea
of ‘linguistic collectivization’ is relevant.
There are still several problems that remain open for counting with good enough
theoretical foundations of Zadeh’s ‘Computing with Words and Perceptions’,
whose ground lies in plain language and ordinary reasoning. For instance, ambi-
guity also permeates plain language and no mathematical model for scientiﬁcally
managing ambiguity is currently known.
Notwithstanding, what can be asserted is that only one kind of fuzzy set actually
exists, and that names like ‘type-two fuzzy set’ only can refer to the range in which
the membership function takes its values.
But, What Is It Actually a Fuzzy Set?
221

References
1. Zadeh, L.A.: Fuzzy Sets. Inf. Control 8, 338–353 (1965)
2. Quine, W.V.O.: Ontological Relativity and Other Essays. Columbia University Press, New
York (1969)
3. Trillas, E.: On a model for the meaning of Predicates. In: Seising, R. (ed.) Views on Fuzzy
Sets and Systems from Different Perspectives, pp. 175–205. Springer, Berlin (2009)
4. Halmos, P.R.: Naive Set Theory. Van Nostrand, New York (1960)
5. Trillas, E., Urtubey, L.A.: Towards the dissolution of the sorites paradox. J. Appl. Soft
Comput. 11(2), 1506–1510 (2011)
6. Trillas, E., Eciolaza, L.: Fuzzy logic. An Introductory Course for Engineers. Springer, Berlin
(2015)
7. Wittgenstein, L.: Philosophical Investigations. Basil Blackwell, London (1958)
8. Trillas, E.: On a Model for the meaning of predicates. In: Seising, R. (ed.) Views of Fuzzy
Sets and Systems from Different Perspectives pp. 175–2005. Springer, Berlin (2009)
9. Sugeno, M.: Theory of fuzzy integrals and its applications. Ph.D. Tokyo Institute of
Technology, Tokyo (1974)
10. de Luca, A., Termini, S.: A deﬁnition of a non-probabilistic entropy in the setting of fuzzy sets
theory. Inf. Control 20, 301–312 (1972)
11. Lord Kelvin, W.T.: Lecture on electrical units of measurement; popular lectures and
addresses, I/79. Mc Millan, London (1889)
12. Castro, J.L., Delgado, M.: Fuzzy systems with defuzziﬁcation are universal approximators.
IEEE Trans. Syst. Man Cybern. 26, 149–152 (1996)
222
E. Trillas

Gradual Numbers and Fuzzy Solutions
to Fuzzy Optimization Problems
Didier Dubois and Henri Prade
Abstract This short paper indicates that early examples of fuzzy elements in a fuzzy
set, that is, entities that assign elements to membership values, in contrast with fuzzy
sets that assign membership values to elements, can be found in papers by Verdegay
in the early 1980, following a line of thought opened by Orlovsky. They are so-called
fuzzy solutions to fuzzy optimization problems. The notion of fuzzy element, and
more speciﬁcally gradual number sheds some light on the ambiguous notion of fuzzy
number often viewed as generalizing a number while it generalizes intervals. The
notion of fuzzy solution is in fact a parameterized solution, in the style of parametric
programming. These considerations show the pioneering contributions of Verdegay
to the development of fuzzy optimization.
1
Introduction
In the literature of fuzzy optimisation, initiated by Bellman and Zadeh [1], the stan-
dard formulation of optimizing an objective function under some rigidly deﬁned
constraints is replaced by the search for a solution with maximal membership grade
in a fuzzy set deﬁned by intersecting the fuzzy sets of feasible solutions accord-
ing to several constraints, and the fuzzy sets of good solutions according to one or
several objectives. It is clear that under this relaxed form of optimization problem,
constraints and objective functions play the same role and because of the use of the
minimum operator to aggregate the various fuzzy sets, there is no compensation
allowed between local satisfaction degrees. In some sense, as argued in [5], Bell-
man and Zadeh’s formulation is a pioneering generalization of constraint satisfaction
problems, understood as ﬂexible or soft constraint satisfaction problems [10] where
D. Dubois(✉) ⋅H. Prade
CNRS-IRIT, Université Paul, Sabatier 118, Route de Narbonne,
31062 Toulouse, Cedex 9, France
e-mail: dubois@irit.fr
H. Prade
e-mail: prade@irit.fr
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_13
223

224
D. Dubois and H. Prade
the satisfaction of constraints is a matter of degree. This formulation of fuzzy opti-
mization problems, including multiobjective ones, was extensively applied to linear
programming, especially by Tanaka et al. [15], Zimmermann [19] and Chanas [3],
and further on used by many scholars.
This formulation of fuzzy optimization now belongs to history as almost every
possible development of this formulation has been studied. However, what is strik-
ing with this approach is that the diﬀerence between constraints and objectives is
blurred. Once changed into fuzzy sets, they play the same role (of ﬂexible con-
straints) in the mathematical formulation. Yet quite early, some scholars in the late
1970s such as Orlovsky [13] and in the early 1980s, especially our colleague and
friend Curro Verdegay [16, 17] envisaged the fuzzy optimization problem in a dif-
ferent way, acknowledging the speciﬁc role of the objective function as opposed
to fuzzy constraints, and proposed the idea of fuzzy solution to fuzzy optimisation
problems.
More recently, and in a totally diﬀerent context, the authors of this note pointed
out the ambiguity of the terminology “fuzzy number” [7, 8], often viewed as the
fuzzy extension of a number while it generalizes intervals, and the questionable
understanding of the notion of defuzziﬁcation of a fuzzy set that is often supposed to
yield a precise element rather than a crisp set. These considerations led us to consider
the concept of fuzzy (or yet gradual) element in a fuzzy set, for which the notion of
membership function is in some sense taken the other way around.
The aim of this note is to indicate the close connection between Verdegay’s pro-
posal of fuzzy solution, and the notion of fuzzy element in a fuzzy set: a fuzzy solu-
tion is a fuzzy element in the fuzzy set of feasible solutions to the fuzzy optimization
problem.
2
Fuzzy Solutions to Fuzzy Mathematical Programming
Problems
At the origin, the introduction of fuzzy sets in optimization problems led to the
maximisation of an objective function f(x) under fuzzy constraints on a set X
of potential solutions x [13, 15]. Typically, the objective function is a mapping
x ∈ℝn ↦f(x) ∈ℝ, and the constraints form a fuzzy set of feasible solutions in ℝn.
The fuzzy constraint set is built by considering ﬁrst a set of crisp constraints deﬁn-
ing a set of feasible solutions U ⊂X , and a set of fuzzy constraints ̃Ci, i = 1, … m.
Let ̃C be the intersection of the ̃Ci’s and U obtained using the intersection operation
minimum.
Then we have to make sense of an optimal solution over the set of feasible solu-
tions ̃C when the latter is fuzzy.
Following Bellman and Zadeh [1], one way is to build a fuzzy set from f and
combine it with the fuzzy constraint set ̃C using again the intersection operation
minimum. The simple way of constructing a fuzzy set is to consider the optimal

Gradual Numbers and Fuzzy Solutions to Fuzzy Optimization Problems
225
value z∗of f(x) in the support of the fuzzy constraint set {x ∶𝜇̃C(x) > 0}, and let
the fuzzy optimizing set F be deﬁned as 𝜇F(x) = f(x)
z∗. Then we can deﬁne the fuzzy
set of optimal solutions as F ∩̃C, an optimal solution x∗being the one with maximal
membership grade in F ∩̃C, a path initiated by the late Tanaka and colleagues [15].
However this is just one way of deﬁning the fuzzy optimizing set as there can be
many increasing functions 𝜙∶ℝ→ℝwith which we can deﬁne 𝜇F as 𝜙(f(x))
𝜙(z∗) . So in
order to properly deﬁne the fuzzy set F∩̃C, we need to deﬁne a scaling function 𝜙that
makes the objective function and the membership function of the fuzzy constraint set
commensurable. Clearly, the choice of function 𝜙aﬀects the choice of the optimal
solution, which can be written x∗
𝜙. Zimmermann [19] found the way to choose a
scaling function that makes sense, using a fuzzy expectation level on the objective
function, hence giving up the idea of optimizing f. Namely, suppose the user can
ﬁnd thresholds z and z such that f(x) ≥z is judged suﬃcient for a solution x to
be good enough, and completely insuﬃcient if f(x) ≤z. Then 𝜇F is often chosen
as a linear increasing membership function such that 𝜇F(x) = 0 if f(x) ≤z, and 1
if f(x) ≥z. In this case, if the fuzzy constraints are deﬁned by linear membership
functions, maximizing min(𝜇F(x), 𝜇̃C(x)) becomes a reasonable approach, whereby
the objective function is handled as another fuzzy constraint. It readily extends to
the case of several objective functions.
An alternative approach which dispenses with the choice of a scaling function was
proposed by Verdegay [16], after Orlovsky [13], and sticks to the idea that objective
functions and constraints, be they fuzzy, do not play the same role [16, 17]. Namely
consider a feasibility threshold 𝛼and the crisp feasible set C𝛼= {x ∶𝜇̃C(x) ≥𝛼}.
It is then clear that we can get a standard optimization problem of maximizing f(x)
over C𝛼. The set of optimal solutions to this problem is deﬁned by S(𝛼) = {x∗∶
f(x∗) = supx∈C𝛼f(x)}. Verdegay noticed that this is a form of parametric mathemati-
cal programming (MP) problem, in the sense that each choice of threshold 𝛼yields a
diﬀerent set S(𝛼) of optimal solutions. The fuzzy set ̃S of optimal solutions to a fuzzy
mathematical programming problem is then deﬁned by Verdegay [16] by applying
Zadeh’s result for reconstructing a fuzzy set from its alpha-cuts:
𝜇̃S(x) = sup{𝛼∶x ∈S(𝛼)}.
It is known since Orlovsky [13] (see also [6], pp. 102–103) that if S = ∪𝛼>0S(𝛼) then
̃S = S∩̃C. In contrast with the fuzzy set of solutions based on rescaling the objective
function, this deﬁnition of a fuzzy solution to a fuzzy MP problem does not depend
on any scaling function. It is up to the decision maker to choose the decision ̂x that
maximizes 𝜇̃S(x), or to choose some solution in the set S(𝛼) for a suitable choice of
𝛼. This view is also very close to the parametric programming approach to fuzzy
linear programming ﬁrst proposed by the late Stefan Chanas [3] (see also [2]).
Verdegay [16] also showed that the two paradigms of fuzzy MP can be related if
it is noticed that we can solve the ﬁxed point problem 𝜓(𝛼) = supx∈C𝛼𝜇F(x) = 𝛼,
which has a solution under continuity conditions since 𝜓is decreasing with 𝛼. The
ﬁxed point solution 𝛼∗yields a set of solutions to the fuzzy MP x∗∈S(𝛼∗) that do

226
D. Dubois and H. Prade
maximize min(𝜇F(x), 𝜇̃C(x)), i.e., correspond to the optimal solutions in the other
paradigm.
The fuzzy solution of Verdegay is a fuzzy set constructed from crisp sets S(𝛼) that
are not necessarily nested. Indeed we do not have that if 𝛼> 𝛽then S(𝛼) ⊆S(𝛽).
This is because the optimal solutions in C𝛽may fail to lie in its subset C𝛼. So we can
make two observations:
∙The sets S(𝛼)’s are not nested. In fact, if 𝛼> 𝛽, either S(𝛼) ⊆S(𝛽) or S(𝛼)∩S(𝛽) =
∅, i.e., either S(𝛼) = C𝛼∩S(𝛽) or S(𝛼) ∩S(𝛽) = ∅. Moreover, the S(𝛼)’s can be
singletons if the optimal solutions are unique.
∙The deﬁnition of a fuzzy optimal set in the sense of Verdegay is given by the
mapping 𝛼∈(0, 1] ↦S(𝛼) ⊂X , not originally a membership function X →
[0, 1].
This type of layered representation has been studied in the literature since Negoita
and Ralescu [12] in the nested case, but the non-nested one has been more recently
studied under the names of gradual sets [7], or RL-representations by Sanchez et al.
[14] and by Martin and Azvine [9] in the so-called X𝜇approach. When S(𝛼)’s are
singletons, the fuzzy solution corresponds to a fuzzy element whose deﬁnition is now
recalled.
3
Fuzzy Elements and Gradual Numbers
Given a distributive lattice L with top 1 and bottom 0, and a set S, a fuzzy element ̃s
is deﬁned by a mapping Ãs ∶L+ →S, where L+ = L⧵{0} [7]. L is called a relevance
scale, and Ãs an assignment function. The idea is that the choice of elements in S is
parameterized by elements in L that are ordered in terms of relevance (e.g., quality,
excellence, plausibility etc.): an element s is determined by 𝛼∈L+ in the sense that
Ãs(𝛼) = s; we may write s𝛼∈S for simplicity.
Note that an assignment function does not always determine a fuzzy set. How-
ever, given a fuzzy set F on S, with values in L, the inverse function 𝜇−1
F exists and is
an assignment function if this membership function 𝜇F is bijective. So, some fuzzy
sets can be viewed as fuzzy elements of the universe where the fuzzy set lies, for-
mally. However, it is clear that beyond their possible mathematical identity, the two
notions are distinct and will not be processed identically, e.g., fuzzy elements cannot
be intersected like fuzzy sets do.
It is possible to view a fuzzy set as a fuzzy element of the power set ℘(S) using
alpha-cuts, namely, the mapping 𝛼↦F𝛼= {s ∶𝜇F(s) ≥𝛼} deﬁnes what can be
called a gradual set, which is another description of a fuzzy set. However, there is no
monotony constraint for a fuzzy element ̃G in ℘(S), namely 𝛼≥𝛼′ does not imply
̃G(𝛼) ⊆̃G(𝛼′) like for fuzzy sets. Gradual sets are akin to so-called soft sets [11]
(although the domain of a soft set is not supposed to be ordered, so that it is just the
well-known notion of a relation), but they have been studied by other researchers, as
pointed out in the previous section [9, 14].

Gradual Numbers and Fuzzy Solutions to Fuzzy Optimization Problems
227
A fuzzy set F can be viewed as a crisp set of fuzzy elements. Indeed, fuzzy ele-
ments are obtained by picking one element s𝛼in each cut F𝛼, namely F = {̃s ∶s𝛼∈
F𝛼, 𝛼∈L+}, and we can show that the fuzzy set F is the set of its fuzzy elements
via the reconstruction formula
𝜇F(s) = max{𝛼∶s𝛼= s}.
A gradual real number ̃r is a fuzzy element of the reals, letting S = ℝand L =
[0, 1] [8]. Contrary to numbers, gradual real numbers are not totally ordered, and a
partial order on gradual numbers can be naturally deﬁned as follows: Let ̃r and ̃s be
two gradual numbers. A gradual real number ̃r pointwisely dominates a gradual real
number ̃s (written ̃r ≥̃s) if and only if ∀𝛼∈(0, 1] r𝛼≥s𝛼.
Given a fuzzy interval M, that is a fuzzy set of reals whose alpha-cuts are closed
intervals [m𝛼, m𝛼], we have that M = {̃r ∶m𝛼≤r𝛼≤m𝛼, 𝛼∈L+}. As suggested in
the introduction, the term fuzzy number, often used in place of fuzzy interval, may be
misleading. Note that the diﬀerence between a gradual number and a fuzzy number
in the usual acception is that the latter is a parametrized interval (depending on the
choice of 𝛼) while a gradual number is a parameterized number, again controlled
by 𝛼.
Consider two bijective functions
𝜇M−(r) =
{
𝜇M(r) if r ≤m1;
1 otherwise.
and
𝜇M+(r) =
{
𝜇M(r) if r ≥m1;
1 otherwise.
A fuzzy interval can be viewed as a crisp interval bounded by gradual numbers ̃m
and ̃m deﬁned by assignment functions A ̃m(r) = 𝜇−1
M−(r) and A ̃m(r) = 𝜇−1
M+(r).
Then we can view a fuzzy interval M as the interval [ ̃m, ̃m] = {̃r ∶̃m ≤̃r ∶≤̃m}.
These gradual numbers have the shape of cumulative ( ̃m) or survival ( ̃m) probability
functions, but can be considered as fuzzy thresholds oﬀering a gradual transition
from one half of the reals to the other half.
Operations on gradual numbers are deﬁned in [8] as standard pointwise operations
on assignment functions. For instance:
∙The sum of two gradual numbers ̃r and ̃s is deﬁned by summing their assignment
functions. It is ̃r + ̃s such that ∀𝛼∈(0, 1] Ãr+̃s(𝛼) = Ãr(𝛼) + Ãs(𝛼). For simplicity,
we write ∀𝛼∈(0, 1], (̃r + ̃s)(𝛼) = ̃r(𝛼) + ̃s(𝛼) = r𝛼+ s𝛼.
∙The set of gradual numbers equipped with the addition operation forms a com-
mutative group with identity ̃0 such that Ã0(𝛼) = 0 ∀𝛼∈(0, 1]. The gradual
number ̃r has the inverse −̃r under the addition ∀𝛼∈(0, 1] A−̃r(𝛼) = −Ãr(𝛼) and
̃r + (−(̃r)) = ̃0.

228
D. Dubois and H. Prade
∙From the group structure, the subtraction operation is deﬁned by Ãr−̃s = Ãr+A−̃s =
Ãr −Ãs.
Any algebraic operation on real numbers can be straightforwardly extended to grad-
ual numbers. For details see [8]. Note that since gradual numbers are a fuzzy exten-
sion of numbers, not intervals, the algebraic structure of numbers is for the most part
inherited by gradual numbers. This fact sheds useful light in the issue of confus-
ing a fuzzy number understood as a fuzzy interval and a fuzzy number understood
as a gradual number. For instance, a monotonic gradual number ̃r in the form of a
cumulative distribution function (a form of fuzzy threshold used in fuzzy linear pro-
gramming [19]) can be viewed as the gradual extension of a crisp number r ∈ℝ, and
the fuzzy extension of a semi-open interval [r, +∞). Under the ﬁrst view it is natural
to have that the diﬀerence Ãr−̃s(𝛼) = r𝛼−s𝛼, which is the gradual number extending
r −s. In contrast, the result of the fuzzy subtraction ̃r ⊖̃s based on the extension
principle, where we regard the monotonic gradual numbers as fuzzy intervals, is not
obtained by computing r𝛼−s𝛼as it is most of the time the whole real line. Namely,
the interval subtraction
[r, +∞) −[s, +∞) = {x −y ∶x ≥r, y ≥s} = ℝ.
At this point it should be clear that the fuzzy solution to a fuzzy MP problem
in the sense of Verdegay [16] is a very early example of a fuzzy element in (or a
gradual subset of) the fuzzy constraint set, since we do have, by construction, that
S(𝛼) ⊂C𝛼, ∀𝛼∈(0, 1]. This is just one example of gradual set or fuzzy element that
can be found in the literature. In this case, it turns out that the speciﬁc features of
such a fuzzy solution (any two sets in the family are nested or disjoint) allow us to
view it as a fuzzy set from which we can recompute the gradual set via alpha-cutting.
Other more recent examples of fuzzy elements and non-nested gradual sets are for
instance the relative fuzzy cardinality of a fuzzy set [4], the fuzzy probability of a
fuzzy event, the midpoint of a fuzzy interval [8], or the Haussdorf distance between
fuzzy sets (see [7] for discussions).
4
Conclusion
This note has the only ambition to demonstrate the pioneering role of our colleague
and friend J.L. Verdegay in the early times of fuzzy optimization, showing that he
developed an alternative view to a fuzzy solution set, that he recalled in his recent
retrospective position paper [18] and that would perhaps deserve to be studied fur-
ther, as opposed to the popular max-min approach to fuzzy MP. It seems that recent
research from Granada university focused on so-called SL-representations [4, 14].
The necessity to properly formalize fuzzy set related notions,such as fuzzy solutions
to fuzzy optimization problems, led to deﬁne fuzzy elements in fuzzy sets and the
like, where counterparts to alpha-cuts are no longer nested. Moreover, the connec-

Gradual Numbers and Fuzzy Solutions to Fuzzy Optimization Problems
229
tion to parametric programming also makes it clear that, just as a fuzzy solution is
also a parametric solution driven by membership grades, a fuzzy set can be deﬁned
by letting a set depend on a parameter ranging on an ordered scale, and any scalar
evaluation of this set deﬁnes a gradual element.
References
1. Bellman, R.E., Zadeh, L.A.: Decision making in a fuzzy environment. Manag. Sci. 17, B141–
B164 (1970)
2. Carlsson, C., Korhonen, P.: A parametric approach to fuzzy linear programming. Fuzzy Sets
Syst. 20, 17–30 (1986)
3. Chanas, S.: The use of parametric programming in fuzzy linear programming. Fuzzy Sets Syst.
11, 243–251 (1983)
4. Delgado, M., Ruiz, M.D., Sánchez, D., Vila, M.A.: Fuzzy quantiﬁcation: a state of the art.
Fuzzy Sets Syst. 242, 1–30 (2014)
5. Dubois, D., Fargier, H., Prade, H.: Possibility theory in constraint satisfaction problems: han-
dling priority, preference and uncertainty. Appl. Intell. 6, 287–309 (1996)
6. Dubois, D., Prade, H.: Fuzzy Sets and Systems: Theory and Applications, Mathematics in
Science and Engineering Series, vol. 144. Academic Press, New York (1980)
7. Dubois, D., Prade, H.: Fuzzy elements in a fuzzy set. Soft Comput. 12, 165–175 (2008)
8. Fortin, J., Dubois, D., Fargier, H.: Gradual numbers and their application to fuzzy interval
analysis. IEEE Trans. Fuzzy Syst. 16, 388–402 (2008)
9. Martin, T.P., Azvine, B.: The X-mu approach: Fuzzy quantities, fuzzy arithmetic and fuzzy
association rules. In: Proceedings of IEEE Symposium on Foundations of Computational Intel-
ligence (FOCI), pp. 24–29 (2013)
10. Meseguer P., Rossi F., Schiex T.: Soft Constraints, Chapter 9 in Foundations of Artiﬁcial Intel-
ligence, vol. 2, pp. 281–328. Elsevier (2006)
11. Molodtsov, D.: Soft set theory—ﬁrst results. Comput. Math. Appl. 37(4/5), 19–31 (1999)
12. Negoita, C.V., Ralescu, D.A.: Applications of Fuzzy Sets to Systems Analysis. Birkhauser,
Basel (1975)
13. Orlovsky, S.A.: Programming with fuzzy constraint sets. Kybernetes 6, 197–201 (1977)
14. Sanchez, D., Delgado, M., Vila, M.A., Chamorro-Martinez, J.: On a non-nested level-based
representation of fuzziness. Fuzzy Sets Syst. 192, 159–175 (2012)
15. Tanaka, H., Okuda, T., Asai, K.: On fuzzy mathematical programming. J. Cybernet. 3, 37–46
(1974)
16. Verdegay, J.L.: Fuzzy mathematical programming. In: Gupta, M.M., Sanchez, E. (eds.) Fuzzy
Information and Decision Processes, North Holland, pp. 231–237 (1982)
17. Verdegay, J.L.: A dual approach to solve the fuzzy linear programming problem. Fuzzy Sets
Syst. 14(2), 131–141 (1984)
18. Verdegay, J.L.: Progress on fuzzy mathematical programming: a personal perspective. Fuzzy
Sets Syst. 281, 219–226 (2015)
19. Zimmermann, H.-J.: Description and optimization of fuzzy systems. Int. J. General Syst. 2,
209–215 (1975)

Using Fuzzy Measures to Construct
Multi-criteria Decision Functions
Ronald R. Yager
Abstract We are interested in the formulation of multi-criteria decision functions
based on the use of a measure over the space of criteria. Speciﬁcally the relationship
between the criteria is expressed using a fuzzy measure. We then use the Choquet
integral to construct decision functions based on the measure. We look at a number
of different decision functions generated from speciﬁc classes of measures.
1
Introduction
One approach to multi-criteria decision-making is to construct decision function by
aggregating an alternative’s satisfaction to the individual criteria and then selecting
the alternative with the largest aggregated value of the individual criteria [1]. Our
focus here is on the formulation of multi-criteria decision functions where our
aggregation method is based on the use of a fuzzy measure (monotonic set measure)
and the Choquet integral [2–5]. In this framework the measure is used to convey
information about criteria importance’s and relationships between the constituent
criteria. We ﬁrst describe the general approach to the formulation of decision
functions using this framework. We look at the types of aggregation functions that
are generated from various classes of measures. We show to how to aggregate the
underlying measures to enable the modeling of more complex relationships
between the criteria from simple relationships.
R.R. Yager (✉)
Machine Intelligence Institute, Iona College, New Rochelle, NY 10801, USA
e-mail: yager@panix.com
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_14
231

2
Measure Based Approach to Multi-criteria
Decision Making
Assume we have a collection C = {C1, ⋅⋅⋅, Cn} of criteria of interest in a decision
problem. Let X be a set of alternatives from among which we must select the one
that best satisﬁes the criteria. Here for each alternative x we let Ci(x) ∈[0, 1]
indicate the degree to which criteria Ci is satisﬁed by alternative x. In order to select
the alternative that best satisﬁes the collection C we must provide some function F
that indicates the degree to which each x satisﬁes the collection of criteria. We shall
denote this as F(x) = Agg(C1(x), C2(x), ⋅⋅⋅, Cn(x)).
A very general formulation for Agg can be obtained with the aid of a fuzzy
measure on the space of criteria and the use of an appropriate integral [3]. In this
approach the fuzzy measure is used to express the structural relationship between
the criteria. A fuzzy measure on the space C of criteria is a mapping μ: 2C →[0, 1]
such that
μðCÞ = 1, 2.μð∅Þ = 0 and 3.μ A
ð Þ ≥μ B
ð Þ if A⊇B
ð1Þ
An interpretation of μ in this environment of multi-criteria decision-making is
that for any subset A in C, μ(A) is the importance associated with the subset of
criteria in A.
As we indicated we shall use the fuzzy measure to guide the construction of the
aggregation function F(x) = Agg(C1(x), ⋅⋅⋅, Cn(x)). One general approach for
obtaining a decision function F that makes use of this fuzzy measure on the col-
lection of criteria is the Choquet integral, F(x) = Chμ(C1(x), C2(x), ⋅⋅⋅, Cn(x))
[2–5]. In anticipation of introducing the Choquet integral we provide some for-
malism. For a given alternative x we let id(j) be the index of the jth most satisﬁed
criteria. Thus Cid(j)(x) is the degree of satisfaction of the jth most satisﬁed criteria.
We now let Hj = {C id(k)| k = 1 to j}, it is the collection of the j most satisﬁed
criteria. Here we see Hn = C. We shall let H0 = ∅by convention. We see here that
Hk ⊆Hj for j ≥k. We further see from the monotonicity of μ that μ(Hj) is a
monotonically non-decreasing function of j, μ(Hi) ≥μ(Hk) if i ≥k. Using the
μ(Hj) we can obtain a collection of weights, wj = μ(Hj) −μ(Hj−1), for j = 1 to n. It
is easily to show that wj ≥0 for all j and ∑n
j = 1 wj = 1. Using this we get
F x
ð Þ = Chμ C1 x
ð Þ, C2 x
ð Þ, ⋯, Cn x
ð Þ
ð
Þ = ∑n
j = 1 wjCid jð Þ x
ð Þ.
It is easy to show that this type of aggregation function is a mean. In particular it
is known that F(x) is bounded, Mini[Ci(x)] ≤F(x) ≤Maxi[Ci(x)], and monotonic,
if Ci(x) ≥Ci(y) for all i then F(x) ≥F(y). Another property we can show is that if
μ1(A) ≥μ2(A) for all A then for x, F1(x) ≥F2(x).
232
R.R. Yager

3
Combining Measures
We shall make some general observations about fuzzy measures. First we recall the
deﬁnition of an aggregation function [5]. A mapping Agg: In →I is called an
aggregation function if it satisﬁes the three conditions. (1) Agg(0, 0, ⋅⋅⋅, 0) = 0,
(2) Agg(1, ⋅⋅⋅, 1) = 1 and (3) Agg(a1, ⋅⋅⋅, an) ≥Agg(b1, ⋅⋅⋅, bn) if ai ≥bi for
all i.
We note that there are many aggregation functions [5], among the more notable
of these are the Max, the Min and the average. We also note that product is an
aggregation function. All mean operators are aggregation functions as are t-norms
and t-conorms [5].
In the following we note a fundamental theorem of fuzzy measures [6].
Theorem: Assume μ1, …, μq are a collection of fuzzy measures on the space Z.
If Agg is an aggregation function then the set function μ deﬁned such that
μ(A) = Agg(μ1(A), ⋅⋅⋅, μn(A)) for all subsets A of Z is itself a fuzzy measure.
We shall refer to this as FTAM, the Fundamental Theorem on Aggregation of
Measures. The FTAM provides a very general approach to constructing set mea-
sures from other measures. We shall use the notation μ = Agg(μ1, …, μn) to
indicate that μ is deﬁned so that μ(A) = Agg(μ(A), …, μ(A)) for all subsets A.
Since product is an aggregation function then μ = μ1μ2, …, μn is a measure.
Also we note that μ = Max(μ1, …, μn) is a measure as well as Min(μ1, …, μn).
Furthermore if wj for j = 1 to j are such that wj ∈[0, 1] and Σ wj = 1 then
μ = ∑n
j = 1 wjμj is a measure. Here μ A
ð Þ = ∑n
j = 1 wjμjðAÞ for all subsets A of Z.
We now recall that the Choquet integral generates an aggregation function. Thus
this can provide a methodology for constructing new measures. Let C = {C1, ⋅⋅⋅,
Cq} be a collection of criteria. Let R = {μ1, …, μr} be a collection of measures on
the space C of criteria. Let m be a measure on the space R. We now use this to form
a compound measure μ on C deﬁned so that for any subset A of the criteria C we
have
μ A
ð Þ = Choqmðμ1 A
ð Þ, μ2 A
ð Þ, ⋯, μr A
ð ÞÞ
Here μ(A) is the Choquet integral with respect to m with arguments μj(A). We
now show how this measure μ can be used to determine the overall satisfaction of
alternative x, F(x).
Assume Ci(x) = a1, is the satisfaction of criteria Ci by alternative x. Without loss
of generality we shall assume the indexing has done so that ai ≥ak if i < k. In this
case Hj = {C1, C2, ⋅⋅⋅, Cj}.
Using the Choquet integral we have F x
ð Þ = F a1, ⋯, an
ð
Þ = ∑n
j = 1 wjaj where
wj = μ(Hj) −μ(Hj−1). However here μ(Hj) = Choqm(μ1(Hj), μ2(Hj), ⋅⋅⋅, μr(Hj).
Using Fuzzy Measures to Construct Multi-criteria …
233

4
Basic Weighted Average Aggregation
In the following we shall look at the types of multi-criteria decision functions we
get using the Choquet integral under some notable examples of fuzzy measures. The
most basic example is the additive measure. For this measure we deﬁne μ as
follows. For each Ci we associate a value αi such αi ∈[0, 1] and ∑q
i = 1 αi = 1. Here
for any subset A ⊆C we have μ A
ð Þ = ∑k, Ck ∈A αk. Here if A(Ck) is the mem-
bership grade of Ck in A, A(Ck) = 1 if Ck ∈A and (Ck) = 0 if Ck ∉A then
μ A
ð Þ = ∑q
k = 1 αkAðCkÞ
Let us now obtain the Choquet integral in this case. If Ci(x) is the satisfaction of
Ci by x then F(x) = Choqμ[C1(x), ⋅⋅⋅, Cq(x)]. Let id(j) be the index of jth largest
of the Ci(x) using this we have Hj = {Cid(k)/k = 1 to j}. In this case
F x
ð Þ = ∑q
j = 1 ðμðHjÞ −μðHj −1ÞÞCid jð Þ x
ð Þ. Since μ Hj


= ∑j
k = 1 αidðkÞ and μ Hj


=
∑j −1
k = 1 αidðkÞ then μ(Hj) −μ(Hj−1) = αid(j) and hence F x
ð Þ = ∑q
j = 1 αid ðjÞCid jð Þ x
ð Þ =
∑q
i = 1 αiCiðx). It is the simple weighted average of the satisfactions where the
weight associated with criteria Ci is αi. Here we have the notable feature that for a
given criteria, Ci, no matter what position it appears in the ordering id its associated
weight is always αi. Thus in this case it appears justiﬁable to refer to αI as the
importance associated with Ci.
5
Cardinality Based Measures and OWA Aggregation
An important class of measures studied by Yager [7] are the cardinality based
measures. A fuzzy measure μ is called a cardinality based measure if μ(A) = V|A|.
Here the measure of a subset just depends upon the number of elements in it. It is
understood here if A = ∅, μ(∅) = 0 and hence V0 = 0 and since μ(C) = 1 we have
Vn = 1. Thus we see that a cardinality based measure is deﬁned by a set of values
0 = V0 ≤V1 ≤V2 ≤⋅⋅⋅≤Vn = 1 such that μ(A) = V|A|.
Let us see the Choquet integral in the case of a cardinality-based measure. Since
Fμ x
ð Þ = ∑
n
J = 1
ðμðHjÞ −μ Hj −1


ÞCid jð Þ x
ð Þ
where Hj is the set of the j criteria with the largest satisfaction to x. We note that
since the cardinality of Hj is j thus Fμ x
ð Þ = ∑n
J = 1 ðVj −Vj −1ÞCid jð Þ x
ð Þ. If we denote
Vj −Vj−1 = wj we have Fμ x
ð Þ = ∑n
J = 1 wjCid jð Þ x
ð Þ where each wj ≥
0 and
∑n
J = 1 wj = 1. We see here that this is the OWA aggregation operator introduced by
Yager [8].
234
R.R. Yager

A number of important examples of this case are (1) μ* where V0 = 0 and
Vj = 1 for j > 1, (2) μ* where Vn = 1 and Vj = 0 for j < n and (3) μS where
Vj = j/n for all j. We easily see that
(1) μ* ⇒Fμ* x
ð Þ = Maxi Ci x
ð Þ
½

(2) μ* ⇒Fμ* x
ð Þ = Mini Ci x
ð Þ
½

(3) μS ⇒FμS x
ð Þ = 1n ∑n
i = 1 Ciðx)
In [9] Yager suggested we can obtain the parameters for this the type of car-
dinality based measure using a function called a weight generating function g: [0, 1]
→[0, 1] having the properties: (1) g(0) = 0, (2) g(1) = 1 and (3) g(x) ≥g(y) if
x > y (monotonicity). Using this weight generating function we can obtain
Vj = g
j
n
 
and wj = g
j
n
 
−g j −1
n


.
We note a special case of g is linear, g(x) = x. Here we get Vj = j/n and
wj = 1/n.
In [9] Yager discussed various semantics that can be associated with g. One
particularly notable semantics is where g is a quantiﬁer indicating the proportion of
criteria that must be satisﬁed.
Earlier we showed that if μ1 and μ2 are two measures such that μ1(A) ≥μ2(A)
for all A then if F1(x) and F2(x) are the respectively Choquet integrals obtained
using these measures then F1(x) ≥F2(x) for all x. Let us look at the implication of
this for the case of cardinality-based measures and the related OWA operator. If μ1
and μ2 are two cardinality based measures such that V1k and V2k are their respected
parameters then μ1(A) = V1|A| and μ2(A) = V2|A| and if V1k ≥V2k for k = 1 to n
we have F1(x) ≥F2(x) for all x.
Consider now an OWA operator deﬁned in terms of a collection of weights w1,
…, wn. From the preceding we see that this is equal to a cardinality-based for-
mulation in which Vj = ∑j
k = 1 wk. From this we can conclude the following.
Assume wik and w2k are two collections of OWA weights. Let OWA1(C1(x), ⋅⋅⋅,
Cn(x)) and OWA2(C1(x), ⋅⋅⋅, Cn(x)) be the OWA aggregations under these
respective weights. Then we see that if for all j we have that ∑j
k = 1 w1k ≥∑j
k = 1 w2k
then OWA1(C1(x), ⋅⋅⋅, Cn(x)) ≥OWA2(C1(x), ⋅⋅⋅, Cn(x)). Furthermore if g1
and g2 are two weight generating functions such that g1(y) ≥g2(y) for all y ∈[0,
1] then the aggregation obtained using g1 will always be at least as large as that
obtained using g2.
A related class of measures can be obtained using a function g and a set of
weights, αj, associated with each Cj such that αj ∈[0, 1] and Σjαj = 1. Here αj is
seen as some kind of importance associated with criterion j. Using this information
we deﬁne μ A
ð Þ = g ∑Ci ∈A αi


. We see in this case μ Hj


= g ∑i, Ci ∈Hj αi


and
hence
Using Fuzzy Measures to Construct Multi-criteria …
235

wj = g
∑
i ∈Hj
αi
 
!
−g
∑
i ∈Hj −1
αi
 
!
.
If we let id(j) denote the index of the jth most satisﬁed criteria then
μ Hj


= g = ∑k = 1 to j αidðkÞ


where αid(k) is the importance weight associated for
kth most satisﬁed criteria. Using this notation we see that
F x
ð Þ = ∑
n
j = 1
g
∑
j
k = 1
αidðkÞ


−g
∑
j −1
k = 1
αidðkÞ




CidðkÞðxÞ.
In the special case when g is linear, g(x) = x, then we see that F x
ð Þ =
∑n
k = 1 αidðkÞCidðkÞðxÞ = ∑n
i = 1 αiCiðxÞ. It is simply the importance weighted average.
6
Prioritized Multi-criteria Aggregation
An important type of relationship between criteria is illustrated by the following
example. Consider we are choosing a bicycle for a child and we have two criteria of
interest, safety and price. Assume the decision maker’s preference is that the safety
is of utmost importance. In particular, he is not willing to let high satisfaction to the
criteria of price compensate for poor satisfaction to the criteria of safety. Here we
say that safety has a priority over cost and denote this Safety > Cost.
In [10] we suggested a formulation for a fuzzy measure that can be used to
implement a priority relationship between the criteria. Assume C = {C1, ⋅⋅⋅, Cn}
are prioritized so that C1 > C2 > ⋅⋅⋅> Cn. As noted above our basic idea of
prioritization is that lack of satisfaction to higher priority criteria is not easily
compensated by satisfaction to lesser priority criteria. In the following we introduce
a measure to implement this type of imperative. We ﬁrst deﬁne Lj = {Ck | k = 1 to
j} for j = 1 to n and L0 = ∅. We now associate with each subset Lj a value
Vj = j/n. Using this we deﬁne the measure μ such that μ A
ð Þ = Maxj = 1 to n VjGjðA)

	
where Gj(A) = 1 if Lj ⊆A and Gj(A) = 0 if Lj ⊄A. We see that μ(A) = j/n where
Lj is the largest Lj that is contained in A. We easily see that μ(∅) = 0, μ(C) = 1 and
μ(A) ≥μ(B) if A ⊇B. Thus μ is a fuzzy measure.
Let us look at μ for some subsets of C. Consider the case of singleton sets
μ({Ck}). We see that μ({C1}) = 1/q while μ({Ck}) = 0 for k ≠1. Thus only the
singleton set consisting of C1, the highest priority element, has a non-zero measure.
In the case of subsets A consisting of two criteria:
μ fAg
ð
Þ = 2 ̸q if A ∩C1, C2
f
gÞ = C1, C2
f
g
μ fAg
ð
Þ = 1 ̸q if A ∩fC1, C2Þ = C1
f
g
236
R.R. Yager

μ A
ð Þ = 0 if A ∩fC1Þ = ∅.
Additionally we see
μ fC1, C2g
ð
Þ = 2 ̸q
μ C1, CkÞ
f
g = 1 ̸q for Ck ≠C2
μfCi, CkÞ = ∅
if neither i or k is 1
We observe that for any subset A such that C1 ∉A then Gj(A) = 0 for all j and
μ(A) = 0, for any other subset A, μ(A) is equal to Vj where Lj is the maximum Lk
contained in A.
We now shall investigate the use of the Choquet integral to obtain an
aggregation
function
using
this
prioritization
type
measure.
Here
F x
ð Þ =
Agg C1 x
ð Þ, . . . , Cn x
ð Þ
ð
Þ = ∑n
j = 1 ðμðHjÞ −μ Hj −1


ÞCid jð Þ x
ð Þ where Cid(j) is the jth
most satisﬁed criteria and Hj = {Cid(k) | k = 1 to j}, the collection of the j criteria
with
the
largest
satisfactions.
Letting
wj = μ(Hj) −μ(Hj−1)
we
have
F x
ð Þ = ∑n
j = 1 wjCid jð Þ x
ð Þ. We note that since μ is monotonic then F(x) is monotonic
in the Ci(x) and also we have Mini[Ci(x)] ≤F(x) ≤Maxi[Ci(x)].
Consider the case where the highest priority criteria, C1, is the least satisﬁed
criteria. Here we have that C1 ∉Hj for j = 1 to n −1 and only C1 ∉Hn. In this case
μ(Hj) = 0 for j = 1 to n −1 and μ(Hn) = 1. Here then Agg(C1(x),
⋅⋅⋅,
Cn(x)) = C1(1). Since C1(x) = Mini[Ci(x)]) then here we have Agg(C1(x), ⋅⋅⋅,
Cn(x) = Minj[Cj(x)]. It is the smallest value and there is no compensation by any
other criteria.
Consider now the more general case where C1(x) is the pth largest of the
satisfactions. Here C1 ∉Hj for j = 1 to p −1 and thus μ(Hj) = 0 for j = 1 to p −1.
From this we conclude F x
ð Þ = ∑n
j = 1 wjCid jð Þ x
ð Þ = ∑n
j = P wjCid jð Þ x
ð Þ. Furthermore
since for j = p + 1 to n we have that Cid(j)(x) ≤C1(x) combining this with the fact
that ∑n
j = 1 wj = 1 then we have F x
ð Þ = ∑n
j = 1 wiCid jð Þ x
ð Þ ≤∑n
j = P wjCid jð Þ x
ð Þ ≤C1 x
ð Þ.
We see that it is always the case in this priority aggregation that Agg(C1(x), ⋅⋅⋅,
Cn(x)) ≤C1(x).
Consider the case where ind(j) = j, the satisfactions are ordered the same as the
priority.
In
this
case
Hj = {C1,
..,
Cj} = Lj
and
therefore
wj = μ Hj


−
μ Hj −1


= μ Lj


−μ Lj −1


= 1n. Thus here we have F x
ð Þ = 1n ∑n
j = 1 CjðxÞ. It is
the average of all the criteria satisfactions.
A slightly more general formulation of this prioritized aggregation can be had.
Again assume C1 > C2 > ⋅⋅⋅> Cn and let Lj = {Ck | k = 1 to j} and L0 = ∅.
Here we associate with each Lj a value λj ≥0 such that λi ≥λk for i > k and
λn = 1. We now deﬁne our measure μ such that μ A
ð Þ = Maxjj = 1 to n [λjGj(A)] where
Using Fuzzy Measures to Construct Multi-criteria …
237

Gj A
ð Þ = 1
if Lj⊆A
Gj A
ð Þ = 0
if Lj6⊂A
So here we have a priority allowing different weights. In this case using the
Choquet integral we again get F(x) = ∑n
j = 1 ðμðHjÞ −μðHj −1ÞÞCidðjÞðxÞ with Hj =
{Cid(k)/k = 1 to j). An interesting special case is where λj = 0 for j = 1 to n −1
and λn = 1. In this case we get F(x) = Mini[Ci(x)].
We now brieﬂy consider a situation closely related to a prioritization of criteria.
Assume C1 and C2 are two criteria such that for C1 to be of any use we must satisfy
criteria C2. Here we say criterion C1 requires criterion C2. We can represent this
using a measure μ by specifying for that any subset A if A ∩{C2} = ∅then μ(A
∪{C1}) = μ(A). Consider now the case where C1(x) > C2(x). Here then if id
(j) = 1 then id(k) = 2 for k > j. Here we see that C2 ⊄Hj and C2 ⊄Hj−1 where C1
⊆Hj and Cj ⊄Hj−1. Furthermore the weight associated with C1, Cid(j),
wj = μ(Hj) −μ(Hj−1). Since {C2} ∩Hj -1 = ∅then μ(Hj) = μ(Hj−1 ∪{C1}) =
(Hj−1) and hence wj = 0. Thus we see for the case C1(x) > C2(x) the weight
associated with C1 is zero, it makes no contribution.
7
Multi-criteria Aggregation Based on Quasi-additive
Measures
Another class of measures useful for modeling multi-criteria decision function are
the “quasi-additive” measures. Here we associate with the space C of criteria a
collection S1, .., Sr of subsets. We note that these subsets need not be disjoint or that
their union covers C We further associate with each Sj a value αj ∈[0, 1] such
that ∑r
j = 1 αj = 1. Using this we deﬁne the measure μS on C such that
μS A
ð Þ = ∑r
j = 1 Rj A
ð Þαj where
Rj A
ð Þ = 1 if A ∩Sj ≠∅and Rj A
ð Þ = 0 if A ∩Sj = ∅
Here we are giving an importance weight of αj to getting satisfaction to any
criteria in Sj. This called a plausibility measure.
Let us consider the aggregation of criteria satisfactions under μS using the
Choquet integral. Again we have
F x
ð Þ = ∑r
j = 1 ðμSðHjÞ −μS Hj −1


ÞCid jð Þ x
ð Þ = ∑r
j = 1 wjCid jð Þ x
ð Þ,
Consider now the special case where Sj = {Cj} for j = 1 to n. Here with
Hj = {Cid(k)/k = 1 to j} we have μS Hj


= ∑j
k = 1 αidðjÞ and with Hj −1 = {Cid(k)/ k =
1 to j −1} we have μS Hj −1


= ∑j −1
k = 1 αidðjÞ. Here wj = α
id(j) and we get
238
R.R. Yager

F(x) = ∑n
j = 1 αidðjÞCidðjÞðxÞ = ∑n
i = 1 αiCi x
ð Þ. Thus in this case we get the simple
importance weighted criteria satisfaction as a special case.
Another special case is where r = 1 and α1 = 1. Here we just have one subset
S1. Here we see μS(Hj) = 1 if S1 ∩Hj ≠∅and μS(Hj) = 0 if S1 ∩Hj = ∅. We
see that in this case μS(Hj) = 1 the ﬁrst time we get an element from S1 in Hj. Thus
here F x
ð Þ = Max
j ∈S1 Cj x
ð Þ

	
. Thus it is the value of the maximally satisﬁed criteria
in S1.
Another special case is where S1 = C and S2 some arbitrary subset. Here we can
show that
F x
ð Þ = α1Maxi Ci x
ð Þ
½
 + ð1 −α1 Max
j ∈S2 Ci x
ð Þ
½

A related measure can be obtained if we deﬁne Rj(A) = 1 if Sj ⊆A and
Rj(A) = 0 if Sj ⊄A and we deﬁne μS such that μS A
ð Þ = ∑r
j = 1 RjðA)αj. Here we are
giving an importance weigh αj for satisfying all the criteria in Sj.
We can show in this case that if Sj = {Cj} for j = 1 to n then this also reduces
F x
ð Þ = ∑n
j = 1 αiCiðx). We also can show that in the case where α1 = 1 then
F x
ð Þ = MinCj ∈S1 Cj x
ð Þ.
References
1. Bellman, R.E., Zadeh, L.A.: Decision-making in a fuzzy environment. Manag. Sci. 17(4),
141–164 (1970)
2. Mesiar, R., Mesiarová, A.: Fuzzy integrals—what are they ? Int. J. Intell. Syst. 23, 199–212
(2008)
3. Klement, E.P., Mesiar, R., Pap, E.: A universal integral as common frame for Choquet and
Sugeno. IEEE Trans. Fuzzy Syst. 18, 178–187 (2010)
4. Grabisch, M., Murofushi, T., Sugeno, M.: Fuzzy Measures and Integrals. Physica-Verlag,
Heidelberg (2000)
5. Beliakov, G., Pradera, A., Calvo, T.: Aggregation Functions: A Guide for Practitioners.
Springer, Heidelberg (2007)
6. Yager, R.R.: A measure based approach to the fusion of possibilistic and probabilistic
uncertainty. Fuzzy Optim. Decis. Making 10, 91–113 (2011)
7. Yager, R.R.: On the cardinality index and attitudinal character of fuzzy measures. Int. J. Gen.
Syst. 31, 303–329 (2002)
8. Yager, R.R.: On ordered weighted averaging aggregation operators in multi-criteria decision
making. IEEE Trans. Syst. Man Cybern. 18, 183–190 (1988)
9. Yager, R.R.: Quantiﬁer guided aggregation using OWA operators. Int. J. Intell. Syst. 11,
49–73 (1996)
10. Yager, R.R.: On prioritized multiple criteria aggregation. IEEE Trans. Syst. Man Cybern.
B 42, 1297–1305 (2012)
Using Fuzzy Measures to Construct Multi-criteria …
239

A Modal Account of Preference
in a Fuzzy Setting
Francesc Esteva, Lluís Godo and Amanda Vidal
Abstract In this paper we ﬁrst consider the problem of extending fuzzy (weak and
strict) preference relations, represented by fuzzy preorders on a set to a fuzzy pref-
erences on subsets, and we characterise diﬀerent possibilities. Based on their prop-
erties, we then semantically deﬁne and axiomatize several two-tiered graded modal
logics to reason about the corresponding diﬀerent notions of fuzzy preferences.
1
Introduction
Reasoning about preferences is a hot topic in Artiﬁcial Intelligence since many years,
see for instance [5, 17, 18]. Two main approaches for representing and handling
preferences have been developed: the relational and the logic-based approaches.
This paper is our humble contribution to the tribute, in the occasion of his 65th birthday, to José
Luis “Curro” Verdegay. Excellent researcher and better person, he has been one of the pioneers
of fuzzy logic in Spain and founder and driving force of the research group on Decision Making
and Optimization at the University of Granada. Our contribution is devoted to logic and fuzzy
preferences, a topic that, although it is not central on the research of Curro, is ubiquitous in
fuzzy decision making models and we hope it may be of his interest. Along many years, we
have jointly participated in many events around the world with Curro and with our friends from
Granada, we have learnt a lot from his research ideas and organizational competences, but more
importantly, we have enjoyed his friendship and shared many unforgettable moments. Thanks
for all Curro, and congratulations for this well-deserved homage!
F. Esteva ⋅L. Godo (✉)
IIIA - CSIC, 08193 Bellaterra, Spain
e-mail: godo@iiia.csic.es
F. Esteva
e-mail: esteva@iiia.csic.es
A. Vidal
ICS, Czech Academy of Sciences, Prague, Czech Republic
e-mail: amanda@cs.cas.cz
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_15
241

242
F. Esteva et al.
In classical preference relations, every preorder R (and more in general every
reﬂexive relation) can be regarded as a preference relation by assuming that (a, b) ∈
R means that a is preferred or indiﬀerent to b. From R we can deﬁne three disjoint
relations:
∙the strict preference P = R ∩Rd,
∙the indiﬀerence relation I = R ∩Rt, and
∙the incomparability relation J = Rc ∩Rd.
where Rd = {(a, b) ∈R ∶(b, a) ∉R}, Rc = {(a, b) ∈R ∶(b, a) ∈R} and Rt =
{(a, b) ∶(b, a) ∈R}. It is clear that P is a strict order (irreﬂexive, antisymmetric
and transitive), I is an equivalence relation (reﬂexive, symmetric and transitive) and
J is irreﬂexive and symmetric. The triple (P, I, J) is called a preference structure,
where the initial weak preference relation can be recovered as R = P ∪I.
In the fuzzy setting, preference relations can be attached degrees (usually belong-
ing to the unit interval [0, 1]) of fulﬁlment or strength, so they become fuzzy rela-
tions. A weak fuzzy preference relation on a set X will be now a fuzzy preorder
R ∶X × X →[0, 1], where R(a, b) is interpreted as the degree in which b is at least as
preferred as a. Given a t-norm ⊙, a fuzzy ⊙-preorder satisﬁes reﬂexivity (R(a, a) = 1
for each a ∈X) and ⊙-transitivity (R(a, b) ⊙R(b, c) ≤R(a, c) for each a, b, c ∈X).
The most inﬂuential reference is the book by Fodor and Roubens [6], that was fol-
lowed by many other works like, for example [7–11]. The problem in this setting is
how to deﬁne the corresponding strict preference, indiﬀerence and incomparability
relations from the initial fuzzy preorder. Many questions arise since it is possible to
generalise the classical case in many diﬀerent ways. In particular, several works have
paid attention to how suitably interrelate a weak preference (a fuzzy preorder) with
its associated indiﬀerence relation (a indistinguishability relation) and strict pref-
erence (a strict fuzzy order). In this sense, relevant publications are, among others,
Bodenhofer’s papers [2–4]. There, the author studies ⊙-E fuzzy preorders related to a
t-norm ⊙and an indistinguishability, or fuzzy equivalence, relation E (reﬂexive, sym-
metric and ⊙-transitive), as well as their strict associated fuzzy orders in a general
context, which is also applies to the context of preference modelling. Indeed, given
a t-norm ⊙and an indistinguishability relation E, a ⊙-E fuzzy preorder is deﬁned as
a fuzzy relation R ∶X × X →[0, 1] satisfying: E-reﬂexivity: R(x, y) ≥E(x, y), ⊙-E-
antisymmetry: R(x, y) ⊙R(y, x) ≤E(x, y), ⊙-transitivity: R(x, y) ⊙R(y, z)) ≤R(x, z).
Bodenhofer also studies how to extend such a ⊙-E fuzzy preorder to the set ()
of fuzzy subsets of a universe X, as well as the associated indistinguishability rela-
tion and the strict fuzzy order, and discusses diﬀerent possible deﬁnitions. In such
a setting, he considers both the cases of crisp and fuzzy preorders, but he does not
consider the particular case we will study in this paper, namely the interaction of a
fuzzy preferences over crisp subsets of X.
The basic assumption in logical approaches is that preferences have structural
properties that can be suitably described in a fomalized language.This is the main
goal of the so-called preference logics, see e.g. [17]. The ﬁrst logical systems to rea-
son about preferences go back to Halldén [20] and to von Wright [16, 22, 23]. More
recently van Benthem et al. in [1] have presented a modal logic-based formalization

A Modal Account of Preference in a Fuzzy Setting
243
of preferences. In that paper the authors ﬁrst deﬁne a basic modal logic with two
unary modal operators ◊≤and ◊<, together with the universal and existential modal-
ities, A and E respectively, and axiomatize it. Using these primitive modalities, they
consider several (deﬁnable) binary modalities to capture diﬀerent notions of prefer-
ence relations on classical propositions, and show completeness with respect to the
intended preference semantics. Finally they discuss their systems in relation with
von Wright axioms for ceteris paribus preferences [22]. On the other hand, with the
motivation of formalising a comparative notion of likelihood, Halpern studies in [15]
diﬀerent ways to extend preorders on a set X to preorders on subsets of X and their
associated strict orders. He studies their properties and relations among them, and he
also provides an axiomatic system for a logic of relative likelihood, that is proved to
be complete with respect to what he calls preferential structures, i.e. Kripke models
with preorders as accessibility relations.
In this paper we begin by studying in Sect. 2 diﬀerent forms to deﬁne fuzzy rela-
tions on the set (W) of subsets of W, from a fuzzy preorder on W, in a similar way
to the one followed in [1, 15] for classical preorders, and in [2, 3] for fuzzy preorders.
In Sect. 3 we characterize them and discuss which are the most appropriate from the
point of view of preference modelling, while in Sect. 4 we deal with the problem of
deﬁning a fuzzy strict order in a set associated to a given fuzzy preorder, and how
to lift them to susbsets. Finally, in Sect. 5, and based on the previous results, we
semantically deﬁne and axiomatize several two-tiered graded modal logics to reason
about diﬀerent notions of preferences.
This paper is a proper extended version of the conference paper [13].
2
Extending a Fuzzy Preorder on a Set 𝐖to a Fuzzy
Relation on Subsets of 𝐖
2.1
Precedents in the Classical Case
In the classical logic setting, van Benthem et al. deﬁne in [1] preference models
as triples = (W, ⪯, ) where W is a set of states or worlds, ⪯is a preorder
(reﬂexive and transitive) relation on W, and is a standard propositional evaluation,
that is, a mapping assigning to every propositional variable p a subset (p) ⊆W
of states where p is true. can be extended to any propositional formula 𝜑by
using the classical Boolean deﬁnitions. For simplicity, we will also denote (𝜑) by
[𝜑] = {w ∈W ∶w(𝜑) = 1}.
Then they consider the following four binary preference operators on proposi-
tions.
Deﬁnition 1 (cf. [1]) Given a preference model = (W, ⪯, ), one can deﬁne the
following four binary preference operators on classical propositions:
∙⊧𝜑≤∃∃𝜓iﬀthere exist u ∈[𝜑], v ∈[𝜓] such that u ≤v.

244
F. Esteva et al.
∙⊧𝜑≤∃∀𝜓iﬀthere exists u ∈[𝜑], such that for all v ∈[𝜓], u ≤v.
∙⊧𝜑≤∀∃𝜓iﬀfor all u ∈[𝜑], there exists v ∈[𝜓] such that u ≤v.
∙⊧𝜑≤∀∀𝜓iﬀfor all u ∈[𝜑] and v ∈[𝜓], then u ≤v.
Notice that these deﬁnitions of the truth conditions for the four preference opera-
tors can be interpreted as deﬁning corresponding preference relations on (W), the
power set of W (which contains the sets [𝜑]) arising from a preorder on the set of
worlds W. One can furthermore deﬁne two more preference operators on proposi-
tions:
∙⊧𝜑≤∃∀2 𝜓iﬀthere exists v ∈[𝜓], such that for all u ∈[𝜑], u ≤v
∙⊧𝜑≤∀∃2 𝜓iﬀfor all v ∈[𝜓], there exists u ∈[𝜑] such that u ≤v.
Therefore, from a given preorder on W we can consider six relations on subsets
of W. The basic set-inclusions between these relations are given in the following
proposition.
Proposition 1 The following inclusions hold:
≤∀∀⊆≤∀∃⊆≤∃∃, ≤∀∀⊆≤∃∀⊆≤∃∃, ≤∀∀⊆≤∀∃2 ⊆≤∃∃, ≤∀∀⊆≤∃∀2 ⊆≤∃∃
Moreover, the four intermediate relations are not comparable, except for the follow-
ing inclusions:
≤∃∀2 ⊆≤∀∃,
≤∃∀⊆≤∀∃2 .
Proof All the inclusion relations are easy to check. Moreover, the inclusions given
in Proposition 1 are the only ones that are valid among the four intermediate rela-
tions, as the following examples show: Take W = {u1, u2, u3, u4, u5, u6} and A =
{u1, u2, u3}, B = {u4, u5, u6}. Then,
∙If the preorder is deﬁned by reﬂexivity plus u1 ≤u4, u2 ≤u5 and u3 ≤u5, then
A ≤∀∃B is the unique intermediate relation that is satisﬁed.
∙If the preorder is deﬁned by reﬂexivity plus u1 ≤u4, u2 ≤u5 and u2 ≤u6, then
A ≤∀∃2 B is the unique intermediate relation that is satisﬁed.
∙If the preorder is deﬁned by reﬂexivity plus u2 ≤u4, u2 ≤u5 and u2 ≤u6, then
A ≤∃∀B and A ≤∀∃2 B are the unique intermediate relations that are satisﬁed.
∙If the preorder is deﬁned by reﬂexivity plus u1 ≤u4, u2 ≤u4 and u3 ≤u4, then
A ≤∃∀2 B and A ≤∀∃B are the unique intermediate relations that are satisﬁed.
2.2
The Fuzzy Preorder Case
Now we study the case when ≤is a fuzzy ⊙-preorder on W, i.e., ≤∶W × W ⟶
[0, 1] satisfying reﬂexivity ([u ≤u] = 1 for all u ∈W) and ⊙-transitivity with respect
to a given t-norm ⊙(for all u, v, w ∈W, ([u ≤v] ⊙[v ≤w]) ≤[u ≤w]), where
[u ≤v] denotes the value in [0, 1] of the fuzzy relation ≤applied to the ordered

A Modal Account of Preference in a Fuzzy Setting
245
pair of elements u, v ∈W. We will assume that W is a ﬁnite set, and we will denote
by 𝛿u the singleton {u}.
Generalising the classical case, we can deﬁne the following fuzzy relations on
(W) from a fuzzy preorder on W.
Deﬁnition 2 Given a fuzzy preorder ≤on W, we can deﬁne the following six fuzzy
relations on (W). For any A, B ∈(W) we let:
∙[A ≤∃∃B] = supu∈A supv∈B [u ≤v]
∙[A ≤∃∀B] = supu∈A infv∈B [u ≤v]
∙[A ≤∀∃B] = infu∈A supv∈B [u ≤v]
∙[A ≤∀∀B] = infu∈A infv∈B [u ≤v]
∙[A ≤∀∃2 B] = infv∈B supu∈A [u ≤v]
∙[A ≤∃∀2 B] = supv∈B infu∈A [u ≤v].
where the value of A ≤◦B is denoted by [A ≤◦B] with ≤◦being anyone of the six
relations.
It is clear that, since the preorder ≤is valued on [0, 1], these relations are also
[0, 1]-valued. For each a ∈(0, 1], we will write A ≤a
∃∃B when [A ≤∃∃B] ≥a and
analogously for the other relations.
Proposition 2 For any sets A, B ∈(W), we have:
∙[A ≤∀∀B] ≤[A ≤∀∃B] ≤[A ≤∃∃B],
∙[A ≤∀∀B] ≤[A ≤∀∃2 B] ≤[A ≤∃∃B],
∙[A ≤∀∀B] ≤[A ≤∃∀B] ≤[A ≤∃∃B], and
∙[A ≤∀∀B] ≤[A ≤∃∀2 B] ≤[A ≤∃∃B].
Moreover the four intermediate relations are not comparable, except for the same
two cases (now inequalities) of Proposition 1.
Proof Analogous to the proof of Proposition 1.
Out of the above six possibilities, we will mainly focus on two of them, ≤∀∃and
≤∀∃2, in the rest of the paper. These are well-behaved extensions of an initial fuzzy
⊙-preorder to model a weak preference relation on subsets, since in particular they
keep being ⊙-preorders. Moreover, combining them, we can capture a very natural
(preference) ordering related to orderings of intervals. Indeed, suppose (W, ≤) is a
totally (classical) pre-ordered set, and we want to extend ≤to an ordering on the set
Int(W) of intervals of W. The two most usual ways to do this are the following:
(i) [a, b] ≤1 [c, d] if a ≤c and b ≤d,
(ii) [a, b] ≤2 [c, d] if b ≤c.
The relation ≤1 is considered for example in [2], and it turns out to be deﬁnable as
the intersection of the ≤∀∃and ≤∀∃2 relations on Int(W), that is, ≤1 = ≤∀∃∩≤∀∃2,
while the second, ≤2, coincides with the (crisp) relation ≤∀∀on Int(A). Actually, ≤∀∀
is not a preorder because it is only reﬂexive for singletons, but it is enough for our
purposes. In next sections, we will study in the fuzzy case these three basic relations
(≤∀∃, ≤∀∃2, ≤∀,∀) on (W) arising from a fuzzy preorder ≤on W.

246
F. Esteva et al.
3
Characterizing the Relations ≤∀∃, ≤∀∃𝟐and ≤∀∀
The following propositions describe the main properties satisﬁed by each one of
these relations. In what follows, we assume a given a fuzzy ⊙-preorder ≤on W and
the fuzzy relations ≤∀∃, ≤∀∃2 and ≤∀∀which are deﬁned as in Deﬁnition 2.
Proposition 3 The relation ≤∀∃satisﬁes the following properties, for all A, B, C ∈
(W):
1. Inclusion: [A ≤∀∃B] = 1, if A ⊆B
2. ⊙-Transitivity: [A ≤∀∃B] ⊙[B ≤∀∃C] ≤[A ≤∀∃C]
3. Left-OR: [(A ∪B) ≤∀∃C] = min([A ≤∀∃C], [B ≤∀∃C])
4. Restricted Right-OR:
[A ≤∀∃(B ∪C)]≥max([A ≤∀∃B], [A ≤∀∃C]).
The
inequality becomes an equality if A is a singleton.
Proposition 4 The relation ≤∀∃2 satisﬁes the following properties, for all A, B, C ∈
(W):
1. Inclusion: [A ≤∀∃2 B] = 1, if B ⊆A
2. ⊙-Transitivity: [A ≤∀∃2 B] ⊙[B ≤∀∃2 C] ≤[A ≤∀∃2 C]
3. Restricted Left-OR: [(A ∪B) ≤∀∃2 C] ≥max([A ≤∀∃2 C], [B ≤∀∃2 C]). The
inequality becomes an equality if C is a singleton.
4. Right-OR: [A ≤∀∃2 (B ∪C)] = min([A ≤∀∃2 B], [A ≤∀∃2 C]).
Proposition 5 The relation ≤∀∀satisﬁes the following properties, for all A, B, C ∈
(W):
1. Restricted reﬂexivity: [A ≤∀∀A] = 1 iﬀA is a singleton
2. ⊙-Transitivity: [A ≤∀∀B] ⊙[B ≤∀∀C] ≤[A ≤∀∀C]
3. Left-OR: [(A ∪B) ≤∀∀C] = min([A ≤∀∀C], [B ≤∀∀C])
4. Right-OR: [A ≤∀∀(B ∪C)] = min([A ≤∀∀B], [A ≤∀∀C])
5. Inclusions: [A ≤∀∀B] ≤[A′ ≤∀∀B′], if A′ ⊆A, B′ ⊆B.
The proofs of the these propositions are easy and we omit them. Observe that, as
already mentioned above, ≤∀∀is not reﬂexive.
Actually, the properties given above fully characterize the diﬀerent relations on
(W) as showed in the next theorem.
Theorem 1 The following characterizations hold:
(i) Let ⪯AE be a relation between sets of (W) satisfying Properties 1, 2, 3 and 4
of Proposition 3. Then there exists a fuzzy ⊙-preorder ≤on the set W such that
⪯AE coincides with ≤∀∃as deﬁned in Deﬁnition 2.
(ii) Let ⪯AE2 be a relation between sets of (W) satisfying Properties 1, 2, 3 and 4
of Proposition 4. Then there exists a fuzzy ⊙-preorder ≤on the set W such that
⪯AE2 coincides with ≤∃∀2 as deﬁned in Deﬁnition 2.
(iii) Let ⪯AA be a relation between sets of (W) satisfying Properties 1, 2, 3, 4 and
5 of Proposition 5. Then there exists a fuzzy ⊙-preorder ≤on the set W such
that ⪯AA coincides with ≤∀∀as deﬁned in Deﬁnition 2.

A Modal Account of Preference in a Fuzzy Setting
247
Proof We show the case of ≤AE, the rest of cases are proved in a similar way. Deﬁne a
relation on W by [u ≤v] = [𝛿u ⪯AE 𝛿v]. Clearly ≤is a fuzzy preorder on W. Now take
into account that, for all A ∈(W), A = ⋃{𝛿u ∶u ∈A} and, applying Properties 3
and 4, it is obvious that for all A, B ∈W, then [A ⪯AE B] = infu∈A supv∈B[𝛿u ⪯𝛿v].
Thus (i) is proved.
□
4
Characterizing Strict Fuzzy Orders Associated to Fuzzy
Preorders
It is well known that any crisp preorder ≤on an universe W induces an equivalence
(or indiﬀerence) relation ≡and an strict order <, deﬁned as follows:
∙x ≡y iﬀx ≤y and y ≤x,
∙x < y iﬀx ≤y and x ≠y or, alternatively iﬀx ≤y and y ≰x.
Observe that these relations satisfy that x ≤y iﬀeither x ≡y or x < y. We will use
this condition to deﬁne an strict fuzzy order associated to a fuzzy preorder.
In the fuzzy setting (see for example [2, 14]), from a fuzzy ⊙-preorder ≤∶W ×
W →[0, 1] we can deﬁne:
∙the maximal indistinguishability relation v ≡w contained in the fuzzy preorder,
deﬁned by [x ≡y] = [x ≤y] ∧[y ≤x];
∙the minimal strict fuzzy ⊙-order < that satisﬁes the following equation
[x ≤y] = [x < y] ⊕[x ≡y]
(1)
where ⊕is a T-conorm (for example the maximum or the bounded sum).
So deﬁned, the relation ≡is reﬂexive, symmetric and ⊙-transitive, and thus it is a
⊙-indistinguishability relation (the generalization of the crisp notion of equivalence
relation). On the other hand, the minimal solution for b of the equation a ≤b ⊕c
in [0, 1], is the so-called dual resituated implication, or implication associated to the
T-conorm ⊕, which is deﬁned as, It should be:
c →⊕a = inf{b ∣c ⊕b ≥a}.
Therefore, we take as the strict fuzzy order relation < associated to ≤for the T-
conorm ⊕the fuzzy relation deﬁned as
[x < y] = [x ≡y] →⊕[x ≤y] = [y ≤x] →⊕[x ≤y].
An easy computation shows that the strict fuzzy order relation for ⊕= max is
deﬁned as
[x < y] =
{
[x ≤y], if [x ≤y] > [y ≤x],
0,
otherwise.
(2)

248
F. Esteva et al.
And for ⊕being the bounded sum (i.e. the Łukasiewicz T-conorm) is1
[x < y] =
{
[x ≤y] −[y ≤x], if [x ≤y] > [y ≤x],
0,
otherwise.
The strict relation associated to ≤is a irreﬂexive ([x < x] = 0) and antisymmetric
(min([x < y], [y < x]) = 0) but, as far as we know, it is not known whether it is ⊙-
transitive in general. Nevertheless we have the following result.
Proposition 6 Let ≤be a min-preorder on a universe W and let < be the associated
strict relation w.r.t. ⊕= max. Then < is min-transitive.
Proof The proof is by contradiction. Suppose the strict relation is not min-transitive.
Then there must exist elements u, v, w ∈W such that [u < v], [v, w] > 0 and [u <
w] = 0 which is equivalent that [u ≤v] = a > b = [v ≤u], [v ≤w] = c > d = [w ≤
v] and [u ≤w] = [w ≤u] = f. Thus we have ﬁve values a, b, c, d, f and we know that
a > b and c > d.
(∗)
We can now reason by cases:
(1) Suppose a ≥c and b ≥d. Combining this assumption with (∗) we have that
a ≥c > d. By transitivity, f ≥min(a, c) = c and f ≥min(d, b) = d by hypoth-
esis. Moreover min([w ≤u], [u ≤v]) = min(f, a) ≤d = [w ≤v]. This implies
that a ≤d, in contradiction with the fact that d < a.
(2) Suppose a ≥c and b < d. Combining this assumption with (∗) we have that
d < c ≤a. By transitivity, f ≥min(a, c) = c and f ≥min(d, b) = b by hypoth-
esis. Moreover min([w ≤u], [u ≤v]) = min(f, a) ≤d = [w ≤v]. This implies
that f ≤d, and by hypothesis f ≤d < c, in contradiction with f ≥c previously
proved.
(3) Suppose a ≤c and b ≥d. Combining this assumption with (∗) we have that b <
a ≤c. By transitivity, f ≥min(a, c) = a and f ≥min(d, b) = d by hypothesis.
Moreover min([v ≤w], [w ≤u]) = min(c, f) ≤b = [v ≤u]. This imply that f ≤
b and by hypothesis f ≤b < a, in contradiction with f ≥a previously proved.
(4) Suppose a ≤c and b ≤d. Combining this assumption with (∗) we have that
b < a ≤c. By transitivity, f ≥min(a, c) = a and f ≥min(d, b) = b by hypoth-
esis. Moreover min([v ≤w], [w ≤u]) = min(c, f) ≤b = [v ≤u]. This implies
that f ≤b, and by hypothesis f ≤b < a, in contradiction with f ≥a previously
proved.
□
From now on, we consider the strict fuzzy order < associated to ≤the one deﬁned
by taking ⊕= max according to (2).
Now we can come back to the topic of how to deﬁne a strict fuzzy order relation
on sets of (W) corresponding to a fuzzy preorder in W. Halpern notices in [15] that
1This is the strict order companion deﬁned and studied in [7].

A Modal Account of Preference in a Fuzzy Setting
249
there are two diﬀerent ways to deﬁne (in the crisp case) a strict relation on (W) from
a preorder on W. The same idea applied to the fuzzy case gives rise to the following
two possible deﬁnitions for the strict relations:
∙The standard method, that amounts to deﬁne
[A <◦B] =
{
[A ≤◦B],
if [A ≤◦B] > [B ≤◦A]
0,
otherwise.
This means in fact to use (2) to deﬁne [A <◦B] as the value of the strict order
associated to the preorder ≤◦, where ≤◦is either ≤∀∃, ≤∀∃2 or ≤∀∀.
∙The alternative method, that ﬁrst considers the strict order < on companion of ≤
in W according to (2), and then deﬁnes <∀∃, <∀∃2 and <∀∀on (W) according to
Deﬁnition 2, but replacing ≤by <.
In general, these two methods give rise to two diﬀerent irreﬂexive and (restricted)
antisymmetric strict relations as the following examples show:
Example 1 Consider the ∀∃extension. Notice ﬁrst that the alternative method gives
[A <∀∃B] = inf
u∈A sup
v∈B
[u < v].
The counterexample is the following. Take the four element set W = {u1, u2, u3, u4},
with the following fuzzy preorder: reﬂexivity ([x ≤x] = 1) plus [u1 ≤u3] = [u3 ≤
u1] = a and [u2 ≤u4] = b, with a, b ≠0. The associated strict relation on W is the
one having only one pair of elements with value diﬀerent from 0. Indeed an easy
computation shows that [u2 < u4] = b. Let A = {u1, u2} and let B = {u3, u4}. Then:
∙To compute the value of [A <∀∃B] according to the standard method, we have
to compute ﬁrst: [A ≤∀∃B] = ([u1 ≤u3] ∨[u1 ≤u4]) ∧([u2 ≤u3] ∨[u2 ≤u4]) =
a ∧b ≠0,
[B ≤∀∃A] = ([u3 ≤u1] ∨[u3 ≤u2]) ∧([u4 ≤u1] ∨[u4 ≤u2]) = a ∧
0 = 0. Then, by deﬁnition, we have [A <∀∃B] = a ∧b ≠0.
∙With the alternative method, the value of [A <∀∃B] is computed as [A <∀∃B] =
infu∈A supv∈B[u < v] = 0.
Therefore the obtained strict relations are diﬀerent.
□
Example 2 Consider now the ∀∀extension. Take W = {w1, w2} with the preorder
[w1 ≤w1] = [w2 ≤w2] = [w1 ≤w2] = 1 and [w2 ≤w1] = 0. Further, take A = {w1}
and B = W. Then it is obvious that [A ≤∀∀B] = 1 and [B ≤∀∀A] = 0. Therefore,
according to the standard method, we have [A <∀∀B] = 1, while according to the
alternative method, we have [A <∀∀B] = infu∈A infv∈B[u < v] = 0.
□
Notice that strict relations obtained by the alternative method are ⊙-transitive (so
they are strict orders), but this is not clear for strict relations obtained by the standard
method. In fact we have the following open problems:

250
F. Esteva et al.
∙Let ≤be a strictly monotonic fuzzy preorder on W and let ≤◦be one of the fuzzy
preorders deﬁned on (W) considered in the previous sections. Is the strict relation
obtained by the standard method ⊙-transitive?
∙Is there some order relation between the strict orders obtained by the standard and
the alternative methods?
∙It is obvious that the strict order < on W and the strict order on (W) obtained
from the preorder by the standard method satisfy the following anti-symmetry
property: for all A, B ∈(W), min([A <◦B], [B <◦A]) = 0). It is clear that for
singletons the strict order obtained by the alternative method satisﬁes the same
anti-symmetry property but, is this true for the strict order obtained by the alter-
native method in general? Otherwise, what type of anti-symmetry property does
it satisfy?
Therefore, taking into account that we are interested in obtaining strict fuzzy orders
(irreﬂexive and ⊙-transitive relations), in the rest of the paper we will consider the
strict relations obtained by the alternative method and its characteristics properties.
Next theorem provides a characterization result for these strict orders.
Theorem 2 The following characterizations hold:
(i) Let ≺AE be a relation between sets of (W) satisfying Properties 2, 3 and 4 of
Proposition 3 plus irreﬂexivity ([A ≺AE A] = 0) and restricted anti-symmetry
(min([A ≺AE B], [B ≺AE A]) = 0 for all singletons A, B ∈(W)). Then there
exists a fuzzy ⊙-preorder ≤on the set W such that ≺AE =<∀∃.
(ii) Let ≺AE2 be a relation between sets of (W) satisfying Properties 2, 3 and 4 of
Proposition 4 plus irreﬂexivity and restricted anti-symmetry. Then there exists
a fuzzy ⊙-preorder ≤on the set W such that ≺AE2 =<∃∀2.
(iii) Let ≺AA be a relations between sets of (W) satisfying Properties 2, 3, 4 and
5 of Proposition 5 plus irreﬂexivity and anti-symmetry. Then there exists a
fuzzy ⊙-preorder ≤on the set W such that ≺AA =<∀∀.
At the end of Sect. 2.2 we mentioned that one of the preorders we were inter-
ested in was the (crisp) relation ≤1, whose fuzzy counterpart can be deﬁned by
[x ≤1 y] = min([x ≤∀∃y], [x ≤∀∃2 y]). Consequently, in Sect. 3 we separately char-
acterized the fuzzy preorders ≤∀∃and ≤∀∃2, and the same is applicable to the corre-
sponding strict orders studied in this section. We will move now to a logical approach
to preference relations and to the previously studied fuzzy relations. In particular, in
the next section we study a logical setting to reason about fuzzy preferences on clas-
sical propositions by means of several two-tiered modal logics, with binary modal
operators corresponding to fuzzy preorders and strict orders separately, and after we
show the desired preorder and strict order are deﬁnable in a yet another modal logic
combining the previous ones.

A Modal Account of Preference in a Fuzzy Setting
251
5
A Modal Logic to Reason About Preferences
In this section three logics to reason about conditional (syntactic) objects captur-
ing the idea of the preference relations ≤◦(for ◦∈{∀∃, ∀∃2, ∀∀}) are deﬁned and
studied, using similar techniques from [12].
Throughout this section, in order to simplify matters, rather than deﬁning the logic
relative to preference degrees in [0, 1] and a t-norm, we will restrict ourselves to deal
with a totally ordered ﬁnite set V of preference degrees (with 1 and 0 as its top and
bottom elements), and we will ﬁx a ﬁnite t-norm ⊙(see e.g. [19]) on V.
On these grounds, we deﬁne, model-theoretically, a common framework for sev-
eral logics of preference relations, 𝖫𝖠𝖯for short, as follows.
Deﬁnition 3 The language of 𝖫𝖠𝖯is two levelled:
∙The ﬁrst level (0 language) contains propositional formulas of 𝖫𝖠𝖯that are built
up from a ﬁnite set of variables Var = {p1, … , pN} and the constants ⊥, ⊤by
means of the binary operators ∧and ∨and the unary operator ¬. The set of propo-
sitional formulas is denoted by .
∙The second level (1 language) contains:
– Atomic graded preference formulas of 𝖫𝖠𝖯that are triples
𝜑⪯a 𝜓
consisting of two propositional formulas 𝜑and 𝜓from 0, and a value a ∈
V ⧵{0}.
– (General) preference formulas of 𝖫𝖠𝖯are built up from atomic graded prefer-
ences and the constants ⊥, ⊤by means of the binary connectives ∧and ∨and
the unary connective ¬.
The semantics is given by ⊙-preference Kripke models = (W, ≤, e) where W
is a ﬁnite set of worlds, ≤∶W × W →V is a ⊙-fuzzy preorder relation, and e ∶
W × Var ↦{0, 1} is a Boolean evaluation of propositional variables in every world,
which is extended to propositions of 0 in the usual way for classical propositions.
For each 0-proposition 𝜑, we will denote by [𝜑]the set {w ∈W ∶e(w, 𝜑) = 1}
of worlds satisfying 𝜑.
For each ◦∈{∀∃, ∀∃2, ∀∀}, each Kripke model = (W, S, e) induces a Boolean
truth-evaluation of 1-formulas e◦
∶1 →{0, 1} deﬁned as follows:
∙for atomic preference formulas: e◦
(𝜑⪯a 𝜓) = 1 if [[𝜑]≤◦[𝜓]] ≥a, and
e(𝜑⪯a 𝜓) = 0 otherwise.
∙for compound formulas, use the usual Boolean truth functions.
From there, we can deﬁne the notion of logical consequence in the logic LAP for
preference formulas.
Deﬁnition 4 Let ◦∈{∀∃, ∀∃2, ∀∀}. Let T ∪{𝛷} be a set of preference formulas.
We say that 𝛷logically follows from T under the ≤◦semantics, written T ⊧◦
LAP

252
F. Esteva et al.
𝛷, if for every Kripke model = (W, ≤, e), if e◦
(𝛹) = 1 for every 𝛹∈T, then
e◦
(𝛷) = 1 as well.
In the following, for every Boolean evaluation 𝜔of the propositional variables Var,
we will denote by 𝜔the maximally elementary conjunction (m.e.c. for short) of all
the N literals made true by 𝜔. Obviously, every proposition is logically equivalent to
a disjunction of m.e.c.’s.
Next subsections are devoted to the axiomatization of the particular logics for
≤∀∃, ≤∀∃2 and ≤∀∀.
5.1
The Logic LAP∀∃Corresponding to the ≤∀∃Preference
Relation
Recall that, when ◦= ∀∃, the semantics we have in each Kripke model is:
e(𝜑⪯a 𝜓) = 1 iﬀ[[𝜑]≤∀∃[𝜓]] = ( inf
u∈[𝜑]
sup
w∈[𝜓]
[u ≤w]) ≥a.
Building on this intended semantics, we propose the following axiomatization of
LAP∀∃.
Deﬁnition 5 The following are the axioms for LAP∀∃:
(A1)
Axioms of classical propositional calculus (CPC) for 1-formulas
(A2)
𝜑⪯1 𝜓, where 𝜑→𝜓is a tautology of CPC
(A3)
(𝜑⪯a 𝜓) ∧(𝜓⪯b 𝜒) →(𝜑⪯a⊙b 𝜒)
(transitivity)
(A4)
(𝜑⪯a 𝜓) →(𝜑⪯b 𝜓), where a ≤b
(nestedness)
(A5)
(𝜑∨𝜓⪯a 𝜒) ↔(𝜑⪯a 𝜒) ∧(𝜓⪯a 𝜒)
(Left-OR)
(A6)
(𝜔⪯a 𝜑∨𝜓) ↔(𝜔⪯a 𝜑) ∨(𝜔⪯a 𝜓)
(restricted Right-OR)
The only rule of 𝖫𝖠𝖯∀∃is Modus Ponens.
We will denote by ⊢∀∃
𝖫𝖠𝖯the notion of deduction relative to the axiomatic system just
deﬁned.
Theorem 3 For any set T ∪{𝛷} of 1-formulas, it holds that T ⊧∀∃
𝖫𝖠𝖯𝛷if, and only
if, T ⊢∀∃
𝖫𝖠𝖯𝛷.
Proof One direction is soundness, and it is an easy computation, see Proposition
3. As for the other direction, assume T ⊬∀∃
𝖫𝖠𝖯𝛷. The idea is to consider the graded
expressions 𝜑⪯a 𝜓as propositional (Boolean) variables that are ruled by the axioms
together with the laws of classical propositional logic CPC. Let 𝛤be the set of all
possible instantiations of axioms (A1)–(A6). Then it holds that 𝛷does not follow
from T ∪𝛤using CPC reasoning, i.e. T ∪𝛤⊬CPC 𝛷. By completeness of CPC,
there exists a Boolean interpretation v such that v(𝛹) = 1 for all 𝛹∈T ∪𝛤and

A Modal Account of Preference in a Fuzzy Setting
253
v(𝛷) = 0. Now we will build a ⊙-preference Kripke model such that e(𝛹) = 1
for all 𝛹∈T and e(𝛷) = 0. To do that, we take 𝛺= {𝜔∶Var ⟶{0, 1}}, i.e.
the set of interpretations of propositional language, and deﬁne ≤∶𝛺× 𝛺→V by
[𝜔≤𝜔′] = max{a ∈V ∣v(𝜔⪯a 𝜔′) = 1}.
By axioms (A2), (A3), ≤is a ⊙-preorder. Consider the model = (𝛺, ≤, e), where
for each 𝜔∈𝛺and p ∈Var, e(𝜔, p) = 𝜔(p). What remains to check is that e(𝛹) =
v(𝛹) for every LAP∀∃-formula 𝛹. In order to prove this equality it suﬃces to show
that, for every 𝜑, 𝜓∈0 and a ∈[0, 1], we have e(𝜑⪯a 𝜓) = v(𝜑⪯a 𝜓), that is,
to prove that
v(𝜑⪯a 𝜓) = 1
iﬀ
inf
𝜔∈[𝜑]
sup
𝜔′∈[𝜓]
[𝜔≤𝜔′] ≥a.
By axioms (A5) and (A6), we have that 𝖫𝖠𝖯∀∃proves
𝜑⪯a 𝜓↔
⋀
𝜔∈𝛺∶𝜔(𝜑)=1
⋁
𝜔′∈𝛺∶𝜔′(𝜓)=1
𝜔≤a 𝜔′.
Therefore, v(𝜑⪯a 𝜓) = 1 iﬀfor all 𝜔∈𝛺such that 𝜔(𝜑) = 1, there exists w′ ∈
𝛺such that 𝜔′(𝜓) = 1 and v(𝜔⪯a 𝜔′) = 1. But, as we have previously observed,
v(𝜔⪯a 𝜔′) = 1 holds iﬀ[𝜔≤𝜔′] ≥a. In other words, we actually have v(𝜑⪯a 𝜓) =
1 iﬀmin𝜔∈[𝜑]max𝜔′∈[𝜓][𝜔≤𝜔′] ≥a. This concludes the proof.
□
5.2
The Logics LAP∀∃𝟐and LAP∀∀Corresponding to the
≤∀∃𝟐and ≤∀∀Preference Relations
In a very similar way, with the obvious changes, we can deﬁne axiomatic systems for
the logics of LAP∀∃2 and LAP∀∀. We do not include the completeness proofs since
they are analogous to the one for LAP∀∃.
Recall that, under the ∀∃2 semantics, the evaluation of atomic preference formu-
las in a preference Kripke model is as follows:
e(𝜑⪯a 𝜓) = 1 iﬀ[[𝜑]≤∀∃2 [𝜓]] = ( inf
w∈[𝜓]sup
u∈[𝜑]
[u ≤w]) ≥a.
Theorem 4 Let 𝖫𝖠𝖯∀∃2 be the axiomatic system whose axioms are:
(A1)
Axioms of CPC for 1-formulas
(A2)
𝜑⪯1 𝜓, where 𝜓→𝜑is a tautology of CPC
(A3)
(𝜑⪯a 𝜓) ∧(𝜓⪯b 𝜒) →(𝜑⪯a⊙b 𝜒)
(transitivity)
(A4)
(𝜑⪯a 𝜓) →(𝜑⪯b 𝜓), for all a ≤b
(nestedness)
(A5)
(𝜑⪯a 𝜓∨𝜒) ↔(𝜑⪯a 𝜓) ∧(𝜑⪯a 𝜒)
(Right-OR)

254
F. Esteva et al.
(A6)
(𝜑∨𝜓⪯a 𝜔) ↔(𝜑⪯a 𝜔) ∨(⪯a 𝜓⪯a 𝜔)
(restricted Left-OR)
and whose only inference rule is modus ponens. Then 𝖫𝖠𝖯∀∃2 is sound and complete
with respect to the class of ⊙-preference Kripke models under the ∀∃2 semantics.
As for the ∀∀semantics, the evaluation of atomic preference formulas in a pref-
erence Kripke model is:
e(𝜑⪯a 𝜓) = 1 iﬀ[[𝜑]≤∀∀[𝜓]] = ( inf
u∈[𝜑]
inf
w∈[𝜓][u ≤w]) ≥a.
Theorem 5 Let 𝖫𝖠𝖯∀∀be the axiomatic system whose axioms are:
(A1)
Axioms of CPC for 1-formulas
(A2)
(𝜑⪯a 𝜓) →(𝜑′ ⪯a 𝜓′), where 𝜑′ →𝜑, 𝜓′ →𝜓are tautologies of CPC
(A3)
𝜔⪯1 𝜔
(restricted reﬂexivity)
(A4)
(𝜑⪯a 𝜓) ∧(𝜓⪯b 𝜒) →(𝜑⪯a⊙b 𝜒)
(transitivity)
(A5)
(𝜑⪯a 𝜓) →(𝜑⪯b 𝜓), for all a ≤b
(nestedness)
(A6)
(𝜑∨𝜓⪯a 𝜒) ↔(𝜑⪯a 𝜒) ∧(𝜓⪯a 𝜒)
(Left-OR)
(A7)
(𝜓⪯a 𝜑∨𝜓) ↔(𝜓⪯a 𝜑) ∧(𝜓⪯a 𝜓)
(Right-OR)
and whose only inference rule is modus ponens. Then 𝖫𝖠𝖯∀∀is sound and complete
with respect to the class of ⊙-preference Kripke models under the ∀∀semantics.
Moreover, in the same way, we could axiomatize the logics LAPs
∀∃, LAPs
∀∃2 and
LAPs
∀∀corresponding to the associated strict preference orders.
Nevertheless our goal is to axiomatize the logic modeling preference triples
⟨≤, <, ≡⟩corresponding to the preference relations ≤1 = ≤∀∃∧≤∀∃2 and ≤2 = ≤∀∀.
The axiomatizations of these logics are given in the next section.
5.3
The Logic LAP𝟏
In this subsection we deﬁne and study the logic corresponding to the fuzzy preorder
≤1 = ≤∀∃∧≤∀∃2.
The language of LAP1 is as the one for LAP with the diﬀerence that now we have
four kinds of atomic preference formulas:
𝜑⪯a
𝛼𝜓,
𝜑⪯a
𝛽𝜓,
𝜑≺a
𝛼𝜓,
𝜑≺a
𝛽𝜓,
where a ∈V ⧵{0}. The semantics is still given by ⊙-preference Kripke models
= (W, ≤, e), where eevaluates the above kinds of atomic preference formu-
las in the expected way:
∙e(𝜑⪯a
𝛼𝜓) = 1
if [[𝜑]≤∀∃[𝜓]] = (infu∈[𝜑]supw∈[𝜓][u ≤w]) ≥a
∙e(𝜑⪯a
𝛽𝜓) = 1
if [[𝜑]≤∀∃2 [𝜓]] = (infw∈[𝜓]supu∈[𝜑][u ≤w]) ≥a
∙e(𝜑≺a
𝛼𝜓) = 1
if [[𝜑]<∀∃[𝜓]] = (infu∈[𝜑]supw∈[𝜓][u < w]) ≥a

A Modal Account of Preference in a Fuzzy Setting
255
∙e(𝜑≺a
𝛽𝜓) = 1
if [[𝜑]<∀∃2 [𝜓]] = (infw∈[𝜓]supu∈[𝜑][u < w]) ≥a.
The notion of logical consequence is deﬁned as usual, and will be denoted by ⊧𝖫𝖠𝖯1.
Next we propose an axiomatic system for 𝖫𝖠𝖯1.
Deﬁnition 6 The axioms for 𝖫𝖠𝖯1 are:
∙Axioms of 𝖫𝖠𝖯∀∃for the ⪯a
𝛼operators.
∙Axioms of 𝖫𝖠𝖯∀∃2 for the ⪯a
𝛽operators.
∙Axioms for the ≺a
𝛼operators:
(AS1)
¬(𝜑≺a
𝛼𝜑)
(irreﬂexivity)
(AS2)
¬((𝜔≺a
𝛼𝜔′) ∧(𝜔′ ≺b
𝛼𝜔))
(restricted anti-symmetry)
(AS3)
(𝜑≺a
𝛼𝜓) ∧(𝜓≺b
𝛼𝜒) →(𝜑≺a∗b
𝛼
𝜒)
(⊙-transitivity)
(AS4)
(𝜑≺a
𝛼𝜓) →(𝜑≺b
𝛼𝜓), for all a ≤b
(nestedness)
(AS5)
(𝜑≺a
𝛼𝜔) ∧(𝜓≺a
𝛼𝜔) ↔(𝜑∨𝜓≺a
𝛼𝜔)
(Left-OR)
(AS6)
(𝜒≺a
𝛼𝜑∨𝜓) ↔(𝜒≺a
𝛼𝜑) ∨(𝜒≺a
𝛼𝜓)
(Restricted Right-OR)
∙Axioms for the ≺a
𝛽operators:
(BS1)
¬(𝜑≺a
𝛽𝜑)
(irreﬂexivity)
(BS2)
¬((𝜔≺a
𝛽𝜔′) ∧(𝜔′ ≺b
𝛽𝜔))
(restricted anti-symmetry)
(BS3)
(𝜑≺a
𝛽𝜓) ∧(𝜓≺b
𝛽𝜒) →(𝜑≺a∗b
𝛽
𝜒)
(⊙-transitivity)
(BS4)
(𝜑≺a
𝛽𝜓) →(𝜑≺b
𝛽𝜓), for all a ≤b
(nestedness)
(BS5)
(𝜑∨𝜓≺a
𝛽𝜒) ↔(𝜑≺a
𝛽𝜒) ∧(𝜓≺a
𝛽𝜒)
(Restricted Left-OR)
(BS6)
(𝜔≺a
𝛽𝜑∨𝜓) ↔(𝜔≺a
𝛽𝜑) ∨(𝜔≺a
𝛽𝜓)
(Right-OR)
∙Connecting axioms:
(AB)
𝜔⪯a
𝛼𝜔′ ↔𝜔⪯a
𝛽𝜔′
(ABS)
𝜔≺a
𝛼𝜔′ ↔𝜔≺a
𝛽𝜔′
(SA1)
⋀(
(𝜔⪯a
𝛼𝜔′) →(𝜔′ ⪯a
𝛼𝜔) ∶a > 0
)
→¬(𝜔≺a0
𝛼𝜔′), where a0 is the
minimum element of V ⧵{0}.
(SA2)
¬ ⋀(
(𝜔⪯a
𝛼𝜔′) →(𝜔′ ⪯a
𝛼𝜔) ∶a > 0
)
→
(
(𝜔≺b
𝛼𝜔′) ↔(𝜔⪯b
𝛼𝜔′)
)
The only inference rule for 𝖫𝖠𝖯1 is Modus Ponens.
Observe that axiom (AB) is related to the fact that (semantically), over m.e.c.’s,
the weak relations ⪯𝛼and ⪯𝛽coincide, and the same for axiom (ABS) regarding
the strict relations ≺𝛼and ≺𝛽. Finally, axioms (SA1) and (SA2) are for ≺𝛼a logical
translation of the deﬁnition of strict order < from the preorder ≤on W according to
Eq. (2). Note that analogous axioms for ≺𝛽can be derived using axiom (AB).
Denoting by ⊢LAP1 the notion of proof in 𝖫𝖠𝖯1, we have the following complete-
ness result.
Theorem 6 For any set T ∪{𝛷} of 1-formulas, it holds that T ⊧𝖫𝖠𝖯1 𝛷if, and only
if, T ⊢𝖫𝖠𝖯1 𝛷.

256
F. Esteva et al.
Proof One direction is soundness. Let M = (W, ≤, e) a ⊙-preference Kripke model.
Axiom (AB) holds in M since both preorders ≤∀∃and ≤∀∃2 are deﬁned from the same
preorder ≤on W, and thus they coincide over the singletons. The same argument
is valid for (ABS), exchanging preorder by strict order. Axioms (SA1) and (SA2)
correspond to the deﬁnition of the strict order < on W from the preorder ≤. The
interpretation of (AS1) roughly says that, for elements of W, if [u ≤v] ≤[v ≤u]
then [u < v] = 0 and (AS2) says that if [u ≤v] > [v ≤u] then [u < v] = [u ≤v].
As for the converse direction, assume T ⊬LAP1 𝛷. The construction of the coun-
termodel is very similar to that of Theorem 3, and the idea is again to consider
all atomic preference formulas 𝜑⊲a 𝜓(with ⊲∈{⪯𝛼, ⪯𝛽, ≺𝛼, ≺𝛽}) as propositional
(Boolean) variables that are ruled by the laws of classical propositional logic CPC.
Let 𝛤be the set of all possible instantiations of axioms of LAP1. Then it follows
that 𝛷does not follow from T ∪𝛤using CPC reasoning, i.e. T ∪𝛤⊬CPC 𝛷. By
completeness of CPC, there exists a Boolean interpretation v such that v(𝛹) = 1
for all 𝛹∈T ∪𝛤and v(𝛷) = 0. Now we will build a ⊙-preference Kripke model
= (𝛺, ≤, e) such that e(𝛹) = 1 for all 𝛹∈T and e(𝛷) = 0. We take 𝛺=
{𝜔∶Var ⟶{0, 1}}, i.e. the set of Boolean interpretations of propositional vari-
ables, and deﬁne ≤∶𝛺× 𝛺→[0, 1] by
[𝜔≤𝜔′] = max{a ∈V ∣v(𝜔⪯a
𝛼𝜔′) = 1}.
Notice that, by axiom(AB), this value is equal to max{a ∈V ∣v(𝜔⪯a
𝛽𝜔′) = 1}.
Based on ≤, we can deﬁne the corresponding strict order <, and from we can
deﬁne the strict relations on subsets of W, <∀∃and <∀∃2, that coincide on the sin-
gletons by axiom (ABS). By the transitivity axioms of 𝖫𝖠𝖯∀∃and 𝖫𝖠𝖯∀∃2, ≤is a
⊙-preorder. We deﬁne now the evaluation function e, where for each w ∈𝛺and
p ∈Var, e(w, p) = w(p). What remains to be checked is that e(𝛹) = v(𝛹) for every
𝖫𝖠𝖯1-formula 𝛹. In order to prove this equality it suﬃces to show that, for every
𝜑, 𝜓∈0 and a ∈V ⧵{0}, we have e(𝜑⊲a 𝜓) = v(𝜑⊲a 𝜓). As mentioned above
the proof is very similar to the one in Theorem 3 for all the atomic preference for-
mulas, but specially when ⊲∈{⪯𝛼, ⪯𝛽}. Therefore we only prove the equality for
atomic preference formulas of type 𝜑≺a
𝛽𝜓. By the semantics of 𝖫𝖠𝖯1,
e(𝜑≺a
𝛽𝜓) = 1
iﬀ
inf
𝜔′∈[𝜓]sup
𝜔∈[𝜑]
[𝜔< 𝜔′] ≥a.
By axioms (BS5) and (BS6), we have that LAP1 proves
𝜑≺a
𝛽𝜓↔
⋀
𝜔′∈𝛺∶𝜔′(𝜓)=1
⋁
𝜔∈𝛺∶𝜔(𝜑)=1
𝜔≤a 𝜔′.
Therefore, v(𝜑≺a
𝛽𝜓) = 1 iﬀfor all 𝜔′ ∈𝛺such that 𝜔′(𝜓) = 1, there exists 𝜔∈𝛺
such that 𝜔(𝜑) = 1 and v(𝜔≺a
𝛽𝜔′) = 1. But v(𝜔≺a
𝛽𝜔′) = 1 holds iﬀ[𝜔< 𝜔′] ≥a.

A Modal Account of Preference in a Fuzzy Setting
257
In other words, we actually have v(𝜑≺a
𝛽𝜓) = 1 iﬀmin𝜔′∈[𝜓]max𝜔∈[𝜑][𝜔≤𝜔′] ≥
a. This concludes the proof.
□
In the logic LAP1 we can deﬁne the following modal operators for the indiﬀerence
relations corresponding to the preference modalities ⪯a
𝛼and ⪯a
𝛼:
∙𝜑≡a
𝛼𝜓as (𝜑⪯a
𝛼𝜓) ∧(𝜓⪯a
𝛼𝜑),
∙𝜑≡a
𝛽𝜓as (𝜑⪯a
𝛽𝜓) ∧(𝜓⪯a
𝛽𝜑),
and, from them, we can in turn deﬁne the modalities
∙𝜑⪯a
1 𝜓as (𝜑⪯a
𝛼𝜓) ∧(𝜑⪯a
𝛽𝜓),
∙𝜑≡a
1 𝜓as (𝜑≡a
𝛼𝜓) ∧(𝜑≡a
𝛽𝜓),
∙𝜑≺a
1 𝜓as ((𝜑≡a
𝛼𝜓) ∧(𝜑≺a
𝛽𝜓))∨
((𝜑≡a
𝛽𝜓) ∧(𝜑≺a
𝛼𝜓)) ∨((𝜑≺a
𝛼𝜓) ∧(𝜑≺a
𝛽𝜓)).
that eventually determine ⟨⪯1, ≡1, ≺1⟩as the preference structure of the logic 𝖫𝖠𝖯1.
We ﬁnish this section with one remark justifying the above deﬁnition of ≺a
1.
Observe that given a preorder ≤on W, the preorder ≤1 on (W) satisﬁes the fol-
lowing equation:
[A ≤1 B] = min([A ≤∀∃B], [A ≤∀∃2 B]),
that, by Eq. (1), is equal to
min(max([A ≡∀∃B], [A <∀∃B]), max([A ≡∀∃2 B], [A <∀∃2 B])),
and hence, also equal to
max( min([A ≡∀∃B], [A ≡∀∃2 B]), min([A ≡∀∃B], [A <∀∃2 B]),
min([A <∀∃B], [A ≡∀∃2 B]), min([A <∀∃B], [A <∀∃2 B]) ).
Thus, once we deﬁne [A ≡1 B] = min([A ≡∀∃B], [A ≡∀∃2 B])i, then, again according
to Eq. (1), it seems very reasonable to deﬁne the strict order value [A <1 B] by the
maximum of the three remaining terms above, that is:
[A <1 B] = max( min([A ≡∀∃B], [A <∀∃2 B]),
min([A <∀∃B], [A ≡∀∃2 B]), min([A <∀∃B], [A <∀∃2 B]) ).
This motivates the deﬁnition of 𝜑≺a
1 𝜓above.
5.4
The Logic LAP𝟐
In this subsection we deﬁne and study the logic corresponding to the fuzzy preorder
≤2 = ≤∀∀.

258
F. Esteva et al.
The logic LAP2 is deﬁned as the expansion of LAP∀∀with modal operators for the
strict preference ≺a, for each a ∈V ⧵{0}. We just need to take into account that the
semantics for the ≺a operators is as expected: given a Kripke model = (W, ≤, e),
∙e(𝜑≺a 𝜓) = 1 if [[𝜑]<∀∀[𝜓]] = (infu∈[𝜑]infw∈[𝜓][u < w]) ≥a.
Deﬁnition 7 The axioms for 𝖫𝖠𝖯2 are the ones of 𝖫𝖠𝖯∀∀for the ⪯a operators plus:
(AS1)
(𝜑≺a 𝜓) →(𝜑′ ≺a 𝜓′), where 𝜑′ →𝜑, 𝜓′ →𝜓are tautologies of CPC
(AS2)
¬(𝜑≺a 𝜑)
(irreﬂexivity)
(AS3)
(𝜑≺a 𝜓) ∧(𝜓≺b 𝜒) →(𝜑≺a⊙b 𝜒)
(⊙-transitivity)
(AS4)
(𝜑≺a 𝜓) →(𝜑≺b 𝜓), for all a ≤b
(nestedness)
(AS5)
(𝜑∨𝜓≺a 𝜒) ↔(𝜑≺a 𝜒) ∧(𝜓≺a 𝜒)
(Left-OR)
(AS6)
(𝜓≺a 𝜑∨𝜒) ↔(𝜓≺a 𝜒)∧(𝜓≺a 𝜒)
(Right-OR)
(SA1)
⋀(
(𝜔⪯a 𝜔′) →(𝜔′ ⪯a 𝜔) ∶a > 0
)
→¬(𝜔≺a0 𝜔′), where a0 is the min-
imum element of V ⧵{0}.
(SA2)
¬ ⋀(
(𝜔⪯a 𝜔′) →(𝜔′ ⪯a 𝜔) ∶a > 0
)
→
(
(𝜔≺b 𝜔′) ↔(𝜔⪯b 𝜔′)
)
The only rule of LAP2 is modus ponens.
Note that axioms (SA1) and (SA2) above are analogous to the ones in LAP1, and
the remark after the deﬁnition LAP1 justifying them applies here as well.
The completeness theorem is ready and the proof is also analogous to previous
ones, thus we omit it.
Theorem 7 For any set T ∪{𝛷} of 1-formulas, it holds that T ⊧LAP1 𝛷if, and only
if, T ⊢LAP1 𝛷.
Finally, let us observe that in LAP2 we can also deﬁne now the preference structure
⟨⪯2, ≡2, ≺2⟩in the obvious way:
∙The weak preference statement 𝜑⪯2 𝜓is deﬁned as 𝜑⪯a 𝜓,
∙The equivalence statement 𝜑≡2 𝜓is deﬁned as (𝜑⪯a 𝜓) ∧(𝜓⪯a 𝜑),
∙The strict preference statement 𝜑≺2 𝜓is deﬁned by (𝜑≺a 𝜓).
Notice however that, strictly speaking, 𝜑⪯2 𝜓is not a fuzzy preorder and ≡2 is not
a fuzzy similarity since they are not reﬂexive.
6
Conclusions and Future Work
In this paper we have studied preference structures on classical sets arising from
fuzzy preference relations, a topic that, as far as we know, has not been very studied
in the literature. We have approached the question both from a relational and logical
points of view. In the relational approach we have studied and characterized possi-
ble extensions of fuzzy preorders on a crisp set W (interpreted as fuzzy preferences

A Modal Account of Preference in a Fuzzy Setting
259
between the elements of W) to crisp subsets of W (fuzzy preferences on crisps sub-
sets). Within the logical approach, we have deﬁned and studied several two-tiered
modal logics capturing, at the syntactical level, the corresponding preference struc-
tures. The same scheme can be generalized to fuzzy preference relations on fuzzy
sets. Given a fuzzy preorder ≤on a universe W, we can deﬁne corresponding exten-
sions to fuzzy relations on the set (W) of fuzzy subsets of W. For example, for all
A, B ∈(W), corresponding extensions for ∀∃and ∀∀could be deﬁned as
(A ≤∀∃B) = [inf
u∈W((𝜇A(u)) →(sup
v∈W
([u ≤v] ⊙𝜇B(v))))]
(A ≤∀∀B) = [inf
u∈W((𝜇A(u)) →(inf
v∈W([u ≤v] →𝜇B(v))))].
As future work we plan to study and characterize these type of fuzzy preference rela-
tions taking into account the works by Bodenhofer et al. [2–4], where the authors
study some of these relations in the purely fuzzy relational setting. Finally we plan
to connect the corresponding fuzzy preference structures with a modal many-valued
logic framework, with necessity, possibility, universal and existential modal opera-
tors (see [21] for a ﬁrst approach) in a similar way that it is done in [1] in the classical
setting.
Acknowledgements Esteva and Godo acknowledges partial support by the Spanish FEDER
/MINECO project TIN2015-71799-C2-1-P. Vidal acknowledges partial support by the joint project
of the Austrian Science Fund (FWF) I1897-N25 and the Czech Science Foundation (GACR) 15-
34650L, and by the institutional grant RVO:67985807.
References
1. van Benthem, J., Girard, P., Roy, O.: Everything else being equal: a modal logic for ceteris
paribus preferences. J. Philos. Logic 38, 83–125 (2009)
2. Bodenhofer, U.: Orderings of fuzzy sets based on fuzzy orderings part I: the basic approach.
Mathw. Soft Comput. 15(2), 201–218 (2008)
3. Bodenhofer, U.: Orderings of fuzzy sets based on fuzzy orderings part II: generalizations.
Mathw. Soft Comput. 15(3), 219–249 (2008)
4. Bodenhofer, U., Demirci, M.: Strict fuzzy orderings with a given context of similarity. Int. J.
Uncertain. Fuzziness Knowl. Based Syst. 16(2), 147–178 (2008)
5. Domshlak, C., Hüllermeier, E., Kaci, S., Prade, H.: Preferences in AI: an overview. Artif. Intell.
175, 1037–1052 (2011)
6. Fodor, J., Roubens, M.: Fuzzy Preference Modeling and Multicriteria Decision Support.
Kluwer Academic Publishers (1994)
7. Díaz, S., De Baets, B., Montes, S.: Additive decomposition of fuzzy pre-orders. Fuzzy Sets
Syst. 158(8), 830–842 (2007)
8. Díaz, S., De Baets, B., Montes, S.: General results on the decomposition of transitive fuzzy
relations. Fuzzy Optim. Decis. Making 9(1), 1–29 (2010)
9. Díaz, S., Montes, S., De Baets, B.: Transitive decomposition of fuzzy preference relations: the
case of nilpotent minimum. Kybernetika 40(1), 71–88 (2004)
10. Díaz, S., De Baets, B., Montes, S.: On the compositional characterization of complete fuzzy
pre-orders. Fuzzy Sets Syst. 159(17), 2221–2239 (2008)

260
F. Esteva et al.
11. Díaz, S., García-Lapresta, J.L., Montes, S.: Consistent models of transitivity for reciprocal
preferences on a ﬁnite ordinal scale. Inf. Sci. 178(13), 2832–2848 (2008)
12. Esteva, F., Godo, L., Rodriguez, R.O., Vetterlein, T.: Logics for approximate and strong entail-
ments. Fuzzy Sets Syst. 197, 59–70 (2012)
13. Esteva, F., Godo, L., Vidal, A.: On a graded modal logic approach to reason with fuzzy pref-
erences. In: Proceedings of CCIA 2017 (to appear)
14. Fono, L.A., Andjiga, N.G.: Fuzzy strict preference and social choice. Fuzzy Sets Syst. 155,
372–389 (2005)
15. Halpern, J.Y.: Deﬁning relative likelihood in partially-ordered preference structures. J. Artif.
Intell. Res. 7, 1–24 (1997)
16. Liu, F.: Von Wright’s “The Logic of Preference” revisited. Synthese 175(1), 69–88 (2010)
17. Hansson, S.O., Grüne-Yanoﬀ, T.: Preferences. In: Zalta, E.N. (ed.) The Stanford Encyclope-
dia of Philosophy (Winter 2012 Edition). https://plato.stanford.edu/archives/win2012/entries/
preferences/ (2012)
18. Kaci, S.: Working with Preferences: Less is More. Springer (2011)
19. Mayor, G., Torrens, J.: On a class of operators for expert systems. Int. J. Intell. Syst. 8, 771–778
(1993)
20. Sören, H.: On the logic of better. Library of Theoria, no. 2, Uppsala (1957)
21. Vidal, A., Esteva, F., Godo, L.: On ﬁnite-valued bimodal logics with an application to reasoning
about preferences In: Proceedings of EUSFLAT 2017 (to appear)
22. von Wright, G.H.: The Logic of Preference. Edinburgh University Press (1963)
23. von Wright, G.H.: The logic of preference reconsidered. Theory Decis. 3(2), 140–169 (1972)

On Possibilistic Dependencies: A Short
Survey of Recent Developments
Robert Fullér and István Á. Harmati
Abstract Carlsson and Fullér introduced the notions of possibilistic mean value and
variance of fuzzy numbers. Fullér and Majlender introduced a measure of possibilis-
tic covariance between marginal distributions of a joint possibility distribution as the
average value of the interactivity relation between the level sets of its marginal dis-
tributions. Fullér et al. introduced the possibilistic correlation ratio, the possibilistic
correlation coeﬃcient and the possibilistic informational coeﬃcient of correlation.
In this paper we give a short survey of some later works which extend and develop
these notions.
1
Introduction
In probability theory the notion of mean value of functions of random variables plays
a fundamental role in deﬁning the basic characteristic measures of probability dis-
tributions: the measure of covariance, variance and correlation of random variables
can all be computed as probabilistic means of their appropriately chosen real-valued
functions. Similarly, in possibility theory we can use the principle of average value
of appropriately chosen real-valued functions to deﬁne mean value, variance, covari-
ance and correlation of possibility distributions. Marginal probability distributions
are determined from the joint one by the principle of ‘falling integrals’ and marginal
possibility distributions are determined from the joint possibility distribution by the
principle of ‘falling shadows’. Probability distributions can be interpreted as carriers
of incomplete information [43], and possibility distributions can be interpreted as
carriers of imprecise information. A function f∶[0, 1] →ℝis said to be a weighting
R. Fullér (✉)
Department of Informatics, Széchenyi István University, Győr, Hungary
e-mail: rfuller@sze.hu
I.Á. Harmati
Department of Mathematics and Computational Sciences,
Széchenyi István University, Győr, Hungary
e-mail: harmati@sze.hu
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_16
261

262
R. Fullér and I.Á. Harmati
function if f is non-negative, monotone increasing and satisﬁes the following nor-
malization condition ∫1
0 f(𝛾)d𝛾= 1. Diﬀerent weighting functions can give diﬀerent
(case-dependent) importances to 𝛾-levels sets of fuzzy numbers. It is motivated in
part by the desire to give less importance to the lower levels of fuzzy sets [34] (it is
why f should be monotone increasing).
We can deﬁne the mean value (variance) of a possibility distribution as the
f-weighted average of the probabilistic mean values (variances) of the respective
uniform distributions deﬁned on the 𝛾-level sets of that possibility distribution. A
measure of possibilistic covariance (correlation) between marginal possibility dis-
tributions of a joint possibility distribution can be deﬁned as the f-weighted aver-
age of probabilistic covariances (correlations) between marginal probability distri-
butions whose joint probability distribution is deﬁned to be uniform on the 𝛾-level
sets of their joint possibility distribution [29]. We should note here that the choice
of uniform probability distribution on the level sets of possibility distributions is
not without reason. Namely, these possibility distributions are used to represent
imprecise human judgments and they carry non-statistical uncertainties. Therefore
we will suppose that each point of a given level set is equally possible. Then we
apply Laplace’s principle of Insuﬃcient Reason: if elementary events are equally
possible, they should be equally probable (for more details and generalization of
principle of Insuﬃcient Reason see [26], page 59). The main new idea here is to
equip the level sets of joint possibility distributions with uniform probability distrib-
utions and to derive possibilistic mean value, variance, covariance and correlation of
possibility distributions, in such a way that they would be consistent with the exten-
sion principle. The idea of equipping the level sets of fuzzy numbers with a uniform
probability refers to early ideas of simulation of fuzzy sets by Yager [60], and possi-
bility/probability transforms by Dubois et al. [25] as well as the pignistic transform
of Smets [55].
2
Possibilistic Mean Value, Variance, Covariance,
Correlation Coeﬃcient and Correlation Ratio
In this section we will recall the possibilistic mean value, variance, covariance and
correlation of fuzzy numbers, which are consistent with the extension principle and
with the well-known deﬁnitions of expectation and variance in probability theory.
A fuzzy number A is a fuzzy set ℝwith a normal, fuzzy convex and continuous
membership function of bounded support. The family of fuzzy numbers is denoted
by F. Fuzzy numbers can be considered as possibility distributions [24, 63]. A fuzzy
set C in ℝ2 is said to be a joint possibility distribution of fuzzy numbers A, B ∈F, if
it satisﬁes the relationships max{x ∣C(x, y)} = B(y) and max{y ∣C(x, y)} = A(x) for
all x, y ∈ℝ. Furthermore, A and B are called the marginal possibility distributions
of C.

On Possibilistic Dependencies: A Short Survey of Recent Developments
263
The possibilistic mean (or expected value), variance, covariance and correlation
were originally deﬁned from the measure of possibilistic interactivity (as shown in
[10, 29]) but for simplicity, we will present the concept of possibilistic mean value,
variance, covariance and possibilistic correlation in a probabilistic setting and point
out the fundamental diﬀerence between the standard probabilistic approach and the
possibilistic one. Let A ∈F be fuzzy number with [A]𝛾= [a1(𝛾), a2(𝛾)] and let U𝛾
denote a uniform probability distribution on [A]𝛾, 𝛾∈[0, 1]. Recall that the proba-
bilistic mean value of U𝛾is equal to
M(U𝛾) =
a1(𝛾) + a2(𝛾)
2
,
and its probabilistic variance is computed by
var(U𝛾) =
(a2(𝛾) −a1(𝛾))2
12
.
In 1987 Dubois and Prade [23] deﬁned an interval-valued expectation of fuzzy
numbers, viewing them as consonant random sets. They also showed that this expec-
tation remains additive in the sense of addition of fuzzy numbers. In 2003 Fullér and
Majlender [28] introduced the f-weighted possibilistic mean value of A ∈F as
Ef (A) = ∫
1
0
M(U𝛾)f(𝛾)d𝛾= ∫
1
0
a1(𝛾) + a2(𝛾)
2
f(𝛾)d𝛾,
where U𝛾is a uniform probability distribution on [A]𝛾for all 𝛾∈[0, 1]. If f(𝛾) = 1
for all 𝛾∈[0, 1] then we get
E(A) = ∫
1
0
M(U𝛾)f(𝛾)d𝛾= ∫
1
0
a1(𝛾) + a2(𝛾)
2
d𝛾,
which the possibilistic mean value of A originally introduced in 2001 by Carlsson and
Fullér [3]. In 2003 Fullér and Majlender [28] introduced the f-weighted possibilistic
variance of A ∈F as
Varf (A) = ∫
1
0
var(U𝛾)f(𝛾)d𝛾= ∫
1
0
(a2(𝛾) −a1(𝛾))2
12
f(𝛾)d𝛾.
In 2004 Fullér and Majlender [29] introduced a measure of possibilistic covari-
ance between marginal distributions of a joint possibility distribution C as the
expected value of the interactivity relation between the 𝛾-level sets of its marginal
distributions. In 2005 Carlsson et al. [10] showed that the possibilistic covariance
between fuzzy numbers A and B can be written as the weighted average of the prob-
abilistic covariances between random variables with uniform joint distribution on the

264
R. Fullér and I.Á. Harmati
level sets of their joint possibility distribution C. The f-weighted measure of possi-
bilistic covariance between A, B ∈F (with respect to their joint distribution C) [29]
can be written as,
Covf (A, B) = ∫
1
0
cov(X𝛾, Y𝛾)f(𝛾)d𝛾,
and the f-weighted possibilistic correlation coeﬃcient of A, B ∈F (with respect to
their joint distribution C) is deﬁned by [31],
𝜌f (A, B) = ∫
1
0
𝜌(X𝛾, Y𝛾)f(𝛾)d𝛾
where
𝜌(X𝛾, Y𝛾) =
cov(X𝛾, Y𝛾)
√var(X𝛾)√var(Y𝛾)
and X𝛾and Y𝛾are random variables whose joint distribution is uniform on [C]𝛾,
cov(X𝛾, Y𝛾) denotes their covariance, for all 𝛾∈[0, 1].
In statistics, the correlation ratio is a measure of the relationship between the
statistical dispersion within individual categories and the dispersion across the whole
population or sample. The correlation ratio was originally introduced by Pearson [52]
and it was extended to random variables by Kolmogorov [44] as,
𝜂2(X|Y) = var[M(X|Y)]
var(X)
,
where X and Y are random variables. If X and Y have a joint probability density func-
tion, denoted by g(x, y), then we can compute 𝜂2(X|Y) using the following formulas
M(X|Y = y) = ∫
∞
−∞
xg(x|y)dx
and
var[M(X|Y)] = M(M(X|y) −M(X))2,
and where,
g(x|y) =
g(x, y)
g(y) .
In 2010 Fullér et al. [30] introduced the f-weighted possibilistic correlation ratio
of marginal possibility distribution A with respect to marginal possibility distribution
B as
𝜂2
f (A|B) = ∫
1
0
𝜂2(X𝛾|Y𝛾)f(𝛾)d𝛾

On Possibilistic Dependencies: A Short Survey of Recent Developments
265
where X𝛾and Y𝛾are random variables whose joint distribution is uniform on [C]𝛾,
cov(X𝛾, Y𝛾) denotes their covariance and 𝜂(X𝛾|Y𝛾) denotes their probabilistic corre-
lation ratio [44], for all 𝛾∈[0, 1].
In 2012 Fullér et al. [32] introduced the f-weighted possibilistic informational
coeﬃcient of correlation. For any two continuous random variables X and Y (admit-
ting a joint probability density), their mutual information is given by,
I(X, Y) = ∬g(x, y) ln
g(x, y)
g1(x)g2(y) dx dy
where g(x, y) is the joint probability density function of X and Y, and g1(x) and g2(y)
are the marginal density functions of X and Y, respectively. The informational coef-
ﬁcient of correlation of X and Y is deﬁned by
L(X, Y) =
√
1 −e−2I(X,Y)
Then the f-weighted possibilistic informational coeﬃcient of correlation of marginal
possibility distributions A and B is deﬁned by
L(A, B) = ∫
1
0
L(X𝛾, Y𝛾)f(𝛾)d𝛾
where X𝛾and Y𝛾are random variables whose joint distribution is uniform on [C]𝛾,
for all 𝛾∈[0, 1].
Note 1 There exist several other ways to deﬁne correlation coeﬃcient for fuzzy
numbers, e.g. Liu and Kao [47] used fuzzy measures to deﬁne a fuzzy correla-
tion coeﬃcient of fuzzy numbers and they formulated a pair of nonlinear pro-
grams to ﬁnd the 𝛼-cut of this fuzzy correlation coeﬃcient, then, in a special case,
Hong [39] showed an exact calculation formula for this fuzzy correlation coeﬃcient.
Vaidyanathan [57] introduced a new measure for the correlation coeﬃcient between
triangular fuzzy variables called credibilistic correlation coeﬃcient.
Fullér and colleagues have extensively used the possibilistic mean value, vari-
ance, covariance and correlation in their later works for real option valuation [4, 8,
14], portfolio selection problems [6, 7, 11, 12] and strategic planning [9, 13]. For
example, in 2007 Carlsson et al. [12] developed a methodology for valuing options
on R&D projects, when future cash ﬂows are estimated by trapezoidal fuzzy num-
bers. In particular, they presented the following fuzzy mixed integer programming
model for the R&D optimal portfolio selection problem,

266
R. Fullér and I.Á. Harmati
maximize
N
∑
i=1
uiFi
subject to
N
∑
i=1
uiXi +
N
∑
i=1
(1 −ui)ci ≤B
ui ∈{0, 1}, i = 1, … N,
where N is the number of R&D projects, B is the whole investment budget, ui is the
decision variable that takes value one if the i-th project should start now (at time zero)
or takes the value zero if it should be postponed and started at a later time, ci denotes
the cost of the postponment (i.e. keep the option alive), Xi is the investment cost, and
Fi denotes the possibilistic deferral ﬂexibility of the i-th project for i = 1, … , N.
Furthermore, they discussed how their methodology can be used to build decision
support tools for optimal R&D project selection in a corporate environment. They
also claimed that the imprecision we encounter when judging or estimating future
cash ﬂows is not stochastic in nature, and the use of probability theory gives us a
misleading level of precision and a notion that consequences somehow are repetitive.
This is not the case, the uncertainty is genuine, i.e. we simply do not know the exact
levels of future cash ﬂows. Without introducing fuzzy real option models it would
not be possible to formulate this genuine uncertainty.
In 2009 Collan et al. [18] presented a new method (fuzzy pay-oﬀmethod) for
real option valuation using fuzzy numbers that is based on ﬁndings from earlier real
option valuation methods and from fuzzy real option valuation. They also presented
the use of number of diﬀerent types of fuzzy numbers with the method and an appli-
cation of the new method in an industry setting. In 2010 Carlsson et al. [13] used
fuzzy real option models for the problem of closing/not closing a production plant in
the forest products industry sector. In 2013 Carlsson and Fullér [15] implemented a
hybrid probabilistic and possibilistic model to assess the success of computing tasks
in a Grid. Using the predictive probabilistic approach they developed a framework
for resource management in grid computing, and by introducing an upper limit for
the number of possible failures, they approximated the probability that a particular
computing task can be executed. Coroianu and Fullér [19] studied he problem of
additivity property of the weighted possibilistic mean operator for interactive fuzzy
numbers. They showed that the weighted possibilistic mean operator is additive on
the set of symmetric fuzzy numbers if their joint possibility distribution is deﬁned
by a triangular norm. They also showed some results for general joint-possibility-
distribution-based additions of fuzzy numbers of symmetrical opposite sides.
3
Recent Developments
The notions of possibilistic mean value, variance, covariance and correlation are
used in many diﬀerent research areas and by many diﬀerent authors (Google Scholar
ﬁnds over 2,000 citations to papers [3, 8, 10, 12, 28, 29]).

On Possibilistic Dependencies: A Short Survey of Recent Developments
267
In 2005 Yoshida et al. [61] evaluated the randomness and fuzziness in fuzzy sto-
chastic processes by the probabilistic expectation and the mean values deﬁned by
fuzzy measures and 𝜆-weighting functions. The mean values are demonstrated par-
ticularly in three kinds of important fuzzy measures: possibility measure, necessity
measure and credibility measure. Furthermore, by introducing fuzziness to stochas-
tic processes in optimization/decision-making, they considered a new model with
uncertainty of both randomness and fuzziness, which is a reasonable and natural
extension of the original stochastic process.
In 2006 Fang et al. [27] proposed a portfolio rebalancing model with transaction
costs based on fuzzy decision theory and illustrated the behaviour of their proposed
model using real data from the Shanghai Stock Exchange. Huang [41] selected the
optimal portfolio with fuzzy returns by criteria of chance represented by credibility
measure. He introduced two types of credibility-based portfolio selection models:
(i) by one chance criterion, the objective is to maximize the investor’s return at a
given threshold conﬁdence level; (ii) by another chance criterion, the objective is
to maximize the credibility of achieving a speciﬁed return level subject to the con-
straints. To solve the resulting problems he designed a hybrid intelligent algorithm
integrating fuzzy simulation and genetic algorithm.
In 2007 Zhang et al. [64] proposed two kinds of portfolio selection models based
on lower and upper possibilistic means and possibilistic variances, respectively, and
introduced the notions of lower and upper possibilistic eﬃcient portfolios. They also
presented an algorithm which can derive the explicit expression of the possibilis-
tic eﬃcient frontier for the possibilistic mean-variance portfolio selection problem
dealing with lower bounds on asset holdings. Zhang and Wang [65] investigated the
relationship between several crisp possibilistic variances and covariances of fuzzy
numbers. Silva et al. [54] presented and developed an original and novel fuzzy sets
based method that solves a class of quadratic programming problems with vagueness
in the set of constraints. The method uses two phases to solve fuzzy quadratic pro-
gramming problems, which eventually can be considered in the portfolio context. In
the ﬁrst phase they parametrize the fuzzy problem in several classical alpha-problems
with diﬀerent cutting levels. In the second phase each of these alpha-problems is
solved by using conventional solving techniques.
In 2008 by introducing the concept of semivariance of fuzzy variable Huang [42]
proposed two fuzzy mean-semivariance models for portfolio selection problems in
fuzzy environment. To solve the new models in general cases, he presented a fuzzy
simulation based genetic algorithm. By morphing mean-variance optimization port-
folio model into semi-absolute deviation model, Gupta et al. [36] applied multi cri-
teria decision making via fuzzy mathematical programming to develop comprehen-
sive models of asset portfolio optimization for the investors’ pursuing either of the
aggressive or conservative strategies.
In 2009 Chen et al. [16] considered a possibilistic mean-variance (FMVC) port-
folio selection model and proposed a cutting plane algorithm to solve it. Xu et al.
[59] presented a fuzzy normal jump-diﬀusion model for European option pricing,
with uncertainty of both randomness and fuzziness in the jumps, which is a reason-
able and a natural extension of the Merton [50] normal jump-diﬀusion model. Based

268
R. Fullér and I.Á. Harmati
on the crisp weighted possibilistic mean values of the fuzzy variables in fuzzy nor-
mal jump-diﬀusion model, they also obtained the crisp weighted possibilistic mean
normal jump-diﬀusion model. Yoshida [62] discussed value-at-risk portfolio model
under uncertainty. In his proposed model the means, the variances and the measure-
ments of imprecision for fuzzy numbers/fuzzy random variables are evaluated in the
possibility case and the necessity case, and the rate of return in portfolio is estimated
regarding the both random factors and imprecise factors. Zhang et al. [66] proposed a
new portfolio selection model with the maximum utility based on the interval-valued
possibilistic mean and possibilistic variance, which is a two-parameter quadratic pro-
gramming problem. They also presented a sequential minimal optimization (SMO)
algorithm to obtain the optimal portfolio. The remarkable feature of their algorithm
is that it is extremely easy to implement, and it can be extended to any size of port-
folio selection problems for ﬁnding an exact optimal solution.
In 2010 Zhang et al. [67] proposed a possibilistic portfolio adjusting model with
transaction costs and bounded constraints on holdings of assets, which can be trans-
formed into a linear programming problem. Both the lower bounds on holdings and
the total investment constraints inﬂuence the optimal portfolio adjusting strategies.
Gładysz and Kasperski [33] discussed the problem of computing the mean absolute
deviation in a set of uncertain variables. The uncertainty is modelled by closed inter-
vals and fuzzy intervals. Some polynomial algorithms for determining the lower and
upper bounds for the mean absolute deviation under interval uncertainty are pro-
posed. Possibility theory is then applied to generalize the interval uncertainty repre-
sentation to the fuzzy one.
In 2011 Ho and Liao [38] proposed a fuzzy binomial approach for investment
project valuation in uncertain environments from the aspect of real options. Their
approach also reveals the value of ﬂexibilities embedded in the project. Duan and
Stahlecker [22] considered static portfolio selection problem, in which future returns
of securities are given as fuzzy sets. In contrast to traditional analysis, they assume
that investment decisions are not based on statistical expectation values, but rather on
maximal and minimal potential returns resulting from the 𝛼-cuts of these fuzzy sets.
By aggregating over all 𝛼-cuts and assigning weights for both best and worst possible
cases they get a new objective function to derive an optimal portfolio. Lee and Lee
[45] examined the strategic characteristic of RFID (Radio Frequency Identiﬁcation)
investment and proposed a fuzzy real options technique that can consider various
situations of expected cash ﬂow or investment costs as a plan to support investment
decisions.
In 2012 Deng and Li [20] proposed a portfolio selection model with borrow-
ing constraint by means of possibilistic mean, possibilistic variance, and possibilis-
tic covariance under the assumption that the returns of assets are fuzzy numbers.
They presented a quadratic programming model with inequality constraints when
the returns of assets are trapezoid fuzzy numbers and utilized the Lemke algorithm
to solve the problem. Zhang et al. [68] dealt with a multi-period portfolio selection
problem with fuzzy returns and presented a possibilistic mean-semivariance-entropy
model for multi-period portfolio selection by taking into account four criteria: return,
risk, transaction cost and diversiﬁcation degree of portfolio. In their proposed model,

On Possibilistic Dependencies: A Short Survey of Recent Developments
269
the return level is quantiﬁed by the possibilistic mean value of return, the risk level is
characterized by the lower possibilistic semivariance of return, and the diversiﬁca-
tion degree of portfolio is measured by the originally presented possibilistic entropy.
Based on the possibilistic mean and the possibilistic variance/covariance of fuzzy
numbers, Chrysaﬁs [17] proposed a method to reduce some problems arising from
the Capital Asset Pricing Model (CAPM) assumptions.
In 2013 Thavaneswaran et al. [56] used fuzzy set theory to price binary options.
Namely, they studied binary options by fuzzifying the maturity value of the stock
price using trapezoidal, parabolic and adaptive fuzzy numbers. Hsieh and Tsaur
[40] proposed a simpliﬁed fuzzy regression equation based on possibilistic mean
and variance method and used it for modeling the constraints and objective func-
tion of a fuzzy regression model without determining the membership function of
extrapolative values. Liu and Zhang [48] discussed a multi-objective portfolio opti-
mization problem for practical portfolio selection in fuzzy environment, in which the
return rates and the turnover rates are characterized by fuzzy variables. Based on the
possibility theory, they quantiﬁed fuzzy return and liquidity by possibilistic mean,
and market risk and liquidity risk are measured by lower possibilistic semivariance.
They proposed a fuzzy multi-objective programming technique to transform their
proposed models into corresponding single-objective models and then designed a
genetic algorithm for their solution.
In 2014 Wang et al. [58] employed the weighted possibilistic mean (WPM),
weighted interval valued possibilistic mean (WIVPM) of fuzzy number as a sort
of representative values for the fuzzy attribute data, and establish new fuzzy con-
trol charts with WPM and WIVPM. They compared the performance of the charts
to the existing fuzzy charts with a fuzzy c-chart example via newly deﬁned average
number of inspection for variation of control state. Based on possibility theory and
the assumption that the returns of assets are triangular fuzzy numbers, Deng and
Li [21] proposed a bi-objective nonlinear portfolio selection model. They show that
their nonlinear bi-objective model is equivalent to the linear bi-objective minimizing
programming model on the basis of possibilistic mean and possibilistic variance.
In 2015 Nguyen et al. [51] initiated the fuzzy Sharpe ratio in the fuzzy model-
ing context. In addition to the introduction of the new risk measure, they also put
forward the reward-to-uncertainty ratio to assess the portfolio performance in fuzzy
modeling. Zhang [69] considered a multi-period portfolio selection problem in a
fuzzy investment environment, in which the return and risk of assets are character-
ized by possibilistic mean value and possibilistic semivariance, respectively. Based
on the theories of possibility, he proposed a new multi-period possibilistic portfo-
lio selection model, which contains risk control, transaction costs, borrowing con-
straints, threshold constraints and cardinality constraints. By redeﬁning the concepts
of mean and variance for fuzzy numbers, Li et al. [46] formulated a fuzzy mean-
variance-skewness portfolio selection model.
In 2016 Mashayekhi and Omrani [49] proposed a novel multi objective model
for portfolio selection, where the asset returns are considered as trapezoidal fuzzy
numbers. Their model incorporates the DEA cross-eﬃciency into Markowitz mean-
variance model and considers return, risk and eﬃciency of the portfolio. Rubio et al.

270
R. Fullér and I.Á. Harmati
[53] proposed the weighted fuzzy time series method to forecast the future perfor-
mance of returns on portfolios. They modelled the uncertain parameters of the fuzzy
portfolio selection models using a possibilistic interval-valued mean approach, and
approximate the uncertain future return on a given portfolio by means of a trape-
zoidal fuzzy number. Guo et al. [35] considered a fuzzy multi-period portfolio selec-
tion problem with V-Shaped transaction cost. Compared with the traditional studies
assuming that assets have the same investment horizon, they handled the practical
but complicated situation in which assets have diﬀerent investment horizons. Within
the framework of credibility theory, they formulate a mean-variance model with the
objective of maximizing the terminal return under the total risk constraint over the
whole investment.
In 2017 Babazadeh et al. [1] presented a multi-objective possibilistic program-
ming model to design a second-generation biodiesel supply chain network under
risk. Their model minimizes the total costs of biodiesel supply chain from feedstock
supply centers to customer centers besides minimizing the environmental impact of
all involved processes under a well-to-wheel perspective. Brunelli and Mezei [2]
presented an analysis of approximate operations on fuzzy numbers. By focusing on
the ranking and defuzziﬁcation procedures as essential tools in fuzzy decision mak-
ing problems, they studied the errors produced by the application of approximate
operations.
4
Concluding Remarks
Possibility theory is mathematically the simplest uncertainty theory for dealing with
incomplete information [26]. This may be the reason why possibilistic dependencies
are used in many diﬀerent research areas like information sciences, geosciences,
social sciences, economics, mathematical and computer modelling, ﬁnancial engi-
neering, systems engineering, military engineering, and robotics. We have shown
several applications of possibilistic dependencies ranging from multi-period portfo-
lio selection problem with fuzzy returns to designing a second-generation biodiesel
supply chain network. However, it is still an open problem to construct joint possi-
bility distribution for correlated variables in applications [37].
References
1. Babazadeh, R., Razmi, J., Pishvaee, M.S., Rabbani, M.: A sustainable second-generation
biodiesel supply chain network design problem under risk. Omega 66, 258–277 (2017)
2. Brunelli, M., Mezei, J.: An inquiry into approximate operations on fuzzy numbers. Int. J.
Approx. Reas. 81, 147–159 (2017)
3. Carlsson, C., Fullér, R.: On possibilistic mean value and variance of fuzzy numbers. Fuzzy
Sets Syst. 122, 315–326 (2001)

On Possibilistic Dependencies: A Short Survey of Recent Developments
271
4. Carlsson, C., Fullér, R.: On optimal investment timing with fuzzy real options. In: Proceedings
of the EUROFUSE 2001 Workshop on Preference Modelling and Applications, April 25–28,
pp. 235–239. Granada, Spain (2001)
5. Carlsson, C., Fullér, R., Majlender, P.: Project selection with fuzzy real options. In: Proceedings
of the Second International Symposium of Hungarian Researchers on Computational Intelli-
gence, pp. 81–88 (2001)
6. Carlsson, C., Fullér, R.: Project scheduling with fuzzy real options. In: Trappl, R. (ed.) Cyber-
netics and Systems 2002, Proceedings of the Sixteenth European Meeting on Cybernetics and
Systems Research, Vienna, April 2–4, 2002, Austrian Society for Cybernetic Studies, pp. 511–
513 (2002)
7. Carlsson, C., Fullér, R., Majlender, P.: A possibilistic approach to selecting portfolios with
highest utility score. Fuzzy Sets Syst. 131, 13–21 (2002)
8. Carlsson, C., Fullér, R.: A fuzzy approach to real option valuation. Fuzzy Sets Syst. 139, 297–
312 (2003)
9. Carlsson, C., Fedrizzi, M., Fullér, R.: Fuzzy Logic in Management, International Series in
Operations Research and Management Science, vol. 66. Kluwer Academic Publishers, Boston
(2004)
10. Carlsson, C., Fullér, R., Majlender, P.: On possibilistic correlation. Fuzzy Sets Syst. 155, 425–
445 (2005)
11. Carlsson, C., Fullér, R., Majlender, P.: A fuzzy real options model for R&D project evaluation.
In: Liu, Y., Chen, G., Ying, M. (eds.) Proceedings of the Eleventh IFSA World Congress,
Beijing, China, July 28–31, 2005, pp. 1650–1654. Tsinghua University Press and Springer,
Beijing (2005)
12. Carlsson, C., Fullér, R., Heikkilä, M., Majlender, P.: A fuzzy approach to R&D project portfolio
selection. Int. J. Approximate Reasoning 44, 93–105 (2007)
13. Carlsson, C., Heikkilä, M., Fullér, R.: Fuzzy real options models for closing/not closing a
production plant. In: Kahraman, C., Yavuz, M. (eds.) Production Engineering and Management
under Fuzziness, Studies in Fuzziness and Soft Computing, vol. 252, pp. 537–560. Springer
[ISBN 978-3-642-12051-0] (2010)
14. Carlsson, C., Fullér, R.: Possibility for Decision: A Possibilistic Approach to Real Life Deci-
sions, Studies in Fuzziness and Soft Computing Series, vol. 270. Springer (2011)
15. Carlsson, C., Fullér, R.: Probabilistic versus possibilistic risk assessment models for optimal
service level agreements in grid computing. Inf. Syst. e-Bus. Manage. 11, 13–28 (2013)
16. Chen, G., Liao, X., Wang, S.: A cutting plane algorithm for MV portfolio selection model.
Appl. Math. Comput. 215, 1456–1462 (2009)
17. Chrysaﬁs, K.A.: Corporate investment appraisal with possibilistic CAPM. Math. Comput.
Modell. 55, 1041–1050 (2012)
18. Collan, M., Fullér, R., Mezei, J.: A fuzzy pay-oﬀmethod for real option valuation. J. Appl.
Math. Decis. Sci. 2009, Article ID 238196 (2009)
19. Coroianu, L., Fullér, R.: On additivity of the weighted possibilistic mean operator. In: Pro-
ceedings of the Fourteenth IEEE International Symposium on Computational Intelligence and
Informatics, November 19–21, pp. 303–308. Budapest, Hungary (2013)
20. Deng, X., Li, R.: A portfolio selection model with borrowing constraint based on possibility
theory. Appl. Soft Comput. 12, 754–758 (2012)
21. Deng, X., Li, R.: Gradually tolerant constraint method for fuzzy portfolio based on possibility
theory. Inf. Sci. 259, 16–24 (2014)
22. Duan, L., Stahlecker, P.: A portfolio selection model using fuzzy returns. Fuzzy Optim. Decis.
Making 10, 167–191 (2011)
23. Dubois, D., Prade, H.: The mean value of a fuzzy number. Fuzzy Sets Syst. 24, 279–300 (1987)
24. Dubois, D., Prade, H.: Possibility Theory: An Approach to Computerized Processing of Uncer-
tainty. Plenum Press, New York (1988)
25. Dubois, D., Prade, H., Sandri, S.: On possibility/probability transformations. In: Lowen, R.,
Roubens, M. (eds.) Fuzzy Logic: State of the Art, pp. 103–112. Dordrecht, Kluwer Academic
Publishers (1993)

272
R. Fullér and I.Á. Harmati
26. Dubois, D.: Possibility theory and statistical reasoning. Comput. Stat. Data Anal. 51, 47–69
(2006)
27. Fang, Y., Lai, K.K., Wang, S.-Y.: Portfolio rebalancing model with transaction costs based on
fuzzy decision theory. Eur. J. Oper. Res. 175, 879–893 (2006)
28. Fullér, R., Majlender, P.: On weighted possibilistic mean and variance of fuzzy numbers. Fuzzy
Sets Syst. 136, 363–374 (2003)
29. Fullér, R., Majlender, P.: On interactive fuzzy numbers. Fuzzy Sets Syst. 143, 355–369 (2004)
30. Fullér, R., Mezei, J., Várlaki, P.: A correlation ratio for possibility distributions. In: Hüller-
meier, E., Kruse, R., Hoﬀmann, F. (eds.) Computational Intelligence for Knowledge-Based
Systems Design, Proceedings of the International Conference on Information Processing and
Management of Uncertainty in Knowledge-Based Systems (IPMU 2010), June 28–July 2,
2010, Dortmund, Germany. Lecture Notes in Artiﬁcial Intelligence, vol. 6178, pp. 178–187.
Springer, Berlin (2010)
31. Fullér, R., Mezei, J., Várlaki, P.: An improved index of interactivity for fuzzy numbers. Fuzzy
Sets Syst. 165, 56–66 (2011)
32. Fullér, R., Harmati, I.Á., Várlaki, P., Rudas, I.: On weighted possibilistic informational coeﬃ-
cient of correlation. Int. J. Math. Models Meth. Appl. Sci. 6, 592–599 (2012)
33. Gładysz, B., Kasperski, A.: Computing mean absolute deviation under uncertainty. Appl. Soft
Comput. 10, 361–366 (2010)
34. Goetschel, R., Voxman, W.: Elementary fuzzy calculus. Fuzzy Sets Syst. 18, 31–43 (1986)
35. Guo, S., Yu, L., Li, X., Kar, S.: Fuzzy multi-period portfolio selection with diﬀerent investment
horizons. Eur. J. Oper. Res. 254, 1026–1035 (2016)
36. Gupta, P., Kumar, M., Mehlawat, A.: Saxena Asset portfolio optimization using fuzzy mathe-
matical programming. Inf. Sci. 178, 1734–1755 (2008)
37. He, Y., et al.: An uncertainty visualization technique using possibility theory: possibilistic
marching cubes. Int. J. Uncertainty Quantiﬁcation 5, 433–451 (2015)
38. Ho, S.-H., Liao, S.-H.: A fuzzy real option approach for investment project valuation. Expert
Syst. Appl. 38, 15296–15302 (2011)
39. Hong, D.H.: Fuzzy measures for a correlation coeﬃcient of fuzzy numbers under TW (the
weakest t-norm)-based fuzzy arithmetic operations. Inf. Sci. 176, 150–160 (2006)
40. Hsieh, W.-Y., Tsaur, R.-C.: Epidemic forecasting with a new fuzzy regression equation. Qual.
Quant. 47, 3411–3422 (2013)
41. Huang, X.: Fuzzy chance-constrained portfolio selection. Appl. Math. Comput. 177, 500–507
(2006)
42. Huang, X.: Mean-semivariance models for fuzzy portfolio selection. J. Comput. Appl. Math.
217, 1–8 (2008)
43. Jaynes, E.T.: Probability Theory: The Logic of Science. Cambridge University Press (2003)
44. Kolmogorov, A.N.: Grundbegriﬀe der Wahrscheinlichkeitsrechnung, p. 62. Julius Springer,
Berlin (1933)
45. Lee, Y.-C., Lee, S.-S.: The valuation of RFID investment using fuzzy real option. Expert Syst.
Appl. 38, 12195–12201 (2011)
46. Li, X., Guo, S., Yu, L.: Skewness of fuzzy numbers and its applications in portfolio selection.
IEEE Trans. Fuzzy Syst. 23, 2135–2143 (2015)
47. Liu, S.T., Kao, C.: Fuzzy measures for correlation coeﬃcient of fuzzy numbers. Fuzzy Sets
Syst. 128, 267–275 (2002)
48. Liu, Y.-J., Zhang, W.-G.: Fuzzy portfolio optimization model under real constraints. Insur.
Math. Econ. 53, 704–711 (2013)
49. Mashayekhi, Z., Omrani, H.: An integrated multi-objective Markowitz-DEA cross-eﬃciency
model with fuzzy returns for portfolio selection problem. Appl. Soft Comput. 38, 1–9 (2016)
50. Merton, R.C.: Option pricing when underlying stock returns are discontinuous. J. Financ. Econ.
3, 125–144 (1976)
51. Nguyen, T.T., Gordon-Brown, L., Khosravi, A., Creighton, D., Nahavandi, S.: Fuzzy portfolio
allocation models through a new risk measure and fuzzy sharpe ratio. IEEE Trans. Fuzzy Syst.
23, 656–676 (2015)

On Possibilistic Dependencies: A Short Survey of Recent Developments
273
52. Pearson, K.: On a new method of determining correlation, when one variable is given by alter-
native and the other by multiple categories. Biometrika 7(3), 248–257 (1910)
53. Rubio, A., Bermdez, J.D., Vercher, E.: Forecasting portfolio returns using weighted fuzzy time
series methods. Int. J. Approx. Reason. 75, 1–12 (2016)
54. Silva, R.C., Verdegay, J.L., Yamakami, A.: Two-phase method to solve fuzzy quadratic pro-
gramming problems. In: IEEE International Fuzzy Systems Conference, vol. 2007, pp. 1–6.
London (2007)
55. Smets, P.: Constructing the pignistic probability function in a context of uncertainty. In: Hen-
rion, M., et al. (eds.) Uncertainty in Artiﬁcial Intelligence, vol. 5, pp. 29–39. North-Holland,
Amsterdam (1990)
56. Thavaneswaran, A., Appadoo, S.S., Frank, J.: Binary option pricing using fuzzy numbers.
Appl. Math. Lett. 26, 65–72 (2013)
57. Vaidyanathan, V.S.: Correlation of triangular fuzzy variables using credibility theory. Int. J.
Comput. Cogn. 8, 21–23 (2010)
58. Wang, D., Li, P., Yasuda, M.: Construction of fuzzy control charts based on weighted possi-
bilistic mean. Commun. Stat. Theory Meth. 43, 3186–3207 (2014)
59. Xu, W., Wu, C., Xu, W., Li, H.: A jump-diﬀusion model for option pricing under fuzzy envi-
ronments. Insur. Math. Econ. 44(200), 337–344
60. Yager, R.R.: A procedure for ordering fuzzy subsets of the unit interval. Inf. Sci. 24, 143–161
(1981)
61. Yoshida, Y., Yasuda, M., Nakagami, J.-I., Kurano, M.: A discrete-time American put option
model with fuzziness of stock prices. Fuzzy Optim. Decis. Making 4, 191–207 (2005)
62. Yoshida, Y.: An estimation model of value-at-risk portfolio under uncertainty. Fuzzy Sets Syst.
160, 3250–3262 (2009)
63. Zadeh, L.A.: Concept of a linguistic variable and its application to approximate reasoning, I,
II, III. Inf. Sci. 8(1975) 199–249, 301–357; 9(1975) 43–80
64. Zhang, W.-G., Wang, Y.-L., Chen, Z.-P., Nie, Z.-K.: Possibilistic mean-variance models and
eﬃcient frontiers for portfolio selection problem. Inf. Sci. 177, 2787–2801 (2007)
65. Zhang, W.-G., Wang, Y.-L.: A comparative analysis of possibilistic variances and covariances
of fuzzy numbers. Fundamenta Informaticae 79, 257–263 (2007)
66. Zhang, W.-G., Zhang, X.-L., Xiao, W.-L.: Portfolio selection under possibilistic mean-variance
utility and a SMO algorithm. Eur. J. Oper. Res. 197, 693–700 (2009)
67. Zhang, W.-G., Xiao, W.-L., Xu, W.-J.: A possibilistic portfolio adjusting model with new added
assets. Econ. Modell. 2, 208–213 (2010)
68. Zhang, W.-G., Liu, Y.-J., Xu, W.-J.: A possibilistic mean-semivariance-entropy model for
multi-period portfolio selection with transaction costs. Eur. J. Oper. Res. 222, 341–349 (2012)
69. Zhang, P.: Multi-period possibilistic mean semivariance portfolio selection with cardinality
constraints and its algorithm. J. Math. Modell. Algorithms Oper. Res. 14, 239–253 (2015)

Penalty Function in Optimization Problems:
A Review of Recent Developments
Humberto Bustince, Javier Fernandez and Pedro Burillo
Abstract In this chapter we make a brief revision of some recent developments
on the notion of penalty function as a tool for the fusion of information, including
the most recently published deﬁnition as well as the extension of the concept to the
lattice setting.
1
Introduction
Penalty functions provide a method to determine up to what extent a given output is
similar (or dissimilar) to a set of inputs. This information can be used, by means of
an appropriate minimization procedure, to deﬁne a function (the so-called penalty-
based function) for fusing the considered inputs. In this sense, it is a powerful tool
for those applications where fusion of information is crucial, as it is the case for
almost every real-world application. For this reason, and since it was ﬁrst suggested
by Yager in 1993 [18], the notion of penalty and penalty-based functions have been
gaining an increasing interest among the scientiﬁc community, specially in order to
overcome the diﬀerent technical diﬃculties which arise to properly deﬁne such a
class of functions, see [6] for a complete discussion.
In this chapter, and basing ourselves in [6], we make a brief review of the evolution
of the ideas of penalty and penalty-based functions, from its origin in the works of
Yager up to the last deﬁnition in the literature, which has ﬁnally encompassed all the
desired properties for such functions. Furthermore, we also discuss brieﬂy how this
concept can also be extended to more general setting, considering in particular the
case of a Cartesian product of lattices, which is of interest in applications such as
image processing [3, 8].
H. Bustince (✉) ⋅J. Fernandez ⋅P. Burillo
Departamento de Automatica y Computacion, Universidad Publica de Navarra,
Campus Arrosadia s/n, 31006 Pamplona, Spain
e-mail: bustince@unavarra.es
J. Fernandez
e-mail: fcojavier.fernandez@unavarra.es
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_17
275

276
H. Bustince et al.
The structure of this chapter is as follows. First we discuss some preliminary
notions and results, and, in Sect. 3, we make the review of the evolution of the idea
of penalty functions and penalty-based function. In Sect. 4, we provide an idea of
how a similar concept can be deﬁned in a Cartesian product of lattices. We ﬁnish
with some conclusions and references.
2
Preliminaries
We denote by 𝕀a closed subinterval of the extended real line, i.e., 𝕀= [a, b] ⊆ℝ.
The notion of penalty-function relies on the ideas of convexity and quasi-
convexity, that we recall now.
Deﬁnition 1 A function f ∶𝕀→ℝis convex if for every x, y ∈𝕀and for every 𝜆∈
[0, 1] the inequality
f(𝜆x + (1 −𝜆)y) ≤𝜆f(x) + (1 −𝜆)f(y)
holds.
Deﬁnition 2 A function f ∶𝕀→ℝis quasi-convex if for every x, y ∈𝕀and for every
𝜆∈[0, 1] the inequality
f(𝜆x + (1 −𝜆)y) ≤max{f(x), f(y)}
holds.
Quasi-convex functions are relevant for us due to the following result about min-
imization [15]:
Proposition 1 Let f ∶𝕀→ℝbe a quasi-convex function. Then, the set of minimizers
of f is either empty or a connected set.
We also recall here the idea of lower semicontinuity.
Deﬁnition 3 A function f ∶𝕀→ℝis lower semicontinuous at x0 ∈𝕀if
lim inf
x→x0 f(x) ≥f(x0).
Analogously one deﬁnes upper semicontinuity. Observe that a function is contin-
uous at x0 ∈𝕀if and only if it is upper and lower semicontinuous there.
Again, lower semicontinuity is important for us because, in a compact domain,
the set of minimizers of a lower semicontinuous function is not empty.
Proposition 2 Let f ∶𝕀→ℝbe a lower semicontinuous function, with 𝕀bounded.
Then, the set of minimizers of f is not empty.

Penalty Function in Optimization Problems: A Review of Recent Developments
277
We ﬁnally review the notions related to aggregation functions. We take, from now
on, n ≥2.
Deﬁnition 4 [2, 12] A function A ∶[0, 1]n →[0, 1] is said to be an n-ary aggrega-
tion function if:
(A1)
A is increasing in each argument: for each i ∈{1, … , n}, if xi ≤y, then
A(x1, … , xn) ≤A(x1, … , xi−1, y, xi+1, … , xn);
(A2)
A satisﬁes the boundary conditions: A(0, … , 0) = 0 and A(1, … , 1) = 1.
It is well known that an aggregation function f ∶[0, 1]n →[0, 1] is called averag-
ing if, for all (x1, … , xn) ∈[0, 1]n, it holds that:
min{x1, … , xn} ≤f(x1, … , xn) ≤max{x1, … , xn}.
In particular, an aggregation function is averaging if and only if it is idempotent,
i.e., f(x, … , x) = x for every x ∈[0, 1].
Finally, and for the sake of completeness, we recall that a fuzzy set A on an uni-
verse U is a mapping A ∶U →[0, 1]. The value A(u) is called membership degree
of the element u. We will denote by FS (U) the class of all fuzzy sets deﬁned on
the referential U.
3
The Evolution of the Idea of Penalty Functions
The ﬁrst approaches to the notion of penalty function in information aggregation
procedures may be traced back to Yager [18]. A few years later, in 1997, Yager and
Rybalov [19] considered the possibility of obtaining an appropriate function in order
to fuse information by means of a minimization procedure. The information to be
fused may even be of diﬀerent nature. Formally, and considering only numerical
dates, their proposal was the following:
Deﬁnition 5 The function LP ∶ℝ2 →ℝ+ is said to be a local penalty function if,
for any xi, xj, y ∈ℝand i, j = 1, … , n, it satisﬁes:
(LP3.1-1)
LP(xi, y) = 0, if xi = y;
(LP3.1-2)
LP(xi, y) > 0, if xi ≠y;
(LP3.1-3)
LP(xi, y) ≥LP(xj, y), if ∣xi −y ∣>∣xj −y ∣,
where y is called the fused value related to each observation in 𝐱.
Deﬁnition 6 Let LP ∶ℝ2 →ℝ+ be a local penalty function. A penalty function P ∶
ℝn+1 →ℝ+ is deﬁned, for any 𝐱∈ℝn and y ∈ℝas:
P(𝐱, y) =
n
∑
i=1
LP(xi, y),
(1)

278
H. Bustince et al.
where y ∈ℝis called the fused value of 𝐱∈ℝn.
The main problem with this approach is that the minimizer y∗may not exist or
may be not unique [19]
The following step in this step was done by Calvo et al. [10], proposing the fol-
lowing deﬁnition:
Deﬁnition 7 The function LP ∶𝕀2 →[0, ∞] is a local penalty function on 𝕀if and
only if, for any x, y ∈𝕀, it satisﬁes:
(LP3.4-1)
LP(x, y) = 0 if x = y, and
(LP3.4-2)
LP(x, y) ≥LP(z, y), whenever x ≥z ≥y or x ≤z ≤y.
By considering local penalty functions such that
LP = K ◦f, that is, LP(x, y) = K(f(x), f(y)),
(2)
where f ∶𝕀→[−∞, +∞] is a continuous strictly monotonic function and
K ∶[−∞, +∞]2 →[0, ∞] is a local penalty function, which is convex in each com-
ponent, Calvo et al. [10] avoided the problem of non-existence of minimizers. This
kind of functions were called faithful local penalty functions, and lad to the following
deﬁnition of penalty function.
Deﬁnition 8 Let LP ∶𝕀2 →[0, ∞] be a faithful local penalty function. A function
fP ∶⋃
n∈N 𝕀n →𝕀, deﬁned for all 𝐱∈𝕀n and n ∈N, by
fP(𝐱) = l𝐱+ r𝐱
2
,
where
l𝐱= inf{u ∈𝕀∣∀v ∈𝕀∶P(𝐱, u) ≤P(𝐱, v)}
r𝐱= sup{u ∈𝕀∣∀v ∈𝕀∶P(𝐱, u) ≤P(𝐱, v)}
is called a penalty-based function, or P-function, for short.
Nevertheless, the next step was to try to recover most relevant aggregation func-
tions, and in this sense, the deﬁnition provided by Calvo et al. did not allow to recover
idempotent (and hence, averaging) aggregation functions.
This study led to Calvo and Beliakov to propose, in [9], the following deﬁnition
of a penalty function:
Deﬁnition 9 The function P ∶𝕀n+1 →[0, ∞] is a penalty function if and only if it
satisﬁes:
(P3.6-1)
P(𝐱, y) ≥0, for all 𝐱,y;

Penalty Function in Optimization Problems: A Review of Recent Developments
279
(P3.6-2)
P(𝐱, y) = 0 if 𝐱= 𝐲, and1
(P3.6-3)
For every ﬁxed 𝐱, the set of minimizers of P(𝐱, y) is either a singleton or
an interval.
Deﬁnition 10 A penalty-based function f ∶𝕀n →𝕀is deﬁned, for all 𝐱∈Xn, by
f(𝐱) = arg min
y P(𝐱, y),
(3)
if y is the unique minimizer, and y = a+b
2 if the set of minimizers is the interval ]a, b[
(or [a, b]).
We use P-function to shorten the expression penalty function.
Remark 1 In Deﬁnition 9, observe that:
1. The condition (P3.6-1) is redundant, since this is a consequence of the fact that
the range of the function P is [0, ∞].
2. A singleton can be seen as a degenerated interval {k} = [k, k].
Deﬁnition 9 presents some problems identiﬁed below.
Furthermore, in [1], Beliakov and James considered the problem of aggregating
some special kind of discontinuous intervals, called non-convex intervals, and pro-
pose the following deﬁnition:
Deﬁnition 11 The function P ∶[0, 1]n+1 →[0, ∞] is a penalty function if and only
if it satisﬁes:
(P3.8-1)
P(𝐱, y) = 0 if xi = y, for all i;
(P3.8-2)
P(𝐱, y) > 0 if xi ≠y for some i, and
(P3.8-3)
For every ﬁxed 𝐱, the set of minimizers of P(𝐱, y) is either a singleton or
an interval.
The concept of penalty-based function, for a penalty function in the sense of Def-
inition 11, is deﬁned analogously to Deﬁnition 10. It is worth to mention that Deﬁ-
nitions 9 and 11 are not equivalent.
Furthermore, the restriction of the domain to [0, 1]n+1 is not very signiﬁcant, since
the results can be extended to any other bounded interval in a straightforward way.
With this new deﬁnition, many averaging functions that are P-functions. For
example, the arithmetic mean AM can be generated from
P(𝐱, y) =
n∑
i=1
(xi −y)2
n
,
1Observe that the vector 𝐲should be deﬁned as 𝐲= (y, … , y
⏟⏟⏟
n times
), although the authors had not men-
tioned that in [9].

280
H. Bustince et al.
which is a penalty function in the sense of Deﬁnition 11.
But, in fact, it not yet possible to recover every aggregation function as a penalty-
based function. So a further modiﬁcation of the deﬁnition is required in order to get
such a result. This step was done in [6], obtaining the following result:
Theorem 1 A function f ∶[0, 1]n →[0, 1] is a P-function, for a penalty function in
the sense of Deﬁnition 11, if and only if f is idempotent.
It follows that each averaging aggregation function is a P-function, for a penalty
function in the sense of Deﬁnition 11. But it is worth mentioning that the proof of
Theorem 1 uses the same argument as that in [9], but with an extra term, which
produces a discontinuity for the penalty function.
Regarding monotonicity issues, Wilkin and Beliakov [16] considered also the
use of weakly monotonic aggregation functions (see also [7, 13]). In this way, non-
monotone operators, such as the mode, can be recovered.
Deﬁnition 12 For any closed, nonempty interval 𝕀⊆[−∞, +∞], the function P ∶
𝕀n+1 →ℝis a penalty function if and only if it satisﬁes:
(P3.9-1)
P(𝐱, y) ≥c, for all 𝐱∈𝕀n,y ∈𝕀, for some constant c ∈ℝ;
(P3.9-2)
P(𝐱, y) = c if and only if all xi = y, for all i = 1 … n, and
(P3.9-3)
P is quasi-convex in y for any 𝐱.
All the previous discussed deﬁnitions, however, share diﬀerent diﬃculties which
made its use for applications complicate. For this reason, in [ ], a new deﬁnition of
penalty functions encompassing most of the advantages of the discussed ones and
all the desired properties of these functions was proposed.
Let 𝕀⊆ℝbe a closed interval.
Deﬁnition 13 The function P ∶𝕀n+1 →ℝ+ is a penalty function if and only if there
exists c ∈ℝ+ such that:
(P4.1-1)
P(𝐱, y) ≥c, for all 𝐱∈𝕀n,y ∈𝕀;
(P4.1-2)
P(𝐱, y) = c if and only if xi = y, for all i = 1 … n, and
(P4.1-3)
P is quasi-convex lower semi-continuous in y for each 𝐱∈𝕀n.
Deﬁnition 14 Let f ∶𝕀n →𝕀be a function and P be a penalty function in the sense
of Deﬁnition 13. Then f is said to be a P-function if, for each 𝐱∈𝕀n, one has that
f(𝐱) = a + b
2
(4)
where
[a, b] = cl(Minz(P(𝐱, ⋅)))
and Minz(P(𝐱, ⋅)) is the set of minimizers of P(𝐱, ⋅), that is:
Minz(P(𝐱, ⋅)) = {y ∈𝕀∣P(𝐱, y) ≤P(𝐱, z), for each z ∈𝕀},
(5)
and cl(S) is the closure of S ⊆𝕀.

Penalty Function in Optimization Problems: A Review of Recent Developments
281
Note that the requirement of quasi convexity and lower semicontinuity ensure that
the set of minimizers of P(𝐱, ⋅) is either a singleton or an interval. In particular, we
have now the following result [6]:
Theorem 2 A function f ∶𝕀n →𝕀is a P-function in the sense of Deﬁnition 14 if and
only if f is idempotent.
It is also worth to say that it is always possible to get a continuous penalty function
Pf , for an idempotent function f.
So, ﬁnally, we have the following result.
Proposition 3 Let A ∶𝕀n →𝕀be an increasing function. A is an averaging aggre-
gation function if and only if A is a P-function in the sense of Deﬁnition 14.
4
Penalty Functions in a Cartesian Product of Lattices
In the same way as in the real case averaging functions may be constructed by means
of penalty functions, an analogous construction can be done for general lattices. In
particular, in [3, 8] this construction inpires itsels on the notion faithful penalty func-
tions and it is done using as a ﬁrst step restricted dissimilarity functions [4, 5]
Deﬁnition 15 [5] A mapping dR ∶[0, 1]2 →[0, 1] is a restricted dissimilarity func-
tion if:
1. dR(x, y) = dR(y, x) for every x, y ∈[0, 1];
2. dR(x, y) = 1 if and only if x = 0 and y = 1 or x = 1 and y = 0; that is, {x, y} =
{0, 1};
3. dR(x, y) = 0 if and only if x = y;
4. For any x, y, z ∈[0, 1], if x ≤y ≤z, then dR(x, y) ≤dR(x, z) and dR(y, z) ≤dR(x, z).
Note that distances between fuzzy sets can be deﬁned in terms of restricted dis-
similarity functions.
Deﬁnition 16 [17] A mapping D ∶FS (U)2 →[0, 1] is a distance over FS (U)
if
1. D(A, B) = D(B, A) for every A, B ∈FS (U);
2. D(A, B) = 0 if and only if A = B;
3. D(A, B) = 1 if and only if A and B are complementary crisp sets;
4. if A ≤A′ ≤B′ ≤B, then D(A, B) ≥D(A′, B′).
Along this section, and to avoid possible confusion, we will denote by the letter
M an aggregation function.
Theorem 3 [5] Let M be an aggregation function such that it satisﬁes
(A1)
M(x1, … , xn) = 1 if and only if x1 = ⋯= xn = 1;
(A2)
M(x1, … , xn) = 0 if and only if x1 = ⋯= xn = 0,

282
H. Bustince et al.
and let dR ∶[0, 1]2 →[0, 1] be a restricted dissimilarity function. Then
D(A, B) =
n
M
i=1(dR(A(ui), B(ui)))
for all A, B ∈FS (U) deﬁnes a distance in the sense of Liu.
In this paper, whenever we speak of distances between fuzzy sets, we mean dis-
tances in the sense of Liu [17].
From the point of view of application it is enough to consider as lattices the
Cartesian product of ﬁnite chains C . Moreover, and since all the ﬁnite chains
of the same length are isomorphic to each other, we can always assume that we
are working with chains of the type C = 0 ≤1 ≤2 ≤⋯≤n −1. Recall that, if
Lk = {C1 × ⋯× Ck, ≤, ∧, ∨} and a, b ∈Lk such that a ≤b, every maximal chain
joining a and b has the same length.
So the distance between x, y ∈L can be deﬁned as the length of the chain C with
minimal element a = ∧(x, y) and maximal element b = ∨(x, y), minus one. That is,
d(x, y) = length(C ) −1.
This deﬁnition is equivalent to the following.
d(x, y) =
m
∑
i=1
di(xi, yi) =
m
∑
i=1
|xi −yi|
(6)
where di is the distance in the i-th chain. It is easy to see that Eq. (6) is a distance,
called natural distance.
We restrict to chains with supremum and inﬁmum. We need to extend the deﬁni-
tion of distance to L-fuzzy sets using the notion of restricted dissimilatity function.
Consider the lattice Lm = {C1 × ⋯× Cm, ≤, ∧, ∨}. For each chain Ci we denote
by ∨(Ci) and ∧(Ci) its top and bottom elemnts, respectively. We also denote
1Lm = (∨(C1), … , ∨(Cm)),
0Lm = (∧(C1), … , ∧(Cm)).
Deﬁnition 17 Take Lm = {C1 × ⋯× Cm, ≤, ∧, ∨}. A mapping
𝛿R ∶Lm × Lm →Lm
is a lattice restricted dissimilarity function if
1. 𝛿R(x, y) = 𝛿R(y, x) for any x, y ∈Lm;
2. 𝛿R(x, y) = 1Lm if and only if for any i = 1, … , m,

Penalty Function in Optimization Problems: A Review of Recent Developments
283
xi = ∨(Ci) and yi = ∧(Ci),
or
xi = ∧(Ci) and yi = ∨(Ci);
3. 𝛿R(x, y) = 0Lm if and only if x = y;
4. If x ≤y ≤z then 𝛿R(x, y) ≤𝛿R(x, z) and 𝛿R(y, z) ≤𝛿R(x, z).
Proposition 4 Let each 𝛿Ri ∶Ci × Ci →Ci be a lattice restricted dissimilarity func-
tion. Then the mapping deﬁned as
𝛿R(x, y) = (𝛿R1(x1, y1), … , 𝛿Rm(xm, ym))
(7)
for every x, y ∈Lm is a lattice restricted dissimilarity function.
Let FS (U)m denote the class A = (A1, … , Am) with Ai ∶U →Ci such that
A(ui) = (A1(ui), … , Am(ui)) for every ui ∈U. Notice that each of the Ai is an L-
fuzzy set in the sense of Goguen [11]; i.e., each Ai is a fuzzy set deﬁned over the
lattice {Ci, ≤i, ∧i, ∨i}.
Deﬁnition 18 Take Lm = {C1 × ⋯× Cm, ≤, ∧, ∨}. A mapping
𝛺∶FS (U)m × FS (U)m →Lm
is a lattice distance in FS (U)m if
1. 𝛺(A, B) = 𝛺(B, A) for every A, B ∈FS (U)m;
2. 𝛺(A, B) = 0Lm if and only if Ai = Bi for every i = 1, … , m;
3. 𝛺(A, B) = 1Lm if and only if for every i = 1, … , m, Ai and Bi are sets such that
for every uj
Ai(uj) = ∨(Ci) and Bi(uj) = ∧(Ci)
or
Ai(uj) = ∧(Ci) and Bi(uj) = ∨(Ci);
4. If A ≤A′ ≤B′ ≤B, then 𝛺(A, B) ≥𝛺(A′, B′) where A = (A1, … , Am) ≤
(A′
1, … , A′
m) = A′ if Ai ≤A′
i for every i.
we have the nest straight result.
Proposition 5 Let M1, … , Mm be aggregation functions
Mi ∶Ci × Ci →Ci

284
H. Bustince et al.
Then the mapping
F ∶Lm × Lm →Lm given by
F(x, y) = (M1(x1, y1), … , Mm(xm, ym))
is an aggregation function over Lm.
Now we can introduce a method to build lattice distances.
Proposition 6 Let 𝛿R1, … , 𝛿Rm be lattice restricted dissimilarity functions 𝛿Ri ∶Ci ×
Ci →Ci. Let M1, … , Mm be aggregation functions Mi ∶Ci × ⋯× Ci →Ci such that
(L1)
Mi(x1, … , xn) = 1L if and only if xi = ∨(Ci) for every i = 1, … , n
(L2)
Mi(x1, … , xn) = 0L if and only if xi = ∧(Ci) for every i = 1, … , n
Then
𝛺(A, B) =
( n
M1
i=1
(𝛿R1(A1(ui), B1(ui))), … ,
n
Mm
i=1
(𝛿Rm(Am(ui), Bm(ui)))
)
(8)
deﬁnes a lattice distance in FS (U)m.
We know that the arithmetic mean of convex functions is also a convex function.
Next, we consider aggregation functions such that applied to convex functions we
obtain another convex function, as in the arithmetic mean case.
Theorem 4 Let Y = (y1, … , ym) ∈Lm. For each yi (i = 1, … , m) we consider the
set
Byi(uj) = yi for all uj ∈U
(9)
and let BY = (By1, … , Bym) ∈FS (U)m. Let M1, … , Mm be aggregation functions
Mi ∶Ci × ⋯× Ci →Ci such that each of them when composed with convex func-
tions is also convex. Take the lattice restricted dissimilarity function 𝛿R(x, y) =
(𝛿R1(x1, y1), … , 𝛿Rm(xm, ym)) such that each 𝛿Ri with i = 1, … , m is convex in one
variable. Then
P𝛺∶FS (U)m+1 →Lm given by
P𝛺(A, Y) = 𝛺(A, BY) =
( n
M1
i=1
(𝛿R1(A1(ui), y1)), … ,
n
Mm
i=1
(𝛿Rm(Am(ui), ym))
)
(10)
satisﬁes:
1. P𝛺(A, Y) ≥0Lm;
2. P𝛺(A, Y) = 0Lm if Ak(uj) = yk for every k and for every j;
3. Each of its components is convex with respect to the corresponding yk (k =
1, … , m).

Penalty Function in Optimization Problems: A Review of Recent Developments
285
Analogously to the case of (see [10, 14]), we introduce the deﬁnition of lattice
faithful restricted dissimilarity functions:
𝛿R(x, y) = K(d(x, y)) = K(
m
∑
i=1
|xi −yi|)
(11)
with K ∶C →C a convex with a unique minimum at K(0) = 0.
Theorem 5 In the setting of Theorem 4, if 𝛿R1, … , 𝛿Rm are lattice faithful restricted
dissimilarity functions, then the mapping
FLm ∶FS (U)m →Lm given by
FLm(A) = arg min
Y P𝛺(A, Y) = arg min
Y 𝛺(A, BY)
=
(
arg min
y1 (
n
M1
i=1
(K1(d(A1(ui), y1))), … , arg min
ym (
n
Mm
i=1
(Km(d(Am(ui), ym)))
)
=
(
arg min
y1 (
n
M
i=1(K1(|A1(ui) −y1|))), … , arg min
ym (
n
M
i=1(Km(|Am(ui) −ym|)))
)
is such that each of its components is an averaging aggregation function over
FS (U) and FLm(A) is an averaging aggregation function over the Cartesian prod-
uct FS (U)m.
From now on we will denote by Byq the fuzzy set over U such that all its mem-
bership values are equal to yq ∈[0, 1]; that is, Byq(ui) = yq ∈[0, 1] for all ui ∈U.
Let Y = (y1, … , ym) and BY = (By1, … , Bym) ∈FS (U)m. We will denote by C ∗
a chain whose elements belong to [0, 1] and by L ∗
m the product such that L ∗
m =
C ∗× ⋯× C ∗.
Finally, the penalty functions can be obtained as follows.
Theorem 6 Let Ki ∶ℝ→ℝ+ be convex functions with a unique minimum at Ki(0) =
0 (i = 1, … , m), and take the distance between fuzzy sets deﬁned as
D(A, B) =
n
∑
i=1
|A(ui) −B(ui)|
(12)
where A, B ∈FS (U) and Cardinal(U) = n. Then the mapping
P∇∶FS (U)m × L ∗
m →ℝ+ given by
P∇(A, Y) =
m
∑
q=1
Kq(D(Aq, Byq)) =
m
∑
q=1
Kq
( n
∑
p=1
|Aq(up) −yq|
)
(13)

286
H. Bustince et al.
satisﬁes
1. P∇(A, Y) ≥0;
2. P∇(A, Y) = 0 if and only if Aq = yq for every q = 1, … , m;
3. is convex in yq for every q = 1, … , m.
Observe that P∇is a penalty function deﬁned over the Cartesian product of lattices
L ∗n+1
m
.
Theorem 7 In the setting of Theorem 6, the mapping
F(A) = 𝜇= arg min
Y P∇(A, Y)
(14)
where 𝜇is the rounding to the smallest closest element, is an averaging aggregation
function.
Penalty functions on Cartesian product of lattices have shown themselves very
useful in decision making and consensus, see [3, 8].
5
Conclusions
In this chapter we have made a revision of the ideas of penalty and penalty-based
functions. We have also discussed how such notions can be extended to deal with
data in Cartesian products of lattices.
Acknowledgements This work was supported by research project TIN2016-77356-P(AEI/FEDER,
UE)
References
1. Beliakov, G., James, S.: A penalty-based aggregation operator for non-convex intervals.
Knowl.-Based Syst. 70, 335–344 (2014)
2. Beliakov, G., Pradera, A., Calvo, T.: Aggregation Functions: A Guide for Practitioners.
Springer, Berlin (2007)
3. Bustince, H., Barrenechea, E., Calvo, T., James, S., Beliakov, G.: Consensus in multi-expert
decision making problems using penalty functions deﬁned over a cartesian product of lattices.
Inf. Fusion 17, 56–64 (2014)
4. Bustince, H., Barrenechea, E., Pagola, M.: Image thresholding using restricted equivalence
functions and maximizing the measures of similarity. Fuzzy Sets Syst. 158(5), 496–516 (2007)
5. Bustince, H., Barrenechea, E., Pagola, M.: Relationship between restricted dissimilarity func-
tions, restricted equivalence functions and normal en-functions: Image thresholding invariant.
Pattern Recogn. Lett. 29(4), 525–536 (2008)
6. Bustince, H., Beliakov, G., Dimuro, G.P., Bedregal, B., Mesiar, R.: On the deﬁnition of penalty
functions in data aggregation. Fuzzy Sets Syst. (2016)

Penalty Function in Optimization Problems: A Review of Recent Developments
287
7. Bustince, H., Fernandez, J., Kolesárová, A., Mesiar, R.: Directional monotonicity of fusion
functions. Eur. J. Oper. Res. 244(1), 300–308 (2015)
8. Bustince, H., Jurio, A., Pradera, A., Mesiar, R., Beliakov, G.: Generalization of the weighted
voting method using penalty functions constructed via faithful restricted dissimilarity func-
tions. Eur. J. Oper. Res. 225(3), 472–478 (2013)
9. Calvo, T., Beliakov, G.: Aggregation functions based on penalties. Fuzzy Sets Syst. 161(10),
1420–1436 (2010)
10. Calvo, T., Mesiar, R., Yager, R.R.: Quantitative weights and aggregation. IEEE Trans. Fuzzy
Syst. 12(1), 62–69 (2004)
11. Goguen, J.: L-fuzzy sets. J. Math. Anal. Appl. 18(4), 145–174 (1967)
12. Grabisch, M., Marichal, J., Mesiar, R., Pap, E.: Aggregation Functions. Cambridge University
Press, Cambridge (2009)
13. Lucca, G., Sanz, J., Dimuro, G., Bedregal, B., Mesiar, R., Kolesárová, A., Bustince, H.: Pre-
aggregation functions: construction and an application. IEEE Trans. Fuzzy Syst. (2015)
14. Mesiar, R.: Fuzzy set approach to the utility, preference relations, and aggregation operators.
Eur. J. Oper. Res. 176, 414–422 (2007)
15. Rockafellar, R.T.: Convex Analysis. Princeton University Press, Princeton (1970)
16. Wilkin, T., Beliakov, G.: Weakly monotonic averaging functions. Int. J. Intell. Syst. 30(2),
144–169 (2015)
17. Xuecheng, L.: Entropy, distance measure and similarity measure of fuzzy sets and their rela-
tions. Fuzzy Sets Syst. 52, 305–318 (1992)
18. Yager, R.R.: Toward a general theory of information aggregation. Inf. Sci. 68(3), 191–206
(1993)
19. Yager, R.R., Rybalov, A.: Understanding the median as a fusion operator. Int. J. Gen. Syst.
26(3), 239–263 (1997)

The Single Parameter Family of Gini
Bonferroni Welfare Functions and the
Binomial Decomposition, Transfer Sensitivity
and Positional Transfer Sensitivity
Silvia Bortot, Mario Fedrizzi, Ricardo Alberto Marques Pereira
and Anastasia Stamatopoulou
Abstract We consider the binomial decomposition of generalized Gini welfare
functions in terms of the binomial welfare functions Cj, j = 1, … , n and we exam-
ine the weighting structure of the latter, which progressively focus on the poorest
part of the population. In relation with the generalized Gini welfare functions, we
introduce measures of transfer sensitivity and positional transfer sensitivity and we
illustrate the behaviour of the binomial welfare functions Cj, j = 1, … , n with respect
to these measures. We investigate the binomial decomposition of the Gini Bonferroni
welfare functions and we illustrate the dependence of the binomial decomposition
coeﬃcients in relation with the single parameter which describes the family. More-
over we examine the family of Gini Bonferroni welfare functions with respect to the
transfer sensitivity and positional transfer sensitivity principles.
Keywords
Generalized Gini welfare functions ⋅Binomial decomposition ⋅Single
parameter family of Gini Bonferroni welfare functions ⋅Principle of transfer sensi-
tivity ⋅Principle of positional transfer sensitivity
1
Introduction
The study of welfare and inequality has been the research interest of many economi-
cal and social scientists, and has been understood as an investigation on the departure
from the ideal situation of economic equalitarianism, where each individual of the
population has an equal share of the total income. In this sense, diﬀerent welfare
and inequality measures, with diﬀerent characteristics, have been introduced in the
literature in order to express the fairness of the income distribution in society.
S. Bortot(✉) ⋅R.A. Marques Pereira ⋅A. Stamatopoulou
Department of Economics and Management, University of Trento,
Via Inama, 5, 38122 Trento, Italy
e-mail: silvia.bortot@unitn.it
M. Fedrizzi
Department of Industrial Engineering, University of Trento,
Via Sommarive, 9, 38123 Trento, Italy
© Springer International Publishing AG 2018
D.A. Pelta and C. Cruz Corona (eds.), Soft Computing Based Optimization
and Decision Models, Studies in Fuzziness and Soft Computing 360,
DOI 10.1007/978-3-319-64286-4_18
289

290
S. Bortot et al.
The generalized Gini welfare functions introduced by Weymark [60], and the
associated inequality indices in Atkinson-Kolm-Sen’s (AKS) framework, see Atkin-
son [5], Kolm [48, 49], and Sen [55], are related by Blackorby and Donaldson’s
correspondence formula [13, 15], A(x) = ̄x −G(x), where A(x) denotes a general-
ized Gini welfare function, G(x) is the associated absolute inequality index, and ̄x is
the plain mean of the income distribution x = (x1, … , xn) ∈𝔻n of a population of
n ≥2 individuals, with income domain 𝔻= [0, ∞).
The generalized Gini welfare functions [60] have the form A(x) = ∑n
i=1wi x(i)
where x(1) ≤x(2) ≤⋯≤x(n) and, as required by the principle of inequality aversion,
w1 ≥w2 ≥⋯≥wn ≥0 with ∑n
i=1 wi = 1. These welfare functions correspond to
a particular class of the ordered weighted averaging (OWA) functions introduced by
Yager [63], which in turn correspond [34] to the Choquet integrals associated with
symmetric capacities.
In this paper we recall the binomial decomposition of generalized Gini welfare
functions due to Calvo and De Baets [22], see also Bortot and Pereira [20]. The
binomial decomposition is formulated in terms of the functional basis formed by the
binomial welfare functions.
The binomial welfare functions, denoted Cj with j = 1, … , n, have null weights
associated with the j −1 richest individuals in the population and therefore they are
progressively focused on the poorest sector of the population.
The paper is organized as follows. In Sect. 2 we review the notions of general-
ized Gini welfare function and associated generalized Gini inequality index, and we
introduce general measures of transfer sensitivity and positional transfer sensitivity.
In Sect. 3 we brieﬂy review the Gini and Bonferroni welfare functions and inequal-
ity indices, and we examine them with respect to the principles of transfer sensitivity
and positional transfer sensitivity.
In Sect. 4 we consider the binomial decomposition of generalized Gini welfare
functions in terms of the binomial welfare functions Cj, j = 1, … , n. We illustrate the
weights of the binomial welfare functions Cj, j = 1, … , n, which progressively focus
on the poorest sector of the population, and we examine their transfer sensitivity and
positional transfer sensitivity properties.
Finally, in Sect. 5 we investigate the Gini Bonferroni welfare functions with para-
meter 𝛾∈[0, 1], particularly in the context of the binomial decomposition. More-
over, we illustrate the weighting structure of the Gini Bonferroni welfare functions
and we study their measures of transfer sensitivity and positional transfer sensitivity
in terms of the parameter 𝛾∈[0, 1]. Section 6 contains some conclusive remarks.
2
Generalized Gini Welfare Functions and Inequality
Indices
In this section we consider populations of n ≥2 individuals and we brieﬂy review the
notions of generalized Gini welfare function and generalized Gini inequality index
over the income domain 𝔻= [0, ∞). The income distributions in this framework are

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
291
represented by points x, y ∈𝔻n. We introduce general measures of transfer sensitiv-
ity and positional transfer sensitivity.
We begin by presenting notation and basic deﬁnitions regarding averaging func-
tions on the domain 𝔻n, with n ≥2 throughout the text. Comprehensive reviews of
averaging functions can be found in Chisini [27], Fodor and Roubens [35], Calvo et
al. [23], Beliakov et al. [10], Grabisch et al. [46], and Beliakov et al. [9].
Notation. Points in 𝔻n are denoted x = (x1, … , xn), with 1 = (1, … , 1), 0 =
(0, … , 0) . Accordingly, for every x ∈𝔻, we have x⋅1 = (x, … , x). Given x, y ∈𝔻n,
by x ≥y we mean xi ≥yi for every i = 1, … , n, and by x > y we mean x ≥y and
x ≠y. Given x ∈𝔻n, the increasing and decreasing reorderings of the coordinates
of x are indicated as x(1) ≤⋯≤x(n) and x[1] ≥⋯≥x[n], respectively. In particular,
x(1) = min{x1, … , xn} = x[n] and x(n) = max{x1, … , xn} = x[1] . In general, given a
permutation 𝜎on {1, … , n}, we denote x𝜎= (x𝜎(1), … , x𝜎(n)). Finally, the arithmetic
mean is denoted ̄x = (x1 + ⋯+ xn)∕n.
Deﬁnition 1 Let A ∶𝔻n ⟶𝔻be a function. We say that
1. A is monotonic if x ≥y
⇒
A(x) ≥A(y), for all x, y ∈𝔻n. Moreover, A is
strictly monotonic if x > y ⇒A(x) > A(y), for all x, y ∈𝔻n.
2. A is idempotent if A(x ⋅1) = x, for all x ∈𝔻. On the other hand, A is nilpotent if
A(x ⋅1) = 0, for all x ∈𝔻.
3. A is symmetric if A(x𝜎) = A(x), for any permutation 𝜎on {1, … , n} and all
x ∈𝔻n.
4. A is invariant for translations if A(x + t ⋅1) = A(x), for all t ∈𝔻and x ∈𝔻n.
On the other hand, A is stable for translations if A(x + t ⋅1) = A(x) + t, for all
t ∈𝔻and x ∈𝔻n.
5. A is invariant for dilations if A(t ⋅x) = A(x), for all t ∈𝔻and x ∈𝔻n. On the
other hand, A is stable for dilations if A(t ⋅x) = t A(x), for all t ∈𝔻and x ∈𝔻n.
The terms positive (negative), increasing (decreasing), and monotonic are used in
the weak sense. Otherwise these properties are said to be strict.
We introduce the majorization relation on 𝔻n and we discuss the concept of
income transfer following the approach in Marshall and Olkin [51], focusing on the
classical results relating majorization, income transfers, see Marshall and Olkin [51,
Chap. 4, Proposition A.1].
Deﬁnition 2 The majorization relation ⪯on 𝔻n is deﬁned as follows: given x, y ∈
𝔻n with ̄x = ̄y, we say that
x ⪯y
if
k∑
i=1
x(i) ≥
k∑
i=1
y(i)
k = 1, … , n
(1)
where the case k = n is an equality due to ̄x = ̄y. As usual, we write x ≺y if x ⪯y
and not y ⪯x, and we write x ∼y if x ⪯y and y ⪯x. We say that y majorizes x if
x ≺y, and we say that x and y are indiﬀerent if x ∼y.

292
S. Bortot et al.
The majorization relation is a partial preorder, in the sense that x, y ∈𝔻n are
comparable only when ̄x = ̄y, and x ∼y if and only if x and y diﬀer by a permutation.
Given an income distribution x ∈𝔻n, with mean income ̄x, it holds that ̄x ⋅1 ⪯x
since k ̄x ≥∑k
i=1 x(i) for k = 1, … , n. The majorization is strict, ̄x ⋅1 ≺x, when x is
not a uniform income distribution.
Deﬁnition 3 Given x, y ∈𝔻n with ̄x = ̄y, we say that x is derived from y by means
of an income transfer if, for some pair i, j = 1, … , n with yi ≤yj, we have
xi = (1 −𝜀) yi + 𝜀yj
xj = 𝜀yi + (1 −𝜀) yj
𝜀∈[0, 1]
(2)
and xk = yk for k ≠i, j. These formulas express an income transfer, from a richer to a
poorer individual, of an income amount 𝜀(yj −yi). The income transfer obtains x = y
if 𝜀= 0, and exchanges the relative positions of donor and recipient in the income
distribution if 𝜀= 1, in which case x ∼y. In the intermediate cases 𝜀∈(0, 1) the
income transfer produces an income distribution x which is majorized by the original
y, that is x ≺y.
In general, for the majorization relation ⪯and income distributions x, y ∈𝔻n with
̄x = ̄y, it holds that x ⪯y if and only if x can be derived from y by means of a ﬁnite
sequence of income transfers. Moreover, x ≺y if any of the income transfers is not
a permutation.
Deﬁnition 4 A function A ∶𝔻n ⟶𝔻is an averaging function if it is monotonic
and idempotent. An averaging function is said to be strict if it is strictly monotonic.
Note that monotonicity and idempotency implies that min(x) ≤A(x) ≤max(x), for
all x ∈𝔻n.
Particular instances of averaging functions are weighted averaging (WA) func-
tions, ordered weighted averaging (OWA) functions, and Choquet integrals. The for-
mer two are special cases of Choquet integration.
Deﬁnition 5 Given a weighting vector w = (w1, … , wn) ∈[0, 1]n, with ∑n
i=1 wi = 1,
the Weighted Averaging (WA) function associated with w is the averaging function
A ∶𝔻n ⟶𝔻deﬁned as
A(x) =
n
∑
i=1
wi xi.
(3)
Deﬁnition 6 Given a weighting vector w = (w1, … , wn) ∈[0, 1]n, with ∑n
i=1 wi = 1,
the Ordered Weighted Averaging (OWA) function associated with w is the averaging
function A ∶𝔻n ⟶𝔻deﬁned as
A(w) =
n
∑
i=1
wi x(i).
(4)

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
293
The traditional form of OWA functions as introduced by Yager [63] is as follows,
A(x) = ∑n
i=1 ̃wi x[i] where ̃wi = wn−i+1. In [64, 65] the theory and applications of
OWA functions are discussed in detail. The following is a classical result particularly
relevant in our framework. This result regards a form of dominance relation between
OWA functions and the associated weighting structures, see for instance Bortot and
Pereira [20] and references therein.
A class of welfare functions which plays a central role in this paper is that of the
generalized Gini welfare functions introduced by Weymark [60], see also Mehran
[52], Donaldson and Weymark [30, 31], Yaari [61, 62], Ebert [33], Quiggin [54],
Ben-Porath and Gilboa [11].
Deﬁnition 7 Given a weighting vector w = (w1, … , wn) ∈[0, 1]n, with w1 ≥⋯≥
wn ≥0 and ∑n
i=1 wi = 1, the generalized Gini welfare function associated with w is
the function A ∶𝔻n ⟶𝔻deﬁned as
A(x) =
n
∑
i=1
wix(i)
(5)
and, in the AKS framework, the associated generalized Gini inequality index is
deﬁned as
G(x) = ̄x −A(x) = −
n
∑
i=1
(wi −1
n) x(i) .
(6)
Generalized Gini welfare functions are strict if and only if w1 > ⋯> wn > 0. More-
over, generalized Gini welfare functions are stable for translations and the associated
generalized Gini inequality indices are invariant for translations. Both are stable for
dilations.
In relation with generalized Gini welfare functions, the principles of transfer sen-
sitivity (TS) and positional transfer sensitivity (PTS) are based on the central notion
of a progressive income transfer. Given an income distribution
x = (x(1), … , x(i), … , x(j), … , x(n)
)
and i < j and x(i) ≤x(j), we consider the progressive transfer of an income amount 𝛿
from x(j) to x(i), such that x(i) + 𝛿≤x(j) −𝛿. This progressive transfer results in a new
income distribution
x′ = (x(1), … , x(i) + 𝛿, … , x(j) −𝛿, … , x(n)
) .
We consider thus a progressive income transfer 𝛿from x(j) to x(i) with i < j. This
transfer results in a new income distribution in which x′
(i) = x(i)+𝛿, x′
(j) = x(j)−𝛿, and
x′
(k) = x(k) for k ≠i, j. From the deﬁnition (5) of generalized Gini welfare functions,
we obtain

294
S. Bortot et al.
A(x′) −A(x) =
n
∑
k=1
wkx′
(k) −
n
∑
k=1
wkx(k)
=
[(w1x(1) + ⋯+ wi(x(i) + 𝛿) + ⋯+ wj(x(j) −𝛿) + ⋯+ wnx(n)
)
−(w1x(1) + ⋯+ wix(i) + ⋯+ wjx(j) + ⋯+ wnx(n)
)]
= (wi −wj
)𝛿.
(7)
Given that the weight diﬀerence wi−wj is non negative, the generalized Gini welfare
of the distribution x′ is greater or equal than that of the original distribution x. This
means that the generalized Gini welfare function A satisﬁes the transfer sensitivity
(TS) principle, or Pigou-Dalton principle, which states that welfare (inequality) mea-
sures should be non decreasing (non increasing) under progressive income transfers.
On the other hand, the principle of positional transfer sensitivity (PTS) states that
the eﬀect of an income transfer generates higher welfare values when it occurs at
lower income levels. In fact, the non negative weight diﬀerence wi−wj can vary with
the position indicated by the indices i, j. In particular, with j = i + 1, we may have
constant weight diﬀerences (the classical Gini case) or decreasing weight diﬀerences
(the classical Bonferroni case), as we will see below.
We can measure the transfer sensitivity of generalized Gini welfare functions
A(x) = ∑n
i=1 wi x(i) by means of
TS(A) =
n−1
∑
i=1
wi −wi+1 = w1 −wn
∈[0, 1]
(8)
where wi are the weights of the generalized Gini welfare function, with i = 1, … , n.
The TS measure takes values in the unit interval [0, 1]. A TS value further away
from zero indicates a higher level of transfer sensitivity. More speciﬁcally, as the
value of the TS measure increases, transfer sensitivity increases too.
We can measure the positional transfer sensitivity of generalized Gini welfare
functions A(x) = ∑n
i=1 wi x(i) ≠̄x by means of
PTS(A) = 1 −
n−1
∑
i=1
𝜔i ln 𝜔i
ln(1∕(n −1))
∈[0, 1]
(9)
where 𝜔i with i = 1, … , n −1 is given by
𝜔i = wi −wi+1
w1 −wn
i = 1, … , n −1
(10)
with 𝜔1, … , 𝜔n−1 ≥0 and 𝜔1 + ⋯+ 𝜔n−1 = 1. In the case 𝜔= 0 we conventionally
take 𝜔ln 𝜔= 0.
This measure takes values in the unit interval [0, 1]. In fact, the summation term
in (9), corresponding to the Shannon entropy of the 𝜔1, … , 𝜔n−1 distribution, takes

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
295
values in [0, 1] and reaches the maximum value 1 when such distribution is uniform,
𝜔1 = ⋯= 𝜔n−1 = 1∕(n−1). Therefore, the higher the value of the PTS(A) measure,
the greater the positional transfer sensitivity of generalized Gini welfare function A in
relation with income transfers from individual j+1 to individual j, with j = 1, … , n.
3
Gini and Bonferroni Welfare Functions and the
Associated inequality Indices
The classical Gini [37–39], Bonferroni [18, 19], and De Vergottini [28, 29] welfare
functions and the associated inequality indices are classical instances of the AKS
generalized Gini framework. In this section we recall the basic facts about the Gini
and Bonferroni welfare functions and inequality indices and we examine their prop-
erties regarding transfer sensitivity and positional transfer sensitivity.
The classical Gini welfare function AG(x) and the associated classical Gini inequal-
ity index G(x) = ̄x −AG(x) are deﬁned as
AG(x) =
n
∑
i=1
wG
i x(i)
wG
i = 2(n −i) + 1
n2
(11)
where the weights of AG(x) are positive and strictly decreasing with unit sum,
∑n
i=1 wG
i = 1, and
G(x) =
n
∑
i=1
(1
n −wG
i
)
x(i) = −
n
∑
i=1
n −2i + 1
n2
x(i)
(12)
where the coeﬃcients of G(x) have zero sum.
The classical absolute Gini inequality index G is traditionally deﬁned as
G(x) =
1
2n2
n
∑
i,j=1
|xi −xj| = −1
n2
n−1
∑
i=1
n
∑
j=i+1
(
x(i) −x(j)
)
(13)
where the double summation expression for −n2G(x) in (13) can be written as
(−(n −1))x(1) + (1 −(n −2))x(2) + ⋯+ ((n −2) −1)x(n−1) + (n −1)x(n)
(14)
which corresponds to (12).
The classical Bonferroni welfare function AB(x) and the associated classical Bon-
ferroni inequality index B(x) = ̄x −AB(x) are deﬁned as

296
S. Bortot et al.
AB(x) =
n
∑
i=1
wB
i x(i)
wB
i =
n
∑
j=i
1
jn
(15)
where the weights of AB(x) are positive and strictly decreasing with unit sum,
∑n
i=1 wB
i = 1, and
B(x) =
n
∑
i=1
(1
n −wB
i
)
x(i)
(16)
where the coeﬃcients of B(x) have zero sum.
The classical absolute Bonferroni inequality index B is traditionally deﬁned as
B(x) = ̄x −1
n
n
∑
i=1
mi(x)
(17)
where the mean income of the i poorest individuals in the population is given by
mi(x) = 1
i
i∑
j=1
x(j)
for
i = 1, … , n .
(18)
Therefore we have
AB(x) = 1
n
n
∑
i=1
mi(x)
(19)
= 1
n
[(
x(1)
)
+ 1
2
(
x(1) + x(2)
)
+ ⋯+ 1
n
(
x(1) + ⋯+ x(n)
)]
(20)
= 1
n
[
n
∑
j=1
1
j x(1) +
n
∑
j=2
1
j x(2) + ⋯
n
∑
j=n
1
j x(n)
]
(21)
which corresponds to (15).
The rich literature on the three classical cases of generalized Gini welfare
functions—Gini, Bonferroni and De Vergottini—includes, for instance, Kolm [47],
Atkinson [5], Sen [55, 56], Mehran [52], Blackorby and Donaldson [13–16], Loren-
zen [50], Donaldson and Weymark [30, 31], Nygård and Sandström [53], Blackorby
et al. [17], Weymark [60], Yitzhaki [66], Giorgi [40, 41], Benedetti [12], Ebert [32],
Shorrocks and Foster [57], Yaari [62], Silber [58], Bossert [21], Tarsitano [59], Ben
Porath and Gilboa [11], Zoli [68], Gajdos [36], Aaberge [1–3], Giorgi and Crescenzi
[42, 43], Chakravarty and Muliere [26], Chakravarty [24, 25], Bárcena and Imedio
[6], Giorgi and Nadarajah [44], Bárcena and Silber [7, 8], Aristondo et al. [4], and
Zenga [67].

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
297
We now consider a progressive transfer 𝛿from x(j) to x(i) with i < j. This transfer
results in a new income distribution in which x′
(i) = x(i) + 𝛿,
x′
(j) = x(j) −𝛿, and
x′
(k) = x(k) for k ≠i, j. From (11) and (15) we obtain
AG(x′) −AG(x) =
n
∑
k=1
wG
k x′
(k) −
n
∑
k=1
wG
k x(k)
=
[(wG
1 x(1) + ⋯+ wG
i (x(i) + 𝛿) + ⋯+ wG
j (x(j) −𝛿) + ⋯+ wG
n x(n)
)
−
(
wG
1 x(1) + ⋯+ wG
i x(i) + ⋯+ wG
j x(j) + ⋯+ wG
n x(n)
)]
=
(
wG
i 𝛿−wG
j 𝛿
)
=
( 2(n −i) + 1
n2
−2(n −j) + 1
n2
)
𝛿
= 2
n2 (j −i) 𝛿
(22)
AB(x′) −AB(x) =
n
∑
k=1
wB
k x′
(k) −
n
∑
k=1
wB
k x(k)
=
[(
wB
1x(1) + ⋯+ wB
i (x(i) + 𝛿) + ⋯+ wB
j (x(j) −𝛿) + ⋯+ wB
nx(n)
)
−(wB
1x(1) + ⋯+ wB
i x(i) + ⋯+ wB
j x(j) + ⋯+ wB
nx(n)
)]
= (wB
i 𝛿−wB
j 𝛿) =
(
n
∑
k=i
1
nk −
n
∑
k=j
1
nk
)
𝛿
= 1
n
( 1
i +
1
i + 1 + ⋯+
1
j −1
)
𝛿=
(1
n
j−1
∑
k=i
1
k
)
𝛿.
(23)
Since AG(x′) −AG(x) > 0 and AB(x′) −AB(x) > 0, both welfare functions satisfy the
principle of transfer sensitivity. Expression (22) implies that the increase in welfare,
in the Gini case, depends on the diﬀerence (j−i), irrespectively of the actual positions
i, j. The Bonferroni welfare function, on the other hand, does depend on the actual
positions i, j. Expression (23) indicates that the increase in welfare is greater if the
transfer occurs at lower income levels and therefore the Bonferroni welfare function
satisﬁes the principle of positional transfer sensitivity.
4
The Binomial Decomposition
In this section we review the binomial decomposition of generalized Gini welfare
functions due to Calvo and De Baets [22] and Bortot and Pereira [20]. We examine
the weighting structures of the binomial welfare functions Cj, with j = 1, … , n,
and we illustrate their properties regarding transfer sensitivity and positional transfer
sensitivity.

298
S. Bortot et al.
Deﬁnition 8 The binomial welfare functions Cj ∶𝔻n ⟶𝔻, with j = 1, … , n, are
deﬁned as
Cj(x) =
n
∑
i=1
wji x(i) =
n
∑
i=1
(n−i
j−1
)
(n
j
) x(i)
j = 1, … , n
(24)
where the binomial weights wji, i, j = 1, … , n are null when i + j > n + 1, according
to the usual convention that (p
q
) = 0 when p < q, with p, q = 0, 1, … Given that the
binomial weights are decreasing, wj1 ≥wj2 ≥⋯≥wjn for j = 1, … , n, the binomial
welfare functions are generalized Gini welfare functions.
With the exception of C1(x) = ̄x, the binomial welfare functions Cj, j = 2, … , n
have an increasing number of null weights, in correspondence with x(n−j+2), … , x(n).
The weight normalization of the binomial welfare functions, ∑n
i=1 wji = 1 for j =
1, … , n, is due to the column-sum property of binomial coeﬃcients,
n
∑
i=1
(
n −i
j −1
)
=
n−1
∑
i=0
(
i
j −1
)
=
(
n
j
)
j = 1, … , n .
(25)
The binomial welfare functions Cj, j = 1, … , n are continuous, idempotent, and
stable for translations, where the latter two properties follow immediately from the
unit sum normalization of the binomial weights. Moreover, due to the cumulative
property of the binomial weights, see Calvo and De Baets [22], see also Bortot
and Pereira [20], the binomial welfare functions satisfy the relations ̄x = C1(x) ≥
C2(x) ≥⋯≥Cn(x) ≥0, for any x ∈𝔻n.
Proposition 1 Generalized Gini welfare functions A ∶𝔻n ⟶𝔻can be written
uniquely as
A(x) = 𝛼1C1(x) + 𝛼2C2(x) + ⋯+ 𝛼nCn(x)
(26)
where the coeﬃcients 𝛼j, j = 1, … , n are subject to the following conditions,
𝛼1 = 1 −
n
∑
j=2
𝛼j ≥0
(27)
n
∑
j=2
[
1 −n
(i−1
j−1
)
(n
j
)
]
𝛼j ≤1
i = 2, … , n
(28)
n
∑
j=2
(n−i
j−2
)
(n
j
) 𝛼j ≥0
i = 2, … , n .
(29)
The binomial welfare functions constitute therefore a functional basis for the
generalized Gini welfare functions, which can be uniquely expressed as

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
299
A(x) = ∑n
j=1 𝛼j Cj(x) where the coeﬃcients 𝛼j, j = 1, … , n satisfy the constraints
(27)–(29), one of which is ∑n
j=1 𝛼j = 1. However, the binomial decomposition does
not express a simple convex combination of the binomial welfare functions, as the
condition 𝛼1 + ⋯+ 𝛼n = 1 might suggest. In fact, condition (27) ensures 𝛼1 ≥0 but
conditions (28) and (29) allow for negative 𝛼2, … , 𝛼n values.
Notice that C1(x) = ̄x and C2(x), which has n −1 positive linearly decreasing
weights and one null last weight, is the only strict binomial welfare function. In
terms of the classical Gini welfare function we have that
Ac(x) = 1
n C1(x) + n −1
n
C2(x) .
(30)
The remaining binomial welfare functions Cj(x), j = 3, … , n, have n −j + 1
positive non-linear decreasing weights and j −1 null last weights.
In dimensions n = 4, 8 the weights wij ∈[0, 1], i, j = 1, … , n of the binomial
welfare functions Cj, j = 1, … , n are as follows,
n = 4 C1 ∶( 1
4, 1
4, 1
4, 1
4)
n = 8 C1 ∶( 1
8, 1
8, 1
8, 1
8, 1
8, 1
8, 1
8, 1
8)
C2 ∶( 3
6, 2
6, 1
6, 0)
C2 ∶( 7
28, 6
28, 5
28, 4
28, 3
28, 2
28, 1
28, 0)
C3 ∶( 3
4, 1
4, 0, 0)
C3 ∶( 21
56, 15
56, 10
56, 6
56, 3
56, 1
56, 0, 0)
C4 ∶(1, 0, 0, 0)
C4 ∶( 35
70, 20
70, 10
70, 4
70, 1
70, 0, 0, 0)
C5 ∶( 35
56, 15
56, 5
56, 1
56, 0, 0, 0, 0)
C6 ∶( 21
28, 6
28, 1
28, 0, 0, 0, 0, 0)
C7 ∶( 7
8, 1
8, 0, 0, 0, 0, 0, 0)
C8 ∶(1, 0, 0, 0, 0, 0, 0, 0)
The binomial welfare functions Cj, j = 1, … , n have null weights associated with
the j −1 richest individuals in the population and therefore, as j increases from 1 to
n, they behave in analogy with poverty measures which progressively focus on the
poorest part of the population.
In order to measure the transfer sensitivity of the binomial welfare functions Cj,
with j = 1, … , n we consider a transfer from the richest to the poorest individ-
ual. To measure the transfer sensitivity of the binomial welfare functions Cj, with
j = 1, … , n, we use expression (8).
In Fig. 1 we can see the values of the TS(Cj) measure of the binomial welfare
functions Cj, with j = 1, … , n for the cases n = 4, 8.
In both cases n = 4, 8 we observe that TS increases linearly for j = 3, … , n, which
means that the TS diﬀerence Cj −Cj−1 between any 2 consecutive binomial welfare
functions is the same. This can be proved as follows,

300
S. Bortot et al.
0.00
0.25
0.50
0.75
1.00
1
2
3
4
k
value
C_k
C_1
C_2
C_3
C_4
0.00
0.25
0.50
0.75
1.00
1
2
3
4
5
6
7
8
k
value
C_k
C_1
C_2
C_3
C_4
C_5
C_6
C_7
C_8
Fig. 1
Transfer sensitivity of Cj, for j = 1, … , n
(wj1 −wjn) −(wj−1,1 −wj−1,n) =
1
(n
j
)
[(
n −1
j −1
)
−
(
n −n
j −1
)]
−
1
( n
j−1
)
[(
n −1
j −2
)
−
(
n −n
j −2
)]
= (n −1)! j!
(j −1)! n! −(n −1)!(j −1)!
n!(j −2)!
= j
n −j −1
n
= 1
n
where wji are the binomial weights in (24) with i, j = 1, … , n.
In order to measure the positional transfer sensitivity of the binomial welfare func-
tions Cj, with j = 1, … , n we consider n −1 income transfers, each time from an
individual in position j to the individual in position j −1, with j = 1, … , n. To
measure the positional transfer sensitivity of the binomial welfare functions Cj, with
j = 1, … , n we use expression (9)
0.00
0.25
0.50
0.75
1.00
1
2
3
4
k
value
C_k
C_1
C_2
C_3
C_4
0.00
0.25
0.50
0.75
1.00
3
1
2
4
5
6
7
8
k
value
C_k
C_1
C_2
C_3
C_4
C_5
C_6
C_7
C_8
Fig. 2
Positional transfer sensitivity of Cj, for j = 1, … , n

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
301
In Fig. 2 we illustrate the PTS values of the binomial welfare functions Cj, with
j = 1, … , n in the cases n = 4, 8. We observe in both cases that PTS is null for
j = 1, 2 while for j = 3, … , n it increases monotonically, not linearly.
5
The Single Parameter Gini Bonferroni Welfare Functions
The single parameter family of Gini Bonferroni (GB) welfare functions, which inter-
polates between the classical Gini and Bonferroni cases, has been introduced by
Bárcena and Silber [8]. We recall the deﬁnition of the single parameter GB wel-
fare functions and we examine their binomial decomposition. Moreover, we study
the measures of transfer sensitivity and positional transfer sensitivity in terms of the
parameter 𝛾∈[0, 1].
The welfare functions of the GB family are of the form
AGB(x) =
n
∑
i=1
wGB
i
x(i)
(31)
with
wGB
i
= (1∕n2)
[
n −i(n∕i)𝛾+
n
∑
j=i
(n∕j)𝛾]
𝛾∈[0, 1]
(32)
where the classical Gini and Bonferroni welfare functions are special cases with
𝛾= 0, 1. Note that when 𝛾= 0 we obtain the “equally distributed equivalent level
of income” corresponding to the Gini welfare function, while when 𝛾= 1 we obtain
the “equally distributed equivalent level of income” corresponding to the Bonferroni
welfare function.
Given that the weights of the GB welfare functions are strictly decreasing, wGB
1
>
wGB
2
> ⋯> wGB
n
= 1∕n2, the GB welfare functions are generalized Gini welfare
functions. The weighting structure of the GB welfare functions is illustrated in Fig. 3
in the cases n = 4, 8.
Fig. 3
Weights of the GB welfare functions for parameter values 𝛾= 0, 0.1, … , 1

302
S. Bortot et al.
Fig. 4
Coeﬃcients of the binomial decomposition for n = 4, 8
In the framework of the binomial decomposition (26), each GB welfare function
AGB(x) can be uniquely expressed in terms of the binomial Gini welfare functions
C1, C2, … Cn as follows,
AGB(x) = 𝛼1C1(x) + 𝛼2C2(x) + ⋯+ 𝛼nCn(x)
𝛾∈[0, 1]
(33)
which can be written as
n
∑
i=1
wGB
i x(i) = 𝛼1
n
∑
i=1
w1i x(i)+𝛼2
n
∑
i=1
w2i x(i)+⋯+𝛼n
n
∑
i=1
wni x(i)
𝛾∈[0, 1] . (34)
The expression of the binomial decomposition is unique and therefore, for each value
of the parameter 𝛾∈[0, 1], we obtain a unique vector (𝛼1, … , 𝛼n) by solving the
linear system
⎧
⎪
⎪
⎨
⎪
⎪⎩
wGB
1
= 𝛼1w11 + 𝛼2w21 + ⋯+ 𝛼nwn1
wGB
2
= 𝛼1w12 + 𝛼2w22 + ⋯+ 𝛼nwn2
…
wGB
n
= 𝛼1w1n + 𝛼2w2n + ⋯+ 𝛼nwnn
(35)
where the binomial weights wji, i, j = 1, … , n are as in (24).
In Fig. 4 we depict the vector (𝛼1, … , 𝛼n) as a function of the parameter 𝛾∈[0, 1]
in the cases n = 4, 8.
We observe, as expected, that 𝛼1 = 1∕n is independent of the parameter 𝛾∈[0, 1]
since, in the last equation of the linear system (35), we have wGB
n
= 1∕n2 and w1n =
1∕n and w2n = ⋯= wnn = 0.
On the other hand, we observe that only 𝛼2 is decreasing, whereas 𝛼3, … 𝛼n are
increasing with respect to 𝛾∈[0, 1].

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
303
It is well known that the classical Gini welfare function is 2-additive, see for
instance Grabisch [45] and Bortot and Pereira [20] and references therein. On the
other hand, the classical Bonferroni welfare function is n-additive. In fact in Fig. 4
we observe that only 𝛼1, 𝛼2 ≠0 in the classical Gini case 𝛾= 0, and 𝛼1, … , 𝛼n ≠0
in the classical Bonferroni case 𝛾= 1.
In order to illustrate the PTS principle in relation with the classical Gini and
Bonferroni welfare functions, corresponding to the extreme values of the parameter
𝛾= 0, 1, consider ﬁrst the classical Gini welfare function AG(x), whose weighting
structure for n = 8 is given by (11) as follows,
wG =
(15
64, 13
64, 11
64, 9
64, 7
64, 5
64, 3
64, 1
64
)
.
(36)
Consider now a progressive income transfer 𝛿from x(j) to x(i) with j = i + 1. This
transfer results in a new income distribution in which x′
(i) = x(i) + 𝛿, x′
(j) = x(j) −𝛿,
and x′
(k) = x(k) for k ≠i, j. According to the expression for the classical Gini welfare
diﬀerence (22), we obtain
for i = 1, j = 2 ∶AG(x′) −AG(x) = (wG
1 −wG
2 )𝛿= 1
32 𝛿
for i = 2, j = 3 ∶AG(x′) −AG(x) = (wG
2 −wG
3 )𝛿= 1
32 𝛿
for i = 3, j = 4 ∶AG(x′) −AG(x) = (wG
3 −wG
4 )𝛿= 1
32 𝛿
for i = 4, j = 5 ∶AG(x′) −AG(x) = (wG
4 −wG
5 )𝛿= 1
32 𝛿
for i = 5, j = 6 ∶AG(x′) −AG(x) = (wG
5 −wG
6 )𝛿= 1
32 𝛿
for i = 6, j = 7 ∶AG(x′) −AG(x) = (wG
6 −wG
7 )𝛿= 1
32 𝛿
for i = 7, j = 8 ∶AG(x′) −AG(x) = (wG
7 −wG
8 )𝛿= 1
32 𝛿.
We can see that any progressive income transfer generates the same increase in wel-
fare, meaning that the classical Gini welfare function does not satisﬁes PTS.
Consider now the classical Bonferroni welfare function AB(x), whose weighting
structure for n = 8 is given by (15) as follows,
wB =
( 761
2240, 481
2240, 341
2240, 743
6720, 533
6720, 73
1344, 15
448, 1
64
)
.
(37)
As before, consider a progressive income transfer 𝛿from x(j) to x(i) with j = i+1. This
transfer results in a new income distribution in which x′
(i) = x(i) + 𝛿, x′
(j) = x(j) −𝛿,
and x′
(k) = x(k) for k ≠i, j. According to the expression for the classical Bonferroni
welfare diﬀerence (23), we obtain

304
S. Bortot et al.
for i = 1, j = 2 ∶AB(x′) −AB(x) = (wB
1 −wB
2)𝛿= 1
8 𝛿
for i = 2, j = 3 ∶AB(x′) −AB(x) = (wB
2 −wB
3)𝛿= 1
16 𝛿
for i = 3, j = 4 ∶AB(x′) −AB(x) = (wB
3 −wB
4)𝛿= 1
24 𝛿
for i = 4, j = 5 ∶AB(x′) −AB(x) = (wB
4 −wB
5)𝛿= 1
32 𝛿
for i = 5, j = 6 ∶AB(x′) −AB(x) = (wB
5 −wB
6)𝛿= 1
40 𝛿
for i = 6, j = 7 ∶AB(x′) −AB(x) = (wB
6 −wB
7)𝛿= 1
48 𝛿
for i = 7, j = 8 ∶AB(x′) −AB(x) = (wB
7 −wB
8)𝛿= 1
56 𝛿.
We can see in this case that the actual position in which the progressive income
transfer occurs has a diﬀerentiated impact on welfare. More speciﬁcally, the increase
in welfare is greater when the transfer applies to the lowest income levels.
In general, we can measure the transfer sensitivity and positional transfer sensi-
tivity of the GB welfare functions in terms of the parameter 𝛾∈[0, 1] using the
measures in (8) and (9) as follows,
TS(𝛾) =
n−1
∑
i=1
wGB
i
−wGB
i+1 = wGB
1
−wGB
n
,
(38)
where wGB
i
are the weights of the single parameter GB welfare functions AGB asso-
ciated with the parameter 𝛾∈[0, 1], with i = 1, … , n.
PTS(𝛾) = 1 +
n−1
∑
i=1
𝜔i ln 𝜔i
ln(n −1) ,
(39)
where 𝜔i, with i = 1, … , n −1, is given by
𝜔i =
wGB
i
−wGB
i+1
wGB
1
−wGB
n
where wGB
i
are the weights of the GB welfare functions, with i = 1, … , n.
In Figs. 5 and 6 we can see the measures of transfer sensitivity and positional
transfer sensitivity of the GB welfare functions associated with the parameter 𝛾∈
[0, 1], in the cases n = 4, 8. As the parameter 𝛾value increases, we observe that both
transfer sensitivity and positional transfer sensitivity of the AGB welfare function
increase too. Notice the fact that transfer sensitivity is not null for 𝛾= 0, corre-
sponding to the classical Gini case.

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
305
0.2
0.3
0.4
0.5
(a) TS in the case n = 4
0.2
0.3
0.4
0.5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
(b) TS in the case n = 8
Fig. 5
Transfer sensitivity of the AGB for parameter values 𝛾∈[0, 1], with n = 4, 8
0.00
0.04
0.08
0.12
(a) PTS in the case n = 4
0.00
0.04
0.08
0.12
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
(b) PTS in the case n = 8
Fig. 6
Positional transfer sensitivity of the AGB for parameter values 𝛾∈[0, 1], with n = 4, 8
6
Conclusions
We have examined the binomial decomposition of the single parameter family of GB
welfare functions and we have illustrated the dependence of the binomial decompo-
sition coeﬃcients in relation with the parameter which describes the GB family. We
have found that 𝛼1 = 1∕n is independent of the parameter 𝛾∈[0, 1] and we have
observed that only 𝛼2 is decreasing, whereas 𝛼3, … , 𝛼n are increasing with respect
to 𝛾∈[0, 1]. In particular, since the binomial coeﬃcients 𝛼j with j = 1, … , n are
non negative with unit sum, the decrease in 𝛼2 compensates the increase in 𝛼3, … , 𝛼n

306
S. Bortot et al.
with respect to 𝛾∈[0, 1]. The Bonferroni welfare function, associated with 𝛾= 1,
is obtained by means of this compensatory mechanism.
Moreover, we have illustrated the transfer sensitivity and positional transfer sensi-
tivity of the binomial welfare functions, and we have examined their properties with
respect to these principles. For this purpose, we have introduced measures of transfer
sensitivity and positional transfer sensitivity for generalized Gini welfare functions
and, in particular, we have illustrated the behaviour of these measures in the case of
the GB welfare functions, in relation with the values of the parameter 𝛾∈[0, 1].
References
1. Aaberge, R.: Characterizations of Lorenz curves and income distributions. Soc. Choice Welf.
17(4), 639–653 (2000)
2. Aaberge, R.: Axiomatic characterization of the Gini coeﬃcient and Lorenz curve orderings. J.
Econ. Theory 101(1), 115–132 (2001)
3. Aaberge, R.: Ginis nuclear family. J. Econ. Inequality 5(3), 305–322 (2007)
4. Aristondo, O., García-Lapresta, J.L., de la Vega, C.L., Pereira, R.A.M.: Classical inequality
indices, welfare and illfare functions, and the dual decomposition. Fuzzy Sets Syst. 228, 114–
136 (2013)
5. Atkinson, A.B.: On the measurement of inequality. J. Econ. Theory 2(3), 244–263 (1970)
6. Bárcena Martín, E., Imedio, L.J.: The Bonferroni, Gini, and De Vergottini indices. Inequal-
ity, welfare, and deprivation in the European union in 2000. In: Bishop, J., Zheng, B. (eds.)
Inequality and Opportunity: Papers from the Second ECINEQ Society Meeting, Research on
Economic Inequality, vol. 16, pp. 231–257. Emerald Group Publishing Limited (2008)
7. Bárcena Martín, E., Silber, J.: On the concepts of Bonferroni segregation index and curve.
Rivista Italiana di Economia Demograﬁa e. Statistica 65(2), 57–76 (2011)
8. Bárcena Martín, E., Silber, J.: On the generalization and decomposition of the Bonferroni
index. Soc. Choice Welf. 41(4), 763–787 (2013)
9. Beliakov, G., Bustince Sola, H., Calvo, T.: A practical guide to averaging functions. Studies in
Fuzziness and Soft Computing, vol. 329. Springer, Heidelberg (2016)
10. Beliakov, G., Pradera, A., Calvo, T.: Aggregation functions: a guide for practitioners. Studies
in Fuzziness and Soft Computing, vol. 221. Springer, Heidelberg (2007)
11. Porath, E.B., Gilboa, I.: Linear measures, the Gini index, and the income-equality trade-oﬀ. J.
Econ. Theory 64, 443–467 (1994)
12. Benedetti, C.: Sulla interpretazione benesseriale di noti indici di concentrazione e di altri.
Metron 44(1), 421–429 (1986)
13. Blackorby, C., Donaldson, D.: Measures of relative equality and their meaning in terms of
social welfare. J. Econ. Theory 18(1), 59–80 (1978)
14. Blackorby, C., Donaldson, D.: Ethical indices for the measurement of poverty. Econometrica
48(4), 1053–1060 (1980)
15. Blackorby, C., Donaldson, D.: A theoretical treatment of indices of absolute inequality. Int.
Econ. Rev. 21(1), 107–136 (1980)
16. Blackorby, C., Donaldson, D.: Ethical social index numbers and the measurement of eﬀective
tax/beneﬁt progressivity. Can. J. Econ. 17(4), 683–694 (1984)
17. Blackorby, C., Donaldson, D., Auersperg, M.: A new procedure for the measurement of
inequality within and among population subgroups. Can. J. Econ. 14(4), 665–685 (1981)
18. Bonferroni, C.E.: Elementi di Statistica Generale. Seeber, Firenze (1930)
19. Bonferroni, C.E.: Elementi di Statistica Generale. Ristampa con aggiunte, anno accademico
1932–1933. Litograﬁa Felice Gili, Torino (1933)

The Gini Bonferroni Family of Welfare Functions and the Binomial Decomposition ...
307
20. Bortot, S., Pereira, R.A.M.: The binomial Gini inequality indices and the binomial decompo-
sition of welfare functions. Fuzzy Sets Syst. 255, 92–114 (2014)
21. Bossert, W.: An axiomatization of the single-series Ginis. J. Econ. Theory 50(1), 82–92 (1990)
22. Calvo, T., De Baets, B.: Aggregation operators deﬁned by k-order additive/maxitive fuzzy mea-
sures. Int. J. Uncertainty Fuzziness Knowl.-Based Syst. 6(6), 533–550 (1998)
23. Calvo, T., Kolesárová, A., Komorníková, M., Mesiar, R.: Aggregation operators: properties,
classes and construction methods. In: Calvo, T., Mayor, M., Mesiar, R. (eds.) Aggregation
Operators: New Trends and Applications. Studies in Fuzziness and Soft Computing, vol. 97,
pp. 3–104. Springer, Heidelberg (2002)
24. Chakravarty, S.R.: The Bonferroni indices of inequality. In: International Conference in Mem-
ory of C. Gini and M. O. Lorenz (2005)
25. Chakravarty, S.R.: A deprivation-based axiomatic characterization of the absolute Bonferroni
index of inequality. J. Econ. Inequality 5(3), 339–351 (2007)
26. Chakravarty, S.R., Muliere, P.: Welfare indicators: a review and new perspectives. measure-
ment of inequality. Metron 61(3), 457–497 (2003)
27. Chisini, O.: Sul concetto di media. Periodico di matematiche 9(4), 106–116 (1929)
28. De Vergottini, M.: Sul signiﬁcato di alcuni indici di concentrazione. Giornale degli Economisti
e Annali di Economia 2(5/6), 317–347 (1940)
29. De Vergottini, M.: Sugli indici di concentrazione. Statistica 10(4), 445–454 (1950)
30. Donaldson, D., Weymark, J.A.: A single-parameter generalization of the Gini indices of
inequality. J. Econ. Theory 22(1), 67–86 (1980)
31. Donaldson, D., Weymark, J.A.: Ethically ﬂexible Gini indices for income distributions in the
continuum. J. Econ. Theory 29(2), 353–358 (1983)
32. Ebert, U.: Size and distribution of incomes as determinants of social welfare. J. Econ. Theory
41(1), 23–33 (1987)
33. Ebert, U.: Measurement of inequality: an attempt at uniﬁcation and generalization. Soc. Choice
Welf. 5(2), 147–169 (1988)
34. Fodor, J., Marichal, J.L., Roubens, M.: Characterization of the ordered weighted averaging
operators. IEEE Trans. Fuzzy Syst. 3(2), 236–240 (1995)
35. Fodor, J., Roubens, M.: Fuzzy preference modelling and multicriteria decision support. Theory
and Decision Library, series D, vol. 14. Kluwer Academic Publishers, Dordrecht (1994)
36. Gajdos, T.: Measuring inequalities without linearity in envy: choquet integrals for symmetric
capacities. J. Econ. Theory 106(1), 190–200 (2002)
37. Gini, C.: Variabilità e mutabilità Contributo alla studio delle Distribuzioni e delle Relazioni
Statistiche. P. Cuppini, Bologna (1912)
38. Gini, C.: Sulla misura della concentrazione e della variabilità dei caratteri. Atti del Reale Isti-
tuto Veneto di Scienze, Lettere ed Arti 78, 1203–1248 (1914) (English translation given in
Metron 63, 1–38 (2005))
39. Gini, C.: Measurement of inequality of incomes. Econ. J. 31(121), 124–126 (1921)
40. Giorgi, G.M.: A methodological survey of recent studies for the measurement of inequality of
economic welfare carried out by some Italian statisticians. Econ. Notes 13(1), 146–157 (1984)
41. Giorgi, G.M.: Concentration index, Bonferroni. In: Kotz, S., Read, C.B., Banks, D.L. (eds.)
Encyclopedia of Statistical Sciences, pp. 141–146. Wiley, New York (1998)
42. Giorgi, G.M., Crescenzi, M.: A look at the Bonferroni inequality measure in a reliability frame-
work. Statistica 61(4), 571–584 (2001)
43. Giorgi, G.M., Crescenzi, M.: A proposal of poverty measures based on the Bonferroni inequal-
ity index. Metron 59(3–4), 3–16 (2001)
44. Giorgi, G.M., Nadarajah, S.: Bonferroni and Gini indices for various parametric families of
distributions. Metron 68(1), 23–46 (2010)
45. Grabisch, M.: k-order additive discrete fuzzy measures and their representation. Fuzzy Sets
Syst. 92(2), 167–189 (1997)
46. Grabisch, M., Marichal, J.L., Mesiar, R., Pap, E.: Aggregation functions. Encyclopedia of
Mathematics and its Applications, vol. 127. Cambridge University Press (2009)

308
S. Bortot et al.
47. Kolm, S.C.: The optimal production of social justice. In: Margolis, J., Guitton, H. (eds.) Pub-
lic Economics. International Economic Association Series, pp. 145–200. Macmillan, London
(1969)
48. Kolm, S.C.: Unequal inequalities I. J. Econ. Theory 12(3), 416–442 (1976)
49. Kolm, S.C.: Unequal inequalities II. J. Econ. Theory 13(1), 82–111 (1976)
50. Lorenzen, G.: A Generalized Gini-Coeﬃcient. Universität Hamburg, Institut für Statistik und
Ökometrie (1979)
51. Marshall, A.W., Olkin, I.: Inequalities: Theory of Majorization and Its Applications. Mathe-
matics in Science and Engineering, vol. 143. Academic Press, New York (1979)
52. Mehran, F.: Linear measures of income inequality. Econometrica 44(4), 805–809 (1976)
53. Nygård, F., Sandström, A.: Measuring Income Inequality. Almqvist & Wiksell International,
Stockholm (1981)
54. Quiggin, J. (ed.): Generalized Expected Utility Analysis: The Rank-Dependent Model. Kluwer
Academic Publishers, Dordrecht (1993)
55. Sen, A.: On Economic Inequality. Claredon Press, Oxford (1973)
56. Sen, A.: Ethical measurement of inequality: some diﬃculties. In: Krelle, W., Shorrocks, A.F.
(eds.) Personal Income Distribution. North-Holland, Amsterdam (1978)
57. Shorrocks, A.F., Foster, J.E.: Transfer sensitive inequality measures. Rev. Econ. Stud. 54(3),
485–497 (1987)
58. Silber, J.: Factor components, population subgroups and the computation of the Gini index of
inequality. Rev. Econ. Stat. 71(1), 107–115 (1989)
59. Tarsitano, A.: The Bonferroni index of income inequality. In: Dargum, C., Zenga, M. (eds.)
Income and Wealth Distribution. Inequality and Poverty, pp. 228–242. Springer, Heidelberg
(1990)
60. Weymark, J.A.: Generalized Gini inequality indices. Math. Soc. Sci. 1(4), 409–430 (1981)
61. Yaari, M.E.: The dual theory of choice under risk. Econometrica 55(1), 95–115 (1987)
62. Yaari, M.E.: A controversial proposal concerning inequality measurement. J. Econ. Theory
44(2), 381–397 (1988)
63. Yager, R.R.: On ordered weighted averaging aggregation operators in multi-criteria decision
making. IEEE Trans. Syst. Man Cybern. 18(1), 183–190 (1988)
64. Yager, R.R., Kacprzyk, J.: The Ordered Weighted Averaging Operators: Theory and Applica-
tions. Kluwer Academic Publisher, Dordrecht (1997)
65. Yager, R.R., Kacprzyk, J., Beliakov, G.: Recent developments in the ordered weighted aver-
aging operators: theory and practice. Studies in Fuzziness and Soft Computing, vol. 265.
Springer, Heidelberg (2011)
66. Yitzhaki, S.: On an extension of the Gini inequality index. Int. Econ. Rev. 24(3), 617–628
(1983)
67. Zenga, M.M.: Decomposition by sources of the Gini, Bonferroni and Zenga inequality indexes.
Stat. Appl. 11(2), 133–161 (2013)
68. Zoli, C.: Intersecting generalized Lorenz curves and the Gini index. Soc. Choice Welf. 16(2),
183–196 (1999)


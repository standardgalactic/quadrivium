
AN INTRODUCTION
TO THE
ANALYSIS OF ALGORITHMS
Second Edition

This page intentionally left blank 

AN INTRODUCTION
TO THE
ANALYSIS OF ALGORITHMS
Second Edition
Robert Sedgewick
Princeton University
Philippe Flajolet
INRIA Rocquencourt
Upper Saddle River, NJ • Boston • Indianapolis • San Francisco
New York • Toronto • Montreal • London • Munich • Paris • Madrid
Capetown • Sydney • Tokyo • Singapore • Mexico City

Many of the designations used by manufacturers and sellers to distinguish their prod-
ucts are claimed as trademarks. Where those designations appear in this book, and
the publisher was aware of a trademark claim, the designations have been printed
with initial capital letters or in all capitals.
Ļe authors and publisher have taken care in the preparation of this book, but make
no expressed or implied warranty of any kind and assume no responsibility for er-
rors or omissions. No liability is assumed for incidental or consequential damages in
connection with or arising out of the use of the information or programs contained
herein.
Ļe publisher oﬀers excellent discounts on this book when ordered in quantity for
bulk purchases or special sales, which may include electronic versions and/or custom
covers and content particular to your business, training goals, marketing focus, and
branding interests. For more information, please contact:
U.S. Corporate and Government Sales
(800) 382-3419
corpsales@pearsontechgroup.com
For sales outside the United States, please contact:
International Sales
international@pearsoned.com
Visit us on the Web: informit.com/aw
Library of Congress Control Number: 2012955493
Copyright c⃝2013 Pearson Education, Inc.
All rights reserved. Printed in the United States of America. Ļis publication is
protected by copyright, and permission must be obtained from the publisher prior to
any prohibited reproduction, storage in a retrieval system, or transmission in any form
or by any means, electronic, mechanical, photocopying, recording, or likewise. To
obtain permission to use material from this work, please submit a written request to
Pearson Education, Inc., Permissions Department, One Lake Street, Upper Saddle
River, New Jersey 07458, or you may fax your request to (201) 236-3290.
ISBN-13: 978-0-321-90575-8
ISBN-10:
0-321-90575-X
Text printed in the United States on recycled paper at Courier in Westford, Massachusetts.
First printing, January 2013

F O R E W O R D
P
EOPLE who analyze algorithms have double happiness. First of all they
experience the sheer beauty of elegant mathematical patterns that sur-
round elegant computational procedures. Ļen they receive a practical payoﬀ
when their theories make it possible to get other jobs done more quickly and
more economically.
Mathematical models have been a crucial inspiration for all scientiŀc
activity, even though they are only approximate idealizations of real-world
phenomena. Inside a computer, such models are more relevant than ever be-
fore, because computer programs create artiŀcial worlds in which mathemat-
ical models often apply precisely. I think that’s why I got hooked on analysis
of algorithms when I was a graduate student, and why the subject has been
my main life’s work ever since.
Until recently, however, analysis of algorithms has largely remained the
preserve of graduate students and post-graduate researchers. Its concepts are
not really esoteric or diﬃcult, but they are relatively new, so it has taken awhile
to sort out the best ways of learning them and using them.
Now, after more than 40 years of development, algorithmic analysis has
matured to the point where it is ready to take its place in the standard com-
puter science curriculum. Ļe appearance of this long-awaited textbook by
Sedgewick and Flajolet is therefore most welcome. Its authors are not only
worldwide leaders of the ŀeld, they also are masters of exposition. I am sure
that every serious computer scientist will ŀnd this book rewarding in many
ways.
D. E. Knuth

This page intentionally left blank 

P R E F A C E
T
HIS book is intended to be a thorough overview of the primary tech-
niques used in the mathematical analysis of algorithms. Ļe material
covered draws from classical mathematical topics, including discrete mathe-
matics, elementary real analysis, and combinatorics, as well as from classical
computer science topics, including algorithms and data structures. Ļe focus
is on “average-case” or “probabilistic” analysis, though the basic mathematical
tools required for “worst-case” or “complexity” analysis are covered as well.
We assume that the reader has some familiarity with basic concepts in
both computer science and real analysis. In a nutshell, the reader should be
able to both write programs and prove theorems. Otherwise, the book is
intended to be self-contained.
Ļe book is meant to be used as a textbook in an upper-level course on
analysis of algorithms. It can also be used in a course in discrete mathematics
for computer scientists, since it covers basic techniques in discrete mathemat-
ics as well as combinatorics and basic properties of important discrete struc-
tures within a familiar context for computer science students. It is traditional
to have somewhat broader coverage in such courses, but many instructors may
ŀnd the approach here to be a useful way to engage students in a substantial
portion of the material. Ļe book also can be used to introduce students in
mathematics and applied mathematics to principles from computer science
related to algorithms and data structures.
Despite the large amount of literature on the mathematical analysis of
algorithms, basic information on methods and models in widespread use has
not been directly accessible to students and researchers in the ŀeld. Ļis book
aims to address this situation, bringing together a body of material intended
to provide readers with both an appreciation for the challenges of the ŀeld and
the background needed to learn the advanced tools being developed to meet
these challenges. Supplemented by papers from the literature, the book can
serve as the basis for an introductory graduate course on the analysis of algo-
rithms, or as a reference or basis for self-study by researchers in mathematics
or computer science who want access to the literature in this ŀeld.
Preparation. Mathematical maturity equivalent to one or two years’ study
at the college level is assumed. Basic courses in combinatorics and discrete
mathematics may provide useful background (and may overlap with some

viii
P Ş ő Œ ō ŏ ő
material in the book), as would courses in real analysis, numerical methods,
or elementary number theory. We draw on all of these areas, but summarize
the necessary material here, with reference to standard texts for people who
want more information.
Programming experience equivalent to one or two semesters’ study at
the college level, including elementary data structures, is assumed. We do
not dwell on programming and implementation issues, but algorithms and
data structures are the central object of our studies. Again, our treatment is
complete in the sense that we summarize basic information, with reference
to standard texts and primary sources.
Related books. Related texts include Ļe Art of Computer Programming by
Knuth; Algorithms, Fourth Edition, by Sedgewick and Wayne; Introduction
to Algorithms by Cormen, Leiserson, Rivest, and Stein; and our own Analytic
Combinatorics. Ļis book could be considered supplementary to each of these.
In spirit, this book is closest to the pioneering books by Knuth. Our fo-
cus is on mathematical techniques of analysis, though, whereas Knuth’s books
are broad and encyclopedic in scope, with properties of algorithms playing a
primary role and methods of analysis a secondary role. Ļis book can serve as
basic preparation for the advanced results covered and referred to in Knuth’s
books. We also cover approaches and results in the analysis of algorithms that
have been developed since publication of Knuth’s books.
We also strive to keep the focus on covering algorithms of fundamen-
tal importance and interest, such as those described in Sedgewick’s Algorithms
(now in its fourth edition, coauthored by K. Wayne). Ļat book surveys classic
algorithms for sorting and searching, and for processing graphs and strings.
Our emphasis is on mathematics needed to support scientiŀc studies that can
serve as the basis of predicting performance of such algorithms and for com-
paring diﬀerent algorithms on the basis of performance.
Cormen, Leiserson, Rivest, and Stein’s Introduction to Algorithms has
emerged as the standard textbook that provides access to the research litera-
ture on algorithm design. Ļe book (and related literature) focuses on design
and the theory of algorithms, usually on the basis of worst-case performance
bounds. In this book, we complement this approach by focusing on the anal-
ysis of algorithms, especially on techniques that can be used as the basis for
scientiŀc studies (as opposed to theoretical studies). Chapter 1 is devoted
entirely to developing this context.

P Ş ő Œ ō ŏ ő
ix
Ļis book also lays the groundwork for our Analytic Combinatorics, a
general treatment that places the material here in a broader perspective and
develops advanced methods and models that can serve as the basis for new
research, not only in the analysis of algorithms but also in combinatorics and
scientiŀc applications more broadly. A higher level of mathematical matu-
rity is assumed for that volume, perhaps at the senior or beginning graduate
student level. Of course, careful study of this book is adequate preparation.
It certainly has been our goal to make it suﬃciently interesting that some
readers will be inspired to tackle more advanced material!
How to use this book. Readers of this book are likely to have rather diverse
backgrounds in discrete mathematics and computer science. With this in
mind, it is useful to be aware of the implicit structure of the book: nine chap-
ters in all, an introductory chapter followed by four chapters emphasizing
mathematical methods, then four chapters emphasizing combinatorial struc-
tures with applications in the analysis of algorithms, as follows:
ANALYSIS OF ALGORITHMS
RECURRENCE RELATIONS
GENERATING FUNCTIONS
ASYMPTOTIC APPROXIMATIONS
ANALYTIC COMBINATORICS
TREES
PERMUTATIONS
STRINGS AND TRIES
WORDS AND MAPPINGS
INTRODUCTION
 
DISCRETE MATHEMATICAL METHODS
ALGORITHMS AND COMBINATORIAL STRUCTURES
ONE
TWO
THREE
FOUR
FIVE
SIX
SEVEN
EIGHT
NINE
Chapter 1 puts the material in the book into perspective, and will help all
readers understand the basic objectives of the book and the role of the re-
maining chapters in meeting those objectives. Chapters 2 through 4 cover

x
P Ş ő Œ ō ŏ ő
methods from classical discrete mathematics, with a primary focus on devel-
oping basic concepts and techniques. Ļey set the stage for Chapter 5, which
is pivotal, as it covers analytic combinatorics, a calculus for the study of large
discrete structures that has emerged from these classical methods to help solve
the modern problems that now face researchers because of the emergence of
computers and computational models. Chapters 6 through 9 move the fo-
cus back toward computer science, as they cover properties of combinatorial
structures, their relationships to fundamental algorithms, and analytic results.
Ļough the book is intended to be self-contained, this structure sup-
ports diﬀerences in emphasis when teaching the material, depending on the
background and experience of students and instructor. One approach, more
mathematically oriented, would be to emphasize the theorems and proofs in
the ŀrst part of the book, with applications drawn from Chapters 6 through 9.
Another approach, more oriented towards computer science, would be to
brieły cover the major mathematical tools in Chapters 2 through 5 and em-
phasize the algorithmic material in the second half of the book. But our
primary intention is that most students should be able to learn new mate-
rial from both mathematics and computer science in an interesting context
by working carefully all the way through the book.
Supplementing the text are lists of references and several hundred ex-
ercises, to encourage readers to examine original sources and to consider the
material in the text in more depth.
Our experience in teaching this material has shown that there are nu-
merous opportunities for instructors to supplement lecture and reading ma-
terial with computation-based laboratories and homework assignments. Ļe
material covered here is an ideal framework for students to develop exper-
tise in a symbolic manipulation system such as Mathematica, MAPLE, or
SAGE. More important, the experience of validating the mathematical stud-
ies by comparing them against empirical studies is an opportunity to provide
valuable insights for students that should not be missed.
Booksite. An important feature of the book is its relationship to the booksite
aofa.cs.princeton.edu. Ļis site is freely available and contains supple-
mentary material about the analysis of algorithms, including a complete set
of lecture slides and links to related material, including similar sites for Algo-
rithms and Analytic Combinatorics. Ļese resources are suitable both for use
by any instructor teaching the material and for self-study.

P Ş ő Œ ō ŏ ő
xi
Acknowledgments. We are very grateful to INRIA, Princeton University,
and the National Science Foundation, which provided the primary support
for us to work on this book. Other support has been provided by Brown Uni-
versity, European Community (Alcom Project), Institute for Defense Anal-
yses, Ministère de la Recherche et de la Technologie, Stanford University,
Université Libre de Bruxelles, and Xerox Palo Alto Research Center. Ļis
book has been many years in the making, so a comprehensive list of people
and organizations that have contributed support would be prohibitively long,
and we apologize for any omissions.
Don Knuth’s inłuence on our work has been extremely important, as is
obvious from the text.
Students in Princeton, Paris, and Providence provided helpful feedback
in courses taught from this material over the years, and students and teach-
ers all over the world provided feedback on the ŀrst edition. We would like
to speciŀcally thank Philippe Dumas, Mordecai Golin, Helmut Prodinger,
Michele Soria, Mark Daniel Ward, and Mark Wilson for their help.
Corfu, September 1995
Ph. F. and R. S.
Paris, December 2012
R. S.

This page intentionally left blank 

N O T E O N T H E S E C O N D E D I T I O N
I
N March 2011, I was traveling with my wife Linda in a beautiful but some-
what remote area of the world. Catching up with my mail after a few days
oﬄine, I found the shocking news that my friend and colleague Philippe had
passed away, suddenly, unexpectedly, and far too early. Unable to travel to
Paris in time for the funeral, Linda and I composed a eulogy for our dear
friend that I would now like to share with readers of this book.
Sadly, I am writing from a distant part of the world to pay my respects to my
longtime friend and colleague, Philippe Flajolet. I am very sorry not to be there
in person, but I know that there will be many opportunities to honor Philippe in
the future and expect to be fully and personally involved on these occasions.
Brilliant, creative, inquisitive, and indefatigable, yet generous and charming,
Philippe’s approach to life was contagious. He changed many lives, including
my own. As our research papers led to a survey paper, then to a monograph, then
to a book, then to two books, then to a life’s work, I learned, as many students
and collaborators around the world have learned, that working with Philippe
was based on a genuine and heartfelt camaraderie. We met and worked together
in cafes, bars, lunchrooms, and lounges all around the world. Philippe’s routine
was always the same. We would discuss something amusing that happened to one
friend or another and then get to work. After a wink, a hearty but quick laugh,
a puﬀof smoke, another sip of a beer, a few bites of steak frites, and a drawn
out “Well...” we could proceed to solve the problem or prove the theorem. For so
many of us, these moments are frozen in time.
Ļe world has lost a brilliant and productive mathematician. Philippe’s un-
timely passing means that many things may never be known. But his legacy is
a coterie of followers passionately devoted to Philippe and his mathematics who
will carry on. Our conferences will include a toast to him, our research will build
upon his work, our papers will include the inscription “Dedicated to the memory
of Philippe Flajolet ,” and we will teach generations to come. Dear friend, we
miss you so very much, but rest assured that your spirit will live on in our work.
Ļis second edition of our book An Introduction to the Analysis of Algorithms
was prepared with these thoughts in mind. It is dedicated to the memory of
Philippe Flajolet, and is intended to teach generations to come.
Jamestown RI, October 2012
R. S.

This page intentionally left blank 

T A B L E O F C O N T E N T S
CŔōŜŠőŞ OŚő: AŚōŘťşŕş śŒ AŘœśŞŕŠŔřş
3
1.1
Why Analyze an Algorithm?
3
1.2
Ļeory of Algorithms
6
1.3
Analysis of Algorithms
13
1.4
Average-Case Analysis
16
1.5
Example: Analysis of Quicksort
18
1.6
Asymptotic Approximations
27
1.7
Distributions
30
1.8
Randomized Algorithms
33
CŔōŜŠőŞ Tţś: RőŏšŞŞőŚŏő RőŘōŠŕśŚş
41
2.1
Basic Properties
43
2.2
First-Order Recurrences
48
2.3
Nonlinear First-Order Recurrences
52
2.4
Higher-Order Recurrences
55
2.5
Methods for Solving Recurrences
61
2.6
Binary Divide-and-Conquer Recurrences and Binary
70
Numbers
2.7
General Divide-and-Conquer Recurrences
80
CŔōŜŠőŞ TŔŞőő: GőŚőŞōŠŕŚœ FšŚŏŠŕśŚş
91
3.1
Ordinary Generating Functions
92
3.2
Exponential Generating Functions
97
3.3
Generating Function Solution of Recurrences
101
3.4
Expanding Generating Functions
111
3.5
Transformations with Generating Functions
114
3.6
Functional Equations on Generating Functions
117
3.7
Solving the Quicksort Median-of-Ļree Recurrence
120
with OGFs
3.8
Counting with Generating Functions
123
3.9
Probability Generating Functions
129
3.10 Bivariate Generating Functions
132
3.11 Special Functions
140
xv

xvi
T ō Ŏ Ř ő
ś Œ
C ś Ś Š ő Ś Š ş
CŔōŜŠőŞ FśšŞ: AşťřŜŠśŠŕŏ AŜŜŞśŤŕřōŠŕśŚş
151
4.1
Notation for Asymptotic Approximations
153
4.2
Asymptotic Expansions
160
4.3
Manipulating Asymptotic Expansions
169
4.4
Asymptotic Approximations of Finite Sums
176
4.5
Euler-Maclaurin Summation
179
4.6
Bivariate Asymptotics
187
4.7
Laplace Method
203
4.8
“Normal” Examples from the Analysis of Algorithms
207
4.9
“Poisson” Examples from the Analysis of Algorithms
211
CŔōŜŠőŞ FŕŢő: AŚōŘťŠŕŏ CśřŎŕŚōŠśŞŕŏş
219
5.1
Formal Basis
220
5.2
Symbolic Method for Unlabelled Classes
221
5.3
Symbolic Method for Labelled Classes
229
5.4
Symbolic Method for Parameters
241
5.5
Generating Function Coeﬃcient Asymptotics
247
CŔōŜŠőŞ SŕŤ: TŞőőş
257
6.1
Binary Trees
258
6.2
Forests and Trees
261
6.3
Combinatorial Equivalences to Trees and Binary Trees
264
6.4
Properties of Trees
272
6.5
Examples of Tree Algorithms
277
6.6
Binary Search Trees
281
6.7
Average Path Length in Catalan Trees
287
6.8
Path Length in Binary Search Trees
293
6.9
Additive Parameters of Random Trees
297
6.10 Height
302
6.11 Summary of Average-Case Results on Properties of Trees
310
6.12 Lagrange Inversion
312
6.13 Rooted Unordered Trees
315
6.14 Labelled Trees
327
6.15 Other Types of Trees
331

T ō Ŏ Ř ő
ś Œ
C ś Ś Š ő Ś Š ş
xvii
CŔōŜŠőŞ SőŢőŚ: PőŞřšŠōŠŕśŚş
345
7.1
Basic Properties of Permutations
347
7.2
Algorithms on Permutations
355
7.3
Representations of Permutations
358
7.4
Enumeration Problems
366
7.5
Analyzing Properties of Permutations with CGFs
372
7.6
Inversions and Insertion Sorts
384
7.7
Left-to-Right Minima and Selection Sort
393
7.8
Cycles and In Situ Permutation
401
7.9
Extremal Parameters
406
CŔōŜŠőŞ EŕœŔŠ: SŠŞŕŚœş ōŚŐ TŞŕőş
415
8.1
String Searching
416
8.2
Combinatorial Properties of Bitstrings
420
8.3
Regular Expressions
432
8.4
Finite-State Automata and the Knuth-Morris-Pratt
437
Algorithm
8.5
Context-Free Grammars
441
8.6
Tries
448
8.7
Trie Algorithms
453
8.8
Combinatorial Properties of Tries
459
8.9
Larger Alphabets
465
CŔōŜŠőŞ NŕŚő: WśŞŐş ōŚŐ MōŜŜŕŚœş
473
9.1
Hashing with Separate Chaining
474
9.2
Ļe Balls-and-Urns Model and Properties of Words
476
9.3
Birthday Paradox and Coupon Collector Problem
485
9.4
Occupancy Restrictions and Extremal Parameters
495
9.5
Occupancy Distributions
501
9.6
Open Addressing Hashing
509
9.7
Mappings
519
9.8
Integer Factorization and Mappings
532
List of Ļeorems
543
List of Tables
545
List of Figures
547
Index
551

This page intentionally left blank 

N O T A T I O N
⌊x⌋
łoor function
largest integer less than or equal to x
⌈x⌉
ceiling function
smallest integer greater than or equal to x
{x}
fractional part
x −⌊x⌋
lgN
binary logarithm
log2N
lnN
natural logarithm
logeN
(
n
k
)
binomial coeﬃcient
number of ways to choose k out of n items
[
n
k
]
Stirling number of the ŀrst kind
number of permutations of n elements that have k cycles
{
n
k
}
Stirling number of the second kind
number of ways to partition n elements into k nonempty subsets
ϕ
golden ratio
(1 +
√
5)/2 = 1.61803 · · ·
γ
Euler’s constant
.57721 · · ·
σ
Stirling’s constant
√
2π = 2.50662 · · ·

This page intentionally left blank 

C H A P T E R O N E
A N A L Y S I S O F A L G O R I T H M S
M
ATHEMATICAL studies of the properties of computer algorithms
have spanned a broad spectrum, from general complexity studies to
speciŀc analytic results. In this chapter, our intent is to provide perspective
on various approaches to studying algorithms, to place our ŀeld of study into
context among related ŀelds and to set the stage for the rest of the book.
To this end, we illustrate concepts within a fundamental and representative
problem domain: the study of sorting algorithms.
First, we will consider the general motivations for algorithmic analysis.
Why analyze an algorithm? What are the beneŀts of doing so? How can we
simplify the process? Next, we discuss the theory of algorithms and consider
as an example mergesort, an “optimal” algorithm for sorting. Following that,
we examine the major components of a full analysis for a sorting algorithm of
fundamental practical importance, quicksort. Ļis includes the study of vari-
ous improvements to the basic quicksort algorithm, as well as some examples
illustrating how the analysis can help one adjust parameters to improve per-
formance.
Ļese examples illustrate a clear need for a background in certain areas
of discrete mathematics. In Chapters 2 through 4, we introduce recurrences,
generating functions, and asymptotics—basic mathematical concepts needed
for the analysis of algorithms. In Chapter 5, we introduce the symbolic method,
a formal treatment that ties together much of this book’s content. In Chap-
ters 6 through 9, we consider basic combinatorial properties of fundamental
algorithms and data structures. Since there is a close relationship between
fundamental methods used in computer science and classical mathematical
analysis, we simultaneously consider some introductory material from both
areas in this book.
1.1 WhyAnalyzeanAlgorithm? Ļere are several answers to this basic ques-
tion, depending on one’s frame of reference: the intended use of the algo-
rithm, the importance of the algorithm in relationship to others from both
practical and theoretical standpoints, the diﬃculty of analysis, and the accu-
racy and precision of the required answer.
Ț

ț
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ș
Ļe most straightforward reason for analyzing an algorithm is to dis-
cover its characteristics in order to evaluate its suitability for various appli-
cations or compare it with other algorithms for the same application. Ļe
characteristics of interest are most often the primary resources of time and
space, particularly time. Put simply, we want to know how long an imple-
mentation of a particular algorithm will run on a particular computer, and
how much space it will require. We generally strive to keep the analysis inde-
pendent of particular implementations—we concentrate instead on obtaining
results for essential characteristics of the algorithm that can be used to derive
precise estimates of true resource requirements on various actual machines.
In practice, achieving independence between an algorithm and char-
acteristics of its implementation can be diﬃcult to arrange. Ļe quality of
the implementation and properties of compilers, machine architecture, and
other major facets of the programming environment have dramatic eﬀects on
performance. We must be cognizant of such eﬀects to be sure the results of
analysis are useful. On the other hand, in some cases, analysis of an algo-
rithm can help identify ways for it to take full advantage of the programming
environment.
Occasionally, some property other than time or space is of interest, and
the focus of the analysis changes accordingly. For example, an algorithm on
a mobile device might be studied to determine the eﬀect upon battery life,
or an algorithm for a numerical problem might be studied to determine how
accurate an answer it can provide. Also, it is sometimes appropriate to address
multiple resources in the analysis. For example, an algorithm that uses a large
amount of memory may use much less time than an algorithm that gets by
with very little memory. Indeed, one prime motivation for doing a careful
analysis is to provide accurate information to help in making proper tradeoﬀ
decisions in such situations.
Ļe term analysis of algorithms has been used to describe two quite diﬀer-
ent general approaches to putting the study of the performance of computer
programs on a scientiŀc basis. We consider these two in turn.
Ļe ŀrst, popularized by Aho, Hopcroft, and Ullman [2] and Cormen,
Leiserson, Rivest, and Stein [6], concentrates on determining the growth of
the worst-case performance of the algorithm (an “upper bound”). A prime
goal in such analyses is to determine which algorithms are optimal in the sense
that a matching “lower bound” can be proved on the worst-case performance
of any algorithm for the same problem. We use the term theory of algorithms

§Ș.Ș
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
Ȝ
to refer to this type of analysis. It is a special case of computational complexity,
the general study of relationships between problems, algorithms, languages,
and machines. Ļe emergence of the theory of algorithms unleashed an Age
of Design where multitudes of new algorithms with ever-improving worst-
case performance bounds have been developed for multitudes of important
problems. To establish the practical utility of such algorithms, however, more
detailed analysis is needed, perhaps using the tools described in this book.
Ļe second approach to the analysis of algorithms, popularized by Knuth
[17][18][19][20][22], concentrates on precise characterizations of the best-
case, worst-case, and average-case performance of algorithms, using a method-
ology that can be reŀned to produce increasingly precise answers when de-
sired. A prime goal in such analyses is to be able to accurately predict the
performance characteristics of particular algorithms when run on particular
computers, in order to be able to predict resource usage, set parameters, and
compare algorithms. Ļis approach is scientiŀc: we build mathematical mod-
els to describe the performance of real-world algorithm implementations,
then use these models to develop hypotheses that we validate through ex-
perimentation.
We may view both these approaches as necessary stages in the design
and analysis of eﬃcient algorithms. When faced with a new algorithm to
solve a new problem, we are interested in developing a rough idea of how
well it might be expected to perform and how it might compare to other
algorithms for the same problem, even the best possible. Ļe theory of algo-
rithms can provide this. However, so much precision is typically sacriŀced
in such an analysis that it provides little speciŀc information that would al-
low us to predict performance for an actual implementation or to properly
compare one algorithm to another. To be able to do so, we need details on
the implementation, the computer to be used, and, as we see in this book,
mathematical properties of the structures manipulated by the algorithm. Ļe
theory of algorithms may be viewed as the ŀrst step in an ongoing process of
developing a more reŀned, more accurate analysis; we prefer to use the term
analysis of algorithms to refer to the whole process, with the goal of providing
answers with as much accuracy as necessary.
Ļe analysis of an algorithm can help us understand it better, and can
suggest informed improvements. Ļe more complicated the algorithm, the
more diﬃcult the analysis. But it is not unusual for an algorithm to become
simpler and more elegant during the analysis process. More important, the

ȝ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.ș
careful scrutiny required for proper analysis often leads to better and more ef-
ŀcient implementation on particular computers. Analysis requires a far more
complete understanding of an algorithm that can inform the process of pro-
ducing a working implementation. Indeed, when the results of analytic and
empirical studies agree, we become strongly convinced of the validity of the
algorithm as well as of the correctness of the process of analysis.
Some algorithms are worth analyzing because their analyses can add to
the body of mathematical tools available. Such algorithms may be of limited
practical interest but may have properties similar to algorithms of practical
interest so that understanding them may help to understand more important
methods in the future. Other algorithms (some of intense practical inter-
est, some of little or no such value) have a complex performance structure
with properties of independent mathematical interest. Ļe dynamic element
brought to combinatorial problems by the analysis of algorithms leads to chal-
lenging, interesting mathematical problems that extend the reach of classical
combinatorics to help shed light on properties of computer programs.
To bring these ideas into clearer focus, we next consider in detail some
classical results ŀrst from the viewpoint of the theory of algorithms and then
from the scientiŀc viewpoint that we develop in this book. As a running
example to illustrate the diﬀerent perspectives, we study sorting algorithms,
which rearrange a list to put it in numerical, alphabetic, or other order. Sort-
ing is an important practical problem that remains the object of widespread
study because it plays a central role in many applications.
1.2 Ļeory of Algorithms. Ļe prime goal of the theory of algorithms
is to classify algorithms according to their performance characteristics. Ļe
following mathematical notations are convenient for doing so:
Deŀnition Given a function f(N),
O(f(N)) denotes the set of all g(N) such that |g(N)/f(N)| is bounded
from above as N →∞.

(f(N)) denotes the set of all g(N) such that |g(N)/f(N)| is bounded
from below by a (strictly) positive number as N →∞.
(f(N)) denotes the set of all g(N) such that |g(N)/f(N)| is bounded
from both above and below as N →∞.
Ļese notations, adapted from classical analysis, were advocated for use in
the analysis of algorithms in a paper by Knuth in 1976 [21]. Ļey have come

§Ș.ș
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
Ȟ
into widespread use for making mathematical statements about bounds on
the performance of algorithms. Ļe O-notation provides a way to express an
upper bound; the 
-notation provides a way to express a lower bound; and
the -notation provides a way to express matching upper and lower bounds.
In mathematics, the most common use of the O-notation is in the con-
text of asymptotic series. We will consider this usage in detail in Chapter 4.
In the theory of algorithms, the O-notation is typically used for three pur-
poses: to hide constants that might be irrelevant or inconvenient to compute,
to express a relatively small “error” term in an expression describing the run-
ning time of an algorithm, and to bound the worst case. Nowadays, the 
-
and - notations are directly associated with the theory of algorithms, though
similar notations are used in mathematics (see [21]).
Since constant factors are being ignored, derivation of mathematical re-
sults using these notations is simpler than if more precise answers are sought.
For example, both the “natural” logarithm lnN ≡logeN and the “binary”
logarithm lgN ≡log2N often arise, but they are related by a constant factor,
so we can refer to either as being O(logN) if we are not interested in more
precision. More to the point, we might say that the running time of an al-
gorithm is (NlogN) seconds just based on an analysis of the frequency of
execution of fundamental operations and an assumption that each operation
takes a constant number of seconds on a given computer, without working
out the precise value of the constant.
Exercise 1.1 Show that f(N) = NlgN + O(N) implies that f(N) = Θ(NlogN).
As an illustration of the use of these notations to study the performance
characteristics of algorithms, we consider methods for sorting a set of num-
bers in an array. Ļe input is the numbers in the array, in arbitrary and un-
known order; the output is the same numbers in the array, rearranged in as-
cending order. Ļis is a well-studied and fundamental problem: we will con-
sider an algorithm for solving it, then show that algorithm to be “optimal” in
a precise technical sense.
First, we will show that it is possible to solve the sorting problem ef-
ŀciently, using a well-known recursive algorithm called mergesort. Merge-
sort and nearly all of the algorithms treated in this book are described in
detail in Sedgewick and Wayne [30], so we give only a brief description here.
Readers interested in further details on variants of the algorithms, implemen-
tations, and applications are also encouraged to consult the books by Cor-

ȟ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.ș
men, Leiserson, Rivest, and Stein [6], Gonnet and Baeza-Yates [11], Knuth
[17][18][19][20], Sedgewick [26], and other sources.
Mergesort divides the array in the middle, sorts the two halves (recur-
sively), and then merges the resulting sorted halves together to produce the
sorted result, as shown in the Java implementation in Program 1.1. Merge-
sort is prototypical of the well-known divide-and-conquer algorithm design
paradigm, where a problem is solved by (recursively) solving smaller sub-
problems and using the solutions to solve the original problem. We will an-
alyze a number of such algorithms in this book. Ļe recursive structure of
algorithms like mergesort leads immediately to mathematical descriptions of
their performance characteristics.
To accomplish the merge, Program 1.1 uses two auxiliary arrays b and
c to hold the subarrays (for the sake of eﬃciency, it is best to declare these
arrays external to the recursive method). Invoking this method with the call
mergesort(0, N-1) will sort the array a[0...N-1]. After the recursive
private void mergesort(int[] a, int lo, int hi)
{
if (hi <= lo) return;
int mid = lo + (hi - lo) / 2;
mergesort(a, lo, mid);
mergesort(a, mid + 1, hi);
for (int k = lo; k <= mid; k++)
b[k-lo] = a[k];
for (int k = mid+1; k <= hi; k++)
c[k-mid-1] = a[k];
b[mid-lo+1] = INFTY; c[hi - mid] = INFTY;
int i = 0, j = 0;
for (int k = lo; k <= hi; k++)
if (c[j] < b[i]) a[k] = c[j++];
else
a[k] = b[i++];
}
Program 1.1 Mergesort

§Ș.ș
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
Ƞ
calls, the two halves of the array are sorted. Ļen we move the ŀrst half of
a[] to an auxiliary array b[] and the second half of a[] to another auxiliary
array c[]. We add a “sentinel” INFTY that is assumed to be larger than all
the elements to the end of each of the auxiliary arrays, to help accomplish the
task of moving the remainder of one of the auxiliary arrays back to a after the
other one has been exhausted. With these preparations, the merge is easily
accomplished: for each k, move the smaller of the elements b[i] and c[j]
to a[k], then increment k and i or j accordingly.
Exercise 1.2 In some situations, deŀning a sentinel value may be inconvenient or
impractical. Implement a mergesort that avoids doing so (see Sedgewick [26] for
various strategies).
Exercise 1.3 Implement a mergesort that divides the array into three equal parts,
sorts them, and does a three-way merge. Empirically compare its running time with
standard mergesort.
In the present context, mergesort is signiŀcant because it is guaranteed
to be as eﬃcient as any sorting method can be. To make this claim more
precise, we begin by analyzing the dominant factor in the running time of
mergesort, the number of compares that it uses.
Ļeorem 1.1 (Mergesort compares).
Mergesort uses NlgN +O(N) com-
pares to sort an array of N elements.
Proof. If CN is the number of compares that the Program 1.1 uses to sort N
elements, then the number of compares to sort the ŀrst half is C⌊N/2⌋, the
number of compares to sort the second half is C⌈N/2⌉, and the number of
compares for the merge is N (one for each value of the index k). In other
words, the number of compares for mergesort is precisely described by the
recurrence relation
CN = C⌊N/2⌋+ C⌈N/2⌉+ N
for N ≥2 with C1 = 0.
(1)
To get an indication for the nature of the solution to this recurrence, we con-
sider the case when N is a power of 2:
C2n = 2C2n−1 + 2n
for n ≥1 with C1 = 0.
Dividing both sides of this equation by 2n, we ŀnd that
C2n
2n = C2n−1
2n−1 + 1 = C2n−2
2n−2 + 2 = C2n−3
2n−3 + 3 = . . . = C20
20 + n = n.

Șȗ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.ș
Ļis proves that CN = NlgN when N = 2n; the theorem for general
N follows from (1) by induction. Ļe exact solution turns out to be rather
complicated, depending on properties of the binary representation of N. In
Chapter 2 we will examine how to solve such recurrences in detail.
Exercise 1.4 Develop a recurrence describing the quantity CN+1 −CN and use this
to prove that
CN =
∑
1≤k<N
(⌊lgk⌋+ 2).
Exercise 1.5 Prove that CN = N⌈lgN⌉+ N −2⌈lgN⌉.
Exercise 1.6 Analyze the number of compares used by the three-way mergesort pro-
posed in Exercise 1.2.
For most computers, the relative costs of the elementary operations used
Program 1.1 will be related by a constant factor, as they are all integer mul-
tiples of the cost of a basic instruction cycle. Furthermore, the total running
time of the program will be within a constant factor of the number of com-
pares. Ļerefore, a reasonable hypothesis is that the running time of merge-
sort will be within a constant factor of NlgN.
From a theoretical standpoint, mergesort demonstrates that NlogN is
an “upper bound” on the intrinsic diﬃculty of the sorting problem:
Ļere exists an algorithm that can sort any
N-element ŀle in time proportional to NlogN.
A full proof of this requires a careful model of the computer to be used in terms
of the operations involved and the time they take, but the result holds under
rather generous assumptions. We say that the “time complexity of sorting is
O(NlogN).”
Exercise 1.7 Assume that the running time of mergesort is cNlgN + dN, where c
and d are machine-dependent constants. Show that if we implement the program on
a particular machine and observe a running time tN for some value of N, then we
can accurately estimate the running time for 2N by 2tN(1 + 1/lgN), independent of
the machine.
Exercise 1.8 Implement mergesort on one or more computers, observe the running
time for N = 1,000,000, and predict the running time for N = 10,000,000 as in the
previous exercise. Ļen observe the running time for N = 10,000,000 and calculate
the percentage accuracy of the prediction.

§Ș.ș
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȘȘ
Ļe running time of mergesort as implemented here depends only on
the number of elements in the array being sorted, not on the way they are
arranged. For many other sorting methods, the running time may vary sub-
stantially as a function of the initial ordering of the input. Typically, in the
theory of algorithms, we are most interested in worst-case performance, since
it can provide a guarantee on the performance characteristics of the algorithm
no matter what the input is; in the analysis of particular algorithms, we are
most interested in average-case performance for a reasonable input model,
since that can provide a path to predict performance on “typical” input.
We always seek better algorithms, and a natural question that arises is
whether there might be a sorting algorithm with asymptotically better per-
formance than mergesort. Ļe following classical result from the theory of
algorithms says, in essence, that there is not.
Ļeorem 1.2 (Complexity of sorting).
Every compare-based sorting pro-
gram uses at least ⌈lgN!⌉> NlgN −N/(ln2) compares for some input.
Proof. A full proof of this fact may be found in [30] or [19]. Intuitively the
result follows from the observation that each compare can cut down the num-
ber of possible arrangements of the elements to be considered by, at most, only
a factor of 2. Since there are N! possible arrangements before the sort and
the goal is to have just one possible arrangement (the sorted one) after the
sort, the number of compares must be at least the number of times N! can be
divided by 2 before reaching a number less than unity—that is, ⌈lgN!⌉. Ļe
theorem follows from Stirling’s approximation to the factorial function (see
the second corollary to Ļeorem 4.3).
From a theoretical standpoint, this result demonstrates that NlogN is
a “lower bound” on the intrinsic diﬃculty of the sorting problem:
All compare-based sorting algorithms require time
proportional to NlogN to sort some N-element input ŀle.
Ļis is a general statement about an entire class of algorithms. We say that
the “time complexity of sorting is 
(NlogN).” Ļis lower bound is sig-
niŀcant because it matches the upper bound of Ļeorem 1.1, thus showing
that mergesort is optimal in the sense that no algorithm can have a better
asymptotic running time. We say that the “time complexity of sorting is
(NlogN).” From a theoretical standpoint, this completes the “solution” of
the sorting “problem:” matching upper and lower bounds have been proved.

Șș
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.ș
Again, these results hold under rather generous assumptions, though
they are perhaps not as general as it might seem. For example, the results say
nothing about sorting algorithms that do not use compares. Indeed, there
exist sorting methods based on index calculation techniques (such as those
discussed in Chapter 9) that run in linear time on average.
Exercise 1.9 Suppose that it is known that each of the items in an N-item array has
one of two distinct values. Give a sorting method that takes time proportional to N.
Exercise 1.10 Answer the previous exercise for three distinct values.
We have omitted many details that relate to proper modeling of comput-
ers and programs in the proofs of Ļeorem 1.1 and Ļeorem 1.2. Ļe essence
of the theory of algorithms is the development of complete models within
which the intrinsic diﬃculty of important problems can be assessed and “ef-
ŀcient” algorithms representing upper bounds matching these lower bounds
can be developed. For many important problem domains there is still a sig-
niŀcant gap between the lower and upper bounds on asymptotic worst-case
performance. Ļe theory of algorithms provides guidance in the development
of new algorithms for such problems. We want algorithms that can lower
known upper bounds, but there is no point in searching for an algorithm that
performs better than known lower bounds (except perhaps by looking for one
that violates conditions of the model upon which a lower bound is based!).
Ļus, the theory of algorithms provides a way to classify algorithms
according to their asymptotic performance. However, the very process of
approximate analysis (“within a constant factor”) that extends the applicability
of theoretical results often limits our ability to accurately predict the perfor-
mance characteristics of any particular algorithm. More important, the theory
of algorithms is usually based on worst-case analysis, which can be overly pes-
simistic and not as helpful in predicting actual performance as an average-case
analysis. Ļis is not relevant for algorithms like mergesort (where the running
time is not so dependent on the input), but average-case analysis can help us
discover that nonoptimal algorithms are sometimes faster in practice, as we
will see. Ļe theory of algorithms can help us to identify good algorithms,
but then it is of interest to reŀne the analysis to be able to more intelligently
compare and improve them. To do so, we need precise knowledge about the
performance characteristics of the particular computer being used and math-
ematical techniques for accurately determining the frequency of execution of
fundamental operations. In this book, we concentrate on such techniques.

§Ș.Ț
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȘȚ
1.3 Analysis of Algorithms. Ļough the analysis of sorting and merge-
sort that we considered in §1.2 demonstrates the intrinsic “diﬃculty” of the
sorting problem, there are many important questions related to sorting (and
to mergesort) that it does not address at all. How long might an implemen-
tation of mergesort be expected to run on a particular computer? How might
its running time compare to other O(NlogN) methods? (Ļere are many.)
How does it compare to sorting methods that are fast on average, but per-
haps not in the worst case? How does it compare to sorting methods that are
not based on compares among elements? To answer such questions, a more
detailed analysis is required. In this section we brieły describe the process of
doing such an analysis.
To analyze an algorithm, we must ŀrst identify the resources of primary
interest so that the detailed analysis may be properly focused. We describe the
process in terms of studying the running time since it is the resource most rel-
evant here. A complete analysis of the running time of an algorithm involves
the following steps:
• Implement the algorithm completely.
• Determine the time required for each basic operation.
• Identify unknown quantities that can be used to describe the frequency
of execution of the basic operations.
• Develop a realistic model for the input to the program.
• Analyze the unknown quantities, assuming the modeled input.
• Calculate the total running time by multiplying the time by the fre-
quency for each operation, then adding all the products.
Ļe ŀrst step in the analysis is to carefully implement the algorithm on a
particular computer. We reserve the term program to describe such an imple-
mentation. One algorithm corresponds to many programs. A particular im-
plementation not only provides a concrete object to study, but also can give
useful empirical data to aid in or to check the analysis. Presumably the im-
plementation is designed to make eﬃcient use of resources, but it is a mistake
to overemphasize eﬃciency too early in the process. Indeed, a primary appli-
cation for the analysis is to provide informed guidance toward better imple-
mentations.
Ļe next step is to estimate the time required by each component in-
struction of the program. In principle and in practice, we can often do so
with great precision, but the process is very dependent on the characteristics

Șț
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ț
of the computer system being studied. Another approach is to simply run
the program for small input sizes to “estimate” the values of the constants, or
to do so indirectly in the aggregate, as described in Exercise 1.7. We do not
consider this process in detail; rather we focus on the “machine-independent”
parts of the analysis in this book.
Indeed, to determine the total running time of the program, it is neces-
sary to study the branching structure of the program in order to express the
frequency of execution of the component instructions in terms of unknown
mathematical quantities. If the values of these quantities are known, then we
can derive the running time of the entire program simply by multiplying the
frequency and time requirements of each component instruction and adding
these products. Many programming environments have tools that can sim-
plify this task. At the ŀrst level of analysis, we concentrate on quantities that
have large frequency values or that correspond to large costs; in principle the
analysis can be reŀned to produce a fully detailed answer. We often refer
to the “cost” of an algorithm as shorthand for the “value of the quantity in
question” when the context allows.
Ļe next step is to model the input to the program, to form a basis for
the mathematical analysis of the instruction frequencies. Ļe values of the
unknown frequencies are dependent on the input to the algorithm: the prob-
lem size (usually we name that N) is normally the primary parameter used to
express our results, but the order or value of input data items ordinarily af-
fects the running time as well. By “model,” we mean a precise description of
typical inputs to the algorithm. For example, for sorting algorithms, it is nor-
mally convenient to assume that the inputs are randomly ordered and distinct,
though the programs normally work even when the inputs are not distinct.
Another possibility for sorting algorithms is to assume that the inputs are
random numbers taken from a relatively large range. Ļese two models can
be shown to be nearly equivalent. Most often, we use the simplest available
model of “random” inputs, which is often realistic. Several diﬀerent models
can be used for the same algorithm: one model might be chosen to make the
analysis as simple as possible; another model might better rełect the actual
situation in which the program is to be used.
Ļe last step is to analyze the unknown quantities, assuming the mod-
eled input. For average-case analysis, we analyze the quantities individually,
then multiply the averages by instruction times and add them to ŀnd the run-
ning time of the whole program. For worst-case analysis, it is usually diﬃcult

§Ș.Ț
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȘȜ
to get an exact result for the whole program, so we can only derive an upper
bound, by multiplying worst-case values of the individual quantities by in-
struction times and summing the results.
Ļis general scenario can successfully provide exact models in many sit-
uations. Knuth’s books [17][18][19][20] are based on this precept. Unfortu-
nately, the details in such an exact analysis are often daunting. Accordingly,
we typically seek approximate models that we can use to estimate costs.
Ļe ŀrst reason to approximate is that determining the cost details of all
individual operations can be daunting in the context of the complex architec-
tures and operating systems on modern computers. Accordingly, we typically
study just a few quantities in the “inner loop” of our programs, implicitly
hypothesizing that total cost is well estimated by analyzing just those quan-
tities. Experienced programmers regularly “proŀle” their implementations to
identify “bottlenecks,” which is a systematic way to identify such quantities.
For example, we typically analyze compare-based sorting algorithms by just
counting compares. Such an approach has the important side beneŀt that it
is machine independent. Carefully analyzing the number of compares used by
a sorting algorithm can enable us to predict performance on many diﬀerent
computers. Associated hypotheses are easily tested by experimentation, and
we can reŀne them, in principle, when appropriate. For example, we might
reŀne comparison-based models for sorting to include data movement, which
may require taking caching eﬀects into account.
Exercise 1.11 Run experiments on two diﬀerent computers to test the hypothesis
that the running time of mergesort divided by the number of compares that it uses
approaches a constant as the problem size increases.
Approximation is also eﬀective for mathematical models. Ļe second
reason to approximate is to avoid unnecessary complications in the mathe-
matical formulae that we develop to describe the performance of algorithms.
A major theme of this book is the development of classical approximation
methods for this purpose, and we shall consider many examples. Beyond
these, a major thrust of modern research in the analysis of algorithms is meth-
ods of developing mathematical analyses that are simple, suﬃciently precise
that they can be used to accurately predict performance and to compare algo-
rithms, and able to be reŀned, in principle, to the precision needed for the
application at hand. Such techniques primarily involve complex analysis and
are fully developed in our book [10].

Șȝ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.ț
1.4 Average-Case Analysis. Ļe mathematical techniques that we con-
sider in this book are not just applicable to solving problems related to the
performance of algorithms, but also to mathematical models for all manner
of scientiŀc applications, from genomics to statistical physics. Accordingly,
we often consider structures and techniques that are broadly applicable. Still,
our prime motivation is to consider mathematical tools that we need in or-
der to be able to make precise statements about resource usage of important
algorithms in practical applications.
Our focus is on average-case analysis of algorithms: we formulate a rea-
sonable input model and analyze the expected running time of a program
given an input drawn from that model. Ļis approach is eﬀective for two
primary reasons.
Ļe ŀrst reason that average-case analysis is important and eﬀective in
modern applications is that straightforward models of randomness are often
extremely accurate. Ļe following are just a few representative examples from
sorting applications:
• Sorting is a fundamental process in cryptanalysis, where the adversary has
gone to great lengths to make the data indistinguishable from random
data.
• Commercial data processing systems routinely sort huge ŀles where keys
typically are account numbers or other identiŀcation numbers that are
well modeled by uniformly random numbers in an appropriate range.
• Implementations of computer networks depend on sorts that again involve
keys that are well modeled by random ones.
• Sorting is widely used in computational biology, where signiŀcant devi-
ations from randomness are cause for further investigation by scientists
trying to understand fundamental biological and physical processes.
As these examples indicate, simple models of randomness are eﬀective, not
just for sorting applications, but also for a wide variety of uses of fundamental
algorithms in practice. Broadly speaking, when large data sets are created by
humans, they typically are based on arbitrary choices that are well modeled
by random ones. Random models also are often eﬀective when working with
scientiŀc data. We might interpret Einstein’s oft-repeated admonition that
“God does not play dice” in this context as meaning that random models are
eﬀective, because if we discover signiŀcant deviations from randomness, we
have learned something signiŀcant about the natural world.

§Ș.ț
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȘȞ
Ļe second reason that average-case analysis is important and eﬀective
in modern applications is that we can often manage to inject randomness
into a problem instance so that it appears to the algorithm (and to the ana-
lyst) to be random. Ļis is an eﬀective approach to developing eﬃcient algo-
rithms with predictable performance, which are known as randomized algo-
rithms. M. O. Rabin [25] was among the ŀrst to articulate this approach, and
it has been developed by many other researchers in the years since. Ļe book
by Motwani and Raghavan [23] is a thorough introduction to the topic.
Ļus, we begin by analyzing random models, and we typically start with
the challenge of computing the mean—the average value of some quantity
of interest for N instances drawn at random. Now, elementary probability
theory gives a number of diﬀerent (though closely related) ways to compute
the average value of a quantity. In this book, it will be convenient for us to
explicitly identify two diﬀerent approaches to doing so.
Distributional. Let N be the number of possible inputs of size N and Nk
be the number of inputs of size N that cause the algorithm to have cost k, so
that N = ∑
k Nk. Ļen the probability that the cost is k is Nk/N and
the expected cost is
1
N
∑
k
kNk.
Ļe analysis depends on “counting.” How many inputs are there of size N
and how many inputs of size N cause the algorithm to have cost k? Ļese
are the steps to compute the probability that the cost is k, so this approach is
perhaps the most direct from elementary probability theory.
Cumulative. Let N be the total (or cumulated) cost of the algorithm on
all inputs of size N. (Ļat is, N = ∑
k kNk, but the point is that it is
not necessary to compute N in that way.) Ļen the average cost is simply
N/N. Ļe analysis depends on a less speciŀc counting problem: what is
the total cost of the algorithm, on all inputs? We will be using general tools
that make this approach very attractive.
Ļe distributional approach gives complete information, which can be
used directly to compute the standard deviation and other moments. Indi-
rect (often simpler) methods are also available for computing moments when
using the cumulative approach, as we will see. In this book, we consider
both approaches, though our tendency will be toward the cumulative method,

Șȟ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ȝ
which ultimately allows us to consider the analysis of algorithms in terms of
combinatorial properties of basic data structures.
Many algorithms solve a problem by recursively solving smaller sub-
problems and are thus amenable to the derivation of a recurrence relationship
that the average cost or the total cost must satisfy. A direct derivation of a
recurrence from the algorithm is often a natural way to proceed, as shown in
the example in the next section.
No matter how they are derived, we are interested in average-case results
because, in the large number of situations where random input is a reasonable
model, an accurate analysis can help us:
• Compare diﬀerent algorithms for the same task.
• Predict time and space requirements for speciŀc applications.
• Compare diﬀerent computers that are to run the same algorithm.
• Adjust algorithm parameters to optimize performance.
Ļe average-case results can be compared with empirical data to validate the
implementation, the model, and the analysis. Ļe end goal is to gain enough
conŀdence in these that they can be used to predict how the algorithm will
perform under whatever circumstances present themselves in particular appli-
cations. If we wish to evaluate the possible impact of a new machine archi-
tecture on the performance of an important algorithm, we can do so through
analysis, perhaps before the new architecture comes into existence. Ļe suc-
cess of this approach has been validated over the past several decades: the
sorting algorithms that we consider in the section were ŀrst analyzed more
than 50 years ago, and those analytic results are still useful in helping us eval-
uate their performance on today’s computers.
1.5 Example: Analysis of Quicksort. To illustrate the basic method just
sketched, we examine next a particular algorithm of considerable importance,
the quicksort sorting method. Ļis method was invented in 1962 by C. A. R.
Hoare, whose paper [15] is an early and outstanding example in the analysis
of algorithms. Ļe analysis is also covered in great detail in Sedgewick [27]
(see also [29]); we give highlights here. It is worthwhile to study this analysis
in detail not just because this sorting method is widely used and the analytic
results are directly relevant to practice, but also because the analysis itself is
illustrative of many things that we will encounter later in the book. In partic-
ular, it turns out that the same analysis applies to the study of basic properties
of tree structures, which are of broad interest and applicability. More gen-

§Ș.Ȝ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȘȠ
erally, our analysis of quicksort is indicative of how we go about analyzing a
broad class of recursive programs.
Program 1.2 is an implementation of quicksort in Java. It is a recursive
program that sorts the numbers in an array by partitioning it into two inde-
pendent (smaller) parts, then sorting those parts. Obviously, the recursion
should terminate when empty subarrays are encountered, but our implemen-
tation also stops with subarrays of size 1. Ļis detail might seem inconse-
quential at ŀrst blush, but, as we will see, the very nature of recursion ensures
that the program will be used for a large number of small ŀles, and substantial
performance gains can be achieved with simple improvements of this sort.
Ļe partitioning process puts the element that was in the last position
in the array (the partitioning element) into its correct position, with all smaller
elements before it and all larger elements after it. Ļe program accomplishes
this by maintaining two pointers: one scanning from the left, one from the
right. Ļe left pointer is incremented until an element larger than the parti-
private void quicksort(int[] a, int lo, int hi)
{
if (hi <= lo) return;
int i = lo-1, j = hi;
int t, v = a[hi];
while (true)
{
while (a[++i] < v) ;
while (v < a[--j]) if (j == lo) break;
if (i >= j) break;
t = a[i]; a[i] = a[j]; a[j] = t;
}
t = a[i]; a[i] = a[hi]; a[hi] = t;
quicksort(a, lo, i-1);
quicksort(a, i+1, hi);
}
Program 1.2 Quicksort

șȗ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ȝ
tioning element is found; the right pointer is decremented until an element
smaller than the partitioning element is found. Ļese two elements are ex-
changed, and the process continues until the pointers meet, which deŀnes
where the partitioning element is put. After partitioning, the program ex-
changes a[i] with a[hi] to put the partitioning element into position. Ļe
call quicksort(a, 0, N-1) will sort the array.
Ļere are several ways to implement the general recursive strategy just
outlined; the implementation described above is taken from Sedgewick and
Wayne [30] (see also [27]). For the purposes of analysis, we will be assuming
that the array a contains randomly ordered, distinct numbers, but note that
this code works properly for all inputs, including equal numbers. It is also
possible to study this program under perhaps more realistic models allowing
equal numbers (see [28]), long string keys (see [4]), and many other situations.
Once we have an implementation, the ŀrst step in the analysis is to
estimate the resource requirements of individual instructions for this program.
Ļis depends on characteristics of a particular computer, so we sketch the
details. For example, the “inner loop” instruction
while (a[++i] < v) ;
might translate, on a typical computer, to assembly language instructions such
as the following:
LOOP
INC
I,1
# increment i
CMP
V,A(I)
# compare v with A(i)
BL
LOOP
# branch if less
To start, we might say that one iteration of this loop might require four time
units (one for each memory reference). On modern computers, the precise
costs are more complicated to evaluate because of caching, pipelines, and
other eﬀects. Ļe other instruction in the inner loop (that decrements j)
is similar, but involves an extra test of whether j goes out of bounds. Since
this extra test can be removed via sentinels (see [26]), we will ignore the extra
complication it presents.
Ļe next step in the analysis is to assign variable names to the frequency
of execution of the instructions in the program. Normally there are only a few
true variables involved: the frequencies of execution of all the instructions can
be expressed in terms of these few. Also, it is desirable to relate the variables to

§Ș.Ȝ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
șȘ
the algorithm itself, not any particular program. For quicksort, three natural
quantities are involved:
A – the number of partitioning stages
B – the number of exchanges
C – the number of compares
On a typical computer, the total running time of quicksort might be expressed
with a formula, such as
4C + 11B + 35A.
(2)
Ļe exact values of these coeﬃcients depend on the machine language pro-
gram produced by the compiler as well as the properties of the machine being
used; the values given above are typical. Such expressions are quite useful in
comparing diﬀerent algorithms implemented on the same machine. Indeed,
the reason that quicksort is of practical interest even though mergesort is “op-
timal” is that the cost per compare (the coeﬃcient of C) is likely to be sig-
niŀcantly lower for quicksort than for mergesort, which leads to signiŀcantly
shorter running times in typical practical applications.
Ļeorem 1.3 (Quicksort analysis).
Quicksort uses, on the average,
(N −1)/2
partitioning stages,
2(N + 1) (HN+1 −3/2) ≈2NlnN −1.846N
compares, and
(N + 1) (HN+1 −3) /3 + 1 ≈.333NlnN −.865N
exchanges
to sort an array of N randomly ordered distinct elements.
Proof. Ļe exact answers here are expressed in terms of the harmonic numbers
HN =
∑
1≤k≤N
1/k,
the ŀrst of many well-known “special” number sequences that we will encoun-
ter in the analysis of algorithms.
As with mergesort, the analysis of quicksort involves deŀning and solv-
ing recurrence relations that mirror directly the recursive nature of the al-
gorithm. But, in this case, the recurrences must be based on probabilistic

șș
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ȝ
statements about the inputs. If CN is the average number of compares to sort
N elements, we have C0 = C1 = 0 and
CN = N + 1 + 1
N
∑
1≤j≤N
(Cj−1 + CN−j),
for N > 1.
(3)
To get the total average number of compares, we add the number of compares
for the ŀrst partitioning stage (N +1) to the number of compares used for the
subarrays after partitioning. When the partitioning element is the jth largest
(which occurs with probability 1/N for each 1 ≤j ≤N), the subarrays after
partitioning are of size j −1 and N −j.
Now the analysis has been reduced to a mathematical problem (3) that
does not depend on properties of the program or the algorithm. Ļis recur-
rence relation is somewhat more complicated than (1) because the right-hand
side depends directly on the history of all the previous values, not just a few.
Still, (3) is not diﬃcult to solve: ŀrst change j to N −j + 1 in the second
part of the sum to get
CN = N + 1 + 2
N
∑
1≤j≤N
Cj−1
for N > 0.
Ļen multiply by N and subtract the same formula for N −1 to eliminate the
sum:
NCN −(N −1)CN−1 = 2N + 2CN−1
for N > 1.
Now rearrange terms to get a simple recurrence
NCN = (N + 1)CN−1 + 2N
for N > 1.
Ļis can be solved by dividing both sides by N(N + 1):
CN
N + 1 = CN−1
N
+
2
N + 1
for N > 1.
Iterating, we are left with the sum
CN
N + 1 = C1
2 + 2
∑
3≤k≤N+1
1/k

§Ș.Ȝ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
șȚ
which completes the proof, since C1 = 0.
As implemented earlier, every element is used for partitioning exactly
once, so the number of stages is always N; the average number of exchanges
can be found from these results by ŀrst calculating the average number of
exchanges on the ŀrst partitioning stage.
Ļe stated approximations follow from the well-known approximation
to the harmonic number HN ≈lnN + .57721 · · · . We consider such ap-
proximations below and in detail in Chapter 4.
Exercise1.12 Give the recurrence for the total number of compares used by quicksort
on all N! permutations of N elements.
Exercise 1.13 Prove that the subarrays left after partitioning a random permutation
are themselves both random permutations. Ļen prove that this is not the case if, for
example, the right pointer is initialized at j:=r+1 for partitioning.
Exercise 1.14 Follow through the steps above to solve the recurrence
AN = 1 + 2
N
∑
1≤j≤N
Aj−1
for N > 0.
2,118,000
2N lnN
Gray dot: one experiment
Black dot: mean for 100 experiments 
2N lnN – 1.846N
166,000
12,000
1,000
Problem size (length of array to be sorted)
Cost (quicksort compares)
10,000
100,000
Figure 1.1 Quicksort compare counts: empirical and analytic

șț
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ȝ
Exercise 1.15 Show that the average number of exchanges used during the ŀrst par-
titioning stage (before the pointers cross) is (N −2)/6. (Ļus, by linearity of the
recurrences, BN = 1
6CN −1
2AN.)
Figure 1.1 shows how the analytic result of Ļeorem 1.3 compares to
empirical results computed by generating random inputs to the program and
counting the compares used. Ļe empirical results (100 trials for each value
of N shown) are depicted with a gray dot for each experiment and a black
dot at the mean for each N. Ļe analytic result is a smooth curve ŀtting the
formula given in Ļeorem 1.3. As expected, the ŀt is extremely good.
Ļeorem 1.3 and (2) imply, for example, that quicksort should take
about 11.667NlnN −.601N steps to sort a random permutation of N el-
ements for the particular machine described previously, and similar formulae
for other machines can be derived through an investigation of the properties of
the machine as in the discussion preceding (2) and Ļeorem 1.3. Such formu-
lae can be used to predict (with great accuracy) the running time of quicksort
on a particular machine. More important, they can be used to evaluate and
compare variations of the algorithm and provide a quantitative testimony to
their eﬀectiveness.
Secure in the knowledge that machine dependencies can be handled
with suitable attention to detail, we will generally concentrate on analyzing
generic algorithm-dependent quantities, such as “compares” and “exchanges,”
in this book. Not only does this keep our focus on major techniques of anal-
ysis, but it also can extend the applicability of the results. For example, a
slightly broader characterization of the sorting problem is to consider the
items to be sorted as records containing other information besides the sort
key, so that accessing a record might be much more expensive (depending on
the size of the record) than doing a compare (depending on the relative size
of records and keys). Ļen we know from Ļeorem 1.3 that quicksort com-
pares keys about 2NlnN times and moves records about .667NlnN times,
and we can compute more precise estimates of costs or compare with other
algorithms as appropriate.
Quicksort can be improved in several ways to make it the sorting method
of choice in many computing environments. We can even analyze compli-
cated improved versions and derive expressions for the average running time
that match closely observed empirical times [29]. Of course, the more intri-
cate and complicated the proposed improvement, the more intricate and com-

§Ș.Ȝ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
șȜ
plicated the analysis. Some improvements can be handled by extending the
argument given previously, but others require more powerful analytic tools.
Smallsubarrays. Ļe simplest variant of quicksort is based on the observation
that it is not very eﬃcient for very small ŀles (for example, a ŀle of size 2 can
be sorted with one compare and possibly one exchange), so that a simpler
method should be used for smaller subarrays. Ļe following exercises show
how the earlier analysis can be extended to study a hybrid algorithm where
“insertion sort” (see §7.6) is used for ŀles of size less than M. Ļen, this
analysis can be used to help choose the best value of the parameter M.
Exercise 1.16 How many subarrays of size 2 or less are encountered, on the average,
when sorting a random ŀle of size N with quicksort?
Exercise 1.17 If we change the ŀrst line in the quicksort implementation above to
if r-l<=M then insertionsort(l,r) else
(see §7.6), then the total number of compares to sort N elements is described by the
recurrence
CN =





N + 1 + 1
N
∑
1≤j≤N
(Cj−1 + CN−j)
for N > M;
1
4N(N −1)
for N ≤M.
Solve this exactly as in the proof of Ļeorem 1.3.
Exercise 1.18 Ignoring small terms (those signiŀcantly less than N) in the answer
to the previous exercise, ŀnd a function f(M) so that the number of compares is
approximately
2NlnN + f(M)N.
Plot the function f(M), and ŀnd the value of M that minimizes the function.
Exercise 1.19 As M gets larger, the number of compares increases again from the
minimum just derived. How large must M get before the number of compares ex-
ceeds the original number (at M = 0)?
Median-of-three quicksort.
A natural improvement to quicksort is to use
sampling: estimate a partitioning element more likely to be near the middle
of the ŀle by taking a small sample, then using the median of the sample. For
example, if we use just three elements for the sample, then the average number

șȝ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ȝ
of compares required by this “median-of-three” quicksort is described by the
recurrence
CN = N +1+
∑
1≤k≤N
(N −k)(k −1)
(N
3
)
(Ck−1 +CN−k)
for N > 3 (4)
where
(N
3
) is the binomial coeﬃcient that counts the number of ways to
choose 3 out of N items. Ļis is true because the probability that the kth
smallest element is the partitioning element is now (N −k)(k −1)/
(N
3
) (as
opposed to 1/N for regular quicksort). We would like to be able to solve re-
currences of this nature to be able to determine how large a sample to use and
when to switch to insertion sort. However, such recurrences require more
sophisticated techniques than the simple ones used so far. In Chapters 2
and 3, we will see methods for developing precise solutions to such recur-
rences, which allow us to determine the best values for parameters such as the
sample size and the cutoﬀfor small subarrays. Extensive studies along these
lines have led to the conclusion that median-of-three quicksort with a cutoﬀ
point in the range 10 to 20 achieves close to optimal performance for typical
implementations.
Radix-exchange sort.
Another variant of quicksort involves taking advan-
tage of the fact that the keys may be viewed as binary strings. Rather than
comparing against a key from the ŀle for partitioning, we partition the ŀle
so that all keys with a leading 0 bit precede all those with a leading 1 bit.
Ļen these subarrays can be independently subdivided in the same way using
the second bit, and so forth. Ļis variation is referred to as “radix-exchange
sort” or “radix quicksort.” How does this variation compare with the basic
algorithm? To answer this question, we ŀrst have to note that a diﬀerent
mathematical model is required, since keys composed of random bits are es-
sentially diﬀerent from random permutations. Ļe “random bitstring” model
is perhaps more realistic, as it rełects the actual representation, but the mod-
els can be proved to be roughly equivalent. We will discuss this issue in more
detail in Chapter 8. Using a similar argument to the one given above, we
can show that the average number of bit compares required by this method is
described by the recurrence
CN = N + 1
2N
∑
k
(
N
k
)
(Ck + CN−k)
for N > 1 with C0 = C1 = 0.

§Ș.ȝ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
șȞ
Ļis turns out to be a rather more diﬃcult recurrence to solve than the one
given earlier—we will see in Chapter 3 how generating functions can be used
to transform the recurrence into an explicit formula for CN, and in Chapters
4 and 8, we will see how to develop an approximate solution.
One limitation to the applicability of this kind of analysis is that all of
the preceding recurrence relations depend on the “randomness preservation”
property of the algorithm: if the original ŀle is randomly ordered, it can be
shown that the subarrays after partitioning are also randomly ordered. Ļe
implementor is not so restricted, and many widely used variants of the algo-
rithm do not have this property. Such variants appear to be extremely diﬃcult
to analyze. Fortunately (from the point of view of the analyst), empirical stud-
ies show that they also perform poorly. Ļus, though it has not been analyt-
ically quantiŀed, the requirement for randomness preservation seems to pro-
duce more elegant and eﬃcient quicksort implementations. More important,
the versions that preserve randomness do admit to performance improve-
ments that can be fully quantiŀed mathematically, as described earlier.
Mathematical analysis has played an important role in the development
of practical variants of quicksort, and we will see that there is no shortage
of other problems to consider where detailed mathematical analysis is an
important part of the algorithm design process.
1.6 Asymptotic Approximations. Ļe derivation of the average running
time of quicksort given earlier yields an exact result, but we also gave a more
concise approximate expression in terms of well-known functions that still can
be used to compute accurate numerical estimates. As we will see, it is often
the case that an exact result is not available, or at least an approximation is far
easier to derive and interpret. Ideally, our goal in the analysis of an algorithm
should be to derive exact results; from a pragmatic point of view, it is perhaps
more in line with our general goal of being able to make useful performance
predications to strive to derive concise but precise approximate answers.
To do so, we will need to use classical techniques for manipulating such
approximations. In Chapter 4, we will examine the Euler-Maclaurin sum-
mation formula, which provides a way to estimate sums with integrals. Ļus,
we can approximate the harmonic numbers by the calculation
HN =
∑
1≤k≤N
1
k ≈
∫N
1
1
xdx = lnN.

șȟ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.ȝ
But we can be much more precise about the meaning of ≈, and we can con-
clude (for example) that HN = lnN + γ + 1/(2N) + O(1/N2) where
γ = .57721 · · · is a constant known in analysis as Euler’s constant. Ļough the
constants implicit in the O-notation are not speciŀed, this formula provides
a way to estimate the value of HN with increasingly improving accuracy as N
increases. Moreover, if we want even better accuracy, we can derive a formula
for HN that is accurate to within O(N−3) or indeed to within O(N−k) for
any constant k. Such approximations, called asymptotic expansions, are at the
heart of the analysis of algorithms, and are the subject of Chapter 4.
Ļe use of asymptotic expansions may be viewed as a compromise be-
tween the ideal goal of providing an exact result and the practical requirement
of providing a concise approximation. It turns out that we are normally in the
situation of, on the one hand, having the ability to derive a more accurate
expression if desired, but, on the other hand, not having the desire, because
expansions with only a few terms (like the one for HN above) allow us to com-
pute answers to within several decimal places. We typically drop back to using
the ≈notation to summarize results without naming irrational constants, as,
for example, in Ļeorem 1.3.
Moreover, exact results and asymptotic approximations are both subject
to inaccuracies inherent in the probabilistic model (usually an idealization of
reality) and to stochastic łuctuations. Table 1.1 shows exact, approximate,
and empirical values for number of compares used by quicksort on random
ŀles of various sizes. Ļe exact and approximate values are computed from
the formulae given in Ļeorem 1.3; the “empirical” is a measured average,
taken over 100 ŀles consisting of random positive integers less than 106; this
tests not only the asymptotic approximation that we have discussed, but also
the “approximation” inherent in our use of the random permutation model,
ignoring equal keys. Ļe analysis of quicksort when equal keys are present is
treated in Sedgewick [28].
Exercise 1.20 How many keys in a ŀle of 104 random integers less than 106 are likely
to be equal to some other key in the ŀle? Run simulations, or do a mathematical
analysis (with the help of a system for mathematical calculations), or do both.
Exercise 1.21 Experiment with ŀles consisting of random positive integers less than
M for M = 10,000, 1000, 100 and other values. Compare the performance of quick-
sort on such ŀles with its performance on random permutations of the same size.
Characterize situations where the random permutation model is inaccurate.

§Ș.ȝ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
șȠ
Exercise 1.22 Discuss the idea of having a table similar to Table 1.1 for mergesort.
In the theory of algorithms, O-notation is used to suppress detail of
all sorts: the statement that mergesort requires O(NlogN) compares hides
everything but the most fundamental characteristics of the algorithm, imple-
mentation, and computer. In the analysis of algorithms, asymptotic expan-
sions provide us with a controlled way to suppress irrelevant details, while
preserving the most important information, especially the constant factors
involved. Ļe most powerful and general analytic tools produce asymptotic
expansions directly, thus often providing simple direct derivations of concise
but accurate expressions describing properties of algorithms. We are some-
times able to use asymptotic estimates to provide more accurate descriptions
of program performance than might otherwise be available.
ŀle size
exact solution
approximate
empirical
10,000
175,771
175,746
176,354
20,000
379,250
379,219
374,746
30,000
593,188
593,157
583,473
40,000
813,921
813,890
794,560
50,000
1,039,713
1,039,677
1,010,657
60,000
1,269,564
1,269,492
1,231,246
70,000
1,502,729
1,502,655
1,451,576
80,000
1,738,777
1,738,685
1,672,616
90,000
1,977,300
1,977,221
1,901,726
100,000
2,218,033
2,217,985
2,126,160
Table 1.1
Average number of compares used by quicksort

Țȗ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ȟ
1.7 Distributions. In general, probability theory tells us that other facts
about the distribution Nk of costs are also relevant to our understanding
of performance characteristics of an algorithm. Fortunately, for virtually all
of the examples that we study in the analysis of algorithms, it turns out that
knowing an asymptotic estimate for the average is enough to be able to make
reliable predictions. We review a few basic ideas here. Readers not familiar
with probability theory are referred to any standard text—for example, [9].
Ļe full distribution for the number of compares used by quicksort for
small N is shown in Figure 1.2. For each value of N, the points CNk/N! are
plotted: the proportion of the inputs for which quicksort uses k compares.
Each curve, being a full probability distribution, has area 1. Ļe curves move
to the right, since the average 2NlnN + O(N) increases with N. A slightly
diﬀerent view of the same data is shown in Figure 1.3, where the horizontal
axes for each curve are scaled to put the mean approximately at the center and
shifted slightly to separate the curves. Ļis illustrates that the distribution
converges to a “limiting distribution.”
For many of the problems that we study in this book, not only do lim-
iting distributions like this exist, but also we are able to precisely characterize
them. For many other problems, including quicksort, that is a signiŀcant
challenge. However, it is very clear that the distribution is concentrated near
0
100
200
300
400
.1
.05
0
Figure 1.2 Distributions for compares in quicksort, 15 ≤N ≤50

§Ș.Ȟ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȚȘ
the mean. Ļis is commonly the case, and it turns out that we can make pre-
cise statements to this eﬀect, and do not need to learn more details about the
distribution.
As discussed earlier, if N is the number of inputs of size N and Nk
is the number of inputs of size N that cause the algorithm to have cost k, the
average cost is given by
µ =
∑
k
kNk/N.
Ļe variance is deŀned to be
σ2 =
∑
k
(k −µ)2Nk/N =
∑
k
k2Nk/N −µ2.
Ļe standard deviation σ is the square root of the variance. Knowing the
average and standard deviation ordinarily allows us to predict performance
Figure 1.3
2NlnN −.846N
Distributions for compares in quicksort, 15 ≤N ≤50
(scaled and translated to center and separate curves)

Țș
C Ŕ ō Ŝ Š ő Ş
O Ś ő
§Ș.Ȟ
reliably. Ļe classical analytic tool that allows this is the Chebyshev inequal-
ity: the probability that an observation will be more than c multiples of the
standard deviation away from the mean is less than 1/c2. If the standard devi-
ation is signiŀcantly smaller than the mean, then, as N gets large, an observed
value is very likely to be quite close to the mean. Ļis is often the case in the
analysis of algorithms.
Exercise 1.23 What is the standard deviation of the number of compares for the
mergesort implementation given earlier in this chapter?
Ļe standard deviation of the number of compares used by quicksort is
√
(21 −2π2)/3 N ≈.6482776N
(see §3.9) so, for example, referring to Table 1.1 and taking c =
√
10 in
Chebyshev’s inequality, we conclude that there is more than a 90% chance
that the number of compares when N = 100,000 is within 205,004 (9.2%) of
2,218,033. Such accuracy is certainly adequate for predicting performance.
As N increases, the relative accuracy also increases: for example, the dis-
tribution becomes more localized near the peak in Figure 1.3 as N increases.
Indeed, Chebyshev’s inequality underestimates the accuracy in this situation,
as shown in Figure 1.4. Ļis ŀgure plots a histogram showing the number
of compares used by quicksort on 10,000 diﬀerent random ŀles of 1000 el-
ements. Ļe shaded area shows that more than 94% of the trials fell within
one standard deviation of the mean for this experiment.
11,000
12,000
13,000
14,000
15,000
16,000
Figure 1.4 Empirical histogram for quicksort compare counts
(10,000 trials with N=1000)

§Ș.ȟ
A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȚȚ
For the total running time, we can sum averages (multiplied by costs)
of individual quantities, but computing the variance is an intricate calculation
that we do not bother to do because the variance of the total is asymptotically
the same as the largest variance. Ļe fact that the standard deviation is small
relative to the average for large N explains the observed accuracy of Table 1.1
and Figure 1.1. Cases in the analysis of algorithms where this does not happen
are rare, and we normally consider an algorithm “fully analyzed” if we have
a precise asymptotic estimate for the average cost and knowledge that the
standard deviation is asymptotically smaller.
1.8 Randomized Algorithms. Ļe analysis of the average-case perfor-
mance of quicksort depends on the input being randomly ordered. Ļis as-
sumption is not likely to be strictly valid in many practical situations. In gen-
eral, this situation rełects one of the most serious challenges in the analysis
of algorithms: the need to properly formulate models of inputs that might
appear in practice.
Fortunately, there is often a way to circumvent this diﬃculty: “random-
ize” the inputs before using the algorithm. For sorting algorithms, this simply
amounts to randomly permuting the input ŀle before the sort. (See Chapter 7
for a speciŀc implementation of an algorithm for this purpose.) If this is done,
then probabilistic statements about performance such as those made earlier
are completely valid and will accurately predict performance in practice, no
matter what the input.
Often, it is possible to achieve the same result with less work, by making
a random choice (as opposed to a speciŀc arbitrary choice) whenever the algo-
rithm could take one of several actions. For quicksort, this principle amounts
to choosing the element to be used as the partitioning element at random,
rather than using the element at the end of the array each time. If this is
implemented with care (preserving randomness in the subarrays) then, again,
it validates the probabilistic analysis given earlier. (Also, the cutoﬀfor small
subarrays should be used, since it cuts down the number of random numbers
to generate by a factor of about M.) Many other examples of randomized
algorithms may be found in [23] and [25]. Such algorithms are of interest
in practice because they take advantage of randomness to gain eﬃciency and
to avoid worst-case performance with high probability. Moreover, we can
make precise probabilistic statements about performance, further motivating
the study of advanced techniques for deriving such results.

Țț
C Ŕ ō Ŝ Š ő Ş
O Ś ő
T
HE example of the analysis of quicksort that we have been considering
perhaps illustrates an idealized methodology: not all algorithms can be
as smoothly dealt with as this. A full analysis like this one requires a fair
amount of eﬀort that should be reserved only for our most important algo-
rithms. Fortunately, as we will see, there are many fundamental methods that
do share the basic ingredients that make analysis worthwhile, where we can
• Specify realistic input models.
• Derive mathematical models that describe costs.
• Develop concise, accurate solutions.
• Use the solutions to compare variants and compare with other algo-
rithms, and help adjust values of algorithm parameters.
In this book, we consider a wide variety of such methods, concentrating on
mathematical techniques validating the second and third of these points.
Most often, we skip the parts of the methodology outlined above that are
program-speciŀc (dependent on the implementation), to concentrate either
on algorithm design, where rough estimates of the running time may suﬃce,
or on the mathematical analysis, where the formulation and solution of the
mathematical problem involved are of most interest. Ļese are the areas in-
volving the most signiŀcant intellectual challenge, and deserve the attention
that they get.
As we have already mentioned, one important challenge in analysis of
algorithms in common use on computers today is to formulate models that re-
alistically represent the input and that lead to manageable analysis problems.
We do not dwell on this problem because there is a large class of combinatorial
algorithms for which the models are natural. In this book, we consider ex-
amples of such algorithms and the fundamental structures upon which they
operate in some detail. We study permutations, trees, strings, tries, words,
and mappings because they are all both widely studied combinatorial struc-
tures and widely used data structures and because “random” structures are
both straightforward and realistic.
In Chapters 2 through 5, we concentrate on techniques of mathemat-
ical analysis that are applicable to the study of algorithm performance. Ļis
material is important in many applications beyond the analysis of algorithms,
but our coverage is developed as preparation for applications later in the book.
Ļen, in Chapters 6 through 9 we apply these techniques to the analysis of
some fundamental combinatorial algorithms, including several of practical
interest. Many of these algorithms are of basic importance in a wide variety

A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȚȜ
of computer applications, and so are deserving of the eﬀort involved for de-
tailed analysis. In some cases, algorithms that seem to be quite simple can
lead to quite intricate mathematical analyses; in other cases, algorithms that
are apparently rather complicated can be dealt with in a straightforward man-
ner. In both situations, analyses can uncover signiŀcant diﬀerences between
algorithms that have direct bearing on the way they are used in practice.
It is important to note that we teach and present mathematical deriva-
tions in the classical style, even though modern computer algebra systems
such as Maple, Mathematica, or Sage are indispensable nowadays to check
and develop results. Ļe material that we present here may be viewed as
preparation for learning to make eﬀective use of such systems.
Much of our focus is on eﬀective methods for determining performance
characteristics of algorithm implementations.
Ļerefore, we present pro-
grams in a widely used programming language (Java). One advantage of this
approach is that the programs are complete and unambiguous descriptions of
the algorithms. Another is that readers may run empirical tests to validate
mathematical results. Generally our programs are stripped-down versions of
the full Java implementations in the Sedgewick and Wayne Algorithms text
[30]. To the extent possible, we use standard language mechanisms, so peo-
ple familiar with other programming environments may translate them. More
information about many of the programs we cover may be found in [30].
Ļe basic methods that we cover are, of course, applicable to a much
wider class of algorithms and structures than we are able to discuss in this
introductory treatment. We cover only a few of the large number of combi-
natorial algorithms that have been developed since the advent of computers in
mid-20th century. We do not touch on the scores of applications areas, from
image processing to bioinformatics, where algorithms have proved eﬀective
and have been investigated in depth. We mention only brieły approaches
such as amortized analysis and the probabilistic method, which have been
successfully applied to the analysis of a number of important algorithms. Still,
it is our hope that mastery of the introductory material in this book is good
preparation for appreciating such material in the research literature in the
analysis of algorithms. Beyond the books by Knuth, Sedgewick and Wayne,
and Cormen, Leiserson, Rivest, and Stein cited earlier, other sources of in-
formation about the analysis of algorithms and the theory of algorithms are
the books by Gonnet and Baeza-Yates [11], by Dasgupta, Papadimitriou, and
Vazirani [7], and by Kleinberg and Tardos [16].

Țȝ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
Equally important, we are led to analytic problems of a combinatorial
nature that allow us to develop general mechanisms that may help to ana-
lyze future, as yet undiscovered, algorithms. Ļe methods that we use are
drawn from the classical ŀelds of combinatorics and asymptotic analysis, and
we are able to apply classical methods from these ŀelds to treat a broad vari-
ety of problems in a uniform way. Ļis process is described in full detail in
our book Analytic Combinatorics [10]. Ultimately, we are not only able to
directly formulate combinatorial enumeration problems from simple formal
descriptions, but also we are able to directly derive asymptotic estimates of
their solution from these formulations.
In this book, we cover the important fundamental concepts while at the
same time developing a context for the more advanced treatment in [10] and
in other books that study advanced methods, such as Szpankowski’s study of
algorithms on words [32] or Drmota’ study of trees [8]. Graham, Knuth, and
Patashnik [12] is a good source of more material relating to the mathematics
that we use; standard references such as Comtet [5] (for combinatorics) and
Henrici [14] (for analysis) also have relevant material. Generally, we use el-
ementary combinatorics and real analysis in this book, while [10] is a more
advanced treatment from a combinatorial point of view, and relies on complex
analysis for asymptotics.
Properties of classical mathematical functions are an important part of
our story. Ļe classic Handbook of Mathematical Functions by Abramowitz and
Stegun [1] was an indispensable reference for mathematicians for decades and
was certainly a resource for the development of this book. A new reference
that is intended to replace it was recently published, with associated online
material [24]. Indeed, reference material of this sort is increasingly found on-
line, in resources such as Wikipedia and Mathworld [35]. Another important
resource is Sloane’s On-Line Encyclopedia of Integer Sequences [31].
Our starting point is to study characteristics of fundamental algorithms
that are in widespread use, but our primary purpose in this book is to pro-
vide a coherent treatment of the combinatorics and analytic methods that we
encounter. When appropriate, we consider in detail the mathematical prob-
lems that arise naturally and may not apply to any (currently known!) algo-
rithm. In taking such an approach we are led to problems of remarkable scope
and diversity. Furthermore, in examples throughout the book we see that the
problems we solve are directly relevant to many important applications.

A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȚȞ
References
1. M. AŎŞōřśţŕŠŦ ōŚŐ I. SŠőœšŚ. Handbook of Mathematical Functions,
Dover, New York, 1972.
2. A. AŔś, J. E. HśŜŏŞśŒŠ, ōŚŐ J. D. UŘŘřōŚ. Ļe Design and Analysis
of Algorithms, Addison-Wesley, Reading, MA, 1975.
3. B. CŔōŞ, K. GőŐŐőş, G. GśŚŚőŠ, B. LőśŚœ, M. MśŚōœōŚ, ōŚŐ
S. WōŠŠ. Maple V Library Reference Manual, Springer-Verlag, New York,
1991. Also Maple User Manual, Maplesoft, Waterloo, Ontario, 2012.
4. J. CŘŻřőŚŠ, J. A. FŕŘŘ, P. FŘōŖśŘőŠ, ōŚŐ B. VōŘŻő. “Ļe number of
symbol comparisons in quicksort and quickselect,” 36th International
Colloquium on Automata, Languages, and Programming, 2009, 750–763.
5. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
6. T. H. CśŞřőŚ, C. E. LőŕşőŞşśŚ, R. L. RŕŢőşŠ, ōŚŐ C. SŠőŕŚ. Intro-
duction to Algorithms, MIT Press, New York, 3rd edition, 2009.
7. S. DōşœšŜŠō, C. PōŜōŐŕřŕŠŞŕśš, ōŚŐ U. VōŦŕŞōŚŕ. Algorithms, Mc-
Graw-Hill, New York, 2008.
8. M. DŞřśŠō. Random Trees: An Interplay Between Combinatorics and
Probability, Springer Wein, New York, 2009.
9. W. FőŘŘőŞ. An Introduction to Probability Ļeory and Its Applications,
John Wiley, New York, 1957.
10. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
11. G. H. GśŚŚőŠ ōŚŐ R. BōőŦō-YōŠőş. Handbook of Algorithms and Data
Structures in Pascal and C, 2nd edition, Addison-Wesley, Reading, MA,
1991.
12. R. L. GŞōŔōř, D. E. KŚšŠŔ, ōŚŐ O. PōŠōşŔŚŕŗ. Concrete Mathemat-
ics, 1st edition, Addison-Wesley, Reading, MA, 1989. Second edition,
1994.
13. D. H. GŞőőŚő ōŚŐ D. E. KŚšŠŔ. Mathematics for the Analysis of Algo-
rithms, Birkhäuser, Boston, 3rd edition, 1991.
14. P. HőŚŞŕŏŕ. Applied and Computational Complex Analysis, 3 volumes,
John Wiley, New York, 1974 (volume 1), 1977 (volume 2), 1986 (vol-
ume 3).
15. C. A. R. HśōŞő. “Quicksort,” Computer Journal 5, 1962, 10–15.

Țȟ
C Ŕ ō Ŝ Š ő Ş
O Ś ő
16. J. KŘőŕŚŎőŞœ ōŚŐ
E. TōŞŐśş. Algorithm Design, Addison-Wesley,
Boston, 2005.
17. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 1: Fundamen-
tal Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1968. Ļird
edition, 1997.
18. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 2: Seminumerical
Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1969. Ļird
edition, 1997.
19. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.
20. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 4A: Combina-
torial Algorithms, Part 1, Addison-Wesley, Boston, 2011.
21. D. E. KŚšŠŔ. “Big omicron and big omega and big theta,” SIGACT
News, April-June 1976, 18–24.
22. D. E. KŚšŠŔ. “Mathematical analysis of algorithms,” Information Pro-
cessing 71, Proceedings of the IFIP Congress, Ljubljana, 1971, 19–27.
23. R. MśŠţōŚŕ ōŚŐ P. RōœŔōŢōŚ. Randomized Algorithms, Cambridge
University Press, 1995.
24. F. W. J. OŘŢőŞ, D. W. LśŦŕőŞ, R. F. BśŕşŢőŞŠ, ōŚŐ C. W. CŘōŞŗ,
őŐ., NIST Handbook of Mathematical Functions, Cambridge University
Press, 2010. Also accessible as Digital Library of Mathematical Functions
http://dlmf.nist.gov.
25. M. O. RōŎŕŚ. “Probabilistic algorithms,” in Algorithms and Complexity,
J. F. Traub, ed., Academic Press, New York, 1976, 21–39.
26. R. SőŐœőţŕŏŗ. Algorithms (3rd edition) in Java: Parts 1-4: Fundamen-
tals, Data Structures, Sorting, and Searching, Addison-Wesley, Boston,
2003.
27. R. SőŐœőţŕŏŗ. Quicksort, Garland Publishing, New York, 1980.
28. R. SőŐœőţŕŏŗ. “Quicksort with equal keys,” SIAM Journal on Comput-
ing 6, 1977, 240–267.
29. R. SőŐœőţŕŏŗ. “Implementing quicksort programs,” Communications
of the ACM 21, 1978, 847–856.
30. R. SőŐœőţŕŏŗ ōŚŐ K. WōťŚő. Algorithms, 4th edition, Addison-Wesley,
Boston, 2011.

A Ś ō Ř ť ş ŕ ş
ś Œ
A Ř œ ś Ş ŕ Š Ŕ ř ş
ȚȠ
31. N. SŘśōŚő ōŚŐ S. PŘśšŒŒő. Ļe Encyclopedia of Integer Sequences, Aca-
demic Press, San Diego, 1995. Also accessible as On-Line Encyclopedia
of Integer Sequences, http://oeis.org.
32. W. SŦŜōŚŗśţşŗŕ. Average-Case Analysis of Algorithms on Sequences, John
Wiley and Sons, New York, 2001.
33. E. TšŒŠő. Ļe Visual Display of Quantitative Information, Graphics Press,
Chesire, CT, 1987.
34. J. S. VŕŠŠőŞ ōŚŐ P. FŘōŖśŘőŠ, “Analysis of algorithms and data struc-
tures,” in Handbook of Ļeoretical Computer Science A: Algorithms and Com-
plexity, J. van Leeuwen, ed., Elsevier, Amsterdam, 1990, 431–524.
35. E. W. WőŕşşŠőŕŚ, őŐ., MathWorld, mathworld.wolfram.com.

This page intentionally left blank 

C H A P T E R T W O
R E C U R R E N C E R E L A T I O N S
T
HE algorithms that we are interested in analyzing normally can be ex-
pressed as recursive or iterative procedures, which means that, typically,
we can express the cost of solving a particular problem in terms of the cost
of solving smaller problems. Ļe most elementary approach to this situation
mathematically is to use recurrence relations, as we saw in the quicksort and
mergesort analyses in the previous chapter. Ļis represents a way to realize
a direct mapping from a recursive representation of a program to a recursive
representation of a function describing its properties. Ļere are several other
ways to do so, though the same recursive decomposition is at the heart of the
matter. As we will see in Chapter 3, this is also the basis for the application
of generating function methods in the analysis of algorithms.
Ļe development of a recurrence relation describing the performance of
an algorithm is already a signiŀcant step forward in the analysis, since the
recurrence itself carries a great deal of information. Speciŀc properties of the
algorithm as related to the input model are encapsulated in a relatively simple
mathematical expression. Many algorithms may not be amenable to such a
simple description; fortunately, many of our most important algorithms can
be rather simply expressed in a recursive formulation, and their analysis leads
to recurrences, either describing the average case or bounding the worst-case
performance. Ļis point is illustrated in Chapter 1 and in many examples
in Chapters 6 through 9. In this chapter, we concentrate on fundamental
mathematical properties of various recurrences without regard to their origin
or derivation. We will encounter many of the types of recurrences seen in this
chapter in the context of the study of particular algorithms, and we do revisit
the recurrences discussed in Chapter 1, but our focus for the moment is on
the recurrences themselves.
First, we examine some basic properties of recurrences and the ways in
which they are classiŀed. Ļen, we examine exact solutions to “ŀrst-order”
recurrences, where a function of n is expressed in terms of the function eval-
uated at n −1. We also look at exact solutions to higher-order linear recur-
rences with constant coeﬃcients. Next, we look at a variety of other types
țȘ

țș
C Ŕ ō Ŝ Š ő Ş
T ţ ś
of recurrences and examine some methods for deriving approximate solutions
to some nonlinear recurrences and recurrences with nonconstant coeﬃcients.
Following that, we examine solutions to a class of recurrence of particular
importance in the analysis of algorithms: the “divide-and-conquer” class of
recurrence. Ļis includes the derivation of and exact solution to the merge-
sort recurrence, which involves a connection with the binary representation
of integers. We conclude the chapter by looking at general results that apply
to the analysis of a broad class of divide-and-conquer algorithms.
All the recurrences that we have considered so far admit to exact solu-
tions. Such recurrences arise frequently in the analysis of algorithms, espe-
cially when we use recurrences to do precise counting of discrete quantities.
But exact answers may involve irrelevant detail: for example, working with
an exact answer like (2n −(−1)n)/3 as opposed to the approximate answer
2n/3 is probably not worth the trouble. In this case, the (−1)n term serves
to make the answer an integer and is negligible by comparison to 2n; on the
other hand, we would not want to ignore the (−1)n term in an exact answer
like 2n(1 + (−1)n). It is necessary to avoid the temptations of being overly
careless in trading accuracy for simplicity and of being overzealous in trading
simplicity for accuracy. We are interested in obtaining approximate expres-
sions that are both simple and accurate (even when exact solutions may be
available). In addition, we frequently encounter recurrences for which exact
solutions simply are not available, but we can estimate the rate of growth of
the solution, and, in many cases, derive accurate asymptotic estimates.
Recurrence relations are also commonly called diﬀerence equations because
they may be expressed in terms of the discrete diﬀerence operator ∇fn ≡fn−
fn−1. Ļey are the discrete analog of ordinary diﬀerential equations. Tech-
niques for solving diﬀerential equations are relevant because similar tech-
niques often can be used to solve analogous recurrences. In some cases, as
we will see in the next chapter, there is an explicit correspondence that allows
one to derive the solution to a recurrence from the solution to a diﬀerential
equation.
Ļere is a large literature on the properties of recurrences because they
also arise directly in many areas of applied mathematics. For example, iter-
ative numerical algorithms such as Newton’s method directly lead to recur-
rences, as described in detail in, for example, Bender and Orszag [3].
Our purpose in this chapter is to survey the types of recurrences that
commonly arise in the analysis of algorithms and some elementary techniques

§ș.Ș
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
țȚ
for deriving solutions. We can deal with many of these recurrence relations in
a rigorous and systematic way using generating functions, as discussed in detail
in the next chapter. We will also consider tools for developing asymptotic
approximations in some detail in Chapter 4. In Chapters 6 through 9 we will
encounter many diﬀerent examples of recurrences that describe properties of
basic algorithms.
Once we begin to study advanced tools in detail, we will see that re-
currences may not necessarily be the most natural mathematical tool for the
analysis of algorithms. Ļey can introduce complications in the analysis that
can be avoided by working at a higher level, using symbolic methods to derive
relationships among generating functions, then using direct analysis on the
generating functions. Ļis theme is introduced in Chapter 5 and treated in
detail in [12]. In many cases, it turns out that the simplest and most direct
path to solution is to avoid recurrences. We point this out not to discourage
the study of recurrences, which can be quite fruitful for many applications, but
to assure the reader that advanced tools perhaps can provide simple solutions
to problems that seem to lead to overly complicated recurrences.
In short, recurrences arise directly in natural approaches to algorithm
analysis, and can provide easy solutions to many important problems. Because
of our later emphasis on generating function techniques, we give only a brief
introduction to techniques that have been developed in the literature for solv-
ing recurrences. More information about solving recurrences may be found
in standard references, including [3], [4], [6], [14], [15], [16], [21], and [22].
2.1 Basic Properties. In Chapter 1, we encountered the following three
recurrences when analyzing quicksort and mergesort:
CN =
(
1 + 1
N
)
CN−1 + 2
for N > 1 with C1 = 2.
(1)
CN = C⌊N/2⌋+ C⌈N/2⌉+ N
for N > 1 with C1 = 0.
(2)
CN = N + 1 + 1
N
∑
1≤j≤N
(Cj−1 + CN−j)
for N > 0 with C0 = 0. (3)
Each of the equations presents special problems. We solved (1) by multiplying
both sides by an appropriate factor; we developed an approximate solution

țț
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ș
to (2) by solving for the special case N = 2n, then proving a solution for
general N by induction; and we transformed (3) to (1) by subtracting it from
the same equation for N −1.
Such ad hoc techniques are perhaps representative of the “bag of tricks”
approach often required for the solution of recurrences, but the few tricks just
mentioned do not apply, for example, to many recurrences that commonly
arise, including perhaps the best-known linear recurrence
Fn = Fn−1 + Fn−2
for n > 1 with F0 = 0 and F1 = 1,
which deŀnes the Fibonacci sequence {0, 1, 1, 2, 3, 5, 8, 13, 21, 34, . . .}. Fibo-
nacci numbers are well studied and actually arise explicitly in the design and
analysis of a number of important algorithms. We consider a number of tech-
niques for solving these and other recurrences in this chapter, and we consider
other applicable systematic approaches in the next and later chapters.
Recurrences are classiŀed by the way in which terms are combined, the
nature of the coeﬃcients involved, and the number and nature of previous
recurrence type
typical example
ŀrst-order
linear
an = nan−1 −1
nonlinear
an = 1/(1 + an−1)
second-order
linear
an = an−1 + 2an−2
nonlinear
an = an−1an−2 + √an−2
variable coeﬃcients
an = nan−1 + (n −1)an−2 + 1
tth order
an = f(an−1, an−2, . . . , an−t)
full-history
an = n + an−1 + an−2 . . . + a1
divide-and-conquer
an = a⌊n/2⌋+ a⌈n/2⌉+ n
Table 2.1
Classiŀcation of recurrences

§ș.Ș
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
țȜ
terms used. Table 2.1 lists some of the recurrences that we will be considering,
along with representative examples.
Calculating values. Normally, a recurrence provides an eﬃcient way to cal-
culate the quantity in question. In particular, the very ŀrst step in attacking
any recurrence is to use it to compute small values in order to get a feeling for
how they are growing. Ļis can be done by hand for small values, or it is easy
to implement a program to compute larger values.
For example, Program 2.1 will compute the exact values for the average
number of comparisons for quicksort for all N less than or equal to maxN,
corresponding to the recurrence (3) (see Table 1.1). Ļis program uses an
array of size maxN to save previously computed values. Ļe temptation to
use a purely recursive program based directly on the recurrence should be
avoided: computing CN by computing all the values CN−1, CN−2, . . . , C1
recursively would be extremely ineﬃcient because many, many values would
be unnecessarily recomputed.
We could avoid delving too deeply into the mathematics of the situa-
tion if something like Program 2.1 would suﬃce. We assume that succinct
mathematical solutions are more desirable—indeed, one might view the anal-
ysis itself as a process that can make Program 2.1 more eﬃcient! At any
rate, such “solutions” can be used, for example, to validate analyses. At the
other extreme on this continuum would be a brute-force (usually impractical)
method for computing the average running time of a program by running it
for all possible inputs.
C[0] = 0.0;
for (int N = 1; N <= maxN; N++)
{
C[N] = N+1.0;
for (int k = 0; k < N; k ++)
C[N] += (C[k] + C[N-1-k])/N;
}
Program 2.1 Calculating values (quicksort recurrence)

țȝ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ș
Exercise 2.1 Write recursive and nonrecursive programs to compute values for the
Fibonacci recurrence and try to use each to compute F20. Explain the behavior of
each program in this case.
Exercise 2.2 How many arithmetic operations are used by Program 2.1, as a function
of Nmax?
Exercise 2.3 Write a recursive program to compute values using the recurrence (1)
directly. How does the number of arithmetic operations used by this program com-
pare with Program 2.1 (see the previous exercise)?
Exercise 2.4 Estimate how many operations would be required by both recursive and
nonrecursive programs to compute values using the recurrences (2) and (3).
Exercise 2.5 Write a program to compare quicksort, its median-of-three variant, and
radix-exchange sort, calculating values from the recurrences given in Chapter 1. For
quicksort, check values against the known solution; for the others, make conjectures
about properties of the solution.
Scaling and shifting. An essential property of recurrences is that they depend
on their initial values: changing the initial condition in the linear recurrence
an = f(an−1)
for n > 0 with a0 = 1
from a0 = 1 to a0 = 2 will change the value of an for all n (if f(0) = 0, the
value will be doubled). Ļe “initial” value can be anywhere: if we have
bn = f(bn−1)
for n > t with bt = 1
then we must have bn = an−t. Changing the initial values is referred to as
scaling the recurrence; moving the initial values is referred to as shifting it. Our
initial values are most often directly implied from a problem, but we often use
scaling or shifting to simplify the path to the solution. Rather than state the
most general form of the solution to a recurrence, we solve a natural form and
presume that the solution can be scaled or shifted as appropriate.
Linearity. Linear recurrences with more than one initial value can be “scaled”
by changing initial values independently and combining solutions. If f(x, y)
is a linear function with f(0, 0) = 0, then the solution to
an = f(an−1, an−2)
for n > 1

§ș.Ș
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
țȞ
(a function of the initial values a0 and a1) is a0 times the solution to
un = f(un−1, un−2)
for n > 1 with u0 = 1 and u1 = 0
plus a1 times the solution to
vn = f(vn−1, vn−2)
for n > 1 with v0 = 0 and v1 = 1.
Ļe condition f(0, 0) = 0 makes the recurrence homogeneous: if there is a
constant term in f, then that, as well as the initial values, has to be taken
into account. Ļis generalizes in a straightforward way to develop a general
solution for any homogeneous linear tth-order recurrence (for any set of ini-
tial values) as a linear combination of t particular solutions. We used this
procedure in Chapter 1 to solve the recurrence describing the number of ex-
changes taken by quicksort, in terms of the recurrences describing the number
of comparisons and the number of stages.
Exercise 2.6 Solve the recurrence
an = an−1 + an−2
for n > 1 with a0 = p and a1 = q,
expressing your answer in terms of the Fibonacci numbers.
Exercise 2.7 Solve the inhomogeneous recurrence
an = an−1 + an−2 + r
for n > 1 with a0 = p and a1 = q,
expressing your answer in terms of the Fibonacci numbers.
Exercise 2.8 For f linear, express the solution to the recurrence
an = f(an−1, an−2)
for n > 1
in terms of a0, a1, f(0, 0) and the solutions to an = f(an−1, an−2) −f(0, 0) for
a1 = 1, a0 = 0 and a0 = 1, a1 = 0.

țȟ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ș
2.2 First-Order Recurrences. Perhaps the simplest type of recurrence
reduces immediately to a product. Ļe recurrence
an = xnan−1
for n > 0 with a0 = 1
is equivalent to
an =
∏
1≤k≤n
xk.
Ļus, if xn = n then an = n!, and if xn = 2 then an = 2n, and so on.
Ļis transformation is a simple example of iteration: apply the recur-
rence to itself until only constants and initial values are left, then simplify.
Iteration also applies directly to the next simplest type of recurrence, much
geometric series
∑
0≤k<n
xk = 1 −xn
1 −x
arithmetic series
∑
0≤k<n
k = n(n −1)
2
=
(n
2
)
binomial coeﬃcients
∑
0≤k≤n
( k
m
)
=
(n + 1
m + 1
)
binomial theorem
∑
0≤k≤n
(n
k
)
xkyn−k = (x + y)n
harmonic numbers
∑
1≤k≤n
1
k = Hn
sum of harmonic numbers
∑
1≤k<n
Hk = nHn −n
Vandermonde convolution
∑
0≤k≤n
(n
k
)( m
t −k
)
=
(n + m
t
)
Table 2.2
Elementary discrete sums

§ș.ș
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
țȠ
more commonly encountered, which reduces immediately to a sum:
an = an−1 + yn
for n > 0 with a0 = 0
is equivalent to
an =
∑
1≤k≤n
yk.
Ļus, if yn = 1 then an = n, and if yn = n −1 then an = n(n −1)/2, and
so on.
Table 2.2 gives a number of commonly encountered discrete sums. A
much more comprehensive list may be found in a standard reference such as
Graham, Knuth, and Patashnik [14] or Riordan [23].
Exercise 2.9 Solve the recurrence
an =
n
n + 2an−1
for n > 0 with a0 = 1.
Exercise 2.10 Solve the recurrence
an = an−1 + (−1)nn
for n > 0 with a0 = 1.
If we have a recurrence that is not quite so simple, we can often simplify
by multiplying both sides of the recurrence by an appropriate factor. We have
seen examples of this already in Chapter 1. For example, we solved (1) by
dividing both sides by N + 1, giving a simple recurrence in CN/(N + 1) that
transformed directly into a sum when iterated.
Exercise 2.11 Solve the recurrence
nan = (n −2)an−1 + 2
for n > 1 with a1 = 1.
(Hint : Multiply both sides by n −1.)
Exercise 2.12 Solve the recurrence
an = 2an−1 + 1
for n > 1 with a1 = 1.
(Hint : Divide both sides by 2n.)

Ȝȗ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ș
Solving recurrence relations (diﬀerence equations) in this way is analo-
gous to solving diﬀerential equations by multiplying by an integrating factor
and then integrating. Ļe factor used for recurrence relations is sometimes
called a summation factor. Proper choice of a summation factor makes it pos-
sible to solve many of the recurrences that arise in practice. For example, an
exact solution to the recurrence describing the average number of compar-
isons used in median-of-three quicksort was developed by Knuth using such
techniques [18] (see also [24]).
Ļeorem 2.1 (First-order linear recurrences).
Ļe recurrence
an = xnan−1 + yn
for n > 0 with a0 = 0
has the explicit solution
an = yn +
∑
1≤j<n
yjxj+1xj+2 . . . xn.
Proof. Dividing both sides by xnxn−1 . . . x1 and iterating, we have
an = xnxn−1 . . . x1
∑
1≤j≤n
yj
xjxj−1 . . . x1
= yn +
∑
1≤j<n
yjxj+1xj+2 . . . xn.
Ļe same result can be derived by multiplying both sides by xn+1xn+2 . . . (pro-
vided it converges) and iterating.
For example, the proof of Ļeorem 2.1 says that we should solve the
recurrence
CN =
(
1 + 1
N
)
CN−1 + 2
for N > 1 with C1 = 2
by dividing both sides by
N + 1
N
N
N −1
N −1
N −2 . . . 3
2
2
1 = N + 1,

§ș.ș
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȜȘ
which is precisely what we did in §1.5. Alternatively, the solution
2(N + 1)(HN+1 −1)
follows directly from the explicit form of the solution given in the theorem
statement.
Ļeorem 2.1 is a complete characterization of the transformation from
ŀrst-order linear recurrences, with constant or nonconstant coeﬃcients, to
sums. Ļe problem of solving the recurrence is reduced to the problem of
evaluating the sum.
Exercise 2.13 Solve the recurrence
an =
n
n + 1an−1 + 1
for n > 0 with a0 = 1.
Exercise 2.14 Write down the solution to
an = xnan−1 + yn
for n > t
in terms of the x’s, the y’s, and the initial value at.
Exercise 2.15 Solve the recurrence
nan = (n + 1)an−1 + 2n
for n > 0 with a0 = 0.
Exercise 2.16 Solve the recurrence
nan = (n −4)an−1 + 12nHn
for n > 4 with an = 0 for n ≤4.
Exercise 2.17 [Yao] (“Fringe analysis of 2–3 trees [25]”) Solve the recurrence
AN = AN−1 −2AN−1
N
+ 2
(
1 −2AN−1
N
)
for N > 0 with A0 = 0.
Ļis recurrence describes the following random process: A set of N elements is col-
lected into “2-nodes” and “3-nodes.” At each step each 2-node is likely to turn into a
3-node with probability 2/N and each 3-node is likely to turn into two 2-nodes with
probability 3/N. What is the average number of 2-nodes after N steps?

Ȝș
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ț
2.3 Nonlinear First-Order Recurrences. When a recurrence consists of
a nonlinear function relating an and an−1, a broad variety of situations arise,
and we cannot expect to have a closed-form solution like Ļeorem 2.1. In this
section we consider a number of interesting cases that do admit to solutions.
Simple convergence. One convincing reason to calculate initial values is that
many recurrences with a complicated appearance simply converge to a con-
stant. For example, consider the equation
an = 1/(1 + an−1)
for n > 0 with a0 = 1.
Ļis is a so-called continued fraction equation, which is discussed in §2.5.
By calculating initial values, we can guess that the recurrence converges to a
constant:
n
an
|an −(
√
5 −1)/2|
1
0.500000000000
0.118033988750
2
0.666666666667
0.048632677917
3
0.600000000000
0.018033988750
4
0.625000000000
0.006966011250
5
0.615384615385
0.002649373365
6
0.619047619048
0.001013630298
7
0.617647058824
0.000386929926
8
0.618181818182
0.000147829432
9
0.617977528090
0.000056460660
Each iteration increases the number of signiŀcant digits available by a con-
stant number of digits (about half a digit). Ļis is known as simple convergence.
If we assume that the recurrence does converge to a constant, we know that
the constant must satisfy α = 1/(1 + α), or 1 −α −α2 = 0, which leads to
the solution α = (
√
5 −1)/2) ≈.6180334.
Exercise 2.18 Deŀne bn = an −α with an and α deŀned as above. Find an approx-
imate formula for bn when n is large and a0 is between 0 and 1.
Exercise 2.19 Show that an = cos(an−1) converges when a0 is between 0 and 1,
and compute limn→∞an to ŀve decimal places.
Quadratic convergence and Newton’s method.
Ļis well-known iterative
method for computing roots of functions can be viewed as a process of cal-
culating an approximate solution to a ŀrst-order recurrence (see, for example,

§ș.Ț
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȜȚ
[3]). For example, Newton’s method to compute the square root of a positive
number β is to iterate the formula
an = 1
2
(
an−1 +
β
an−1
)
for n > 0 with a0 = 1.
Changing variables in this recurrence, we can see why the method is so eﬀec-
tive. Letting bn = an −α, we ŀnd by simple algebra that
bn = 1
2
b2
n−1 + β −α2
bn−1 + α
,
so that if α = √β we have, roughly, bn ≈b2
n−1. For example, to compute
the square root of 2, this iteration gives the following sequence:
n
an
an −
√
2
1
1.500000000000
0.085786437627
2
1.416666666667
0.002453104294
3
1.414215686275
0.000002123901
4
1.414213562375
0.000000000002
5
1.414213562373
0.000000000000
Each iteration approximately doubles the number of signiŀcant digits avail-
able. Ļis is a case of so-called quadratic convergence.
Exercise 2.20 Discuss what happens when Newton’s method is used to attempt com-
puting √−1:
an = 1
2
(
an−1 −
1
an−1
)
for n > 0 with a0 ̸= 0.
Slow convergence. Consider the recurrence
an = an−1(1 −an−1)
for n > 0 with a0 = 1
2.
In §6.10, we will see that similar recurrences play a role in the analysis of the
height of “random binary trees.” Since the terms in the recurrence decrease

Ȝț
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ț
and are positive, it is not hard to see that limn→∞an = 0. To ŀnd the speed
of convergence, it is natural to consider 1/an. Substituting, we have
1
an
=
1
an−1
(
1
1 −an−1
)
=
1
an−1
(1 + an−1 + a2
n−1 + . . .)
>
1
an−1
+ 1.
Ļis telescopes to give 1/an > n, or an < 1/n. We have thus found that
an = O(1/n).
Exercise 2.21 Prove that an = Θ(1/n). Compute initial terms and try to guess a
constant c such that an is approximated by c/n. Ļen ŀnd a rigorous proof that nan
tends to a constant.
Exercise 2.22 [De Bruijn] Show that the solution to the recurrence
an = sin(an−1)
for n > 0 with a0 = 1
satisŀes limn→∞an = 0 and an = O(1/√n ). (Hint : Consider the change of
variable bn = 1/an.)
Ļe three cases just considered are particular cases of the form
an = f(an−1)
for some continuous function f. If the an converge to a limit α, then nec-
essarily α must be a ŀxed point of the function, with α = f(α). Ļe three
cases above are representative of the general situation: if 0 < |f′(α)| < 1,
the convergence is simple; if f′(α) = 0, the convergence is quadratic; and if
|f′(α)| = 1, the convergence is “slow.”
Exercise 2.23 What happens when f ′(α) > 1?
Exercise 2.24 State suﬃcient criteria corresponding to the three cases above for local
convergence (when a0 is suﬃciently close to α) and quantify the speed of convergence
in terms of f ′(α) and f ′′(α).

§ș.ț
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȜȜ
2.4 Higher-OrderRecurrences. Next, we consider recurrences where the
right-hand side of the equation for an is a linear combination of an−2, an−3,
and so on, as well as an−1, and where the coeﬃcients involved are constants.
For a simple example, consider the recurrence
an = 3an−1 −2an−2
for n > 1 with a0 = 0 and a1 = 1.
Ļis can be solved by ŀrst observing that an −an−1 = 2(an−1 −an−2), an
elementary recurrence in the quantity an −an−1. Iterating this product gives
the result an −an−1 = 2n−1; iterating the sum for this elementary recurrence
gives the solution an = 2n −1. We could also solve this recurrence by ob-
serving that an −2an−1 = an−1 −2an−2. Ļese manipulations correspond
precisely to factoring the quadratic equation 1−3x+2x2 = (1−2x)(1−x).
Similarly, we can ŀnd that the solution to
an = 5an−1 −6an−2
for n > 1 with a0 = 0 and a1 = 1
is an = 3n −2n by solving elementary recurrences on an −3an−1 or an −
2an−1.
Exercise 2.25 Give a recurrence that has the solution an = 4n −3n + 2n.
Ļese examples illustrate the general form of the solution, and recur-
rences of this type can be solved explicitly.
Ļeorem 2.2 (Linear recurrences with constant coeﬃcients).
All solutions
to the recurrence
an = x1an−1 + x2an−2 + . . . + xtan−t
for n ≥t
can be expressed as a linear combination (with coeﬃcients depending on the
initial conditions a0, a1, . . . , at−1) of terms of the form njβn, where β is a
root of the “characteristic polynomial”
q(z) ≡zt −x1zt−1 −x2zt−2 −. . . −xt
and j is such that 0 ≤j < ν if β has multiplicity ν.
Proof. It is natural to look for solutions of the form an = βn. Substituting,
any such solution must satisfy
βn = x1βn−1 + x2βn−2 + . . . + xtβn−t
for n ≥t

Ȝȝ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ț
or, equivalently,
βn−tq(β) = 0.
Ļat is, βn is a solution to the recurrence for any root β of the characteristic
polynomial.
Next, suppose that β is a double root of q(z). We want to prove that
nβn is a solution to the recurrence as well as βn. Again, by substitution, we
must have
nβn = x1(n−1)βn−1 +x2(n−2)βn−2 +. . .+xt(n−t)βn−t
for n ≥t
or, equivalently,
βn−t((n −t)q(β) + βq′(β)) = 0.
Ļis is true, as desired, because q(β) = q′(β) = 0 when β is a double root.
Higher multiplicities are treated in a similar manner.
Ļis process provides as many solutions to the recurrence as there are
roots of the characteristic polynomial, counting multiplicities. Ļis is the
same as the order t of the recurrence. Moreover, these solutions are linearly
independent (they have diﬀerent orders of growth at ∞). Since the solutions
of a recurrence of order t form a vector space of dimension t, each solution of
our recurrence must be expressible as a linear combination of the particular
solutions of the form njβn.
Finding the coeﬃcients. An exact solution to any linear recurrence can be
developed from Ļeorem 2.2 by using the initial values a0, a1, . . . , at−1 to
create a system of simultaneous equations that can be solved to yield the con-
stants in the linear combination of the terms that comprise the solution. For
example, consider the recurrence
an = 5an−1 −6an−2
for n ≥2 with a0 = 0 and a1 = 1.
Ļe characteristic equation is z2 −5z + 6 = (z −3)(z −2) so
an = c03n + c12n.
Matching this formula against the values at n = 0 and n = 1, we have
a0 = 0 = c0 + c1
a1 = 1 = 3c0 + 2c1.
Ļe solution to these simultaneous equations is c0 = 1 and c1 = −1, so
an = 3n −2n.

§ș.ț
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȜȞ
Degenerate cases.
We have given a method for ŀnding an exact solution
for any linear recurrence. Ļe process makes explicit the way in which the
full solution is determined by the initial conditions. When the coeﬃcients
turn out to be zero and/or some roots have the same modulus, the result can
be somewhat counterintuitive, though easily understood in this context. For
example, consider the recurrence
an = 2an−1 −an−2
for n ≥2 with a0 = 1 and a1 = 2.
Since the characteristic equation is z2 −2z +1 = (z −1)2 (with a single root,
1, of multiplicity 2), the solution is
an = c01n + c1n1n.
Applying the initial conditions
a0 = 1 = c0
a1 = 2 = c0 + c1
gives c0 = c1 = 1, so an = n + 1. But if the initial conditions were a0 =
a1 = 1, the solution would be an = 1, meaning constant instead of linear
growth. For a more dramatic example, consider the recurrence
an = 2an−1 + an−2 −2an−3
for n > 3.
Here the solution is
an = c01n + c1(−1)n + c22n,
and various choices of the initial conditions can make the solution constant,
exponential in growth, or łuctuating in sign! Ļis example points out that
paying attention to details (initial conditions) is quite important when dealing
with recurrences.
Fibonacci numbers. We have already mentioned the familiar Fibonacci se-
quence {0, 1, 1, 2, 3, 5, 8, 13, 21, 34, . . .} that is deŀned by the prototypical
second-order recurrence
Fn = Fn−1 + Fn−2
for n > 1 with F0 = 0 and F1 = 1.

Ȝȟ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ț
Since the roots of u2 −u −1 are ϕ = (1 +
√
5)/2 = 1.61803 · · · and bϕ =
(1 −
√
5)/2 = −.61803 · · ·, Ļeorem 2.2 says that the solution is
FN = c0ϕN + c1 bϕN
for some constants c0 and c1. Applying the initial conditions
F0 = 0 = c0 + c1
F1 = 1 = c0ϕ + c1 bϕ
yields the solution
FN =
1
√
5(ϕN −bϕN).
Since ϕ is larger than 1 and bϕ is smaller than 1 in absolute value, the con-
tribution of the bϕN term in the above expression for FN is negligible, and it
turns out that FN is always the nearest integer to ϕN/
√
5. As N gets large,
the ratio FN+1/FN approaches ϕ, which is well known in mathematics, art,
architecture, and nature as the golden ratio.
While Ļeorem 2.2 provides a way to develop complete exact solu-
tions to ŀxed-degree high-order linear recurrences, we will revisit this topic in
Chapters 3 and 4 because the advanced tools there provide convenient ways
to get useful results in practice. Ļeorem 3.3 gives an easy way to compute
coeﬃcients, and in particular identify those terms that vanish. Moreover,
the phenomenon just observed for Fibonacci numbers generalizes: since the
terms njβn are all exponential, the ones (among those with nonzero coeﬃ-
cient) with largest β will dominate all the others for large n, and among those,
the one with largest j will dominate. Generating functions (Ļeorem 3.3)
and asymptotic analysis (Ļeorem 4.1) provide us with convenient ways to
identify the leading term explicitly and evaluate its coeﬃcient for any linear
recurrence. Ļis can provide a shortcut to developing a good approximation to
the solution in some cases, especially when t is large. For small t, the method
described here for getting the exact solution is quite eﬀective.
Exercise 2.26 Explain how to solve an inhomogeneous recurrence of the form
an = x1an−1 + x2an−2 + . . . + xtan−t + r
for n ≥t.
Exercise 2.27 Give initial conditions a0, a1 for which the solution to
an = 5an−1 −6an−2
for n > 1
is an = 2n. Are there initial conditions for which the solution is an = 2n −1?

§ș.ț
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȜȠ
Exercise 2.28 Give initial conditions a0, a1, and a2 for which the growth rate of the
solution to
an = 2an−1 −an−2 + 2an−3
for n > 2
is (i) constant, (ii) exponential, and (iii) łuctuating in sign.
Exercise 2.29 Solve the recurrence
an = 2an−1 + 4an−2
for n > 1 with a1 = 2 and a0 = 1.
Exercise 2.30 Solve the recurrence
an = 2an−1 −an−2
for n > 1 with a0 = 0 and a1 = 1.
Solve the same recurrence, but change the initial conditions to a0 = a1 = 1.
Exercise 2.31 Solve the recurrence
an = an−1 −an−2
for n > 1 with a0 = 0 and a1 = 1.
Exercise 2.32 Solve the recurrence
2an = 3an−1 −3an−2 + an−3
for n > 2 with a0 = 0, a1 = 1 and a2 = 2.
Exercise 2.33 Find a recurrence describing a sequence for which the order of growth
decreases exponentially for odd-numbered terms, but increases exponentially for even-
numbered terms.
Exercise 2.34 Give an approximate solution for the “third-order” Fibonacci recur-
rence
F (3)
N
= F (3)
N−1+F (3)
N−2+F (3)
N−3
for N > 2 with F (3)
0
= F (3)
1
= 0 and F (3)
2
= 1.
Compare your approximate result for F (3)
20
with the exact value.
Nonconstant coeﬃcients.
If the coeﬃcients are not constants, then more
advanced techniques are needed because Ļeorem 2.2 does not apply. Typ-
ically, generating functions (see Chapter 3) or approximation methods (dis-
cussed later in this chapter) are called for, but some higher-order problems
can be solved with summation factors. For example, the recurrence
an = nan−1 + n(n −1)an−2
for n > 1 with a1 = 1 and a0 = 0
can be solved by simply dividing both sides by n!, leaving the Fibonacci recur-
rence in an/n!, which shows that an = n!Fn.

ȝȗ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ț
Exercise 2.35 Solve the recurrence
n(n −1)an = (n −1)an−1 + an−2
for n > 1 with a1 = 1 and a0 = 1.
Symbolic solution. Ļough no closed form like Ļeorem 2.2 is available for
higher-order recurrences, the result of iterating the general form
an = sn−1an−1 + tn−2an−2
for n > 1 with a1 = 1 and a0 = 0
has been studied in some detail. For suﬃciently large n, we have
a2 = s1,
a3 = s2s1 + t1,
a4 = s3s2s1 + s3t1 + t2s1,
a5 = s4s3s2s1 + s4s3t1 + s4t2s1 + t3s2s1 + t3t1,
a6 = s5s4s3s2s1 + s5s4s3t1 + s5s4t2s1 + s5t3s2s1 + s5t3t1
+ t4s3s2s1 + t4s3t1 + t4t2s1,
and so forth. Ļe number of monomials in the expansion of an is exactly Fn,
and the expansions have many other properties: they are related to the so-
called continuant polynomials, which are themselves closely related to con-
tinued fractions (discussed later in this chapter). Details may be found in
Graham, Knuth, and Patashnik [14].
Exercise 2.36 Give a simple algorithm to determine whether a given monomial
si1si2 . . . siptj1tj2 . . . tjq appears in the expansion of an. How many such mono-
mials are there?
We argued earlier that for the case of constant coeﬃcients, we are most
interested in a derivation of the asymptotic behavior of the leading term because
exact solutions, though available, are tedious to use. For the case of noncon-
stant coeﬃcients, exact solutions are generally not available, so we must be
content with approximate solutions for many applications. We now turn to
techniques for developing such approximations.

§ș.Ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȝȘ
2.5 Methods for Solving Recurrences. Nonlinear recurrences or re-
currences with variable coeﬃcients can normally be solved or approximated
through one of a variety of approaches. We consider a number of such ap-
proaches and examples in this section.
We have been dealing primarily with recurrences that admit to exact
solutions. While such problems do arise very frequently in the analysis of
algorithms, one certainly can expect to encounter recurrences for which no
method for ŀnding an exact solution is known. It is premature to begin treat-
ing advanced techniques for working with such recurrences, but we give some
guidelines on how to develop accurate approximate solutions and consider
several examples.
We consider four general methods: change of variable, which involves
simplifying a recurrence by recasting it in terms of another variable; repertoire,
which involves working backward from a given recurrence to ŀnd a solution
space; bootstrapping, which involves developing an approximate solution, then
using the recurrence itself to ŀnd a more accurate solution, continuing until
a suﬃciently accurate answer is obtained or no further improvement seems
likely; and perturbation, which involves studying the eﬀects of transforming
a recurrence into a similar, simpler, one with a known solution. Ļe ŀrst two
of these methods often lead to exact solutions of recurrences; the last two are
more typically used to develop approximate solutions.
Change of Variables. Ļeorem 2.1 actually describes a change of variable:
if we change variables to bn = an/(xnxn−1 . . . x1), then bn satisŀes a simple
recurrence that reduces to a sum when iterated. We also used a change of
variable in the previous section, and in other places earlier in the chapter.
More complicated changes of variable can be used to derive exact solutions to
formidable-looking recurrences. For instance, consider the nonlinear second-
order recurrence
an = √an−1an−2
for n > 1 with a0 = 1 and a1 = 2.
If we take the logarithm of both sides of this equation and make the change
of variable bn = lgan, then we ŀnd that bn satisŀes
bn = 1
2(bn−1 + bn−2)
for n > 1 with b0 = 0 and b1 = 1,
a linear recurrence with constant coeﬃcients.

ȝș
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ȝ
Exercise 2.37 Give exact formulae for bn and an.
Exercise 2.38 Solve the recurrence
an =
√
1 + a2
n−1
for n > 0 with a0 = 0.
Our next example arises in the study of register allocation algorithms [11]:
an = a2
n−1 −2
for n > 0.
For a0 = 0 or a0 = 2, the solution is an = 2 for n > 1, and for a0 = 1, the
solution is an = −1 for n > 1, but for larger a0 the dependence on the initial
value a0 is more complicated for this recurrence than for other ŀrst-order
recurrences that we have seen.
Ļis is a so-called quadratic recurrence, and it is one of the few quadratic
recurrences that can be solved explicitly, by change of variables. By setting
an = bn + 1/bn, we have the recurrence
bn + 1
bn
= b2
n−1 +
1
b2
n−1
for n > 0 with b0 + 1/b0 = a0.
But this implies that we can solve by making bn = b2
n−1, which iterates im-
mediately to the solution
bn = b2n
0 .
By the quadratic equation, b0 is easily calculated from a0:
b0 = 1
2
(
a0 ±
√
a2
0 −4
)
.
Ļus,
an =
(1
2
(
a0 +
√
a2
0 −4
))2n
+
(1
2
(
a0 −
√
a2
0 −4
))2n
.
For a0 > 2, only the larger of the two roots predominates in this expression—
the one with the plus sign.
Exercise 2.39 From the above discussion, solve the register allocation recurrence for
a0 = 3, 4. Discuss what happens for a0 = 3/2.

§ș.Ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȝȚ
Exercise 2.40 Solve the register allocation recurrence for a0 = 2 + ϵ, where ϵ is an
arbitrary ŀxed positive constant. Give an accurate approximate answer.
Exercise 2.41 Find all values of the parameters α, β, and γ such that an = αa2
n−1 +
βan−1+γ reduces to bn = b2
n−1−2 by a linear transformation (bn = f(α, β, γ)an+
g(α, β, γ)). In particular, show that an = a2
n−1 + 1 does not reduce to this form.
Exercise 2.42 [Melzak] Solve the recurrence
an = 2an−1
√
1 −a2
n−1
for n > 0 with a0 = 1
2
and with a0 = 1/3. Plot a6 as a function of a0 and explain what you observe.
On the one hand, underlying linearity may be diﬃcult to recognize,
and ŀnding a change of variable that solves a nonlinear recurrence is no easier
than ŀnding a change of variable that allows us to evaluate a deŀnite integral
(for example). Indeed, more advanced analysis (iteration theory) may be used
to show that most nonlinear recurrences cannot be reduced in this way. On
the other hand, a variable change that simpliŀes a recurrence that arises in
practice may not be diﬃcult to ŀnd, and a few such changes might lead to a
linear form. As illustrated by the register allocation example, such recurrences
do arise in the analysis of algorithms.
For another example, consider using change of variables to get an exact
solution to a recurrence related to continued fractions.
an = 1/(1 + an−1)
for n > 0 with a0 = 1.
Iterating this recurrence gives the sequence
a0 = 1
a1 =
1
1 + 1 = 1
2
a2 =
1
1 +
1
1 + 1
=
1
1 + 1
2
= 2
3

ȝț
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ȝ
Continuing, we have
a3 =
1
1 +
1
1 +
1
1 + 1
=
1
1 + 2
3
= 3
5
a4 =
1
1 +
1
1 +
1
1 +
1
1 + 1
=
1
1 + 3
5
= 5
8
and so on, which reveals the Fibonacci numbers. Ļe form an = bn−1/bn is
certainly suggested: substituting this equation into the recurrence gives
bn−1
bn
= 1 /
(
1 + bn−2
bn−1
)
for n > 1 with b0 = b1 = 1.
Dividing both sides by bn−1 gives
1
bn
=
1
bn−1 + bn−2
for n > 1 with b0 = b1 = 1,
which implies that bn = Fn+1, the Fibonacci sequence. Ļis argument gen-
eralizes to give a way to express general classes of “continued fraction” repre-
sentations as solutions to recurrences.
Exercise 2.43 Solve the recurrence
an = αan−1 + β
γan−1 + δ
for n > 0 with a0 = 1.
Exercise 2.44 Consider the recurrence
an = 1/(sn + tnan−1)
for n > 0 with a0 = 1,
where {sn} and {tn} are arbitrary sequences. Express an as the ratio of two successive
terms in a sequence deŀned by a linear recurrence.

§ș.Ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȝȜ
Repertoire. Another path to exact solutions in some cases is the so-called
repertoire method, where we use known functions to ŀnd a family of solutions
similar to the one sought, which can be combined to give the answer. Ļis
method primarily applies to linear recurrences, involving the following steps:
• Relax the recurrence by adding an extra functional term.
• Substitute known functions into the recurrence to derive identities sim-
ilar to the recurrence.
• Take linear combinations of such identities to derive an equation iden-
tical to the recurrence.
For example, consider the recurrence
an = (n −1)an−1 −nan−2 + n −1
for n > 1 with a0 = a1 = 1.
We generalize this by introducing a quantity f(n) to the right-hand side, so
we want to solve
an = (n −1)an−1 −nan−2 + f(n)
for n > 1 and a0 = a1 = 1 with f(n) = n −1. To do so, we inject various
possibilities for an and look at the resulting f(n) to get a “repertoire” of re-
currences that we can solve (forgetting momentarily about initial conditions).
For this example, we arrive at the table
an
an −(n −1)an−1 + nan−2
1
2
n
n −1
n2
n + 1
Ļe ŀrst row in this table says that an = 1 is a solution with f(n) = 2 (and
initial conditions a0 = 1 and a1 = 1); the second row says that an = n is a
solution with f(n) = n −1 (and initial conditions a0 = 0 and a1 = 1); and
the third row says that an = n2 is a solution with f(n) = n + 1 (and initial
conditions a0 = 0 and a1 = 1). Now, linear combinations of these also give
solutions. Subtracting the ŀrst row from the third gives the result that means
that an = n2 −1 is a solution with f(n) = n −1 (and initial conditions
a0 = −1 and a1 = 0). Now we have two (linearly independent) solutions for
f(n) = n −1, which we combine to get the right initial values, yielding the
result an = n2 −n + 1.

ȝȝ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ȝ
Ļe success of this method depends on being able to ŀnd a set of inde-
pendent solutions, and on properly handling initial conditions. Intuition or
knowledge about the form of the solution can be useful in determining the
repertoire. Ļe classic example of the use of this method is in the analysis of
an equivalence algorithm by Knuth and Schönhage [20].
For the quicksort recurrence, we start with
an = f(n) + 2
n
∑
1≤j≤n
aj−1
for n > 0 with a0 = 0. Ļis leads to the following repertoire table:
an
an −(2 ∑
0≤j<n aj)/n
1
−1
Hn
−Hn + 2
n
1
λ(n + 1)
0
nHn
1
2(n −1) + Hn
n(n −1)
1
3(n2 −1) + n −1
Ļus 2nHn + 2Hn + λ(n + 1) + 2 is a solution with f(n) = n + 1; resolving
the initial value with λ = −2 gives the solution
2(n + 1)Hn −2n,
as expected (see Ļeorem 1.4). Ļe solution depends on the ŀfth line in the
table, which we are obliged to try because we might expect for other reasons
that the solution might be O(nlogn). Note that the repertoire table can con-
veniently also give the solution for other f(n), as might be required by more
detailed analysis of the algorithm.
Exercise 2.45 Solve the quicksort recurrence for f(n) = n3.
Exercise 2.46 [Greene and Knuth] Solve the quicksort median-of-three recurrence
(see equation (4) in Chapter 1) using the repertoire method. (See [18] or [24] for a
direct solution to this recurrence using diﬀerencing and summation factors, and see
Chapter 3 for a solution using generating functions.)

§ș.Ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȝȞ
Bootstrapping.
Often we are able to guess the approximate value of the
solution to a recurrence. Ļen, the recurrence itself can be used to place con-
straints on the estimate that can be used to give a more accurate estimate.
Informally, this method involves the following steps:
• Use the recurrence to calculate numerical values.
• Guess the approximate form of the solution.
• Substitute the approximate solution back into the recurrence.
• Prove tighter bounds on the solution, based on the guessed solution and
the substitution.
For illustrative purposes, suppose that we apply this method to the Fibonacci
recurrence:
an = an−1 + an−2
for n > 1 with a0 = 0 and a1 = 1.
First, we note that an is increasing. Ļerefore, an−1 > an−2 and an > 2an−2.
Iterating this inequality implies that an > 2n/2, so we know that an has
at least an exponential rate of growth. On the other hand, an−2 < an−1
implies that an < 2an−1, or (iterating) an < 2n. Ļus we have proved upper
and lower exponentially growing bounds on an and we can feel justiŀed in
“guessing” a solution of the form an ∼c0αn, with
√
2 < α < 2. From the
recurrence, we can conclude that α must satisfy α2 −α −1 = 0, which leads
to ϕ and bϕ. Having determined the value α, we can bootstrap and go back to
the recurrence and the initial values to ŀnd the appropriate coeﬃcients.
Exercise 2.47 Solve the recurrence
an = 2/(n + an−1)
for n > 0 with a0 = 1.
Exercise 2.48 Use bootstrapping to show that the number of compares used by
median-of-three quicksort is αNlnN + O(N). Ļen determine the value of α.
Exercise 2.49 [Greene and Knuth] Use bootstrapping to show that the solution to
an = 1
n
∑
0≤k<n
ak
n −k
for n > 0 with a0 = 1
satisŀes n2an = O(1).

ȝȟ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ȝ
Perturbation. Another path to an approximate solution to a recurrence is
to solve a simpler related recurrence. Ļis is a general approach to solving
recurrences that consists of ŀrst studying simpliŀed recurrences obtained by
extracting what seems to be dominant parts, then solving the simpliŀed recur-
rence, and ŀnally comparing solutions of the original recurrence to those of
the simpliŀed recurrence. Ļis technique is akin to a class of methods familiar
in numerical analysis, perturbation methods. Informally, this method involves
the following steps:
• Modify the recurrence slightly to ŀnd a known recurrence.
• Change variables to pull out the known bounds and transform into a
recurrence on the (smaller) unknown part of the solution.
• Bound the unknown “error” term.
For example, consider the recurrence
an+1 = 2an + an−1
n2
for n > 1 with a0 = 1 and a1 = 2.
It seems reasonable to assume that the last term, because of its coeﬃcient
1/n2, makes only a small contribution to the recurrence, so that
an+1 ≈2an.
Ļus a growth of the rough form an ≈2n is anticipated. To make this precise,
we thus consider the simpler sequence
bn+1 = 2bn
for n > 0 with b0 = 1
(so that bn = 2n) and compare the two recurrences by forming the ratio
ρn = an
bn
= an
2n .
From the recurrences, we have
ρn+1 = ρn +
1
4n2 ρn−1
for n > 0 with ρ0 = 1.
Clearly, the ρn are increasing. To prove they tend to a constant, note that
ρn+1 ≤ρn
(
1 +
1
4n2
)
for n ≥1 so that
ρn+1 ≤
n
∏
k=1
(
1 +
1
4k2
)
.

§ș.Ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȝȠ
But the inŀnite product corresponding to the right-hand side converges mono-
tonically to
α0 =
∞
∏
k=1
(
1 +
1
4k2
)
= 1.46505 · · · .
Ļus, ρn is bounded from above by α0 and, as it is increasing, it must converge
to a constant. We have thus proved that
an ∼α · 2n,
for some constant α < 1.46505 · · · . (In addition, the bound is not too crude
as, for instance, ρ100 = 1.44130 · · · .)
Ļe example above is only a simple one, meant to illustrate the approach.
In general, the situation is likely to be more complex, and several steps of it-
eration of the method may be required, possibly introducing several interme-
diate recurrences. Ļis relates to bootstrapping, which we have just discussed.
Hardships may also occur if the simpliŀed recurrence admits of no closed form
expression. Ļe perturbation method is nonetheless an important technique
for the asymptotic solution of recurrences.
Exercise 2.50 Find the asymptotic growth of the solution to the “perturbed” Fibo-
nacci recurrence
an+1 =
(
1 + 1
n
)
an +
(
1 −1
n
)
an−1
for n > 1 with a0 = 0 and a1 = 1.
Exercise 2.51 Solve the recurrence
an = nan−1 + n2an−2
for n > 1 with a1 = 1 and a0 = 0.
Exercise 2.52 [Aho and Sloane] Ļe recurrence
an = a2
n−1 + 1
for n > 0 with a0 = 1
satisŀes an ∼λα2n for some constants α and λ. Find a convergent series for α and
determine α to 50 decimal digits. (Hint : Consider bn = lgan.)
Exercise 2.53 Solve the following perturbation of the Fibonacci recurrence:
an =
(
1 −1
n
)
(an−1 + an−2)
for n > 1 with a0 = a1 = 1.
Try a solution of the form nαϕn and identify α.

Ȟȗ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ȝ
2.6 Binary Divide-and-Conquer Recurrences and Binary Numbers.
Good algorithms for a broad variety of problems have been developed by ap-
plying the following fundamental algorithmic design paradigm: “Divide the
problem into two subproblems of equal size, solve them recursively, then use
the solutions to solve the original problem.” Mergesort is a prototype of such
algorithms. For example (see the proof of Ļeorem 1.2 in §1.2), the number
of comparisons used by mergesort is given by the solution to the recurrence
CN = C⌊N/2⌋+ C⌈N/2⌉+ N
for N > 1 with C1 = 0.
(4)
Ļis recurrence, and others similar to it, arise in the analysis of a variety of
algorithms with the same basic structure as mergesort. It is normally pos-
sible to determine the asymptotic growth of functions satisfying such recur-
rences, but it is necessary to take special care in deriving exact results, primar-
ily because of the simple reason that a problem of “size” N cannot be divided
 
128
256
64
384
896
2048
 
Figure 2.1 Solutions to binary divide-and-conquer recurrences
CN = C⌊N/2⌋+ C⌈N/2⌉+ N (bottom)
CN = C⌈N/2⌉+ C⌈N/2⌉+ N (top)

§ș.ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȞȘ
 
128
256
64
32
48
96
192
384
 
Figure 2.2 Periodic terms in binary divide-and-conquer recurrences
CN = C⌊N/2⌋+ C⌈N/2⌉+ N (bottom)
CN = C⌈N/2⌉+ C⌈N/2⌉+ N (top)
into equal-sized subproblems if N is odd: the best that can be done is to make
the problem sizes diﬀer by one. For large N, this is negligible, but for small
N it is noticeable, and, as usual, the recursive structure ensures that many
small subproblems will be involved.
As we shall soon see, this means that exact solutions tend to have period-
icities, sometimes even severe discontinuities, and often cannot be described
in terms of smooth functions. For example, Figure 2.1 shows the solution to
the mergesort recurrence (4) and the similar recurrence
CN = 2C⌈N/2⌉+ N
for N > 1 with C1 = 0.
Ļe former appears to be relatively smooth; the erratic fractal-based behavior
that characterizes the solution to the latter is common in divide-and-conquer
recurrences.
Both of the functions illustrated in Figure 2.1 are ∼NlgN and pre-
cisely equal to NlgN when N is a power of 2. Figure 2.2 is a plot of the
same functions with NlgN subtracted out, to illustrate the periodic behavior
of the linear term for both functions. Ļe periodic function associated with
mergesort is quite small in magnitude and continuous, with discontinuities in
the derivative at powers of 2; the other function can be relatively large and is
essentially discontinuous. Such behavior can be problematic when we are try-
ing to make precise estimates for the purposes of comparing programs, even
asymptotically. Fortunately, however, we typically can see the nature of the
solutions quite easily when the recurrences are understood in terms of number
representations. To illustrate this, we begin by looking at another important

Ȟș
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ȝ
algorithm that is a speciŀc instance of a general problem-solving strategy that
dates to antiquity.
Binary search.
One of the simplest and best-known binary divide-and-
conquer algorithms is called binary search. Given a ŀxed set of numbers, we
wish to be able to determine quickly whether a given query number is in the
set. To do so, we ŀrst sort the table. Ļen, for any query number, we can use
the method shown in Program 2.2: Look in the middle and report success if
the query number is there. Otherwise, (recursively) use the same method to
look in the left half if the number is smaller than the middle number and in
the right half if the query number is larger than the middle number.
Ļeorem 2.3 (Binary search).
Ļe number of comparisons used during an
unsuccessful search with binary search in a table of size N in the worst case
is equal to the number of bits in the binary representation of N. Both are
described by the recurrence
BN = B⌊N/2⌋+ 1
for N ≥2 with B1 = 1,
which has the exact solution BN = ⌊lgN⌋+ 1.
Proof. After looking in “the middle,” one element is eliminated, and the two
halves of the ŀle are of size ⌊(N −1)/2⌋and ⌈(N −1)/2⌉. Ļe recurrence
public static int search(int key, int lo, int hi)
{
if (lo > hi) return -1;
int mid = lo + (hi - lo) / 2;
if (key < a[mid])
return search(key, lo, mid - 1);
else if (key > a[mid])
return search(key, mid + 1, hi);
else return mid;
}
Program 2.2 Binary search

§ș.ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȞȚ
is established by checking separately for N odd and N even that the larger of
these two is always ⌊N/2⌋. For example, in a table of size 83, both subŀles
are of size 41 after the ŀrst comparison, but in a table of size 82, one is of size
40 and the other of size 41.
Ļis is equal to the number of bits in the binary representation of N (ig-
noring leading 0s) because computing ⌊N/2⌋is precisely equivalent to shift-
ing the binary representation right by one bit position. Iterating the recur-
rence amounts to counting the bits, stopping when the leading 1 bit is en-
countered.
Ļe number of bits in the binary representation of N is n + 1 for 2n ≤
N < 2n+1, or, taking logarithms, for n ≤lgN < n + 1; that is to say, by
deŀnition, n = ⌊lgN⌋.
Ļe functions lgN and ⌊lgN⌋are plotted in Figure 2.3, along with the
fractional part {lgN} ≡lgN −⌊lgN⌋.
Exercise2.54 What is the number of comparisons used during an unsuccessful search
with binary search in a table of size N in the best case?
Exercise 2.55 Consider a “ternary search” algorithm, where the ŀle is divided into
thirds, two comparisons are used to determine where the key could be, and the al-
gorithm is applied recursively. Characterize the number of comparisons used by that
algorithm in the worst case, and compare it to a binary search.
Exact solution of mergesort recurrence.
Ļe mergesort recurrence (2) is
easily solved by diﬀerencing: if DN is deŀned to be CN+1 −CN, then DN
satisŀes the recurrence
DN = D⌊N/2⌋+ 1
for N ≥2 with D1 = 2,
which iterates to
DN = ⌊lgN⌋+ 2,
and, therefore,
CN = N −1 +
∑
1≤k<N
(⌊lgk⌋+ 1).
Ļere are a number of ways to evaluate this sum to give an exact formula
for CN: as mentioned earlier, it is useful to adopt the approach of noting a
relationship to the binary representations of integers. In particular, we just
saw that ⌊lgk⌋+ 1 is the number of bits in the binary representation of k

Ȟț
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ȝ
 
16
128
1
4
8
 
 
16
128
1
4
8
 
 
16
128
1
4
8
 
Figure 2.3 lgN (top); ⌊lgN⌋(middle); {lgN} (bottom)
(ignoring leading 0s), so CN is precisely the number of bits in the binary
representations of all the positive numbers less than N plus N −1.
Ļeorem 2.4 (Mergesort).
Ļe number of comparisons used by mergesort
is equal to N −1 plus the number of bits in the binary representations of all
the numbers less than N. Both quantities are described by the recurrence
CN = C⌊N/2⌋+ C⌈N/2⌉+ N
for N ≥2 with C1 = 0,
which has the exact solution CN = N⌊lgN⌋+ 2N −2⌊lgN⌋+1.
Proof. Ļe ŀrst part of the theorem is established by the earlier discussion.
Now, all N −1 of the numbers less than N have a rightmost bit; N −2 of
them (all except 1) have a second-to-rightmost bit; N −4 of them (all except
1, 2, and 3) have a third-to-rightmost bit; N −8 of them have a fourth-to-
rightmost bit, and so on, so we must have
CN = (N −1) + (N −1) + (N −2) + (N −4) + · · · + (N −2⌊lgN⌋)
= (N −1) + N(⌊lgN⌋+ 1) −(1 + 2 + 4 + . . . + 2⌊lgN⌋)
= N⌊lgN⌋+ 2N −2⌊lgN⌋+1.
As noted earlier, ⌊lgN⌋is a discontinuous function with periodic behavior.
Also as mentioned, however, CN itself is continuous, so the discontinuities
(but not the periodicities) in the two functions involving ⌊lgN⌋cancel out.
Ļis phenomenon is illustrated in Figure 2.4 and supported by the calculations
in the corollary that follows.

§ș.ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȞȜ
Corollary CN = NlgN + Nθ(1 −{lgN}), where θ(x) = 1 + x −2x is
a positive function satisfying θ(0) = θ(1) = 0 and 0 < θ(x) < .086 for
0 < x < 1.
Proof. Straightforward by substituting the decomposition ⌊lgN⌋= lgN −
{lgN}. Ļe value .086 ≈1−lge+lglge is calculated by setting the derivative
of θ(x) to zero.
Exercise 2.56 By considering the rightmost bits, give a direct proof that the number
of bits in the binary representations of all the numbers less than N satisŀes (4), but
with an additive term of N −1 instead of N.
Exercise 2.57 Prove that N⌊lgN⌋+ 2N −2⌊lgN⌋+1 = N⌈lgN⌉+ N −2⌈lgN⌉for
all positive N. (See Exercise 1.4.)
Other properties of binary numbers. We often study properties of binary
integers because they naturally model the (binary) decision-making process in
many basic algorithms. Ļe quantities encountered earlier are likely to arise
in the analysis of any algorithm that solves a problem by recursively dividing
 
64
128
256
.086
0
-1
1
0
Figure 2.4 Composition of the periodic function θ(1 −{lgN})
1 −{lgN} (top)
1 −21−{lgN} (middle)
2 −{lgN} −21−{lgN} (bottom)

Ȟȝ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ȝ
it in two, the way that binary search and mergesort do, and similar quantities
clearly will arise in other divide-and-conquer schemes. To complete our study
of binary divide-and-conquer recurrences, we consider two more properties
of the binary representation of numbers that frequently arise in the analysis
of algorithms.
Deŀnition Given an integer N, deŀne the population count function νN to
be the number of 1s in the binary representation of N and the cumulated pop-
ulation count function PN to be the number of 1s in the binary representations
of all the numbers less than N.
Deŀnition Given an integer N, deŀne the ruler function ψN to be the num-
ber of trailing 1s in the binary representation of N and the cumulated ruler
function RN to be the number of trailing 1s in the binary representations of
all the numbers less than N.
Table 2.3 gives the values of the functions for N < 16. Ļe reader may
ŀnd it instructive to try to compute values of these functions for larger N. For
example, the binary representation of 83 is 1010011, so ν83 = 4 and ψ83 = 2.
Ļe cumulated values
PN ≡
∑
0≤j<N
νj
and
RN ≡
∑
0≤j<N
ψj
are less easily computed. For example, P84 = 215 and R84 = 78.
It is not hard to see, for example, that
P2n = n2n−1 and R2n = 2n −1
N
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111
νN
1
1
2
1
2
2
3
1
2
2
3
2
3
3
4
PN
0
1
2
4
5
7
9
12
13
15
17
20
22
25
28
ψN
1
0
2
0
1
0
3
0
1
0
2
0
1
0
4
RN
0
1
1
3
3
4
4
7
7
8
8
10
10
11
11
Table 2.3
Ruler and population count functions

§ș.ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȞȞ
and that
PN = 1
2NlgN + O(N) and RN = N + O(lgN),
but exact representations or more precise asymptotic estimates are diﬃcult to
derive, as indicated by the plot of PN −(NlgN)/2 in Figure 2.5.
As noted previously, functions of this type satisfy recurrences that are
simply derived but can diﬀer markedly, even when describing the same quan-
tity. For example, considering the leftmost bits, we see that 2⌊lgN⌋of the
numbers less than N start with 0 and the rest start with 1, so we have
PN = P2⌊lgN⌋+ (N −2⌊lgN⌋) + PN−2⌊lgN⌋.
But also, considering the rightmost bit, it is clear that
PN = P⌊N/2⌋+ P⌈N/2⌉+ ⌊N/2⌋
for N > 1 with P1 = 0.
Ļis is similar to the mergesort recurrence, which is associated with counting
all the bits less than N (there should be about half as many 1 bits), but the
function is remarkably diﬀerent. (Compare Figure 2.5 with Figure 2.2.) Ļe
function counting the number of 0 bits is similar: both are fractal in behavior,
but they cancel out to get the periodic but continuous function that we have
seen before (see Figure 2.2). Delange [7] studied the function PN in some
detail, and expressed it in terms of a function that is nowhere diﬀerentiable.
256
512
128
64
32
108
54
27
0
-18
-36
-72
Figure 2.5 Periodic and fractal terms in bit counting
{# 1 bits in numbers less than N} −(NlgN)/2 (top)
{# 0 bits in numbers less than N} −(NlgN)/2 (bottom)
{# bits in numbers less than N} −NlgN (middle)

Ȟȟ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.ȝ
Other recurrences. Binary search and mergesort are typical “binary divide-
and-conquer” algorithms. Ļe examples that we have given reveal relation-
ships to properties of the binary representations of numbers that we can use to
develop more precise solutions for other, similar, recurrences along the lines
we have been discussing.
Table 2.4 shows a number of recurrences that commonly arise, along
with approximate solutions derived from the general theorems given in the
next section. In the table, aN/2 means “a⌊N/2⌋or a⌈N/2⌉,” 2aN/2 means
“aN/2 + aN/2,” and so forth—in the next section we discuss the fact that
minor variations of this sort do not aﬀect the asymptotic results given in Ta-
ble 2.4 (though they do prevent us from giving more general estimates).
Normally, applications involving such recurrences involve worst-case re-
sults, as in Ļeorem 2.5 and Ļeorem 2.6, but if the subproblems are inde-
pendent and still “random” after the divide step, then these results can also
yield expected costs for some problems.
It is worth recognizing that it is only a small step from these properties
of numbers to properties of bitstrings that are certainly more combinatorial
aN = aN/2 + 1
lgN + O
(
1
)
aN = aN/2 + N
2N + O
(
lgN
)
aN = aN/2 + NlgN
Θ(NlgN)
aN = 2aN/2 + 1
Θ(N)
aN = 2aN/2 + lgN
Θ(N)
aN = 2aN/2 + N
NlogN + O
(
N
)
aN = 2aN/2 + NlgN
1
2NlgN 2 + O
(
NlogN
)
aN = 2aN/2 + Nlgδ−1N
δ−1NlgδN + O
(
Nlgδ−1N
)
aN = 2aN/2 + N 2
2N 2 + O
(
N
)
aN = 3aN/2 + N
Θ(N lg3)
aN = 4aN/2 + N
Θ(N 2)
Table 2.4
Binary divide-and-conquer recurrences and solutions

§ș.ȝ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȞȠ
in nature. For example, suppose that we wanted to know the average length
of the longest string of consecutive 0s in a random bitstring, say, to enable
certain optimizations in designing an arithmetic unit. Is this a property of
the number, or the bits that represent it? Such questions lead immediately
to more generally applicable combinatorial studies, which we will consider in
Chapter 8.
We encounter functions of this type frequently in the analysis of algo-
rithms, and it is worthwhile to be cognizant of their relationship to simple
properties of binary numbers. Beyond divide-and-conquer algorithms, these
functions also arise, of course, in the direct analysis of arithmetic algorithms
on numbers represented in binary. A famous example of this is the analysis of
the expected length of the carry propagation chain in an adder, a problem dat-
ing back to von Neumann that was completely solved by Knuth [19]. Ļese
results relate directly to analyses of fundamental algorithms on strings, as we
will see in Chapter 8. Binary divide-and-conquer recurrences and properties
of their solutions are studied in more detail by Allouche and Shallit [2] and
by Flajolet and Golin [9].
Exercise 2.58 Give recurrences for the functions plotted in Figure 2.5.
Exercise 2.59 Derive recurrences for RN similar to those given above for PN.
Exercise 2.60 Plot the solution to the recurrence
AN = A⌊N/2⌋+ A⌈N/2⌉+ ⌊lgN⌋
for N ≥2 with A1 = 0,
for 1 ≤N ≤512.
Exercise 2.61 Plot the solution to the recurrence
BN = 3B⌈N/2⌉+ N
for N ≥2 with B1 = 0,
for 1 ≤N ≤512.
Exercise 2.62 Plot the solution to
DN = D⌈N/2⌉+ D⌈N/2⌉+ CN
for N > 1 with D1 = 0
where CN is the solution to
CN = C⌈N/2⌉+ C⌈N/2⌉+ N
for N > 1 with C1 = 0.
Consider the variants of this problem derived by changing ⌈N/2⌉to ⌊N/2⌋in each
of the terms.

ȟȗ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ȟ
Exercise 2.63 Take the binary representation of N, reverse it, and interpret the result
as an integer, ρ(N). Show that ρ(N) satisŀes a divide-and-conquer recurrence. Plot
its values for 1 ≤N ≤512 and explain what you see.
Exercise 2.64 What is the average length of the initial string of 1s in the binary
representation of a number less than N, assuming all such numbers are equally likely?
Exercise 2.65 What is the average length of the initial string of 1s in a random
bitstring of length N, assuming all such strings are equally likely?
Exercise 2.66 What is the average and the variance of the length of the initial string
of 1s in a (potentially inŀnite) sequence of random bits?
Exercise 2.67 What is the total number of carries made when a binary counter in-
crements N times, from 0 to N?
2.7 General Divide-and-Conquer Recurrences. More generally, eﬃ-
cient algorithms and upper bounds in complexity studies are very often de-
rived by extending the divide-and-conquer algorithmic design paradigm along
the following lines: “Divide the problem into smaller (perhaps overlapping)
subproblems, solve them recursively, then use the solutions to solve the orig-
inal problem.” A variety of “divide-and-conquer” recurrences arise that de-
pend on the number and relative size of subproblems, the extent to which they
overlap, and the cost of recombining them for the solution. It is normally
possible to determine the asymptotic growth of functions satisfying such re-
currences, but, as above, the periodic and fractal nature of functions that are
involved make it necessary to specify details carefully.
In pursuit of a general solution, we start with the recursive formula
a(x) = αa(x/β) + f(x)
for x > 1 with a(x) = 0 for x ≤1
deŀning a function over the positive real numbers. In essence, this corresponds
to a divide-and-conquer algorithm that divides a problem of size x into α
subproblems of size x/β and recombines them at a cost of f(x). Here a(x)
is a function deŀned for positive real x, so that a(x/β) is well deŀned. In
most applications, α and β will be integers, though we do not use that fact in
developing the solution. We do insist that β > 1, of course.
For example, consider the case where f(x) = x and we restrict ourselves
to the integers N = βn. In this case, we have
aβn = αaβn−1 + βn
for n > 0 with a1 = 0.

§ș.Ȟ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȟȘ
Dividing both sides by αn and iterating (that is, applying Ļeorem 2.1) we
have the solution
aβn = αn
∑
1≤j≤n
(β
α
)j.
Now, there are three cases: if α > β, the sum converges to a constant; if
α = β, it evaluates to n; and if α < β, the sum is dominated by the latter
terms and is O(β/α)n. Since αn = (βlogβα)n = (βn)logβα, this means that
the solution to the recurrence is O(Nlogβα) when α > β, O(NlogN) when
α = β, and O(N) when α < β. Ļough this solution only holds for N = βn,
it illustrates the overall structure encountered in the general case.
Ļeorem 2.5 (Divide-and-conquer functions).
If the function a(x) satis-
ŀes the recurrence
a(x) = αa(x/β) + x
for x > 1 with a(x) = 0 for x ≤1
then
if α < β
a(x) ∼
β
β −αx
if α = β
a(x) ∼xlogβx
if α > β
a(x) ∼
α
α −β
(β
α
){logβα}xlogβα.
Proof. Ļe basic idea, which applies to all divide-and-conquer recurrences,
is to iterate the recurrence until the initial conditions are met for the sub-
problems. Here, we have
a(x) = x + αa(x/β)
= x + αx
β + αa(x/β2)
= x + αx
β + α2 x
β2 + αa(x/β3)
and so on. After t = ⌊logβx⌋iterations, the term a(x/βt) that appears can
be replaced by 0 and the iteration process terminates. Ļis leaves an exact
representation of the solution:
a(x) = x
(
1 + α
β + . . . + αt
βt
)
.

ȟș
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ȟ
Now, as mentioned earlier, three cases can be distinguished. First, if α < β
then the sum converges and
a(x) ∼x
∑
j≥0
(α
β
)j =
β
β −αx.
Second, if α = β then each of the terms in the sum is 1 and the solution is
simply
a(x) = x(⌊logβx⌋+ 1) ∼xlogβx.
Ļird, if α > β then the last term in the sum predominates, so that
a(x) = x
(α
β
)t(
1 + β
α + . . . + βt
αt
)
∼x
α
α −β
(α
β
)t.
As mentioned previously, the periodic behavior of the expression in the
third case can be isolated by separating the integer and fractional part of logβx
and writing t ≡⌊logβx⌋= logβx −{logβx}. Ļis gives
x
(α
β
)t = x
(α
β
)logβx(α
β
)−{logβx} = xlogβα(β
α
){logβx},
since αlogβx = xlogβα. Ļis completes the proof.
For α ≤β, the periodic behavior is not in the leading term, but for
α > β, the coeﬃcient of xlogβα is a periodic function of logβx that is bounded
and oscillates between α/(α −β) and β/(α −β).
Figure 2.6 illustrates how the relative values of α and β aﬀect the asymp-
totic growth of the function. Boxes in the ŀgures correspond to problem sizes
for a divide-and-conquer algorithm. Ļe top diagram, where a problem is
split into two subproblems, each a third of the size of the original, shows how
the performance is linear because the problem sizes go to 0 exponentially fast.
Ļe middle diagram, where a problem is split into three subproblems, each
a third of the size of the original, shows how the total problem size is well
balanced so a “log” multiplicative factor is needed. Ļe last diagram, where a
problem is split into four subproblems, each a third of the size of the original,

§ș.Ȟ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȟȚ
Figure 2.6 Divide-and-conquer for β = 3 and α = 2, 3, 4
shows how the total problem size grows exponentially, so the total is domi-
nated by the last term. Ļis shows the asymptotic growth and is representative
of what happens in general situations.
To generalize this to the point where it applies to practical situations,
we need to consider other f(x) and less restrictive subdivision strategies than
precisely equal subproblem sizes (which will allow us to move back to recur-
rences on integers). For other f(x), we proceed precisely as done earlier: at
the top level we have one problem of cost f(x), then we have α problems of
cost f(x/β), then α2 problems of cost f(x/β2), and so on, so the total cost
is
f(x) + αf(x/β) + α2f(x/β2) + · · · .
As earlier, there are three cases: if α > β, the later terms in the sum dominate;
if α = β, the terms are roughly equal; and if α < β, the early terms dominate.
Some “smoothness” restrictions on the function f are necessary to derive a
precise answer. For example, if we restrict f to be of the form xγ(logx)δ—

ȟț
C Ŕ ō Ŝ Š ő Ş
T ţ ś
§ș.Ȟ
which actually represents a signiŀcant portion of the functions that arise in
complexity studies—an argument similar to that given previously can be used
to show that
if γ < logβα
a(x) ∼c1xγ(logx)δ
if γ = logβα
a(x) ∼c2xγ(logx)δ+1
if γ > logβα
a(x) = (xlogβα)
where c1 and c2 are appropriate constants that depend on α, β, and γ.
Exercise 2.68 Give explicit formulae for c1 and c2. Start by doing the case δ = 0.
Intuitively, we expect the same kind of result even when the subproblems
are almost, but not necessarily exactly, the same size. Indeed, we are bound
to consider this case because “problem sizes” must be integers: of course, di-
viding a ŀle whose size is odd into two parts gives subproblems of almost,
but not quite, the same size. Moreover, we expect that we need not have an
exact value of f(x) in order to estimate the growth of a(x). Of course, we
are also interested in functions that are deŀned only on the integers. Putting
these together, we get a result that is useful for the analysis of a variety of
algorithms.
Ļeorem 2.6 (Divide-and-conquer sequences).
If a divide-and-conquer
algorithm works by dividing a problem of size n into α parts, each of size
n/β +O(1), and solving the subproblems independently with additional cost
f(n) for dividing and combining, then if f(n) = (nγ(logn)δ), the total
cost is given by
if γ < logβα
an = (nγ(logn)δ)
if γ = logβα
an = (nγ(logn)δ+1)
if γ > logβα
an = (nlogβα).
Proof. Ļe general strategy is the same as used earlier: iterate the recurrence
until the initial conditions are satisŀed, then collect terms. Ļe calculations
involved are rather intricate and are omitted here.

§ș.Ȟ
R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȟȜ
In complexity studies, a more general formulation is often used, since
less speciŀc information about f(n) may be available. Under suitable condi-
tions on the smoothness of f(n), it can be shown that
if f(n) = O(nlogβα−ϵ)
an = (nlogβα)
if f(n) = (nlogβα)
an = (nlogβαlogn)
if f(n) = 
(nlogβα+ϵ)
an = (f(n)).
Ļis result is primarily due to Bentley, Haken, and Saxe [4]; a full proof of a
similar result may also be found in [5]. Ļis type of result is normally used to
prove upper bounds and lower bounds on asymptotic behavior of algorithms,
by choosing f(n) to bound true costs appropriately. In this book, we are
normally interested in deriving more accurate results for speciŀc f(n).
Exercise 2.69 Plot the periodic part of the solution to the recurrence
aN = 3a⌊N/3⌋+ N
for N > 3 with a1 = a2 = a3 = 1
for 1 ≤N ≤972.
Exercise 2.70 Answer the previous question for the other possible ways of dividing a
problem of size N into three parts with the size of each part either ⌊N/3⌋or ⌈N/3⌉.
Exercise 2.71 Give an asymptotic solution to the recurrence
a(x) = αax/β + 2x
for x > 1 with a(x) = 0 for x ≤1.
Exercise 2.72 Give an asymptotic solution to the recurrence
aN = a3N/4 + aN/4 + N
for N > 2 with a1 = a2 = a3 = 1.
Exercise 2.73 Give an asymptotic solution to the recurrence
aN = aN/2 + aN/4 + N
for N > 2 with a1 = a2 = a3 = 1.
Exercise 2.74 Consider the recurrence
an = af(n) + ag(n) + ah(n) + 1
for n > t with an = 1 for n < t
with the constraint that f(n) + g(n) + h(n) = n. Prove that an = Θ(n).
Exercise 2.75 Consider the recurrence
an = af(n) + ag(n) + 1
for N > t with an = 1 for n < t
with f(n) + g(n) = n −h(n). Give the smallest value of h(n) for which you can
prove that an/n →0 as n →∞.

ȟȝ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
R
ECURRENCE relations correspond naturally to iterative and recursive
programs, and they can serve us well in a variety of applications in the
analysis of algorithms, so we have surveyed in this chapter the types of recur-
rence relations that can arise and some ways of coping with them. Under-
standing an algorithm suﬃciently well to be able to develop a recurrence rela-
tion describing an important performance characteristic is often an important
ŀrst step in analyzing it. Given a recurrence relation, we can often compute
or estimate needed parameters for practical applications even if an analytic
solution seems too diﬃcult to obtain. On the other hand, as we will see, the
existence of a recurrence often signals that our problem has suﬃcient structure
that we can use general tools to develop analytic results.
Ļere is a large literature on “diﬀerence equations” and recurrences, from
which we have tried to select useful and relevant tools, techniques, and ex-
amples. Ļere are general and essential mathematical tools for dealing with
recurrences, but ŀnding the appropriate path to solving a particular recurrence
is often challenging. Nevertheless, a careful analysis can lead to understand-
ing of the essential properties of a broad variety of the recurrences that arise
in practice. We calculate values of the recurrence to get some idea of its rate
of growth; try telescoping (iterating) it to get an idea of the asymptotic form
of the solution; perhaps look for a summation factor, change of variable, or
repertoire suite that can lead to an exact solution; or apply an approximation
technique such as bootstrapping or perturbation to estimate the solution.
Our discussion has been exclusively devoted to recurrences on one index
N. We defer discussion of multivariate and other types of recurrences until
we have developed more advanced tools for solving them.
Studies in the theory of algorithms often depend on solving recurrences
for estimating and bounding the performance characteristics of algorithms.
Speciŀcally, the “divide-and-conquer” recurrences that we considered at the
end of the chapter arise particularly frequently in the theoretical computer
science literature, as divide-and-conquer is a principal tool in algorithm de-
sign. Most such recurrences have a similar structure, which rełects the degree
of balance in the algorithm design. Ļey also are closely related to properties
of number systems, and thus tend to exhibit fractal-like behavior. Approx-
imate bounds such as those we have seen are appropriate (and widely used)
for deriving upper bounds in complexity proofs, but not necessarily used for
analyzing the performance of algorithms, because they do not always provide
suﬃciently accurate information to allow us to predict performance. We can

R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȟȞ
often get more precise estimates in situations where we have more precise
information about f(n) and the divide-and-conquer method.
Recurrences arise in a natural way in the study of performance charac-
teristics of algorithms. As we develop detailed analyses of complicated algo-
rithms, we encounter rather complex recurrences to be solved. In the next
chapter, we introduce generating functions, which are fundamental to the
analysis of algorithms. Not only can they help us solve recurrences, but also
they have a direct connection to algorithms at a high level, allowing us to leave
the detailed structure described by recurrences below the surface for many ap-
plications.

ȟȟ
C Ŕ ō Ŝ Š ő Ş
T ţ ś
References
1. A. V. AŔś ōŚŐ N. J. A. SŘśōŚő. “Some doubly exponential sequences,”
Fibonacci Quarterly 11, 1973, 429–437.
2. J.-P. AŘŘśšŏŔő ōŚŐ J. SŔōŘŘŕŠ. “Ļe ring of k-regular sequences,” Ļe-
oretical Computer Science 98, 1992, 163–197.
3. C. M. BőŚŐőŞ ōŚŐ S. A. OŞşŦōœ. Advanced Mathematical Methods for
Scientists and Engineers, McGraw-Hill, New York, 1978.
4. J. L. BőŚŠŘőť, D. HōŗőŚ, ōŚŐ J. B. SōŤő. “A general method for solv-
ing divide-and-conquer recurrences,” SIGACT News, Fall 1980, 36–44.
5. T. H. CśŞřőŚ, C. E. LőŕşőŞşśŚ, R. L. RŕŢőşŠ, ōŚŐ C. SŠőŕŚ. Intro-
duction to Algorithms, MIT Press, New York, 3rd, edition, 2009.
6. N. G. Dő BŞšŕŖŚ. Asymptotic Methods in Analysis, Dover Publications,
New York, 1981.
7. H. DőŘōŚœő. “Sur la fonction sommatoire de la fonction somme des
chiﬀres,” L’enseignement Mathématique XXI, 1975, 31–47.
8. P. FŘōŖśŘőŠ ōŚŐ M. GśŘŕŚ. “Exact asymptotics of divide-and-conquer
recurrences,” in Automata, Languages, and Programming, A. Lingas, R.
Karlsson, and S. Carlsson, eds., Lecture Notes in Computer Science
#700, Springer Verlag, Berlin, 1993, 137–149.
9. P. FŘōŖśŘőŠ ōŚŐ M. GśŘŕŚ. “Mellin transforms and asymptotics: the
mergesort recurrence,” Acta Informatica 31, 1994, 673–696.
10. P. FŘōŖśŘőŠ, P. GŞōŎŚőŞ, ōŚŐ P. KŕŞşŏŔőŚŔśŒőŞ. “Mellin transforms
and asymptotics: digital sums,” Ļeoretical Computer Science 123, 1994,
291–314.
11. P. FŘōŖśŘőŠ, J.-C. RōśšŘŠ, ōŚŐ J. VšŕŘŘőřŕŚ. “Ļe number of reg-
isters required to evaluate arithmetic expressions,” Ļeoretical Computer
Science 9, 1979, 99–125.
12. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
13. R. L. GŞōŔōř, D. E. KŚšŠŔ, ōŚŐ O. PōŠōşŔŚŕŗ. Concrete Mathemat-
ics, 1st edition, Addison-Wesley, Reading, MA, 1989. Second edition,
1994.
14. D. H. GŞőőŚő ōŚŐ D. E. KŚšŠŔ. Mathematics for the Analysis of Algo-
rithms, Birkhäuser, Boston, 1981.

R ő ŏ š Ş Ş ő Ś ŏ ő
R ő Ř ō Š ŕ ś Ś ş
ȟȠ
15. P. HőŚŞŕŏŕ. Applied and Computational Complex Analysis, 3 volumes,
John Wiley, New York, 1974 (volume 1), 1977 (volume 2), 1986 (vol-
ume 3).
16. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 2: Seminumerical
Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1969. Ļird
edition, 1997.
17. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.
18. D. E. KŚšŠŔ. “Ļe average time for carry propagation,” Indagationes
Mathematicae 40, 1978, 238–242.
19. D. E. KŚšŠŔ ōŚŐ A. SŏŔƙŚŔōœő. “Ļe expected linearity of a simple
equivalence algorithm,” Ļeoretical Computer Science 6, 1978, 281–315.
20. G. LšőŗőŞ. “Some techniques for solving recurrences,” Computing Sur-
veys 12, 1980, 419–436.
21. Z. A. MőŘŦōŗ. Companion to Concrete Mathematics, John Wiley, New
York, 1968.
22. J. RŕśŞŐōŚ. Combinatorial Identities, John Wiley, New York, 1968.
23. R. SőŐœőţŕŏŗ. “Ļe analysis of quicksort programs,” Acta Informatica
7, 1977, 327–355.
24. A. Yōś. “On random 2–3 trees,” Acta Informatica 9, 1978, 159–170.

This page intentionally left blank 

C H A P T E R T H R E E
G E N E R A T I N G F U N C T I O N S
I
N this chapter we introduce the central concept that we use in the analysis
of algorithms and data structures: generating functions. Ļis mathemat-
ical material is so fundamental to the rest of the book that we shall concen-
trate on presenting a synopsis somewhat apart from applications, though we
do draw some examples from properties of algorithms.
After deŀning the basic notions of “ordinary” generating functions and
“exponential” generating functions, we begin with a description of the use
of generating functions to solve recurrence relations, including a discussion
of necessary mathematical tools. For both ordinary and exponential gener-
ating functions, we survey many elementary functions that arise in practice,
and consider their basic properties and ways of manipulating them. We dis-
cuss a number of examples, including a detailed look at solving the quicksort
median-of-three recurrence from Chapter 1.
We normally are interested not just in counting combinatorial struc-
tures, but also in analyzing their properties. We look at how to use “bivariate”
generating functions for this purpose, and how this relates to the use of “prob-
ability” generating functions.
Ļe chapter concludes with a discussion of various special types of gen-
erating functions that can arise in applications in the analysis of algorithms.
Because they appear throughout the book, we describe basic proper-
ties and techniques for manipulating generating functions in some detail and
provide a catalog of the most important ones in this chapter, for reference.
We introduce a substantial amount of material, with examples from combina-
torics and the analysis of algorithms, though our treatment of each particular
topic is relatively concise. Fuller discussion of many of these topics may be
found in our coverage of various applications in Chapters 6 through 9 and in
the other references listed at the end of the chapter, primarily [1], [5], [4],
and [19].
More important, we revisit generating functions in Chapter 5, where we
characterize generating functions as our central object of study in the analysis
of algorithms.
ȠȘ

Ƞș
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ș
3.1 Ordinary Generating Functions. As we have seen, it is often our
goal in the analysis of algorithms to derive speciŀc expressions for the values
of terms in a sequence of quantities a0, a1, a2, . . . that measure some perfor-
mance parameter. In this chapter we see the beneŀts of working with a single
mathematical object that represents the whole sequence.
Deŀnition Given a sequence a0, a1, a2, . . . , ak, . . ., the function
A(z) =
∑
k≥0
akzk
is called the ordinary generating function (OGF ) of the sequence. We use the
notation [zk]A(z) to refer to the coeﬃcient ak.
Some elementary ordinary generating functions and their correspond-
ing sequences are given in Table 3.1. We discuss later how to derive these
functions and various ways to manipulate them. Ļe OGFs in Table 3.1
are fundamental and arise frequently in the analysis of algorithms. Each se-
quence can be described in many ways (with simple recurrence relations, for
example), but we will see that there are signiŀcant advantages to representing
them directly with generating functions.
Ļe sum in the deŀnition may or may not converge—for the moment
we ignore questions of convergence, for two reasons. First, the manipulations
that we perform on generating functions are typically well-deŀned formal
manipulations on power series, even in the absence of convergence. Second,
the sequences that arise in our analyses are normally such that convergence is
assured, at least for some (small enough) z. In a great many applications in
the analysis of algorithms, we are able to exploit formal relationships between
power series and the algorithms under scrutiny to derive explicit formulae for
generating functions in the ŀrst part of a typical analysis; and we are able to
learn analytic properties of generating functions in detail (convergence plays
an important role in this) to derive explicit formulae describing fundamental
properties of algorithms in the second part of a typical analysis. We develop
this theme in detail in Chapter 5.
Given generating functions A(z) = ∑
k≥0 akzk and B(z) = ∑
k≥0 bkzk
that represent the sequences {a0, a1, . . . , ak, . . .} and {b0, b1, . . . , bk, . . .},
respectively, we can perform a number of simple transformations to get gen-
erating functions for other sequences. Several such operations are shown in
Table 3.2. Examples of the application of these operations may be found in
the relationships among the entries in Table 3.1.

§Ț.Ș
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȠȚ
1, 1, 1, 1, . . . , 1, . . .
1
1 −z =
∑
N≥0
zN
0, 1, 2, 3, 4, . . . , N, . . .
z
(1 −z)2 =
∑
N≥1
NzN
0, 0, 1, 3, 6, 10, . . . ,
(N
2
)
, . . .
z2
(1 −z)3 =
∑
N≥2
(N
2
)
zN
0, . . . , 0, 1, M + 1, . . . ,
(N
M
)
, . . .
zM
(1 −z)M+1 =
∑
N≥M
(N
M
)
zN
1, M,
(M
2
)
. . . ,
(M
N
)
, . . . , M, 1
(1 + z)M =
∑
N≥0
(M
N
)
zN
1, M + 1,
(M + 2
2
)
,
(M + 3
3
)
, . . .
1
(1 −z)M+1 =
∑
N≥0
(N + M
N
)
zN
1, 0, 1, 0, . . . , 1, 0, . . .
1
1 −z2 =
∑
N≥0
z2N
1, c, c2, c3, . . . , cN, . . .
1
1 −cz =
∑
N≥0
cNzN
1, 1, 1
2!, 1
3!, 1
4!, . . . , 1
N!, . . .
ez =
∑
N≥0
zN
N!
0, 1, 1
2, 1
3, 1
4, . . . , 1
N , . . .
ln
1
1 −z =
∑
N≥1
zN
N
0, 1, 1 + 1
2, 1 + 1
2 + 1
3, . . . , HN, . . .
1
1 −z ln
1
1 −z =
∑
N≥1
HNzN
0, 0, 1, 3
(1
2 + 1
3
)
, 4
(1
2 + 1
3 + 1
4
)
, . . .
z
(1 −z)2 ln
1
1 −z =
∑
N≥0
N(HN −1)zN
Table 3.1
Elementary ordinary generating functions

Ƞț
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ș
A(z) =
∑
n≥0
anzn
a0, a1, a2, . . . , an, . . .
B(z) =
∑
n≥0
bnzn
b0, b1, b2, . . . , bn, . . .
right shift
zA(z) =
∑
n≥1
an−1zn
0, a0, a1, a2, . . . , an−1, . . .
left shift
A(z) −a0
z
=
∑
n≥0
an+1zn
a1, a2, a3, . . . , an+1, . . .
index multiply (diﬀerentiation)
A′(z) =
∑
n≥0
(n + 1)an+1zn
a1, 2a2, . . . , (n + 1)an+1, . . .
index divide (integration)
∫z
0
A(t)dt =
∑
n≥1
an−1
n
zn
0, a0, a1
2 , a2
3 . . . , an−1
n
, . . .
scaling
A(λz) =
∑
n≥0
λnanzn
a0, λa1, λ2a2, . . . , λnan, . . .
addition
A(z) + B(z) =
∑
n≥0
(an + bn)zn
a0 + b0, . . . , an + bn, . . .
diﬀerence
(1 −z)A(z) =a0 +
∑
n≥1
(an −an−1)zn
a0, a1 −a0, . . . , an −an−1, . . .
convolution
A(z)B(z) =
∑
n≥0
( ∑
0≤k≤n
akbn−k
)
zn
a0b0, a1b0 + a0b1, . . . ,
∑
0≤k≤n
akbn−k,
partial sum
A(z)
1 −z =
∑
n≥0
( ∑
0≤k≤n
ak
)
zn
a1, a1 + a2, . . . ,
∑
0≤k≤n
ak, . . .
Table 3.2
Operations on ordinary generating functions

§Ț.Ș
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȠȜ
Ļeorem 3.1 (OGF operations).
If two sequences a0, a1, . . . , ak, . . . and
b0, b1, . . . , bk, . . . are represented by the OGFs A(z) = ∑
k≥0 akzk and B(z) =
∑
k≥0 bkzk, respectively, then the operations given in Table 3.2 produce OGFs
that represent the indicated sequences. In particular:
A(z) + B(z) is the OGF for a0 + b0, a1 + b1, a2 + b2, . . .
zA(z)
is the OGF for 0, a0, a1, a2, . . .
A′(z)
is the OGF for a1, 2a2, 3a3, . . .
A(z)B(z)
is the OGF for a0b0, a0b1 + a1b0, a0b2 + a1b1 + a2b0, . . .
Proof. Most of these are elementary and can be veriŀed by inspection. Ļe
convolution operation (and the partial sum special case) are easily proved by
manipulating the order of summation:
A(z)B(z) =
∑
i≥0
aizi ∑
j≥0
bjzj
=
∑
i,j≥0
aibjzi+j
=
∑
n≥0
(
∑
0≤k≤n
akbn−k
)
zn.
Taking B(z) = 1/(1 −z) in this formula gives the partial sum operation.
Ļe convolution operation plays a special role in generating function manip-
ulations, as we shall see.
Corollary Ļe OGF for the harmonic numbers is
∑
N≥1
HNzN =
1
1 −z ln
1
1 −z .
Proof. Start with 1/(1−z) (the OGF for 1, 1, . . . , 1, . . .), integrate (to get the
OGF for 0, 1, 1/2, 1/3, . . . , 1/k, . . . .), and multiply by 1/(1 −z). Similar
examples may be found in the relationships among the entries in Table 3.1.
Readers unfamiliar with generating functions are encouraged to work
through the following exercises to gain a basic facility for applying these trans-
formations.

Ƞȝ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ș
Exercise 3.1 Find the OGFs for each of the following sequences:
{2k+1}k≥0,
{k2k+1}k≥0,
{kHk}k≥1,
{k3}k≥2.
Exercise 3.2 Find [zN] for each of the following OGFs:
1
(1 −3z)4 ,
(1 −z)2ln
1
1 −z ,
1
(1 −2z2)2 .
Exercise 3.3 Diﬀerentiate the OGF for harmonic numbers to verify the last line of
Table 3.1.
Exercise 3.4 Prove that
∑
1≤k≤N
Hk = (N + 1)(HN+1 −1).
Exercise 3.5 By factoring
zM
(1 −z)M+1 ln
1
1 −z
in two diﬀerent ways (and performing the associated convolutions), prove a general
identity satisŀed by the harmonic numbers and binomial coeﬃcients.
Exercise 3.6 Find the OGF for
{ ∑
0<k<n
1
k(n −k)
}
n>1.
Generalize your answer.
Exercise 3.7 Find the OGF for {Hk/k}k≥1.
Exercise 3.8 Find [zN] for each of the following OGFs:
1
1 −z
(
ln
1
1 −z
)2
and
(
ln
1
1 −z
)3
.
Use the notation
H(2)
N ≡1 + 1
22 + 1
32 + . . . + 1
N 2
for the “generalized harmonic numbers” that arise in these expansions.

§Ț.ș
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȠȞ
Such elementary manipulations suﬃce to derive many of the sequences
that we encounter in the analysis of algorithms, though more advanced tools
are needed for many algorithms. From this point forward, it should become
very clear that the analysis of algorithms revolves around the twin questions
of determining an explicit formula for the generating function of a sequence
and, conversely, determining an exact formula for members of the sequence
from the generating-function representation. We will see many examples of
this later in this chapter and in Chapters 6 through 9.
Formally, we could use any kernel family of functions wk(z) to deŀne a
“generating function”
A(z) =
∑
k≥0
akwk(z)
that encapsulates a sequence a0, a1, . . . , ak, . . . . Ļough we focus almost ex-
clusively on the kernels zk and zk/k! (see the next section) in this book, other
types do occasionally arise in the analysis of algorithms; we mention them
brieły at the end of this chapter.
3.2 Exponential Generating Functions. Some sequences are more con-
veniently handled by a generating function that involves a normalizing factor:
Deŀnition Given a sequence a0, a1, a2, . . . , ak, . . . , the function
A(z) =
∑
k≥0
ak
zk
k!
is called the exponential generating function (EGF ) of the sequence. We use
the notation k![zk]A(z) to refer to the coeﬃcient ak.
Ļe EGF for {ak} is nothing more than the OGF for {ak/k!}, but
it arises in combinatorics and the analysis of algorithms for a speciŀc and
simple reason. Suppose that the coeﬃcients ak represent a count associated
with a structure of k items. Suppose further that the k items are “labelled” so
that each has a distinct identity. In some cases, the labelling is relevant (and
EGFs are appropriate); in other cases it is not (and OGFs are appropriate).
Ļe factor of k! accounts for all the arrangements of the labelled items that
become indistinguishable if they are unlabelled. We will consider this situa-
tion in more detail in Chapter 5 when we look at the “symbolic method” for

Ƞȟ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ș
associating generating functions and combinatorial objects; for the moment,
we oﬀer this explanation simply as justiŀcation for considering properties of
EGFs in detail. Ļey are well studied because labelled objects naturally arise
in many applications.
Table 3.3 gives a number of elementary exponential generating functions
that we will be encountering later in the book, and Table 3.4 gives some of
the basic manipulations on EGFs. Note that the shift left/right operations
for EGFs are the same as the index multiply/divide operations for OGFs (see
1, 1, 1, 1, . . . , 1, . . .
ez =
∑
N≥0
zN
N!
0, 1, 2, 3, 4, . . . , N, . . .
zez =
∑
N≥1
zN
(N −1)!
0, 0, 1, 3, 6, 10, . . . ,
(N
2
)
, . . .
1
2z2ez = 1
2
∑
N≥2
zN
(N −2)!
0, . . . , 0, 1, M + 1, . . . ,
(N
M
)
, . . .
1
M!zMez =
1
M!
∑
N≥M
zN
(N −M)!
1, 0, 1, 0, . . . , 1, 0, . . .
1
2(ez + e−z) =
∑
N≥0
1 + (−1)N
2
zN
N!
1, c, c2, c3, . . . , cN, . . .
ecz =
∑
N≥0
cNzN
N!
1, 1
2, 1
3, . . . ,
1
N + 1, . . .
ez −1
z
=
∑
N≥0
zN
(N + 1)!
1, 1, 2, 6, 24, . . . , N!, . . .
1
1 −z =
∑
N≥0
N!zN
N!
Table 3.3
Elementary exponential generating functions

§Ț.ș
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȠȠ
Table 3.2), and vice versa. As with OGFs, application of the basic operations
from Table 3.4 on the basic functions in Table 3.3 yields a large fraction of the
EGFs that arise in practice, and the reader is encouraged to work the exercises
below to become familiar with these functions. Also as with OGFs, we can
easily establish the validity of the basic operations.
Ļeorem 3.2 (EGF operations).
If two sequences a0, a1, . . . , ak, . . . and
b0, b1, . . . , bk, . . . are represented by the EGFs A(z) = ∑
k≥0 akzk/k! and
B(z) = ∑
k≥0 bkzk/k!, respectively, then the operations given in Table 3.4
produce EGFs that represent the indicated sequences. In particular,
A(z) + B(z) is the EGF for a0 + b0, a1 + b1, a2 + b2 . . .
A′(z)
is the EGF for a1, a2, a3 . . .
zA(z)
is the EGF for 0, a0, 2a1, 3a2, . . .
A(z)B(z)
is the EGF for a0b0, a0b1 + a1b0, a0b2 + 2a1b1 + a2b0, . . .
Proof. As for Ļeorem 3.1, these are elementary and can be veriŀed by inspec-
tion with the possible exception of binomial convolution, which is easily ver-
iŀed with OGF convolution:
A(z)B(z) =
∑
n≥0
∑
0≤k≤n
ak
k!
bn−k
(n −k)!zn
=
∑
n≥0
∑
0≤k≤n
(
n
k
)
akbn−k
zn
n! .
Exercise 3.9 Find the EGFs for each of the following sequences:
{2k+1}k≥0,
{k2k+1}k≥0,
{k3}k≥2.
Exercise 3.10 Find the EGFs for 1, 3, 5, 7, . . . and 0, 2, 4, 6, . . ..
Exercise 3.11 Find N![zN]A(z) for each of the following EGFs:
A(z) =
1
1 −z ln
1
1 −z ,
A(z) =
(
ln
1
1 −z
)2
,
A(z) = ez+z2.
Exercise 3.12 Show that
N![zN]ez
∫z
0
1 −e−t
t
dt = HN.
(Hint : Form a diﬀerential equation for the EGF H(z) = ∑
N≥0 HNzN/N!.)

Șȗȗ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ș
A(z) =
∑
n≥0
an
zn
n!
a0, a1, a2, . . . , an, . . .
B(z) =
∑
n≥0
bn
zn
n!
b0, b1, b2, . . . , bn, . . .
right shift (integration)
∫z
0
A(t)dt =
∑
n≥1
an−1
zn
n!
0, a0, a1, . . . , an−1, . . .
left shift (diﬀerentiation)
A′(z) =
∑
n≥0
an+1
zn
n!
a1, a2, a3, . . . , an+1, . . .
index multiply
zA(z) =
∑
n≥0
nan−1
zn
n!
0, a0, 2a1, 3a, . . . , nan−1, . . .
index divide
(A(z) −A(0))/z =
∑
n≥1
an+1
n + 1
zn
n!
a1, a2
2 , a3
3 . . . , an+1
n + 1, . . .
addition
A(z) + B(z) =
∑
n≥0
(an + bn)zn
n!
a0 + b0, . . . , an + bn, . . .
diﬀerence
A′(z) −A(z) =
∑
n≥0
(an+1 −an)zn
n!
a1 −a0, . . . , an+1 −an, . . .
binomial convolution
A(z)B(z) =
∑
n≥0
( ∑
0≤k≤n
(n
k
)
akbn−k
)zn
n!
a0b0, a1b0 + a0b1, . . . ,
∑
0≤k≤n
(n
k
)
akbn−k, . . .
binomial sum
ezA(z) =
∑
n≥0
( ∑
0≤k≤n
(n
k
)
ak
)zn
n!
a0, a0 + a1, . . . ,
∑
0≤k≤n
(n
k
)
ak, . . .
Table 3.4
Operations on exponential generating functions

§Ț.Ț
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȗȘ
It is not always clear whether an OGF or an EGF will lead to the most
convenient solution to a problem: sometimes one will lead to a trivial solution
and the other to diﬃcult technical problems; other times either will work well.
For many of the combinatorial and algorithmic problems that we encounter,
the choice of whether to use OGFs or EGFs comes naturally from the struc-
ture of the problem. Moreover, interesting questions arise from an analytic
standpoint: for example, can we automatically convert from the OGF for a
sequence to the EGF for the same sequence, and vice versa? (Yes, by the
Laplace transform; see Exercise 3.14.) In this book, we will consider many
examples involving applications of both OGFs and EGFs.
Exercise 3.13 Given the EGF A(z) for a sequence {ak}, ŀnd the EGF for the se-
quence
{ ∑
0≤k≤N
N!ak
k!
}
.
Exercise 3.14 Given the EGF A(z) for a sequence {ak}, show that the OGF for the
sequence is given by
∫∞
0
A(zt)e−tdt,
if the integral exists. Check this for sequences that appear in Tables 3.1 and 3.3.
3.3 Generating Function Solution of Recurrences. Next, we examine
the role that generating functions can play in the solution of recurrence re-
lations, the second step in a classical approach to the analysis of algorithms:
after a recurrence relationship describing some fundamental property of an al-
gorithm is derived, generating functions can be used to solve the recurrence.
Some readers may be familiar with this approach because of its widespread
use and its basic and fundamental nature. We will see in Chapter 5 that it
is often possible to avoid the recurrence and work with generating functions
directly.
Generating functions provide a mechanical method for solving many
recurrence relations. Given a recurrence describing some sequence {an}n≥0,
we can often develop a solution by carrying out the following steps:
• Multiply both sides of the recurrence by zn and sum on n.
• Evaluate the sums to derive an equation satisŀed by the OGF.
• Solve the equation to derive an explicit formula for the OGF.

Șȗș
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ț
• Express the OGF as a power series to get expressions for the coeﬃcients
(members of the original sequence).
Ļe same method applies for EGFs, where we multiply by zn/n! and sum on
n in the ŀrst step. Whether OGFs or EGFs are more convenient depends on
the recurrence.
Ļe most straightforward example of this method is its use in solving
linear recurrences with constant coeﬃcients (see Chapter 2).
Trivial linear recurrence. To solve the recurrence
an = an−1 + 1
for n ≥1 with a0 = 0
we ŀrst multiply by zn and sum to get
∑
n≥1
anzn =
∑
n≥1
an−1zn +
z
1 −z .
In terms of the generating function A(z) = ∑
n≥0 anzn, this equation says
A(z) = zA(z) +
z
1 −z
or A(z) = z/(1 −z)2, and an = n, as expected.
Simple exponential recurrence. To solve the recurrence
an = 2an−1 + 1
for n ≥1 with a0 = 1
we proceed as above to ŀnd that the generating function A(z) = ∑
n≥0 anzn
satisŀes
A(z) −1 = 2zA(z) +
z
1 −z ,
which simpliŀes to
A(z) =
1
(1 −z)(1 −2z).
From Table 3.1 we know that 1/(1 −2z) is the generating function for the
sequence {2n}, and from Table 3.2 we know that multiplying by 1/(1 −z)
corresponds to taking partial sums:
an =
∑
0≤k≤n
2k = 2n+1 −1.

§Ț.Ț
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȗȚ
Partial fractions. An alternative method for ŀnding the coeﬃcients in the
preceding problem that is instructive preparation for more diﬃcult problems
is to use the partial fractions expansion for A(z). By factoring the denomina-
tor, the generating function can be expressed as the sum of two fractions
1
(1 −z)(1 −2z) =
c0
1 −2z +
c1
1 −z
where c0 and c1 are constants to be determined. Cross-multiplying, we see
that these constants must satisfy the simultaneous equations
c0 + c1 = 1
−c0 −2c1 = 0
so c0 = 2 and c1 = −1. Ļerefore,
[zn]
1
(1 −z)(1 −2z) = [zn]
(
2
1 −2z −
1
1 −z
)
= 2n+1 −1.
Ļis technique can be applied whenever we have a polynomial in the denomi-
nator, and leads to a general method for solving high-order linear recurrences
that we discuss later in this section.
Fibonacci numbers.
Ļe generating function F(z) = ∑
k≥0 Fkzk for the
Fibonacci sequence
Fn = Fn−1 + Fn−2
for n > 1 with F0 = 0 and F1 = 1
satisŀes
F(z) = zF(z) + z2F(z) + z.
Ļis implies that
F(z) =
z
1 −z −z2 =
1
√
5
(
1
1 −ϕz −
1
1 −bϕz
)
by partial fractions, since 1 −z −z2 factors as (1 −zϕ)(1 −z bϕ) where
ϕ = 1 +
√
5
2
and
bϕ = 1 −
√
5
2
are the reciprocals of the roots of 1 −z −z2. Now the series expansion is
straightforward from Table 3.4:
Fn =
1
√
5(ϕn −bϕn).
Of course, this strongly relates to the derivation given in Chapter 2. We
examine this relationship in general terms next.

Șȗț
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ț
Exercise 3.15 Find the EGF for the Fibonacci numbers.
High-order linear recurrences.
Generating functions make explicit the
“factoring” process described in Chapter 2 to solve high-order recurrences
with constant coeﬃcients. Factoring the recurrence corresponds to factor-
ing the polynomial that arises in the denominator of the generating func-
tion, which leads to a partial fraction expansion and an explicit solution. For
example, the recurrence
an = 5an−1 −6an−2
for n > 1 with a0 = 0 and a1 = 1
implies that the generating function a(z) = ∑
n≥0 anzn is
a(z) =
z
1 −5z + 6z2 =
z
(1 −3z)(1 −2z) =
1
1 −3z −
1
1 −2z
so that we must have an = 3n −2n.
Exercise 3.16 Use generating functions to solve the following recurrences:
an = −an−1 + 6an−2
for n > 1 with a0 = 0 and a1 = 1;
an = 11an−2 −6an−3
for n > 2 with a0 = 0 and a1 = a2 = 1;
an = 3an−1 −4an−2
for n > 1 with a0 = 0 and a1 = 1;
an = an−1 −an−2
for n > 1 with a0 = 0 and a1 = 1.
In general, the explicit expression for the generating function is the
ratio of two polynomials; then partial fraction expansion involving roots of
the denominator polynomial leads to an expression in terms of powers of
roots. A precise derivation along these lines can be used to obtain a proof
of Ļeorem 2.2.
Ļeorem 3.3 (OGFs for linear recurrences).
If an satisŀes the recurrence
an = x1an−1 + x2an−2 + . . . + xtan−t
for n ≥t, then the generating function a(z) = ∑
n≥0 anzn is a rational
function a(z) = f(z)/g(z), where the denominator polynomial is g(z) =
1 −x1z −x2z2 −. . . −xtzt and the numerator polynomial is determined by
the initial values a0, a1, . . . , at−1.

§Ț.Ț
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȗȜ
Proof. Ļe proof follows the general paradigm for solving recurrences de-
scribed at the beginning of this section. Multiplying both sides of the recur-
rence by zn and summing for n ≥t yields
∑
n≥t
anzn = x1
∑
n≥t
an−1zn + · · · + xt
∑
n≥t
an−tzn.
Ļe left-hand side evaluates to a(z) minus the generating polynomial of the
initial values; the ŀrst sum on the right evaluates to za(z) minus a polynomial,
and so forth. Ļus a(z) satisŀes
a(z) −u0(z) = (x1za(z) −u1(z)) + . . . + (xtzta(z) −ut(z)),
where the polynomials u0(z), u1(z), . . . , ut(z) are of degree at most t −1
with coeﬃcients depending only on the initial values a0, a1, . . . , at−1. Ļis
functional equation is linear.
Solving the equation for a(z) gives the explicit form a(z) = f(z)/g(z),
where g(z) has the form announced in the statement and
f(z) ≡u0(z) −u1(z) −. . . −ut(z)
depends solely on the initial values of the recurrence and has degree less than
t.
Ļe general form immediately implies an alternate formulation for the
dependence of f(z) on the initial conditions, as follows. We have f(z) =
a(z)g(z) and we know that the degree of f is less than t. Ļerefore, we must
have
f(z) = g(z)
∑
0≤n<t
anzn
(mod zt).
Ļis gives a shortcut to computing the coeﬃcients of f(z), which provides a
quick exact solution to many recurrences.
Simple example. To solve the recurrence
an = 2an−1 + an−2 −2an−3
for n > 2 with a0 = 0 and a1 = a2 = 1
we ŀrst compute
g(z) = 1 −2z −z2 + 2z3 = (1 −z)(1 + z)(1 −2z)

Șȗȝ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ț
then, using the initial conditions, we write
f(z) = (z + z2)(1 −2z −z2 + 2z3)
(mod z3)
= z −z2 = z(1 −z).
Ļis gives
a(z) = f(z)
g(z) =
z
(1 + z)(1 −2z) = 1
3
(
1
1 −2z −
1
1 + z
)
,
so that an = 1
3(2n −(−1)n).
Cancellation. In the above recurrence, the 1−z factor canceled, so there was
no constant term in the solution. Consider the same recurrence with diﬀerent
initial conditions:
an = 2an−1 + an−2 −2an−3
for n > 2 with a0 = a1 = a2 = 1.
Ļe function g(z) is the same as above, but now we have
f(z) = (1 + z + z2)(1 −2z −z2 + 2z3)
(mod z3)
= 1 −z −2z2 = (1 −2z)(1 + z).
In this case, we have cancellation to a trivial solution: a(z) = f(z)/g(z) =
1/(1 −z) and an = 1 for all n ≥0. Ļe initial conditions can have drastic
eﬀects on the eventual growth rate of the solution by leading to cancellation
of factors in this way.
We adopt the convention of factoring g(z) in the form
g(z) = (1 −β1z) · (1 −β2z) · · · (1 −βnz)
since it is slightly more natural in this context. Note that if a polynomial g(z)
satisŀes g(0) = 1 (which is usual when g(z) is derived from a recurrence as
above), then the product of its roots is 1, and the β1, β2, . . . , βn in the equa-
tion above are simply the inverses of the roots. If q(z) is the “characteristic
polynomial” of Ļeorem 2.2, we have g(z) = ztq(1/z), so the β’s are the
roots of the characteristic polynomial.

§Ț.Ț
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȗȞ
Complex roots. All the manipulations that we have been doing are valid for
complex roots, as illustrated by the recurrence
an = 2an−1−an−2+2an−3
for n > 2 with a0 = 1, a1 = 0, and a2 = −1.
Ļis gives
g(z) = 1 −2z + z2 −2z3 = (1 + z2)(1 −2z)
and
f(z) = (1 −z4)(1 −2z)
(mod z4) = 1 −2z,
so
a(z) = f(z)
g(z) =
1
1 + z2 = 1
2
(
1
1 −iz −
1
1 + iz
)
,
and an = 1
2(in + (−i)n). From this, it is easy to see that an is 0 for n odd,
1 when n is a multiple of 4, and −1 when n is even but not a multiple of 4
(this also follows directly from the form a(z) = 1/(1 + z2)). For the initial
conditions a0 = 1, a1 = 2, and a2 = 3, we get f(z) = 1, so the solution
grows like 2n, but with periodic varying terms caused by the complex roots.
Multiple roots. When multiple roots are involved, we ŀnish the derivation
with the expansions given on the second and third lines of Table 3.1. For
example, the recurrence
an = 5an−1−8an−2+4an−3
for n > 2 with a0 = 0, a1 = 1, and a2 = 4
gives
g(z) = 1 −5z + 8z2 −4z3 = (1 −z)(1 −2z)2
and
f(z) = (z + 4z2)(1 −5z + 8z2 −4z3)
(mod z3) = z(1 −z),
so a(z) = z/(1 −2z)2 and an = n2n−1 from Table 3.1.
Ļese examples illustrate a straightforward general method for develop-
ing exact solutions to linear recurrences:
• Derive g(z) from the recurrence.
• Compute f(z) from g(z) and the initial conditions.

Șȗȟ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ț
• Eliminate common factors in f(z)/g(z).
• Use partial fractions to represent f(z)/g(z) as a linear combination of
terms of the form (1 −βz)−j.
• Expand each term in the partial fractions expansion, using
[zn](1 −βz)−j =
(
n + j −1
j −1
)
βn.
In essence, this process amounts to a constructive proof of Ļeorem 2.2.
Exercise 3.17 Solve the recurrence
an = 5an−1 −8an−2 + 4an−3
for n > 2 with a0 = 1, a1 = 2, and a2 = 4.
Exercise 3.18 Solve the recurrence
an = 2an−2 −an−4
for n > 4 with a0 = a1 = 0 and a2 = a3 = 1.
Exercise 3.19 Solve the recurrence
an = 6an−1 −12an−2 + 18an−3 −27an−4
for n > 4
with a0 = 0 and a1 = a2 = a3 = 1.
Exercise 3.20 Solve the recurrence
an = 3an−1 −3an−2 + an−3
for n > 2 with a0 = a1 = 0 and a2 = 1.
Solve the same recurrence with the initial condition on a1 changed to a1 = 1.
Exercise 3.21 Solve the recurrence
an = −
∑
1≤k≤t
(t
k
)
(−1)kan−k
for n ≥t
with a0 = · · · = at−2 = 0 and at−1 = 1.

§Ț.Ț
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȗȠ
Solving the quicksort recurrence with an OGF. When coeﬃcients in a
recurrence are polynomials in the index n, then the implied relationship con-
straining the generating function is a diﬀerential equation. As an example,
let us revisit the basic recurrence from Chapter 1 describing the number of
comparisons used by quicksort:
NCN = N(N + 1) + 2
∑
1≤k≤N
Ck−1
for N ≥1 with C0 = 0.
(1)
We deŀne the generating function
C(z) =
∑
N≥0
CNzN
(2)
and proceed as described earlier to get a functional equation that C(z) must
satisfy. First, multiply both sides of (1) by zN and sum on N to get
∑
N≥1
NCNzN =
∑
N≥1
N(N + 1)zN + 2
∑
N≥1
∑
1≤k≤N
Ck−1zN.
Now, we can evaluate each of these terms in a straightforward manner. Ļe
left-hand side is zC′(z) (diﬀerentiate both sides of (2) and multiply by z) and
the ŀrst term on the right is 2z/(1−z)3 (see Table 3.1). Ļe remaining term,
the double sum, is a partial sum convolution (see Table 3.2) that evaluates
to zC(z)/(1 −z). Ļerefore, our recurrence relationship corresponds to a
diﬀerential equation on the generating function
C′(z) =
2
(1 −z)3 + 2 C(z)
1 −z .
(3)
We obtain the solution to this diﬀerential equation by solving the correspond-
ing homogeneous equation ρ′(z) = 2ρ(z)/(1 −z) to get an “integration
factor” ρ(z) = 1/(1 −z)2. Ļis gives
((1 −z)2C(z))′ = (1 −z)2C′(z) −2(1 −z)C(z)
= (1 −z)2(
C′(z) −2 C(z)
1 −z
)
=
2
1 −z .

ȘȘȗ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ț
Integrating, we get the result
C(z) =
2
(1 −z)2 ln
1
1 −z .
(4)
Ļeorem 3.4 (Quicksort OGF). Ļe average number of comparisons used
by quicksort for a random permutation is given by
CN = [zN]
2
(1 −z)2 ln
1
1 −z = 2(N + 1)(HN+1 −1).
Proof. Ļe preceding discussion yields the explicit expression for the gener-
ating function, which completes the third step of the general procedure for
solving recurrences with OGFs given at the beginning of this section. To
extract coeﬃcients, diﬀerentiate the generating function for the harmonic
numbers.
Ļe general approach for solving recurrences with OGFs that we have
been discussing, while powerful, certainly cannot be relied upon to give solu-
tions for all recurrence relations: various examples from the end of Chapter
2 can serve testimony to that. For some problems, it may not be possible to
evaluate the sums to a simple form; for others, an explicit formula for the
generating function can be diﬃcult to derive; and for others, the expansion
back to power series can present the main obstacle. In many cases, algebraic
manipulations on the recurrence can simplify the process. In short, solving
recurrences is not quite as “automatic” a process as we might like.
Exercise 3.22 Use generating functions to solve the recurrence
nan = (n −2)an−1 + 2
for n > 1 with a1 = 1.
Exercise 3.23 [Greene and Knuth [6]] Solve the recurrence
nan = (n + t −1)an−1
for n > 0 with a0 = 1.
Exercise 3.24 Solve the recurrence
an = n + 1 + t
n
∑
1≤k≤n
ak−1
for n ≥1 with a0 = 0
for t = 2 −ϵ and t = 2 + ϵ, where ϵ is a small positive constant.

§Ț.ț
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȘȘ
3.4 ExpandingGeneratingFunctions. Given an explicit functional form
for a generating function, we would like a general mechanism for ŀnding the
associated sequence. Ļis process is called “expanding” the generating func-
tion, as we take it from a compact functional form into an inŀnite series of
terms. As we have seen in the preceding examples, we can handle many func-
tions with algebraic manipulations involving the basic identities and transfor-
mations given in Tables 3.1–3.4. But where do the elementary expansions in
Table 3.1 and Table 3.3 originate?
Ļe Taylor theorem permits us to expand a function f(z) given its deriva-
tives at 0:
f(z) = f(0) + f′(0)z + f′′(0)
2!
z2 + f′′′(0)
3!
z3 + f′′′′(0)
4!
z4 + . . . .
Ļus, by calculating derivatives, we can, in principle, ŀnd the sequence asso-
ciated with any given generating function.
Exponential sequence. Since all the derivatives of ez are ez, the easiest ap-
plication of Taylor’s theorem is the fundamental expansion
ez = 1 + z + z2
2! + z3
3! + z4
4! + . . . .
Geometric sequence.
From Table 3.1, we know that the generating func-
tion for the sequence {1, c, c2, c3, . . .} is (1 −cz)−1. Ļe kth derivative of
(1 −cz)−1 is k!ck(1−cz)−k−1, which is simply k!ck when evaluated at z = 0,
so Taylor’s theorem veriŀes that the expansion of this function is given by
1
1 −cz =
∑
k≥0
ckzk,
as stated in Table 3.1.
Binomial theorem. Ļe kth derivative of the function (1 + z)x is
x(x −1)(x −2) · · · (x −k + 1)(1 + z)x−k,
so by Taylor’s theorem, we get a generalized version of the binomial theorem
known as Newton's formula:
(1 + z)x =
∑
k≥0
(
x
k
)
zk,

ȘȘș
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ț
where the binomial coeﬃcients are deŀned by
(
x
k
)
≡x(x −1)(x −2) · · · (x −k + 1)/k!.
A particularly interesting case of this is
1
√1 −4z =
∑
k≥0
(
2k
k
)
zk,
which follows from the identity
(
−1/2
k
)
= −1
2(−1
2 −1)(−1
2 −2) · · · (−1
2 −k + 1)
k!
= (−1)k
2k
1 · 3 · 5 · · · (2k −1)
k!
2 · 4 · 6 · · · 2k
2kk!
= (−1)k
4k
(
2k
k
)
.
An expansion closely related to this plays a central role in the analysis of algo-
rithms, as we will see in several applications later in the book.
Exercise 3.25 Use Taylor’s theorem to ŀnd the expansions of the following functions:
sin(z),
2z,
zez.
Exercise 3.26 Use Taylor’s theorem to verify that the coeﬃcients of the series expan-
sion of (1 −az −bz2)−1 satisfy a second-order linear recurrence with constant coef-
ŀcients.
Exercise 3.27 Use Taylor’s theorem to verify directly that
H(z) =
1
1 −z ln
1
1 −z
is the generating function for the harmonic numbers.
Exercise 3.28 Find an expression for
[zn]
1
√1 −z ln
1
1 −z .
(Hint: Expand (1 −z)−α and diﬀerentiate with respect to α.)

§Ț.ț
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȘȚ
Exercise 3.29 Find an expression for
[zn]
(
1
1 −z
)t
ln
1
1 −z
for integer t > 0.
In principle, we can always compute generating function coeﬃcients by
direct application of Taylor’s theorem, but the process can become too com-
plex to be helpful. Most often, we expand a generating function by decom-
posing it into simpler parts for which expansions are known, as we have done
for several examples earlier, including the use of convolutions to expand the
generating functions for binomial coeﬃcients and the harmonic numbers and
the use of partial fraction decomposition to expand the generating function
for the Fibonacci numbers. Indeed, this is the method of choice, and we
will be using it extensively throughout this book. For speciŀc classes of prob-
lems, other tools are available to aid in this process—for example the Lagrange
inversion theorem, which we will examine in §6.12.
Moreover, there exists something even more useful than a “general tool”
for expanding generating functions to derive succinct representations for co-
eﬃcients: a tool for directly deriving asymptotic estimates of coeﬃcients,
which allows us to ignore irrelevant detail, even for problems that may not
seem amenable to expansion by decomposition. Ļough the general method
involves complex analysis and is beyond the scope of this book, our use of par-
tial fractions expansions for linear recurrences is based on the same intuition.
For example, the partial fraction expansion of the Fibonacci numbers imme-
diately implies that the generating function F(z) does not converge when
z = 1/ϕ or z = 1/bϕ. But it turns out that these “singularities” completely
determine the asymptotic growth of the coeﬃcients FN. In this case, we are
able to verify by direct expansion that the coeﬃcients grow as ϕN (to within
a constant factor). It is possible to state general conditions under which co-
eﬃcients grow in this way and general mechanisms for determining other
growth rates. By analyzing singularities of generating functions, we are very
often able to reach our goal of deriving accurate estimates of the quantities
of interest without having to resort to detailed expansions. Ļis topic is dis-
cussed in §5.5, and in detail in [3].
But there are a large number of sequences for which the generating func-
tions are known and for which simple algebraic manipulations of the generat-
ing function can yield simple expressions for the quantities of interest. Basic

ȘȘț
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ȝ
generating functions for classic combinatorial sequences are discussed in fur-
ther detail in this chapter, and Chapters 6 through 9 are largely devoted to
building up a repertoire of familiar functions that arise in the analysis of com-
binatorial algorithms. We will proceed to discuss and consider detailed ma-
nipulations of these functions, secure in the knowledge that we have powerful
tools available for getting the coeﬃcients back, when necessary.
3.5 Transformations with Generating Functions. Generating func-
tions succinctly represent inŀnite sequences. Often, their importance lies
in the fact that simple manipulations on equations involving the generat-
ing function can lead to surprising relationships involving the underlying se-
quences that otherwise might be diﬃcult to derive. Several basic examples of
this follow.
Vandermonde’s convolution. Ļis identity relating binomial coeﬃcients (see
Chapter 2),
∑
k
(
r
k
)(
s
N −k
)
=
(
r + s
N
)
,
is trivial to derive, as it is the convolution of coeﬃcients that express the func-
tional relation
(1 + z)r(1 + z)s = (1 + z)r+s.
Similar identities can be derived in abundance from more complicated con-
volutions.
Quicksort recurrence. Multiplying OGFs by (1 −z) corresponds to diﬀer-
encing the coeﬃcients, as stated in Table 3.2, and as we saw in Chapter 1
(without remarking on it) in the quicksort recurrence. Other transformations
were involved to get the solution. Our point here is that these various ma-
nipulations are more easily done with the generating function representation
than with the sequence representation. We will examine this in more detail
later in this chapter.
Fibonacci numbers. Ļe generating function for Fibonacci numbers can be
written
F(z) =
z
1 −y
with y = z + z2.

§Ț.Ȝ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȘȜ
Expanding this in terms of y, we have
F(z) = z
∑
N≥0
yN = z
∑
N≥0
(z + z2)N
=
∑
N≥0
∑
k
(
N
k
)
zN+k+1.
But FN is simply the coeﬃcient of zN in this, so we must have
FN =
∑
k
(
N −k −1
k
)
,
a well-known relationship between Fibonacci numbers and diagonals in Pas-
cal’s triangle.
Binomial transform.
If an = (1 −b)n for all n, then, obviously, bn =
(1 −a)n. Surprisingly, this generalizes to arbitrary sequences: given two
sequences {an} and {bn} related according to the equation
an =
∑
k
(
n
k
)
(−1)kbk,
we know that the associated generating functions satisfy B(−z) = ezA(z)
(see Table 3.4). But then, of course, A(−z) = ezB(z), which implies that
bn =
∑
k
(
n
k
)
(−1)kak.
We will see more examples of such manipulations in ensuing chapters.
Exercise 3.30 Show that
∑
k
(2k
k
)(2N −2k
N −k
)
= 4N.
Exercise 3.31 What recurrence on {CN} corresponds to multiplying both sides of
the diﬀerential equation (3) for the quicksort generating function by (1 −z)2?

ȘȘȝ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ȝ
Exercise 3.32 Suppose that an OGF satisŀes the diﬀerential equation
A′(z) = −A(z) + A(z)
1 −z .
What recurrence does this correspond to? Multiply both sides by 1−z and set coeﬃ-
cients equal to derive a diﬀerent recurrence, then solve that recurrence. Compare this
path to the solution with the method of directly ŀnding the OGF and expanding.
Exercise 3.33 What identity on binomial coeﬃcients is implied by the convolution
(1 + z)r(1 −z)s = (1 −z2)s(1 + z)r−s
where r > s?
Exercise 3.34 Prove that
∑
0≤k≤t
(t −k
r
)(k
s
)
=
(
t + 1
r + s + 1
)
.
Exercise 3.35 Use generating functions to evaluate ∑
0≤k≤N Fk.
Exercise 3.36 Use generating functions to ŀnd a sum expression for [zn]
z
1 −ez .
Exercise 3.37 Use generating functions to ŀnd a sum expression for [zn]
1
2 −ez .
Exercise 3.38 [Dobinski, cf. Comtet] Prove that
n![zn]eez−1 = e−1 ∑
k≥0
kn
n! .
Exercise 3.39 Prove the binomial transform identity using OGFs. Let A(z) and
B(z) be related by
B(z) =
1
1 −z A
(
z
z −1
)
,
and then use the change of variable z = y/(y −1).
Exercise 3.40 Prove the binomial transform identity directly, without using generat-
ing functions.
Exercise3.41 [Faà di Bruno’s formula, cf. Comtet] Let f(z) = ∑
n fnzn and g(z) =
∑
n gnzn. Express [zn]f(g(z)) using the multinomial theorem.

§Ț.ȝ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȘȞ
3.6 Functional Equations on Generating Functions. In the analysis
of algorithms, recursion in an algorithm (or recurrence relationships in its
analysis) very often leads to functional equations on the corresponding gen-
erating functions. We have seen some cases where we can ŀnd an explicit
solution to the functional equation and then expand to ŀnd the coeﬃcients.
In other cases, we may be able to use the functional equation to determine
the asymptotic behavior without ever ŀnding an explicit form for the gener-
ating function, or to transform the problem to a similar form that can be more
easily solved. We oﬀer a few comments on the diﬀerent types of functional
equations in this section, along with some exercises and examples.
Linear. Ļe generating function for the Fibonacci numbers is the prototyp-
ical example here:
f(z) = zf(z) + z2f(z) + z.
Ļe linear equation leads to an explicit formula for the generating function,
which perhaps can be expanded. But linear here just refers to the function
itself appearing only in linear combinations—the coeﬃcients and consequent
formulae could be arbitrarily complex.
Nonlinear. More generally, it is typical to have a situation where the gener-
ating function can be shown to be equal to an arbitrary function of itself, not
necessarily a linear function. Famous examples of this include the GF for the
Catalan numbers, which is deŀned by the functional equation
f(z) = zf(z)2 + 1
and the GF for trees, which satisŀes the functional equation
f(z) = zef(z).
Ļe former is discussed in some detail in §3.3 and the latter in §6.14. De-
pending on the nature of the nonlinear function, it may be possible to derive
an explicit formula for the generating function algebraically.
Diﬀerential. Ļe equation might involve derivatives of the generating func-
tion. We have already seen an example of this with quicksort,
f′(z) =
2
(1 −z)3 + 2 f(z)
1 −z ,
and will see a more detailed example below. Our ability to ŀnd an explicit
formula for the generating function is, of course, directly related to our ability
to solve the diﬀerential equation.

ȘȘȟ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ȝ
Compositional. In still other cases, the functional equation might involve
linear or nonlinear functions on the arguments of the generating function of
interest, as in the following examples from the analysis of algorithms:
f(z) = ez/2f(z/2)
f(z) = z + f(z2 + z3).
Ļe ŀrst is related to binary tries and radix-exchange sort (see Chapter 8),
and the second counts 2–3 trees (see Chapter 6). Clearly, we could concoct
arbitrarily complicated equations, with no assurance that solutions are readily
available. Some general tools for attacking such equations are treated in [3].
Ļese examples give some indication of what we can expect to encoun-
ter in the use of generating functions in the analysis of algorithms. We will
be examining these and other functional equations on generating functions
throughout the book. Often, such equations serve as a dividing line where
detailed study of the algorithm leaves oﬀand detailed application of analytic
tools begins. However diﬃcult the solution of the functional equation might
appear, it is important to remember that we can use such equations to learn
properties of the underlying sequence.
As with recurrences, the technique of iteration, simply applying the
equation to itself successively, can often be useful in determining the nature of
a generating function deŀned by a functional equation. For example, consider
an EGF that satisŀes the functional equation
f(z) = ezf(z/2).
Ļen, provided that f(0) = 1, we must have
f(z) = ezez/2f(z/4)
= ezez/2ez/4f(z/8)
...
= ez+z/2+z/4+z/8+...
= e2z.
Ļis proves that 2n is the solution to the recurrence
fn =
∑
k
(
n
k
)
fk
2k
for n > 0 with f0 = 1.

§Ț.ȝ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȘȠ
Technically, we need to justify carrying out the iteration indeŀnitely, but the
solution is easily veriŀed from the original recurrence.
Exercise 3.42 Show that the coeﬃcients fn in the expansion
ez+z2/2 =
∑
n≥0
fn
zn
n!
satisfy the second-order linear recurrence fn = fn−1 + (n −1)fn−2. (Hint : Find a
diﬀerential equation satisŀed by the function f(z) = ez+z2/2.)
Exercise 3.43 Solve
f(z) = e−zf
(z
2
)
+ e2z −1
and, assuming that f(z) is an EGF, derive the corresponding recurrence and solution.
Exercise 3.44 Find an explicit formula for the OGF of the sequence satisfying the
divide-and-conquer recurrence
f2n = f2n−1 + fn
for n > 1 with f0 = 0;
f2n+1 = f2n
for n > 0 with f1 = 1.
Exercise 3.45 Iterate the following equation to obtain an explicit formula for f(z):
f(z) = 1 + zf
(
z
1 + z
)
.
Exercise 3.46 [Polya] Given f(z) deŀned by the equation
f(z) =
z
1 −f(z2),
ŀnd explicit expressions for a(z) and b(z) with f(z) = a(z)/b(z).
Exercise3.47 Prove that there is only one power series of the form f(z) = ∑
n≥1 fnzn
that satisŀes f(z) = sin(f(z)).
Exercise 3.48 Derive an underlying recurrence from the functional equation for 2–3
trees and use the recurrence to determine the number of 2–3 trees of 100 nodes.

Șșȗ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ȟ
3.7 SolvingtheQuicksortMedian-of-ĻreeRecurrencewithOGFs.
As a detailed example of manipulating functional equations on generating
functions, we revisit the recurrence given in §1.5 that describes the average
number of comparisons taken by the median-of-three quicksort. Ļis recur-
rence would be diﬃcult to handle without generating functions:
CN = N + 1 +
∑
1≤k≤N
(N −k)(k −1)
(N
3
)
(Ck−1 + CN−k)
for N > 2
with C0 = C1 = C2 = 0. We use N + 1 as the number of comparisons
required to partition N elements for convenience in the analysis. Ļe actual
cost depends on how the median is computed and other properties of the
implementation, but it will be within a small additive constant of N +1. Also,
the initial condition C2 = 0 (and the implied C3 = 4) is used for convenience
in the analysis, though diﬀerent costs are likely in actual implementations.
As in §1.5, we can account for such details by taking linear combinations of
the solution to this recurrence and other, similar, recurrences such as the one
counting the number of partitioning stages (the same recurrence with cost 1
instead of N + 1).
We follow through the standard steps for solving recurrences with gen-
erating functions. Multiplying by
(N
3
) and removing the symmetry in the
sum, we have
(
N
3
)
CN = (N + 1)
(
N
3
)
+ 2
∑
1≤k≤N
(N −k)(k −1)Ck−1.
Ļen, multiplying both sides by zN−3 and summing on N eventually leads to
the diﬀerential equation:
C′′′(z) =
24
(1 −z)5 + 12 C′(z)
(1 −z)2 .
(5)
One cannot always hope to ŀnd explicit solutions for high-order diﬀer-
ential equations, but this one is in fact of a type that can be solved explicitly.
First, multiply both sides by (1 −z)3 to get
(1 −z)3C′′′(z) = 12(1 −z)C′(z) +
24
(1 −z)2 .
(6)

§Ț.Ȟ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘșȘ
Now, in this equation the degree equals the order of each term. Such a diﬀer-
ential equation is known in the theory of ordinary diﬀerential equations as an
Euler equation. We can decompose it by rewriting it in terms of an operator
that both multiplies and diﬀerentiates. In this case, we deŀne the operator
	C(z) ≡(1 −z) d
dz C(z),
which allows us to rewrite (6) as
	(	 + 1)(	 + 2)C(z) = 12	C(z) +
24
(1 −z)2 .
Collecting all the terms involving 	 into one polynomial and factoring,
we have
	(	 + 5)(	 −2)C(z) =
24
(1 −z)2 .
Ļe implication of this equation is that we can solve for C(z) by successively
solving three ŀrst-order diﬀerential equations:
	U(z) =
24
(1 −z)2
or
U′(z) =
24
(1 −z)3 ,
(	 + 5)T(z) = U(z)
or
T ′(z) = −5 T(z)
1 −z + U(z)
1 −z ,
(	 −2)C(z) = T(z)
or
C′(z) = 2 C(z)
1 −z + T(z)
1 −z .
Solving these ŀrst-order diﬀerential equations exactly as for the simpler case
that we solved to analyze regular quick sort, we arrive at the solution.
Ļeorem 3.5 (Median-of-three Quicksort).
Ļe average number of com-
parisons used by the median-of-three quicksort for a random permutation is
given by
CN = 12
7 (N + 1)
(
HN+1 −23
14
)
for N ≥6.

Șșș
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ȟ
Proof. Continuing the earlier discussion, we solve the diﬀerential equations to
get the result
U(z) =
12
(1 −z)2 −12;
T(z) = 12
7
1
(1 −z)2 −12
5 + 24
35(1 −z)5;
C(z) = 12
7
1
(1 −z)2 ln
1
1 −z −54
49
1
(1 −z)2 + 6
5 −24
245(1 −z)5.
Expanding this expression for C(z) (and ignoring the last term) gives the
result (see the exercises in §3.1). Ļe leading term in the OGF diﬀers from
the OGF for standard quicksort only by a constant factor.
We can translate the decomposition into U(z) and T(z) into recurrences
on the corresponding sequences. Consider the generating functions U(z) =
∑UNzN and T(z) = ∑TNzN. In this case, manipulations on generating
functions do correspond to manipulations on recurrences, but the tools used
are more generally applicable and somewhat easier to discover and apply than
would be a direct solution of the recurrence. Furthermore, the solution with
generating functions can be used in the situation when a larger sample is used.
Further details may be found in [9] or [14].
Besides serving as a practical example of the use of generating func-
tions, this rather detailed example illustrates how precise mathematical state-
ments about performance characteristics of interest can be used to help choose
proper values for controlling parameters of algorithms (in this case, the size of
the sample). For instance, the above analysis shows that we save about 14% of
the cost of comparisons by using the median-of-three variant for quicksort,
and a more detailed analysis, taking into account the extra costs (primarily,
the extra exchanges required because the partitioning element is nearer the
middle), shows that bigger samples lead to marginal further improvements.
Exercise 3.49 Show that (1 −z)tC(t)(z) = Ψ(Ψ + 1) . . . (Ψ + t + 1)C(z).
Exercise 3.50 Find the average number of exchanges used by median-of-three quick-
sort.
Exercise 3.51 Find the number of comparisons and exchanges used, on the average,
by quicksort when modiŀed to use the median of ŀve elements for partitioning.

§Ț.ȟ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘșȚ
Exercise 3.52 [Euler] Discuss the solution of the diﬀerential equation
∑
0≤j≤r
(1 −z)r−j dj
dzj f(z) = 0
and the inhomogeneous version where the right-hand side is of the form (1 −z)α.
Exercise 3.53 [van Emden, cf. Knuth] Show that, when the median of a sample of
2t+1 elements is used for partitioning, the number of comparisons used by quicksort
is
1
H2t+2 −Ht+1
NlnN + O(N).
3.8 Counting with Generating Functions. So far, we have concentrated
on describing generating functions as analytic tools for solving recurrence re-
lationships. Ļis is only part of their signiŀcance—they also provide a way to
count combinatorial objects systematically. Ļe “combinatorial objects” may
be data structures being operated upon by algorithms, so this process plays a
fundamental role in the analysis of algorithms as well.
Our ŀrst example is a classical combinatorial problem that also corre-
sponds to a fundamental data structure that will be considered in Chapter 6
and in several other places in the book. A binary tree is a structure deŀned
recursively to be either a single external node or an internal node that is con-
nected to two binary trees, a left subtree and a right subtree. Figure 3.1 shows
the binary trees with ŀve or fewer nodes. Binary trees appear in many prob-
lems in combinatorics and the analysis of algorithms: for example, if internal
nodes correspond to two-argument arithmetic operators and external nodes
correspond to variables, then binary trees correspond to arithmetic expres-
sions. Ļe question at hand is, how many binary trees are there with N ex-
ternal nodes?
Counting binary trees. One way to proceed is to deŀne a recurrence. Let
TN be the number of binary trees with N +1 external nodes. From Figure 3.1
we know that T0 = 1, T1 = 1, T2 = 2, T3 = 5, and T4 = 14. Now, we can
derive a recurrence from the recursive deŀnition: if the left subtree in a binary
tree with N + 1 external nodes has k external nodes (there are Tk−1 diﬀerent
such trees), then the right subtree must have N −k + 1 external nodes (there

Șșț
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ȟ
T2 = 2
T1 = 1
T0 = 1
T3 = 5
T4 = 14
Figure 3.1 All binary trees with 1, 2, 3, 4, and 5 external nodes
are TN−k possibilities), so TN must satisfy
TN =
∑
1≤k≤N
Tk−1TN−k
for N > 0 with T0 = 1.
Ļis is a simple convolution: multiplying by zN and summing on N, we ŀnd
that the corresponding OGF must satisfy the nonlinear functional equation
T(z) = zT(z)2 + 1.
Ļis formula for T(z) is easily solved with the quadratic equation:
zT(z) = 1
2(1 ±
√
1 −4z ).
To get equality when z = 0, we take the solution with a minus sign.

§Ț.ȟ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘșȜ
Ļeorem 3.6 (OGF for binary trees).
Ļe number of binary trees with N +
1 external nodes is given by the Catalan numbers:
TN = [zN+1]1 −√1 −4z
2
=
1
N + 1
(
2N
N
)
.
Proof. Ļe explicit representation of the OGF was derived earlier. To extract
coeﬃcients, use the binomial theorem with exponent 1/2 (Newton’s formula):
zT(z) = −1
2
∑
N≥1
(
1/2
N
)
(−4z)N.
Setting coeﬃcients equal gives
TN = −1
2
(
1/2
N + 1
)
(−4)N+1
= −1
2
1
2( 1
2 −1)( 1
2 −2) . . . ( 1
2 −N)(−4)N+1
(N + 1)!
= 1 · 3 · 5 · · · (2N −1) · 2N
(N + 1)!
=
1
N + 1
1 · 3 · 5 · · · (2N −1)
N!
2 · 4 · 6 · · · 2N
1 · 2 · 3 · · · N
=
1
N + 1
(
2N
N
)
.
As we will see in Chapter 6, every binary tree has exactly one more
external node than internal node, so the Catalan numbers TN also count the
binary trees with N internal nodes. In the next chapter, we will see that the
approximate value is TN ≈4N/N
√
πN.
Counting binary trees (direct).
Ļere is a simpler way to determine the
explicit expression for the generating function above, which gives more in-
sight into the intrinsic utility of generating functions for counting. We deŀne
T to be the set of all binary trees, and adopt the notation |t| to represent, for

Șșȝ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ȟ
t ∈T , the number of internal nodes in t. Ļen we have the following deriva-
tion:
T(z) =
∑
t∈T
z|t|
= 1 +
∑
tL∈T
∑
tR∈T
z|tL|+|tR|+1
= 1 + zT(z)2
.
Ļe ŀrst line is an alternative way to express T(z) from its deŀnition. Each
tree with exactly k external nodes contributes exactly 1 to the coeﬃcient of
zk, so the coeﬃcient of zk in the sum “counts” the number of trees with k
internal nodes. Ļe second line follows from the recursive deŀnition of binary
trees: either a binary tree has no internal nodes (which accounts for the 1),
or it can be decomposed into two independent binary trees whose internal
nodes comprise the internal nodes of the original tree, plus one for the root.
Ļe third line follows because the index variables tL and tR are independent.
Readers are advised to study this fundamental example carefully—we will be
seeing many other similar examples throughout the book.
Exercise 3.54 Modify the above derivation to derive directly the generating function
for the number of binary trees with N external nodes.
Changing a dollar (Polya). A classical example of counting with generating
functions, due to Polya, is to answer the following question: “How many ways
are there to change a dollar, using pennies, nickels, dimes, quarters, and ŀfty-
cent coins?” Arguing as in the direct counting method for binary trees, we
ŀnd that the generating function is given by
D(z) =
∑
p,n,d,q,f≥0
zp+5n+10d+25q+50f
Ļe indices of summation p, n, d, and so on, are the number of pennies, nick-
els, dimes, and other coins used. Each conŀguration of coins that adds up to
k cents clearly contributes exactly 1 to the coeﬃcient of zk, so this is the de-
sired generating function. But the indices of summation are all independent
in this expression for D(z), so we have
D(z) =
∑
p
zp ∑
n
z5n ∑
d
z10d ∑
q
z25q ∑
f
z50f
=
1
(1 −z)(1 −z5)(1 −z10)(1 −z25)(1 −z50).

§Ț.ȟ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘșȞ
By setting up the corresponding recurrence, or by using a computer algebra
system, we ŀnd that [z100]D(z) = 292.
Exercise 3.55 Discuss the form of an expression for [zN]D(z).
Exercise 3.56 Write an eﬃcient computer program that can compute [zN]D(z),
given N.
Exercise 3.57 Show that the generating function for the number of ways to express
N as a linear combination (with integer coeﬃcients) of powers of 2 is
∏
k≥1
1
1 −z2k .
Exercise 3.58 [Euler] Show that
1
1 −z = (1 + z)(1 + z2)(1 + z4)(1 + z8) · · · .
Give a closed form for the product of the ŀrst t factors. Ļis identity is sometimes
called the “computer scientist’s identity.” Why?
Exercise 3.59 Generalize the previous exercise to base 3.
Exercise 3.60 Express [zN](1−z)(1−z2)(1−z4)(1−z8) · · · in terms of the binary
representation of N.
Binomial distribution. How many binary sequences of length N have ex-
actly k bits that are 1 (and N −k bits that are 0)? Let BN denote the set of all
binary sequences of length N and BNk denote the set of all binary sequences
of length N with the property that k of the bits are 1. Now we consider the
generating function for the quantity sought:
BN(z) =
∑
k
|BNk|zk.
But we can note that each binary string b in BN with exactly k 1s contributes
exactly 1 to the coeﬃcient of zk and rewrite the generating function so that
it “counts” each string:
BN(z) ≡
∑
b∈BN
z{# of 1 bits in b} =
∑
b∈BNk
zk
(
=
∑
k
|BNk|zk
)
.

Șșȟ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ȟ
Now the set of all strings of N bits with k 1s can be formed by taking the
union of the set of all strings with N −1 bits and k 1s (adding a 0 to the
beginning of each string) and the set of all strings with N −1 bits and k −1
1s (adding a 1 to the beginning of each string). Ļerefore,
BN(z) =
∑
b∈B(N−1)k
zk +
∑
b∈B(N−1)(k−1)
zk
= BN−1(z) + zBN−1(z)
so BN(z) = (1 + z)N. Expanding this function with the binomial theorem
yields the expected answer |BNk| =
(N
k
).
To summarize informally, we can use the following method to “count”
with generating functions:
• Write down a general expression for the GF involving a sum indexed
over the combinatorial objects to be counted.
• Decompose the sum in a manner corresponding to the structure of the
objects, to derive an explicit formula for the GF.
• Express the GF as a power series to get expressions for the coeﬃcients.
As we saw when introducing generating functions for the problem of counting
binary trees at the beginning of the previous section, an alternative approach
is to use the objects’ structure to derive a recurrence, then use GFs to solve the
recurrence. For simple examples, there is little reason to choose one method
over the other, but for more complicated problems, the direct method just
sketched can avoid the tedious calculations that sometimes arise with recur-
rences. In Chapter 5, we will consider a powerful general approach based on
this idea, and we will see many applications later in the book.

§Ț.Ƞ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘșȠ
3.9 Probability Generating Functions. An application of generating
functions that is directly related to the analysis of algorithms is their use for
manipulating probabilities, to simplify the calculation of averages and vari-
ances.
Deŀnition Given a random variable X that takes on only nonnegative inte-
ger values, with pk ≡Pr{X = k}, the function P(u) = ∑
k≥0 pkuk is called
the probability generating function (PGF) for the random variable.
We have been assuming basic familiarity with computing averages and
standard deviations for random variables in the discussion in §1.7 and in the
examples of average-case analysis of algorithms that we have examined, but
we review the deŀnitions here because we will be doing related calculations
in this and the next section.
Deŀnition Ļe expected value of X, or E(X), also known as the mean value
of X, is deŀned to be ∑
k≥0 kpk. In terms of rk ≡Pr{X ≤k}, this is
equivalent to E(X) = ∑
k≥0(1 −rk). Ļe variance of X, or var(X), is
deŀned to be ∑
k≥0(k −E(X))2pk. Ļe standard deviation of X is deŀned
to be
√var(X).
Probability generating functions are important because they can pro-
vide a way to ŀnd the average and the variance without tedious calculations
involving discrete sums.
Ļeorem 3.7 (Mean and variance from PGFs).
Given a PGF P(z) for a
random variable X, the expected value of X is given by P ′(1) with variance
P ′′(1) + P ′(1) −P ′(1)2.
Proof. If pk ≡Pr{X = k}, then
P ′(1) =
∑
k≥0
kpkuk−1|u=1 =
∑
k≥0
kpk,
the expected value, by deŀnition. Similarly, noting that P(1) = 1, the stated
result for the variance follows directly from the deŀnition:
∑
k≥0
(k −P ′(1))2pk =
∑
k≥0
k2pk −2
∑
k≥0
kP ′(1)pk +
∑
k≥0
P ′(1)2pk
=
∑
k≥0
k2pk −P ′(1)2 = P ′′(1) + P ′(1) −P ′(1)2.

ȘȚȗ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Ƞ
Ļe quantity E(Xr) = ∑
k krpk is known as the rth moment of X. Ļe
expected value is the ŀrst moment and the variance is the diﬀerence between
the second moment and the square of the ŀrst.
Composition rules such as the theorems that we will consider in §5.2 and
§5.3 for enumeration through the symbolic method translate into statements
about combining PGFs for independent random variables.
For example,
if P(u), Q(u) are probability generating functions for independent random
variables X and Y , then P(u)Q(u) is the probability generating function for
X + Y . Moreover, the average and variance of the distribution represented
by the product of two probability generating functions is the sum of the in-
dividual averages and variances.
Exercise 3.61 Give a simple expression for var(X) in terms of rk = Pr{X ≤k}.
Exercise 3.62 Deŀne mean(P) ≡P ′(1) and var(P) ≡P ′′(1) + P ′(1) −P ′(1)2.
Prove that mean(PQ) = mean(P)+ mean(Q) and var(PQ) = var(P) + var(Q) for
any diﬀerentiable functions P and Q with P(1) = Q(1) = 1, not just PGFs.
Uniform discrete distribution. Given an integer n > 0, suppose that Xn is
a random variable that is equally likely to take on each of the integer values
0, 1, 2, . . . , n −1. Ļen the probability generating function for Xn is
Pn(u) = 1
n + 1
nu + 1
nu2 + · · · + 1
nun−1,
the expected value is
P ′
n(1) = 1
n(1 + 2 + · · · + (n −1)) = n −1
2
,
and, since
P ′′
n(1) = 1
n(1 · 2 + 2 · 3 + · · · + (n −2)(n −1)) = 1
6(n −2)(n −1),
the variance is
P ′′
n(1) + P ′
n(1) −P ′
n(1)2 = n2 −1
12
.
Exercise 3.63 Verify the above results from the closed form
Pn(u) =
1 −un
n(1 −u),
using l’H^opital’s rule to compute the derivatives at 1.

§Ț.Ƞ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȚȘ
Exercise 3.64 Find the PGF for the random variable that counts the number of
leading 0s in a random binary string, and use the PGF to ŀnd the mean and standard
deviation.
Binomial distribution. Consider a random string of N independent bits,
where each bit is 0 with probability p and 1 with probability q = 1 −p. We
can argue that the probability that exactly k of the N bits are 0 is
(
N
k
)
pkqN−k,
so the corresponding PGF is
PN(u) =
∑
0≤k≤N
(
N
k
)
pkqN−kuk = (pu + q)N.
Alternatively, we could observe that PGF for 0s in a single bit is (pu+q) and
the N bits are independent, so the PGF for the number of 0s in the N bits is
(pu+q)N. Now, the average number of 0s is P ′(1) = pN and the variance is
P ′′(1)+P ′(1)−P ′(1)2 = pqN, and so forth. We can make these calculations
easily without ever explicitly determining individual probabilities.
One cannot expect to be so fortunate as to regularly encounter a full
decomposition into independent PGFs in this way. In the binomial distribu-
tion, the count of the number of structures 2N trivially factors into N simple
factors, and, since this quantity appears as the denominator in calculating the
average, it is not surprising that the numerator decomposes as well. Con-
versely, if the count does not factor in this way, as for example in the case
of the Catalan numbers, then we might not expect to ŀnd easy independence
arguments like these. For this reason, as described in the next section, we em-
phasize the use of cumulative and bivariate generating functions, not PGFs,
in the analysis of algorithms.
Quicksort distribution.
Let QN(u) be the PGF for the number of com-
parisons used by quicksort. We can apply the composition rules for PGFs to
show that function to satisfy the functional equation
QN(u) = 1
N
∑
1≤k≤N
uN+1Qk−1(u)QN−k(u).

ȘȚș
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Șȗ
Ļough using this equation to ŀnd an explicit expression for QN(u) appears
to be quite diﬃcult, it does provide a basis for calculation of the moments.
For example, diﬀerentiating and evaluating at u = 1 leads directly to the
standard quicksort recurrence that we addressed in §3.3. Note that the PGF
corresponds to a sequence indexed by the number of comparisons; the OGF
that we used to solve (1) in §3.3 is indexed by the number of elements in the
ŀle. In the next section we will see how to treat both with just one double
generating function.
Ļough it would seem that probability generating functions are natural
tools for the average-case analysis of algorithms (and they are), we generally
give this point of view less emphasis than the approach of analyzing para-
meters of combinatorial structures, for reasons that will become more clear in
the next section. When dealing with discrete structures, the two approaches
are formally related if not equivalent, but counting is more natural and allows
for more łexible manipulations.
3.10 Bivariate Generating Functions. In the analysis of algorithms, we
are normally interested not just in counting structures of a given size, but also
in knowing values of various parameters relating to the structures.
We use bivariate generating functions for this purpose. Ļese are func-
tions of two variables that represent doubly indexed sequences: one index for
the problem size, and one index for the value of the parameter being analyzed.
Bivariate generating functions allow us to capture both indices with just one
generating function, of two variables.
Deŀnition Given a doubly indexed sequence {ank}, the function
A(z, u) =
∑
n≥0
∑
k≥0
ankznuk
is called the bivariate generating function (BGF) of the sequence. We use the
notation [znuk]A(z, u) to refer to ank; [zn]A(z, u) to refer to ∑
k≥0 ankuk;
and [uk]A(z, u) to refer to ∑
n≥0 ankzn.
As appropriate, a BGF may need to be made “exponential” by dividing
by n!. Ļus the exponential BGF of {ank} is
A(z, u) =
∑
n≥0
∑
k≥0
ank
zn
n! uk.

§Ț.Șȗ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȚȚ
Most often, we use BGFs to count parameter values in combinatorial
structures as follows. For p ∈P, where P is a class of combinatorial struc-
tures, let cost(p) be a function that gives the value of some parameter deŀned
for each structure. Ļen our interest is in the BGF
P(z, u) =
∑
p∈P
z|p|u{cost(p)} =
∑
n≥0
∑
k≥0
pnkznuk,
where pnk is the number of structures of size n and cost k. We also write
P(z, u) =
∑
n≥0
pn(u)zn
where
pn(u) = [zn]A(z, u) =
∑
k≥0
pnkuk
to separate out all the costs for the structures of size n, and
P(z, u) =
∑
k≥0
qk(z)uk
where
qk(z) = [uk]P(z, u) =
∑
n≥0
pnkzn
to separate out all the structures of cost k. Also, note that
P(z, 1) =
∑
p∈P
z|p| =
∑
n≥0
pn(1)zn =
∑
k≥0
qk(z)
is the ordinary generating function that enumerates P.
Of primary interest is the fact that pn(u)/pn(1) is the PGF for the
random variable representing the cost, if all structures of size n are taken as
equally likely. Ļus, knowing pn(u) and pn(1) allows us to compute average
cost and other moments, as described in the previous section. BGFs provide a
convenient framework for such computations, based on counting and analysis
of cost parameters for combinatorial structures.
Binomial distribution. Let B be the set of all binary strings, and consider
the “cost” function for a binary string to be the number of 1 bits. In this case,
{ank} is the number of n-bit binary strings with k 1s, so the associated BGF
is
P(z, u) =
∑
n≥0
∑
k≥0
(
n
k
)
ukzn =
∑
n≥0
(1 + u)nzn =
1
1 −(1 + u)z .

ȘȚț
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Șȗ
BGF expansions. Separating out the structures of size n as [zn]P(z, u) =
pn(u) is often called the “horizontal” expansion of the BGF. Ļis comes from
the natural representation of the full BGF expansion as a two-dimensional
table, with powers of u increasing in the horizontal direction and powers of
z increasing in the vertical direction. For example, the BGF for the binomial
distribution may be written as follows:
z0(u0)+
z1(u0 + u1)+
z2(u0 + 2u1 + u2)+
z3(u0 + 3u1 + 3u2 + u3)+
z4(u0 + 4u1 + 6u2 + 4u3 + u4)+
z5(u0 + 5u1 + 10u2 + 10u3 + 5u4 + u5) + . . . .
.
Or, proceeding vertically through such a table, we can collect [uk]P(z, u) =
qk(z). For the binomial distribution, this gives
u0(z0 + z1 + z2 + z3 + z4 + z5 + . . .)+
u1(z1 + 2z2 + 3z3 + 4z4 + 5z5 + . . .)+
u2(z2 + 3z3 + 6z4 + 10z5 . . .)+
u3(z3 + 4z4 + 10z5 + . . .)+
u4(z4 + 5z5 + . . .)+
u5(z5 + . . .) + . . . ,
the so-called vertical expansion of the BGF. As we will see, these alternate
representations are important in the analysis of algorithms, especially when
explicit expressions for the full BGF are not available.
Calculating moments “horizontally.” With these notations, calculations of
probabilities and moments are straightforward. Diﬀerentiating with respect
to u and evaluating at u = 1, we ŀnd that
p′
n(1) =
∑
k≥0
kpnk.

§Ț.Șȗ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȚȜ
Ļe partial derivative with respect to u of P(z, u) evaluated at u = 1 is the
generating function for this quantity. Now, pn(1) is the number of members
of P of size n. If we consider all members of P of size n to be equally likely,
then the probability that a structure of size n has cost k is pnk/pn(1) and the
average cost of a structure of size n is p′
n(1)/pn(1).
Deŀnition Let P be a class of combinatorial structures with BGF P(z, u).
Ļen the function
∂P(z, u)
∂u

u=1=
∑
p∈P
cost(p)z|p|
is deŀned to be the cumulative generating function (CGF) for the class. Also,
let Pn denote the class of all the structures of size n in P. Ļen the sum
∑
p∈Pn
cost(p)
is deŀned to be the cumulated cost for the structures of size n.
Ļis terminology is justiŀed since the cumulated cost is precisely the
coeﬃcient of zn in the CGF. Ļe cumulated cost is sometimes referred to as
the unnormalized mean, since the true mean is obtained by “normalizing,” or
dividing by the number of structures of size n.
Ļeorem 3.8 (BGFs and average costs).
Given a BGF P(z, u) for a class
of combinatorial structures, the average cost for all structures of a given size
is given by the cumulated cost divided by the number of structures, or
[zn]∂P(z, u)
∂u

u=1
[zn]P(1, z)
.
Proof. Ļe calculations are straightforward, following directly from the obser-
vation that pn(u)/pn(1) is the associated PGF, then applying Ļeorem 3.7.
Ļe importance of the use of BGFs and Ļeorem 3.8 is that the average
cost can be calculated by extracting coeﬃcients independently from
∂P(z, u)
∂u

u=1
and
P(1, z)

ȘȚȝ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Șȗ
and dividing. For more compact notation, we often write the partial derivative
as Pu(z, 1). Ļe standard deviation can be calculated in a similar manner.
Ļese notations and calculations are summarized in Table 3.5.
For the example given earlier involving the binomial distribution, the
number of binary strings of length n is
[zn]
1
1 −(1 + u)z

u=1= [zn]
1
(1 −2z) = 2n,
and the cumulated cost (number of 1 bits in all n-bit binary strings) is
[zn] ∂
∂u
1
1 −(1 + u)z

u=1= [zn]
z
(1 −2z)2 = n2n−1,
so the average number of 1 bits is thus n/2. Or, starting from pn(u) =
(1 + u)n, the number of structures is pn(1) = 2n and the cumulated cost is
p′
n(1) = n2n−1. Or, we can compute the average by arguing directly that
the number of binary strings of length n is 2n and the number of 1 bits in all
binary strings of length n is n2n−1, since there are a total of n2n bits, half of
which are 1 bits.
Exercise 3.65 Calculate the variance for the number of 1 bits in a random binary
string of length n, using Table 3.5 and pn(u) = (1 + u)n, as shown earlier.
Calculating moments “vertically.” Alternatively, the cumulated cost may
be calculated using the vertical expansion:
[zn]
∑
k≥0
kqk(z) =
∑
k≥0
kpnk.
Corollary Ļe cumulated cost is also equal to
[zn]
∑
k≥0
(P(1, z) −rk(z))
where
rk(z) ≡
∑
0≤j≤k
qj(z).
Proof. Ļe function rk(z) is the generating function for all structures with cost
no greater than k. Since rk(z) −rk−1(z) = qk(z), the cumulated cost is
[zn]
∑
k≥0
k(rk(z) −rk−1(z)),
which telescopes to give the stated result.

§Ț.Șȗ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȚȞ
P(z, u) =
∑
p∈P
z|p|u{cost(p)} =
∑
n≥0
∑
k≥0
pnkukzn =
∑
n≥0
pn(u)zn =
∑
k≥0
qk(z)uk
GF of costs for structures of size n
[zn]P(z, u) ≡pn(u)
GF enumerating structures with cost k
[uk]P(z, u) ≡qk(z)
cumulative generating function (CGF)
∂P(z, u)
∂u

u=1≡q(z)
=
∑
k≥0
kqk(z)
number of structures of size n
[zn]P(1, z) = pn(1)
cumulated cost
[zn]∂P(z, u)
∂u

u=1=
∑
k≥0
kpnk
= p′
n(1)
= [zn]q(z)
average cost
[zn]∂P(z, u)
∂u

u=1
[zn]P(1, z)
= p′
n(1)
pn(1)
= [zn]q(z)
pn(1)
variance
p′′
n(1)
pn(1) + p′
n(1)
pn(1) −
(p′
n(1)
pn(1)
)2
Table 3.5
Calculating moments from a bivariate generating function

ȘȚȟ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.Șȗ
As k increases in this sum, initial terms cancel (all small structures have
cost no greater than k), so this representation lends itself to asymptotic approx-
imation. We will return to this topic in detail in Chapter 6, where we ŀrst
encounter problems for which the vertical formulation is appropriate.
Exercise 3.66 Verify from the vertical expansion that the mean for the binomial dis-
tribution is n/2 by ŀrst calculating rk(z) as described earlier.
Quicksort distribution.
We have studied the average-case analysis of the
running time of quicksort in some detail in §1.5 and §3.3, so it will be in-
structive to examine that analysis, including calculation of the variance, from
the perspective of BGFs. We begin by considering the exponential BGF
Q(z, u) =
∑
N≥0
∑
k≥0
qNkuk zN
N!
where qNk is the cumulative count of the number of comparisons taken by
quicksort on all permutations of N elements. Now, because there are N!
permutations of N elements, this is actually a “probability” BGF: [zN]Q(z, u)
is nothing other than the PGF QN(u) introduced at the end of the previous
section. As we will see in several examples in Chapter 7, this relationship
between exponential BGFs and PGFs holds whenever we study properties
of permutations. Ļerefore, by multiplying both sides of the recurrence from
§3.9,
QN(u) = 1
N
∑
1≤k≤N
uN+1Qk−1(u)QN−k(u),
by zN and summing on N, we can derive the functional equation
∂
∂z Q(z, u) = u2Q2(zu, u)
with
Q(u, 0) = 1
that must be satisŀed by the BGF. Ļis carries enough information to allow
us to compute the moments of the distribution.
Ļeorem 3.9 (Quicksort variance).
Ļe variance of the number of com-
parisons used by quicksort is
7N2 −4(N + 1)2H(2)
N −2(N + 1)HN + 13N ∼N2(
7 −2π2
3
)
.

§Ț.Șȗ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘȚȠ
Proof. Ļis calculation is sketched in the previous discussion and the following
exercises, and is perhaps best done with the help of a computer algebra system.
Ļe asymptotic estimate follows from the approximations HN ∼lnN (see
the ŀrst corollary to Ļeorem 4.3) and H(2)
N ∼π2/6 (see Exercise 4.56). Ļis
result is due to Knuth [9].
As discussed in §1.7, the standard deviation (≈.65N) is asymptotically
smaller than the average value (≈2NlnN −.846N). Ļis means that the
observed number of comparisons when quicksort is used to sort a random
permutation (or when partitioning elements are chosen randomly) should be
close to the mean with high probability, and even more so as N increases.
Exercise 3.67 Conŀrm that
q[1](z) ≡∂
∂uQ(z, u)

u=1=
1
(1 −z)2 ln
1
1 −z
and show that
q[2](z) ≡∂2
∂u2 Q(z, u)

u=1 =
6
(1 −z)3 +
8
(1 −z)3 ln
1
1 −z +
8
(1 −z)3 ln2
1
1 −z
−
6
(1 −z)2 −
12
(1 −z)2 ln
1
1 −z −
4
(1 −z)2 ln2
1
1 −z
.
Exercise 3.68 Extract the coeﬃcient of zN in q [2](z) + q [1](z) and verify the exact
expression for the variance given in Ļeorem 3.9. (See Exercise 3.8.)
TŔő ōŚōŘťşŕş śŒ ŘőōŢőş in binary trees and the analysis of the number of
comparisons taken by quicksort are representative of numerous other exam-
ples, which we will see in Chapters 6 through 9, of the use of bivariate gen-
erating functions in the analysis of algorithms. As our examples here have
illustrated, one reason for this is our ability to use symbolic arguments to en-
capsulate properties of algorithms and data structures in relationships among
their generating functions. As also illustrated by our examples, another rea-
son for this is the convenient framework provided by BGFs for computing
moments, particularly the average.

Șțȗ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ȘȘ
3.11 Special Functions. We have already encountered a number of “spe-
cial” sequences of numbers—such as the harmonic numbers, the Fibonacci
numbers, binomial coeﬃcients, and N!—that are intrinsic to the problems
under examination and that appear in so many diﬀerent applications that they
are worthy of study on their own merit. In this section, we brieły consider
several more such sequences.
We deŀne these sequences in Table 3.6 as the coeﬃcients in the gener-
ating functions given. Alternatively, there are combinatorial interpretations
that could serve to deŀne these sequences, but we prefer to have the gener-
ating function serve as the deŀnition to avoid biasing our discussion toward
any particular application. We may view these generating functions as adding
to our toolkit of “known” functions—these particular ones have appeared so
frequently that their properties are quite well understood.
Ļe primary heritage of these sequences is from combinatorics: each of
them “counts” some basic combinatorial object, some of which are brieły de-
scribed in this section. For example, N! is the number of permutations of N
objects, and HN is the average number of times we encounter a value larger
than all previously encountered when proceeding from left to right through
a random permutation (see Chapter 7). We will avoid a full survey of the
combinatorics of the special numbers, concentrating instead on those that
play a role in fundamental algorithms and in the basic structures discussed in
Chapters 6 through 9. Much more information about the special numbers
may be found, for example, in the books by Comtet [1], by Graham, Knuth,
and Patashnik [5], and by Goulden and Jackson [4]. Ļe sequences also arise
in analysis. For example, we can use them to translate from one way to rep-
resent a polynomial to another. We mention a few examples here but avoid
considering full details.
Ļe analysis of algorithms perhaps adds a new dimension to the study
of special sequences: we resist the temptation to deŀne the special sequences
in terms of basic performance properties of fundamental algorithms, though
it would be possible to do so for each of them, as discussed in Chapters 6
through 9. In the meantime, it is worthwhile to become familiar with these
sequences because they arise so frequently—either directly, when we study
algorithms that turn out to be processing fundamental combinatorial objects,
or indirectly, when we are led to one of the generating functions discussed
here. Whether or not we are aware of a speciŀc combinatorial connection,
well-understood properties of these generating functions are often exploited

§Ț.ȘȘ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘțȘ
binomial coeﬃcients
1
1 −z −uz =
∑
n,k≥0
(n
k
)
ukzn
zk
(1 −z)k+1 =
∑
n≥k
(n
k
)
zn
(1 + u)n =
∑
k≥0
(n
k
)
uk
Stirling numbers of the ŀrst kind
1
(1 −z)u =
∑
n,k≥0
[n
k
]
uk zn
n!
1
k!
(
ln
1
1 −z
)k
=
∑
n≥0
[n
k
]zn
n!
u(u + 1) . . . (u + n −1) =
∑
k≥0
[n
k
]
uk
Stirling numbers of the second kind
eu(ez−1) =
∑
n,k≥0
{n
k
}
uk zn
n!
1
k!(ez −1)k =
∑
n≥0
{n
k
}zn
n!
zk
(1 −z)(1 −2z) . . . (1 −kz) =
∑
n≥k
{n
k
}
zn
Bernoulli numbers
z
(ez −1) =
∑
n≥0
Bn
zn
n!
Catalan numbers
1 −√1 −4z
2z
=
∑
n≥0
1
n + 1
(2n
n
)
zn
harmonic numbers
1
1 −z ln
1
1 −z =
∑
n≥1
Hnzn
factorials
1
1 −z =
∑
n≥0
n!zn
n!
Fibonacci numbers
z
1 −z −z2 =
∑
n≥0
Fnzn
Table 3.6
Classic “special” generating functions

Șțș
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ȘȘ
in the analysis of algorithms. Chapters 6 through 9 will cover many more
details about these sequences with relevance to speciŀc algorithms.
Binomial coeﬃcients. We have already been assuming that the reader is fa-
miliar with properties of these special numbers: the number
(n
k
) counts the
number of ways to choose k objects out of n, without replacement; they are
the coeﬃcients that arise when the polynomial (1 + x)n is expanded in pow-
ers of x. As we have seen, binomial coeﬃcients appear often in the analysis
of algorithms, ranging from elementary problems involving Bernoulli trials
to Catalan numbers to sampling in quicksort to tries to countless other ap-
plications.
Stirling numbers. Ļere are two kinds of Stirling numbers; they can be used
to convert back and forth between the standard representation of a poly-
nomial and a representation using so-called falling factorial powers xk =
x(x −1)(x −2) . . . (x −k + 1):
xn =
∑
k
[
n
k
]
(−1)n−kxk
and
xn =
∑
k
{
n
k
}
xk.
Stirling numbers have combinatorial interpretations similar to those for bi-
nomial coeﬃcients:
{n
k
} is the number of ways to divide a set of n objects into
k nonempty subsets; and
[n
k
] is the number of ways to divide n objects into
k nonempty cycles. We have touched on the
[n
k
] Stirling distribution already
in §3.9 and will cover it in detail in Chapter 7. Ļe
{n
k
} Stirling distribution
makes an appearance in Chapter 9, in our discussion of the coupon collector
problem.
Bernoulli numbers. Ļe sequence with EGF z/(ez −1) arises in a number
of combinatorial applications. For example, we need these numbers if we
want to write down an explicit expression for the sum of the tth powers of
the integers less than N, as a standard polynomial in N. We can deduce the
ŀrst few terms in the sequence by setting coeﬃcients of z equal in
z =
(
B0 + B1z + B2
2 z2 + B3
6 z3 + . . .
)(
z + z2
2 + z3
6 + . . .
)
.
Ļis gives B0 = 1, then B1 +B0/2 = 0 so B1 = −1/2, then B2/2+B1/2+
B0/6 = 0 so B2 = 1/6, and so on. If we let
SNt =
∑
0≤k<N
kt,

§Ț.ȘȘ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘțȚ
then the EGF is given by
SN(z) =
∑
t≥0
∑
0≤k<N
kt zt
t! =
∑
0≤k<N
ekz = eNz −1
ez −1 .
Ļis is now a convolution of “known” generating functions, from which the
explicit formula
SNt =
1
t + 1
∑
0≤k≤t
(
t + 1
k
)
BkNt+1−k
follows. We have
∑
1≤k≤N
k = N2
2 + N
2 = N(N + 1)
2
,
∑
1≤k≤N
k2 = N3
3 + N2
2 + N
6 = N(N + 1)(2N + 1)
6
,
∑
1≤k≤N
k3 = N4
4 + N3
2 + N2
4
= N2(N + 1)2
4
,
and in general
∑
1≤k≤N
kt ∼Nt+1
t + 1.
Beyond this basic use, Bernoulli numbers play an essential role in the Euler-
Maclaurin summation formula (which is discussed in §4.5) and they arise
naturally in other applications in the analysis of algorithms. For example,
they appear in the generating functions for a family of algorithms related to
the digital trees discussed in Chapter 8.
Bernoulli polynomials. Ļe polynomials
Bm(x) =
∑
k
(
m
k
)
Bkxm−k

Șțț
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
§Ț.ȘȘ
have the EGF
∑
m≥0
Bm(x)zm
m! =
z
ez −1exz
from which a number of interesting properties can be proved. For example,
diﬀerentiating this EGF gives the identity
B′
m(x) = mBm−1(x)
for m > 1.
Our primary interest in the Bernoulli polynomials is an analytic application
for approximating integrals—the Euler-Maclaurin summation formula—that
we will examine in detail in the next chapter.
Exercise 3.69 Give closed-form expressions for the following:
∑
n,k≥0
(n
k
)
uk zn
n!
∑
n,k≥0
k!
[n
k
]
uk zn
n!
∑
n,k≥0
k!
{n
k
}
uk zn
n! .
Exercise 3.70 Prove from the generating function that
(n
k
)
=
(n −1
k
)
+
(n −1
k −1
)
.
Exercise 3.71 Prove from the generating function that
[n
k
]
= (n −1)
[n −1
k
]
+
[n −1
k −1
]
.
Exercise 3.72 Prove from the generating function that
{n
k
}
= k
{n −1
k
}
+
{n −1
k −1
}
.
Exercise 3.73 Prove that Bm(0) = Bm(1) = Bm for all m > 1.
Exercise 3.74 Prove from the generating function that Bk is zero for k odd, k ≥3.
Other types of generating functions.
As mentioned at the beginning of
the chapter, kernel functions other than zk and zk/k! lead to other types of
generating functions. For example, using k−z as the kernel function gives
Dirichlet generating functions (DGFs), which play an important role in number

§Ț.ȘȘ
G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘțȜ
theory and in the analysis of several algorithms. Ļese are best understood
as functions of complex z, so their analytic properties are beyond the scope
of this book. Nevertheless, we mention them to motivate the utility of other
kernels and illustrate the kinds of formal manipulations that can arise. Ļe
Dirichlet generating function for 1, 1, 1, . . . is
ζ(z) =
∑
k≥1
1
kz ,
the Riemann zeta function. Ļis function plays a central role in analytic num-
ber theory.
In the analysis of algorithms, Dirichlet generating functions normally
represent number-theoretic properties of sequences and are expressed in terms
of the zeta function. For example,
ζ(z)2 =
∑
k≥1
1
kz
∑
j≥1
1
jz =
∑
k≥1
∑
j≥1
1
(kj)z =
∑
N≥1
∑
j divides N
1
Nz =
∑
N≥1
dN
Nz
where dN is the number of divisors of N. In other words, ζ(z)2 is the Dirich-
let generating function for {dN}.
Dirichlet generating functions turn out to be especially useful when we
need to work with the binary representation of numbers. For example, we
can easily ŀnd the DGF for the characteristic sequence for the even numbers:
∑
N≥1
N even
1
Nz =
∑
N≥1
1
(2N)z = 1
2z ζ(z).
Again, while these formal manipulations are interesting, the analytic prop-
erties of these functions in the complex plane are an important facet of their
use in the analysis of algorithms. Details may be found in [3] or [9].
By using other kernels—such as zk/(1 −zk) (Lambert),
(z
k
) (Newton),
or zk/(1−z)(1−z2) . . . (1−zk) (Euler)—we obtain other types of generat-
ing functions that have proved over the centuries to have useful properties in
analysis. Such functions arise occasionally in the analysis of algorithms, and
exploration of their properties is fascinating. We mention them in passing but
we do not consider them in detail because they do not play the central role
that OGFs and EGFs do. Much more information on these can be found in
[5], [7], [11], and [17].

Șțȝ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
Exercise 3.75 Show that, for any k ≥0, the DGF for the characteristic sequence of
numbers whose binary representation ends in k 0s is ζ(z)/2kz.
Exercise 3.76 Find the DGF for the function ψN, the number of trailing 0s in the
binary representation of N.
Exercise 3.77 Find the DGF for the characteristic function of {N 2}.
Exercise 3.78 Prove that
∑
k
zk
1 −zk =
∑
N
dNzN,
where dN is the number of divisors of N.
G
ENERATING functions have long been used in combinatorics, proba-
bility theory, and analytic number theory; hence a rich array of mathe-
matical tools have been developed that turn out to be germane to the analysis
of algorithms. As described in detail in Chapter 5, the role of generating
functions in the analysis of algorithms is central: we use generating functions
both as formal objects to aid in the process of precisely accounting for quan-
tities of interest and as analytic objects to yield solutions.
We have introduced generating functions as a tool to solve the recur-
rences that arise in the analysis of algorithms in order to emphasize their direct
relationship to the quantity being studied (running time, or other characteris-
tic parameter, as a function of problem size). Ļis direct relationship provides
a mathematical model that is amenable to classical techniques of all sorts to
provide information about the algorithm at hand, as we have seen.
But we have also noted that a recurrence is simply one characterization
of a sequence; the corresponding generating function itself is another. For
many problems it is the case that direct arguments can yield explicit expres-
sions for generating functions and recurrences can be avoided entirely. Ļis
theme is developed in the chapters that follow and formalized in [3].
Using the generating function representation, we are often able to trans-
form a problem to see how the sequence of interest is expressed in terms of
classical special number sequences. If an exact representation is not available,
the generating function representation positions us to employ powerful math-
ematical techniques based on properties of functions of a complex variable in
order to learn properties of our algorithms. For a great many of the kinds of

G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘțȞ
functions that arise in the analysis of algorithms, we can ŀnd precise estimates
of asymptotic behavior of coeﬃcients by expansion in terms of classical func-
tions. If not, we can be secure that complex asymptotic methods are available
for extracting estimates of the asymptotic values of coeﬃcients, as discussed
brieły in Chapter 5 and in detail in [3].
Ordinary, exponential, and bivariate generating functions provide a fun-
damental framework that enables us to develop a systematic approach to ana-
lyzing a large number of the fundamental structures that play a central role in
the design of algorithms. With the additional help of the asymptotic meth-
ods to be developed in the next chapter, we are able to use the tools to develop
results that we can use to predict the performance characteristics of a variety
of important and useful algorithms. Again, this theme will be developed in
detail in Chapters 6 through 9.

Șțȟ
C Ŕ ō Ŝ Š ő Ş
T Ŕ Ş ő ő
References
1. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
2. P. FŘōŖśŘőŠ, B. SōŘŢť, ōŚŐ P. ZŕřřőŞřōŚ. “Automatic average-case
analysis of algorithms,” Ļeoretical Computer Science 79, 1991, 37–109.
3. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
4. I. GśšŘŐőŚ ōŚŐ D. JōŏŗşśŚ. Combinatorial Enumeration, John Wiley,
New York, 1983.
5. R. L. GŞōŔōř, D. E. KŚšŠŔ, ōŚŐ O. PōŠōşŔŚŕŗ. Concrete Mathemat-
ics, 1st edition, Addison-Wesley, Reading, MA, 1989. Second edition,
1994.
6. D. H. GŞőőŚő ōŚŐ D. E. KŚšŠŔ. Mathematics for the Analysis of Algo-
rithms, Birkhäuser, Boston, 1981.
7. G. H. HōŞŐť. Divergent Series, Oxford University Press, 1947.
8. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 1: Fundamen-
tal Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1968. Ļird
edition, 1997.
9. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.
10. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 4A: Combina-
torial Algorithms, Part 1, Addison-Wesley, Boston, 2011.
11. N. E. NśŞŘšŚŐ. Vorlesungen uber Diﬀerenzenrechnung, Chelsea Pub-
lishing Company, New York, 1954.
12. G. PƖŘťō, R. E. TōŞŖōŚ, ōŚŐ D. R. WśśŐş. Notes on Introductory Com-
binatorics, Birkhauser, Boston, 1983.
13. J. RŕśŞŐōŚ. Introduction to Combinatorial Analysis, Princeton University
Press, Princeton, NJ, 1980.
14. R. SőŐœőţŕŏŗ. “Ļe analysis of quicksort programs,” Acta Informatica
7, 1977, 327–355.
15. R. SőŐœőţŕŏŗ. Quicksort, Garland Publishing, New York, 1980.
16. R. P. SŠōŚŘőť. Enumerative Combinatorics, Wadsworth & Brooks/Cole,
1986, 2nd edition, Cambridge, 2011.

G ő Ś ő Ş ō Š ŕ Ś œ
F š Ś ŏ Š ŕ ś Ś ş
ȘțȠ
17. R. P. SŠōŚŘőť. “Generating functions,” in Studies in Combinatorics (MAA
Studies in Mathematics, 17, G. C. Rota, ed.), Ļe Mathematical Asso-
ciation of America, 1978, 100–141.
18. J. S. VŕŠŠőŞ ōŚŐ P. FŘōŖśŘőŠ. “Analysis of algorithms and data struc-
tures,” in Handbook of Ļeoretical Computer Science A: Algorithms and Com-
plexity, J. van Leeuwen, ed., Elsevier, Amsterdam, 1990, 431–524.
19. H. WŕŘŒ. Generatingfunctionology, Academic Press, San Diego, 1990,
2nd edition, A. K. Peters, 2006.

This page intentionally left blank 

C H A P T E R F O U R
A S Y M P T O T I C A P P R O X I M A T I O N S
O
UR initial general orientation in the analysis of algorithms is toward
deriving exact mathematical results. However, such exact solutions may
not be always available, or if available they may be too unwieldy to be of much
use. In this chapter, we will examine some methods of deriving approximate
solutions to problems or of approximating exact solutions; as a result, we may
modify our primary orientation to be toward deriving concise, accurate, and
precise estimates of quantities of interest.
In a manner similar to Chapter 3, our primary goal in this chapter is to
provide an overview of the basic properties of asymptotic expansions, meth-
ods of manipulating them, and a catalog of those that we encounter most
often in the analysis of algorithms. At times, this may seem to take us rather
far from the analysis of algorithms, though we continue to draw examples
and exercises directly from problems introduced in relation to speciŀc algo-
rithms in Chapter 1, and to lay the groundwork for studying a broad variety
of algorithms in Chapters 6 through 9.
As we have been doing, we focus in this chapter on methods from real
analysis. Asymptotic methods using complex analysis are sometimes required
for the problems that we face in the analysis of algorithms. Such methods are a
primary topic of [11]; the treatment in this chapter is a necessary background
for learning them. We brieły introduce the principle upon which one of the
most important techniques is based in §5.5.
We have seen that the analysis of computer algorithms involves tools
from discrete mathematics, leading to answers most easily expressed in terms
of discrete functions (such as harmonic numbers or binomial coeﬃcients)
rather than more familiar functions from analysis (such as logarithms or pow-
ers). However, it is generally true that these two types of functions are closely
related—one reason to do asymptotic analysis is to “translate” between them.
Generally, a problem carries a notion of “size” and we are interested in
approximations that become more accurate as the size becomes large. By the
nature of the mathematics and the problems that we are solving, it is also
often true that our answers, if they are to be expressed in terms of a single
ȘȜȘ

ȘȜș
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
parameter N, will be (primarily) expressed in terms of asymptotic series in
N and log N. Ļese series are not necessarily convergent (indeed, they are
often divergent), but the initial terms give very accurate estimates for many
quantities that arise in the analysis of algorithms. Our approach will be to
use such series to represent quantities of interest, manipulating them in well-
deŀned ways to develop concise, accurate, and precise expressions.
One motivation for considering asymptotic methods is simply to ŀnd a
convenient way to calculate good approximations to speciŀc values for quan-
tities of interest. Another motivation is to get all our quantities in a canonical
form so that we can compare and combine them easily. For example, as we
saw in Chapter 1, it is helpful to know that the number of comparisons taken
by quicksort approaches 2NHN as compared to the optimal lgN!, but it is
more useful to know that both are proportional to NlgN with coeﬃcient
1.4421 · · · for the former and 1 for the latter, and that even more accurate
estimates are available.
For another example, in §7.6 we encounter a sorting algorithm whose
average running time is proportional to N4N−1/
(2N
N
). Ļis is a concise exact
result, but how can we know the value of the quantity for, say, N = 1000?
Using the formula to compute the value for large N is not a straightforward
task, since it involves dividing two very large numbers or rearranging the com-
putation to avoid doing so. In this chapter, we will see how to show that this
quantity is very close to N
√
πN/4, which evaluates to 14,012 for N = 1000
and is only about a hundredth of a percent oﬀthe exact value, which is about
14,014. More important, the way in which the value of the quantity grows
as N grows is clearly indicated by the approximate result (for example, we
know by inspection that the value associated with 100N is about 1000 times
the value associated with N), making it easier to compare the approximate
result with similar results for other algorithms or other versions of the same
algorithm.
Another important reason for working with approximate values is that
they can substantially simplify symbolic calculations that might be involved
in the analysis for many problems, allowing derivation of concise answers that
might otherwise not be available. We touched upon this in Chapter 2 when
discussing the solution of recurrences by solving similar, simpler, recurrences
and estimating the error. Asymptotic analysis gives a systematic approach to
aid in such arguments.

§ț.Ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȜȚ
A primary topic of the chapter is our treatment of methods for comput-
ing approximate values of sums for which exact evaluation may be diﬃcult or
impossible. Speciŀcally, we consider how to evaluate sums by approximating
them with integrals using the Euler-Maclaurin summation formula. We also
will look at the Laplace method for evaluating sums by adjusting the range of
summation to make diﬀerent approximations applicable in diﬀerent parts of
the range.
We consider several examples of the application of these concepts to
ŀnd approximate values of some of the special number sequences introduced
in Chapter 3 and other quantities that are likely to appear in the analysis of
algorithms. In particular, we consider in some detail the Ramanujan-Knuth
Q-function and related distributions, which arise frequently in the analysis.
Ļen, we consider limits of the binomial distribution under various circum-
stances. Ļe normal approximation and the Poisson approximation are classical
results that are very useful in the analysis of algorithms and also provide ex-
cellent examples of the application of tools developed in this chapter.
Ļe standard reference on these topics is the book by de Bruijn [6],
which certainly should be read by anyone with a serious interest in asymptotic
analysis. Ļe survey by Odlyzko [18] also provides a great deal of information
and a wealth of examples. Speciŀc information about the normal and Poisson
approximations may be found, for example, in Feller [8]. Detailed coverage
of many of the topics that we consider may also be found in [2], [12], [13],
[15], [19], and other references listed at the end of this chapter. Methods
based on complex analysis are covered in detail in [11].
4.1 Notation for Asymptotic Approximations. Ļe following nota-
tions, which date back at least to the beginning of the 20th century, are widely
used for making precise statements about the approximate value of functions:
Deŀnition Given a function f(N), we write
g(N) = O(f(N))
if and only if |g(N)/f(N)| is bounded from above as N →∞,
g(N) = o(f(N))
if and only if g(N)/f(N) →0 as N →∞,
g(N) ∼f(N)
if and only if g(N)/f(N) →1 as N →∞.

ȘȜț
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ș
Ļe O- and o-notations provide ways to express upper bounds (with o
being the stronger assertion), and the ∼-notation provides a way to express
asymptotic equivalence. Ļe O-notation here coincides with the deŀnition
given in Chapter 1 for use in our discussion on computational complexity.
A variety of similar notations and deŀnitions have been proposed. A reader
interested in pursuing implications may wish to read the discussion in [6] or
[12].
Exercise 4.1 Show that
N/(N + 1) = O
(
1
)
,
2N = o(N!),
and
N√e ∼1.
Exercise 4.2 Show that
N
N + 1 = 1 + O
( 1
N
)
and
N
N + 1 ∼1 −1
N .
Exercise 4.3 Show that N α = o(N β) if α < β.
Exercise 4.4 Show that, for r ŀxed,
(N
r
)
= N r
r! + O
(
N r−1)
and
(N + r
r
)
= N r
r! + O
(
N r−1)
.
Exercise 4.5 Show that logN = o(N ϵ) for all ϵ > 0.
Exercise 4.6 Show that
1
2 + lnN = o(1)
and
1
2 + cos N = O
(
1
)
but not o(1).
As we will see later, it is not usually necessary to directly apply the def-
initions to determine asymptotic values of quantities of interest, because the
O-notation makes it possible to develop approximations using a small set of
basic algebraic manipulations.
Ļe same notations are used when approximating functions of real or
complex variables near any given point. For example, we say that
1
1 + x = 1
x −1
x2 + 1
x3 + O
( 1
x4
)
as
x →∞

§ț.Ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȜȜ
and
1
1 + x = 1 −x + x2 −x3 + O(x4)
as
x →0.
A more general deŀnition of the O-notation that encompasses such uses is
obtained simply by replacing N →∞by x →x0 in the preceding deŀnition,
and specifying any restrictions on x (for example, whether it must be integer,
real, or complex). Ļe limiting value x0 is usually 0 or ∞, but it could be any
value whatever. It is usually obvious from the context which set of numbers
and which limiting value are of interest, so we normally drop the qualifying
“x →x0” or “N →∞.” Of course, the same remarks apply to the o−and
∼−notations.
In the analysis of algorithms, we avoid direct usages such as “the average
value of this quantity is O(f(N))” because this gives scant information for the
purpose of predicting performance. Instead, we strive to use the O-notation
to bound “error” terms that have far smaller values than the main, or “leading,”
term. Informally, we expect that the terms involved should be so small as to
be negligible for large N.
O-approximations.
We say that g(N) = f(N) + O(h(N)) to indicate
that we can approximate g(N) by calculating f(N) and that the error will be
bounded above by a constant factor of h(N). As usual with the O-notation,
the constant involved is unspeciŀed, but the assumption that it is not large is
often justiŀed. As discussed later, we normally use this notation with h(N) =
o(f(N)).
o-approximations.
A stronger statement is to say that g(N) = f(N) +
o(h(N)) to indicate that we can approximate g(N) by calculating f(N) and
that the error will get smaller and smaller compared to h(N) as N gets larger.
An unspeciŀed function is involved in the rate of decrease, but the assumption
that it is never large numerically (even for small N) is often justiŀed.
∼-approximations. Ļe notation g(N) ∼f(N) is used to express the weak-
est nontrivial o-approximation g(N) = f(N) + o(f(N)).
Ļese notations are useful because they can allow suppression of unim-
portant details without loss of mathematical rigor or precise results. If a more
accurate answer is desired, one can be obtained, but most of the detailed cal-
culations are suppressed otherwise. We will be most interested in methods
that allow us to keep this “potential accuracy,” producing answers that could
be calculated to arbitrarily ŀne precision if desired.

ȘȜȝ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ș
Exponentially small terms. When logarithms and exponentials are involved,
it is worthwhile to be cognizant of “exponential diﬀerences” and avoid calcu-
lations that make truly negligible contributions to the ultimate answer of in-
terest. For example, if we know that the value of a quantity is 2N +O(logN),
then we can be reasonably conŀdent that 2N is within a few percent or a few
thousandths of a percent of the true value when N is 1 thousand or 1 mil-
lion, and that it may not be worthwhile to ŀnd the coeﬃcient of logN or
sharpen the expansion to within O(1). Similarly, an asymptotic estimate of
2N + O(N2) is quite sharp. On the other hand, knowing that a quantity is
2NlnN +O(N) might not be enough to estimate it within a factor of 2, even
when N is 1 million. To highlight exponential diﬀerences, we often refer
informally to a quantity as being exponentially small if it is smaller than any
negative power of N—that is, O(1/NM) for any positive M. Typical exam-
ples of exponentially small quantities are e−N, e−log2N, and (logN)−logN.
Exercise 4.7 Prove that e−N ϵ is exponentially small for any positive constant ϵ. (Ļat
is, given ϵ, prove that e−N ϵ = O
(
N −M)
for any ŀxed M > 0.)
Exercise 4.8 Prove that e−log2N and (logN)−logN are exponentially small.
Exercise 4.9 If α < β, show that αN is exponentially small relative to βN. For β =
1.2 and α = 1.1, ŀnd the absolute and relative errors when αN +βN is approximated
by βN, for N = 10 and N = 100.
Exercise 4.10 Show that the product of an exponentially small quantity and any
polynomial in N is an exponentially small quantity.
Exercise 4.11 Find the most accurate expression for an implied by each of the fol-
lowing recurrence relationships:
an = 2an/2 + O
(
n
)
an = 2an/2 + o(n)
an ∼2an/2 + n.
In each case assume that an/2 is taken to be shorthand notation for a⌊n/2⌋+ O(1).
Exercise4.12 Using the deŀnitions from Chapter 1, ŀnd the most accurate expression
for an implied by each of the following recurrence relationships:
an = 2an/2 + O
(
n
)
an = 2an/2 + Θ(n)
an = 2an/2 + Ω(n).
In each case assume that an/2 is taken to be shorthand notation for a⌊n/2⌋+ O(1).

§ț.Ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȜȞ
Exercise 4.13 Let β > 1 and take f(x) = xα with α > 0. If a(x) satisŀes the
recurrence
a(x) = a(x/β) + f(x)
for x ≥1 with a(x) = 0 for x < 1
and b(x) satisŀes the recurrence
b(x) = b(x/β + c) + f(x)
for x ≥1 with b(x) = 0 for x < 1
prove that a(x) ∼b(x) as x →∞. Extend your proof to apply to a broader class of
functions f(x).
Asymptotics of linear recurrences. Linear recurrences provide an illustra-
tion of the way that asymptotic expressions can lead to substantial simpliŀca-
tions. We have seen in §2.4 and §3.3 that any linear recurrent sequence {an}
has a rational OGF and is a linear combination of terms of the form βnnj.
Asymptotically speaking, it is clear that only a few terms need be considered,
because those with larger β exponentially dominate those with smaller β (see
Exercise 4.9). For example, we saw in §2.3 that the exact solution to
an = 5an−1 −6an−2,
n > 1;
a0 = 0 and a1 = 1
is 3n −2n, but the approximate solution 3n is accurate to within a thousandth
of a percent for n > 25. In short, we need keep track only of terms associated
with the largest absolute value or modulus.
Ļeorem 4.1 (Asymptotics of linear recurrences).
Assume that a ratio-
nal generating function f(z)/g(z), with f(z) and g(z) relatively prime and
g(0) ̸= 0, has a unique pole 1/β of smallest modulus (that is, g(1/α) = 0 and
α ̸= β implies that |1/α| > |1/β|, or |α| < |β|). Ļen, if the multiplicity of
1/β is ν, we have
[zn]f(z)
g(z) ∼Cβnnν−1
where
C = ν (−β)νf(1/β)
g(ν)(1/β)
.
Proof. From the discussion in §3.3, [zn]f(z)/g(z) can be expressed as a sum of
terms, one associated with each root 1/α of g(z), that is of the form [zn]c0(1−

ȘȜȟ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ș
αz)−να, where να is the multiplicity of α. For all α with |α| < |β|, such terms
are exponentially small relative to the one associated with β because
[zn]
1
(1 −αz)να =
(
n + να −1
να −1
)
αn
and αnnM = o(βn) for any nonnegative M (see Exercise 4.10).
Ļerefore, we need only consider the term associated with β:
[zn]f(z)
g(z) ∼[zn]
c0
(1 −βz)ν ∼c0
(
n + ν −1
ν −1
)
βn ∼
c0
(ν −1)!nν−1βn
(see Exercise 4.4) and it remains to determine c0. Since (1 −βz) is not a
factor of f(z), this computation is immediate from l’H^opital’s rule:
c0 = lim
z→1/β(1−βz)ν f(z)
g(z) = f(1/β)
lim
z→1/β(1 −βz)ν
lim
z→1/β g(z)
= f(1/β) ν!(−β)ν
g(ν)(1/β).
For recurrences leading to g(z) with a unique pole of smallest modulus,
this gives a way to determine the asymptotic growth of the solution, including
computation of the coeﬃcient of the leading term. If g(z) has more than one
pole of smallest modulus, then, among the terms associated with such poles,
the ones with highest multiplicity dominate (but not exponentially). Ļis
leads to a general method for determining the asymptotic growth of the solu-
tions to linear recurrences, a modiŀcation of the method for exact solutions
given at the end of §3.3.
• Derive g(z) from the recurrence.
• Compute f(z) from g(z) and the initial conditions.
• Eliminate common factors in f(z)/g(z). Ļis could be done by factor-
ing both f(z) and g(z) and cancelling, but full polynomial factorization
of the functions is not required, just computation of the greatest com-
mon divisor.
• Identify terms associated with poles of highest multiplicity among those
of smallest modulus.
• Determine the coeﬃcients, using Ļeorem 4.1. As indicated above, this
gives very accurate answers for large n because the terms neglected are
exponentially small by comparison with the terms kept.

§ț.Ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȜȠ
Ļis process leads immediately to concise, accurate, and precise approxima-
tions to solutions for linear recurrences. For example, consider the recurrence
an = 2an−1 + an−2 −2an−3,
n > 2;
a0 = 0, a1 = a2 = 1.
We found in §3.3 that the generating function for the solution is
a(z) = f(z)
g(z) =
z
(1 + z)(1 −2z).
Here β = 2, ν = 1, g′(1/2) = −3, and f(1/2) = 1/2, so Ļeorem 4.1 tells
us that an ∼2n/3, as before.
Exercise 4.14 Use Ļeorem 4.1 to ŀnd an asymptotic solution to the recurrence
an = 5an−1 −8an−2 + 4an−3
for n > 2 with a0 = 1, a1 = 2, and a2 = 4.
Solve the same recurrence with the initial conditions on a0 and a1 changed to a0 = 1
and a1 = 2.
Exercise 4.15 Use Ļeorem 4.1 to ŀnd an asymptotic solution to the recurrence
an = 2an−2 −an−4
for n > 4 with a0 = a1 = 0 and a2 = a3 = 1.
Exercise 4.16 Use Ļeorem 4.1 to ŀnd an asymptotic solution to the recurrence
an = 3an−1 −3an−2 + an−3
for n > 2 with a0 = a1 = 0 and a2 = 1.
Exercise 4.17 [Miles, cf. Knuth] Show that the polynomial zt −zt−1 −. . . −z −1
has t distinct roots and that exactly one of the roots has modulus greater than 1, for
all t > 1.
Exercise 4.18 Give an approximate solution for the “tth-order Fibonacci” recurrence
F [t]
N = F [t]
N−1 + F [t]
N−2 + . . . + F [t]
N−t
for N ≥t
with F [t]
0
= F [t]
1
= . . . = F [t]
t−2 = 0 and F [t]
t−1 = 1.
Exercise 4.19 [Schur] Show that the number of ways to change an N-denomination
bill using coin denominations d1, d2, . . . , dt with d1 = 1 is asymptotic to
N t−1
d1d2 . . . dt(t −1)!.
(See Exercise 3.55.)

Șȝȗ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ș
4.2 Asymptotic Expansions. As mentioned earlier, we prefer the equa-
tion f(N) = c0g0(N) + O(g1(N)) with g1(N) = o(g0(N)) to the equation
f(N) = O(g0(N)) because it provides the constant c0, and therefore al-
lows us to provide speciŀc estimates for f(N) that improve in accuracy as
N gets large. If g0(N) and g1(N) are relatively close, we might wish to
ŀnd a constant associated with g1 and thus derive a better approximation:
if g2(N) = o(g1(N)), we write f(N) = c0g0(N) + c1g1(N) + O(g2(N)).
Ļe concept of an asymptotic expansion, developed by Poincaré (cf. [6]),
generalizes this notion.
Deŀnition Given a sequence of functions {gk(N)}k≥0 having the property
that gk+1(N) = o(gk(N)) for k ≥0, the formula
f(N) ∼c0g0(N) + c1g1(N) + c2g2(N) + . . .
is called an asymptotic series for f, or an asymptotic expansion of f. Ļe asymp-
totic series represents the collection of equations
f(N) = O(g0(N))
f(N) = c0g0(N) + O(g1(N))
f(N) = c0g0(N) + c1g1(N) + O(g2(N))
f(N) = c0g0(N) + c1g1(N) + c2g2(N) + O(g3(N))
...
and the gk(N) are referred to as an asymptotic scale.
Each additional term that we take from the asymptotic series gives a
more accurate asymptotic estimate. Full asymptotic series are available for
many functions commonly encountered in the analysis of algorithms, and we
primarily consider methods that could be extended, in principle, to provide
asymptotic expansions describing quantities of interest. We can use the ∼-
notation to simply drop information on error terms or we can use the O-
notation or the o-notation to provide more speciŀc information.
For example, the expression 2NlnN + (2γ −2)N + O(logN) allows
us to make far more accurate estimates of the average number of comparison
required for quicksort than the expression 2NlnN +O(N) for practical values

§ț.ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȝȘ
of N, and adding the O(logN) and O(1) terms provides even more accurate
estimates, as shown in Table 4.1.
Asymptotic expansions extend the deŀnition of the ∼−notation that
we considered at the beginning of §4.1. Ļe earlier use normally would involve
just one term on the right-hand side, whereas the current deŀnition calls for
a series of (decreasing) terms.
Indeed, we primarily deal with ŀnite expansions, not (inŀnite) asymp-
totic series, and use, for example, the notation
f(N) ∼c0g0(N) + c1g1(N) + c2g2(N)
to refer to a ŀnite expansion with the implicit error term o(g2(N)). Most
often, we use ŀnite asymptotic expansions of the form
f(N) = c0g0(N) + c1g1(N) + c2g2(N) + O(g3(N)),
obtained by simply truncating the asymptotic series. In practice, we generally
use only a few terms (perhaps three or four) for an approximation, since the
usual situation is to have an asymptotic scale that makes later terms extremely
small in comparison to early terms for large N. For the quicksort example
shown in Table 4.1, the “more accurate” formula 2NlnN + (2γ −2)N +
2lnN + 2γ + 1 gives an absolute error less than .1 already for N = 10.
Exercise 4.20 Extend Table 4.1 to cover the cases N = 105 and 106.
Ļe full generality of the Poincaré approach allows asymptotic expan-
sions to be expressed in terms of any inŀnite series of functions that decrease
(in an o-notation sense). However, we are most often interested in a very
N
2(N + 1)(HN+1 −1)
2NlnN
+(2γ −2)N +2(lnN + γ) + 1
10
44.43
46.05
37.59
44.35
100
847.85
921.03
836.47
847.84
1000
12,985.91
13,815.51
12,969.94
12,985.91
10,000
175,771.70
184,206.81
175,751.12
175,771.70
Table 4.1
Asymptotic estimates for quicksort comparison counts

Șȝș
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ș
restricted set of functions: indeed, we are very often able to express approxi-
mations in terms of decreasing powers of N when approximating functions as
N increases. Other functions occasionally are needed, but we normally will
be content with an asymptotic scale consisting of terms of decreasing series
of products of powers of N, logN, iterated logarithms such as loglogN, and
exponentials.
When developing an asymptotic estimate, it is not necessarily clear a
prioiri how many terms should be carried in the expansion to get the de-
sired accuracy in the result. For example, frequently we need to subtract or
divide quantities for which we only have asymptotic estimates, so cancella-
tions might occur that necessitate carrying more terms. Typically, we carry
three or four terms in an expansion, perhaps redoing the derivation to stream-
line it or to add more terms once the nature of the result is known.
Taylor expansions. Taylor series are the source of many asymptotic expan-
sions: each (inŀnite) Taylor expansion gives rise to an asymptotic series as
x →0. Table 4.2 gives asymptotic expansions for some of the basic func-
tions, derived from truncating Taylor series. Ļese expansions are classical,
exponential
ex = 1 + x + x2
2 + x3
6 + O(x4)
logarithmic
ln(1 + x) = x −x2
2 + x3
3 + O(x4)
binomial
(1 + x)k = 1 + kx +
(k
2
)
x2 +
(k
3
)
x3 + O(x4)
geometric
1
1 −x = 1 + x + x2 + x3 + O(x4)
trigonometric
sin(x) = x −x3
6 + x5
120 + O(x7)
cos(x) = 1 −x2
2 + x4
24 + O(x6)
Table 4.2
Asymptotic expansions derived from Taylor series (x →0)

§ț.ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȝȚ
and follow immediately from the Taylor theorem. In the sections that fol-
low, we describe methods of manipulating asymptotic series using these ex-
pansions. Other similar expansions follow immediately from the generating
functions given in the previous chapter. Ļe ŀrst four expansions serve as the
basis for many of the asymptotic calculations that we do (actually, the ŀrst
three suﬃce, since the geometric expansion is a special case of the binomial
expansion).
For a typical example of the use of Table 4.2, consider the problem of
ŀnding an asymptotic expansion for ln(N −2) as N →∞. We do so by
pulling out the leading term, writing
ln(N −2) = lnN + ln
(
1 −2
N
)
= lnN −2
N + O
( 1
N2
)
.
Ļat is, in order to use Table 4.2, we ŀnd a substitution (x = −2/N) with
x →0.
Or, we can use more terms of the Taylor expansion to get a more general
asymptotic result. For example, the expansion
ln(N +
√
N) = lnN +
1
√
N
−1
2N + O
(
1
N3/2
)
follows from factoring out lnN, then taking x = 1/
√
N in the Taylor expan-
sion for ln(1 + x). Ļis kind of manipulation is typical, and we will see many
examples of it later.
Exercise 4.21 Expand ln(1 −x + x2) as x →0, to within O
(
x4)
.
Exercise 4.22 Give an asymptotic expansion for ln(N α + N β), where α and β are
positive constants with α > β.
Exercise 4.23 Give an asymptotic expansion for
N
N −1ln
N
N −1.
Exercise 4.24 Estimate the value of e0.1 + cos(.1) −ln(.9) to within 10−4, without
using a calculator.
Exercise 4.25 Show that
1
9801 = 0.000102030405060708091011 · · · 47484950 · · ·
to within 10−100. How many more digits can you predict? Generalize.

Șȝț
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ș
Nonconvergentasymptoticseries. Any convergent series leads to a full asymp-
totic approximation, but it is very important to note that the converse is not
true—an asymptotic series may well be divergent. For example, we might
have a function
f(N) ∼
∑
k≥0
k!
Nk
implying (for example) that
f(N) = 1 + 1
N + 2
N2 + 6
N3 + O
( 1
N4
)
even though the inŀnite sum does not converge. Why is this allowed? If we
take any ŀxed number of terms from the expansion, then the equality implied
from the deŀnition is meaningful, as N →∞. Ļat is, we have an inŀnite
collection of better and better approximations, but the point at which they
start giving useful information gets larger and larger.
Stirling’s formula. Ļe most celebrated example of a divergent asymptotic
series is Stirling's formula, which begins as follows:
N! =
√
2πN
(N
e
)N(
1 +
1
12N +
1
288N2 + O
( 1
N3
))
.
N
N!
√
2πN
(N
e
)N(
1 +
1
12N +
1
288N 2
)
absolute relative
error
error
1
1
1.002183625
.0022 10−2
2
2
2.000628669
.0006 10−3
3
6
6.000578155
.0006 10−4
4
24
24.00098829
.001
10−4
5
120
120.0025457
.002
10−4
6
720
720.0088701
.009
10−4
7
5040
5040.039185
.039
10−5
8
40,320
40320.21031
.210
10−5
9
362,880
362881.3307
1.33
10−5
10
3,628,800
3628809.711
9.71
10−5
Table 4.3
Accuracy of Stirling’s formula for N!

§ț.ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȝȜ
In §4.6 we show how this formula is derived, using a method that gives a full
(but divergent!) series in decreasing powers of N. Ļe fact that the series is
divergent is of little concern in practice because the ŀrst few terms give an
extremely accurate estimate, as shown in Table 4.3 and discussed in further
detail below. Now, the constant implicit in the O-notation means that, strictly
speaking, such a formula does not give complete information about a speciŀc
value of N, since the constant is arbitrary (or unspeciŀed). In principle, one
can always go to the source of the asymptotic series and prove speciŀc bounds
on the constant to overcome this objection. For example, it is possible to show
that
N! =
√
2πN
(N
e
)N(
1 + θN
12N
)
.
for all N > 1 where 0 < θN < 1 (see, for example, [1]). As in this example,
it is normally safe to assume that the constants implicit in the O-notation are
small and forgo the development of precise bounds on the error. Typically, if
more accuracy is desired, the next term in the asymptotic series will eventually
provide it, for large enough N.
Exercise 4.26 Use the nonasymptotic version of Stirling’s formula to give a bound
on the error made in estimating N4N−1/
(2N
N
)
with N
√
πN/4.
Absolute error. As deŀned earlier, a ŀnite asymptotic expansion has only one
O-term, and we will discuss here how to perform various standard manipu-
lations that preserve this property. If possible, we strive to express the ŀnal
answer in the form f(N) = g(N) + O(h(N)), so that the unknown error
N
HN
lnN
+γ
+ 1
2N
+
1
12N 2
10
2.9289683
2.3025851
2.8798008
2.9298008
2.9289674
100
5.1873775
4.6051702
5.1823859
5.1873859
5.1873775
1000
7.4854709
6.9077553
7.4849709
7.4854709
7.4854709
10,000
9.7876060
9.2103404
9.7875560
9.7876060
9.7876060
100,000
12.0901461
11.5129255
12.0901411
12.0901461
12.0901461
1,000,000
14.3927267
13.8155106
14.3927262
14.3927267
14.3927267
Table 4.4
Asymptotic estimates of the harmonic numbers

Șȝȝ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ș
represented by the O-notation becomes negligible in an absolute sense as N
increases (which means that h(N) = o(1)). In an asymptotic series, we get
more accurate estimates by including more terms in g(N) and taking smaller
h(N). For example, Table 4.4 shows how adding terms to the asymptotic
series for the harmonic numbers gives more accurate estimates. We show
how this series is derived later in this section. Like Stirling’s formula, it is a
divergent asymptotic series.
Relative error.
We can always express estimates in the alternative form
f(N) = g(N)(1 + O(h(N))), where h(N) = o(1). In some situations,
we have to be content with an absolute error that may increase with N. Ļe
relative error decreases as N increases, but the absolute error is not necessarily
“negligible” when trying to compute f(N). We often encounter this type of
estimate when f(N) grows exponentially. For example, Table 4.3 shows the
absolute and relative error in Stirling’s formula. Ļe logarithm of Stirling’s
expansion gives an asymptotic series for lnN! with very small absolute error,
as shown in Table 4.5.
We normally use the “relative error” formulation only when working
with quantities that are exponentially large in N, like N! or the Catalan num-
bers. In the analysis of algorithms, such quantities typically appear at inter-
mediate stages in the calculation; then operations such as dividing two such
quantities or taking the logarithm take us back into the realm of absolute error
for most quantities of interest in applications.
Ļis situation is normal when we use the cumulative counting method
for computing averages. For example, to ŀnd the number of leaves in binary
trees in Chapter 3, we counted the total number of leaves in all trees, then di-
N
lnN!
(N + 1
2)lnN −N + ln
√
2π +
1
12N
error
10
15.104413
15.104415
10−6
100
363.739375556
363.739375558
10−11
1000
5912.128178488163
5912.128178488166
10−15
10,000
82,108.9278368143533455
82,108.9278368143533458
10−19
Table 4.5
Absolute error in Stirling’s formula for lnN!

§ț.ș
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȝȞ
vided by the Catalan numbers. In that case, we could compute an exact result,
but for many other problems, it is typical to divide two asymptotic estimates.
Indeed, this example illustrates a primary reason for using asymptotics. Ļe
average number of nodes satisfying some property in a tree of, say, 1000 nodes
will certainly be less than 1000, and we may be able to use generating func-
tions to derive an exact formula for the number in terms of Catalan numbers
and binomial coeﬃcients. But computing that number (which might involve
multiplying and dividing numbers like 21000 or 1000!) might be a rather com-
plicated chore without asymptotics. In the next section, we show basic tech-
niques for manipulating asymptotic expansions that allow us to derive accu-
rate asymptotic estimates in such cases.
Table 4.6 gives asymptotic series for special number sequences that are
encountered frequently in combinatorics and the analysis of algorithms. Many
of these approximations are derived in this chapter as examples of manipulat-
ing and deriving asymptotic series. We refer to these expansions frequently
later in the book because the number sequences themselves arise naturally
when studying properties of algorithms, and the asymptotic expansions there-
fore provide a convenient way to accurately quantify performance character-
istics and appropriately compare algorithms.
Exercise 4.27 Assume that the constant C implied in the O-notation is less than
10 in absolute value. Give speciŀc bounds for H1000 implied by the absolute formula
HN = lnN +γ +O(1/N) and by the relative formula HN = lnN(1+O(1/logN)).
Exercise 4.28 Assume that the constant C implied in the O-notation is less than 10
in absolute value. Give speciŀc bounds for the 10th Catalan number implied by the
relative formula
1
N + 1
(2N
N
)
=
4N
√
πN 3
(
1 + O
( 1
N
))
.
Exercise 4.29 Suppose that f(N) admits a convergent representation
f(N) =
∑
k≥0
akN −k
for N ≥N0 where N0 is a ŀxed constant. Prove that, for any M > 0,
f(N) =
∑
0≤k<M
akN −k + O(N −M).
Exercise 4.30 Construct a function f(N) such that f(N) ∼
∑
k≥0
k!
N k .

Șȝȟ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ș
factorials (Stirling’s formula)
N! =
√
2πN
(N
e
)N(
1 +
1
12N +
1
288N 2 + O
( 1
N 3
))
lnN! =
(
N + 1
2
)
lnN −N + ln
√
2π +
1
12N + O
( 1
N 3
)
harmonic numbers
HN = lnN + γ + 1
2N −
1
12N 2 + O
( 1
N 4
)
binomial coeﬃcients (N
k
)
= N k
k!
(
1 + O
( 1
N
))
for k = O(1)
= 2N/2
√
πN
(
1 + O
( 1
N
))
for k = N
2 + O(1)
normal approximation to the binomial distribution
( 2N
N −k
) 1
22N = e−k2/N
√
πN
+ O
(
1
N 3/2
)
Poisson approximation to the binomial distribution
(N
k
)
pk(1 −p)N−k = λke−λ
k!
+ o(1)
for p = λ/N
Stirling numbers of the ŀrst kind
[N
k
]
∼(N −1)!
(k −1)! (lnN)k−1
for k = O
(
1
)
Stirling numbers of the second kind
{N
k
}
∼kN
k!
for k = O
(
1
)
Bernoulli numbers
B2N = (−1)N (2N)!
(2π)2N (−2 + O(4−N))
Catalan numbers
TN ≡
1
N + 1
(2N
N
)
=
4N
√
πN 3
(
1 + O
( 1
N
))
Fibonacci numbers
FN = ϕN
√
5 + O
(
ϕ−N)
where ϕ = 1+
√
5
2
Table 4.6
Asymptotic expansions for special numbers (N →∞)

§ț.Ț
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȝȠ
4.3 Manipulating Asymptotic Expansions. We use asymptotic series,
especially ŀnite expansions, not only because they provide a succinct way to
express approximate results with some control on accuracy, but also because
they are relatively easy to manipulate, and allow us to perform complicated
operations while still working with relatively simple expressions. Ļe reason
for this is that we rarely insist on maintaining the full asymptotic series for the
quantity being studied, only the ŀrst few terms of the expansion, so that we
are free to discard less signiŀcant terms each time we perform a calculation.
In practice, the result is that we are able to get accurate expressions describing
a wide variety of functions in a canonical form involving only a few terms.
BasicpropertiesoftheO-notation. A number of elementary identities, eas-
ily proven from the deŀnition, facilitate manipulating expressions involving
O-notation. Ļese are intuitive rules, some of which we have been implicitly
using already. We use an arrow to indicate that any expression containing
the left-hand side of one of these identities can be simpliŀed by using the
corresponding right-hand side, on the right-hand side of an equation:
f(N) →O(f(N))
cO(f(N)) →O(f(N))
O(cf(N)) →O(f(N))
f(N) −g(N) = O(h(N)) →f(N) = g(N) + O(h(N))
O(f(N))O(g(N)) →O(f(N)g(N))
O(f(N)) + O(g(N)) →O(g(N))
if f(N) = O(g(N)).
It is not strictly proper to use the O-notation on the left-hand side of an
equation. We do often write expressions like N2+N+O(1) = N2+O(N) =
O(N2) to avoid cumbersome formal manipulations with notations like the
arrow used above, but we would not use the equations N = O(N2) and
N2 = O(N2) to reach the absurd conclusion N = N2.
Ļe O-notation actually makes possible a wide range of ways of describ-
ing any particular function, but it is common practice to apply these rules
to write down simple canonical expressions without constants. We write
O(N2), never NO(N) or 2O(N2) or O(2N2), even though these are all
equivalent. It is conventional to write O(1) for an unspeciŀed constant, never
something like O(3). Also, we write O(logN) without specifying the base of
the logarithm (when it is a constant), since specifying the base amounts to
giving a constant, which is irrelevant because of the O-notation.

ȘȞȗ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ț
Manipulating asymptotic expansions generally reduces in a straight-
forward manner to the application of one of several basic operations, which
we consider in turn. In the examples, we will normally consider series with
one, two, or three terms (not counting the O-term). Of course, the methods
apply to longer series as well.
Exercise 4.31 Prove or disprove the following, for N →∞:
eN = O
(
N 2)
,
eN = O
(
2N)
,
2−N = O
( 1
N 10
)
,
and
N lnN = O
(
e(lnN)2)
.
Simpliŀcation. Ļe main principle that we must be cognizant of when doing
asymptotics is that an asymptotic series is only as good as its O-term, so any-
thing smaller (in an asymptotic sense) may as well be discarded. For example,
the expression lnN + O(1) is mathematically equivalent to the expression
lnN + γ + O(1), but simpler.
Substitution. Ļe simplest and most common asymptotic series derive from
substituting appropriately chosen variable values into Taylor series expansions
such as those in Table 4.2, or into other asymptotic series. For example, by
taking x = −1/N in the geometric series
1
1 −x = 1 + x + x2 + O(x3)
as x →0
we ŀnd that
1
N + 1 = 1
N −1
N2 + O
( 1
N3
)
as N →∞.
Similarly,
e1/N = 1 + 1
N +
1
2N2 +
1
6N3 + · · · +
1
k!Nk + O
(
1
Nk+1
)
.
Exercise 4.32 Give an asymptotic expansion for e1/(N+1) to within O
(
N −3)
.
Factoring.
In many cases, the “approximate” value of a function is obvi-
ous upon inspection, and it is worthwhile to rewrite the function making
this explicit in terms of relative or absolute error. For example, the function

§ț.Ț
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȞȘ
1/(N2+N) is obviously very close to 1/N2 for large N, which we can express
explicitly by writing
1
N2 + N =
1
N2
1
1 + 1/N
=
1
N2
(
1 + 1
N + O
( 1
N2
))
=
1
N2 + 1
N3 + O
( 1
N4
)
.
If we are confronted with a complicated function for which the approximate
value is not immediately obvious, then a short trial-and-error process might
be necessary.
Multiplication. Multiplying two asymptotic series is simply a matter of do-
ing the term-by-term multiplications, then collecting terms. For example,
(HN)2 =
(
lnN + γ + O
( 1
N
))(
lnN + γ + O
( 1
N
))
=
(
(lnN)2 + γlnN + O
(logN
N
))
+
(
γlnN + γ2 + O
( 1
N
))
+
(
O
(logN
N
)
+ O
( 1
N
)
+ O
( 1
N2
))
= (lnN)2 + 2γlnN + γ2 + O
(logN
N
)
.
In this case, the product has less absolute asymptotic “accuracy” than the
factors—the result is only accurate to within O(logN/N). Ļis is normal,
and we typically need to begin a derivation with asymptotic expansions that
have more terms than desired in the result. Often, we use a two-step pro-
cess: do the calculation, and if the answer does not have the desired accuracy,
express the original components more accurately and repeat the calculation.
Exercise 4.33 Calculate (HN)2 to within O
(
1/N
)
, then to within o(1/N).
For another example, we estimate N factorial squared:
N!N! =
(√
2πN
(N
e
)N(
1 + O
( 1
N
)))2
= 2πN
(N
e
)2N(
1 + O
( 1
N
))

ȘȞș
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ț
since
(
1 + O
( 1
N
))2 = 1 + 2O
( 1
N
)
+ O
( 1
N2
)
= 1 + O
( 1
N
)
.
Division.
To compute the quotient of two asymptotic series, we typically
factor and rewrite the denominator in the form 1/(1 −x) for some symbolic
expression x that tends to 0, then expand as a geometric series, and multiply.
For example, to compute an asymptotic expansion of tan x, we can divide the
series for sin x by the series for cos x, as follows:
tan x = sin x
cos x = x −x3/6 + O(x5)
1 −x2/2 + O(x4)
=
(
x −x3/6 + O(x5)
)
1
1 −x2/2 + O(x4)
=
(
x −x3/6 + O(x5)
)
(1 + x2/2 + O(x4))
= x + x3/3 + O(x5).
Exercise 4.34 Derive an asymptotic expansion for cot x to O
(
x4)
.
Exercise 4.35 Derive an asymptotic expansion for x/(ex −1) to O
(
x5)
.
For another example, consider approximating the middle binomial co-
eﬃcients
(2N
N
). We divide the series
(2N)! = 2
√
πN
(2N
e
)2N(
1 + O
( 1
N
))
by (from above)
N!N! = 2πN
(N
e
)2N(1 + O
( 1
N
))
to get the result
(
2N
N
)
= 22N
√
πN
(
1 + O
( 1
N
))
.
Multiplying this by 1/(N + 1) = 1/N −1/N2 + O(1/N3) gives the approx-
imation for the Catalan numbers in Table 4.6.

§ț.Ț
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȞȚ
Exponentiation/logarithm. Writing f(x) as exp{ln(f(x))} is often a conve-
nient start for doing asymptotics involving powers or products. For example,
an alternative way to derive the asymptotics of the Catalan numbers using the
Stirling approximation is to write
1
N + 1
(
2N
N
)
= exp{ln((2N)!) −2lnN! −ln(N + 1)}
= exp
{(
2N + 1
2
)
ln(2N) −2N + ln
√
2π + O
( 1
N
)
−2
(
N + 1
2
)
lnN + 2N −2ln
√
2π + O
( 1
N
)
−lnN + O
( 1
N
)}
= exp
{(
2N + 1
2
)
ln2 −3
2lnN −ln
√
2π + O
( 1
N
)}
which is again equivalent to the approximation in Table 4.6 for the Catalan
numbers.
Exercise 4.36 Carry out the expansion for the Catalan numbers to within O
(
N −4)
accuracy.
Exercise 4.37 Calculate an asymptotic expansion for
(3N
N
)
/(N + 1).
Exercise 4.38 Calculate an asymptotic expansion for (3N)!/(N!)3.
Another standard example of the exp/log manipulation is the following
approximation for e:
(
1 + 1
N
)N = exp
{
Nln
(
1 + 1
N
)}
= exp
{
N
( 1
N + O
( 1
N2
))}
= exp
{
1 + O
( 1
N
)}
= e + O
( 1
N
)
.
Ļe last step of this derivation is justiŀed in the next subsection. Again, we
can appreciate the utility of asymptotic analysis by considering how to com-
pute the value of this expression when (say) N is 1 million or 1 billion.

ȘȞț
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ț
Exercise 4.39 What is the approximate value of
(
1 −λ
N
)N
?
Exercise 4.40 Give a three-term asymptotic expansion of
(
1 −lnN
N
)N
.
Exercise 4.41 Suppose that interest on a bank account is “compounded daily”—that
is, 1/365 of the interest is added to the account each day, for 365 days. How much
more interest is paid in a year on an account with $10,000, at a 10% interest rate
compounded daily, as opposed to the $1000 that would be paid if interest were paid
once a year?
Composition. From substitution into the expansion of the exponential it is
obvious that
e1/N = 1 + 1
N + O
( 1
N2
)
,
but this is slightly diﬀerent from
eO(1/N) = 1 + O
( 1
N
)
,
which was assumed in the two derivations just given. In this case, substituting
O(1/N) into the expansion of the exponential is still valid:
eO(1/N) = 1 + O
( 1
N
)
+ O
((
O
( 1
N
))2)
= 1 + O
( 1
N
)
.
Since we usually deal with relatively short expansions, we can often develop
simple asymptotic estimates for functions with a rather complicated appear-
ance, just by power series substitution. Speciŀc conditions governing such
manipulations are given in [7].
Exercise4.42 Simplify the asymptotic expression exp{1+1/N +O
(
1/N 2)
} without
losing asymptotic accuracy.
Exercise 4.43 Find an asymptotic estimate for ln(sin((N!)−1)) to within O
(
1/N 2)
.
Exercise 4.44 Show that sin(tan(1/N)) ∼1/N and tan(sin(1/N)) ∼1/N. Ļen
ŀnd the order of growth of sin(tan(1/N)) −tan(sin(1/N)).
Exercise 4.45 Find an asymptotic estimate for HTN , where TN is the Nth Catalan
number, to within O
(
1/N
)
.

§ț.Ț
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȞȜ
Reversion. Suppose that we have an asymptotic expansion
y = x + c2x2 + c3x3 + O(x4).
We omit the constant term and the coeﬃcient of the linear term to simplify
calculations. Ļis expansion can be transformed into an equation expressing x
in terms of y through a bootstrapping process similar to that used to estimate
approximate solutions to recurrences in Chapter 2. First, we clearly must have
x = O(y)
because x/y = x/(x + c2x2 + O(x3)) is bounded as x →0. Substituting
into the original expansion, this means that y = x + O(y2), or
x = y + O(y2).
Substituting into the original expansion again, we have y = x + c2(y +
O(y2))2 + O(y3), or
x = y −c2y2 + O(y3).
Each time we substitute back into the original expansion, we get another
term. Continuing, we have y = x + c2(y −c2y2 + O(y3))2 + c3(y −c2y2 +
O(y3))3 + O(y4), or
x = y −c2y2 + (2c2
2 −c3)y3 + O(y4).
Exercise 4.46 Let an be deŀned as the unique positive root of the equation
n = anean
for n > 1. Find an asymptotic estimate for an, to within O
(
1/(logn)3)
.
Exercise 4.47 Give the reversion of the power series
y = c0 + c1x + c2x2 + c3x3 + O(x4).
(Hint : Take z = (y −c0)/c1.)

ȘȞȝ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ț
4.4 Asymptotic Approximations of Finite Sums. Frequently, we are
able to express a quantity as a ŀnite sum, and therefore we need to be able to
accurately estimate the value of the sum. As we saw in Chapter 2, some sums
can be evaluated exactly, but in many more cases, exact values are not avail-
able. Also, it may be the case that we only have estimates for the quantities
themselves being summed.
In [6], De Bruijn considers this topic in some detail. He outlines a
number of diﬀerent cases that frequently arise, oriented around the observa-
tion that it is frequently the case that the terms in the sum vary tremendously
in value. We brieły consider some elementary examples in this section, but
concentrate on the Euler-Maclaurin formula, a fundamental tool for estimat-
ing sums with integrals. We show how the Euler-Maclaurin formula gives
asymptotic expansions for the harmonic numbers and factorials (Stirling’s for-
mula).
We consider a number of applications of Euler-Maclaurin summation
throughout the rest of this chapter, particularly concentrating on summands
involving classical “bivariate” functions exempliŀed by binomial coeﬃcients.
As we will see, these applications are predicated upon estimating summands
diﬀerently in diﬀerent parts of the range of summation, but they ultimately
depend on estimating a sum with an integral by means of Euler-Maclaurin
summation. Many more details on these and related topics may be found in
[2], [3], [6], [12], and [19].
Bounding the tail. When the terms in a ŀnite sum are rapidly decreasing,
an asymptotic estimate can be developed by approximating the sum with an
inŀnite sum and developing a bound on the size of the inŀnite tail. Ļe fol-
lowing classical example, which counts the number of permutations that are
“derangements” (see Chapter 6), illustrates this point:
N!
∑
0≤k≤N
(−1)k
k!
= N!e−1 −RN
where
RN = N!
∑
k>N
(−1)k
k!
.
Now we can bound the tail RN by bounding the individual terms:
|RN| <
1
N + 1 +
1
(N + 1)2 +
1
(N + 1)3 + . . . = 1
N
so that the sum is N!e−1 + O(1/N). In this case, the convergence is so rapid
that it is possible to show that the value is always equal to N!e−1 rounded to
the nearest integer.

§ț.ț
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȞȞ
Ļe inŀnite sum involved converges to a constant, but there may be no
explicit expression for the constant. However, the rapid convergence normally
means that it is easy to calculate the value of the constant with great accuracy.
Ļe following example is related to sums that arise in the study of tries (see
Chapter 7):
∑
1≤k≤N
1
2k −1 =
∑
k≥1
1
2k −1 −RN,
where
RN =
∑
k>N
1
2k −1.
In this case, we have
0 < RN <
∑
k>N
1
2k−1 =
1
2N−1
so that the constant 1 + 1/3 + 1/7 + 1/15 + . . . = 1.6066 · · · is an extremely
good approximation to the ŀnite sum. It is a trivial matter to calculate the
value of this constant to any reasonable desired accuracy.
Using the tail. When the terms in a ŀnite sum are rapidly increasing, the last
term often suﬃces to give a good asymptotic estimate for the whole sum. For
example,
∑
0≤k≤N
k! = N!
(
1 + 1
N +
∑
0≤k≤N−2
k!
N!
)
= N!
(
1 + O
( 1
N
))
.
Ļe latter equality follows because there are N −1 terms in the sum, each less
than 1/(N(N −1)).
Exercise 4.48 Give an asymptotic estimate for ∑
1≤k≤N 1/(k2Hk).
Exercise 4.49 Give an asymptotic estimate for ∑
0≤k≤N 1/Fk.
Exercise 4.50 Give an asymptotic estimate for ∑
0≤k≤N 2k/(2k + 1).
Exercise 4.51 Give an asymptotic estimate for ∑
0≤k≤N 2k2.
Approximating sums with integrals.
More generally, we expect that we
should be able to estimate the value of a sum with an integral and to take
advantage of the wide repertoire of known integrals.

ȘȞȟ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ț
What is the magnitude of the error made when we use
∫b
a
f(x)dx
to estimate
∑
a≤k<b
f(k)?
Ļe answer to this question depends on how “smooth” the function f(x) is.
Essentially, in each of the b −a unit intervals between a and b, we are using
f(k) to estimate f(x). Letting
δk =
max
k≤x<k+1 |f(x) −f(k)|
denote the maximum error in each interval, we can get a rough approximation
to the total error:
∑
a≤k<b
f(k) =
∫b
a
f(x)dx + ,
with || ≤
∑
a≤k<b
δk.
If the function is monotone increasing or decreasing over the whole inter-
val [a, b], then the error term telescopes to simply  ≤|f(a) −f(b)|. For
example, for the harmonic numbers, this gives the estimate
HN =
∑
1≤k≤N
1
k =
∫N
1
1
xdx +  = lnN + 
with || ≤1 −1/N, an easy proof that HN ∼lnN; and for N! this gives
the estimate
lnN! =
∑
1≤k≤N
lnk =
∫N
1
lnx dx +  = NlnN −N + 1 + 
with || ≤lnN for lnN!, an easy proof that lnN! ∼NlnN −N. Ļe
accuracy in these estimates depends on the care taken in approximating the
error.
More precise estimates of the error terms depend on the derivatives of
the function f. Taking these into account leads to an asymptotic series derived
using the Euler-Maclaurin summation formula, one of the most powerful tools
in asymptotic analysis.

§ț.Ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȞȠ
4.5 Euler-Maclaurin Summation. In the analysis of algorithms, we
approximate sums with integrals in two distinct ways. In the ŀrst case, we
have a function deŀned on a ŀxed interval, and we evaluate a sum corre-
sponding to sampling the function at an increasing number of points along
the interval, with smaller and smaller step sizes, with the diﬀerence between
the sum and the integral converging to zero. Ļis is akin to classical Riemann
integration. In the second case, we have a ŀxed function and a ŀxed discrete
step size, so the interval of integration gets larger and larger, with the diﬀer-
ence between the sum and the integral converging to a constant. We consider
these two cases separately, though they both embody the same basic method,
which dates back to the 18th century.
General form for Euler-Maclaurin summation formula. Ļe method is
based on integration by parts, and involves Bernoulli numbers (and Bernoulli
polynomials), which are described in §3.11. We start from the formula
∫1
0
g(x)dx =
(
x −1
2
)
g(x)

1
0 −
∫1
0
(
x −1
2
)
g′(x)dx,
which is obtained by partial integration of g(x) with the “clever” choice of
the integration constant in x −1
2 = B1(x). Using this formula with g(x) =
f(x + k), we get
∫k+1
k
f(x)dx = f(k + 1) + f(k)
2
−
∫k+1
k
(
{x} −1
2
)
f′(x)dx
where, as usual, {x} ≡x −⌊x⌋denotes the fractional part of x. Taking
all values of k greater than or equal to a and less than b and summing these
formulae gives
∫b
a
f(x)dx =
∑
a≤k≤b
f(k) −f(a) + f(b)
2
−
∫b
a
(
{x} −1
2
)
f′(x)dx
because f(k) appears in two formulae for each value of k except a and b.
Ļus, rearranging terms, we have a precise relationship between a sum and
the corresponding integral:
∑
a≤k≤b
f(k) =
∫b
a
f(x)dx + f(a) + f(b)
2
+
∫b
a
(
{x} −1
2
)
f′(x)dx.

Șȟȗ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ȝ
To know how good an approximation this is, we need to be able to develop a
bound for the integral at the end. We could do so by developing an absolute
bound as at the end of the previous section, but it turns out that we can iterate
this process, often leaving a very small error term because the derivatives of
f(x) tend to get smaller and smaller (as functions of N) and/or because the
polynomial in {x} that is also involved in the integral becomes smaller and
smaller.
Ļeorem 4.2 (Euler-Maclaurin summation formula, ŀrst form).
Let f(x)
be a function deŀned on an interval [ a, b ] with a and b integers, and suppose
that the derivatives f(i)(x) exist and are continuous for 1 ≤i ≤2m, where
m is a ŀxed constant. Ļen
∑
a≤k≤b
f(k) =
∫b
a
f(x)dx + f(a) + f(b)
2
+
∑
1≤i≤m
B2i
(2i)!f(2i−1)(x)

b
a +Rm,
where B2i are the Bernoulli numbers and Rm is a remainder term satisfying
|Rm| ≤|B2m|
(2m)!
∫b
a
|f(2m)(x)|dx <
4
(2π)2m
∫b
a
|f(2m)(x)|dx.
Proof. We continue the argument above, using integration by parts and basic
properties of the Bernoulli polynomials. For any function g(x) that is dif-
ferentiable in [ 0, 1) and any i > 0, we can integrate g(x)B′
i+1(x) by parts to
get
∫1
0
g(x)B′
i+1(x)dx = Bi+1(x)g(x)

1
0 −
∫1
0
g′(x)Bi+1(x)dx.
Now, from §3.11, we know that B′
i+1(x) = (i + 1)Bi(x) so, dividing by
(i + 1)!, we get a recurrence relation:
∫1
0
g(x)Bi(x)
i!
dx = Bi+1(x)
(i + 1)!g(x)

1
0 −
∫1
0
g′(x)Bi+1(x)
(i + 1)!dx.
Now, starting at i = 0 and iterating, this gives, formally,
∫1
0
g(x)dx = B1(x)
1!
g(x)

1
0 −B2(x)
2!
g′(x)

1
0 + B3(x)
3!
g′′(x)

1
0 −. . .
where the expansion can be pushed arbitrarily far for functions that are in-
ŀnitely diﬀerentiable. More precisely, we stop the iteration after m steps, and

§ț.Ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȟȘ
also note that B1(x) = x−1
2 and Bi(0) = Bi(1) = Bi for i > 1 with Bi = 0
for i odd and greater than 1 (see Exercises 3.86 and 3.87) to get the formula
∫1
0
g(x)dx = g(0) + g(1)
2
−
∑
1≤i≤m
B2i
(2i)!g2i−1(x)

1
0 −
∫1
0
g2m(x)B2m(x)
(2m)! dx.
Substituting g(x) = f(x + k) and summing for a ≤k < b, this telescopes
to the stated result with remainder term
|Rm| =
∫b
a
B2m({x})
(2m)!
f(2m)(x)
dx
in the same way as shown earlier. Ļe stated bound on the remainder term
follows from asymptotic properties of the Bernoulli numbers. (See De Bruijn
[6] or Graham, Knuth, and Patashnik [12] for more details.)
For example, taking f(x) = ex, the left-hand side is (eb −ea)/(e −1),
and all the derivatives are the same on the right-hand side, so we can divide
through by eb −ea and increase m to conŀrm that 1/(e −1) = ∑
k Bk/k!.
Since the Bernoulli numbers grow to be quite large, this formula is a
often a divergent asymptotic series, typically used with small values of m. Ļe
ŀrst few Bernoulli numbers B0 = 1, B1 = −1/2, B2 = 1/6, B3 = 0, and
B4 = −1/30 suﬃce for typical applications of Ļeorem 4.2. We write the
simpler form
∑
a≤k≤b
f(k) =
∫b
a
f(x)dx+1
2f(a)+1
2f(b)+ 1
12f′(x)

b
a −1
720f′′′(x)

b
a + . . .
with the understanding that the conditions of the theorem and the error
bound have to be checked for the approximation to be valid.
Taking f(x) = xt, the derivatives and remainder term vanish for large
enough m, conŀrming the Bernoulli numbers as coeﬃcients in expressing
sums of powers of integers. We have
∑
1≤k≤N
k = N2
2 + N
2 = N(N + 1)
2
,
∑
1≤k≤N
k2 = N3
3 + N2
2 + N
6 = N(N + 1)(2N + 1)
6
,
and so forth, precisely as in §3.11.

Șȟș
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ȝ
Exercise 4.52 Use Euler-Maclaurin summation to determine the coeﬃcients when
∑
1≤k≤N kt is expressed as a sum of powers of N (see §3.11).
Corollary If h is an inŀnitely diﬀerentiable function, then
∑
0≤k≤N
h(k/N) ∼N
∫1
0
h(x)dx+h(0) + h(1)
2
+
∑
i≥1
B2i
(2i)!
1
N2i−1 h(2i−1)(x)|1
0.
Proof. Apply Ļeorem 4.2 with f(x) = h(x/N).
Dividing by N gives a Riemann sum relative to h. In other words, this
corollary is a reŀnement of
lim
N→∞
1
N
∑
0≤k≤N
h(k/N) =
∫1
0
h(x)dx.
Euler-Maclaurin summation will be eﬀective for obtaining asymptotic ex-
pansions for related sums of this type, such as
∑
0≤k≤N
h(k2/N).
We will be seeing applications of this shortly.
Exercise 4.53 Develop an asymptotic expansion for
∑
0≤k≤N
1
1 + k/N .
Exercise 4.54 Show that
∑
0≤k≤N
1
1 + k2/N 2 = πN
4
+ 3
4 −
1
24N + O
( 1
N 2
)
.
As stated, Ļeorem 4.2 will not provide suﬃciently accurate estimates
when the interval of summation/integration grows and the step size is ŀxed.
For example, if we try to estimate
Hk =
∑
1≤k≤N
1
k
with
∫b
a
f(x)dx,
we encounter a diﬃculty because the diﬀerence between the sum and the
integral tends to an unknown constant as N →∞. Next, we turn to a form
of Euler-Maclaurin summation that addresses this problem.

§ț.Ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȟȚ
Discrete form of Euler-Maclaurin summation. Taking a = 1 and b = N
in the discussion preceding Ļeorem 4.2 gives
∫N
1
f(x)dx =
∑
1≤k≤N
f(k) −1
2(f(1) + f(N)) −
∫N
1
(
{x} −1
2
)
f′(x)dx.
Ļis formula relates the sum and the integral up to a constant factor if f′(x)
has a fast enough decay to 0 as N →∞. In particular, if the quantity
Cf = 1
2f(1) +
∫∞
1
(
{x} −1
2
)
f′(x)dx
exists, it deŀnes the Euler-Maclaurin constant of f, and we have proved that
lim
N→∞
(
∑
1≤k≤N
f(k) −
∫N
1
f(x)dx −1
2f(N)
)
= Cf.
Taking f(x) = 1/x, gives an approximation for the harmonic num-
bers. Ļe Euler-Maclaurin constant for this case is known plainly as Euler ’s
constant:
γ = 1
2 −
∫∞
1
(
{x} −1
2
)dx
x2 .
Ļus
HN = lnN + γ + o(1).
Ļe constant γ is approximately .57721 · · · and is not known to be a simple
function of other fundamental constants.
Taking f(x) = lnx gives Stirling’s approximation to lnN!. In this case,
the Euler-Maclaurin constant is
∫∞
1
(
{x} −1
2
)dx
x .
Ļis constant does turn out to be a simple function of other fundamental con-
stants: it is equal to ln
√
2π −1. We will see one way to prove this in §4.7.
Ļe value σ =
√
2π is known as Stirling ’s constant. It arises frequently in the
analysis of algorithms and many other applications. Ļus
lnN! = NlnN −N + 1
2lnN + ln
√
2π + o(1).

Șȟț
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ȝ
When the Euler-Maclaurin constant is well deŀned, pursuing the anal-
ysis to obtain more terms in the asymptotic series is relatively easy. Summa-
rizing the preceding discussion, we have shown that
∑
1≤k≤N
f(k) =
∫N
1
f(x)dx + 1
2f(N) + Cf −
∫∞
N
(
{x} −1
2
)
f′(x)dx.
Now, repeatedly integrating the remaining integral by parts, in the same fash-
ion as earlier, we get an expansion involving Bernoulli numbers and higher-
order derivatives. Ļis often leads to a complete asymptotic expansion because
it is a common fact that the high-order derivatives of functions of smooth be-
havior get smaller and smaller at ∞.
Ļeorem 4.3 (Euler-Maclaurin summation formula, second form).
Let
f(x) be a function deŀned on the interval [ 1, ∞) and suppose that the deriva-
tives f(i)(x) exist and are absolutely integrable for 1 ≤i ≤2m, where m is a
ŀxed constant. Ļen
∑
1≤k≤N
f(k) =
∫N
1
f(x)dx+ 1
2f(N)+Cf +
∑
1≤k≤m
B2k
(2k)!f(2k−1)(N)+Rm,
where Cf is a constant associated with the function and R2m is a remainder
term satisfying
|R2m| = O
(∫∞
N
|f(2m)(x)|dx
)
.
Proof. By induction, extending the argument in the earlier discussion. Details
may be found in [6] or [12].
Corollary Ļe harmonic numbers admit a full asymptotic expansion in de-
scending powers of N:
HN ∼lnN + γ + 1
2N −
1
12N2 +
1
120N4 −. . . .

§ț.Ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȟȜ
Proof. Take f(x) = 1/x in Ļeorem 4.3, use the constant γ discussed earlier
and note that the remainder term is of the same order as the last term in the
sum, to show that
HN = lnN + γ + 1
2N −
∑
1≤k<M
B2k
2kN2k + O
(
1
N2m
)
for any ŀxed m, which implies the stated result.
Corollary (Stirling’s formula.) Ļe functions lnN! and N! admit a full asymp-
totic expansion in descending powers of N:
lnN! ∼
(
N + 1
2
)
lnN −N + ln
√
2π +
1
12N −
1
360N3 + . . .
and
N! ∼
√
2πN
(N
e
)N(
1 +
1
12N +
1
288N2 −
139
5140N3 + . . .
)
.
Proof. Take f(x) = lnx in Ļeorem 4.3 and argue as above to develop the
expansion for lnN!. As mentioned earlier, the ŀrst derivative is not abso-
lutely integrable, but the Euler-Maclaurin constant exists, so Ļeorem 4.3
clearly holds. Ļe expansion for N! follows by exponentiation and basic ma-
nipulations discussed in §4.3.
Corollary Ļe Catalan numbers admit a full asymptotic expansion in de-
scending powers of N:
1
N + 1
(
2N
N
)
∼
4N
N
√
πN
(
1 −9
8N +
145
128N2 −. . .
)
.
Proof. Ļis follows from elementary manipulations with the asymptotic series
for (2N)! and N!. Many of the details are given in the examples in §4.3.
Euler-Maclaurin summation is a general tool, which is useful only sub-
ject to the caveats that the function must be “smooth” (as many derivatives
must exist as terms in the asymptotic series desired), and we must be able
to calculate the integral involved. Ļe asymptotic expansions just given for
factorials, harmonic numbers, and Catalan numbers play a central role in the
analysis of many fundamental algorithms, and the method arises in many
other applications.

Șȟȝ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ȝ
Exercise 4.55 Evaluate γ to 10 decimal places.
Exercise 4.56 Show that the generalized (second-order) harmonic numbers admit
the asymptotic expansion
H(2)
N ≡
∑
1≤k≤N
1
k2 ∼π2
6 −1
N +
1
2N 2 −
1
6N 3 + . . . .
Exercise 4.57 Derive an asymptotic expansion for
H(3)
N ≡
∑
1≤k≤N
1
k3
to within O
(
N 3)
.
Exercise 4.58 Use Euler-Maclaurin summation to estimate
∑
1≤k≤N
√
k,
∑
1≤k≤N
1
√
k
,
and
∑
1≤k≤N
1
3√
k
to within O
(
1/N 2)
.
Exercise 4.59 Derive full asymptotic expansions for
∑
1≤k≤N
(−1)k
k
and
∑
1≤k≤N
(−1)k
√
k
.
Exercise 4.60 Ļe Gamma function is deŀned for positive real x as follows:
Γ(x) =
∫∞
0
e−ttx−1dt.
Since Γ(1) = 1 and Γ(x + 1) = xΓ(x) (integration by parts), Γ(n) = (n −1)!
for positive integers n. In other words, the Gamma function extends the factorial
to the reals (and eventually to the complex plane, except for negative integers). Use
Euler-Maclaurin summation to approximate the generalized binomial coeﬃcient that
follows immediately. Speciŀcally, show that
(n + α
n
)
≡
Γ(n + α + 1)
Γ(n + 1)Γ(α + 1) ∼
nα
Γ(α + 1).

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȟȞ
4.6 Bivariate Asymptotics. Many of the most challenging problems that
we face in approximating sums have to do with so-called bivariate asymp-
totics, where the summands depend both on the index of summation and on
the “size” parameter that describes asymptotic growth. Suppose that the two
parameters are named k and N, respectively, as we have done many times.
Now, the relative values of k and N and the rate at which they grow certainly
dictate the signiŀcance of asymptotic estimates. For a simple example of this,
note that the function kN/k! grows exponentially as N →∞for ŀxed k, but
is exponentially small as k →∞for ŀxed N.
In the context of evaluating sums, we generally need to consider all val-
ues of k less than N (or some function of N), and therefore we are interested
in developing accurate asymptotic estimates for as large a range of k as possi-
ble. Evaluating the sum eliminates the k and leaves us back in the univariate
case. In this section we will conduct a detailed examination of some bivari-
ate functions of central importance in the analysis of algorithms: Ramanujan
functions and binomial coeﬃcients. In subsequent sections, we will see how
the estimates developed here are used to obtain asymptotic approximations
to sums involving these functions, and how these relate to applications in the
analysis of algorithms.
Ramanujan distributions. Our ŀrst example concerns a distribution that
was ŀrst studied by Ramanujan (see [4]) and later, because it arises in so many
applications in the analysis of algorithms, by Knuth [16]. As we will see in
Chapter 9, the performance characteristics of a variety of algorithms depend
on the function
Q(N) ≡
∑
1≤k≤N
N!
(N −k)!Nk .
Ļis function is also well known in probability theory as the birthday function:
Q(N) + 1 is the expected number of trials needed to ŀnd two people with
the same birthday (when the year has N days). Ļe summand is tabulated
for small values of N and k in Table 4.7 and plotted in Figure 4.1. In the
ŀgure, separate curves are given for each value of N, with successive values of
k connected by straight lines. Ļe k-axis for each curve is scaled so that the
curve ŀlls the whole ŀgure. In order to be able to estimate the value of the
sum, we ŀrst need to be able to estimate the value of the summand accurately
for all values of k, as N grows.

Șȟȟ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
Ļeorem 4.4 (Ramanujan Q-distribution).
As N →∞, the following
(relative) approximation holds for k = o(N2/3):
N!
(N −k)!Nk = e−k2/(2N)(
1 + O
( k
N
)
+ O
( k3
N2
))
.
In addition, the following (absolute) approximation holds for all k:
N!
(N −k)!Nk = e−k2/(2N) + O
( 1
√
N
)
.
Proof. Ļe relative error bound is proved with the “exp/log” technique given
in §4.3. We write
N!
(N −k)!Nk = N(N −1)(N −2) . . . (N −k + 1)
Nk
= 1 ·
(
1 −1
N
)(
1 −2
N
)
· · ·
(
1 −1
N
)
= exp
{
ln
(
1 −1
N
)(
1 −2
N
)
· · ·
(
1 −1
N
)}
= exp
{ ∑
1≤j<k
ln
(
1 −j
N
)}
.
N
↓k →1
2
3
4
5
6
7
8
9
10
2
1 .5000
3
1 .6667 .2222
4
1 .7500 .3750 .0938
5
1 .8000 .4800 .1920 .0384
6
1 .8333 .5556 .2778 .0926 .0154
7
1 .8571 .6122 .3499 .1499 .0428 .0061
8
1 .8750 .6563 .4102 .2051 .0769 .0192 .0024
9
1 .8889 .6914 .4609 .2561 .1138 .0379 .0084 .0009
10
1 .9000 .7200 .5040 .3024 .1512 .0605 .0181 .0036 .0004
Table 4.7
Ramanujan Q-distribution
N!
(N −k)!Nk

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȟȠ
Now, for k = o(N), we can apply the approximation
ln(1 + x) = x + O(x2)
with x = −j/N
from Table 4.2 and evaluate the sum:
N!
(N −k)!Nk = exp
{ ∑
1≤j<k
(
−j
N + O
( j2
N2
))}
= exp
{
−k(k −1)
2N
+ O
( k3
N2
)}
.
Finally, for k = o(N2/3) we can use the approximation ex = 1 + O(x) from
Table 4.2 to get the stated relative approximation.
We need to carry both O-terms to cover the range in values of k. Ļe
O(k3/N2) term is not suﬃcient by itself because, for example, it is O(1/N2)
0
.5
.25
.75
1
0
Figure 4.1
N/2
N
N!
(N −k)!Nk
Ramanujan Q-distribution, 2 ≤N ≤60 (k-axes scaled to N)

ȘȠȗ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
for k = O(1), when O(1/N) is called for. Ļe O(k/N) term is not suﬃ-
cient by itself because, for example, it is O(1/N2/5) for k = O(N3/5), when
O(1/N1/5) is called for. Now, N1/5 is quite small for any practical value of
N, and the precise value 1/5 is not particularly critical, but we need to choose
some cutoﬀto be able to discard a large number of terms that are even smaller.
Ļis situation illustrates the care that is necessary in bivariate asymptotics.
To develop the absolute error bound, we ŀrst consider the case where k
is “small,” say k ≤k0 where k0 is the nearest integer to N3/5. Ļe relative
approximation certainly holds, and we have
N!
(N −k)!Nk = e−k2/(2N) + e−k2/(2N)O
( k
N
)
+ e−k2/(2N)O
( k3
N2
)
.
Now, the second term is O(1/
√
N ) because we can rewrite it in the form
xe−x2O(1/
√
N ) and xe−x2 = O(1) for all x ≥0. Similarly, the third term
is of the form x3e−x2O(1/
√
N ) and is O(1/
√
N ) because x3e−x2 = O(1)
for all x ≥0.
Next, consider the case where k is “large,” or k ≥k0. Ļe argument just
given shows that
N!
(N −k0)!Nk0 = e−5√
N/2 + O
( 1
√
N
)
.
Ļe ŀrst term is exponentially small, and the coeﬃcients decrease as k in-
creases, so this implies that
N!
(N −k)!Nk = O
( 1
√
N
)
for k ≥k0. But exp{−k2/(2N)} is also exponentially small for k ≥k0, so
the absolute error bound in the statement holds for k ≥k0.
Ļe above two paragraphs establish the absolute error bound for all k ≥
0. As mentioned above, the cutoﬀpoint N3/5 is not particularly critical in
this case: it need only be small enough that the relative error bound holds
for smaller k (slightly smaller than N2/3) and large enough that the terms are
exponentially small for larger k (slightly larger than
√
N ).

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȠȘ
Corollary For all k and N,
N!
(N −k)!Nk ≤e−k(k−1)/(2N).
Proof. Use the inequality ln(1 −x) ≤−x instead of the asymptotic estimate
in the preceding derivation.
A virtually identical set of arguments apply to another function studied
by Ramanujan, the so-called R-distribution. Ļis function is tabulated for
small values of N and k in Table 4.8 and plotted in Figure 4.2. We include
a detailed statement of the asymptotic results for this function as well, for
reasons that will become clear later.
Ļeorem 4.5 (Ramanujan R-distribution).
As N →∞, the following
(relative) approximation holds for k = o(N2/3):
N!Nk
(N + k)! = e−k2/(2N)(
1 + O
( k
N
)
+ O
( k3
N2
))
.
In addition, the following (absolute) approximation holds for all k:
N!Nk
(N + k)! = e−k2/(2N) + O
( 1
√
N
)
.
N
↓k →
1
2
3
4
5
6
7
8
9
2
.6667 .3333
3
.7500 .4500 .2250
4
.8000 .5333 .3048 .1524
5
.8333 .5952 .3720 .2067 .1033
6
.8571 .6429 .4286 .2571 .1403 .0701
7
.8750 .6806 .4764 .3032 .1768 .0952 .0476
8
.8889 .7111 .5172 .3448 .2122 .1212 .0647 .0323
9
.9000 .7364 .5523 .3823 .2458 .1475 .0830 .0439 .0220
Table 4.8
Ramanujan R-distribution
N!Nk
(N + k)!

ȘȠș
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
Proof. After the ŀrst step, the proof is virtually identical to the proof for the
Q-distribution:
N!Nk
(N −k)! =
Nk
(N + k)(N + k −1) . . . (N + 1)
=
1
(
1 + k
N
)(
1 + k −1
N
)
· · ·
(
1 + 1
N
)
= exp
{
−
∑
1≤j≤k
ln
(
1 + j
N
)}
= exp
{ ∑
1≤j≤k
(
−j
N + O
( j2
N2
))}
= exp
{
−k(k + 1)
2N
+ O
( k3
N2
)}
= e−k2/(2N)(
1 + O
( k
N
)
+ O
( k3
N2
))
.
Ļe absolute error bound follows as for the Q-distribution.
Corollary For all k and N with k ≤N,
N!Nk
(N + k)! ≤e−k(k+1)/(4N).
Proof. Use the inequality −ln(1 + x) ≤−x/2 (valid for 0 ≤x ≤1) instead
of the asymptotic estimate in the derivation above.
Exercise 4.61 Prove that
N!N k
(N + k)! ≥e−k(k+1)/(2N) for all k and N.
We will return to the Ramanujan distributions for several applications
later in this chapter. Before doing so, however, we turn our attention to a
bivariate distribution that plays an even more central role in the analysis of
algorithms, the familiar binomial distribution. It turns out that the develop-
ment of asymptotic approximations for the Ramanujan distributions given
earlier encapsulates the essential aspects of approximating the binomial dis-
tribution.
Exercise 4.62 Use Stirling’s formula for lnN! to prove the relative bounds for the
Ramanujan Q- and R-distributions given in Ļeorem 4.4 and Ļeorem 4.5, respec-
tively.

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȠȚ
0
.5
.25
.75
1
0
Figure 4.2
N/2
N
N!Nk
(N + k)!
Ramanujan R-distribution, 2 ≤N ≤60 (k-axes scaled to N)
Binomial distribution. Given N random bits, the probability that exactly k
of them are 0 is the familiar binomial distribution, also known as the Bernoulli
distribution:
1
2N
(
N
k
)
= 1
2N
N!
k!(N −k)!.
Ļe interested reader may consult Feller’s classic text [8] or any of a number of
standard references for basic information on properties of this distribution and
applications in probability theory. Since it appears frequently in the analysis
of algorithms, we summarize many of its important properties here.
Table 4.9 gives exact values of the binomial distribution for small N and
approximate values for larger N. As usual, one motivation for doing asymp-
totic analysis of this function is the desire to compute such values. Ļe value
of
(10000
5000
)/25000 is about .007979, but one would not compute that by ŀrst
computing 10000!, then dividing by 5000!, and so on. Indeed, this distribu-
tion has been studied for three centuries, and the motivation for ŀnding easily

ȘȠț
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
computed approximations was present well before computers arrived on the
scene.
Exercise 4.63 Write a program to compute exact values of the binomial distribution
to single-precision łoating point accuracy.
We have already computed the approximate value of the middle bino-
mial coeﬃcients:
(
2N
N
)
= 22N
√
πN
(
1 + O
( 1
N
))
.
Ļat is, the middle entries in Table 4.9 decrease like 1/
√
N. How does the
distribution behave for other values of k? Figure 4.3 shows a scaled version of
the distribution that gives some indication, and a precise asymptotic analysis
is given here.
Ļe limiting curve is the familiar “bell curve” described by the nor-
mal probability density function e−x2/2/
√
2π. Ļe top of the curve is at
(
N
⌊N/2⌋
)/2N ∼1/
√
πN/2, which is about .103 for N = 60.
Our purpose here is to analyze properties of the bell curve, using the
asymptotic tools that we have been developing. Ļe results that we present
are classical, and play a central role in probability and statistics. Our interest
N
↓k →
0
1
2
3
4
5
6
7
8
9
1
.5000 .5000
2
.2500 .5000 .2500
3
.1250 .3750 .3750 .1250
4
.0625 .2500 .3750 .2500 .0625
5
.0312 .1562 .3125 .3125 .1562 .0312
6
.0156 .0938 .2344 .3125 .2344 .0938 .0156
7
.0078 .0547 .1641 .2734 .2734 .1641 .0547 .0078
8
.0039 .0312 .1094 .2188 .2734 .2188 .1094 .0312 .0039
9
.0020 .0176 .0703 .1641 .2461 .2461 .1641 .0703 .0176 .0020
Table 4.9
Binomial distribution
(
N
k
)
/2N

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȠȜ
0
.5
.25
.125
.103
0
Figure 4.3
N/2
N
1
2N
(
N
k
)
Binomial distribution, 2 ≤N ≤60 (k-axes scaled to N)
in these results stems not only from the fact that they are directly useful in the
analysis of many algorithms, but also from the fact that the techniques used
to develop asymptotic estimates for the binomial distribution are of direct use
in a plethora of similar problems that arise in the analysis of algorithms. For
a treatment of the normal approximation in the context of probability and
statistics, see, for example, Feller [8].
Figure 4.3 makes it plain that the most signiŀcant part of the curve is
near the center—as N increases, the values near the edge become negligible.
Ļis is intuitive from the probabilistic model that we started with: we expect
the number of 0s and 1s in a random bitstream to be about equal, and the
probability that the bits are nearly all 0 or nearly all 1 to become vanishingly
small as the size of the bitstream increases. We now turn to quantifying such
statements more precisely.
Normal approximation.
Since the signiŀcant values of the binomial dis-
tribution are near the center, it is convenient to rewrite it and consider es-

ȘȠȝ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
timating
( 2N
N−k
)/22N. Ļis is symmetric about k = 0 and decreases as |k|
increases from 0 to N. Ļis is an important step in working with any distri-
bution: putting the largest terms at the beginning and the small terms in the
tails makes it more convenient to bound the tails and concentrate on the main
terms, particularly when using the approximation in evaluating sums, as we
will see here.
As it turns out, we have already seen the basic methods required to prove
the classic normal approximation to the binomial distribution.
Ļeorem 4.6 (Normal approximation).
As N →∞, the following (rela-
tive) approximation holds for k = o(N3/4):
1
22N
(
2N
N −k
)
= e−k2/N
√
πN
(
1 + O
( 1
N
)
+ O
( k4
N3
))
.
In addition, the following (absolute) approximation holds for all k:
1
22N
(
2N
N −k
)
= e−k2/N
√
πN
+ O
(
1
N3/2
)
.
Proof. If we write
1
22N
(
2N
N −k
)
=
1
22N
(2N)!
N!N!
N!
(N −k)!Nk
N!Nk
(N + k)!
we see that the binomial distribution is precisely the product of
1
22N
(
2N
N
)
=
1
√
πN
(
1 + O
( 1
N
))
,
the Ramanujan Q-distribution, and the Ramanujan R-distribution (!). Accord-
ingly, we can obtain a relative approximation accurate to O(k3/N2) by simply
multiplying the asymptotic estimates for these quantities (given in Ļeorem 4.4,
Ļeorem 4.5, and the third corollary to Ļeorem 4.3) to get the result. How-
ever, taking an extra term in the derivation leads to cancellation that gives

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȠȞ
more accuracy. As in the proofs of Ļeorem 4.4 and Ļeorem 4.5, we have
N!
(N −k)!
N!
(N + k)! = exp
{ ∑
1≤j<k
ln
(
1 −j
N
)
−
∑
1≤j≤k
ln
(
1 + j
N
)}
= exp
{ ∑
1≤j<k
(
−j
N −j2
2N2 + O
( j3
N3
))
−
∑
1≤j≤k
( j
N −j2
2N2 + O
( j3
N3
))}
= exp
{
−k(k −1)
2N
−k(k + 1)
2N
+ O
( k2
N2
)
+ O
( k4
N3
)}
= exp
{
−k2
N + O
( k2
N2
)
+ O
( k4
N3
)}
= e−k2/N(
1 + O
( k2
N2
)
+ O
( k4
N3
))
.
Ļe improved accuracy results from cancellation of the j2/N2 terms in the
sums. Ļe O(k2/N2) term can be replaced by O(1/N) because k2/N2 is
O(1/N) if k ≤
√
N and O(k4/N3) if k ≥
√
N.
Ļe same procedure as in the proof of Ļeorem 4.4 can be used to estab-
lish the absolute error bound for all k ≥0, and by symmetry in the binomial
coeﬃcients, it holds for all k.
Ļis is the normal approximation to the binomial distribution: the func-
tion e−k2/N/
√
πN is the well-known “bell curve” at the bottom of Figure 4.3.
Most of the curve is within plus or minus a small constant times
√
N of the
mean, and the tails decay exponentially outside that range. Typically, when
working with the normal approximation, we need to make use of both of these
facts.
Corollary For all k and N,
1
22N
(
2N
N −k
)
≤e−k2/(2N).
Proof. Note that
(2N
N
)/22N < 1, and multiply the bounds in the corollaries to
Ļeorem 4.4 and Ļeorem 4.5.

ȘȠȟ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
Ļis kind of result is often used to bound the tail of the distribution in
the following manner. Given ϵ > 0, we have, for k >
√
2N1+ϵ,
1
22N
(
2N
N −k
)
≤e−(2N)ϵ.
Ļat is, the tails of the distribution are exponentially small when k grows
slightly faster than
√
N.
Exercise 4.64 Carry out the normal approximation to O
(
1/N 2)
for the case k =
√
N + O
(
1
)
.
Exercise 4.65 Plot the smallest k for which the binomial probabilities are greater
than .001 as a function of N.
Poisson approximation.
In slightly more general form, the binomial dis-
tribution gives the probability of k successes in N independent trials, each
having a probability p of success:
(
N
k
)
pk(1 −p)N−k.
As we discuss in detail in Chapter 9, this situation is often studied in terms
of the “occupancy distribution” when N balls are distributed into M urns.
Taking p = 1/M, the probability that exactly k balls fall into a particular urn
(for instance, the ŀrst urn) is
(
N
k
)( 1
M
)k(
1 −1
M
)N−k.
Ļe “balls-and-urns” model is classical (see, for example, [8]), and it also turns
out to be directly applicable to a variety of fundamental algorithms that are
in widespread use, as we will see in Chapters 8 and 9.
Whenever M is a constant, the distribution can still be approximated
by a normal distribution centered at Np, as veriŀed in Exercise 4.67 and il-
lustrated in Figure 4.4, for p = 1/5.
Ļe case where M varies with N is of special interest. In other words,
we take p = 1/M = λ/N, where λ is a constant. Ļis corresponds to per-
forming N trials, each of which has a small (λ/N) probability of success. We

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
ȘȠȠ
0
.512
.4096
.14
0
Figure 4.4
N/5
N
(
N
k
)
(p)k (1 −p)N−k
p = 1/5
Binomial distribution, 3 ≤N ≤60 (k-axes scaled to N)
thus expect an average of λ trials to succeed. Ļe probability law correspond-
ing to this (in the asymptotic limit) is called the Poisson law. It applies when
we want to describe the collective behavior of a large number of “agents,” each
with a small probability of being “active,” “successful,” or somehow otherwise
distinguished. One of the very ŀrst applications of the Poisson law (due to
Von Bortkiewicz in the 19th century) was to characterize the number of cav-
alrymen killed by horse kicks in the Prussian Army. Since then, it has been
used to describe a large variety of situations, from radioactive decay to bomb
hits on London (cf. [8]). In the present context, we are interested in the Pois-
son law because it is an appropriate model for many fundamental algorithms,
most notably hashing algorithms (see Chapter 9).
Ļe Poisson law describes the situation of any given urn (e.g., the ŀrst
one) in the balls-and-urns model when the number of urns is within a con-
stant factor of the number of balls. Ultimately, this behavior is described by
a simple asymptotic expression

șȗȗ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
Ļeorem 4.7 (Poisson approximation).
For ŀxed λ, N →∞and all k
(
N
k
)( λ
N
)k(
1 −λ
N
)N−k = λke−λ
k!
+ o(1).
In particular, for k = O(N1/2)
(
N
k
)( λ
N
)k(
1 −λ
N
)N−k = λke−λ
k!
(
1 + O
( 1
N
)
+ O
( k
N
))
.
Proof. Rewriting the binomial coeﬃcient in yet another form,
(
N
k
)( λ
N
)k(
1 −λ
N
)N−k = λk
k!
N!
(N −k)!Nk
(
1 −λ
N
)N−k,
we see that the Ramanujan Q-distribution again appears. Combining the
result of Ļeorem 4.4 with
(
1 −λ
N
)N−k = exp
{
(N −k)ln
(
1 −λ
N
)}
= exp
{
(N −k)
(
−λ
N + O
( 1
N2
))}
= e−λ(
1 + O
( 1
N
)
+ O
( k
N
))
we get the stated relative error bound.
To prove the ŀrst part of the theorem (the absolute error bound), observe
that for k >
√
N (and λ ŀxed) both the Poisson and the binomial terms are
exponentially small, and in particular o(1).
As k grows, the λk/k! term grows until k = ⌊λ⌋, then becomes very
small very quickly. As with the normal approximation, a bound on the tail can
easily be derived by working through the above derivation, using inequalities
rather than the O-notation. Figure 4.5 shows how the distribution evolves
for small ŀxed λ as N grows. Table 4.10 gives the binomial distribution and
Poisson approximation for λ = 3. Ļe approximation is a fairly accurate
estimate for the binomial distribution whenever probabilities are relatively
small, even for small N.

§ț.ȝ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȗȘ
.018
.195
.050
.224
0
.135
.270
0
.368
.184
.061
0
Figure 4.5
λ = 4
λ = 3
λ = 2
λ = 1
(N
k
)( λ
N
)k (
1 −λ
N
)N−k
Binomial distributions, 3 ≤N ≤60 with p = λ/N, tending
to Poisson distributions λke−λ/k! (k-axes scaled to N)

șȗș
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȝ
Ļe function λke−λ/k! is the Poisson distribution, which, as mentioned
earlier, models many stochastic processes. Ļe PGF of this distribution is
eλ(z−1), whence both the mean and the variance are λ. See Feller [8] for
more details.
Exercise 4.66 Give an asymptotic approximation to
( N
pN
)
ppN(1 −p)N−pN.
Exercise 4.67 Give an asymptotic approximation to
(N
k
)
pk(1 −p)N−k for p ŀxed.
(Hint : Shift so that the largest terms in the distribution are at k = 0.)
Exercise 4.68 Give an asymptotic approximation of the binomial distribution for the
case where p = λ/
√
N.
Exercise 4.69 Give an asymptotic approximation of the binomial distribution for the
case where p = λ/lnN.
N
↓k →
0
1
2
3
4
5
6
7
8
9
4
.0039 .0469 .2109 .4219 .3164
5
.0102 .0768 .2304 .3456 .2592 .0778
6
.0156 .0938 .2344 .3125 .2344 .0938 .0156
7
.0199 .1044 .2350 .2938 .2203 .0991 .0248 .0027
8
.0233 .1118 .2347 .2816 .2112 .1014 .0304 .0052 .0004
9
.0260 .1171 .2341 .2731 .2048 .1024 .0341 .0073 .0009
10
.0282 .1211 .2335 .2668 .2001 .1029 .0368 .0090 .0014 .0001
11
.0301 .1242 .2329 .2620 .1965 .1031 .0387 .0104 .0019 .0002
12
.0317 .1267 .2323 .2581 .1936 .1032 .0401 .0115 .0024 .0004
13
.0330 .1288 .2318 .2550 .1912 .1033 .0413 .0124 .0028 .0005
14
.0342 .1305 .2313 .2523 .1893 .1032 .0422 .0132 .0031 .0006
100
.0476 .1471 .2252 .2275 .1706 .1013 .0496 .0206 .0074 .0023
∞
.0498 .1494 .2240 .2240 .1680 .1008 .0504 .0216 .0027 .0009
Table 4.10
Binomial distribution
(N
k
)(
3
N
)k(
1 −3
N
)N−k, tending to
Poisson distribution 3ke−3/k!

§ț.Ȟ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȗȚ
4.7 Laplace Method. In the theorems of the previous section, we saw that
diﬀerent bounds are appropriate for diﬀerent parts of the range for bivariate
distributions. When estimating sums across the whole range, we want to take
advantage of our ability to get accurate estimates of the summand in various
diﬀerent parts of the range. On the other hand, it is certainly more convenient
if we can stick to a single function across the entire range of interest.
In this section, we discuss a general method that allows us to do both—
the Laplace method for estimating the value of sums and integrals. We fre-
quently encounter sums in the analysis of algorithms that can be estimated
with this approach. Generally, we are also taking advantage of our ability to
approximate sums with integrals in such cases. Full discussion and many ex-
bound tails
approximate
extend tails
Figure 4.6 Laplace method

șȗț
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ȟ
amples may be found in Bender and Orszag [2] or De Bruijn [6]. Ļe method
is centered on the following three steps for evaluating sums:
• Restrict the range to an area that contains the largest summands.
• Approximate the summand and bound the tails.
• Extend the range and bound the new tails, to get a simpler sum.
Figure 4.6 illustrates the method in a schematic manner. Actually, when
approximating sums, the functions involved are all step functions; usually a
“smooth” function makes an appearance at the end, in an application of the
Euler-Maclaurin formula.
A typical case of the application of the Laplace method is the evalua-
tion of the Ramanujan Q-function that we introduced at the beginning of the
previous section. As mentioned at that time, it is of interest in the analysis
of algorithms because it arises in many applications, including hashing algo-
rithms, random maps, equivalence algorithms, and analysis of memory cache
performance (see Chapter 9).
Ļeorem 4.8 (Ramanujan Q-function).
As N →∞,
Q(N) ≡
∑
1≤k≤N
N!
(N −k)!Nk =
√
πN/2 + O(1).
Proof. Neither of the estimates given in Ļeorem 4.4 is useful across the entire
range of summation, so we have to restrict each to the portion of the range to
which it applies. More precisely, we deŀne k0 to be an integer that is o(N2/3)
and divide the sum into two parts:
∑
1≤k≤N
N!
(N −k)!Nk =
∑
1≤k≤k0
N!
(N −k)!Nk +
∑
k0<k≤N
N!
(N −k)!Nk .
We approximate the two parts separately, using the diﬀerent restrictions on
k in the two parts to advantage. For the ŀrst (main) term, we use the relative
approximation in Ļeorem 4.4. For the second term (the tail), the restriction
k > k0 and the fact that the terms are decreasing imply that they are all
exponentially small, as discussed in the proof of Ļeorem 4.4. Putting these
two observations together, we have shown that
Q(N) =
∑
1≤k≤k0
e−k2/(2N)(
1 + O
( k
N
)
+ O
( k3
N2
))
+ .

§ț.Ȟ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȗȜ
Here we use  as a notation to represent a term that is exponentially small,
but otherwise unspeciŀed. Moreover, exp(−k2/(2N)) is also exponentially
small for k > k0 and we can add the terms for k > k0 back in, so we have
Q(N) =
∑
k≥1
e−k2/(2N) + O(1).
Essentially, we have replaced the tail of the original sum with the tail of the
approximation, which is justiŀed because both are exponentially small. We
leave the proof that the error terms contribute an absolute error that is O(1)
as an exercise below, because it is a slight modiŀcation of the proof for the
main term, which is discussed in the next paragraph. Of course, the O(1)
also absorbs the exponentially small terms.
Ļe remaining sum is the sum of values of the function ex2/2 at regularly
spaced points with step 1/
√
N. Ļus, the Euler-Maclaurin theorem provides
the approximation
∑
k≥1
e−k2/(2N) =
√
N
∫∞
0
e−x2/2dx + O(1).
Ļe value of this integral is well known to be
√
π/2. Substituting into the
above expression for Q(N) gives the stated result.
Note that, in this case, the large terms occur for small values of k, so
that we had only one tail to take care of. In general, as depicted in Figure 4.6,
the dominant terms occur somewhere in the middle of the range, so that both
left and right tails have to be treated.
Exercise4.70 By applying Euler-Maclaurin summation to the functions xe−x2/2 and
x3e−x3/2, show that
∑
1≤k≤k0
e−k2/(2N)O
( k
N
)
and
∑
1≤k≤k0
e−k2/(2N)O
( k3
N 2
)
are both O
(
1
)
.
Ļe Q-function takes on several forms; also Knuth [15] deŀnes two
related functions, the P-function and the R-function (which is a sum of the

șȗȝ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ȟ
Q(N) =
∑
1≤k≤N
N!
(N −k)!N k
=
∑
1≤k≤N
∏
1≤j<k
(
1 −j
N
)
=
∑
1≤k≤N
(
1 −1
N
)(
1 −2
N
)
. . .
(
1 −k −1
N
)
=
∑
0≤k<N
N!
k!
N k
N N
=
∑
k
(N
k
) k!
N k −1
=
√
πN
2
−1
3 + 1
12
√π
2N + O
( 1
N
)
P(N) =
∑
0≤k≤N
(N −k)k(N −k)!
N!
=
∑
1≤k≤N
∏
1≤j<k
(N −k
N −j
)
=
∑
0≤k<N
k!
N!
kN
kk
=
∑
k
(N −k)k
k!
(N
k
)
=
√
πN
2
−2
3 + 11
24
√π
2N + O
( 1
N
)
R(N) =
∑
k≥0
N!N k
(N + k)!
=
∑
1≤k≤N
∏
1≤j<k
(
N
N + j
)
=
∑
k≥N
N!
k!
N k
N N
=
√
πN
2
+ 1
3 + 1
12
√π
2N + O
( 1
N
)
Table 4.11
Ramanujan P-, Q-, and R-functions

§ț.ȟ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȗȞ
R-distribution that we considered earlier). Ļese, along with the asymptotic
estimates given by Knuth [15], are shown in Table 4.11. Note that
Q(N) + R(N) =
∑
k
N!
k!
Nk
NN = N!
NN eN =
√
2πN + 1
6
√π
2N + O
( 1
N
)
by Stirling’s approximation.
Exercise 4.71 Show that
P(N) =
∑
k≥0
(N −k)k(N −k)!
N!
=
√
πN/2 + O
(
1
)
Exercise 4.72 Find a direct argument showing that R(N) −Q(N) ∼2/3.
4.8 “Normal” Examples from the Analysis of Algorithms. Ļe analy-
sis of several algorithms depends on the evaluation of a sum that is similar to
the binomial distribution:
∑
k
F(k)
(
2N
N −k
) / (
2N
N
)
.
When F(k) is reasonably well behaved, we can use the Laplace method and
the Euler-Maclaurin formula (Ļeorem 4.2) to accurately estimate such a
sum. We consider this sum in some detail because it is representative of many
similar problems that arise in the analysis of algorithms and because it pro-
vides a good vehicle for further illustrating the Laplace method.
Ļe nature of the applications involving various F(k) is only brieły
sketched here, since the applications are described fully in the cited sources,
and we will cover related basic concepts in the chapters that follow. Our pur-
pose here is to provide a concrete example of how asymptotic methods can
give accurate estimates for complex expressions that arise in practice. Such
sums are sometimes called Catalan sums as they arise in connection with tree
enumerations and Catalan numbers, as we will see in Chapter 5. Ļey also
occur in connection with path enumerations and merging algorithms, as we
will see in Chapter 6.

șȗȟ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȟ
2-ordered sequences. A sequence of numbers is said to be 2-ordered if the
numbers in the odd positions are in ascending order and the numbers in the
even positions are in ascending order. Taking a pair of ordered sequences and
“shuﬄing” them (alternate taking one number from each sequence) gives a
2-ordered sequence. In Chapters 6 and 7, we will examine the combinatorial
properties of such sequences. Analysis of a number of merging and sort-
ing methods leads to the study of properties of 2-ordered sequences. Taking
F(k) = k2 in the Catalan sum gives the average number of inversions in a
2-ordered sequence, which is proportional to the average running time of a
simple sorting method for the sequence.
Batcher’s odd-even merge. Another example involves a sorting method due
to Batcher (see [16] and [20]) that is suitable for hardware implementation.
Taking F(k) = klogk gives the leading term in the running time for this
method, and a more accurate analysis is possible, involving a much more com-
plicated F(k).
For F(k) = 1, we immediately have the result 4N/
(2N
N
), which we
have already shown to be asymptotic to
√
πN. Similarly, for F(k) = k and
F(k) = k2, it is also possible to derive an exact value for the sum as a lin-
ear combination of a few binomial coeﬃcients, then develop asymptotic esti-
mates using Stirling’s approximation. Ļe methods of this section come into
play when F(k) is more complicated, as in the analysis of Batcher’s method.
Ļe primary assumption that we make about F(k) is that it is bounded by a
polynomial.
As discussed in the proof of Ļeorem 4.6, we are working with the prod-
uct of the Ramanujan Q-distribution and R-distribution:
(
2N
N −k
)
(
2N
N
)
=
(2N)!
(N −k)!(N + k)!
(2N)!
N!N!
=
N!N!
(N −k)!(N + k)! =
N!
(N −k)!Nk
N!Nk
(N + k)!.
Ļus we can use the results derived during the proof of Ļeorem 4.6 and its
corollary in an application of the Laplace method for this problem. Choos-
ing the cutoﬀpoint k0 =
√
2N1+ϵ for a small constant ϵ > 0, we have the

§ț.ȟ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȗȠ
approximations
N!N!
(N −k)!(N + k)! =





e−k2/N(
1 + O
(
1
N1−2ϵ
))
for k < k0
O
(
e−(2N)ϵ)
for k ≥k0.
Ļis is the basic information that we need for approximating the summand
and then using the Laplace method. As earlier, we use the ŀrst part of this to
approximate the main contribution to the sum and the second part to bound
the tails:
∑
k
(
2N
N −k
)
(
2N
N
) F(k) =
∑
|k|≤k0
F(k)e−k2/N(
1 + O
(
1
N1−2ϵ
))
+ 
where, again,  represents an exponentially small term. As before, we can add
the tails back in because exp(−k2/N) is also exponentially small for k > k0,
which leads to the following theorem.
Ļeorem 4.9 (Catalan sums).
If F(k) is bounded by a polynomial, then
∑
k
F(k)
(
2N
N −k
) / (
2N
N
)
=
∑
k
e−k2/NF(k)
(
1 + O
(
1
N1−2ϵ
))
+ ,
where  denotes an exponentially small error term.
Proof. See the discussion above. Ļe condition on F is required to keep the
error term exponentially small.
If the sequence {F(k)} is the specialization of a real function F(x) that
is suﬃciently smooth, then the sum is easily approximated with the Euler-
Maclaurin formula. In fact, the exponential and its derivatives vanish very
quickly at ∞, so all the error terms vanish there, and, under suitable condi-
tions on the behavior of F, we expect to have
∑
−∞<k<∞
F(k)
(
2N
N −k
) / (
2N
N
)
=
∫∞
−∞
e−x2/NF(x)dx
(
1+O
(
1
N1−2ϵ
))
.
A similar process works for one-sided sums, where k is restricted to be, say,
nonnegative, leading to integrals that are similarly restricted.

șȘȗ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.ȟ
Stirling’s constant. Taking F(x) = 1 gives a familiar integral that leads to
the expected solution
√
πN. Indeed, this constitutes a derivation of the value
of Stirling’s constant, as promised in §4.5: we know that the sum is equal to
4N/
(2N
N
), which is asymptotic to σ
√
N/2 by the same elementary manip-
ulations we did in §4.3, but leaving σ as an unknown in Stirling’s formula.
Taking N →∞, we get the result σ =
√
2π.
Other examples. Ļe average number of inversions in a 2-ordered ŀle is given
by the one-sided version (that is, k ≥0) of the Catalan sum with F(x) = x2.
Ļis integral is easily evaluated (integration by parts) to give the asymptotic
result N
√
πN/4. For Batcher’s merging method, we use the one-sided sum
with F(x) = xlgx + O(x) to get the estimate
∫∞
0
e−x2/Nxlgx dx + O(N).
Ļe substitution t = x2/N transforms the integral to another well-known
integral, the “exponential integral function,” with the asymptotic result
1
4NlgN + O(N).
It is not easy to get a better approximation for F(x), and it turns out that
complex analysis can be used to get a more accurate answer, as described in
[20]. Ļese results are summarized in Table 4.12.
F(k)
∑
k≥0
F(k)
( 2N
N −k
) / (2N
N
)
1
∼
√
πN
2
k
N
2
klgk
∼NlgN
4
k2
N4N−1 / (2N
N
)
∼
√
πN 3
4
Table 4.12
Catalan sums

§ț.Ƞ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȘȘ
Exercise 4.73 Find an asymptotic estimate for
∑
k≥0
(N
k
)3
.
Exercise 4.74 Find an asymptotic estimate for
∑
k≥0
N!
(N −2k)!k!2k .
Exercise 4.75 Find an asymptotic estimate for
∑
k≥0
(N −k
k
)2
.
4.9 “Poisson” Examples from the Analysis of Algorithms. Several
other basic algorithms lead to the evaluation of sums where the largest terms
are at the beginning, with an eventual exponential decrease. Ļis kind of sum
is more like the Poisson distribution with λ < 1. An example of such a sum
is
∑
k
(
N
k
)
f(k)
2k
where, for example, f(k) is a fast-decreasing function.
For an example, we consider the radix-exchange sorting method men-
tioned in Chapter 1, which is closely related to the “trie” data structure de-
scribed in detail in Chapter 8. Below we show that the solution to the recur-
rence describing the number of bit inspections in radix-exchange sorting in-
volves the “trie sum”
SN =
∑
j≥0
(
1 −
(
1 −1
2j
)N)
.
Expanding the binomial gives sums of terms of the form shown above with
f(k) = (−1)k, (−1/2)k, (−1/4)k, (−1/8)k, and so forth. Precise evaluation
of this sum is best done with complex analysis, but a very good estimate is easy
to obtain from the approximation
1 −
(
1 −1
2j
)N ∼1 −e−N/2j.

șȘș
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
§ț.Ƞ
We can show that the approximation is good to within O(1/N) for j < lgN
and that both sides are very small for large j ≫lgN, so it is elementary to
show that
SN =
∑
j≥0
(1 −e−N/2j) + o(1).
By splitting the range of summation into three parts, it is not diﬃcult to get
a good estimate for this sum. As shown in Figure 4.7, the summand is near
1 for small j, near 0 for large j, and in transition from 1 to 0 for j near lgN.
More precisely, for j < (1 −ϵ)lgN, the summand is very close to 1; for
j > (1 + ϵ)lgN, it is very close to 0; and for j in between the bounds, the
sum is certainly between 0 and 1. Ļis argument proves that, up to smaller-
order terms, the sum is between (1 −ϵ)lgN and (1 + ϵ)lgN for any ϵ; a more
careful choice of the bounds will show that the sum is ∼lgN.
Ļeorem 4.10 (Trie sum).
As N →∞,
SN =
∑
j≥0
(
1 −
(
1 −1
2j
)N)
= lgN + O(loglogN).
Proof. Use bounds of lgN ± lnlnN in the argument above. A more detailed
argument can be used to show that SN = lgN + O(1). We will analyze the
function SN −lgN in some detail in Chapter 8.
Figure 4.7 Asymptotic behavior of the terms in ∑
j≥0(1 −e−N/2j)

§ț.Ƞ
A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȘȚ
Corollary Ļe average number of bit inspections used by radix-exchange sort
is NlgN + O(N).
Proof. As discussed in §1.5 this quantity satisŀes the recurrence
CN = N + 2
2N
∑
k
(
N
k
)
Ck
for N > 1 with C0 = 0 and C1 = 0.
Multiplying by zN and summing on N leaves a straightforward convolution
that simpliŀes to demonstrate that the EGF ∑
N≥0 CNzN/N! must satisfy
the functional equation
C(z) = zez −z + 2ez/2C(z/2).
As we might expect, this equation is also available via the symbolic method
[10]. Iterating the equation, we ŀnd that
C(z) = zez −z + 2ez/2C(z/2)
= zez −z + 2ez/2(z
2ez/2 −z
2 + 2ez/4C(z/4)
)
= z(ez −1) + z(ez −ez/2) + 4e3z/4C(z/4)
= z(ez −1) + z(ez −ez/2) + z(ez −e3z/4) + 8e7z/8C(z/8)
...
= z
∑
j≥0
(
ez −e(1−2−j)z)
and therefore
CN = N![zN]C(z) = N
∑
j≥0
(
1 −
(
1 −1
2j
)N−1)
.
Ļus we have CN = NSN−1, and the claimed result is established.
As discussed further in Chapter 8, the linear term oscillates in value.
Ļis is perhaps not surprising because the algorithm deals with bits and has
the “binary divide-and-conquer” łavor of some of the algorithms discussed
in §2.6. It presents signiŀcant analytic challenges nonetheless, which are best
met by complex analysis methods.

șȘț
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
Exercise 4.76 Find an asymptotic estimate for
∑
0≤j≤N
(
1 −1
2j
)N
.
Exercise 4.77 Find an asymptotic estimate for
∑
j≥0
(1 −e−N/jt) for t > 1.
A
SYMPTOTIC methods play an essential role in the analysis of algo-
rithms. Without asymptotics we might be left with hopelessly complex
exact answers or hopelessly diﬃcult closed-form solutions to evaluate. With
asymptotics, we can focus on those parts of the solution that contribute most
to the answer. Ļis extra insight from the analysis also plays a role in the
algorithm design process: when seeking to improve the performance of an
algorithm, we focus on precisely those parts of the algorithm that lead to the
terms we focus on in the asymptotic analysis.
Understanding distinctions among the various notations that are com-
monly used in asymptotic analysis is critically important. We have included
many exercises and examples in this chapter to help illustrate such distinc-
tions, and the reader is urged to study them carefully. Proper use of elemen-
tary deŀnitions and manipulations can greatly simplify asymptotic formulas.
As with GFs, a good deal of the basic material for asymptotic analy-
sis follows from combining well-known classical expansions, for generating
functions associated with well-known special numbers such as Stirling num-
bers, harmonic numbers, geometric series, and binomial coeﬃcients. Alge-
braic manipulations and simpliŀcation also play an important role. Indeed,
the ability to suppress detail in such calculations makes the asymptotic repre-
sentation very attractive.
We considered in detail the use of these methods to derive two of the
most important approximations to the binomial distribution: normal and
Poisson. We also considered the related functions that were studied by Ra-
manujan and Knuth that appear in the analysis of many algorithms. Not only
are these approximations extremely useful in the analysis of algorithms, but
they also are prototypical of the kind of manipulations that arise often.
We have concentrated on so-called elementary methods, from real anal-
ysis. Proŀciency in elementary methods is important, especially simplifying
complicated asymptotic expressions, reŀning estimates, approximating sums
with integrals, and bounding tails, including the Laplace method. Better
understanding of asymptotic analysis relies on understanding of properties
of functions in the complex plane. Surprisingly, powerful methods derive

A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȘȜ
from only a few basic properties, especially singularities of generating func-
tions. We have given a general idea of the basic method, which is considered
in detail in [11]. Advanced techniques often cannot be avoided in detailed
asymptotics, since complex values appear in answers. In particular, we see
many examples where an oscillating phenomenon appears as the function un-
der study grows. Ļis seems surprising, until we recall that it appears in our
most fundamental algorithms, including divide-and-conquer methods such
as mergesort or methods involving binary representation of integers. Com-
plex analysis provides a simple way to explain such phenomena.
We can conclude this chapter in a similar fashion to the previous chap-
ters: the techniques given here can take us reasonably far in studying important
properties of fundamental computer algorithms. Ļey play a crucial role in
our treatment of algorithms associated with trees, permutations, strings, and
maps in Chapters 6 through 9.

șȘȝ
C Ŕ ō Ŝ Š ő Ş
F ś š Ş
References
1. M. AŎŞōřśţŕŠŦ ōŚŐ I. SŠőœšŚ. Handbook of Mathematical Functions,
Dover, New York, 1972.
2. C. M. BőŚŐőŞ ōŚŐ S. A. OŞşŦōœ. Advanced Mathematical Methods for
Scientists and Engineers, McGraw-Hill, New York, 1978.
3. E. A. BőŚŐőŞ. “Asymptotic methods in enumeration,” SIAM Review
16, 1974, 485–515.
4. B. C. BőŞŚŐŠ. Ramanujan ’s Notebooks, Parts I and II, Springer-Verlag,
Berlin, 1985 and 1989.
5. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
6. N. G. Őő BŞšŕŖŚ. Asymptotic Methods in Analysis, Dover, New York,
1981.
7. A. EŞŐŻŘťŕ. Asymptotic Expansions, Dover, New York, 1956.
8. W. FőŘŘőŞ. An Introduction to Probability Ļeory and Its Applications, vol-
ume 1, John Wiley, New York, 1957, 2nd edition, 1971.
9. P. FŘōŖśŘőŠ ōŚŐ A. OŐŘťŦŗś. “Singularity analysis of generating func-
tions,” SIAM Journal on Discrete Mathematics 3, 1990, 216–240.
10. P. FŘōŖśŘőŠ, M. RőœŚŕőŞ, ōŚŐ D. SśŠŠőōš. “Algebraic methods for
trie statistics,” Annals of Discrete Mathematics 25, 1985, 145–188.
11. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
12. R. L. GŞōŔōř, D. E. KŚšŠŔ, ōŚŐ O. PōŠōşŔŚŕŗ. Concrete Mathemat-
ics, 1st edition, Addison-Wesley, Reading, MA, 1989. Second edition,
1994.
13. D. H. GŞőőŚő ōŚŐ D. E. KŚšŠŔ. Mathematics for the Analysis of Algo-
rithms, Birkhäuser, Boston, 1981.
14. PőŠőŞ HőŚŞŕŏŕ. Applied and Computational Complex Analysis, 3 vol-
umes, John Wiley, New York, 1977.
15. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 1: Fundamen-
tal Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1968. Ļird
edition, 1997.
16. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.

A ş ť ř Ŝ Š ś Š ŕ ŏ
A Ŝ Ŝ Ş ś Ť ŕ ř ō Š ŕ ś Ś ş
șȘȞ
17. D. E. KŚšŠŔ. “Big omicron and big omega and big theta,” SIGACT
News, April-June 1976, 18–24.
18. A. OŐŘťŦŗś. “Asymptotic enumeration methods,” in Handbook of Com-
binatorics, volume 2, R. Graham, M. Grötschel, and L. Lovász, eds.,
North Holland, 1995.
19. F. W. J. OŘŢőŞ. Asymptotics and Special Functions, Academic Press, New
York, 1974, reprinted by A. K. Peters, 1997.
20. R. SőŐœőţŕŏŗ. “Data movement in odd-even merging,” SIAM Journal
on Computing 7, 1978, 239–272.
21. E. T. WŔŕŠŠōŗőŞ ōŚŐ G. N. WōŠşśŚ. A Course of Modern Analysis,
Cambridge University Press, 4th edition, 1927.
22. H. WŕŘŒ. Generatingfunctionology, Academic Press, San Diego, 1990,
2nd edition, A. K. Peters, 2006.

This page intentionally left blank 

C H A P T E R F I V E
A N A L Y T I C C O M B I N A T O R I C S
T
HIS chapter introduces analytic combinatorics, a modern approach to
the study of combinatorial structures of the sort that we encounter fre-
quently in the analysis of algorithms. Ļe approach is predicated on the idea
that combinatorial structures are typically deŀned by simple formal rules that
are the key to learning their properties. One eventual outgrowth of this obser-
vation is that a relatively small set of transfer theorems ultimately yields accu-
rate approximations of the quantities that we seek. Figure 5.1 gives an general
overview of the process.
Generating functions are the central objects of study in analytic com-
binatorics. In the ŀrst place, we directly translate formal deŀnitions of combi-
natorial objects into deŀnitions of generating functions that enumerate objects
or describe their properties. In the second place, we use classical mathematical
analysis to extract estimates of generating function coeﬃcients.
First, we treat generating functions as formal objects, which provides
a convenient, compact, and elegant approach to understanding relationships
among classes of combinatorial objects. Ļe key idea is to develop a set of in-
tuitive combinatorial constructions that immediately translate to equations that
the associated generating functions must satisfy. We refer to this approach as
the symbolic method, as it formalizes the idea that an object’s description can
be captured via symbolic mathematics.
generating
function
equation
coeﬃcient
asymptotics
combinatorial
construction
symbolic
transfer
theorem
“the symbolic method”
analytic
transfer
theorem
Figure 5.1 An overview of analytic combinatorics
șȘȠ

șșȗ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ș
Ļe symbolic method yields explicit or implicit descriptions of gener-
ating functions. Treating those same generating functions as analytic objects
is fruitful, as we have already seen. We can extract the coeﬃcients that carry
the information that we seek (as described in Chapter 3) and then derive
asymptotic estimates of their growth (as described in Chapter 4). Ļis pro-
cess can often present signiŀcant technical challenges, but it is often the case
that general transfer theorems can immediately provide exact or asymptotic in-
formation. In general, complex analysis is really required, so we only provide
an introduction here, for examples where real analysis suﬃces.
Classical research in combinatorics provides the underlying framework
for analytic combinatorics, but the motivation provided by the analysis of
algorithms is immediate. Modern programmers deŀne data structures to sys-
tematically organize the data that needs to be processed in applications. A
data structure is nothing more than a combinatorial object that is formally de-
ŀned (in a programming language) and so is immediately amenable to study
with analytic combinatorics. Our current state of knowledge stops far short
of being able to analyze any program that can be deŀned in any programming
language, but the approach has proved to be quite successful in studying many
of the most important algorithms and data structures that play a critical role
in the computational infrastructure that surrounds us.
5.1 Formal Basis. To understand the motivation for the symbolic method,
consider the problem of enumerating binary trees. Ļe functional equation
for binary trees that we derived in §3.8 is quite simple, so the question nat-
urally arises whether an even more direct derivation is available. Indeed, the
similarity between the recursive deŀnition of trees and the quadratic equation
of the OGF is striking. With the symbolic method, we can demonstrate that
this similarity is not coincidental, but essential; we can interpret
T(z) = 1 + zT(z)2
as a direct consequence of the deŀnition “a binary tree is either an external
node or an internal node connected to two binary trees.”
Ļe approach that we use to do this has two primary features. First, it
is symbolic, using only a few algebraic rules to manipulate symbolic informa-
tion. Some authors emphasize this by actually using symbolic diagrams rather
than variables like z in the generating formulae. Second, it directly mirrors

§Ȝ.ș
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șșȘ
the way in which we deŀne the structure. We generate structures, as opposed
to dissecting them for analysis.
Ļe formal basis for the symbolic method is a precise deŀnition that
captures the essential properties of any combinatorial enumeration problem.
Deŀnition A combinatorial class is a set of discrete objects and an associated
size function, where the size of each object is a nonnegative integer and the
number of objects of any given size is ŀnite.
Combinatorial objects are built from atoms, which are deŀned to be ob-
jects of size 1. Examples of atoms include 0 or 1 bits in bitstrings and internal
or external nodes in trees. For a class A, we denote the number of members
of the class of size n by an and refer to this sequence as the counting sequence
associated with A. Typically the size function enumerates certain atoms, so
that a combinatorial object of size n consists of n such atoms.
To specify classes, we also make use of neutral objects ϵ of size 0 and the
neutral class E that contains a single neutral object. We use Z to represent the
class containing a single atom and we use subscripts to distinguish diﬀerent
types of atoms. Also, we denote the empty class by ϕ.
Objects may be unlabelled (where atoms are indistinguishable) or la-
belled (where the atoms are all diﬀerent and we consider objects with atoms
appearing in diﬀerent order to be diﬀerent). We consider the simpler of the
two cases (unlabelled objects) ŀrst.
5.2 Symbolic Method for Unlabelled Classes. For reference, Figure 5.2
gives examples of three basic unlabelled combinatorial classes. Ļe ŀrst is
just a sequence of atoms—there is only one object of each size, so this is
tantamount to encoding natural numbers in unary. Ļe second is a sequence
built from one of two atoms—the objects of size N are the N-bit binary
numbers, or bitstrings. Ļe third is the class of binary trees that we considered
in Chapter 3 (just built from internal nodes in this example).
Given an unlabelled class A with counting sequence {an}, we are inter-
ested in the OGF
A(z) =
∑
n≥0
anzn.
Note that E has OGF 1, and ϕ has OGF 0. Ļe OGF of Z is z when the
associated atom is counted in the size function and 1 when it is not (this
distinction will be made clear in later examples).

șșș
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.ș
Binary trees (sized by number of internal nodes)
Bitstrings (sequences of bits)
Natural numbers (sequences or sets of atoms)
I 1=1
I 2=1
I 3=1
I 4=1
T2 = 2
B1 = 2
0
1
B2 = 4
0 0
0 1
1 0
1 1
B3 = 8
0 0 0
0 0 1
0 1 0
0 1 1
1 0 0
1 0 1
1 1 0
1 1 1
0 0 0 0
0 0 0 1
0 0 1 0
0 0 1 1
0 1 0 0
0 1 0 1
0 1 1 0
0 1 1 1
1 0 0 0
1 0 0 1
1 0 1 0
1 0 1 1
1 1 0 0
1 1 0 1
1 1 1 0
1 1 1 1
T1 = 1
T3 = 5
T4 = 14
B4 = 16
Figure 5.2 Basic unlabelled classes, 1 ≤N ≤4

§Ȝ.ș
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șșȚ
As we have seen, the fundamental identity
A(z) =
∑
n≥0
anzn =
∑
a∈A
z|a|.
allows us to view the OGF as an analytic form representing the counting
sequence (the left sum) or as a combinatorial form representing all the indi-
vidual objects (the right sum). Table 5.1 gives the counting sequences and
OGFs for the classes in Figure 5.2. Generally, we use the same letter in dif-
ferent fonts to refer to a combinatorial class, its counting sequence, and its
generating function. Often, we use lowercase when discussing generic classes
and uppercase when discussing speciŀc classes.
Ļe essence of the symbolic method is a mechanism for deriving such
OGFs while at the same time specifying the corresponding class. Instead of the
informal English-language descriptions that we have been using, we turn to
simple formal operations. With just a little experience, you will see that such
operations actually simplify the process of specifying the combinatorial objects
and classes that we are studying. To start, we deŀne three simple and intuitive
operations. Given two classes A and B of combinatorial objects, we can build
new classes as follows:
A + B is the class consisting of disjoint copies of the members of A and B,
A × B is the class of ordered pairs of objects, one from A and one from B,
and SEQ (A) is the class ϵ + A + A × A + A × A × A + . . ..
atoms
size function
counting sequence
OGF
natural numbers
•
# of •s
1
1
1 −z
bitstrings
0 bits
1 bits
# of bits
2N
1
1 −2z
binary trees
•
# of •s
1
N + 1
(2N
N
)
1 −√1 −4z
2
Table 5.1
Counting sequences and OGFs for classes in Figure 5.2

șșț
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.ș
0 0
0 1
(    +    ) × (     +      +     ) =  
0 0 1 0 1
0 0 1 1 0
0 0 1 1 1
0 1 1 0 1
0 1 1 1 0
0 1 1 1 1
1 0 1
1 1 0
1 1 1
Figure 5.3 A simple combinatorial construction (unlabelled)
We refer to these operations as the disjoint union, Cartesian product, and se-
quence operations, respectively. Ļe disjoint union operation is sometimes
called the combinatorial sum. It is the same as the standard set union if the
two arguments are disjoint but builds multisets when they are not. Ļe se-
quence operation is like the concatenation operation that is used to deŀned
formal languages (sets of strings).
A combinatorial construction is an algebraic expression involving these
operations, where each operand may be a symbol representing an atom, a
symbol representing a combinatorial class, or a parenthesized combinatorial
construction. Figure 5.3 gives an example of a combinatorial construction.
At this introductory level, combinatorial constructions are formally quite
similar to arithmetic expressions in elementary algebra or regular expressions
and other formal languages in theoretical computer science. We shall examine
this latter connection in more detail in Chapter 8. Of course, the diﬀerence
is in the interpretation: with combinatorial constructions, our purpose is to
specify combinatorial classes in such a manner that we can solve enumeration
problems. Ļe disjoint union, Cartesian product, and sequence operations
are only a beginning: several other operations that greatly expand the com-
binatorial classes that we can study are described in [8] (see also Exercise 5.3
and Exercise 5.4).
Ļe importance of combinatorial constructions in analytic combina-
torics stems from not just the fact that they provide a way to specify com-
binatorial classes of all sorts but also the fact that they imply functional rela-
tionships on generating functions. Ļe following theorem provides a simple

§Ȝ.ș
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șșȜ
correspondence between the three operations in combinatorial constructions
that we have deŀned and their associated generating functions.
Ļeorem 5.1 (Symbolic method for unlabelled class OGFs).
Let A and
B be unlabelled classes of combinatorial objects. If A(z) is the OGF that
enumerates A and B(z) is the OGF that enumerates B, then
A(z) + B(z) is the OGF that enumerates A + B
A(z)B(z)
is the OGF that enumerates A × B
1
1 −A(z)
is the OGF that enumerates SEQ (A).
Proof. Ļe ŀrst part of the proof is trivial. If an is the number of objects of
size n in A and bn is the number of objects of size n in B, then an + bn is the
number of objects of size n in A + B.
To prove the second part, note that, for every k from 0 to n, we can pair
any of the ak objects of size k from A with any of the bn−k objects of size
n−k from B to get an object of size n in A×B. Ļus, the number of objects
of size n in A × B is
∑
0≤k≤n
akbn−k,
a simple convolution that implies the stated result. Alternatively, using the
combinatorial form of the OGFs, we have
∑
γ∈A×B
z|γ| =
∑
α∈A
∑
β∈B
z|α|+|β| =
∑
α∈A
z|α| ∑
β∈B
z|β| = A(z)B(z).
Ļis equation is deceptively simple: be sure that you understand it before
reading further.
Ļe result for sequences follows from the deŀnition
SEQ (A) = ϵ + A + A × A + A × A × A + . . . .
From the ŀrst two parts of the theorem, the generating function that enu-
merates this class is
1 + A(z) + A(z)2 + A(z)3 + A(z)4 + . . . =
1
1 −A(z).

șșȝ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.ș
Applying Ļeorem 5.1 to ŀnd OGFs associated with simple classes is a
good way to clarify the deŀnitions of disjoint union, Cartesian product, and
sequence. For example, Z + Z + Z is a multiset containing three objects
of size 1, with OGF 3z, while Z × Z × Z contains one object of size 3
(a sequence of length 3), with OGF z3. Ļe class SEQ (Z) represents the
natural numbers (one object corresponding to each number, by size) and has
OGF 1/(1 −z), as you might expect.
Bitstrings. Let B be the set of all binary strings (bitstrings), where the size of
a bitstring is its length. Enumeration is elementary: the number of bitstrings
of length N is 2N. One way to deŀne B is to use recursion, as follows: A
bitstring is either empty or corresponds precisely to an ordered pair consisting
of a 0 or a 1 followed by a bitstring. Symbolically, this argument gives the
combinatorial construction
B = ϵ + (Z0 + Z1) × B.
Ļeorem 5.1 allows us to translate directly from this symbolic form to a func-
tional equation satisŀed by the generating function. We have
B(z) = 1 + 2zB(z),
so B(z) = 1/(1−2z) and BN = 2N, as expected. Alternatively, we may view
B as being formed of sequences of bits, so that we can use the combinatorial
construction
B = SEQ (Z0 + Z1)
and then B(z) = 1/(1 −2z) by the sequence rule of Ļeorem 5.1, since the
OGF of Z0 + Z1 is just 2z. Ļis example is fundamental, and just a starting
point.
A variation. Ļe importance of the symbolic method is that it vastly simpli-
ŀes the analysis of variations of such fundamental constructs. As an example,
consider the class G of binary strings having no two consecutive 0 bits. Such
strings are either ϵ, a single 0, or 1 or 01 followed by a string with no two
consecutive 0 bits. Symbolically,
G = ϵ + Z0 + (Z1 + Z0 × Z1) × G.

§Ȝ.ș
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șșȞ
G1 = 2
0
1
G2 = 3
1 0
1 1
0 1
G3 = 5
G4 = 8
G5 = 13
1 1 1 0
1 1 1 1
1 1 0 1
1 0 1 0
1 0 1 1
0 1 1 0
0 1 1 1
0 1 0 1
1 1 1 1 0
1 1 1 1 1
1 1 1 0 1
1 1 0 1 0
1 1 0 1 1
1 0 1 1 0
1 0 1 1 1
1 0 1 0 1
0 1 1 1 0
0 1 1 1 1
0 1 1 0 1
0 1 0 1 0
0 1 0 1 1
1 1 0
1 1 1
1 0 1
0 1 0
0 1 1
Figure 5.4 Bitstrings with no 00, 1 ≤N ≤4
Again, Ļeorem 5.1 allows us to translate this immediately into a formula for
the generating function G(z) that enumerates such strings:
G(z) = 1 + z + (z + z2)G(z).
Ļus we have G(z) = (1 + z)/(1 −z −z2), which leads directly to the
result that the number of strings of length N with no two consecutive 0 bits
is FN +FN+1 = FN+2, a Fibonacci number. Chapter 8 covers many variations
of this kind.
Exercise 5.1 How many bitstrings of length N have no 000?
Exercise 5.2 How many bitstrings of length N have no 01?

șșȟ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.ș
Binary trees. For the class of binary trees, we have the construction
T = Z⊓⊔+ Z• × T × T .
If the size of a tree is its number of internal nodes, we translate Z⊓⊔to 1 and
Z• to z to get
T•(z) = 1 + zT•(z)2,
the functional equation deŀning the Catalan numbers that we have derived.
If the size of a tree is its number of external nodes, we translate Z• to 1 and
Z⊓⊔to z to get
T ⊓⊔(z) = z + T ⊓⊔(z)2,
which we can solve in a similar manner. Ļese equations also imply, for
example, that T ⊓⊔(z) = zT•(z), a consequence of the fact that the number of
external nodes in a binary tree is one greater than the number of internal nodes
(see Chapter 6). Ļe simplicity of the symbolic approach in studying prop-
erties of various types of trees is compelling, especially by comparison with
analyses using recurrences. We shall cover this topic in detail in Chapter 6.
Exercise 5.3 Let U be the set of binary trees with the size of a tree deŀned to be the
total number of nodes (internal plus external), so that the generating function for its
counting sequence is U(z) = z + z3 + 2z5 + 5z7 + 14z9 + . . . . Derive an explicit
expression for U(z).
Exercise 5.4 Deŀne a ”superleaf” in a binary tree to be an internal node whose four
grandchildren are all external nodes. What fraction of binary trees with N nodes
have no superleaves? From Figure 5.2, the answer for N = 1, 2, 3, and 4 is 0, 0, 4/5,
and 6/7, respectively.
TŔőśŞőř Ȝ.Ș ŕş ŏōŘŘőŐ a “transfer theorem” because it directly transfers one
mathematical formulation to another—it transfers a symbolic formula that
deŀnes structures to an equation involving a generating function that enu-
merates them. Ļe ability to do so is quite powerful. As long as we use
operations to deŀne a structure for which we have such a theorem, we know
that we will be able to learn something about the corresponding generating
function.
Ļere are a number of other operations that we use to build combina-
torial structures besides the union, Cartesian product, and sequence operations
corresponding to Ļeorem 5.1. Examples include set of or multiset of, as ex-
plored in the following two exercises. Ļese and other operations are covered

§Ȝ.Ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șșȠ
thoroughly in [8]. With these sorts of operations, we can deŀne and study
an unlimited range of of combinatorial structures, including a great many of
those that have been studied in classical combinatorics and a great many of
those that are important in the analysis of algorithms, as we will see in Chap-
ters 6 through 9.
Exercise 5.5 Let B be deŀned as the collection of all ŀnite subsets of A. If A(z) and
B(z) are the OGFs of A and B, show that
B(z) =
∏
n≥1
(1 + zn)An = exp
(
A(z) −1
2A(z2) + 1
3A(z3) −. . .
)
.
Exercise 5.6 Let B be deŀned as the collection of all ŀnite multisets of A (subsets
with repetitions allowed). If A(z) and B(z) are the OGFs of A and B, show that
B(z) =
∏
n≥1
1
(1 −zn)An = exp
(
A(z) + 1
2A(z2) + 1
3A(z3) + . . .
)
.
5.3 Symbolic Method for Labelled Classes. A primary feature of the
preceding section is that the individual items from which the combinatorial
objects are assembled are indistinguishable. An alternative paradigm is to as-
sume that the individual items are labelled and that, therefore, the order in
which items appear when assembled to make combinatorial objects is sig-
niŀcant. Labelled objects are normally enumerated by exponential generat-
ing functions. We will illustrate fundamental principles with basic examples
here, then we will study various families of labelled objects in detail in later
chapters.
To be speciŀc, if a combinatorial structure consists of N atoms, we con-
sider their labels to be the integers 1 to N, and we consider objects to be
diﬀerent when the labels are in diﬀerent order in the structure.
Figure 5.5 gives examples of three basic labelled combinatorial classes.
Ļe ŀrst is just a set of atoms—again, there is only one object of each size and
this is another way to encode natural numbers in unary (for reasons that will
become clear in Chapter 9, we refer to these as urns). Ļe second consists
of sequences of labelled atoms—the objects of size N are the permutations of
length N, all the possible ways to order the integers 1 through N, so there are
N! permutations of size N. Ļe third consists of cyclic sequences of labelled

șȚȗ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ț
C1=1
C2 = 1
1
2
C3 = 2
1
2
3
1
3
2
C4 = 6
4
3
1
2
4
2
1
3
3
2
1
4
3
4
1
2
2
4
1
3
2
3
1
4
Cycles (cyclic sequences of labelled atoms)
Permutations (sequences of labelled atoms)
Urns (sets of labelled atoms)
1
1
P1=1
U1=1
1  2
2  1
P2 = 2
1  2  3  
1  3  2  
2  1  3  
2  3  1  
3  1  2  
3  2  1  
P3 = 6
1  2  3  4
1  2  4  3
1  3  2  4
1  3  4  2
1  4  2  3
1  4  3  2
2  1  3  4
2  1  4  3
2  3  1  4
2  3  4  1
2  4  1  3
2  4  3  1
3  1  2  4
3  1  4  2
3  2  1  4
3  2  4  1
3  4  1  2
3  4  2  1
4  1  2  3
4  1  3  2
4  2  1  3
4  2  3  1
4  3  1  2
4  3  2  1
P4 = 24
U2 = 1
1
2
U3 = 1
3
1
2
U4 = 1
3
4  
1
2
1
Figure 5.5 Basic labelled classes, 1 ≤N ≤4

§Ȝ.Ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȚȘ
atoms—the objects of size N are known as the cycles of length N. Ļere are
(N −1)! cycles of size N since any of the (N −1)! permutations of size N −1
can appear after the 1 in order on the cycle.
Given a labelled class A with counting sequence {an}, we are interested
in the EGF
A(z) =
∑
n≥0
an
zn
n! =
∑
a∈A
z|a|
|a|!.
Again, this fundamental identity allows us to view the EGF as an analytic
form representing the counting sequence (the left sum) or as a combinatorial
form representing all the individual objects (the right sum). Table 5.2 gives
the counting sequences and EGFs for the classes in Figure 5.5.
For unlabelled objects, we described ways to assemble combinatorial
structures by means of the disjoint union, Cartesian product, and sequence
operations. Here, we are going to deŀne analogous constructions for labelled
objects. Ļe primary diﬀerence is that, in the analog to the Cartesian prod-
uct operation, it is necessary to relabel in a consistent manner so that only the
labels 1 to N appear if the resulting object has size N.
Speciŀcally, we deŀne the star product operation for two classes A and
B of labelled objects A ⋆B that gives the class of ordered pairs of objects,
one from A and one from B relabelled in all consistent ways. Figure 5.5 depicts
an example, the star product of a 3-cycle and a 2-cycle. Ļe result is a pair
of cycles that must be labelled with the labels 1 to 5. To do so, pick the
(5
3
) = 10 diﬀerent possibilities for the 3-cycle labels, and assign them to the
atoms of the 3-cycle, replacing the 1 with the smallest of the three, the 2 with
atoms
size function
counting sequence
EGF
urns
⃝
i
# of ⃝
i s
1
ez
permutations
⃝
i
# of ⃝
i s
N!
1
1 −z
cycles
⃝
i
# of ⃝
i s
(N −1)!
ln
1
1 −z
Table 5.2
Familiar labelled combinatorial classes

șȚș
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ț
    =  
1
2
1
2
3
1
2
3
4
5
1
5
2
1
3
4
1
4
2
3
5
1
3
2
4
5
2
3
1
4
5
2
4
3
5
2
5
1
3
4
1
3
4
1
2
5
3
5
2
4
4
5
1
2
3
Figure 5.6 Star product example
the middle value, and the 3 with the largest of the three. Ļen assign the
remaining 2 labels to the 2-cycle, in the same way. Ļis relabelling algorithm
is eﬀective for any pair of labelled objects.
Our basic toolbox for assembling new classes of structured labelled ob-
jects consists of the sum, star product, and these three additional operations:
SEQ (A) is the class of sequences of elements of A,
SET (A) is the class of sets of elements of A, and
CYC (A) is the class of cyclic sequences of elements of A.
As for unlabelled objects, SEQ (A) is the class ϵ + A + A ⋆A + . . .. With
these deŀnitions, we have the following transfer theorem for labelled classes.

§Ȝ.Ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȚȚ
Ļeorem 5.2 (Symbolic method for labelled class EGFs).
Let A and B be
classes of labelled combinatorial objects. If A(z) is the EGF that enumerates
A and B(z) is the EGF that enumerates B, then
A(z) + B(z) is the EGF that enumerates A + B,
A(z)B(z)
is the EGF that enumerates A ⋆B,
1
1 −A(z)
is the EGF that enumerates SEQ (A),
eA(z)
is the EGF that enumerates SET (A), and
ln
1
1 −A(z) is the EGF that enumerates CYC (A).
Proof. Ļe ŀrst part of the proof is the same as for Ļeorem 5.1. To prove
the second part, note that, for every k from 0 to n, we can pair any of the ak
objects of size k from A with any of the bn−k objects of size n −k from B to
get an object of size n in in A ⋆B. Ļe relabellings can be done in
(n
k
) ways
(simply choose the labels; their assignment is determined). Ļus, the number
of objects of size n in A ⋆B is
∑
0≤k≤n
(
n
k
)
akbn−k.
Again, a simple convolution leads to the desired result.
Ļe result for sequences is the same as for Ļeorem 5.1. Now, if there are
k identical components in a sequential arrangement and we relabel them in all
possible ways, each set of components appears k! times, because we “forget”
about the order between components. Ļus, we can compute the number of
sets by dividing the number of sequential arrangements by k!. Similarly, each
cyclic sequence appears k times, so the number of sequences is the number of
sequential arrangements divided by k. From these observations, if we deŀne
the more speciŀc operations
SEQ k(A), the class of k-sequences of elements of A;
SET k(A), the class of k-sets of elements of A;
CYC k(A), the class of k-cycles of elements of A;

șȚț
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ț
then it is immediate that
A(z)k
is the EGF that enumerates SEQ k(A),
A(z)k/k!
is the EGF that enumerates SET k(A),
A(z)k/k
is the EGF that enumerates CYC k(A),
and the results stated in the theorem follow by summing on k.
Again, thinking about tiny examples is a good way to clarify these def-
initions and associated EGF operations. For example, Z + Z + Z is a class
containing three objects of size 1 (a multiset), with EGF 3z; ⃝
1 ⃝
3 ⃝
2 is an
object of size 3 (a sequence), with EGF z3/6; SEQ 3(Z) = Z ⋆Z ⋆Z is a
class that contains six objects of size 3, with EGF z3; SET 3(Z) is a class that
contains one object of size 3, with EGF z3/6; and CYC 3(Z) is a class that
contains two objects of size 3, with EGF z3/3.
Permutations.
Let P be the set of all permutations, where the size of a
permutation is its length. Enumeration is elementary: the number of per-
mutations of length N is N!. One way to deŀne P is to use recursion, as
follows:
P = ϵ + Z ⋆P.
A permutation is either empty or corresponds precisely to the star product
of an atom and a permutation. Or, more speciŀcally, if PN is the class of
permutations of size N, then P0 = ϕ and, for N > 0,
PN = Z ⋆PN−1,
as illustrated in Figure 5.6. Ļeorem 5.2 allows us to translate directly from
these symbolic forms to functional equation satisŀed by the corresponding
generating functions. We have
P(z) = 1 + zP(z),
so P(z) = 1/(1 −z) and PN = N![zN]P(z) = N!, as expected. Or, we can
translate the construction for PN to get
PN(z) = zPN−1(z),
which telescopes to give PN(z) = zN and PN = N![zN]PN(z) = N! or
P(z) = 1/(1 −z) by the sum rule of Ļeorem 5.2, since P = ∑
N≥0 PN. A

§Ȝ.Ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȚȜ
      =  
      =  
      =  
      =  
      =  
      =  
1
1  2  3  
1  2  3  4
2  1  3  4
3  1  2  4
4  1  2  3
1
2  1  3  
1  3  2  4
2  3  1  4
3  2  1  4
4  2  1  3
1
3  1  2  
1  4  2  3
2  4  1  3
3  4  1  2
4  3  1  2
1
1  3  2  
1  2  4  3
2  1  4  3
3  1  4  2
4  1  3  2
1
2  3  1  
1  3  4  2
2  3  4  1
3  2  4  1
4  2  3  1
1
3  2  1  
1  4  3  2
2  4  3  1
3  4  2  1
4  3  2  1
Figure 5.7 Z ⋆P3 = P4
third alternative is to view P as being formed of sequences of labelled atoms,
deŀned by the combinatorial construction P = SEQ (Z), which again gives
P(z) = 1/(1 −z) by the sequence rule of Ļeorem 5.2. Again, this example
is fundamental, and just a starting point.
Sets and cycles. Similarly, we may view the class of urns U as being formed
of sets of labelled atoms and the class of cycles C as being formed of cycles
of labelled atoms, so that U(z) = ez by the set rule of Ļeorem 5.2 and
C(z) = ln(1/(1 −z)) by the cycle rule of Ļeorem 5.2, which gives UN = 1
and CN−1 = (N −1)! as expected.
Sets of cycles.
Next, we consider a fundamental example where we com-
pound two combinatorial constructions. Speciŀcally, consider the class P∗
formed by taking sets of cycles, as illustrated for small sizes in Figure 5.7.

șȚȝ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ț
Symbolically, we have the construction
P∗= SET (CYC (Z)),
which, by the cycle and sequence rules in Ļeorem 5.2, gives the EGF
P ∗(z) = exp
(
ln
1
1 −z
)
=
1
1 −z ,
so that P ∗
N = N![zN]P ∗(z) = N!. Ļat is, the number of labelled sets of
cycles of N items is precisely N!, the number of permutations. Ļis is a fun-
damental result, which we now brieły examine in more detail (foreshadowing
Chapter 7).
1  2  3  
1  2
1
P1=1
P2 = 2
P3 = 6
1
2
3
2
1
3
3
1
2
3
4
1
3
2
4
1
4
2
3
1
2
3
4
1
1
2
1
2
3
2
3
4
1
2
4
3
2
1
3
4
2
1
4
3
3
1
2
4
3
1
4
2
4
1
2
3
4
1
3
2
1
3
2
1  2  3  4  
1  2  
2
4
1  3  
2
3
1  4  
1
4
2  3  
1
3
2  4  
1
2
3  4  
P4 = 24
4
3
1
2
3
4
1
2
4
2
1
3
2
4
1
3
3
2
1
4
2
3
1
4
Figure 5.8 Sets of cycles, 1 ≤N ≤4

§Ȝ.Ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȚȞ
A bijection. Figure 5.9 illustrates a 1:1 correspondence between permuta-
tions and sets of cycles. To more easily visualize the problem, suppose that N
students have the names 1 through N and each owns a hat labelled with their
name. On graduation each throws their hat in the air and then grabs a ran-
dom hat. Ļis process deŀnes a random permutation, drawn from the labelled
class that we have just considered. Permutations are so well studied in com-
binatorics that many other colorful images have been proposed, ranging from
hats at the opera to beds occupied by drunken sailors—use your imagination!
One way to write down a permutation, called the two-line representation,
is to write the student names 1 through N in order on the ŀrst line and the
number of the hat that each student grabbed below her or his name on the
second line. In the permutation depicted in Figure 5.9, 1 grabbed 9’s hat,
2 grabbed 3’s hat, and so forth. Now, imagine that student 1, having hat 9,
asks student 9 whether he or she has hat 1, then since student 9 has hat 4,
goes on to ask student 4, and so forth, continuing until ŀnding hat 1. (Take a
moment to convince yourself that you could always ŀnd your hat in this way.)
Ļis leads to another way to write down a permutation, called the set-of-cycles
representation, In the permutation depicted in Figure 5.9, 1 has 9’s hat, 9 has
4’s hat, and 4 has 1’s hat, and so forth. Everyone is on a cycle and the cycles
are disjoint since everyone has one hat, and their order is irrelevant, so every
permutation corresponds to a set of cycles. Conversely, given a set of cycles,
one can easily write down the two-line representation, so we have deŀned a
bijection. Ļis bijection is an alternate proof that the number of labelled sets
of cycles of N items is precisely N!.
In the present context, our interest is in the fact that the simple struc-
ture of the set-of-cycles combinatorial construction leads to easy solutions to
problems that are considerably more diﬃcult to solve otherwise. We consider
one example next and several similar examples in Chapters 7 and 9.
5
8
6
2
3
1
9
4
7  
set-of-cycles representation
two-line representation
1   2   3   4   5   6   7   8   9
9   3   2   1   8   5   7   6   4 
Figure 5.9 A permutation is a set of cycles.

șȚȟ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ț
Derangements. Again, the true power of analytic combinatorics is apparent
when we consider variations of fundamental constructions. As an example, we
consider the famous derangements problem: When our N graduating students
throw their hats in the air as described previously, what is the probability that
everyone gets someone else’s hat? As you can see from Figure 5.10, the answer
for N = 2, 3, and 4 is 1/2, 1/3, and 3/8, respectively. Ļis is precisely the
number of sets of cycles of N items with no singleton cycles, divided by N!.
For the present our interest is to note that the desired quantity is [zN]D(z),
where D(z) is the EGF associated with the combinatorial class D, the sets of
cycles of N items where all cycles are of size greater than 1. (In this case, the
EGF is a PGF.) Ļen the simple combinatorial construction
D = SET (CYC 2(Z) + CYC 3(Z) + CYC 4(Z) + . . .)
immediately translates to the EGF
D2 = 1
D3 = 2
1
3
2
4
1
4
2
3
1
2
3
4
1
2
1
2
3
1
3
2
D4 = 9
4
3
1
2
3
4
1
2
4
2
1
3
2
4
1
3
3
2
1
4
2
3
1
4
1 2 3 4
2 1 4 3 
1 2 3 4
3 4 1 2 
1 2 3 4
4 3 2 1 
1 2 3 4
2 3 4 1 
1 2 3 4
2 4 1 3 
1 2 3 4
3 4 2 1 
1 2 3 4
3 1 4 2 
1 2 3 4
4 3 1 2 
1 2 3 4
4 1 2 3 
1 2 3
2 3 1 
1 2
2 1
1 2 3
3 1 2 
Figure 5.10 Derangements, 2 ≤N ≤4

§Ȝ.Ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȚȠ
D(z) = exp
(z2
2 + z3
3 + z4
4 + . . .
)
= exp
(
ln
1
1 −z −z
)
= e−z
1 −z .
An even simpler derivation is to note that a permutation is the star prod-
uct of a set of singleton cycles (an urn) and a derangement, so we have the
construction
SET (Z) ⋆D = P,
which translates to the EGF equation
ezD(z) =
1
1 −z
and leads to the same result. Ļus D(z) is a simple convolution, and
DN =
∑
0≤k≤N
(−1)k
k!
∼1/e.
Note that the exact formula agrees with the observed values for N = 2, 3,
and 4; the asymptotic estimate is one that we discussed in §4.4.
Generalized derangements. Again, the true power of analytic combinatorics
is apparent when we consider variations of fundamental constructions. For
example, when our students throw their hats in the air, what is the probability
that every group of M students gets a hat from someone outside the group?
Ļis is precisely the number of sets of cycles of N items where all cycles are
of size greater than M, divided by N!. As earlier, the desired quantity is
[zN]P ∗
>M(z), where P ∗
>M(z) is the EGF associated with the combinatorial
class P>M, the combinatorial class of sets of cycles of N items where all cycles
are of size greater than M. Ļen we have the construction
P∗
>M = SET (CYC >M(Z))

șțȗ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ț
(where, for brevity, we use the abbreviation CYC >M(Z) for CYC M+1(Z) +
CYC M+2(Z)+CYC M+3(Z)+. . .), which immediately translates to the EGF
P ∗
>M(z) = exp
( zM+1
M + 1 + zM+2
M + 2 + zM+3
M + 3 + . . .
)
= exp
(
ln
1
1 −z −z −z2
2 −. . . −zM
M
)
= e−z−z2/2...−zM/M
1 −z
.
In summary, the symbolic method immediately leads to a simple expres-
sion for the EGF that might otherwise be complicated to derive. Extracting
the coeﬃcients from this generating function appears to be not so easy, but
we shall soon consider a transfer theorem that gives an asymptotic estimate
directly.
Exercise 5.7 Derive an EGF for the number of permutations whose cycles are all of
odd length.
Exercise 5.8 Derive an EGF for sequences of cycles.
Exercise 5.9 Derive an EGF for cycles of sequences.
TŔőśŞőř Ȝ.ș ŕş ō transfer theorem for labelled objects that immediately gives
EGF equations for the broad variety of combinatorial classes that can be de-
scribed by the operations we have considered. We will consider a number
of examples in Chapters 6 through 9. As for unlabelled classes, several ad-
ditional operations for labelled classes have been invented (and are covered
thoroughly in [8]), but these basic operations serve us well for studying a
broad variety of classes in Chapters 6 through 9.

§Ȝ.ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șțȘ
5.4 Symbolic Method for Parameters. Ļe symbolic method is also ef-
fective for developing equations satisŀed by BGFs associated with combina-
torial parameters, as introduced in Chapter 3. Indeed, transfer theorems to
BGF equations associated with natural parameters from the very same com-
binatorial constructions that we have already considered are readily available.
In this section, we state the theorems for unlabelled and labelled classes
and then give a basic example of the application of each, reserving more appli-
cations for our studies of various kinds of combinatorial classes in Chapters 6
through 9. Once one has understood the basic theorems for enumeration, the
corresponding theorems for analysis of parameters are straightforward. For
brevity, we state the theorems only for the basic constructions and leave the
proofs for exercises.
Given an unlabelled class A with a parameter deŀned as a cost function
that is deŀned for every object in the class, we are interested in the ordinary
bivariate generating function (OBGF)
A(z, u) =
∑
n≥0
∑
k≥0
ankznuk,
where ank is the number of objects of size n and cost k. As we have seen, the
fundamental identity
A(z, u) =
∑
n≥0
∑
k≥0
ankznuk =
∑
a∈A
z|a|ucost(a)
allows us to view the OGF as an analytic form representing the double count-
ing sequence for size and cost (the left sum) or as a combinatorial form rep-
resenting all the individual objects (the right sum).
Ļeorem 5.3 (Symbolic method for unlabelled class OBGFs).
Let A and
B be unlabelled classes of combinatorial objects. If A(z, u) and B(z, u) are
the OBGFs associated with A and B, respectively, where z marks size and u
marks a parameter, then
A(z, u) + B(z, u) is the OBGF associated with A + B,
A(z, u)B(z, u)
is the OBGF associated with A × B, and
1
1 −A(z, u)
is the OBGF associated with SEQ (A).
Proof. Omitted.

șțș
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.ț
Similarly, given a labelled class A with a parameter deŀned as a cost
function that is deŀned for every object in the class, we are interested in the
exponential bivariate generating function (EBGF)
A(z, u) =
∑
n≥0
∑
k≥0
ank
zn
n! uk,
where ank is the number of objects of size n and cost k. Again, the funda-
mental identity
A(z, u) =
∑
n≥0
∑
k≥0
ank
zn
n! uk =
∑
a∈A
z|a|
|a|!ucost(a)
allows us to view the OGF as an analytic form representing the double count-
ing sequence for size and cost (the left sum) or as a combinatorial form rep-
resenting all the individual objects (the right sum).
Ļeorem 5.4 (Symbolic method for labelled class EBGFs).
Let A and B
be classes of labelled combinatorial objects. If A(z, u) and B(z, u) are the
EBGFs associated with A and B, respectively, where z marks size and u marks
a parameter, then
A(z, u) + B(z, u) is the EBGF associated with A + B,
A(z, u)B(z, u)
is the EBGF associated with A ⋆B,
1
1 −A(z, u)
is the EBGF associated with SEQ (A),
eA(z,u)
is the EBGF associated with SET (A), and
ln
1
1 −A(z, u)
is the EBGF associated with CYC (A).
Proof. Omitted.
Exercise 5.10 Extend the proof of Ļeorem 5.1 to give a proof of Ļeorem 5.3.
Exercise 5.11 Extend the proof of Ļeorem 5.2 to give a proof of Ļeorem 5.4.

§Ȝ.ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șțȚ
Note that taking u = 1 in these theorems transforms them to Ļeorem 5.1
and Ļeorem 5.2. Developing equations satisŀed by BGFs describing para-
meters of combinatorial classes is often immediate from the same construc-
tions that we used to derive GFs that enumerate the classes, slightly aug-
mented to mark parameter values as well as size. To illustrate the process, we
consider three classic examples.
Bitstrings.
How many bitstrings of length N have k 1-bits? Ļis well-
known quantity, which we already discussed in Chapter 3, is the binomial
coeﬃcient
(N
k
). Ļe derivation is immediate with the symbolic method. In
the construction
B = ϵ + (Z0 + Z1) × B,
we use the BGF z for Z0 and the BGF zu for Z1 and then use Ļeorem 5.4
to translate directly to the BGF equation
B(z, u) = 1 + z(1 + u)B(z, u),
so that
B(z, u) =
1
1 −(1 + u)z =
∑
N≥0
(1 + u)NzN =
∑
N≥0
∑
k≥0
(
N
k
)
zNuk,
as expected. Alternatively, we could use the construction
B = SEQ (Z0 + Z1)
to get the same result by the sequence rule of Ļeorem 5.4. Ļis example is
fundamental, and just a starting point.
Cycles in permutations. What is the average number of cycles in a permu-
tation of length N? From inspection of Figure 5.8, you can check that the
cumulative counts for N = 1, 2, 3, and 4 are 1, 3, 11, and 50, respectively.
Symbolically, we have the construction
P∗= SET (CYC (Z)),
which, by the cycle and sequence rules in Ļeorem 5.4 (marking each cycle
with u), gives the EGF
P ∗(z) = exp
(
u ln
1
1 −z
)
=
1
(1 −z)u .

șțț
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.ț
From this explicit representation of the BGF, we can use the techniques de-
scribed at length in Chapter 3 to analyze parameters. In this case,
P(z, 1) =
1
1 −z ,
as expected, and
Pu(z, 1) =
1
1 −z ln
1
1 −z ,
so the average number of cycles in a random permutation is
N![zN]Pu(z, 1)
N![zN]P(z, 1) = HN.
Leaves in binary trees. What proportion of the internal nodes in a binary
tree of size N have two external children? Such nodes are called leaves. From
inspection of Figure 5.2, you can check that the total numbers of such nodes
for N = 0, 1, 2, 3, and 4 are 0, 1, 2, 6, and 20, respectively. Dividing by the
Catalan numbers, the associated proportions are 0, 1, 1, 6/5 and 10/7. In
terms of the BGF
T(z, u) =
∑
t∈T
z|t|uleaves(t)
the following are the coeﬃcients of z0,z1, z2, z3, z4, respectively, and are
rełected directly in the trees in Figure 5.2:
u0
u1
u1 + u1
u1 + u1 + u2 + u1 + u1
u1 + u1 + u2 + u1 + u1 + u2 + u2 + u1 + u1 + u2 + u1 + u1 + u2 + u2.
Adding these terms, we know that
T(z, u) = 1 + z1u + 2z2u + z3(4u + u2) + z4(8u + 6u2) + . . . .

§Ȝ.ț
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șțȜ
Checking small values, we ŀnd that
T(z, 1) = 1 + z1 + 2z2 + 5z3 + 14z4 + . . .
and
Tu(z, 1) = z1 + 2z2 + 6z3 + 20z4 + . . .
as expected. To derive a GF equation with the symbolic method, we add Z•
to both sides of the standard recursive construction to get
T + Z• = E + Z• + Z• × T × T .
Ļis gives us a way to mark leaves (by using the BGF zu for the Z• term
on the right) and to balance the equation for the tree of size 1. Applying
Ļeorem 5.3 (using the BGF z for the Z• term on the left and the Z• factor
on the rightmost term, since neither corresponds to a leaf) immediately gives
the functional equation
T(z, u) + z = 1 + zu + zT(z, u)2.
Setting u = 1 gives the OGF for the Catalan numbers as expected and dif-
ferentiating with respect to u and evaluating at u = 1 gives
Tu(z, 1) = z + 2zT(1, z)Tu(z, 1)
=
z
1 −2zT(z, 1)
=
z
√1 −4z .
Ļus, by the standard BGF calculation shown in Table 3.6, the average num-
ber of internal nodes with both nodes external in a binary tree of size n is
[zn]
z
√1 −4z
1
n + 1
(
2n
n
) =
(
2n −2
n −1
)
1
n + 1
(
2n
n
) = (n + 1)n
2(2n −1)
(see §3.4 and §3.8), which tends to n/4 in the limit. About 1/4 of the internal
nodes in a binary tree are leaves.

șțȝ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.ț
Exercise 5.12 Conŀrm that the average number of 1 bits in a random bitstring is
N/2 by computing Bu(z, 1).
Exercise 5.13 What is the average number of 1 bits in a random bitstring of length
N having no 00?
Exercise 5.14 What is the average number of cycles in a random derangement?
Exercise 5.15 Find the average number of internal nodes in a binary tree of size n
with both children internal.
Exercise 5.16 Find the average number of internal nodes in a binary tree of size n
with one child internal and one child external.
Exercise 5.17 Find an explicit formula for T(z, u) and compute the variance of the
number of leaves in binary trees.
TŔŕş ŎŞŕőŒ ŕŚŠŞśŐšŏŠŕśŚ śŚŘť scratches the surface of what is known about
the symbolic method, which is one of the cornerstones of modern combinato-
rial analysis. Ļe symbolic method summarized by Ļeorem 5.1, Ļeorem 5.2,
Ļeorem 5.3, and Ļeorem 5.4 works for an ever-expanding set of structures,
though it cannot naturally solve all problems: some combinatorial objects
just have too much internal “cross-structure” to be amenable to this treat-
ment. But it is a method of choice for combinatorial structures that have a
nice decomposable form, such as trees (Chapter 6), the example par excel-
lence; permutations (Chapter 7); strings (Chapter 8); and words or mappings
(Chapter 9). When the method does apply, it can succeed spectacularly, espe-
cially in allowing quick analysis of variants of basic structures.
Much more information about the symbolic method may be found in
Goulden and Jackson [9] or Stanley [13]. In [8], we give a thorough treat-
ment of the method (see also [14] for more information in the context of the
analysis of algorithms). Ļe theory is suﬃciently complete that it has been
embodied in a computer program that can automatically determine generat-
ing functions for a structure from a simple recursive deŀnition, as described
in Flajolet, Salvy, and Zimmerman [7].
Next, we consider the second stage of analytic combinatorics, where
we pivot from the symbolic to the analytic so that we may consider consider
transfer theorems that take us from GF representations to coeﬃcient approx-
imations, with similar ease.

§Ȝ.Ȝ
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șțȞ
5.5 Generating Function Coeﬃcient Asymptotics. Ļe constructions
associated with the symbolic method yield an extensive variety of generating
function equations. Ļe next challenge in analytic combinatorics is to transfer
those GF equations to useful approximations of counting sequences for those
classes. In this section, we brieły review examples of theorems that we have
seen that are eﬀective for such transfers and develop another theorem. While
indicative of the power of analytic combinatorics and useful for many of the
classes that we consider in this book, these theorems are only a starting point.
In [8], we use complex-analytic techniques to develop remarkably general
transfer theorems that, paired with the symbolic method, provide a basis for
the assertion “if you can specify it, you can analyze it.”
Taylor's theorem. A ŀrst example of a transfer theorem is the ŀrst method
that we considered in Chapter 3 for extracting GF coeﬃcients. Simply put,
Taylor’s theorem says that
[zn]f(z) = f(n)(0)
n!
,
provided that the derivatives exist. As we have seen, it is an eﬀective method
for extracting coeﬃcients for 1/(1 −2z) (the OGF for bitstrings), ez (the
EGF for permutations), and many other elementary GF equations that derive
from the symbolic method. While Taylor’s theorem is eﬀective in principle
for a broad class of GF expansions, it speciŀes exact values that can involve
detailed calculations, so we generally prefer transfer theorems that can directly
give the asymptotic estimates that we ultimately seek.
Exercise 5.18 Use Taylor’s theorem to ŀnd [zN] e−z
1 −z .
Rational functions.
A second example of a transfer theorem is Ļeorem
4.1, which gives asymptotics for coeﬃcients of rational functions (of the form
f(z)/g(z), where f(z) and g(z) are polynomials). To review from §4.1, the
growth of the coeﬃcients depends on the root 1/β of g(z) of largest modulus.
If the multiplicity of 1/β is 1, then
[zn]f(z)
g(z) = −βf(1/β)
g′(β)
βn.
For example, this is an eﬀective method for extracting coeﬃcients for (1 +
z)/(1 −z −z2) (the OGF for bitstrings with no 00) and similar GFs. We
study such applications in detail in Chapter 8.

șțȟ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ȝ
Exercise 5.19 Use Ļeorem 4.1 to show that the number of bitstrings having no
occurrence of 00 is ∼ϕN/
√
5.
Exercise 5.20 Find an approximation for the number of bitstrings having no occur-
rence of 01.
Radius-of-convergencebounds. It has been known since Euler and Cauchy
that knowledge of the radius of convergence of a generating function provides
information on the rate of growth of its coeﬃcients. Speciŀcally, if f(z) is a
power series that has radius of convergence R > 0, then [zn]f(z) = O(r−n)
for any positive r < R. Ļis fact is easy to prove: Take any r such that
0 < r < R and let fn = [zn]f(z). Ļe series ∑
n fnrn converges; hence its
general term fnrn tends to zero and in particular is bounded from above by a
constant.
For example, the Catalan generating function converges for |z| < 1/4
since it involves (1 −4z)1/2 and the binomial series (1 + u)1/2 converges for
|u| < 1. Ļis gives us the bound
[zn]1 −√1 −4z
2
= O((4 + ϵ)n)
for any ϵ. Ļis is weaker form of what we derived from Stirling’s formula in
§4.4.
In the case of combinatorial generating functions, we can considerably
strengthen these bounds. More generally, let f(z) have positive coeﬃcients.
Ļen
[zn]f(z) ≤min
x∈(0,R)
f(x)
xn .
Ļis follows simply from fnxn ≤f(x), as fnxn is just one term in a conver-
gent sum of positive quantities. In particular, we will again make use of this
very general bounding technique in Chapter 8 when we discuss permutations
with restricted cycle lengths.
Exercise 5.21 Prove that there exists a constant C such that
[zn] exp(z/(1 −z)) = O(exp(C√n)).
Exercise 5.22 Establish similar bounds for the OGF of integer partitions
[zn]
∏
k≥1
(1 −zk)−1.

§Ȝ.Ȝ
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șțȠ
More speciŀcally, we can use convolution and partial fraction decom-
position to develop coeﬃcient asymptotics for GFs that involve powers of
1/(1−z), which arise frequently in derivations from combinatorial construc-
tions. For instance, if f(z) is a polynomial and r is an integer, then partial
fraction decomposition (Ļeorem 4.1) yields
[zn]
f(z)
(1 −z)r ∼f(1)
(
n + r −1
n
)
∼f(1) nr−1
(r −1)!,
provided of course f(1) ̸= 0. A much more general result actually holds.
Ļeorem 5.5 (Radius-of-convergence transfer theorem).
Let f(z) have
radius of convergence strictly larger than 1 and assume that f(1) ̸= 0. For
any real α ̸∈{0, −1, −2, . . .}, there holds
[zn]
f(z)
(1 −z)α ∼f(1)
(
n + α −1
n
)
∼f(1)
 (α)nα−1.
Proof. Let f(z) have radius of convergence > r, where r > 1. We know
that fn ≡[zn]f(z) = O(r−n) from radius-of-convergence bounds, and in
particular the sum ∑
n fn converges to f(1) geometrically fast.
It is then a simple matter to analyze the convolution:
[zn]
f(z)
(1 −z)α = f0
(
n + α −1
n
)
+ f1
(
n + α −2
n −1
)
+ · · · + fn
(
α −1
0
)
=
(
n + α −1
n
)(
f0 + f1
n
n + α −1
+ f2
n(n −1)
(n + α −1)(n + α −2)
+ f3
n(n −1)(n −2)
(n + α −1)(n + α −2)(n + α −3) + · · ·
)
.
Ļe term of index j in this sum is
fj
n(n −1) · · · (n −j + 1)
(n + α −1)(n + α −2) · · · (n + α −j),

șȜȗ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ȝ
which tends to fj when n →+∞. From this, we deduce that
[zn]f(z)(1 −z)−α ∼
(
n + α −1
n
)
(f0 + f1 + · · · fn) ∼f(1)
(
n + α −1
n
)
,
since the partial sums f0 + · · · + fn converge to f(1) geometrically fast. Ļe
approximation to the binomial coeﬃcient follows from the Euler-Maclaurin
formula (see Exercise 4.60).
In general, coeﬃcient asymptotics are determined by the behavior of the
GF near where it diverges. When the radius of convergence is not 1, we can
rescale to a function for which the theorem applies. Such rescaling always
introduces a multiplicative exponential factor.
Corollary Let f(z) have radius of convergence strictly larger than ρ and as-
sume that f(ρ) ̸= 0. For any real α ̸∈{0, −1, −2, . . .}, there holds
[zn]
f(z)
(1 −z/ρ)α ∼f(ρ)
 (α)ρnnα−1.
Proof. Let g(z) = f(z/ρ). Ļen [zn]g(z) = ρn[zn]g(ρz) = ρn[zn]f(z).
While it has limitations, Ļeorem 5.5 (and its corollary) is eﬀective for
extracting coeﬃcients from many of the GFs that we encounter in this book.
It is an outstanding example of the analytic transfer theorems that comprise
the second phase of analytic combinatorics. We consider next three classic
examples of applying the theorem.
Generalized derangements. In §5.3, we used the symbolic method to show
that the probability that a given permutation has no cycles of length less than
or equal to M is is [zN]P ∗
>M(z), where
P ∗
>M(z) = e−z−z2/2...−zM/M
1 −z
From this expression, Ļeorem 5.5 gives immediately
[zN]P ∗
>M(z) ∼
1
eHM .

§Ȝ.Ȝ
A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȜȘ
symbolic
transfer
Teorem 5.2
analytic
transfer
Teorem 5.5
combinatorial
construction
GF equation
coeﬃcient
asymptotics
SET (CYC>M(Z ))
e−z−z2/2−...−zM/M
1 −z
∼N!
eHM
Figure 5.11 Analytic combinatorics to count generalized derangements
Figure 5.11 summarizes the ease with which analytic combinatorics gets to
this result through two general transfer theorems. While it is possible to get
to the same result with direct calculations along the lines of the proofs of
the theorems, such calculations can be lengthy and complicated. One of the
prime tenets of analytic combinatorics is that such detailed calculations are
often not necessary, as general transfer theorems can provide accurate results
for a great variety of combinatorial classes, as they did in this case.
Catalan numbers. Similarly, the corollary to Ļeorem 5.5 immediately pro-
vides a transfer from the Catalan GF
T(z) = 1 −√1 −4z
2
to the asymptotic form of its coeﬃcients: Ignore the constant term, and take
α = −1/2 and f(z) = −1/2 to get the asymptotic result
TN ∼
4N
N
√
πN
.
While this simple derivation is very appealing, we note that is actually possible
to derive the result directly from the form
T(z) = z + T(z)2
using a general transfer theorem based on complex-analytic techniques that
is beyond the scope of this book (see [8]).

șȜș
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
§Ȝ.Ȝ
Classic application. A famous example of the application of this technique
(see Comtet [5]) is to the function
f(z) = ez/2+z2/4
√1 −z
that is the EGF of the so-called 2-regular graphs. It involves a numerator
g(z) = exp(z/2 + z2/4) that has radius of convergence clearly equal to ∞.
Again Ļeorem 5.5 immediately gives the result
[zn]f(z) ∼e3/4
√πn.
Ļe ease with the asymptotic form of the coeﬃcients can be read from the
GFs in these and many other applications is quite remarkable.
WŕŠŔ ŞőşšŘŠş şšŏŔ ōş Ļeorem 5.5, the asymptotic form of coeﬃcients is
directly “transferred” from elements like (1−z)−α (called “singular elements”)
that play a role very similar to partial fraction elements in the analysis of ra-
tional functions. And deeper mathematical truths are at play. Ļeorem 5.5
is only the simplest of a whole set of similar results originating with Darboux
in the last century (see [5] and [16]), and further developed by Pólya and
Szegö, Bender, and others [1][6]. Ļese methods are discussed in detail in
[8]; unlike what we could do here, their full development requires the the-
ory of functions of a complex variable. Ļis approach to asymptotics is called
singularity analysis.
Ļe transfer theorems that we have considered in this section yield co-
eﬃcient asymptotics from explicit GF formulae. It is also possible to work
directly with implicit GF representations, like the T(z) = z +T(z)2 equation
that we get for binary trees. In §6.12, we will consider the Lagrange Inversion
Ļeorem, a very powerful tool for extracting coeﬃcients from such implicit
representations. General transfer theorems based on inversion play a central
role in analytic combinatorics (see [8]).
Exercise 5.23 Show that the probability that all the cycles are of odd length in a
random permutation of length N is 1/
√
πN/2 (see Exercise 5.7).
Exercise 5.24 Give a more precise version of Ļeorem 5.5—extend the asymptotic
series to three terms.

A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȜȚ
Exercise 5.25 Show that
[zn]f(z)ln
1
1 −z ∼f(1)
n .
Exercise 5.26 Give a transfer theorem like Ļeorem 5.5 for
[zn]f(z)
1
1 −z ln
1
1 −z .
A
NALYTIC combinatorics is a calculus for the quantitative study of large
combinatorial structures. It has been remarkably successful as a general
approach for analyzing classical combinatorial structures of all sorts, including
those that arise in the analysis of algorithms. Table 5.3, which summarizes
just the results derived in this chapter, indicates the breadth of applicability
of analytic combinatorics, even though we have considered only a few basic
transfer theorems. Interested readers may ŀnd extensive coverage of advanced
transfer theorems and many more applications in our book [8].
More important, analytic combinatorics remains an active and vibrant
ŀeld of research. New transfer theorems, an ever-widening variety of combi-
natorial structures, and applications in a variety of scientiŀc disciplines con-
tinue to be discovered. Ļe full story of analytic combinatorics certainly re-
mains to be written.
Ļe remainder of this book is devoted to surveying classic combinato-
rial structures and their relationship to a variety of important computer algo-
rithms. Since we are primarily studying fundamental structures, we will of-
ten use analytic combinatorics. But another important theme is that plenty
of important problems stemming from the analysis of algorithms are still not
fully understood. Indeed, some of the most challenging problems in analytic
combinatorics derive from structures deŀned in classic computer algorithms
that are simple, elegant, and broadly useful.

șȜț
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
construction
symbolic
transfer
GF
analytic
transfer
coeﬃcient
asymptotics
Unlabelled classes
integers
SEQ (Z)
5.1
1
1 −z
Taylor
1
bistrings
SEQ (Z0 + Z1)
5.1
1
1 −2z
Taylor
2N
bitstrings
with no 00
G = ϵ + Z0
+(Z1 + Z01) × G
5.1
1 + z
1 −z −z2
4.1
∼ϕN
√
5
binary
trees
T = ⊓⊔+ T × T
5.1
1 −√1 −4z
2
5.5
corollary
4N
N
√
πN
bytestrings SEQ (Z0 + . . . + ZM−1)
5.1
1
1 −Mz
Taylor
M N
Labelled classes
urns
SET (Z)
5.2
ez
Taylor
1
permutations
SEQ (Z)
5.2
1
1 −z
Taylor
N!
cycles
CYC (Z)
5.2
ln
1
1 −z
Taylor
(N −1)!
derangements
SET (CYC >1(Z ))
5.2
e−z
1 −z
5.5
∼N!
e
generalized
derangements
SET (CYC >M(Z ))
5.2
e−z...−zM/M
1 −z
5.5
∼N!
eHM
Table 5.3
Analytic combinatorics examples in this chapter

A Ś ō Ř ť Š ŕ ŏ
C ś ř Ŏ ŕ Ś ō Š ś Ş ŕ ŏ ş
șȜȜ
References
1. E. A. BőŚŐőŞ. “Asymptotic methods in enumeration,” SIAM Review
16, 1974, 485–515.
2. E. A. BőŚŐőŞ ōŚŐ J. R. GśŘŐřōŚ. “Enumerative uses of generating
functions,” Indiana University Mathematical Journal 2, 1971, 753–765.
3. F. BőŞœőŞśŚ, G. LōŎőŘŘő, ōŚŐ P. LőŞśšŤ. Combinatorial Species and
Tree-like Structures, Cambridge Universtiy Press, 1998.
4. N. CŔśřşŗť ōŚŐ M. P. SŏŔƩŠŦőŚŎőŞœőŞ. “Ļe algebraic theory of
context-free languages,” in Computer Programming and Formal Languages,
P. Braﬀort and D. Hirschberg, eds., North Holland, 1963, 118–161.
5. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
6. P. FŘōŖśŘőŠ ōŚŐ A. OŐŘťŦŗś. “Singularity analysis of generating func-
tions,” SIAM Journal on Discrete Mathematics 3, 1990, 216–240.
7. P. FŘōŖśŘőŠ, B. SōŘŢť, ōŚŐ P. ZŕřřőŞřōŚ. “Automatic average-case
analysis of algorithms,” Ļeoretical Computer Science 79, 1991, 37–109.
8. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
9. I. GśšŘŐőŚ ōŚŐ D. JōŏŗşśŚ. Combinatorial Enumeration, John Wiley,
New York, 1983.
10. G. PƖŘťō. “On picture-writing,” American Mathematical Monthly 10,
1956, 689–697.
11. G. PƖŘťō, R. E. TōŞŖōŚ, ōŚŐ D. R. WśśŐş. Notes on Introductory Com-
binatorics, Progress in Computer Science, Birkhäuser, 1983.
12. V. N. SōŏŔŗśŢ. Combinatorial Methods in Discrete Mathematics, volume
55 of Encyclopedia of Mathematics and its Applications, Cambridge Uni-
verstiy Press, 1996.
13. R. P. SŠōŚŘőť. Enumerative Combinatorics, Wadsworth & Brooks/Cole,
1986, 2nd edition, Cambridge, 2011.
14. J. S. VŕŠŠőŞ ōŚŐ P. FŘōŖśŘőŠ. “Analysis of algorithms and data struc-
tures,” in Handbook of Ļeoretical Computer Science A: Algorithms and Com-
plexity, J. van Leeuwen, ed., Elsevier, Amsterdam, 1990, 431–524.
15. E. T. WŔŕŠŠōŗőŞ ōŚŐ G. N. WōŠşśŚ. A Course of Modern Analysis,
Cambridge University Press, 4th edition, 1927.

șȜȝ
C Ŕ ō Ŝ Š ő Ş
F ŕ Ţ ő
16. H. WŕŘŒ. Generatingfunctionology, Academic Press, San Diego, 1990,
2nd edition, A. K. Peters, 2006.

C H A P T E R S I X
T R E E S
T
REES are fundamental structures that arise implicitly and explicitly in
many practical algorithms, and it is important to understand their prop-
erties in order to be able to analyze these algorithms. Many algorithms con-
struct trees explicitly; in other cases trees assume signiŀcance as models of
programs, especially recursive programs. Indeed, trees are the quintessential
nontrivial recursively deŀned objects: a tree is either empty or a root node
connected to a sequence (or a multiset) of trees. We will examine in detail
how the recursive nature of the structure leads directly to recursive analyses
based upon generating functions.
We begin with binary trees, a particular type of tree ŀrst introduced in
Chapter 3. Binary trees have many useful applications and are particularly
well suited to computer implementations. We then consider trees in gen-
eral, including a correspondence between general and binary trees. Trees and
binary trees are also directly related to several other combinatorial structures
such as paths in lattices, triangulations, and ruin sequences. We discuss sev-
eral diﬀerent ways to represent trees, not only because alternate representa-
tions often arise in applications, but also because an analytic argument often
may be more easily understood when based upon an alternate representation.
We consider binary trees both from a purely combinatorial point of view
(where we enumerate and examine properties of all possible diﬀerent struc-
tures) and from an algorithmic point of view (where the structures are built
and used by algorithms, with the probability of occurrence of each structure
induced by the input). Ļe former is important in the analysis of recursive
structures and algorithms, and the most important instance of the latter is a
fundamental algorithm called binary tree search. Binary trees and binary tree
search are so important in practice that we study their properties in consider-
able detail, expanding upon the approach begun in Chapter 3.
As usual, after considering enumeration problems, we move on to anal-
ysis of parameters. We focus on the analysis of path length, a basic parameter
of trees that is natural to study and useful to know about, and we consider
height and some other parameters as well. Knowledge of basic facts about
șȜȞ

șȜȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ș
path length and height for various kinds of trees is crucial for us to be able
to understand a variety of fundamental computer algorithms. Our analysis of
these problems is prototypical of the relationship between classical combina-
toric and modern algorithmic studies of fundamental structures, a recurring
theme throughout this book.
As we will see, the analysis of path length in trees łows naturally from
the basic tools that we have developed, and generalizes to provide a way to
study a broad variety of tree parameters. We will also see that, by contrast, the
analysis of tree height presents signiŀcant technical challenges. Ļough there
is not necessarily any relationship between the ease of describing a problem
and the ease of solving it, this disparity in ease of analysis is somewhat sur-
prising at ŀrst look, because path length and height are both parameters that
have simple recursive descriptions that are quite similar.
As discussed in Chapter 5, analytic combinatorics can unify the study
of tree enumeration problems and analysis of tree parameters, leading to very
straightforward solutions for many otherwise unapproachable problems. For
best eﬀect, this requires a certain investment in some basic combinatorial ma-
chinery (see [15] for full details). We provide direct analytic derivations for
many of the important problems considered in this chapter, alongside sym-
bolic arguments or informal descriptions of how they might apply. Detailed
study of this chapter might be characterized as an exercise in appreciating the
value of the symbolic method.
We consider a number of diﬀerent types of trees, and some classical
combinatorial results about properties of trees, moving from the speciŀcs of
the binary tree to the general notion of a tree as an acyclic connected graph.
Our goal is to provide access to results from an extensive literature on the
combinatorial analysis of trees, while at the same time providing the ground-
work for a host of algorithmic applications.
6.1 Binary Trees. In Chapter 3, we encountered binary trees, perhaps the
simplest type of tree. Binary trees are recursive structures that are made up of
two diﬀerent types of nodes that are attached together according to a simple
recursive deŀnition:
Deŀnition A binary tree is either an external node or an internal node at-
tached to an ordered pair of binary trees called the left subtree and the right
subtree of that node.

§ȝ.Ș
T Ş ő ő ş
șȜȠ
We refer to empty subtrees in a binary tree as external nodes. As such,
they serve as placeholders. Unless in a context where both types are being
considered, we refer to the internal nodes of a tree simply as the “nodes” of
the tree. We normally consider the subtrees of a node to be connected to the
node with two links, the left link and the right link.
For reference, Figure 6.1 shows three binary trees. By deŀnition, each
internal node has exactly two links; by convention, we draw the subtrees of
each node below the node on the page, and represent the links as lines con-
necting nodes. Each node has exactly one link “to” it, except the node at the
top, a distinguished node called the root. It is customary to borrow terminol-
ogy from family trees: the nodes directly below a node are called its children;
nodes farther down are called descendants; the node directly above each node
is called its parent; nodes farther up are called ancestors. Ļe root, drawn at
the top of the tree by convention, has no parent.
External nodes, represented as open boxes in Figure 6.1, are at the bot-
tom of the tree and have no children. To avoid clutter, we often refrain from
drawing the boxes representing external nodes in ŀgures with large trees or
large numbers of trees. A leaf in a binary tree is an internal node with no
children (both subtrees empty), which is not the same thing as an external
node (a placeholder for an empty subtree).
We have already considered the problem of enumerating binary trees.
Figures 3.1 and 5.2 show all the binary trees with 1, 2, 3, and 4 internal
nodes, and the following result is described in detail in Chapters 3 and 5.
root
internal node
leaf
external node
Figure 6.1 Ļree binary trees

șȝȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ș
Ļeorem 6.1 (Enumeration of binary trees).
Ļe number of binary trees
with N internal nodes is given by the Catalan numbers:
TN =
1
N + 1
(
2N
N
)
∼
4N
√
πN 3 .
Proof. See §3.8 and §5.2. Figure 6.2 summarizes the analytic combinatorics
of the proof.
By the following lemma (and as noted in the analytic proof in Chapter
5), the Catalan numbers also count the number of binary trees with N + 1
external nodes. In this chapter, we consider other basic parameters, in the
context of comparisons with other types of trees.
Lemma Ļe number of external nodes in any binary tree is exactly one greater
than the number of internal nodes.
Proof. Let e be the number of external nodes and i the number of internal
nodes. We count the links in the tree in two diﬀerent ways. Each internal
node has exactly two links “from” it, so the number of links is 2i. But the
number of links is also i + e −1, since each node but the root has exactly one
link “to” it. Equating these two gives 2i = i + e −1, or i = e −1.
Exercise 6.1 Develop an alternative proof of this result using induction.
Exercise 6.2 What proportion of the binary trees with N internal nodes have both
subtrees of the root nonempty? For N = 1, 2, 3, and 4, the answers are 0, 0, 1/5, and
4/14, respectively (see Figure 5.2).
Exercise 6.3 What proportion of the binary trees with 2N + 1 internal nodes have
N internal nodes in each of the subtrees of the root?
symbolic
transfer
Teorem 5.2
combinatorial
construction
GF equation
coeﬃcient
asymptotics
T = E + Z × T × T
T(z) = 1 + zT(z)2
= 1 −√1 −4z
2
∼
4N
√
πN 3
analytic
transfer
Teorem 5.5
corollary
Figure 6.2 Enumerating binary trees via analytic combinatorics

§ȝ.ș
T Ş ő ő ş
șȝȘ
6.2 Forests and Trees. In a binary tree, no node has more than two chil-
dren. Ļis characteristic makes it obvious how to represent and manipulate
such trees in computer implementations, and it relates naturally to “divide-
and-conquer” algorithms that divide a problem into two subproblems. How-
ever, in many applications (and in more traditional mathematical usage), we
need to consider a more general kind of tree:
Deŀnition A forest is a sequence of disjoint trees. A tree is a node (called the
root ) connected to the roots of trees in a forest.
Ļis deŀnition is recursive, as are tree structures. When called for in
context, we sometimes use the term general tree to refer to trees. As for binary
trees, the order of the trees in a forest is signiŀcant. When applicable, we
use the same nomenclature as for binary trees: the subtrees of a node are its
children, a root node has no parents, and so forth. Trees are more appropriate
models than binary trees for certain computations.
For reference, Figure 6.3 shows a three-tree forest. Again, roots of trees
have no parents and are drawn at the top by convention. Ļere are no ex-
ternal nodes; instead, a node at the bottom with no children is called a leaf.
Figure 6.4 depicts all the forests of 1 through 4 nodes and all the trees of 1
through 5 nodes. Ļe number of forests with N nodes is the same as the
number of trees with N + 1 nodes—just add a root and make its children
the roots of the trees in the forest. Moreover, the Catalan numbers are im-
mediately apparent in Figure 6.4. A well-known 1:1 correspondence with
binary trees is one way to enumerate binary trees. Before considering that
correspondence, we consider an analytic proof using the symbolic method.
root
node
leaf
Figure 6.3 A three-tree forest

șȝș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ș
G3 = 2
G2 = 1
G1 = 1
G4 = 5
G5 = 14
F2 = 2
F1 = 1
F3 = 5
F4 = 14
General trees (with N nodes,  1 ≤ N  ≤ 5 )
Forests (with N nodes,  1 ≤ N  ≤ 4 )
Figure 6.4 Forests and trees

§ȝ.ș
T Ş ő ő ş
șȝȚ
Ļeorem 6.2 (Enumeration of forests and trees).
Let FN be the number
of forests with N nodes and GN be the number of trees with N nodes. Ļen
GN is exactly equal to FN−1 and FN is exactly equal to the number of binary
trees with N internal nodes and is given by the Catalan numbers:
FN = TN =
1
N + 1
(
2N
N
)
∼
4N
√
πN 3 .
Proof. Immediate via the symbolic method. A forest is a sequence of trees and
a tree is a node connected to a forest
F = SEQ (G)
and
G = Z × F,
which translates directly (see Ļeorem 5.1) into
F(z) =
1
1 −G(z)
and
G(z) = zF(z),
which implies that GN = FN−1 and
F(z) −zF(z)2 = 1.
so FN = TN because their GFs satisfy the same functional equation (see
Section 5.2).
Exercise 6.4 For what proportion of the trees with N internal nodes does the root
have a single child? For N = 1, 2, 3, 4, and 5, the answers are 0, 1, 1/2, 2/5, and
5/14, respectively (see Figure 6.4).
Exercise 6.5 Answer the previous questions for the root having t children, for t =
2, 3, and 4.
Exercise 6.6 What proportion of the forests with N nodes have no trees consisting
of a single node? For N = 1, 2, 3, and 4, the answers are 0, 1/2, 2/5, and 3/7,
respectively (see Figure 6.4).

șȝț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ț
6.3 Combinatorial Equivalences to Trees and Binary Trees. In this
section, we address the broad reach of trees and binary trees as combinatorial
models. We begin by showing that trees and binary trees may be viewed as
two speciŀc ways to represent the same combinatorial objects (which are enu-
merated by the Catalan numbers). Ļen we summarize other combinatorial
objects that arise in numerous applications and for which similar correspon-
dences have been developed (see, for example, [31]).
Rotation correspondence. A fundamental one-to-one correspondence be-
tween forests and binary trees provides a direct proof that FN = TN. Ļis
correspondence, called the rotation correspondence, is illustrated in Figure 6.5.
Given a forest, we construct the corresponding binary tree as follows: the root
of the binary tree is the root of the ŀrst tree in the forest; its right link points
to the representation of the remainder of the forest (not including the ŀrst
tree); its left link points to the representation for the forest comprising the
subtrees of the root of the ŀrst tree. In other words, each node has a left link
to its ŀrst child and a right link to its next sibling in the forest (the correspon-
dence is also often called the “ŀrst child, next sibling correspondence”). As
illustrated in the ŀgure, the nodes in the binary tree appear to be placed by
rotating the general tree 45 degrees clockwise.
Conversely, given a binary tree with root x, construct the corresponding
forest as follows: the root of the ŀrst tree in the forest is x; the children of that
node are the trees in the forest constructed from the left subtree of x; and the
rest of the forest comprises the trees in the forest constructed from the right
subtree of x.
Ļis correspondence is of interest in computing applications because it
provides an eﬃcient way to represent forests (with binary trees). Next, we see
Figure 6.5 Rotation correspondence between trees and binary trees

§ȝ.Ț
T Ş ő ő ş
șȝȜ
that many other types of combinatorial objects not only have this property,
but also can provide alternative representations of trees and binary trees.
Parenthesis systems. Each forest with N nodes corresponds to a set of N
pairs of parentheses: from the deŀnition, there is a sequence of trees, each
consisting of a root and a sequence of subtrees with the same structure. If
we consider that each tree should be enclosed in parentheses, then we are led
immediately to a representation that uses only parentheses: for each tree in
the forest, write a left parenthesis, followed by the parenthesis system for
the forest comprising the subtrees (determined recursively), followed by a
right parenthesis. For example, this system represents the forest at the left
in Figure 6.5:
( ( ( ) ) ( ) ( ) ) ( ) ( ( ) ( ) ) ( ( ) ).
You can see the relationship between this and the tree structure by writing
parentheses at the levels corresponding to the root nodes of the tree they en-
close, as follows:
(
) ( ) (
) (
)
(
) ( ) ( )
( ) ( )
( )
( )
Collapsing this structure gives the parenthesis system representation.
Cast in terms of tree traversal methods, we can ŀnd the parenthesis sys-
tem corresponding to a tree by recursively traversing the tree, writing “(” when
going “down” an edge and “)” when going “up” an edge. Equivalently, we may
regard “(” as corresponding to “start a recursive call” and “)” as corresponding
to “ŀnish a recursive call.”
In this representation, we are describing only the shape of the tree, not
any information that might be contained in the nodes. Next we consider
representations that are appropriate if nodes may have an associated key or
other additional information.
Exercise 6.7 Give parenthesis systems that correspond to the forests in Figure 6.2.
Space-eﬃcient representations of tree shapes.
Ļe parenthesis system en-
codes a tree of size N with a sequence of 2N + O(1) bits. Ļis is apprecia-
bly smaller than standard representations of trees with pointers. Actually, it

șȝȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ț
comes close to the information-theoretic optimal encoding length, the loga-
rithm of the Catalan numbers:
lgTN = 2N −O(logN).
Such representations of tree shapes are useful in applications where very large
trees must be stored (e. g., index structures in databases) or transmitted (e. g.,
representations of Huﬀman trees; see [33]).
Preorder and postorder representations of trees.
In §6.5, we discuss basic
tree traversal algorithms that lead immediately to various tree representations.
Speciŀcally, we extend the parenthesis system to include information at the
nodes. As with tree traversal, the root can be listed either before the subtrees
(preorder) or after the subtrees (postorder). Ļus the preorder representation
of the forest on the left in Figure 6.5 is
(• (• (•) ) (•) (•) ) (•) (• (•) (•) ) (• (•) )
and the postorder representation is
( ( (•) •) (•) (•) •) (•) ( (•) (•) •) ( (•) •).
When we discuss reŀnements below, we do so in terms of preorder, though
of course the representations are essentially equivalent, and the reŀnements
apply to postorder as well.
Preorder degree representation. Another way to represent the shape of the
forest in Figure 6.5 is the string of integers
3 1 0 0 0 0 2 0 0 1 0.
Ļis is simply a listing of the numbers of children of the nodes, in preorder.
To see why it is a unique representation, it is simpler to consider the same
sequence, but subtracting 1 from each term:
2 0 -1 -1 -1 -1 1 -1 -1 0 -1.
Moving from left to right, this can be divided into subsequences that have
the property that (i) the sum of the numbers in the subsequence is −1, and
(ii) the sum of any preŀx of the numbers in the subsequence is greater than
or equal to −1. Delimiting the subsequences by parentheses, we have

§ȝ.Ț
T Ş ő ő ş
șȝȞ
( 2 0 -1 -1 -1 ) ( -1 ) ( 1 -1 -1 ) ( 0 -1 ).
Ļis gives a correspondence to parenthesis systems: each of these subse-
quences corresponds to a tree in the forest. Deleting the ŀrst number in each
sequence and recursively decomposing gives the parenthesis system. Now,
each parenthesized sequence of numbers not only sums to −1 but also has
the property that the sum of any preŀx is nonnegative. It is straightforward
to prove by induction that conditions (i) and (ii) are necessary and suﬃcient
to establish a direct correspondence between sequences of integers and trees.
Binary tree traversal representations. Binary tree representations are sim-
pler because of the marked distinction between external (degree 0) and inter-
nal (degree 2) nodes. For example. in §6.5 we consider binary tree representa-
tions of arithmetic expressions (see Figure 6.11). Ļe familiar representation
corresponds to to list the subtrees, parenthesized, with the character asso-
ciated with the root in between:
((x + y) * z) - (w + ((a - (v + y)) / ((z + y) * x))).
Ļis is called inorder or inŀx when referring speciŀcally to arithmetic expres-
sions. It corresponds to an inorder tree traversal where we write “(,” then
traverse the left subtree, then write the character at the root, then traverse the
right subtree, then write “).”
Representations corresponding to preorder and postorder traversals can
be deŀned in an analogous manner. But for preorder and postorder, paren-
theses are not needed: external (operands) and internal (operators) nodes are
identiŀed; thus the preorder node degree sequence is implicit in the represen-
tation, which determines the tree structure, in the same manner as discussed
ealier. In turn, the preorder, or preŀx, listing of the above sequence is
- * + x y z + w / - a + v y * + z y x,
and the postorder, or postŀx, listing is
x y + z * w a v y + - z y + x * / + -.
Exercise 6.8 Given an (ordered) tree, consider its representation as a binary tree
using the rotation correspondence. Discuss the relationship between the preorder and
postorder representations of the ordered tree and the preorder, inorder, and postorder
representations of the corresponding binary tree.

șȝȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ț
Gambler's ruin and lattice paths. For binary trees, nodes have either two or
zero children. If we list, for each node, one less than the number of children,
in preorder, then we get either +1 or −1, which we abbreviate simply as +
or −. Ļus a binary tree can be uniquely represented as a string of + and −
symbols. Ļe tree structure in Figure 6.11 is
+ + + - - - + - + + - + - - + + - - -.
Ļis encoding is a special case of the preorder degree representation. Which
strings of + and −symbols correspond to binary trees?
Gambler’s ruin sequences. Ļese strings correspond exactly to the following
situation. Suppose that a gambler starts with $0 and makes a $1 bet. If he
loses, he has $−1 and is ruined, but if he wins, he has $1 and bets again.
Ļe plot of his holdings is simply the plot of the partial sums of the plus-
minus sequence (number of pluses minus number of minuses). Any path
that does not cross the $0 point (except at the last step) represents a possible
path to ruin for the gambler, and such paths are also in direct correspondence
with binary trees. Given a binary tree, we produce the corresponding path as
just described: it is a gambler’s ruin path by the same inductive reasoning as
we used earlier to prove the validity of the preorder degree representation of
ordered trees. Given a gambler’s ruin path, it can be divided in precisely one
way into two subpaths with the same characteristics by deleting the ŀrst step
and splitting the path at the ŀrst place that it hits the $0 axis. Ļis division
(inductively) leads to the corresponding binary tree.
Ballot problems.
A second way of looking at this situation is to consider
an election where the winner has N + 1 votes and the loser has N votes. A
plus-minus sequence then corresponds to the set of ballots, and plus-minus
sequences corresponding to binary trees are those where the winner is never
behind as the ballots are counted.
Paths in a lattice. A third way of looking at the situation is to consider paths
in an N-by-N square lattice that proceed from the upper left corner down to
the lower right corner using “right” and “down” steps. Ļere are
(2N
N
) such
paths, but only one out of every N + 1 starts “right” and does not cross the
diagonal, because there is a direct correspondence between such paths and
binary trees, as shown in Figure 6.6. Ļis is obviously a graph of the gambler’s
holdings (or of the winner’s margin as the ballots are counted) rotated 45

§ȝ.Ț
T Ş ő ő ş
șȝȠ
Figure 6.6 Lattice-path representations of binary trees in Figure 6.1
degrees. It also is a graph of the stack size if the corresponding tree is traversed
in preorder.
We will study properties of these gambler's ruin sequences, or ballot se-
quences, in Chapters 7 and 8. Ļey turn out to be relevant in the analysis
of sorting and merging algorithms (see §7.6), and they can be studied using
general tools related to string enumeration (see §8.5).
Exercise 6.9 Find and prove the validity of a correspondence between N-step gam-
bler’s ruin paths and ordered forests of N −1 nodes.
Exercise 6.10 How many N-bit binary strings have the property that the number of
ones in the ŀrst k bits does not exceed the number of zeros, for all k?
Exercise 6.11 Compare the parenthesis representation of an ordered forest to the
plus-minus representation of its associated binary tree. Explain your observation.
Planar subdivision representations. We mention another classical corre-
spondence because it is so well known in combinatorics: the “triangulated
N-gon” representation, shown in Figure 6.7 for the binary tree at the left in
Figure 6.1. Given a convex N-gon, how many ways are there to divide it into
triangles with noncrossing “diagonal” lines connecting vertices? Ļe answer
is a Catalan number, because of the direct correspondence with binary trees.
Ļis application marked the ŀrst appearance of Catalan numbers, in the work
of Euler and Segner in 1753, about a century before Catalan himself. Ļe
correspondence is plain from Figure 6.7: given a triangulated N-gon, put an
internal node on each diagonal and one (the root) on one exterior edge and
an external node on each remaining exterior edge. Ļen connect the root to

șȞȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ț
Figure 6.7 Binary tree corresponding to a triangulated N-gon
the other two nodes in its triangle and continue connecting in the same way
down to the bottom of the tree.
Ļis particular correspondence is classical in combinatorics, and there
are other planar subdivisions that have been more recently developed and are
of importance in the design and analysis of some geometric algorithms. For
example, Bentley’s 2D-tree data structure [4] is based on dividing a rectan-
gular region in the plane with horizontal lines, then further dividing the re-
sulting regions with vertical lines, and so on, continuing to divisions as ŀne
as desired, alternating horizontal and vertical lines. Ļis recursive division
corresponds to a tree representation. Many planar subdivisions of this sort
have been devised to subdivide multidimensional spaces for point location
and other applications.
Figure 6.8 summarizes the most well-known tree representations that
we have discussed in this section, for ŀve-node trees. We dwell on these
representations to underscore the ubiquity of trees in combinatorics and—
since trees arise explicitly as data structures and implicitly as models of re-
cursive computation—the analysis of algorithms, as well. Familiarity with
various representations is useful because properties of a particular algorithm
can sometimes be more clearly seen in one of the equivalent representations
than in another.
Exercise6.12 Give a method for representing a tree with a subdivided rectangle when
the ratio of the height to the width of any rectangle is between α and 1/α for constant
α > 1. Find a solution for α as small as you can.
Exercise 6.13 Ļere is an obvious correspondence where left-right symmetry in tri-
angulations is rełected in left-right symmetry in trees. What about rotations? Is

§ȝ.Ț
T Ş ő ő ş
șȞȘ
  ( ( ( ) ) ( ) ( ) )  
  ( ( ) ( ( ) ( ) ) )  
  ( ( ( ) ) ( ( ) ) )  
  ( ( ( ( ( ) ) ) ) )  
  ( ( ( ) ( ) ( ) ) )  
  ( ( ( ) ( ( ) ) ) )  
  ( ( ( ( ) ) ( ) ) )  
  ( ( ( ( ) ( ) ) ) )  
  ( ( ( ) ( ) ) ( ) )  
  ( ( ( ( ) ) ) ( ) )  
  ( ( ) ( ( ( ) ) ) )  
  ( ( ) ( ( ) ) ( ) )  
  ( ( ) ( ) ( ( ) ) )  
  ( ( ) ( ) ( ) ( ) )  
  + + + + - - - - -  
  + + + - + - - - -  
  + + + - - + - - -  
  + + - + + - - - -  
  + + - + - + - - -  
  + + + - - - + - -  
  + + - + - - + - -  
  + + - - + + - - -  
  + + - - + - + - -  
  + - + + + - - - -  
  + - + + - + - - -  
  + - + + - - + - -  
  + - + - + + - - -  
  + - + - + - + - -  
Figure 6.8 Binary trees, trees, parentheses, triangulations, and ruin sequences

șȞș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ț
there any relationship among the N trees corresponding to the N rotations of an
asymmetric triangulation?
Exercise 6.14 Consider strings of N integers with two properties: ŀrst, if k > 1 is in
the string, then so is k −1, and second, some larger integer must appear somewhere
between any two occurrences of any integer. Show that the number of such strings
of length N is described by the Catalan numbers, and ŀnd a direct correspondence
with trees or binary trees.
6.4 Properties of Trees. Trees arise naturally in a variety of computer
applications. Our primary notion of size is generally taken to be the number
of nodes for general trees and, depending on context, the number of internal
nodes or the number of external nodes for binary trees. For the analysis of
algorithms, we are primarily interested in two basic properties of trees of a
given size: path length and height.
To deŀne these properties, we introduce the notion of the level of a node
in a tree: the root is at level 0, children of the root are at level 1, and in general,
children of a node at level k are at level k +1. Another way of thinking of the
level is as the distance (number of links) we have to traverse to get from the
root to the node. We are particularly interested in the sum of the distances
from each node to the root:
Deŀnition Given a tree or forest t, the path length is the sum of the levels
of each of the nodes in t and the height is the maximum level among all the
nodes in t.
We use the notation |t| to refer to the number of nodes in a tree t, the
notation pl(t) to refer to the path length, and the notation h(t) to refer to
the height. Ļese deŀnitions hold for forests as well. In addition, the path
length of a forest is the sum of the path lengths of the constituent trees and
the height of a forest is the maximum of the heights of the constituent trees.
Deŀnition Given a binary tree t, the internal path length is the sum of the
levels of each of the internal nodes in t, the external path length is the sum of
the levels of each of the external nodes in t, and the height is the maximum
level among all the external nodes in t.
We use the notation ipl(t) to refer to the internal path length of a binary
tree, xpl(t) to refer to the external path length, and h(t) to refer to the height.

§ȝ.ț
T Ş ő ő ş
șȞȚ
0
1
2
3
4
5
height :
path length :
2
0·4 + 1·6 + 2·1 = 8
height :
internal path length :
external path length :
5
0·1 + 1·2 + 2·3 + 3·3 + 4·2 = 25
2·1 + 3·3 + 4·4 + 5·4 = 47
level
Figure 6.9 Path length and height in a forest and in a binary tree
We use |t| to refer to the number of internal nodes in a binary tree unless
speciŀcally noted in contexts where it is more appropriate to count by external
nodes or all nodes.
Deŀnition In general trees, leaves are the nodes with no children. In binary
trees, leaves are the (internal) nodes with both children external.
Figure 6.9 gives examples for the purpose of reinforcing these deŀni-
tions. Ļe forest on the left has height 2 and path length 8, with 7 leaves; the
binary tree on the right has height 5, internal path length 25, and external
path length 47, with 4 leaves. To the left of each tree is a proŀle—a plot of
the number of nodes on each level (internal nodes for the binary tree), which
facilitates calculating path lengths.
Recursive deŀnitions and elementary bounds.
It is often convenient to
work with recursive deŀnitions for tree parameters. In a binary tree t, the
parameters we have deŀned are all 0 if t is an external node; otherwise, if the
root of t is an internal node and the left and right subtrees, respectively, are
denoted by tl and tr, we have following recursive formulae:
|t| = |tl| + |tr| + 1
ipl(t) = ipl(tl) + ipl(tr) + |t| −1
xpl(t) = xpl(tl) + xpl(tr) + |t| + 1
h(t) = 1 + max(h(tl), h(tr)).
Ļese are equivalent to the deŀnitions given earlier. First, the internal node
count of a binary tree is the sum of the node counts for its subtrees plus 1 (the

șȞț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ț
root). Second, the internal path length is the sum of the internal path lengths
of the subtrees plus |t| −1 because each of the |t| −1 nodes in the subtrees is
moved down exactly one level when the subtrees are attached to the tree. Ļe
same argument holds for external path length, noting that there are |t| + 1
external nodes in the two subtrees of a binary tree with |t| internal nodes. Ļe
result for height again follows from the fact that the levels of all the nodes in
the subtrees are increased by exactly 1.
Exercise 6.15 Give recursive formulations describing path length and height in gen-
eral trees.
Exercise 6.16 Give recursive formulations for the number of leaves in binary trees
and in general trees.
Ļese deŀnitions will serve as the basis for deriving functional equations
on associated generating functions when we analyze the parameters below.
Also, they can be used for inductive proofs about relationships among the
parameters.
Lemma Path lengths in any binary tree t satisfy xpl(t) = ipl(t) + 2|t|.
Proof. Subtracting the recursive formula for xpl(t) from the recursive formula
for ipl(t), we have
ipl(t) −xpl(t) = ipl(tl) −xpl(tl) + ipl(tr) −xpl(tr) + 2
and the lemma follows directly by induction.
Path length and height are not independent parameters: if the height is
very large, then so must be the path length, as shown by the following bounds,
which are relatively crude, but useful.
Lemma Ļe height and internal path length of any nonempty binary tree t
satisfy the inequalities
ipl(t) ≤|t|h(t)
and
h(t) ≤
√
2 ipl(t) + 1.
Proof. If h(t) = 0 then ipl(t) = 0 and the stated inequalities hold. Otherwise,
we must have ipl(t) < |t|h(t), since the level of each internal node must be
strictly smaller than the tree height. Furthermore, there is at least one internal
node at each level less than the height, so we must have 0 + 1 + 2 + . . . +
h(t) −1 ≤ipl(t). Hence 2ipl(t) ≥h(t)2 −h(t) ≥(h(t) −1)2 (subtract the
quantity h(t) −1, which is nonnegative, from the right-hand side), and thus
h(t) ≤
√
2ipl(t) + 1.

§ȝ.ț
T Ş ő ő ş
șȞȜ
Exercise 6.17 Prove that the height of a binary tree with N external nodes has to be
at least lgN.
Exercise 6.18 [Kraft equality] Let kj be the number of external nodes at level j
in a binary tree. Ļe sequence {k0, k1, . . . , kh} (where h is the height of the tree)
describes the proŀle of the tree. Show that a vector of integers describes the proŀle
of a binary tree if and only if ∑
j 2−kj = 1.
Exercise 6.19 Give tight upper and lower bounds on the path length of a general tree
with N nodes.
Exercise 6.20 Give tight upper and lower bounds on the internal and external path
lengths of a binary tree with N internal nodes.
Exercise 6.21 Give tight upper and lower bounds on the number of leaves in a binary
tree with N nodes.
IŚ ŠŔő ōŚōŘťşŕş śŒ algorithms, we are particularly interested in knowing the
average values of these parameters, for various types of “random” trees. One
of our primary topics of discussion for this chapter is how these quantities
relate to fundamental algorithms and how we can determine their expected
values. Figure 6.10 gives some indication of how these diﬀer for diﬀerent
types of trees. At the top is a random forest, drawn from the distribution
where each forest with the same number of nodes is equally likely to occur.
At the bottom is a random binary tree, drawn from the distribution where
each binary tree with the same number of nodes is considered to be equally
likely to occur. Ļe ŀgure also depicts a proŀle for each tree (the number of
nodes at each level), which makes it easier to calculate the height (number of
levels) and path length (sum over i of i times the number of nodes at level i).
Ļe random binary tree clearly has larger values for both path length and
height than the random forest. One of the prime objectives of this chapter is
to quantify these and similar observations, precisely.

șȞȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ț
Random binary tree (with 237 internal nodes)
Random forest (with 237 nodes)
height :
path length :
29
3026
height :
internal path length :
external path length :
44
4614
4851
Figure 6.10 A random forest and a random binary tree

§ȝ.Ȝ
T Ş ő ő ş
șȞȞ
6.5 Examples of Tree Algorithms. Trees are relevant to the study of anal-
ysis of algorithms not only because they implicitly model the behavior of re-
cursive programs but also because they are involved explicitly in many basic
algorithms that are widely used. We will brieły describe a few of the most
fundamental such algorithms here. Ļis brief description certainly cannot do
justice to the general topic of the utility of tree structures in algorithm design,
but we can indicate that the study of tree parameters such as path length and
height provides the basic information needed to analyze a host of important
algorithms.
Traversal.
In a computer representation, one of the fundamental opera-
tions on trees is traversal: systematically processing each of the nodes of the
tree. Ļis operation also is of interest combinatorially, as it represents a way
to establish a correspondence between (two-dimensional) tree structures and
various (one-dimensional) linear representations.
Ļe recursive nature of trees gives rise to a simple recursive procedure for
traversal. We “visit” the root of the tree and recursively “visit” the subtrees.
Depending on whether we visit the root before, after, or (for binary trees) in
between the subtrees, we get one of three diﬀerent traversal methods:
To visit all the nodes of a tree in preorder:
• Visit the root
• Visit the subtrees (in preorder)
To visit all the nodes of a tree in postorder:
• Visit the subtrees (in postorder)
• Visit the root
To visit all the nodes of a binary tree in inorder:
• Visit the left subtree (in inorder)
• Visit the root
• Visit the right subtree (in inorder)
In these methods, “visiting” the root might imply any procedure at all that
should be systematically applied to nodes in the tree. Program 6.1 is an im-
plementation of preorder traversal of binary trees.
To implement a recursive call, the system uses a pushdown stack to save
the current “environment,” to be restored upon return from the procedure.
Ļe maximum amount of memory used by the pushdown stack when travers-
ing a tree is directly proportional to tree height. Ļough this memory usage

șȞȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ȝ
may be hidden from the programmer in this case, it certainly is an important
performance parameter, so we are interested in analyzing tree height.
Another way to traverse a tree is called level order: ŀrst list all the nodes
on level 0 (the root); then list all the nodes on level 1, left to right; then list all
the nodes on level 2 (left to right); and so on. Ļis method is not suitable for
a recursive implementation, but it is easily implemented just as shown earlier
using a queue (ŀrst-in-ŀrst-out data structure) instead of a stack.
Tree traversal algorithms are fundamental and widely applicable. For
many more details about them and relationships among recursive and nonre-
cursive implementations, see Knuth [24] or Sedgewick [32].
Expression evaluation. Consider arithmetic expressions consisting of op-
erators, such as +, −, ∗, and /, and operands, denoted by numbers or letters.
Such expressions are typically parenthesized to indicate precedence between
operations. Expressions can be represented as binary trees called parse trees.
For example, consider the case where an expression uses only binary opera-
tors. Such an expression corresponds to a binary tree, with operators in the
internal nodes and operands in the external nodes, as shown in Figure 6.11.
Ļe operands corresponding to each operator are the expressions represented
by the left and right subtrees of its corresponding internal node.
Given a parse tree, we can develop a simple recursive program to com-
pute the value of the corresponding expression: (recursively) evaluate the two
subtrees, then apply the operator to the computed values. Evaluation of an
external node gives the current value of the associated variable. Ļis program
private void preorder(Node x)
{
if (x == null) return;
process(x.key);
preorder(x.left);
preorder(x.right);
}
Program 6.1 Preorder traversal of a binary tree

§ȝ.Ȝ
T Ş ő ő ş
șȞȠ
-
*
+
+
/
-
+
*
+
x
y
z
w
a
v
y
z
y
x
 ((x + y) * z) - (w + ((a - (v + y)) / ((z + y) * x)))
arithmetic
expression
corresponding
binary tree
Figure 6.11 Binary tree representation of an arithmetic expression
is equivalent to a tree traversal such as Program 6.1 (but in postorder). As with
tree traversal, the space consumed by this program will be proportional to the
tree height. Ļis approach is often used to evaluate arithmetic expressions in
computer applications systems.
From the advent of computing, one of the main goals was to evalu-
ate arithmetic expressions by translating them into machine code that can
eﬃciently do the job. To this end, one approach that is typically used by
programming-language compilers is to ŀrst build an expression tree associated
with part of a program, then convert the expression tree into a list of instruc-
tions for evaluating an expression at execution time, such as the following:
r1 ←x+y
r2 ←r1*z
r3 ←v+y
r4 ←a-r3
r5 ←z+y
r6 ←r5*x
r7 ←r4/r6
r8 ←w+r7
r9 ←r2-r8
Ļese instructions are close to machine instructions, for example, involving
binary arithmetic operations using machine registers (indicated by the tem-
porary variables r1 through r9), a (limited) machine resource for holding
results of arithmetic operations. Generally, a reasonable goal is to use as few

șȟȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ȝ
registers as possible. For example, we could replace the last two instructions
by the instructions
r7 ←w+r7
r7 ←r2-r7
and use two fewer registers. Similar savings are available at other parts of the
expression. Ļe minimum number of registers needed to evaluate an expres-
sion is a tree parameter of direct practical interest. Ļis quantity is bounded
from above by tree height, but it is quite diﬀerent. For example, the de-
generate binary tree where all nodes but one have exactly one null link has
height N but the corresponding expression can be evaluated with one regis-
ter. Determining the minimum number of registers needed (and how to use
them) is known as the register allocation problem. Expression evaluation is of
interest in its own right, and it is also indicative of the importance of trees
in the process of translating computer programs from higher-level languages
to machine languages. Compilers generally ŀrst “parse” programs into tree
representations, then process the tree representation.
Exercise 6.22 What is the minimum number of registers needed to evaluate the
expression in Figure 6.11?
Exercise 6.23 Give the binary tree corresponding to the expressions (a + b) ∗d and
((a + b) ∗(d −e) ∗(f + g)) −h ∗i. Also give the preorder, inorder, and postorder
traversals of those trees.
Exercise 6.24 An expression where operators have varying numbers of operands
corresponds to a tree, with operands in leaves and operators in nonleaves. Give
the preorder and postorder traversals of the tree corresponding to the expression
((a2 +b+c)∗(d4 −e2)∗(f +g +h))−i∗j, then give the binary tree representation
of that tree and the preorder, inorder, and postorder traversals of the binary tree.
TŞőő ŠŞōŢőŞşōŘ ōŚŐ őŤŜŞőşşŕśŚ manipulation are representative of many
applications where the study of parameters such as the path length and height
of trees is of interest. To consider the average value of such parameters, of
course, we need to specify a model deŀning what is meant by a “random” tree.
As a starting point, we study so-called Catalan models, where each of the TN
binary trees of size N or general trees of size N + 1 are taken with equal
probability. Ļis is not the only possibility—in many situations, the trees are
induced by external data, and other models of randomness are appropriate.
Next, we consider a particularly important example of this situation.

§ȝ.ȝ
T Ş ő ő ş
șȟȘ
6.6 BinarySearchTrees. One of the most important applications of binary
trees is the binary tree search algorithm, a method based on explicitly con-
structing binary trees to provide an eﬃcient solution to a fundamental prob-
lem that arises in numerous applications. Ļe analysis of binary tree search
illustrates the distinction between models where all trees are equally likely to
occur and models where the underlying distribution is determined by other
factors. Ļis juxtaposition of models is an essential concept in this chapter.
Ļe dictionary, symbol table, or simply search problem is a fundamen-
tal one in computer science: a set of distinct keys is to be organized so that
client queries whether or not a given key is in the set can be eﬃciently an-
swered. More generally, with distinct keys, we can use binary search trees to
implement an associative array, where we associate information with each key
and can use the key to store or retrieve such information. Ļe binary search
method discussed in §2.6 is one basic method for solving this problem, but
that method is of limited use because it requires a preprocessing step where
all the keys are ŀrst put into sorted order, while typical applications intermix
the operations of searching for keys and inserting them.
A binary tree structure can be used to provide a more łexible solution to
the dictionary problem, by assigning a key to each node and keeping things
arranged so that the key in every node is larger than any key in the left subtree
and smaller than any key in the right subtree.
AA
AA
AL
EF
MC
JB
MS
MS
MS
PD
PD
PD
MC
MC
JB
JB
EF
EF
CN
CN
CN
AL
AL
AA
AB
AB
AB
Figure 6.12 Ļree binary search trees

șȟș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȝ
Deŀnition A binary search tree is a binary tree with keys associated with the
internal nodes, satisfying the constraint that the key in every node is greater
than all the keys in its left subtree and smaller than all the keys in its right
subtree.
Binary search trees can be built from any type of data for which a total
order is deŀned. Typically, keys are numbers in numerical order or strings
in alphabetical order. Many diﬀerent binary search trees may correspond to
a given set of keys. For reference, consider Figure 6.12, which shows three
diﬀerent binary search trees containing the same set of two-character keys
AA AB AL CN EF JB MC MS PD.
Program 6.2 demonstrates the utility of binary search trees in solving
the dictionary problem. It assumes that the set of keys is stored in a binary
search tree and uses a recursive implementation of a “search” procedure that
determines whether of not a given key is somewhere in the binary search tree.
To search for a node with a key v, terminate the search (unsuccessfully) if
the tree is empty and terminate the search (successfully) if the key in the root
node is v. Otherwise, look in the left subtree if v is less than the key in the
root node and look in the right subtree if v is greater than the key in the root
node. It is a simple matter to verify that, if started on the root of a valid binary
search tree with search key key, Program 6.2 returns true if and only if there
is a node containing key in the tree; otherwise, it returns false.
Indeed, any given set of N ordered keys can be associated with any of
the TN binary tree shapes—to make a binary search tree, visit the nodes of
private boolean search(Node x, Key key)
{
if (x == null) return false;
if (key < x.key) return search(x.left, key);
if (key > x.key) return search(x.right, key);
return true;
}
Program 6.2 Binary tree search

§ȝ.ȝ
T Ş ő ő ş
șȟȚ
the tree in postorder, assigning the next key in the order when visiting each
node. Ļe search algorithm works properly for any such binary search tree.
How do we construct a binary search tree containing a given set of keys?
One practical approach is to add keys one by one to an initially empty tree,
using a recursive strategy similar to search. We assume that we have ŀrst
done a search to determine that the new key is not in the tree, to maintain
the property that the keys in the tree are all diﬀerent. To insert a new key
into an empty tree, create a node containing the key and make its left and
right pointers null, and return a reference to the node. (We use the value
null to represent all external nodes.) If the tree is nonempty, insert the key
into the left subtree if it is less than the key at the root, and into the right
subtree if it is greater than the key at the root, resetting the link followed to
the reference returned. Ļis is equivalent to doing an unsuccessful search for
the key, then inserting a new node containing the key in place of the external
node where the search ends. Ļe shape of the tree and the cost of building it
and searching are dependent on the order in which the keys are inserted.
Program 6.3 is an implementation of this method. For example, if the
key DD were to be inserted into any of the trees in Figure 6.9, Program 6.3
would create a new node with the key DD as the left child of EF.
In the present context, our interest is that this insertion algorithm de-
ŀnes a mapping from permutations to binary trees: Given a permutation,
private Node insert(Key key)
{
if (x == null)
{ x = new Node(); x.key = key; }
if (key < x.key)
x.left = insert(x.left, key);
else if (key > x.key)
x.right = insert(x.right, key);
return x;
}
Program 6.3 Binary search tree insertion

șȟț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȝ
build a binary tree by inserting the elements in the permutation into an ini-
tially empty tree, proceeding from left to right. Figure 6.13 illustrates this
correspondence for the three examples we started with. Ļis correspondence
is important in the study of search algorithms. Whatever the data type, we
can consider the permutation deŀned by the relative order of the keys when
they are inserted into the data structure, which tells us which binary tree is
built by successive insertions.
In general, many diﬀerent permutations may map to the same tree.
Figure 6.14 shows the mapping between the permutations of four elements
and the trees of four nodes. Ļus, for example, it is not true that each tree
is equally likely to occur if keys are inserted in random order into an initially
empty tree. Indeed, it is fortunately the case that the more “balanced” tree
structures, for which search and construction costs are low, are more likely to
occur than tree structures for which the costs are high. In the analysis, we will
quantify this observation.
Some trees are much more expensive to construct than others. In the
worst case, a degenerate tree where each node has at least one external child,
i −1 internal nodes are examined to insert the ith node for each i between 1
and N, so a total of N(N −1)/2 nodes are examined in order to construct the
3 2 4 8 6 5 7 1 9
1 9 2 8 3 7 4 6 5
6 8 7 4 2 9 1 3 5
3
2
1
4
8
6
5
9
3
2
1
4
8
6
5
7
9
3
2
1
4
8
6
5
9
permutations
corresponding
BSTs
7
7
Figure 6.13 Permutations and binary search trees

§ȝ.ȝ
T Ş ő ő ş
șȟȜ
tree. In the best case, the middle node will be at the root for every subtree,
with about N/2 nodes in each subtree, so the standard divide-and-conquer
recurrence TN = 2TN/2+N holds, which implies that a total of about NlgN
steps are required to construct the tree (see §2.6).
Ļe cost of constructing a particular tree is directly proportional to its
internal path length, since nodes are not moved once inserted, and the level
of a node is exactly the number of compares required to insert it. Ļus, the
cost of constructing a tree is the same for each of the insertion sequences that
could lead to its construction.
We could obtain the average construction cost by computing the sum
of the internal path lengths of the trees resulting from all N! permutations
(the cumulated cost) and then dividing by N!. Or, we could compute the
cumulated cost by adding, for all trees, the product of the internal path length
1
2 1
1 2
3 2 1
3 1 2
2 1 3
2 3 1
1 3 2
1 2 3
4 3 2 1
4 3 1 2
4 1 3 2
4 1 2 3
4 2 1 3
4 2 3 1
3 2 1 4
3 2 4 1
3 4 2 1
3 1 2 4
3 1 4 2
3 4 1 2
2 1 4 3
2 4 1 3
2 4 3 1
2 1 3 4
2 3 1 4
2 3 4 1
1 2 3 4
1 2 4 3
1 4 2 3
1 4 3 2
1 3 2 4
1 3 4 2
Figure 6.14 Permutations associated with N-node BSTs, 1 ≤N ≤4

șȟȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȝ
and the number of permutations that lead to the tree being constructed. Ļe
result of this computation is the average internal path length that we expect
after N random insertions into an initially empty binary search tree, but it
is not the same as the average internal path length of a random binary tree,
under the model where all trees are equally likely. Instead, it assumes that all
permutations are equally likely.
Ļe diﬀerences in the models are evident even for the 3-node and 4-
node trees shown in Figure 6.14. Ļere are ŀve diﬀerent 3-node trees, four
with internal path length 3 and one with internal path length 2. Of the six
permutations of size 3, four correspond to the trees with larger path length and
two correspond to the balanced tree. Ļerefore, if QN is the average internal
path length of a binary tree and CN is the average internal path length of a
binary search tree built from a random permutation, then
Q3 = (3+3+2+3+3)/5 = 2.8
and
C3 = (3+3+2·2+3+3)/6 .= 2.667.
For 4-node trees, the corresponding calculations are
Q4 = (6 + 6 + 5 + 6 + 6 + 4 + 4 + 4 + 4 + 6 + 6 + 5 + 6 + 6)/14 .= 5.286
for random 4-node binary trees and
C4 = (6+6+5·2+6+6+4·3+4·3+4·3+4·3+6+6+5·2+6+6)/24 .= 4.833
for binary search trees built from random permutations of size 4. In both
cases, the average path length for the binary search trees is smaller because
more permutations map to the balanced trees. Ļis diﬀerence is fundamental.
We will consider a full analysis for the “random tree” case in the next section
and for the “binary search trees built from a random permutation” case in
§6.8.
Exercise 6.25 Compute Q5 and C5.
Exercise 6.26 Show that two diﬀerent permutations cannot give the same degenerate
tree structure. If all N! permutations are equally likely, what is the probability that a
degenerate tree structure will result?
Exercise 6.27 For N = 2n −1, what is the probability that a perfectly balanced
tree structure (all 2n external nodes on level n) will be built, if all N! key insertion
sequences are equally likely?
Exercise 6.28 Show that traversing a binary search tree in preorder and inserting
the keys into an initially empty tree results in the original tree. Is the same true for
postorder and/or level order? Prove your answer.

§ȝ.Ȟ
T Ş ő ő ş
șȟȞ
6.7 Average Path Length in Random Catalan Trees. To begin our
analysis of tree parameters, we consider the model where each tree is equally
likely to occur. To avoid confusion with other models, we add the modiŀer
Catalan to refer to random trees under this assumption, since the probability
that a particular tree occurs is the inverse of a Catalan number. Ļis model is
a reasonable starting point for many applications, and the combinatorial tools
developed in Chapters 3 and 5 are directly applicable in the analysis.
Binary Catalan trees. What is the average (internal) path length of a binary
tree with N internal nodes, if each N-node tree is considered to be equally
likely? Our analysis of this important question is prototypical of the general
approach to analyzing parameters of combinatorial structures that we consid-
ered in Chapters 3 and 5:
• Deŀne a bivariate generating function (BGF), with one variable mark-
ing the size of the tree and the other marking the internal path length.
• Derive a functional equation satisŀed by the BGF, or its associated cu-
mulative generating function (CGF).
• Extract coeﬃcients to derive the result.
We will start with a recurrence-based argument for the second step, because
the underlying details are of interest and related to familiar problems. We
know from Chapter 5 that direct generating-function-based arguments are
available for such problems. We shall consider two such derivations in the
next subsection.
To begin, we observe that the probability that the left subtree has k
nodes (and the right subtree has N −k−1 nodes) in a random binary Catalan
tree with N nodes is TkTN−k−1/TN (where TN =
(2N
N
) / (N +1) is the Nth
Catalan number). Ļe denominator is the number of possible N-node trees
and the numerator counts the number of ways to make an N-node tree by
using any tree with k nodes on the left and any tree with N −k −1 nodes on
the right. We refer to this probability distribution as the Catalan distribution.
Figure 6.14 shows the Catalan distribution as N grows. One of the
striking facts about the distribution is that the probability that one of the
subtrees is empty tends to a constant as N grows: it is 2TN−1/TN ∼1/2.
Random binary trees are not particularly well balanced.
One approach to analyzing path length in a random binary tree is to
use the Catalan distribution to write down a recurrence very much like the
one that we have studied for quicksort: the average internal path length in a

șȟȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ȟ
0
.4
.357
.25
0
Figure 6.15
(N −1)/2
N −1
TkTN−k−1
TN
Catalan distribution (subtree sizes in random binary trees)
(k-axes scaled to N)
random binary Catalan tree is described by the recurrence
QN = N −1 +
∑
1≤k≤N
Tk−1TN−k
TN
(Qk−1 + QN−k)
for N > 0
with Q0 = 0. Ļe argument underlying this recurrence is general, and can
be used to analyze random binary tree structures under other models of ran-
domness, by substituting other distributions for the Catalan distribution. For
example, as discussed later, the analysis of binary search trees leads to the
uniform distribution (each subtree size occurs with probability 1/N) and the
recurrence becomes like the quicksort recurrence of Chapter 1.
Ļeorem6.3 (Pathlengthinbinarytrees).
Ļe average internal path length
in a random binary tree with N internal nodes is
(N + 1)4N
(2N
N
)
−3N −1 = N
√
πN −3N + O(
√
N ).

§ȝ.Ȟ
T Ş ő ő ş
șȟȠ
Proof. We develop a BGF as in §3.10. First, the probability generating func-
tion QN(u) = ∑
k≥0 qNkuk with qNk the probability that k is the total in-
ternal path length satisŀes the recurrence relation
QN(u) = uN−1
∑
1≤k≤N
Tk−1TN−k
TN
Qk−1(u)QN−k(u)
for N > 0
with Q0(u) = 1. To simplify this recurrence, we move to an enumerative
approach, where we work with pNk = TNqNk (the number of trees of size N
with internal path length k) instead of the probabilities. Ļese satisfy, from
the above recurrence,
∑
k≥0
pNkuk = uN−1
∑
1≤k≤N
∑
r≥0
p(k−1)rur ∑
s≥0
p(N−k)sus
for N > 0.
To express this in terms of the bivariate generating function
P(z, u) =
∑
N≥0
∑
k≥0
pNkzNuk,
we multiply the above by zN and sum on N to get
P(z, u) =
∑
N≥1
∑
1≤k≤N
∑
r≥0
p(k−1)rur ∑
s≥0
p(N−k)suszNuN−1 + 1
= z
∑
k≥0
∑
r≥0
pkr(zu)kur ∑
N≥k
∑
s≥0
p(N−k)s(zu)N−kus + 1
= z
∑
k≥0
∑
r≥0
pkr(zu)kur ∑
N≥0
∑
s≥0
pNs(zu)Nus + 1
= zP(zu, u)2 + 1.
Later, we will also see a simple direct argument for this equation. Now, we
can use Ļeorem 3.11 to get the desired result: setting u = 1 gives the familiar
functional equation for the generating function for the Catalan numbers, so
P(z, 1) = T(z) = (1 −√1 −4z )/(2z). Ļe partial derivative Pu(z, 1) is
the generating function for the cumulative total if we add the internal path
lengths of all binary trees. From Ļeorem 3.11, the average that we seek is
[zN]Pu(z, 1)/[zN]P(z, 1).

șȠȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ȟ
Diﬀerentiating both sides of the functional equation for the BGF with
respect to u (using the chain rule for partial derivatives) gives
Pu(z, u) = 2zP(zu, u)(Pu(zu, u) + zPz(zu, u)).
Evaluating this at u = 1 gives a functional equation for the CGF:
Pu(z, 1) = 2zT(z)(Pu(z, 1) + zT ′(z)),
which yields the solution
Pu(z, 1) = 2z2T(z)T ′(z)
1 −2zT(z) .
Now, T(z) = (1−√1 −4z )/(2z), so 1−2zT(z) = √1 −4z and zT ′(z) =
−T(z) + 1/√1 −4z. Substituting these gives the explicit expression
zPu(z, 1) =
z
1 −4z −
1 −z
√1 −4z + 1,
which expands to give the stated result.
Ļis result is illustrated by the large random binary tree in Figure 6.10:
asymptotically, a large tree roughly ŀts into a
√
N-by-
√
N square.
Direct combinatorial argument for the BGF. Ļe proof of Ļeorem 6.3 in-
volves the bivariate generating function
P(z, u) =
∑
N≥0
∑
k≥0
pNkukzN
where pNk is the number of trees with N nodes and internal path length k.
As we know from Chapter 5, this may be expressed equivalently as
P(z, u) =
∑
t∈T
z|t|uipl(t).
Now the recursive deŀnitions in §6.4 lead immediately to
P(z, u) =
∑
tl∈T
∑
tr∈T
z|tl|+|tr|+1uipl(tl)+ipl(tr)+|tl|+|tr| + 1.

§ȝ.Ȟ
T Ş ő ő ş
șȠȘ
Ļe number of nodes is 1 plus the number of nodes in the subtrees, and the
internal path length is the sum of the internal path lengths of the subtrees
plus 1 for each node in the subtrees. Now, it is easy to rearrange this double
sum to make two independent sums:
P(z, u) = z
∑
tl∈T
(zu)|tl|uipl(tl) ∑
tr∈T
(zu)|tr|uipl(tr) + 1
= zP(zu, u)2 + 1,
as before. Ļe reader may wish to study this example carefully, to appreciate
both its simplicity and its subtleties. It is also possible to directly derive equa-
tions of this form via the symbolic method (see [15]).
Cumulative generating function. An even simpler path to the same result is
to derive the functional equation for the CGF directly. We deŀne the CGF
CT (z) ≡Pu(z, 1) =
∑
t∈T
ipl(t)z|t|.
Ļe average path length is [zn]CT (z)/[zn]T(z). In precisely the same manner
as above, the recursive deŀnition of binary trees leads immediately to
CT (z) =
∑
tl∈T
∑
tr∈T
(ipl(tl) + ipl(tr) + |tl| + |tr|)z|tl|+|tr|+1
= 2zCT (z)T(z) + 2z2T(z)T ′(z),
which is the same as the functional equation derived for Ļeorem 6.3.
Exercise 6.29 Derive this equation from the recurrence for path length.
Ļe three derivations just considered are based on the same combina-
torial decomposition of binary trees, but the CGF suppresses the most detail
and is certainly the preferred method for ŀnding the average. Ļe contrast
between the complex recurrence given in the proof to Ļeorem 6.3 and this
“two-line” derivation of the same result given here is typical, and we will see
many other problems throughout this book where the amount of detail sup-
pressed using CGFs is considerable.

șȠș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ȟ
General Catalan trees.
We can proceed in the same manner to ŀnd the
expected path length in a random general tree via BGFs. Readers not yet
convinced of the utility of BGFs and CGFs are invited to go through the
exercise of deriving this result from a recurrence.
Ļeorem6.4 (Pathlengthingeneraltrees).
Ļe average internal path length
in a random general tree with N internal nodes is
N
2
( 4N−1
(2N−2
N−1
) −1
)
= N
2 (
√
πN −1) + O(
√
N ).
Proof. Proceed as described earlier:
Q(z, u) ≡
∑
t∈G
z|t|uipl(t)
=
∑
k≥0
∑
t1∈G
. . .
∑
tk∈G
uipl(t1)+···+ipl(tk)+|t1|+...+|tk|z|t1|+...+|tk|+1
= z
∑
k≥0
Q(zu, u)k
=
z
1 −Q(zu, u).
Setting u = 1, we see that Q(z, 1) = G(z) = zT(z) = (1 −√1 −4z )/2
is the Catalan generating function, which enumerates general trees, as we
found in §6.2. Diﬀerentiating the BGF derived above with respect to u and
evaluating at u = 1 gives the CGF
CG(z) ≡Qu(z, 1) = zCG(z) + z2G′(z)
(1 −G(z))2
.
Ļis simpliŀes to give
CG(z) = 1
2
z
1 −4z −1
2
z
√1 −4z .
Next, as before, we use Ļeorem 3.11 and compute [zN]CG(z)/[zN]G(z),
which immediately leads to the stated result.

§ȝ.ȟ
T Ş ő ő ş
șȠȚ
Exercise 6.30 Justify directly the equation given in the proof of Ļeorem 6.4 for the
CGF for path length in general trees (as we did for binary trees).
Exercise6.31 Use the rotation correspondence between general trees and binary trees
to derive the average path length in random general trees from the corresponding
result on random binary trees.
6.8 Path Length in Binary Search Trees. As we have noted, the analysis
of path length in binary search trees is actually the study of a property of per-
mutations, not trees, since we start with a random permutation. In Chapter 7,
we discuss properties of permutations as combinatorial objects in some detail.
We consider the analysis of path length in BSTs here not only because it is
interesting to compare it with the analysis just given for random trees, but
also because we have already done all the work, in Chapters 1 and 3.
Figure 6.14 indicates—and the analysis proves—that the binary search
tree insertion algorithm maps more permutations to the more balanced trees
with small internal path length than to the less balanced trees with large in-
ternal path length. Binary search trees are widely used because they accom-
modate intermixed searches, insertions, and other operations in a uniform
and łexible manner, and they are primarily useful because the search itself
is eﬃcient. In the analysis of the costs of any searching algorithm, there are
two quantities of interest: the construction cost and the search cost, and, for the
latter, it is normally appropriate to consider separately the cases where the
search is successful or unsuccessful. In the case of binary search trees, these
cost functions are closely related to path length.
Construction cost. We assume that a binary search tree is built by successive
insertions, drawing from a random source of keys (for example, independent
and uniformly distributed random numbers between 0 and 1). Ļis implies
that all N! key orderings are equally likely, and is thus equivalent to assuming
that the keys are a random permutation of the integers 1 to N. Now, observe
that the trees are formed by a splitting process: the ŀrst key inserted becomes
the node at the root, then the left and right subtrees are built independently.
Ļe probability that the kth smallest of the N keys is at the root is 1/N
(independent of k), in which case subtrees of size k −1 and N −k are built
on the left and right, respectively. Ļe total cost of building the subtrees is
one larger for each node (a total of k−1+N −k = N −1) than if the subtree

șȠț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȟ
were at the root, so we have the recurrence
CN = N −1 + 1
N
∑
1≤k≤N
(Ck−1 + CN−k)
for N > 0 with C0 = 0.
Of course, as mentioned in §6.6, this recurrence also describes the average
internal path length for binary search trees. Ļis is also the recurrence solved
in Chapter 1 for the number of comparisons taken by quicksort, except with
N −1 instead of N + 1.
Ļus, we have already done the analysis of the cost of constructing a
binary search tree, in §1.5 and in §3.10.
Ļeorem6.5 (ConstructioncostofBSTs).
Ļe average number of compar-
isons involved in the process of constructing a binary search tree by inserting
N distinct keys in random order into an initially empty tree (the average in-
ternal path length of a random binary search tree) is
2(N + 1)(HN+1 −1) −2N ≈1.386NlgN −2.846N
with variance asymptotic to (7 −2π2/3)N2.
Proof. From the earlier discussion, the solution for the average follows directly
from the proof and discussion of Ļeorem 1.2.
Ļe variance follows precisely as in the proof of Ļeorem 3.12. Deŀne
the BGF
Q(z, u) =
∑
p∈P
z|p|
|p|!uipl(p)
where P denotes the set of all permutations and ipl(p) denotes the internal
path length of the binary search tree constructed when the elements of p are
inserted into an initially empty tree using the standard algorithm. By virtu-
ally the same computation as in §3.10, this BGF must satisfy the functional
equation
∂
∂z Q(z, u) = Q2(zu, u)
with
Q(0, u) = 1.
Ļis equation diﬀers from the corresponding equation for quicksort only in
that it lacks a u2 factor (which originates in the diﬀerence between N +1 and
N −1 in the recurrences). To compute the variance, we proceed just as in
§3.10, with exactly the same result (the u2 factor does not contribute to the
variance).

§ȝ.ȟ
T Ş ő ő ş
șȠȜ
Ļus, a “random” binary search tree (a tree built from a random permu-
tation) costs only about 40% more than a perfectly balanced tree. Figure 6.16
shows a large random binary search tree, which is quite well balanced by com-
parison with the bottom tree in Figure 6.10, a “random” binary tree under the
assumption that all trees are equally likely.
Ļe relationship to the quicksort recurrence highlights a fundamental
reason why trees are important to study in the analysis of algorithms: recursive
programs involve implicit tree structures. For example, the tree on the left in
Figure 6.12 can also be viewed as a precise description of the process of sorting
the keys with Program 1.2: we view the key at the root as the partitioning
element; the left subtree as a description of the sorting of the left subŀle; and
the right subtree as a description of the sorting of the right subŀle. Binary
trees could also be used to describe the operation of mergesort, and other
types of trees are implicit in the operation of other recursive programs.
Exercise 6.32 For each of the trees in Figure 6.12, give permutations that would
cause Program 1.2 to partition as described by the tree.
Search costs. A successful search is a search where a previously inserted key is
found. We assume that each key in the tree is equally likely to be sought. An
unsuccessful search is a search for a key that has not been previously inserted.
Ļat is, the key sought is not in the tree, so the search terminates at an external
node. We assume that each external node is equally likely to be reached. For
example, this is the case for each search in our model, where new keys are
drawn from a random source.
We want to analyze the costs of searching in the tree, apart from its
construction. Ļis is important in applications because we normally expect a
tree to be involved in a very large number of search operations, and the con-
struction costs are small compared to the search costs for many applications.
To do the analysis, we adopt the probabilistic model that the tree was built by
Figure 6.16 A binary search tree built from 237 randomly ordered keys

șȠȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȟ
random insertions and that the searches are “random” in the tree, as described
in the previous paragraph. Both costs are directly related to path length.
Ļeorem 6.6 (Search costs in BSTs).
In a random binary search tree of
N nodes, the average cost of a successful search is 2HN −3 −2HN/N and
the average cost of an unsuccessful search is 2HN+1 −2. In both cases, the
variance is ∼2HN.
Proof. Ļe number of comparisons needed to ŀnd a key in the tree is exactly
one greater than the number that was needed to insert it, since keys never
move in the tree. Ļus, the result for successful search is obtained by di-
viding the cost of constructing the tree (the internal path length, given in
Ļeorem 6.5) by N and adding 1.
Since the level of an external node is precisely the cost of reaching it
during an unsuccessful search, the average cost of an unsuccessful search is
exactly the external path length divided by N + 1, so the stated result follows
directly from the ŀrst lemma in §6.3 and Ļeorem 6.5.
Ļe variances require a diﬀerent calculation, discussed below.
Analysis with PGFs. Ļe proof of Ļeorem 6.6 is a convenient application
of previously derived results to give average costs; however, it does not give a
way to calculate, for example, the standard deviation.
Ļis is true because of diﬀerences in the probabilistic models. For in-
ternal path length (construction cost), there are N! diﬀerent possibilities to
be accounted for, while for successful search cost, there are N · N! possibil-
ities. Internal path length is a quantity that varies between NlgN and N2
(roughly), while successful search cost varies between 1 and N. For a par-
ticular tree, we get the average successful search cost by dividing the internal
path length by N, but characterizing the distribution of search costs is an-
other matter. For example, the probability that the successful search cost is
1 is 1/N, which is not at all related to the probability that the internal path
length is N, which is 0 for N > 1.
Probability generating functions (or, equivalently in this case, the sym-
bolic method) provide an alternative derivation for search costs and also can
allow calculation of moments. For example, the PGF for the cost of an un-
successful search satisŀes
pN(u) =
(N −1
N + 1 +
2u
N + 1
)
pN−1(u),

§ȝ.Ƞ
T Ş ő ő ş
șȠȞ
since the Nth insertion contributes 1 to the cost of an unsuccessful search if
the search terminates at one of its two external nodes, which happens with
probability 2/(N + 1); or 0 otherwise. Diﬀerentiating and evaluating at 1
gives a simple recurrence for the average that telescopes directly to the result of
Ļeorem 6.6, and the variance follows in a similar manner. Ļese calculations
are summarized in the exercises that follow.
Exercise 6.33 What is the probability that the successful search cost is 2?
Exercise 6.34 Construct a random 1000-node binary search tree by inserting 1000
random keys into an initially empty tree, then do 10,000 random searches in that tree
and plot a histogram of the search costs, for comparison with Figure 1.4.
Exercise 6.35 Do the previous exercise, but generate a new tree for each trial.
Exercise 6.36 [Lynch, cf. Knuth] By calculating p′′
N(1)+p′
N(1)−p′(1)2. show that
the variance of unsuccessful search cost is 2HN+1 −4H(2)
N+1 + 2.
Exercise 6.37 [Knott, cf. Knuth] Using a direct argument with PGFs, ŀnd the
average and variance for the cost of a successful search.
Exercise 6.38 Express the PGF for successful search in terms of the PGF for unsuc-
cessful search. Use this to express the average and variance for successful search in
terms of the average and variance for unsuccessful search.
6.9 Additive Parameters of Random Trees. Ļe CGF-based method
that we used earlier to analyze path length in Catalan trees and binary search
trees generalizes to cover a large class of parameters that are deŀned additively
over subtrees. Speciŀcally, deŀne an additive parameter to be any parameter
whose cost function satisŀes the linear recursive schema
c(t) = e(t) +
∑
s
c(s)
where the sum is over all the subtrees of the root of t. Ļe function e is called
the “toll,” the portion of the cost associated with the root. Ļe following table
gives examples of cost functions and associated tolls:
toll function e(t)
cost function c(t)
1
size |t|
|t| −1
internal path length
δ|t|1
number of leaves

șȠȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ƞ
We normally take the toll function to be 0 for the empty binary tree.
It is possible to develop a fully general treatment of the average-case
analysis of any additive parameter for both of the Catalan tree models and for
the BST model. Indeed, this encompasses all the theorems about properties
of trees that we have seen to this point.
Ļeorem 6.7 (Additive parameters in random trees).
Let CT (z), CG(z),
and CB(z) be the CGFs of an additive tree parameter c(t) for the binary
Catalan, general Catalan, and binary search tree models, respectively, and let
ET (z), EG(z), and EB(z) be the CGFs for the associated toll function e(t).
(For the binary search tree case, use exponential CGFs.) Ļese functions are
related by the equations
CT (z) =
ET (z)
√1 −4z
(binary Catalan trees)
CG(z) = 1
2EG(z)
(
1 +
1
√1 −4z
)
(general Catalan trees)
CB(z) =
1
(1 −z)2
(
EB(0) +
∫z
0
(1 −x)2E′
B(x)dx
)
(binary search trees).
Proof. Ļe proofs follow precisely the same lines as the arguments that we
have given for path length.
First, let T be the set of all binary Catalan trees. Ļen, just as in §6.6,
we have
CT (z) ≡
∑
t∈T
c(t)z|t|
=
∑
t∈T
e(t)z|t| +
∑
tl∈T
∑
tr∈T
(c(tl) + c(tr))z|tl|+|tr|+1
= ET (z) + 2zT(z)CT (z),
where T(z) = (1−√1 −4z )/(2z) is the OGF for the Catalan numbers TN.
Ļis leads directly to the stated result.

§ȝ.Ƞ
T Ş ő ő ş
șȠȠ
Next, for general Catalan trees, let G be the set of trees. Again, just as
in §6.6, we have
CG(z) ≡
∑
t∈G
c(t)z|t|
=
∑
t∈G
e(t)z|t| +
∑
k≥0
∑
t1∈G
. . .
∑
tk∈G
(c(t1) + · · · + c(tk))z|t1|+...+|tk|+1
= EG(z) + z
∑
k≥0
kCG(z)Gk−1(z)
= EG(z) +
zCG(z)
(1 −G(z))2
where G(z) = zT(z) = (1 −√1 −4z )/2 OGF for the Catalan numbers
TN−1, enumerating general trees. Again, substituting this and simplifying
leads directly to the stated result.
For binary search trees, we let cN and eN, respectively, denote the ex-
pected values of c(t) and e(t) over random BSTs of size N. Ļen the expo-
nential CGFs C(z) and E(z) are the same as the OGFs for these sequences,
and we follow the derivation in §3.3. We have the recurrence
cN = eN + 2
N
∑
1≤k≤N
ck−1
for N ≥1 with c0 = e0,
which leads to the diﬀerential equation
C′
B(z) = E′
B(z) + 2CB(z)
1 −z
with CB(0) = EB(0),
which can be solved precisely as in §3.3 to yield the stated solution.
Corollary Ļe mean values of the additive parameters are given by
[zN]CT (z)/TN
(binary Catalan trees)
[zN]CG(z)/TN−1
(general Catalan trees)
[zN]CB(z)
(binary search trees).
Proof. Ļese follow directly from the deŀnitions and Ļeorem 3.11.

Țȗȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Ƞ
Ļis vastly generalizes the counting and path length analyses that we
have done and permits us to analyze many important parameters. Ļe count-
ing and path length results that we have derived in the theorems earlier in this
chapter all follow from a simple application of this theorem. For example, to
compute average path length in binary Catalan trees, we have
ET (z) = 1 +
∑
t∈T
(|t| −1)z|t| = 1 + zT ′(z) −T(z)
and therefore
CT (z) = zT ′(z) −T(z) + 1
√1 −4z
,
which is equivalent to the expression derived in the proof of Ļeorem 6.3.
Leaves. As an example of the use of Ļeorem 6.7 for a new problem, we
consider the analysis of the average number of leaves for each of the three
models. Ļis is representative of an important class of problems related to
memory allocation for recursive structures. For example, in a binary tree, if
space is at a premium, we might seek a representation that avoids the null
pointers in leaves. How much space could be saved in this way? Ļe answer
to this question depends on the tree model: determining the average number
of leaves is a straightforward application of Ļeorem 6.7, using e(t) = δ|t|1
and therefore ET (z) = EG(z) = EB(z) = z.
First, for binary Catalan trees, we have CT (z) = z/√1 −4z. Ļis
matches the result derived in §3.10.
Second, for general Catalan trees, we have
CG(z) = z
2 +
z
2√1 −4z ,
which leads to the result that the average is N/2 exactly for N > 1.
Ļird, for binary search trees, we get
CB(z) = 1
3
1
(1 −z)2 + 1
3(z −1),
so the mean number of leaves is (N + 1)/3 for N > 1.

§ȝ.Ƞ
T Ş ő ő ş
ȚȗȘ
Corollary For N > 1, the average number of leaves is given by
N(N + 1)
2(2N −1) ∼N
4
in a random binary Catalan tree with N nodes,
N
2
in a random general Catalan tree with N nodes, and
N + 1
3
in a binary search tree built from N random keys.
Proof. See the discussion provided earlier.
Ļe techniques that we have been considering are clearly quite useful in
analyzing algorithms involving trees, and they apply in some other situations,
as well. For example, in Chapter 7 we analyze properties of permutations via
a correspondence with trees (see §7.5).
Exercise 6.39 Find the average number of children of the root in a random Catalan
tree of N nodes. (From Figure 6.3, the answer is 2 for N = 5.)
Exercise 6.40 In a random Catalan tree of N nodes, ŀnd the proportion of nodes
with one child.
Exercise 6.41 In a random Catalan tree of N nodes, ŀnd the proportion of nodes
with k children for k = 2, 3, and higher.
Exercise 6.42 Internal nodes in binary trees fall into one of three classes: they have
either two, one, or zero external children. What fraction of the nodes are of each
type, in a random binary Catalan tree of N nodes?
Exercise 6.43 Answer the previous question for random binary search trees.
Exercise 6.44 Set up BGFs for the number of leaves and estimate the variance for
each of the three random tree models.
Exercise 6.45 Prove relationships analogous to those in Ļeorem 6.7 for BGFs.

Țȗș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șȗ
6.10 Height. What is the average height of a tree? Path length analysis
(using the second lemma in §6.4) suggests lower and upper bounds of order
N1/2 and N3/4 for Catalan trees (either binary or general) and of order logN
and √NlogN for binary search trees. Developing more precise estimates
for the average height turns out to be a more diﬃcult question to answer,
even though the recursive deŀnition of height is as simple as the recursive
deŀnition of path length. Ļe height of a tree is 1 plus the maximum of the
heights of the subtrees; the path length of a tree is 1 plus the sum of the
path lengths of the subtrees plus the number of nodes in the subtrees. As
we have seen, the latter decomposition can correspond to “constructing” trees
from subtrees, and additivity is mirrored in the analysis (by the linearity of
the cost GF equations). No such treatment applies to the operation of taking
the maximum over subtrees.
Generating functions for binary Catalan trees. We begin with the prob-
lem of ŀnding the height of a binary Catalan tree. Attempting to proceed as
for path length, we start with the bivariate generating function
P(z, u) =
∑
N≥0
∑
h≥0
PNhzNuh =
∑
t∈T
z|t|uh(t).
Now the recursive deŀnition of height leads to
P(z, u) =
∑
tl∈T
∑
tr∈T
z|tl|+|tr|+1umax(h(tl),h(tr)).
For path length, we were able to rearrange this into independent sums, but
the “max” precludes this.
In contrast, using the “vertical” formulation for bivariate sequences that
is described in §3.10, we can derive a simple functional equation. Let Th be
the class of binary Catalan trees of height no greater than h, and
T [h](z) =
∑
t∈Th
z|t|.
Proceeding in precisely the same manner as for enumeration gives a simple
functional equation for T [h](z): any tree with height no greater than h + 1 is

§ȝ.Șȗ
T Ş ő ő ş
ȚȗȚ
either empty or a root node and two subtrees with height no greater than h,
so
T [h+1](z) = 1 +
∑
tL∈Th
∑
tR∈Th
z|tL|+|tR|+1
= 1 + zT [h](z)2.
Ļis result is also available via the symbolic method: it corresponds to the
symbolic equation
Th+1 = ⊓⊔+ • × Th × Th.
Iterating this recurrence, we have
T [0](z) = 1
T [1](z) = 1 + z
T [2](z) = 1 + z + 2z2 + z3
T [3](z) = 1 + z + 2z2 + 5z3 + 6z4 + 6z5 + 4z6 + z7
...
T [∞](z) = 1 + z + 2z2 + 5z3 + 14z4 + 42z5 + 132z6 + . . . = T(z).
Ļe reader may ŀnd it instructive to check these against the initial values for
the small trees given in Figure 5.2. Next, the corollary to Ļeorem 3.11 tells
us that the cumulated cost (the sum of the heights of all trees of N nodes) is
given by
[zN]
∑
h≥0
(T(z) −T [h](z)).
But now our analytic task is much harder. Rather than estimating coeﬃ-
cients in an expansion on one function for which we have a deŀning functional
equation, we need to estimate coeﬃcients in an entire series of expansions of
functions deŀned by interrelated functional equations. Ļis turns out to be
an extremely challenging task for this particular problem.
Ļeorem 6.8 (Binary tree height).
Ļe average height of a random binary
Catalan tree with N nodes is 2
√
πN + O(N1/4+ϵ) for any ϵ > 0.
Proof. Omitted, though see the comments above. Details may be found in
Flajolet and Odlyzko [12].

Țȗț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șȗ
Average height of Catalan trees. For general Catalan trees, the problem of
determining the average height is still considerably more diﬃcult than ana-
lyzing path length, but we can sketch the solution. (Warning: Ļis “sketch”
involves a combination of many of the advanced techniques from Chapters 2
through 5, and should be approached with caution by novice readers.)
First, we construct Gh+1, the set of trees of height ≤h + 1, by
Gh+1 = {•}×(ϵ+Gh+(Gh×Gh)+(Gh×Gh×Gh)+(Gh×Gh×Gh×Gh)+. . .),
which translates by the symbolic method to
G[h+1](z) = z(1 + G[h](z) + G[h](z)2 + G[h](z)3 + . . .) =
z
1 −G[h](z).
Iterating this recurrence, we see that
G[0](z) = z
G[1](z) = z
1
1 −z
G[2](z) =
z
1 −
z
1 −z
= z 1 −z
1 −2z
G[3](z) =
z
1 −
z
1 −
z
1 −z
= z
1 −2z
1 −3z + z2
...
G[∞](z) = z + z2 + 2z3 + 5z3 + 14z5 + 42z6 + 132z7 + . . . = zT(z)
Ļese are rational functions with enough algebraic structure that we can derive
exact enumerations for the height and obtain asymptotic estimates.
Ļeorem 6.9 (Catalan tree height GF ). Ļe number of Catalan trees with
N + 1 nodes and height greater than or equal to h −1 is
GN+1 −G[h−2]
N+1
=
∑
k≥1
((
2N
N + 1 −kh
)
−2
(
2N
N −kh
)
+
(
2N
N −1 −kh
))
.

§ȝ.Șȗ
T Ş ő ő ş
ȚȗȜ
Proof. From the basic recurrence and initial values given previously, it fol-
lows that G[h](z) can be expressed in the form G[h](z) = zFh+1(z)/Fh+2(z),
where Fh(z) is a family of polynomials
F0(z) = 0
F1(z) = 1
F2(z) = 1
F3(z) = 1 −z
F4(z) = 1 −2z
F5(z) = 1 −3z + z2
F6(z) = 1 −4z + 3z2
F7(z) = 1 −5z + 6z2 −z3
...
that satisfy the recurrence
Fh+2(z) = Fh+1(z) −zFh(z)
for h ≥0 with F0(z) = 0 and F1(z) = 1.
Ļese functions are sometimes called Fibonacci polynomials, because they gen-
eralize the Fibonacci numbers, to which they reduce when z = −1.
When z is kept ŀxed, the Fibonacci polynomial recurrence is simply a
linear recurrence with constant coeﬃcients (see §2.4). Ļus its solutions are
expressible in terms of the solutions
β = 1 + √1 −4z
2
and
bβ = 1 −√1 −4z
2
of the characteristic equation y2 −y + z = 0. Solving precisely as we did for
the Fibonacci numbers in §2.4, we ŀnd that
Fh(z) = βh −bβh
β −bβ
and therefore
G[h](z) = z βh+1 −bβh+1
βh+2 −bβh+2 .
Notice that the roots are closely related to the Catalan GF:
bβ = G(z) = zT(z)
and
β = z/ bβ = z/G(z) = 1/T(z)

Țȗȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șȗ
and that we have the identities z = β(1 −β) = bβ(1 −bβ).
In summary, the GF for trees of bounded height satisŀes the formula
G[h](z) = 2z (1 + √1 −4z )h+1 −(1 −√1 −4z )h+1
(1 + √1 −4z )h+2 −(1 −√1 −4z )h+2 ,
and a little algebra shows that
G(z) −G[h](z) =
√
1 −4z
uh+2
1 −uh+2
where u ≡bβ/β = G2(z)/z. Ļis is a function of G(z), which is implic-
itly deŀned by z = G(z)(1 −G(z)), so the Lagrange inversion theorem
(see §6.12) applies, leading (after some calculation) to the stated result for
[zN+1](G(z) −G[h−2](z)).
Corollary Ļe average height of a random Catalan tree with N nodes is
√
πN + O(1).
Proof Sketch. By the corollary to Ļeorem 3.11, the average height is given by
∑
h≥1
[zN](G(z) −G[h−1](z))
GN
.
For Ļeorem 6.9, this reduces to three sums that are very much like Catalan
sums, and can be treated in a manner similar to the proof of Ļeorem 4.9.
From asymptotic results on the tails of the binomial coeﬃcients (the corollary
to Ļeorem 4.6), the terms are exponentially small for large h. We have
[zN](G(z) −G[h−1](z)) = O(N4Ne−(log2N))
for h >
√
N logN by applying tail bounds to each term in the binomial
sum in Ļeorem 6.9. Ļis already shows that the expected height is itself
O(N1/2logN).
For smaller values of h, the normal approximation of Ļeorem 4.6 ap-
plies nicely. Using the approximation termwise as we did in the proof of
Ļeorem 4.9, it is possible to show that
[zN](G(z) −G[h−1](z))
GN
∼H(h/
√
N )

§ȝ.Șȗ
T Ş ő ő ş
ȚȗȞ
where
H(x) ≡
∑
k≥1
(4k2x2 −2)e−k2x2.
Like the trie sum diagrammed in Figure 4.7, the function H(h/
√
N ) is close
to 1 when h is small and close to 0 when h is large, with a transition from 1
to 0 when h is close to
√
N. Ļen, the expected height is approximately
∑
h≥1
H(h/
√
N ) ∼
√
N
∫∞
0
H(x)dx ∼
√
πN
by Euler-Maclaurin summation and by explicit evaluation of the integral.
In the last few steps, we have ignored the error terms, which must be
kept suitably uniform. As usual for such problems, this is not diﬃcult because
the tails are exponentially small, but we leave the details for the exercises be-
low. Full details for a related but diﬀerent approach to proving this result are
given in De Bruijn, Knuth, and Rice [8].
Ļe analyses of tree height in binary trees and binary Catalan trees are
the hardest nuts that we are cracking in this book. While we recognize that
many readers may not be expected to follow a proof of this scope and com-
plexity without very careful study, we have sketched the derivation in some
detail because height analysis is extremely important to understanding basic
properties of trees. Still, this sketch allows us to appreciate (i) that analyzing
tree height is not an easy task, but (ii) that it is possible to do so, using the
basic techniques that we have covered in Chapters 2 through 5.
Exercise 6.46 Prove that Fh+1(z) = ∑
j
(h−j
j
)
(−z)j.
Exercise 6.47 Show the details of the expansion of G(z) −G[h−2](z) with the La-
grange inversion theorem (see §6.12).
Exercise 6.48 Provide a detailed proof of the corollary, including proper attention to
the error terms.
Exercise 6.49 Draw a plot of the function H(x).

Țȗȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șȗ
Height of binary search trees. For binary search trees built from random
permutations, the problem of ŀnding the average height is also quite diﬃcult.
Since the average path length is O(NlogN), we would expect the average
height of a binary search tree to be ∼clogN, for some constant c; this is in
fact the case.
Ļeorem 6.10 (Binary search tree height).
Ļe expected height of a binary
search tree built from N random keys is ∼clogN, where c ≈4.31107... is
the solution c > 2 of cln(2e/c) = 1.
Proof. Omitted; see Devroye [9] or Mahmoud [27].
Ļough the complete analysis is at least as daunting as the Catalan tree
height analysis provided earlier, it is easy to derive functional relationships
among the generating functions. Let q[h]
N be the probability that a BST built
with N random keys has height no greater than h. Ļen, using the usual
splitting argument, and noting that the subtrees have height no greater than
h −1, we have the recurrence
q[h]
N = 1
N
∑
1≤k≤N
q[h−1]
k−1 q[h−1]
N−1−k,
which leads immediately to the schema
d
dz q[h](z) = (q[h−1](z))2.
Stack height. Tree height appears frequently in the analysis of algorithms.
Fundamentally, it measures not only the size of the stack needed to traverse
a tree, but also the space used when a recursive program is executed. For
example, in the expression evaluation algorithm discussed earlier, the tree
height η(t) measures the maximum depth reached by the recursive stack when
the expression represented by t is evaluated. Similarly, the height of a binary
search tree measures the maximum stack depth reached by a recursive inorder
traversal to sort the keys, or the implicit stack depth used when a recursive
quicksort implementation is used.
Tree traversal and other recursive algorithms also can be implemented
without using recursion by directly maintaining a pushdown stack (last-in-
ŀrst-out data structure). When there is more than one subtree to visit, we

§ȝ.Șȗ
T Ş ő ő ş
ȚȗȠ
save all but one on the stack; when there are no subtrees to visit, we pop the
stack to get a tree to visit. Ļis uses fewer stack entries than are required
in a stack supporting a recursive implementation, because nothing is put on
the stack if there is only one subtree to visit. (A technique called end recur-
sion removal is sometimes used to get equivalent performance for recursive
implementations.) Ļe maximum stack size needed when a tree is traversed
using this method is a tree parameter called the stack height, which is similar
to height. It can be deŀned by the recursive formula:
s(t) =







0,
if t is an external node;
s(tl),
if tr is an external node;
s(tr),
if tl is an external node;
1 + max(s(tl), s(tr))
otherwise.
Because of the rotation correspondence, it turns out that the stack height
of binary Catalan trees is essentially distributed like the height of general
Catalan trees. Ļus, the average stack height for binary Catalan trees is also
studied by De Bruijn, Knuth, and Rice [8], and shown to be ∼
√
πN.
Exercise 6.50 Find a relationship between the stack height of a binary tree and the
height of the corresponding forest.
Register allocation. When the tree represents an arithmetic expression, the
minimum number of registers needed to evaluate the expression can be de-
scribed by the following recursive formula:
r(t) =











0,
if t is an external node;
r(tl),
if tr is an external node;
r(tr),
if tl is an external node;
1 + r(tl),
if r(tl) = r(tr);
max(r(tl), r(tr))
otherwise.
Ļis quantity was studied by Flajolet, Raoult, and Vuillemin [14] and by
Kemp [23]. Ļough this recurrence seems quite similar to the corresponding
recurrences for height and stack height, the average value is not O(
√
N ) in
this case, but rather ∼(lgN)/2.

ȚȘȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȘ
6.11 Summary of Average-Case Results on Properties of Trees. We
have discussed three diﬀerent tree structures (binary trees, trees, and binary
search trees) and two basic parameters (path length and height), giving a to-
tal of six theorems describing the average values of these parameters in these
structures. Each of these results is fundamental, and it is worthwhile to con-
sider them in concert with one another.
As indicated in §6.9, the basic analytic methodology for these para-
meters extends to cover a wide variety of properties of trees, and we can place
new problems in proper context by examining relationships among these para-
meters and tree models. At the same time, we brieły sketch the history of
these results, which are summarized in Tables 6.1 and 6.2.
For brevity in this section, we refer to binary Catalan trees simply as
“binary trees,” Catalan trees as “trees,” and binary search trees as “BSTs,”
recognizing that a prime objective in the long series of analyses that we have
discussed has been to justify these distinctions in terminology and quantify
diﬀerences in the associated models of randomness.
Figures 6.10 and 6.16 show a random forest (random tree with its root
removed), binary tree, and binary search tree, respectively. Ļese reinforce the
analytic information given in Tables 6.1 and 6.2: heights for binary trees and
trees are similar (and proportional to
√
N ), with trees about half as high as
binary trees; and paths in binary search trees are much shorter (proportional to
logN). Ļe probability distribution imposed on binary search tree structures
is biased toward trees with short paths.
functional equation on GF
asymptotic estimate of [zN]
tree
Q(z, u) =
z
1 −Q(zu, u)
N
2
√
πN −N
2 + O
(√
N
)
binary tree
Q(z, u) = zQ(zu, u)2 + 1
N
√
πN −3N + O(
√
N )
BST
∂
∂z Q(z, u) = Q(zu, z)2
2NlnN + (2γ −4)N + O(logN)
Table 6.1
Expected path length of trees

§ȝ.ȘȘ
T Ş ő ő ş
ȚȘȘ
Perhaps the easiest problem on the list is the analysis of path length in
binary search trees. Ļis is available with elementary methods, and dates back
at least to the invention of quicksort in 1960 [22]. Ļe variance for tree con-
struction costs (the same as the variance for quicksort) was evidently ŀrst pub-
lished by Knuth [25]; Knuth indicates that recurrence relations describing the
variance and results about search costs were known in the 1960s. By contrast,
the analysis of the average height of binary search trees is a quite challenging
problem, and was the last problem on the list to be completed, by Devroye in
1986 [9][10]. Path length in random trees and random binary trees is also not
diﬃcult to analyze, though it is best approached with generating-function-
based or symbolic combinatorial tools. With such an approach, analysis of
this parameter (and other additive parameters) is not much more diﬃcult
than counting.
Ļe central role of tree height in the analysis of computer programs
based on trees and recursive programs was clear as such programs came into
widespread use, but it was equally clear that the analysis of nonadditive para-
meters in trees such as height can present signiŀcant technical challenges. Ļe
analysis of the height of trees (and stack height for binary trees)—published
in 1972 by De Bruijn, Knuth, and Rice [8]—showed that such challenges
could be overcome, with known analytic techniques, as we have sketched in
§6.10. Still, developing new results along these lines can be a daunting task,
even for experts. For example, the analysis of height of binary trees was not
completed until 1982, by Flajolet and Odlyzko [12].
functional equation on GF
asymptotic estimate of mean
tree
q[h+1](z) =
z
1 −q[h](z)
√
πN + O(1)
binary tree
q[h+1](z) = z(q[h](z))2 + 1
2
√
πN + O(N 1/4+ϵ)
BST
d
dz q[h+1](z) = (q[h](z))2
(4.3110 · · ·)lnN + o(logN)
Table 6.2
Expected height of trees

ȚȘș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șș
Path length and height in random trees are worthy of careful study
because they illustrate the power of generating functions, and the contrasting
styles in analysis that are appropriate for “additive” and “nonadditive” para-
meters in recursive structures. As we saw in §6.3, trees relate directly to a
number of classical problems in probability and combinatorics, so some of
the problems that we consider have a distinguished heritage, tracing back a
century or two. But the motivation for developing precise asymptotic results
for path length and height as we have been doing certainly can be attributed to
the importance of trees in the analysis of algorithms (see Knuth [8][24][25]).
6.12 Lagrange Inversion. Next, we turn to the study of other types of
trees, using analytic combinatorics. Ļe symbolic method often leaves us
with the need to extract coeﬃcients from generating functions that are im-
plicitly deŀned through functional equations. Ļe following transfer theorem
is available for this task, and is of particular importance for tree enumeration.
Ļeorem 6.11 (Lagrange inversion theorem).
Suppose that a generating
function A(z) = ∑
n≥0 anzn satisŀes the functional equation z = f(A(z)),
where f(z) satisŀes f(0) = 0 and f′(0) ̸= 0. Ļen
an ≡[zn]A(z) = 1
n[un−1]
( u
f(u)
)n.
Also,
[zn](A(z))m = m
n [un−m]
( u
f(u)
)n
and
[zn]g(A(z)) = 1
n[un−1]g′(u)
( u
f(u)
)n.
Proof. Omitted; see, for example, [6]. Ļere is a vast literature on this formula,
dating back to the 18th century.
Ļe functional inverse of a function f is the function f−1 that satisŀes
f−1(f(z)) = f(f−1(z)) = z.
Applying f−1 to both sides of the equation z = f(A(z)), we see that the
function A(z) is the functional inverse of f(z). Ļe Lagrange theorem is a

§ȝ.Șș
T Ş ő ő ş
ȚȘȚ
general tool for inverting power series, in this sense. Its surprising feature is to
provide a direct relation between the coeﬃcients of the functional inverse of
a function and the powers of that function. In the present context, Lagrange
inversion is a very useful tool for extracting coeﬃcients for implicit GFs. Be-
low we show how it applies tp binary trees and then give two examples that
emphasize the formal manipulations and motivate the utility of the theorem,
which will prepare us for the study of many other types of trees.
Binary trees. Let T [2](z) = zT(z) be the OGF for binary trees, counted by
external nodes. Rewriting the functional equation T [2](z) = z + T [2](z)2 as
z = T [2](z) −T [2](z)2,
we can apply Lagrange inversion with f(u) = u −u2. Ļis gives the result
[zn]T [2](z) = 1
n[un−1]
(
u
u −u2
)n = 1
n[un−1]
(
1
1 −u
)n.
Now, from Table 3.1, we know that
un−1
(1 −u)n =
∑
k≥n−1
(
k
n −1
)
uk
so that, considering the term k = 2n −2,
[un−1]
(
1
1 −u
)n =
(
2n −2
n −1
)
which leads to the Catalan numbers, as expected.
Ternary trees.
One way to generalize binary trees is to consider ternary
trees where every node is either external or has three subtrees (left, mid-
dle, and right). Note that the number of external nodes in a ternary tree
is odd. Ļe sequence of counts for ternary trees with n external nodes for
n = 1, 2, 3, 4, 5, 6, 7, . . . is 1, 0, 1, 0, 3, 0, 3, . . . . Ļe symbolic method im-
mediately gives the GF equation
z = T [3](z) −T [3](z)3.

ȚȘț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șș
Proceeding as in §3.8 for binary trees does not succeed easily because this
is a cubic equation, not a quadratic. But applying Lagrange inversion with
f(u) = u −u3 immediately gives the result
[zn]T [3](z) = 1
n[un−1]
(
1
1 −u2
)n.
Proceeding in the same manner as used earlier, we know from Table 3.1 that
u2n−2
(1 −u2)n =
∑
k≥n−1
(
k
n −1
)
u2k.
Considering the term 2k = 3n −3 (which only exists when n is odd) gives
[zn]T [3](z) = 1
n
(
(3n −3)/2
n −1
)
for n odd and 0 for n even, which is OEIS A001764 [34] alternating with 0s.
Forests of binary trees. Another way to generalize binary trees is to consider
sets of them, or so-called forests. A k-forest of binary trees is simply an ordered
sequence of k binary trees. By Ļeorem 5.1, the OGF for k-forests is just
(zT(z))k, where T(z) is the OGF for binary trees, and by Lagrange inversion
(using the second case in Ļeorem 6.11), the number of k-forests of binary
trees with n external nodes is therefore
[zn]
(1 −√1 −4z
2
)k = k
n
(
2n −k −1
n −1
)
.
Ļese numbers are also known as the ballot numbers (see Chapter 8).
Exercise 6.51 Find [zn]A(z) when A(z) is deŀned by z = A(z)/(1 −A(z)).
Exercise 6.52 What is the functional inverse of ez −1? What do we get in terms of
power series by applying Lagrange inversion?
Exercise 6.53 Find the number of n-node 3-forests of ternary trees.

§ȝ.ȘȚ
T Ş ő ő ş
ȚȘȜ
Exercise 6.54 Find the number of 4-ary trees, where every node either is external or
has a sequence of four subtrees.
6.13 Rooted Unordered Trees. An essential aspect of the deŀnition of
trees and forests given previously is the notion of a sequence of trees: the order
in which individual trees appear is considered signiŀcant. Indeed, the trees
that we have been considering are also called ordered trees. Ļis is natural
when we consider various computer representations or, for example, when
we draw a tree on paper, because we must somehow put down one tree af-
ter another. Forests with trees in diﬀering orders look diﬀerent and they are
typically processed diﬀerently by computer programs. In some applications,
however, the sequence is actually irrelevant. We will see examples of such
algorithms as we consider the basic deŀnitions, then we will consider enu-
meration problems for unordered trees.
Deŀnition An unordered tree is a node (called the root) attached to a multiset
of unordered trees. (Such a multiset is called an unordered forest.)
U3 = 2
U2 = 1
U1 = 1
U4 = 4
U5 = 9
Figure 6.17 Rooted unordered trees with N nodes, 1 ≤N ≤5

ȚȘȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȚ
Figure 6.17 shows the small rooted unordered trees, derived from Figure
6.4 by deleting every tree that can be transformed to a tree to its left by inter-
changing the order of subtrees at any node.
Sample application. As an example of an algorithm where rooted unordered
trees are an appropriate underlying data structure, we consider the union-ŀnd
problem: Ļe goal is to process a sequence of “union-ŀnd” operations on pairs
of N distinct items. Each operation combines a “ŀnd” operation that returns
T if the two items are equivalent or F if they are not equivalent with a “union”
operation that makes the two items equivalent by taking the union of their
equivalence classes. A familiar application is a social network, where a new
link between two friends merges their sets of friends. For example, given the
16 items (again, we use two initials for brevity) MS, JL, HT, JG, JB,
GC, PL, PD, MC, AB, AA, HF, EF, CN, AL, and JC, the sequence of
operations
MS ≡JL
MS ≡HT
JL ≡HT
AL ≡EF
AB ≡MC
JB ≡JG
AL ≡CN
PL ≡MS
JB ≡GC
JL ≡JG
AL ≡JC
GC ≡MS
should result in the sequence of return values
F
F
T
F
F
F
F
F
F
F
F
T
because the ŀrst two instructions make MS, JL, and HT equivalent, then the
third ŀnds JL and HT to be already equivalent, and so on.
public boolean unionfind(int p, int q)
{
int i = p; while (id[i] != i) i = id[i];
int j = q; while (id[j] != j) j = id[j];
if (i == j) return true;
id[i] = j; // Union operation
return false;
}
Program 6.4 Union-ŀnd

§ȝ.ȘȚ
T Ş ő ő ş
ȚȘȞ
Program 6.4 gives a solution to this problem. As it stands, the code is
opaque, but it is easy to understand in terms of an explicit parent link rep-
resentation of unordered forests, where each tree in the forest represents an
equivalence class. First, we use a symbol table to associate each item with an
integer between 0 and N −1. Ļen we represent the forest as an item-indexed
array: the entry corresponding to each node is the index of its parent in the
tree containing them, where a root has its own index. Ļe algorithm uses the
roots to determine whether or not two items are equivalent.
Given the index corresponding to an item, the unionfind method in
Program 6.4 ŀnds its corresponding root by following parent links until it
reaches a root. Accordingly, unionfind starts by ŀnding the roots corre-
sponding to the two given items. If both items correspond to the same root,
then they belong to the same equivalence class; otherwise, the relation con-
nects heretofore disconnected components. Ļe forest depicted in Figure 6.18
is the one built for the sequence of operations in the example given earlier.
Ļe shape of the forest depends on the relations seen so far and the order in
which they are presented.
0
6
1
5
 MS  JL  HT  JG  JB  GC  PL  PD  MC  AB  AA  HF  EF  CN  AL  JC 
 0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
4
3
2
7
9
8
AA
PD
MC
AB
10
MS
HT
JL
JB
PL
GC
JG
HF
11
EF
CN
AL
JC
13
15
14
12
symbol table
0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
0   0   0   4   0   4   0   7   8   9  10  11  14  14  14  14
parent-link representation
Figure 6.18 Representations of a rooted (unordered) forest

ȚȘȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȚ
Ļe algorithm moves up through the tree and never examines the sub-
trees of a node (or even tests how many there are). Combinatorially, the
union-ŀnd algorithm is a mapping from permutations of relations to un-
ordered forests. Program 6.4 is quite simple, and a variety of improvements
to the basic idea have been suggested and analyzed. Ļe key point to note
in the present context is that the order of appearance of children of a node is
not signiŀcant to the algorithm, or to the internal representation of the asso-
ciated tree—this algorithm provides an example of unordered trees naturally
occurring in a computation.
Unrooted (free) trees. Still more general is the concept of a tree where no
root node is distinguished. Figure 6.18 depicts all such trees with less than 7
nodes. To properly deŀne “unrooted, unordered trees,” or “free trees,” or just
“trees,” it is convenient to move from the general to the speciŀc, starting with
graphs, the fundamental structure underlying all combinatorial objects based
on sets of nodes and connections between them.
Deŀnition A graph is a set of nodes together with a set of edges that connect
pairs of distinct nodes (with at most one edge connecting any pair of nodes).
We can envision starting at some node and “following” an edge to the
constituent node for the edge, then following an edge to another node, and
F3 = 1
F2 = 1
F1 = 1
F4 = 2
F5 = 3
F6 = 6
Figure 6.19 Unrooted unordered (free) trees with N nodes, 1 ≤N ≤6

§ȝ.ȘȚ
T Ş ő ő ş
ȚȘȠ
so on. Ļe shortest sequence of edges leading from one node to another in
this way is called a simple path. A graph is connected if there is a simple path
connecting any pair of nodes. A simple path from a node back to itself is
called a cycle.
Every tree is a graph—but which graphs are trees? It is well known that
any one of the following four conditions is necessary and suﬃcient to ensure
that a graph G with N nodes is an (unrooted unordered) tree:
(i)
G has N −1 edges and no cycles.
(ii) G has N −1 edges and is connected.
(iii) Exactly one simple path connects each pair of vertices in G.
(iv) G is connected, but does not remain connected if any edge is re-
moved.
Ļat is, we could use any one of these conditions to deŀne free trees. To be
concrete, we choose the following descriptive combination:
Deŀnition A tree is a connected acyclic graph.
As an example of an algorithm where free trees arise in a natural way,
consider perhaps the most basic question that we can ask about a graph: is it
connected? Ļat is, is there some path connecting every pair of vertices? If so,
then there is a minimal set of edges comprising such paths called the spanning
tree of the graph. If the graph is not connected, then there is a spanning forest,
one tree for each connected component. Figure 6.20 gives examples of two
spanning trees of a large graph.
Deŀnition A spanning tree of a graph of N vertices is a set of N −1 of the
edges of the graph that form a tree.
By the basic properties of trees, a spanning tree must include all of the
nodes, and its existence demonstrates that all pairs of nodes are connected by
some path. In general, a spanning tree is an unrooted, unordered tree.
One well-known algorithm for ŀnding a spanning tree is to consider
each edge in turn, checking whther adding the next edge to the set comprising
the partial spanning tree built so far would cause a cycle. If not, add it to
the spanning tree and go on to consider the next edge. When the set has
N −1 edges in it, the edges represent an unordered, unrooted tree; indeed,
it is a spanning tree for the graph. One way to implement this algorithm
is to use the union-ŀnd algorithm given earlier for the cycle test. Ļat is,
we just run unionfind until getting a single component. If the edges have

Țșȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȚ
graph
spanning tree (rooted)
spanning tree (unrooted)
root
Figure 6.20 A large graph and two of its spanning trees
lengths that satisfy the triangle equality and we consider the edges in order of
their length, we get Kruskal's algorithm, which computes a minimal spanning
tree (no other spanning tree has smaller total edge length), shown on the
right in Figure 6.20. Ļe key point to note now is that Kruskal’s algorithm
is an example of free trees naturally occurring in a computation. Many other
algorithms for ŀnding spanning trees have been devised and analyzed—for
example, the breadth-ŀrst search algorithm picks a root and considers vertices
in order of their distance from the root, thereby computing a rooted spanning
tree, shown in the middle in Figure 6.20.
Ļe combinatorics literature contains a vast amount of material on the
theory of graphs, including many textbooks, and the computer science lit-
erature contains a vast amount of material about algorithms on graphs, also
including many textbooks. Full coverage of this material is beyond the scope
of this book, but understanding the simpler structures and algorithms that
we do cover is good preparation for addressing more diﬃcult questions about
properties of random graphs and the analysis of algorithms on graphs. Exam-
ples of graph problems where the techniques we have been considering apply
directly may be found in [15] and the classical reference Harary and Palmer
[21]. We will consider some special families of graphs again in Chapter 9,
but let us return now to our study of various types of trees.
Exercise 6.55 How many of the 2(
N
2) graphs on N labelled vertices are free trees?
Exercise 6.56 For each of the four properties listed earlier, show that the other three
are implied. (Ļis is 12 exercises in disguise!)

§ȝ.ȘȚ
T Ş ő ő ş
ȚșȘ
Tree hierarchy.
Ļe four major types of trees that we have deŀned form
a hierarchy, as summarized and illustrated in Figure 6.21. (i) Ļe free tree
is the most general, simply an acyclic connected graph. (ii) Ļe rooted tree
has a distinguished root node. (iii) Ļe ordered tree is a rooted tree where
the order of the subtrees of a node is signiŀcant. (iv) Ļe binary tree is an
ordered tree with the further restriction that every node has degree 0 or 2. In
the nomenclature that we use, the adjective describes the characteristic that
separates each type of tree from the one above it in the hierarchy. It is also
common to use nomenclature that separates each type from the one below it
in the hierarchy. Ļus, we sometimes refer to free trees as unrooted trees,
rooted trees as unordered trees, and ordered trees as general Catalan trees.
A few more words on nomenclature are appropriate because of the vari-
ety of terms found in the literature. Ordered trees are often called plane or pla-
nar trees and unordered trees are referred to as nonplane trees. Ļe term plane
is used because the structures can be transformed to one another with con-
tinuous deformations in the plane. Ļough this terminology is widely used,
we prefer ordered because of its natural implications with regard to computer
representations. Ļe term oriented in Figure 6.21 refers to the fact that the
free tree
unrooted tree
tree
connected
acyclic
ordered tree
planar tree
tree
Catalan tree
signiﬁcant
subtree order
rooted tree
planted tree
oriented tree
unordered tree
speciﬁed
root node
other
names
basic
properties
identical
trees
diﬀerent
trees
binary tree
binary
Catalan tree
rooted, ordered
2-ary internal nodes
0-ary external nodes 
Figure 6.21 Summary of tree nomenclature

Țșș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȚ
root is distinguished, so there is an orientation of the edges toward the root;
we prefer the term rooted, and we omit even that modiŀer when it is obvious
from the context that there is a root involved.
As the deŀnitions get more restrictive, the number of trees that are re-
garded as diﬀerent gets larger, so, for a given size, there are more rooted trees
than free trees and more ordered trees than rooted trees. It turns out that
the ratio between the number of rooted trees and the number of free trees is
proportional to N; the corresponding ratio of ordered trees to rooted trees
grows exponentially with N. It is also the case that the ratio of the number of
binary trees to the number of ordered trees with the same number of nodes is
a constant. Ļe rest of this section is devoted to a derivation of analytic results
that quantify these distinctions. Ļe enumeration results are summarized in
Table 6.3.
Figure 6.22 is an illustration of the hierarchy for trees with ŀve nodes.
Ļe 14 diﬀerent ŀve-node ordered trees are depicted in the ŀgure, and they
are further organized into equivalence classes using a meta-forest where all
the trees equivalent to a given tree are its children. Ļere are 3 diﬀerent ŀve-
node free trees (hence three trees in the forest), 9 diﬀerent ŀve-node rooted
trees (those at level 1 in the forest), and 14 diﬀerent ŀve-node ordered trees
(those at the bottom level in the forest). Note that the counts just given for
Figure 6.22 correspond to the fourth column (N = 5) in Table 6.3.
From a combinatorial point of view, we perhaps might be more inter-
ested in free trees because they diﬀerentiate structures at the most essential
2
3
4
5
6
7
8
9
10
N
free
1
1
2
3
6
11
23
47
106
∼c1αN/N 5/2
rooted
1
2
4
9
20
48
115
286
719
∼c2αN/N 3/2
ordered
1
2
5
14
42
132
429
1430
4862
∼c34N/N 3/2
binary
2
5
14
42
132
429
1430
4862 16796 ∼(4c3)4N/N 3/2
α ≈2.9558, c1 ≈.5350, c2 ≈.4399, c3 = 1/4√π ≈.1410
Table 6.3
Enumeration of unlabelled trees

§ȝ.ȘȚ
T Ş ő ő ş
ȚșȚ
rooted
ordered
trees
rooted
trees
free
trees
Figure 6.22 Trees with ŀve nodes (ordered, unordered, and unrooted)
level. From the point of view of computer applications, we are perhaps more
interested in binary trees and ordered trees because they have the property
that the standard computer representation uniquely determines the tree, and
in rooted trees because they are the quintessential recursive structure. In
this book, we consider all these types of trees not only because they all arise
in important computer algorithms, but also because their analysis illustrates
nearly the full range of analytic techniques that we present. But we maintain
our algorithmic bias, by reserving the word tree for ordered trees, which arise
in perhaps the most natural way in computer applications. Combinatorics
texts more typically reserve the unmodiŀed “tree” to describe unordered or
free trees.
Exercise 6.57 Which free tree structure on six nodes appears most frequently among
all ordered trees on six nodes? (Figure 6.22 shows that the answer for ŀve nodes is
the tree in the middle.)
Exercise 6.58 Answer the previous exercise for seven, eight, and more nodes, going
as high as you can.
In binary tree search and other algorithms that use binary trees, we
directly represent the ordered pair of subtrees with an ordered pair of links to
subtrees. Similarly, a typical way to represent the subtrees in general Catalan
trees is an ordered list of links to subtrees. Ļere is a one-to-one correspon-
dence between the trees and their computer representation. Indeed, in §6.3,

Țșț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȚ
we considered a number of diﬀerent ways to represent trees and binary trees
in an unambiguous manner. Ļe situation is diﬀerent when it comes to rep-
resenting rooted trees and free trees, where we are faced with several ways to
represent the same tree. Ļis has many implications in algorithm design and
analysis. A typical example is the “tree isomorphism” problem: given two dif-
ferent tree representations, determine whether they represent the same rooted
tree, or the same free tree. Not only are no eﬃcient algorithms known for this
problem, but also it is one of the few problems whose diﬃculty remains un-
classiŀed (see [16]).
Ļe ŀrst challenge in analyzing algorithms that use trees is to ŀnd a
probabilistic model that realistically approximates the situation at hand. Are
the trees random? Is the tree distribution induced by some external random-
ness? How does the tree representation aﬀect the algorithm and analysis?
Ļese lead to a host of analytic problems. For example, the “union-ŀnd”
problem mentioned earlier has been analyzed using a number of diﬀerent
models (see Knuth and Schönhage [26]). We can assume that the sequence
of equivalence relations consists of random node pairs, or that they corre-
spond to random edges from a random forest, and so on. As we have seen
with binary search trees and binary Catalan trees, the fundamental recursive
decomposition leads to similarities in the analysis, but the induced distribu-
tions lead to signiŀcant diﬀerences in the analysis.
For various applications we may be interested in values of parameters
that measure fundamental characteristics of the various types of trees, so we
are faced with a host of analytic problems to consider. Ļe enumeration results
are classical (see [21], [30], and [24]), and are summarized in Table 6.3. Ļe
derivation of some of these results is discussed next. Functional equations on
the generating functions are easily available through the symbolic method,
but asymptotic estimates of coeﬃcients are slightly beyond the scope of this
book, in some cases. More details may be found in [15].
Exercise 6.59 Give an eﬃcient algorithm that takes as input a set of edges that rep-
resents a tree and produces as output a parenthesis system representation of that tree.
Exercise 6.60 Give an eﬃcient algorithm that takes as input a set of edges that rep-
resents a tree and produces as output a binary tree representation of that tree.
Exercise 6.61 Give an eﬃcient algorithm that takes as input two binary trees and
determines whether they are diﬀerent when considered as unordered trees.

§ȝ.ȘȚ
T Ş ő ő ş
ȚșȜ
Exercise 6.62 [cf. Aho, Hopcroft, and Ullman] Give an eﬃcient algorithm that
takes as input two parenthesis systems and determines whether they represent the
same rooted tree.
Counting rooted unordered trees. Ļis sequence is OEIS A000081 [34].
Let U be the set of all rooted (unordered) trees with associated OGF
U(z) =
∑
u∈U
z|u| =
∑
N≥0
UNzN,
where UN is the number of rooted trees with N nodes. Since each rooted
tree comprises a root and a multiset of rooted trees, we can also express this
generating function as an inŀnite product, in two ways:
U(z) = z
∏
u∈U
(1 −z|u|)−1 = z
∏
N
(1 −zN)−UN.
Ļe ŀrst product is an application of the “multiset” construction associated
with the symbolic method (see Exercise 5.6): for each tree u, the term (1 −
z|u|)−1 allows for the presence of an arbitrary number of occurrences of u in
the set. Ļe second product follows by grouping the UN terms corresponding
to the trees with N nodes.
Ļeorem 6.12 (Enumeration of rooted unordered trees).
Ļe OGF that
enumerates unordered trees satisŀes the functional equation
U(z) = z exp
{
U(z) + 1
2U(z2) + 1
3U(z3) + . . .
}
.
Asymptotically,
UN ≡[zN]U(z) ∼cαN/N3/2
where c ≈0.4399237 and α ≈2.9557649.
Proof. Continuing the discussion above, take the logarithm of both sides:
lnU(z)
z
= −
∑
N≥1
UNln(1 −zN)
=
∑
N≥1
UN(zN + 1
2z2N + 1
3z3N + 1
4z4N + . . .)
= U(z) + 1
2U(z2) + 1
3U(z3) + 1
4U(z4) + . . . .

Țșȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȚ
Ļe stated functional equation follows by exponentiating both sides.
Ļe asymptotic analysis is beyond the scope of this book. It depends on
complex analysis methods related to the direct generating function asymp-
totics that we introduced in Chapter 4. Details may be found in [15], [21],
and [30].
Ļis result tells us several interesting lessons. First, the OGF admits no
explicit form in terms of elementary functions of analysis. However, it is fully
determined by the functional equation. Indeed, the same reasoning shows
that the OGF of trees of height ≤h satisŀes
U[0](z) = z;
U[h+1](z) = z exp
(
U[h](z)+ 1
2U[h](z2)+ 1
3U[h](z3)+. . .
)
.
Moreover, U[h](z) →U(z) as h →∞, and both series agree to h + 1 terms.
Ļis provides a way to compute an arbitrary number of initial values:
U(z) = z + z2 + 2z3 + 4z4 + 9z5 + 20z6 + 48z7 + 115z8 + 286z9 + . . . .
It is also noteworthy that a precise asymptotic analysis can be eﬀected even
though the OGF admits no closed form. Actually, this analysis is the histor-
ical source of the so-called Darboux-Polya method of asymptotic enumera-
tion, which we introduced brieły in §5.5. Polya in 1937 realized in this way
the asymptotic analysis of a large variety of tree types (see Polya and Read
[30], a translation of Polya’s classic paper), especially models of chemical iso-
mers of hydrocarbons, alcohols, and so forth.
Exercise 6.63 Write a program to compute all the values of UN that are smaller than
the maximum representable integer in your machine, using the method suggested in
the text. Estimate how many (unlimited precision) arithmetic operations would be
required for large N, using this method.
Exercise 6.64 [cf. Harary-Palmer] Show that
NUN+1 =
∑
1≤k≤N
(
kUk
∑
k≤kl≤N
TN+1−kl
)
and deduce that UN can be determined in O
(
N 2)
arithmetic operations. (Hint :
Diﬀerentiate the functional equation.)
Exercise 6.65 Give a polynomial-time algorithm to generate a random rooted tree
of size N.

§ȝ.Șț
T Ş ő ő ş
ȚșȞ
Counting free trees. Ļis sequence is OEIS A000055 [34]. Without a root
to ŀx upon, the combinatorial argument is more sophisticated, though it has
been known at least since 1889 (see Harary and Palmer [21]). Ļe asymptotic
estimate follows via a generating function argument, using the asymptotic
formula for rooted trees just derived. We leave details for exercises.
Exercise 6.66 Show that the number of rooted trees of N nodes is bounded below
by the number of free trees of N nodes and bounded above by N times the number
of free trees of N nodes. (Ļus, the exponential order of growth of the two quantities
is the same.)
Exercise 6.67 Let F(z) be the OGF for free trees. Show that
F(z) = U(z) −1
2U(z)2 + 1
2U(z2).
Exercise 6.68 Derive the asymptotic formula for free trees given in Table 6.3, using
the formula given in Ļeorem 6.12 for rooted (unordered) trees and the previous
exercise.
6.14 Labelled Trees. Ļe counting results above assume that the nodes in
the trees are indistinguishable. If, on the contrary, we assume that the nodes
have distinct identities, then there are many more ways to organize them into
trees. For example, diﬀerent trees result when diﬀerent nodes are used for the
root. As mentioned earlier, the number of “diﬀerent” trees increases when
we specify a root and when we consider the order of subtrees signiŀcant. As
in Chapter 5, we are using “labels” as a combinatorial device to distinguish
nodes. Ļis, of course, has nothing to do with keys in binary search trees,
which are application data associated with nodes.
Ļe diﬀerent types of labelled trees are illustrated in Figure 6.23, which
corresponds to Figure 6.22. Ļe trees at the bottom level are diﬀerent rooted,
ordered, and labelled trees; those in the middle level are diﬀerent unordered
labelled trees; and those at the top level are diﬀerent unrooted, unordered
labelled trees. As usual, we are interested in knowing how many labelled
trees there are, of each of the types that we have considered. Table 6.4 gives
small values and asymptotic estimates for the counts of the various labelled
trees. Ļe second column (N = 3) in Table 6.4 corresponds to the trees in
Figure 6.23.

Țșȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șț
rooted
ordered
trees
rooted
trees
free
trees
2
1
3
3
1
2
2
1
3
2
1
3
3
1
2
1
2
3
1
3
2
1
3
2
1
2
3
1
2
3
3
2
1
2
1
3
1
3
2
1
3
2
2
3
1
3
1
2
1
2
3
3
2
1
2
1
3
2
3
1
1
3
2
2
3
1
3
1
2
3
2
1
Figure 6.23 Labelled trees with three nodes
(ordered, unordered, and unrooted)
As discussed in Chapter 5, EGFs are the appropriate tool for approach-
ing the enumeration of labelled trees, not just because there are so many more
possibilities, but also because the basic combinatorial manipulations that we
use on labelled structures are naturally understood through EGFs.
Exercise 6.69 Which tree of four nodes has the most diﬀerent labellings? Answer
this question for ŀve, six, and more nodes, going as high as you can.
Counting ordered labelled trees. An unlabelled tree is uniquely determined
by a preorder traversal, and any of the N! permutations can be used with the
preorder traversal to assign labels to an ordered tree with N nodes, so the
number of labelled trees is just N! times the number of unlabelled trees. Such
2
3
4
5
6
7
N
ordered
2 12
120
1680
46656
665280
(2N −2)!
(N −1)!
rooted
2
9
64
625
7976
117649
N N−1
free
1
3
16
125
1296
16807
N N−2
Table 6.4
Enumeration of labelled trees

§ȝ.Șț
T Ş ő ő ş
ȚșȠ
an argument is clearly general. For ordered trees, the labelled and unlabelled
varieties are closely related and their counts diﬀer only by a factor of N!. Ļese
simple combinatorial arguments are appealing and instructive, but it is also
instructive to use the symbolic method.
Ļeorem 6.13 (Enumeration of ordered labelled trees).
Ļe number of or-
dered rooted labelled trees with N nodes is (2N −2)!/(N −1)!.
Proof. An ordered labelled forest is either empty or a sequence of ordered
labelled trees, so we have the combinatorial construction
L = Z × SEQ (L)
and by the symbolic method we have
L(z) =
z
1 −L(z).
Ļis is virtually the same argument as that used previously for ordered (un-
labelled) trees, but we are now working with EGFs (Ļeorem 5.2) for labelled
objects, where before we were using OGFs for unlabelled objects (Ļeorem 5.1).
Ļus L(z) = (1 −√1 −4z )/2 and the number of ordered rooted labelled
trees with N nodes is given by
N![zN]L(z) = N! 1
N
(
2N −2
N −1
)
= (2N −2)!
(N −1)! .
Ļis sequence is OEIS A001813 [34].
Counting unordered labelled trees. Unordered (rooted) labelled trees are
also called Cayley trees, because they were enumerated by A. Cayley in the
19th century. A Cayley forest is either empty or a set of Cayley trees, so we
have the combinatorial construction
C = Z × SET (C)
and by the symbolic method we have
C(z) = zeC(z).

ȚȚȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.Șț
Ļeorem 6.14 (Enumeration of unordered labelled trees).
Ļe EGF that
enumerates unordered labelled trees satisŀes the functional equation
C(z) = zeC(z).
Ļe number of such trees of size N is
CN = N![zN]C(z) = NN−1
and the number of unordered k-forests of such trees is
C[k]
N = N![zN](C(z))k
k!
=
(
N −1
k −1
)
NN−k.
Proof. Following the derivation of the EGF via the symbolic method, La-
grange inversion (see §6.12) immediately yields the stated results. Ļis se-
quence is OEIS A000169 [34].
2
1
3
3
1
2
1
2
3
3
2
1
1
3
2
2
3
1
shape
L3 = 9
L4 = 64
6
3
1
2
3
2
1
3
3
1
2
1
2
2
1
shape
ways to
label
ways to
label
ways to
label
L2 = 2
2
1
shape
L1 = 1
1
shape
24
24
4
12
ways to
label
Figure 6.24 Cayley (labelled rooted unordered) trees, 1 ≤N ≤4.

§ȝ.ȘȜ
T Ş ő ő ş
ȚȚȘ
Combinatorial proof ? As illustrated in Figure 6.24, Cayley tree enumeration
is a bit magical. Adding the ways to label each unordered labelled tree shape
(each count needing a separate argument) gives a very simple expression. Is
there a simple combinatorial proof? Ļe answer to this question is a classic
exercise in elementary combinatorics: Devise a 1:1 correspondence between
N-node Cayley trees and sequences of N −1 integers, all between 1 and N.
Readers are encouraged to think about this problem before ŀnding a solution
in a combinatorics text (or [15] or [24]). Such constructions are interest-
ing and appealing, but they perhaps underscore the importance of general
approaches that can solve a broad variety of problems, such as the symbolic
method and Lagrange inversion.
For reference, the enumeration generating functions for both unlabelled
trees and labelled trees are given in Table 6.5. Ļe values of the coeﬃcients
for labelled trees are given in Table 6.4.
Exercise 6.70 What is the number of labelled rooted forests of N nodes?
Exercise 6.71 Show that the EGF that enumerates labelled free trees is equal to
C(z) −C(z)2/2.
6.15 Other Types of Trees. It is often convenient to place various local
and global restrictions on trees—for example, to suit requirements of a partic-
ular application or to try to rule out degenerate cases. From a combinatorial
standpoint, any restriction corresponds to a new class of tree, and a new col-
lection of problems need to be solved to enumerate the trees and to learn their
unlabelled (OGF)
labelled (EGF)
ordered
G(z) =
z
1 −G(z)
L(z) =
z
1 −L(z)
rooted
U(z) = z exp{
∑
i≥1
U(zi)/i}
C(z) = zeC(z)
free
U(z) −U(z)2/2 + U(z2)/2
C(z) −C(z)2/2
Table 6.5
Tree enumeration generating functions

ȚȚș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȜ
Figure 6.25
3-ary
4-ary
3-restricted
4-restricted
2-3
2-3-4
red-black
AVL
Examples of various other types of trees

§ȝ.ȘȜ
T Ş ő ő ş
ȚȚȚ
statistical properties. In this section, we catalog many well-known and widely
used special types of trees, for reference. Examples are drawn in Figure 6.25,
and deŀnitions are given in the discussion below. (Note on nomenclature: In
this section, we use T(z) to denote the OGF for various generalizations of
the Catalan OGF to emphasize similarities in the analysis, while sparing the
reader from excessive notational baggage.)
Deŀnition A t-ary tree is either an external node or an internal node attached
to an ordered sequence of t subtrees, all of which are t-ary trees.
Ļis is the natural generalization of binary trees that we considered as an
example when looking at Lagrange inversion in §6.12. We insist that every
node have exactly t descendants. Ļese trees are normally considered to be
ordered—this matches a computer representation where t links are reserved
for each node, to point to its descendants. In some applications, keys might
be associated with internal nodes; in other cases, internal nodes might corre-
spond to sequences of t −1 keys; in still other cases data might be associated
with external nodes. One important tree of this type is the quad tree, where
information about geometric data is organized by decomposing an area into
four quadrants, proceeding recursively.
Ļeorem 6.15 (Enumeration of t-ary trees).
Ļe OGF that enumerates
t-ary trees (by external nodes) satisŀes the functional equation
T(z) = z + (T(z))t.
Ļe number of t-ary trees with N internal nodes and (t −1)N + 1 external
nodes is
1
(t −1)N + 1
(
tN
N
)
∼ct(αt)N/N3/2
where αt = tt/(t −1)t−1 and ct = 1/
√
(2π)(t −1)3/t.
Proof. We use Lagrange inversion, in a similar manner as for the solution
given in §6.12 for the case t = 3. By the symbolic method, the OGF with
size measured by external nodes satisŀes
T(z) = z + T(z)3.

ȚȚț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȜ
Ļis can be subjected to the Lagrange theorem, since z = T(z)(1 −T(z)2),
so we have an expression for the number of trees with 2N + 1 external nodes
(N internal nodes):
[z2N+1]T(z) =
1
2N + 1[u2N]
1
(1 −u2)2N+1
=
1
2N + 1[uN]
1
(1 −u)2N+1
=
1
2N + 1
(
3N
N
)
.
Ļis is equivalent to the expression given in §6.12, and it generalizes imme-
diately to give the stated result. Ļe asymptotic estimate also follows directly
when we use the same methods as for the Catalan numbers (see §4.3).
Exercise 6.72 Find the number of k-forests with a total of N internal nodes.
Exercise 6.73 Derive the asymptotic estimate given in Ļeorem 6.15 for the number
of t-ary trees with N internal nodes.
Deŀnition A t-restricted tree is a node (called the root) containing links to t
or fewer t-restricted trees.
Ļe diﬀerence between t-restricted trees and t-ary trees is that not every
internal node must have t links. Ļis has direct implications in the computer
representation: for t-ary trees, we might as well reserve space for t links in
all internal nodes, but t-restricted trees might be better represented as binary
trees, using the standard correspondence. Again, we normally consider these
to be ordered, though we might also consider unordered and/or unrooted
t-restricted trees. Every node is linked to at most t + 1 other nodes in a
t-restricted tree, as shown in Figure 6.25.
Ļe case t = 2 corresponds to the so-called Motzkin numbers, for which
we can get an explicit expression for the OGF M(z) by solving the quadratic
equation. We have
M(z) = z(1 + M(z) + M(z)2)
so that
M(z) = 1 −z −
√
1 −2z −3z2
2z
= 1 −z −
√
(1 + z)(1 −3z)
2z
.

§ȝ.ȘȜ
T Ş ő ő ş
ȚȚȜ
Now, Ļeorem 4.11 provides an immediate proof that [zN]M(z) is O(3N),
and methods from complex asymptotics yield the more accurate asymptotic
estimate 3N/
√
3/4πN 3. Actually, with about the same amount of work, we
can derive a much more general result.
Ļeorem6.16 (Enumeration of t-restricted trees).
Let θ(u) = 1+u+u2+
. . . + ut. Ļe OGF that enumerates t-restricted trees satisŀes the functional
equation
T(z) = zθ(T(z))
and the number of t-restricted trees is
[zN]T(z) = 1
N [uN−1](θ(u))N ∼ctαN
t /N3/2
where τ is the smallest positive root of θ(τ) −τθ′(τ) = 0 and the constants
αt and ct are given by αt = θ′(τ) and ct =
√θ(τ)/2πθ′′(τ).
Proof. Ļe ŀrst parts of the theorem are immediate from the symbolic method
and Lagrange inversion. Ļe asymptotic result requires singularity analysis,
using an extension of Ļeorem 4.12 (see [15]).
Ļis result follows from a theorem proved by Meir and Moon in 1978
[28], and it actually holds for a large class of polynomials θ(u) of the form
1+a1u+a2u2+. . . , subject to the constraint that the coeﬃcients are positive
and that a1 and at least one other coeﬃcient are nonzero.
Ļe asymptotic estimates of the number of t-restricted trees for small t
are given in the following table:
t
ct
αt
2
.4886025119
3.0
3
.2520904538
3.610718613
ctαN
t /N3/2
4
.1932828341
3.834437249
5
.1691882413
3.925387252
6
.1571440515
3.965092635
∞
.1410473965
4.0
For large t, the values of αt approaches 4, which is perhaps to be expected,
since the trees are then like general Catalan trees.

ȚȚȝ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȜ
Exercise 6.74 Use the identity 1 + u + u2 + . . . + ut = (1 −ut+1)/(1 −u) to ŀnd
a sum expression for the number of t-restricted trees with N nodes.
Exercise 6.75 Write a program that, given t, will compute the number of t-restricted
trees for all values of N for which the number is smaller than the maximum repre-
sentable integer in your machine.
Exercise 6.76 Find the number of “even” t-restricted trees, where all nodes have an
even number of, and less than t, children.
Height-restricted trees. Other types of trees involve restrictions on height.
Such trees are important because they can be used as binary search tree re-
placements that provide a guaranteed O(logN) search time. Ļis was ŀrst
shown in 1960 by Adel’son-Vel’skii and Landis [1], and such trees have been
widely studied since (for example, see Bayer and McCreight [3] or Guibas
and Sedgewick [20]). Balanced trees are of practical interest because they
combine the simplicity and łexibility of binary tree search and insertion with
good worst-case performance. Ļey are often used for very large database
applications, so asymptotic results on performance are of direct practical in-
terest.
Deŀnition An AVL tree of height 0 or height −1 is an external node; an AVL
tree of height h > 0 is an internal node linked to a left and a right subtree,
both of height h −1 or h −2.
Deŀnition A B-tree of height 0 is an external node; a B-tree of order M and
height h > 0 is an internal node connected to a sequence of between ⌈M/2⌉
and M B-trees of order M and height h −1.
B-trees of order 3 and 4 are normally called 2-3 trees and 2-3-4 trees,
respectively. Several methods, known as balanced tree algorithms, have been
devised using these and similar structures, based on the general theme of map-
ping permutations into tree structures that are guaranteed to have no long
paths. More details, including relationships among the various types, are
given by Guibas and Sedgewick [20], who also show that many of the struc-
tures (including AVL trees and B-trees) can be mapped into binary trees with
marked edges, as in Figure 6.25.
Exercise 6.77 Without solving the enumeration problem in detail, try to place the
following classes of trees in increasing order of their cardinality for large N: 3-ary,
3-restricted, 2-3, and AVL.

§ȝ.ȘȜ
T Ş ő ő ş
ȚȚȞ
Exercise 6.78 Build a table giving the number of AVL and 2-3 trees with fewer than
15 nodes that are diﬀerent when considered as unordered trees.
Balanced tree structures illustrate the variety of tree structures that arise
in applications. Ļey lead to a host of analytic problems of interest, and they
fall at various points along the continuum between purely combinatoric struc-
tures and purely “algorithmic” structures. None of the binary tree structures
has been precisely analyzed under random insertions for statistics such as path
length, despite their importance. It is even challenging to enumerate them
(for example, see Aho and Sloane [2] or Flajolet and Odlyzko [13]).
For each of these types of structures, we are interested in knowing how
many essentially diﬀerent structures there are of each size, plus statistics about
various important parameters. For some of the structures, developing func-
tional equations for enumeration is relatively straightforward, because they
are recursively deŀned. (Some of the balanced tree structures cannot even be
easily deŀned and analyzed recursively, but rather need to be deŀned in terms
of the algorithm that maps permutations into them.) As with tree height,
the functional equation is only a starting point, and further analysis of these
structures turns out to be quite diﬃcult. Functional equations for generating
functions for several of the types we have discussed are given in Table 6.6.
Exercise 6.79 Prove the functional equations on the generating functions for AVL
and 2-3 trees given in Table 6.6.
More important, just as we analyzed both binary trees (uniformly dis-
tributed) and binary search trees (binary trees distributed as constructed from
random permutations by the algorithm), we often need to know statistics on
various classes of trees according to a distribution induced by an algorithm
that transforms some other combinatorial object into a tree structure, which
leads to more analytic problems. Ļat is, several of the basic tree structures
that we have deŀned serve many algorithms. We used the term binary search
tree to distinguish the combinatorial object (the binary tree) from the algo-
rithm that maps permutations into it; balanced tree and other algorithms need
to be distinguished in a similar manner.
Indeed, AVL trees, B-trees, and other types of search trees are primar-
ily of interest when distributed as constructed from random permutations.
Ļe “each tree equally likely” combinatorial objects have been studied both
because the associated problems are more amenable to combinatorial analysis
and because knowledge of their properties may give some insight into solv-

ȚȚȟ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
§ȝ.ȘȜ
ing problems that arise when analyzing them as data structures. Even so, the
basic problem of just enumerating the balanced tree structures is still quite
diﬃcult (for example, see [29]). None of the associated algorithms has been
analyzed under the random permutation model, and the average-case analysis
of balanced tree algorithms is one of the outstanding problems in the analysis
of algorithms.
Figure 6.26 gives some indication of the complexity of the situation.
It shows the distribution of the subtree sizes in random AVL trees (all trees
equally likely) and may be compared with Figure 6.10, the corresponding
ŀgure for Catalan trees. Ļe corresponding ŀgure for BSTs is a series of
straight lines, at height 1/N.
Where Catalan trees have an asymptotically constant probability of hav-
ing a ŀxed number of nodes in a subtree for any tree size N, the balance
condition for AVL trees means that small subtrees cannot occur for large N.
Indeed, we might expect the trees to be “balanced” in the sense that the sub-
tree sizes might cluster near the middle for large N. Ļis does seem to be
tree type
functional equation on generating function
(size measure)
from symbolic method
3-ary
(external nodes)
T(z) = z + T(z)3
3-ary
(internal nodes)
T(z) = 1 + zT(z)3
3-restricted
(nodes)
T(z) = z(1 + T(z) + T(z)2 + T(z)3)
AVL of height h
(internal nodes)
Ah(z) =



1
h < 0
1
h = 0
zAh−1(z)2 + 2zAh−1(z)Ah−2(z)
h > 0
2-3 of height h
(external nodes)
Bh(z) =
{ z
h = 0
Bh−1(z2 + z3)
h > 0
Table 6.6
Generating functions for other types of trees

§ȝ.ȘȜ
T Ş ő ő ş
ȚȚȠ
Figure 6.26 AVL distribution (subtree sizes in random AVL trees)
(scaled and translated to separate curves)

Țțȗ
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
the case for some N, but it also is true that for some other N, there are two
peaks in the distribution, which means that a large fraction of the trees have
signiŀcantly fewer than half of the nodes on one side or the other. Indeed,
the distribution exhibits an oscillatory behavior, roughly between these two
extremes. An analytic expression describing this has to account for this os-
cillation and so may not be as concise as we would like. Presumably, sim-
ilar eﬀects are involved when balanced trees are built from permutations in
searching applications, but this has not yet been shown.
T
REES are pervasive in the algorithms we consider, either as explicit
structures or as models of recursive computations. Much of our knowl-
edge of properties of our most important algorithms can be traced to proper-
ties of trees.
We will encounter other types of trees in later chapters, but they all share
an intrinsic recursive nature that makes their analysis natural using generating
functions as just described: the recursive structure leads directly to an equa-
tion that yields a closed-form expression or a recursive formulation for the
generating function. Ļe second part of the analysis, extracting the desired
coeﬃcients, requires advanced techniques for some types of trees.
Ļe distinction exhibited by comparing the analysis of tree path length
with tree height is essential. Generally, we can describe combinatorial para-
meters recursively, but “additive” parameters such as path length are much
simpler to handle than “nonadditive” parameters such as height, because gen-
erating function constructions that correspond to combinatorial constructions
can be exploited directly in the former case.
Our ŀrst theme in this chapter has been to introduce the history of the
analysis of trees as combinatorial objects. In recent years, general techniques
have been found that help to unify some of the classical results and make it
possible to learn characteristics of ever more complicated new tree structures.
We discuss this theme in detail and cover many examples in [15], and Dr-
mota’s book [11] is a thorough treatment that describes the extensive amount
of knowledge about random trees that has been developed in the years since
the early breakthroughs that we have described here.
Beyond classical combinatorics and speciŀc applications in algorithmic
analysis, we have endeavored to show how algorithmic applications lead to
a host of new mathematical problems that have an interesting and intricate
structure in their own right. Ļe binary search tree algorithm is prototypical

T Ş ő ő ş
ȚțȘ
of many of the problems that we know how to solve: an algorithm transforms
some input combinatorial object (permutations, in the case of binary search
trees) into some form of tree. Ļen we are interested in analyzing the com-
binatorial properties of trees, not under the uniform model, but under the
distribution induced by the transformation. Knowing detailed properties of
the combinatorial structures that arise and studying eﬀects of such transfor-
mations are the bases for our approach to the analysis of algorithms.
construction
GF
equation
approximate
asymptotics
Unlabelled classes
binary
trees
T = Z + T 2
T(z) = z + T 2(z)
.56 4N
N 3/2
3-ary
trees
T = Z + T 3
T(z) = z + T 3(z)
.246.75N
N 3/2
trees
G = Z × SEQ (G)
G(z) =
z
1 −G(z)
.14 4N
N 3/2
unordered
trees
U = Z × MSET (U)
U(z) = zeU(z)+U(z)2/2+...
.542.96N
N 3/2
Motzkin
trees
T = Z × (E + T + T 2)
T(z) = z(1 + T(z) + T(z)2) .49 3N
N 3/2
Labelled classes
trees
L = Z × SEQ (L)
L(z) =
z
1 −L(z)
(2N −2)!
(N −1)!
Cayley
trees
C = Z × SET (C)
C(z) = zeC(z)
N N−1
Table 6.7
Analytic combinatorics examples in this chapter

Țțș
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
References
1. G. AŐőŘ’şśŚ-VőŘ’şŗŕŕ ōŚŐ E. LōŚŐŕş. Doklady Akademii Nauk SSR
146, 1962, 263–266. English translation in Soviet Math 3.
2. A. V. AŔś ōŚŐ N J. A. SŘśōŚő. “Some doubly exponential sequences,”
Fibonacci Quarterly 11, 1973, 429–437.
3. R. BōťőŞ ōŚŐ E. MŏCŞőŕœŔŠ. “Organization and maintenance of large
ordered indexes,” Acta Informatica 3, 1972, 173–189.
4. J. BőŚŠŘőť. “Multidimensional binary search trees used for associative
searching,” Communications of the ACM 18, 1975, 509–517.
5. B. BśŘŘśŎŭş. Random Graphs, Academic Press, London, 1985.
6. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
7. T. H. CśŞřőŚ, C. E. LőŕşőŞşśŚ, R. L. RŕŢőşŠ, ōŚŐ C. SŠőŕŚ. Intro-
duction to Algorithms, MIT Press, New York, 3rd edition, 2009.
8. N. G. Dő BŞšŕŖŚ, D. E. KŚšŠŔ, ōŚŐ S. O. Rŕŏő. “Ļe average height
of planted plane trees,” in Graph Ļeory and Computing, R. C. Read, ed.,
Academic Press, New York, 1971.
9. L. DőŢŞśťő. “A note on the expected height of binary search trees,”
Journal of the ACM 33, 1986, 489–498.
10. L. DőŢŞśťő. “Branching processes in the analysis of heights of trees,”
Acta Informatica 24, 1987, 279–298.
11. M. DŞřśŠō. Random Trees: An Interplay Between Combinatorics and
Probability, Springer Wein, New York, 2009.
12. P. FŘōŖśŘőŠ ōŚŐ A. OŐŘťŦŗś. “Ļe average height of binary trees and
other simple trees,” Journal of Computer and System Sciences 25, 1982,
171–213.
13. P. FŘōŖśŘőŠ ōŚŐ A. OŐŘťŦŗś. “Limit distributions for coeﬃcients of
iterates of polynomials with applications to combinatorial enumerations,”
Mathematical Proceedings of the Cambridge Philosophical Society 96, 1984,
237–253.
14. P. FŘōŖśŘőŠ, J.-C. RōśšŘŠ, ōŚŐ J. VšŕŘŘőřŕŚ. “Ļe number of registers
required to evaluate arithmetic expressions,” Ļeoretical Computer Science
9, 1979, 99–125.
15. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.

T Ş ő ő ş
ȚțȚ
16. M. R. GōŞőť ōŚŐ D. S. JśŔŚşśŚ. Computers and Intractability: A Guide
to the Ļeory of NP-Completeness, W. H. Freeman, New York, 1979.
17. G. H. GśŚŚőŠ ōŚŐ R. BōőŦō-YōŠőş. Handbook of Algorithms and Data
Structures in Pascal and C, 2nd edition, Addison-Wesley, Reading, MA,
1991.
18. I. GśšŘŐőŚ ōŚŐ D. JōŏŗşśŚ. Combinatorial Enumeration, John Wiley,
New York, 1983.
19. R. L. GŞōŔōř, D. E. KŚšŠŔ, ōŚŐ O. PōŠōşŔŚŕŗ. Concrete Mathemat-
ics, 1st edition, Addison-Wesley, Reading, MA, 1989. Second edition,
1994.
20. L. GšŕŎōş ōŚŐ R. SőŐœőţŕŏŗ. “A dichromatic framework for balanced
trees,” in Proceedings 19th Annual IEEE Symposium on Foundations of
Computer Science, 1978, 8–21.
21. F. HōŞōŞť ōŚŐ E. M. PōŘřőŞ. Graphical Enumeration, Academic Press,
New York, 1973.
22. C. A. R. HśōŞő. “Quicksort,” Computer Journal 5, 1962, 10–15.
23. R. KőřŜ. “Ļe average number of registers needed to evaluate a binary
tree optimally,” Acta Informatica 11, 1979, 363–372.
24. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 1: Fundamen-
tal Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1968. Ļird
edition, 1997.
25. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.
26. D. E. KŚšŠŔ ōŚŐ A. SŏŔƙŚŔōœő. “Ļe expected linearity of a simple
equivalence algorithm,” Ļeoretical Computer Science 6, 1978, 281–315.
27. H. MōŔřśšŐ. Evolution of Random Search Trees, John Wiley, New
York, 1992.
28. A. MőŕŞ ōŚŐ J. W. MśśŚ. “On the altitude of nodes in random trees,”
Canadian Journal of Mathematics 30, 1978, 997–1015.
29. A. M. OŐŘťŦŗś. “Periodic oscillations of coeﬃcients of power series
that satisfy functional equations,” Advances in Mathematics 44, 1982,
180–205.

Țțț
C Ŕ ō Ŝ Š ő Ş
S ŕ Ť
30. G. PƖŘťō ōŚŐ R. C. RőōŐ. Combinatorial Enumeration of Groups, Graphs,
and Chemical Compounds, Springer-Verlag, New York, 1987. (English
translation of original paper in Acta Mathematica 68, 1937, 145–254.)
31. R. C. RőōŐ. “Ļe coding of various kinds of unlabelled trees,” in Graph
Ļeory and Computing, R. C. Read, ed., Academic Press, New York,
1971.
32. R. SőŐœőţŕŏŗ. Algorithms, 2nd edition, Addison-Wesley, Reading, MA,
1988.
33. R. SőŐœőţŕŏŗ ōŚŐ K. WōťŚő. Algorithms, 4th edition, Addison-Wesley,
Boston, 2011.
34. N. SŘśōŚő ōŚŐ S. PŘśšŒŒő. Ļe Encyclopedia of Integer Sequences, Aca-
demic Press, San Diego, 1995. Also accessible as On-Line Encyclopedia
of Integer Sequences, http://oeis.org.
35. J. S. VŕŠŠőŞ ōŚŐ P. FŘōŖśŘőŠ, “Analysis of algorithms and data struc-
tures,” in Handbook of Ļeoretical Computer Science A: Algorithms and Com-
plexity, J. van Leeuwen, ed., Elsevier, Amsterdam, 1990, 431–524.

C H A P T E R S E V E N
P E R M U T A T I O N S
C
OMBINATORIAL algorithms often deal only with the relative order
of a sequence of N elements; thus we can view them as operating on the
numbers 1 through N in some order. Such an ordering is called a permutation,
a familiar combinatorial object with a wealth of interesting properties. We
have already encountered permutations: in Chapter 1, where we discussed the
analysis of two important comparison-based sorting algorithms using random
permutations as an input model; and in Chapter 5, where they played a fun-
damental role when we introduced the symbolic method for labelled objects.
In this chapter, we survey combinatorial properties of permutations and use
probability, cumulative, and bivariate generating functions (and the symbolic
method) to analyze properties of random permutations.
From the standpoint of the analysis of algorithms, permutations are of
interest because they are a suitable model for studying sorting algorithms. In
this chapter, we cover the analysis of basic sorting methods such as insertion
sort, selection sort, and bubble sort, and discuss several other algorithms that
are of importance in practice, including shellsort, priority queue algorithms,
and rearrangement algorithms. Ļe correspondence between these methods
and basic properties of permutations is perhaps to be expected, but it un-
derscores the importance of fundamental combinatorial mechanisms in the
analysis of algorithms.
We begin the chapter by introducing several of the most important
properties of permutations and considering some examples as well as some
relationships between them. We consider both properties that arise imme-
diately when analyzing basic sorting algorithms and properties that are of
independent combinatorial interest.
Following this, we consider numerous diﬀerent ways to represent per-
mutations, particularly representations implied by inversions and cycles and
a two-dimensional representation that exposes relationships between a per-
mutation and its inverse. Ļis representation also helps deŀne explicit rela-
tionships between permutations, binary search trees, and “heap-ordered trees”
ȚțȜ

Țțȝ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
and reduces the analysis of certain properties of permutations to the study of
properties of trees.
Next, we consider enumeration problems on permutations, where we
want to count permutations having certain properties, which is equivalent
to computing the probability that a random permutation has the property.
We attack such problems using generating functions (including the symbolic
method on labelled objects). Speciŀcally, we consider properties related to the
“cycle structure” of the permutations in some detail, extending the analysis
based on the symbolic method that we began in Chapter 5.
Following the same general structure as we did for trees in Chapter 6,
we proceed next to analysis of parameters. For trees, we considered path
length, height, number of leaves and other parameters. For permutations,
we consider properties such as the number of runs and the number of inver-
sions, many of which can be easily analyzed. As usual, we are interested in
the expected “cost” of permutations under various measures relating to their
properties, assuming all permutations equally likely. For such analyses, we
emphasize shortcuts based on generating functions, like the use of CGFs.
We consider the analysis of parameters in the context of two fundamen-
tal sorting methods, insertion sort and selection sort, and their relationship to
two fundamental characteristics of permutations, inversions and left-to-right
minima. We show how CGFs lead to relatively straightforward analyses of
these algorithms. We also consider the problem of permuting an array in
place and its relationship to the cycle structure of permutations. Some of
these analyses lead to familiar generating functions for special numbers from
Chapter 3—for example, Stirling and harmonic numbers.
We also consider problems analogous to height in trees in this chapter,
including the problems of ŀnding the average length of the shortest and
longest cycles in a random permutation. As with tree height, we can set up
functional equations on indexed “vertical” generating functions, but asymp-
totic estimates are best developed using more advanced tools.
Ļe study of properties of permutations illustrates that there is a ŀne
dividing line indeed between trivial and diﬃcult problems in the analysis of
algorithms. Some of the problems that we consider can be easily solved with
elementary arguments; other (similar) problems are not elementary but can
be studied with generating functions and the asymptotic methods we have
been considering; still other (still similar) problems require advanced complex
analysis or probabilistic methods.

§Ȟ.Ș
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚțȞ
7.1 Basic Properties of Permutations. Permutations may be represented
in many ways. Ļe most straightforward, introduced in Chapter 5, is simply
a rearrangement of the numbers 1 through N:
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
In §5.3, we saw that one way to think of a permutation is as a speciŀcation of
a rearrangement: “1 goes to 9, 2 goes to 14, 3 goes to 4,” and so on. In this
section, we introduce a number of basic characteristics of permutations that
not only are of inherent interest from a combinatorial standpoint, but also are
signiŀcant in the study of a number of important algorithms. We also present
some analytic results—in later sections we discuss how the results are derived
and relate them to the analysis of algorithms.
We will be studying inversions, left-to-right minima and maxima, cy-
cles, rises, runs, falls, peaks, valleys, and increasing subsequences in permu-
tations; inverses of permutations; and special types of permutations called
involutions and derangements. Ļese are all explained, in terms of a permu-
tation p1 p2 p3 . . . pN of the integers 1 to N, in the deŀnitions and text that
follow, also with reference to the sample permutation.
Deŀnition An inversion is a pair i < j with pi > pj. If qj is the num-
ber of i < j with pi > pj, then q1q2 . . . qN is called the inversion table of
p1p2 . . . pN. We use the notation inv(p) to denote the number of inversions
in a permutation p, the sum of the entries in the inversion table.
Ļe sample permutation given above has 49 inversions, as evidenced by
adding the elements in its inversion table.
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
inversion table
0
0
2
3
1
4
2
1
5
5
3
9
6
0
8
By deŀnition, the entries in the inversion table q1q2 . . . qN of a permutation
satisfy 0 ≤qj < j for all j from 1 to N. As we will see in §7.3, a unique per-
mutation can be constructed from any sequence of numbers satisfying these
constraints. Ļat is, there is a 1:1 correspondence between inversion tables
of size N and permutations of N elements (and there are N! of each). Ļat
correspondence will be exploited later in this chapter in the analysis of basic
sorting methods such as insertion sort and bubble sort.

Țțȟ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ș
Deŀnition A left-to-right minimum is an index i with pj > pi for all j < i.
We use the notation lrm(p) to refer to the number of left-to-right minima in
a permutation p.
Ļe ŀrst element in every permutation is a left-to-right minimum; so is
the smallest element. If the smallest element is the ŀrst, then it is the only
left-to-right minimum; otherwise, there are at least two (the ŀrst and the
smallest). In general, there could be as many as N left-to-right minima (in
the permutation N . . . 2 1). Ļere are three in our sample permutation, at
positions 1, 3, and 4. Note that each left-to-right minimum corresponds to
an entry qk = k −1 in the inversion table (all entries to the left are smaller),
so counting left-to-right minima in permutations is the same as counting
such entries in inversion tables. Left-to-right maxima and right-to-left min-
ima and maxima are deŀned analogously. In probability theory, left-to-right
minima are also known as records because they represent new “record” low
values that are encountered when moving from left to right through the per-
mutation.
Exercise 7.1 Explain how to compute the number of left-to-right maxima, right-to-
left minima, and right-to-left maxima from the inversion table.
Deŀnition A cycle is an index sequence i1i2 . . . it with pi1 = i2, pi2 = i3,
. . ., pit = i1. An element in a permutation of length N belongs to a unique
cycle of length from 1 to N; permutations of length N are sets of from 1 to
N cycles. A derangement is a permutation with no cycles of length 1.
We use the notation ( i1 i2 . . . it ) to specify a cycle, or simply draw a
circular directed graph, as in §5.3 and Figure 7.1. Our sample permutation is
made up of four cycles. One of the cycles is of length 1 so the permutation is
not a derangement.
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
cycles
( 1 9 5 12 3 4 )
( 2 14 15 7 10 6 )
( 8 13 )
( 11 )
Ļe cycle representation might be read as “1 goes to 9 goes to 5 goes to 12
goes to 3 goes to 4 goes to 1,” and so on. Ļe longest cycle in this permutation
is of length 6 (there are two such cycles); the shortest is of length 1. Ļere
are t equivalent ways to list any cycle of length t, and the cycles constituting
a permutation may be themselves listed in any order. In §5.3 we proved the
fundamental bijection between permutations and sets of cycles.

§Ȟ.Ș
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚțȠ
Foata’s correspondence. Figure 7.1 also illustrates that if we choose to list
the smallest element in each cycle (the cycle leaders) ŀrst and then take the
cycles in decreasing order of their leaders, then we get a canonical form that
has an interesting property: the parentheses are unnecessary, since each left-to-
right minimum in the canonical form corresponds to a new cycle (everything
in the same cycle is larger by construction). Ļis constitutes a combinato-
rial proof that the number of cycles and the number of left-to-right minima
are identically distributed for random permutations, a fact we also will ver-
ify analytically in this chapter. In combinatorics, this is known as “Foata’s
correspondence,” or the “fundamental correspondence.”
Exercise 7.2 How many diﬀerent ways are there to write the sample permutation in
cycle notation?
Exercise 7.3 How many permutations of 2N elements have exactly two cycles, each
of length N? How many have N cycles, each of length 2?
Exercise 7.4 Which permutations of N elements have the maximum number of dif-
ferent representations with cycles?
8
13
set-of-cycles
representation
Foata’s correspondence
two-line representation 1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6
7
7
7
8
8
8
9
9
9
10
10
10
11
11
11
12
12
12
13
13
13
14
14
14
15
15
15
1
9
5
12
3
4
2
14
15
7
10
6
11
left-to-right minima
Figure 7.1 Two-line, cycle, and Foata representations of a permutation

ȚȜȗ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ș
Deŀnition Ļe inverse of a permutation p1 . . . pN is the permutation q1 . . . qN
with qpi = pqi = i. An involution is a permutation that is its own inverse:
ppi = i.
For our sample permutation, the 1 is in position 4, the 2 in position 6,
the 3 in position 12, the 4 in position 3, and so forth.
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
inverse
4
6
12
3
9
10 15 13
1
7
11
5
8
2
14
By the deŀnition, every permutation has a unique inverse, and the inverse of
the inverse is the original permutation. Ļe following example of an involu-
tion and its representation in cycle form expose the important properties of
involutions.
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
involution
9
2
12
4
7
10
5
13
1
6
11
3
8
15 14
cycles
(1 9) (2) (3 12) (4) (5 7) (6 10) (8 13) (11) (14 15)
Clearly, a permutation is an involution if and only if all its cycles are of length
1 or 2. Determining a precise estimate for the number of involutions of length
N turns out to be an interesting problem that illustrates many of the tools that
we consider in this book.
Deŀnition A rise is an occurrence of pi < pi+1. A fall is an occurrence of
pi−1 > pi. A run is a maximal increasing contiguous subsequence in the
permutation. A peak is an occurrence of pi−1 < pi > pi+1. A valley is an
occurrence of pi−1 > pi < pi+1. A double rise is an occurrence of pi−1 < pi <
pi+1. A double fall is an occurrence of pi−1 > pi > pi+1. We use the notation
runs(p) to refer to the number of runs in a permutation p.
In any permutation, the number of rises plus the number of falls is the
length minus 1. Ļe number of runs is one greater than the number of falls,
since every run except the last one in a permutation must end with a fall. Ļese
facts and others are clear if we consider a representation of N −1 plus signs
and minus signs corresponding to the sign of the diﬀerence between successive
elements in the permutation; falls correspond to + and rises correspond to -.
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
rises/falls
-
+
+
-
+
-
-
+
-
-
+
-
-
+

§Ȟ.Ș
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȜȘ
Counting + and - characters, it is immediately clear that there are eight rises
and six falls. Also, the plus signs mark the ends of runs (except the last), so
there are seven runs. Double rises, valleys, peaks, and double falls correspond
to occurrences of - -, + -, - +, and + +
respectively. Ļis permutation
has three double rises, four valleys, ŀve peaks, and one double fall.
Figure 7.2 is an intuitive graphical representation that also illustrates
these quantities. When we draw a line connecting (i, pi) to (i + 1, pi+1) for
1 ≤i < N, then rises go up, falls go down, peaks point upward, valleys point
downward, and so forth. Ļe ŀgure also has an example of an “increasing
subsequence”—a dotted line that connects points on the curve and rises as it
moves from left to right.
Deŀnition An increasing subsequence in a permutation is an increasing se-
quence of indices i1, i2, . . . , ik with pi1 < pi2 < . . . < pik.
By convention, the empty subsequence is considered to be “increasing.”
For example, the increasing permutation 1 2 3 . . . N has 2N increasing
subsequences, one corresponding to every set of the indices, and the decreas-
ing permutation N N-1 N-2 . . . 1 has just N + 1 increasing subsequences.
We may account for the increasing subsequences in a permutation as we did
for inversions: we keep a table s1s2 . . . sN with si the number of increasing
increasing
subsequence
left-to-right
minimum
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
9  14   4   1   12   2  10  13   5   6  11   3   8   15   7
rise
run
fall
peak
valley
Figure 7.2 Anatomy of a permutation

ȚȜș
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ș
subsequences that begin at position i. Our sample permutation has 9 increas-
ing subsequences starting at position 1, 2 starting at position 2, and so forth.
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
subseq. table
9
2
33 72
4
34
5
2
8
7
2
5
2
1
1
For example, the ŀfth entry in this table corresponds to the four increasing
subsequences 12, 12 13, 12 15, and 12 13 15. Adding the entries in
this table (plus one for the empty subsequence) shows that the number of
increasing subsequences in our sample permutation is 188.
Exercise 7.5 Write a program that computes the number of increasing subsequences
in a given permutation in polynomial time.
Table 7.1 gives values for all of these properties for several random per-
mutations of nine elements, and Table 7.2 gives their values for all of the
permutations of four elements. Close examination of Tables 7.1 and 7.2 will
reveal characteristics of these various properties of permutations and relation-
ships among them that we will be proving in this chapter. For example, we
have already mentioned that the distribution for the number of left-to-right
maxima is the same as the distribution for the number of cycles.
Intuitively, we would expect rises and falls to be equally likely, so that
there should be about N/2 of each in a random permutation of length N.
Similarly, we would expect about half the elements to the left of each element
to be larger, so the number of inversions should be about ∑
1≤i≤N i/2, which
permutation inversions left-right
minima cycles
runs
longest
cycle
inversion
table
inverse
961534872
21
3
2
6
7
012233127 395642871
412356798
4
2
5
3
4
011100001 234156798
732586941
19
4
4
6
3
012102058 932846157
236794815
15
2
2
3
7
000003174 812693475
162783954
13
1
4
5
4
001003045 136982457
259148736
16
2
2
4
4
000321253 418529763
Table 7.1
Basic properties of some random permutations of nine elements

§Ȟ.Ș
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȜȚ
permutation
subseqs
inversions left-right
minima
cycles longest
cycle
runs inversion
table
inverse
1234
16
0
1
4
1
1
0000
1234
1243
14
1
1
3
2
2
0001
1243
1324
13
1
1
3
2
2
0010
1324
1342
10
2
1
2
3
2
0002
1423
1423
10
2
1
2
3
2
0011
1342
1432
8
3
1
3
3
3
0012
1432
2134
12
1
2
2
3
2
0100
2134
2143
10
2
2
2
2
3
0101
2143
2314
10
2
2
2
3
2
0020
3124
2341
9
3
2
1
4
2
0003
4123
2413
8
4
2
1
4
2
0013
3142
2431
7
4
2
2
3
3
0013
4132
3124
9
1
2
2
3
2
0010
2314
3142
8
3
2
1
4
3
0002
2413
3214
8
3
3
3
2
3
0120
3214
3241
7
4
3
2
3
3
0003
4213
3412
7
4
2
2
2
2
0022
3412
3421
6
5
3
1
4
3
0023
4312
4123
9
3
2
1
4
2
0111
2341
4132
8
4
2
2
3
3
0112
2431
4213
7
4
3
2
3
3
0122
3142
4231
6
5
3
3
2
3
0113
4231
4312
6
5
3
1
4
3
0122
3421
4321
5
6
4
2
2
4
0123
4321
Table 7.2
Basic properties of all permutations of four elements

ȚȜț
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ș
is about N2/4. We will see how to quantify these arguments precisely, how to
compute other moments for these quantities, and how to study left-to-right
minima and cycles using similar techniques.
Of course, if we ask more detailed questions, then we are led to more
diﬃcult analytic problems. For example, what proportion of permutations are
involutions? Derangements? How many permutations have no cycles with
more than three elements? How many have no cycles with fewer than three
elements? What is the average value of the maximum element in the inversion
table? What is the expected number of increasing subsequences in a permu-
tation? What is the average length of the longest cycle in a permutation? Ļe
longest run? Such questions arise in the study of speciŀc algorithms and have
also been addressed in the combinatorics literature.
In this chapter, we answer many of these questions. Some of the average-
case results that we will develop are summarized in Table 7.3. Some of these
analyses are quite straightforward, but others require more advanced tools, as
we will see when we consider the use of generating functions to derive these
and other results throughout this chapter. We also will consider relationships
to sorting algorithms in some detail.
exact
asymptotic
2
3
4
5
6
7
average
estimate
permutations
2
6
24
120
720
5040
1
1
inversions
1
9
72
600
5400 52,920
N(N −1)
4
∼N 2
4
left-right
minima
3
11
50
274
1764 13,068
HN
∼lnN
cycles
3
11
50
274
1764 13,068
HN
∼lnN
rises
6
36
48
300
2160 17,640
N −1
2
∼N
2
increasing
subsequences
5
27
169
1217
7939 72,871
∑
k≥0
1
k!
(N
k
)
∼
1
2√πe
e2
√
N
N 1/4
Table 7.3
Cumulative counts and averages for properties of permutations

§Ȟ.ș
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȜȜ
7.2 Algorithms on Permutations. Permutations, by their very nature,
arise directly or indirectly in the analysis of a wide variety of algorithms. Per-
mutations specify the way data objects are ordered, and many algorithms need
to process data in some speciŀed order. Typically, a complex algorithm will
invoke a sorting procedure at some stage, and the direct relationship to sort-
ing algorithms is motivation enough for studying properties of permutations
in detail. We also consider a number of related examples.
Sorting. As we saw in Chapter 1, we very often assume that the input to a
sorting method is a list of randomly ordered records with distinct keys. Keys
in random order will in particular be produced by any process that draws them
independently from an arbitrary continuous distribution. With this natural
model, the analysis of sorting methods is essentially equivalent to the analysis
of properties of permutations. Beginning with the comprehensive coverage
in Knuth [10], there is a vast literature on this topic. A broad variety of sort-
ing algorithms have been developed, appropriate for diﬀering situations, and
the analysis of algorithms has played an essential role in our understanding
of their comparative performance. For more information, see the books by
Knuth [10], Gonnet and Baeza-Yates [5], and Sedgewick and Wayne [15].
In this chapter, we will study direct connections between some of the most
basic properties of permutations and some fundamental elementary sorting
methods.
Exercise 7.6 Let a1, a2, and a3 be “random” numbers between 0 and 1 produced
independently as values of a random variable X satisfying the continuous distribution
F(x) = Pr{X ≤x}. Show that the probability of the event a1 < a2 < a3 is 1/3!.
Generalize to any ordering pattern and any number of keys.
Rearrangement. One way to think of a permutation is as a speciŀcation of
a rearrangement to be put into eﬀect. Ļis point of view leads to a direct
connection with the practice of sorting. Sorting algorithms are often imple-
mented to refer to the array being sorted indirectly: rather than moving ele-
ments around to put them in order, we compute the permutation that would
put the elements in order. Virtually any sorting algorithm can be imple-
mented in this way: for the methods we have seen, we maintain an “index”
array p[] that will contain the permutation.
For simplicity in this discussion, we maintain compatibility with our
convention for specifying permutations by working with arrays of N items

ȚȜȝ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ș
indexed from 1 to N, even though modern programming languages index ar-
rays of N items from 0 to N-1.
Initially, we set p[i]=i; then we modify the sorting code to refer to
a[p[i]] instead of a[i] for any comparison, but to refer to p instead of a
when doing any data movement. Ļese changes ensure that, at any point dur-
ing the execution of the algorithm, a[p[1]], a[p[2]], ..., a[p[N]]
is identical to a[1], a[2], ..., a[N] in the original algorithm.
For example, if a sorting method is used in this way to put the sample
input ŀle
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
keys
29 41 77 26 58 59 97 82 12 44 63 31 53 23 93
into increasing order, it produces the permutation
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
as the result. One way of interpreting this is as instructions that the original
input can be printed out (or accessed) in sorted order by ŀrst printing out the
ninth element (12), then the fourteenth (23), then the fourth (26), then the
ŀrst (29), and so on. In the present context, we note that the permutation
computed is the inverse of the permutation that represents the initial ordering
of the keys. For our example the following permutation results:
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
inverse
4
6
12
3
9
10 15 13
1
7
11
5
8
2
14
With this approach, sorting amounts to computing the inverse of a permuta-
tion. If an output array b[1]...b[N] is available, the program that actually
ŀnishes a sort is trivial:
for (int i = 1; i <= N; i++) b[i] = a[p[i]]
For sorting applications where data movement is expensive (for example, when
records are much larger than keys), this change can be very important. If space
is not available for the output array, it is still possible to do the rearrangement
“in place”—we will examine an algorithm for doing so later in this chapter.
It is amusing to note that, of course, the result of the sort can be an
involution. For example, consider the input ŀle

§Ȟ.ș
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȜȞ
58
23
77
29
44
59
31
82
12
41
63
26
53
97 93.
For this ŀle, the same permutation not only represents the initial ordering of
the input ŀle but also is the permutation that speciŀes how to rearrange the
ŀle to sort it. Ļat is, the permutation
9
2
12
4
7
10
5
13
1
6
11
3
8
15
11
can be interpreted in two ways: not only is it the case that 58 is the ninth
smallest element, 23 the second smallest, 77 the twelfth smallest, and so on,
but it is also the case that the ninth element in the ŀle (12) is the smallest,
the second element (26) is the second smallest, the twelfth element (29) is
the third smallest, and so on.
Randompermutations. Many important applications involve randomization.
For example, if the assumption that the inputs to a sorting algorithm are not
randomly ordered is not necessarily justiŀed, we can use the method shown
in Program 7.1 to create a random ordering, and then sort the array. Ļis
randomization technique assumes a procedure that can produce a “random”
integer in a given range (such programs are well studied; see Knuth [9]). Each
of the N! orderings of the input is equally likely to occur: the ith time through
the loop, any one of i distinct rearrangements could happen, for a grand total
of 2 · 3 · 4 · . . . · N = N!.
As mentioned in Chapter 1, from the standpoint of the analysis of algo-
rithms, randomizing the order of the inputs in this way turns any sorting algo-
rithm into a “probabilistic algorithm” whose performance characteristics are
precisely described by the average-case results that we study. Indeed, this is
one of the very ŀrst randomized algorithms, as it was proposed for quicksort
by Hoare in 1960 (see Chapter 1).
public void exch(Item[] a, int i, int j)
{ Item t = a[i]; a[i] = a[j]; a[j] = t; }
for ( int i = 1; i < N; i++)
exch(a, i, StdRandom.uniform(0, i));
Program 7.1 Randomly permuting an array

ȚȜȟ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ț
Priority queues.
It is not always necessary that an algorithm rearrange its
inputs into sorted order before it examines any of them. Another widely used
option is to develop a data structure, consisting of records with keys, that
supports two operations: insert a new item into the data structure and re-
move smallest, which retrieves the record with the smallest key from the data
structure. Such a data structure is called a priority queue.
Priority queue algorithms are closely related to sorting algorithms: for
example, we could use any priority queue algorithm to implement a sorting
algorithm simply by inserting all the records, then removing them all. (Ļey
will come out in increasing order.) But priority queue algorithms are much
more general, and they are widely used, both because the insert and remove
operations can be intermixed, and because several other operations can be
supported as well. Ļe study of the properties of sophisticated priority queue
algorithms is among the most important and most challenging areas of re-
search in the analysis of algorithms. In this chapter, we will examine the
relationship between a fundamental priority queue structure (heap-ordered
trees) and binary search trees, which is made plain by the association of both
with permutations.
7.3 Representations of Permutations. While it is normally most conve-
nient to represent permutations as we have been doing—as rearrangements of
the numbers 1 through N—many other ways to represent permutations are
often appropriate. We will see that various diﬀerent representations can show
relationships among basic properties of permutations and can expose essen-
tial properties germane to some particular analysis or algorithm. Since there
are N! diﬀerent permutations of N elements, any set of N! diﬀerent com-
binatorial objects might be used to represent permutations—we will examine
several useful ones in this section.
Cycle structure. We have already considered the representation of permu-
tations as a directed graph with sets of cycles, as illustrated in Figure 7.2.
Ļis representation is of interest because it makes the cycle structure obvious.
Also, extending this representation leads to structures for general functions
that have many interesting properties, which we will study in Chapter 9.
Foata’s representation. We also have already considered Foata’s representa-
tion, also illustrated in Figure 7.2, where we write cycles starting with their

§Ȟ.Ț
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȜȠ
smallest element and put the cycles in decreasing order of these elements.
Ļis representation establishes an important relationship between two prop-
erties of permutations (number of cycles and number of left-right minima)
that would not otherwise seem to be related.
Exercise 7.7 Alternatively, we might put the cycles in decreasing order of their small-
est elements, writing the smallest element in the cycle ŀrst. Give this representation
for our sample permutation.
Exercise 7.8 Write a programs that will compute Foata’s representation of a given
permutation, and vice versa.
Inversion tables. A one-to-one correspondence between permutations and
lists of N integers q1q2 . . . qN with 0 ≤qi < i is easy to establish. Given a
permutation, its inversion table is such a list; and given such a list, the corre-
sponding permutation can be constructed from right to left: for i decreasing
from N down to 1, set pi to be the qith largest of the integers not yet used.
Consider the example:
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
inversion table
0
0
2
3
1
4
2
1
5
5
3
9
6
0
8
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
Ļe permutation can be constructed from the inversion table by moving right
to left: 7 is the eighth largest of the integers from 1 to 15, 15 is the largest of
the remaining integers, 8 is the sixth largest of what’s left, and so forth. Ļere
are N! inversion tables (and permutations!) since there are i possibilities for
the ith entry.
Ļis correspondence is important in the analysis of algorithms because
a random permutation is equivalent to a “random” inversion table, built by
making its jth entry a random integer in the range 0 to j −1. We will make
use of this fact when setting up GFs for inversions and left-to-right maxima
that nicely decompose in product form.
Exercise 7.9 Give an eﬃcient algorithm for computing the inversion table corre-
sponding to a given permutation, and another algorithm for computing the permu-
tation corresponding to a given inversion table.
Exercise 7.10 Another way to deŀne inversion tables is with qi equal to the number
of integers to the left of i in the permutation that are greater. Prove the one-to-one
correspondence for this kind of inversion table.

Țȝȗ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ț
Lattice representation. Figure 7.3 shows a two-dimensional representation
that is useful for studying a number of properties of permutations: the permu-
tation p1p2 . . . pN is represented by labelling the cell at row pi and column i
with the number pi for each i. Reading these numbers from right to left gives
back the permutation. Ļere is one label in each row and in each column, so
each cell in the lattice corresponds to a unique pair of labels: the one in its row
and the one in its column. If one member of the pair is below and the other
to the right, then that pair is an inversion in the permutation, and the corre-
sponding cell is marked in Figure 7.3. Note in particular that the diagram for
the permutation and its inverse are simply transposes of each other—this is an
elementary proof that every permutation has the same number of inversions
as does its inverse.
Exercise 7.11 Show that the number of ways to place k mutually nonattacking rooks
in an N-by-N chessboard is
(N
k
)2k!.
Exercise 7.12 Suppose that we mark cells whose marks are above and to the left in
the lattice representation. How many cells are marked? Answer the same question
for the other two possibilities (“above and right” and “below and left”).
Exercise7.13 Show that the lattice representation of an involution is symmetric about
the main diagonal.
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
Figure 7.3 Lattice representation of a permutation and its inverse

§Ȟ.Ț
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȝȘ
Binary search trees.
In Chapter 6, we analyzed properties of binary trees
built by using Program 6.3 to insert keys from a random permutation succes-
sively into an initially empty tree, which implies a correspondence between
permutations and BSTs (see Figure 6.14).
Figure 7.4 illustrates the correspondence in terms of a lattice represen-
tation: each label corresponds to a node with its row number as the key value,
and left and right subtrees built from the parts of the lattice above and below
the label, respectively. Speciŀcally, the binary search tree corresponding to
rows l, l+1, . . ., r, with the leftmost (lowest column number) mark in row k,
is deŀned recursively by a node with key k, left subtree corresponding to rows
l, l + 1, . . ., k −1, and right subtree corresponding to rows k + 1, k + 2, . . .,
r. Note that many permutations might correspond to the same binary search
tree: interchanging columns corresponding to a node above and a node be-
low any node will change the permutation, not the tree. Indeed, we know
that the number of diﬀerent binary tree structures is counted by the Catalan
numbers, which are relatively small (about 4N/N
√
πN ) compared to N!, so
a large number of permutations certainly must correspond to each tree. Ļe
analytic results on BSTs in the previous chapter might be characterized as a
study of the nature of this relationship.
Exercise 7.14 List ŀve permutations that correspond to the BST in Figure 7.4.
9
14
4
1
12
2
10
13
5
6
11
3
8
15
7
9
4
14
1
5
6
8
7
2
3
15
12
10
13
11
Figure 7.4 Binary search tree corresponding to a permutation

Țȝș
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ț
Heap-ordered trees. A tree can also be built from the lattice representation in
a similar manner involving columns, as illustrated in Figure 7.5. Ļese trees,
in which the key at the root has a smaller key than the keys in the subtrees, are
called heap-ordered trees (HOTs). In the present context, they are important
because properties of permutations that are of interest are easily seen to be
tree properties. For example, a node with two nonempty subtrees in the tree
corresponds to a valley in the permutation, and leaves correspond to peaks.
Such trees are also of interest as explicit data structures, to implement
priority queues. Ļe smallest key is at the root, so the “remove smallest”
operation can be implemented by (recursively) replacing the node at the root
with the node at the root of the subtree with the smaller key. Ļe “insert”
operation can be implemented by (recursively) inserting the new node in the
right subtree of the root, unless it has a smaller key than the node at the root,
in which case it becomes the root, with the old root as its left subtree and a
null right subtree.
Combinatorially, it is important to observe that an HOT is a complete
encoding of a unique permutation (contrary to BSTs). See Vuillemin [20] for
detailed information on HOTs, including other applications.
Direct counts of HOTs. To complete the cycle, it is instructive to consider
the problem of enumerating HOTs directly (we already know that there are
9
14
4
1
12
2
10
13
5
6
11
3
8
15
7
1
4
2
9
12
3
14
5
7
10
6
8
13
11
15
Figure 7.5 Heap-ordered tree corresponding to a permutation

§Ȟ.Ț
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȝȚ
N! of them). Let H be the set of HOTs and consider the EGF
H(z) =
∑
t∈H
z|t|
|t|!.
Now, every HOT with |tL| + |tR| + 1 nodes can be constructed in a unique
way by combining any HOT of size |tL| on the left with any HOT of size
|tR| on the right, assigning the label “1” to the root, and assigning labels to
the subtrees by dividing the |tL| + |tR| labels into one set of size |tL| and one
set of size |tR|, sorting each set, and assigning them to the subtrees in order of
their index. Ļis is essentially a restatement of the labelled product construct
described in Chapter 3. In terms of the EGF, this leads to the equation
H(z) =
∑
tL∈H
∑
tR∈H
(
|tL| + |tR|
|tL|
)
z|tL|+|tR|+1
(|tL| + |tR| + 1)!.
Diﬀerentiating both sides immediately gives
H′(z) = H2(z)
or
H(z) = 1 +
∫z
0
H2(t)dt.
Ļis formula is also available directly via the symbolic method for labelled
objects (see Chapter 3 and [4]). Two basic operations explain the formula:
labelling the root 1 corresponds to integration, and combining two subtrees
corresponds to a product. Now, the solution to the diﬀerential equation is
H(z) = 1/(1 −z), which checks with our knowledge that there are N!
HOTs on N nodes.
Similar computations will give access to statistics on parameters of per-
mutations that are characterized on the HOT representation (e.g., peaks, val-
leys, rises, and falls), as introduced in the following exercises and discussed
further in §7.5.
Exercise 7.15 Characterize the nodes in an HOT that correspond to rises, double
rises, falls, and double falls in permutations.
Exercise 7.16 How many permutations are strictly alternating, with pi−1 and pi+1
either both less than or both greater than pi for 1 < i < N?
Exercise 7.17 List ŀve permutations corresponding to the HOT in Figure 7.5.

Țȝț
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ț
Exercise 7.18 Let K(z) = z/(1 −z) be the EGF for nonempty HOTs. Give a
direct argument showing that K′(z) = 1 + 2K(z) + K2(z).
Exercise 7.19 Use an argument similar to HOT enumeration by EGF to derive the
diﬀerential equation for the exponential CGF for internal path length in binary search
trees (see the proof of Ļeorem 5.5 and §6.6).
How many permutations correspond to a given binary tree shape via
the HOT or BST construction? Ļis turns out to be expressed with a simple
formula. Given a tree t, let f(t) be the number of ways to label it as an HOT.
By the argument just given, this function satisŀes the recursive equation
f(t) =
(
|tL| + |tR|
|tL|
)
f(tL)f(tR).
Ļe same formula holds if f is counting the number of permutations corre-
sponding to a BST since there is a unique label for the root (|tL| + 1) and
an unrestricted partition of the rest of the labels for the subtrees. Noting that
the number of nodes in t is |tL| + |tR| + 1, and dividing both sides by that
quantity, we get
f(|tL| + |tR| + 1)
(|tL| + |tR| + 1)! =
1
|tL| + |tR| + 1
f(tL)
tL!
f(tR)
tR! .
Ļis leads directly to the following theorem:
Ļeorem 7.1 (Frequency of HOTs and BSTs).
Ļe number of permuta-
tions corresponding to a tree t, as either an HOT or BST shape, is
f(t) =
|t|!
|u1||u2| · · · |u|t||
where u1, . . . , u|t| are the subtrees rooted at each of the nodes of |t|. In other
words, f(t) is the number of ways to label t as an HOT using the labels
1 . . . |t| and the number of permutations of 1 . . . |t| that lead to a BST with
the shape of t when built with the standard algorithm.
Proof. Iterate the recursive formula just given, using the initial condition that
f(t) = 1 for |t| = 1.

§Ȟ.Ț
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȝȜ
For example, the number of ways to label the HOT in Figure 7.5 is
15!/(15 · 3 · 2 · 11 · 9 · 5 · 2 · 2 · 3) = 1223040.
Notice that the fact that frequencies for HOTs and BSTs coincide is
also to be expected combinatorially. Ļe two correspondences exempliŀed by
Figures 7.4 and 7.5 are structurally the same, with only the axes being inter-
changed. In other words, a stronger property holds: the BST corresponding
to a permutation has the same shape as the HOT corresponding to the inverse
permutation (but not the same labels).
Exercise 7.20 How many permutations correspond to the BST in Figure 7.4?
Exercise 7.21 Give the number of permutations that correspond to each of the BST
shapes in Figure 6.1.
Exercise 7.22 Characterize the binary trees of size N to which the smallest number
of permutations correspond and those to which the largest number of permutations
correspond.

Țȝȝ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ț
7.4 Enumeration Problems. Many of the problems listed in the intro-
ductory section of this chapter are enumeration problems. Ļat is, we want
to know the number of permutations that satisfy a certain property for various
properties of interest. Equivalently, dividing by N! gives the probability that
a random permutation satisŀes the property of interest. We work with the
EGF that enumerates the permutations, which is equivalent to the OGF for
the probability.
A variety of interesting properties of permutations can be expressed in
terms of simple restrictions on the cycle lengths, so we begin by considering
such problems in detail. We are interested in knowing, for a given parameter
k, exact counts of the number of permutations having (i) cycles only of length
equal to k, (ii) no cycles of length greater than k, and (iii) no cycles of length
less than k. In this section, we develop analytic results for each of these enu-
meration problems. We already considered (ii) in §5.3; we will brieły review
that analysis in context here. Table 7.4 gives the counts for N ≤10 and
k ≤4. For N ≤4, you can check the values against Figure 5.8.
Cycles of equal length. How many permutations of size N consist solely of
cycles of length k? We start with a simple combinatorial argument for k = 2.
Ļere are zero permutations consisting solely of doubleton cycles if N is odd,
so we consider N even. We choose half the elements for the ŀrst element of
cycle lengths
1
2
3
4
5
6
7
8
9
all = 1
1
1
1
1
1
1
1
1
1
= 2
0
1
0
3
0
15
0
105
0
= 3
0
0
2
0
0
40
0
0
2240
= 4
0
0
0
6
0
0
0
1260
0
< 3 (involutions)
1
2
4
10
26
76
232
764
2620
< 4
1
2
6
18
66
276
1212
5916
31,068
< 5
1
2
6
24
96
456
2472
14,736
92,304
> 1 (derangements)
0
1
2
9
44
265
1854
14,833 133,496
> 2
0
0
2
6
24
160
1140
8988
80,864
> 3
0
0
0
6
24
120
720
6300
58,464
no restrictions
1
2
6
24
120
720
5040
40,320 362,880
Table 7.4
Enumeration of permutations with cycle length restrictions

§Ȟ.ț
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȝȞ
each cycle, then assign the second element in each of (N/2)! possible ways.
Ļe order of the elements is immaterial, so this counts each permutation 2N/2
times. Ļis gives a total of
(
N
N/2
)
(N/2)!/2N/2 =
N!
(N/2)!2N/2
permutations made up of cycles of length 2. Multiplying by zN and dividing
by N! we get the EGF
∑
N even
zN
(N/2)!2N/2 = ez2/2.
As we saw in Chapter 5, the symbolic method also gives this EGF directly:
the combinatorial construction SET (CYC 2(Z)) immediately translates to
ez2/2 via Ļeorem 5.2 (and its proof). Ļis argument immediately extends
to show that the EGF for the number of permutations consisting solely of
cycles of length k is ezk/k.
Derangementsandlowerboundsoncyclelength. Perhaps the most famous
enumeration problem for permutations is the derangement problem. Suppose
that N students throw their hats in the air, then each catches a random hat.
What is the probability that none of the hats go to their owners? Ļis prob-
lem is equivalent to counting derangements—permutations with no singleton
cycles—and immediately generalizes to the problem of counting the number
of permutations with no cycles of length M or less. As we saw in §5.3 and
§5.5, solving this problem is straightforward with analytic combinatorics. For
completeness, we restate the result here.
Ļeorem 7.2 (Minimal cycle lengths).
Ļe probability that a random per-
mutation of length N has no cycles of length M or less is ∼e−HM.
Proof. (Quick recap of the proof discussed at length in §5.3). Let P>M be
the class of permutations with no cycles of length M or less. Ļe symbolic
equation for permutations
P = SET (CYC 1(Z)) × SET (CYC 2(Z)) × . . . × SET (CYC M(Z)) × P>M

Țȝȟ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ț
transfers immediately to the generating function equation
1
1 −z = ezez2/2 · · · ezM/MP>M(z).
which immediately gives the following EGF:
P>M(z) =
1
1 −z e−z−z2/2−z3/3−...−zM/M.
Ļe stated asymptotic result is a direct consequence of Ļeorem 5.5.
In particular, this theorem answers our original question: the probability
that a random permutation of length N is a derangement is
[zN]P>0(z) =
∑
0≤k≤N
(−1)k
k!
∼1/e ≈.36787944.
Upper bounds on cycle length and involutions. Continuing in this manner,
developing the EGF for the number of permutations that have a speciŀed
upper bound on cycle length is straightforward.
Ļeorem7.3 (Maximalcyclelengths).
Ļe EGF that enumerates the num-
ber of permutations with no cycles of length greater than M is
exp(z + z2/2 + z3/3 + . . . + zM/M).
Proof. Immediate via the symbolic method: Let P≤M be the class of per-
mutations with no cycle lengths larger than M. Ļe symbolic equation for
permutations
P≤M = SET (CYC 1(Z)) × SET (CYC 2(Z)) × . . . × SET (CYC M(Z))
transfers immediately to the stated EGF.
As mentioned brieły in the previous section, involutions can be charac-
terized by cycle length restrictions, and thus enumerated by the argument just
given. If pi = j in a permutation, then pj = i in the inverse: both of these
must hold for i ̸= j in an involution, or the cycle (i, j) must be present. Ļis
observation implies that involutions consist of cycles of length 2 (ppi = i) or
1 (pi = i). Ļus involutions are precisely those permutations composed solely
of singleton and doubleton cycles, and the EGF for involutions is ez+z2/2.

§Ȟ.ț
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȝȠ
Ļeorem 7.4 (Involutions).
Ļe number of involutions of length N is
∑
0≤k≤N/2
N!
(N −2k)!2kk! ∼
1
√
2√e
(N
e
)N/2e
√
N.
Proof. From the preceding discussion, the associated EGF is ez+z2/2. Ļe gen-
eral analytic transfer theorem from analytic combinatorics to extract coeﬃ-
cients from such functions is the saddle point method from complex analysis,
as described in [4]. Ļe following sketch, using real analysis, is from Knuth
[10]. First, the summation follows from the convolution
ez+z2/2 =
∑
j≥0
1
j!
(
z + z2
2
)j =
∑
j,k≥0
1
j!
(
j
k
)
zj−k(z2
2
)k
and collecting [zN]. Next, use the Laplace method of Chapter 4. Taking the
ratio of successive terms in the sum, we have
N!
(N −2k)!2kk!
/
N!
(N −2k −2)!2k+1(k + 1)! =
2(k + 1)
(N −2k)(N −2k −1),
which shows that the terms in the sum increase until k is approximately
(N −
√
N )/2, then decrease. Using Stirling’s approximation to estimate
the dominant contribution near the peak and a normal approximation to
bound the tails, the result follows in the same way as in various examples
in Chapter 4. Details are worked out in [10].
Direct derivation of involution EGF. It is also instructive to derive the exponen-
tial generating function for involutions directly. Every involution of length
|p| corresponds to (i) one involution of length |p| + 1, formed by appending
the singleton cycle consisting of |p| + 1; and (ii) |p| + 1 involutions of length
|p| + 2, formed by, for each k from 1 to |p| + 1, adding 1 to permutation
elements greater than k, then appending the doubleton cycle consisting of k
and |p| + 2. Ļis implies that the EGF must satisfy
B(z) ≡
∑
p∈P
p involution
z|p|
|p|! =
∑
p∈P
p involution
z|p|+1
(|p| + 1)! +
∑
p∈P
p involution
(|p|+1)
z|p|+2
(|p| + 2)!.

ȚȞȗ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ț
Diﬀerentiating, this simpliŀes to the diﬀerential equation
B′(z) = (1 + z)B(z)
which has the solution
B(z) = ez+z2/2
as expected. Ļe diﬀerential equation is also available via the symbolic method
(see [4]).
FśŞ ŞőŒőŞőŚŏő, TōŎŘő Ȟ.Ȝ ŘŕşŠş ŠŔő solutions that are discussed here to
enumeration problems for permutations with cycle length restrictions. All of
the EGFs are easy to derive with the symbolic method; the asymptotic esti-
mates require a range of techniques. Next, we move on to consider properties
of permutations, using bivariate and cumulative generating functions.
EGF
asymptotic estimate of N![zN]
singleton cycles
ez
1
cycles of length M
ezM/M
—
all permutations
1
1 −z
N! ∼
√
2πN
(N
e
)N
derangements
e−z
1 −z
∼N!
e
all cycles > M
e−z−z2/2...−zM/M
1 −z
∼N!
eHM
involutions
ez+z2/2
∼
1
√
2√e
e
√
N(N
e
)N/2
all cycles ≤M
ez+z2/2+...+zM/M
—
Table 7.5
EGFs for permutations with cycle length restrictions

§Ȟ.ț
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȞȘ
Exercise 7.23 Show that the number of involutions of size N satisŀes the recurrence
bN+1 = bN + NbN−1
for N > 0 with b0 = b1 = 1.
(Ļis recurrence can be used, for example, to compute the entries on the row corre-
sponding to involutions in Table 7.4.)
Exercise 7.24 Derive a recurrence that can be used to compute the number of per-
mutations that have no cycle of length > 3.
Exercise 7.25 Use the methods from §5.5 to derive a bound involving N N(1−1/k)
for the number of permutations with no cycle of length greater than k.
Exercise 7.26 Find the EGF for the number of permutations that consist only of
cycles of even length. Generalize to ŀnd the EGF for the number of permutations
that consist only of cycles of length divisible by t.
Exercise 7.27 By diﬀerentiating the relation (1 −z)D(z) = ez and setting co-
eﬃcients equal, obtain a recurrence satisŀed by the number of derangements of N
elements.
Exercise 7.28 Write a program to, given k, print a table of the number of permuta-
tions of N elements with no cycles of length < k for N < 20.
Exercise 7.29 An arrangement of N elements is a sequence formed from a subset
of the elements. Prove that the EGF for arrangements is ez/(1 −z). Express the
coeﬃcients as a simple sum and give a combinatorial interpretation of that sum.

ȚȞș
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȝ
7.5 Analyzing Properties of Permutations with CGFs. In this section,
we outline the basic method that we will use for the analysis of properties of
permutations for many of the problems given in this chapter, using cumulative
generating functions (CGFs). Introduced in Chapter 3, this method may be
summarized as follows:
• Deŀne an exponential CGF of the form B(z) = ∑
p∈P cost(p)z|p|/|p|!.
• Identify a combinatorial construction and use it to derive a functional
equation for B(z).
• Solve the equation or use analytic techniques to ŀnd [zN]B(z).
Ļe second step is accomplished by ŀnding a correspondence among permu-
tations, most often one that associates |p| + 1 permutations of length |p| + 1
with each of the |p|! permutations of length |p|. Speciŀcally, if Pq is the set
of permutations of length |q| + 1 that correspond to a given permutation q,
then the second step corresponds to rewriting the CGF as follows:
B(z) =
∑
q∈P
∑
p∈Pq
cost(p)
z|q|+1
(|q| + 1)!.
Ļe permutations in Pq are typically closely related, and then the inner sum
is easy to evaluate, which leads to an alternative expression for B(z). Often,
it is also convenient to diﬀerentiate to be able to work with z|q|/|q|! on the
right-hand side.
For permutations, the analysis is also somewhat simpliŀed because of
the circumstance that the factorial for the exponential CGF also counts the
total number of permutations, so the exponential CGF is also an ordinary GF
for the average value sought. Ļat is, if
B(z) =
∑
p∈P
cost(p)z|p|/|p|!,
then it follows that
[zN]B(z) =
∑
k
k{# of perms of length N with cost k}/N!,
which is precisely the average cost. For other combinatorial structures it is
necessary to divide the cumulative count obtained from the CGF by the total
count to get the average, though there are other cases where the cumulative
count has a suﬃciently simple form that the division can be incorporated into
the generating function. We will see another example of this in Chapter 8.

§Ȟ.Ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȞȚ
Combinatorial constructions. We consider several combinatorial construc-
tions for permutations. Typically, we use these to derive a recurrence, a CGF
or even a full BGF. BGFs give a stronger result since explicit knowledge about
the BGF for a parameter implies knowledge about the distribution of values.
CGFs lead to simpler computations because they are essentially average val-
ues, which can be manipulated without full knowledge of the distribution.
“First” or “last” construction. Given any one of the N! diﬀerent permuta-
tions of length N, we can identify N + 1 diﬀerent permutations of length
N + 1 by, for each k from 1 to N, prepending k and then incrementing all
numbers larger than or equal to k. Ļis deŀnes the “ŀrst” correspondence. It
is equivalent to the P = Z ⋆P construction that we used in §5.3 (see Figure
5.7). Alternatively, we can deŀne a “last” construction that puts the new item
at the end and is equivalent to P = P ⋆Z (see Figure 7.6).
“Largest” or “smallest” construction. Given any permutation p of length N,
we can identify N + 1 diﬀerent permutations of length N + 1 by putting
the largest element in each of the N + 1 possible positions between elements
of p. Ļis deŀnes the “largest” construction, illustrated in Figure 7.6. Ļe
ŀgure uses a diﬀerent type of star to denote the construction, to distinguish
it from the ⋆that we use in analytic combinatorics. It is often possible to
embrace such constructions within the symbolic method, but we will avoid
confusion by refraining from doing so. We will also refrain from associating
unique symbols with all the constructions that we consider, since there are
so many possibilities. For example, we can base a similar construction on
any other element, not just the largest, by renumbering the other elements
appropriately. Using the smallest element involves adding one to each other
element, then placing 1 in each possible position. We refer to this one as the
“smallest” construction.
Binary search tree construction.
Given two permutations pl, pr, we can
create a total of
(
|pl| + |pr|
|pl|
)
permutations of size |pl| + |pr| + 1 by (i) adding |pl| + 1 to each element
of pr; (ii) intermixing pl and pr in all possible ways; and (iii) preŀxing each
permutation so obtained by |pl| + 1. As we saw in §6.3, all the permutations
obtained in this way lead to the construction of the same binary search tree

ȚȞț
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȝ
“Last” construction
“Largest” construction
  =  
  =  
  =  
  =  
  =  
  =  
1
1  2  3  
2  3  4  1
1  3  4  2
1  2  4  3
1  2  3  4
1
2  1  3  
3  2  4  1
3  1  4  2
2  1  4  3
2  1  3  4
1
1
1
1
3  1  2  
4  2  3  1
4  1  3  2
4  1  2  3
3  1  2  4
1  3  2  
2  4  3  1
1  4  3  2
1  4  2  3
1  3  2  4
2  3  1  
3  4  2  1
3  4  1  2
2  4  1  3
2  3  1  4
3  2  1  
4  3  2  1
4  3  1  2
4  2  1  3
3  2  1  4
  =  
  =  
  =  
  =  
  =  
  =  
1
1  2  3  
1  2  3  4
1  2  4  3
1  4  2  3
4  1  2  3
1
2  1  3  
2  1  3  4
2  1  4  3
2  4  1  3
4  2  1  3
1
1
1
1
3  1  2  
3  1  2  4
3  1  4  2
3  4  1  2
4  3  1  2
1  3  2  
1  3  2  4
1  3  4  2
1  4  3  2
4  1  3  2
2  3  1  
2  3  1  4
2  3  4  1
2  4  3  1
4  2  3  1
3  2  1  
3  2  1  4
3  2  4  1
3  4  2  1
4  3  2  1
Figure 7.6 Two combinatorial constructions for permutations

§Ȟ.Ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȞȜ
using the standard algorithm. Ļerefore this correspondence can be used as
a basis for analyzing BST and related algorithms. (See Exercise 6.19.)
Heap-ordered tree construction.
Ļe combinatorial construction P ⋆P,
used recursively, is useful for parameters that have a natural interpretation in
terms of heap-ordered trees, as follows. Ļe construction identiŀes “left” and
“right” permutations. Adding 1 to each element and placing a 1 between the
left permutation and the right permutation corresponds to an HOT.
NśŠő ŠŔōŠ ōŘŘ śŒ these decompositions lead to diﬀerential equations in
the CGFs because adding an element corresponds to shifting the counting
sequences, which translates into diﬀerentiating the generating function (see
Table 3.4). Next, we consider several examples.
Runs and rises. As a ŀrst example, consider the average number of runs in a
random permutation. Elementary arguments given previously show that the
average number of runs in a permutation of length N is (N + 1)/2, but the
full distribution is interesting to study, as discovered by Euler (see [1], [7],
and [10] for many details). Figure 7.7 illustrates this distribution for small
values of N and k. Ļe sequence for k = 2 is OEIS A000295 [18]; the full
sequence is OEIS A008292. We start with the (exponential) CGF
A(z) =
∑
p∈P
runs(p)z|p|
|p|!
and use the “largest” construction: if the largest element is inserted at the end
of a run in p, there is no change in the number of runs; otherwise, the num-
ber of runs is increased by 1. Ļe total number of runs in the permutations
corresponding to a given permutation p is
(|p| + 1)runs(p) + |p| + 1 −runs(p) = |p|runs(p) + |p| + 1.
Ļis leads to the alternative expression
A(z) =
∑
p∈P
(|p|runs(p) + |p| + 1)
z|p|+1
(|p| + 1)!,

ȚȞȝ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȝ
which simpliŀes considerably if we diﬀerentiate:
A′(z) =
∑
p∈P
(|p|runs(p) + |p| + 1)z|p|
|p|!
= zA′(z) +
z
(1 −z)2 +
1
1 −z .
Ļerefore A′(z) = 1/(1 −z)3, so, given the initial conditions,
A(z) =
1
2(1 −z)2 −1
2
and we have the anticipated result [zN]A(z) = (N + 1)/2.
We will be doing several derivations of this form in this chapter, because
the CGF usually gives the desired results without much calculation. Still, it is
instructive to note that the same construction often yields an explicit equation
for the BGF, either directly through the symbolic method or indirectly via a
recurrence. Doing so is worthwhile because the BGF carries full information
about the distribution—in this case it is subject to “perturbation” methods
from analytic combinatorics that ultimately show it to be asymptotically nor-
mal, as is apparent in Figure 7.7.
Ļeorem7.5 (Eulerian numbers).
Permutations of N elements with k runs
are counted by the Eulerian numbers, ANk, with exponential BGF
A(z, u) ≡
∑
N≥0
∑
k≥0
ANk
zN
N!uk =
1 −u
1 −uez(1−u) .
Proof. Ļe argument given for the CGF generalizes to provide a partial dif-
ferential equation for the BGF using the “largest” construction.
We leave

§Ȟ.Ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȞȞ
N
↓k →1
2
3
4
5
6
7
8
9
10
1
1
2
1
1
3
1
4
1
4
1
11
11
1
5
1
26
66
26
1
6
1
57
302
302
57
1
7
1
120
1191
2416
1191
120
1
8
1
247
4293
15,619
15,619
4293
247
1
9
1
502 14,608
88,234
156,190
88,234
14,608
502
1
10
1 1013 47,840 455,192 1,310,354 1,310,354 455,192 47,840 1013
1
0
.667
.167
0
Figure 7.7
N/2
N
Distribution of runs in permutations (Eulerian numbers)

ȚȞȟ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȝ
that for an exercise and consider the recurrence-based derivation, derived
from the same construction. To get a permutation with k runs, there are
k possibilities that the largest element is inserted at the end of an existing run
in a permutation with k runs, and N −k + 1 possibilities that the largest
element “breaks” an existing run of a permutation with k −1 runs, thereby
increasing the number of runs to k. Ļis leads to
ANk = (N −k + 1)A(N−1)(k−1) + kA(N−1)k,
which, together with the initial conditions A00 = 1 and AN0 = 0 for N ≥0
or k > N, fully speciŀes the ANk.
Multiplying by zNuk and summing on N and k leads directly to the
partial diﬀerential equation
Az(z, u) =
1
1 −uz (uA(z, u) + u(1 −u)Au(z, u)).
It is then easily checked that the stated expression for A(z, u) satisŀes this
equation.
Corollary Ļe average number of runs in a permutation of size N > 1 is
(N + 1)/2 with variance (N + 1)/12.
Proof. We calculate the mean and variance as in Table 3.6, but taking into
account that using an exponential BGF automatically includes division by
N!, as usual for permutations. Ļus, the mean is given by
[zN]∂A(z, u)
∂u

u=1
= [zN]
1
2(1 −z)2 = N + 1
2
,
and we can compute the variance in a similar manner.
As noted above, all runs but the last in a permutation are terminated by
a fall, so Ļeorem 7.5 also implies that the number of falls in a permutation
has mean (N −1)/2 and variance (N + 1)/12. Ļe same result also applies
to the number of rises.
Exercise7.30 Give a simple noncomputational proof that the mean number of rises in
a permutation of N elements is (N−1)/2. (Hint : For every permutation p1p2 . . . pN,
consider the “complement” q1q2 . . . qN formed by qi = N + 1 −pi.)

§Ȟ.Ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȞȠ
Exercise 7.31 Generalize the CGF argument given earlier to provide an alternative
direct proof that the BGF A(z, u) = ∑
p∈P uruns(p)z|p| satisŀes the partial diﬀeren-
tial equation given in the proof of Ļeorem 7.5.
Exercise 7.32 Prove that
ANk =
∑
0≤j≤k
(−1)j
(N + 1
j
)
(k −j)N.
Exercise 7.33 Prove that
xN =
∑
1≤k≤N
ANk
(x + k −1
N
)
for N ≥1.
Increasing subsequences. Another way to develop an explicit formula for a
CGF is to ŀnd a recurrence on the cumulative cost. For example, let
S(z) =
∑
p∈P
{# increasing subsequences in p}z|p|
|p|! =
∑
N≥0
SN
zN
N!
so SN represents the total number of increasing subsequences in all permu-
tations of length N. Ļen, from the “largest” correspondence, we ŀnd that
SN = NSN−1+
∑
0≤k<N
(
N −1
k
)
(N−1−k)!Sk
for N > 0 with S0 = 1.
Ļis accounts for the N copies of the permutation of length N −1 (all the
increasing subsequences in that permutation appear N times) and in a separate
accounting, all the increasing subsequences ending in the largest element. If
the largest element is in position k + 1, then all the permutations for each of
the choices of elements for the ŀrst k positions appear (N −1 −k)! times
(one for each arrangement of the larger elements), each contributing Sk to
the total. Ļis argument assumes that the empty subsequence is counted as
“increasing,” as in our deŀnition. Dividing by N! and summing on N, we get
the functional equation
(1 −z)S′(z) = (2 −z)S(z)
which has the solution
S(z) =
1
1 −z exp
(
z
1 −z
)
.
Ļe appropriate transfer theorem for extracting coeﬃcients from such GFs
involves complex-analytic methods (see [4]), but, as with involutions, it is a
single convolution that we can handle with real analysis.

Țȟȗ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȝ
Ļeorem 7.6 (Increasing subsequences).
Ļe average number of increasing
subsequences in a random permutation of N elements is
∑
0≤k≤N
(
N
k
)
1
k! ∼
1
2√πe
e2
√
N
N1/4 .
Proof. Ļe exact formula follows directly from the previous discussion, com-
puting [zN]S(z) by convolving the two factors in the explicit formula just
given for the generating function.
Ļe Laplace method is eﬀective for the asymptotic estimate of the sum.
Taking the ratio of successive terms, we have
(
N
k
)
1
k!
/ (
N
k + 1
)
1
(k + 1)! = (k + 1)2
N −k ,
which shows that a peak occurs when k is about
√
N. As in several examples
in Chapter 4, Stirling’s formula provides the local approximations and the tails
are bounded via a normal approximation. Details may be found in Lifschitz
and Pittel [13].
Exercise 7.34 Give a direct combinatorial derivation of the exact formula for SN.
(Hint : Consider all places at which an increasing subsequence may appear.)
Exercise7.35 Find the EGF and an asymptotic estimate for the number of increasing
subsequences of length k in a random permutation of length N (where k is ŀxed
relative to N).
Exercise7.36 Find the EGF and an asymptotic estimate for the number of increasing
subsequences of length at least 3 in a random permutation of length N.
Peaks and valleys. As an example of the use of the heap-ordered tree de-
composition of permutations, we now will derive results that reŀne the rise
and run statistics. Ļe nodes in an HOT are of three types: leaves (nodes
with both children external), unary nodes (with one child internal and one
external), and binary nodes (with both children internal). Ļe study of the
diﬀerent types of nodes is directly relevant to the study of peaks and valleys in
permutations (see Exercise 6.18). Moreover, these statistics are of indepen-
dent interest because they can be used to analyze the storage requirements for
HOTs and BSTs.

§Ȟ.Ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȟȘ
Given a heap-ordered tree, its associated permutation is obtained by
simply listing the node labels in inŀx (left-to-right) order. In this correspon-
dence, it is clear that a binary node in the HOT corresponds to a peak in the
permutation: in a left-to-right scan, a binary node is preceded by a smaller
element from its left subtree and followed by another smaller element from its
right subtree. Ļus the analysis of peaks in random permutations is reduced
to the analysis of the number of binary nodes in random HOTs.
Binary nodes in HOTs. Using the symbolic method to analyze heap-ordered
trees requires an additional construction that we have not covered (see [4],
where HOTs are called “increasing binary trees”), but they are also easily han-
dled with familiar tree recurrences. A random HOT of size N is composed
of a left subtree of size k and a right subtree of size N −k −1, where all
values of k between 0 and N −1 are equally likely and hence have probability
1/N. Ļis can be seen directly (the minimum of a permutation assumes each
possible rank with equal likelihood) or via the HOT-BST equivalence. Mean
values are thus computed by the same methods as those developed for BSTs
in Chapter 6.
For example, the average number of binary nodes in a random HOT
satisŀes the recurrence
VN = 1
N
∑
0≤k≤N−1
(Vk + VN−k−1) + N −2
N
for N ≥3
since the number of binary nodes is the sum of the number of binary nodes
in the left and right subtrees plus 1 unless the minimal element is the ŀrst or
last in the permutation (an event that has probability 2/N). We have seen
this type of recurrence on several occasions, starting in §3.3. Multiplying by
zN−1 and summing leads to the diﬀerential equation
V ′(z) = 2 V (z)
1 −z +
z2
(1 −z)2 ,
which has the solution
V (z) = 1
3
z3
(1 −z)2
so that
VN = N −2
3
.
Ļus, the average number of valleys in a random permutation is (N −2)/3,
and similar results about related quantities follow immediately.

Țȟș
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȝ
Ļeorem 7.7 (Local properties of permutations and nodes in HOTs/BSTs).
In a random permutation of N elements, the average numbers of valleys,
peaks, double rises, and double falls are, respectively,
N −2
3
,
N −2
3
,
N + 1
6
,
N + 1
6
.
In a random HOT or BST of size N, the average numbers of binary nodes,
leaves, left-branching, and right-branching nodes are, respectively,
N −2
3
,
N + 1
3
,
N + 1
6
,
N + 1
6
.
Proof. Ļese results are straightforward by arguments similar to that given
above (or just applying Ļeorem 5.7) and using simple relationships among
the various quantities. See also Exercise 6.15.
For example, a fall in a permutation is either a valley or a double fall, so
the average number of double falls is
N −1
2
−N −2
3
= N + 1
6
.
For another example, we know that the expected number of leaves in a
random BST is (N + 1)/3 (or a direct proof such as the one cited for HOTs
could be used) and the average number of binary nodes is (N −2)/3 by the
argument above. Ļus the average number of unary nodes is
N −N −2
3
−N + 1
3
= N + 1
3
,
with left- and right-branching nodes equally likely.
Table 7.7 summarizes the results derived above and some of the results
that we will derive in the next three sections regarding the average values of
various parameters for random permutations. Permutations are suﬃciently
simple combinatorial objects that we can derive some of these results in several
ways, but, as the previous examples make clear, combinatorial proofs with
BGFs and CGFs are particularly straightforward.

§Ȟ.Ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȟȚ
Exercise 7.37 Suppose that the space required for leaf, unary, and binary nodes is
proportional to c0, c1, and c2, respectively. Show that the storage requirement for
random HOTs and for random BSTs is ∼(c0 + c1 + c2)N/3.
Exercise 7.38 Prove that valleys and peaks have the same distribution for random
permutations.
Exercise 7.39 Under the assumption of the previous exercise, prove that the storage
requirement for random binary Catalan trees is ∼(c0 + 2c1 + c2)N/4.
Exercise 7.40 Show that a sequence of N random real numbers between 0 and 1
(uniformly and independently generated) has ∼N/6 double rises and ∼N/6 double
exponential CGF
average ( [zN] )
left-to-right minima
1
1 −z ln
1
1 −z
HN
cycles
1
1 −z ln
1
1 −z
HN
singleton cycles
z
1 −z
1
cycles = k
zk
k
1
1 −z
1
k
(N ≥k)
cycles ≤k
1
1 −z
(
z + z2
2 + . . . + zk
k
)
Hk
(N ≥k)
runs
1
2(1 −z)2 −1
2
N + 1
2
inversions
z2
2(1 −z)3
N(N −1)
4
increasing
subsequences
1
1 −z exp(
z
1 −z )
∼
1
2√πe
e2
√
N
N 1/4
peaks, valleys
z3
3(1 −z)2
N −2
3
Table 7.7
Analytic results for properties of permutations (average case)

Țȟț
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ȝ
falls, on the average. Deduce a direct continuous-model proof of this asymptotic
result.
Exercise 7.41 Generalize Exercise 6.18 to show that the BGF for right-branching
nodes and binary nodes in HOTs satisŀes
Kz(z, u) = 1 + (1 + u)K(z, u) + K2(z, u)
and therefore
K(z, u) = 1 −e(u−1)z
u −e(u−1)z .
(Note: Ļis provides an alternative derivation of the BGF for Eulerian numbers, since
A(z, u) = 1 + uK(z, u).)
7.6 InversionsandInsertionSorts. Program 7.2 is an implementation of
insertion sort, a simple sorting method that is easily analyzed. In this method,
we “insert” each element into its proper position among those previously con-
sidered, moving larger elements over one position to make room. Ļe left
portion of Figure 7.8 shows the operation of Program 7.2 on a sample array
of distinct keys, mapped to a permutation. Ļe highlighted elements in the
ith line in the ŀgure are the elements moved to do the ith insertion.
Ļe running time of insertion sort is proportional to c1N + c2B + c3,
where c1, c2, and c3 are appropriate constants that depend on the implemen-
tation and B, a function of the input permutation, is the number of exchanges.
Ļe number of exchanges to insert each element is the number of larger ele-
ments to the left, so we are led directly to consider inversion tables. Ļe right
portion of Figure 7.8 is the inversion table for the permutation as the sort
proceeds. After the ith insertion (shown on the ith line), the ŀrst i elements
in the inversion table are zero (because the ŀrst i elements of the permutation
for (int i = 1; i < N; i++)
for (int j = i; j >= 1 && a[j-1] > a[j]; j--)
exch(a, j, j-1);
Program 7.2 Insertion sort

§Ȟ.ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȟȜ
 9 14  4  1 12  2 10 13  5  6 11  3  8 15  7
 9 14  4  1 12  2 10 13  5  6 11  3  8 15  7
 4  9 14  1 12  2 10 13  5  6 11  3  8 15  7
 1  4  9 14 12  2 10 13  5  6 11  3  8 15  7
 1  4  9 12 14  2 10 13  5  6 11  3  8 15  7
 1  2  4  9 12 14 10 13  5  6 11  3  8 15  7
 1  2  4  9 10 12 14 13  5  6 11  3  8 15  7
 1  2  4  9 10 12 13 14  5  6 11  3  8 15  7
 1  2  4  5  9 10 12 13 14  6 11  3  8 15  7
 1  2  4  5  6  9 10 12 13 14 11  3  8 15  7
 1  2  4  5  6  9 10 11 12 13 14  3  8 15  7
 1  2  3  4  5  6  9 10 11 12 13 14  8 15  7
 1  2  3  4  5  6  8  9 10 11 12 13 14 15  7
 1  2  3  4  5  6  8  9 10 11 12 13 14 15  7
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
AA AB AL CN EF HF HT JB JC JG JL MC MS PD PL 
JC PD CN AA MC AB JG MS EF HF JL AL JB PL HT
8
0
6
9
3
5
5
1
2
4
1
3
2
0
0
8
0
6
9
3
5
5
1
2
4
1
3
2
0
0
8
0
6
9
3
5
5
1
2
4
1
3
0
0
0
8
0
6
9
3
5
5
1
2
4
1
0
0
0
0
8
0
6
9
3
5
5
1
2
4
0
0
0
0
0
8
0
6
9
3
5
5
1
2
0
0
0
0
0
0
8
0
6
9
3
5
5
1
0
0
0
0
0
0
0
8
0
6
9
3
5
5
0
0
0
0
0
0
0
0
8
0
6
9
3
5
0
0
0
0
0
0
0
0
0
8
0
6
9
3
0
0
0
0
0
0
0
0
0
0
8
0
6
9
0
0
0
0
0
0
0
0
0
0
0
8
0
6
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Figure 7.8 Insertion sort and inversions
are sorted), and the next element in the inversion table speciŀes how many
elements are going to be moved in the next insertion, because it speciŀes the
number of larger elements to the left of the (i + 1)st element. Ļe only eﬀect
of the ith insertion on the inversion table is to zero its ith entry. Ļis implies
that the value of the quantity B when insertion sort is run on a permutation
is equal to the sum of the entries in the inversion table—the total number of
inversions in the permutation.
Exercise 7.42 How many permutations of N elements have exactly one inversion?
Two? Ļree?
Exercise 7.43 Show how to modify insertion sort to also compute the inversion table
for the permutation associated with the original ordering of the elements.
As mentioned previously, there is a one-to-one correspondence between
permutations and inversion tables. In any inversion table q1q2 . . . qN, each
entry qi must be between 0 and i −1 (in particular, q1 is always 0). Ļere are
i possible values for each of the qi, so there are N! diﬀerent inversion tables.
Inversion tables are simpler to use in the analysis because their entries are
independent: each qi takes on its i diﬀerent values independent of the values
of the other entries.

Țȟȝ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ȝ
Ļeorem 7.8 (Inversion distribution).
Ļe number of permutations of size
N with k inversions is
[uk]
∏
1≤k≤N
1 −uk
1 −u = [uk](1 + u)(1 + u + u2) · · · (1 + u + . . . + uN−1).
A random permutation of N elements has N(N −1)/4 inversions on the
average, with standard deviation N(N −1)(2N + 5)/72.
Proof. We present the derivation using PGFs; a combinatorial derivation would
follow along (almost) identical lines.
In the inversion table for a random permutation, the ith entry can take
on each value between 0 and i −1 with probability 1/i, independently of
the other entries. Ļus, the probability generating function for the number
of inversions involving the Nth element is (1 + u + u2 + . . . + uN−1)/N,
independent of the arrangement of the previous elements. As discussed in
Chapter 3, the PGF for the sum of independent random variables is the prod-
uct of the individual PGFs, so the generating function for the total number
of inversions in a random permutation of N elements satisŀes
bN(u) = 1 + u + u2 + . . . + uN−1
N
bN−1(u).
Ļat is, the number of inversions is the sum, for j from 1 to N, of independent
uniformly distributed random variables with OGF (1+u+u2+. . .+uj−1)/j.
Ļe counting GF of the theorem statement equals N! times bN(u). Ļe
average is the sum of the individual averages (j −1)/2, and the variance
is the sum of the individual variances (j2 −1)/12.
Ļe full distribution [uk]bN(u) is shown in Figure 7.9. Ļe curves are
symmetric about N(N −1)/4, and they shrink toward the center (albeit
slowly) as N grows. Ļis curve can be characterized as the distribution for
the sum of independent random variables. Ļough they are not identically
distributed, it can be shown by the classical Central Limit Ļeorem of prob-
ability theory that the distribution is normal in the limit (see, for example,
David and Barton [2]). Ļis outcome is not atypical in the analysis of algo-
rithms (see, for example, Figure 7.7); indeed, such results are common in the
BGF-based limit laws of analytic combinatorics (see [4]).

§Ȟ.ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȟȞ
0
.333
.167
0
Figure 7.9
N(N −1)/4
N(N −1)/2
Distribution of inversions, 3 ≤N ≤60 (k-axes scaled to
(N
2
))
Corollary Insertion sort performs ∼N2/4 comparisons and ∼N2/4 moves,
on the average, to sort a ŀle of N randomly ordered records with distinct
keys.
Solution with CGFs. Ļe proof of Ļeorem 7.8 calculates the cumulated cost
from the “horizontal” generating functions for inversions; we now consider an
alternative derivation that uses the CGF directly. Consider the CGF
B(z) =
∑
p∈P
inv(p)z|p|
|p|!.
As mentioned previously, the coeﬃcient of zN/N! in B(z) is the total number
of inversions in all permutations of length N, so that B(z) is the OGF for
the average number of inversions in a permutation.
Using the “largest” construction, every permutation of length |p| corre-
sponds to |p| + 1 permutations of length |p| + 1, formed by putting element
|p|+1 between the kth and (k+1)st element, for k between 0 and |p|. Such a

Țȟȟ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ȝ
permutation has |p|−k more inversions than p, which leads to the expression
B(z) =
∑
p∈P
∑
0≤k≤|p|
(inv(p) + |p| −k)
z|p|+1
(|p| + 1)!.
Ļe sum on k is easily evaluated, leaving
B(z) =
∑
p∈P
inv(p)z|p|+1
|p|! +
∑
p∈P
(
|p| + 1
2
)
z|p|+1
(|p| + 1)!.
Ļe ŀrst sum is zB(z), and the second is simple to evaluate because it depends
only on the length of the permutation, so the k! permutations of length k can
be collected for each k, leaving
B(z) = zB(z) + z
2
∑
k≥0
kzk = zB(z) + 1
2
z2
(1 −z)2 ,
so B(z) = z2/(2(1 −z)3), the GF for N(N −1)/4, as expected.
Wő ţŕŘŘ Ŏő şŠšŐťŕŚœ other properties of inversion tables later in the chapter,
since they can describe other properties of permutations that arise in the anal-
ysis of some other algorithms. In particular, we will be concerned with the
number entries in the inversion table that are at their maximum value (for
selection sort) and the value of the largest element (for insertion sort).
Exercise 7.44 Derive a recurrence relation satisŀed by pNk, the probability that a
random permutation of N elements has exactly k inversions.
Exercise 7.45 Find the CGF for the total number of inversions in all involutions of
length N. Use this to ŀnd the average number of inversions in an involution.
Exercise 7.46 Show that N!pNk is a ŀxed polynomial in N for any ŀxed k, when N
is suﬃciently large.

§Ȟ.ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȟȠ
Shellsort. Program 7.3 gives a practical improvement to insertion sort, called
shellsort, which reduces the running time well below N2 by making sev-
eral passes through the ŀle, each time sorting h independent subŀles (each
of size about N/h) of elements spaced by h. Ļe sequence of “increments”
h[t],h[t-1],...,h[1] that control the sort is usually chosen to be de-
creasing and must end in 1. Ļough it is a simple extension to insertion sort,
shellsort has proved to be extremely diﬃcult to analyze (see [16]).
In principle, mathematical analysis should guide us in choosing an in-
crement sequence, but the average-case analysis of shellsort remains an un-
solved problem, even for simple increment sequences that are widely used in
practice such as ..., 364, 121, 40, 13, 4, 1. Yao [22] has done
an analysis of (h, k, 1) shellsort using techniques similar to those we used for
insertion sort, but the results and methods become much more complicated.
For general shellsort, not even the functional form of the order of growth of
the running time is known for any practical increment sequence.
Two-ordered permutations.
Ļe analysis of shellsort for the case where h
takes on only the values 2 and 1 is interesting to consider because it is closely
related to the analysis of path length in trees of Chapter 6. Ļis is equivalent
to a merging algorithm: the ŀles in odd- and even-numbered positions are
sorted independently (with insertion sort), then the resulting permutation is
sorted with insertion sort. Such a permutation, which consists of two inter-
leaved sorted permutations, is said to be 2-ordered. Properties of 2-ordered
permutations are of interest in the study of other merging algorithms as well.
Since the ŀnal pass of shellsort, with h=1, is just insertion sort, its average
for (int k = 0; k < incs.length; k++)
{
int h = incs[k];
for (int i = h; i < N; i++)
for (int j = i; j >= h && a[j-h] > a[j]; j--)
exch(a, j, j-h);
}
Program 7.3 Shellsort

ȚȠȗ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ȝ
running time will depend on the average number of inversions in a 2-ordered
permutation. Ļree sample 2-ordered permutations, and their inversion ta-
bles, are given in Table 7.8.
Let S(z) be the OGF that enumerates 2-ordered permutations. It is
obvious that
S(z) =
∑
N≥0
(
2N
N
)
zN =
1
√1 −4z ,
but we will consider an alternative method of enumeration to expose the struc-
ture. Figure 7.10 illustrates the fact that 2-ordered permutations correspond
to paths in an N-by-N lattice, similar to those described for the “gambler’s
ruin” representation of trees in Chapter 5. Starting at the upper left corner,
move right if i is in an odd-numbered position and down if i is in an even-
numbered position. Since there are N moves to the right and N moves down,
we end up in the lower right corner.
Now, the lattice paths that do not touch the diagonal correspond to trees,
as discussed in Chapter 5, and are enumerated by the generating function
zT(z) = G(z) = (1 −
√
1 −4z )/2.
For 2-ordered permutations, the restriction on touching the diagonal is re-
moved. However, any path through the lattice must touch the diagonal for
the ŀrst time, which leads to the symbolic equation
S(z) = 2G(z)S(z) + 1
for the enumerating OGF for 2-ordered permutations. Ļat is, any path
through the lattice can be uniquely constructed from an initial portion that
4
1
5
2
6
3
9
7 10 8 13 11 15 12 16 14 19 17 20 18
0
1
0
2
0
3
0
1
0
2
0
1
0
2
0 2
0
1
0
2
1
4
2
5
3
6
8
7
9 12 10 13 11 14 17 15 18 16 19 20
0
0
1
0
2
0
0
1
0
0
1
0
2
0
0
1
0
2
0
0
4
1
5
2
6
3
7
8 12 9 13 10 14 11 15 17 16 18 20 19
0
1
0
2
0
3
0
0
0
1
0
2
0
3
0
0
1
0
0
1
Table 7.8
Ļree 2-ordered permutations, with inversion tables

§Ȟ.ȝ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȠȘ
Figure 7.10 Lattice paths for 2-ordered permutations in Table 7.8
does not touch the diagonal except at the endpoints, followed by a general
path. Ļe factor of 2 accounts for the fact that the initial portion may be
either above or below the diagonal. Ļis simpliŀes to
S(z) =
1
1 −2G(z) =
1
1 −(1 −√1 −4z ) =
1
√1 −4z ,
as expected. Knuth [10] (see also Vitter and Flajolet [19]) shows that this
same general structure can be used to write explicit expressions for the BGF
for inversions, with the eventual result that the cumulative cost (total number
of inversions in all 2-ordered permutations of length 2N) is simply N4N−1.
Ļe argument is based on the observation that the number of inversions in a
2-ordered permutation is equal to the number of lattice squares between the
corresponding lattice path and the “down-right-down-right. . .” diagonal.
Ļeorem 7.9 (Inversions in 2-ordered permutations).
Ļe average number
of inversions in a random 2-ordered permutation of length 2N is
N4N−1 / (
2N
N
)
∼
√
π/128 (2N)3/2.
Proof. Ļe calculations that lead to this simple result are straightforward but
intricate and are left as exercises. We will address this problem again, in a
more general setting, in §8.5.

ȚȠș
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ȝ
Corollary Ļe average number of comparisons used by (2, 1) shellsort on a
ŀle of N elements is N2/8 +
√
π/128 N3/2 + O(N).
Proof. Assume that N is even. Ļe ŀrst pass consists of two independent sorts
of N/2 elements and therefore involves 2((N/2)(N/2 −1)/4) = N2/8 +
O(N) comparisons, and leaves a random 2-ordered ŀle. Ļen an additional
√
π/128 N3/2 comparisons are used during the second pass.
Ļe same asymptotic result follows for the case when N is odd. Ļus,
even though it requires two passes over the ŀle, (2, 1) shellsort uses a factor
of 2 fewer comparisons than insertion sort.
Exercise 7.47 Show that the number of inversions in a 2-ordered permutation is
equal to the number of lattice squares between the path and the “down-right-down-
right. . .” diagonal.
Exercise 7.48 Let T be the set of all 2-ordered permutations, and deŀne the BGF
P(z, u) =
∑
p∈T
u{# inversions in p} z|p|
|p|! .
Deŀne Q(z, u) in the same way, but restricted to the set of 2-ordered permutations
whose corresponding lattice paths do not touch the diagonal except at the endpoints.
Moreover, deŀne S(z, u) and T(z, u) similarly, but restricted to 2-ordered permuta-
tions whose corresponding lattice paths lie entirely above the diagonal except at the
endpoints. Show that P(z, u) = 1/(1 −Q(z, u)) and S(z, u) = 1/(1 −T(z, u)).
Exercise7.49 Show that T(z, u) = uzS(uz, u) and Q(uz, u) = T(uz, u)+T(z, u).
Exercise 7.50 Using the result of the previous two exercises, show that
S(z, u) = uzS(z, u)S(uz, u) + 1
and
P(z, u) = (uzS(uz, u) + zS(z, u))P(z, u) + 1.
Exercise 7.51 Using the result of the previous exercise, show that
Pu(1, z) =
z
(1 −4z)2 .
Exercise 7.52 Give an asymptotic formula for the average number of inversions in a
3-ordered permutation, and analyze shellsort for the case when the increments are 3
and 1. Generalize to estimate the leading term of the cost of (h, 1) shellsort, and the
asymptotic cost when the best value of h is used (as a function of N).

§Ȟ.Ȟ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȠȚ
Exercise 7.53 Analyze the following sorting algorithm: given an array to be sorted,
sort the elements in odd positions and in even positions recursively, then sort the
resulting 2-ordered permutation with insertion sort. For which values of N does this
algorithm use fewer comparisons, on the average, than the pure recursive quicksort
of Chapter 1?
7.7 Left-to-RightMinimaandSelectionSort. Ļe trivial algorithm for
ŀnding the minimum element in an array is to scan through the array, from
left to right, keeping track of the minimum found so far. By successively
ŀnding the minimum, we are led to another simple sorting method called
selection sort, shown in Program 7.4. Ļe operation of selection sort on our
sample ŀle is diagrammed in Figure 7.11: again, the permutation is shown
on the left and the corresponding inversion table on the right.
Finding the minimum. To analyze selection sort, we ŀrst need to analyze
the algorithm for “ŀnding the minimum” in a random permutation: the ŀrst
(i = 0) iteration of the outer loop of Program 7.4. As for insertion sort,
the running time of this algorithm can be expressed in terms of N and a
quantity whose value depends on the particular permutation—in this case the
number of times the “current minimum” is updated (the number of exchanges
in Program 7.4 while i = 0.). Ļis is precisely the number of left-to-right
minima in the permutation.
Foata’s correspondence gives a 1-1 correspondence between left-to-right
minima and cycles, so our analysis of cycles in §5.4 tells us that the average
for (int i = 0; i < N-1; i++)
{
int min = i;
for (int j = i+1; j < N; j++)
if (a[j] < a[min]) min = j;
exch(a, i, min);
}
Program 7.4 Selection sort

ȚȠț
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȟ
 9 14  4  1 12  2 10 13  5  6 11  3  8 15  7
 1 14  4  9 12  2 10 13  5  6 11  3  8 15  7
 1  2  4  9 12 14 10 13  5  6 11  3  8 15  7
 1  2  3  9 12 14 10 13  5  6 11  4  8 15  7
 1  2  3  4 12 14 10 13  5  6 11  9  8 15  7
 1  2  3  4  5 14 10 13 12  6 11  9  8 15  7
 1  2  3  4  5  6 10 13 12 14 11  9  8 15  7
 1  2  3  4  5  6  7 13 12 14 11  9  8 15 10
 1  2  3  4  5  6  7  8 12 14 11  9 13 15 10
 1  2  3  4  5  6  7  8  9 14 11 12 13 15 10
 1  2  3  4  5  6  7  8  9 10 11 12 13 15 14
 1  2  3  4  5  6  7  8  9 10 11 12 13 15 14
 1  2  3  4  5  6  7  8  9 10 11 12 13 15 14
 1  2  3  4  5  6  7  8  9 10 11 12 13 15 14
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
 AA AB AL CN EF HF HT JB JC JG JL MC MS PD PL 
 JC PD CN AA MC AB JG MS EF HF JL AL JB PL HT
8
0
6
9
3
5
5
1
2
4
1
3
2
0
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
8
0
6
9
3
5
5
1
2
4
1
1
1
0
0
8
0
6
9
3
5
5
1
2
0
0
0
0
0
0
8
0
6
8
3
5
5
1
2
0
0
0
0
0
0
8
0
6
4
3
4
4
1
2
0
0
0
0
0
0
8
0
6
4
3
4
2
1
1
0
0
0
0
0
0
8
0
6
5
3
0
1
0
0
0
0
0
0
0
0
8
0
5
4
3
0
1
0
0
0
0
0
0
0
0
5
0
1
3
2
1
0
0
0
0
0
0
0
0
0
5
0
1
1
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Figure 7.11 Selection sort and left-to-right minima
number of left-to-right minima in a random permutation of N elements is
HN. Ļe following direct derivation is also of interest.
Ļe number of left-to-right minima is not diﬃcult to analyze with the
help of the inversion table: each entry in the inversion table q1q2 . . . qN for
which qi = i −1 corresponds to a left-to-right minimum, since all elements
to the left are larger (for example, this condition holds for q1, q3, and q4 in
the ŀrst line of the right part of Figure 7.11). Ļerefore, each entry in the
inversion table is a left-to-right minimum with probability 1/i, independent
of the other entries, so the average is ∑
1≤i≤N 1/i = HN. A slight general-
ization of this argument gives the PGF.
Ļeorem 7.10 (Left-to-right minima distribution).
Permutations of N el-
ements with k left-to-right minima are counted by the Stirling numbers of
the ŀrst kind:
[
N
k
]
= [uk]u(u + 1) . . . (u + N −1).
A random permutation of N elements has HN left-to-right minima on the
average, with variance HN −H(2)
N .

§Ȟ.Ȟ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȠȜ
Proof. Consider the probability generating function PN(u) for the number
of left-to-right minima in a random permutation of N elements. As earlier,
we can decompose this into two independent random variables: one for a
random permutation of N −1 elements (with PGF PN−1(u)) and one for
the contribution of the last element (with PGF (N −1+u)/N, since the last
element adds 1 to the number of left-to-right minima with probability 1/N,
0 otherwise). Ļus we must have
PN(u) = N −1 + u
N
PN−1(u),
and, as earlier, we ŀnd the mean and variance of the number of left-to-right
minima by summing the means and variance from the simple probability gen-
erating functions (z + k −1)/k. Ļe counting GF equals N!pN(u).
Solution with CGFs. As usual, we introduce the exponential CGF
B(z) =
∑
p∈P
lrm(p)z|p|
|p|!
so that [zN]B(z) is the average number of left-to-right minima in a random
permutation of N elements. As before, we can directly derive a functional
equation, in this case using the “last” construction. Of the |p|+1 permutations
of size |p| + 1 that we construct from a given permutation p, one of them
ends in 1 (and so has one more left-to-right minimum than p), and |p| do
not end in 1 (and so have the same number of left-to-right minima as p).
Ļis observation leads to the formulation
B(z) =
∑
p∈P
(lrm(p) + 1)
z|p|+1
(|p| + 1)! +
∑
p∈P
|p|lrm(p)
z|p|+1
(|p| + 1)!
=
∑
p∈P
lrm(p)z|p|+1
|p|! +
∑
p∈P
z|p|+1
(|p| + 1)!
= zB(z) +
∑
k≥0
zk+1
(k + 1) = zB(z) + ln
1
1 −z ,
which leads to the solution
B(z) =
1
1 −z ln
1
1 −z ,

ȚȠȝ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȟ
the generating function for the harmonic numbers, as expected. Ļis deriva-
tion can be extended, with just slightly more work, to give an explicit expres-
sion for the exponential BGF describing the full distribution.
Stirlingnumbersoftheŀrstkind. Continuing this discussion, we start with
B(z, u) =
∑
p∈P
z|p|
|p|!ulrm(p) =
∑
N≥0
∑
k≥0
pNkzNuk
where pNk is the probability that a random permutation of N elements has
k left-to-right minima. Ļe same combinatorial construction as used earlier
leads to the formulation
B(z, u) =
∑
p∈P
z|p|+1
(|p| + 1)!ulrm(p)+1 +
∑
p∈P
z|p|+1
(|p| + 1)!|p|ulrm(p).
Diﬀerentiating with respect to z, we have
Bz(z, u) =
∑
p∈P
z|p|
|p|!ulrm(p)+1 +
∑
p∈P
z|p|
(|p| −1)!ulrm(p)
= uB(z, u) + zBz(z, u).
Solving for Bz(z, u), we get a simple ŀrst-order diﬀerential equation
Bz(z, u) =
u
1 −z B(z, u),
which has the solution
B(z, u) =
1
(1 −z)u
(since B(0, 0) = 1). Diﬀerentiating with respect to u, then evaluating at
u = 1 gives the OGF for the harmonic numbers, as expected. Expanding
B(z, u) = 1 + u
1!z + u(u + 1)
2!
z2 + u(u + 1)(u + 2)
3!
z3 + . . .
gives back the expression for the Stirling numbers of the ŀrst kind in the
statement of Ļeorem 7.10.

§Ȟ.Ȟ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȠȞ
Ļe BGF B(z, u) = (1 −z)−u is a classical one that we saw in §5.4.
As we know from Foata’s correspondence, the number of permutations of N
elements with exactly k left-to-right minima is the same as the number of
permutations of N elements with exactly k cycles. Both are counted by the
Stirling numbers of the ŀrst kind, which are therefore sometimes called the
Stirling “cycle” numbers. Ļis distribution is OEIS A130534 [18], and is
shown in Figure 7.12.
Selection sort. Ļe leading term in the running time of selection sort is the
number of comparisons, which is (N + 1)N/2 for every input permutation,
and the number of exchanges is N −1 for every input permutation. Ļe
only quantity whose value is dependent on the input in the running time of
Program 7.4 is the total number of left-to-right minima encountered during
the sort: the number of times the if statement succeeds.
Ļeorem 7.11 (Selection sort).
Selection sort performs ∼N2/2 compar-
isons, ∼NlnN minimum updates, and moves ∼N moves, on the average, to
sort a ŀle of N records with randomly ordered distinct keys.
Proof. See the preceding discussion for comparisons and exchanges. It remains
to analyze BN, the expected value of the total number of left-to-right minima
encountered during the sort for a random permutation of N elements. In
Figure 7.12, it is obvious that the leftmost i elements of the inversion table
are zero after the ith step, but the eﬀect on the rest of the inversion table is
more diﬃcult to explain. Ļe reason for this is that the passes in selection sort
are not independent: after we complete one pass, the part of the permutation
that we process in the next pass is very similar (certainly not random), as it
diﬀers only in one position, where we exchanged away the minimum.
We can use the following construction to ŀnd BN: given a permutation
p of N elements, increment each element and prepend 1 to construct a per-
mutation of N + 1 elements, then construct N additional permutations by
exchanging the 1 with each of the other elements. Now, if any of these N +1
permutations is the initial input to the selection sort algorithm, the result will
be equivalent to p for subsequent iterations. Ļis correspondence implies that
BN = BN−1 + HN = (N + 1)HN −N.

ȚȠȟ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȟ
N
↓k →
1
2
3
4
5
6
7
8
9
10
1
1
2
1
1
3
2
3
1
4
6
11
6
1
5
24
50
35
10
1
6
120
274
225
85
15
1
7
720
1764
1624
735
175
21
1
8
5040
13,068
13,132
6769
1960
322
28
1
9
40,320
109,584
118,124
67,284
22,449
4536
546
36
1
10
362,880 1,026,576 1,172,700 723,680 269,325 63,273 9450 870 45
1
0
.5
.333
.167
0
Figure 7.12
N
Distribution of left-to-right minima and cycles
(Stirling numbers of the ŀrst kind)

§Ȟ.Ȟ
P ő Ş ř š Š ō Š ŕ ś Ś ş
ȚȠȠ
More speciŀcally, let cost(p) denote the total number of left-to-right minima
encountered during the sort for a given permutation p, and consider the CGF
B(z) =
∑
N≥0
BNzN =
∑
p∈P
cost(p)z|p|
|p|!.
Ļe construction deŀned above says that the algorithm has uniform behavior
in the sense that each permutation costs lrm(p) for the ŀrst pass; then, if we
consider the result of the ŀrst pass (applied to all |p|! possible inputs), each
permutation of size |p| −1 appears the same number of times. Ļis leads to
the solution
B(z) =
∑
p∈P
lrm(p)z|p|
|p|! +
∑
p∈P
(|p| + 1)cost(p)
z|p|+1
(|p| + 1)!
=
1
1 −z ln
1
1 −z + zB(z).
Ļerefore,
B(z) =
1
(1 −z)2 ln
1
1 −z ,
which is the generating function for partial sums of the harmonic numbers.
Ļus BN = (N+1)(HN+1−1) as in Ļeorem 3.4 and therefore BN ∼NlnN,
completing the proof.
Ļis proof does not extend to yield the variance or other properties of this
distribution. Ļis is a subtle but important point. For left-to-right minima
and other problems, we are able to transform the CGF derivation easily into
a derivation for the BGF (which yields the variance), but the above argument
does not extend in this way because the behavior of the algorithm on one pass
may provide information about the next pass (for example, a large number of
left-to-right minima on the ŀrst pass would imply a large number of left-to-
right minima on the second pass). Ļe lack of independence seems to make
this problem nearly intractable: it remained open until 1988, when a delicate
analysis by Yao [21] showed the variance to be O(N3/2).
Exercise 7.54 Let pNk be the probability that a random permutation of N elements
has k left-to-right minima. Give a recurrence relation satisŀed by pNk.

țȗȗ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ȟ
Exercise 7.55 Prove directly that ∑
k k
[N
k
]
= N!HN.
Exercise 7.56 Specify and analyze an algorithm that determines, in a left-to-right
scan, the two smallest elements in an array.
Exercise 7.57 Consider a situation where the cost of accessing records is 100 times
the cost of accessing keys, and both are large by comparison with other costs. For
which values of N is selection sort preferred over insertion sort?
Exercise 7.58 Answer the previous question for quicksort versus selection sort, as-
suming that an “exchange” costs twice as much as a “record access.”
Exercise 7.59 Consider an implementation of selection sort for linked lists, where
on each interation, the smallest remaining element is found by scanning the “input”
list, but then it is removed from that list and appended to an “output” list. Analyze
this algorithm.
Exercise 7.60 Suppose that the N items to be sorted actually consist of arrays of
N words, the ŀrst of which is the sort key. Which of the four comparison-based
methods that we have seen so far (quicksort, mergesort, insertion sort, and selection
sort) adapts best to this situation? What is the complexity of this problem, in terms
of the amount of input data?

§Ȟ.ȟ
P ő Ş ř š Š ō Š ŕ ś Ś ş
țȗȘ
7.8 Cycles and In Situ Permutation. In some situations, an array might
need to be permuted “in place.” As described in §7.2, a sorting program
can be organized to refer to records indirectly, computing a permutation that
speciŀes how to do the arrangement instead of actually rearranging them.
Here, we consider how the rearrangement might be done in place, in a second
phase. Here is an example:
index
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
input keys
CN HF MC AL JC JG PL MS AA HT JL EF JB AB PD
permutation
9
14
4
1
12
2
10 13
5
6
11
3
8
15
7
Ļat is, to put the array in sorted order, a[9] has to be moved to position
1, a[14] to position 2, a[4] to position 3, and so on. One way to do this is
to start by saving a[1] in a register, replace it by a[p[1]], set j to p[1], and
continue until p[j] becomes 1, when a[j] can be set to the saved value. Ļis
process is then repeated for each element not yet moved, but if we permute
the p array in the same way, then we can easily identify elements that need to
be moved, as in Program 7.5.
In the example above, ŀrst a[9] = AA is moved to position 1, then
a[5] = JC is moved to position 9, then a[12] = EF is moved to position
5, then a[3] = MC is moved to position 12, then a[4] = AL is moved to
for (int i = 1; i <= N; i++)
if (p[i] != i)
{
int j, t = a[i], k = i;
do
{
j = k; a[j] = a[p[j]];
k = p[j]; p[j] = j;
} while (k != i);
a[j] = t;
}
Program 7.5 In situ permutation

țȗș
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ȟ
position 3, then CN, which was originally in position 1, is put in position 4.
At the same time, p[1] is set to 1, p[9] is set to 9, and so on, to rełect in
the permutation the fact that all of these elements are now in place. Ļen,
the program increments i to ŀnd elements to move, ignoring elements that
are already in place, with p[i] = i. Ļis program moves each element at
most once.
Clearly, the running time of Program 7.5 depends on the cycle structure
of the permutation—the program itself is actually a succinct demonstration
of the concept of cycles. Note also that every item that is on a cycle of length
greater than 1 is moved precisely once, so the number of data moves is N
minus the number of singleton cycles. We also need to know the number of
cycles, since the overhead of the inner loop is incurred once for every cycle.
Now, we know from §5.4 that the average number of cycles is HN and we
know the full distribution from our earlier analysis of left-to-right minima,
by Foata’s correspondence (see §7.1), but a direct derivation is also instructive.
Ļeorem 7.12 (Cycle distribution).
Ļe distribution of the number of cy-
cles in a random permutation is the same as the distribution of the number of
left-to-right minima, and is given by the Stirling numbers of the ŀrst kind.
Ļe average number of cycles in a random permutation is HN, with standard
deviation HN −H(2)
N .
Proof. As usual, we can directly derive a functional equation for the bivariate
generating function
B(z, u) =
∑
p∈P
z|p|ucyc(p)
where cyc(p) is the number of cycles in p. from a combinatorial construction.
Given a permutation p, we can create |p| + 1 permutations of size |p| + 1 by
adding element |p| + 1 at every position in every cycle (including the “null”
cycle). Of these permutations, one has one more cycle than p and |p| have
the same number of cycles as p. Ļis correspondence is structurally the same
as the correspondence that we established for left-to-right minima. Using
precisely the same argument as given previously, we of course ŀnd that the
generating function is identical to the generating function for the number of
left-to-right minima:
B(z, u) =
1
(1 −z)u .

§Ȟ.ȟ
P ő Ş ř š Š ō Š ŕ ś Ś ş
țȗȚ
Ļerefore the distributions are identical: the average is as stated; the number
of permutations of N elements with exactly k cycles is given by the Stirling
numbers of the ŀrst kind, and so on.
Cycle lengths. To analyze Program 7.5 in detail, we have noted that we need
to know the number of singleton cycles. We know from the enumeration re-
sults on permutations with cycle length restrictions in §7.4 that the probability
that a random permutation has at least one singleton cycle is 1−1/e, but how
many might we expect to ŀnd? It turns out that the number of singleton cy-
cles in a random permutation is 1, on the average, and that we can analyze
other facts about the cycle length distribution.
Singleton cycles. For comparison with other derivations, we ŀnd the average
number of singleton cycles using the combinatorial construction given earlier
and the CGF
B(z) =
∑
p∈P
cyc1(p)z|p|
|p|!
where cyc1(p) is the number of singleton cycles in a permutation p, so that the
desired answer is [zN]B(z). By the earlier construction, the permutations of
length |p| can be grouped into groups of size |p|, each of which corresponds
to a permutation q of size |p| −1. In the group corresponding to q, one has
cyc1(q) + 1 singleton cycles, cyc1(q) have cyc1(q) −1 singleton cycles, and
the rest have cyc1(q) singleton cycles, for a total of
cyc1(q)+1+cyc1(q)(cyc1(q)−1)+(|p|−1−cyc1(q))cyc1(q) = 1+|q|cyc1(q)
singleton cycles for all of the permutations corresponding to q. Ļerefore,
B(z) =
∑
q∈P
(1 + |q|cyc1(q))
z|q|+1
(|q| + 1)!.
Diﬀerentiating this formula leads to the simple form
B′(z) =
∑
q∈P
z|q|
|q|! +
∑
q∈P
|q|cyc1(q)z|q|
|q|!
=
1
1 −z + zB′(z),
so B′(z) = 1/(1 −z)2 and B(z) = 1/(1 −z), as expected.

țȗț
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.ȟ
Cycles of length k. Ļe symbolic method for parameters gives average num-
ber of cycles of length k in a random permutation. By adapting the arguments
in §7.4, we can write down the (exponential) BGF for the number of cycles
of length k:
exp
(
z + z2
2 + z3
3 + . . . + zk−1
k −1 + zk
k u + zk+1
k + 1 + . . .
)
.
When this is expanded, each term represents a permutation, where the expo-
nent of u counts the number of times the term zk/k is used, or the number
of cycles of length k in the corresponding permutation. Now, the BGF can
be rewritten in the form
exp
(
z + z2
2 + . . . + zk
k + . . .
)
exp
(
(u −1)zk
k
)
=
1
1 −z exp((u −1)zk/k).
Ļis form allows calculation of the quantities of interest.
Ļeorem 7.13 (Singleton cycle distribution).
Ļe probability that a per-
mutation of N elements has j singleton cycles is asymptotic to e−1/j!. Ļe
average number of cycles of length k in a random permutation of size N ≥k
is 1/k, with variance 1/k.
Proof. Ļe probability sought is given by the coeﬃcients of the BGF derived
in the above discussion:
[ujzN]e(u−1)z
1 −z = 1
j![zN−j] e−z
1 −z
∼e−1
j! ,
by Ļeorem 7.3.
Ļe computation of the average is a simple application of Ļeorem 3.6:
diﬀerentiating with respect to u and evaluating at 1 gives
[zN]zk
k
1
1 −z = 1
k
for N ≥k,
and the variance follows from a similar calculation.

§Ȟ.ȟ
P ő Ş ř š Š ō Š ŕ ś Ś ş
țȗȜ
We might also sum on j to generalize Ļeorem 7.11 and derive the result
that the average number of cycles of length ≤k in a random permutation of
N elements is Hk. Of course, this result holds only as long as k is not larger
than N.
Exercise 7.61 Use asymptotics from generating functions (see §5.5) or a direct ar-
gument to show that the probability for a random permutation to have j cycles of
length k is asymptotic to the Poisson distribution e−λλj/j! with λ = 1/k.
Exercise 7.62 For a permutation of length 100, what is the probablility that the loop
in Program 7.5 never iterates more than 50 times?
Exercise 7.63 [Knuth] Consider a situation where the permutation array cannot be
modiŀed and no other extra memory is available. An algorithm to perform in situ
permutation can be developed as follows: the elements in each cycle will be permuted
when the smallest index in the cycle is encountered. For j from 1 to N, test each index
to see if it is the smallest in the cycle by starting with k = j and setting k = p[k]
while k > j. If it is, then permute the cycle as in Program 7.5. Show that the BGF
for the number of times this k = p[k] instruction is executed satisŀes the functional
equation
Bu(z, u) = B(z, u)B(z, zu).
From this, ŀnd the mean and variance for this parameter of random permutations.
(See [12].)

țȗȝ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ƞ
7.9 Extremal Parameters. In Chapter 5, we found that tree height was
much more diﬃcult to analyze than path length because calculating the height
involves taking the maximum subtree values, whereas path length involves
just enumeration and addition, and the latter operations correspond more
naturally to operations on generating functions. In this section, we consider
analogous parameters on permutations. What is the average length of the
longest or shortest cycle in a permutation? What is the average length of the
longest run? Ļe longest increasing subsequence? What is the average value
of the largest element in the inversion table of a random permutation? Ļis
last question arises in the analysis of yet another elementary sorting algorithm,
to which we now turn.
Bubble sort. Ļis method is simple to explain: to sort an array, pass through
it repeatedly, exchanging each element with the next to put them in order,
if necessary. When a pass through the array is completed without any ex-
changes (each element is not larger than the next), the sort is completed. An
implementation is given in Program 7.6. To analyze this algorithm, we need
to count the exchanges and the passes.
Exchanges are straightforward: each exchange is with an adjacent ele-
ment (as in insertion sort), and so removes exactly one inversion, so the total
number of exchanges is exactly the number of inversions in the permutation.
Ļe number of passes used is also directly related to the inversion table, as
shown in Figure 7.14: each pass actually reduces each nonzero entry in the
inversion table by 1, and the algorithm terminates when there are no more
nonzero entries. Ļis implies immediately that the number of passes required
to bubble sort a permutation is precisely equal to the largest element in the
inversion table. Ļe distribution of this quantity is shown in Figure 7.15. Ļe
sequence is OEIS A056151 [18].
for (int i = N-1; i > 1; i--)
for (int j = 1; j <= i; j++)
if (a[j-1] > a[j]) exch(a, j-1, j);
Program 7.6 Bubble sort

§Ȟ.Ƞ
P ő Ş ř š Š ō Š ŕ ś Ś ş
țȗȞ
8
0
6
9
3
5
5
1
2
4
1
3
2
0
0
0
7
0
5
8
2
4
4
0
1
3
0
2
1
0
0
0
6
0
4
7
1
3
3
0
0
2
0
1
0
0
0
0
5
0
3
6
0
2
2
0
0
1
0
0
0
0
0
0
4
0
2
5
0
1
1
0
0
0
0
0
0
0
0
0
3
0
1
4
0
0
0
0
0
0
0
0
0
0
0
0
2
0
0
3
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
 9 14  4  1 12  2 10 13  5  6 11  3  8 15  7
 9  4  1 12  2 10 13  5  6 11  3  8 14  7 15
 4  1  9  2 10 12  5  6 11  3  8 13  7 14 15
 1  4  2  9 10  5  6 11  3  8 12  7 13 14 15
 1  2  4  9  5  6 10  3  8 11  7 12 13 14 15
 1  2  4  5  6  9  3  8 10  7 11 12 13 14 15
 1  2  4  5  6  3  8  9  7 10 11 12 13 14 15
 1  2  4  5  3  6  8  7  9 10 11 12 13 14 15
 1  2  4  3  5  6  7  8  9 10 11 12 13 14 15
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
AA AB AL CN EF HF HT JB JC JG JL MC MS PD PL 
JC PD CN AA MC AB JG MS EF HF JL AL JB PL HT
Figure 7.13 Bubble sort (permutation and associated inversion table)
Ļeorem 7.14 (Maximum inversion table entry).
Ļe largest element in
the inversion table of a random permutation has mean value ∼N −
√
πN/2.
Proof. Ļe number of inversion tables of length N with all entries less than
k is simply k!kN−k since the ith entry can be anything between 0 and i −1
for i ≤k and anything between 0 and k −1 for i > k. Ļus, the probability
that the maximum entry is less than k is k!kN−k/N!, and the average value
sought is
∑
0≤k≤N
(
1 −k!kN−k
N!
)
.
Ļe second term in this sum is the “Ramanujan P-function” whose asymp-
totic value is given in Table 4.11.
Corollary Bubble sort performs ∼N2/2 comparisons and ∼N2/2 moves (in
∼N −
√πN/2 passes), on the average, to sort a ŀle of N randomly ordered
records distinct keys.
Proof. See above discussion.
Exercise 7.64 Consider a modiŀcation of bubble sort where the passes through the
array alternate in direction (right to left, then left to right). What is the eﬀect of two
such passes on the inversion table?

țȗȟ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
§Ȟ.Ƞ
N
↓k →0
1
2
3
4
5
6
7
8
9
1
1
2
1
1
3
1
3
2
4
1
7
10
6
5
1
15
38
42
24
6
1
31
130
222
216
120
7
1
63
422
1050
1464
1320
720
8
1 127
1330
4686
8856
10920
9360
5040
9
1 255
4118 20,202
50,424
80,520
91,440
75,600
40,320
10
1 511 12610 85,182 27,6696 558,120 795,600 851,760 685,440 362,880
0
.5
.333
.167
0
Figure 7.14
N
Distribution of maximum inversion table entry

§Ȟ.Ƞ
P ő Ş ř š Š ō Š ŕ ś Ś ş
țȗȠ
Longest and shortest cycles. What is the average length of the longest cycle
in a permutation? We can immediately write down an expression for this.
Earlier in this chapter, we derived the exponential GFs that enumerate per-
mutations with no cycle of length > k (see Ļeorem 7.2):
ez = 1 + z + z2
2! + z3
3!
+
z4
4!
+
z5
5!
+
z6
6! + . . .
ez+z2/2 = 1 + z + 2z2
2! + 4z3
3!
+ 10z4
4! + 26z5
5! + 76z6
6! + . . .
ez+z2/2+z3/3 = 1 + z + 2z2
2! + 6z3
3!
+ 18z4
4! + 66z5
5! + 276z6
6! + . . .
ez+z2/2+z3/3+z4/4 = 1 + z + 2z2
2! + 6z3
3! + 24 z4
4 <! + 96z5
5! + 456z6
6! + . . .
...
e−ln(1−z) =
1
1 −z = 1 + z + 2z2
2! + 6z3
3!
+ 24z4
4! + 120z5
5! + 720z6
6! + . . . .
From these, we can write down the generating functions for the permutations
with at least one cycle of length > k, or, equivalently, those for which the
maximum cycle length is > k:
1
1 −z −e0 =
z+ 2z2
2! + 6z3
3! + 24z4
4! + 120z5
5! + 720z6
6! + . . .
1
1 −z −ez =
z2
2! + 5z3
3! + 23z4
4! + 119z5
5! + 719z6
6! + . . .
1
1 −z −ez+z2/2 =
2z3
3! + 14z4
4! + 94z5
5! + 644z6
6! + . . .
1
1 −z −ez+z2/2+z3/3 =
6z4
4! + 54z5
5! + 444z6
6! + . . .
1
1 −z −ez+z2/2+z3/3+z4/4 =
24z5
5! + 264z6
6! + . . .
...

țȘȗ
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
From Table 3.6, the average length of the longest cycle in a random permu-
tation is found by summing these and may be expressed as follows:
[zN]
∑
k≥0
(
1
1 −z −ez+z2/2+z3/3+...+zk/k)
.
As is typical for extremal parameters, derivation of an asymptotic result from
this information is rather intricate. We can compute the exact value of this
quantity for small N by summing the equations to get the initial terms of the
exponential CGF for the length of the longest cycle:
1z1
1! + 3z2
2! + 13z3
3! + 67z4
4! + 411z5
5! + . . . .
It turns out that the length of the longest cycle in a random permutation is
∼λN where λ ≈.62433 · · · . Ļis result was ŀrst derived by Golomb, Shepp,
and Lloyd in 1966 [17]. Ļe sequence is OEIS A028418 [18].
Exercise 7.65 Find the average length of the shortest cycle in a random permutation
of length N, for all N < 10. (Note: Shepp and Lloyd show this quantity to be
∼e−γlnN, where γ is Euler’s constant.)
P
ERMUTATIONS are well studied as fundamental combinatorial ob-
jects, and we would expect that knowledge of their properties could help
in the understanding of the performance characteristics of sorting algorithms.
Ļe direct correspondence between fundamental properties such as cycles and
inversions and fundamental algorithms such as insertion sort, selection sort,
and bubble sort conŀrms this expectation.
Research on new sorting algorithms and the analysis of their perfor-
mance is quite active. Variants on sorting such as priority queues, merging
algorithms, and sorting “networks” continue to be of practical interest. New
types of computers and new applications demand new methods and better
understanding of old ones, and the kind of analysis outlined in this chapter is
an essential ingredient in designing and using such algorithms.
As suggested throughout this chapter, general tools are available [4] that
can answer many of the more complicated questions raised in this chapter. We
have emphasized the use of cumulative generating functions to analyze prop-
erties of permutations because they provide a straightforward “systematic”

P ő Ş ř š Š ō Š ŕ ś Ś ş
țȘȘ
path to the average value of quantities of interest. For analysis of properties
of permutations, the cumulative approach can often yield results in a sim-
pler, more direct manner than available with recurrences or BGFs. As usual,
extremal parameters (those deŀned by a “maximum” or “minimum” rule as
opposed to an additive rule) are more diﬃcult to analyze, though “vertical”
GFs can be used to compute small values and to start the analysis.
Despite the simplicity of the permutation as a combinatorial object, the
wealth of analytic questions to be addressed is often quite surprising to the
uninitiated. Ļe fact that we can use a standard methodology to answer ba-
sic questions about the properties of permutations is encouraging, not only
because many of these questions arise in important applications, but also
because we can hope to be able to study more complicated combinatorial
structures as well.

țȘș
C Ŕ ō Ŝ Š ő Ş
S ő Ţ ő Ś
References
1. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
2. F. N. DōŢŕŐ ōŚŐ D. E. BōŞŠśŚ. Combinatorial Chance, Charles Griﬃn,
London, 1962.
3. W. FőŘŘőŞ. An Introduction to Probability Ļeory and Its Applications,
John Wiley, New York, 1957.
4. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
5. G. H. GśŚŚőŠ ōŚŐ R. BōőŦō-YōŠőş. Handbook of Algorithms and Data
Structures in Pascal and C, 2nd edition, Addison-Wesley, Reading, MA,
1991.
6. I. GśšŘŐőŚ ōŚŐ D. JōŏŗşśŚ. Combinatorial Enumeration, John Wiley,
New York, 1983.
7. R. L. GŞōŔōř, D. E. KŚšŠŔ, ōŚŐ O. PōŠōşŔŚŕŗ. Concrete Mathemat-
ics, 1st edition, Addison-Wesley, Reading, MA, 1989. Second edition,
1994.
8. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 1: Fundamen-
tal Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1968. Ļird
edition, 1997.
9. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 2: Seminumerical
Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1969. Ļird
edition, 1997.
10. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.
11. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 4A: Combina-
torial Algorithms, Part 1, Addison-Wesley, Boston, MA, 2011.
12. D. E. KŚšŠŔ. “Mathematical Analysis of Algorithms,” Information Pro-
cessing 71, Proceedings of the IFIP Congress, Ljubljana, 1971, 19–27.
13. V. LŕŒşŏŔŕŠŦ ōŚŐ B. PŕŠŠőŘ. “Ļe number of increasing subsequences
of the random permutation,” Journal of Combinatorial Ļeory (Series A)
31, 1981, 1–20.
14. B. F. LśœōŚ ōŚŐ L. A. SŔőŜŜ. “A variational problem from random
Young tableaux,” Advances in Mathematics 26, 1977, 206–222.

P ő Ş ř š Š ō Š ŕ ś Ś ş
țȘȚ
15. R. SőŐœőţŕŏŗ. “Analysis of shellsort and related algorithms,” European
Symposium on Algorithms, 1986.
16. R. SőŐœőţŕŏŗ ōŚŐ K. WōťŚő. Algorithms, 4th edition, Addison-Wesley,
Boston, 2011.
17. L. SŔőŜŜ ōŚŐ S. P. LŘśťŐ. “Ordered cycle lengths in a random per-
mutation,” Transactions of the American Mathematical Society 121, 1966,
340–357.
18. N. SŘśōŚő ōŚŐ S. PŘśšŒŒő. Ļe Encyclopedia of Integer Sequences, Aca-
demic Press, San Diego, 1995. Also accessible as On-Line Encyclopedia
of Integer Sequences, http://oeis.org.
19. J. S. VŕŠŠőŞ ōŚŐ P. FŘōŖśŘőŠ. “Analysis of algorithms and data struc-
tures,” in Handbook of Ļeoretical Computer Science A: Algorithms and Com-
plexity, J. van Leeuwen, ed., Elsevier, Amsterdam, 1990, 431–524.
20. J. VšŕŘŘőřŕŚ. “A unifying look at data structures,” Communications of
the ACM 23, 1980, 229–239.
21. A. Yōś. “An analysis of (h, k, 1) shellsort,” Journal of Algorithms 1, 1980,
14–50.
22. A. Yōś. “On straight selection sort,” Technical report CS-TR-185-88,
Princeton University, 1988.

This page intentionally left blank 

C H A P T E R E I G H T
S T R I N G S A N D T R I E S
S
EQUENCES of characters or letters drawn from a ŀxed alphabet are
called strings. Algorithms that process strings range from fundamental
methods at the heart of the theory of computation to practical text-processing
methods with a host of important applications. In this chapter, we study
basic combinatorial properties of strings, some fundamental algorithms for
searching for patterns in strings, and related data structures.
We use the term bitstring to refer to strings comprised of just two char-
acters; if the alphabet is of size M > 2, we refer to the strings as bytestrings,
or words, or M-ary strings. In this chapter, we assume that M is a small
ŀxed constant, a reasonable assumption given our interest in text- and bit-
processing algorithms. If M can grow to be large (for example, increasing
with the length of the string), then we have a somewhat diﬀerent combina-
torial object, an important distinction that is one of the main subjects of the
next chapter. In this chapter, our primary interest is in potentially long strings
made from constant-size alphabets, and in their properties as sequences.
From an algorithmic point of view, not much generality is lost by fo-
cusing on bitstrings rather than on bytestrings: a string built from a larger
alphabet corresponds to a bitstring built by encoding the individual charac-
ters in binary. Conversely, when an algorithm, data structure, or analysis of
strings built from a larger alphabet depends in some way on the size of the
alphabet, that same dependence can be rełected in a bitstring by considering
the bits in blocks. Ļis particular correspondence between M-ary strings and
bitstrings is exact when M is a power of 2. It is also often very easy to gener-
alize an algorithm or analysis from bitstrings to M-ary strings (essentially by
changing “2” to “M” throughout), so we do so when appropriate.
Random bitstrings correspond precisely to sequences of independent
Bernoulli trials, which are well studied in classical probability theory; in the
analysis of algorithms such results are of interest because many algorithms
naturally depend explicitly on properties of binary strings. We review some
relevant classical results in this chapter and the next. As we have been doing
for trees and permutations in the previous two chapters, we consider the prob-
țȘȜ

țȘȝ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ș
lems from a computational standpoint and use generating functions as tools
for combinatorial analysis. Ļis approach yields simple solutions to some clas-
sical problems and gives a very general framework within which a surprising
range of problems can be considered.
We consider basic algorithms for searching for the occurrence of a ŀxed
pattern in a given string, which are best described in terms of pattern-speciŀc
ŀnite-state automata (FSAs). Not only do FSAs lead to uniform, compact
and eﬃcient implementations, but also it turns out that the automata corre-
spond precisely to generating functions associated with the patterns. In this
chapter, we study some examples of this in detail.
Certain computational tasks require that we manipulate sets of strings.
Sets of strings (generally, inŀnite sets) are called languages and are the basis
of an extensive theory of fundamental importance in computer science. Lan-
guages are classiŀed according to the diﬃculty of describing their constituent
strings. In the present context, we will be most concerned with regular lan-
guages and context-free languages, which describe many interesting combina-
torial structures. In this chapter, we revisit the symbolic method to illustrate
the utility of generating functions in analyzing properties of languages. Re-
markably, generating functions for both regular and context-free languages
can be fully characterized and shown to be essentially diﬀerent in nature.
A data structure called the trie is the basis for numerous eﬃcient algo-
rithms that process strings and sets of strings. Tries are treelike objects with
structure determined by values in a set of strings. Ļe trie is a combinatorial
object with a wealth of interesting properties. Not found in classical combina-
torics, it is the quintessential example of a new combinatorial object brought
to the ŀeld by the analysis of algorithms. In this chapter, we look at basic trie
algorithms, properties of tries, and associated generating functions. Not only
are tries useful in a wide range of applications, but also their analysis exhibits
and motivates many important tools for the analysis of algorithms.
8.1 String Searching. We begin by considering a basic algorithm for
“string searching:” given a pattern of length P and some text of length N,
look for occurrences of the pattern in the text. Program 8.1 gives the straight-
forward solution to this problem. For each position in the text, the program
checks if there is a match by comparing the text, starting at this position,
character-by-character with the pattern, starting at the beginning. Ļe pro-
gram assumes that two diﬀerent sentinel characters are used, one at the end

§ȟ.Ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȘȞ
of the pattern (the (P + 1)st pattern character) and one at the end of the text
(the (N +1)st text character). Ļus, all string comparisons end on a character
mismatch, and we tell whether the pattern was present in the text simply by
checking whether the sentinel(s) caused the mismatch.
Depending on the application, one of a number of diﬀerent variations
of the basic algorithm might be of interest:
• Stop when the ŀrst match is found.
• Print out the position of all matches.
• Count the number of matches.
• Find the longest match.
Ļe basic implementation given in Program 8.1 is easy to adapt to imple-
ment such variations, and it is a reasonable general-purpose method in many
contexts.
Still, it is worthwhile to consider improvements. For example, we could
search for a string of P consecutive 0s (a run) by maintaining a counter and
scanning through the text, resetting the counter when a 1 is encountered, in-
crementing it when a 0 is encountered, and stopping when the counter reaches
public static int search(char[] pattern, char[] text)
{
int P = pattern.length;
int N = text.length;
for (int i = 0; i <= N - P; i++)
{
int j;
for (j = 0; j < P; j++)
if (text[i+j] != pattern[j]) break;
if (j == P) return i;
// Found at offset i.
}
return N;
// Not found.
}
Program 8.1 Basic method for string searching

țȘȟ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ș
P. By contrast, consider the action of Program 8.1 when searching for a string
of P consecutive zeros and it encounters a small run of, say, ŀve 0s followed
by a 1. It examines all ŀve 0s, determines there is a mismatch on ŀnding the
1, then increments the text pointer just by one so that it ŀnds a mismatch by
checking four 0s and a 1, then checks three 0s and a 1, and so on. Ļe pro-
gram ends up checking k(k + 1)/2 extra bits for every run of k 0s. Later in
the chapter, we will examine improvements to the basic algorithm that avoid
such rechecking. For the moment, our interest is to examine how to analyze
the basic method.
Analysis of “all matches” variant. We are interested in ŀnding the average
running time of Program 8.1 when searching for a given pattern in random
text. Clearly, the running time is proportional to the number of characters
examined during the search. Since each string comparison ends on a mis-
match, its cost is 1 plus the number of characters that match the pattern at
that text position. Table 8.1 shows this for each of the patterns of four bits
0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1
total
0000
1 0 0 0 1 0 3 2 1 0 4 4 3 2 1 0 2 1 0 4 4 3 2 1 0 0
39
0001
1 0 0 0 1 0 4 2 1 0 3 3 4 2 1 0 2 1 0 3 3 4 2 1 0 0
39
0010
1 0 0 0 1 0 2 4 1 0 2 2 2 4 1 0 4 1 0 2 2 2 3 1 0 0
35
0011
1 0 0 0 1 0 2 3 1 0 2 2 2 3 1 0 3 1 0 2 2 2 4 1 0 0
33
0100
2 0 0 0 4 0 1 1 4 0 1 1 1 1 4 0 1 4 0 1 1 1 1 2 0 0
31
0101
2 0 0 0 3 0 1 1 3 0 1 1 1 1 3 0 1 3 0 1 1 1 1 2 0 0
27
0110
3 0 0 0 2 0 1 1 2 0 1 1 1 1 2 0 1 2 0 1 1 1 1 3 0 0
22
0111
4 0 0 0 2 0 1 1 2 0 1 1 1 1 2 0 1 2 0 1 1 1 1 3 0 0
25
1000
0 1 1 2 0 4 0 0 0 4 0 0 0 0 0 3 0 0 4 0 0 0 0 0 1 1
21
1001
0 1 1 2 0 3 0 0 0 3 0 0 0 0 0 4 0 0 3 0 0 0 0 0 1 1
19
1010
0 1 1 2 0 2 0 0 0 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 1 1
14
1011
0 1 1 2 0 2 0 0 0 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 1 1
14
1100
0 2 3 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 2 1
13
1101
0 2 4 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 2 1
14
1110
0 4 2 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 2 1
14
1111
0 3 2 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 2 1
13
Table 8.1
Cost of searching for 4-bit patterns (basic method)

§ȟ.Ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȘȠ
in a sample text string. With each position in the text string we associate an
integer—the number of character positions that match the pattern, starting
at that text position. Ļe total of these plus N (for the mismatches) is the
number of times the inner loop in Program 8.1 is iterated, clearly the domi-
nant term in the running time of the program. Using the cumulative method,
we can calculate this with a simple counting argument.
Ļeorem 8.1 (Pattern occurrence enumeration).
Ļe expected number of
occurrences of an arbitrary ŀxed pattern of length P in a random bitstring of
length N is (N −P + 1)/2P .
Proof. Using cumulative counting, we count the total number of occurrences
of the pattern in all of the 2N bitstrings of N bits. Ļe P bits can start in any
one of N −P +1 positions, and, for each position, there are 2N−P bitstrings
with the pattern at that position. Collecting all these terms gives the total
(N −P + 1)2N−P , and dividing by 2N gives the stated result.
Corollary Ļe expected number of bit comparisons made by the basic string-
searching algorithm when seeking all occurrences of an arbitrary ŀxed pattern
of length P in a text string of length N is N(2 −2−P ) + O(1).
Proof. To ŀnd the number of bits examined by the algorithm in the search, we
note that we can interpret the numbers in Table 8.1 another way: they count
the number of preŀxes of the pattern that occur starting at the corresponding
position in the text. Now we can use the above formula to count the preŀxes
as well. Ļis approach yields the expression
∑
1≤k≤P
(N −k + 1)2N−k
for the total number of occurrences of preŀxes of the pattern in all of the 2N
bitstrings of N bits. Evaluating this sum, dividing by 2N, and adding N (for
the mismatch comparisons) gives the stated result.
Corollary Ļe average number of bits examined by the basic string-searching
algorithm to ŀnd the longest match with an arbitrary inŀnitely long pattern
in a random bitstring is ∼2N.
Ļese results are independent of the pattern and seem to run counter
to the intuition just given, where we know that the number of bits examined

țșȗ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ș
during a search for a long string of 0s is quadratic in the length of each run
of 0s in the text, whereas a search for a 1 followed by a long string of 0s does
not have such evident quadratic behavior. Ļis diﬀerence is perhaps explained
by noting that preŀxes of the latter pattern are somewhere in the text, but not
bunched together as for strings of 0s. Since we use cumulated totals, we need
not worry about independence among diﬀerent instances of preŀxes; we count
them all. By contrast, the time required to ŀnd the ŀrst match does depend on
the pattern, even for random text. Ļis is directly related to another quantity
of interest, the probability that the pattern does not occur in the text. In the
next section, we will look at details of these analytic results.
Later in the chapter, we will look at algorithmic improvements to the
basic string-searching algorithm. First, we will look at the Knuth-Morris-
Pratt algorithm, a method that uses preprocessing time proportional to the
length of the pattern to get an “optimal” search time, examining each charac-
ter in the text at most once. At the end of the chapter, we will see that with
(much) more investment in preprocessing, the text can be built into a data
structure related to a general structure known as a trie that allows searches for
patterns to be done in time proportional to the length of the pattern. Tries
also provide eﬃcient support for a number of other algorithms on bitstrings,
but we postpone considering those until after covering basic analytic results.
8.2 Combinatorial properties of bitstrings. We are interested in study-
ing the properties of random strings of 0 and 1 values (bitstrings) when we
consider each of the 2N bitstrings of length N to be equally likely. As we saw
in §5.2, bitstrings are enumerated by the OGF
B(z) =
∑
b∈B
z|b| =
∑
N≥0
{# of bitstrings of length N}zN
where B denotes the set of all bitstrings. Bitstrings are either empty or begin
with a 0 or a 1, so they are generated by the combinatorial construction
B = ϵ + (Z0 + Z1) × B,
which immediately transfers to
B(z) = 1 + 2zB(z)

§ȟ.ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țșȘ
and therefore B(z) = (1 −2z)−1 and the number of bitstrings of length
N is 2N as expected. As with permutations and trees in the previous two
chapters, we can modify this basic argument to study properties of bitstrings
that are a bit more interesting.
For example, we have already encountered the problem of enumerating
the 1s in a random bitstring as an example in §3.9 and §5.4: the associated
bivariate generating function involves the binomial distribution
B(z, u) =
1
1 −z(1 + u) =
∑
N
∑
k
(
N
k
)
ukzN.
We used Ļeorem 3.11 to calculate that the average number of 1 bits in a
random N-bit string is
[zN]Bu(z, 1)/2N = N/2,
and so forth. We will consider “global” properties like this in much more
detail in Chapter 9; in this chapter our focus is more on “local” properties
involving bits that are near one another in the string.
Studying properties of random bitstrings is equivalent to studying prop-
erties of independent Bernoulli trials (perhaps coin łips), so classical results
from probability theory are relevant. Our speciŀc focus in this chapter is not
just on the trials, but also on the sequence of events. Ļere are also classi-
cal results related to this. In probability theory, we consider properties of
a sequence of random trials and study “waiting times” (see Feller [9], for
example); in the analysis of algorithms, we consider the equivalent problem
of searching for patterns in strings by taking the bits in sequence, a natural
way to think about the same problem. As we will see, it turns out that formal
language theory and generating functions combine to provide a clear expla-
nation of analytic phenomena relating to the study of sequences of Bernoulli
trials.
Pattern occurrence enumeration.
Any bitstring containing an arbitrary
ŀxed pattern of length P is constructed by concatenating an arbitrary bit-
string, the pattern, and another arbitrary bitstring. Ļerefore, by the symbolic
method, the generating function enumerating such occurrences is
1
1 −2z zP
1
1 −2z .

țșș
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ș
A particular bitstring may be counted several times in this enumeration (once
for each occurrence of the pattern), which is precisely what we seek. Ļus,
the number of occurrences of a ŀxed pattern of length P in all bitstrings of
length N is
[zN]
1
1 −2z zP
1
1 −2z = [zN−P ]
1
(1 −2z)2 = (N −P + 1)2N−P
as in Ļeorem 8.1. Again, this is not the same as the number of bitstrings of
length N containing (one or more occurrences of) an arbitrary ŀxed pattern.
Runs of 0s. Where should we expect to ŀnd the ŀrst run of P consecutive
0s in a random bitstring? Table 8.2 gives ten long random bitstrings and the
positions where one, two, three, and four 0s ŀrst occur in each. It turns out
that generating functions lead to a simple solution to this problem, and that
it is representative of a host of similar problems.
1
2
3
4
01010011101010000110011100010111011000110111111010
0
4
13 13
01110101010001010010011001010000100001010110100101
0
10 10 28
01011101011011110010001110000001010110010011110000
0
16 19 25
10101000010100111010101011110000111110000111001001
1
5
5
5
11111111001000001001001100100110000000100000110001
8
8
11 11
00100010001100101110011100001100101000001011001111
0
0
3
24
01011011110110010110000100101001010000101001111110
0
13 19 19
10011010000010011001010010100011000001111010011010
1
1
7
7
01100010011001101100010111110001001000111001111010
0
3
3
—
01011001000110000001000110010101011100111100100110
0
5
8
13
average
1.0 6.5 9.8 —
Table 8.2
Position of ŀrst runs of 0s on sample bitstrings

§ȟ.ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țșȚ
Ļeorem 8.2 (Runs of 0s).
Ļe generating function enumerating the num-
ber of bitstrings with no runs of P consecutive 0s is given by
BP (z) =
1 −zP
1 −2z + zP+1 .
Proof. Let BP be the class of bitstrings having no runs of P consecutive 0s
and consider the OGF
BP (z) =
∑
b∈BP
z|b| =
∑
N≥0
{# of N-bit strings having no runs of P 0s}zN.
Now, any bitstring without P consecutive 0s is either (i) null or consisting of
from zero to P −1 0s; or (ii) a string of from zero to P −1 0s, followed by
a 1, followed by any bitstring without P consecutive 0s. Ļe symbolic method
immediately gives the functional equation
SP (z) = (1 + z + . . . + zP−1)(1 + zSP (z)).
Noting that 1 + z + . . . + zP−1 = (1 + zP )/(1 −z), we can calculate the
following explicit expression for the OGF:
BP (z) =
1 −zP
1 −z
1 −z 1 −zP
1 −z
=
1 −zP
1 −2z + zP+1 .
Each BP (z) is a rational function and is therefore easy to expand. Check-
ing small values, we have the following expansions for P = 1, 2, 3:
1 −z
1 −2z + z2 = 1 + z + z2 + z3 + z4 + z5 + z6 + z7 + . . .
1 −z2
1 −2z + z3 = 1 + 2z + 3z2 + 5z3 + 8z4 + 13z5 + 21z6 + 34z7 + . . .
1 −z3
1 −2z + z4 = 1 + 2z + 4z2 + 7z3 + 13z4 + 24z5 + 44z6 + 81z7 + . . . .

țșț
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ș
For P = 1, the expansion conŀrms the fact that there is one string of length
N with no runs of one 0 (the string that is all 1s). For P = 2, we have
B2(z) =
1 + z
1 −z −z2
so
[zN]B2(z) = FN+1 + FN = FN+2,
the Fibonacci numbers (see §2.4). Indeed, the representation
BP (z) = 1 + z + z2 + . . . + zP−1
1 −z −z2 −. . . −zP
shows that [zN]BP (z) satisŀes the same recurrence as the generalized Fibo-
nacci numbers of Exercise 4.18, but with diﬀerent initial values. Ļus, we can
use Ļeorem 4.1 to ŀnd asymptotic estimates for [zN]BP (z).
Corollary Ļe number of bitstrings of length N containing no runs of P
consecutive 0s is asymptotic to cβN, where β is the root of largest modulus
of the polynomial zP −zP−1 −. . .−z −1 = 0 and c = (βP +βP−1 +. . .+
β)/(βP−1 + 2βP−2 + 3βP−3 + . . . + (P −1)β + P).
Proof. Immediate from Exercise 4.18 and Ļeorem 4.1. Approximate values
of c and β for small values of P are given in Table 8.3.
[zN]BP (z) ∼cP βN
P
P
BP (z)
cP
βP
2
1 −z2
1 −2z + z3
1.17082· · ·
1.61803· · ·
3
1 −z3
1 −2z + z4
1.13745· · ·
1.83929· · ·
4
1 −z4
1 −2z + z5
1.09166· · ·
1.92756· · ·
5
1 −z5
1 −2z + z6
1.05753· · ·
1.96595· · ·
6
1 −z6
1 −2z + z7
1.03498· · ·
1.98358· · ·
Table 8.3
Enumerating bitstrings with no runs of P 0s

§ȟ.ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țșȜ
Exercise 8.1 Give two recurrences satisŀed by [zN]BP (z).
Exercise 8.2 How long a string of random bits should be taken to be 99% sure that
there are at least three consecutive 0s?
Exercise 8.3 How long a string of random bits should be taken to be 50% sure that
there are at least 32 consecutive 0s?
Exercise 8.4 Show that
[zN]BP (z) =
∑
i
(−1)i2N−(P +1)i
((N −Pi
i
)
−2−P
(N −P(i + 1)
i
))
.
First run of 0s. A shortcut to computing the average position of the ŀrst run
of P 0s is available because the OGF that enumerates bitstrings with no runs
of P consecutive 0s is very closely related to the PGF for the position of the
last bit (the end) in the ŀrst run of P consecutive 0s in a random bitstring, as
shown by the following manipulations:
BP (z) =
∑
b∈BP
z|b|
=
∑
N≥0
{# of bitstrings of length N with no runs of P 0s}zN
BP (1/2) =
∑
N≥0
{# of bitstrings of length N with no runs of P 0s}/2N
=
∑
N≥0
Pr {1st N bits of a random bitstring have no runs of P 0s}
=
∑
N≥0
Pr {position of end of ŀrst run of M 0s is > N }.
Ļis sum of cumulative probabilities is equal to the expectation.
Corollary Ļe average position of the end of the ŀrst run of M 0s in a random
bitstring is BP (1/2) = 2P+1 −2.
Generating functions simplify the computation of the expectation con-
siderably; any reader still unconvinced of this fact is welcome, for example,
to verify the result of this corollary by developing a direct derivation based on
calculating probabilities. For permutations, we found that the count N! of the
number of permutations on N elements led us to EGFs; for bitstrings, the
count 2N of the number of bitstrings of N elements will lead us to functional
equations involving z/2, as above.

țșȝ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ș
Existence. Ļe proof of the second corollary to Ļeorem 8.2 also illustrates
that ŀnding the ŀrst occurrence of a pattern is roughly equivalent to counting
the number of strings that do not contain the pattern, and tells us that the
probability that a random bitstring contains no run of P 0s is [zN]BP (z/2).
For example, for P = 1 this value is 1/2N, since only the bitstring that is all
1s contains no runs of P 0s. For P = 2 the probability is O((ϕ/2)N) (with
ϕ = (1 +
√
5 )/2 = 1.61803 · · ·), exponentially decreasing in N. For ŀxed
P, this exponential decrease is always the case because the βP ’s in Table 8.3
remain strictly less than 2. A slightly more detailed analysis reveals that once
N increases past 2M, it becomes increasingly unlikely that some P-bit pattern
does not occur. For example, a quick calculation from Table 8.3 shows that
there is a 95% chance that a 10-bit string does not contain a run of six 0s, a
45% chance that a 100-bit string does not contain a run of six 0s, and a .02%
chance that a 1000-bit string does not contain a run of six 0s.
Longest run. What is the average length of the longest run of 0s in a random
bitstring? Ļe distribution of this quantity is shown in Figure 8.1. As we did
for tree height in Chapter 6 and cycle length in permutations in Chapter 7,
we can sum the “vertical” GFs given previously to get an expression for the
average length of the longest string of 0s in a random N-bit string:
1
2N [zN]
∑
k≥0
(
1
1 −2z −
1 −zk
1 −2z + zk+1
)
.
Knuth [24] studied a very similar quantity for the application of determining
carry propagation time in an asynchronous adder, and showed this quantity to
be lgN+O(1). Ļe constant term has an oscillatory behavior; close inspection
of Figure 8.1 will give some insight into why this might be so. Ļe function
describing the oscillation turns out to be the same as one that we will study
in detail for the analysis of tries at the end of this chapter.
Exercise 8.5 Find the bivariate generating function associated with the number of
leading 1 bits in a random bitstring and use it to calculate the average and standard
deviation of this quantity.
Exercise 8.6 By considering bitstrings with no runs of two consecutive 0s, evaluate
the following sum involving Fibonacci numbers: ∑
j≥0 Fj/2j.
Exercise 8.7 Find the BGF for the length of the longest run of 0s in bitstrings.

§ȟ.ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țșȞ
0
20
Figure 8.1 Distribution of longest run of 0s in a random bitstring
(horizontal axes translated to separate curves)

țșȟ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ș
Exercise 8.8 What is the standard deviation of the random variable marking the ŀrst
occurrence of a run of P 0s in a random bitstring?
Exercise 8.9 Use a computer algebra system to plot the average length of the longest
run of 0s in a random bitstring of N bits, for 2 < N < 100.
Exercise 8.10 How many bits are examined by the basic algorithm given in the pre-
vious section to ŀnd the ŀrst string of P 0s in a random bitstring?
Arbitrary patterns. At ŀrst, one might suspect that these results hold for
any ŀxed pattern of P bits, but that is simply not true: the average position of
the ŀrst occurrence of a ŀxed bit pattern in a random bitstring depends very
much on the pattern itself. For example, it is easy to see that a pattern like
0001 tends to appear before 0000, on the average, by the following observa-
tion: once 000 has already been matched, in both cases a match occurs on
the next character with probability 1/2, but a mismatch for 0000 means that
0001 was in the text (and no match is possible for four more positions), while
a mismatch for 0001 means that 0000 is in the text (and a match can happen
at the next position). Ļe dependence on the pattern turns out to be easily
expressed in terms of a function matching the pattern against itself:
Deŀnition Ļe autocorrelation of a bitstring b0b1 . . . bP−1 is the bitstring
c0c1 . . . cP−1 with ci deŀned to be 1 if bj = bi+j for 0 ≤j ≤P −1−i, 0 oth-
erwise. Ļe corresponding autocorrelation polynomial is obtained by taking
the bits as coeﬃcients: c(z) = c0 + c1z + . . . + cP−2zP−2 + cP−1zP−1.
Ļe autocorrelation is easily computed: the ith bit is determined by
shifting left i positions, then putting 1 if the remaining bits match the original
1
0
1
0
0
1
0
1
0
1
0
1
0
0
1
0
1
0
1
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
1
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1
0
1
1
0
1
0
0
1
0
1
0
0
Table 8.4
Autocorrelation of 101001010

§ȟ.ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țșȠ
pattern, 0 otherwise. For example, Table 8.4 shows that the autocorrelation
of 101001010 is 100001010, and the corresponding autocorrelation poly-
nomial is 1 + z5 + z7. Note that c0 is always 1.
Ļeorem 8.3 (Pattern autocorrelation).
Ļe generating function for the
number of bitstrings not containing a pattern p0p1 . . . pP−1 is given by
Bp(z) =
c(z)
zP + (1 −2z)c(z),
where c(z) is the autocorrelation polynomial for the pattern.
Proof. We use the symbolic method to generalize the proof given earlier for
the case where the pattern is P consecutive 0s. We start with the OGF for
Sp, the set of bitstrings with no occurrence of p:
Sp(z) =
∑
s∈Sp
z|s|
=
∑
N≥0
{# of bitstrings of length N with no occurrence of p}zN.
Similarly, we deŀne Tp to be the class of bitstrings that end with p but have
no other occurrence of p, and name its associated generating function Tp(z).
Now, we consider two symbolic relationships between Sp and Tp that
translate to simultaneous equations involving Sp(z) and Tp(z). First, Sp and
Tp are disjoint, and if we remove the last bit from a bitstring in either, we get
a bitstring in Sp (or the empty bitstring). Expressed symbolically, this means
that
Sp + Tp = ϵ + Sp × (Z0 + Z1),
which, since the OGF for (Z0 + Z1) is 2z, translates to
Sp(z) + Tp(z) = 1 + 2zSp(z).
Second, consider the set of strings consisting of a string from Sp followed by
the pattern. For each position i in the autocorrelation for the pattern, this
gives a string from Tp followed by an i-bit “tail.” Expressed symbolically, this
gives
Sp × <pattern> = Tp ×
∑
ci=1
<tail>i,

țȚȗ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ș
which, since the OGF for <pattern> is zP and the OGF for <tail>i is zi,
translates to
Sp(z)zP = Tp(z)
∑
ci=1
zi = Tp(z)c(z).
Ļe stated result follows immediately as the solution to the two simul-
taneous equations relating the OGFs S(z) and T(z).
For patterns consisting of P 0s (or P 1s), the autocorrelation polynomial
is 1 + z + z2 + . . . + zP−1 = (1 −zP )/(1 −z), so Ļeorem 8.3 matches our
previous result in Ļeorem 8.2.
Corollary Ļe expected position of the end of the ŀrst occurrence of a bit-
string with autocorrelation polynomial c(z) is given by 2P c(1/2).
Table 8.5 shows the generating functions for the number of bitstrings
not containing each of the 16 patterns of four bits. Ļe patterns group into
four diﬀerent sets of patterns with equal autocorrelation. For each set, the
table also gives the dominant root of the polynomial in the denominator of
the OGF, and the expected “wait time” (position of the ŀrst occurrence of
the pattern), computed from the autocorrelation polynomial. We can develop
these approximations using Ļeorem 4.1 and apply them to approximate the
wait times using the corollaries to Ļeorem 8.2 in the same way we did for
Table 8.3. Ļat is, the probability that an N-bit string has no occurrence of
the pattern 1000 is about (1.83929/2)N, and so forth. Here, we are ignoring
pattern
autocorrelation
OGF
dominant root wait
0000 1111
1111
1 −z4
1 −2z + z5
1.92756· · ·
30
0001 0011 0111
1000
1
1 −2z + z4
1.83929· · ·
16
1000 1100 1110
0010 0100 0110
1001
1 + z3
1 −2z + z3 −z4
1.86676· · ·
18
1001 1011 1101
0101 1010
1010
1 + z2
1 −2z + z2 −2z3 + z4 1.88320· · ·
20
Table 8.5
Generating functions and wait times for 4-bit patterns

§ȟ.ș
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȚȘ
the constants like the ones in Table 8.3, which are close to, but not exactly,
1. Ļus, for example, there is about a 43% chance that a 10-bit string does
not contain 1000, as opposed to the 69% chance that a 10-bit string does not
contain 1111.
It is rather remarkable that such results are so easily accessible through
generating functions. Despite their fundamental nature and wide applicabil-
ity, it was not until systematic analyses of string-searching algorithms were
attempted that this simple way of looking at such problems became apparent.
Ļese and many more related results are developed fully in papers by Guibas
and Odlyzko [17][18].
Exercise 8.11 Calculate the expected position of the ŀrst occurrence of each of the
following patterns in a random bitstring: (i) P −1 0s followed by a 1; (ii) a 1 followed
by P −1 0s; (iii) alternating 0-1 string of even length; (iv) alternating 0-1 string of
odd length.
Exercise 8.12 Which bit patterns of length P are likely to appear the earliest in a
random bitstring? Which patterns are likely to appear the latest?
Exercise 8.13 Does the standard deviation of the random variable marking the ŀrst
position of a bit pattern of length P in a random bitstring depend on the pattern?
Larger alphabets. Ļe methods just described apply directly to larger alpha-
bets. For example, a proof virtually identical to the proof of Ļeorem 8.3
will show that the generating function for strings from an M-character al-
phabet that do not contain a run of P consecutive occurrences of a particular
character is
1 −zP
1 −Mz + (M −1)zP+1 .
Similarly, as in the second corollary to Ļeorem 8.3, the average position of
the end of the ŀrst run of P occurrences of a particular character in a random
string taken from an M-character alphabet is M(MP −1)/(M −1).
Exercise 8.14 Suppose that a monkey types randomly at a 32-key keyboard. What
is the expected number of characters typed before the monkey hits upon the phrase
THE QUICK BROWN FOX JUMPED OVER THE LAZY DOG?
Exercise 8.15 Suppose that a monkey types randomly at a 32-key keyboard. What
is the expected number of characters typed before the monkey hits upon the phrase
TO BE OR NOT TO BE?

țȚș
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ț
8.3 Regular Expressions. Ļe basic method using generating functions
as described in the previous section generalizes considerably. To determine
properties of random strings, we ended up deriving generating functions that
count the cardinality of sets of strings with well-deŀned properties. But de-
veloping speciŀc descriptions of sets of strings falls within the domain of for-
mal languages, the subject of a vast literature. We use only basic principles,
as described in any standard text—for example, Eilenberg [8]. Ļe simplest
concept from formal language theory is the regular expression (RE), a way to
describe sets of strings based on the union, concatenation, and “star” opera-
tions, which are described later in this section. A set of strings (a language) is
said to be regular if it can be described by a regular expression. For example,
the following regular expression describes all bitstrings with no run of four
consecutive 0s:
S4 = (1 + 01 + 001 + 0001)∗(ϵ + 0 + 00 + 000).
In this expression, + denotes unions of languages; the product of two lan-
guages is the language of strings formed by concatenating a string from the
ŀrst with a string from the second; and * is shorthand for concatenating a
language with itself an arbitrary number of times (including zero). As usual,
ϵ represents the empty string. Earlier, we derived the corresponding OGF
S4(z) =
∑
s∈S4
z|s| =
1 −z4
1 −2z + z5
and deduced basic properties of the language by manipulating the OGF.
Other problems that we have considered also correspond to languages that
can be deŀned with REs and thus analyzed with OGFs, as we shall see.
Ļere is a relatively simple mechanism for transforming the formal de-
scription of the sets of strings (the regular expression) into the formal analytic
tool for counting them (the OGF). Ļis is due to Chomsky and Schützen-
berger [3]. Ļe sole requirement is that the regular expression be unambiguous:
there must be only one way to derive any string in the language. Ļis is not a
fundamental restriction because it is known from formal language theory that
any regular language can be speciŀed by an unambiguous regular expression.
In practice, however, it often is a restriction because the theoretically guar-
anteed unambiguous RE can be complicated, and checking for ambiguity or
ŀnding a usable unambiguous RE can be challenging.

§ȟ.Ț
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȚȚ
Ļeorem 8.4 (OGFs for regular expressions).
Let A and B be unambigu-
ous regular expressions and suppose that A + B, A × B, and A∗are also
unambiguous. If A(z) is the OGF that enumerates A and B(z) is the OGF
that enumerates B, then
A(z) + B(z) is the OGF that enumerates A + B,
A(z)B(z)
is the OGF that enumerates AB, and
1
1 −A(z)
is the OGF that enumerates A∗.
Moreover, OGFs that enumerate regular languages are rational functions.
Proof. Ļe ŀrst part is essentially the same as our basic theorem on the sym-
bolic method for OGFs (Ļeorem 5.1), but it is worth restating here because
of the fundamental nature of this application. If aN is the number of strings of
length N in A and bN is the number of strings of length N in B, then aN +bN
is the number of strings of length N in A+B, since the requirement that the
languages are unambiguous implies that A ∩B is empty. Similarly, we can
use a simple convolution to prove the translation of AB, and the symbolic
representation
A∗= ϵ + A + A2 + A3 + A4 + . . .
implies the rule for A∗, exactly as for Ļeorem 5.1.
Ļe second part of the theorem results from the remark that every reg-
ular language can be speciŀed by an unambiguous regular expression. For the
reader with knowledge of formal languages: if a language is regular, it can be
recognized by a deterministic FSA, and the classical proof of Kleene’s theorem
associates an unambiguous regular expression to a deterministic automaton.
We explore some algorithmic implications of associations with FSAs later in
this chapter.
Ļus, we have a simple and direct way to transform a regular expression
into an OGF that counts the strings described by that regular expression,
provided only that the regular expression is unambiguous. Furthermore, an
important implication of the fact that the generating function that results
when successively applying Ļeorem 8.4 is always rational is that asymptotic
approximations for the coeﬃcients are available, using general tools such as
Ļeorem 4.1. We conclude this section by considering some examples.

țȚț
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ț
Strings with no runs of k 0s.
Earlier, we gave a regular expression for Sk,
the set of bitstrings with no occurrence of k consecutive 0s. Consider, for
instance, S4. From Ļeorem 8.4, we ŀnd immediately that the OGF for
1 + 01 + 001 + 0001
is
z + z2 + z3 + z4
and the OGF for
ϵ + 0 + 00 + 000
is
1 + z + z2 + z3
so the construction for S4 given earlier immediately translates to the OGF
equation
S4(z) =
1 + z + z2 + z3
1 −(z + z2 + z3 + z4) =
1 −z4
1 −z
1 −z 1 −z4
1 −z
=
1 −z4
1 −2z + z5 ,
which matches the result that we derived in §8.2.
Multiples of three. Ļe regular expression (1(01*0)*10*)* generates the
set of strings 11, 110, 1001, 1100, 1111, . . . , that are binary representations
of multiples of 3. Applying Ļeorem 8.4, we ŀnd the generating function for
the number of such strings of length N:
1
1 −
z2
1 −
z2
1 −z
(
1
1 −z
) =
1
1 −
z2
1 −z −z2
= 1 −z −z2
1 −z −2z2
= 1 +
z2
(1 −2z)(1 + z).
Ļis GF is very similar to one of the ŀrst GFs encountered in §3.3: it expands
by partial fractions to give the result (2N−1 + (−1)N)/3. All the bitstrings
start with 1: about a third of them represent numbers that are divisible by 3,
as expected.

§ȟ.Ț
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȚȜ
Height of a gambler’s ruin sequence. Taking 1 to mean “up” and 0 to mean
“down,” we draw a correspondence between bitstrings and random walks. If
we restrict the walks to terminate when they ŀrst reach the start level (without
ever going below it), we get walks that are equivalent to the gambler’s ruin
sequences that we introduced in §6.3.
We can use nested REs to classify these walks by height, as follows: To
construct a sequence of height bounded by h + 1, concatenate any number of
sequences of height bounded by h, each bracketed by a 1 on the left and a 0 on
the right. Table 8.6 gives the resulting REs and corresponding OGFs for h =
1, 2, 3, and 4. Figure 8.2 (on the next page) gives examples that illustrate this
construction. Ļe OGF translation is immediate from Ļeorem 8.4, except
that, since 0s and 1s are paired, we only translate one of them to z. Ļese GFs
match those involving the Fibonacci polynomials for the height of Catalan
trees in §6.10, so the corollary to Ļeorem 6.9 tells us that the average height
of a gambler’s ruin sequence is ∼
√
πN.
regular expression
generating function
height ≤1
(10)*
1
1 −z
height ≤2
(1(10)*0)*
1
1 −
z
1 −z
= 1 −z
1 −2z
height ≤3
(1(1(10)*0)*0)*
1
1 −
z
1 −
z
1 −z
=
1 −2z
1 −3z + z2
height ≤4
(1(1(1(10)*0)*0)*0)*
1
1 −
z
1 −
z
1 −
z
1 −z
= 1 −3z + z2
1 −4z + 3z2
Table 8.6
REs and OGFs for gambler’s ruin sequences

țȚȝ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ț
Exercise 8.16 Give the OGFs and REs for gambler’s ruin sequences with height no
greater than 4, 5, and 6.
Exercise 8.17 Give a regular expression for the set of all strings having no occurrence
of the pattern 101101. What is the corresponding generating function?
Exercise 8.18 What is the average position of the second disjoint string of P 0s in a
random bitstring?
Exercise 8.19 Find the number of diﬀerent ways to derive each string of N 0s with
the RE 0*00. Answer the same question for the RE 0*00*.
Exercise 8.20 One way to generalize REs is to specify the number of copies implicit
in the star operation. In this notation the ŀrst sequence in Figure 8.2 is (10)22 and
the second sequence is (10)31(10)50(10)31(10)70(10)2, which better expose
their structure. Give the generalized REs for the other two sequences in Figure 8.2.
Exercise 8.21 Find the average number of 0s appearing before the ŀrst occurrence of
each of the bit patterns of length 4 in a random bitstring.
Exercise 8.22 Suppose that a monkey types randomly at a 2-key keyboard. What
is the expected number of bits typed before the monkey hits upon a string of 2k
alternating 0s and 1s?
1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
height ≤ 1
1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0
height ≤ 2
1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0
height ≤ 3
1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0
height ≤ 4
Figure 8.2 Gambler’s ruin sequences

§ȟ.ț
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȚȞ
8.4 Finite-State Automata and the Knuth-Morris-Pratt Algorithm.
Ļe brute-force algorithm for string matching is quite acceptable for many
applications, but, as we saw earlier, it can run slowly for highly self-repetitive
patterns. Eliminating this problem leads to an algorithm that not only is of
practical interest, but also links string matching to basic principles of theo-
retical computer science and leads to more general algorithms.
For example, when searching for a string of P 0s, it is very easy to over-
come the obvious ineﬃciency in Program 8.1: When a 1 is encountered at
text position i, reset the “pattern” pointer j to the beginning and start look-
ing again at position i + 1. Ļis is taking advantage of speciŀc properties of
the all-0s pattern, but it turns out that the idea generalizes to give an optimal
algorithm for all patterns, which was developed by Knuth, Morris, and Pratt
in 1977 [25].
Ļe idea is to build a pattern-speciŀc ŀnite-state automaton that begins
at an initial state, examining the ŀrst character in the text; scans text char-
acters; and makes state transitions based on the value scanned. Some of the
states are designated as ŀnal states, and the automaton is to terminate in a
ŀnal state if and only if the associated pattern is found in the text. Ļe im-
plementation of the string search is a simulation of the FSA, based on a table
indexed by the state. Ļis makes the implementation extremely simple, as
shown in Program 8.2.
public static int search(char[] pattern, char[] text)
{
int P = pattern.length;
int N = text.length;
int i, j;
for (i = 0, j = 0; i < N && j < P; i++)
j = dfa[text[i]][j]
if (j == P) return i - P;
// Found at offset i-P.
return N;
// Not found.
}
Program 8.2 String searching with an FSA (KMP algorithm)

țȚȟ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ț
0
1
0
0
1
1
0
1
2
0
1
3
0
1
4
0
1
5
0
1
6
0
1
7
8
0  1  2  3  4  5  6  7
0  2  0  4  5  0  2  8  
1  1  3  1  3  6  7  1
state
0-transition
1-transition
Figure 8.3 Knuth-Morris-Pratt FSA for 10100110
Ļe key to the algorithm is the computation of the transition table, which de-
pends on the pattern. For example, the proper table for the pattern 10100110
is shown, along with a graphic representation of the FSA, in Figure 8.3.
When this automaton is run on the sample piece of text given below, it takes
the state transitions as indicated—below each character is given the state the
FSA is in when that character is examined.
01110101110001110010000011010011000001010111010001
0011123431120011120120000112345678
Exercise 8.23 Give the state transitions for the FSA in Figure 8.3 for searching in
the text 010101010101010101010.
Exercise 8.24 Give the state transitions for the FSA in Figure 8.3 for searching in
the text 1110010111010110100010100101010011110100110.
Exercise 8.25 Give a text string of length 25 that maximizes (among all strings of
length 25) the number of times the KMP automaton from Figure 8.3 reaches step 2.

§ȟ.ț
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȚȠ
Once the state table has been constructed (see below), the KMP algo-
rithm is a prime example of an algorithm that is suﬃciently sophisticated that
it is trivial to analyze, since it just examines each character in the text once.
Remarkably, the algorithm also can build the transition table by examining
each character in the pattern just once!
Ļeorem 8.5 (KMP string matching).
Ļe Knuth-Morris-Pratt algorithm
does N bit comparisons when seeking a pattern of length P in a binary text
string of length N.
Proof. See the previous discussion.
Ļe construction of the state transition table depends on correlations of
preŀxes of the pattern, as shown in Table 8.7. We deŀne state i to correspond
to this situation where the ŀrst i −1 characters in the pattern have been
matched in the text, but the ith character does not match. Ļat is, state i
corresponds to a speciŀc i-bit pattern in the text. For example, for the pattern
in Table 8.7, the FSA is in state 4 if and only if the previous four bits in the
text were 1010. If the next bit is 0, we would go on to state 5; if the next
bit is 1, we know that this is not the preŀx of a successful match and that
the previous ŀve bits in the text were 10101. What is required is to go to the
state corresponding to the ŀrst point at which this pattern matches itself when
shifted right—in this case, state 3. In general, this is precisely the position
(measured in bits from the right) of the second 1 in the correlation of this
bitstring (0 if the ŀrst 1 is the only one).
10100110
0 1
0
1
0
0 1
11
11
1
2 1
100
100
0
0 3
1011
1001
1
4 1
10101
10101
3
5 3
101000
100000
0
0 6
1010010
1000010
2
2 7
10100111
10000001
1
8 1
mismatch
autocorrelation
ŀrst match
table
Table 8.7
Example of KMP state transition table

țțȗ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ț
Exercise 8.26 Give the KMP state transition table for the pattern 110111011101.
Exercise 8.27 Give the state transitions made when using the KMP method to deter-
mine whether the text 01101110001110110111101100110111011101 contains
the pattern in the previous exercise.
Exercise 8.28 Give the state transition table for a string of 2k alternating 0s and 1s.
TŔő şŠŞŕŚœ-şőōŞŏŔŕŚœ ŜŞśŎŘőř ŠŔōŠ we have been considering is equiv-
alent to determining whether the text string is in the language described by
the (ambiguous) regular expression
(0+1)* <pattern> (0+1)*.
Ļis is the recognition problem for regular expressions: given a regular expres-
sion and a text string, determine if the string is in the language described
by the regular expression. In general, regular expression recognition can be
done by building an FSA, and properties of such automata can be analyzed
using algebraic techniques as we have been doing. As is usually the case,
however, more specialized problems can be solved eﬀectively with more spe-
cialized techniques. Ļe KMP ŀnite-state automaton is a prime example of
this principle.
Generalization to larger alphabets involves a transition table of size pro-
portional to the size of the pattern times the size of the alphabet, though var-
ious improvements upon this have been studied. Details on such issues and
on numerous text searching applications may be found in books by Gonnet
and Baeza-Yates [15] and by Gusŀeld [20].
Exercise 8.29 Give the KMP state transition table for the pattern 313131, assuming
a 4-character alphabet 0, 1, 2, and 3. Give the state transitions made when using the
KMP method to determine whether the text 1232032313230313131 contains this
pattern.
Exercise 8.30 Prove directly that the language recognized by a deterministic FSA
has an OGF that is rational.
Exercise 8.31 Write a computer algebra program that computes the standard rational
form of the OGF that enumerates the language recognized by a given deterministic
FSA.

§ȟ.Ȝ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țțȘ
8.5 Context-Free Grammars. Regular expressions allow us to deŀne lan-
guages in a formal way that turns out to be amenable to analysis. Next in
the hierarchy of languages comes the context-free languages. For example, we
might wish to know:
• How many bitstrings of length 2N have N 0s and N 1s?
• Given a random bitstring of length N, how many of its preŀxes have
equal numbers of 0s and 1s, on the average?
• At what point does the number of 0s in a random bitstring ŀrst exceed
the number of 1s, on the average?
All these problems can be solved using context-free grammars, which are more
expressive than regular expressions. Ļough the ŀrst question is trivial com-
binatorially, it is known from language theory that such a set cannot be de-
scribed with regular expressions, and context-free languages are needed. As
with REs, automatic mechanisms involving generating functions that cor-
respond to the symbolic method are eﬀective for enumerating unambiguous
context-free languages and thus open the door to studying a host of interest-
ing questions.
To begin, we brieły summarize some basic deŀnitions from formal lan-
guage theory. A context-free grammar is a collection of productions relat-
ing nonterminal symbols and letters (also called terminal symbols) by means of
unions and concatenation products. Ļe basic operations are similar to those
for regular expressions (the “star” operation is not needed), but the introduc-
tion of nonterminal symbols creates a more powerful descriptive mechanism
because of the possibility of nonlinear recursion. A language is context-free if
it can be described by a context-free grammar. Indeed, we have actually been
using mechanisms equivalent to context-free grammars to deŀne some of the
combinatorial structures that we have been analyzing.
For example, our deŀnition of binary trees, in Chapter 6, can be recast
formally as the following unambiguous grammar:
<bin tree>:= <ext node> | <int node> <bin tree> <bin tree>
<int node>:= 0
<ext node>:= 1
Nonterminal symbols are enclosed in angle brackets. Each nonterminal can
be considered as representing a context-free language, deŀned by direct as-
signment to a letter of the alphabet, or by the union or concatenation product

țțș
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ȝ
operations. Alternatively, we can consider each equation as a rewriting rule
indicating how the nonterminal can be rewritten, with the vertical bar de-
noting alternate rewritings and juxtaposition denoting concatenation. Ļis
grammar generates bitstrings associated with binary trees according to the
one-to-one correspondence introduced in Chapter 6: visit the nodes of the
tree in preorder, writing 0 for internal nodes and 1 for external nodes.
Now, just as in the symbolic method, we view each nonterminal as rep-
resenting the set of strings that can be derived from it using rewriting rules
in the grammar. Ļen, we again have a general approach for translating un-
ambiguous context-free grammars into functional equations on generating
functions:
• Deŀne an OGF corresponding to each nonterminal symbol.
• Translate occurrences of terminal symbols to variables.
• Translate concatenation in the grammar to multiplication of OGFs.
• Translate union in the grammar to addition of OGFs.
When this process is carried out, there results a system of polynomial equa-
tions on the OGFs. Ļe following essential relationship between CGFs and
OGFs was ŀrst observed by Chomsky and Schützenberger [3].
Ļeorem 8.6 (OGFs for context-free grammars).
Let <A> and <B> be
nonterminal symbols in an unambiguous context-free grammar and suppose
that <A> | <B> and <A><B> are also unambiguous. If A(z) is the OGF that
enumerates the strings that can be derived from <A> and B(z) is the OGF
that enumerates <B>, then
A(z) + B(z) is the OGF that enumerates <A> | <B>
A(z)B(z)
is the OGF that enumerates <A><B>.
Moreover, any OGF that enumerates an unambiguous context-free language
satisŀes a polynomial equation whose terms are themselves polynomials with
rational coeﬃcients. (Such functions are said to be algebraic.)
Proof. Ļe ŀrst part of the theorem follows immediately as for the symbolic
method. Each production in the CFG corresponds to a OGF equation, so
the result is a system of polynomial equations on the OGFs. Solving for the
OGF that enumerates the language may be achieved by an elimination process
that reduces a polynomial system to a unique equation relating the variable
z and the OGF under consideration. For instance, Gröbner basis algorithms

§ȟ.Ȝ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țțȚ
that are implemented in some computer algebra systems are eﬀective for this
purpose (see Geddes, et al. [14]). If L(z) is the OGF of an unambiguous
context-free language, this process leads to a bivariate polynomial P(z, y)
such that P(z, L(z)) = 0, which proves that L(z) is algebraic.
Ļis theorem relates basic operations on languages to OGFs using the
symbolic method in the same way as Ļeorem 8.4, but the expressive power
of context-free grammars by comparison to regular expressions leads to dif-
ferences in the result in two important respects. First, a more general type
of recursive deŀnition is allowed (it can be nonlinear) so that the resulting
OGF has a more general form—the system of equations is in general non-
linear. Second, ambiguity plays a more essential role. Not every context-free
language has an unambiguous grammar (the ambiguity problem is even un-
decidable), so we can claim the OGF to be algebraic only for languages that
have an unambiguous grammar. By contrast, it is known that there exists an
unambiguous regular expression for every regular language, so we can make
the claim that OGFs for all regular languages are rational.
Ļeorem 8.6 spells out a method for solving a “context-free” counting
problem, by these last two steps:
• Solve to get an algebraic equation for the OGF.
• Solve, expand, and/or develop asymptotic estimates for coeﬃcients.
In some cases, the solution of that equation admits to explicit forms that
can be expanded, as we see in a later example. In some other cases, this
solution can be a signiŀcant challenge, even for computer algebra systems.
But one of the hallmarks of analytic combinatorics (see [12]) is a universal
transfer theorem of sweeping generality that tells us that the growth rate of
the coeﬃcients is of the form βn /
√
n3. Since we cannot do justice to this
theorem without appealing to complex asymptotics, we restrict ourselves in
this book to examples where explicit forms are easily derived.
2-orderedpermutations. Ļe discussion in §7.6 about enumerating 2-ordered
permutations corresponds to developing the following unambiguous context-
free grammar for strings with equal numbers of 0s and 1s:
<S>:= <U>1<S> | <D>0<S> | ϵ
<U>:= <U><U>1 | 0
<D>:= <D><D>0 | 1

țțț
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ȝ
Ļe nonterminals in this grammar may be interpreted as follows: <S> cor-
responds to all bitstrings with equal numbers of 0s and 1s; <U> corresponds
to all bitstrings with precisely one more 0 than 1, with the further constraint
that no preŀx has equal numbers of 0s and 1s; and <D> corresponds to all
bitstrings with precisely one more 1 than 0, with the further constraint that
no preŀx has equal numbers of 0s and 1s.
Now, by Ļeorem 8.6, each production in the grammar translates to a
functional equation on the generating functions:
S(z) = zU(z)S(z) + zD(z)S(z) + 1
U(z) = z + zU 2(z)
D(z) = z + zD2(z).
In this case, of course, U(z) and D(z) are familiar generating functions from
tree enumeration, so we can solve explicitly to get
U(z) = D(z) = 1
2z (1 −
√
1 −4z2 ),
then substitute to ŀnd that
S(z) =
1
√
1 −4z2
so
[z2N]S(z) =
(
2N
N
)
as expected.
Gröbner basis elimination. In general, explicit solutions might not be avail-
able, so we sketch for this problem how the Gröbner basis elimination pro-
cess will systematically solve this system. First, we note that D(z) = U(z)
because both satisfy the same (irreducible) equation. Ļus, what is required
is to eliminate U from the system of equations
P1 ≡S −2zUS −1 = 0
P2 ≡U −zU 2 −z = 0.
Ļe general strategy consists of eliminating higher-degree monomials from
the system by means of repeated combinations of the form AP −BQ, with

§ȟ.Ȝ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țțȜ
A, B monomials and P, Q polynomials subject to elimination. In this case,
forming UP1 −2SP2 cross-eliminates the U2 to give
P3 ≡−US −U + 2zS = 0.
Next, the US term can be eliminated by forming 2zP3 −P1, so we have
P4 ≡−2Uz + 4Sz2 −S + 1 = 0.
Finally, the combination P1 −SP4 completely eliminates U, and we get
P5 ≡S2 −1 −4S2z2 = 0
and therefore S(z) = 1/
√
1 −4z2 as before. We have included these details
for this example to illustrate the fundamental point that Ļeorem 8.6 gives
an “automatic” way to enumerate unambiguous context-free languages. Ļis
is of particular importance with the advent of computer algebra systems that
can perform the routine calculations involved.
Ballot problems. Ļe ŀnal result above is elementary, but context-free lan-
guages are of course very general, so the same techniques can be used to solve
a diverse class of problems. For example, consider the classical ballot problem:
Suppose that, in an election, candidate 0 receives N + k votes and candidate
1 receives N votes. What is the probability that candidate 0 is always in the
lead during the counting of the ballots? In the present context, this problem
can be solved by enumerating the number of bitstrings with N + k 0s and
N 1s that have the property that no preŀx has an equal number of 0s and 1s.
Ļis is also the number of paths through an (N + k)-by-N lattice that do
not touch the main diagonal. For k = 0 the answer is zero, because, if both
candidates have N votes, they must be tied somewhere during the counting,
if only at the end. For k = 1 the count is precisely [z2N+1]U(z) from our
discussion of 2-ordered permutations. For k = 3, we have the grammar
<B>:= <U><U><U>
<U>:= <U><U>1 | 0
and the answer is is [z2N+3](U(z))3. Ļis immediately generalizes to give the
result for all k.

țțȝ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ȝ
Ļeorem 8.7 (Ballot problem).
Ļe probability that a random bitstring
with k more 0s than 1s has the property that no preŀx has an equal num-
ber of 0s and 1s is k/(2N + k).
Proof. By the previous discussion, this result is given by
[z2N+k]U(z)k
(
2N + k
N
)
=
k
2N + k.
Here, the coeﬃcients are extracted by a direct application of Lagrange inversion
(see §6.12).
Ļe ballot problem has a rich history, dating back to 1887. For detailed
discussions and numerous related problems, see the books by Feller [9] and
Comtet [7].
BőťśŚŐ ŠŔő ŐŕŞőŏŠ ŞőŘōŠŕśŚşŔŕŜ to trees, the problems like those we have
been considering arise frequently in the analysis of algorithms in connection
with so-called history or sequence of operations analysis of dynamic algorithms
and data structures. For example, the gambler’s ruin problem is equivalent
to determining the probability that a random sequence of “push” and “pop”
operations on an initially empty pushdown stack is “legal” in the sense that it
never tries to pop an empty stack and leaves an empty stack. Ļe ballot prob-
lem generalizes to the situation where the sequence is legal but leaves k items
on the stack. Other applications may involve more operations and diﬀerent
deŀnitions of legal sequences—some examples are given in the exercises be-
low. Such problems typically can be approached via context-free grammars.
A number of applications of this type are discussed in an early paper by Pratt
[29]; see also Knuth [23].
Exercise 8.32 Given a random bitstring of length N, how many of its preŀxes have
equal numbers of 0s and 1s, on the average?
Exercise 8.33 What is the probability that the number of 0s in a random bitstring
never exceeds the number of 1s?
Exercise 8.34 Given a random bitstring of length N, how many of its preŀxes have
k more 0s than 1s, on the average? What is the probability that the number of 0s in
a random bitstring never exceeds the number of 1s by k?

§ȟ.Ȝ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țțȞ
Exercise 8.35 Suppose that a stack has a ŀxed capacity M. What is the probability
that a random sequence of N push and pop operations on an initially empty push-
down stack never tries to pop the stack when it is empty or push when it is full?
Exercise 8.36 [Pratt] Consider a data structure with one “insert” and two diﬀerent
types of “remove” operations. What is the probability that a random sequence of
operations of length N is legal in the sense that the data structure is empty before
and after the sequence, and “remove” is always applied to a nonempty data structure?
Exercise 8.37 Answer the previous exercise, but replace one of the “remove” opera-
tions with an “inspect” operation, which is applied to a nonempty data structure but
does not remove any items.
Exercise8.38 Suppose that a monkey types random parentheses, hitting left and right
with equal probability. What is the expected number of characters typed before the
monkey hits upon a legal balanced sequence? For example, ((())) and (()()())
are legal but ((()) and (()(() are not.
Exercise 8.39 Suppose that a monkey types randomly at a 26-key keyboard that has
26 letters A through Z. What is the expected number of characters typed before the
monkey types a palindrome of length at least 10? Ļat is for some k ≥10, what is the
expected number of characters typed before the last k characters are the same when
taken in reverse order? Example: KJASDLKUYMBUWKASDMBVJDMADAMIMADAM.
Exercise 8.40 Suppose that a monkey types randomly at a 32-key keyboard that has
26 letters A through Z; the symbols +, *, (, and ); a space key; and a period. What is
the expected number of characters typed before the monkey hits upon a legal regular
expression? Assume that spaces can appear anywhere in a regular expression and
that a legal regular expression must be enclosed in parentheses and have exactly one
period, at the end.

țțȟ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ȝ
8.6 Tries. Any set of N distinct bitstrings (which may vary in length) cor-
responds to a trie, a binary tree structure where we associate links with bits.
Since tries can be used to represent sets of bitstrings, they provide an alterna-
tive to binary trees for conventional symbol-table applications. In this section,
we focus on the fundamental relationship between sets of bitstrings and binary
tree structures that tries embody. In the next section, we describe many ap-
plications of tries in computer science. Following that, we look at the analysis
of properties of tries, one of the classic problems in the analysis of algorithms.
Ļen, we brieły discuss the algorithmic opportunities and analytic challenges
presented by extensions to M-way tries and sets of bytestrings (strings drawn
from an M-character alphabet).
We start with a few examples that illustrate how to associate sets of
strings to binary trees. Given a binary tree, imagine that the left links are
labelled 0 and the right links are labelled 1, and identify each external node
with the labels of the links on the path from the root to that node. Ļis gives
a mapping from binary trees to sets of bitstrings. For example, the tree on the
left in Figure 8.4 maps to the set of strings
000 001 01 10 11000 11001 11010 11011 1110 1111,
and the tree on the right to
0000 0001 0010 0011 010 011 100 101 110 111.
(Which set of bitstrings does the trie in the middle represent?) Ļere is one
and only one set of bit strings associated with any given trie in this way. All
implicit labels 
on links
represents
the string 11000
represents
the string 010
0
0
0
1
1
Figure 8.4 Ļree tries, each representing 10 bitstrings

§ȟ.ȝ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țțȠ
the sets of bitstrings that are obtained in this way have by construction the
preŀx-free property: no string is the preŀx of another.
Conversely (and more generally), given a set of bitstrings that satisfy
the preŀx-free property, we can uniquely construct an associated binary tree
structure if we can associate bitstrings with external nodes, by recursively di-
viding the set according to the leading bit of the strings, as in the following
formal deŀnition. Ļis is one of several possible ways to associate tries with
sets of bit strings; we will soon consider alternatives.
Deŀnition Given a set B of bitstrings that is preŀx-free, the associated trie
is a binary tree deŀned recursively as follows: If B is empty, the trie is null
and represented by a void external node. If |B| = 1, the trie consists of one
external node corresponding to the bitstring. Otherwise, deŀne B0 (respec-
tively, B1) to be the set of bitstrings obtained by taking all the members of
B that begin with 0 (respectively, 1) and removing the initial bit from each.
Ļen the trie for B is an internal node connected to the trie for B0 on the left
and the trie for B1 on the right.
A trie for N bitstrings has N nonvoid external nodes, one corresponding
to each bitstring, and may have any number of void external nodes. As earlier,
by considering 0 as “left” and 1 as “right,” we can reach the external node
corresponding to any bitstring by starting at the root and proceeding down
the trie, moving left or right according to the bits in the string read from left
to right. Ļis process ends at the external node corresponding to the bitstring
as soon as it can be distinguished from all the other bitstrings in the trie.
Our deŀnition is convenient and reasonable for studying properties of
tries, but several practical questions naturally arise that are worth considering:
• How do we handle sets of strings that are not preŀx-free?
• What role do void nodes play? Are they necessary?
• Adding more bits to the bitstrings does not change the structure. How
do we handle the leftover bits?
Each of these has important implications, both for applications and analysis.
We consider them in turn.
First, the preŀx-free assumption is justiŀed, for example, if the strings
are inŀnitely long, which is also a convenient assumption to make when ana-
lyzing properties of tries. Indeed, some applications involve implicit bitstrings
that are potentially inŀnitely long. It is possible to handle preŀx strings by

țȜȗ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ȝ
associating extra information with the internal nodes; we leave this variant
for exercises.
Second, the void external nodes correspond to situations where bit-
strings have bits in common that do not distinguish them from other members
of the set. For example, if all the bitstrings start with a 0-bit, then the right
child of the root of the associated trie would be such a node, not correspond-
ing to any bitstring in the set. Such nodes can appear throughout the trie and
need to be marked to distinguish them from external nodes that represent bit
strings. For example, Figure 8.5 shows three tries with 10 external nodes,
of which 3, 8, and 0 are void, respectively. In the ŀgure, the void nodes are
represented by small black squares and the nonvoid nodes (which each corre-
spond to a string) are represented by larger open squares. Ļe trie on the left
represents the set of bitstrings
000 001 11000 11001 11100 11101 1111
and the trie on the right represents the set
0000 0001 0010 0011 010 011 100 101 110 111.
(Which set of bitstrings does the trie in the middle represent?) It is possible to
do a precise analysis of the number of void external nodes needed for random
bitstrings. It is also possible to arrange matters so that unneeded bits do not
directly correspond to unneeded nodes in the trie structure but are represented
otherwise. We discuss these matters in some detail later.
represents
the string 11000
void, does not
represent a string
represents
the string 010
Figure 8.5 Ļree tries, representing 7, 2, and 10 bitstrings, respectively

§ȟ.ȝ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȜȘ
Ļird, the fact that adding more bits to the bitstrings does not change
the structure follows from the “if |B| = 1” clause in the deŀnition, which is
there because in many applications it is convenient to stop the trie branching
as soon as the bitstrings are distinguished. For ŀnite bitstrings, this condition
could be removed, and the branching could continue until the end of each
bitstring is reached. We refer to such a trie as a full trie for the set of bitstrings.
Enumeration. Ļe recursive deŀnition that we have given gives rise to binary
tree structures with the additional properties that (i) external nodes may be
void and (ii) children of leaves must be nonvoid. Ļat is, we never have two
void nodes that are siblings, or a void and a nonvoid node as siblings. To enu-
merate all the diﬀerent tries, we need to consider all the possible trie structures
and all the diﬀerent way to mark the external nodes as void or nonvoid, con-
sistent with these rules. Figure 8.6 shows all the diﬀerent tries with four or
fewer external nodes.
Minimal sets. We also refer to the minimal set of bitstrings for a trie. Ļese
sets are nothing more than encodings of the paths from the root to each non-
void external node. For example, the bitstring sets that we have given for
X3 = 4
X2 = 1
X1 = 1
X4 = 17
Figure 8.6 Tries with 1, 2, 3, and 4 external nodes

țȜș
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ȝ
Figure 8.4 and for Figure 8.5 are both minimal. Figure 8.7 gives the minimal
bitstring sets associated with each of the tree shapes with ŀve external nodes
(each of the tries with ŀve external nodes, all of which are nonvoid). To ŀnd
the minimal bitstring sets associated with any trie with ŀve external nodes,
simply delete the bitstrings corresponding to any void external nodes in the
associated tree structure in the ŀgure.
Wő řŕœŔŠ ōŚōŘťŦő ŜŞśŜőŞŠŕőş of trie structures using the symbolic method,
in a manner analogous to Catalan trees in Chapter 5; we leave such questions
for exercises. Instead, in the context of the analysis of algorithms, we fo-
cus on sets of bitstrings and bitstring algorithms—where tries are most often
used—and concentrate on viewing tries as mechanisms to eﬃciently distin-
guish among a set of strings, and as structures to eﬃciently represent sets of
strings. Moreover, we work with probability distributions induced when the
strings are random, an appropriate model in many situations. Before doing
so, we consider algorithmic applications of tries.
0 0
0 1
1 0 0
1 0 1
1 1
0 0 0 0
0 0 0 1
0 0 1
0 1
1
0 0 0
0 0 1 0
0 0 1 1
0 1
1
0 0 0
0 0 1
0 1 0
0 1 1
1
0 0
0 1 0 0
0 1 0 1
0 1 1
1
0 0
0 1 0
0 1 1 0
0 1 1 1
1
0 0 0
0 0 1
0 1
1 0
1 1
0 0
0 1 0
0 1 1
1 0
1 1
0 0
0 1
1 0
1 1 0
1 1 1
0
1 0 0 0
1 0 0 1
1 0 1
1 1
0
1 0 0
1 0 1 0
1 0 1 1
1 1
0
1 0 0
1 0 1
1 1 0
1 1 1
0
1 0
1 1 0 0
1 1 0 1
1 1 1
0
1 0
1 1 0
1 1 1 0
1 1 1 1
Figure 8.7 Bitstring sets for tries with ŀve external nodes (none void)

§ȟ.Ȟ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȜȚ
Exercise 8.41 Give the three tries corresponding to the minimal sets of strings for
Figure 8.5, but reading each string in right-to-left order.
Exercise 8.42 Ļere are
(8
5
)
= 56 diﬀerent sets of ŀve three-bit bitstrings. Which
trie is associated with the most of these sets? Ļe least?
Exercise 8.43 Give the number of diﬀerent tries that have the same structure as each
of the tries in Figures 8.4 and 8.5.
Exercise 8.44 How many diﬀerent tries are there with N external nodes?
Exercise 8.45 What proportion of the external nodes are void in a “random” trie
(assuming each diﬀerent trie structure to be equally likely to occur)?
Exercise 8.46 Given a ŀnite set of strings, devise a simple test to determine whether
there are any void external nodes in the corresponding trie.
8.7 Trie Algorithms. Binary strings are ubiquitous in digital computing,
and trie structures are naturally associated with sets of binary strings, so it
should not be surprising that there are a number of important algorithmic
applications of tries. In this section, we survey a few such algorithms, to
motivate the detailed study of the properties of tries that we tackle in §8.8.
Tries and digital searching. Tries can be used as the basis for algorithms
for searching through a collection of binary data in a manner similar to binary
search trees, but with bit comparisons replacing key comparisons.
Search tries. Treating bitstrings as keys, we can use tries as the basis for a
conventional symbol table implementation such as Program 6.2. Nonvoid
external nodes hold references to keys that are in the symbol table. To search,
set x to the root and b to 0, then proceed down the trie until an external node
is encountered, incrementing b and setting x to x.left if the bth bit of the
key is 0, or setting x to x.right if the bth bit of the key is 1. If the external
node that terminates the search is void, then the bitstring is not in the trie;
otherwise, we can compare the key to the bitstring referenced by the nonvoid
external node. To insert, follow the same procedure, then store a reference to
the key in the void external node that terminates the search, making it non-
void. Ļis can be a very eﬃcient search algorithm under proper conditions
on the set of keys involved; for details see Knuth [23] or Sedgewick [32]. Ļe
analysis that we will consider can be used to determine how the performance

țȜț
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ȟ
of tries might compare with that of binary search trees for a given applica-
tion. As discussed in Chapter 1, the ŀrst consideration in attempting to an-
swer such a question is to consider properties of the implementation. Ļis is
especially important in this particular case, because accessing individual bits
of keys can be very expensive on some computers if not done carefully.
Patricia tries. We will see below that about 44% of the external nodes in a
random trie are void. Ļis factor may be unacceptably high. Ļe problem can
be avoided by “collapsing” one-way internal nodes and keeping the index of
the bit to be examined with each node. Ļe external path length of this trie is
somewhat smaller, though some extra information has to be associated with
each node. Ļe critical property that distinguishes Patricia tries is that there
are no void external nodes, or, equivalently, there are N −1 internal nodes.
Various techniques are available for implementing search and insertion using
Patricia tries. Again, details may be found in Knuth [23] or Sedgewick [32].
Radix-exchange sort. As mentioned in Chapter 1, a set of bitstrings of equal
length can be sorted by partitioning them to put all those beginning with 0
before all those beginning with 1 (using a process similar to the partitioning
process of quicksort) then sorting the two parts recursively. Ļis method,
called radix-exchange sort, bears the same relationship to tries as quicksort does
to binary search trees. Ļe time required by the sort is essentially proportional
to the number of bits examined. For keys comprised of random bits, this turns
out to be the same as the “nonvoid external path length” of a random trie—the
sum of the distances from the root to each of the nonvoid external nodes.
Trie encoding. Any trie with labelled external nodes deŀnes a preŀx code for
the labels of the nodes. For example, if the external nodes in the trie on the
left in Figure 8.4 are labelled, left to right, with the letters
(space) D O E F C R I P X
then the bitstring
1110110101011000110111111000110010100110
encodes the phrase
PREFIX CODE.
Decoding is simple: starting at the root of the trie and the beginning of the
bitstring, travel through the trie as directed by the bitstring (left on 0, right on

§ȟ.Ȟ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȜȜ
1), and, each time an external node is encountered, output the label and restart
at the root. If frequently used letters are assigned to nodes with short paths,
then the number of bits used in such an encoding will be signiŀcantly fewer
than for the standard encoding. Ļe well-known Huﬀman encoding method
ŀnds an optimal trie structure for given letter frequencies (see Sedgewick and
Wayne [33] for details).
Tries and pattern matching. Tries can also be used as a basic data structure
for searching for multiple patterns in text ŀles. For example, tries have been
used successfully in the computerization of large dictionaries for natural lan-
guages and other similar applications. Depending upon the application, the
trie can contain the patterns or the text, as described below.
String searching with suﬃx tries. In an application where the text string is
ŀxed (as for a dictionary) and many pattern lookups are to be handled, the
search time can be dramatically reduced by preprocessing the text string, as
follows: Consider the text string to be a set of N strings, one starting at each
position of the text string and running to the end of the string (stopping k
characters from the end, where k is the length of the shortest pattern to be
sought). Build a trie from this set of strings (such a trie is called the suﬃx trie
for the text string). To ŀnd out whether a pattern occurs in the text, proceed
down the trie from the root, going left on 0 and right on 1 as usual, according
to the pattern bits. If a void external node is hit, the pattern is not in the text;
if the pattern exhausts on an internal node, it is in the text; and if an external
node is hit, compare the remainder of the pattern to the text bits represented
in the external node as necessary to determine whether there is a match. Ļis
algorithm was used eﬀectively in practice for many years before it was ŀnally
shown by Jacquet and Szpankowski that a suﬃx trie for a random bitstring
is roughly equivalent to a search trie built from a set of random bitstrings
[21][22]. Ļe end result is that a string search requires a small constant times
lgN bit inspections on the average—a very substantial improvement over the
cost of the basic algorithm in situations where the initial cost of building the
trie can be justiŀed (for example, when a huge number of patterns are to be
sought in the same text).
Searching for multiple patterns. Tries can also be used to ŀnd multiple pat-
terns in one pass through a text string, as follows: First, build a full trie from
the pattern strings. Ļen, for each position i in the text string, start at the top
of the trie and match characters in the text while proceeding down the trie,

țȜȝ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ȟ
going left for 0s in the text and right for 1s. Such a search must terminate at
an external node. If the external node is not void, then the search was success-
ful: one of the strings represented by the trie was found starting at position i
in the text. If the external node is void, then none of the strings represented
by the trie starts at i, so that the text pointer can be incremented to i + 1 and
the process restarted at the top of the trie. Ļe analysis below implies that
this requires O(NlgM) bit inspections, as opposed to the O(NM) cost of
applying the basic algorithm M times.
Trie-based ŀnite-state automata.
When the search process just described
terminates in a void external node, we can do better than going to the top of
the trie and backing up the text pointer, in precisely the same manner as with
the Knuth-Morris-Pratt algorithm. A termination at a void node tells us not
just that the sought string is not in the database, but also which characters in
the text precede this mismatch. Ļese characters show exactly where the next
search will require us to examine a text character where we can totally avoid
the comparisons wasted by the backup, just as in the KMP algorithm. Indeed,
Program 8.2 can be used for this application with no modiŀcation; we need
only build the FSA corresponding to a set of strings rather than to a single
string. For example, the FSA depicted in Figure 8.8 corresponds to the set
of strings 000 011 1010. When this automaton is run on the sample piece
of text shown on the next page, it takes the state transitions as indicated—
0
1
0
0
1
1
0
1
2
0
1
0
1
4
0
1
5
0
1
6
7
8
9
3
0  1  2  3  4  5  6
1  3  5  7  5  3  9
2  4  2  4  8  6  8
state
0-transition
1-transition
Figure 8.8 Aho-Corasick FSA for 000, 011, and 1010

§ȟ.Ȟ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȜȞ
below each character is given the state the FSA is in when that character is
examined.
11110010010010111010000011010011000001010111010001
02222534534534568
In this case, the FSA stops in state 8, having found the pattern 011. Ļe
process of building such an automaton for a given set of patterns is described
by Aho and Corasick [1].
Distributed leader election. Ļe random trie model is very general. It corre-
sponds to a general probabilistic process where “individuals” (keys, in the case
of trie search) are recursively separated by coin łippings. Ļat process can be
taken as the basis of various resource allocation strategies, especially in a dis-
tributed context. As an example, we will consider the following distributed
algorithm for electing a leader among N individuals sharing an access chan-
nel. Ļe method proceeds by rounds; individuals are selected or eliminated
according to coin łips. Given a set of individuals:
• If the set is empty, then report failure.
• If the set has one individual, then declare that individual the leader.
• If the set has more than one individual, łip independent 0-1 coins for
all members of the set and invoke the procedure recursively for the subset
of individuals who got 1.
AA AB AL CN MC MS PD
0  1  1  1  1  0  1
AA MS
0  0
AB AL CN MC PD
1  1  1  1  1
AA MS
1  0
AB AL CN MC PD
1  0  1  1  0
AB CN MC
1  0  0
MS
AA
AL PD
1  0
AL PD
0  0
PD
AL
CN MC
1  0
MC
CN
AB
the winner!
Figure 8.9 Distributed leader election

țȜȟ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ȟ
Figure 8.9 shows an example. We show the full trie where we imag-
ine that the losers need to select a winner among that group, and so forth,
extending the method to give a full ranking of the individuals. At the ŀrst
stage, AB, AL, CN, MC, and PD all łip heads and survive. At the second stage,
they all łip heads again, so no one is eliminated. Ļis leads to a void node
in the trie. Eventually, AB is declared the winner, the only individual to łip
heads at every opportunity. If we start with N individuals, then we expect N
to be reduced roughly to N/2, N/4, . . . in the course of the execution of the
algorithm. Ļus, we expect the procedure to terminate in about lgN steps,
and a more precise analysis may be desirable. Also, the algorithm may fail
(if everyone łips 0 and is eliminated with no leader elected), and we are also
interested in knowing the probability that the algorithm is successful.
TŔŕş ŘŕşŠ śŒ ōŘœśŞŕŠŔřş and applications is representative, and demon-
strates the fundamental importance of the trie data structure in computer ap-
plications. Not only are tries important as explicit data structures, but also
they arise implicitly in algorithms based on bits, or in algorithms where truly
“binary” decisions are made. Ļus, analytic results describing properties of
random tries have a variety of applications.
Exercise 8.47 How many bits are examined when using the trie in the middle in
Figure 8.5 to search for one of the patterns 1010101010 or 1010101011 in the
text string 10010100111110010101000101010100010010?
Exercise 8.48 Given a set of pattern strings, describe a method for counting the
number of times one of the patterns occurs in a text string.
Exercise 8.49 Build the suﬃx trie for patterns of eight bits or longer from the text
string 10010100111110010101000101010100010010.
Exercise 8.50 Give the suﬃx tries corresponding to all four-bit strings.
Exercise 8.51 Give the Aho-Corasick FSA for the set of strings 01 100 1011 010.

§ȟ.ȟ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȜȠ
8.8 Combinatorial Properties of Tries. As combinatorial objects, tries
have been studied only recently, especially by comparison with classical com-
binatorial objects such as permutations and trees. As we will see, full under-
standing of even the most basic properties of tries requires the full array of
analytic tools that we consider in this book.
Certain properties of tries naturally present themselves for analysis. How
many void external nodes might be expected? What is the average external
path length or the average height? As with binary search trees, knowledge of
these basic properties gives the information necessary to analyze string search-
ing and other algorithms that use trees.
A related, more fundamental, point to consider is that the model of
computation used in the analysis needs to diﬀer in a fundamental way: for
binary search trees, only the relative order of the keys is of interest; for tries,
the binary representation of the keys as bitstrings must come into play. What
exactly is a random trie? Ļough several models are possible, it is natural to
consider a random trie to be one built from a set of N random inŀnite bit-
strings. Ļis model is appropriate for many of the important trie algorithms,
such as symbol table implementations for bitstring keys. As mentioned ear-
lier, the model is suﬃciently robust that it well approximates other situations
as well, such as suﬃx tries.
Ļus, we will consider the analysis of properties of tries under the as-
sumption that each bit in each bitstring is independently 0 or 1 with proba-
bility 1/2.
Ļeorem 8.8 (Trie path length and size).
Ļe trie corresponding to N ran-
dom bitstrings has external path length ∼NlgN, on the average. Ļe mean
number of internal nodes is asymptotic to (1/ln2 ± 10−5)N.
Proof. We start with a recurrence: for N > 0, the probability that exactly k
of the N bitstrings begins with a 0 is the Bernoulli probability
(N
k
)/2N, so if
we deŀne CN to be the average external path length in a trie corresponding
to N random bitstrings, we must have
CN = N + 1
2N
∑
k
(
N
k
)
(Ck + CN−k)
for N > 1 with C0 = C1 = 0.
Ļis is precisely the recurrence describing the number of bit inspections used
by radix-exchange sort that we examined for Ļeorem 4.9 in §4.9, where we

țȝȗ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ȟ
showed that
CN = N![zN]C(z) = N
∑
j≥0
(
1 −
(
1 −1
2j
)N−1)
and then we used the exponential approximation to deduce that
CN ∼N
∑
j≥0
(1 −e−N/2j) ∼NlgN.
A more precise estimate exposes a periodic łuctuation in the value of
this quantity as N increases. As we saw in Chapter 4, the terms in the sum
are exponentially close to 1 for small k and exponentially close to 0 for large
k, with a transition when k is near lgN (see Figure 4.5). Accordingly, we split
the sum:
CN/N ∼
∑
0≤j<⌊lgN⌋
(1 −e−N/2j) +
∑
j≥⌊lgN⌋
(1 −e−N/2j)
= ⌊lgN⌋−
∑
0≤j<⌊lgN⌋
e−N/2j +
∑
j≥⌊lgN⌋
(1 −e−N/2j)
= ⌊lgN⌋−
∑
j<⌊lgN⌋
e−N/2j +
∑
j≥⌊lgN⌋
(1 −e−N/2j) + O(e−N)
= ⌊lgN⌋−
∑
j<0
e−N/2j+⌊lgN⌋+
∑
j≥0
(1 −e−N/2j+⌊lgN⌋) + O(e−N).
Now, separating out the fractional part of lgN as we did earlier in Chapter 2
(see Figures 2.3 and 2.4), we have
⌊lgN⌋= lgN −{lgN}
and
N/2⌊lgN⌋= 2lgN−⌊lgN⌋= 2{lgN}.
Ļis leads to the expression
CN/N ∼lgN −ϵ(N)
where
ϵ(N) ≡{lgN} +
∑
j<0
e−2{lgN}−j −
∑
j≥0
(1 −e−2{lgN}−j).

§ȟ.ȟ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȝȘ
Ļis function is clearly periodic, with ϵ(2N) = ϵ(N) because its dependence
on N is solely in terms of {lgN}. Ļis observation does not immediately rule
out the possibility that ϵ(N) might be constant, but it is easy to check that
it is not: (CN −NlgN)/N does not approach a limit, but it łuctuates as N
grows. Figure 8.10 is a plot of the function after subtracting its mean value
for N < 256, and Table 8.7 shows exact values of ϵ(N) for 4 ≤N ≤16
(also for any power of 2 times these values). Ļe function ϵ(N) is very close
to 1.332746 numerically, and the amplitude of the łuctuating part is < 10−5.
A similar analysis can be used to analyze the number of internal nodes
in a trie on N bitstrings, which is described by the recurrence
AN = 1 + 1
2N
∑
k
(
N
k
)
(Ak + AN−k)
for N > 1 with A0 = A1 = 0.
In this case, the periodic łuctuation appears in the leading term:
AN ∼N
ln2(1 + ^ϵ(N))
where the absolute value of ^ϵ(N) is less than 10−5. Ļe details of this analysis
are similar to the preceding analysis, and are left as an exercise.
N
{lgN}
∑
j<0 e−2{lgN}−j
∑
j≥0(1 −e−2{lgN}−j)
−ϵ(N)
4
0.000000000
0.153986497
1.486733879
1.332747382
5
0.321928095
0.088868348
1.743543002
1.332746559
6
0.584962501
0.052271965
1.969979089
1.332744624
7
0.807354922
0.031110097
2.171210673
1.332745654
8
0.000000000
0.153986497
1.486733879
1.332747382
9
0.169925001
0.116631646
1.619304291
1.332747643
10
0.321928095
0.088868348
1.743543002
1.332746559
11
0.459431619
0.068031335
1.860208218
1.332745265
12
0.584962501
0.052271965
1.969979089
1.332744624
13
0.700439718
0.040279907
2.073464469
1.332744844
14
0.807354922
0.031110097
2.171210673
1.332745654
15
0.906890596
0.024071136
2.263708354
1.332746622
16
0.000000000
0.153986497
1.486733879
1.332747382
γ/ln2 + 1/2 ≈1.332746177
Table 8.8
Periodic term in trie path length

țȝș
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ȟ
0
32
64
128
256
0
0.00000156
— 0.00000156
Figure 8.10 Periodic łuctuation in trie path length
Ļis result is due to Knuth [23], who derived explicit expressions for
the mean value and for ϵ(N). Ļis and related problems have been studied
in detail by Guibas, Ramshaw, and Sedgewick [19] and by Flajolet, Gour-
don, and Dumas [10] (see also [12] and [37]). Knuth’s analysis uses complex
asymptotics and introduced to the analysis of algorithms a method he called
the “Gamma-function method.” It is a special case of the Mellin transform,
which has emerged as one of the most important tools in analytic combina-
torics and the analysis of algorithms. For example, this method is eﬀective in
explaining the periodic terms in the analysis of mergesort (see §2.6) and the
similar terms that arise in the analysis of many other algorithms. We con-
clude this section with a brief discussion of some of the algorithms that we
have already discussed.
Trie search. Ļeorem 8.8 directly implies that the average number of bits in-
spected during a search in a trie built from random bitstrings is ∼lgN, which
is optimal. Knuth also shows that this is true for both Patricia and digital
tree searching (a variant where keys are also stored in internal nodes). It is
possible to get accurate asymptotics and explicit expressions for the constants
involved and for the oscillating term; see the references mentioned above for
full details.

§ȟ.ȟ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȝȚ
Figure 8.11
NlgN −1.333N
Distributions for path length in tries, even N from 10 to 50
(scaled and translated to center and separate curves)
Radix-exchange sort. As noted in the proof, Ļeorem 8.8 also directly im-
plies that radix-exchange sort uses ∼NlgN bit comparisons to sort an array
of N random bitstrings, since the average number of bit inspections for the
sort is the same as the average path length of the trie. Again, our assumption
is that the bitstrings are suﬃciently long (say, much more than lgN bits) that
we can consider them to be “inŀnite.” Figure 8.11 shows the distribution of
costs, for comparison with Figure 1.3.
Exercise 8.52 Show that AN/N is equal to 1/ln2 plus a łuctuating term.
Exercise 8.53 Write a program to compute AN to within 10−9 for N < 106 and
explore the oscillatory nature of AN/N.
Exercise 8.54 Multiply both sides of the functional equation for C(z) by e−z to
transform it into a simpler equation on ˆC(z) ≡e−zC(z). Use this equation to ŀnd
ˆCN = [zN] ˆC(z). Ļen apply the convolution implied by C(z) = ez ˆC(z) to show
that
CN =
∑
2≤k≤N
(N
k
)
k(−1)k
1 −1/2k−1 .
Exercise 8.55 Show directly that the sum given in the previous exercise is equivalent
to the expression for CN given in the proof of Ļeorem 8.8.

țȝț
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.ȟ
Distributed leader election. As an example of the analysis of an algorithm
where we do not build an explicit trie, where the model of inŀnite random
strings is certainly valid, but where analysis of properties of tries is still of
practical interest, we turn to the distributed leader election problem that we
introduced at the end of the previous section.
Ļeorem 8.9 (Leader election).
Ļe average number of rounds used by the
randomized algorithm to seek a leader out of N contenders is lgN + O(1),
with probability of success asymptotic to 1/(2ln2) ± 10−5 ≈.72135.
Proof. By representing an execution of the algorithm as a trie, it is clear that
the average number of rounds is the expected length of the rightgoing branch
in a trie built from N random bitstrings, and satisŀes the recurrence
RN = 1 + 1
2N
∑
k
(
N
k
)
Rk
for N > 1 with R0 = R1 = 0.
Similarly, the probability of success satisŀes the recurrence
pN = 1
2N
∑
k
(
N
k
)
pk
for N > 1 with p0 = 0 and p1 = 1.
Ļese recurrences are quite similar to the path length recurrence that we con-
sidered in detail earlier. Solving them is left for exercises. As usual, the stated
result for pN is accurate to within an oscillating term of mean 0 and amplitude
less than 10−5.
If the method fails, it can be executed repeatedly to yield an algorithm
that succeeds with probability 1. On average, this will require about 2ln2 ≈
1.3863 iterations, for a grand total of ∼2lnN rounds. Ļis algorithm is a
simpliŀed version of an algorithm of Prodinger that succeeds in an average of
lgN + O(1) rounds [30].
Exercise 8.56 Solve the recurrence for RN given in the proof of Ļeorem 8.9, to
within the oscillating term.
Exercise 8.57 Solve the recurrence for pN given in the proof of Ļeorem 8.9, to
within the oscillating term.
Exercise8.58 Analyze the version of the leader election algorithm that repeats rounds
until a success occurs.

§ȟ.Ƞ
S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȝȜ
8.9 Larger Alphabets. Ļe results of this chapter generalize to strings
comprising characters from alphabets of size M with M > 2. None of the
combinatorial techniques used depends in an essential way on the strings be-
ing binary. In practice, it is reasonable to assume that the alphabet size M is
a constant that is not large: values such as 26, 28, or even 216 are what might
be expected in practice. We will not examine results for larger alphabets in
detail, but we will conclude with a few general comments.
In §8.2 we noted that the ŀrst run of P 0s in a random M-bytestring
ends at position M(M P −1)/(M −1). Ļis makes it seem that the alphabet
size can be a very signiŀcant factor: for example, we expect to ŀnd a string of
ten 0s within the ŀrst few thousand bits of a random bitstring, but a string
of ten identical characters would be extremely unlikely in a random string
composed of eight-bit bytes, since the expected position of the ŀrst such string
is about at the 280th byte. But this is just looking at the results of the analysis
in two ways, since we can transform a bitstring into a string of bytes in the
obvious way (consider eight bits at a time). Ļat is, the alphabet size is less
of a factor than it seems, because it intervenes only through its logarithm.
From both practical and theoretical standpoints, it generally suﬃces to
consider bitstrings instead of bytestrings, since the straightforward encoding
can transform any eﬀective algorithm on bytestrings into an eﬀective algo-
rithm on bitstrings, and vice versa.
Ļe primary advantage of considering bits as groups is that they can be
used as indices into tables or treated as integers, in essence using the computer
addressing or arithmetic hardware to match a number of bits in parallel. Ļis
can lead to substantial speedups for some applications.
Multiway tries. If M is not large (say, keys are decimal numbers), multiway
tries are the method of choice for symbol table applications. An M-ary trie
built from a random set of M-bytestrings with M nodes has external path
length asymptotic to NlgMN, on the average. Ļat is, using an alphabet of
size M can save a factor of lgN/lgM in search time. But this comes at a cost
of about M times more storage, since each node must have M links, most of
which will be null if M is large. Various modiŀcations to the basic algorithm
have been developed to address the excess space usage, and Knuth’s analysis
extends to help evaluate the schemes (see [23]).
Ternary tries. Bentley and Sedgewick [2] developed a way to represent tries
with ternary trees, which essentially amounts to replacing each trie node with

țȝȝ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
§ȟ.Ƞ
a BST symbol table where keys are characters and links are values. Ļis
approach keeps the space cost low (3N links) while still limiting the number
of characters examined for a search miss. Clément, Flajolet, and Vallée [4][5]
showed that ∼lnN characters are examined for a search miss in a ternary tree
as opposed to ∼2lnN string compares in a binary search tree. Ļis analysis
used a “dynamical systems” model that extends to give a true comparison of
the costs of searching and sorting with string keys [6]. TSTs are the method
of choice for string keys as the implementations are compact, easy to under-
stand, and eﬃcient (see Sedgewick and Wayne [33] for details).
Right-left string searching. Consider the problem of searching for a (rela-
tively short) pattern in a (relatively long) text when both are bytestrings. To
begin the search, we test the Mth character in the text with the last character
in the pattern. If it matches, we then test the (M −1)st text character against
the next-to-last pattern character, and so on until a mismatch is found or a
full match is veriŀed. One approach to this process is to develop an “optimal
shift” based on the mismatch, in much the same way as for the KMP algo-
rithm. But a simpler idea is probably more eﬀective in this case. Suppose
that the text character does not appear in the pattern. Ļen this one character
is evidence that we can skip M more characters, because any positioning of
the right end of the pattern over any of those characters would necessitate
matching the “current” character, which does not appear in the pattern. For
many reasonable values of the relevant parameters, this event is likely, so this
takes us close to the goal of examining N/M text characters when searching
for a pattern of length M in a text string of length N. Even when the text
character does appear in the pattern, we can precompute the appropriate shift
for each character in the alphabet (distance from the right end of the pattern)
to line up the pattern with its “next possible” match in the text. Details of
this method, which originates from ideas of Boyer and Moore, are discussed
by Gonnet and Baeza-Yates [15].
LZW data compression. Multiway tries or ternary trees are the method of
choice for implementing one of the most widely used data compression algo-
rithms: the Lempel-Ziv-Welch (LZW) method [39][38]. Ļis algorithm
requires a data structure that can support the “longest preŀx match” and “in-
sert a new key formed by appending a character to an existing key” operations,
which are perfectly suited to a trie implementation. See [33] for details. Ļis
is one of a family of algorithms that dates to the 1978 seminal paper by Ziv

S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȝȞ
and Lempel. Despite their importance, the analysis of the eﬀectiveness of
these algorithms was open for nearly two decades until it was resolved by
Jacquet, Szpankowski, and Louchard in the 1990s. Ļis work is founded on
basic research on the properties of tries that applies to many other problems.
Details may be found in Szpankowski’s book [35].
Exercise 8.59 How many bits are examined, on the average, by an algorithm that
searches for runs of M 0s in a text string (M not small) by, for k = 1, 2, 3, and
higher, checking the t bits ending at kM and, if they are all 0, checking the bits on
each side to determine the length of the run? Assume that the text string is random
except for a run of M 0s hidden somewhere within it.
Exercise 8.60 Give a way to adapt the method of the previous exercise to ŀnd the
longest string of 0s in a random bitstring.
B
ITSTRINGS are arguably the fundamental combinatorial object for com-
puter science, and we have touched on a number of basic paradigms
in considering the combinatorics of strings and tries. Some of the results
that we have examined relate to classical results from probability theory, but
many others relate both to fundamental concepts of computer science and to
important practical problems.
In this chapter, we discussed a number of basic string-processing algo-
rithms. Such algorithms are quite interesting from the standpoint of algo-
rithm design and analysis, and they are the focus of extensive research because
of the importance of modern applications that range from processing genetic
sequencing data to developing search engines for the Internet. Indeed, such
applications are so important (and the topic so interesting) that the study of
strings has emerged on its own as an important area of research, with several
books devoted to the topic [20][27][28][35].
Considering sets of strings leads to the study of formal languages, and the
useful result that there is a speciŀc connection between fundamental prop-
erties of formal language systems and analytic properties of the generating
functions. Not the least of the implications of this is that it is possible to
analyze a wide class of problems in an “automatic” fashion, particularly with
the aid of modern computer algebra systems.
Ļe trie is a combinatorial structure and a data structure that is of im-
portance in all of these contexts. It is a practical data structure for classical
search problems and it models a basic sort procedure; it is a natural repre-
sentation for ŀxed sets of strings and applicable to string-search problems; as

țȝȟ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
a type of tree, it directly relates to the basic divide-and-conquer paradigm;
and association with bits relates it directly to low-level representations and
probabilistic processes.
Finally, analysis of the basic properties of tries presents a signiŀcant
challenge that cannot be fully met with elementary methods. Detailed anal-
ysis requires describing oscillatory functions. As we have seen, this kind
of function arises in a variety of algorithms, because many algorithms pro-
cess binary strings either explicitly (as in this chapter) or implicitly (as in
algorithms based on divide-and-conquer recursion). Advanced mathematical
techniques to deal with the kinds of functions that arise in such problems are a
prime focus of modern research in analytic combinatorics (see [10][12][35]).

S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȝȠ
References
1. A. V. AŔś ōŚŐ M. J. CśŞōşŕŏŗ. “Eﬃcient string matching: An aid to
bibliographic search,” Communications of the ACM 18, 1975, 333–340.
2. J. BőŚŠŘőť ōŚŐ R. SőŐœőţŕŏŗ. “Fast algorithms for sorting and search-
ing strings,” 8th Symposium on Discrete Algorithms, 1997, 360–369.
3. N. CŔśřşŗť ōŚŐ M. P. SŏŔƩŠŦőŚŎőŞœőŞ. “Ļe algebraic theory of
context-free languages,” in Computer Programming and Formal Languages,
P. Braﬀort and D. Hirschberg, eds., North Holland, 1963, 118–161.
4. J. CŘŻřőŚŠ, J. A. FŕŘŘ, P. FŘōŖśŘőŠ, ōŚŐ B. VōŘŘŻő. “Ļe number
of symbol comparisons in quicksort and quickselect,” 36th International
Colloquium on Automata, Languages, and Programming, 2009, 750–763.
5. J. CŘŻřőŚŠ, P. FŘōŖśŘőŠ, ōŚŐ B. VōŘŘŻő. “Analysis of hybrid tree
structures,” 9th Symposium on Discrete Algorithms, 1998, 531–539.
6. J. CŘŻřőŚŠ, P. FŘōŖśŘőŠ, ōŚŐ B. VōŘŘŻő. “Dynamical sources in in-
formation theory: A general analysis of trie structures,” Algorithmica 29,
2001, 307–369.
7. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
8. S. EŕŘőŚŎőŞœ. Automata, Languages, and Machines, Volume A, Aca-
demic Press, New York, 1974.
9. W. FőŘŘőŞ. An Introduction to Probability Ļeory and Its Applications,
John Wiley, New York, 1957.
10. P. FŘōŖśŘőŠ, X. GśšŞŐśŚ, ōŚŐ P. Dšřōş. “Mellin transforms and
asymptotics: Harmonic sums,” Ļeoretical Computer Science 144, 1995,
3–58.
11. P. FŘōŖśŘőŠ, M. RőœŚŕőŞ, ōŚŐ D. SśŠŠőōš. “Algebraic methods for
trie statistics,” Annals of Discrete Math. 25, 1985, 145–188.
12. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
13. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. “Digital search trees revisited,” SIAM
Journal on Computing 15, 1986, 748–767.
14. K. O. GőŐŐőş, S. R. CŦōŜśŞ, ōŚŐ G. LōŎōŔŚ. Algorithms for Computer
Algebra, Kluwer Academic Publishers, Boston, 1992.

țȞȗ
C Ŕ ō Ŝ Š ő Ş
E ŕ œ Ŕ Š
15. G. H. GśŚŚőŠ ōŚŐ R. BōőŦō-YōŠőş. Handbook of Algorithms and Data
Structures in Pascal and C, 2nd edition, Addison-Wesley, Reading, MA,
1991.
16. I. GśšŘŐőŚ ōŚŐ D. JōŏŗşśŚ. Combinatorial Enumeration, John Wiley,
New York, 1983.
17. L. GšŕŎōş ōŚŐ A. OŐŘťŦŗś. “Periods in strings,” Journal of Combina-
torial Ļeory, Series A 30, 1981.
18. L. GšŕŎōş ōŚŐ A. OŐŘťŦŗś. “String overlaps, pattern matching, and
nontransitive games,” Journal of Combinatorial Ļeory, Series A 30, 1981,
19–42.
19. L. GšŕŎōş, L. RōřşŔōţ, ōŚŐ R. SőŐœőţŕŏŗ. Unpublished work, 1979.
20. D. GšşŒŕőŘŐ. Algorithms on Strings, Trees and Sequences: Computer Sci-
ence and Computational Biology, Cambridge University Press, 1997.
21. P. JōŏŝšőŠ ōŚŐ W. SŦŜōŚŗśţşŗŕ. “Analytic approach to pattern match-
ing,” in [28].
22. P. JōŏŝšőŠ ōŚŐ W. SŦŜōŚŗśţşŗŕ. “Autocorrelation on words and its
applications,” Journal of Combinatorial Ļeory, Series A 66, 1994, 237–
269.
23. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.
24. D. E. KŚšŠŔ. “Ļe average time for carry propagation,” Indagationes
Mathematicae 40, 1978, 238–242.
25. D. E. KŚšŠŔ, J. H. MśŞŞŕş, ōŚŐ V. R. PŞōŠŠ. “Fast pattern matching
in strings,” SIAM Journal on Computing, 1977, 323–350.
26. M. LśŠŔōŕŞő. Applied Combinatorics on Words, Encylopedia of Mathe-
matics and its Applications 105, Cambridge University Press, 2005.
27. M. LśŠŔōŕŞő. Combinatorics on Words, Addison-Wesley, Reading, MA,
1983.
28. G. LśšŏŔōŞŐ ōŚŐ W. SŦŜōŚŗśţşŗŕ. “On the average redundancy rate
of the Lempel-Ziv code,” IEEE Transactions on Information Ļeory 43,
1997, 2–8.
29. V. PŞōŠŠ. “Counting permutations with double-ended queues, parallel
stacks and parallel queues,” in Proceedings 5th Annual ACM Symposium
on Ļeory of Computing, 1973, 268–277.

S Š Ş ŕ Ś œ ş
ō Ś Ő
T Ş ŕ ő ş
țȞȘ
30. H. PŞśŐŕŚœőŞ. “How to select a loser,” Discrete Mathematics 120, 1993,
149–159.
31. A. SōŘśřōō ōŚŐ M. SśŕŠŠśŘō. Automata-Ļeoretic Aspects of Formal
Power Series, Springer-Verlag, Berlin, 1978.
32. R. SőŐœőţŕŏŗ. Algorithms (3rd edition) in Java: Parts 1-4: Fundamen-
tals, Data Structures, Sorting, and Searching, Addison-Wesley, Boston,
2003.
33. R. SőŐœőţŕŏŗ ōŚŐ K. WōťŚő. Algorithms, 4th edition, Addison-Wesley,
Boston, 2011.
34. N. SŘśōŚő ōŚŐ S. PŘśšŒŒő. Ļe Encyclopedia of Integer Sequences, Aca-
demic Press, San Diego, 1995. Also accessible as On-Line Encyclopedia
of Integer Sequences, http://oeis.org.
35. W. SŦŜōŚŗśţşŗŕ. Average-Case Analysis of Algorithms on Sequences, John
Wiley and Sons, New York, 2001.
36. L. TŞōŎŎ-PōŞŐś. Ph.D. thesis, Stanford University, 1977.
37. J. S. VŕŠŠőŞ ōŚŐ P. FŘōŖśŘőŠ, “Analysis of algorithms and data struc-
tures,” in Handbook of Ļeoretical Computer Science A: Algorithms and Com-
plexity, J. van Leeuwen, ed., Elsevier, Amsterdam, 1990, 431–524.
38. T. A. WőŘŏŔ, “A Technique for high-performance data compression,”
IEEE Computer, June 1984, 8–19.
39. J. ZŕŢ ōŚŐ A. LőřŜőŘ, “Compression of individual sequences via variable-
rate encoding,” IEEE Transactions on Information Ļeory, September 1978.

This page intentionally left blank 

C H A P T E R N I N E
W O R D S A N D M A P P I N G S
S
TRINGS of characters from a ŀxed alphabet, or words, are of interest in
a broad variety of applications beyond the types of algorithms considered
in the previous chapter. In this chapter, we consider the same family of com-
binatorial objects studied in Chapter 8 (where we called them “bytestrings”),
but from a diﬀerent point of view.
A word may be viewed as a function taking an integer i in the interval 1
to N (the character position) into another integer j in the interval 1 to M (the
character value, from an M-character alphabet). In the previous chapter, we
primarily considered “local” properties, involving relationships among values
associated with successive indices (correlations and so forth); in this chapter,
we consider “global” properties, including the frequency of occurrence of val-
ues in the range and more complex structural properties. In Chapter 8, we
generally considered the alphabet size M to be a small constant and the string
size N to be large (even inŀnite); in this chapter, we consider various other
possibilities for the relative values of these parameters.
As basic combinatorial objects, words arise often in the analysis of algo-
rithms. Of particular interest are hashing algorithms, a fundamental and
widely used family of algorithms for information retrieval. We analyze a
number of variations of hashing, using the basic generating function count-
ing techniques of Chapter 3 and asymptotic results from Chapter 4. Hashing
algorithms are very heavily used in practice, and they have a long history in
which the analysis of algorithms has played a central role. Conversely, the
ability to accurately predict performance of this important class of practical
methods has been an impetus to the development of techniques for the anal-
ysis of algorithms. Indeed, Knuth mentions that the analysis of a hashing
algorithm had a “strong inłuence” on the structure of his pioneering series of
books [23][24][25][27].
Ļe elementary combinatorial properties of words have been heavily
studied, primarily because they model sequences of independent Bernoulli
trials. Ļe analysis involves properties of the binomial distribution, many of
which we have already examined in detail in Chapters 3 and 4. Some of the
țȞȚ

țȞț
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ș
problems we consider are called occupancy problems, because they can be cast
in a model where N balls are randomly distributed into M urns, and the “oc-
cupancy distribution” of the balls in the urns is to be studied. Many of these
classical problems are elementary, though, as usual, simple algorithms can
lead to variations that are quite diﬃcult to analyze.
If the alphabet size M is small, the normal approximation to the bino-
mial distribution is appropriate for studying occupancy problems; if M grows
with N, then we use the Poisson approximation. Both situations arise in the
analysis of hashing algorithms and in other applications in the analysis of
algorithms.
Finally, we introduce the concept of a mapping: a function from a ŀnite
domain into itself. Ļis is another fundamental combinatorial object that
arises often in the analysis of algorithms. Mappings are related to words in
that they might be viewed as N-letter words in an N-letter alphabet, but it
turns out that they are also related to trees, forests, and permutations; anal-
ysis of their properties unveils a rich combinatorial structure that generalizes
several of those that we have studied. Ļe symbolic method is particularly
eﬀective at helping us study the basic properties of mappings. We conclude
with an application of the properties of random mappings in an algorithm for
factoring integers.
9.1 Hashing with Separate Chaining. Program 9.1 shows a standard
method for information retrieval that cuts the search time through a table of
N keys by a factor of M. By making M suﬃciently large, it is often possible
to make this basic algorithm outperform the symbol-table implementations
based on arrays, trees, and tries that we examined in §2.6, §6.6, and §8.7.
We transform each key into an integer between 1 and M using a so-
called hash function, assuming that, for any key, each value is equally likely
to be produced by the hash function. When two keys hash to the same value
(we refer to this as a collision), we keep them in a separate symbol table. To
ŀnd out if a given key is in the table, we use the hash function to identify
the secondary table containing the keys with the same hash value and do a
secondary search there. Ļis is simple to arrange in modern programming
languages that support data abstraction: we maintain an array st[] of symbol
tables where st[i] holds all the keys that hash to the value i. A typical choice
for the secondary symbol table (which we expect to be small) is to use a linked

§Ƞ.Ș
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȞȜ
list, where a new element is added in constant time, but all the elements on
the list need to be examined for a search (see [33]).
Ļe performance of a hashing algorithm depends on the eﬀectiveness
of the hash function, which converts arbitrary keys into values in the range
1 to M with equal likelihood. A typical way to do this is to use a prime M,
then convert keys to large numbers in some natural way and use that number
modulo M for the hash value. More sophisticated schemes have also been de-
vised. Hashing algorithms are used widely for a broad variety of applications,
and experience has shown that, if some attention is paid to the process, it is
not diﬃcult to ensure that hash functions transform keys into hash values that
appear to be random. Analysis of algorithms is thus particularly eﬀective for
predicting performance because a straightforward randomness assumption is
justiŀed, being built into the algorithm.
Ļe performance of a hashing algorithm also depends on the data struc-
tures that are used for the secondary search. For example, we might consider
keeping the elements in the secondary table in increasing order in an array.
Or, we might consider using a binary search tree for keys that hash to the
same value. Analysis of algorithms can help us choose among these varia-
tions, bearing in mind it is common practice to choose M suﬃciently large
that the secondary tables are so small that further improvements are marginal.
If N is large by comparison to M and the hash function produces ran-
dom values, we expect that each list will have about N/M elements, cutting
the search time by a factor of M. Ļough we cannot make M arbitrarily
large because increasing M implies the use of extra space to maintain the sec-
ondary data structure, we perhaps could make N a constant multiple of M,
which gives constant search time. Variations of hashing that can achieve this
performance goal are in widespread use.
public void insert(int key)
{ return st[hash(key)].insert(); }
public int search(int key)
{ return st[hash(key)].search(key); }
Program 9.1 Hashing with separate chaining

țȞȝ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ș
We are interested in analyzing hashing algorithms not only to deter-
mine how they compare to one another, but also to determine how they com-
pare to other symbol-table implementations and how best to set parameters
(such as hash table size). To focus on the mathematics, we adopt the normal
convention of measuring the performance of hashing algorithms by count-
ing the number of probes used, or key comparisons made with elements in
the data structure, for successful and unsuccessful searches. As usual, more
detailed analysis involving the cost of computing the hash function, the cost
of accessing and comparing keys, and so forth is necessary to make deŀnitive
statements about the relative performance of algorithms. For instance, even
when hashing involves only a constant number of probes, it might be slower
than a search based on tries (for example) if keys are long, because comput-
ing the hash function involves examining the whole key, while a trie-based
method might distinguish among keys after examining relatively few bits.
A sequence of N hash values into a table of size M is nothing more than
a word of length N comprising letters from an alphabet of size M. Ļus, the
analysis of hashing corresponds directly to the analysis of the combinatorial
properties of words. We consider this analysis next, including the analysis of
hashing with separate chaining, then we look at other hashing algorithms.
9.2 Ļe Balls-and-Urns Model and Properties of Words. To set the
stage for the rest of the chapter, we begin by deŀning words, considering the
basic combinatorics surrounding them, and placing them into context both
among the other combinatorial objects that we have been studying and among
classical results in combinatorics.
Deŀnition An M-word of length N is a function f mapping integers from
the interval [1 . . . N] into the interval [1 . . . M].
As with permutations and trees, one way to specify a word is to write
down its functional table:
index
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
word
3
1
4
1
5
9
2
6
5
3
5
8
9
7
9
3
2
3
8
4
We drop the index and specify the function by writing f(1)f(2) . . . f(N),
which makes it plain that an M-word of length N is equivalent to a bytestring
of length N (bytesize M). When studying words, we normally concentrate
on properties of the set of values in the word. How many 1s are there? How

§Ƞ.ș
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȞȞ
many of the possible values do not appear? Which value appears most often?
Ļese are the sorts of questions we address in this chapter.
In discrete probability, these combinatorial objects are often studied in
the context of a balls-and-urns model. We imagine N balls to be randomly
distributed among M urns, and we ask questions about the result of the dis-
tribution. Ļis directly corresponds to words: N balls correspond to the N
letters in a word, and M urns correspond to the M diﬀerent letters in an al-
phabet, so we specify which urn each ball lands in. Ļat is, a word of length
N is a sequence of M urns containing N balls labelled from 1 to N, where the
ŀrst urn gives the indices in the word having the ŀrst letter, the second urn
gives the indices in the word having the second letter, and so forth. Figure 9.1
illustrates the balls-and-urns model for our example.
Combinatorially, the balls-and-urns model is equivalent to viewing an
M-word as an M-sequence of sets of labelled objects. Ļis view immediately
leads to the combinatorial construction
WM = SEQ M(SET (Z))
which immediately transfers via Ļeorem 5.2 to the EGF equation
WM(z) = (ez)M
and therefore WMN = N![zN]ezM = M N as expected. Ļis result is simple,
but, as usual with the symbolic method, we will be making slight modiŀca-
tions to this construction to develop easy solutions to problems that otherwise
would be much more diﬃcult to solve.
4
2
7
1
3
9
5
8
11
12
10
6
0
 
1
 
2
 
3
 
4
 
5
 
6
 
7
 
8
 
9
13
16
18
19
20
17
15
14
10 11 12 13 14 15 16 17 18 19 20
3 1 4 1 5 9 2 6 5 3 5 8 9 7 9 3 2 3 8 4  
1 2 3 4 5 6 7 8 9
index
word
Figure 9.1 A 10-word of length 20
(20 balls thrown into 10 urns)

țȞȟ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ș
As detailed in Table 9.1, this view represents a completely diﬀerent view
of the same set of objects from the view taken in Chapter 8. Are they N-
sequences drawn from M unlabelled objects or M urns ŀlled with N labelled
objects? Ļe answer to this question is that both are valid views, each leading
to useful information with all sorts of applications. To be consistent, we try
in this book to consistently use the term “bytestring” when taking the view
detailed in Chapter 8 and the terms “word,” “balls-and-urns,” or even “hash
table” when taking the viewpoint just outlined. Figure 9.2 illustrates, in our
standard style, all of the 2-words and all of the 3-words for some small values
of N.
To add to the confusion (or, thinking positively, to reinforce the con-
cept), we normally drop the labels when working with balls and urns, with
the implicit understanding that we are analyzing algorithms that map random
words to diﬀerent unlabeled urn occupancy distributions with diﬀerent prob-
abilities. For example, from Figure 9.2 you can immediately see that when
we throw three balls at random into two urns, there are four possibilities:
• 3 balls in the left urn (which occurs with probability 1/8),
• 2 balls in the left urn and 1 ball in the right urn (probability 3/8),
• 1 ball in the left urn and 2 balls in the right urn (probability 3/8),
• 3 balls in the right urn (probability 1/8).
Ļis situation arises often in the analysis of algorithms, and we have encoun-
tered it on several occasions before. For example, BSTs map permutations to
construction
symbolic
transfer
GF
analytic
transfer
coeﬃcient
asymptotics
Unlabelled class
bytestrings
SEQ (Z0 + . . . + ZM−1)
5.1
1
1 −Mz
Taylor
M N
Labelled class
words
SEQ M(SET (Z))
5.2
eMz
Taylor
M N
Table 9.1
Two ways to view the same set of objects

§Ƞ.ș
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȞȠ
W21= 2
W22= 4
W23= 8
W31= 3
W32= 9
0
1
0
1
1
1
1
0
2
2
1
1
1
0
1
2-words on 1, 2, and 3 characters
3-words on 1 and 2 characters
0 0
0 1
1 0
1 1
0
1
2
1
1
2
2
1
2
1
0 2  
1
0
2
0 1
1
2
1
2
0 0
2
1
1 2  
1
0
2
1 1
2
1
1
2
1 0
2
1
2
1
2 2  
1
0
2
2 1
2
1
1
2 0
2
0 0 0
0 0 1
0 1 0
0 1 1
0
1
1 0 0
1 0 1
1 1 0
1 1 1
0
1
3
2
1
2
1
3
3
1
2
3
1
2
3
2
1
2
1
3
3
2
1
3
2
1
Figure 9.2 2-words and 3-words with small numbers of characters

țȟȗ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ș
binary trees and binary tries map bitstrings to tries. In all of these cases, we
have three interesting combinatorial problems to consider: the source model,
the target model, and the target model with probabilities induced by the map-
ping. For permutations and trees, we considered all three (random permuta-
tions, random binary Catalan trees, and BSTs built from random permuta-
tions). In this chapter, the ŀrst is straightforward and we concentrate on the
third, leaving the second for an exercise, as we did for tries in Chapter 8.
Exercise 9.1 How many diﬀerent occupancy distributions are there for N unlabelled
balls in a sequence of M urns? For example, if we denote the desired quantity by
CMN, we have C2N = N + 1 because there is one conŀguration with k balls in the
ŀrst urn and N −k balls in the second urn for each k from 0 to N.
Ļe relative growth rates of M and N are of central importance in the
analysis, and a number of diﬀerent situations arise in practice. For strings of
text or other kinds of “words,” we normally think of M as being ŀxed and
N as being the variable of interest. We make words of varying length from
letters in a ŀxed alphabet, or we throw varying numbers of balls at a ŀxed
number of urns. For other applications, particularly hashing algorithms, we
think of M growing with N, typically M = αN with α between 0 and 1. Ļe
characteristics of these two diﬀerent situations are illustrated in Figure 9.3.
At the top is an example with a relatively large number of balls thrown into
a relatively small number of urns; in this case we are interested in knowing
that the urns have roughly the same number of balls and in quantifying the
discrepancy. At the bottom are ŀve trials with α = 1: many of the urns tend
to be empty, some have just one ball, and very few have several balls.
In both cases, as we will see, our analysis leads to a precise characteri-
zation of the occupancy distributions illustrated in Figure 9.3. As the reader
may have surmised by now, the distribution in question is of course the bi-
nomial distribution, and the results that we consider are classical (nearly two
centuries old); we already covered them in some detail in Chapter 4.
Table 9.2 gives all the 3-words of length 4 (or all the ways to distribute
4 balls into 3 urns), along with the occupancy counts showing the number of
letters that are used 0, 1, 2, 3, and 4 times (or the number of urns that contain
0, 1, 2, 3, and 4 balls). A few other statistics of the type we are considering
are also included.
Generally speaking, when N balls are randomly distributed among M
urns, we are interested in the following sorts of questions:

§Ƞ.ș
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȟȘ
Random distribution of 1024 balls into 26 urns (one trial)
Random distribution of 26 balls into 26 urns (ﬁve trials)
Figure 9.3 Examples of balls-and-urns experiments

țȟș
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ș
word
0 1 2 3 4
word
0 1 2 3 4
word
0 1 2 3 4
1111
2 0 0 0 1
2111
1 1 0 1 0
3111
1 1 0 1 0
1112
1 1 0 1 0
2112
1 0 2 0 0
3112
0 2 1 0 0
1113
1 1 0 1 0
2113
0 2 1 0 0
3113
1 0 2 0 0
1121
1 1 0 1 0
2121
1 0 2 0 0
3121
0 2 1 0 0
1122
1 0 2 0 0
2122
1 1 0 1 0
3122
0 2 1 0 0
1123
0 2 1 0 0
2123
0 2 1 0 0
3123
0 2 1 0 0
1131
1 1 0 1 0
2131
0 2 1 0 0
3131
1 0 2 0 0
1132
0 2 1 0 0
2132
0 2 1 0 0
3132
0 2 1 0 0
1133
1 0 2 0 0
2133
0 2 1 0 0
3133
1 1 0 1 0
1211
1 1 0 1 0
2211
1 0 2 0 0
3211
0 2 1 0 0
1212
1 0 2 0 0
2212
1 1 0 1 0
3212
0 2 1 0 0
1213
0 2 1 0 0
2213
0 2 1 0 0
3213
0 2 1 0 0
1221
1 0 2 0 0
2221
1 1 0 1 0
3221
0 2 1 0 0
1222
1 1 0 1 0
2222
2 0 0 0 1
3222
1 1 0 1 0
1223
0 2 1 0 0
2223
1 1 0 1 0
3223
1 0 2 0 0
1231
0 2 1 0 0
2231
0 2 1 0 0
3231
0 2 1 0 0
1232
0 2 1 0 0
2232
1 1 0 1 0
3232
1 0 2 0 0
1233
0 2 1 0 0
2233
1 0 2 0 0
3233
1 1 0 1 0
1311
1 1 0 1 0
2311
0 2 1 0 0
3311
1 0 2 0 0
1312
0 2 1 0 0
2312
0 2 1 0 0
3312
0 2 1 0 0
1313
1 0 2 0 0
2313
0 2 1 0 0
3313
1 1 0 1 0
1321
0 2 1 0 0
2321
0 2 1 0 0
3321
0 2 1 0 0
1322
0 2 1 0 0
2322
1 1 0 1 0
3322
1 0 2 0 0
1323
0 2 1 0 0
2323
1 0 2 0 0
3323
1 1 0 1 0
1331
1 0 2 0 0
2331
0 2 1 0 0
3331
1 1 0 1 0
1332
0 2 1 0 0
2332
1 0 2 0 0
3332
1 1 0 1 0
1333
1 1 0 1 0
2333
1 1 0 1 0
3333
2 0 0 0 1
total
48 96 72 24 3
Occupancy distributions
3 × 20001 + 24 × 11010 + 18 × 10200 + 36 × 02100
Pr{no urn empty}
36/81 ≈0.444
Pr{urn occupancies all < 3}
(18 + 36)/81 ≈0.667
Pr{urn occupancies all < 4}
(24 + 18 + 36)/81 ≈0.963
Avg. number empty urns
(1 · 42 + 2 · 3)/81 ≈0.593
Avg. maximum occupancy
(2 · 54 + 3 · 24 + 4 · 3)/81 ≈2.370
Avg. occupancy (1 · 96 + 2 · 72 + 3 · 24 + 4 · 3)/3 · 81 ≈1.333
Avg. minimum occupancy
(1 · 36)/81 ≈0.444
Table 9.2
Occupancy distribution and properties of 3-words of length 4
or conŀgurations of 4 balls in 3 urns
or hash sequences of length 4 (table size 3)

§Ƞ.ș
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȟȚ
• What is the probability that no urn will be empty?
• What is the probability that no urn will contain more than one ball?
• How many of the urns are empty?
• How many balls are in the urn containing the most balls?
• How many balls are in the urn containing the fewest balls?
Ļese are immediately relevant to practical implementations of hashing and
other algorithms: we want to know how long we may expect the lists to be
when using hashing with separate chaining, how many empty lists we might
expect, and so on. Some of the questions are enumeration problems akin to
our enumeration of permutations with cycle length restrictions in Chapter 7;
others require analysis of properties of words in more detail. Ļese and related
questions depend on the occupancy distribution of the balls in the urns, which
we study in detail in this chapter.
Table 9.3 gives the values of some of these quantities for three urns with
the number of balls ranging from 1 to 10, calculated using the results given
in the next section. Ļe fourth column corresponds to Table 9.2, which il-
lustrates how these values are calculated. As the number of balls grows, we
have a situation similar to that depicted at the top in Figure 9.3, with balls
distributed about equally into urns, roughly N/M balls per urn. Other phe-
nomena that we expect intuitively are exhibited in this table. For example, as
balls →
1
2
3
4
5
6
7
8
9
10
Probability
urn occ. all < 2
1
.667 .222
0
0
0
0
0
0
0
urn occ. all < 3
1
1
.889 .667 .370 .123
0
0
0
0
urn occ. all < 4
1
1
1
.963 .864 .700 .480 .256 .085
0
urn occ. all > 0
0
0
.222 .444 .617 .741 .826 .883 .922 .948
urn occ. all > 1
0
0
0
0
0
.123 .288 .448 .585 .693
urn occ. all > 2
0
0
0
0
0
0
0
0
.085 .213
Average
# empty urns
2
1.33 .889 .593 .395 .263 .176 .117 .0780 .0520
max. occupancy
1
1.33 1.89 2.37 2.78 3.23 3.68 4.08 4.50 4.93
min. occupancy
0
0
.222 .444 .617 .864 1.11 1.33 1.59 1.85
Table 9.3
Occupancy parameters for balls in three urns

țȟț
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ș
the number of balls increases, the number of empty urns becomes small, and
the probability that no urn is empty becomes large.
Ļe relative values of M and N dictate the extent to which the answers
to the various questions posed earlier are of interest. If there are many more
balls than urns (N ≫M), then it is clear that the number of empty urns will
be very low; indeed, we expect there to be about N/M balls per urn. Ļis
is the case illustrated at the top in Figure 9.3. If there are many fewer balls
than urns (N ≪M), most urns are empty. Some of the most interesting and
important results describe the situation when N and M are within a constant
factor of each other. Even when M = N, urn occupancy is relatively low, as
illustrated at the bottom in Figure 9.3.
Table 9.4 gives the values corresponding to Table 9.3, but for the larger
value M = 8, again with N ranging from 1 to 10. When the number of
balls is small, we have a situation similar to that depicted at the bottom in
Figure 9.3, with many empty urns and few balls per urn generally. Again, we
are able to calculate exact values from the analytic results given §9.3.
Exercise 9.2 Give a table like Table 9.2 for three balls in four urns.
Exercise 9.3 Give tables like Tables 9.2 and 9.4 for two urns.
Exercise 9.4 Give necessary and suﬃcient conditions on N and M for the average
number of empty urns to equal the average minimum urn occupancy.
balls →
1
2
3
4
5
6
7
8
9
10
Probability
occupancies all < 2
1
.875 .656 .410 .205 .077 .019 .002
0
0
occupancies all < 3
1
1
.984 .943 .872 .769 .642 .501 .361 .237
occupancies all < 4
1
1
1
.998 .991 .976 .950 .910 .855 .784
occupancies all > 0
0
0
0
0
0
0
0
.002 .011 .028
occupancies all > 1
0
0
0
0
0
0
0
.000 .000 .000
Average
# empty urns
7
6.13 5.36 4.69 4.10 3.59 3.14 2.75 2.41 2.10
max. occupancy
1
1.13 1.36 1.65 1.93 2.18 2.39 2.60 2.81 3.02
min. occupancy
0
0
0
0
0
0
0
.002 .011 .028
Table 9.4
Occupancy parameters for balls in eight urns

§Ƞ.Ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȟȜ
Aş ţŕŠŔ ŜőŞřšŠōŠŕśŚş, ţő can develop combinatorial constructions among
words for use in deriving functional relationships among CGFs that yield an-
alytic results of interest. Choosing among the many possible correspondences
for a particular application is part of the art of analysis. Our constructions for
M-words of length N build on words with smaller values of N and M.
“First” or “last” construction. Given an M-word of length N −1, we can
construct M diﬀerent M-words of length N simply by, for each k from 1
to M, prepending k. Ļis deŀnes the “ŀrst” construction. For example, the
4-word 3 2 4 gives the following 4-words:
1 3 2 4
2 2 3 4
3 2 3 4
4 2 3 4
One can clearly do the same thing with any other position, not just the ŀrst.
Ļis construction implies that the number of M-words of length N is M
times the number of M-words of length N −1, a restatement of the obvious
fact that the count is M N.
“Largest” construction. Given an M-word of length N, consider the (M −
1)-word formed by simply removing all occurrences of M. If there were k
such occurrences (k could range from 0 to N), this word is of length N −k,
and corresponds to exactly
(N
k
) diﬀerent words of length N, one for every
possible way to add k elements. Conversely, for example, we can build the
following 3-words of length 4 from the 3-word 2 1:
3 3 2 1
3 2 3 1
3 2 1 3
2 3 3 1
2 3 1 3
2 1 3 3
Ļis construction leads to the recurrence
M N =
∑
0≤k≤N
(
N
k
)
(M −1)N−k,
a restatement of the binomial theorem.
9.3 Birthday Paradox and Coupon Collector Problem. We know that
the distribution of balls in urns is the binomial distribution, and we discuss
properties of that distribution in detail later in this chapter. Before doing so,
however, we consider two classical problems about ball-and-urn occupancy
distributions that have to do with the dynamics of the process of the urns

țȟȝ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ț
ŀlling up with balls. As N balls are randomly distributed, one after another,
among M urns, we are interested in knowing how many balls are thrown, on
the average, before
• A ball falls into a nonempty urn for the ŀrst time; and
• No urns are empty for the ŀrst time.
Ļese are called the birthday problem and the coupon collector problem, respec-
tively. Ļey are immediately relevant to the study of hashing and other algo-
rithms. Ļe solution to the birthday problem will tell us how many keys we
should expect to insert before we ŀnd the ŀrst collision; the solution to the
coupon collector problem will tell us how many keys we should expect to in-
sert before ŀnding that there are no empty lists.
Birthday problem. Perhaps the most famous problem in this realm is often
stated as follows: How many people should one gather in a group for it to be
more likely than not that two of them have the same birthday? Taking people
one at a time, the probability that the second has a diﬀerent birthday than the
ŀrst is (1 −1/M); the probability that the third has a diﬀerent birthday than
the ŀrst two is (independently) (1 −2/M), and so on, so (if M = 365) the
probability that N people have diﬀerent birthdays is
(
1 −1
M
)(
1 −2
M
)
. . .
(
1 −N −1
M
)
= N!
M N
(
M
N
)
.
Subtracting this quantity from 1, we get the answer to our question, plotted
in Figure 9.4.
 
24
180
365
.25
.50
1.00
 
N
Figure 9.4 Probability that N people do not all have diﬀerent birthdays

§Ƞ.Ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȟȞ
Ļeorem 9.1 (Birthday problem).
Ļe probability that there are no colli-
sions when N balls are thrown into M urns is given by
(
1 −1
M
)(
1 −2
M
)
. . .
(
1 −N −1
M
)
.
Ļe expected number of balls thrown until the ŀrst collision occurs is
1 + Q(M) =
∑
k
(
M
k
)
k!
M k ∼
√
πM
2
+ 2
3
where Q(M) is the Ramanujan Q-function.
Proof. See the previous discussion for the probability distribution. To ŀnd the
expected value, let X denote the random variable for the number of balls until
the ŀrst collision occurs. Ļen the given probability is precisely Pr{X > N}.
Summing these, we get an expression for the expectation:
∑
N≥0
(
1 −1
M
)(
1 −2
M
)
. . .
(
1 −N −1
M
)
,
which is precisely 1+Q(M) by the deŀnition of Q(M) (see §4.7). Ļe asymp-
totic form follows from Ļeorem 4.8.
Ļe value of 1 + Q(365) is between 24 and 25, so that it is more likely
than not that at least two people in a group of 25 or larger have the same birth-
day. Ļis is often referred to as the “birthday paradox” because one might
expect the number to be much higher. For generations, teachers have sur-
prised skeptical students by ŀnding two having the same birthday, knowing
that the experiment is more likely than not to succeed in a class of 25 or larger,
with the chance of success much improved for larger classes.
It is also of interest to ŀnd the median value: the value of N for which
the probability given above is closest to 1/2. Ļis could be done with a quick
computer calculation, or an asymptotic calculation can also be used. Asymp-

țȟȟ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ț
totically, the cutoﬀpoint is determined by
(
1 −1
M
)(
1 −2
M
)
. . .
(
1 −N −1
M
)
∼1
2
∑
1≤k<N
ln
(
1 −k
M
)
∼ln(1/2)
∑
1≤k<N
k
M ∼ln2
N(N −1)
2M
∼ln2
N ∼
√
2Mln2,
which slightly underestimates the answer N = 23 for M = 365.
Ļe coeﬃcient of
√
M for the mean number of people to be gathered
before a birthday collision occurs is
√π/2 ≈1.2533, and the coeﬃcient of
√
M for the median (the number of people to be gathered to be 50% sure
that a birthday collision will occur) is
√
2ln2 ≈1.1774. Ļese give rise to the
approximate values 24.6112 and 22.4944, respectively, for M = 365, and it is
interesting to note that the mean and median are not asymptotically equivalent
in this case.
Exercise 9.5 For M = 365, how many people are needed to be 99% sure that two
have the same birthday?
Exercise 9.6 Estimate the variance associated with the birthday distribution that is
given in Ļeorem 9.1, and explain the apparent discrepancy concerning the asymp-
totic values of the mean and the median.
Coupon collector problem.
Another famous and classical problem in this
realm is often posed as follows: if each box of a product contains one of a
set of M coupons, how many boxes must one buy, on the average, before
getting all the coupons? Ļis value is equivalent to the expected number of
balls thrown until all urns have at least one ball or the expected number of
keys added until all the chains in a hash table built by Program 9.1 have at
least one key.
To solve this problem, we deŀne a k-collection to be a word that consists
of k diﬀerent letters, with the last letter in the word being the only time that

§Ƞ.Ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȟȠ
letter occurs, and PMk to be the combinatorial class of words that are k-
collections of M possible coupons. Ļe number of k-collections of length N
divided by MN is the probability that N coupons need to be collected to get
k diﬀerent ones. In the balls-and-urns model, this is the probability that the
last ball falls into an empty urn, making the number of nonempty urns equal
to k.
For this problem, it is convenient to view words as bytestrings and use
unlabelled objects, OGFs, and PGFs. In any k-collection w, the ŀrst coupon
is either in the collection of k −1 coupons (not including the last one) in
the rest of w, in which case the rest of w is a k-collection, or it is one of the
M −(k −1) coupons that is not in the rest of w, in which case the rest of w
is a (k −1)-collection. Ļerefore, we have the combinatorial construction
PMk = ((k −1)Z) × PMk + ((M −(k −1))Z) × PM(k−1),
which immediately translates, via Ļeorem 5.1, to the OGF equation
PMk(z) = (k −1)zPMk(z) + (M −(k −1))zPM(k−1)(z)
= (M −(k −1))z
1 −(k −1)z
PM(k−1)(z),
with P0(z) = 1. Note that PMk(z/M) is the PGF for the length of k-
collections, so we have PMk(1/M) = 1 and the average length of a k-
collection is P ′
Mk(z/M)|z=1. Diﬀerentiating both sides of the equation
PMk(z/M) = (M −(k −1))z
M −(k −1)z PM(k−1)(z/M),
evaluating at z = 1, and simplifying gives the recurrence
d
dz PMk(z/M)|z=1 = 1 +
(k −1)
M −(k −1) + d
dz PM(k−1)(z/M)|z=1,
which telescopes to the solution
d
dz PMk(z/M)|z=1 =
∑
0≤j<k
M
M −j = M(HM −HM−k).

țȠȗ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ț
Ļeorem 9.2 (Coupon collector problem).
Ļe number of balls thrown
until all M urns are ŀlled is on average
MHM = MlnM + Mγ + O(1)
with variance
M 2H(2)
M −MHM ∼M2π2/6.
Ļe probability that the Nth ball ŀlls the last empty urn is
M!
M N
{
N −1
M −1
}
.
Proof. Ļe mean is derived in the earlier discussion. Alternatively, telescoping
the recurrence on the OGF for k-collections gives the explicit form
PMk(z) =
M(M −1) . . . (M −k + 1)
(1 −z)(1 −2z) . . . (1 −(k −1)z)zk,
which leads immediately to the PGF
PMM(z/M) =
M!zM
M(M −z)(M −2z) . . . (M −(M −1)z).
It is easily veriŀed that PMM(1/M) = 1 and that diﬀerentiating with re-
spect to z and evaluating at z = 1 gives MHM, as above. Diﬀerentiating
a second time and applying Ļeorem 3.10 leads to the stated expression for
the variance. Ļe distribution follows immediately by extracting coeﬃcients
from the PGF using the identity
∑
N≥M
{
N
M
}
zN =
zM
(1 −z)(1 −2z) . . . (1 −Mz),
from Table 3.7 in §3.11.
Exercise 9.7 Find all the 2-collections and 3-collections in Table 9.2, then compute
P2(z) and P3(z) and check the coeﬃcients of z4.

§Ƞ.Ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȠȘ
Stirling numbers of the second kind. As pointed out already in §3.11, the
Stirling “subset” numbers
{N
M
} also represent the number of ways to partition
an N-element set into M nonempty subsets. We will see a derivation of this
fact later in this section. Starting with this deŀnition leads to an alternate
derivation of the coupon collector distribution, as follows. Ļe quantity
M
{
N −1
M −1
}
(M −1)!
gives the number of ways the last of N balls ŀlls the last of M urns, because
the Stirling number is the number of ways for the N −1 balls to fall into
M −1 diﬀerent urns, the factor of (M −1)! accounts for all possible orders
of those urns, and the factor of M accounts for the fact that any of the urns
could be the last urn to be ŀlled. Dividing by M N gives the same result as
given in Ļeorem 9.2.
Ļe classical derivation of the mean for the coupon collector problem
(see, for example, Feller [9]) is elementary and proceeds as follows. Once k
coupons have been collected, the probability that j or more additional boxes
are needed to get the next coupon is (k/M)j, so the average number of such
boxes is
∑
j≥0
( k
M
)j =
1
1 −k/M =
M
M −k,
and summing on k gives the result MHM, as above. Ļis requires less compu-
tation than the derivation above, but the generating functions capture the full
structure of the problem, making it possible to calculate the variance without
explicitly worrying about dependencies among the random variables involved,
and also giving the complete probability distribution.
Exercise 9.8 Expand the PGF by partial fractions to show that the probability that
the Nth ball ŀlls the last empty urn can also be expressed as the alternating sum
∑
0≤j<M
(M
j
)
(−1)j(
1 −j
M
)N−1
.
Exercise 9.9 Give an expression for the probability that collecting at least N boxes
gives a full collection of M coupons.

țȠș
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ț
Enumeration of M-surjections.
Balls-and-urns sequences with M urns,
none of which are empty (M-words with at least one occurrence of each let-
ter), are called M-surjections. Ļese combinatorial objects arise in many con-
texts, since they correspond to dividing N items into exactly M distinguished
nonempty groups. Determining the EGF for M-surjections is easy via the
symbolic method, as it is a slight modiŀcation of our derivation for words at
the beginning of §9.2. An M-surjection is an M-sequence of nonempty urns,
so we have the combinatorial construction
FM = SEQ M(SET >0(Z))
which immediately transfers via Ļeorem 5.2 to the EGF equation
FM(z) = (ez −1)M.
Ļis is an exponential generating function for the Stirling numbers of the sec-
ond kind (see Table 3.7), so we have proved that the number of M-surjections
of length N is
N![zN](ez −1)M = M!
{
N
M
}
.
Again, starting with the combinatorial deŀnition that Stirling numbers of the
second kind enumerate partitions of N elements into M subsets gives a direct
proof of this same result, since each of the M! orderings of the sets yields a
surjection.
Enumeration of surjections. We digress slightly to consider words that are
made up of at least one occurrence of M characters for some M, which are
known as surjections, because they are well studied in combinatorics. Ļe
analysis follows the same argument as just considered for M-surjections and
is a good example of the power of analytic combinatorics. A surjection is a
sequence of nonempty urns, so we have the combinatorial construction
F = SEQ (SET >0(Z)),
which immediately transfers via Ļeorem 5.2 to the EGF equation
F(z) =
1
1 −(ez −1) =
1
2 −ez .

§Ƞ.Ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȠȚ
Elementary complex asymptotics (see [14]) provides the coeﬃcient asymp-
totics, with the result that the number of surjections of length N is
N![zN]F(z) ∼N!
2ln2
( 1
ln2
)N.
Exercise 9.10 Consider the “largest” construction among M-surjections: given an
M-surjection of length N, consider the (M −1)-surjection formed by removing all
occurrences of M. Find the EGF for surjections using this construction.
Exercise 9.11 Write a program that takes N and M as parameters and prints out all
M-surjections of length N whenever the number of such objects is less than 1000.
Exercise 9.12 Expand (ez −1)M by the binomial theorem to show that
N![zN]FM(z) =
∑
j
(M
j
)
(−1)M−jjN = M!
{N
M
}
.
(See Exercise 9.8.)
Exercise 9.13 Show that the number of partitions of N elements into nonempty
subsets is
N![zN]eez−1.
(Ļis deŀnes the so-called Bell numbers.)
Exercise 9.14 Show that
N![zN]eez−1 = 1
e
∑
k≥0
kN
k! .
Exercise 9.15 Prove that the bivariate EGF for the Stirling numbers of the second
kind is exp(u(ez −1)).
Exercise 9.16 Applying the “largest” construction to ŀnd the number of M-words
of length N might lead to the recurrence
FNM =
∑
0≤k≤M
(N
k
)
F(N−k)(M−1).
Show how to solve this recurrence using BGFs.

țȠț
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ț
Caching algorithms. Coupon collector results are classical, and they are of
direct interest in the analysis of a variety of useful algorithms. For example,
consider a “demand paging” system where a k-page cache is used to increase
the performance of an M-page memory by keeping the k most recently ref-
erenced pages in the cache. If the “page references” are random, then the
coupon collector analysis gives the number of references until the cache ŀlls
up. From the computation of Pk(z) given in the proof of Ļeorem 9.2, it is
immediately obvious that the average number of page references until a cache
of size k ŀlls up in an M-page memory system, assuming page references
are independent and uniformly distributed, is M(HM −HM−k). Speciŀ-
cally, note that, although it takes ∼MlnM references before all the pages
are hit, a cache of size αM ŀlls up after about Mln(1/(1 −α)) references.
For instance, a cache of size M/2 will ŀll after about Mln2 ≈.69M page
references, resulting in a 19% savings. In practice, references are not random,
but correlated and nonuniform. For example, recently referenced pages are
likely to be referenced again, resulting in much higher savings from caching.
Ļe analysis thus should provide lower bounds on cache eﬃciency in practi-
cal situations. In addition, the analysis provides a starting point for realistic
analyses under nonuniform probabilistic models [10].
TŔő ŎŕŞŠŔŐōť ŜŞśŎŘőř ōŚŐ the coupon collector problem appear at oppo-
site ends of the process of ŀlling cells with balls. In the birthday problem, we
add balls and look at the ŀrst time some cell gets more than one ball, which
takes about about
√πM/2 steps, on the average. Continuing to add the balls,
we eventually ŀll each cell with at least one ball, after about MlnM steps on
the average, by the coupon collector result. In between, when M = N, we
will see in the next section that about 1 −1/e ≈36% of the cells are empty,
and, furthermore, one of the cells should have about lnN/lnlnN balls.
Ļese results have various practical implications for hashing algorithms.
First, the birthday problem implies that collisions tend to occur early, so a
collision resolution strategy must be designed. Second, the ŀlling tends to be
rather uneven, with a fair number of empty lists and a few long lists (almost
logarithmic in length). Ļird, empty lists do not completely disappear until
after a rather large number of insertions.
Many more details on the birthday problem, coupon collector problems,
and applications are given in Feller’s classic text [9] and in the 1992 paper by
Flajolet, Gardy, and Ļimonier [10].

§Ƞ.ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȠȜ
9.4 Occupancy Restrictions and Extremal Parameters. Solving the
birthday problem involves enumerating arrangements (the number of words
with no letter occurring twice) and solving the coupon collector problem in-
volves enumerating surjections (the number of words with at least one occur-
rence of each letter). In this section, we describe generalizations of these two
enumeration problems on words.
In Chapter 7 we discussed enumeration of permutations with restric-
tions on cycle length; in Chapter 8 we discussed enumeration of bitstrings
with restrictions on patterns of consecutive bits. Here we use similar tech-
niques to discuss enumeration of words with restrictions on frequency of
occurrence of letters, or, equivalently, ball-and-urn occupancy distributions
with restrictions, or hash sequences with collision frequency restrictions, or
functions with range frequency restrictions.
Table 9.5 shows the situation for 3-words. Ļe top four rows in Ta-
ble 9.5 give the numbers of 3-words with no more than 1, 2, 3, and 4 occur-
rences of any letter, and the bottom four rows give the number of 3-words
with at least 1, 2, 3, and 4 occurrences of every letter. (Ļe ŀfth row cor-
responds to 3-surjections and is OEIS A001117 [35].) Ļese are frequency
counts, so dividing each entry by M N yields Table 9.3. Ļe fourth column
corresponds to Table 9.2. Ļe many relationships among these numbers are
most easily uncovered via the symbolic method.
frequency
balls →1 2
3
4
5
6
7
8
9
10
11
12
< 2
3 6
6
< 3
3 9 24 54
90
90
< 4
3 9 27 78 210 510 1050 1680
1680
< 5
3 9 27 81 240 690 1890 4830 11130 22050
34650
34650
> 0
6 36 150 540 1806 5796 18150 55980 171006 519156
> 1
90
630 2940 11508 40950 125100 445896
> 2
1680 12600
62370 256410
> 3
34650
Table 9.5
Enumeration of 3-words with letter frequency restrictions
or occupancy distributions of balls in 3 urns with restrictions
or hash sequences (table size 3) with collision restrictions

țȠȝ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ț
Maximal occupancy. Ļe EGF for a word comprising at most k occurrences
of a given letter is 1 + z + z2/2! + . . . + zk/k!. Ļerefore,
(1 + z + z2/2! + . . . + zk/k!)M
is the EGF for words comprising at most k occurrences of each of M diﬀer-
ent letters. Ļis is a straightforward application of the symbolic method for
labelled objects (see Ļeorem 3.8). As we have seen, removing the restriction
gives the EGF
(1 + z + z2/2! + . . . + zk/k! + . . .)M = ezM
and the total number of M-words of length N is N![zN]ezM = MN as
expected. Taking k = 1 gives the EGF
(1 + z)M,
which says that the number of words with at most one occurrence of each
letter (no duplicates) is
N![zN](1 + z)M = M(M −1)(M −2) . . . (M −N + 1) = N!
(
M
N
)
.
Ļis quantity is also known as the number of arrangements, or ordered combi-
nations, of N elements chosen among M possibilities. Dividing the number
of arrangements by MN gives the probability distribution in Ļeorem 9.1 for
the birthday problem, and the use of the symbolic method provides a straight-
forward generalization.
Ļeorem 9.3 (Maximal occupancy).
Ļe number of words of length N
with at most k occurrences of each letter is
N![zN]
(
1 + z
1! + z2
2! + . . . + zk
k!
)M
.
In particular, the number of arrangements (k = 1) is N!
(M
N
).
Proof. See the above discussion.

§Ƞ.ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȠȞ
Ļe numbers in the top half of Table 9.5 correspond to computing co-
eﬃcients of these EGFs for 1 ≤k ≤4. For example, the second line corre-
sponds to the expansion
(
1 + z + z2
2!
)3 = 1 + 3z + 9
2z2 + 4z3 + 9
4z4 + 3
4z5 + 1
8z6
= 1 + 3z + 9z2
2! + 24z3
3! + 54z4
4! + 90z5
5! + 90z6
6! .
Exercise 9.17 Find the EGF for M-words with all letter frequencies even.
Exercise 9.18 Prove that the number of ways to distribute M(k + 1) balls among
M urns with all urns having > k balls is equal to the number of ways to distribute
M(k +1)−1 balls among M urns with all urns having < (k +2) balls, for all k ≥0.
(See Table 9.5.) Give an explicit formula for this quantity as a quotient of factorials.
Exercise 9.19 What is the expected number of balls thrown in N urns before the
second collision occurs? Assume that “collision” here means the event “ball falling
into nonempty urn.”
Exercise 9.20 What is the expected number of balls thrown in N urns before the
second collision occurs, when we assume that “collision” means the event “ball falling
into urn with exactly one ball in it?”
Exercise 9.21 Give an explicit expression for the number of M-words with no three
occurrences of the same letter.
Exercise 9.22 Give a plot like Figure 9.3 for the probability that three people have
the same birthday.
Exercise 9.23 For M = 365, how many people are needed to be 50% sure that three
have the same birthday? Four?
Minimal occupancy. By arguments similar to those used earlier for surjec-
tions and for maximal occupancy, the EGF for words comprising more than
k occurrences of each letter is
(ez −1 −z −z2/2! −. . . −zk/k!)M.
In particular, taking k = 1 gives the EGF for M-surjections, or the number
of words with at least one occurrence of each of M characters, or the number
of ball-and-urn sequences with M urns, none of which are empty:
(ez −1)M.
Ļus, we are considering a generalization of the coupon collector problem of
the previous section.

țȠȟ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ț
Ļeorem9.4 (Minimaloccupancy).
Ļe number of words of length N with
at least k occurrences of each letter is
N![zN]
(
ez −1 −z
1! −z2
2! −. . . −
zk−1
(k −1)!
)M
.
In particular, the number of M-surjections of length N is
N![zN](ez −1)M = M!
{
N
M
}
.
Proof. See the earlier discussion.
Ļe numbers in the bottom half of Table 9.5 correspond to comput-
ing coeﬃcients of these EGFs. For example, the third line from the bottom
corresponds to the expansion
(ez −1 −z)3 =
(z2
2 + z3
6 + z4
24 + z5
120 + . . .
)3
= 1
8z6 + 1
8z7 + 7
96z8 + 137
4320z9 +
13
1152z10 + . . .
= 90z6
6! + 630z7
7! + 2940z8
8! + 11508z9
9! + 40950z10
10! + . . . .
As for maximal occupancy, generating functions succinctly describe the com-
putation of these values.
Table 9.6 gives a summary of the generating functions in Ļeorems 9.3
and 9.4, for enumerating words with letter frequency restrictions. Ļese theo-
rems are the counterparts to Ļeorems 7.2 and 7.3 for permutations with cycle
length restrictions. Ļese four theorems, containing the analysis of arrange-
ments, surjections, involutions, derangements, and their generalizations, are
worthy of review, as they account for a number of basic combinatorial struc-
tures and classical enumeration problems in a uniform manner.
Characterizing the asymptotic values of the functions in Ļeorems 9.3
and 9.4 involves addressing multivariate asymptotic problems, each with dif-
ferent asymptotic regimes according to the ranges considered. Ļis may be
viewed as a generalization of our treatment of the binomial distribution (see
Chapter 4 and the following discussion), where diﬀerent approximations are

§Ƞ.ț
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
țȠȠ
used for diﬀerent ranges of the parameter values. For instance, for ŀxed M,
the coeﬃcients [zN](1+z+z2/2)M are eventually 0, as N →∞. At the same
time, for ŀxed M, the coeﬃcients sum to (5/2)M, with a peak near 5M/4,
and a normal approximation near the peak can be developed for the coeﬃ-
cients. Ļus, there are interesting regions when M and N are proportional.
Similarly, consider [zN](ez −1 −z)M, and ŀxed M, as N →∞. Here,
we are counting functions that assume each value at least twice. But all but
a very small fraction of the functions will assume all values at least twice (ac-
tually about N/M times). Ļus, this coeﬃcient is asymptotic to [zN](ez)M.
Again, there will be an interesting transition when M grows and becomes
O(N). Such asymptotic results are best quantiŀed by saddle point methods,
as discussed in detail by Kolchin [28] (see also [14]).
Exercise 9.24 What is the average number of balls thrown into M urns before each
urn is ŀlled at least twice?
Exercise 9.25 Derive an expression for the exponential CGF for the expected mini-
mal occupancy when N balls are distributed into M urns. Tabulate the values for M
and N less than 20.
Expected maximum occupancy. What is the average of the maximum num-
ber of balls in an urn, when N balls are distributed randomly among M urns?
Ļis is an extremal parameter similar to several others that we have encoun-
tered. As we have done for tree height and maximum cycle length in permu-
tations, we can use generating functions to compute the maximum occupancy.
one urn, occupancy k
zk/k!
all words
ezM
all occupancies > 1 (surjections)
(ez −1)M
all occupancies > k
(ez −1 −z −z2/2! . . . −zk/k!)M
no occupancies > 1 (arrangements)
(1 + z)M
no occupancies > k
(1 + z + z2/2! + . . . + zk/k!)M
Table 9.6
EGFs for words with letter frequency restrictions
or ball-and-urn occupancy distributions with restrictions
or hash sequences with collision frequency restrictions

Ȝȗȗ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ț
From Ļeorem 9.4, we can write down the generating functions for the
ball-and-urn conŀgurations with at least one urn with occupancy > k, or,
equivalently, those for which the maximum occupancy is > k:
e3z −(1)3 = 3z +
9z2
2! + 27z3
3! + 81z4
4! + 243z5
5! + . . .
e3z −(1 + z)3 =
3z2
2! + 21z3
3! + 81z4
4! + 243z5
5! + . . .
e3z −
(
1 + z + z2
2!
)3 =
6z3
3! + 27z4
4! + 153z5
5! + . . .
e3z −
(
1 + z + z2
2! + z3
3!
)3 =
3z4
4! + 33z5
5! + . . .
e3z −
(
1 + z + z2
2! + z3
3! + z4
4!
)3 =
3z5
5! + . . .
...
and so on, and we can sum these to get the exponential CGF
= 3z + 12z2
2! + 54z3
3! + 192z4
4! + 675z5
5! + . . .
for the (cumulative) maximum occupancy when balls are distributed in three
urns. Dividing by 3N yields the average values given in Table 9.3. In general,
the average maximum occupancy is given by
N!
M N [zN]
∑
k≥0
(
eMz −
( ∑
0≤j≤k
zj
j!
)M)
.
Ļis quantity was shown by Gonnet [16] to be ∼lnN/lnlnN as N, M →∞
in such a way that N/M = α with α constant (the leading term is indepen-
dent of α). Ļus, for example, the length of the longest list when Program 9.1
is used will be ∼lnN/lnlnN, on the average.
Exercise 9.26 What is the average number of blocks of contiguous equal elements in
a random word?
Exercise 9.27 Analyze “rises” and “runs” in words (cf. §7.1).

§Ƞ.Ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȗȘ
9.5 Occupancy Distributions. Ļe probability that an M-word of length
N contains exactly k instances of a given character is
(
N
k
)( 1
M
)k(
1 −1
M
)N−k.
Ļis is established by a straightforward calculation: the binomial coeﬃcient
counts the ways to pick the positions, the second factor is the probability that
those letters have the value, and the third factor is the probability that the
other letters do not have the value. We studied this distribution, the familiar
binomial distribution, in detail in Chapter 4. and have already encountered it
on several diﬀerent occasions throughout this book. For example, Table 4.6
gives values for M = 2. Another example is given in Table 9.7, which gives
corresponding values for M = 3; the fourth line in this table corresponds to
Table 9.2.
Involvement of two distinct variables (number of balls and number of
urns) and interest in diﬀerent segments of the distribution mean that care
needs to be taken to characterize it accurately for particular applications. Ļe
intuition of the balls-and-urns model is often quite useful for this purpose.
In this section, we will be examining precise formulae and asymptotic
estimates of the distributions for many values of the parameters. A few sample
values are given in Table 9.8. For example, when 100 balls are distributed in
100 urns, we expect that about 18 of the urns will have 2 balls, but the chance
N
↓k →
0
1
2
3
4
5
6
1
0.666667 0.333333
2
0.444444 0.444444 0.111111
3
0.296296 0.444444 0.222222 0.037037
4
0.197531 0.395062 0.296296 0.098765 0.012346
5
0.131687 0.329218 0.329218 0.164609 0.041152 0.004115
6
0.087791 0.263375 0.329218 0.219479 0.082305 0.016461 0.001372
Table 9.7
Occupancy distribution for M = 3:
(N
k
)(1/3)k(2/3)N−k
Pr{an urn has k balls after N balls are distributed in 3 urns}

Ȝȗș
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȝ
that any urn has as many as 10 balls is negligible. On the other hand, when
100 balls are distributed in 10 urns, 1 or 2 of the urns are likely to have 10
balls (the others are likely to have between 7 and 13), but very few are likely
to have 2 balls. As we saw in Chapter 4, these results can be described with
the normal and Poisson approximations, which are accurate and useful for
characterizing this distribution for a broad range of values of interest.
Ļe total number of M-words of length N having a character appear k
times is given by
M
(
N
k
)
(M −1)N−k,
by the following argument: Ļere are M characters; the binomial coeﬃcient
counts the indices where a given value may occur, and the third factor is the
urns
balls
occupancy
average # urns with k balls
M
N
k
M
(
N
k
)( 1
M
)k(
1 −1
M
)N−k
2
2
2
0.500000000
2
10
2
0.087890625
2
10
10
0.001953125
10
2
2
0.100000000
10
10
2
1.937102445
10
10
10
0.000000001
10
100
2
0.016231966
10
100
10
1.318653468
100
2
2
0.010000000
100
10
2
0.415235112
100
10
10
0.000000000
100
100
2
18.486481882
100
100
10
0.000007006
Table 9.8
Occupancy distribution examples

§Ƞ.Ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȗȚ
number of ways the other indices can be ŀlled with the other characters. Ļis
leads to the BGF, which we can use to compute moments, as usual. Dividing
by M N leads us to the classical formulation of the distribution, which we
restate here in the language of balls and urns, along with asymptotic results
summarized from Chapter 4.
Ļeorem 9.5 (Occupancy distribution).
Ļe average number of urns with
k balls, when N balls are randomly distributed in M urns, is
M
(
N
k
)( 1
M
)k(
1 −1
M
)N−k.
For k = N/M + x
√
N/M with x = O(1), this is
M e−x2
√
2π + O
( 1
√
N
)
(normal approximation),
and for N/M = α > 0 ŀxed and k = O(1), this is
M αke−α
k!
+ o(M)
(Poisson approximation).
Proof. See earlier discussion. Ļe stated approximations to the binomial dis-
tribution are from Chapter 4 (Exercise 4.66 and Ļeorem 4.7).
Corollary When N/M = α (constant), the average number of empty urns
is asymptotic to Me−α.
Corollary Ļe average number of balls per urn is N/M, with standard devi-
ation
√
N/M −N/M2.
Proof. Multiplying the cumulative cost given above by uk and zN, we get the
BGF
C[M](z, u) =
∑
N≥0
∑
k≥0
C[M]
Nk zNuk =
∑
N≥0
∑
k≥0
(
N
k
)
(M −1)N−kukzN
=
∑
N≥0
(M −1 + u)NzN
=
1
1 −(M −1 + u)z .

Ȝȗț
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȝ
Dividing by MN or, equivalently, replacing z by z/M converts this cumu-
lative BGF into a PGF that is slightly more convenient to manipulate. Ļe
cumulated cost is given by diﬀerentiating this with respect to u and evaluating
at u = 1, as in Table 3.6.
[zN]∂C[M](z/M, u)
∂u

u=1= [zN] 1
M
z
(1 −z)2 = N
M
and
[zN]∂2C[M](z/M, u)
∂u2

u=1= [zN] 1
M 2
z2
(1 −z)3 = N(N −1)
M 2
so the average is N/M and the variance is N(N −1)/M2+N/M −(N/M)2,
which simpliŀes to the stated result.
Alternative derivations. We have presented these calculations along familiar
classical lines, but the symbolic method of course provides a quick derivation.
For a particular urn, the BGF for a ball that misses the urn is (M −1)z and
the BGF for a ball that hits the urn is uz; therefore the ordinary BGF for a
sequence of balls is
∑
N≥0
((M −1 + u)z)N =
1
1 −(M −1 + u)z ,
as before.
Alternatively, the exponential BGF
F(z, u) =
(
ez + (u −1)zk
k!
)M
gives the cumulated number of urns with k balls:
N![zN]∂F(z, u)
∂u

u=1= N![zN]Me(M−1)z zk
k! = M
(
N
k
)
(M −1)N−k
as before.
Ļe occupancy and binomial distributions have a broad variety of ap-
plications and have been very widely studied, so many other ways to derive

§Ƞ.Ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȗȜ
these results are available. Indeed, it is important to note that the average
number of balls per urn, which would seem to be the most important quan-
tity to analyze, is completely independent of the distribution. No matter how
the balls are distributed in the urns, the cumulative cost is N: counting the
balls in each urn, then adding the counts, is equivalent to just counting the
balls. Ļe average number of balls per urn is N/M, whether or not they were
distributed randomly. Ļe variance is what tells us whether the number of
balls in a given urn can be expected to be near N/M.
Figures 9.4 and 9.5 show the occupancy distribution for various values
of M. Ļe bottom series of curves in Figure 9.4 corresponds precisely to
Figure 4.4, the binomial distribution centered at 1/5. For large M, illustrated
in Figure 9.5, the Poisson approximation is appropriate. Ļe limiting curves
for the bottom two families in Figure 9.5 are the same as the limiting curves
for the top two families in Figure 4.5, the Poisson distribution for N = 60
with λ = 1 and λ = 2. (Ļe other limiting curves in Figure 4.5, for N = 60,
are the occupancy distributions for M = 20 and M = 15.) As M gets smaller
with respect to N, we move into the domain illustrated by Figure 9.4, where
the normal approximation is appropriate.
Exercise 9.28 What is the probability that one urn will get all the balls when 100
balls are randomly distributed among 100 urns?
Exercise 9.29 What is the probability that each urn will get one ball when 100 balls
are randomly distributed among 100 urns?
Exercise 9.30 What is the standard deviation for the average number of empty urns?
Exercise 9.31 What is the probability that each urn will contain an even number of
balls when N balls are distributed among M urns?
Exercise 9.32 Prove that
C[M]
Nk = (M −1)C[M]
(N−1)k + C[M]
(N−1)(k−1)
for N > 1 and use this fact to write a program that will print out the occupancy
distribution for any given M.
Analysis of hashing with separate chaining. Properties of occupancy dis-
tributions are the basis for the analysis of hashing algorithms. For example,
an unsuccessful search in a hash table using separate chaining involves access-
ing a random list, then following it to the end. Ļe cost of such a search thus
satisŀes an occupancy distribution.

Ȝȗȝ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȝ
.64
0
.562
0
.444
0
.5
0
Figure 9.5
N/5
N/4
N/3
N/2
M = 5
M = 4
M = 3
M = 2
(N
k
)( 1
M
)k(
1 −1
M
)N−k
Occupancy distributions for small M and 2 ≤N ≤60
(k-axes scaled to N)

§Ƞ.Ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȗȞ
1
.135
0
1
.368
0
1
.513
0
Figure 9.6
M = 30
M = 60
M = 90
(N
k
)( 1
M
)k(
1 −1
M
)N−k
Occupancy distributions for large M and 2 ≤N ≤60
(k-axes scaled to N)

Ȝȗȟ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȝ
Ļeorem 9.6 (Hashing with separate chaining).
Using a table of size M
for N keys, hashing with separate chaining requires N/M probes for an un-
successful search and (N + 1)/(2M) probes for asuccessful search, on the
average.
Proof. Ļe result for an unsuccessful search follows directly from the earlier
discussion. Ļe cost of accessing a key that is in the table is the same as the
cost of putting it into the table, so the average cost of a successful search is
the average cost of all the unsuccessful searches used to build the table—in
this case
1
N
∑
1≤k≤N
k
M = N + 1
2M .
Ļis relationship between unsuccessful and successful search costs holds for
many searching algorithms, including binary search trees.
Ļe Chebyshev inequality says that with 1000 keys, we could use 100
lists and expect about 10 items per list, with at least 90% conŀdence that a
search will examine no more than 20 items. With 1 million keys, one might
use a 1000 lists, and the Chebyshev inequality says that there is at least 99.9%
conŀdence that a search will examine no more than 2000 items. Ļough
generally applicable, the Chebyshev bounds are actually quite crude in this
case, and we can show through direct numerical computation or through use
of the Poisson approximation that for 1 million keys and a table size of 1000,
the probability that more than 1300 probes are needed is on the order of
10−20, and the probability that more than 2000 probes are needed is around
10−170.
 
1
2
3
4
25
50
100
 
Figure 9.7 Percentage of empty urns as a function of load factor N/M

§Ƞ.ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȗȠ
Furthermore, we can know many other properties of the hash structure
that may be of interest. For example, Figure 9.6 is a plot of the function e−α,
which tells us the percentage of empty lists as a function of the ratio of the
number of keys to the number of lists. Such information can be instrumental
in tuning an algorithm to best performance.
A number of variants to the basic separate chaining scheme have been
devised to economize on space in light of these two observations. Ļe most
notable of these is coalesced hashing, which has been analyzed in detail by Vitter
and Chen [37] (see also Knuth [25]). Ļis is an excellent example of the use
of analysis to set values of performance parameters in a practical situation.
Exercise 9.33 For 1000 keys, which value of M will make hashing with separate
chaining access fewer keys than a binary tree search? For 1 million keys?
Exercise 9.34 Find the standard deviation of the number of comparisons required
for a successful search in hashing with separate chaining.
Exercise 9.35 Determine the average and standard deviation of the number of com-
parisons used for a search when the lists in the table are kept in sorted order (so that
a search can be cut short when a key larger than the search key is found).
Exercise 9.36 [Broder and Karlin] Analyze the following variant of Program 9.1:
compute two hash functions and put the key on the shorter of the two lists.
9.6 Open Addressing Hashing. If we take M to be a constant multiple
of N in hashing with separate chaining, then our search time is constant, but
we use a signiŀcant amount of space, in the form of pointers, to maintain
the data structure. So-called open addressing methods do not use pointers and
directly address a set of N keys within a table of size M with M ≥N.
Ļe birthday paradox tells us that the table does not need to be very large
for some keys to have the same hash values, so collision resolution strategy is
immediately needed to decide how to deal with such conłicts.
Linearprobing. Perhaps the simplest such strategy is linear probing: keep all
the keys in an array of size M and use the hash value of the key as an index into
the array. If, when inserting a key into the table, the addressed position (given
by the hash value) is occupied, then simply examine the previous position. If
that is also occupied, examine the one before that, continuing until an empty
position is found. (If the beginning of the table is reached, simply cycle back

ȜȘȗ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ȝ
Figure 9.8 Hashing with linear probing
to the end.) In the balls-and-urns model, we might imagine linear probing
to be a sort of pachinko machine, where one ball ŀlls up an urn and new balls
bounce to the left until an empty urn is found. An implementation of search
and insertion for linear probing is given in Program 9.2. Ļe program keeps
the keys in an array a[] and assumes that the hash function does not return
zero, so that zero can be used to mark empty positions in the hash table.
We will see later that linear probing performs badly for a nearly full
table but reasonably well for a table with enough empty space. As the table
ŀlls up, the keys tend to “cluster” together, producing long chains that must be
searched to ŀnd an empty space. Figure 9.7 shows an example of a table ŀlling
up with linear probing, with a cluster developing in the last two insertions.

§Ƞ.ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȘȘ
An easy way to avoid clustering is to look not at the previous position but
at the tth previous position each time a full table entry is found, where t is
computed by a second hash function. Ļis method is called double hashing.
Uniform hashing. Linear probing and double hashing are diﬃcult to an-
alyze because of interdependencies among the lists. A simple approximate
model is to assume that each occupancy conŀguration of N keys in a table
of size M is equally likely to occur. Ļis is equivalent to the assumption
that a hash function produces a random permutation and the positions in the
hash table are examined in random order (diﬀerent for diﬀerent keys) until
an empty position is found.
Ļeorem 9.7 (Uniform hashing).
Using a table of size M for N keys, the
number of probes used for successful and unsuccessful searches with uniform
hashing is
M + 1
M −N + 1
and
M + 1
N
(HM+1 −HM−N+1),
public void insert(int key)
{
for (i = hash(key); a[i] != 0; i = (i - 1) % M)
if (a[i] == key) return;
a[i] = key;
}
public boolean search(int key)
{
int i;
for (i = hash(key); a[i] != 0; i = (i - 1) % M)
if (a[i] == key)
return true;
return false;
}
Program 9.2 Hashing with linear probing

ȜȘș
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ȝ
(respectively), on the average.
Proof. An unsuccessful search will require k probes if k −1 table locations
starting at the hashed location are full and the kth empty. With k locations
and k −1 keys accounted for, the number of conŀgurations for which this
holds is the number of ways to distribute the other N −k +1 keys among the
other M −k locations. Ļe total unsuccessful search cost in all the occupancy
conŀgurations is therefore
∑
1≤k≤M
k
(
M −k
N −k + 1
)
=
∑
1≤k≤M
k
(
M −k
M −N −1
)
=
(
M + 1
M −N + 1
)
(see Exercise 3.34) and the average cost for unsuccessful search is obtained by
dividing this by the total number of conŀgurations
(M
N
).
Ļe average cost for a successful search is obtained by averaging the
unsuccessful search cost, as in the proof of Ļeorem 9.6.
Ļus, for α = N/M, the average cost for a successful search is asymp-
totic to 1/(1−α). Intuitively, for small α, we expect that the probability that
the ŀrst cell examined is full to be α, the probability that the ŀrst two cells
examined are full to be α2, and so on, so the average cost should be asymptotic
to
1 + α + α2 + α3 + . . . .
Ļis analysis validates that intuition under the uniformity assumption. Ļe
cost for successful searches can be calculated by averaging the average costs
for unsuccessful searches, as in the proof of Ļeorem 9.6.
Ļe uniform hashing algorithm is impractical because of the cost of gen-
erating a permutation for each key, but the corresponding model does provide
a performance goal for other collision resolution strategies. Double hashing
is an attempt at approximating such a “random” collision resolution strategy,
and it turns out that its performance approximates the results for uniform
hashing, though this is a diﬃcult result that was some years in the making
(see Guibas and Szemeredi [20] and Lueker and Molodowitch [30]).
Analysis of linear probing.
Linear probing is a fundamental searching
method, and an analytic explanation of the clustering phenomenon is clearly
of interest. Ļe algorithm was ŀrst analyzed by Knuth in [25], where he states
in a footnote that this derivation had a strong inłuence on the structure of

§Ƞ.ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȘȚ
his books. Since Knuth’s books certainly have had a strong inłuence on the
structure of research in the mathematical analysis of algorithms, we begin by
presenting Kunth’s classic derivation as a prototype example showing how a
simple algorithm can lead to nontrivial and interesting mathematical prob-
lems.
Following Knuth, we deŀne three quantities that we will use to develop
an exact expression for the cumulative cost for unsuccessful searches:
fNM = {# words where 0 is left empty}
gNMk = {# words where 0 and k + 1 are left empty, 1 through k full}
pNMj = {# words where inserting the (N + 1)st key takes j + 1 steps}.
As usual, by words in this context, we mean “M-words of length N,” the
sequences of hashed values that we assume to be equally likely, each occurring
with probability 1/MN.
First, we get an explicit expression for fNM by noting that position 0 is
equally likely to be empty as any other table position. Ļe MN hash sequences
each leave M −N empty table positions, for a grand total of (M −N)M N,
and dividing by M gives
fNM = (M −N)MN−1.
Second, we can use this to get an explicit expression for gNMk. Ļe
empty positions divide each hash sequence to be included in the count into
two independent parts, one containing k elements hashing into positions 0
through k and leaving 0 empty, and the other containing N −k elements
hashing into positions k+1 through M−1 and leaving k+1 empty. Ļerefore,
gNMk =
(
N
k
)
fk(k+1)f(N−k)(M−k−1)
=
(
N
k
)
(k + 1)k−1(M −N −1)(M −k −1)N−k−1.
Ļird, a word will involve j+1 steps for the insertion of the (N+1)st key
whenever the hashed position is in the jth position of a block of k consecutive

ȜȘț
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ȝ
occupied cells (with k ≥j) delimited at both ends by unoccupied cells. Again
by circular symmetry, the number of such words is gNMk, so
pNMj =
∑
j≤k≤N
gNMk.
Now, we can use the cumulated counts pNMj to calculate the average
search costs, just as we did earlier. Ļe cumulated cost for an unsuccessful
search is
∑
j≥0
(j + 1)pNMj =
∑
j≥0
(j + 1)
∑
j≤k≤N
gNMk =
∑
k≥0
gNMk
∑
0≤j≤k
(j + 1)
= 1
2
∑
k≥0
(k + 1)(k + 2)gNMk
= 1
2
∑
k≥0
((k + 1) + (k + 1)2)gNMk.
Substituting the expression for gMNk just derived, dividing by MN, and sim-
plifying, we ŀnd that the average cost for an unsuccessful search in linear
probing is
1
2(S[1]
NM1 + S[2]
NM1)
where
S[i]
NMt ≡M −t −N
MN
∑
k
(
N
k
)
(k + t)k−1+i(M −k −t)N−k−i.
Ļis rather daunting function is actually rather easily evaluated with Abel’s
identity (Exercise 3.66 in §3.11). Ļis immediately gives the result
tS[0]
NMt = 1 −N
M .
For larger i it is easy to prove (by taking out one factor of (k + t)) that
S[i]
NMt = N
M S[i]
(N−1)M(t+1) + tS[i−1]
NMt.

§Ƞ.ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȘȜ
Ļerefore,
S[1]
NMt = N
M S[1]
(N−1)M(t+1) + 1 −N
M ,
which has the solution
S[1]
NMt = 1.
Ļis is to be expected, since, for example, S[1]
NM1 = ∑
k(pNMk)/MN, a sum
of probabilities. Finally, for i = 2, we have
S[2]
NMt = N
M S[2]
(N−1)M(t+1) + t,
which has the solution
S[2]
NM1 =
∑
0≤i≤N
i
N!
M i(N −i)!.
Ļeorem 9.8 (Hashing with linear probing).
Using a table of size M for N
keys, linear probing requires
1
2 + 1
2
∑
0≤i<N
(N −1)!
Mi(N −i −1)! = 1
2
(
1 +
1
1 −α
)
+ O
( 1
N
)
probes for a successful search and
1
2 + 1
2
∑
0≤i≤N
i
N!
Mi(N −i)! = 1
2
(
1 +
1
(1 −α)2
)
+ O
( 1
N
)
for an unsuccessful search, on the average. Ļe asymptotic forms hold for
α = N/M with α < 1.
Proof. See the earlier discussion. Ļe expression for a successful search is ob-
tained by averaging the result for an unsuccessful search, as usual.
If α is strictly less than 1, the sum is similar to the Ramanujan Q-
function of Ļeorem 4.8, and it is not diﬃcult to estimate using the Laplace
method. We have
∑
0≤i≤N
N!
M i(N −i)! =
∑
0≤i≤N
( N
M
)i
N!
Ni(N −i)!.

ȜȘȝ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ȝ
Splitting the sum into two parts, we can use the fact that terms in this sum
begin to get negligibly small after i >
√
N to prove that this sum is
∑
i≥0
( N
M
)i(
1 + O
( i2
N
))
=
1
1 −α + O
( 1
N
)
.
Adding 1 and dividing by 2 gives the stated result for a successful search. A
similar calculation gives the stated estimate for an unsuccessful search.
Corollary Ļe average number of table entries examined by linear probing
during a successful search in a full table is ∼
√
πN/2.
Proof. Taking M = N gives precisely the Ramanujan Q-function, whose
approximate value is proved in Ļeorem 4.8.
Despite the relatively simple form of the solution, a derivation for the
average cost of linear probing via analytic combinatorics challenged researchers
for many years. Ļere are many interesting relationships among the quanti-
ties that arise in the analysis. For example, if we multiply the expression for
a successful search in Ļeorem 9.8 by zN−1, divide by (N −1)!, and sum for
all N > 0, we get the rather compact explicit result
1
2
(
ez +
eM
1 −z/M
)
.
Ļis is not directly meaningful for linear probing because the quantities are
deŀned only for N ≤M but it would seem a ŀne candidate for a combina-
torial interpretation.
In 1998, motivated by a challenge in a footnote in the ŀrst edition of this
book [13], Flajolet, Poblete, Viola, and Knuth, in a pair of companion pa-
pers [12][26], developed independent analytic-combinatoric analyses of lin-
ear probing that uncover the rich combinatorial structure of this problem.
Ļe end results provide in themselves a convincing example of the utility of
analytic combinatorics in the analysis of algorithms. Ļey include moments
and even full distributions in sparse and full tables and relate the problem to
graph connectivity, inversions in Cayley trees, path length in trees, and other
problems. Analysis of hashing algorithms remains an area of active research.

§Ƞ.ȝ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȘȞ
Exact costs for N keys in a table of size M
successful search
unsuccessful search
separate chaining
1 + N
2M
1 + N
M
uniform hashing
M + 1
M −N + 1
M + 1
N
(HM−1 −HM−N+1)
linear probing
1
2
(
1 +
∑
k
k!
M k
(N −1
k
))
1
2
(
1 +
∑
k
k k!
M k
(N
k
))
Asymptotic costs as N, M →∞with α ≡N/M
average
.5 .9 .95
small α
unsuccessful search
separate chaining
1 + α
2 2
2
1 + α
uniform hashing
1
1 −α
2 10 20
1 + α + α2 + . . .
double hashing
1
1 −α
2 10 20
1 + α + α2 + . . .
linear probing
1
2
(
1 +
1
(1 −α)2
)
3 51 201
1 + α + 3α2
2
+ . . .
successful search
separate chaining
1 + α
2
1 1
1
1 + α
2
uniform hashing
1
αln(1 + α)
1 3
4
1 + α
2 + α2
3 + . . .
double hashing
1
αln(1 + α)
1 3
4
1 + α
2 + α2
3 + . . .
linear probing
1
2
(
1 +
1
1 −α
)
2 6 11
1 + α
2 + α2
2 + . . .
Table 9.9
Analytic results for hashing methods

ȜȘȟ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ȝ
TŔő ōşťřŜŠśŠŕŏ ŜőŞŒśŞřōŚŏő śŒ the hashing methods we have discussed
is summarized in Table 9.9. Ļis table includes the asymptotic cost as a func-
tion of the load factor α ≡N/M as the table size M and the number of
keys N grow; an expansion of the cost function that estimates the cost for
small α; and approximate values of the functions for typical values of α. Ļe
table shows that all the methods perform roughly the same for small α; that
linear probing begins to degrade to an unacceptable level when the table gets
80–90% full; and that the performance of double hashing is quite close to
“optimal” (same as separate chaining) unless the table is very full. Ļese and
related results can be quite useful in the application of hashing in practice.
Exercise 9.37 Find [zn]eαC(z) where C(z) is the Cayley function (see the discussion
at the end of §6.14 and in §9.7 in this chapter).
Exercise 9.38 (“Abel’s binomial theorem.”) Use the result of the previous exercise
and the identity e(α+β)C(z) = eαC(z)eβC(z) to prove that
(α + β)(n + α + β)n−1 = αβ
∑
k
(n
k
)
(k + α)k−1(n −k + β)n−k−1.
Exercise 9.39 How many keys can be inserted into a linear probing table of size M
before the average search cost gets to be greater than lnN?
Exercise 9.40 Compute the exact cost of an unsuccessful search using linear probing
for a full table.
Exercise 9.41 Give an explicit representation for the EGF for the cost of an unsuc-
cessful search.
Exercise 9.42 Use the symbolic method to derive the EGF of the number of probes
required by linear probing in a successful search, for ŀxed M.*
* Ļe temptation to include a footnote at this point cannot be resisted: while
we still do not quite know the answer to this exercise (see the comment at the end of
[26]), it is perhaps irrelevant because we do have full information on the performance
of a large class of hashing algorithms that includes linear probing (see [21] and [36]).

§Ƞ.Ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȘȠ
9.7 Mappings. Ļe study of hashing into a full table leads naturally to
looking at properties of mappings from the set of integers between 1 and N
onto itself. Ļe study of these leads to a remarkable combinatorial structure
that is simply deŀned, but it encompasses much of what we have studied in
this book.
Deŀnition An N-mapping is a function f mapping integers from the inter-
val [1 . . . N] into the interval [1 . . . N].
As with words, permutations, and trees, we specify a mapping by writing
down its functional table:
index
1
2
3
4
5
6
7
8
9
mapping
9
6
4
2
4
3
7
8
6
As usual, we drop the index and specify a mapping simply as a sequence of
N integers in the interval 1 to N (the image of the mapping). Clearly, there
are NN diﬀerent N-mappings. We have used similar representations for per-
mutations (Chapter 7) and trees (Chapter 6)—mappings encompass both of
these as special cases. For example, a permutation is a mapping where the
integers in the image are distinct.
Naturally, we deŀne a random mapping to be a sequence of N random
integers in the range 1 to N. We are interested in studying properties of
random mappings. For example, the probability that a random mapping is a
permutation is N!/NN ∼
√
2πN/eN.
Image cardinality. Some properties of mappings may be deduced from prop-
erties of words derived in the previous section. For example, by Ļeorem 9.5,
we know that the average number of integers that appear k times in the map-
ping is ∼Ne−1/k!, the Poisson distribution with α = 1. A related question
of interest is the distribution of the number of diﬀerent integers that appear,
the cardinality of the image. Ļis is N minus the number of integers that do
not appear, or the number of “empty urns” in the occupancy model, so the
average is ∼(1 −1/e)N by the corollary to Ļeorem 9.5. A simple count-
ing argument says that the number of mappings with k diﬀerent integers in
the image is given by
(N
k
) (choose the integers) times k!
{N
k
} (count all the
surjections with image of cardinality k). Ļus,
CNk = k!
(
N
k
){
N
k
}
.

Ȝșȗ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȟ
0
.111
.222
.667
0
Figure 9.9
(.63212 · · ·)N
N
Image cardinality of random mappings for 3 ≤N ≤50
(k-axes scaled to N)
Ļis distribution is plotted in Figure 9.8.
Exercise 9.43 Find the exponential BGF for the image cardinality distribution.
Exercise 9.44 Use a combinatorial argument to ŀnd the exponential BGF for the
image cardinality distribution.
Exercise 9.45 Give a recurrence relationship for the number of mappings of size N
with k diﬀerent integers in the image, and use that to obtain a table of values for
N < 20.
Exercise 9.46 Give an explicit expression for the number of M-words of length N
having k diﬀerent letters.
Random number generators. A random N-mapping is any function f with
the integers 1 to N as both domain and range, where all NN such functions

§Ƞ.Ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜșȘ
are taken with equal likelihood. For example, the following mapping is de-
ŀned by the function f(i) ≡1 + i2 mod 9:
index
1
2
3
4
5
6
7
8
9
mapping
2
5
1
8
8
1
5
2
1
One application of such functions is to model random number generators: sub-
routines that return sequences of numbers with properties as similar as pos-
sible to those of random sequences. Ļe idea is to choose a function that is
an N-mapping, then produce a (pseudo) random sequence by iterating f(x)
starting with an initial value called the seed. Given a seed u0, we get the se-
quence
u0
u1 = f(u0)
u2 = f(u1) = f(f(u0))
u3 = f(u2) = f(f(f(u0)))
...
For example, linear congruential random number generators are based on
f(x) = (ax + b) mod N,
and quadratic random number generators are based on
f(x) = (ax2 + bx + c) mod N.
Quadratic random number generators are closely related to the middle
square method, an old idea that dates back to von Neumann’s time: Starting
with a seed u0, repeatedly square the previously generated value and extract
the middle digits. For example, using four-digit decimal numbers, the se-
quence generated from the seed u0 = 1234 is u1 = 5227 (since 12342 =
01522756), u2 = 3215 (since 52272 = 27321529), u3 = 3362 (since 32152 =
10336225), and so forth.
It is easy to design a linear congruential generator so that it produces a
permutation (that is, it goes through N diﬀerent values before it repeats). A
complete algebraic theory is available, for which we refer the reader to Knuth
[24].

Ȝșș
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȟ
Quadratic random number generators are harder to analyze mathemat-
ically. However, Bach [2] has shown that, on average, quadratic functions
have characteristics under iteration that are essentially equivalent to those of
random mappings. Bach uses deep results from algebraic geometry; as we
are going to see, properties of random mappings are somewhat easier to an-
alyze given all the techniques developed so far in this book. Ļere are N3
quadratic trinomials modulo N, and we are just asserting that these are rep-
resentative of the NN random mappings, in the sense that the average values
of certain quantities of interest are asymptotically the same. Ļe situation
is somewhat analogous to the situation for double hashing described earlier:
in both cases, the practical method (quadratic generators, double hashing) is
studied through asymptotic equivalence to the random model (random map-
pings, uniform hashing).
In other words, quadratic generators provide one motivation for the
study of what might be called random random number generators, where a ran-
domly chosen function is iterated to produce a source of random numbers. In
this case, the result of the analysis is negative, since it shows that linear con-
gruential generators might be preferred to quadratic generators (because they
have longer cycles), but an interesting outcome of these ideas is the design
and analysis of the Pollard rho method for integer factoring, which we discuss
at the end of this section.
Exercise 9.47 Prove that every random mapping must have at least one cycle.
Exercise 9.48 Explore properties of the random mappings deŀned by f(i) ≡1 +
(i2 + 1) mod N for N = 100, 1000, 10,000, and primes near these values.
Path length and connected components. Since the operation of applying a
mapping to itself is well deŀned, we are naturally led to consider what happens
if we do so successively. Ļe sequence f(k), f(f(k)), f(f(f(k))), . . . is well
deŀned for every k in a mapping: what are its properties? It is easy to see that,
since only N distinct values are possible, the sequence must ultimately repeat a
value, at which point it becomes cyclic. For example, as shown in Figure 9.10,
if we start at x0 = 3 in the mapping deŀned by f(x) = x2 + 1 mod 99, we
have the ultimately cyclic sequence 3, 10, 2, 5, 26, 83, 59, 17, 92, 50, 26, . . . .
Ļe sequence always has a cycle preceded by a “tail” of values leading to the
cycle. In this case, the cycle is of length 6 and the tail of length 4. We are
interested in knowing the statistical properties of both cycle and tail lengths
for random mappings.

§Ƞ.Ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜșȚ
3
10
2
5
26
26
83
59
17
92
50
Figure 9.10 Tail and cycle iterating f(x) = x2 + 1 mod 99 from x0 = 3
Cycle and tail length depend on the starting point. Figure 9.11 is a
graphical representation showing i connected to f(i) for each i for three sam-
ple mappings. For example, in the top mapping, if we start at 7 we immedi-
ately get stuck in the cycle 7 8 7 . . . , but if we start at 1 we encounter a
two-element tail followed by a four-element cycle. Ļis representation more
clearly exposes the structure: every mapping decomposes into a set of con-
nected components, also called connected mappings. Each component consists
of the set of all points that wind up on the same cycle, with each point on the
cycle attached to a tree of all points that enter the cycle at that point. From the
point of view of each individual point, we have a tail-cycle as in Figure 9.10,
but the structure as a whole is certainly more informative about the mapping.
Mappings generalize permutations: if we have the restriction that each
element in the range must appear once, we have a set of cycles. All tail lengths
are 0 for mappings that correspond to permutations. If all the cycle lengths in
a mapping are 1, it corresponds to a forest. In general, we are naturally led to
consider the idea of path length:
Deŀnition Ļe path length or rho length for an index k in a mapping f is the
number of distinct integers obtained by iterating
f(k), f(f(k)), f(f(f(k))), f(f(f(f(k)))), . . . .
Ļe cycle length for an index k in a mapping f is the length of the cycle reached
in the iteration, and the tail length for an index k in a mapping f is the rho
length minus the cycle length or, equivalently, the number of steps taken to
connect to the cycle.

Ȝșț
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȟ
Ļe path length of an index is called the “rho length” because the shape
of the tail plus the cycle is reminiscent of the Greek letter ρ (see Figure 9.10).
Beyond these properties of the mapping as seen from a single point, we are
also interested in global measures that involve all the points in the mapping.
Deŀnition Ļe rho length of a mapping f is the sum, over all k, of the rho
length for k in f. Ļe tree path length of a mapping f is the sum, over all k,
of the tail length for k in f.
Ļus, from Figure 9.11, it is easy to verify that 9 6 4 2 4 3 8 7 6 has
rho length 36 and tree path length 4; 3 2 3 9 4 9 9 4 4 has rho length
20 and tree path length 5; and 1 3 1 3 3 6 4 7 7 has rho length 27 and
tree path length 18. In these deŀnitions, tree path length does not include
costs for any nodes on cycles, while rho length includes the whole length of
1
2
3
4
5
6
7
8
9
1
77
33
1
2
3
4
5
6
7
8
9
3
4
99
4
1
2
3
4
5
6
7
8
9
7
8
2
3
4
6
9 6 4 2 4 3 8 7 6  
1 2 3 4 5 6 7 8 9
index
mapping
index
mapping
index
mapping
3 2 3 9 4 9 9 4 4  
1 2 3 4 5 6 7 8 9
1 3 1 3 3 6 4 7 7  
1 2 3 4 5 6 7 8 9
Figure 9.11 Tree-cycle representation of three random mappings

§Ƞ.Ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜșȜ
mapping
cycles trees
rho
length
longest
cycle
longest
path
123    3    0    3    1    1
113    2    1    4    1    2
121    2    1    4    1    2
122    2    1    4    1    2
133    2    1    4    1    2
223    2    1    4    1    2
323    2    1    4    1    2
112    1    1    6    1    3
131    1    1    6    1    3
221    1    1    6    1    3
322    1    1    6    1    3
233    1    1    6    1    3
313    1    1    6    1    3
111    1    2    5    1    2
222    1    2    5    1    2
333    1    2    5    1    2
213    2    0    5    2    2
321    2    0    5    2    2
132    2    0    5    2    2
211    1    1    7    2    3
212    1    1    7    2    3
232    1    1    7    2    3
311    1    1    7    2    3
331    1    1    7    2    3
332    1    1    7    2    3
231    1    0    9    3    3
312    1    0    9    3    3
Figure 9.12 Basic properties of all mappings of three elements

Ȝșȝ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȟ
the cycle for each node in the structure. Both deŀnitions give the standard
notion of path length for mappings that are trees.
We are interested in knowing basic properties of the kinds of structures
shown in Figure 9.11:
• How many cycles are there?
• How many points are on cycles, and how many on trees?
• What is the average cycle size?
• What is the average rho length?
• What is the average length of the longest cycle?
• What is the average length of the longest path to a cycle?
• What is the average length of the longest rho-path?
Figure 9.12 gives an exhaustive list of the basic measures for all 3-mappings,
and Table 9.11 gives six random 9-mappings. On the right in Figure 9.12 are
the seven diﬀerent tree-cycle structures that arise in 3-mappings, reminding
us that our tree-cycle representations of mappings are labelled and ordered
combinatorial objects.
As with several other problems that we have seen in this chapter, some
properties of random mappings can be analyzed with a straightforward prob-
abilistic argument. For example, the average rho length of a random mapping
is easily derived.
Ļeorem 9.9 (Rho length).
Ļe rho length of a random point in a random
mapping is ∼
√πN/2, on the average. Ļe rho length of a random mapping
is ∼N
√πN/2, on the average.
rho
longest longest
mapping
occupancy
distribution cycles trees length cycle
path
323949944
012300003
5112
3
3
20
2
3
131336477
203101200
4221
2
1
27
1
5
517595744
100230201
4221
2
4
29
3
5
215681472
220111110
2520
1
2
42
2
8
213693481
212101011
2520
3
2
20
2
4
964243876
011212111
1620
2
2
36
4
6
Table 9.10
Basic properties of some random 9-mappings

§Ƞ.Ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜșȞ
Proof. Suppose that we start at x0. Ļe probability that f(x0) ̸= x0 is clearly
(N −1)/N. Ļis is the same as the probability that the rho length is greater
than or equal to 1. Similarly, the probability that the rho length is greater
than or equal to 2 is the probability that the ŀrst two elements are diﬀerent
(f(x0) ̸= x0) and the third is diﬀerent from both of the ŀrst two (f(f(x0)) ̸=
x0 and f(f(x0)) ̸= f(x0)), or (N −1)/N times (N −2)/N. Continuing,
we have
Pr{rho length ≥k} = N −1
N
N −2
N
· · · N −k
N
.
Ļus, the average rho length of a random point in a random mapping is the
sum of these cumulative probabilities, which is precisely the Ramanujan Q-
function, so the approximation of Ļeorem 4.8 provides our answer. Ļe same
argument holds for each of the N points in the mapping, so the expected rho
length of the mapping is obtained by multiplying this by N.
Ļis problem is equivalent to the birthday problem of §9.3, though the
models of randomness are not formally identical.
Exercise 9.49 Show that the analysis of the rho length of a random point in a random
mapping is equivalent to that for the birthday problem.
Generating functions.
Many other properties of mappings depend more
upon global structural interactions. Ļey are best analyzed with generating
functions. Mappings are sets of cycles of trees, so their generating functions
are easily derived with the symbolic method. We proceed exactly as for count-
ing “sets of cycles” when we introduced the symbolic method in Chapter 5,
but with trees as the basic object.
We begin, from §6.14, with the EGF for Cayley trees:
C(z) = zeC(z).
As in Chapter 5, the EGF that enumerates cycles of trees (connected map-
pings) is
∑
k≥1
C(z)k
k
= ln
1
1 −C(z)
and the EGF that enumerates sets of connected mappings is
exp
(
ln
1
1 −C(z)
)
=
1
1 −C(z).

Ȝșȟ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȟ
Ļe functional equations involve the implicitly deŀned Cayley function C(z),
and the Lagrange inversion theorem applies directly. For example, applying
the theorem to the EGF just derived leads to the following computation:
[zN]
1
1 −C(z) = 1
N [uN−1]
1
(1 −u)2 eNu
=
∑
0≤k≤N
(N −k)Nk−1
k!
=
∑
0≤k≤N
Nk
k! −
∑
1≤k≤N
Nk−1
(k −1)!
= NN
N! .
Ļis is a check on the fact that there are NN mappings of size N.
Table 9.11 gives the EGFs for mappings, all derived using the fol-
lowng Lemma, which summarizes the application of the Lagrange inversion
theorem to functions of the Cayley function.
Lemma For the Cayley function C(z), we have
[zN]g(C(z)) =
∑
0≤k<N
(N −k)gN−k
Nk−1
k!
when
g(z) ≡
∑
k≥0
gkzk.
Proof. Immediate from Ļeorem 6.11.
class
EGF
coeﬃcient
trees
C(z) = zeC(z)
(N −1)![uN−1]eNu = N N−1
cycles of trees
ln
1
1 −C(z)
(N −1)![uN−1]
1
1 −ueNu ∼N N/
√
πN
mappings
exp
(
ln
1
1 −C(z)
)
(N −1)![uN−1]
1
(1 −u)2 eNu = N N
Table 9.11
Exponential generating functions for mappings

§Ƞ.Ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜșȠ
Ļus, the number of connected mappings of N nodes is given by
N![zN]ln
1
1 −C(z) = N!
∑
0≤k<N
(N −k)
1
N −k
Nk−1
k!
= NN−1Q(N).
Ļe Ramanujan Q-function again makes an appearance.
Ļe above results imply that the probability that a random mapping is
a tree is exactly 1/N and the probability that a random mapping is a single
connected component is asymptotically
√
π/(2N). We can use BGFs in a
similar manner to analyze other properties of random mappings.
Ļeorem9.10 (Componentsandcycles).
A random N-mapping has ∼1
2lnN
components and ∼
√
πN nodes on cycles, on the average.
Proof. By the earlier discussion, the BGF for the distribution of the number
of components is given by
exp
(
uln
1
1 −C(z)
)
=
1
(1 −C(z))u .
Ļe average number of components is then given by
1
NN [zN] ∂
∂u
1
(1 −C(z))u

u=1
=
1
NN [zN]
1
1 −C(z)ln
1
1 −C(z)
=
∑
0≤k≤N
(N −k)HN−k
Nk−1
k!
by the lemma given previously. Ļe stated asymptotic result is a straight-
forward calculation in the manner of the proof of Ļeorem 4.8.
Ļe BGF for the number of nodes on cycles is
exp
(
ln
1
1 −uC(z)
)
=
1
1 −uC(z),
from which coeﬃcients can be extracted exactly as shown earlier.

ȜȚȗ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.Ȟ
Average as seen from a random point
rho length
√
πN/2
tail length
√
πN/8
cycle length
√
πN/8
tree size
N/3
component size
2N/3
Average number of
k-nodes
Ne−1
k!
k-cycles
1
k
k-components
e−k
k! {# k-node connected mappings}
k-trees
(
√
πN/2 )e−k
k! {# k-node trees}
Extremal parameters (expected number of nodes in)
longest tail
√
2πN ln2 ≈1.74
√
N
longest cycle
≈0.78
√
N
longest rho-path
≈2.41
√
N
largest tree
≈0.48N
largest component
≈0.76N
Table 9.12
Asymptotic properties of random mappings

§Ƞ.Ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȚȘ
Other properties. Various other properties can be handled similarly. Also,
proceeding just as for permutations in Chapter 7 and for words earlier in this
chapter, we ŀnd that the number of mappings having exactly k components
has EGF C(z)k/k!; the number of mappings having at most k components
has EGF 1 + C(z) + C(z)2/2! + . . . + C(z)k/k!; and so forth. Ļese can
be used to give explicit expressions for extremal parameters, as we have done
several times before. Asymptotic methods for estimating these quantities are
discussed in detail by Flajolet and Odlyzko [11] and by Kolchin [28] (see also
[14]). Ļe results given in [11] are summarized in Table 9.13. Ļe various
constants in the extremal parameters can be expressed explicitly, though with
rather complicated deŀnite integrals. It is interesting to note that cycle length
plus tail length equals rho length for the averages, but not for the extremal
parameters. Ļis means that the tallest tree is not attached to the longest cycle
for a signiŀcant number of mappings.
Exercise9.50 Which N-mappings have maximal and minimal rho length? Tree path
length?
Exercise9.51 Write a program to ŀnd the rho length and tree path length of a random
mapping. Generate 1000 random mappings for N as large as you can and compute
the average number of cycles, rho length, and tree path length.
Exercise9.52 Write a program to ŀnd the rho length and tree path length of a random
mapping without using any extra storage.
Exercise 9.53 A mapping with no repeated integers is a permutation. Give an eﬃ-
cient algorithm for determining whether a mapping is a tree.
Exercise 9.54 Compute the average size of the largest component in a random N-
mapping, for all N < 10.
Exercise 9.55 Prove Ļeorem 9.9 using BGFs.
Exercise 9.56 What is the average number of diﬀerent integers in the image when a
random mapping is iterated twice?
Exercise 9.57 Consider the N N tree-cycle structures that correspond to all the N-
mappings. How many of these are diﬀerent when considered as unlabelled, unordered
objects, for N ≤7? (Ļese are called random mapping patterns.)
Exercise 9.58 Describe the graph structure of partial mappings, where the image of
a point may be undeŀned. Set up the corresponding EGF equations and check that
the number of partial mappings of size N is (N + 1)N.

ȜȚș
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ȟ
Exercise9.59 Analyze “path length” in sequences of 2N random integers in the range
1 to N.
Exercise 9.60 Generate 100 random mappings of size 10, 100, and 1000 and empir-
ically verify the statistics given in Table 9.13.
9.8 Integer Factorization and Mappings. In this section we will exam-
ine Pollard's rho method, an eﬃcient algorithm for factoring integers (in the
“intermediate” range of 10 to 30 decimal digits) that relies on structural and
probabilistic properties of mappings. Ļe method is based on exploiting the
following two facts:
• A point on some cycle can be found quickly (in time proportional to the
rho length of a starting point), using O(1) memory.
• A random point on a random mapping has rho length O(
√
N ) on the
average.
First, we will consider the cycle detection problem, and then we look at the
Pollard rho method itself.
Cycledetection. Ļe naive method for ŀnding a point on a cycle of a mapping
is to iterate the mapping, storing all the function values in a search structure
and looking up each new value to see if some value already in the structure has
been reached again. Ļis algorithm is impractical for large mappings because
one cannot aﬀord the space to save all the function values.
Program 9.3 gives a method due to Floyd (cf. Knuth [24]) for ŀnding
cycle points in an arbitrary mapping using only constant space without sac-
riŀcing time. In the program, we can view the point a as moving along the
rho-graph (see Figure 9.9) at speed 1 and the point b as moving along the
rho-graph at speed 2. Ļe algorithm depends on the fact that the two points
must collide at some point once they are on the cycle. For example, suppose
int a = x, b = f(x), t = 0;
while (a != b)
{ a = f(a); b = f(f(b)); t++; }
Program 9.3 Floyd method for cycle detection

§Ƞ.ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȚȚ
that the method is used for the mapping of Figure 9.9 with the starting point
10. Ļen the cycle is detected in 7 steps, as shown by the following trace of
the values taken on by a and b:
a
10
2
5
26
83
59
17
b
10
5
83
17
50
83
17
Ļeorem 9.11 (Cycle detection).
Given a mapping and a starting point x,
the Floyd method ŀnds a cycle point on the mapping using constant space
and time proportional to the rho length of x.
Proof. Let λ be the tail length of x, µ the cycle length, and ρ = λ + µ the rho
length. After λ steps of the loop, point a reaches the cycle, while point b is
already on the cycle. We now have a race on the cycle, with a speed diﬀerential
of 1. After at most µ steps, b will catch up with a.
Let t be the value of the variable t when the algorithm terminates. We
also must have
t ≤ρ ≤2t.
Ļe inequality on the left holds because t is the position of point a, and the
algorithm terminates before a starts on a second trip around the cycle. Ļe
inequality on the right holds because 2t is the position of point b, and the
algorithm terminates after b has been around the cycle at least once. Ļus,
the algorithm gives not only a point on the cycle (the value of the variables a
and b on termination), but also an estimate of the rho length of the starting
point to within a factor of 2.
By saving some more function values, it is possible to virtually eliminate
the extra factor of 2 in the time taken by the algorithm (and the uncertainty
in the estimate of the rho length), while still using a reasonable amount of
space. Ļis issue is studied in detail by Sedgewick, Szymanksi, and Yao [34].
Exercise 9.61 Use Floyd’s method to test the random number generators on your
machine for short cycles.
Exercise 9.62 Use Floyd’s algorithm to test the middle square random number gen-
erator.
Exercise 9.63 Use Floyd’s method to estimate the rho length associated with various
starting values, c, and N for the function f(x) = (x2 + c) mod N.

ȜȚț
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
§Ƞ.ȟ
Pollard's rho method.
Ļe rho method is a randomized algorithm that
factors integers with high probability. An implementation is given in Pro-
gram 9.4, with the caveat that it assumes that arithmetic operations on very
large integers are available. Ļe method is based on choosing a value c at
random, then iterating the quadratic function f(x) = (x2 + c) mod N from
a randomly chosen starting point until a cyclic point is found.
For simplicity, assume that N = pq where p and q are primes to be
found by the algorithm. By the Chinese remainder theorem, any integer y
modulo N is determined by its values mod p and mod q. In particular, the
function f is determined by the pair
fp(x) = x2 + c mod p
and
fq(x) = x2 + c mod q.
If the cycle detection algorithm were applied to fp starting at an initial value
x, then a cycle would be detected after tp steps where tp is at most twice the
rho length of x (modulo p). Similarly, a cycle (modulo q) for tq would be
detected after tq steps. Ļus, if tp ̸= tq (which should occur with a very high
probability for large integers), we ŀnd after min(tp, tq) steps that the values
a and b of the variables a and b in the algorithm satisfy
a ≡b
(mod p)
and
a ̸≡b
(mod q),
if
tp < tq
a ̸≡b
(mod p)
and
a ≡b
(mod q),
if
tp > tq.
In either case, the greatest common divisor of a −b and N is a nontrivial
divisor of N.
int a = (int) (Math.random()*N), b = a;
int c = (int) (Math.random()*N), d = 1;
while (d == 1)
{
a = (a*a + c) % N;
b = (b*b + c)*(b*b + c) + c % N;
d = gcd((a - b) % N, N);
}
// d is a factor of N
Program 9.4 Pollard’s rho method for factoring

§Ƞ.ȟ
W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȚȜ
Connection with random mappings. Assume that quadratic functions of the
form x2+c mod N have path length properties that are asymptotically equiv-
alent to path lengths in random mappings. Ļis heuristic assumption asserts
that properties of the N quadratic mappings (there are N possible choices
for c) are similar to properties of the NN random mappings. In other words,
quadratic functions are assumed to be a “representative sample” of random
mappings. Ļis assumption is quite plausible and has been extensively vali-
dated by simulations, but it has only been partially proven [2]. Nevertheless,
it leads to a useful approximate analysis for Pollard’s method.
Ļe earlier discussion revealed that the number of steps taken by the
algorithm is min(tp, tq), where tp and tq are the rho lengths of fp and fq,
respectively. By Ļeorem 9.9, the rho length of a random point on a random
N-mapping is O(
√
N ), so under the assumption discussed in the previous
paragraph we should expect the algorithm to terminate in O(min (√p, √q ))
steps, which is O(N1/4). Ļis argument obviously generalizes to the situation
where N has more than two factors.
Ļeorem 9.12 (Pollard’s rho method).
Under the heuristic assumption that
path length in quadratic functions is asymptotic to path length in random
mappings on average, Pollard’s rho method factors a composite integer num-
ber N in O(√p) steps, on the average, where p is the smallest prime factor of
N. In particular, the method factors N in O(N1/4) steps, on the average.
N
number of steps
13·23
4
127·331
10
1237·4327
21
12347·54323
132
123457·654323
243
1234577·7654337
1478
12345701·87654337
3939
123456791·987654323
11225
1234567907·10987654367
23932
Table 9.13
Sample applications of Pollard’s algorithm (c = 1)

ȜȚȝ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
Proof. See the earlier discussion. Ļe global bound follows from the fact that
p ≤
√
N.
In 1980, Brent [31] used Pollard’s method to factor the eighth Fermat
number for the ŀrst time. Brent discovered that
F8 = 228 + 1 ≈1.11579 · 1077
has the prime factor
1238926361552897
(also see Knuth [24]). Ļe fact that the approximate analysis currently rests
on a partly unproven assumption does not detract from the utility of the algo-
rithm. Indeed, knowledge of properties of random mappings gives conŀdence
that the method should factor eﬃciently, and it does.
Table 9.13 shows the number of steps used by Pollard’s method to factor
numbers of the form N = pq where p and q are chosen to be primes near
numbers of the form 1234 · · · and · · · 4321, respectively. Ļough c and the
starting points are supposed to be chosen at random, the value c = 1 and
starting points a = b = 1 work suﬃciently well for this application (and
make it easy to reproduce the results). From the table, we can see that the
cost rises roughly by a factor of 3 when N rises by a factor of about 100, in
excellent agreement with Ļeorem 9.12. Ļe method factors the last number
on the list, which is ≈1.35·1019, in fewer than 24,000 steps, while exhaustive
trials would have required about 109 operations.
W
ORDS and mappings relate directly to classical problems in combina-
torics and “classical” problems in the analysis of algorithms. Many of
the methods and results that we have discussed are well known in mathematics
(Bernoulli trials, occupancy problems) and are widely applicable outside the
analysis of algorithms. Ļey are directly relevant to modern applications such
as predicting the performance of hashing algorithms, and detailed study of
problems in this new domain leads to new problems of independent interest.
Hashing algorithms were among the ŀrst to be analyzed mathemati-
cally, and they are still of paramount practical importance. New types of
applications and changes of fundamental characteristics in hardware and soft-
ware contribute to the continued relevance of the techniques and results about
hashing algorithms presented here and in the literature.

W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȚȞ
Ļe analysis of random mappings succinctly summarizes our general
approach to the analysis of algorithms. We develop functional equations on
generating functions corresponding to the underlying combinatorial struc-
ture, then use analytic tools to extract coeﬃcients. Ļe symbolic method is
particularly eﬀective for the former in this case, and the Lagrange inversion
theorem is an important tool for the latter.
Mappings are characterized by the fact that each element maps to pre-
cisely one other element. In the graphical representation, this means that
there are precisely N edges and that each element has exactly one edge point-
ing “from” it, though many elements might point “to” a particular element.
Ļe next generalization is to graphs, where this restriction is removed and
each element can point “to” any number of other elements. Graphs are more
complicated than mappings or any of the other combinatorial structures that
we have examined in this book because they are more diﬃcult to decompose
into simpler substructures—normally our basis for analysis by solving recur-
rences or exploiting structural decomposition to develop relationships among
generating functions.
Random graphs have a wealth of interesting properties. A number of
books have been written on the subject, and it is an active research area.
(See, for example, Bollobás [3] for a survey of the ŀeld.) Analysis of ran-
dom graphs centers on the “probabilistic method,” where the focus is not on
exactly enumerating properties of all graphs, but rather on developing suitable
inequalities that relate complex parameters to tractable ones. Ļere are many
important fundamental algorithms for processing graphs, and there are many
examples in the literature of the analysis of such algorithms. Diﬀerent mod-
els of randomness are appropriate for diﬀerent applications, making analysis
along the lines we have been studying appropriate in many cases. Learning
properties of random graphs is a fruitful area of study in the analysis of algo-
rithms.
Random mappings are an appropriate topic on which to close for many
reasons. Ļey generalize basic and widely-used structures (permutations and
trees) that have occupied so much of our attention in this book; they are of
direct practical interest in the use of random number generators and random
sequences; their analysis illustrates the power, simplicity, and utility of the
symbolic enumeration method and other tools that we have been using; and
they represent the ŀrst step toward studying random graphs (for example, see
Janson, Knuth, Luczak, and Pittel [22]), which are fundamental and widely

ȜȚȟ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
applicable structures. It is our hope that the basic tools and techniques that we
have covered in this book will provide readers with the interest and expertise
to attack these and other problems in the analysis of algorithms that arise in
the future.

W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜȚȠ
References
1. M. AŎŞōřśţŕŠŦ ōŚŐ I. SŠőœšŚ. Handbook of Mathematical Functions,
Dover, New York, 1970.
2. E. BōŏŔ. “Toward a theory of Pollard’s rho method,” Information and
Computation 30, 1989, 139–155.
3. B. BśŘŘśŎŭş. Random Graphs, Academic Press, London, 1985.
4. R. P. BŞőŚŠ ōŚŐ J. M. PśŘŘōŞŐ. “Factorization of the eighth Fermat
number,” Mathematics of Computation 36, 1981, 627–630.
5. B. CŔōŞ, K. GőŐŐőş, G. GśŚŚőŠ, B. LőśŚœ, M. MśŚōœōŚ, ōŚŐ S.
WōŠŠ. Maple V Library Reference Manual, Springer-Verlag, New York,
1991.
6. L. CśřŠőŠ. Advanced Combinatorics, Reidel, Dordrecht, 1974.
7. T. H. CśŞřőŚ, C. E. LőŕşőŞşśŚ, R. L. RŕŢőşŠ, ōŚŐ C. SŠőŕŚ. Intro-
duction to Algorithms, MIT Press, New York, 3rd edition, 2009.
8. F. N. DōŢŕŐ ōŚŐ D. E. BōŞŠśŚ. Combinatorial Chance, Charles Griﬃn,
London, 1962.
9. W. FőŘŘőŞ. An Introduction to Probability Ļeory and Its Applications,
John Wiley, New York, 1957.
10. P. FŘōŖśŘőŠ, D. GōŞŐť, ōŚŐ L. TŔŕřśŚŕőŞ. “Birthday paradox, coupon
collectors, caching algorithms and self-organizing search,” Discrete Ap-
plied Mathematics 39, 1992, 207–229.
11. P. FŘōŖśŘőŠ ōŚŐ A. M. OŐŘťŦŗś. “Random mapping statistics,” in
Advances in Cryptology, J.-J. Quisquater and J. Vandewalle, eds., Lecture
Notes in Computer Science No. 434, Springer-Verlag, New York, 1990,
329–354.
12. P. FŘōŖśŘőŠ, P. PśŎŘőŠő, ōŚŐ A. VŕśŘō. “On the analysis of linear
probing hashing,” Algorithmica 22, 1988, 490–515.
13. P. FŘōŖśŘőŠ ōŚŐ R. SőŐœőţŕŏŗ. Analytic Combinatorics, Cambridge
University Press, 2009.
14. D. FśōŠō ōŚŐ J. RŕśŞŐōŚ. “Mappings of acyclic and parking func-
tions,” Aequationes Mathematicae 10, 1974, 10–22.
15. G. H. GśŚŚőŠ. “Expected length of the longest probe sequence in hash
code searching,” Journal of the ACM 28, 1981, 289–309.

Ȝțȗ
C Ŕ ō Ŝ Š ő Ş
N ŕ Ś ő
16. G. H. GśŚŚőŠ ōŚŐ R. BōőŦō-YōŠőş. Handbook of Algorithms and Data
Structures in Pascal and C, 2nd edition, Addison-Wesley, Reading, MA,
1991.
17. I. GśšŘŐőŚ ōŚŐ D. JōŏŗşśŚ. Combinatorial Enumeration, John Wiley,
New York, 1983.
18. R. L. GŞōŔōř, D. E. KŚšŠŔ, ōŚŐ O. PōŠōşŔŚŕŗ. Concrete Mathemat-
ics, 1st edition, Addison-Wesley, Reading, MA, 1989. Second edition,
1994.
19. L. GšŕŎōş ōŚŐ E. SŦőřőŞőŐŕ. “Ļe analysis of double hashing,” Jour-
nal of Computer and Systems Sciences 16, 1978, 226–274.
20. S. JōŚşśŚ. “Individual displacements for linear probing hashing with
diﬀerent insertion policies,” ACM Transactions on Algorithms 1, 2005,
177–213.
21. S. JōŚşśŚ, D. E. KŚšŠŔ, T. LšŏŦōŗ, ōŚŐ B. PŕŠŠőŘ. “Ļe birth of the
giant component,” Random Structures and Algorithms 4, 1993, 233–358.
22. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 1: Fundamen-
tal Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1968. Ļird
edition, 1997.
23. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 2: Seminumerical
Algorithms, 1st edition, Addison-Wesley, Reading, MA, 1969. Ļird
edition, 1997.
24. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 3: Sorting and
Searching, 1st edition, Addison-Wesley, Reading, MA, 1973. Second
edition, 1998.
25. D. E. KŚšŠŔ. Ļe Art of Computer Programming. Volume 4: Combinato-
rial Algorithms, Part I, Addison-Wesley, Boston, 2011.
26. D. E. KŚšŠŔ. “Linear probing and graphs,” Algorithmica 22, 1988,
561–568.
27. V. F. KśŘŏŔŕŚ. Random Mappings, Optimization Software, New York,
1986.
28. V. F. KśŘŏŔŕŚ, B. A. SőŢōşŠťōŚśŢ, ōŚŐ V. P. CŔŕşŠťōŗśŢ. Random
Allocations, John Wiley, New York, 1978.
29. G. LšőŗőŞ ōŚŐ M. MśŘśŐśţŕŠŏŔ. “More analysis of double hash-
ing,” in Proceedings 20th Annual ACM Symposium on Ļeory of Comput-
ing, 1988, 354–359.

W ś Ş Ő ş
ō Ś Ő
M ō Ŝ Ŝ ŕ Ś œ ş
ȜțȘ
30. J. M. PśŘŘōŞŐ. “A Monte Carlo method for factorization,” BIT 15,
1975, 331–334.
31. R. SőŐœőţŕŏŗ. Algorithms (3rd edition) in Java: Parts 1-4: Fundamen-
tals, Data Structures, Sorting, and Searching, Addison-Wesley, Boston,
2002.
32. R. SőŐœőţŕŏŗ ōŚŐ P. FŘōŖśŘőŠ. An Introduction to the Analysis of Algo-
rithms, Addison-Wesley, Reading, MA, 1996.
33. R. SőŐœőţŕŏŗ, T. SŦťřōŚşŗŕ, ōŚŐ A. Yōś. “Ļe complexity of ŀnding
cycles in periodic functions,” SIAM Journal on Computing 11, 1982, 376–
390.
34. R. SőŐœőţŕŏŗ ōŚŐ K. WōťŚő. Algorithms, 4th edition, Addison-Wesley,
Boston, 2011.
35. N. SŘśōŚő ōŚŐ S. PŘśšŒŒő. Ļe Encyclopedia of Integer Sequences, Aca-
demic Press, San Diego, 1995. Also accessible as On-Line Encyclopedia
of Integer Sequences, http://oeis.org.
36. A. VŕśŘō. “Exact distribution of individual displacements in linear prob-
ing hashing,” ACM Transactions on Algorithms 1, 2005, 214–242.
37. J. S. VŕŠŠőŞ ōŚŐ W. CŔőŚ. Design and Analysis of Coalesced Hashing,
Oxford University Press, New York, 1987.
38. J. S. VŕŠŠőŞ ōŚŐ P. FŘōŖśŘőŠ, “Analysis of algorithms and data struc-
tures,” in Handbook of Ļeoretical Computer Science A: Algorithms and Com-
plexity, J. van Leeuwen, ed., Elsevier, Amsterdam, 1990, 431–524.

This page intentionally left blank 

L I S T O F T H E O R E M S
1.1 Mergesort compares, 9
1.2 Complexity of sorting, 11
1.3 Quicksort analysis, 21
2.1 First-order linear recurrences,
49
2.2 Linear recurrences with con-
stant coeﬃcients, 54
2.3 Binary search, 71
2.4 Mergesort, 73
2.5 Divide-and-conquer functions,
80
2.6 Divide-and-conquer sequences,
83
3.1 OGF operations, 93
3.2 EGF operations, 97
3.3 OGFs for linear recurrences,
102
3.4 Quicksort OGF, 108
3.5 Median-of-three Quicksort,
119
3.6 OGF for binary trees, 123
3.10 Mean and variance from
PGFs, 127
3.11 BGFs and average costs, 133
3.12 Quicksort variance, 136
4.1 Asymptotics for linear recur-
rences, 155
4.2 Euler-Maclaurin summation
formula, ŀrst form, 178
4.3 Euler-Maclaurin summation
formula, second form, 182
4.4 Ramanujan Q-distribution, 186
4.5 Ramanujan R-distribution, 189
4.6 Normal approximation, 194
4.7 Poisson approximation, 198
4.8 Ramanujan Q-function, 202
4.9 Catalan sums, 207
4.10 Trie sum, 210
5.1 Symbolic method for unla-
belled class OGFs, 223
5.2 Symbolic method for labelled
class EGFs, 231
5.3 Symbolic method for unla-
belled class OBGFs, 239
5.4 Symbolic method for labelled
class EBGFs, 240
5.5 Radius-of-convergence transfer
theorem, 247
6.1 Enumeration of binary trees,
258
6.2 Enumeration of forests and
trees, 261
6.3 Path length in binary trees,
286
6.4 Path length in general trees,
290
ȜțȚ

Ȝțț
L ŕ ş Š
ś Œ
T Ŕ ő ś Ş ő ř ş
6.5 Construction cost of BSTs,
292
6.6 Search costs in BSTs, 294
6.7 Additive parameters in random
trees, 296
6.8 Binary tree height, 301
6.9 Catalan tree height GF, 302
6.10 Binary search tree height, 306
6.11 Lagrange inversion theorem,
310
6.12 Enumeration of rooted un-
ordered trees, 323
6.13 Enumeration of unordered
labelled trees, 328
6.14 Enumeration of t-ary trees,
331
6.15 Enumeration of t-restricted
trees, 333
7.1 Frequency of HOTs and BSTs,
362
7.2 Maximal cycle lengths, 365
7.3 Minimal cycle lengths, 366
7.4 Involutions, 367
7.5 Eulerian numbers, 374
7.6 Increasing subsequences, 378
7.7 Local properties of per-
mutations and nodes in
HOTs/BSTs, 380
7.8 Inversion distribution, 384
7.9 Inversions in 2-ordered permu-
tations, 389
7.10 Left-to-right minima distribu-
tion, 392
7.11 Selection sort, 396
7.12 Cycle distribution, 400
7.13 Singleton cycle distribution,
402
7.14 Maximum inversion table
entry, 405
8.1 Pattern occurrence enumera-
tion, 415
8.2 Runs of 0s, 419
8.3 Pattern autocorrelation, 425
8.4 OGFs for regular expressions,
429
8.5 KMP string matching, 435
8.6 OGFs for context-free gram-
mars, 438
8.7 Ballot problem, 442
8.8 Trie path length and size, 455
8.9 Leader election, 460
9.1 Birthday problem, 483
9.2 Coupon collector problem, 486
9.3 Maximal occupancy, 492
9.4 Minimal occupancy, 494
9.5 Occupancy distribution, 499
9.6 Hashing with separate chain-
ing, 504
9.7 Uniform hashing, 507
9.8 Hashing with linear probing,
511
9.9 Rho length, 522
9.10 Components and cycles, 525
9.11 Cycle detection, 529
9.12 Pollard rho method, 531

L I S T O F T A B L E S
1.1 Average number of compares
used by quicksort, 29
2.1 Classiŀcation of recurrences, 44
2.2 Elementary discrete sums, 48
2.3 Ruler and population count
functions, 76
2.4 Binary divide-and-conquer
recurrences and solutions, 78
3.1 Elementary OGFs, 93
3.2 Operations on OGFs, 94
3.3 Elementary EGFs, 98
3.4 Operations on EGFs, 100
3.5 Calculating moments from a
BGF, 137
3.6 Classic “special” generating
functions, 141
4.1 Asymptotic estimates for
quicksort comparison counts,
161
4.2 Asymptotic expansions derived
from Taylor series, 162
4.3 Accuracy of Stirling’s formula
for N!, 164
4.4 Asymptotic estimates of the
harmonic numbers, 165
4.5 Absolute error in Stirling’s
formula for lnN!, 166
4.6 Asymptotic expansions for
special numbers, 168
4.7 Ramanujan Q-distribution, 188
4.8 Ramanujan R-distribution, 191
4.9 Binomial distribution, 194
4.10 Poisson distribution, 202
4.11 Ramanujan P-, Q-, and R-
functions, 206
4.12 Catalan sums, 210
5.1 Counting sequences and OGFs
for classes in Figure 5.1, 223
5.2 Familiar labelled combinatorial
classes, 231
5.3 Analytic combinatorics exam-
ples in this chapter, 254
6.1 Expected path length of trees,
310
6.2 Expected height of trees, 311
6.3 Enumeration of unlabelled
trees, 322
6.4 Enumeration of labelled trees,
328
6.5 Tree enumeration generating
functions, 331
6.6 Generating functions for other
types of trees, 338
6.7 Analytic combinatorics exam-
ples in this chapter, 341
ȜțȜ

Ȝțȝ
L ŕ ş Š
ś Œ
T ō Ŏ Ř ő ş
7.1 Basic properties of some ran-
dom permutations of nine ele-
ments, 352
7.2 Basic properties of all permuta-
tions of four elements, 353
7.3 Cumulative counts and aver-
ages for properties of permuta-
tions, 354
7.4 Enumeration of permutations
with cycle length restrictions,
366
7.5 EGFs for permutations with
cycle length restrictions, 370
7.6 Distribution of runs in per-
mutations (Eulerian numbers),
377
7.7 Analytic results for properties
of permutations (average case),
383
7.8 Ļree 2-ordered permutations,
with inversion tables, 390
7.9 Distribution of left-to-right
minima and cycles (Stirling
numbers of the ŀrst kind), 398
7.10 Distribution of maximum
inversion table entry, 408
8.1 Cost of searching for 4-bit
patterns (basic method), 418
8.2 Position of ŀrst runs of 0s on
sample bitstrings, 422
8.3 Enumerating bitstrings with no
runs of P 0s, 424
8.4 Autocorrelation of 101001010,
428
8.5 Generating functions and wait
times for 4-bit patterns, 430
8.6 REs and OGFs for gambler’s
ruin sequences, 435
8.7 Example of KMP state transi-
tion table, 439
8.8 Periodic term in trie path
length, 461
9.1 Two ways to view the same set
of objects, 478
9.2 Occupancy distribution and
properties of 3-words of length
4, 482
9.3 Occupancy parameters for balls
in three urns, 483
9.4 Occupancy parameters for balls
in eight urns, 484
9.5 Enumeration of 3-words with
letter frequency restrictions,
495
9.6 EGFs for words with letter
frequency restrictions, 499
9.7 Occupancy distribution for
M = 3, 501
9.8 Occupancy distribution exam-
ples, 502
9.9 Analytic results for hashing
methods, 517
9.10 Basic properties of some ran-
dom 9-mappings, 526
9.11 Exponential generating func-
tions for mappings, 528
9.12 Asymptotic properties of ran-
dom mappings, 530
9.13 Sample applications of Pollard’s
algorithm (c = 1), 535

L I S T O F F I G U R E S
1.1 Quicksort compare counts:
empirical and analytic, 23
1.2 Distributions for compares in
quicksort, 30
1.3 Distributions for compares in
quicksort (scaled and centered),
31
1.4 Empirical histogram for quick-
sort compare counts, 32
2.1 Solutions to binary divide-and-
conquer recurrences, 70
2.2 Periodic terms in binary
divide-and-conquer recurrences,
71
2.3 lgN (top); ⌊lgN⌋(middle);
{lgN} (bottom), 74
2.4 Composition of the periodic
function θ(1 −{lgN}), 75
2.5 Periodic and fractal terms in
bit counting, 77
2.6 Divide-and-conquer for β = 3
and α = 2, 3, 4, 83
3.1 All binary trees with 1, 2, 3, 4,
and 5 external nodes, 124
4.1 Ramanujan Q-distribution, 189
4.2 Ramanujan R-distribution, 193
4.3 Binomial distribution, 195
4.4 Binomial distribution (p =
1/5), 199
4.5 Binomial distributions tending
to Poisson distributions, 201
4.6 Laplace method, 203
4.7 Asymptotic behavior of the
terms in ∑
j≥0(1 −e−N/2j),
212
5.1 An overview of analytic com-
binatorics, 219
5.2 Basic unlabelled classes, 222
5.3 A simple combinatorial con-
struction (unlabelled), 224
5.4 Bitstrings with no 00, 227
5.5 Basic labelled classes, 230
5.6 Star product example, 232
5.7 Z ⋆P3 = P4, 235
5.8 Sets of cycles, 236
5.9 A permutation is a set of cy-
cles., 237
5.10 Derangements, 238
5.11 Enumerating generalized de-
rangements via analytic combi-
natorics, 251
6.1 Ļree binary trees, 259
6.2 Enumerating binary trees via
analytic combinatorics, 260
6.3 A three-tree forest, 261
6.4 Forests and trees, 262
ȜțȞ

Ȝțȟ
L ŕ ş Š
ś Œ
F ŕ œ š Ş ő ş
6.5 Rotation correspondence be-
tween trees and binary trees,
264
6.6 Lattice-path representations of
binary trees in, 269
6.7 Binary tree corresponding to a
triangulated N-gon, 270
6.8 Binary trees, trees, parenthe-
ses, triangulations, and ruin
sequences, 271
6.9 Path length and height in a
forest and in a binary tree, 273
6.10 A random forest and a random
binary tree, 276
6.11 Binary tree representation of
an arithmetic expression, 279
6.12 Ļree binary search trees, 281
6.13 Permutations and binary search
trees, 284
6.14 Permutations associated with
BSTs, 285
6.15 Catalan distribution (subtree
sizes in random binary trees),
288
6.16 A binary search tree built from
237 randomly ordered keys,
295
6.17 Rooted unordered trees , 315
6.18 Representations of a rooted
(unordered) forest, 317
6.19 Unrooted unordered (free)
trees, 318
6.20 A large graph and two of its
spanning trees, 320
6.21 Summary of tree nomenclature,
321
6.22 Trees with ŀve nodes (ordered,
unordered, and unrooted), 323
6.23 Labelled trees with three nodes
(ordered, unordered, and un-
rooted), 328
6.24 Cayley (labelled rooted un-
ordered) trees, 330
6.25 Examples of various other
types of trees, 332
6.26 AVL distribution (subtree sizes
in random AVL trees), 339
7.1 Two-line, cycle, and Foata rep-
resentations of a permutation,
349
7.2 Anatomy of a permutation,
351
7.3 Lattice representation of a
permutation and its inverse,
360
7.4 Binary search tree correspond-
ing to a permutation, 361
7.5 Heap-ordered tree correspond-
ing to a permutation, 362
7.6
Two combinatorial construc-
tions for permutations, 374
7.7 Distribution of runs in per-
mutations (Eulerian numbers),
377
7.8 Insertion sort and inversions,
385
7.9 Distribution of inversions, 387
7.10 Lattice paths for 2-ordered
permutations, 391
7.11 Selection sort and left-to-right
minima, 394
7.12 Distribution of left-to-right
minima and cycles (Stirling
numbers of the ŀrst kind), 398
7.13 Bubble sort (permutation and
associated inversion table), 407

L ŕ ş Š
ś Œ
F ŕ œ š Ş ő ş
ȜțȠ
7.14 Distribution of maximum
inversion table entry, 408
8.1 Distribution of longest run of
0s in a random bitstring, 427
8.2 Gambler’s ruin sequences, 436
8.3 Knuth-Morris-Pratt FSA for
10100110, 438
8.4 Ļree tries, each representing
10 bitstrings, 448
8.5 Ļree tries, representing 7, 2,
and 10 bitstrings, respectively,
450
8.6 Tries with 1, 2, 3, and 4 exter-
nal nodes, 451
8.7 Bitstring sets for tries with ŀve
external nodes (none void), 452
8.8 Aho-Corasick FSA for 000,
011, and 1010, 456
8.9 Distributed leader election, 457
8.10 Periodic łuctuation in trie path
length, 462
8.11 Distributions for path length
in tries, 463
9.1 A 10-word of length 20 (20
balls thrown into 10 urns), 477
9.2 2-words and 3-words with
small numbers of characters,
479
9.3 Examples of balls-and-urns
experiments, 481
9.4 Probability that N people do
not all have diﬀerent birthdays,
486
9.5 Occupancy distributions for
small M, 506
9.6 Occupancy distributions for
large M, 507
9.7 Percentage of empty urns as a
function of load factor N/M,
508
9.8 Hashing with linear probing,
510
9.9 Image cardinality of random
mappings, 520
9.10 Tail and cycle iterating f(x) =
x2 + 1 mod 99, 523
9.11 Tree-cycle representation of
three random mappings, 524
9.12 Basic properties of all map-
pings of three elements , 525

This page intentionally left blank 

I N D E X
Abel’s identity, 514
Absolute errors in asymptotics, 165–
166
Acyclic graphs, 319
Additive parameters for random trees,
297–301
Aho-Corasick algorithm, 456–457
Alcohol modeling, 326
Algebraic functions, 442–445
Algebraic geometry, 522
Alphabets. See Strings; Words
Ambiguity
in context-free languages, 443
in regular expressions, 432
Analysis of algorithms, 3, 536
asymptotic approximation, 27–29
average-case analysis, 16–18
distributions, 30–33
linear probing, 512–513
normal approximation, 207–211
Poisson approximation, 211–214
process, 13–15
purpose, 3–6
quicksort, 18–27
randomized, 33
summary, 34–36
theory, 6–12
Analytic combinatorics, 36, 219–220
binary trees, 228, 251, 260
bitstrings, 226
bytestrings, 478
Catalan numbers, 228, 251, 260
coeﬃcient asymptotics, 247–251
cumulative GF, 372
derangements, 239–240, 367–368
formal basis, 220–221
generalized derangements, 239–
240, 251
generating function coeﬃcient
asymptotics, 247–253
increasing subsequences, 380
inversions, 386
involutions, 369
labelled trees, 341
labelled objects, 229–240
linear probing, 516–517
parameters, 244–246
permutations, 234–236, 369
runs, 375–378
summary, 253–254
surjections, 492–493
symbolic methods for parameters,
241–246
tables, 254, 341, 383
transfer theorems, 225, 233, 242,
249–250
trees, 263
unlabelled objects, 221–229
unlabelled trees, 341
words, 478
2-ordered permutations, 443–445
Ancestor nodes in binary trees, 259
Approximations
asymptotic. See Asymptotic ap-
proximations
models for cost estimates, 15
Arbitrary patterns in strings, 428–431
Arithmetic expressions, 278–280
Arrangements
ȜȜȘ

ȜȜș
I Ś Ő ő Ť
maximal occupancy, 496
minimal occupancy, 498
permutations, 355–357
Arrays
associative, 281
sorting. See Sorts
Assembly language instructions, 20–
21
Associative arrays, 281
Asymptotic analysis
coeﬃcient asymptotics, 113, 247–
253, 324, 334
Darboux-Polya method, 326
Euler-Maclaurin summation. See
Euler-Maclaurin summation
Laplace method, 153, 203–207,
369, 380
linear recurrences. See Linear re-
currences
Stirling’s formula. See Stirling’s
formula
Asymptotic approximations, 27–29
bivariate, 187–202
Euler-Maclaurin summation, 179–
186
expansions. See Asymptotic expan-
sions
exponentially small terms, 156–157
ŀnite sums, 176–178
normal examples, 207–211
notation, 153–159
overview, 151–153
Poisson approximation, 211–214
summary, 214–215
Asymptotic expansions, 28
absolute errors, 165–166
deŀnitions, 160–162
nonconvergence, 164
relative errors, 166–167
special numbers, 167–168
Stirling’s formula, 164–165
Taylor, 162–163
Asymptotic notations
o, 153–157
O, 6–7, 153–157, 169–175
Ω, 6–7
Θ, 6–7
∼, 153–157
Asymptotic notations, 6–7, 169–175
Asymptotic scales, 160
Asymptotic series, 160
Atoms in combinatorial classes, 221–
223
Autocorrelation of strings, 428–430
Automata
ŀnite-state, 416, 437–440, 456–457
and regular expressions, 433
and string searching, 437–440
trie-based ŀnite-state, 456–457
Average. See Expected value
Average-case analysis, 16–18
AVL (Adel’son-Vel’skii and Landis)
trees, 336–339
B-trees, 336–337
Bach’s theorem on quadratic maps,
522
Balanced trees, 284, 336
Ballot problem, 268, 314, 445–447
Balls-and-urns model
occupancy distributions, 474, 501–
510
Poisson approximation, 198–199
and word properties, 476–485
Batcher’s odd-even merge, 208–209
Bell curve, 153, 168, 194–195
Bell numbers, 493
Bernoulli distribution. See Binomial
distributions
Bernoulli numbers (BN)

I Ś Ő ő Ť
ȜȜȚ
asymptotics, 168
deŀnition, 140, 142–143
in summations, 179–182
values, 181
Bernoulli polynomials (Bm(x))
deŀnition, 143–144
in summations, 179–180
Bernoulli trials
binomial coeﬃcients in, 142
bitstrings, 421
words, 473
BGF. See Bivariate generating func-
tions (BGF)
Bijections
cycles and left-to-right minima in
permutations, 359
inversion tables and permutations,
359
permutations and HOTs, 362
permutations and sets of cycles,
237, 402–403
tries and sets of strings, 448–452
Binary nodes in heap-ordered trees,
381
Binary number properties in divide-
and-conquer recurrences, 75–77
Binary search, 72–75
Binary search trees, 257
additive parameters, 298–300
combinatorial constructions, 373–
375
construction costs, 293–296
deŀnition, 282
frequency, 363–364
heap-ordered, 361–365
insertion program, 283–286
leaves, 300–301
overview, 281
path length, 288–290, 293–297
permutations, 361
and quicksort, 294–295
search costs, 295–296
search program, 282
Binary trees
combinatorial equivalences, 264–
272
counting, 123–124
deŀnition, 123, 258–259, 321
enumerating, 260, 263
forests, 261–263
general, 261–262
generating functions, 125, 302–
303, 441–442
height, 302–309
Lagrange inversion, 313
leaves, 244–246, 300–301
overview, 257–258
parenthesis systems, 265–267
path lengths, 272–276, 287–291
rotation correspondence, 264–265
table, 124
traversal representation, 267
and tries, 448–452
unlabelled objects, 222–223, 228
Binomial asymptotic expansion, 162
Binomial coeﬃcients
asymptotics, 168, 194, 197
deŀnition, 112, 140
special functions, 142
Binomial convolution, 99–100
Binomial distributions, 127–128
asymptotics, 168, 193–195
bivariate generating functions, 133–
135
and hashing, 480
normal approximation, 195–198
occupancy problems, 501–505
Poisson approximation, 198–202,
474

ȜȜț
I Ś Ő ő Ť
probability generating functions,
131–132
strings, 415, 421
tails, 196–197
words, 473–474
Binomial theorem, 48, 111–112, 125
Binomial transforms, 115–116
Biology, computational, 16
Birthday problem, 187, 485–490,
509, 527
Bitstrings
combinatorial properties, 420–426
deŀnition, 415
symbolic methods for parameters,
243
unlabelled objects, 222–223, 226–
227
Bivariate asymptotics, 187
binomial distributions, 193–195
Ramanujan distributions, 187–193
Bivariate generating functions (BGF)
binomial distributions, 133–135
bitstrings, 243
Catalan trees, 287–292, 294
deŀnition, 132–133
expansions, 134–138
exponential, 242
leaves, 245
ordinary, 241–242
permutations, 243–244, 373
quicksort distribution, 138–139
Bootstrapping method, 61, 67, 69,
175
Bounding tails in asymptotic approxi-
mations, 176–177
BST. See Binary search tree
Caching, 494
Carry propagation, 79, 426
Cartesian products, 224, 228
Catalan distributions, 287–288
Catalan models and trees
AVL trees, 338
binary. See Binary Catalan trees
general, 292–293, 323
random trees, 280, 287–291, 297–
301
Catalan numbers (TN = GN+1)
asymptotics, 165–167, 172–173,
185
and binary trees, 125, 260–261,
263
deŀnition, 140
expansions, 168
forests, 261, 263
generating functions for, 117, 141,
251
history, 269–270
planar subdivisions, 269
Catalan sums, 207–211
Cayley function (C(z) = zeC(z)),
527–528
Cayley trees
enumerating, 329–331
exponential generating functions,
527–528
labelled classes, 341
Central limit theorem, 386
CFG (context-free grammars), 441–
447
CGF. See Cumulative generating
functions (CGF)
Change of variables method for re-
currences, 61–64
Changing a dollar, 126–127
Characteristic polynomial of recur-
rences, 55, 106
Characters (letters). See Strings;
Words
Chebyshev inequality, 32, 508

I Ś Ő ő Ť
ȜȜȜ
Child nodes in binary trees, 259
Chomsky and Schützenberger’s
theorem, 432, 442
Clustering in hashing, 510–513
Coalesced hashing, 509
Coeﬃcient asymptotics
generating functions, 113, 247–253
and trees, 324, 334
Coeﬃcient notation ([zn]f(z)]), 97
Coeﬃcients in ordinary generating
functions, 92
Coin łipping, 421, 457
Collisions
hashing, 474, 486–488, 494, 509,
512
occupancy, 495
Combinatorial constructions, 219,
224
binary trees, 228, 251, 260
bitstrings, 226, 420–426
bytestrings, 478
context-free grammars, 441–443
cumulative generating function,
372
derangements, 239–240, 367–368
formal basis, 220–221
generalized derangements, 239–
240, 251
increasing subsequences, 380
inversions, 386
involutions, 369
labelled cycles, 527
labelled objects, 229–240
labelled trees, 341
linear probing, 516–517
multiset, 325
for parameters, 241–246
permutations, 234–236, 369
regular expressions, 433
rooted unordered trees, 318–320,
322–323
runs, 375–378
sets of cycles, 235–236
summary, 253–254
surjections, 492–493
symbolic methods for parameters,
241–246
tables, 254, 341, 383
transfer theorems, 225, 233, 242,
249–250
trees, 263
unlabelled objects, 221–229
unlabelled trees, 341
words, 478
2-ordered permutations, 443–445
Combinatorial construction opera-
tions
Cartesian product, 223
cycle, 232–233
disjoint union, 223
largest, 373, 395, 485
last, 373, 395, 485
min-rooting, 363
sequence, 223, 232–233
set, 232
star product, 231
Combinatorial classes, 221. See La-
belled classes, Unlabelled classes
Combinatorics, analytic. See Analytic
combinatorics
Comparison-based sorting, 345
Complex analysis, 215
generating functions, 113, 145
rooted unordered trees, 326
t-restricted trees, 335
Complex roots in linear recurrences,
107
Complexity of sorting, 11–12

ȜȜȝ
I Ś Ő ő Ť
Composition in asymptotic series,
174
Compositional functional equations,
118
Computational complexity, 5–13, 85
Computer algebra, 443–445
Connected components, mapping,
522–532
Connected graphs, 319
Construction costs in binary search
trees, 293–296
Constructions. See Combinatorial
constructions
Context-free grammars, 441–447
Continuant polynomials, 60
Continued fractions, 52, 60, 63–64
Convergence, 52
asymptotic expansion, 164
ordinary generating functions, 92
quadratic, 52–53
radius, 248–250
simple, 52
slow, 53–54
Convolutions
binary trees, 125
binomial, 99–100
ordinary generating functions, 95–
96
Vandermonde, 114
Costs
algorithms, 14–15
binary search trees, 293–296
bivariate generating functions, 133,
135–136
cumulated, 17, 135–137
Counting sequence, 221, 223
Counting with generating functions,
123–128
Coupon collector problem, 488–495
Cryptanalysis, sorting in, 16
Cumulated costs, 17, 135–137
Cumulative analysis
average-case analysis, 17–18
string searching, 419–420
trees, 287
words, 503–505
Cumulative generating functions
(CGF), 135, 137
combinatorial constructions, 373–
375
increasing subsequences, 379–384
left-to-right minima, 395–396
peaks and valleys, 380–384
permutation properties, 372–384
random Catalan trees, 291
runs and rises, 375–379
Cumulative population count func-
tion, 76
Cumulative ruler function, 76
Cycle detection in mapping, 532–534
Cycle distribution, 402–403
Cycle leaders, 349
Cycles in mapping, 522–534
Cycles in permutations
deŀnition, 229–232, 348
in situ, 401–405
length, 366–368
longest and shortest, 409–410
singleton, 369, 403–405
structure, 358
symbolic methods for parameters,
243–244
Darboux-Polya method, 326
Data compression with LZW, 466–
467
Decompositions of permutations, 375
Derangements
asymptotics, 176
generating functions, 250–251, 370

I Ś Ő ő Ť
ȜȜȞ
minimal occupancy, 498
in permutations, 238–240, 348,
367
Descendant nodes in binary trees,
259
Dictionaries, tries for, 455
Dictionary problem, 281
Diﬀerence equations, 42, 50, 86
Diﬀerential equations
binary search trees, 299
Eulerian numbers, 376
generating functions, 117
heap-ordered trees, 363
increasing subsequences, 378–379
involutions, 370
maxima, 396
median-of-three quicksort, 120–
123
quicksort, 109
Digital searching. See Tries
Dirichlet generating functions
(DGF), 144–146
Discrete sums from recurrence, 48–
49, 129
Disjoint union operations, 224
Distributed algorithms, 464
Distributed leader election, 457–458
Distributional analysis, 17
Distributions, 30–33
binomial. See Binomial distribu-
tions
Catalan, 287–288
normal, 153, 168, 194–195
occupancy, 198, 501–510
Poisson, 153, 168, 202, 405
quicksort, 30–32
Ramanujan, 187–193, 407
uniform discrete, 130–131
Divergent asymptotics series, 164–
165
Divide-and-conquer recurrences, 70–
72
algorithm, 8
binary number properties, 75–77
binary searches, 72–75
functions, 81–84
general, 80–85
non-binary methods, 78–80
periodicity, 71–72, 82
sequences, 84–85
theorems, 81–85
Division in asymptotic series, 172
Divisor function, 145–146
Dollar, changing a, 126–127
Double falls in permutations, 350–
351
Double hashing, 511–512
Double rises in permutations, 350–
351
Dynamic processes, 485–486
Elementary bounds in binary trees,
273–275
Elimination. See Gröbner basis
Empirical complexity in quicksort, 23
Empty combinatorial class, 221
Empty urns
in hashing, 510
image cardinality, 519
occupancy distribution, 503–505
Encoding tries, 454–455
End recursion removal, 309
Enumerating
binary trees, 260, 263
forests, 263
generating functions, 331
labelled trees, 327–331
pattern occurrences, 419–420
permutations, 366–371
rooted trees, 322, 325–327

ȜȜȟ
I Ś Ő ő Ť
surjections, 492–493
t-ary trees, 333–334
Equal length cycles in permutations,
366–367
Error terms in asymptotic expansions,
160
Euler and Segner on Catalan num-
bers, 269–270
Euler equation, 121
Euler-Maclaurin constants, 183–185
Euler-Maclaurin summation, 27, 153,
176
Bernoulli polynomials, 144
and Catalan sums, 209
discrete form, 183–186
general form, 179–182
and Laplace method, 204–205
overview, 179
and tree height, 307
Eulerian numbers, 376–379, 384
Euler’s constant, 28
Exp-log transformation, 173, 188
Expansions
asymptotic. See Asymptotic expan-
sions
generating functions, 111–114,
134–138
Expectation of discrete variables,
129–132
Exponential asymptotic expansion,
162
Exponential generating functions
(EGF)
bivariate, 242
cumulative, 372
deŀnition, 97
mapping, 527–528
operations, 99–101
permutation involutions, 369–371
symbolic methods for labelled
classes, 233–234
table, 98–99
Exponential sequence, 111
Exponentially small terms
asymptotic approximations, 156–
157
Ramanujan Q-function, 190, 204–
205
Expressions
evaluation, 278–280
register allocation, 62, 280, 309
regular, 432–436, 440
External nodes
binary trees, 123, 258–259
tries, 448–456, 459
External path length for binary trees,
272–273
Extremal parameters for permuta-
tions, 406–410
Faà di Bruno’s formula, 116
Factorials
asymptotics, 168
deŀnition, 140
Factorization of integers, 532–536
Falls in permutations, 350–351
Fibonacci numbers (FN)
asymptotics, 168
deŀnition, 44
generating functions, 103–104,
114–115, 140
golden ratio, 58
recurrences, 57–59, 64
and strings, 424
Fibonacci polynomials, 305
Find operations, union-ŀnd, 316
Finite asymptotic expansions, 161
Finite function. See Mapping
Finite-state automata (FSA)

I Ś Ő ő Ť
ȜȜȠ
description, 416
and string searching, 437–440
trie-based, 456–457
Finite sums in asymptotic approxima-
tions, 176–178
First constructions
cumulative generating functions,
373
occupancy problems, 485
First-order recurrences, 48–51
Floyd’s cycle detection algorithm in
mapping, 532–533
Foata’s correspondence in permuta-
tions, 349, 358–359, 402
Footnote, 518
Forests
deŀnition, 261–262
enumerating, 263
labelled trees, 330
Lagrange inversion, 314–315
parenthesis systems, 265
unordered, 315
Formal languages, 224
deŀnitions, 441
and generating functions, 467
regular expressions, 432–433
Formal objects, 146
Formal power series, 92
Fractals, 71, 77, 86
Fractional part (x)
binary searches, 73
divide-and-conquer methods, 82
Euler-Maclaurin summation, 179
tries, 460
Free trees, 318–321, 323, 327–328
Frequency of instruction execution, 7,
20
Frequency of letters
table, 497–499
in words, 473
Fringe analysis, 51
FSA. See Finite-state automata (FSA)
Full tables in hashing, 510–511
Functional equations
binary Catalan trees, 287–291
binary search trees, 294, 303
binary trees, 125
context-free grammars, 442–444
expectations for trees, 310–311
generating functions, 117–119
in situ permutation, 405
labelled trees, 329–331
radix-exchange sort, 213
rooted unordered trees, 324
tries, 213
Functional inverse of Lagrange
inversion, 312–313
Fundamental correspondence. See
Foata’s correspondence
Gambler’s ruin
lattice paths, 268
regular expressions, 435–436
sequence of operations, 446
Gamma function, 186
General trees. See Trees
Generalized derangements, 239–240,
250–251
Generalized Fibonacci numbers and
strings, 424
Generalized harmonic numbers
(H(2)
N ), 96, 186
Generating functions (GF), 43, 91
bivariate. See Bivariate generating
functions (BGF)
for Catalan trees, 302–303
coeﬃcient asymptotics, 247–253
counting with, 123–128
cumulative. See Cumulative gener-
ating functions (CGF)

Ȝȝȗ
I Ś Ő ő Ť
Dirichlet, 144–146
expansion, 111–114
exponential. See Exponential gen-
erating functions (EGF)
functional equations, 117–119
mapping, 527–531
ordinary. See Ordinary generating
functions (OGF)
probability. See Probability generat-
ing functions (PGF)
recurrences, 101–110, 146
regular expression, 433–435
special functions, 141–146
summary, 146–147
transformations, 114–116
Geometric asymptotic expansion, 162
Geometric sequence, 111
GF. See Generating functions (GF)
Golden ratio (ϕ = (1 +
√
5)/2), 58
Grammars, context-free, 441–447
Graphs, 532
deŀnitions, 318–320
permutations, 358
2-regular, 252
Gröbner basis algorithms, 442–445
Harmonic numbers, 21
approximating, 27–28
asymptotics, 168, 183–186
deŀnition, 140
generalized, 96, 186
ordinary generating functions, 95–
96
in permutations, 396
Hash functions, 474
Hashing algorithms, 473
birthday problem, 485–488
coalesced, 509
collisions, 474, 486–488, 494, 509,
512
coupon collector problem, 488–495
empty urns, 503–505, 510
linear probing, 509–518
longest list, 500
open addressing, 509–518
separate chaining, 474–476, 505–
509
uniform hashing, 511–512
Heap-ordered trees (HOT)
construction, 375
node types, 380–384
permutations, 362–365
Height
expectations for trees, 310–312
in binary trees, 302–303
in binary search trees, 308–309
in general trees, 304–307
in random walk, 435–436
stack height, 308–309
Height-restricted trees, 336–340
Hierarchy of trees, 321–325
High-order linear recurrences, 104
Higher-order recurrences, 55–60
Homogeneous recurrences, 47
Horizontal expansion of BGFs, 134–
136
Horse kicks in Prussian Army, 199
HOT. See Heap-ordered trees (HOT)
Huﬀman encoding, 455
Hydrocarbon modeling, 326
Image cardinality, 519–522
Implementation, analysis for, 6
In situ permutations, 401–405
Increasing subsequences of permuta-
tions, 351–352, 379–384
Inŀx expressions, 267
Information retrieval, 473
Inorder traversal of trees, 277
Input

I Ś Ő ő Ť
ȜȝȘ
models, 16, 33
random, 16–17
Insertion into binary search trees,
283–286
Insertion sort, 384–388
In situ permutation (rearrangement),
401–402
Integer factorization, 532–536
Integer partitions, 248
Integrals in asymptotic approxima-
tions, 177–178
Integration factor in diﬀerential equa-
tions, 109
Internal nodes
binary trees, 123, 259, 301
tries, 449–450, 459–462
Internal path length for binary trees,
272–274
Inversions
bubble sorts, 406
distributions, 386–388
Lagrange. See Lagrange inversion
permutations, 347, 350, 384–388,
391
tables, 347, 359, 394, 407–408
Involutions
minimal occupancy, 498
in permutations, 350, 369–371
Isomorphism of trees, 324
Iterations
functional equations, 118
in recurrences, 48, 63–64, 81
K-forests of binary trees, 314
Keys
binary search trees, 293
hashes, 474–476
search, 281
sort, 24, 355
Kleene’s theorem, 433
Knuth, Donald
analysis of algorithms, 5, 512–513
hashing algorithms, 473
Knuth-Morris-Pratt algorithm
(KMP), 420, 437–440, 456
Kraft equality, 275
Kruskal’s algorithm, 320
Labelled cycle construction, 526
Labelled combinatorial classes, 229–
240
Cayley trees, 329–331
derangements, 239–240, 367–368
generalized derangements, 239–
240, 251
increasing subsequences, 380
cycles, 230–231, 527
trees, 327–331, 341
permutations, 234–236, 369
sets of cycles, 235–236
surjections, 492–493
unordered labelled trees, 329–331
urns, 229–231
words, 478
Labelled objects, 97, 229–240
Lagrange inversion theorem, 113,
312–313
binary trees, 313–315
labelled trees, 330–331
mappings, 528
t-ary trees, 333
ternary trees, 313–314
Lambert series, 145
Languages, 224
context-free grammars, 441–447
deŀnitions, 441
and generating functions, 467
regular expressions, 432–436
strings. See Strings
words. See Words

Ȝȝș
I Ś Ő ő Ť
Laplace method
increasing subsequences, 380
involutions, 369
for sums, 153, 203–207
Laplace transform, 101
Largest constructions
permutations, 373
occupancy problems, 485
Last constructions
permutations, 373, 395
occupancy problems, 485
Lattice paths
ballot problem, 445
gambler’s ruin, 268–269
permutations, 390–392
Lattice representation for permuta-
tions, 360
Leader election, 464
Leaves
binary search trees, 300–301
binary trees, 244–246, 259, 261,
273
heap-ordered trees, 382
Left-to-right maxima and minima in
permutations, 348–349, 393–398
Lempel-Ziv-Welch (LZW) data
compression, 466–467
Letters (characters). See Strings;
Words
Level (of a node in a tree), 273
Level order traversal, 272, 278
L’Hôpital’s rule, 158
Limiting distributions, 30–31
Linear functional equations, 117
Linear probing in hashing, 509–518
Linear recurrences
asymptotics, 157–159
constant coeﬃcients, 55–56
generating functions, 102, 104–108
scaling, 46–47
Linear recurrences in applications
fringe analysis, 51
tree height, 305
Linked lists in hashing, 474–475, 500
Lists in hashing, 474–475, 500
Logarithmic asymptotic expansion,
162
Longest cycles in permutations, 409–
410
Longest lists in hashing, 500
Longest runs in strings, 426–427
Lower bounds
in theory of algorithms, 4, 12
divide-and-conquer recurrences, 80,
85
notation, 7
for sorting, 11
tree height, 302
M-ary strings, 415
Machine-independent algorithms, 15
Mappings, 474
connected components, 522–532
cycles in, 522–534
deŀnition, 519
generating functions, 527–531
image cardinality, 519–522
path length, 522–527
random, 519–522, 535–537
and random number generators,
520–522
summary, 536–538
and trees, 523–531
Maxima in permutations, 348–349,
393–398
Maximal cycle lengths in permuta-
tions, 368
Maximal occupancy in words, 496–
500

I Ś Ő ő Ť
ȜȝȚ
Maximum inversion table entry, 407–
408
Means
and probability generating func-
tions, 129–132
unnormalized, 135
Median-of-three quicksort, 25–26
ordinary generating functions for,
120–123
recurrences, 66
Mellin transform, 462
Mergesort algorithm, 7–11
program, 9–10
recurrences, 9–10, 43, 70–71, 73–
74
theorem, 74–75
Middle square generator method, 521
Minima in permutations, 348–349,
393–398
Minimal cycle lengths in permuta-
tions, 367–368
Minimal occupancy of words, 498–
499
Minimal spanning trees, 320
Models
balls-and-urns. See Balls-and-urns
model
Catalan. See Catalan models and
trees
costs, 15
inputs, 16, 33
random map, 531–532, 535–537
random permutation, 345–346, 511
random string, 415, 419–422
random trie, 457–458
Moments of distributions, 17
and probability generating func-
tions, 130
vertical computation, 136–138
Motzkin numbers, 334
Multiple roots in linear recurrences,
107–108
Multiple search patterns, 455–456
Multiplication in asymptotic series,
171–172
Multiset construction, 325
Multiset operations, 228
Multiway tries, 465
Natural numbers, 222–223
Neutral class (E), 221
Neutral object (ϵ), 221
Newton series, 145
Newton’s algorithm, 52–53
Newton’s theorem, 111–112, 125
Nodes
binary trees, 123, 258–259
heap-ordered trees, 380–384
rooted unordered trees, 322–323,
327–328
tries, 448–456, 459–462
Nonconvergence in asymptotic series,
164
Nonlinear ŀrst-order recurrences, 52–
54
Nonlinear functional equations, 117
Nonplane trees, 321
Nonterminal symbols, 441–447
Nonvoid trie nodes, 449–456
Normal approximation
and analysis of algorithms, 207–
211
binomial distribution, 195–198,
474
and hashing, 502–505
Normal distribution, 153, 168, 194–
195
Notation of asymptotic approxima-
tions, 153–159

Ȝȝț
I Ś Ő ő Ť
Number representations, 71–72, 86
o-notation (o), 153–159
O-notation (O), 6–7, 153–159, 169–
175
Occupancy distributions, 198, 501–
510
Occupancy problems, 474, 478–484,
495–500. See also Hashing algo-
rithms; Words
Occurrences of string patterns, 416–
420
Odd-even merge, 208–209
Omega-notation (Ω), 6–7
Open addressing hashing, 509–518
Ordered trees
enumerating, 328–329
heap-ordered. See Heap-ordered
trees (HOT)
hierarchy, 321
labelled, 315, 327–328
nodes, 322–323
Ordinary bivariate generating func-
tions (OBGF), 241–242
Ordinary generating functions
(OGF), 92
birthday problem, 489–490
context-free grammars, 442–443
linear recurrences, 104–105
median-of-three quicksort, 120–
123
operations, 95–97
quicksort recurrences, 109–110
table, 93–94
unlabelled objects, 222–223, 225
Oriented trees, 321–322
Oscillation, 70–75, 82, 213, 340, 426,
462–464
Pachinko machine, 510
Page references (caching), 494
Paradox, birthday, 485–487, 509
Parameters
additive, 297–301
permutations, 406–410
symbolic methods for, 241–246
Parent links in rooted unordered
trees, 317
Parent nodes in binary trees, 259
Parenthesis systems for trees, 265–
267
Parse trees of expressions, 278
Partial fractions, 103, 113
Partial mappings, 531
Partial sums, 95
Partitioning, 19–20, 23–24, 120–123,
139, 295, 454
Path length
binary search trees, 293–297
binary trees, 257–258, 272–276
mapping, 522–527
Catalan trees, 287–293
table, 310
tries, 459–462
Paths
graphs, 319
lattice, 268–269
permutations, 390–392
Patricia tries, 454
Pattern-matching. See String searches
Patterns
arbitrary, 428–431
autocorrelation, 428–430
multiple, 455–456
occurrences, 416–420
Peaks in permutations, 350–351, 362,
380–384
Periodicities
binary numbers, 70–75
complex roots, 107

I Ś Ő ő Ť
ȜȝȜ
divide-and-conquer, 71–72, 82
mergesort, 70–75
tries, 460–464
Permutations
algorithms, 355–358
basic properties, 352–354
binary search trees, 284, 361
cumulative generating functions,
372–384
cycles. See Cycles in permutations
decompositions, 375
enumerating, 366–371
extremal parameters, 406–410
Foata’s correspondence, 349
heap-ordered trees, 361–365
in situ, 401–405
increasing subsequences, 351–352,
379–384
inversion tables, 347, 359, 394
inversions in, 347, 350, 384–388,
407–408
labelled objects, 229–231, 234–235
lattice representation, 360
left-to-right minima, 348–349,
393–398
local properties, 382–384
overview, 345–346
peaks and valleys, 350–351, 362,
380–384
random, 23–24, 357–359
rearrangements, 347, 355–358, 401
representation, 358–365
rises and falls, 350–351
runs, 350
selection sorts, 397–400
shellsort, 389–393
summary, 410–411
symbolic methods for parameters,
243–244
table of properties, 383
two-line representation, 237
2-ordered, 208, 389–393, 443–444
Perturbation method for recurrences,
61, 68–69
PGF. See Probability generating func-
tions (PGF)
Planar subdivisions, 269–270
Plane trees, 321
Poincaré series, 161
Poisson approximation
analysis of algorithms, 153
binomial distribution, 198–202,
474
and hashing, 502–505
Poisson distribution, 405
analysis of algorithms, 211–214
asymptotics, 168
binomial distribution, 201–202,
474
image cardinality, 519
Poisson law, 199
Poles of recurrences, 157–158
Pollard rho method, 522, 532–536
Polya, Darboux-Polya method, 326
Polygon triangulation, 269–271
Polynomials
Bernoulli, 143–144, 179–180
in context-free grammars, 442–444
Fibonacci, 305
Population count function, 76
Postŀx tree traversal, 266–267, 277–
279
Power series, 92
Preŀx codes, 454
Preŀx-free property, 449
Preŀx tree traversal, 266–267, 277–
278
Preŀxes for strings, 419
Preservation of randomness, 27
Priority queues, 358, 362

Ȝȝȝ
I Ś Ő ő Ť
Probabilistic algorithm, 33
Probability distributions. See Distri-
butions
Probability generating functions
(PGF)
binary search trees, 296–297
binomial, 131–132
birthday problem, 489–490
bivariate, 132–140
mean and variance, 129–130
and permutations, 386, 395
uniform discrete distribution, 130–
131
Probes
in hashing, 476
linear probing, 509–518
Prodinger’s algorithm, 464
Product
Cartesian (unlabelled), 224, 228
Star (labelled), 231–235
Proŀles for binary trees, 273
Program vs. algorithm, 13–14
Prussian Army, horse kicks in, 199
Pushdown stacks, 277, 308, 446
Q-function. See Ramanujan Q-
function
Quad trees, 333
Quadratic convergence, 52–53
Quadratic mapping, 535
Quadratic random number genera-
tors, 521–522
Quadratic recurrences, 62
Queues, priority, 358, 362
Quicksort
algorithm analysis, 18–27
asymptotics table, 161
and binary search trees, 294–295
bivariate generating functions, 138–
139
compares in, 29
distributions, 30–32
empirical complexity, 23
median-of-three, 25–26, 66, 120–
123
ordinary generating functions for,
109–110
partitioning, 19–20, 23–24
probability generating function,
131–132
radix-exchange, 26–27, 211–213,
454, 459–460, 463
recurrences, 21–22, 43, 66, 109–
110
subarrays, 25
variance, 138–139
Radius of convergence bounds, 248–
250
Radix-exchange sorts, 26–27
analysis, 211–213
and tries, 454, 459–460, 463
Ramanujan distributions (P, Q, R)
bivariate asymptotics, 187–193
maximum inversion tables, 407
Ramanujan-Knuth Q-function, 153
Ramanujan Q-function
and birthday problem, 487
LaPlace method for, 204–207
and mapping, 527, 529
Ramanujan R-distribution, 191–193
Random bitstrings, 26
Random input, 16–17
Random mappings, 519–522, 531–
532, 535–537
Random number generators, 520–
522, 533–535
Random permutations, 23–24, 345–
346, 357–359, 511
Random strings

I Ś Ő ő Ť
ȜȝȞ
alphabets, 431, 465
binomial distributions, 131
bitstrings, 420
leader election, 464
regular expressions, 432
Random trees
additive parameters, 297–301
binary search tree, 293–295
analysis of algorithms, 275–276
Catalan models, 280, 287–291
path length, 311–312
Random trie models, 457–458
Random variables, 129–132
Random walks, 435–436
Random words, 474, 478
Randomization in leader election, 464
Randomized algorithms, 33
Randomness preservation, 27
Rational functions, 104, 157
generating function coeﬃcients,
247–248
and regular expression, 433
and runs in strings, 423
Rearrangement of permutations, 347,
355–358, 401
Records
in permutations, 348, 355–356
priority queues, 358
sorting, 24, 387, 397, 407
Recurrences, 18
asymptotics, 157–159
basic properties, 43–47
bootstrapping, 67
calculations, 45–46
change of variables method, 61–64
classiŀcation, 44–45
divide-and-conquer. See Divide-
and-conquer recurrences
Fibonacci numbers, 57–59, 64
ŀrst-order, 48–51
fringe analysis, 51
generating functions, 101–110, 146
higher-order, 55–60
iteration, 81
linear. See Linear recurrences
linear constant coeﬃcient, 55–56
median-of-three quicksort, 26
mergesort, 9–10, 43, 70–71, 73–74
nonlinear ŀrst-order, 52–54
overview, 41–43
perturbation, 61, 68–69
quadratic, 62
quicksort, 21–22, 43, 66
radix-exchange sort, 26–27
repertoire, 61, 65–66
scaling, 46–47
summary, 86–87
tree height, 303–305
Recursion, 18, 257, 295
binary trees, 123–126, 220, 228,
257–260, 273–275
binary search trees, 282–283, 361
context-free grammars, 443
divide-and-conquer, 80
distributed leader election, 457
expression evaluation, 278–279
forests, 261
heap-ordered trees, 362–364
mergesort, 7–9, 75–80
parenthesis systems, 265
quad trees, 333
quicksort, 19–21
radix-exchange sort, 454
and recurrences, 41, 45–46
rooted trees, 323
t-ary trees, 333
triangulated N-gons, 269–270
tree algorithms, 277–278
tree properties, 273–274, 290, 291,
297–312

Ȝȝȟ
I Ś Ő ő Ť
trees, 261, 340
tries, 449–451
Register allocation, 62, 280, 309
Regular expressions, 432–436
and automata, 433, 440
gambler’s ruin, 435–436
and generating function, 433–435
Relabelling objects, 231
Relative errors in asymptotics, 166–
167
Repertoire method in recurrences, 61,
65–66
Representation of permutations, 358–
365
Reversion in asymptotic series, 175
Rewriting rules, 442
Rho length, mapping, 522–527
Rho method, Pollard, 522, 532–536
Riemann sum, 179, 182
Riemann zeta function, 145
Right-left string searching, 466
Rises in permutations, 350–351, 375–
379
Root nodes in binary trees, 259, 261
Rooted unordered trees, 315
deŀnition, 315–316
enumerating, 325–327
graphs, 318–320
hierarchy, 321–325
Kruskal’s algorithm, 320
nodes, 322–323, 327–328
overview, 315
representing, 324
sample application, 316–318
Rotation correspondence between
trees, 264–265, 309
Ruler function, 76
Running time, 7
Runs
in permutations, 350, 375–379
in strings, 420–426, 434
Saddle point method, 499
Scales, asymptotic, 160
Scaling recurrences, 46
Search costs in binary search trees,
293, 295–296
Search problem, 281
Searching algorithms. See Binary
search; Binary search trees; Hash-
ing algorithms; String searches;
Tries
Seeds for random number generators,
521
Selection sort, 397–400
Sentinels, 416–417
Separate chaining hashing algorithms,
474–476, 505–509
Sequence construction, 224, 228
Sequences, 95–97
ternary trees, 314
rooted unordered trees, 325
free trees, 327
ordered labelled trees, 329
unordered labelled trees, 330
runs and rises in permutations, 375
Stirling cycle numbers, 397
maximum inversion table entry,
406–407
3-words tieh restrictions, 495
Series, asymptotic, 160
Set construction, 228
Sets of cycles, 235–237, 527
Sets of strings, 416, 448–452
Shellsort, 389–393
Shifting recurrences, 46
Shortest cycles in permutations, 409–
410
Sim-notation (∼), 153–159
Simple convergence, 52

I Ś Ő ő Ť
ȜȝȠ
Simple paths in graphs, 319
Singleton cycles in permutations, 369,
403–405
Singularities of generating functions,
113
Singularity analysis, 252, 335
Size in combinatorial classes, 221,
223
Slow convergence, 53–54
Smallest construction, 373
Sorting
algorithms, 6–12
bubble, 406–407
comparison-based, 345
complexity, 11–12
insertion, 384–388
mergesort. See Mergesort algo-
rithm
permutations, 355–356, 397–400
quicksort. See Quicksort
radix-exchange, 26–27, 211–213,
454, 459–460, 463
selection, 397–400
Spanning trees of graphs, 319
Special number sequences, 139
asymptotics, 167–168
Bernoulli numbers, 142–143
Bernoulli polynomials, 143–144
binomial coeﬃcients, 142
Dirichlet generating functions,
144–146
harmonic numbers, 21
overview, 141–142
Stirling numbers, 142
tables, 140
Stacks, 277
ballot problem, 446–447
height, 308–309
Standard deviation, 17
bivariate generating functions, 138
distributions, 31–32
probability generating functions,
129–130
Star operations
labelled classes, 231–232
on languages, 432
Stirling numbers, overview, 142
Stirling numbers of the ŀrst kind
([ n
k ]), 140
asymptotics, 168
counting cycles, 402–403
counting minima/maxima, 396–398
Stirling numbers of the second kind
({ n
k }), 140
asymptotics, 168
and coupon collector, 491
subset numbers, 491
surjections, 492–493
Stirling’s constant (σ =
√
2π), 183–
184, 210
Stirling’s formula
asymptotic expansion, 164–165
asymptotics, 173, 185
and Laplace method, 207
table, 166
and trees, 168
String searches
KMP algorithm, 437–440
right-left, 466
and tries, 416–420, 448, 455–458
Strings
arbitrary patterns, 428–431
autocorrelation, 428–430
larger alphabets, 465–467
overview, 415–416
runs, 420–426, 434
sets of. See Languages; Tries
summary, 467–468
words. See Words
Subset numbers, 491

ȜȞȗ
I Ś Ő ő Ť
Subtrees, 123, 258–259, 261
Successful searches, 295, 476, 508–
509, 511, 515–518
Suﬃx tries, 455, 459
Summation factors, 50, 59
Sums
asymptotic approximations, 176–
178
Euler-Maclaurin. See Euler-
Maclaurin summation
Laplace method for, 203–207
Superleaves, 228
Surjections
enumerating, 492–493, 495
image cardinality, 519
maximal occupancy, 499
minimal occupancy, 497–498
Symbol tables
binary search trees, 281, 466
hashing, 474–476
rooted unordered trees, 317
tries, 448, 453, 459, 465
Symbolic method, 219, 221, 229. See
Combinatorial constructions.
t-ary trees, 331–333
deŀnition, 333
enumerating, 333–334
t-restricted trees, 334–336
Tails
asymptotic approximations, 176–
177
binomial distribution, 196–197
Laplace method, 203–205
in mapping, 523–524
Taylor expansions
asymptotic, 162–163
table, 162
Taylor theorem, 111–113, 220, 247
Telescoping recurrences, 86
Terminal symbols, 441–442
Ternary trees, 313–314
Ternary tries, 465–466
Text searching. See String searching
Ļeory of algorithms, 4–12
Ļeta notation (Θ), 6–7
Time complexity of sorting, 10–11
Toll function, 297–298
Transfer theorems, 219–220
bitstrings, 228
derangements, 251
involutions, 369
labelled objects, 232, 240
Lagrange inversion, 312
radius of convergence, 249–250
Taylor’s theorem, 247
universal, 443
unlabelled objects, 228–229
Transformations for generating func-
tions, 114–116
Transitions
ŀnite-state automata, 437–439, 456
state transition tables, 438–440
Traversal of trees
algorithms, 277–278
binary trees, 267, 278–280
labelled trees, 328
parenthesis system, 265
preorder and postorder, 266–267
stacks for, 308
Trees
algorithm examples, 277–280
average path length, 287–293
binary. See Binary search trees;
Binary trees
Catalan. See Catalan models and
trees
combinatorial equivalences, 264–
272
enumerating, 322, 331
expectations for trees, 310–312

I Ś Ő ő Ť
ȜȞȘ
expression evaluation, 278–280
heap-ordered trees, 362–365, 375,
380–384
height. See Height of trees
height-restricted, 336–340
hierarchy, 321–325
isomorphism, 324
labelled, 327–331
Lagrange inversion, 312–315
and mapping, 523–531
nomenclature, 321
ordered. See Ordered trees
parenthesis systems, 265–267
properties, 272–276
random. See Random trees
in random walk, 435–436
rooted unordered. See Rooted un-
ordered trees
rotation correspondence, 264–265
summary, 340–341
t-ary, 331–334
t-restricted, 334–336
traversal. See Traversal of trees
unlabelled, 322, 328–329
unrooted, 318–321, 323, 327–328
Triangulation of polygons, 269–271
Tries
combinatorial properties, 459–464
context-free languages, 416
deŀnitions, 449–451
encoding, 454–455
ŀnite-state automata, 456–457
vs. hashing, 476
multiway, 465
nodes, 448–456, 459–462
overview, 448–449
path length and size, 459–462
Patricia, 454
pattern matching, 455–458
radix-exchange sorts, 211–214,
454, 459–460, 463
random, 457–458
string searching, 416–420
suﬃx, 455
sum, 211–214
summary, 467–468
ternary, 465–466
Trigonometric asymptotic expansion,
162
Two-line representation of permuta-
tions, 237
2-ordered permutations, 208, 389–
393, 443–444
2-regular graphs, 252
2-3 trees, 336
fringe analysis, 51
functional equations, 118
2-3-4 trees, 336
2D-trees, 270
Unambiguous languages, 441–447
Unambiguous regular expressions,
432–433
Uniform discrete distributions, 130–
131
Uniform hashing, 511–512
Union-ŀnd problem, 316, 324
Union operations, 224, 228
Unlabelled combinatorial classes,
221–229
AVL trees, 332, 336, 338
B-trees, 332, 336, 338
binary trees, 228, 251, 260
bitstrings, 226, 420–426
bytestrings, 478
context-free grammars, 441–443
Motzkin trees, 341
ordered trees, 328–329

ȜȞș
I Ś Ő ő Ť
rooted unordered trees, 318–320,
322–323
t-ary trees, 333–334, 341
t-restricted trees, 334–336, 341
trees, 263, 341
unrooted trees, 318–321, 323, 327–
328
2-3 trees, 338
Unlabelled objects, 97, 221–229
Unnormalized mean (cumulated cost),
17, 135–137
Unordered trees
labelled, 329–331
rooted. See Rooted unordered trees
Unrooted trees, 318–321, 323, 327–
328
Unsuccessful searches, 295, 476, 505,
508, 511–515, 517–518
Upper bounds
analysis of algorithms, 4
cycle length in permutations, 368
divide-and-conquer recurrences, 80,
85
notation, 7, 154
and performance, 12
sorts, 10–11
tree height, 302
Urns, 474
labelled objects, 229–230
occupancy distributions, 474, 501–
510
Poisson approximation, 198–199
and word properties, 476–485
Valleys in permutations, 350–351,
362, 380–384
Vandermonde’s convolution, 114
Variance, 31–33
binary search trees, 294, 296, 311
bivariate generating functions, 136–
138
coupon collector problem, 490–491
inversions in permutations, 386
left-to-right minima in permuta-
tions, 394–395
occupancy distribution, 504–505
Poisson distribution, 202
probability generating functions,
129–130
runs in permutations, 378
singleton cycles in permutations,
404
selection sort, 399
quicksort, 138–139
Variations in unlabelled objects, 226–
227
Vertical expansion of bivariate gener-
ating functions, 136–138
Void nodes in tries, 449–456
Words
balls-and-urns model, 476–485
birthday problem, 485–488
caching algorithms, 494
coupon collector problem, 488–495
frequency restrictions, 497–499
hashing algorithms, 474–476
and mappings. See Mappings
maximal occupancy, 496–500
minimal occupancy, 498–499
occupancy distributions, 501–509
occupancy problems, 478–484
overview, 473–474
Worst-case analysis, 78
Zeta function of Riemann, 145

Essential Information about 
Algorithms and Data Structures
Robert Sedgewick’s Algorithms has long been the defi nitive practical guide 
to using algorithms: the book that professionals and students in program-
ming, science, and engineering rely on to solve real-world problems. Now, 
with the fourth edition, Sedgewick and Wayne have thoroughly updated this 
classic to refl ect today’s latest, most powerful algorithms. The authors bring 
together an indispensable body of knowledge for improving computer 
performance and solving larger problems. 
PEARSON
ALWAYS LEARNING
Available in print and eBook formats 
For more information and sample content visit 
informit.com/title/9780321573513
ISBN-13: 978-0-321-57351-3

Activate your FREE Online Edition at 
informit.com/safarifree
STEP 1: 
 Enter the coupon code: DLTLNVH.
STEP 2: 
 New Safari users, complete the brief registration form. 
Safari subscribers, just log in.
If you have diﬃ  culty registering on Safari or accessing the online edition, 
please e-mail customer-service@safaribooksonline.com
Your purchase of An Introduction to the Analysis of Algorithms, Second Edition, includes 
access to a free online edition for 45 days through the Safari Books Online subscription service. 
Nearly every Addison-Wesley Professional book is available online through Safari Books Online, 
along with thousands of books and videos from publishers such as Cisco Press, Exam Cram, IBM 
Press, O’Reilly Media, Prentice Hall, Que, Sams, and VMware Press.
Safari Books Online is a digital library providing searchable, on-demand access to thousands 
of technology, digital media, and professional development books and videos from leading 
publishers. With one monthly or yearly subscription price, you get unlimited access to learning 
tools and information on topics including mobile app and software development, tips and tricks 
on using your favorite gadgets, networking, project management, graphic design, and much more.
FREE 
Online Edition


Volume: 05 | Issue: 07 | Pages: 100 | April 2017
ISSN-2456-4885
` 120
PowerShell 
Open-Sourced!
Gatling: A Lightweight 
Load Testing Tool



14 
OpenForge debuts as India’s 
GitHub for e-governance 
projects
Admin
21 
What Does Big Data  
Mean to You?
24 
PowerShell Open-Sourced!
26 
An Introduction to Minio
Developers
34  Processing Big Data Using 
MongoDB-GridFS
41  Developing a React Redux 
Application Using create- 
react-app
45  The DevOps Series Ansible 
Deployment of LAMP and 
WordPress
52 
Gatling: A Lightweight 
Load Testing Tool
54 
Apache Cassandra: The 
NoSQL Scalable Database
57  Digital Wallet Application  
in App Inventor 2
60 
Gradle: Making Software 
Development Easier 
62 
Is HBase a Helpful Database?
68 
The Importance of Data 
Modelling in MongoDB
72 
A Peek at Three Python 
Databases: pickleDB, 
TinyDB and ZODB
FOR U & ME
78  The Different Types of  
NoSQL Databases
Open Gurus
81  Optimisation of 2D Toy 
Functions Using Scilab
REGULAR FEATURES
06 
FOSSBytes
News ++
  96  Tips & Tricks
37
29
Discover the Many Benefits of the 
Redis Database
Get the Best Out of MariaDB with 
Performance Tuning
ISSN-2456-4885
4 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort
@ef
y.in 
for 
a fre
e re
plac
eme
nt.
CD 
Tea
m e-
mail
: cdt
eam
@ef
y.in
Rec
omm
end
ed S
yste
m Re
quire
me
nts: 
P4, 
1GB
 RA
M, D
VD-R
OM 
Driv
e
April 2017
PALV2
Here’s something fresh 
for your desktop.
DVD OF THE MONTH
• Manjaro KDE Edition (17.0)
• deepin 15.3
• 10 tools for the Windows sysadmin
98
Bruce Momjian, co-founder of PostgreSQL 
Global Development Group and senior 
database architect at EnterpriseDB
“PostgreSQL 
updates are 
solely driven by 
its community 
members”
18
64
Choose the Right Database  
for Your Application
83  Managing Time Series Data 
Using InfluxData
88  The Best Open Source 
Databases for IoT Applications
92  Piwik: The Feature-Rich Open 
Source Analytics Platform
Columns
12 
CodeSport 
16 
Exploring Software: 
How Did I Become a 
Product?
Case Study
76 
MakeMyTrip Travels 
Forward in Time 
Using the Power of 
Open Source
EDITOR
RAHUL CHOPRA
EDITORIAL, SUBSCRIPTIONS & ADVERTISING
DELHI (HQ)
D-87/1, Okhla Industrial Area, Phase I, New Delhi 110020
Ph: (011) 26810602, 26810603; Fax: 26817563
E-mail: info@efy.in
MISSING ISSUES
E-mail: support@efy.in
BACK ISSUES
Kits ‘n’ Spares
New Delhi 110020 
Ph: (011) 26371661,  26371662
E-mail: info@kitsnspares.com
NEWSSTAND DISTRIBUTION
Ph: 011-40596600
E-mail: efycirc@efy.in 
ADVERTISEMENTS 
MUMBAI
Ph: (022) 24950047, 24928520 
E-mail: efymum@efy.in
BENGALURU
Ph: (080) 25260394, 25260023 
E-mail: efyblr@efy.in
PUNE
Ph: 08800295610/ 09870682995 
E-mail: efypune@efy.in
GUJARAT
Ph: (079) 61344948 
E-mail: efyahd@efy.in
CHINA
Power Pioneer Group Inc.  
Ph: (86 755) 83729797, (86) 13923802595 
E-mail: powerpioneer@efy.in
JAPAN
Tandem Inc., Ph: 81-3-3541-4166 
E-mail: tandem@efy.in
SINGAPORE
Publicitas Singapore Pte Ltd 
Ph: +65-6836 2272 
E-mail: publicitas@efy.in
TAIWAN 
J.K. Media, Ph: 886-2-87726780 ext. 10 
E-mail: jkmedia@efy.in
UNITED STATES
E & Tech Media 
Ph: +1 860 536 6677 
E-mail: veroniquelamarque@gmail.com
Printed, published and owned by Ramesh Chopra. Printed at Tara 
Art Printers Pvt Ltd, A-46,47, Sec-5, Noida, on 28th of the previous 
month, and published from D-87/1, Okhla Industrial Area, Phase I, New 
Delhi 110020. Copyright © 2017. All articles in this issue, except for 
interviews, verbatim quotes, or unless otherwise explicitly mentioned, 
will be released under Creative Commons Attribution-NonCommercial 
3.0 Unported License a month after the date of publication. Refer to 
http://creativecommons.org/licenses/by-nc/3.0/  for a copy of the 
licence. Although every effort is made to ensure accuracy, no responsi-
bility whatsoever is taken for any loss due to publishing errors. Articles 
that cannot be used are returned to the authors if accompanied by a 
self-addressed and sufficiently stamped envelope. But no responsibility 
is taken for any loss or delay in returning the material. Disputes, if any, 
will be settled in a New Delhi court only.
 
SUBSCRIPTION RATES 
Year 
Newstand Price 
You Pay 
Overseas
 
(`) 
(`)
Five 
7200 
4320 
—
Three 
4320 
3030 
—
One 
1440 
1150  
US$ 120
Kindly add ` 50/- for outside Delhi cheques.
Please send payments only in favour of EFY Enterprises Pvt Ltd.
Non-receipt of copies may be reported to support@efy.in—do mention 
your subscription number.
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 5

Compiled by: 
Jagmeet Singh
VMware expands open 
source presence by joining 
Linux Foundation 
After supporting the open source world passively for a long time, VMware 
has finally joined the Linux Foundation as a Gold member. This will help the 
company become an active player in the open source community. “The open 
source community will benefit from VMware’s wealth of talent and resources, 
as we collaborate to solve technology challenges from the data centre to cloud 
infrastructure and beyond,” said Jim Zemlin, executive director, the Linux 
Foundation, in a joint statement.
VMware has already contributed to some Linux Foundation projects such as Open 
Network Automation Platform (ONAP), Cloud Foundry and Open vSwitch. This has 
apparently brought the Dell subsidiary closer to the open source world. “VMware looks 
forward to working more closely with the open source community, and we know this 
involvement will enable ever better solutions and services for our customers,” stated 
Dirk Hohndel, vice president and chief open source officer, VMware.
The Gold members of the Linux Foundation include Accenture, Blackrock, 
Citrix, Doky, Facebook, Hart, Panasonic, SUSE, Toshiba and Toyota. 
Semiconductor manufacturer Renesas became a Gold member recently.
This is not the first time that VMware has leaned towards open source. The Palo 
Alto, California-headquartered company started favouring community efforts in the 
enterprise-focused market with the release of its vSphere integrated container code. 
In January 2017, VMware even partnered with the Linux Foundation for its Open-O 
project. That partnership was primarily aimed at constructing the future of network 
functions virtualisation (NFV).
MySQL 8 to bring massive improvements
MySQL 8 is just around the corner. This new version promises to fix all the 
shortcomings of MySQL 5.7, which has been in use as the latest version for over a year.
Considering the number of optimisations and changes, MySQL 8 is advanced 
enough to bump directly from the MySQL 5.7 release. The new version does 
not come with a file system’s maximum number of files that limit the number of 
databases you can use. 
MySQL 8 will allow users to store millions of tables in a particular database. 
The new MySQL will also simplify the hassle of making alterations to these tables.
FOSSBYTES
NASA releases code that 
was used for major scientific 
discoveries 
While travelling into space is still a 
dream for most of us, you can now 
access the code that has powered major 
scientific discoveries related to space 
exploration. NASA has initiated this 
development by releasing its latest online 
software catalogue of more than 1,000 
code descriptions.
The third edition of the annual 
software catalogue provides access 
to codes based on general public 
releases, open source releases, US and 
foreign releases, and US government-
purpose releases. There are categories 
such as business systems and project 
management, system testing, operations, 
design and integration tools, vehicle 
management, propulsion, crew and life 
support, and data and image processing.
“Software has been a critical 
component of each of NASA’s mission 
successes and scientific discoveries,” 
said Dan Lockney, NASA’s Technology 
Transfer programme executive, in a 
statement. “In fact, more than 30 per cent 
of all reported NASA innovations are 
software,” he added.
You can find the codes for some 
advanced drones and quieter aircraft 
models 
from the 
available 
software 
catalogue.  
While the 
code access is free, there are a variety 
of restrictions on some of the software 
solutions. You need to sign usage contracts 
with NASA or a government body to 
access most of the available codes. 
However, the open source material is 
provided through the online catalogue to 
support community efforts.
NASA started publishing its software 
catalogue online back in 2014. The prime 
aim is to broadly release code behind 
the space exploration and discoveries, to 
maximise the benefits to the world.
6 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

FOSSBYTES
Pixel 2.0 brings Arduino to 
wearables world
Kickstarter project Pixel 2.0 has expanded 
the usability of the Arduino platform by 
bringing it to the wearables market.
The square-shaped Pixel 2.0 is an 
Arduino Zero compatible board with an 
OLED panel on one side. The 128×128 
colour display is 3.8cm (1.5 inch) in size, 
which makes it an easy fit for wearables. 
You can use the board either like a 
smartwatch or embed it into a jacket to 
make it a smart wearable.
For performing different tasks on the 
small screen, the Pixel 2.0 has a 32-bit 
48MHz Atmel ATSAMD21G18 Cortex 
M0+ microcontroller along with 32K of 
RAM. There is also a microSD card slot 
that gives you the source to load your 
custom-made graphics and text.
The 4.6cm (1.8 inch) board has 14 
digital I/O pins, six analogue input pins 
and an output pin. It comes preloaded with 
Arduino/Genuino Zero bootloader, and 
is compatible with the Arduino’s SPI and 
SD libraries. You can also use Adafruit’s 
graphics library to easily create new 
images and drawings for the Pixel 2.0.
Though the pre-installed bootloader is 
specific to Arduino developments, there is 
a standard SWD header on the Pixel that 
has full compatibility with Atmel ICE. 
Unlike some other Arduino compatible 
boards, the Pixel 2.0 is entirely an open 
source model. This means that you will get 
access to its schematics and PCB layout. 
Moreover, you can modify the Pixel design 
and even integrate it into your own boards 
— without the need for any prior royalties 
and licensing fees.
The Pixel 2.0 is available at US$ 75 
on Kickstarter, with shipment scheduled 
to begin in June this year.
The MySQL team at Sun Microsystems is planning to bring significant changes 
to Common Table Expressions, Windowing functions, invisible indexes, and roles 
for user privileges. Likewise, MySQL 8 will bring features like Group Replication 
and Document Store from major server releases.
Sun Microsystems has skipped MySQL v6 and v7 for the eighth iteration. The 
MySQL v6 release was in the pipeline before the acquisition of MySQL AB. But the 
new parent company did not develop it further for production use.
You need to wait for some time to experience MySQL 8. Meanwhile, you can 
keep yourself informed on all the changes through the official release notes.
Google hosts ‘Operation Rosehub’ to patch thousands 
of open source projects
To narrow down the impact of the infamous ‘Apache Commons Collections 
Deserialisation Vulnerability’ that had affected several Java-based programs, Google 
has silently kickstarted its ‘Operation Rosehub’. A 50-member team of Google 
employees drove the new initiative for the 
first time in 2016 to help patch over 2,600 
open source projects.
Internally called ‘Mad Gadget’ by 
Google’s engineers, the vulnerability 
was spotted in early 2015. It grabbed the 
attention of companies like Oracle, Cisco, 
Red Hat, Jenkins, VMware, IBM, Intel, Adobe and HP a few months after its discovery 
by security researchers from Foxglove Security. The IT companies immediately issued 
security alerts to let enterprises patch their proprietary offerings. However, a Google 
employee took the step to fix the issue in affected open source projects.
“Operation Rosehub was organised from the bottom-up on companywide 
mailing lists. Employees volunteered and patches were sent out in a matter of 
weeks,” Google’s software engineer Justine Tunney wrote in a blog post.
The researchers had reported that the vulnerability was a part of the seven ‘gadget’ 
classes within the Apache Commons Collections library versions 3.0, 3.1, 3.2, 3.2.1 
and 4.0. These were the classes for handling Java object deserialisation that was used 
alongside the serialisation function to convert data from one format to another.
As the library was vital for many software operations, it was deployed by 
various commercial and open source projects. The Google employee used the pull 
request feature on GitHub to inform developers to patch their community solutions 
at the initial stage. But to reach the masses, a new development was needed.
Google has the Rosie tool that helps developers implement large-scale changes 
to codebases owned by its engineering teams. But GitHub, where a large number 
of Mad Gadget-affected projects were hosted, does not offer any such help. This is 
the reason the search giant formed a special task force and started working on the 
patches. “Patches were sent to many projects, avoiding threats to public security for 
years to come,” said the Google engineer.
Though some of the patches were just one-line changes, Google’s engineers 
took months to fix the vulnerable projects on GitHub. They spent a part of their 
daily routine at Google to jointly work on the patches.
For the remaining projects that are yet to be patched, the Google team is using 
an open source data set on BigQuery. This helps the engineers to identify the 
vulnerability in listed solutions.
“Going forward, we believe the best thing to do is to build awareness. We want 
to draw attention to the fact that the tools now exist for fixing software on a massive 
scale, and that these work best when that software is open,” Tunney concluded.
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 7

FOSSBYTES
The Mad Gadget flaw has led to some serious attacks in the past. It enabled a 
hacker to gain access to the San Francisco Municipal Railway system last November. 
And it caused a PayPal vulnerability that was discovered in December 2015.
Android Studio 2.3 will make it easy to build new apps
Google has upgraded Android Studio to version 2.3. The latest release is designed 
to make it easy for developers to build new Android apps. “We are most excited 
about the quality improvements in Android Studio 2.3, but you will find a small set 
of new features in this release that integrate into each phase of your development 
flow,” wrote Android product manager Jamal Eason, in a blog post.
One of the major changes that Android Studio 2.3 includes is the improved version 
of the Run action. You just need to hit the Run action button from the toolbar to 
automatically restart the app and view the changes in your code. Also, there is a new 
Apply Changes action that swaps the code while your app keeps running on the screen.
Eason mentioned that the Android team has significantly changed the 
underlying implementation to deliver a more reliable experience. The startup lag 
for instant Run-enabled apps has also been eliminated to improve the performance 
on the IDE.
GitHub launches hosted service to serve enterprises with cloud
More than nine years after being a one-stop code repository solution for the open 
source community, GitHub is finally taking a sharp turn to support enterprises. The 
new development is a hosted service by the company to let some large companies 
opt for a cloud solution to store their code.
GitHub previously had three workflow plans, namely, 
Developer, Team and Business. But keeping the fast adoption 
of the cloud in mind, the San Francisco, California based 
company has expanded its Business package with the 
hosted version. The new offering brings SAML single sign-
on, automated provisioning and deprovisioning, real-time 
support on business days and guaranteed uptime. All this 
comes at a monthly charge of US$ 21 per user.
BlackArch Linux adds 50 new ethical hacking tools
BlackArch Linux OS has announced a new ISO that includes 50 new tools for 
ethical hacking. With the latest development, the Linux distribution now comes 
with over 1700 ethical hacking and penetration testing tools pre-installed.
The BlackArch Linux 2017-03-01 release is based on Linux kernel 4.9.11. 
The latest ISO brings a fresh range of tools that eases penetration testing and 
ethical hacking. Additionally, the distribution has a new installer version 0.3.3 
and an in-house BlackArch build. The BlackArch Linux team has also improved 
dependencies and installation scripts in the latest build.
Notably, while Arch Linux has stopped supporting 32-bit machines, the latest build 
by BlackArch Linux offers both 32-bit and 64-bit ISOs. BlackArch Linux also has a 
64-bit OVA image that can be used in VirtualBox, QEMU and VMware virtualisation 
software. You can download the BlackArch Linux 2017-03-01 build from its official 
website. Existing users simply need to get the latest update on their system by running 
the ‘sudo pacman -Syu’ command in the terminal or virtual console.
Raspberry Pi Zero W debuts with Wi-Fi and Bluetooth
After successfully selling over twelve million units of Raspberry Pi over the past 
five years, computer engineer Eben Upton has brought the Raspberry Pi Zero W to 
Google open sources end-
to-end encrypted email 
service project
Google has decided to open source its 
experimental end-to-end encryption 
email system. Called E2EMail, this 
internal project by the search giant 
promises to deliver a secured, email 
communication solution.
E2EMail offers advanced security 
measures to prevent snooping. The 
email service is designed for end users 
as well as enterprise customers, and 
provides optimum security and privacy.
A large number of communication 
services have implemented encryption 
after Edward Snowden’s disclosure 
about NSA’s spying activities. 
However, not every email service out 
there offers end-to-end encryption. This 
is the reason Google wants to magnify 
the reach of end-to-end encryption to a 
wider audience through its E2EMail.
Based on JavaScript crypto 
library, E2EMail integrates OpenPGP 
into Gmail using a Chrome extension 
and keeps all clear text of the message 
body exclusive to the email client. 
The initial model uses a barebones 
central key server for testing. But 
Google is also planning to deploy 
its Key Transparency to improve the 
security of the service further.
“Key Transparency delivers a 
solid, scalable and thus practical 
solution, replacing the problematic 
web-of-trust model traditionally used 
with PGP,” Google’s security and 
privacy engineering team comprising 
KB Sriram, Eduardo Vela Nava and 
Stephan Somogyi said in a blog post.
The E2EMail code is available for 
access through a GitHub repository. The 
open source release comes with Apache 
License version 2.0.
8 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

FOSSBYTES
Microsoft’s Amazon Echo 
competitor features Cortana 
on Linux
While Cortana is yet to debut on 
Linux, Microsoft is reportedly 
building an Amazon Echo competitor 
with a mix of Cortana and the open 
source platform. This smart speaker 
was officially revealed in December 
last year, and is currently under 
development by Samsung-owned 
Harman Kardon.
The Harman Kardon-made device 
has passed through Wi-Fi certification 
that suggests the presence of Linux. 
The certification shows that the 
speaker, called Harman Kardon with 
Cortana, has Linux version 3.8.13. 
Though the listed kernel is a dated one 
as we have Linux 4.10 on board, it is 
enough to let developers play around 
with the Cortana integration.
Microsoft recently started showing 
its “love” for Linux and the open 
source community. Last November, 
the Redmond company brought Linux 
to its Windows 10 platform and even 
joined the Linux Foundation.
However, the Linux power behind 
the smart Cortana speaker is apparently 
quite distinct. It will enable Microsoft 
to rival models such as Amazon Echo 
and Google Home, and let developers 
build new experiences for the digital 
assistant. As Linux is massively spread 
across major Internet of Things (IoT) 
developments, this is likely to give 
Microsoft success in the emerging space.
Going forward, Microsoft is 
apparently aiming to take on Amazon 
Alexa and Google Assistant, which 
can be found on the Echo and Home 
speakers, respectively.
the market. The latest entrant in the Raspberry Pi family is a new variant of the Pi 
Zero model that was launched at US$ 5 in November 2015.
Raspberry Pi Zero W widens the use cases of its previous model by bringing 
wireless connectivity through a built-in Cypress CYW43438 chip. First featured on 
the Pi 3 Model B, the chip provides 802.11n wireless LAN and Bluetooth 4.0.
“We imagine you will find all sorts of uses for Zero W. It makes a better general-
purpose computer because you are less likely to need a hub; if you are using Bluetooth 
peripherals, you might end up with nothing at all plugged into the USB port,” Upton, 
who heads the non-profit Raspberry Pi Foundation, wrote in a blog post.
Excluding the wireless connectivity support, the newly announced Zero W and 
the previous Zero have no differences. Both the single-board computers are 3cm 
(1.2 inch) wide and have a height of 6.6cm (2.6 inches). Also, they are powered by 
the same BCM2835 chipset that has a processing core at a clock speed of 1GHz.
The Raspberry Pi Zero family has 512MB of RAM and a microSD card slot. 
There is a full-size GPIO header with 40 pins to let you easily begin with your 
experiments. Additionally, the board has mini-HDMI ports, USB OTG (On-the-Go) 
ports, microUSB power socket and a CSI camera connector.
Alongside advancing the Pi Zero line-up, the Raspberry Pi Foundation has 
partnered with Kinneir Dufort and T-Zero to launch an official injection-moulded 
case. The new case has three interchangeable lids, and is based on the same 
aesthetics that were developed for the Raspberry Pi 3 case.
Raspberry Pi Zero W is on sale through all Zero distributors at a price of US$ 10. 
Facebook releases Prophet, the open source forecasting tool 
Facebook has expanded its presence in the world of open source by releasing an 
in-house forecasting tool called Prophet. Available in Python and R programming 
languages, the latest development is designed to provide automated forecasts that are 
vital for data scientists and analysts.
Forecasting is critical not just for a 
company like Facebook but even for a 
startup or a small enterprise.  Facebook 
is using Prophet across many of its 
applications to produce forecasts for 
planning and goal setting. Based on an 
additive model, the open source solution 
is mainly used for hourly, daily and weekly observations with at least a few months 
of history. It also has strong multiple ‘human-scale’ seasonalities and works best 
with daily periodicity, with at least one year of historical data.
“With Prophet, you are not stuck with the results of a completely automatic 
procedure if the forecast is not satisfactory — an analyst with no training in 
time series methods can improve or tweak forecasts using a variety of easily 
interpretable parameters,” Facebook engineers Sean J. Taylor and Ben Letham 
wrote in a detailed note.
Facebook’s core data science team is maintaining the code of Prophet. 
HPE and Red Hat partner to build open source NFV solutions
Hewlett-Packard Enterprise (HPE) and Red Hat are working together to offer 
network functions virtualisation (NFV) deployment services for communication 
platform providers (CSPs). The planned infrastructure is designed to be 
completely open source.
NFV solutions by HPE will incorporate OpenStack and Ceph storage by Red 
Hat, whereas Red Hat is planning to productise HPE NFV System 1.4. This whole 
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 9

FOSSBYTES
development will be a mix of hardware and software to simplify NFV deployment. 
Ultimately, the new offering is projected to simplify end-to-end problems like 
ordering, operations, deployment, life cycle management and services.
“Red Hat and HPE share a common vision and commitment to enable a rich 
ecosystem of NFV solutions — our work together is aimed at easing customers’ 
ability to leverage the expansive options we collectively provide,” said David Sliter, 
vice president and general manager of communications solutions business, HPE.
HPE has the opportunity of improving OpenStack by collaborating with Red 
Hat. The company plans to offer new choices to enterprise customers through 
the latest partnership. As a result of the fresh deal between HPE and Red Hat, 
the adoption of OpenStack will see a marginal growth in enterprise customers. 
Red Hat will leverage HPE’s OpenNFV Labs to validate virtual network 
functions (VNF). Additionally, Red Hat customers will get access to HPE VNF 
on-boarding services.
HPE NFV 1.4 is scheduled for launch in April 2017. It will debut for HPE 
Helion OpenStack Carrier Grade and Red Hat OpenStack Platform.
Massive open network merger helps Linux Foundation 
set up ONAP project
The merger of open source ECOMP with the Open Orchestrator Project (OPEN-O) 
has helped the Linux Foundation to establish its Open Network Automation 
Platform (ONAP). The new open source 
project encourages top carriers and 
vendors to offer automation, designing 
and orchestration, and manage services 
and virtual functions for end users.
Operators like AT&T, Bell Canada, 
China Mobile and China Telecom are 
working together with network leaders 
such as Cisco, Ericsson, GigaSpaces, Huawei, IBM, Intel, Nokia, Orange, Tech 
Mahindra, VMware and ZTE to make ONAP successful in the market. Also, 
technology companies including ARM, Canonical, Metaswitch and Raisecom, 
have joined the new project to develop the next-generation model of open 
source network technologies.
“We are excited to see the industry coalesce around ONAP with this 
unprecedented merger. Such a broad effort and investment will expedite our vision 
to deliver an open platform for network automation,” said Jim Zemlin, executive 
director, the Linux Foundation. The merger of ECOMP and OPEN-O to enable 
the ONAP project is considered “to create a harmonised and comprehensive 
framework” for virtual network functions. These functions would be utilised by 
software, networks, as well as IT and cloud providers and developers to advance the 
existing networking models and build new services.
The Linux Foundation is set to establish a governance and membership structure 
for ONAP, while a governing board has been planned to guide business decisions 
and align technical communities and members. “ONAP is a great opportunity for 
the industry to work together towards the goal of a new operational model that 
includes network-wide orchestration and automation,” said David Ward, CTO of 
engineering and chief architect, Cisco Systems.
ONAP will work in line with other open source network projects such as ODL, 
FD.io, PNDA and OPNFV. All these open source projects would ultimately help 
developers build new experiences for emerging technologies like Internet of 
Things (IoT) and the cloud. 
10 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

FOSSBYTES
Linux kernel gets fix for 11-year-old root hole in DCCP code
Linux developer Andrey Konovalov has released a fix for an 11-year old bug in the 
Linux kernel. The security hole is in the support for Datagram Congestion Control 
Protocol (DCCP) that was introduced in 2005.
The flaw can be exploited by malicious software on a vulnerable device or 
gain root-level access when users log into their accounts. Once reached through 
a backdoor, attackers can leverage the vulnerability to compromise the system 
and even acquire a box from a connected network or Internet. Moreover, the 
programming blunder is in how DCCP code handles a socket buffer (skb).
According to the email list announcement by Konovalov, an skb for a DCCP_
PKT_REQUEST packet is forcibly freed via __kfree_skb in dccp_rcv_state_ process 
if the dccp_v6_conn_ request is returned successfully. An attacker can then gain 
access and control what object that would be, and even rewrite its content with 
arbitrary data. If the object has any triggerable functions, the attacker can execute 
the arbitrary code in the kernel. “An attacker can control what object that would 
be and overwrite its content with arbitrary data by using some of the kernel heap 
spraying techniques,” explained Konovalov.
The fix has been released for the Linux community to reduce instances of the 
DCCP flaw. It is recommended to update your system as soon as your distro gets the 
patch. Meanwhile, you can remove the buggy DCCP support from your kernel to 
avoid the impact of the vulnerability.
Microsoft Azure gets Kubernetes support
Microsoft has announced that it has added Kubernetes support to its Azure 
Container Service. The new development has expanded Azure with the inclusion 
of three major orchestration technologies, namely, Windows Server Containers, 
Kubernetes and DC/OS — rivalling other public cloud platforms.
Kubernetes is one of the leading orchestrators in the world of containers. Though 
Google initially promoted the open source container cluster manager, companies 
like Baidu, Canonical and Intel have recently added Kubernetes support. And now, it 
is Microsoft that has favoured Kubernetes for Azure.
Microsoft’s Azure team launched the preview support for Kubernetes 
in November last year. That preview launch helped the Redmond company 
improve the support, and ultimately led to its general availability. “With this 
news, we again deliver on our goal of providing our customers the choice 
of open source orchestrators and tooling that simplifies the deployment of 
container-based applications in the cloud,” said Saurya Das, program manager 
II, Azure Linux.
The Kubernetes support on Azure Container Service comes weeks after Docker 
debuted on Microsoft’s cloud platform. The newly formed combination will upgrade 
container management on Azure and give developers a reason to prefer Microsoft 
over Amazon and Google.
Along with bringing the Kubernetes support, Microsoft has also opted for 
the preview of Windows Server Containers to provide enterprise customers an 
additional choice in orchestrator using the Azure Container Service. The Azure 
team has also updated the existing DC/OS support to version 1.8.8 to deliver an 
improved experience.
You can deploy an Azure Container Service cluster using the Azure portal or 
Azure CLI 2.0, and experience Kubernetes or other orchestrators including Windows 
Server Containers or DC/OS.
Linux 4.10 brings support for 
virtualised GPUs
Linus Torvalds has finally brought out 
the anticipated Linux 4.10 release. 
The new Linux kernel is specifically 
designed to enhance graphics on high-
end hardware and thus, you can find 
native support for virtual GPUs.
Coming after eight release 
candidates, Linux kernel 4.10 is the result 
of nearly 13,000 commits that the Linux 
community received from its members 
all around the world. This is the reason 
Torvalds considers it as a ‘fairly average 
release’ and not as small as it initially 
seemed to be. “On the whole, 4.10 did 
not end up as small as it initially looked,” 
the 47-year-old Torvalds wrote in a 
mailing list announcement.
One of the most noticeable changes 
that you can find in the final Linux 
4.10 release is the arrival of support 
for virtual GPUs. This means that you 
can now have an immersive experience 
when using virtual GPUs on an updated 
Linux system. Additionally, the latest 
kernel includes improved support 
for GPU models and some special 
treatments for AMD Radeon.
Apart from improving support for 
GPUs, Linux 4.10 incorporates several 
improvements to file systems as well. 
You can see some of the instances on 
EXT4, F2FS, XFS and OverlayFS. 
The new Linux kernel also includes a 
new hybrid block polling method that 
reduces the load on CPU.
Linux 4.10 introduces some major 
changes for mobile devices. There is 
support for newer Qualcomm Snapdragon 
chips and compatibility with Allwinner 
A64. The kernel also includes certain 
driver upgrades and better caching options.
“I expected things to be pretty 
quiet, but it ended up very much a 
fairly average release by modern kernel 
standards,” Torvalds stated.
Linux 4.10 would take some time to 
reach your favourite distro. However, 
you can download the new update from 
the official kernel.org website. 
For more news, visit www.opensourceforu.com
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 11

CODE
SPORT
Sandya Mannarswamy
12 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
A 
few months back, we had started a discussion 
on deep learning and its application in natural 
language processing. However, over the last 
couple of months, we digressed a bit due to various 
other requests from our readers. From this month 
onwards, let’s get back to exploring deep learning. 
In prior columns, we had discussed how supervised 
learning techniques have been used to train neural 
networks.  These learning techniques require labelled 
data. Given that today’s state-of-art neural networks 
are extremely deep, with many layers, we require 
large amounts of data so that we can train these 
deep networks without over-fitting. Getting labelled 
annotated data is not easy. For instance, in image 
recognition tasks, we need to have specific pieces of 
images bound together in order to recognise human 
faces or animals.  Labelling millions of images 
requires considerable human effort. On the other hand, 
if we decide to use smaller amounts of labelled data, 
we end up with over-fitting and poor performance 
on test data.  This leads to a situation wherein many 
of the problems, for which deep learning is an ideal 
solution, do not get tackled simply due to the lack of 
large volumes of labelled data. So the question is: is 
it possible to build deep learning systems based on 
unsupervised learning techniques? 
In last month’s column, we had discussed word 
embeddings, which along with paragraph and 
document embeddings are now widely used as inputs 
in various natural language processing tasks.  In 
fact, they are used as input representation for many 
of the deep learning based text processing systems.  
If you have gone through the word2vec paper at 
https://papers.nips.cc/paper/5021-distributed-
representations-of-words-and-phrases-and-their-
compositionality.pdf, you would have seen that there 
is no labelled training data for the neural network 
described in this paper. The word2vec neural 
network is not a deep neural network. There are only 
three layers — an input layer, a hidden layer and 
an output layer.  Just as in the case of supervised 
learning neural networks, back propagation is used 
to train the weights of the neural network.  Here is a 
quick question to our readers — why do we initialise 
the weights of the nodes to be random weights 
instead of initialising them to zero? 
In a supervised learning setting, we have labelled 
data, which provides what is the correct/expected 
output/label for a given input. The neural network 
produces an actual output for a given input. The 
difference between expected output and actual output 
is the error term at the output layer. As we had seen 
earlier during the discussion on back-propagation, 
this error term is back-propagated to earlier layer 
nodes whose weights are determined based on these 
error terms.  The key idea behind back propagation 
is the following — weights of each node are adjusted 
in proportion to how much it contributes to the error 
terms of the next layer’s nodes, for which the first 
node’s output acts as input.  For back propagation to 
work, the desired output for a given input for each 
output layer’s node needs to be known. This can then 
be back-propagated to the hidden layer’s neurons. 
Here is a question to our readers — in the word2vec 
paper, where the neural network is being trained, 
what is the expected output for a given input? 
Well, the answer is simple. The expected output 
that is used for training is the same as the input 
itself.  If you consider the simplest variant of the 
word2vec continuous bag-of-words model, with only 
one word per context, the task is to predict the target 
word, given one word in context. If we make the 
assumption that the context word and target word are 
adjacent, this reduces to the bigram model, and we 
need to predict the succeeding word given a word in a 
continuously moving window of the corpus. This task 
requires no explicitly labelled data except the input 
corpus itself.  Thus, word2vec is a great example for a 
neural network method which can scale to large data, 
but does not require explicit labelled data. 
In this month’s column, we continue our discussion on deep learning. 

Guest Column
CodeSport
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 13
By: Sandya Mannarswamy
The author is an expert in systems software and is currently 
working as a research scientist at Xerox India Research 
Centre. Her interests include compilers, programming 
languages, file systems and natural language processing. If 
you are preparing for systems software interviews, you may 
find it useful to visit Sandya’s  LinkedIn group ‘Computer 
Science Interview Training India’ at http://www.linkedin.com/
groups?home=HYPERLINK “http://www.linkedin.com/group
s?home=&gid=2339182”&HYPERLINK “http://www.linkedin.
com/groups?home=&gid=2339182”gid=2339182
By now, you may be wondering what is the need for 
unsupervised learning with neural networks? After all, supervised 
learning has proved itself to be quite successful with neural 
networks. As mentioned earlier, in many cases, we are not able 
to apply deep learning techniques due to a lack of labelled data. 
And another and more fundamental reason for the attractiveness 
of unsupervised neural network techniques is that they seem to 
be closer to the way human beings develop their knowledge and 
perception of the world.  For instance, as children build their 
knowledge of the world around them, they are not explicitly 
provided labelled data. Also, more importantly, there appears to 
be a hierarchical learning method that human brains follow.  It is 
the ability to represent abstract concepts, based on which higher 
concepts are built. If human beings have the ability to obtain much 
of their knowledge from unsupervised learning techniques, how 
can we employ similar techniques in artificial neural networks 
for deep learning? This requires us to take a brief digression from 
artificial neural networks to human cognition. 
One question which has been debated a lot in human cognition 
is that of ‘localist’ vs ‘distributed’ representation.  A localist 
representation means that one or a very few limited number of 
neurons are used to represent a specific concept in the brain. On 
the other hand, in a distributed representation approach, a concept 
is represented by distribution over a large number of neurons, and 
each neuron participates in representation of multiple concepts.  
In other words, while in a localist representation, it is possible to 
interpret the behaviour of a single neuron as corresponding to a 
specific concept/feature, in a distributed representation, it is not 
possible to attribute/map the activity of a single neuron in isolation 
as its behaviour is dependent on the activity of a host of other 
neurons. An extremist view of the localist approach is represented 
by what is known as ‘grandmother cells’. Are there neurons 
that are associated with a single concept only? For instance, if 
‘grandmother’ is a concept represented in the human brain, is 
there a single neuron that gets fired in recognising this concept? 
Well, an affirmative answer to that question will rule in favour of 
the localist approach.  
There has been a raging debate on the presence/absence of 
grandmother cells. There have been certain experiments in the 
past which seemed to indicate the presence of the grandmother 
cell (http://tandfonline.com/doi/full/10.1080/23273798.2016
.1267782). But recent research indicates a lack of sufficient 
evidence for grandmother cells. Earlier experiments indicated 
that certain specific neurons only fired when recognising a 
specific concept such as ‘grandmother’ or a popular personality 
such as Jennifer Aniston (https://www.newscientist.com/
article/dn7567-why-your-brain-has-a-jennifer-aniston-cell/). 
It is possible now to think of alternate explanations which can 
explain these experimental results. One strong possibility is the 
presence of sparse coding of neurons in human brains. Sparse 
neural coding means that only a relatively small number of 
neurons are used to represent different features of a concept. 
For instance, let us consider an image which has 1000 x 1000 
pixels. Of the million pixels, only some are used to encode 
horizontal lines, certain others to encode vertical lines and 
so on. In fact, each sub-concept or sub-feature is represented 
by a selective, limited set of neurons. This means that out of 
the million pixels, only a few are activated for each image, 
enabling us to recognise a very large number of images. 
Thus, sparse coding provides an alternate explanation for the 
grandmother cell experiments. When you are shown the image 
of your grandmother, there are a few selective neurons that are 
encoding this concept and, hence, they are the ones that get 
activated.  It is possible that these neurons also are involved 
in representing various other concepts. So the sparse coding 
explanation does not preclude a distributed representation for  
knowledge representation in human brains.
By now, you may be thinking that though the human brain 
is interesting, how is it relevant  to deep learning and artificial 
neural networks? Well, it is important because human vision 
recognition and perception is one of the best designed natural 
neural networks, and the ultimate holy grail of deep learning is 
to figure out how to design deep learning networks which can 
match/surpass the power of human cognition. This brings us 
back to the question of designing efficient unsupervised deep 
learning networks. For instance, if we can just provide unlabelled 
YouTube videos to a deep neural network and allow it to learn 
in an unsupervised manner from the videos, what can be learnt 
by the neural network?  For example, given a multi-layer neural 
network for this task, can we interpret the activity of certain 
nodes as recognising colour changes, certain nodes as recognising 
horizontal lines or certain nodes as recognising vertical lines?  
The answer to the above questions is ‘Yes’. There are 
neural networks known as auto encoders which are used 
for unsupervised deep learning. An auto-encoder learns the 
weights of the network using back propagation, wherein the 
expected output is set to be equal to the input. We will discuss 
more on auto-encoders in our next column.  Also, if you were 
wondering about the experiment wherein an auto encoder 
learns without any supervision from YouTube videos, here is 
the paper which describes this: https://arxiv.org/abs/1112.6209. 
If you have any favourite programming questions/software 
topics that you would like to discuss on this forum, please 
send them to me, along with your solutions and feedback, 
at sandyasm_AT_yahoo_DOT_com. Till we meet again next 
month, wishing all our readers a wonderful and productive 
month ahead! 

OpenForge debuts as India’s 
GitHub for e-governance projects
the total development time and costs and, ultimately, 
bring the culture of sharing to the government’s software 
development efforts.
While developers and major open source communities are 
yet to explore the features of OpenForge, more than 50 private 
and public projects are already available on the platform. 
Open Street Map, the Mizoram State eGovernance (MSeGS) 
project, eSign Integration Java class and Open Street Map, 
among others, are already up on this platform.
OpenForge is not just an option for storing e-governance 
projects but can also be a GitHub alternative for open source 
developments, as it offers some 
advanced features. There is a 
tracker to let you track bugs, 
tasks and requirements for your 
hosted projects. Likewise, you 
can use the built-in document 
manager to publish and 
manage documents such as 
administration or user guides, 
API documentation and a 
frequently asked questions bank.
There is Git support to 
handle multiple repositories 
for a given project and manage 
‘personal repositories’ for each developer. The platform also 
allows you to access your Git repository using any of your 
favourite tools.
The NeGD team plans to enhance OpenForge adoption 
among developers by building communication tools. 
These tools would include mailing lists, a news service 
and Web forums.
You first need to sign up on the OpenForge website to 
start storing and sharing your code with the public. The sign-
up process requires a user name, password and a valid email 
address. Organisations can also access the platform using the 
same procedure. 
E
lectronics and IT Minister, Ravi Shankar Prasad, 
has formally launched OpenForge at the 10th 
International Conference on Theory and Practice 
of Electronic Governance (ICeGOV) held on March 
7 in New Delhi. The new platform is designed as an 
Indian alternative to code repository platforms like 
GitHub and SourceForge, and will store and share major 
e-governance projects.
OpenForge has been in development since last 
year with the aim to build an appropriate technology 
infrastructure for implementing the government’s policy on 
open source. Open Source For 
You had exclusively reported 
the development of this 
platform last August.
Based on open source 
Tuleap, OpenForge leverages 
PHP to accept projects 
including applications, 
frameworks, libraries, SDKs 
and APIs. It serves the 
code-sharing model for two 
broad areas — Government 
to Community (G2C) and 
Government to Government 
(G2G). The code available under the G2G section is 
private and controlled by government bodies, while 
the G2C area will bring government and community 
members under one roof and provide a public repository 
platform similar to GitHub.
The National e-Governance Division (NeGD) is 
handling OpenForge under the Ministry of Electronics 
and Information Technology (MeitY), and the NeGD 
team, which had previously developed the cloud-based 
DigiLocker platform, is operating and maintaining its 
features at the backend. The platform is hosted on the data 
centre at the state-run National Informatics Centre (NIC).
The NeGD members feel the platform will “play 
a pivotal role in preventing the problem of duplicity 
and fragmentation” in various e-governance projects 
of the government. It will result in some reduction of 
News ++
OpenForge serves two worlds
By: Jagmeet Singh
The author is an assistant editor at EFY.
Government
to 
Government
Government
to 
Community
Download 
& Reuse 
Download, Reuse 
& Improve
Download  
& Reuse
Download, Reuse 
& Improve
Government 
to 
Community
Government 
to 
Government
OpenForge
Code Repository 
& Version Control
14 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
14 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

T U E S D A Y  -  T H U R S D A Y
BASEMENT 2 to LEVEL 5 
MARINA BAY SANDS, SINGAPORE
www.EnterpriseIT-Asia.com
#EnterpriseIT2017
Enabling Smarter Business
ENTERPRISE  
MOBILITY
SMART CITIES
IoT
CLOUD &  
BIG DATA
SECURITY &  
CYBER-SECURITY
SUSTAINABLE 
TECHNOLOGIES
www.CommunicAsia.com 
@ Marina Bay Sands, Singapore
Organised by:
Incorporating:
www.Satcomm-Asia.com 
Held concurrently with:
www.Broadcast-Asia.com 
@ Suntec Singapore
A Part of:
Supported by:
Supporting Media:
Held in:
In support of:
Hosted by:
Endorsed:
SES
Dedicated zones for:
Pre-Register  
Now for your Free 
Exhibition Pass
@
www.EnterpriseIT-Asia.com
Calling All Senior  
Management, IT and  
Operations Professionals!
PRE-REGISTER  
ONLINE NOW!
PHILIPPINES
C O N N E C T
  Witness the technologies that will enable you to accelerate your company’s  
 
 digital transformation
  Network with more than 1,100 international exhibitors 
  Hear from 170+ industry experts at the high-level CommunicAsia2017 Summit.  
 
 Includes speakers from DBS Bank, Lazada, LG Electronics, Keppel Data  
 
 
 Centres and more
  Gain knowledge and learn from case studies at free seminars and workshops 
  on the show floor such as
  Check out the website for more activities and details

Guest Column
Exploring Software
16 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
But the ISPs did not offer an instant messaging service.
Skype and Google Hangouts have become the default 
options. If one of them has a problem, try the other. Apple 
users may prefer Facetime when communicating with other 
Apple users (imagine the absurdity of accepting the terms of 
being able to use your phone to talk to only those who have 
the same brand of phone as yourself).
Again, open source options would have enabled ISPs 
to offer this service at minimal cost. I see no reason why 
XMPP or similar options cannot dominate the world. India 
itself is a large enough market if the regulator forces ISPs to 
offer such a service!
On social networking or whatever 
happened to Diaspora
The problem with a social networking application is that 
it is useless unless the product has enough users. It cannot 
succeed unless enough people promote and push it. So far, the 
history of social networks has been that they grow until the 
new generation of people join an alternate. Will Facebook be 
pushed away? I am more concerned with what we ought to 
have and how we can get it.
One promising application was Diaspora. Fortunately, 
it is still around and the beauty is that it is inherently 
distributed, and data is owned by the user and not the 
application provider. Ideally, the application should be 
replaced by protocols for sharing information so that multiple 
interoperable social networking applications are available. 
Meanwhile, I hope an enlightened ISP starts offering a 
Diaspora server for its users. If enough do so, Facebook may 
finally see some competition! 
On content
I installed an ad-blocker after being troubled by a number of 
sites with noisy multimedia ads over which I had no control.
A number of newspapers made me aware that the ad-
blocker was installed. Some just insisted that I allow ads from 
their site and some offered subscription options. However, 
A video reminder sets off the author on a train of thought 
about various aspects of his life online. He ponders over 
matters like email, chat, social networking, content and 
what the future holds for us, considering that the data we 
generate is owned by various corporations.
How Did I Become 
a Product?
“The ISPs let me down.”
Recently, YouTube reminded me to again watch the video 
‘The Terrifying Cost of ‘Free’ Websites’at https://www.
youtube.com/watch?v=5pFX2P7JLwA&t=315s. This was 
just after I had been tempted to click on a link on Facebook. 
I noticed that it was sponsored and I was having a tough 
time finalising a short vacation. I did not need yet another 
source of information to make my task even harder. Of 
course, the question of how Facebook knew which link to 
show me, did cross my mind.
On email 
I had signed up for Gmail in the early days but it was not 
supposed to be my primary email. I was satisfied with the 
email of the dial-up ISP and preferred to keep my mails 
on my desktop. Then, the ISP changed my email address. 
It was a pain, but I managed. Then I got a broadband 
connection and the ISP gave me a new email ID. I could not 
keep the old one, unless I retained the dial-up connection as 
well! The promise of Google to not ever change my email 
ID was the push I needed to sign up for it. 
Just as mobile number porting has been made 
mandatory, there should be portability of email IDs too. 
I should be able to use the email ID that was offered by 
my ISP for ever.
There are enough open source options available to the 
ISPs to enable this. And the savings in the cost of data 
traffic routed through the ISP’s network to a third party may 
be more than the operating cost of such email portability.
On chat
I am locked out of WhatsApp messages. My wife tells 
me the significant messages. Three products, Google 
Hangouts, Facebook and Skype are more than I can handle. 
This is just not what was expected. Instant messaging 
protocols like XMPP were the obvious answer. Google 
Chat even supported XMPP and extended it with Jingle for 
multimedia support. I used Pidgin with Google Chat.
Anil Seth

Guest Column
Exploring Software
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 17
my reading of content is determined more by the content itself 
and who wrote it, rather than who published it. For routine 
news, the source may not matter. I may reconfirm surprising 
news by checking on multiple sources. For opinion articles, I 
am more likely to be influenced by the person who is writing 
rather than the paper. Subscribing to numerous news outlets is 
not an acceptable or realistic option. 
We may want to or may even be forced to pay for news, 
like the charges for radio we paid many years ago. The trouble 
is, whom to pay and how much. Donations work to an extent, 
but they probably benefit only the well known sites like 
Wikipedia or WikiLeaks.
The following is more a wish for what seems plausible. 
A part of the ISP’s charges could be used for paying the 
content providers who do not carry ads. An option could be 
that, upon reading an article, a user could click on the Pay 
button. The users’ contributions could be distributed among 
the content providers they choose to pay.
As individuals, we are helpless in avoiding giving data 
to advertisers. So, if I consider all the above scenarios, I 
believe that a major factor in our relying on ‘free’ sites has 
By: Dr Anil Seth
The author has earned the right to do what interests him. 
You can find him online at http://sethanil.com, http://sethanil.
blogspot.com, and reach him via email at anil@sethanil.com.
been because the ISPs just did not live up to expectations 
— either by not offering us options or like AOL or trying to 
restrict what we can do.
Internet based applications have been developing very 
rapidly. But there is still time to take control of our lives 
online. If we do not wish all our data to be owned by just a 
few companies, mostly in the US, our ISPs and regulators 
need to take the initiative and create an environment in 
which competition grows. It matters little if you can choose 
between ISPs, if the data destination options remain the 
same few. In the long run, it is just too risky to trust and rely 
on even well-meaning corporates and governments. What 
will they do with your data when their income levels drop to 
less than they expected? 

Q How has PostgreSQL evolved over the years?
Postgres has evolved from an academic project focused 
on bug fixing to an enterprise project the first added SQL 
standards then emphasized adding enterprise-class features. 
It is currently in the innovation stage where we are adding 
features no other database has. Now begins the Golden Age 
of Postgres when people use it because it does things no 
other database can.
Q Who drives the rollout process at Postgres? Is it solely 
driven by the community, or is there any role played by the 
EnterpriseDB team?
The rollout of PostgreSQL updates are solely driven by 
community members. However, inputs are taken from any 
user or organisation who wants to provide suggestions and 
feedback. EnterpriseDB supports the rollout of PostgreSQL 
releases by hosting software downloads and providing time 
“PostgreSQL 
updates are 
solely driven by 
its community 
members”
The world of databases is taking on 
a new shape with the growth of Big 
Data. But how can an open source 
solution deliver a competitive edge 
against proprietary offerings? 
Bruce Momjian, co-founder of 
PostgreSQL Global Development 
Group and senior database 
architect at EnterpriseDB, 
reveals this secret in an exclusive 
conversation with Rahul Chopra 
and Jagmeet Singh of OSFY. 
Edited excerpts...
for key employees to spend on community work during 
release cycles. 
Q Which would be the closest open source 
competitor for Postgres?
I do not think there is a competitor for Postgres. Several 
people leave MySQL because they prefer Postgres, but very 
few go the other way. The only real advantage MySQL 
has over Postgres is its install base and ISV applications. 
However, MySQL is not developed by an open source 
community. It is an open source option developed by Oracle..
Q What do you think are the key features of Postgres that 
keep it a step ahead of MySQL?
By any measure—features, performance, usability and 
reliability—Postgres is ahead of MySQL and has been for 
years. I think the only advantage MySQL has is its existing 
For U & Me
Interview
18 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

staff skills and pre-built applications that require it.
Q How can an existing MySQL user move to Postgres?
Because of its enterprise-class feature set, PostgreSQL is 
more like the larger, proprietary database solutions than 
MySQL, which has far less functionality. Most MySQL 
users would need to learn more about how to use a full-
featured database management system like Postgres, and to 
unlearn some of the non-standard features of MySQL. 
Q Is it easy for someone using a commercial solution like, 
say, Oracle or Microsoft SQL Server, to migrate to Postgres?
Yes, it is easy. Postgres follows the SQL standard very 
closely, and commercial databases have implemented SQL 
standard features over time. People migrate from those 
database solutions to Postgres every day.
However, administration of Postgres is different 
from those databases. This is the reason some training is 
recommended to perform the entire migration process.
Q Is EDB Postgres Advanced Server by EnterpriseDB distinct 
from the traditional version that is available as a free and 
open source solution?
Yes. EDB Postgres Advanced Server enhances the 
community Postgres with performance, security, 
manageability and developer capabilities to better meet the 
needs of enterprise users and EnterpriseDB customers. 
Q Why would enterprises pick EnterpriseDB’s EDB Postgres  
Advanced Server rather than any proprietary solution today? 
EDB Postgres Advanced Server brings together the 
enhanced version of Postgres, compatibility for Oracle, and 
multiple tool suites for managing Postgres in a mission-
critical, high availability architecture. EDB Postgres 
Advanced Server is the heart of the EDB Postgres Platform  
EDB Postgres Advanced Server allows for the safe and 
efficient deployment of Postgres in enterprises.
Q What is the contribution of the open source community to 
the PostgreSQL project?
Postgres is completely developed by an open source 
community. Some community members are employed 
by companies that use, support or sell solutions based on 
Postgres, but they are community members first.
Q Where does India stand in the list of Postgres contributors?
India has many people employed by the Postgres-enabled 
companies. But we presently do not have many non-affiliated 
contributors to Postgres from India.
Q Does it help to have local communities of developers, or 
is it good enough to have just users?
We like to drive what we call an ‘echo effect’, where 
engineers get signals from multiple directions to deploy 
Postgres. That does not happen if just the government says 
something or the industry says something. What we need is an 
ecosystem, where everything feeds on everything else.
The idea of having a monthly meeting in a large city like New 
York, Mumbai or New Delhi is incredibly important because 
it tells the developer community that this is where the action 
is, and this is where the future jobs are going to be.
Having a user group is good, but we need developers, 
companies and the government to create the ‘echo effect’ and 
say that this is what you need today.
Q What is the secret behind the Postgres community 
working together so efficiently?
To make your community happy to work for you, you need to 
maintain a culture of service.
There are mainly five to seven types of leaders. You can 
find a visionary leader like Steve Jobs, an institutional leader 
like Jack Welch, an inspiring leader like Oprah and a servant 
leader like John Willard Marriott. Each of this leadership 
type works in different industries. You probably do not need 
a servant leader in technology, because it does not have much 
human interaction and needs innovation. You, therefore, 
need to have a visionary leader. Similarly, in the service 
industry, you need a servant leader because, effectively, all the 
employees in this segment are servants to their customers.
Leaders need to train people according to their industry. 
People do not perform actions by accident. In fact, they are 
trained to envision, institute or serve the customers and are 
also trained in the necessary skills in a good manner.
If you do not treat your employees well, they are not going 
to treat your customers well. This is like a one-on-one 
reaction. Organisations adopt this method of success. And, 
undoubtedly, this is vital to grow a community.
Everyone has some deficiencies, but when you interact 
with somebody and are able to bring out the best and 
minimise the deficiencies in that person, you succeed in a big 
way. This is what we consider as the Postgres community.
Q Do you see any difference in open source adoption in India 
now compared to when you visited the country first in 2007?
When I came to India for the first time in 2007, the only 
thing I did was go to the EnterpriseDB office. I do not think 
I went to any customers at that time. I do not even know if 
any Indian clients were using Postgres that year. I had spent 
a day alone in Mumbai and then I went to Pune the next day 
to meet the EnterpriseDB employees. That was just a two-
three day trip, mainly targeted to begin my conversations 
For U & Me
Interview
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 19

with the Indian engineers.
But when I landed in India in 2013, things were a 
little different. I went to Tata Consultancy Services (TCS), 
National Informatics Centre (NIC), Centre for Development 
of Advanced Computing (C-DAC), Infosys and Wipro. Those 
meetings were ‘tyre kicking’ exercises for us. People had no 
plans to move from a proprietary solution.
By 2017, the Indian open source space has been majorly 
transformed. NIC is already developing major projects with 
Postgres. So is the case with C-DAC and India-headquartered 
IT companies like Infosys, TCS and Wipro. So, now, we can 
say that people here have started investing in open source.
Q What happens at the EnterpriseDB Open Source Centre in 
Pune? And how critical is it from a global perspective?
We have had our Pune Open Source Centre since 2007. 
Originally, the Pune centre was mostly for technical support, 
engineering and building software, managing training and 
managing financing. So it was initially a big back office for 
us. But within a couple of years, it became clear to our team 
that there was a significant market in India. So we started 
initiating sales and established backend engineering.
Nowadays, the Open Source Centre is not only supporting 
local teams but is also building solutions for the global 
markets. We can say that the growth in Pune is much quicker, 
and there is a lot of talent there to enable engineering, 
support, training components, sales and marketing.
Q How do you see the entrance of Microsoft SQL Server in 
the world of open source?
Microsoft has fought against open source for a long time, 
but it is good to see that its management team has seen the 
error of its ways. I am unclear what impact the addition of 
Microsoft SQL Server on Linux will be. However, it still has 
the baggage of being a proprietary database.
Q What is next in Postgres?
The next release is coming out in September or October of 
this year. This version will be Postgres 10 and will include 
big things like logic replication, increased parallelism and the 
initial building blocks for sharding. Moreover, it should have 
220-230 new features. 
Q What, do you think, is the future of open source database 
management systems?
Data storage needs are changing rapidly as applications 
become more demanding. Open source databases are in an 
ideal position to lead innovation by integrating data from 
different data storage solutions and process using a distributed 
architecture. I think Postgres has already started in that area. 
Q In this entire journey, was there ever a make or break 
situation for the team? 
When I am with the core team members, I am always paranoid 
and have fears like what are all the risks, how could we get 
exposed and what are the terrible things that can happen? Just 
a year ago, I used to have thoughts that some large companies 
would come along and just hire all our key engineers. But, now, 
I do not think this is going to happen in the future. We have a 
good distribution of companies, we are not exposed to only one 
or two firms, and we have a large number of volunteers.
I was also scared in the recent past that some of our 
assets would get compromised and some of the important 
credentials would be leaked. Likewise, patents are my major 
point of concern. It is not an issue in India, but a vital thing in 
the US. Software patents in the US market are so broad that it 
is difficult to know whether you are infringing or not.
Having said that, I am not sure that I see any risks now, in 
working on Postgres.
Q Lastly, what would be the key learning that you would 
like to share with other communities trying to drive their 
own open source projects?
There was a six-month period years ago, when I kind of backed 
out of all my projects. I did not get involved in emails and would 
get worked up when I saw unusual things happening in the open 
source world. I felt then that the harmony that we had in the past 
had disintegrated, and I could not bring myself to re-engage with 
the community. What I learnt from that experience is that the 
leadership of the group has to really set a tone that is balanced.
Russian author Leo Tolstoy said, “All happy families are 
alike; each unhappy family is unhappy in its own way.” A 
happy open source project really has to have all the things in 
all the right measures to really thrive.
A lot of open source communities have been 
dysfunctional. Sometimes they become dysfunctional because 
their leaders get too strong, or sometimes because there is a 
lack of structure and the individuals start fighting against each 
other. That can become a major problem for growth.
You need to set a tone where everyone in the community 
feels valued and considers that all their contributions matter, 
and that this is part of something that is important to their lives.
People can only contribute to a project in a community if 
they feel like this is the best use of their time, and this is what 
they can do well in their life. That happens only when you 
have the kind of community we have, which makes people 
feel this way, very effectively.
So, the more you empower people, the more dedication 
you will receive from them to uplift the community. 
For U & Me
Interview
20 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 21
Admin
Overview
This is the era of Big Data and these are undoubtedly revolutionary times. Massive 
amounts of data are being generated by the hour, from social media and from 
enterprises. It would be extremely foolish to waste this treasure trove by simply doing 
nothing about it. Enterprises have learnt to harvest Big Data to earn higher profits, offer 
better services and gain a deeper understanding of their target clientèle.
B
ig Data basically refers to the huge amounts of data, 
both organised and unorganised, that enterprises 
generate on a day-to-day basis. In this context, the 
volume of data is not as relevant as what organisations do 
with the data. Analysis of Big Data can lead to insights that 
improve strategic business decision-making.
The importance of Big Data
As mentioned earlier, the value of Big Data does not 
depend on how much information you have, but on what 
you are going to do with it. You can harvest data from 
any point and examine it to find solutions that enable the 
following four things:
 
Price reductions
 
Time reductions 
 
Fresh product development and modified offerings 
 
Making smart judgements 
When you pool Big Data with high-energy analytics, the 
following business-related tasks are possible:
 
Identifying reasons of failures, issues and flaws in real-time.
 
Generating vouchers at the point-of-sale based on the 
customer’s purchasing history.
 
Calculating the full risk of certain functions within 
minutes.
 
Detecting deceitful behaviour before it impacts your 
organisation.
Examples of Big Data
The automotive industry: Ford’s modern-day hybrid Fusion 
model yields up to 25GB of data per hour. This data can 
be used to interpret driving habits and patterns in order to 
prevent accidents, deflect collisions, etc.
Entertainment: The video game industry is using Big 
Data for examining over 500GB of organised data and 4TB of 
functional backlogs, each day.
The social media effect: About 500TB of fresh 
What Does  
Big Data Mean to You?

22 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
Overview
data gets added into the databases of social media site 
Facebook daily.
Types of Big Data
Big Data can be classified into the following three main 
categories.
1. Structured: Data that can be stocked, approached and 
refined in the form of a fixed data format is termed as 
structured data. With time, computer science has been 
able to develop methods for running with such data and 
also deriving value out of it. Nevertheless, these days, we 
are anticipating issues related to the sheer volume of such 
data, which is turning into zettabytes (1 billion terabytes 
equals 1 zettabyte).
2. Unstructured: Data in an unmapped form is known as 
unstructured data. Large volumes of unstructured data 
pose many challenges in terms of how to derive value 
out of it. For example, a heterogeneous data source, 
incorporating a collection of simple text files, pictures, 
audio as well as video recordings, will be difficult to 
analyse. These days, organisations have an abundance 
of data available to them, but unfortunately they don’t 
know how to extract value out of it since this data is in 
an unprocessed form.
3. Semi-structured: This can comprise both forms of data. 
Also, we can consider semi-structured data as a structure 
in form, but in reality, the data itself is not defined, e.g., 
data depicted in an XML file.
The four Vs of Big Data
Some of the common characteristics of Big Data are 
depicted in Figure 2.
1. Volume: The volume of data is an important factor in 
deciding on its value. Hence, volume is one property that 
needs to be considered while handling Big Data.
2. Variety: This refers to assorted data sources and 
the nature of data, both structured and unstructured. 
Previously, spreadsheets and databases were the only 
origins of data considered in most of the practical 
applications. But these days, data in the form of e-mails, 
pictures, recordings, monitoring devices, etc, are also 
being considered in investigation applications.
3. Velocity: This term refers to how swiftly data is 
generated. How fast the data is created and refined to 
meet a particular need, determines its real potential. 
The velocity of Big Data is the rate at which data flows 
from sources like business procedures, application logs, 
websites, etc. The speed at which Big Data flows is very 
high and virtually non-stop.
4. Veracity: This refers to the incompatibility between 
the various formats that the data is being generated in, 
thus constraining the process of mining or managing 
the data profitably.
Big Data architecture
Big Data architecture comprises consistent, scalable and 
completely computerised data pipelines. The skillset needed 
to build such infrastructure requires a deep knowledge of 
every layer in the heap, starting with a cluster design to 
setting up the top chain responsible for processing the data. 
Figure 3 shows the complexity of the stack, along with how 
data pipeline engineering touches every part of it. 
In this figure, the data pipelines collect raw data and 
transform it into something of value. Meanwhile, the Big Data 
engineer has to plan what happens to the data, the way it is stored 
in the cluster, how access is approved internally, what equipment 
to use for processing the data, and finally, the mode of providing 
access to the outside world. Those who design and implement 
this architecture are referred to as Big Data engineers.
Big Data technologies
As we know, the subject of Big Data is very broad and 
permeates many new technology developments. Here is 
an overview of some of the technologies that help users 
monetise Big Data.
1. MapReduce: This allows job implementation, with 
scalability crossing thousands of servers.
• 
Map: Input dataset transforms into a different 
set of values.
• 
Reduce: Many outputs of the Map task are united to 
form a reduced set of values.
BIG DATA
FUNDAMENTALS
Mobile
Databases
Trends
Technology
Discovrey
Strategy
Management
Applications
Intelligence
Analytics
Privacy
Internet
Networks
Information
Security
Storage
Infrastructure
Traabytes to
exabytes of existing
data to process
Volume
Velocity
Variety
Veracity
Streaming data,
milliseconds to
seconds to respond
Structured,
unstructured, text,
multimedia
uncertainty due to
data inconsistency
& incompleteness,
ambiguities, latency,
deception, model
approximations
Data at Rest
Data at Motion
Data at Many
Forms
Data at Doubt
Figure 1: Fundamentals of Big Data
Figure 2: Characteristics of Big Data

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 23
Admin
Overview
By: Meghraj Singh Beniwal
The author has a B. Tech in electronics and communication. 
He is a freelance writer and an Android app developer. He 
currently works as an automation engineer at Infosys, Pune. 
He can be contacted at meghrajsingh01@rediffmail.com or 
meghrajwithandroid@gmail.com.
2. Hadoop: This is the most admired execution of 
MapReduce, being a completely open source platform 
for handling Big Data. Hadoop is flexible enough to be 
able to work with many data sources, like aggregating 
data in order to do large scale processing, reading data 
from a database, etc.
3. Hive: This is an SQL-like link that allows BI 
applications to run queries beside a Hadoop cluster. 
Having been developed by Facebook, it has been 
made open source for a little while and is a higher-
level concept of the Hadoop framework. Also, it 
allows everyone to make queries against data stored 
in a Hadoop cluster and has improved on Hadoop’s 
functionality, making it ideal for BI users. 
Advantages of Big Data processing
The capability of processing Big Data has various benefits.
1. Businesses can make use of outside brainpower while 
taking decisions: The right to use social data from 
search engines and websites like Facebook and Twitter is 
enabling enterprises to improve their business strategies.
2. Enhanced customer service: Customer response 
systems are getting replaced by new systems intended for 
Big Data technologies. Within these new systems, Big 
Data technologies are being utilised to read and assess 
consumer responses.
3. Early recognition of risks for the services: Risk factors 
can be recognised beforehand to deliver the perfect data.
4. Improved operational competence: Big Data 
technologies can be utilised for building staging areas 
or landing zones for new data, prior to deciding what 
data should be moved to the data warehouse. Also, 
such incorporation of Big Data and data warehousing 
technologies helps businesses to bypass data that is not 
commonly accessed.
The challenges
Though it is very easy to get trapped in all the hype 
around Big Data, one of the reasons it is so underutilised 
is that there are many challenges still to be resolved in the 
technologies used to harness it. Some of these are:
1. Companies face problems in identifying the 
correct data and examining how best to utilise it. 
Constructing data-related business cases frequently 
means forming opinions out-of-the-box and looking 
for income models that are extremely different from 
the traditional business model.
2. Companies are reluctant to choose the fine talent that 
is capable of both working with new technologies and 
examining the data to find significant business insights.
3. A bulk of data points have not been linked yet, and 
companies frequently do not have the correct platforms 
to combine and manage the data across the enterprise.
4. The technology in the data world is evolving very fast. 
Leveraging data means functioning with well-built, 
pioneering technology collaborators – companies that 
can help create the right IT design so as to adapt to 
changes in the landscape in a well-organised manner.
The accessibility of Big Data, inexpensive product 
hardware, and new information managing and analytics 
software have come together to create a unique moment 
in the history of data analysis. We now have the capability 
that is necessary to examine these amazing data sets 
rapidly and cost-effectively, for the first time in history. 
This ability symbolises an authentic leap forward, and a 
chance to enjoy massive improvements in terms of work 
productivity, income and success. 
Processing
Physical Hardware
Valu of Data
Data Pipeline
Insight
Data
Ingest
Staging
Data & Worklow
Management
Stack Complexity
Access
Hadoop Framework
Figure 3: Architecture of Big Data
Your favourite Magazine on Open 
Source is now on the Web, too.
OpenSourceForU.com
THE COMPLETE MAGAZINE ON OPEN SOURCE
Follow us on Twitter@LinuxForYou

24 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
Overview
W
ith the 
advent of 
DevOps 
and cloud 
technologies, 
customers are 
experimenting more 
and more with multi-
platform, multi-cloud 
or multi-OS based 
applications. This 
shift in customer 
demand and set-up 
environments has 
led to a new set 
of challenges for 
developers and IT 
administrators who 
manage such complex 
infrastructure, as the 
code written in one platform needs to be supported on other 
platforms, too. In such scenarios, customers also need to look 
for tools that are interoperable across several platforms, and 
ensure faster and continuous business delivery. 
MS Azure is a growing collection of integrated cloud 
services that developers and IT professionals use to build, 
deploy and manage applications. With one in every three 
virtual machines at MS Azure hosting Linux, Microsoft 
realised the need to adopt an open source, customer-centric 
approach to deliver services. As a first step in that direction, 
MS announced that it had made PowerShell open source and 
also made it supportable on the Linux and Mac operating 
systems. Currently, the support is provided for Linux flavours 
like RHEL, Centos and Ubuntu.
PowerShell is a task-based command-line shell and 
scripting language built on the .NET framework to help 
IT professionals control and automate the administration 
of the Windows, Linux and Mac OSs as well as the 
applications that run on them. As PowerShell is built on 
top of the .NET framework, .NET Core has been ported to 
Linux. This enables customers to use a single management 
stack to manage all workloads. Now, developers and IT 
administrators can experience a rich interactive scripting 
language, as well as heterogeneous automation and 
configuration management that just works perfectly. Also, 
MS is partnering with 
a number of industry-
leading third-party 
companies like Chef, 
Amazon Web Services, 
VMware and Google, to 
name a few – to create 
a seamless experience 
across all platforms for 
end users.
Installing 
PowerShell
PowerShell has been 
open sourced to GitHub 
under the MIT licence 
(https://github.com/
PowerShell/PowerShell). 
Installing PowerShell has 
been made easy. 
To install PowerShell on Ubuntu 14.04 and above, 
navigate to https://github.com/PowerShell/PowerShell/
releases/tag/v6.0.0-alpha.17 and download the Debian 
package for the respective Ubuntu OS version (for example, 
if the Ubuntu version is 16.04 then download the package 
powershell_6.0.0-alpha.15-1ubuntu1.16.04.1_amd64.deb). 
Open a terminal and run the following commands:
sudo dpkg -i powershell_6.0.0-alpha.15-1ubuntu1.16.04.1_
amd64.deb
sudo apt-get install –f
To install on CentOS 7, give the following command:
sudo yum install ./powershell-6.0.0_alpha.15-1.el7.centos.
x86_64.rpm
…or type the following:
sudo yum install https://github.com/PowerShell/PowerShell/
releases/download/v6.0.0-alpha.15/powershell-6.0.0_
alpha.15-1.el7.centos.x86_64.rpm
Running PowerShell without installation
If you wish to play around with PowerShell and not 
PowerShell Open-Sourced!
PowerShell is an automation and configuration framework from Microsoft. It 
consists of a command-line shell and an associated scripting language based on 
the .NET framework. Microsoft recently open sourced PowerShell, and made it 
supportable on the Linux and Mac operating systems.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 25
Admin
Overview
PowerShell can be used as the go-to scripting language to 
perform administrative tasks. 
The benefits PowerShell offers to users are:
1. Support for running heterogeneous workloads
2. Interoperable tools for scalability
3. Native Linux management tasks using PowerShell [better 
error handling]
4. Support for a broad range of programming needs 
(interactive shell, scripting, system programming, etc) 
covered under a single umbrella
5. Optimised for structured data, as the workflow is 
object based
PowerShell is a new addition to the Linux environment. 
The use cases are not limited to the ones listed in this 
article. By open sourcing it, Microsoft has paved the way 
for Linux sub-systems to use MS based cloud products like 
Azure Portal, etc. Slowly, this will lead to standardisation 
between various OSs, as Bash and PowerShell will be 
supported across different operating systems. 
Figure 1: Available commands in PowerShell
Figure 2: Piping in PowerShell
install it on your system, then download the AppImage 
from https://github.com/PowerShell/PowerShell/releases/
tag/v6.0.0-alpha.17 (PowerShell-x86_64.AppImage). 
Follow the two steps given below. AppImage is a portable 
application, which contains the bundled PowerShell and 
dependencies:
chmod a+x PowerShell-x86_64.AppImage
./PowerShell-x86_64.AppImage
Alternatively, one can download the source code by 
running the following command: 
git clone --recursive https://github.com/PowerShell/
PowerShell.git
If you are interested in contributing to PowerShell, refer 
to the GitHub link https://github.com/PowerShell/PowerShell/
tree/master/docs/git.
Let’s run some cmdlets
Once the installation is successful, open a terminal and 
enter ‘powershell’. This will open the PowerShell prompt in 
the same terminal. The first cmdlet we run will give us the 
PowerShell version information.
Let’s try to find the number of cmdlets available with this 
alpha build of PowerShell. Currently, there are 349 cmdlets 
ported and more are on the way (refer to Figure 1).
Figure 2 gives another example where we pipe the result 
just like in Linux scripting.
Uninstalling PowerShell
Just run the following command to uninstall PowerShell:
Sudo apt-get remove powershell
Everything in PowerShell is cmdlets (verb-noun 
semantics)—for example, Get-Commands, Get-Process, 
Clear-Host, Set-Location, etc. Another good thing about 
PowerShell is that most of the Linux commands are 
already ported into it.
PowerShell vs Bash
The differences between the Bash shell and PowerShell 
are more with regard to the end-application use cases. 
Both are very strong and feature-rich automation 
languages but are inherently different in the way they 
perceive and handle the inputs and outputs. PowerShell 
deals with structured data in the form of objects, which 
can be pipelined and passed over to different commands, 
while the Bash script deals more with a bunch of strings 
that are not formatted or structured.
Linux native management based tasks are slowly 
being ported to PowerShell. Once this process is complete, 
By: Vinay Patkar and Shubhra Rana
Vinay Patkar works as a software development engineer at 
Dell India R&D Centre, Bengaluru, and has close to two years’ 
experience in automation and the Windows Server OS. He is 
interested in virtualisation and cloud computing technologies.
Shubhra Rana is a software development engineer at Dell 
India R&D Centre, Bengaluru, and is interested in data mining, 
network security and cryptography.

26 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
Let's Try
If you need a minimal and scalable distributed object server, then your best bet is Minio. 
You can use it to store all your unstructured data such as photos, videos, container/VM 
images, log files and archives. Delve into this article to know more. 
An Introduction to Minio
work seamlessly on your local systems and the cloud. 
So, let’s get started with setting up Minio on your local 
machine and writing an application to integrate with it.
Running a Minio server
There are a couple of ways in which you can run Minio. You 
can either download the binaries for your operating system/
architecture from the Downloads page or you can use the 
Minio Docker image, if you are using Docker. We will go 
with the former option.
Go ahead and download the Minio Server distribution 
that applies to you. In my case, I downloaded a Windows 
build. From the Downloads page, you can get both the server 
(minio.exe) and its command-line client (mc.exe). Ensure that 
the binaries are available in the PATH.
While starting the Minio server, the key thing is to 
provide it a PATH value that points to the directory where 
W
e usually tend to think of databases in terms 
of structured data like tables/columns or even 
attributes. However, modern applications generate 
lots of data that cannot be easily classified into structures. 
Instead, it exists in the form of blobs, objects or files. As 
an example, a blogging server may use a SQL database for 
posts and authors, but it also has various other media files 
like videos, images, sound files, etc. Now, imagine a scenario 
where you are responsible for storing away backup files or log 
files in a secure manner and making them accessible. 
Minio is an Apache licensed open source distributed 
object server that can help you here, and do a lot more. 
Written in the Go language, it has a minimal design and the 
goal is to get you started quickly. It is a great option with 
which to introduce an Object Store into your application. 
Minio is designed to be compatible with Amazon S3, which 
makes it easy to have the same API interface and make it 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 27
Admin
Let's Try
Figure 1: Minio access page
Figure 2: Create bucket option
Figure 3: List of buckets
Minio can persist the objects. While there are multiple 
disks and other options that you can provide while starting 
it, we will go with the basic use case, i.e., use one folder 
(directory) to store our persistent data. This can be any 
accessible drive on your machine. In my case, I chose to go 
with the d:\minio-data folder.
Starting the Minio server is as simple as using the server 
sub-command for the Minio executable and providing the 
PATH, as shown below: 
D:\minio>minio server d:\minio-data
Endpoint:  http://169.254.224.241:9000 
http://10.21.11.176:9000 http://169.254.68.108:9000 
http://169.254.83.68:9000 http://192.168.99.1:9000 
http://127.0.0.1:9000
AccessKey: MDMU09WUXO3ZEFDKSA5B
SecretKey: 6V/xJLyW1XTL78kq9Jl3kuGoqnZpJ82at1t/wBaR
Region:    us-east-1
SQS ARNs:  <none>
Browser Access:
   http://169.254.224.241:9000 http://10.21.11.176:9000 
http://169.254.68.108:9000  http://169.254.83.68:9000  
http://192.168.99.1:9000  http://127.0.0.1:9000
Command-line Access: https://docs.minio.io/docs/minio-client-
quickstart-guide
   $ mc.exe config host add myminio 
http://169.254.224.241:9000 MDMU09WUXO3ZEFDKSA5B 6V/
xJLyW1XTL78kq9Jl3kuGoqnZpJ82at1t/wBaR
Object API (Amazon S3 compatible):
   Go:         https://docs.minio.io/docs/golang-client-
quickstart-guide
   Java:       https://docs.minio.io/docs/java-client-
quickstart-guide
   Python:     https://docs.minio.io/docs/python-client-
quickstart-guide
   JavaScript: https://docs.minio.io/docs/javascript-
client-quickstart-guide
Drive Capacity: 64 GiB Free, 304 GiB Total
On successfully starting up, you should note the server 
access key and the secret key that are provided. You will 
need these to access the Web user interface that Minio also 
provides. Additionally, if you are looking to use the Minio 
API to integrate into your applications, you will need these 
key values; so, keep them handy and secure. 
Accessing the Minio server
In the startup log shown in the console, you will notice 
multiple URLs. Since we are using the same machine, 
we can go with localhost for accessing the Minio server. 
Simply launch a browser and visit the http://localhost:9000 
address. This will bring up the page as shown in Figure 1.
Provide the access key and the secret key, after which 
you are good to go. Minio organises the objects into a 
collection of buckets. A bucket can be conceptually thought 
of as a folder that holds a collection of objects.
Click on the + sign at the right bottom to get an option 
to either create a bucket or upload a file (Figure 2).
Select the Create bucket option and give it a name, e.g., 
backups. This will create a bucket in the list of buckets 
shown in the left pane.
You can now use the interface itself to add files to the 
specific bucket. This structure is required because Minio is 
also compatible with cloud storage services like Amazon S3; 
so, it becomes easier for it to synchronise with the same. 
Think for a while about how the process of setting up 
and running Minio is a straightforward exercise. Imagine if 
you set this up on your local network for your personal use 
or even for your team/enterprise. You can immediately start 
accessing the server and working with buckets and files. 
You will definitely need to provide the Object Store in 
your applications. For example, it could be an extension to 
your existing application, using which people can upload 
files that Minio will store and manage. Other use cases 
Create bucket

28 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
Let's Try
could be taking backups of existing databases or folders 
and then storing them in Minio. The possibilities are 
endless, and I recommend readers take a look at the Minio 
community projects page to see how Minio has been 
innovatively integrated as part of larger projects. 
Minio client SDKs
If you wish to interface with Minio via one or more 
programming languages, there are SDKs for Java, Python, 
JavaScript, Go and others. The SDKs are well designed 
wrappers around the Minio REST APIs. The pattern of 
using any Minio client SDK is straightforward—you 
establish a client connection to the Minio server and then 
perform one or more commands that are applicable. The 
commands could include listing/creating buckets, listing/
uploading file objects, and so on.
A simple backup utility
Let us think of how we can potentially use Minio as 
a store to retain multiple backups. These could be 
particular directories that you want to back up, certain 
files (log files) or even database dumps that you need to 
save for restore or audit purposes. 
To use the API, as mentioned earlier, you will need to 
have the hostname and port of your Minio server along 
with the access and secret key. Once you have all this, 
we can establish a connection to the Minio client and 
then perform the operation. In this case, we will simply 
upload a standard file object into a bucket mybackup that 
we have pre-created.
We shall use the Minio Python SDK for this purpose. 
I have modified the code from the official documentation, 
and the code assumes that you have a bucket named 
mybackup in the Minio server:
from minio import Minio
from minio.error import ResponseError
minioClient = Minio('localhost:9000',
                  access_key='YOUR_MINIO_ACCESS_KEY',
                  secret_key='YOUR_MINIO_SECRET_KEY',
                  secure=False)
print(minioClient)
try:
    minioClient.fput_object('mybackup', 'mydb.log', 'mydb.
log')
except ResponseError as error:
    print(error)
The code first creates the Minio client object. 
Remember to use your access details in the parameters. 
It then assumes that you have a bucketname created in the 
Minio server named mybackup and a file mydb.log in the 
current directory that you want to upload. The second and 
third parameters in the fput_object method stand for object 
name (when stored in Minio server) and the file path (where 
the file can be found by the Python program).
To run the Python app, first ensure that you have 
downloaded the Minio Python library as given below:
pip install minio
Minio client
The Minio client (mc) is a command line utility that 
provides an alternative to UNIX commands like ls and cp, 
with the interesting twist that it works for both your local 
system as well as the Amazon S3 service in a seamless 
manner. You can check out https://docs.minio.io/docs/
minio-client-complete-guide for more details.
Contributing to Minio
Minio prides itself on being open source, and a quick look 
at the GitHub page shows an active project with multiple 
contributions from developers. The Minio server and 
client are written using the Go programming language and, 
hence, contributors need to be familiar with this language. 
You can also look at the client SDKs available in multiple 
languages, if you wish to contribute in that way. 
You can also contribute to the growing list of Minio 
community projects that are listed in the community page. 
A quick glance at how multiple projects have used Minio is 
sure to inspire you with some ideas.  
The creators of Minio have focused on minimalism and 
providing well designed APIs that make it easy for you to 
customise it to your needs. This, coupled with the fact that 
Minio is available on multiple target platforms, allows you 
to choose your hardware and scale, as needed. 
By: Romin Irani 
The author has been working in the software industry for 
more than 20 years. His passion is to read, write and teach 
about technology, and make developers successful. He blogs 
at www.rominirani.com.
[1] Minio Home Page: https://www.minio.io/ 
[2] Minio Downloads: https://www.minio.io/
downloads/#minio-server 
[3] Minio Client APIs: https://www.minio.io/
downloads/#minio-sdk 
[4] Minio Blog: https://blog.minio.io/ 
[5] Minio GitHub Page: https://github.com/minio/ 
[6] Minio Community Projects: https://github.com/minio/
awesome-minio
References

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 29
Admin
How To
MariaDB is one of the most popular database servers made by the original 
developers of MySQL, and has a strong developer and user community. Rugged 
as MariaDB is, getting the best out of it requires performance tuning. Read how to 
install MariaDB and tune it for better performance.
Get the Best Out of MariaDB 
with Performance Tuning
supported release by the MariaDB team. It has a lot of new 
features such as Galera cluster and performance improvements. 
Ubuntu 16.04 repositories have the 10.0 version, which is fine 
if you are not interested in the new features. To install 10.1, 
you have to add MariaDB’s repository to your aptitude sources.
list file, which is available at the repositories page located at 
https://downloads.mariadb.org/mariadb/repositories/.
I am choosing the DigitalOcean mirror in my 
example below:
sudo apt-get install software-properties-common
sudo apt-key adv --recv-keys --keyserver hkp://keyserver.
ubuntu.com:80 0xF1656F24C74CD1D8
sudo add-apt-repository 'deb [arch=amd64,i386,ppc64el] 
http://lon1.mirrors.digitalocean.com/mariadb/repo/10.1/ubuntu 
xenial main'
After adding the repository, fire this command to install 
the MariaDB server:
sudo apt update
sudo apt install mariadb-server
Aptitude will download and install the server for you, and 
M
ySQL is one of the most popular open source 
databases in the world of technology. Earlier, 
MySQL was an independent company, which offered 
open source as well as commercial databases. Later, Sun 
Microsystems acquired MySQL, and then Oracle acquired 
Sun. When Oracle acquired Sun, the original MySQL 
developers forked a project called MariaDB, which was 
supported by a company called SkySQL. They took this step 
because they had fears about MySQL becoming closed source 
after its acquisition by Oracle.
After this whole saga, many Linux distributions started 
packaging MariaDB as the default MySQL server. Some 
distributions provide both MySQL (Oracle) and MariaDB, 
and leave the choice to the users.
Just like any other open source project, MariaDB is ahead 
of MySQL in many aspects; it has some enhanced features 
and usually goes through a faster release cycle – it brings out 
new features and bug fixes faster than MySQL.
MariaDB installation on Ubuntu 16.04 server
As of now, MariaDB maintains two versions, which are 
suitable for and usable in a production environment. These are 
10.0 and 10.1. The 10.0 version is MySQL 5.5 with backported 
features from MySQL 5.6, whereas 10.1 is the current 

30 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
How To
Performance tuning
Database performance tuning is a vast topic and depends highly 
on the workload of the application. Yet, there are some common 
principles that can be applied to every case. To begin with, check 
the hardware of your server. Databases require a lot of RAM 
and fast disk IO. The more the RAM and the faster the disk, the 
happier your database server is. Tools such as PHPMyAdmin 
(Web application) and mysqltuner (command line application) 
provide insights on server performance and suggested parameter 
tuning to achieve better performance.
It is highly recommended to have storage such as SSD in 
RAID 10 mode for achieving high throughput with a large 
database system. Just switching from a traditional rotational 
hard drive to SSD can improve the response times of the server 
significantly. 
To achieve good throughput from a database, it is very 
important to design the database schema and use optimal queries. 
For example, if you have a table with a user’s details such as name, 
email address, user name and you are running a query which 
searches for the user name without having an index on the user 
name field, then the performance will be bad because the database 
server has to do a full table scan for every such query.
A full table scan is one in which all the rows have to be 
scanned before arriving at a result. Badly designed queries, 
especially joins, can cause severe performance hits. MySQL 
and MariaDB use the nested loop algorithm for computing 
the result of joins. A nested loop algorithm works something 
like this: let us say you have three tables (t1, t2, t3) for which 
a combined result is required. First, the optimiser decides the 
driving table, and the next table in the join is queried based on 
results received from the driving table. This result set forms 
the base, using which the third table’s data is matched. In 
mathematical terms, this is known as a Cartesian product.
Use caching
MySQL and MariaDB have a query cache, which can provide 
a significant boost in performance. It is useful in cases with less 
writes and mostly reads. If there are a lot of writes happening, then 
the server will be spending more time managing the cache instead 
of working on queries. For this reason it is not recommended 
to have a large size for the built-in MySQL query cache. Up to 
512MB is sufficient. For more control the application should use 
its own caching such as Memcached or Redis.
Use EXPLAIN
When designing queries for the system, it is important to have 
optimal queries. MySQL will provide details of the execution 
plan of a query given to EXPLAIN. Optimizer trace is also 
available, which can be used to explore why one plan was 
chosen instead of the other. Optimizer trace is used by MySQL 
internally, but can be used while designing queries as well. 
To use optimizer trace, type:
mysql> SET optimizer_trace=”enabled=on”
will ask for the server’s root password during set-up. Provide 
the same and keep it secure.
The storage engines available in MariaDB 10.1
Storage engines form the most important part of MySQL and 
MariaDB. They decide how and where the data will be stored 
and provide features for data management. There are many 
storage engines supported by MariaDB, but we will discuss 
only the ones that are most commonly used.
Transactional engines: InnoDB, which originally came 
with MySQL, is the transactional engine by default for 
MySQL. MariaDB has a fork known as XtraDB as the default 
transactional engine; XtraDB is developed and maintained 
by Percona. XtraDB is also available as part of the Percona 
server. InnoDB/XtraDB is the most used ACID-compliant 
engine. It is safe and good for most use cases.
InnoDB is not known to be easy to backup. If you 
are using XtraDB, there is a tool from Percona known as 
xtrabackup that can be used for backing up XtraDB data. 
The use of this tool is related to the way data is stored by the 
engine. One can always use the good old mysqldump utility 
but it strains the CPU, as every row has to be translated 
into proper SQL statements so that it can be reloaded at the 
backup server – and the SQL statements have to be translated 
back to real data and also recreate any indices. On small size 
databases mysqldump is sufficient but for larger ones it is 
better to use xtrabackup. Replication to another server and 
taking backups there is better, too.
There is yet another ACID-compliant transactional 
engine present in MariaDB, called TokuDB. It is designed 
for storing large amounts of data in smaller spaces – up to 
25x compression can be achieved. The typical use cases for 
TokuDB are applications where a lot of data is queried and 
updated at the same time and in environments where the 
data that is being worked upon at a given instant cannot fit 
in the RAM.
Non-transactional engines: MyISAM is the oldest 
engine in MySQL. MariaDB has an improved version – Aria. 
It has many performance related fixes and better safety, which 
is lacking in MyISAM. One disadvantage of MyISAM is that 
it locks the table when there is an update or insert going on 
– MyISAM does not support MVCC while Aria has it in its 
roadmap but is not there yet.
There are two other interesting storage engines – 
SphinxSE and MEMORY. SphinxSE can be used when 
full text search is needed – it relies on the external Sphinx 
search service. The MEMORY engine is useful for storing 
data purely in the RAM – there is no persistence of the 
data to disk and it will be lost when the server is restarted. 
Throwaway cache data is an ideal candidate to be stored in 
the MEMORY engine.
There are more storage engines supported by MariaDB 
such as Federated and Cassandra, which are not covered here. 
Information about the same is available at mariadb.com.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 31
Admin
How To
gives the optimal log file size to store exactly one minute 
worth of recovery data. To store longer periods of redo log, 
the size can be a multiple of the unit time interval. The status 
variable innodb_log_waits should be checked as well if the 
log size really needs tuning. In many set-ups the log size is 
unnecessarily large, which increases the time required for 
recovery when there is a crash or power failure.
Most of the servers these days have multiple CPUs 
(physically, or as SMP). If the InnoDB buffer pool size is more 
than a gigabyte then it can be divided across the number of 
CPUs using the setting innodb-buffer-pool-instances. The ideal 
value for the number of buffer pool instances is the number of 
CPUs you have on the server, and it is important to note that 
each instance must be at least 1 GB.
File system tuning
The file system on which database data is stored is an 
important aspect of performance tuning. Most Linux servers 
use ext4 or xfs but without one important setting being 
enabled – noatime. Whenever a file is read, the access time of 
the file is updated. This is of little utility and disabling it will 
yield a lot of performance benefit. Additionally, it is possible 
to use a raw disk as storage in case of InnoDB – the whole 
InnoDB data file can be stored over a raw disk partition. This 
mode is good if you have a dedicated disk and can help to 
avoid double buffering by the file system as well as MySQL.
As of Ubuntu 16.04, it is possible to run ZFS on Linux. 
Linux file systems by default use the LRU (Least Recently 
Used) algorithm for file system page caching – the oldest page 
is evicted when memory is needed. In contrast, ZFS has ARC – 
Adaptive Replacement Cache, which uses a page replacement 
algorithm that takes into account both recently read blocks 
as well as frequently read blocks— basically a combination 
of LRU and LFU (Least Frequently Used). This does help 
to achieve some performance benefit, but it depends on the 
workload. In addition to ARC, it also supports L2ARC – an 
ARC that can be stored on a memory device which is slightly 
slower than RAM, such as NVMe or SSD. Storing InnoDB 
with default ZFS settings will not provide optimal performance 
– the record size of InnoDB storage should be 16k and that of 
InnoDB log storage should be 128k. In case of the innodb-file-
per-table flag (which is ‘on’ by default), the InnoDB data files 
are created inside individual database directories instead of 
being stored in a single large file. The whole MySQL dataset 
needs to have a record size of 16k in that case.
This covers relatively a large part about MySQL and 
MariaDB performance tuning; the rest depends on the application 
workload. I have successfully configured servers for 1k to 5.5k 
queries per second on an average, using these strategies. 
Setting correct memory parameters
MySQL uses temporary tables a lot when processing complex 
queries that involve joins and sorting. The default size of a 
temporary table is very small and is not sufficient for bigger 
data sets. To set a temporary table size, add the following to 
your my.cnf:
tmp-table-size = 1G
max-heap-table-size = 1G
It is important to note that setting larger values for 
temporary tables means more RAM will be consumed. If 
the system does not have sufficient RAM, then the values 
should not be increased. Mysqltuner can provide information 
on whether the current settings of the server will overrun the 
available RAM or not. A good RAM usage figure is 80-90 per 
cent. Beyond 90 per cent implies operating on the borderline 
and can cause failure, assuming the database is the only 
service on the server. If the same machine is used for other 
services such as the Web, then that needs to be taken care of 
as well. However, it is a bad idea to run the Web and database 
on the same server in large workloads.
Buffer sizes
Two important entities that can be tuned are often missed, 
namely, join buffer size and sort buffer size. These buffers 
are allocated per connection and play a significant role in the 
performance of the system. Join buffer is used, as the name 
suggests, to process joins – but only full joins on which no 
keys are possible. 
Sort buffer size is used to sort data. If your application 
involves a lot of sorting, then this should be increased. 
The system status variable sort_merge_passes will tell you 
whether the value needs to be increased or not. This variable 
should be as low as possible.
InnoDB settings
As stated earlier, the more the RAM, the better the 
performance. The performance of an InnoDB system is 
directly dependent on the size of the data that is present and 
the buffer pool size. A thumb rule is that the size of the buffer 
pool should be at least as large as the data that is being stored 
– this is not feasible in all cases and not required either, since 
not all the data present in the database may be required at the 
same time. But the buffer pool size should be large enough to 
accommodate data that is worked upon frequently.
The InnoDB log file is used for storing a redo log, which 
can be replayed in case of a power failure or database crash. 
If there are a lot of writes in the application, then the size of 
the log file must be increased. The optimal size of the log file 
can be calculated by observing the status variable innodb_os_
log_written. It is a counter (bytes) which gets incremented for 
every byte written to the log file. So the difference between 
the values of the variable in a given time period (60 seconds) 
By: Nilesh Govindrajan
The author is a software engineer based in Pune who 
loves to work on server optimisation. He can be contacted 
at https://nileshgr.com or @nileshgr on Twitter.

Earn up to 
₹1,00,000 
per hour 
 
Curious? Mail us at 
contact@loonycorn.com 
 
 Step 1: You work with us to create a course proposal 
for a 2-10 hour course 
 
 Step 2: We pay you an advance of ₹ 5,000/hour upon 
course approval 
 
 Step 3: You build the course, we help 
 
 Step 4: We grade your work and pay according to the 
rate card below (rates per hour) 
Grade A:  ₹100,000 | B:  ₹50,000 | C:   ₹25,000  | F:  ₹5,000     

Loonycorn 
Our Content: 
 
 The Complete Machine Learning Bundle 
 
10 courses |  63 hours |  $39 
 
 The Complete Computer Science Bundle 
8 courses  |  78 hours |  $39 
 
 The Big Data Bundle 
9 courses  |  64 hours |  $45 
 
 The Complete Web Programming Bundle 
 
8 courses  |  61 hours |  $41 
 
 The Complete Finance & Economics Bundle 
9 courses  |  56 hours |  $49 
 
 The Scientific Essentials Bundle 
 
7 courses  |  41 hours |  $35 
 
 ~20 courses on Pluralsight  
 
~70 on StackSocial 
~65 on Udemy 
 
About Us: 
 
 ex-Google | Stanford | IIM-Ahmedabad | INSEAD 
 50,000+ students  

34 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Let’s Try
MongoDB becomes much more efficient when combined with GridFS. The latter divides 
a document into parts or chunks that are then stored as separate documents. In a 
nutshell, GridFS is a kind of file system used to store files. 
Processing Big Data Using 
MongoDB-GridFS 
T
he rapidly increasing volume and variety of unstructured 
data generates enormous datasets which are difficult 
to analyse and extract further knowledge from. Issues 
related to the storage, processing and knowledge discovery 
from huge amounts of unstructured data are addressed by Big 
Data analytics. There are various applications for which the 
volume, velocity and variety of data increases very frequently, 
and a significant amount of research work is going on to 
understand how Big Data analytics could help in processing 
and understanding such data.
The following list gives a rough idea of why Big Data 
analytics has gained so much importance today:
 
Since the inception of The Indian Railway Catering and 
Tourism Corporation or IRCTC in  2002, online ticket 
bookings have increased from 29 tickets to 1.3 million 
tickets per day.
 
As per recent research at Harvard, the information stored 
in 1 gram of DNA is equivalent to 700 terabytes of data in 
digital format. 
 
According to InternetLiveStats.com, around 10,000 tweets 
are processed per second on Twitter; the Internet traffic 
per second totals more than 42TB; and on YouTube, more 
than 68,000 videos are viewed every second.
 
As per a report on popular statistics portal Statista.com, 
there were more than 1000 million users per day on 
Facebook, as of December 2016. 
MongoDB: A prominent NoSQL database 
for Big Data processing
MongoDB is one of the widely used NoSQL databases 
under free and open source distribution. It is a document-
oriented database written in C++, and a leader in the 
database segment for large scale as well as performance-
aware Big Data based applications. The implementation 
and storage of Aadhaar cards in India is being effectively 
done with the integration of MongoDB.
MongoDB provides a number of modules and 
specifications to support scalable and high performance Big 
Data based applications. These features and modules include 
GridFS, Sharding, Capped Collections, Map Reduce and 
many others. MongoDB can insert more than 200 million 
rows of data in a few seconds.
GridFS—for the storage and retrieval 
of large objects
GridFS is one of the powerful specifications of MongoDB that 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 35
Developers
Let’s Try
helps to store and retrieve large scale files. These files can be 
structured or unstructured and they include documents, audio files, 
images, recorded video clips, binary files, etc. GridFS is similar to 
a file system for the storage of files. MongoDB collections are used 
for storage of data and files using GridFS, which has powerful 
features to store the files of any format, including files that are even 
more than 16MB in size. In classical implementations, there is the 
file storage limit of 16MB but MongoDB-GridFS can store and 
retrieve files beyond this limit too.
Using GridFS, the files are divided into a number of chunks. 
Every single chunk of the dataset is stored in arbitrary but logically 
connected documents, each with the maximum size of 255KB.
GridFS works on two collections called fs.files and 
fs.chunks, for the storage of metadata and a chunk of a file. 
Each chunk is uniquely associated with an ID. The collection 
fs.files acts as the parent document corresponding to the 
chunk. The field files_id in the collection fs.chunks is used to 
link the actual content with its parent.
The format of fs.files collection is:
{
   “filename”: “********”,             // Filename
   “chunkSize”: “********,            // Size of Chunk
   “uploadDate”: ISODate(“”********”),     // Timestamp
   “md5”: “”********”,   // MD5 (Message Digest) Hash of File
   “length”: “********,    // Size of document in bytes
    “contentType”: “********,         // File Type in MIME
   “metadata”: “********    // Additional Information
}
 
The format of fs.chunks collection is:
{
   “ _id”: ObjectId(“”********”,”), // Unique ID of the chunk
   “files_id”: ObjectId(“”***”,”), // Parent ID of the 
document
   “n”: “********”,             // Sequence Number of chunk
   “data”: “”********”,”       // Data in chunk
}
Adding and retrieving large binary 
files using GridFS
In the following example, the storage of a video file will be 
implemented using GridFS. The put command is used for 
this. The utility mongofiles.exe in the bin folder is required 
to be executed. 
First, open the command prompt. Next, change the 
directory to the bin folder of MongoDB. Now start the 
MongoDB server by executing mongod.exe.
Next, execute the following command:
C:\MongoDBDirectory\bin\mongofiles.exe -d gridfs put MyVideo.avi
In the instruction above, mongofiles.exe is the utility for 
executing different commands.
Figure 1: Real-time Big Data cases on InternetLiveStats.com
Figure 3: Portal of the MongoDB NoSQL database
Figure 4: Utilities in the BIN directory of the MongoDB server
Figure 2: Number of active users per day on Facebook
Internet Users in the world
Google searches today
3,581,552,596
3,619,816,785
1,158,565,803
3,380,097
163,674,471,320
461,462,117
Total number of Websites
live
1 second
watch
trends & more
Get our Counters!
Blog posts written today
Emails sent today
Tweets sent today
1,500
1,000
750
Number of users in millions
500
250
Q1
Q1
Q1
Q1
Q1
Q1
'11
'12
'13
'14
'15
'16
Q2
Q2
Q2
Q2
Q2
Q2
'11
'12
'13
'14
'15
'16
Q3
Q3
Q3
Q3
Q3
Q3
'11
'12
'13
'14
'15
'16
Q4
Q4
Q4
Q4
Q4
Q4
'11
'12
'13
'14
'15
'16
0
1,250

36 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Let’s Try
The keyword ‘gridfs’ represents the name of the 
database that is to be used for the storage and retrieval of 
files. In case the database is not already created, MongoDB 
will create a new database with the same name dynamically. 
MyVideo.avi is the video file to be uploaded using GridFS.
To search and view the document in the database, 
execute the following command on a MongoDB prompt:
MongoDB Prompt> db.fs.files.find()
To display all the chunks created, the following 
instruction is executed:
MongoDB Prompt> db.fs.chunks.find({files_id:ObjectId(‘ 
Figure 5: Starting the MongoDB server from the BIN directory
Figure 7: Inserting a line of PHP-MongoDB driver in php.ini
Figure 6: Output from the GridFS database using find()
58bc0b91bf9bde12940a1640’)})
Interfacing MongoDB-GridFS with PHP
The integration of PHP with MongoDB can be done using 
the MongoDB driver available at https://s3.amazonaws.com/
drivers.mongodb.org/php/index.html. 
After downloading php_mongo.dll, the following line is 
inserted in the php.ini file:
extension = php_mongo.dll
Once the PHP-MongoDB driver is ready, GridFS 
can be used.
Use the following script to upload and store a large file 
using PHP-MongoDB-GridFS:
<?php
    $BigDataConnection = new Mongo(“127.0.0.1:27017”);
   $db = $BigDataConnection->BigDatabase; 
   $db->authenticate(“database-username”,”database-
password”);
   $biggrid = $db->getGridFS();
   $name = $_FILES[‘BigFile’][‘name’]; 
   $type = $_FILES[‘BigFile’][‘type’]; 
   $id = $biggrid->storeUpload(‘BigFile’,$name); 
   $files = $db->fs->files;
   $files->update(array(“filename” => $name), array(‘$set’ => 
array(“contentType” => $type,      
   “aliases” => null, “metadata” => null)));
   $BigDataConnection->close();
   exit(0);
?>
To display all files using PHP-MongoDB-GridFS,  use the 
following script:
<?php
    $BigDataConnection = new Mongo(“127.0.0.1:27017”); 
    $db = $BigDataConnection->BigDatabase; 
    $db->authenticate(“database-username”,”database-
password”);
    $biggrid = $db->getGridFS();
    $mycursor = $biggrid->find(); 
    foreach ($mycursor as $myobj) 
         {
         echo ‘Filename: ‘.$myobj->getFilename().’ Size: 
‘.$myobj->getSize().’<br/>’;
        }
    $BigDataConnection->close();
    exit(0);
?>
Continued on Page 40...

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 37
Developers
Let’s Try
Today, apart from relational tables, data is also available in other forms such as images, 
texts, blogs, lists, etc. Redis can be used as a database, cache and message broker. It is 
exceptionally fast, and can support five data types and two special types of data.
Discover the Many Benefits of 
the Redis Database
to fast retrieval, you can show the important data to 
users from Redis. The rest of the data can be fetched 
from the database later. 
4. It acts as a broker for Celery processes. 
Installation of Redis
You can download a stable version of Redis from https://redis.
io/download. We are going to install Redis on Centos 6.7 —you 
can use Ubuntu or Debian.  Let us extract the tar package of 
Redis, for which you can use the following command:
$ tar -xvzf redis-3.2.8.tar.gz
After extraction, a directory redis-3.2.8 will be formed. 
Just use one command, make, to install the Redis. Sometimes, 
an error occurs while installing Redis:
In file included from adlist.c:34:
zmalloc.h:50:31: error: jemalloc/jemalloc.h: No such file or 
directory
zmalloc.h:55:2: error: #error "Newer version of jemalloc 
required"
make[1]: *** [adlist.o] Error 1
make[1]: Leaving directory `/root/redis/redis-3.2.8/src'
R
edis is an open source, in-memory and key/value 
NoSQL database. Known to be an exceptionally fast 
in-memory database, Redis is used as a database, 
cache and message broker, and is written in C. It supports 
five data types— strings, hashes, lists, sets, sorted sets and 
two special types of data—Bitmap and HyperLogLog.
Since Redis runs in memory, it is very fast but is disk 
persistent. So in case a crash happens, data is not lost. Redis 
can perform about 110,000 SETs and about 81,000 GETs per 
second. This popular database is used by many companies 
such as Pinterest, Instagram, StackOverflow, Docker, etc.
There are several problems that Redis addresses, 
which are listed below.
1. Session cache: Redis can be used to cache user 
sessions. Due to its in-memory data structure as well 
as data persistent nature, Redis is a prefect choice for 
this use case.
2. Middle layer: If your program delivers data at a very fast 
rate but the process of storing data in a database such 
as MySQL is very slow, then you can use Redis as the 
middle layer. Your program will just put the data into 
Redis, and another program will pull the data and store it 
in the database.
3. Shows the latest items listed in your home page: Due 

38 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Let’s Try
dir ./
The above option specifies the location of the dump.rdb file.
daemonize yes
By default, the above option is set as ‘no’. When you run 
the Redis server, it shows progress on the terminal but, in a 
development environment, the Redis server must be run in the 
background. So, set it as ‘yes’.
Let us now start Redis. In Figure 2, there is a directory 
src, which contains Redis binaries.
The command to run the Redis server is as follows:
# src/redis-server redis.conf
See Figure 3, which shows that Redis is running.
Redis supports master-slave replication, which allows 
slave Redis servers to be exact copies of master servers. The 
master/slave replication is one way of achieving scalability. 
The slave is read-only and the master is read and write. 
When a data update happens on the master, it eventually 
affects the slave. 
Figure 2: Redis configuration file
Figure 1: Redis installation
If the above error occurs, then use the command 
given below:
make MALLOC=libc  && make install
Figure 1 shows that Redis has been successfully installed. 
Starting Redis
Just take a look at the directory redis-3.2.8 (Figure 2). You 
will find the redis.conf file in it. In order to start Redis, we 
need to configure this file.
 
Let us consider a main configuration of the file:
bind 127.0.0.1
If you are using a static IP, then you can give the static IP 
address; otherwise, 127.0.0.1 is okay. 
The default port of Redis is 6739. 
pidfile /var/run/redis_6379.pid
The above line gives the location of pidfile, and if you are 
not running as root then you can give your custom path:
 
logfile ""
By default, it is blank but you can give the log file path; 
try to give the absolute path. 
Like /var/run/redis_6379.log
Now, give the following commands:
 
logfile "/var/run/redis_6379.log"
dbfilename dump.rdb
The above line gives the name of the dump file that stores 
the database. 
Figure 3: Redis server in the running state
Creating a Redis slave 
In order to start a slave, you will need a conf file. So make a 
copy of redis.conf as redis-slave.conf:
# cp redis.conf redis-slave.conf
Next, open redis-slave.conf. And change the settings as 
shown below:
pidfile /var/run/redis_6380.pid
logfile "/var/run/redis_6379-s.log"
port 6380 
slaveof 127.0.0.1 6379
The above settings set the port number of the slave and 
tell the slave about its master.
Start the slave and follow the process, as shown in Figure 4.
The Redis master and slave are both running now. 
Redis commands
To run commands on the Redis remote server, we will use the 
redis-cli command syntax:
# redis-cli –h <ip_address> -p <port number>

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 39
Developers
Let’s Try
127.0.0.1:6379>
Let us consider two more commands, lpop and rpop.
127.0.0.1:6379> lrange list2 0 5
1) "one"
2) "two"
3) "three"
127.0.0.1:6379> rpop list2
"three"
127.0.0.1:6379> lpop list2
"one"
127.0.0.1:6379> lrange list2 0 5
1) "two"
127.0.0.1:6379>
You can see that rpop pops the value from the right and 
lpop pops the value from the left.
There are many other useful commands that you will find 
in the documentation on Redis.
Using Redis with Python
In order to use Redis with Python, you will need redis-py. You 
can download redis-py from https://pypi.python.org/pypi/redis. 
I am using redis-2.10.5.tar.gz.
Extract the tar package, as follows: 
# tar -xvzf redis-2.10.5.tar.gz
Use the following command to install it: 
python setup.py install
Let us write a Python script to perform Redis operations as 
we did earlier with redis-cli.
I am going to write a simple Python script named redis1.py:
import redis
r = redis.StrictRedis(host='localhost', port=6379, db=0)
r.set("one",1)
r.set("two",2)
r.set("three",3)
print r.get("one")
print r.get("two")
print r.get("three")
See the output in Figure 5.
Let us create another program redis3.py, which pops the 
value from the left as well as from the right.
import redis
r = redis.StrictRedis(host='localhost', port=6379, db=0)
r.rpush("list1","one")
r.rpush("list1","two", "three", "four")
Figure 4: Redis master and slave
[root@example redis-3.2.8]# src/redis-cli -h 127.0.0.1 -p 
6379
127.0.0.1:6379> ping
PONG
127.0.0.1:6379>
Let us take a look at some useful commands for a fresh start.
Set, get and del: These three commands set the key 
values. See the following example.
127.0.0.1:6379> set "one" 1
OK
127.0.0.1:6379> get one
"1"
127.0.0.1:6379> del one
(integer) 1
127.0.0.1:6379> get one
(nil)
127.0.0.1:6379>
In the above example, ‘one’ is the key and 1 is the value. 
Lpush, lrange, rpush: These are three more useful commands.
The lpush command pushes the values from the left side.
127.0.0.1:6379> lpush list1 "one" "two"
(integer) 2
127.0.0.1:6379> lpush list1 "three"
(integer) 3
In order to see the content, we will use lrange.  
The syntax is # lrange key start end.
127.0.0.1:6379> lrange list1 0 5
1) "three"
2) "two"
3) "one"
127.0.0.1:6379> 
The rpush command pushes the values from the right side.
127.0.0.1:6379> rpush list2 "one" "two"
(integer) 2
127.0.0.1:6379> rpush list2 "three"
(integer) 3
127.0.0.1:6379> lrange list2 0 5
1) "one"
2) "two"
3) "three"

40 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Let’s Try
By: Mohit 
The author is a certified ethical hacker and EC Council 
certified security analyst. He has a master’s in computer 
science from Thapar University, and is the author of 
‘Python Penetration Testing Essentials’. You can contact 
him at mohitraj.cs@gmail.com and https://in.linkedin.
com/in/mohit-raj-990a852a.
print r.lrange("list1",0,-1)
print "pop from right ", r.rpop('list1')
print "pop from left ",r.lpop('list1')
print r.lrange("list1",0,-1)
The above program is very easy to understand. The results 
we have achieved in the above program are the same as what 
we achieved in redis-cli mode. See the output in Figure 6.
While there are a lot of advantages of using Redis, there 
are some disadvantages too. You should have more memory 
than your data requires, because Redis forks on each snapshot 
dump, which can consume extra memory. Also, you can't roll 
back a transaction.
In programming, you must always take care of the 
number of connections. Sometimes people are lazy while 
programming, leading to the ‘max client reached problem’.
Alternatives to Redis
Well, this totally depends upon the situation. If you just 
need fast retrieval, you can use Memcache, but this does not 
store data on the disk. NoSQL databases like MongoDB and 
Couchbase might be alternatives for some use cases.
You can compare the Redis alternatives at: 
 
https://www.g2crowd.com/products/redis/competitors/
alternatives
Figure 5: Output of redis1.py
Figure 6: Output of redis3.py
 
https://hazelcast.com/use-cases/nosql/redis-replacement/
It is recommended that you also read up on Redis 
commands and Redis Sentinel connections. 
By: Dr Gaurav Kumar
The author is the MD of Magma Research and 
Consultancy Pvt Ltd, Ambala City. He is associated with 
various academic and research institutes, where he 
delivers lectures and conducts technical workshops. He 
can be contacted at kumargaurav.in@gmail.com.
To delete files using PHP-MongoDB-GridFS, use the 
script given below:
<?php
    $BigDataConnection = new Mongo(“127.0.0.1:27017”); 
    $mydb = $BigDataConnection->BigDatabase; 
    $mydb->authenticate(“database-username”,”database-
password”);
    $biggrid = $db->getGridFS(); 
    $myfilename = $_REQUEST[“myfile”]; 
    $myfile = $biggrid->findOne($myfilename); 
    $myid = $file->file[‘_id’];
    $biggrid->delete($myid);
    $BigDataConnection->close();
    exit(0);
?>
Interfacing MongoDB-GridFS with Python
To integrate Python with MongoDB-GridFS, attach the 
pymongo library with the existing Python set-up. After this 
step, GridFS can be used.
On the Python prompt, the following instructions can be 
executed for Python-MongoDB-GridFS interfacing:
>>> from pymongo import MongoClient
>>> import gridfs
For selecting database and file system, execute the 
following instructions:
>>> mydb = MongoClient().MyGridFS
>>> filesystem = gridfs.GridFS(mydb)
To insert a new file, put() is used:
>>> myfile = filesystem.put(“My New File”)
To read the file contents, get() is used:
>>> filesystem.get(myfile).read()
These executions can be used for assorted 
applications involving Big Data processing. MongoDB-
GridFS interfacing can also be done with other 
programming languages with higher levels of integrity 
and performance, including Java, C++, JavaScript, Ruby, 
Scala, Haskell and many others. 
Continued Page from 36...

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 41
Developers
Let’s Try
R
eact is a JavaScript library for building user interfaces, 
and can be used to build single page applications. It 
is focused on creating the V of MVC, i.e., the View 
layer. Using a concept called Virtual DOM, components 
can be rendered with a good performance. There is large 
amount of state that needs to be managed by single page 
applications. The state includes server responses, cached and 
locally created data, and various UI states. Redux is a popular 
framework for managing the state. The data displayed by 
the client side UI can be provided from a back-end server. 
Express is a commonly used Web framework based on 
Node.js. This article explains the development of a React 
application using Redux for state management and interfacing 
with the Express framework for data, along with using the 
create-react-app for setting up the development environment. 
The create-react-app seamlessly handles the front-end build 
pipeline, enabling us to focus on writing the application logic.
To get an understanding of this article, readers should 
know the basics of React, Redux, Express.js, and Node.js.
Contacts application
The concepts of developing an application using React, 
Redux and Express will be illustrated through a Contacts 
application. We will keep the application simple, as the main 
objective is to illustrate the various concepts in an easy to 
understand manner. Using the application, we will be able 
to view the Contacts list, add a new contact and delete a 
contact. We will also use redux-thunk, a middleware, to 
perform the action asynchronously.
Setting up the development environment
We will develop the application using Windows as the 
development machine.
1. Install Node.js from https://nodejs.org. Download the 
32-bit or 64-bit binary depending upon the architecture of 
the system.
2. Install create-react-app, using which we will start 
building the Contacts application.
npm install –g create-react-app
3. Generate the basic code template, which we will modify 
to add the features required for the Contacts application.
create-react-app contacts
This will create a directory called contacts in the 
current folder. Inside that folder, we can see an initial 
project structure generated; the required dependencies are 
automatically installed. 
Developing a React Redux 
Application Using create-react-app
This article explains the development of a React application, using Redux for state 
management, interfacing with the Express framework for data and using create-react-app 
as the frontend build pipeline.

42 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Let’s Try
4. Change to the directory contacts. Check if the set-up is 
proper, by running the basic application:
npm start
This will start the application in development mode and 
‘Welcome to React’ will be displayed.
5. Install the modules for Redux integration:
npm install –S redux
npm install –S redux-thunk
npm install –S react-redux
6.  Install the modules, toastr and jquery, to display status on 
the browser when performing UI actions.
npm install –S toastr
npm install –S jquery
7.  Install the HTTP client module, axios, to get data 
from the server. 
npm install –S axios
8. Install the express and body-parser modules to serve the 
persistent data required for the Contacts application.
npm install –S express
npm install –S body-parser
9. To proxy API requests during development and avoid 
CORS issues, update package.json present in the contacts 
folder with the proxy setting (assuming that the Express 
server runs at port 4000).
"proxy": "http://localhost:4000"
Now we can start working on adding the features 
required for the Contacts application. While explaining the 
implementation, main code snippets have been provided. 
For the complete code, please refer to the GitHub 
repository at https://github.com/srini-wip/react-redux-
contacts.git. 
Serving the data required by the Contacts 
application using Express
We will now implement the code to serve the data required 
by the Contacts application using the Express Web 
framework. 
Create a file server.js under the folder contacts. To provide 
the list of contacts, we will implement the API endpoint /api/
contacts using the GET method. This will read the JSON file, 
contacts.json, for the list of available contacts and send the 
response as JSON data.
app.get(‘/api/contacts’, function(req, res) {
    // read from JSON file and send the response as JSON data
});
To add a contact to the contact list, we implement the API 
endpoint api/contacts using the POST method. To retrieve the 
data sent by the browser, we use the body-parser middleware, 
which inserts the required data in the request object, from 
which we can easily extract and save it to the JSON file.
app.post(‘/api/contacts’, function(req, res) {
    // extract the data and save to JSON file
});
To delete a contact from the contact list, we implement 
the API endpoint api/contacts/:id using the DELETE method. 
This will delete the contact and update the JSON file.
app.delete(‘/api/contacts/:id’, function(req, res) {
    // delete the contact and update the JSON file
});
Implementing the user interface using 
React and Redux
We will now implement the set of features on the client 
side using React and Redux. First, we will create the folder 
structure required for writing the code to manage the state and 
view. This will help us in organising our code better. We will 
create the folders store, reducers, actions, api, components 
under the src folder.
Configuring the store
Create a file configureStore.js in the folder store. Initialise 
store using the createStore API of Redux. We need to 
provide the rootReducer, initial state (if any) and then a 
store enhancer. Since we will use redux-thunk, to implement 
asynchronous actions, the last parameter will be the 
thunk middleware.
createStore( rootReducer, initialState, 
applyMiddleware(thunk) );
Implementing the reducers
Create a file contactReducer.js under the folder reducers. 
When the store dispatches an action, it passes to the 
rootReducer, the state maintained in the store and the 
action. While creating the rootReducer, we have combined 
all the individual reducers of the application using the 
combineReducers API. In our current application, we 
have only one reducer, i.e., for contacts, which we will 
create shortly. Although we have only one reducer, it 
will be useful to use the combineReducers API because 
we can extend it to add more reducers as our application 
expands in the future.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 43
Developers
Let’s Try
rootReducer = combineReducers({ contacts });
The rootReducer will pass the action and the respective 
state to each reducer. We will implement the reducer 
function contactReducer accepting two parameters—state 
and action. The reducer is supposed to handle the action 
it is interested in and return the new state. The important 
thing to understand is that the reducer is a pure function, 
and it should not mutate the parameters and return a new 
state in an immutable way using only the values passed 
in the parameters. In contactReducer, we will handle 
the actions – successful loading of contacts, successful 
addition of a contact and successful deletion of a contact.
export default function contactReducer(state=initialState.
contacts, action) {
 
switch (action.type) {
 
 case ‘LOADED_CONTACTS’:
 
  
// Return contacts
 
 case ‘ADDED_CONTACT’:
 
  
// Add new contact to state and return
 
 case ‘DELETED_CONTACT’:
 
  
// Delete contact from state and return
 
 default:
 
  
// Return state passed as parameter
 
}
}
Implementing the actions
Create a file actionTypes.js in the folder actions to store all 
the action names as a constant. Having the action types as 
a constant will help in better maintenance of code rather 
than having them directly as a string. 
Next, create a file contactActions.js, in which we will 
implement various actions related to Contacts. We will 
implement the actions—loading of contacts, adding a 
contact and deleting a contact. As we will communicate 
using the REST API calls with a server in the backend, 
we will make asynchronous calls and, hence, we will use 
the middleware, redux-thunk, to perform asynchronous 
dispatch. The thunk function will invoke the Contacts API 
(discussed below), which will communicate with the server 
and fetch or add/modify the data based on the action. The 
Contacts API will use the npm module axios to make 
REST API calls to the Express server. The Contacts API 
will perform the necessary task and return either success or 
error. If success is returned, then the corresponding success 
action —load, add or delete—will be dispatched, after 
which the reducer code discussed above will get executed.
export function loadContacts() {
  return function(dispatch) {
     // Invoke Contacts API to load contacts and process
     // successful result or error
  };
}
export function addContact(contact) {
  return function(dispatch) {
     // Invoke Contacts API to add contact and process
     // successful result or error
  };
}
export function deleteContact(id) {
  return function(dispatch) {
     // Invoke Contacts API to delete contact and process
     // successful result or error
  };
}
Implementing the API invocation
Create a file contactApi.js in the folder api. We will implement 
the APIs getAllContacts, saveContact and deleteContact. To 
implement the invocation of these APIs, we will issue REST 
API calls to the Express server using the npm module axios.
export default class ContactApi {
  static getAllContacts() {
    // Invoke REST endpoint ‘/api/contacts’ using axios.get
  }
  static saveContact(contact) {
    // Invoke REST endpoint ‘/api/contacts’ using axios.post
  }
  static deleteContact(id) {
    // Invoke REST endpoint ‘/api/contacts/<id>’ using axios.
delete
  }
}
Implementing the UI components
We will now use React to implement the UI components. 
Also, we will use the react-redux module to interface 
React with Redux. 
Create a file ContactsComponent.js in the folder 
components. Here, we will implement the top-level React 
container component, which will interface with the Redux 
store. We will write the method render, which will return the 
JSX to generate the Virtual DOM for the Contacts display 
and also a form to get the inputs from the user for adding a 
contact. Also, each contact will render itself using a separate 
React component, ContactComponent. Then, we will write 

44 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Let’s Try
By: Srinivasan M.
The author works at Wipro and has experience in various 
JavaScript frameworks. He can be reached at srinivm.
srinivasan@yahoo.co.in 
addContact and deleteContact methods, which will dispatch 
the add and delete actions we had discussed earlier.
We need to access the Redux store to receive the state 
changes and to dispatch actions. The React container 
component ContactsComponent will interface with the store 
using the connect API provided by the react-redux module. 
The connect API accepts two parameters – a function (which 
we will name mapStateToProps) that receives the state 
from the store, and another function (which we will name 
mapDispatchToProps) that receives the dispatch method. The 
return value of the connect API is a function to which we will 
pass the React container component ContactsComponent as a 
parameter, and then finally export it.
class ContactsComponent extends React.Component {
    render() {
 
// Return the JSX to display Contact list and the 
Contact form
    }
}
function mapStateToProps(state, ownProps) {
 
// Return an object containing state details
}
function mapDispatchToProps(dispatch) {
 
// Return an object containing action dispatch details
}
export default connect( mapStateToProps, mapDispatchToProps ) 
(ContactsComponent);
Create the file ContactForm.js under the folder 
components. We will implement the code to get the form 
inputs from the user for adding a contact. This will be a React 
presentation component as it need not directly interface with 
the store. It will pass the input information to the container 
component, ContactsComponent, which takes care of 
interfacing with the store.
export default class ContactForm extends React.Component {
  render() {
 
// Return the JSX to display input elements
  }
}
Create the file ContactComponent.js under the folder 
components. We will implement the code to render the 
contact information:
export default class ContactComponent extends React.Component 
{
  render() {
 
// Return the JSX for a contact
  }
}
Implementing the startup code
In the file index.js under the src folder, remove the code 
generated by create-react-app and add the code to implement 
the startup code of the application. We will invoke the 
configureStore function discussed earlier. Next, we will 
dispatch the action to load the contacts. Then we will render the 
top level ContactsComponent by wrapping it inside a Provider 
component provided by the react-redux module. We wrap it 
inside a Provider component to provide access to the Redux 
store through the connect API of react-redux, which we had 
discussed earlier.
store = configureStore();
store.dispatch(loadContacts());
// Render the Contacts React component by wrapping it in 
Provider
Updating index.html to include stylesheet
In the file index.html under the public folder, add the link 
to the bootstrap CSS for presentable UI and the stylesheet 
toastr.css to display UI status during add and delete actions.
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/
libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css">
<link  rel="stylesheet" href="css/toastr.css"/>
Remove the generated files that are not needed
Remove the files App.js, App.css, App.test.js, index.css and 
logo.svg generated by create-react-app as we do not need them.
Running the application
1. Go to the Node.js command prompt. Change the 
directory to contacts.
2. Start the server.
start node server.js
3. Start the React application Contacts:
npm start
This will start the application and display the Contacts 
information. We can now add and delete contacts. 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 45
Developers
Let’s Try
This is the second article in the DevOps series, and covers the installation 
of a LAMP stack and WordPress, using Ansible.
The DevOps Series 
Ansible Deployment of LAMP 
and WordPress
folder that contains the following: 
ubuntu ansible_host=192.168.122.250 ansible_connection=ssh 
ansible_user=xetex
Installing Apache 
We will first install and test the Apache Web server on 
the guest Ubuntu system. Let’s then create a playbooks/
configuration/apache.yml file with the following content: 
---
- name: Install Apache web server
  hosts: ubuntu
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [web]
  tasks:
    - name: Update the software package repository
      apt:
        update_cache: yes
    - name: Install Apache
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - apache2
    - wait_for:
        port: 80
On the Ubuntu guest system, the playbook runs apt-get 
update and then installs the apache2 package. The playbook 
finishes after the server has started, and is listening on port 
80. Open a terminal, enter the ansible/ folder, and execute the 
I
n this article, we are going to learn how to automate the 
deployment of a LAMP stack and install WordPress. 
LAMP stands for Linux, Apache (a Web server), 
MySQL (a database) and PHP (server-side scripting). It is 
a technology stack on which you can deploy different Web 
applications. We are also going to explore the installation 
of WordPress, which is free and open source software for 
creating websites and blogs. 
Installing Linux 
A Parabola GNU/Linux-libre x86_64 system is used as the 
host system. An Ubuntu 15.04 image runs as a guest OS using 
KVM/QEMU. Ansible is installed on the host system using 
the distribution package manager. You should be able to issue 
commands from Ansible to the guest OS. For example: 
$ ansible ubuntu -m ping
ubuntu | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
The /etc/hosts file already has an entry for the guest 
Ubuntu VM. 
192.168.122.250 ubuntu
On the host system, we will create a project for our 
playbooks. It has the following directory structure: 
ansible/inventory/kvm/
       /playbooks/configuration/
       /playbooks/admin/
An  ‘inventory’ file is created inside the inventory/kvm 



48 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Let’s Try
 Figure 1: Apache2 Ubuntu default page 
playbook as shown below: 
$ ansible-playbook -i inventory/kvm/inventory playbooks/
configuration/apache.yml -K 
SUDO password: 
PLAY [Install Apache web server] ****************************
TASK [setup] ************************************************
ok: [ubuntu]
TASK [Update the software package repository] ***************
changed: [ubuntu]
TASK [Install Apache] ***************************************
changed: [ubuntu] => (item=[u'apache2'])
TASK [wait_for] *********************************************
ok: [ubuntu]
PLAY RECAP **************************************************
ubuntu                     : ok=4    changed=2    
unreachable=0    failed=0
The ‘-K’ option is to prompt for the sudo password for 
the ‘xetex’ user. You can increase the level of verbosity in the 
Ansible output by passing ‘-vvvv’ at the end of the ansible-
playbook command. The more number of times  ‘v’ occurs, 
the greater is the verbosity level. 
If you now open http://192.168.122.250, you should 
be able to see the default Apache2 index.html page 
as shown in Figure 1. 
      apt:
        update_cache: yes
    - name: Install MySQL
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - mysql-server
        - mysql-client
        - python-mysqldb
    - name: Start the server
      service:
        name: mysql
        state: started
    - wait_for:
        port: 3306
    - mysql_user:
        name: guest
        password: '*F7B659FE10CA9FAC576D358A16CC1BC646762FB2'
        encrypted: yes
        priv: '*.*:ALL,GRANT'
        state: present
The package repository is updated and the necessary 
MySQL packages are installed. The database server is then 
started, and we wait for the server to be up and running. A 
‘guest’ user account with ‘osfy’ as the password is created 
for use in our Web application. The chosen password is just 
for demonstration purposes. When used in production, please 
select a strong password with special characters and numerals. 
You can compute the hash for a password from the 
MySQL client, as shown below: 
mysql> SELECT PASSWORD('osfy');
+-------------------------------------------+
| PASSWORD('osfy')                          |
+-------------------------------------------+
| *F7B659FE10CA9FAC576D358A16CC1BC646762FB2 |
+-------------------------------------------+
1 row in set (0.00 sec)
An execution run to install MySQL is as follows: 
$ ansible-playbook -i inventory/kvm/inventory playbooks/
configuration/mysql.yml -K 
SUDO password: 
PLAY [Install MySQL database server] ************************
TASK [setup] ************************************************
ok: [ubuntu]
TASK [Update the software package repository] ***************
changed: [ubuntu]
TASK [Install MySQL] ****************************************
changed: [ubuntu] => (item=[u'mysql-server', u'mysql-client', 
u'python-mysqldb'])
Installing MySQL 
The next step is to install the MySQL database server. The 
corresponding playbook is provided below: 
---
- name: Install MySQL database server
  hosts: ubuntu
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [database]
  tasks:
    - name: Update the software package repository

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 49
Developers
Let’s Try
TASK [Start the server] *************************************
ok: [ubuntu]
TASK [wait_for] *********************************************
ok: [ubuntu]
TASK [mysql_user] *******************************************
ok: [ubuntu]
PLAY RECAP **************************************************
ubuntu                     : ok=6    changed=2    
unreachable=0    failed=0
 Note: The default MySQL root password is empty. You 
should change it after installation. 
Installing PHP 
PHP is a server-side programming language and stands 
for PHP: Hypertext Preprocessor (a recursive acronym). 
Although we have used PHP5 in this example, it is 
recommended that you use the latest PHP for security reasons. 
The Ansible playbook for installing PHP is given below: 
---
- name: Install PHP
  hosts: ubuntu
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [web]
  tasks:
    - name: Update the software package repository
      apt:
        update_cache: yes
    - name: Install PHP
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - php5
        - php5-mysql
We update the software repository and install 
PHP5. An execution output of the Ansible playbook is 
shown below: 
$  ansible-playbook -i inventory/kvm/inventory playbooks/
configuration/php.yml -K 
SUDO password: 
PLAY [Install PHP] ******************************************
TASK [setup] ************************************************
ok: [ubuntu]
TASK [Update the software package repository] ***************
changed: [ubuntu]
TASK [Install PHP] ******************************************
changed: [ubuntu] => (item=[u'php5', u'php5-mysql'])
PLAY RECAP **************************************************
ubuntu                     : ok=3    changed=2    
unreachable=0    failed=0
Installing WordPress 
As a final step, we will download, install and set up 
WordPress. The complete playbook is as follows: 
---
- name: Setup Wordpress
  hosts: ubuntu
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [database]
  vars:
    wordpress_file: "/home/{{ ansible_user }}/Downloads/
wordpress-latest.zip"
    wordpress_dest: "/var/www/html"
  tasks:
    - name: Update the software package repository
      apt:
        update_cache: yes
    - name: Create a database for wordpress
      mysql_db:
        name: wordpress
        state: present
    - name: Create downloads directory
      file:
        path: "/home/{{ ansible_user }}/Downloads"
        state: directory
    - name: Create target directory
      file:
        path: "{{ wordpress_dest }}/wordpress"
        state: directory
   - name: Download latest wordpress
      get_url:
        url: https://wordpress.org/latest.zip
        dest: "{{ wordpress_file }}"
    - name: Extract to /var/www/html
      unarchive:
        src: "{{ wordpress_file }}"
        dest: "{{ wordpress_dest}}"
        remote_src: True
    - name: Copy wp-config-sample.php to wp-config.php
      command: cp "{{ wordpress_dest }}/wordpress/wp-config-
sample.php" "{{ wordpress_dest }}/wordpress/wp-config.php"
    - name: Update database credentials in the file
      replace:
        dest: "{{ wordpress_dest }}/wordpress/wp-config.php"
        regexp: "{{ item.regexp }}"
        replace: "{{ item.replace }}"
      with_items:
        - { regexp: 'database_name_here', replace: 

50 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Let’s Try
'wordpress' }
        - { regexp: 'username_here', replace: 'guest' }
        - { regexp: 'password_here', replace: 'osfy'}
    - name: Restart apache2 server
      service:
        name: apache2
        state: restarted
We create variables to store the downloaded file for 
WordPress, and the target installation path. After updating the 
software repository, a database is created for the WordPress 
application. The download and target directories are created 
on the guest system, before actually downloading the latest 
WordPress sources. A configuration file is then created, and the 
database settings are updated. Although we explicitly specify 
the password here, the recommended practice is to store the 
encrypted passwords in an Ansible Vault file, and reference the 
same in the playbook. In future articles, I will demonstrate this 
use case. After completing the configuration, the Web server is 
restarted. An execution run of the playbook is shown below: 
 $  ansible-playbook -i inventory/kvm/inventory playbooks/
configuration/wordpress.yml -K 
SUDO password: 
PLAY [Setup Wordpress] **************************************
*******************
TASK [setup] ************************************************
*******************
ok: [ubuntu]
TASK [Update the software package repository] ***************
*******************
changed: [ubuntu]
TASK [Create a database for wordpress] **********************
*******************
changed: [ubuntu]
TASK [Create downloads directory] ***************************
ok: [ubuntu]
TASK [Create target directory] ******************************
changed: [ubuntu]
TASK [Download latest wordpress] ****************************
ok: [ubuntu]
TASK [Extract to /var/www/html] *****************************
changed: [ubuntu]
TASK [Copy wp-config-sample.php to wp-config.php] 
******************************
changed: [ubuntu]
TASK [Update database credentials in the file] ***************
changed: [ubuntu] => (item={u'regexp': u'database_name_here', 
u'replace': u'wordpress'})
changed: [ubuntu] => (item={u'regexp': u'username_here', 
u'replace': u'guest'})
changed: [ubuntu] => (item={u'regexp': u'password_here', 
u'replace': u'osfy'})
Figure 2: WordPress install page 
TASK [Restart apache2 server] *******************************
changed: [ubuntu]
PLAY RECAP **************************************************
ubuntu                     : ok=10   changed=7    
unreachable=0    failed=0
If you open the URL http://192.168.122.250/wordpress 
in a browser on the host system, you will see a screenshot as 
shown in Figure 2. 
You can now proceed to complete the installation process 
from the browser. It is recommended that you follow the 
security best practices as recommended by the WordPress and 
PHP projects to harden this deployment. 
Writing clean-up playbooks 
It is essential to write clean-up playbooks to revert whatever 
changes you have made, so that you can roll back the system 
if things fail. Uninstalling should be done in the reverse order. 
For example, remove WordPress first, followed by PHP, 
MySQL and Apache. 
The removal of WordPress could depend on your data 
retention policy. You might want to back up your PHP files, 
or you may decide to discard them. You might also want to 
retain the database. A complete removal of WordPress and 
the LAMP stack in the playbooks/admin folder is provided 
below for reference: 
---
- name: Uninstall Wordpress
  hosts: ubuntu

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 51
Developers
Let’s Try
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [web]
  vars:
    wordpress_dest: "/var/www/html"
  tasks:
    - name: Delete wordpress folder
      file:
        path: "{{ wordpress_dest }}/wordpress"
        state: absent
    - name: Drop database
      mysql_db:
        name: wordpress
        state: absent
---
- name: Uninstall PHP
  hosts: ubuntu
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [web]
  tasks:
    - name: Uninstall PHP packages
      package:
        name: "{{ item }}"
        state: absent
      with_items:
        - php5-mysql
        - php5
---
- name: Uninstall MySQL
  hosts: ubuntu
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [database]
  tasks:
    - name: Stop the database server
      service:
        name: mysql
        state: stopped
    - name: Uninstall MySQL packages
      package:
        name: "{{ item }}"
        state: absent
      with_items:
        - python-mysqldb
        - mysql-client
        - mysql-server
---
- name: Uninstall Apache web server
  hosts: ubuntu
  become: yes
  become_method: sudo
  gather_facts: true
  tags: [server]
s  tasks:
    - name: Stop the web server
      service:
        name: apache2
        state: stopped
    - name: Uninstall apache2
      package:
        name: "{{ item }}"
        state: absent
      with_items:
        - apache2
The entire suite of playbooks is also available in my 
GitHub project (https://github.com/shakthimaan/introduction-
to-ansible) for your reference. 
By: Shakthi Kannan
The author is a free software enthusiast and blogs at 
shakthimaan.com. 
Your favourite Magazine on Open 
Source is now on the Web, too.
OpenSourceForU.com
Follow us on Twitter@LinuxForYou
THE COMPLETE MAGAZINE ON OPEN SOURCE

52 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Let’s Try
Gatling is the ultimate open source load testing tool for programmers. Though 
focused on Web applications, it can be used to analyse and measure the 
performance of a variety of services. As it is Scala based, it generates reports in the 
form of pretty graphs, from which the results can be analysed. 
Gatling: A Lightweight Load 
Testing Tool  
In this simulation, we have ramped up 1000 users 
in 10 seconds. Once the simulation script is ready, go to the 
bin folder and launch gatling.bat for a Windows system or 
gatling.sh on Linux based computers. The script will run the 
simulation available in the simulations folder and generate the 
reports in the results folder.
A sample report is given in Figure 2.
Now let’s look at how to use Gatling Maven integration and 
additional features like feeders, externalising the properties, etc. 
When we integrate Gatling with Maven, it enables us to do that 
as part of continuous integration. Along with that, we will look 
at how to externalise properties used in the simulation script 
and the dynamic data feed using Gatling feeders. 
As a first step, create a Maven project and add the Gatling 
dependencies pom.xml. You can download the xml file from 
https://github.com/2013techsmarts/Gatling_Maven_Demo.
Now, we will externalise the baseURL property used in 
our simulation. For this, add the application.properties file 
from src/test/resources and use ConfigFactory to get access to 
the properties. The sample simulation is given below:
package org.smarttechie
import io.gatling.core.Predef._
import io.gatling.http.Predef._
import scala.concurrent.duration._
import com.typesafe.config._
class SampleRESTEndPointPrefSimulation extends Simulation {
            val conf = ConfigFactory.load()
            val baseUrl = conf.getString("baseUrl")
            val httpProtocol = http.baseURL(baseUrl) 
            val restEndPoint = "/posts"
            val restEndpointScenario = scenario("Posts_Pref_
G
atling is a lightweight stress testing tool backed by 
Scala, Akka and Netty. It works asynchronously if 
the underlying HTTP request is non-blocking. It uses 
messages for virtual user simulation rather than a thread per 
virtual user. Because of this, it consumes comparatively less 
system resources to run load tests. In this article, let us look at 
a REST endpoint load test by taking the steps that follow.
Step 1: Download the Gatling bundle from http://gatling.
io/#/resources/download. The folder structure of GATLING_
HOME is explained in Figure 1.
Step 2: Set the GATLING_HOME and JAVA_HOME 
environment variables and write a sample simulation using 
Scala. Java developers do not need to have strong Scala 
knowledge to write Gatling simulations with Scala.  A very 
basic knowledge is sufficient. A simulation to test a REST end 
point is given below:
package org.smarttechie
import io.gatling.core.Predef._
import io.gatling.http.Predef._
import scala.concurrent.duration.
class SampleRESTEndPointPrefSimulation extends Simulation {        
        val httpProtocol = http.baseURL("<base url of your 
end point>")
        val restEndPoint = "/posts" //provide the URI of your 
end point
        val restEndpointScenario = scenario("Posts_Pref_
Simulation")
                                           
.exec(http("request_1")
                                           .get(restEndPoint))
setUp(restEndpointScenario.inject(rampUsers(1000)
over(10seconds))).protocols(httpProtocol)
}

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 53
Developers
Let’s Try
By: Siva Prasad Rao Janapati 
The author is a software engineer working in the e-commerce 
domain. He has hands-on experience in Java, JEE, Spring, 
Oracle Commerce and other open source/enterprise 
technologies. He blogs at http://smarttechie.org.
Simulation")
                        .exec(http("request_1")
                        .get(restEndPoint))
            setUp(restEndpointScenario.inject(rampUsers(1000) 
over (10 seconds))).protocols(httpProtocol)
}
Now, we will look at how to add the dynamic data used 
as part of the simulation. For this, I am using Gatling’s CSV 
feeder. You can get more information on Gatling’s feeders 
from http://gatling.io/docs/2.2.3/session/feeder.html. Add the 
feeder file to the src/test/resources/data folder. The sample 
simulation using the CSV feeder is given below:
package org.smarttechie
import io.gatling.core.Predef._
import io.gatling.http.Predef._
import scala.concurrent.duration._
import com.typesafe.config._
class SampleFeederRESTEndPointPrefSimulation extends 
Simulation {
            val conf = ConfigFactory.load()
            val postsFeed = csv("posts.csv").circular //CSV 
Feeder
            val baseUrl = conf.getString("baseUrl")
             val httpProtocol = http.baseURL(baseUrl) //
sample comment
             val restEndPoint = "/posts/${post_id}" // The 
value of the post_id filed from the feed will go here.
             val restEndpointScenario = scenario("Posts_Pref_
Feed_Simulation")
                        .feed(postsFeed) //pass the feed for 
scenario
                        .exec(http("request_1")
                        .get(restEndPoint))
setUp(restEndpointScenario.inject(rampUsers(1000) over (10 
seconds))).protocols(httpProtocol)
}
The entire project used to demonstrate Gatling usage is 
Figure 1: Gatling folder structure
Figure 2: Gatling performance stats
available on https://github.com/2013techsmarts/Gatling_
Maven_Demo. 
GATLING_HOME
user-files
simulations
bin
bin → contains the launch scripts for Gatling and 
the Recorder.
conf → contains the configuration files for Gatling, 
Akka and Logback.
lib → contains the binaries used by Gatling
results → contains simulation.log and reports 
generated in a subdirectory.
simulations → contains your Simulations 
scala files.
data → contains feeder files.
bodies → contains templates for request 
bodies.
user-files








conf
lib
data
bodies
results

54 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Let’s Try
A
pache Cassandra is a free and open source distributed, 
massively scalable database management system 
designed to handle large amounts of data across 
many commodity servers, while providing highly available 
service and no single point of failure. Apache Cassandra 
offers capabilities like continuous availability, linear scale 
performance, operational simplicity and easy data distribution 
across multiple data centres and cloud availability zones.
History
Apache Cassandra was originally developed by Avinash 
Lakshman (one of the authors of Amazon's Dynamo) and 
Prashant Malik at Facebook for inbox search. Cassandra 
was published as an open source project in Google Code 
in July 2008. It was accepted into Apache Incubator in 
March 2009. Since February 2010, Cassandra has been an 
‘Apache top-level project’.
Features of Apache Cassandra
The following are the key features of Apache Cassandra.
Decentralised: There are no single points of failure and 
no network bottlenecks. Every node in the cluster is identical.
Supports replication and multiple data centre 
replication: Cassandra supports replication across multiple 
data centres (in multiple geographies) and multi-cloud 
availability zones for writes/reads. 
High scalability: Read and write throughput increase 
linearly as new machines are added, with no downtime or 
interruption to applications.
Fault-tolerant: Data is automatically replicated to 
multiple nodes for fault-tolerance. Replication across multiple 
data centres is supported. Failed nodes can be replaced with 
no downtime.
Tunable data consistency: Cassandra supports configured 
consistency levels to manage availability versus data accuracy. 
Apache Cassandra:  
The NoSQL Scalable Database
This article introduces readers to the Apache Cassandra NoSQL database, and 
provides them with use cases for which it is suitable. 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 55
Developers
Let’s Try
Cluster: A cluster contains one or more data centres. It 
can span physical locations.
Commit log: All data is written first to the commit log for 
durability. After all its data has been flushed to SSTables, it 
can be archived, deleted or recycled.
Table: This is a collection of ordered columns fetched by 
rows. A row consists of columns and has a primary key. The 
first part of the key is a column name.
SSTable: This is a sorted string table (SSTable) and is an 
immutable data file to which Cassandra writes memtables 
periodically. SSTables are append-only, stored on disk 
sequentially and maintained for each Cassandra table.
The Apache Cassandra data model
The data model of Cassandra is different from the relational 
DBMS. Cassandra does not support joins or sub-queries like 
in RDBMS. Instead, Cassandra emphasises denormalisation 
through features like collections.
Cassandra is basically a key-value and a column-oriented 
(or tabular) database. Rows are organised into tables -- the first 
component of a table's primary key is the partition key. Within a 
partition, rows are clustered by the remaining columns of the key. 
Other columns can be indexed separately from the primary key.
The Cassandra data model consists of keyspace, column 
families, columns and rows. 
Keyspace: This is the outermost container for your 
application data. It is similar to the schema in a relational 
database. The keyspace can include operational elements, 
such as the replication factor and data centre awareness. 
Keyspace is a group of many column families.
Column family: A column family is a container for an 
ordered collection of rows, each of which is itself an ordered 
collection of columns. A column family is similar to a table in 
an RDBMS and is a logical separation of similar data.
Column: This is a basic data structure of Cassandra with 
three values—name, value and timestamp.
Super column: The super column stores a map 
of sub-columns.
Row: This is a collection of columns labelled with a name.
Different use cases of Apache Cassandra
Apache Cassandra can be used for various applications. Here 
are some use cases where Apache Cassandra is the best choice 
compared to other NoSQL databases.
Figure 1: Apache Cassandra
Figure 2: Cassandra supports multiple 
data centre and cloud deployments
We can configure consistency on a Cassandra cluster, data 
centre, or as per individual read or write operations.
Tunable consistency is one of the strongest features of 
Cassandra. There are two types of consistency—strong and 
eventual. To ensure that data is written and read correctly, 
Cassandra extends the concept of eventual consistency 
by offering tunable consistency. Tunable data consistency 
allows individual read or write operations to be as strongly 
consistent as required by the client application. The 
consistency level of each read or write operation can be 
set, so that the data returned is more or less consistent, 
based on need.
Data compression: Data can be compressed up to 80 per 
cent without any performance overhead.
Cassandra query language: Cassandra provides a query 
language that is very similar to the SQL language. It helps 
developers moving from a relational database to Cassandra.
Architecture of Apache Cassandra
Apache Cassandra’s architecture offers the ability to scale, 
perform and provide continuous uptime. Rather than using 
a legacy master-slave or a manual and difficult-to-maintain 
sharded architecture, Apache Cassandra has a masterless 
‘ring’ design that is elegant, easy to set up, and easy to 
maintain. It has a peer-to-peer distributed system across its 
nodes, and data is distributed among all the nodes in a cluster.
All nodes play an identical role—there is no concept 
of a master node, with all nodes communicating with each 
other equally.
In an Apache Cassandra cluster, each node is capable of 
handling large amounts of data and thousands of concurrent 
users or operations per second — even across multiple data 
centres. This is done with as much ease as when managing 
much smaller amounts of data and user traffic.
Apache Cassandra's architecture also ensures that, unlike 
other master-slave or sharded systems, it has no single point 
of failure and therefore is capable of offering true continuous 
availability and uptime — users need to simply add new 
nodes to an existing cluster without having to take it down.
Key structures
The various structures of Apache Cassandra, along with a 
brief description of each, follows.
Node: This is where you store your data. It is the basic 
infrastructure component of Cassandra.
Data centre: This is a collection of related nodes. This 
could be a physical data centre or a virtual one. Different 
workloads should use separate data centres, either physical or 
virtual. Replication is set by the data centre. Using separate 
data centres prevents Cassandra transactions from being 
impacted by other workloads and keeps requests close to each 
other for lower latency. Depending on the replication factor, 
data can be written to multiple data centres. However, data 
centres should never span physical locations.

56 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Let’s Try
Internet of Things applications: Cassandra is the right 
choice for applications where data is travelling at very high 
speeds between different devices or sensors.
In activity-tracking and monitoring applications: 
Numerous entertainment and media organisations use 
Cassandra to monitor user activity based on parameters such 
as movies, music, album, artist, etc. 
In heavy write systems or in time-series based 
applications: Cassandra is perfect for the very heavy write 
system—for example, in Web analytics where data is logged 
for each and every request based on hits, by type of browser, 
traffic sources, location, behaviour, technology, devices, etc. 
Social media analytics and recommendation engines: 
Cassandra is used by many social media providers to analyse 
data and provide suggestions to their customers. 
Product catalogues and retail applications: One very 
popular use case of Cassandra is to quickly display product 
catalogue inputs and lookups in retail applications. 
Messaging: Cassandra serves as the database backbone for 
numerous mobile phone and messaging providers’ applications.
Apache Cassandra is one of the most popular open 
source distributed database systems available. It provides 
Figure 3: Cassandra’s masterless ‘ring’ 
architecture
Figure 4: Data model: Rows in a column family (CF)
By: Roopendra Vishwakarma 
The author is passionate about researching on new 
technologies in DevOps and Web development. He has written 
many articles around various technologies, open source 
software, Web development and DevOps tools. He can be 
reached at roopendra@techieroop.com.
[1] https://adenocarcinomata/planet-cassandra/what-is-
apache-cassandra
[2]  https://en.wikipedia.org/wiki/Apache_Cassandra
[3]  https://docs.datastax.com/en/cassandra/2.1/cassandra/
architecture/architectureIntro_c.html
References
Column Based Family
Row
Row Key 1
Row Key 2
name1:value1
name1:value1
name2:value2
name5:value5
nameN:valueN
nameN:valueN
Column 1
Column 1
Column 2
Column 5
Column N
Column N
Row
a more flexible data model than what’s offered in the 
relational database world. You can scale it up to any 
number of concurrent user connections and/or data volume. 
It can easily distribute data among multiple geographies, 
data centres and the cloud. So if your application has a 
large amount of data, and if you are planning to scale it, 
then Cassandra will definitely help you. 
www.electronicsb2b.com
Log on to www.electronicsb2b.com and be in touch with the Electronics B2B Fraternity 24x7
Read more
stories on
Security in 
• CCTV camera market is expected to double by 2015
• The latest in biometric devices
• CCTV Camera manufacturers can look forward to a bright future
• Video Analytics Systems Turning a CCTV into a proactive tool
• Security cameras evolving with new technologies
• The latest in dome cameras
• The latest in weather-proof and vandal-resistant security cameras
TOPSECURITY STORIES
ELECTRONICS 
INDUSTRY IS AT A 

Developers
How To
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 57
We continue with the article that appeared in the March 2017 issue of OSFY, which 
was a first for this series on App Inventor 2. For newbies joining us now, please refer 
to the previous article, and do try and read the other articles in the series as these 
will help you to create mobile apps successfully.
of your application, there is a button 
at the top right corner, called Blocks to 
switch the window to the block editor. 
Likewise, the Designer button will bring 
you back to the designer screen from the 
block editor view.
Required logic for Screen1
Screen1 will be our first screen when 
the application is loaded. It has two 
buttons, namely: 
 
 New User
 
 Sign In 
This is the way the logic should go:
1. On clicking the ‘New User’ 
button, the user should be moved 
to new_user.
2. Clicking the ‘Sign In’ button should 
bring the user to the sign_in screen.
I
n the last article, we completed the 
design part of our most innovative 
app idea so far, i.e., a digital wallet 
application using App Inventor 2. Now 
you must be curious to see the block 
editor as well. So, continuing from 
where we left off, let’s head towards the 
block editor.
I will summarise the work so far:
1. We started with the idea of making 
a digital wallet application using 
App Inventor 2.
2. We thought of having four different 
screens for dedicated actions that 
a user might perform with the 
application.
3. We decided and designed the GUI 
part for all the four screens.
Switching to block editor
If you have been following our App 
Inventor series, then you are probably 
familiar with switching between the 
App Inventor designer and block editor 
screens. If you are on the designer screen 
where you see the actual appearance 
Digital Wallet Application  
in App Inventor 2
Figure 1: Block editor
Required logic for the New 
User screen
The New User screen will be called 
from Screen1 upon pressing the new_
user button. The purpose of the screen 
is to take the user credentials and save 
them in the database for successful 
login procedure. This is the way the 
logic should go:
1. Upon clicking the Create_Account 
button, tinyDB should store the 
name and password into the 
database.
2. We will store the name under the 
‘name’ tag.
3. We will store the password under 
the ‘password’ tag.
Required logic for Sign In 
screen
If you are already registered on the 
device, you should be able to log in by 
providing validation fields. This is the 
way the logic should go:
1. Upon clicking the ‘Sign In’ button, 
tinyDB should be called for validating 

Developers How To
58 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
the name and password fields.
2. If the name and password match 
the already stored values, the user 
should be able to navigate to the 
‘purchase’ screen.
Required logic for the 
purchase screen
This is one of the complex screens, 
and involves multiple actions and 
validation points.  This is the way 
the logic should flow:
1. Once the screen is opened, the 
database should be called to fetch 
the user name, which will be 
displayed at the top.
2. The database should be called for 
getting the user credit balance for 
the application.
3. The user should be able to select 
any product from the listed 
products.
4. Upon selecting a product, 
its name and price should be 
displayed to confirm the selection.
5. Upon clicking the Purchase 
button, the user’s balance should 
be deducted.
6. Upon clicking the ‘Add Balance’ 
button, the user should be able to 
add a sum to the user application 
account.
Packaging and testing 
To test the app, you need to get it 
on your phone. First, you have to 
download the application to your 
computer and then move it to your 
phone via Bluetooth or USB cable. 
I’ll tell you how to download it.
1. On the top row, click on the 
‘Build’ button. It will show you 
the option to download the apk to 
your computer.
2. After a successful download, 
the application will be placed 
in the Download folder of 
your directory or the preferred 
location you have set for your 
downloading.
3. You now need to get this apk 
file to your mobile phone either 
via Bluetooth or via USB cable. 
Once you have placed the apk 
Figure 2: Block editor for creating an account
Figure 3: Block editor for ‘Sign In’ button
Figure 4: Block editor for purchase
Figure 5: Block editor product list
Figure 6: Block editor ‘purchase’ button
Figure 7: Block editor ‘Add Balance’ button
Figure 8: Block editor notifier
file on your SD card, you need 
to install it. Follow the on-screen 
instructions to install it. You might 
get some notification or warning 
saying that the install is from an 
untrusted source. Allow this from 
the settings, and after successful 
installation you will see the icon 
of your application in the menu of 
your mobile. Here, you will see the 
default icon, which can be changed 
and we will tell you how to do this 
as we move ahead.
I hope your application is working 
exactly as per the requirements you 
have given. Now, depending upon your 
usability and customisation, you can 
change various things like image, sound 
and behaviour.
Debugging the application 
We have just created the prototype 
of the application with very basic 
functionality, but what else might the 
user be interested in? Let us analyse 
various cases, which require serious 
planning, so that using the app does 
not annoy the user. Consider the 
following cases:
1. If a user is registered, can we make 
him/her login automatically to the 
application?
2. How will we enable lower case and 
upper case credential validations?
3. What if the price of the selected 
product is more than the remaining 
balance — how will we prompt 
the user to add balance before 
proceeding for payment?
4. How will a purchase receipt 
be generated after a successful 
purchase?
5. What happens if the user gives 
a text input on the ‘add balance’ 
screen?
These are some of the scenarios that 
might occur and users will be pretty 
happy seeing them addressed.
Think over all these scenarios and 
look at how you can integrate these into 
the application. Do get in touch with 
me if you fail to accomplish any of the 
above cases. 
By: Meghraj Singh Beniwal
The author is a B. Tech in electronics and communication, a freelance writer and 
an Android app developer. He is currently working as an automation engineer 
at Infosys, Pune. He can be contacted at meghrajsingh01@rediffmail.com or 
meghrajwithandroid@gmail.com 


60 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Overview
G
radle is an open source build system which is familiar 
to most Android developers. It uses a DSL (Domain 
Specific Language) built on the Groovy language 
for configurations, while Direct Acyclic Graph is used to 
determine the order of its tasks. This article explores Gradle’s 
features that make software development on Android easier.
Separating logic and resources for flavours or 
build variants
When developing projects, you may want to develop a mock 
project that uses sample data or a library like Stetho for 
network inspection. Consider an example where Application 
class initialises the needed libraries. Here, we can create 
a class named AppController, which has a method attach 
(context) (refer to Figure 1).
 //  check the file path in comments
 //  app/src/debug/java/com/example/appname/AppController.
java
 import android.app.Application;
import com.facebook.stetho.Stetho;
public class AppController {
 
public static void attach(Application application) {
          Stetho.initializeWithDefaults(application);
 
}
}
// app/src/release/java/packagename/AppController.java
public class AppController {
 
public static void attach(Application application) {
                 // analytics sdk
 
}
}
Check the file path in the comments. On building the 
project, if it is a debug build, Cradle uses class files from 
the debug folder, by default, and from the main folder for 
common modules. In the application class (in the main 
folder), you can just use Appcontroller class methods.
Gradle: Making Software 
Development Easier
Gradle is a build tool, which accelerates productivity. As the blurb on its website 
says, “From mobile apps to micro services, from small startups to big enterprises, 
Gradle helps teams build, automate and deliver better software, faster.”

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 61
Developers
Overview
In the example, the resValue method creates a string 
resource with the key app_name and value based on the type 
of flavour. buildConfigField creates a field in the BuildConfig 
class, and you can access the generated values in your code 
using BuildConfig.FIELD_NAME. 
Using the properties files for secret data
When you push your code to remote repositories, you may not 
want to share your API server details or API keys. For such 
cases, you can make use of the properties file and ignore this 
file in your version control (.gitignore file in Git).
// app/build.gradle
// tries to read from environment, useful for  CI systems
def api_url =  System.getenv("API_URL")
Properties properties = new Properties()
// lookups config.properties file in project root
if (project.rootProject.file('config.properties').exists()) {
  properties.load(project.rootProject.file('config.
properties').newDataInputStream())
  api_url = properties.getProperty('apiUrl')
}
In the above example, the Gradle config will read 
the secrets from a properties file (config.properties) and 
assign it to a variable. This variable can be used with 
the resValue or the buildConfigField methods in the 
application code. 
Figure 1: Selecting build types or flavour
// app/src/main/java/com/example/appname/App.java
import android.app.Application;
public class App extends Application {
            
            @Override
 
public void onCreate() {
       super.onCreate();
       AppController.attach(this);
 
}
}
If you need flavour-specific class logic, create a folder 
named with that flavour name. You can also change the 
default path to flavour-specific code using the sourceSets 
method, but it is better to use the default option. You can use 
this flavour path to put specific configuration files like JSON 
files that you get for Google services.
You can conditionally include your dependencies also. 
In the example, Stetho dependency is only needed for your 
debug builds, for which you can use your build variant name 
with the compile method. 
compile 'com.android.support:customtabs:25.2.0'
 // only used on debug builds
 debugCompile 'com.facebook.stetho:stetho:1.4.2'
Creating flavour-specific resources or 
configuration values  
In your app’s Gradle config file, you can also specify a 
flavour- or variant-specific resource value or config value. 
Check the code below:
android{
 productFlavors {
     mock {
              resValue "string", "app_name", "Appname-Mock"
     }
     production {
              resValue "string", "app_name", "Appname"
     }
 }
 buildTypes {
  debug{
    buildConfigField "String", "RAMEN_EFFECT",”0” 
  }
 release{
    buildConfigField "String", "RAMEN_EFFECT", “1”
 }
}
}
By: Renjith Thankachan
The author works in a startup called Pytenlabs as a product 
engineer. His areas of expertise are Django/Python, Android 
and Linux server administration. He can be reached at 
renjith@pytenlabs.com. 

62 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Overview
HBase is a non-relational database that runs on top of HDFS. This article gives an 
overview of HBase, discussing its benefits and limitations.
Is HBase a Helpful Database?
4. HBase supports an easy-to-use Java API for 
programmatic access.
Architecture
HBase’s architecture is composed of three types of 
components — the client library, a master server and a 
region server, the last of which is optional as it can be used 
based on requirements. 
Master server: This acts as a monitoring agent and 
monitors all region server instances present in the cluster. 
It also operates as an interface for all the metadata changes. 
It maintains the state of the cluster by negotiating the 
load balancing. It is responsible for schema changes and 
other metadata operations such as the creation of tables 
and column families. 
Regions: Regions are the basic building elements of the 
HBase cluster, which consists of the distribution of tables 
and column families. They contain multiple stores, one for 
each column family. They comprise mainly two components, 
H
Base is an open source, distributed, non-relational 
database, developed by the Apache Software 
Foundation, which runs on top of HDFS. Initially, 
it was referred to as Google Big Table, and later re-named 
HBase. Mainly written in Java, it is a data model that is 
designed to provide quick random access to a large amount 
of structured data.
One can store data in HDFS either directly or through 
HBase.  It is natively integrated with Hadoop and works 
flawlessly alongside other data access engines through YARN.  
HBase’s features
1. HBase tables are distributed in the cluster via regions, 
which are automatically split and redistributed as the 
data grows.
2. HBase is linearly scalable and has automatic 
failure support.
3. It integrates with Hadoop, both as a source 
and as a destination.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 63
Developers
Overview
HBase vs RDBMS 
HBase
RDBMS
Schemaless in database.
Has fixed schema in data-
base.
Column-oriented data-
base.
Row-oriented database.
Designed to store de-
normalised data.
Designed to store normalised 
data.
Well suited for OLAP 
systems.
Well suited for OLTP systems.
It is good for semi-struc-
tured as well as struc-
tured  data.
It is good for structured data.
HBase’s limitations
1. HBase cannot execute functions like SQL. It doesn't 
support the SQL structure, so it does not contain any 
query optimiser.
2. We cannot expect to completely use HBase as an 
alternative for conventional models, some of which 
cannot hold HBase.
3. HBase, when integrated with Pig and Hive jobs, results in 
some time memory issues in the cluster.
You can download HBase from the Apache website, 
the latest version of which is 1.2.4. The HBase team 
recommends that you install it on a UNIX/Linux 
environment; if you run it in Windows, you might want to 
download and install Cygwin to do so. 
which are Memstore and Hfile. Regions are mainly tables that 
are split up and spread across the region servers.  
Region server: When a region server receives writes 
and read requests from the client, it assigns the request to 
a specific region, where the actual column family resides. 
However, the client can directly make contact with region 
servers — there is no need of mandatory master permission to 
the client for communication with region servers. The client 
requires master help when operations related to metadata and 
schema changes are required. 
Why use HBase?
Today, every Web application consists of billions of rows. 
Searching for a few particular rows from a large amount 
of data takes a lot of time. In such a situation, HBase is the 
ideal choice as the query fetch time is short. Conventional 
relational data models fail to meet the performance 
requirements of very big databases. 
HBase vs HDFS vs Hive
HBase
HDFS
Hive
HBase is built on 
top of HDFS.
HDFS is a distrib-
uted file system 
suitable for storing 
large files.
It is a relational 
DBMS.
HBase provides 
fast look-ups for 
larger tables.
HDFS does not 
support fast 
individual record 
look-ups.
Hive does not 
provide fast 
look-ups.
It provides low 
latency access to 
single rows from 
huge datasets.
It provides high 
latency batch pro-
cessing.
It provides high 
latency for huge 
datasets.
Provides random 
access to data.
Provides sequential 
access to data.
Provides ran-
dom access to 
data.
By: Neetesh Mehrotra
The author works at TCS as a systems engineer. His areas of 
interest are Java development and automation testing. You 
can contact him at mehrotra.neetesh@gmail.com.
MONTH
THEME
March 2017
Open Source Firewall, Network security and Monitoring
April 2017
Databases management and Optimisation
May 2017
Open Source Programming (Languages and tools)
June 2017
Open Source and IoT
July 2017
Mobile App Development and Optimisation
August 2017
Docker and Containers
September 2017
Web and desktop app Development
October 2017
Artificial Intelligence, Deep learning and Machine Learning
November 2017
Open Source on Windows
December 2017
BigData, Hadoop, PaaS, SaaS, Iaas and Cloud
January 2018
Data Security, Storage and Backup
February 2018
Best in the world of Open Source (Tools and Services)
OSFY Magazine Attractions During 2017-18

64 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Insight
E
very other day we discover a new online application 
that tries to make our lives more convenient. And as 
soon as we get to know about it, we register ourselves 
for that application without giving it a second thought. After 
the one-time registration, whenever we want to use that app 
again, we just need to log in with our user name and password 
—the app or system automatically remembers all our data that 
was provided during the registration process. Ever wondered 
how a system is able to identify us and recollect all our data 
on the basis of just a user name and password? It’s all because 
of the database in which all our information or data gets 
stored when we register for any application. 
Similarly, when we browse through millions of product 
items available on various online shopping applications like 
Amazon, or post our selfies on Facebook to let all our friends 
see them, it’s the database that is making all this possible. 
According to Wikipedia, a database is an organised 
collection of data.  Now, why does data need to be in an 
organised form?  Let’s flash back to a few years ago, when 
we didn’t have any database and government offices like 
electricity boards stored large heaps of files containing the 
data of all users. Imagine how cumbersome it must have 
been to enter details pertaining to a customer’s consumption 
of electricity, payments made or pending, etc, if the names 
were not listed alphabetically. It would have been time 
consuming as well.  It’s the same with databases. If the data 
is not present in an organised form, then the processing 
time in fetching any data is quite long. The data stored 
in a database can be in any organised form—schemas, 
reports, tables, views or any other objects. These are 
basically organised in such way as to help easy retrieval of 
information. The data stored in files can get lost when the 
papers of these files get older and, hence, get destroyed. But 
in a database, we can store data for millions of years without 
any such fear. Data  will get lost only when the system 
crashes, which is why we keep a backup.  
Choose the Right Database 
for Your Application
Databases are key components of many an app and choosing the right option is an elaborate 
process. This article examines the role that databases play in apps, giving readers tips on 
selecting the right option. It also discusses the pros and cons of a few select databases.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 65
Developers
Insight
Now, let’s have a look at why any application needs a 
database.
1. It will be difficult for any online app to store huge amounts 
of data for millions of its customers without a database. 
2. Apart from storing data, a database makes it quite easy 
to update any specific data (out of a large volume of data 
already residing in the database) with newer data. 
3. The data stored in a database of an app will be much more 
secure than if it’s stored in any other form.
4. A database helps us easily identify any duplicate set of 
data present in it. It will be quite difficult to do this in any 
other data storage method.
5. There is the possibility of users entering incomplete sets 
of data, which can add to the problems of any application. 
All such cases can be easily identified by any database.
A user cannot directly interact with any database—there 
needs to be an interface or intermediate system, which helps 
the user to interact with it. Such an interface is referred to 
as a database management system (DBMS). It is basically 
a computer software application that interacts with the user 
or other applications, and even with the database itself, in 
order to capture and analyse the data. Any DBMS such as 
MySQL is designed in such a way that it allows the definition, 
querying, creation, updation and administration of the whole 
database. It is where we request the database to give us the 
required data in the query language. 
Different types of databases
Relational database:  This is one of the most common of 
all the different types of available databases. In such types of 
databases, the data is stored in the form of data tables. Each 
table has a unique key field, which is used to connect it to 
other tables. Therefore, all the tables are related to each other 
with the help of several key fields. These databases are used 
extensively in different industries and will be the type we are 
most likely to come across when working in IT.
Operational database: An operational database is 
quite important for organisations. It includes the personal 
database, customer database and inventory database, all 
of which cover details of how much of any product the 
company has, as well as the information on the customers 
who buy the products. The data stored in different 
operational databases can be changed and manipulated based 
on what the company requires.
Data warehouses: Many organisations are required to 
keep all relevant data for several years. This data is also 
important for analysing and comparing the present year 
data with that of the previous year,  to determine key trends. 
All such data, collected over years, is stored in a large data 
warehouse. As the stored data has gone through different 
kinds of editing, screening and integration, it does not require 
any more editing or alteration.
Distributed databases: Many organisations have several 
office locations—regional offices, manufacturing plants, 
branch offices and a head office. Each of these workgroups 
may have their own set of databases, which together will 
form the main database of the company. This is known as a 
distributed database.
End user databases: There is a variety of data available 
at the workstation of all the end users of an organisation. Each 
workstation acts like a small database in itself, which includes 
data in presentations, spreadsheets, Word files, downloaded 
files and Notepad files. 
Choosing the right database for your application
Choosing the right database for an application is actually 
a long-term decision, since  making any changes  at a later 
point can be difficult and even quite expensive. So, we cannot 
even afford to go wrong the first time. Let’s see what benefits 
we will get if we choose the right database the first time itself.
1. Only if we choose the right database will the relevant and 
the required information get stored in the database, putting 
data in a consistent form.
2. It’s always preferable that the database design is 
normalised. It helps to reduce data redundancy and even 
prevents duplication of data. This ultimately leads to 
reducing the size of the database.
3. If we choose the correct database, then the queries 
fired in order to fetch data will be simple and will get 
executed faster.
4. The overall performance of the application will be quite good.
5. Choosing the right database for an application also helps 
in easy maintenance. 
Figure 2: Retrieval of output data from a database using queries 
Figure 1: Block diagram of a database system
Database
User
Simple Query
Output
SQL
Interface
Internal
Operations
Application 
Program1
Application 
Program2
File System
Database
Application 
Program3
DBMS

66 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Insight
Factors to be considered while choosing the 
right database for your application
Well, there is a difference between choosing any database 
for an application and choosing the right database for it. 
Let’s have a look at some of the important factors to be 
considered while choosing a  database for an application.
Structure of data: The structure of the data 
basically decides how we need to store and retrieve it. 
As our applications deal with data present in a variety 
of formats, selecting the right database should include 
picking the right data structures for storing and retrieving 
the data. If we do not select the right data structures for 
persisting our data, our application will take more time 
to retrieve data from the database, and will also require 
more development efforts to work around any data issues.
Size of data to be stored: This factor takes into 
consideration the quantity of data we need to store and 
retrieve as critical application data. The amount of data we 
can store and retrieve may vary depending on a combination 
of the data structure selected, the ability of the database to 
differentiate data across multiple file systems and servers, 
and even vendor-specific optimisations. So we need to 
choose our database keeping in mind the overall volume of 
data generated by the application at any specific time and 
also the size of data to be retrieved from the database.
Speed and scalability: This decides the speed we 
require for reading the data from the database and writing 
the data to the database. It addresses the time taken to 
service all incoming reads and writes to our application. 
Some databases are actually designed to optimise read-
heavy applications, while others are designed in a way 
to support write-heavy solutions. Selecting a database 
that can handle our application’s input/output needs can 
actually go a long way to making a scalable architecture.
Accessibility of data: The number of people or 
users concurrently accessing the database and the level 
of computation involved in accessing any specific data 
are also important factors to consider while choosing the 
right database. The processing speed of the application 
gets affected if the database chosen is not good enough to 
handle large loads.
Data modelling: This helps map our application's 
features into the data structure and we will need to 
implement the same. Starting with a conceptual model, we 
can identify the entities, their associated attributes, and the 
entity relationships that we will need. As we go through 
the process, the type of data structures we will need in 
order to implement the application will become more 
apparent. We can then use these structural considerations 
to select the right category of database that will serve our 
application the best.
Scope for multiple databases: During the modelling 
process, we may realise that we need to store our data 
in a specific data structure, where certain queries cannot 
be optimised fully. This may be because of various reasons 
such as some complex search requirements, the need for 
robust reporting capabilities, or the requirement for a data 
pipeline to accept and analyse the incoming data. In all such 
situations, more than one type of database may be required 
for our application. When choosing more than one database, 
it's quite important to select one database that will own 
any specific set of data. This database acts as the canonical 
database for those entities. Any additional databases that 
work with this same set of data may have a copy, but will 
not be considered as the owner of this data.
Safety and security of data: We should also check 
the level of security that any database provides to the data 
stored in it. In scenarios where the data to be stored is highly 
confidential, we need to have a highly secured database. The 
safety measures implemented by the database in case of any 
system crash or failure is quite a significant factor to keep in 
mind while choosing a database.
A few open source database solutions 
available in the market
MySQL
MySQL has been around since 1995 and is now owned by 
Oracle. Apart from its open source version, there are also 
different paid editions available that offer some additional 
features, like automatic scaling and cluster geo-replication. We 
know that MySQL is an industry standard now, as it’s compatible 
with just about every operating system and is written in both C 
and C++. This database solution is a great option for different 
international users, as the server can provide different error 
messages to clients in multiple languages, encompassing support 
for several different character sets.
Pros
 
It can be used even when there is no network available.
 
It has a flexible privilege and password system.
 
It uses host-based verification.
 
It has security encryption for all the password traffic.
 
It consists of libraries that can be embedded into different 
standalone applications.
Figure 3: Facebook architecture using MySQL as the database 
(Image source: googleimages.com)

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 67
Developers
Insight
By: Vivek Ratan 
The author has completed his B.Tech in electronics and 
instrumentation engineering, and is working as an automation 
test engineer at Infosys, Pune. He can be reached at 
ratanvivek14@gmail.com for any suggestions or queries.
[1] http://www.wikipedia.org/
[2] http://www.capterra.com/
[3] http://www.dbta.com/
References
 
It provides the server as a separate program for a client/
server networked environment.
Cons
 
Different members are unable to fix bugs and craft patches.
 
Users feel that MySQL no longer falls under the category 
of a free OS.
 
It’s no longer community driven.
 
It lags behind others due to its slow updates.
SQLite
SQLite is supposedly one of the most widely deployed 
databases in the world. It was developed in 2000 and, since 
then, it has been used by companies like Facebook, Apple, 
Microsoft and Google. Each of its releases is carefully tested 
in order to ensure reliability. Even if there are any bugs, the 
developers of SQLite are quite honest about the potential 
shortcomings by providing bug lists and the chronologies of 
different code changes for every release.
Pros
 
It has no separate server process.
 
The file format used is cross-platform.
 
It has a compact library, which runs faster even with more 
memory.
 
All its transactions are ACID compliant.
 
Professional support is also available for this database.
Cons
It’s not recommended for:
 
Different client/server applications.
 
All high-volume websites.
 
High concurrency.
 
Large datasets.
MongoDB
MongoDB was developed in 2007 and is well-known as the 
‘database for giant ideas.’ It was developed by the people 
behind ShopWiki, DoubleClick, and Gilt Group. MongoDB is 
also backed by a large group of popular investors such as The 
Goldman Sachs Group Inc., Fidelity Investments, and Intel 
Capital. Since its inception, MongoDB has been downloaded 
over 15 million times and is supported by more than 1,000 
partners. All its partners are dedicated to keeping this free and 
open source solution’s code and database simple and natural.
Pros
 
It has an encrypted storage engine.
 
It enables validation of documents.
 
Common use cases are mobile apps, catalogues, etc.
 
It has real-time apps with an in-memory 
storage engine (beta).
 
It reduces the time between primary failure and recovery.
Cons
 
It doesn’t fit applications which need complex transactions.
 
It’s not a drop-in replacement for different legacy 
applications.
 
It’s a young solution—its software changes and evolves 
quickly.
MariaDB
MariaDB has been developed by the original developers 
of MySQL. It is widely used by tech giants like Facebook, 
Wikipedia and even Google. It’s a database server that offers 
drop-in replacement functionality for MySQL. Security is one 
of the topmost concerns and priorities for MariaDB developers, 
and in each of its releases, the developers also merge in all of 
MySQL’s security patches, even enhancing them if required.
Pros
 
It has high scalability with easier integration.
 
It provides real-time access to data.
 
It has the maximum core functionalities of MySQL 
(MariaDB is an alternative for MySQL).
 
It has alternate storage engines, patches and server 
optimisations.
Cons
 
Password complexity plugin is missing.
 
It does not support the Memcached interface.
 
It has no optimiser trace. 
Figure 4: End-to-end architecture for MongoDB (Image source: googleimages.com)

68 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Insight
MongoDB achieves scalability with ease due to its unique architecture. The key 
component of this architecture is the data model, which is based on schema-
less documents and collections. While adopting MongoDB in any application, it is 
important to base it on data Modelling. 
The Importance of Data Modelling 
in MongoDB
On the other hand, NoSQL databases are generally 
schema-less. Although there is a database schema defined to 
start with, data does not have to follow the schema strictly. 
NoSQL databases accept data of any structure within their 
collections (tables in the relational world), irrespective of 
whether it matches their schema or not. In other words, they 
are flexible regarding their enforcement of schema to data. 
They do not have inbuilt inherent relationships between 
their collections; rather, applications need to implement the 
necessary logic for atomicity and consistency of data between 
collections. And there are no inbuilt constructs to join data 
from multiple collections. 
MongoDB provides two different ways to create 
relationships between data objects – References and 
Embedment.
Data Modelling in the NoSQL world
Data Modelling is equally important in the NoSQL world 
as it is in the relational world. There are important facets of 
applications that cannot be realised without implementing 
proper and optimised data models. Data Modelling is not 
an afterthought. It is an important process in the application 
planning and design phases. Some of the important reasons 
M
ongoDB is one of the most popular NoSQL databases 
around and is gaining popularity exponentially. 
Many more developers have started using it and 
increasing numbers of applications are built using MongoDB. 
As organisations adopt MongoDB as an integral architectural 
component of their solutions, it is important that they build it 
on the solid foundation of data Modelling. 
Data Modelling is the first step in using any database, be 
it relational or NoSQL. It refers to the process of creating 
database design iteratively to meet the application’s needs. 
It involves analysis and depiction of data entities and their 
relationships for an application. 
Traditional databases are primarily relational in nature, 
where the database schema is defined at design time based on 
data objects. The data structure is static in nature, and data 
should follow the rules to be stored and retrieved using this 
static schema. If data does not follow the database schema, 
it cannot be stored in the database. Moreover, database 
structures are normalised and decomposed into multiple 
smaller tables to avoid data repetitions and implement the 
get-what-you-want pattern of data retrieval. These tables have 
relationships built into them to enforce their consistency, 
integrity, atomicity and durability.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 69
Developers
Insight
Applications can resolve these references to access the related 
data. Data models based on References are also known as 
Normalised data models. The example illustrated next shows 
the Users document references in a placeofBirth document.
{
  "_id": "Ritesh",
  "name": "Ritesh Modi",
  "placeofBirth": "RiteshPOB"
}
{
  "PlaceofBirth": {
    "_id":  "RiteshPOB"
    "street": "123 xyz street",
    "city": "xyzcity",
    "state": "xyzState",
    "zip": "12345"
  }
}
References relationship should be used:
 
To implement one-to-many relationships between 
documents.
 
To implement many-to-many relationships between 
documents.
 
If the referenced entities are updated frequently.
 
If the referenced entities grow indefinitely.
Embedment
Embedded relationships in documents refer to storing related 
documents within a original document. The related data is 
part of the schema of embedding documents. In effect, the 
entire data is stored together within a single document, with 
related data stored as an array or sub-object. Data models 
based on Embedment are also known as De-Normalised data 
models. The example illustrated next shows the placeofBirth 
entity embedded in the Users document.
{
  "_id": "Ritesh",
  "name": "Ritesh Modi",
  "placeOfBirth": {
    "street": "123 xyz Street",
    "city": "xyzcity",
    "state": "xyzState",
    "zip": "12345"
  }
}
Embedded documents should be used when:
 
There is a contained relationship between entities.
 
The embedded entity is an integral part of the document.
 
The embedded entities are not updated frequently.
for data Modelling are listed below.
Scalability: Scalability refers to the increase in 
application workload due to increase in traffic. Applications 
should be designed to handle and perform well when 
the usage of the application increases. From the NoSQL 
perspective, it means that collections and data entities should 
be modelled based on the current and future demand for 
the application. There should not be non-availability or 
degradation of performance due to an increase in the number 
of users or transactions in the database. 
Performance: Database Modelling is about trade-offs 
between security, availability, scalability and performance. 
A right balance between these architectural blocks helps in 
creating the optimal database design for an application. A data 
model helps in understanding the read and write needs of an 
application and also helps in deciphering data updates patterns 
frequently while some remains static most of the time. It helps 
in creating an application that performs as expected with an 
increased workload over time.
Application needs: Different applications have different 
demands from the database. Some are read-intensive while 
others are write-centric. Applications can be OLTP based or 
meant for reporting. Some applications have multiple facets 
built into them. Different data Modelling strategies should 
be used based on the nature of the application. Reporting 
applications are read heavy and writes should not reduce their 
performance, while transactional systems should be able to 
read and write with equal ease. 
Data consistency: Data consistency helps in reducing 
redundant data, and understanding relationships, their update 
patterns and taxonomy. It helps in storing only the required 
data in its correct form.  
Capacity: Each document has a defined size and, 
generally, performance falls with re-allocation of size to it. 
Data Modelling helps in creating optimal sized documents, 
reducing redundant data in each document. It also helps in 
identifying the overall capacity needs of the database. 
MongoDB support for data Modelling
MongoDB provides advanced constructs for enabling data 
Modelling. It provides References and Embedment for 
defining structure and relationships of documents. It is 
important to understand these two constructs before getting 
into data Modelling.
References
Relationships are defined based on matching data contained 
in columns in different collections. In MongoDB, these 
relationships are defined based on semantics. The MongoDB 
engine does not enforce this relationship, and it is completely 
dependent on the application to implement and respect this 
relationship while reading and writing data in collections.  
References store the relationships between data by 
including links or references from one document to another. 

70 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Insight
allows for the effective reuse of the freed record space.
When using Embedded documents, it should be carefully 
analysed if the sub-object can grow out of bounds. If it can, 
there is the possibility of performance degradation when 
the size of the document crosses its limit. In such cases, 
References relationship should be used to ensure that growth 
in document size stays within limits. 
Indexing: Indexes are especially useful in improving 
performance while retrieving data. They help in fetching sorted 
data, helping applications to eliminate the need to sort them 
explicitly. Collections that are frequently accessed for read 
operations should implement indexes on the column on which 
frequent searches are made. While indexes are beneficial during 
read operations, they introduce negative performance for 
write operations.  Indexes should be built on columns that are 
updated infrequently and queried frequently. Another drawback 
of indexes is that they consume additional storage space and 
should be considered carefully before being implemented.
Sharding: Sharding is a database load balancing 
technique fully supported by MongoDB. It refers to horizontal 
partitioning of data into multiple MongoDB instances, with 
each instance holding specific and unique data. Each instance 
is referred to as a ‘shard’ and hosts a portion of overall 
collection data. Sharding is typically employed with large 
datasets in collections with heavy operations on them.  
Strategies for MongoDB data Modelling
Data Modelling is equally important in the NoSQL world 
as it is in the relational world. There are important facets of 
applications that cannot be realised without implementing a 
proper and optimised data model. 
One-to-one with Embedded relationship: In this 
strategy, one data entity is embedded into another data entity, 
where both the entities have a one-to-one relationship with 
each other.
An example of a one-to-one Embedded relationship, 
between the user and the details of his place of birth, is 
illustrated here:
{
  "_id": "Ritesh",
  "name": "Ritesh Modi",
  "placeOfBirth": {
    "street": "123 xyz Street",
    "city": "xyzcity",
    "state": "xyzState",
    "zip": "12345"
  }
}
A one-to-one Embedded relationship should be used when:
 
Both the name and place of birth are retrieved together 
frequently.
 
The embedded entities do not grow indefinitely.
 
Relationships range from one to a few, between 
embedding and embedded entities.
Important considerations for MongoDB 
data Modelling
While designing database document structure and data models 
in MongoDB, special consideration should be given to the 
following aspects for deploying highly scalable, performance-
centric and efficient databases. It is to be noted that these 
are not mutually exclusive and should be evaluated in 
combination with each other.
Data usage: While designing a data model, emphasis should 
be laid on the patterns that the applications will be using to 
access the data. The patterns refer to reading, writing, updating 
and deletion of data. Some applications are completely read-
centric (like the reporting application), while other are write-
centric like an e-commerce application. Some are a combination 
of both. In some applications, a particular feature is read-heavy 
while others are write-heavy. There are possibilities that even 
within a single document some data is frequently updated while 
other data remains static. Based on these patterns, appropriate 
strategies should be devised using relationships, indexes, growth 
in document size and atomicity. Documents with Embedded 
relationships perform better than documents with References 
relationships if both the data are needed while reading.
Atomicity: Atomicity in database parlance means that 
operations either succeed or fail as a single unit. If there 
are multiple sub-operations within a parent transaction, the 
parent operation will fail if any of its sub-transactions fail.  
Operations in MongoDB happen at the collection level. A 
single write operation can affect only a single collection. 
Even if it attempts to affect multiple collections, these will be 
treated as separate operations. There is no support from the 
database engine to roll back a part of operations, if the sub-
operations fail. The application should implement the logic 
for affecting multiple collections. 
If updating multiple collections is a requirement, 
Embedded relationships should be used because entire data is 
available within a single document. There is no risk that a part 
of the operation will succeed or fail. However, References 
relationships can be used when it does not matter if sub-
operations fail. 
Document structure: Document structure plays a crucial 
role in data Modelling. The application is written based on the 
structure of documents. The documents can be designed using 
the References or Embedment relationship. 
Document growth: MongoDB assigns a fixed document 
size during the initialisation phase. MongoDB’s storage 
engine will relocate the document on the disk when document 
size exceeds the allocated space for that document, MongoDB 
will relocate the document on the disk. With MongoDB 3.0.0, 
however, the default use of the power of two-sized allocations 
minimises the occurrences of such re-allocations as well as 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 71
Developers
Insight
 
Both the name and place of birth are updated together.
 
Place of birth sub-entity is not growing.
One-to-one with References relationship: In this strategy, 
one data entity references another data entity, where both the 
entities have a one-to-one relationship with each other.
An example of a one-to-one Referenced relationship, 
between the user and the details of his place of birth, is 
illustrated here:
{
  "_id": "Ritesh",
  "name": "Ritesh Modi",
  "placeofBirth": "RiteshPOB"
}
{
  "PlaceofBirth": {
    "_id":  "RiteshPOB"
    "street": "123 xyz Street",
    "city": "xyzcity",
    "state": "xyzState",
    "zip": "12345"
  }
}
One-to-one Referenced relationships should 
be used when:
 
Both the name and place of birth are not retrieved 
together.
 
Both the name and place of birth are updated using 
different operations.
 
Place of birth sub-entity is not growing.
One-to-many with Embedded relationship: In this 
strategy, a multiple data entity is embedded into another data 
entity, where they have a one-to-many relationship with each 
other.
An example of a one-to-many Embedded relationship, 
between an author and the books he has authored, is 
illustrated here:
{
  "_id": "Ritesh",
  "name": "Ritesh Modi",
  "booksAuthored": [
    {
      "name": "Windows server 2016",
      "publisher": "Self Publishing",
      "year": "2016",
      "price": "30"
    },
    {
      "name": "Ubuntu Linux",
      "publisher": "Self Publishing",
      "year": "2017",
      "price": "40"
    }
  ]
} 
One-to-many Embedded relationships should be used when:
 
Both the author and the books published are retrieved 
together frequently.
 
Both the author and the books published are updated 
together.
 
Place of birth sub-entity is not growing out of bounds, i.e., 
there is one-to-few relationship between entities.
One-to-many with References relationship: In this 
strategy, collections are referenced where they have a one-to-
many relationship with each other.
An example of a one-to-many Referenced relationship 
between authors and books published is illustrated here:
{
  "_id": "Ritesh",
  "name": "Ritesh Modi",
}
{
  "_id": "bookid",
  "authorid": "Ritesh",
  "books": [
    {
      "name": "Windows server 2016",
      "publisher": "Self Publishing",
      "year": "2016",
      "price": "30"
    },
    {
      "name": "Ubuntu Linux",
      "publisher": "Self Publishing",
      "year": "2017",
      "price": "40"
    }
  ]
}
}
One-to-many Referenced relationships should be used 
when:
 
Both the author and books published are not retrieved 
together.
 
Both the author and books published are updated at 
different times in different operations.
 
Books authored can grow out of bounds.
Continued on Page 80...

72 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Overview
T
he support for a large number of databases is one of the 
many features that make Python a favourite language 
for software developers. Apart from supporting 
general-purpose database systems such as MySQL, Oracle, 
PostgreSQL, Informix, etc, it has support for many other 
specific systems also. For example, it has support for 
embedded databases such as SQLite and ThinkSQL. And it 
supports graph databases such as Neo4J. 
This article explores three databases -- pickleDB, TinyDB 
and ZODB. These three are implemented in Python and used 
for specific purposes. 
pickleDB
pickleDB is a simple and lightweight data store. It stores 
the data as a key-value store. It is based on the Python 
module named SimpleJSON, which allows developers 
to handle JSON (JavaScript Object Notation) in a 
simple and faster manner. SimpleJSON is a pure Python 
implementation with no dependencies; it functions as 
an encoder and decoder for Python versions 2.5+. The 
complete documentation on SimpleJSON is available at 
https://simplejson.readthedocs.io/. 
Developed by Harrison Erd, pickleDB is available with 
the BSD three-clause licence. It may be installed effortlessly 
with the following command: 
$ pip install pickledb
The name pickleDB is inspired by a Python module 
named pickle, which pickleDB was using earlier. Though 
the later versions of pickleDB started using the SimpleJSON 
module, the name pickle was retained. 
A Peek at Three Python Databases: 
pickleDB, TinyDB and ZODB
Persistence of data plays a critical role in most software applications. This article introduces 
three databases—pickleDB, TinyDB and ZODB—which are implemented in Python.  Let’s 
explore their unique characteristics and use case scenarios.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 73
Developers
Overview
The following code segment illustrates the basics of 
using pickleDB. 
>>> import pickledb 
>>> db = pickledb.load('example.db', False) 
>>> db.set('key', 'value') 
True 
>>> db.get('key') 
'value' 
>>> db.dump() 
True
Some of the popularly used commands of pickleDB are 
explained below. 
 
LOAD path dump: This is used to load a database 
from a file. 
 
SET key value: This is the value of a key with the string. 
 
GET key: Used to retrieve the value of the key. 
 
GETALL: Used to fetch all the keys in a database. 
 
REM key: Deletes the key. 
 
DUMP: Saves the database from the memory into the file 
specified with the load command. 
Apart from the above mentioned commands there are 
various other commands. Interested readers can get the 
complete details from https://pythonhosted.org/pickleDB/
commands.html. 
pickleDB can be used for those scenarios in which the 
key-value store type of format is suitable. 
TinyDB
As the name indicates, TinyDB is a compact, lightweight 
database and is document-oriented. It is written 100 per 
cent in Python and has no external dependencies. As 
the official documentation says, TinyDB is a database 
optimised for your happiness. 
The applications which are best suited to TinyDB are 
small apps for which a traditional SQL-DB server based 
approach would be an overload. The major features of 
TinyDB are listed below: 
 
As the name indicates, TinyDB is very small. The 
complete source code is only 1200 lines. 
 
 TinyDB is based on document-oriented storage. This type 
of storage is adopted in popular tools such as MongoDB. 
 
The API of TinyDB is very simple and clean. Primarily, 
TinyDB has been designed keeping ease of usage in mind. 
 
Another likeable feature of TinyDB is the non-
dependency. In addition, TinyDB supports all the recent 
versions of Python. It works on Python 2.6, 2.7, 3.3 – 3.5. 
 
Extensibility is another major feature of TinyDB. With the 
help of middleware, TinyDB behaviour can be extended 
to suit specific needs. 
Though TinyDB has various advantages, it is not a 
single-size-fits-all solution for problems. It has certain 
limitations as listed below:
 
TinyDB is not suitable for those scenarios in which high 
speed data retrieval is the key. 
 
If you need to access the database from multiple processes 
or threads, then TinyDB won’t be optimal. Similarly, 
HTTP server based access is another scenario in which it 
won’t be suitable. 
Having seen the pros and cons of TinyDB, let us look 
at how to use it.  As is the case with many other packages, 
TinyDB can be installed simply with the following command: 
$ pip install tinydb
The following code snippet explores how to create and 
store the values in TinyDB: 
from tinydb import TinyDB, Query
db = TinyDB('db.json')
db.insert({'type': 'OSFY', 'count': 700})
db.insert({'type': 'EFY', 'count': 800})
After the successful execution of the above mentioned 
snippet, you can retrieve the values as shown below. 
To list all values, give the following commands:
db.all()
[{'count': 700, 'type': 'OSFY'}, {'count': 800, 'type': 
'EFY'}]
Figure 2: Tiny DB features
Figure 1: Python databases
Pythonic Databases
PickleDB
ZODB
TinyDB
Light-Weight
Ease of Use
Extensibility
No External Dependencies
Simple and Clean API
Document Oriented Storage

74 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers Overview
To search and list values, type: 
Magazine = Query()
db.search(Magazine.type == 'OSFY')
[{'count': 700, 'type': 'OSFY'}]
db.search(Magazine.count > 750)
[{'count': 800, 'type': 'EFY'}]
For updating the values, use the following commands: 
db.update({'count': 1000}, Magazine.type == 'OSFY')
db.all()
[{'count': 1000, 'type': 'OSFY'}, {'count': 800, 'type': 
'EFY'}]
To remove values, type: 
db.remove(Magazine.count < 900)
db.all()
[{'count': 800, 'type': 'EFY'}]
To delete all the values, give the following commands: 
db.purge()
db.all()
[]
TinyDB enables developers to handle data in two 
different ways, as listed below: 
 
JSON 
 
In-memory
The default value is JSON, and if you want to change it 
to in-memory, you need to specify that explicitly. 
from tinydb.storages import MemoryStorage
db = TinyDB(storage=MemoryStorage)
The detailed documentation on TinyDB API is 
available at https://tinydb.readthedocs.io/en/latest/api.
html. 
ZODB
ZODB is a native object database for Python. Its major 
features are: 
 
Seamless integration between code and database. 
 
No separate language is needed for operations related 
to the database. 
 
No database mapper is required. 
ZODB is better suited for the following scenarios: 
 
When the developer wants to focus more on the 
application rather than building lots of database code. 
 
 When the application has lots of complex relationships 
and data structures. 
 
When the data read operations are comparatively larger 
than the write operations. 
At the same time, ZODB is not suitable for scenarios 
in which the application requires a high volume of data 
write operations. 
To install ZODB, use the following command: 
$ pip install ZODB
A simple code snippet to establish connection with a 
database is shown below: 
import ZODB, ZODB.FileStorage
storage = ZODB.FileStorage.FileStorage('mydata.fs')
db = ZODB.DB(storage)
connection = db.open()
root = connection.root
ZODB allows developers to use a wide variety of 
storage options, as listed below: 
 
In-memory databases
 
Local files
 
Databases on remote servers
 
Advanced options such as compressed and 
encrypted storage
The members of the object can be directly stored using 
ZODB functions. A detailed tutorial for using ZODB is 
available at http://www.zodb.org/en/latest/tutorial.html.  
ZODB has support for transactions also. 
This article has attempted to provide an insight into 
the world of Python databases. There are many other 
databases apart from the three narrated in this article, 
such as buzhug (http://buzhug.sourceforge.net/) and 
CodernityDB (http://labs.codernity.com/codernitydb/). 
The bottomline is that all these database tools provide a 
Pythonic ambience when you work with them. 
By: Dr K.S. Kuppusamy 
The author is assistant professor of computer science at 
the School of Engineering and Technology, Pondicherry. 
He has 11+ years of teaching and research experience 
in academia and industry. He can be reached via mail at 
kskuppu@gmail.com.
[1]  http://www.zodb.org
[2] https://tinydb.readthedocs.io
[3] https://pythonhosted.org/pickleDB/commands.html
References


M
akeMyTrip has deployed a large number of open 
source innovations. Starting from Apache Hadoop, 
Storm and Spark to advanced solutions such as 
Jenkins, OpenTSDB, Grafana and the ELK (Elasticsearch, 
Logstash and Kibana) stack, the company uses many 
community-based deployments.
CTO Sanjay Mohan believes that there is a growing need 
to focus more on open source because a vast part of the Web 
is now being open sourced. “Open source solutions have 
been tried and tested for scale and security by the top-notch 
companies globally and proven to work well without any 
vendor lock-in period,” he says.
DataShark project for the community
While adopting open source to scale existing offerings 
and solve emerging problems has been quite common for 
MakeMyTrip, Mohan and his team sketched out a plan last 
October to contribute back to the community too with their 
own solution. So they designed dataShark as an advanced 
framework for security and network event analytics. “We felt 
that the security and open source community would greatly 
benefit from this framework and hence released it as an 
open source project,” says Vikram Mehta, senior manager of 
information security, MakeMyTrip.
 The dataShark framework is targeted at security 
researchers, Big Data analysts and operations teams looking 
to ingest data from sources such as the file system, Syslog and 
Kafka in a secure and easy way. Also, the framework provides 
experts with the ability to write custom map and machine 
language algorithms that can operate on ingested data.
Built on Apache Spark, dataShark uses the Python 
language to write custom use cases and power the 
framework model. It comes with two operation modes, 
namely, standalone executable and production. The 
standalone executable mode of the framework provides 
a one-shot analysis of static data, whereas the production 
mode provides a full-fledged production deployment with 
components such as event acquisition, event queuing, the 
core data engine and persistence layer that can all ingest 
data from the file system or HDFS.
 Gurugram based MakeMyTrip presently dominates the travel market with a whopping 
47 per cent share. It has emerged as not just a regular consumer of open source 
technologies but has also, of late, transformed into a contributor.
MakeMyTrip Travels Forward in Time 
Using the Power of Open Source
For U & Me CaseStudy
76 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

“We were leveraging machine learning for Web anomaly 
detection and other use cases, when we found the requirement 
for a framework that would help security teams to quickly 
write their own use cases with minimal effort. That brought 
dataShark to life,” Mehta told Open Source For You.
Challenges in building a community solution
Building dataShark for the community involved some 
challenges for the team. “Being in the security field, it was 
challenging to up-skill ourselves to Big Data systems and 
machine learning,” says Kunal Aggarwal, senior information 
security engineer, MakeMyTrip.
Aggarwal is among the four-member team, which is led 
by Mehta and also includes Security Analyst Avinash Jain 
and Senior Security Analyst Dhruv Kalaan alongside Mehta 
and Aggarwal himself, tasked with maintaining the dataShark 
repository on GitHub, and with updating the open source 
project with new upgrades. Despite being a small team, the 
members preferred to utilise the inhouse skillsets to make 
dataShark successful, before releasing it in the public. “Once 
we had successfully acquired the skillsets, the journey with 
dataShark has been a breeze,” says Aggarwal.
Moving on from proprietary solutions
Mehta’s team is actively developing strategies to move from 
the commercialised proprietary world to the fast-growing open 
By: Jagmeet Singh
The author is an assistant editor at EFY.
Open source technologies help enhancing web traffic of MakeMyTrip
MakeMyTrip team while taking a hands-on experience on open source technologies
source ecosystem. The MakeMyTrip website, which receives 
over 22 million visits in a month, has recently shifted from 
some Microsoft technologies to open source implementations. 
“At an optimal operating expenditure, we were able to 
exercise greater flexibility using open source components on 
our website,” Mehta states.
MakeMyTrip is also considering open source as a 
parameter to find the right talent. The company believes 
that it is important for young professionals to opt for open 
source skills. “Open source platforms nowadays have become 
mainstream in the consumer Internet space, and the new-
age software professional has to consider exposure to these 
platforms,” Mohan told Open Source For You.
Security enhancements through open source
dataShark is one of the publicly released examples of 
how MakeMyTrip is using open source to secure the user 
experience. However, there are several other community-
backed options it uses on the security front. “We leverage 
various open source deployments to manage security. These 
have greatly helped to enhance our security posture in the 
market,” says Mehta.
Open source: An efficient operating model
Like many other open source adopters, the IT experts at 
MakeMyTrip are highly satisfied with the open source success 
rate. “The use of open source software not only reflects well 
on the technical expertise of the organisation, but is also a 
positive and efficient operating model,” says Mohan.
The CTO concludes that MakeMyTrip will continue 
to leverage open source on a large scale in the future and 
contribute to the community wherever possible. 
Major open source solutions powering MakeMyTrip
• Apache Hadoop, Storm and Spark for Big Data analytics
• Guava from Google and CoreNLP from Stanford for 
machine learning
• Jenkins, OpenTSDB, Grafana, and the ELK stack for 
production deployment and monitoring pipeline
• Apache HTTPD, Tomcat, NGINX, Django and MySQL 
for production infrastructure
For U & Me
CaseStudy
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 77

78 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me Insight
N
oSQL database, also called Not Only SQL, is an 
approach to data management and database design 
that's useful for very large sets of distributed data. 
NoSQL, which encompasses a wide range of technologies 
and architectures, seeks to solve the scalability and big 
data performance issues that relational databases weren’t 
designed to address. NoSQL is especially useful when an 
enterprise needs to access and analyse massive amounts of 
unstructured data or data that's stored remotely on multiple 
virtual servers in the cloud.
NoSQL technology was originally created and used by 
Internet leaders such as Facebook, Google, Amazon and 
others, who required database management systems that 
could write and read data anywhere in the world, while 
scaling and delivering performance across massive data sets 
and millions of users.
Benefits of NoSQL databases
NoSQL databases provide various important advantages over 
traditional relational databases. A few core features of NoSQL 
are listed here, which apply to most NoSQL databases. 
Schema agnostic: NoSQL databases are schema 
agnostic. You aren’t required to do a lot on designing your 
schema before you can store data in NoSQL databases. 
You can start coding, and store and retrieve data without 
knowing how the database stores and works internally. If 
you need advanced functionality, then you can customise 
the schema manually before indexing the data. Schema 
agnosticism may be the most significant difference between 
NoSQL and relational databases.
Scalability:  NoSQL databases support horizontal 
scaling methodology that makes it easy to add or reduce 
capacity quickly without tinkering with commodity 
hardware. This eliminates the tremendous cost and 
complexity of manual sharding that is necessary when 
attempting to scale RDBMS.
Performance: Some databases are designed to operate 
best (or only) with specialised storage and processing 
hardware. With a NoSQL database, you can increase 
performance by simply adding cheaper servers, called 
commodity servers. This helps organisations to continue 
to deliver reliably fast user experiences with a predictable 
return on investment for adding resources again, without the 
overhead associated with manual sharding.
High availability: NoSQL databases are generally 
designed to ensure high availability and avoid the complexity 
that comes with a typical RDBMS architecture, which relies 
on primary and secondary nodes. Some ‘distributed’ NoSQL 
databases use a masterless architecture that automatically 
distributes data equally among multiple resources so that 
the application remains available for both read and write 
operations, even when one node fails.
The development of NoSQL databases was triggered by the needs of the companies 
dealing with huge amounts of data like Facebook, Google and Amazon. NoSQL databases 
are increasingly being used in Big Data and real-time Web applications. Read about the 
different types of NoSQL databases in this article.
The Different Types of 
NoSQL Databases
D A T A B A S E S
S
L

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 79
For U & Me
Insight
Global availability: By automatically replicating data 
across multiple servers, data centres or cloud resources, 
distributed NoSQL databases can minimise latency and 
ensure a consistent application experience wherever 
users are located. An added benefit is a significantly 
reduced database management burden of manual RDBMS 
configuration, freeing operations teams to focus on other 
business priorities.
Types of NoSQL databases
Several different varieties of NoSQL databases have been 
created to support specific needs and use cases. These 
databases can broadly be categorised into four types.
Key-value store NoSQL database
From an API perspective, key-value stores are the simplest 
NoSQL data stores to use. The client can either get the value 
for the key, assign a value for a key or delete a key from the 
data store. The value is a blob that the data store just stores, 
without caring or knowing what's inside; it's the responsibility 
of the application to understand what was stored. Since key-
value stores always use primary-key access, they generally 
have great performance and can be easily scaled. The key-
value database uses a hash table to store unique keys and 
pointers (in some databases it’s also called the inverted index) 
with respect to each data value it stores. There are no column 
type relations in the database; hence, its implementation is 
easy. Key-value databases give great performance and can be 
very easily scaled as per business needs.
Use cases: Here are some popular use cases of the key-
value databases:
 
For storing user session data
 
Maintaining schema-less user profiles
 
Storing user preferences
 
Storing shopping cart data
However key-value databases are not the ideal choice for 
every use case when: 
 
We have to query the database by specific data value.
 
We need relationships between data values.
 
We need to operate on multiple unique keys.
 
Our business needs updating a part of the value frequently.
Examples of this database are Redis, MemcacheDB 
and Riak. 
Document store NoSQL database
Document store NoSQL databases are similar to key-value 
databases in that there’s a key and a value. Data is stored as a 
value. Its associated key is the unique identifier for that value. 
The difference is that, in a document database, the value 
contains structured or semi-structured data. This structured/
semi-structured value is referred to as a document and can be 
in XML, JSON or BSON format. 
Use cases: Document store databases are preferable for: 
 
E-commerce platforms
 
Content management systems
 
Analytics platforms
 
Blogging platforms
Document store NoSQL databases are not the right 
choice if you have to run complex search queries or if 
your application requires complex multiple operation 
transactions. 
Examples of document store NoSQL databases are 
MongoDB, Apache CouchDB and Elasticsearch.
Column store NoSQL database
In column-oriented NoSQL databases, data is stored in cells 
grouped in columns of data rather than as rows of data. 
Columns are logically grouped into column families. Column 
families can contain a virtually unlimited number of columns 
that can be created at runtime or while defining the schema. 
Read and write is done using columns rather than rows. 
Column families are groups of similar data that is usually 
accessed together. As an example, we often access customers’ 
names and profile information at the same time, but not the 
information on their orders.
The main advantages of storing data in columns over 
relational DBMS are fast search/access and data aggregation. 
Relational databases store a single row as a continuous disk 
entry. Different rows are stored in different places on the disk 
while columnar databases store all the cells corresponding to 
a column as a continuous disk entry, 
thus making the search/access faster. 
Each column family can be 
compared to a container of rows 
in an RDBMS table, where the 
key identifies the row and the row 
consists of multiple columns. The 
difference is that various rows do not 
have to have the same columns, and 
columns can be added to any row at 
any time without having to add them 
to other rows.
Use cases:  Developers mainly 
use column databases in:
Figure 1: Column based family
Column Based Family
Row
Row Key 1
Row Key 2
name1:value1
name1:value1
name2:value2
name5:value5
nameN:valueN
nameN:valueN
Column 1
Column 1
Column 2
Column 5
Column N
Column N
Row

80 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me Insight
 
Content management systems
 
Blogging platforms
 
Systems that maintain counters
 
Services that have expiring usage
 
Systems that require heavy write requests (like log 
aggregators)
Column store databases should be avoided if you have to 
use complex querying or if your querying patterns frequently 
change. Also avoid them if you don’t have an established 
database requirement, a trend which we are beginning to see 
in new systems.
Examples of column store NoSQL databases are 
Cassandra and Apache Hadoop HBase.
Graph base NoSQL database
Graph databases are basically built upon the Entity - Attribute 
- Value model. Entities are also known as nodes, which have 
properties. It is a very flexible way to describe how data 
relates to other data. Nodes store data about each entity in 
the database, relationships describe a relationship between 
nodes, and a property is simply the node on the opposite end 
of the relationship. Whereas a traditional database stores a 
description of each possible relationship in foreign key fields 
or junction tables, graph databases allow for virtually any 
relationship to be defined on-the-fly.
Use cases: Graph base NoSQL databases are usually used in: 
 
Fraud detection 
 
Graph based search 
 
Network and IT operations
 
Social networks, etc 
Examples of graph base NoSQL databases are Neo4j, 
ArangoDB and OrientDB. 
By: Roopendra Vishwakarma 
The author is passionate about researching on new 
technologies in DevOps and Web development. He has 
written many articles around various technologies, open 
source software, Web development and DevOps tools. He 
can be reached at roopendra@techieroop.com.
[1] http://basho.com/resources/nosql-databases/
[2] http://www.algoworks.com/blog/nosql-database-types/
[3] https://www.thoughtworks.com/insights/blog/nosql-
databases-overview
References
 
Books authored can grow out of bounds.
 
Avoiding repetition of data.
Many-to-many with Embedded relationship: It is not 
advisable to implement the many-to-many strategy with 
Embedded relationships, as it leads to unnecessary repetition 
of data and applications have to write complex logic to 
perform updates and retrievals.
Many-to-many with References relationship: In this 
strategy, collections are Referenced where they have a many-
to-many relationship with each other.
An example of a many-to-many Referenced relationship 
between a bank account and account holders is illustrated 
here. The bank account document uses arrays to reference the 
account holders, and the account holders' document refers to 
the bank account using arrays.
{
  "_id": "123456789",
  "accountNumber": "123456789",
  "accountName": “ManytoManyAccount,
  "accountHoders": [ 111111111, 222222222]
}
{
  "_id": 111111111,
  "name": "Ritesh",
  "age": 25,
  "phone": "9999999999",
  "email": "abc@abc.com",
  "account_id": [ 123456789 ]
}
{
  "_id": 222222222,
  "name": "Sohan",
  "age": 30,
  "phone": "8888888888",
  "email": "xyz@xyz.com",
  "account_id": [ 123456789]
}
Many-to-many Referenced relationships should be used 
when:
 
Many-to-many relationships exist between entities.
 
Both collections can grow out of bounds.
 
Avoiding repetition of data. 
By: Ritesh Modi 
The author is a technology architect and senior technology 
evangelist with Microsoft, with 14+ years of experience in the 
industry.  He blogs at http://automationnext.wordpress.com 
and can be followed on Twitter @automationnext.
Continued from Page 71...

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 81
OpenGurus
How To
Scilab has solutions for standard and large scale optimisation problems in 
engineering. It provides algorithms to solve constrained, unconstrained, 
continuous and discrete problems. 
Optimisation of 2D Toy Functions 
Using Scilab
The 3D plots for these functions are shown in Figures 1, 
2, 3 and 4, respectively, and the equations of each function are 
shown in Figure 5.
Optimisation
Here, we are concerned about minimising the constrained 
optimisation problem. Two dimensional functions have a pair 
of values as the solution. Hence, these two values determine 
the optimal value for the objective function/toy function. We 
use Firefly algorithm as optimisation algorithm for all the non-
linear toy functions. Do check my article in the January 2017 
edition of OSFY for Firefly algorithm and Scilab.
Interpretation
Using 50 flies scattered over Ackley’s function, approximate 
values of (0, 0) and the objective value of 0 is obtained as 
the final solution after 50 iterations. Figure 6 shows the 
initial stage of the 50 flies and Figure 7 shows the final stage 
of convergence. A dark dot at the bottom of the function is 
the collection of flies reaching the final minimum value of 
the problem; or the flies having good food, as we observe 
in nature. Similarly, optimal values of 0, -19.2085 and 0, 
respectively, are obtained for other toy functions.
Scilab software is freely available for numerical optimisation 
and other computation. From Scilab, non-linear toy functions are 
optimised using the Firefly algorithm. The convergence of the 
algorithm is faster as shown in Figures 6 and 7. The trajectory 
O
ptimisation occurs in nature all around us. Water 
droplets optimise themselves into a sphere, which 
is the least possible area for any given volume. 
Migrating flock of birds optimises its use of energy 
by flying in a V-shaped formation, which reduces air 
resistance. We continuously learn from nature to artificially 
engineer things like the honeycomb structure or the flight 
formation of fighter jets.
To optimise engineering objectives, we use toy functions 
that are noisy, non-linear and two-dimensional as benchmarks. 
Scilab is one of the open source platforms in which three-
dimensional (3D) plots are possible. Numerical studies 
followed by visual interpretation help us to understand the 
efficiency of proposed algorithms. Real world assumptions 
of engineering problems are bound to truncate broad 
assumptions into narrow band, making it difficult to solve 
them using conventional methods. These problems are solved 
using algorithms benchmarked with toy functions.
Some toy functions from Scilab
In a broad sense, to maximise or minimise an objective 
function while satisfying some constraints is said to be 
optimisation. Some of the main toy functions used are:
1.  Ackley’s function
2. Levi function
3. Holder-table function
4. Buckin function

82 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
OpenGurus How To
Figure 2: Levi function
Figure 3: Holder table function
Figure 4: Buckin function
Figure 5: Toy functions
Figure 6: Function convergence, initial stage
Figure 7: Final function convergence
By: Hithu Anand
The author has a teaching background and is currently 
researching on smart grids. He can be reached at 
hithuanand@gmail.com.
for convergence using contour plots and other performances 
can be obtained with a few tweaks within the algorithm. For 
now, we have discussed the general idea of implementing toy 
function optimisation in Scilab. 
300
20
15
10
-10
5
4
3
2
1
0
5
10
15
20
25
30
35
40
45
50
5
010
Z
20
15
10 Z
5
010
-10
3.4
3.2
3
2.8
2.6
2.4
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
200
100
0
10
5
X
0
-5
-10
-10
-5
0
Y
5
10
Z
0
-5
-10
-15
Z
-20
10
5
-5
-10
-10
-5
0
5
10
Y
0
X
Figure 1: Ackley’s function
500
400
300
200
100
10
5
0
-5
-5
5
0
-10
-10
10
Y
X
0
Z
10
10
15
20
Z
0
5
5
5
-10
-10
-5
5
10
Y
X
0
0

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 83
OpenGurus
How To
I
n a typical IoT application, data is collected at servers 
and middleware solutions from many devices with 
multiple sensors, and each device cluster generates a 
high volume of data at the rate of thousands to millions of 
records per second. These records are mapped to a time 
scale with nano second precision. Even though traditional 
databases are highly efficient for typical rates of insertions 
and deletions, they may not be able to handle high precision 
data optimally. Moreover, it is very rare to delete individual 
records in IoT or DevOps scenarios, and updates are not 
allowed most of the time. Instead, data may be dropped for 
a particular duration periodically after taking aggregates or 
anomalies. So, specialised storage engines optimised for this 
purpose are required.
A time series data base is an optimised solution for 
the above challenges. The list below gives open source 
databases designed for time series data management; some 
are exclusively designed for time series needs, and a few* 
come with additional support like custom storage engines and 
schema considerations: 
 
InfluxDb, a part of InfluxData TICK stack, with 
independent storage engine
 
OpenTSDB, on top of Apache HBase/Cassandra or 
Google’s BigTable
 
KairosDB, on top of Apache Cassandra
 
Riak TS from Basho – NoSQL Time Series
 
Whisper Database, part of Graphite tool
 
Round Robin Database (RRD) tool
 
Prometheus.io - designed for monitoring metrics 
 
MongoDB*
 
PostgresQL*
In this article, let’s take a tour of the InfluxData TICK 
stack and two of its components —Telegraf for data 
collection and InfluxDb for  data storage. InfluxDb has 
Managing Time Series Data 
Using InfluxData
InfluxData is used when IoT deployments require the support of data from a large 
number, say thousands, of sensors. This data can be collected, stored and used to issue 
alerts. It is a good choice when it comes to building custom monitoring solutions.  

84 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
OpenGurus How To
an independent storage engine, whereas most other time 
series databases depend on other storage back-ends. Recent 
versions of InfluxDb are built on a Time Structured Merge 
(TSM) based BoltDB engine, which is a shift from LSM 
based LevelDB from v0.9.x.
The TICK stack comes with the following four 
components, with its unique ecosystem for complete IoT data 
management and integration with other solutions:
Telegraf
Data collection
InfluxDb
Data storage 
Chronograf
Data visualisation
Kapacitor
Data processing, alerting and ETL jobs
This article is based on v1.2.0 of the above 
components, which is the latest at present (even though 
Telegraf v1.2.1 is available). All these components are 
available in rpm, deb package formats for 64-bit Linux 
platforms and binaries for 64-bit Windows, 32-bit Linux 
and ARM platforms are also available. Official Docker 
images are also available for the above components, 
These images can be started and linked together using 
Docker Compose using the hints given at https://github.
com/influxdata/TICK-docker. One can build a custom 
Docker image using Docker build scripts available at 
https://github.com/influxdata/influxdata-docker or with 
the help of available rpm, deb packages using a base 
OS like Debian, Ubuntu or CentOS. You can also build 
these from sources using the GO build system for desired 
versions, which generates single binaries without external 
dependencies and templates for configuration files and 
necessary scripts.
Let’s now have a walkthrough of InfluxDb.
InfluxDb 
Once InfluxDb is installed, you can launch the server using 
the influxd binary directly or using init control systems like 
systemctl based on init.d or systemd scripts, with the default 
configuration file being available at /etc/influxdb/infludb.conf. 
To enable the Web admin interface, modify the configuration 
file as follows and restart the server daemon:
[admin]
enabled=true
bind-address = “:8083”
 Note: Please refer to Romin Irani’s article on 
InfluxDb published in the November 2016 edition of 
OSFY for initial pointers like database creation, and 
writing data and basic queries using the Influx CLI tool, 
Web console or HTTP REST APIs.
Additionally, InfluxData comes with various libraries to 
support APIs of different languages officially like Go, Java, 
Python, Node.js, PHP, Ruby, etc. It also supports a few third 
party libraries for, among others, Haskell, Lisp, .NET, Perl, 
Rust and Scala. A complete listing is available at
InfluxDb Documentation ==> API Client Libraries.
Input protocols and service plugins
By default, InfluxDb supports HTTP transport and 
data formats as per the line protocol, with the syntax 
shown in Figure 1.
Here, tagset and fieldset are a combination of one or 
more key-value pairs, separated by a comma. Timestamp 
is a value as per the RFC3339 format, with nano precision. 
Measurement name and at least one field key-value pair is a 
must, while tagset or timestamp are optional. 
SQL analogy
In comparison with SQL, measurements are like tables and 
points are like records; fields are equivalent to unindexed 
columns with numeric data only, on which aggregations 
can be applied; whereas tags are like indexed columns with 
any type of data (typically strings). A combination of a 
measurement with a tagset is known as a series. Influx doesn’t 
believe in pre-defined schema, i.e., in the following examples, 
measurements like temperature, humidity and pressure are 
created on-the-fly with suitable tag pairs.
Cross measurement joins are not allowed in 
InfluxQL; so, if you have any such plan, consider a 
single measurement with different tags. But, preferably, 
keep values with different meanings under separate 
measurements. Also, instead of encoding measurements or 
tag names with multiple pieces of information, use separate 
tags for each piece.
Let’s assume a few points have been written into the city 
weather database, as follows:
temperature,city=delhi,metric=celsius  value=35   
 
 
 
 
 1488731314272947906
humidity,city=pune,sensor=dht  internal=24,external=32
pressure,city=mumbai,sensor=bmp180  value=72
Data can be collected through other channels like UDP 
and other data formats, known as protocols supported by 
Graphite and OpenTSDB. This means InfluxDb can act 
like a drop-in replacement for OpenTSDB or Graphite, 
using the respective input plugins. It can also store the data 
sent by CollectD, such as performance and monitoring 
related metrics in a DevOps scenario.
Influx Query Language (QL)
InfluxQL supports SQL like query operations with typical 
Figure 1: Line protocol syntax
Measurement
,
Tag Set
space
Field Set
space
Time stamp

advertising 
mantras
Advertising Mantras • Advertising Mantras  • Advertising Mantras • Advertising Mantras • Advertising Mantras • Advertising Mantras  • Advertising Mantras • Advertising Mantras
Advertising Mantras • Advertising Mantras  • Advertising Mantras • Advertising Mantras • Advertising Mantras • Advertising Mantras  • Advertising Mantras • Advertising Mantras  • Advertising Mantras  • Advertising Mantras • Advertising Mantras  
Advertising Mantras • Advertising Mantras  • Advertising Mantras • Advertising Mantras • Advertising Mantras • Advertising Mantras  • Advertising Mantras • Advertising Mantras  • Advertising Mantras  • Advertising Mantras • Advertising Mantras  
Advertising, if done properly, can do wonders for 
any business. Here are 10 guru mantras to help 
you understand the impact of advertising and 
more importantly—how to do it right. Wish you 
speedy growth this year.
ELECTRONICS FOR YOU
ELECTRONICS BAZAAR
OPEN SOURCE FOR YOU
For more advertising mantras, visit: http://efy.in/advertising-mantras
- Mary Wells Lawrence
The best 
advertising should 
make you nervous 
about what you 
are not buying.
- Mark Twain
Many a small
thing has been
made large by
the right kind 
of advertising.
- Derby Brown
The Business That 
Considers Itself Immune 
to The Necessity for 
Advertising Sooner or 
Later Finds Itself
Immune to Business.
Branding is 
what people 
say about you 
when you are 
not in the room.
- Jef L. Richards
Creative without 
strategy is called 
ART. Creative with 
strategy is called 
ADVERTISING.
- William Bernbach
If your 
advertising goes 
UNNOTICED 
everything else
is ACADEMIC.
- David Ogilvy
A good 
advertisement 
is one which sells 
the product 
without drawing 
attention to itself.
Good advertising 
does not circulate 
information. 
It penetrates the 
public mind with 
desires and belief.
- David Ogilvy
If it 
doesn’t sell, 
it isn’t
creative.
- Stuart H. Britt
Doing Business
Without Advertising is 
Like Dancing in The 
Dark. You Know What 
You're Doing, But 
Nobody Else Does.
- William Bernbach

86 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
OpenGurus How To
clauses like FROM, WHERE, GROUP BY, ORDER BY, 
LIMIT, INTO, etc.
Let’s see some examples of time series specific queries:
SELECT min(value),max(value),mean(value) FROM temperature   
     WHERE city=’pune’and time > now()-6h GROUP BY time(10m)
SELECT min(value),max(value), mean(value) FROM temperature       
     WHERE time > now()-6h GROUP BY city,time(10m) fill(none)
SELECT min(value),max(value), mean(value) FROM temperature   
     WHERE time > now()-6h GROUP BY time(10m) fill 0
SELECT*FROM temperature WHERE time>now()-15h ORDER BY time DESC
In the first query, data is queried for the past six hours 
with a filter of Pune as the city and grouping the aggregates 
every 10 minutes. In the second query, instead of filtering by 
city we are grouping by city, and skipping the results with 
empty values. In the third query, empty values are replaced 
by a default value like zero. The GROUP BY time clause 
requires the WHERE time clause, and it’s obvious that the 
GROUP BY interval exceeding the time slot in the WHERE 
clause is not meaningful. The WHERE time clause can be 
based on absolute timestamps also. The last query provides 
results in the descending order of timestamps, which fall over 
the last 15 hours.
Transforming data into other series
One can store the results of a query into other measurements 
or series. For example:
SELECT mean(value) INTO avg_temperature FROM temperature  
 
 
WHERE time > now() - 6h 
SELECT mean(value) INTO city_wise_temperature FROM   
 
 
 
temperature GROUP BY city
Downsampling of data
Since data volume is high in IoT and DevOps scenarios, it’s 
not feasible to retain all the data for a long time. So one can 
periodically take aggregations, anomalies from original series, 
store in long term series or other forms, and flush the original 
series. InfluxDb comes with downsampling of high precision 
data using the following two features.
 Note: Here, precision refers to data volume and time 
frequency, and not the accuracy of floating values.
Retention policies: Each database comes with 
retention policies, which decide the persistence of data in 
measurements stored under it. By default, ‘autogen’ is the 
retention policy with infinite duration, i.e., data is never 
flushed out from autogen measurements. We can create 
custom retention policies for the desired duration, and select 
one of the policies as default per database.
Here is the syntax for creating a retention policy:
CREATE RETENTION POLICY <rp-name> ON <db-name> DURATION      
 <duration>REPLICATION<n>[SHARD DURATION<duration>][DEFAULT]
Please refer to Storage Engine and Glossary of Terms 
sections of the documentation for more details on storage 
engine, shard groups, shard duration, etc.
Let’s create a few retention policies for the durations of 
one year, three days, two hours and call them long-term, mid-
term and short-term policies.
CREATE RETENTION POLICY rp_lterm ON cityweather DURATION 365d 
REPLICATION 1 
CREATE RETENTION POLICY rp_sterm ON cityweather DURATION 2h 
REPLICATION 1
CREATE RETENTION POLICY rp_mterm ON cityweather DURATION 72h 
 
  
 
 
 
 
REPLICATION 1 SHARD DURATION 3h
Typically, unqualified measurements are considered 
under a default retention policy. While writing data or 
querying, measurements can be qualified with non-
default policies such as <policy-name>.<measurement> 
-- for example, rp_lterm.temperature, rp_sterm.humidity, 
autogen.pressure, etc.
Transformed data can be stored in other databases also, 
using fully qualified names in the form of <db-name>.<rp-
name>.<measurement> -- for example, weatherdb.autogen.
aggr_temperature,  weatherdb.aggr_temperature, etc.
To list out all policies or drop a particular policy, you can 
use the following syntax:
SHOW RETENTION POLICIES on <db-name>
DROP RETENTION POLICY <rp-name> on <db-name>
Continuous queries: Before flushing the data, we need 
to take out aggregations and data beyond thresholds, and 
store the resultant points in measurements under a long-term 
retention policy or autogen. For this, one can schedule a query 
for periodical execution. This is possible through continuous 
queries. Let’s consider a few examples:
CREATE CONTINUOUS QUERY "cq_temp" ON "cityweather"
BEGIN 
 
SELECT MEAN(“value”),MIN("value"),MAX("value") INTO  
 
 
autogen.aggr_temperature FROM rp_short_term.temperature  
 
WHERE city="pune" GROUP BY time(1m) 
END
CREATE CONTINUOUS QUERY "cq_humidity" ON "cityweather"
BEGIN 
 
SELECT * INTO rp_long_term.beyond_humidity FROM rp_  
 
 
short_term.humidity WHERE time > now()-1h humidity > 90 
END

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 87
OpenGurus
How To
By: Rajesh Sola
The author is a faculty member of C-DAC’s Advanced 
Computing Training School, and an evangelist in the 
embedded systems and IoT domains. You can reach him 
at rajeshsola@gmail.com.
[1]  https://www.influxdata.com
[2] http://opensourceforu.com/2016/12/introduction-
influxdb-time-series-database/
References
You can observe that measurements specified in the 
INTO clause come under the autogen or long-term policy to 
keep low precision data forever or for a longer period, and 
the FROM clause is applied on measurements with a policy 
whose duration is smaller than that of the INTO clause.
With the help of the above two features, a high 
volume of data can be efficiently reduced to low 
volumes periodically.
InfluxDb is the right choice for storing high precision 
data like hundreds of thousands of points per second, as 
it has unique downsampling support. Therefore, many 
IoT and DevOps frameworks and solutions are adopting 
Influx in place of traditional alternatives. Here is a small 
listing of use cases:
 
Plugin for monitoring of internal metrics of HiveMQ
 
For storing tenant data from various devices in 
SiteWhere 
 
Apache Spark, Kafka integrations for feeding data or 
storing the results
 
OpenHAB add-ons
 
Resource usage monitoring by Kubernetes
Let’s now take a quick look at Telegraf and its 
relation to InfluxDB.
Telegraf 
 
 
 
Telegraf is a data gathering and feeding agent for InfluxDb, 
with a rich set of plugins for collecting and reporting 
metrics for different use cases like system performance and 
monitoring systems. Once Telegraf is launched directly 
through its binary or by using service managers, it activates 
input and output plugins as specified in the supplied 
configuration file. The default configuration file is stored in 
/etc/telegraf/telegraf.conf;  you can modify it or provide a 
custom config file to the binary.
By default, input plugins related to system resources 
collect statistics like CPU, memory, diskIO, kernel, 
swapping and process related parameters. These are stored in 
respective measurements under the database named telegraf 
using InfluxDb output plugin. To check this, open the Influx 
shell in CLI or Web mode, select telegraf as the database 
and list out all measurements and series using ‘SHOW 
MEASUREMENTS’, ‘SHOW SERIES’, respectively.
Let’s consider the usage of Telegraf in an IoT 
application. If end devices or gateways need to store the 
values into a database on the remote server side, supporting 
HTTP APIs can cause overheads all the time due to frequent 
updations. To minimise this, Telegraf comes with the MQTT 
service plugin, where devices publish data to the MQTT 
broker in a lightweight manner. Here, Telegraf acts like a 
subscriber receiving all the matching data, and stores the 
collected points in InfluxDb. For this, publishing devices 
should take care that the MQTT payload should be in the 
form of InfluxDb Line protocol. To enable this integration 
of MQTT and InfluxDb, edit the telegraf configuration as 
follows. Locate inputs.mqtt_consumer in telegraf.conf and 
uncomment the lines as follows:
[[inputs.mqtt_consumer]]
qos=0
topics = [
      "telegraf/weather/temperature",
      "telegraf/weather/humidity",
       "telegraf/smarthome/#",
 ]
data_format = "influx"
You may fill a few more fields under this section 
optionally. Now, Telegraf collects payloads published with 
specified topics and stores them in the telegraf database 
by default. This has an added advantage of batch inputs, to 
minimise the usage of HTTP requests by merging multiple 
points falling under a particular interval into a single HTTP 
transaction.
So one can consider MQTT broker, Telegraf and InfluxDb 
on the same server node or in a cluster of different nodes with 
good network support; devices behind constrained networks 
publish data using the lightweight MQTT protocol.
Telegraf also comes with the MQTT output plugin 
to publish a few metrics to a broker residing on a remote 
machine. These can be used on Linux compatible targets to 
collect the following types of data:
 
On board system metrics
 
Sensor data collected through input plugin ‘sensors’
 
Data from nearby devices collected through TCP, UDP 
service plugins
Please check Telegraf documentation for a complete 
list of plugins and their usage. Once data is collected 
and stored, a dashboard has the role of visualisation of 
data. Chronograf is an immature visualisation tool from 
the TICK stack, but Grafana is a better choice with good 
integration for InfluxDb and many other databases. Apart 
from visualisation, if you wish to process the data, find 
the anomalies, schedule for alerts and do basic ETL jobs, 
Kapacitor is the right tool. 

88 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
OpenGurus Overview
T
he term ‘Internet of Things’ is used to refer to: (i) the 
global network of smart objects interconnected by 
means of Internet technologies, (ii) the set of supporting 
technologies necessary to realise this, i.e., RFIDs, sensors, 
inter-machine communicating devices, and (iii) the ensemble of 
applications and services leveraging such technologies to open 
new business and marketing opportunities.
According to a report by Gartner, 8.4 billion interconnected 
devices will be in use in the world in 2017. The Internet of 
Things presents highly novel challenges, especially to database 
management systems, like integrating tons of voluminous data 
in real-time, processing events as they stream, and dealing 
with the security of data. An example would be IoT based 
environment temperature sensors fitted in smart cities, which 
produce huge amounts of data on the temperature and humidity 
of the live atmosphere in just a few minutes. 
In order to handle IoT data effectively, it is highly 
important to find the right sort of database. But choosing 
an efficient database for IoT applications could be really 
challenging as the IoT environment is not always the same. 
There are many factors which have to be kept in mind while 
choosing a database for IoT applications.  The most important 
of these are scalability, ability to handle huge amounts of data 
at adequate speeds, flexible schema, portability with varied 
analytical tools, security and costs. 
An IoT database should have the capability of being 
fault-tolerant and highly available. If any node in the database 
cluster goes down, it should still be capable of accepting 
read and write requests. Distributed databases make multiple 
copies or replicas of data, and write the data over multiple 
servers. If any server storing the data fails, then other servers 
take over the task of storing and respond to the query till 
the failed server is up. IoT databases should be highly 
available, as the IoT database handling systems can face 
highly voluminous writes and stores. If any database server is 
down or the data write is too high for a distributed database 
in real-time, data can be stored in the messaging system until 
the database processes the backlog of data or any additional 
servers which are added to the main database cluster. 
The following are some of the top open source databases 
available for IoT based applications.
InfluxDB
InfluxDB is an open source distributed time series database 
developed by InfluxData. It is written in the Go programming 
language, and is based on LevelDB, a key-value database. 
In addition to a front-end, an HTTP interface and libraries 
are provided to users for database interaction. The main 
The Internet of Things (IoT), because of its inherent nature, requires certain features in the 
databases associated with it. This article gives a tiny selection of open source database 
management systems that are suited to usage for the IoT.
The Best Open Source 
Databases for IoT Applications 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 89
OpenGurus
Overview
advantage of InfluxDB is its capacity to aggregate values in 
time buckets on-the-fly without any manual intervention. 
InfluxDB can be accessed by software like Grafana, 
which is a powerful front-end tool providing visualisation 
features for time series data. InfluxDB has no external 
dependencies and SQL like queries are used for querying a 
data structure comprising measurements, series and points. 
Each point consists of varied key-value pairs called fieldset 
and timestamp. Values can be 64-bit integers, 64-bit floating 
points, strings and Booleans. Points are indexed by their time 
and tagset. InfluxDB stores data via HTTP, TCP and UDP.
Features
 
Purely written in the Go programming language and 
facilitates compilation into a single binary with no 
external dependencies.
 
High performance customised data store written especially 
for time series data. The TSM engine of InfluxDB allows 
efficient and high speed data storage and compression.
 
Plugins support for other data ingestion protocols like 
Graphite, collectd, OpenTSDB.
 
In-built Web front-end tool for database and user 
administration.
 
Competent in merging multiple series together.
Official website: https://www.influxdata.com/
Latest version: 1.1.1
CrateDB
CrateDB is an open source distributed SQL database 
management system developed by Crate.io Inc., which fully 
integrates a searchable document-oriented data store. Christian 
Lutz, CEO of Crate.io, said, “When we founded Crate.io we 
set out to reinvent SQL for the machine data era. Today, 75 per 
cent of our customers use CrateDB for managing machine and 
IoT because of its easy usage, performance and versatility.”
CrateDB makes machine data applications accessible 
to SQL developers; prior to this these were only possible 
using NoSQL solutions. CrateDB combines SQL with 
search versatility and ease of scalability of containers. It 
provides a good alternative to analytic data store tools like 
Splunk. The CrateDB platform includes the distributed 
SQL query engine for providing faster joins, aggregations 
and ad-hoc queries; SQL with integrated search for data 
and query versatility; and container architecture and 
automatic data sharding for simple scaling. 
The main language used by CrateDB is SQL but it also 
makes use of the document-oriented approach of NoSQL 
style databases. It uses the SQL parser from Facebook Presto 
for its query and prediction analysis. It includes an in-built 
administration interface. The Crate Shell CLI allows users to 
put up interactive SQL queries. 
Features
 
Highly scalable: Updates to the database are easy and can 
be made by simply adding new machines to update the 
cluster; there is no need for any re-distribution of data in 
the cluster as it is done automatically by CrateDB.
 
Highly available: CrateDB allows the database to be 
highly available if anything goes wrong, as it provides 
automated replication of data across the cluster; even 
hardware and software updates don’t interrupt normal 
data operations. CrateDB has the capability of self-
healing infected nodes.
 
Real-time data ingestion: CrateDB delivers millisecond 
speed query performance even if writes are taking place, 
and removes locking overheads.
 
Supports various data: CrateDB supports both relational 
as well as JSON-documents. And it also provides blob 
storage to store and retrieve videos, pictures or other 
unstructured files.
 
It supports geospatial queries and dynamic schemas, making 
CrateDB fully flexible, which is very good for Agile based 
development and IoT database storage at the back-end.
Official website: https://crate.io
Latest version: 1.0.4
Riak time series database
Riak time series (TS) database from Basho is an open source, 
distributed NoSQL key-value stored optimised database for 
the Internet of Things (IoT). With this database, the user 
can associate a large number of data points with a specific 
point in time. It is based on masterless architecture, in which 
every node in the cluster is capable of serving read and write 
requests; the distributed database automatically co-locates, 
replicates and distributes the data across the cluster to achieve 
high performance and availability. 
Riak TS database is highly optimised for data access 
requirements. It supports Apache Spark integration, which 
makes integration support possible for Spark streaming, 
dataframes and Spark SQL.
Riak TS can be installed directly on the data centre or 
public cloud. AWS Amazon Machine Images (AMI) are also 
available for this database to facilitate users to experience 
Riak TS in the AWS workspace. 
Figure 1: WebUI for InfluxDB
Read Points
Data Interface
InfluxDB features a SQL-like query language
Query
SELECT...
Times Series Name
(---)
Values
Write Point
Write Point
Execute Query

90 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
OpenGurus Overview
Features 
 
Supports addition of new nodes to the existing cluster 
architecture without sharding; data is automatically and 
uniformly distributed across the database cluster.
 
Supports DDL or Data Definition Language for table and 
field definitions, and supports storage of both structured 
and semi-structured data.
 
Supports multi-cluster replication, which facilitates 
systems administrators to replicate the data across the 
in-house data centre and any geo-location data centre 
anywhere in the world.
 
Supports SQL-like data queries by users for easy and 
flexible access to global databases.
 
Supports application integration with APIs and client 
libraries in various languages like Java, Ruby, Python, 
Erlang, Go, Node.js and .NET.
 
Riak Meso framework provides efficient cluster 
resource management and ‘push button’ scale-up/down 
for RIAK nodes.
 
Supports full integration with Apache Spark for 
operational analysis of time series data.
Official website: http://basho.com/products/riak-ts/
Latest version: 1.3
MongoDB
MongoDB is a highly powerful, flexible, free and open 
source, document-oriented, scalable and general-purpose 
database. It has the ability to scale out features such as 
secondary indexes, range queries, sorting, aggregations and 
geospatial indexes. It is classified as a NoSQL database as it 
uses JSON-like documents with schemas. 
MongoDB adds dynamic padding to documents and 
pre-allocates data files to trade extra space usage for 
consistent performance. It makes efficient use of RAM 
for caching and correcting queries for indexes. MongoDB 
supports a rich query language to support read and write 
operations (CRUD) as well as data aggregation, text search 
and geospatial queries.
Features
 
Supports generic secondary indexes for a variety of fast 
queries, and provides unique, compound, geospatial and 
full-text indexing features to users.
 
Supports ‘aggregation pipelines’ to build complex 
aggregations from simple pieces for optimisation 
of the database.
 
Supports TTL (Time-To-Live) collections for data that 
should expire after a certain period of time.
 
Supports easy-to-use protocol for storing large files and 
metadata files.
 
Supports JSON to store and transmit information. JSON, 
being standard protocol, is a great advantage for both the 
Web and the database.
 
Supports Map-Reduce on the server side for information 
processing using JavaScript functions.
 
Supports MongoDB Management Service (MMS) tool for 
allowing users to track databases and backing up the data.
 
Supports automatic load balancing configuration because 
of data placed in shards.
Official website: https://www.mongodb.com/
Latest version: 3.4
RethinkDB 
RethinkDB is an open source, distributed database primarily 
used to store JSON documents; it has the capacity of scaling 
up to multiple machines. RethinkDB is regarded as the first 
and foremost choice for developers, especially IoT based 
developers, for feeding real-time data. It has completely 
revolutionised the traditional database architecture by invoking 
a new access model to update query results to applications 
in real-time. RethinkDB offers a flexible query language for 
monitoring APIs, and is highly easy to set up and learn. 
RethinkDB offers a number of advantages over 
MongoDB. These are:
 
An advanced query language that supports table joins, sub-
queries, and massively parallelised distributed computation.
 
An elegant and powerful operations and monitoring API 
that integrates with the query language, and makes scaling 
RethinkDB dramatically easier.
 
A simple and beautiful administration UI that lets you 
shard and replicate in a few clicks, and offers online 
documentation and query language suggestions.
Features
 
Fault tolerance: It supports the automatic shift to a new 
server if the primary server fails.
 
Easy addition of nodes: Plug-and-play of nodes in real-
time, without any downtime for even a single second.
 
Asynchronous application programming interfaces: 
Supports asynchronous queries via Eventmachine 
in Ruby and Tornado.
Figure 2: Riak TS database—Web interface
Weather Data
Lookup Region*
Info
General
Lookup
Static Store
South Atlantic
Pacific
Pacific
Pacific
Values
SC
CA
OR
WA
Configuration
Lookup Region
Lookup State

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 91
OpenGurus
Overview
By: Prof. Anand Nayyar
The author is an assistant professor in the department of 
computer applications and IT at KCL Institute of Management 
and Technology, Jalandhar, Punjab. He loves to work and 
research on open source technologies, cloud computing, 
sensor networks, hacking and network security. He can be 
reached at anand_nayyar@yahoo.co.in. Watch his YouTube 
videos at Youtube.com/anandnayyar.
[1]  https://www.influxdata.com/
[2]  https://crate.io
[3]  http://basho.com/products/riak-ts/
[4]  https://www.mongodb.com/
[5]  https://rethinkdb.com/
[6]  https://www.sqlite.org
[7]  http://cassandra.apache.org
References
 
Supports SSL access to have secured access to RethinkDB 
via public Internet.
 
More functions: Supports various mathematical operators 
like floor, ceil and round.
Official website: https://rethinkdb.com/
Latest version: 2.3.5
SQLite
SQLite is an open source and embedded relational database, 
which is designed to provide an easy way for applications to 
manage data without the overhead. It is highly portable, easy 
to use, compact, efficient and reliable.
SQLite is ACID-compliant; it implements most SQL 
standards, and uses dynamically and weakly typed SQL 
syntax. SQLite engine is not a standalone process like other 
databases; it can link to static as well as dynamic applications. 
Features
 
Doesn’t require a separate server process or system to 
operate, and can operate in a serverless environment.
 
No requirement for any system administration, and needs 
a low-configuration machine for build up.
 
Self-contained and has no external dependencies.
 
Written in ANSI-C, and provides easy and simple API.
 
Cross-platform: Compatible with UNIX, LINUX, 
Windows, MAC-OS x, etc.
 
Transactions are fully ACID compatible, allowing safe 
access from multiple processes.
 
Supports all SQL queries found in SQL92.
 
Fully tested and verified code in SQLite, which is error-
free and always up-to-date.
Official website: https://www.sqlite.org 
Latest version: 3.17.0
Apache Cassandra
Apache Cassandra is regarded as a highly scalable and 
distributed open source database for managing voluminous 
amounts of structured data across many commodity servers. 
As compared to other open source databases, Cassandra offers 
various additional high performance capabilities in terms of 
availability, linear scale performance, simplicity and easy 
distribution of data across multiple database servers.
Cassandra was developed by Facebook with the prime 
motive of facilitating Inbox search and was made open 
source in 2008. It implements the ‘Dynamo-style replication 
model’ with no single point of failure, and adds a more 
powerful ‘column family’ data model.
Features
 
Massively scalable architecture: Cassandra has a 
masterless design, where all nodes are at the same level, 
which provides operational simplicity and easy scale out.
 
Masterless architecture: Data can be written and 
read on any node.
 
Linear scale performance: As more nodes are added, the 
performance of Cassandra increases.
 
Fault detection and recovery: Failed nodes can easily be 
restored and recovered.
 
Flexible and dynamic data model: Supports datatypes 
with fast writes and reads.
 
Data protection: Data is protected with commit log design 
and built-in security like backup and restore mechanisms.
 
Tunable data consistency: Support for strong data 
consistency across distributed architecture.
 
Multi-data centre replication: Cassandra provides features 
to replicate data across multiple data centres.
 
Data compression: Cassandra can compress up to 80 per 
cent data without any overhead.
 
Cassandra query language: Cassandra provides a query 
language that is similar to SQL language. This makes 
it very easy for developers moving from a relational 
database to Cassandra, to use it.
Official website: http://cassandra.apache.org
Latest version: 3.10  
Figure 3: RethinkDB interface

92 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
OpenGurus Let’s Try
D
ata analytics is today one of the most important 
needs of any organisation. There are many analytics 
platforms available, some of which are free while 
others are proprietary. Being FOSS enthusiasts, let’s analyse 
Piwik, one of the most popular, extensible, free and open 
source analytics platforms that has a range of features.
We can self-host Piwik in our production environment, 
which will provide us with 100 per cent data ownership and 
privacy. If we don’t want to manage this platform, then we 
have the option of cloud hosting— a pay-as-per-use model 
that is quite good for large organisations whose main business 
is not data analytics.
Why Piwik?
Piwik is chosen because of the following reasons:
 
It is a free and open source platform.
 
It offers mobile app support.
 
It is quick and easy to install. 
 
It enables 100 per cent data ownership and privacy. 
 
It has no data limit. 
When should Piwik be used?
Piwik offers four major services. Any organisation that 
falls within one or more of the following categories can 
use this tool.
 
 Web analytics 
If you want to increase the popularity of your website and 
want to know more about visitors and other trends, then Piwik 
can help you. It can measure the achieved targets, sign-ups, 
performance and many other things.
 
E-commerce analytics
We can integrate Piwik with e-commerce websites 
Piwik: The Feature-Rich Open 
Source Analytics Platform 
Piwik is a free and open source analytics platform which provides 100 per cent data 
ownership, is highly customisable, and protects user privacy.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 93
OpenGurus
Let’s Try
Figure 5: Database set-up completed
Figure 2: Local drive path
Figure 4: System check
to analyse profit-loss margins, revenues, and trends like 
maximum orders, average order values, etc. There are 
many platforms like Magneto, Prestashop, etc, with which 
Piwik can easily be integrated. 
 
 Server log analytics
Web servers generate lots of logs. We can use Piwik 
to collect and analyse those logs and generate meaningful 
insights. We can analyse access logs, failure and exceptions 
in logs, forecast the health of the hardware, avoid transaction 
failures and do much more.
 
 Intranet analytics 
Many government organisations, universities and 
enterprises have their own intranet network due to privacy 
issues. Their major requirement is to use analytical tools 
within their premises and Piwik provides this option. 
There are lots of gadgets and plugins available in 
the Piwik marketplace, which we can use based on 
customer use cases.
A demo of Piwik
We will see how we can install Piwik in a WAMP server on 
localhost. The installation is very easy. 
The WAMP server is used in Windows. Similarly, for 
Linux, we can use the LAMP server. Installation steps are 
mostly the same for any distribution. Here, we will consider 
Windows for the demo.
The steps given below demonstrate the configurations for 
Piwik. First, go to https://piwik.org/download/ and download 
the Piwik file as shown in Figure 1.
Now, go to the /www relative path. In my case, as I am 
using a local machine, it’s C:\wamp\www. If you are hosting 
Figure 1: Piwik download page
Open Analytics Platform
Search phrase...
Download
See what’s new in this release
DOWNLOAD PIWIK 3.0.2
ZIP – 17.1 MB
Learn more
Community
Help
Marketplace
Hosting
About
Blog
Developers
Download
Demo
Upload & Install on Your Server
Set up Piwik quickly on your own machine thanks to the 5-minute instalation.
Download the latest version of Piwik (3.0.2) for FREE!  
Piwik comes in a zip file with all the analytics goodness you need to 
unpload to your server
free/libre analytics platform
English
Database Server
127.0.0.1
Login
Password
Database Name
Table Prefix
Adapter
PDO\MYSQL
piwik_
Database Setup
INSTALLATION STATUS
2%
NEXT
Subscribe
Figure 3: Welcome page of Piwik
free/libre analytics platform
English
localhost/piwik/
Welcome!
Piwik is a free/libre web analytics software that makes it easy 
to get the information you want from your visitors.
This process is split up into 8 easy steps and will take 
around 5 minutes.
1. Welcome!
2. System Check
3. Database Setup
4. Creating the Tables
5. Super User
6. Setup a Website
7. JavaScript Tracking Code
8. Congratulations
Next
1. Welcome!
2. System Check
3. Database Setup
4. Creating the Tables
5. Super User
6. Setup a Website
7. JavaScript Tracking Code
8. Congratulations
your website, then search accordingly. Go to the www folder 
where all the projects are listed. Copy the archive file and 
extract it here. See Figure 2.
Next, go to the browser, and open the /piwik related 
path. In my case, it’s http://localhost/piwik/. If you are on a 
production server, then localhost is replaced by your domain 
name, as shown in Figure 3
Now click on Next. There are totally eight steps to 
complete the entire setup of Piwik. We will examine all 
the steps.
Step 1 is Welcome, which is done. Step 2 is the system 
check, when Piwik checks for system compatibility and 
throws errors or warnings if anything is critical. Figure 4 
provides the snapshot for this—we can see some errors. 
From the description, we can resolve the errors.

94 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
OpenGurus Let’s Try
Here, in my case, always_populate_raw_post_data=-1 
is not there in the php.ini file. Add this line in the php.ini 
file, restart the server and reload the page. Once the error is 
resolved, click the Next button.
Now you can see the database set-up screen. Before that, 
create a separate database for Piwik. Go to the phpmyadmin 
panel and create a new database. Complete the database 
set-up stage with your MySQL user name and password, 
and enter the database name which we have just created for 
Piwik. See Figure 5.
Once the database set-up is complete, click Next and go 
to the stage of creating the table.
Now we will create a super user and provide 
credentials for it (Figure 6).
Next, provide the details of the website that has 
to be analysed (Figure 7).
The next thing is the tracking code. Figure 8 shows the 
JavaScript code which we can use in websites, in WordPress, 
or in any blog or CMS platform to track and start analysing 
the website.
The Piwik set-up is now complete. Go with the default 
settings. Log in to Piwik with super user credentials.
Once you log in to Piwik, you can see that we haven’t 
started analysing the website. Go to the sample code.html 
and paste the code.
Once you reload the page, you can see the visitors and 
actions performed by them. See Figure 10
There are specific tabs for visitors, actions, referrers, 
e-commerce and goals. These are self-explanatory, so 
we will not go deep into the topic. This information will 
however help you to kickstart your own analytics platform.
By: Maulik Parekh
The author has an M. Tech degree in cloud 
computing from VIT University, Chennai. He can be 
reached at maulikparekh2@gmail.com. Website: 
https://www.linkedin.com/in/maulikparekh2.
[1] https://piwik.org/blog/
[2] https://forum.piwik.org/
[3] https://plugins.piwik.org/
References
Figure 7: Setting up of a website completed
free/libre analytics platform
English
1. Welcome!
Super User created successfuly!
Please setup the first website you would like to track and analyse with Piwik:
Website name
Ecommerce
Website time zone
Website URL
Not an Ecommerce site
Select a city
▼
▼
http://example.org
Note: once the Piwik Install is finished, you will be able to add more websites to track!
2. System Check
3. Database Setup
4. Creating the Tables
5. Super User
6. Setup a Website
7. JavaScript Tracking Code
8. Congratulations
Setup a Website
INSTALLATION STATUS
71%
Next
Analytics
I
▼
Figure 6: Super user creation completed
Super user login
Password
Password (repeat)
Email
email me with major Piwik community updates
send me information on Piwik PRO services and offers


Super User
INSTALLATION STATUS
3%
NEXT
1. Welcome!
2. System Check
3. Database Setup
4. Creating the Tables
5. Super User
6. Setup a Website
7. JavaScript Tracking Code
8. Congratulations
free/libre analytics platform
English
▼
Website Analytics created successfully!
To track your web traffic with Piwik you need to make sure some extra code is added to each of your webpages.
Make sure this code is on every page of your website before the </body> tag.
In most websites, blogs, CMS, etc, you can use a pre-mode plugin to do the technical work for you. )See our 
list of plugins used to integrate Piwik.) If no plugin exists you can edit your website templates and add this 
code in the “footer” file.
INSTALLATION STATUS
86%
free/libre analytics platform
English
▼
1. Welcome!
2. System Check
3. Database Setup
4. Creating the Tables
5. Super User
6. Setup a Website
7. JavaScript Tracking Code
8. Congratulations
Tracking code for Analytics
JavaScript Tracking Code
Figure 8: JavaScript tracking code
Figure 9: Sample Web page with JavaScript code
Figure 10: Piwik dashboard updates
Hello World
There are many competitors providing analytics platforms 
but Piwik is one of the simplest, user friendly, free and open 
source tools available for the job.
Given below are the links to blogs, forums and additional 
plugins that are readily available for use with Piwik. 


TIPS
TRICKS
&
How to find the serial number of the hard 
disk from the command line
To find the serial number of the hard disk from the 
command line, execute the following command:
#hdparm -I /dev/sda | grep Serial 
The output of the above command is the serial number 
of the /dev/sda hard disk.  
Here’s another tip to get an ASCII man page file, 
without the annoying backspace/underscore attempts at 
underlining, and the weird sequences. For example, to 
get the rsync manual page in a plain text file, execute the 
following command:
#man rsync  | col -b  >/root/rsync 
—Suresh Jagtap, 
smjagtap@gmail.com
Google through the terminal
You can now Google through your terminal, i.e., by 
using Googler. Here is the process of using it on Ubuntu.
To install it in Ubuntu, first make sure you have Python 
version 3.3 or later, by using the following command:
#python3 –version
If the version of Python is not correct, upgrade it. 
Googler requires Python 3.3+ to run.
Though Googler is not available through the package 
repository on Ubuntu, we can easily install it from the 
GitHub repository. All we have to do is run the following 
set of commands:
#cd /tmp
#git clone https://github.com/jarun/googler.git
#cd googler
#sudo make install
#cd auto-completion/bash/
#sudo cp googler-completion.bash /etc/bash_completion.d/
And that’s it. Googler is installed along with the 
command auto-completion feature.
After installing it, just run the command ‘Googler’ and 
start searching on Google.  
—Mounica Revuru,  
ojaswithamonica@gmail.com
 
Nuke a whole directory recursively in 
Posix-type systems
The shred command, which is used to irrecoverably wipe 
out individual files or drives, can be creatively combined 
with find and xargs to destroy a directory's files only 
(say, /home/moi/.ssh), as follows:
date;find /home/moi/.ssh/ -type f -print0 | xargs -0 shred 
-zfun5;date
The two date sub-commands, at the start and at the end, 
will give you the start and end times for the operation, should 
you wish to calculate the time it takes. The n5 overwrites 
each file five times with random data, so you can adjust this 
by simply varying the number five. The z flag finally fills file 
contents with 0s, u removes the files while f tries to force 
shredding by altering permission. Empty directories remain. 
Note: Do not shred the wrong location in a hurry. 
—A. Datta, webmaster@aucklandwhich.org
Creating a new user on Linux
You are working on a project and you want to keep it 
systematically arranged on your system, but looking at your 
desktop makes you wonder how to first arrange it all. Don't 
worry, you can create a new user, which will give you an 
interface similar to a new PC with everything that you have 
on your existing user already installed.
Open the terminal and enter the following command 
96 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

(let my new user's name be ‘newusername’): 
$sudo adduser newusername
It will ask you to enter the user's password twice, so 
do that step. Just press Enter on being asked the user's 
information. When prompted for the question, "Is the 
information correct [Y/N]", type Y and hit Enter. Now 
enter the following command:
$sudo visudo
You will find the following text in the file: 
# User privilege specification
root ALL=(ALL:ALL) ALL
Then add the following code ahead of it:
newusername ALL=(ALL:ALL) ALL 
Press Ctrl-X, Y and then press Enter.
You now have a new user with root access named 
‘newusername’. 
—Yash Jain, yashjain.lnm@gmail.com
Making boot process reports visible in 
Ubuntu
By default, the boot process reports of your operating 
system are hidden behind a nice boot splash screen. For 
troubleshooting and testing purposes, it can be very useful 
to make them visible.
You can do that as follows:
1. In the Grub boot menu, select the boot line of Ubuntu.
2. Press the 'E' key.
3. Remove the words ‘quiet splash’ from the boot line.
4. Press the Ctrl+X keys.
Now your system will boot once, and then your boot 
process reports will be visible. 
—Shouvik Mitra, shouvikmitra@outlook.com
Get your public IP address and host name 
With the help of the following commands you can get 
the external IP address of the server:
$curl ifconfig.me
…or: 
$curl ifconfig.co 
Here is how you can get the hostname and fully qualified 
domain name of the Linux host. Use the following commands:
# hostname 
# hostname -f   
The first command should show the short hostname and 
the second should show the fully qualified domain name 
(FQDN) of the Linux host. 
—Munish Kumar, munishtotech@gmail.com
Copy all files excluding certain file types
We often have to copy all files excluding certain file 
type(s). There are many ways to solve this problem, but 
let’s look at the easiest and most efficient solution. For this 
purpose, we are going to use the 'extglob' flag of the bash 
shell, which enables the extended pattern matching feature 
for pathnames. Let us illustrate this with an example.
We have a directory which contains various files:
[bash]$ ls
a.cpp  a.html  a.java  a.mp3  a.txt  b.cpp  b.html  b.java  
b.mp3  b.txt
Now we want to copy all files except the .txt and .mp3 
files. So first enable extended pattern matching as follows:
[bash]$ shopt -s extglob 
# enables extended pattern matching
Let us execute the following command to exclude .txt 
and .mp3 files:
[bash]$ cp !(*.mp3|*.txt) /tmp/src/
Let us verify that the required files are copied to the 
new location:
bash]$ ls /tmp/src/
a.cpp  a.html  a.java  b.cpp  b.html  b.java
Yes! It worked and we got the expected result. 
—Narendra Kangralkar,  
narendrakangralkar@gmail.com
Share Your Linux Recipes!
The joy of using Linux is in finding ways to get around 
problems—take them head on, defeat them! We invite you 
to share your tips and tricks with us for publication in OSFY 
so that they can reach a wider audience. Your tips could be 
related to administration, programming, troubleshooting or 
general tweaking. Submit them at www.opensourceforu.
com. The sender of each published tip will get a T-shirt.
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | APRIL 2017 | 97

98 | APRIL 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
DVD OF THE MONTH
Here’s something fresh for your desktop.
In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort
@ef
y.in 
for 
a fre
e re
plac
eme
nt.
CD 
Tea
m e-
mail
: cdt
eam
@ef
y.in
Rec
omm
end
ed S
yste
m Re
quire
me
nts: 
P4, 
1GB
 RA
M, D
VD-R
OM 
Driv
e
April 2017
PALV2
What is a live DVD?
A live CD/DVD or live disk contains a bootable 
operating system, the core program of any computer, 
which is designed to run all your programs as well as 
manage your hardware and software.
Live CDs/DVDs have the ability to run a complete, 
modern OS on a computer even without secondary 
storage, such as a hard disk drive. The CD/DVD directly 
runs the OS and other applications from the DVD drive 
itself. Thus, a live disk allows you to try the OS before 
you install it, without erasing or installing anything on 
your current system. Such disks are used to demonstrate 
features or try out a release. They are also used for 
testing hardware functionality, before actual installation. 
To run a live DVD, you need to boot your computer 
using the disk in the ROM drive. To know how to set 
a boot device in BIOS, please refer to the hardware 
documentation for your computer/laptop.
OSFY DVD
Manjaro KDE Edition (17.0)
This is a professional and user friendly Linux distribution 
based on the independently developed Arch operating 
system. KDE is a feature-rich and versatile desktop 
environment that provides several different styles of 
menus to access applications. You can use the 64-bit 
bundled DVD in live mode to boot your computer.
deepin 15.3
This is a Debian-based distribution that aims to provide an 
elegant, user friendly and reliable operating system. The 
bundled DVD has a bootable image of the 32-bit edition of 
deepin 15.2. The ISO image can be found in the software/
deepin folder on the root of the DVD.
10 tools for the Windows sysadmin
There are many open source tools that can be used by 
Windows sysadmins for the day-to-day administration of 
computers. We have bundled a collection of such tools 
that can help you explore open source more. You can find 
Virtual Router, VirtualBox, Performance Analysis of Logs 
(PAL), ClamWin Antivirus, UltraDefrag, Wireshark and 
more, on this DVD.

Speakers
Special Thanks To
We Thank All Our Partners And Supporters
Associate Partners
Media Partners
Digital Life Partner
Supporting Associations
Platinum Partner
Gold Partners
IoT Workshop 
Partner
Lanyard 
Partner
Visitor Badge Partner
Visitor Bag Partner
Skill India Partners
For Making
India Electronics Week 2017
A Great Success
THANK
YOU!
CO-LOCATED SHOWS:
10,367
10,367
400+
150+
11
Buyers 
Representing 
Major Firms
Brands That 
Exhibited
Unique Visitors

Earn up to 
₹1,00,000 
per hour 
 
Curious? Mail us at 
contact@loonycorn.com 
 
 Step 1: You work with us to create a course proposal 
for a 2-10 hour course 
 
 Step 2: We pay you an advance of ₹ 5,000/hour upon 
course approval 
 
 Step 3: You build the course, we help 
 
 Step 4: We grade your work and pay according to the 
rate card below (rates per hour) 
Grade A:  ₹100,000 | B:  ₹50,000 | C:   ₹25,000  | F:  ₹5,000     


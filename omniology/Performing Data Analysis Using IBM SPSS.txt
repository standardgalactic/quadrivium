

PERFORMING DATA ANALYSIS
USING IBM SPSS®


PERFORMING DATA ANALYSIS
USING IBM SPSS®
LAWRENCE S. MEYERS
Department of Psychology
California State University, Sacramento
Sacramento, California
GLENN C. GAMST
Department of Psychology
University of La Verne
La Verne, California
A. J. GUARINO
Department of Biostatistics
MGH Institute of Health Professions
Boston, Massachusetts

Copyright © 2013 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by
any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted
under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written
permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the
Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750–8400, fax (978)
750–4470, or on the web at www.copyright.com. Requests to the Publisher for permission should be
addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030,
(201) 748–6011, fax (201) 748–6008, or online at http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in
preparing this book, they make no representations or warranties with respect to the accuracy or completeness
of the contents of this book and speciﬁcally disclaim any implied warranties of merchantability or ﬁtness for a
particular purpose. No warranty may be created or extended by sales representatives or written sales materials.
The advice and strategies contained herein may not be suitable for your situation. You should consult with a
professional where appropriate. Neither the publisher nor author shall be liable for any loss of proﬁt or any
other commercial damages, including but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762–2974, outside the United States at (317)
572–3993 or fax (317) 572–4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not
be available in electronic formats. For more information about Wiley products, visit our web site at
www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Meyers, Lawrence S.
Performing data analysis using IBM SPSS® / Lawrence S. Meyers, Department of Psychology, California
State University, Sacramento, Sacramento, CA, Glenn C. Gamst, Department of Psychology, University of La
Verne, La Verne, CA, A. J. Guarino, Department of Biostatistics, MGH Institute of Health Professions,
Boston, MA.
pages cm
Includes bibliographical references and index.
ISBN 978-1-118-35701-9 (pbk.) – ISBN 978-1-118-51494-8 – ISBN 978-1-118-51492-4 (ePDF) – ISBN
978-1-118-51493-1 (ePub) – ISBN 978-1-118-51490-0 1. Social sciences–Statistical methods–Computer
programs. 2. SPSS (Computer ﬁle) I. Title.
HA32.M4994 2013
005.5’5–dc23
2013002844
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

CONTENTS
PREFACE
ix
PART 1
| GETTING STARTED WITH IBM SPSS®
1
CHAPTER 1
| INTRODUCTION TO IBM SPSS®
3
CHAPTER 2
| ENTERING DATA IN IBM SPSS®
5
CHAPTER 3
| IMPORTING DATA FROM EXCEL TO IBM SPSS®
15
PART 2
| OBTAINING, EDITING, AND SAVING STATISTICAL
OUTPUT
19
CHAPTER 4
| PERFORMING STATISTICAL PROCEDURES
IN IBM SPSS®
21
CHAPTER 5
| EDITING OUTPUT
27
CHAPTER 6
| SAVING AND COPYING OUTPUT
31
PART 3
| MANIPULATING DATA
37
CHAPTER 7
| SORTING AND SELECTING CASES
39
CHAPTER 8
| SPLITTING DATA FILES
45
CHAPTER 9
| MERGING DATA FROM SEPARATE FILES
51
PART 4
| DESCRIPTIVE STATISTICS PROCEDURES
57
CHAPTER 10 | FREQUENCIES
59
CHAPTER 11 | DESCRIPTIVES
67
CHAPTER 12 | EXPLORE
71
PART 5
| SIMPLE DATA TRANSFORMATIONS
77
CHAPTER 13 | STANDARDIZING VARIABLES TO Z SCORES
79
CHAPTER 14 | RECODING VARIABLES
83
CHAPTER 15 | VISUAL BINNING
97
v

vi
CONTENTS
CHAPTER 16 | COMPUTING NEW VARIABLES
103
CHAPTER 17 | TRANSFORMING DATES TO AGE
111
PART 6
| EVALUATING SCORE DISTRIBUTION
ASSUMPTIONS
121
CHAPTER 18 | DETECTING UNIVARIATE OUTLIERS
123
CHAPTER 19 | DETECTING MULTIVARIATE OUTLIERS
131
CHAPTER 20 | ASSESSING DISTRIBUTION SHAPE: NORMALITY,
SKEWNESS, AND KURTOSIS
139
CHAPTER 21 | TRANSFORMING DATA TO REMEDY STATISTICAL
ASSUMPTION VIOLATIONS
147
PART 7
| BIVARIATE CORRELATION
157
CHAPTER 22 | PEARSON CORRELATION
159
CHAPTER 23 | SPEARMAN RHO AND KENDALL TAU-B RANK-ORDER
CORRELATIONS
165
PART 8
| REGRESSING (PREDICTING) QUANTITATIVE
VARIABLES
171
CHAPTER 24 | SIMPLE LINEAR REGRESSION
173
CHAPTER 25 | CENTERING THE PREDICTOR VARIABLE IN SIMPLE
LINEAR REGRESSION
181
CHAPTER 26 | MULTIPLE LINEAR REGRESSION
191
CHAPTER 27 | HIERARCHICAL LINEAR REGRESSION
211
CHAPTER 28 | POLYNOMIAL REGRESSION
217
CHAPTER 29 | MULTILEVEL MODELING
225
PART 9
| REGRESSING (PREDICTING) CATEGORICAL
VARIABLES
253
CHAPTER 30 | BINARY LOGISTIC REGRESSION
255
CHAPTER 31 | ROC ANALYSIS
265
CHAPTER 32 | MULTINOMINAL LOGISTIC REGRESSION
273
PART 10
| SURVIVAL ANALYSIS
281
CHAPTER 33 | SURVIVAL ANALYSIS: LIFE TABLES
283
CHAPTER 34 | THE KAPLAN–MEIER SURVIVAL ANALYSIS
289
CHAPTER 35 | COX REGRESSION
301

CONTENTS
vii
PART 11
| RELIABILITY AS A GAUGE OF MEASUREMENT
QUALITY
309
CHAPTER 36 | RELIABILITY ANALYSIS: INTERNAL CONSISTENCY
311
CHAPTER 37 | RELIABILITY ANALYSIS: ASSESSING RATER CONSISTENCY
319
PART 12
| ANALYSIS OF STRUCTURE
329
CHAPTER 38 | PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
331
CHAPTER 39 | CONFIRMATORY FACTOR ANALYSIS
353
PART 13
| EVALUATING CAUSAL (PREDICTIVE) MODELS
379
CHAPTER 40 | SIMPLE MEDIATION
381
CHAPTER 41 | PATH ANALYSIS USING MULTIPLE REGRESSION
389
CHAPTER 42 | PATH ANALYSIS USING STRUCTURAL EQUATION
MODELING
397
CHAPTER 43 | STRUCTURAL EQUATION MODELING
419
PART 14
| t TEST
457
CHAPTER 44 | ONE-SAMPLE t TEST
459
CHAPTER 45 | INDEPENDENT-SAMPLES t TEST
463
CHAPTER 46 | PAIRED-SAMPLES t TEST
471
PART 15
| UNIVARIATE GROUP DIFFERENCES: ANOVA AND
ANCOVA
475
CHAPTER 47 | ONE-WAY BETWEEN-SUBJECTS ANOVA
477
CHAPTER 48 | POLYNOMIAL TREND ANALYSIS
485
CHAPTER 49 | ONE-WAY BETWEEN-SUBJECTS ANCOVA
493
CHAPTER 50 | TWO-WAY BETWEEN-SUBJECTS ANOVA
507
CHAPTER 51 | ONE-WAY WITHIN-SUBJECTS ANOVA
521
CHAPTER 52 | REPEATED MEASURES USING LINEAR MIXED MODELS
531
CHAPTER 53 | TWO-WAY MIXED ANOVA
555
PART 16
| MULTIVARIATE GROUP DIFFERENCES: MANOVA
AND DISCRIMINANT FUNCTION ANALYSIS
567
CHAPTER 54 | ONE-WAY BETWEEN-SUBJECTS MANOVA
569
CHAPTER 55 | DISCRIMINANT FUNCTION ANALYSIS
579
CHAPTER 56 | TWO-WAY BETWEEN-SUBJECTS MANOVA
591

viii
CONTENTS
PART 17
| MULTIDIMENSIONAL SCALING
603
CHAPTER 57 | MULTIDIMENSIONAL SCALING: CLASSICAL METRIC
605
CHAPTER 58 | MULTIDIMENSIONAL SCALING: METRIC WEIGHTED
613
PART 18
| CLUSTER ANALYSIS
621
CHAPTER 59 | HIERARCHICAL CLUSTER ANALYSIS
623
CHAPTER 60 | K-MEANS CLUSTER ANALYSIS
631
PART 19
| NONPARAMETRIC PROCEDURES FOR
ANALYZING FREQUENCY DATA
643
CHAPTER 61 | SINGLE-SAMPLE BINOMIAL AND CHI-SQUARE TESTS:
BINARY CATEGORIES
645
CHAPTER 62 | SINGLE-SAMPLE (ONE-WAY) MULTINOMINAL
CHI-SQUARE TESTS
655
CHAPTER 63 | TWO-WAY CHI-SQUARE TEST OF INDEPENDENCE
665
CHAPTER 64 | RISK ANALYSIS
675
CHAPTER 65 | CHI-SQUARE LAYERS
681
CHAPTER 66 | HIERARCHICAL LOGLINEAR ANALYSIS
689
APPENDIX
| STATISTICS TABLES
699
REFERENCES
703
AUTHOR INDEX
713
SUBJECT INDEX
715

PREFACE
The IBM SPSS® software package is one of the most widely used statistical applications
in academia, business, and government. This book, Performing Data Analysis Using IBM
SPSS, provides readers with both a gentle introduction to basic statistical computation
with the IBM SPSS software package and a portal to the more comprehensive and
statistically robust multivariate procedures. This book was written to be a stand-alone
resource as well as a supplementary text for both undergraduate introductory and more
advanced graduate-level statistics courses.
For most of the chapters, we provide a consistent structure that includes the follow-
ing:
• Overview: This is a brief conceptual introduction that furnishes a set of rele-
vant details for each statistical procedure being covered, including a few useful
references that supply additional background information.
• Numerical Example: This includes a description of the research problem or ques-
tion, the name of the data ﬁle, a description of the variables and how they are
coded, and (often) a screenshot of the IBM SPSS Data View.
• Analysis Strategy: When the analysis is performed in stages, or when alternative
data processing strategies are available, we include a description of how we have
structured our data analysis and explain the rationale for why we have performed
the analyses in the way presented in the chapter.
• Analysis Setup: This includes how to conﬁgure each dialog window with screen-
shots and is accompanied (within reason) with explanations for why we chose the
particular options we utilized.
• Analysis Output: This elucidates the major aspects of the statistical output with
pertinent screenshots and discussion.
Because of the multiple audience we are attempting to reach with this book, the com-
plexity of the procedures covered varies substantially across the chapters. For example,
chapters that cover IBM SPSS basics of data entry and ﬁle manipulation, descriptive
statistical procedures, correlation, simple linear regression, multiple regression, one-way
chi-square, t tests, and one and two-way analysis of variance designs are all appro-
priate topics for ﬁrst- or second-level statistics and data analysis courses. The remain-
ing chapters, data transformations, assumption violation assessment, reliability analysis,
logistic regression, multivariate analysis of variance, survival analysis, multidimensional
scaling, cluster analysis, multilevel modeling, exploratory and conﬁrmatory factor anal-
ysis, and structural equation modeling, are all important topics that may be suitable for
more advanced statistics courses.
There are 66 chapters in this book. They are organized into 19 sections or “Parts.”
Different authors might organize the chapters in somewhat different ways and present
them in a somewhat different order, as there is no fully agreed upon organizational
structure for this material. However, except for the chapters presented in the early parts
that show readers how to work with IBM SPSS data ﬁles, most of the data analysis
chapters can be used as a resource on their own, allowing users to work with whatever
analysis procedures meet their needs; the order in which users would choose to work with
ix

x
PREFACE
the chapters is really a function of the foundations on which the material is based (e.g.,
users should undertake structural equation modeling only after acquiring some familiarity
with regression techniques and factor analysis).
Part 1, “Getting Started With IBM SPSS®,” consists of three chapters that provide
the basics of IBM SPSS. Chapter 1 provides an introduction to IBM SPSS, Chapter 2
describes how to enter data, and Chapter 3 demonstrates how to import data from Excel
to IBM SPSS.
Part 2, “Obtaining, Editing, and Saving Statistical Output,” consists of three chapters
that describe ways of manipulating the IBM SPSS statistical output. Chapter 4 conveys
how to perform a statistical procedure. Chapter 5 demonstrates how to edit statistical
output. Chapter 6 provides information on saving and copying output.
Part 3, “Manipulating Data,” contains three chapters that focus on how to organize
existing data. Chapter 7 examines the sorting and selecting of cases. Chapter 8 demon-
strates how to split a data ﬁle. Chapter 9 discusses how to merge cases and variables.
Part 4, “Descriptive Statistics Procedures,” consists of three chapters that provide
descriptive statistical summary capabilities. Chapter 10 focuses on the analysis of fre-
quency counts for categorical variables. Chapter 11 describes how to compute measures
of central tendency and variability. Chapter 12 provides additional options to examine
variables in the data ﬁle.
Part 5, “Simple Data Transformations,” consists of ﬁve chapters that demonstrate
how to manipulate variables. Chapter 13 describes how to standardize a variable through
the creation of z scores. Chapter 14 demonstrates how to recode the values of a variable.
Chapter 15 provides a discussion of visual binning used in categorizing data. Chapter 16
demonstrates how to compute a new variable from existing data, and Chapter 17 shows
how to transform data into time variables.
Part 6, “Evaluating Score Distribution Assumptions,” consists of four chapters that
examine the assumptions underlying most of the statistical procedures covered in the
book. Chapter 18 focuses on the detection of univariate outliers, and Chapter 19 exam-
ines their multivariate counterpart. Chapter 20 focuses on the assessment of normality,
and Chapter 21 demonstrates how to remedy assumption violations through data trans-
formation.
Part 7, “Bivariate Correlation,” consists of two chapters dealing with correlation.
Chapter 22 demonstrates how to perform a Pearson product moment correlation (r), and
Chapter 23 depicts how to compute a Spearman rho and Kendall tau-b correlation.
Part 8, “Regressing (Predicting) Quantitative Variables,” consists of six chapters
dealing with simple and multiple regression and multilevel modeling. Chapter 24 cov-
ers simple linear regression. Chapter 25 demonstrates how to center a predictor variable.
Chapter 26 covers multiple linear regression. Chapter 27 covers hierarchical linear regres-
sion. Chapter 28 describes polynomial (curve estimation) regression. Chapter 29 provides
an introduction to multilevel modeling.
Part 9, “Regressing (Predicting) Categorical Variables,” consists of three chapters that
deal with logistic regression. Chapter 30 covers binary logistic regression. Chapter 31
demonstrates ROC (receiver operator curve) analysis. Chapter 32 examines multinomial
logistic regression.
Part 10, “Survival Analysis,” consists of three chapters that depict various types
of survival analysis. Chapter 33 demonstrates life table analysis. Chapter 34 covers the
Kaplan–Meier procedure. Chapter 35 demonstrates the Cox regression procedure.
Part 11, “Reliability as a Gauge of Measurement Quality,” is covered in two chapters.
Chapter 36 covers reliability analyses related to issues of internal consistency. Chapter
37 covers reliability analyses that focus on inter-rater reliability.
Part 12, “Analysis of Structure,” is covered in two chapters and deals with various
types of factor analysis. Chapter 38 covers principal components analysis and factor
analysis, and Chapter 39 covers conﬁrmatory factor analysis.

PREFACE
xi
Part 13, “Evaluating Causal (Predictive) Models,” contains four chapters that deal
with model building, as it pertains to mediation analysis and structural equation modeling.
Chapter 40 covers simple mediation analysis. Chapters 41 and 42 cover path analysis
using multiple regression and Amos, respectively. Chapter 43 provides an introduction
to structural equation modeling.
Part 14, “t Test,” consists of three chapters that cover various types of t tests. Chapter
44 demonstrates how to conduct a single sample t test. Chapter 45 covers the independent
groups t test, and Chapter 46 covers the correlated samples t test.
Part 15, “Univariate Group Differences: ANOVA and ANCOVA,” consists of seven
chapters that cover various one- and two-way analyses of variance procedures. Chapter
47 demonstrates the one-way between-subjects ANOVA using the IBM SPSS GLM
(general linear model) procedure. Chapter 48 demonstrates a trend analysis using poly-
nomial contrasts. Chapter 49 covers one-way between-subjects ANCOVA (analysis of
covariance). Chapter 50 examines two-way between-subjects ANOVA. Chapter 52 cov-
ers one-way repeated linear mixed models, and Chapter 53 examines the two-way simple
mixed design.
Part 16, “Multivariate Group Differences: MANOVA and Discriminant Function
Analysis,” covers multivariate analysis of variance (MANOVA) and discriminant function
analysis. Chapter 54 examines how to conduct a one-way between-subjects MANOVA.
Chapter 55 covers discriminant function analysis, and Chapter 56 describes how to com-
pute a two-way between-subjects MANOVA.
Part 17, “Multidimensional Scaling,” consists of two multidimensional scaling
chapters. Chapter 57 describes multidimensional scaling using the classic metric
approach, while Chapter 58 describes multidimensional scaling using the individual
differences scaling approach.
Part 18, “Cluster Analysis,” consists of two cluster analysis chapters. Chapter
59 demonstrates hierarchical cluster analysis, while Chapter 60 depicts the k-means
approach.
Part 19, “Nonparametric Procedures for Analyzing Frequency Data,” completes the
book and consists of six chapters dealing with nonparametric statistical procedures.
Chapter 61 covers the binomial test, and Chapter 62 covers the one-way chi-square
test. Chapter 63 demonstrates the two-way chi-square test with observed versus expected
frequencies. Chapter 64 demonstrates how to do a risk analysis, and Chapter 65 covers
the chi-square layers procedure. Lastly, Chapter 66 demonstrates hierarchical log-linear
analysis.
Entering ISBN 9781118357019 at booksupport.wiley.com allows users to access the
IBM SPSS data sets that were used in each of the chapters. These ﬁles can be downloaded,
and users can shadow our analyses of the data on their own computers, assuming that
they have the IBM SPSS software on such systems.
Because this book has multiple intended audiences, we recommend several different
reading strategies. For the beginning IBM SPSS user, we suggest a very careful reading
of Chapters 1 through 9, before moving into Chapters 10, 11, 12, 22, 24, 45, 47, 50,
and 62. For the advanced undergraduate student, graduate student, or researcher, the
remaining chapters should be pursued as the need arises.


P A R T 1
GETTING STARTED
WITH IBM SPSS®


C H A P T E R
1
Introduction to IBM SPSS®
1.1
WHAT IS IBM SPSS?
IBM SPSS is a computer statistical software package. This software can perform many
types of data-oriented tasks such as recoding a variable (e.g., “ﬂipping” the values of a
reverse-worded survey item). It will perform these tasks for each case in the data set,
even if there are tens of thousands of cases (a daunting job to perform by hand). IBM
SPSS can also perform a huge range of statistical procedures, ranging from computing
simple descriptive statistics such as the mean, standard deviation, and standard error of the
mean, through some fundamental procedures such as correlation and linear regression,
to a variety of multivariate procedures such as factor analysis, discriminant function
analysis, and multidimensional scaling.
SPSS at one time was an acronym for Statistical Package for the Social Sciences
but it is now treated as just a familiar array of letters. This is just as well, as researchers
from a wide array of disciplines, not just those in the social sciences, use this software.
Relatively recently, IBM purchased SPSS and beginning with version 19 has ofﬁcially
renamed the software as IBM SPSS.
1.2
BRIEF HISTORY
As described by Gamst, Meyers, and Guarino (2008), in the long-ago days, users did
not have the luxury of pointing and clicking but instead actually typed syntax (SPSS
computer code) as well as their data onto rectangular computer cards that were then
physically read into a very large mainframe computer. Eventually, the cards gave way
to computer terminals where users would type their data together with the syntax to
structure their analysis via a keyboard and CRT (cathode ray tube) screen. The software
ﬁnally reached the relatively early personal computers (PCs) in the middle 1980s, and it
has gained considerable sophistication over the years.
As the program developed, one aspect has remained consistent: the statistical pro-
cedures are still driven by syntax. As we interact with the dialog windows, IBM SPSS
is actually converting our actions and selections into its own code (syntax).
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
3

4
INTRODUCTION TO IBM SPSS
FIGURE 1.1
The three types of IBM SPSS ﬁles
with which we ordinarily work:
data ﬁles (.sav), output ﬁles (.spv),
and syntax ﬁles (.sps).
1.3
TYPES OF IBM SPSS FILES AND FILE NAME EXTENSIONS
There are three kinds of ﬁles with which we ordinarily work when using IBM SPSS:
data ﬁles, output ﬁles, and syntax ﬁles. We constantly deal with data and output ﬁles;
more seasoned users also use syntax ﬁles extensively. Each ﬁle type has its own ﬁle
name extension and distinctive icon, as shown in Figure 1.1. We discuss data ﬁles in
Chapter 2 and output ﬁles in Chapter 4 and leave any discussion of syntax ﬁles for more
specialized applications in some of the later topics covered in the book (e.g., performing
simple effects in analysis of variance). The story on each ﬁle type in simpliﬁed form is
as follows:
• Data File. This is a spreadsheet containing the data that were collected from
the participating entities or cases (e.g., students in a university class, patients
in a clinic, retail stores in a national chain). In the data ﬁle, the variables are
represented as columns; cases, as rows. This ﬁle type uses the extension .sav and
its icon shows a grid.
• Output File. This ﬁle is produced when IBM SPSS has performed the requested
statistical analysis (or other operations such as saving the data ﬁle). It contains the
results of the procedure. This ﬁle type uses the extension .spv and its icon shows
a window with a banner.
• Syntax File. This ﬁle contains the IBM SPSS computer code (syntax) that drives
the analysis. This ﬁle type uses the extension .sps and its icon shows a window
with horizontal lines.
If the extensions do not show on your screen, here is what can be done to show the
ﬁle extensions. If you are using Windows 7
• select Control Panel ➔Folder Options ➔View Tab;
• uncheck the checkbox for Hide extensions for known ﬁle types;
• click OK.
Here is what that can be done to show the ﬁle extensions in Mac OS X:
• Select Finder ➔Preferences ➔Advanced Tab.
• Check the checkbox for Show all ﬁlename extensions.
• Close the window.

C H A P T E R
2
Entering Data in IBM SPSS®
2.1
THE STARTING POINT
When opening the IBM SPSS software program, we are presented with the view shown
in Figure 2.1. We can navigate to an existent ﬁle, run the tutorial, type in data, and so
on. By selecting the choice Type in data or by selecting Cancel, we can reach the IBM
SPSS spreadsheet (the Data View display). We will select Cancel.
2.2
THE TWO TYPES OF DISPLAYS
The spreadsheet that is initially displayed is shown in Figure 2.2. This view, which is
the default display, is called by IBM SPSS the Data View because it is, quite literally,
where we enter and view our data. But as shown in Figure 2.2, it is also possible to
display the Variable View. Whether we are entering our own data or importing an already
constructed data set (as described in Chapter 3), we will need to work in both the Data
View and the Variable View screens. Although we can deal with these screens in any
order, we strongly encourage those new to IBM SPSS to begin with the Variable View
screen when entering a new data set.
2.3
A SAMPLE DATA SET
Figure 2.3 shows a very simple set of ﬁctional results of a research study just to illustrate
how to go through the steps of entering data. The variables and their meaning are as
follows:
• ID. This is an arbitrary identiﬁcation code associated with each research partici-
pant (case). The ID de-identiﬁes participants, thus protecting their anonymity and
guaranteeing conﬁdentiality. The ID also allows us to review the original data
(which should also contain the identiﬁcation codes) if data entry questions or
errors occurred.
• Gender. This indicates the gender of the participant; in Figure 2.3, M stands for
male and F stands for female.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
5

6
ENTERING DATA IN IBM SPSS
FIGURE 2.1
The screen presented on opening
IBM SPSS.
FIGURE 2.2 The initial spreadsheet is presented in the Data View display.

THE VARIABLE VIEW DISPLAY
7
FIGURE 2.3
Raw data that are to be entered in IBM SPSS.
• Extraversion. A personality characteristic indicating, roughly speaking, the degree
to which the person is outgoing. In this study, it is represented by a 10-point scale
with 1 indicating very low and 10 indicating very high.
• Sales in Thousands of Dollars. This indicates sales ﬁgures for each participant
(salesperson) during a given month in a given department of a large retail chain.
2.4
THE VARIABLE VIEW DISPLAY
Selecting Variable View at the bottom of the new (blank) spreadsheet gives rise to the
screen shown in Figure 2.4. This display is editable and allows us to specify the various
properties of the variables in the data ﬁle. The columns address the following variable
speciﬁcations:
• Name. A reasonably short but descriptive name of the variable. There is a 64-
character maximum for English, but we ordinarily want very much shorter names.
No spaces or special characters are allowed but underscores can be used. We
suggest using letters and numbers only.
FIGURE 2.4 The Variable View display.

8
ENTERING DATA IN IBM SPSS
• Type. There are several types of data (e.g., scientiﬁc notation, date, string) that
IBM SPSS can read, but we will restrict ourselves in this example to Numeric
(i.e., regular numbers), which is the default.
• Width. This is the number of spaces the data occupy. The default is 8.
• Decimals. This is the number of decimal places shown in the Data View for that
variable. The default is 2. Note that IBM SPSS computes values to 16 decimals
no matter what; if we ask to see fewer, as most of us do, the displayed values will
be rounded to the number of decimals speciﬁed here.
• Label. A phrase to describe the variable. This is often omitted by researchers if
the variable name is sufﬁciently descriptive.
• Values. When entering information on a categorical variable (such as gender), it is
appropriate for most of the analyses we cover in the book to use arbitrary numeric
codes for the categories. This is the place that we can (and absolutely should)
specify labels for each category code.
• Missing. We can designate a missing value by either leaving the cell empty in
the process of data entry or using an arbitrary numeric code. Arbitrary numeric
codes are useful when there are different reasons for deﬁning a value as missing
(e.g., the original source is not legible, there is a double answer) permitting us to
differentiate why a value may be missing.
• Columns. This is the number of spaces the data are allowed to occupy. The default
is 8.
• Align. This speciﬁes right, left, or center alignment in the Data View display. The
default is Right.
• Measure. This speciﬁes the scale of measurement of the variable. The options
are Scale (representing approximately interval-level data at a minimum), Ordinal
(containing only less than, equal to, and greater than information), and Nominal
(representing categorical data), with the default shown initially as Unknown.
• Role. There are a variety of roles that can be assigned to variables, but we will
restrict ourselves to Input, which is the default (and thus permits variables to be
placed in all of our analyses).
2.5
ENTERING SPECIFICATIONS IN THE VARIABLE
VIEW DISPLAY
In the Variable View window, double-click the cell under Name in the ﬁrst row and
type in the name of the ﬁrst variable (ID). Then click in the cell directly below to allow
IBM SPSS to ﬁll in its defaults in the ﬁrst row. This is shown in Figure 2.5. Modify the
default speciﬁcations as follows:
• Click the Decimals cell and change the speciﬁcation to 0 by clicking on the down
toggle.
• Click the Measure cell and select Scale.
The ﬁnished ﬁrst row for the ID variable is shown in Figure 2.6. We have repeated this
process for the other three variables, and the completed speciﬁcations thus far are shown
in Figure 2.7. Note the following (also shown in Figure 2.7):
• We used a short Name for the dollar amount of sales (sales) and so supplied a
Label to provide a more complete description of the variable.

ENTERING SPECIFICATIONS IN THE VARIABLE VIEW DISPLAY
9
FIGURE 2.5 The Variable View display with the ﬁrst variable typed in and the defaults showing.
FIGURE 2.6 The Variable View display with all of the variables typed in, the defaults modiﬁed,
and the ﬁrst step in specifying the gender codes showing.
• The variable gender is a categorical (nominal) variable and we have indicated that
in the Measure column; extraversion and sales are quantitative variables called
by IBM SPSS as Scale measures.
The only job remaining is to specify the value labels for the gender variable. It is a
categorical (nominal) variable and we will use the values (numerical codes) 1 for female
and 2 for male. Such codes are arbitrary and idiosyncratic to each researcher and thus
need to be speciﬁed so that (a) other researchers can understand the data and (b) these
value labels can appear in some of our output.

10
ENTERING DATA IN IBM SPSS
FIGURE 2.7 The variables now have most of their speciﬁcations.
To accomplish this, we click the cell under Values on the row for gender. This
produces the Value Labels dialog window. Follow these steps as shown in Figure 2.8 to
provide the speciﬁcations:
• Type 1 in the Value panel.
• Type female in the Label panel.
• Click Add.
• Type 2 in the Value panel.
• Type male in the Label panel.
• Click Add.
To register the labels with IBM SPSS, we click OK. The labels are now contained in
the Values cell for gender (see Figure 2.9).
2.6
SAVING THE DATA FILE
We now do something that all users should do on a very frequent basis: we will save
the data ﬁle (and will do so every time we make any modiﬁcation to it). Select File ➔
Save As (or select the Save File icon shown in Figure 2.9). This opens a standard ﬁle-
saving dialog screen in the operating system (e.g., Windows 7, Mac OS X). Navigate to
the desired location, name the ﬁle in some way that makes sense, and save it. (IBM SPSS
will display an acknowledgment of the saving operation—we close that acknowledgment
window without saving it.) The name of the ﬁle will appear in the banner of the IBM
SPSS window; double-clicking the ﬁle icon in the directory will directly open the ﬁle
next time.

ENTERING SPECIFICATIONS IN THE DATA VIEW DISPLAY
11
FIGURE 2.8
The Value Labels window with the gender codes entered.
2.7
ENTERING SPECIFICATIONS IN THE DATA VIEW DISPLAY
With the speciﬁcations now in place and the ﬁle saved, the variable names are visible
after selecting Data View and the ﬁle name is now shown in the banner (see Figure 2.10).
In the Data View spreadsheet, we simply enter our numbers, using the arrow keys or tab
key to move from cell to cell. Once the data have been entered as shown in Figure 2.10,
we again save the data ﬁle (clicking the Save File icon will overwrite the older ﬁle with

12
ENTERING DATA IN IBM SPSS
FIGURE 2.9 The Variable View display with the speciﬁcations for all of the variables now
complete.
FIGURE 2.10 The Data View with the data entered and the ﬁle saved.

ENTERING SPECIFICATIONS IN THE DATA VIEW DISPLAY
13
the modiﬁed ﬁle). A “dimmed” (“grayed”) icon indicates that the current version of the
ﬁle as shown on the screen is saved. As soon as we make any changes to it, the icon
becomes “normal” and signiﬁes that what we see on the screen is not currently saved.
In Figure 2.10, the icon is dim indicating that we have not modiﬁed the data ﬁle since
the last time it was saved.


C H A P T E R
3
Importing Data from Excel
to IBM SPSS®
3.1
THE STARTING POINT
In addition to entering data directly into IBM SPSS, it is possible to bring in (import) data
that have been entered in another software spreadsheet. We illustrate this using Excel.
The data set used in Chapter 2 was entered into an Excel worksheet and is shown in
Figure 3.1. We will import this data set into IBM SPSS.
3.2
THE IMPORTING PROCESS
As described in Section 2.1, open IBM SPSS and reach the Data View display of a
blank (new) data ﬁle. Select File ➔Open ➔Data to reach the Open Data dialog
window shown in Figure 3.2. From the Files of type drop-down menu, select Excel;
these ﬁles have a base .xls extension, some with different extra letters depending on the
version of Excel (e.g., .xlsx, .xlsm). Then navigate through the storage drives to locate
the Excel ﬁle containing the data (sales extraversion.xls). Selecting (clicking) its name
in the panel will cause the ﬁle name to appear in the File name panel. This is also shown
in Figure 3.2.
With the name and type of ﬁle now identiﬁed, select Open. This produces the
Opening Excel Data Source window shown in Figure 3.3. The checkbox for Read
variable names from the ﬁrst row of data is already checked as a default. We keep it
that way because our Excel spreadsheet contains the variable names in the appropriate
SPSS format. If the Excel ﬁle variable name is not in the acceptable SPSS format, SPSS
will assign VAR0001, VAR0002, and so on to the variables. Clicking OK initiates the
importing process, the result of which is shown in Figure 3.4.
The importing process has produced an IBM SPSS data ﬁle. It is currently untitled,
as it has just been created. This ﬁle should be saved. Once saved, we would select the
Variable View, modify the default speciﬁcations to match our data (overruling the IBM
SPSS defaults), and again save the modiﬁed version of the data ﬁle. When ﬁnished, it
would be indistinguishable from the one we created in Chapter 2.
IBM SPSS will always provide an acknowledgment of its actions; acknowledgment
of the importing process is shown in Figure 3.5. This is the syntax that mediated the
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
15

16
IMPORTING DATA FROM EXCEL TO IBM SPSS
FIGURE 3.1 Excel spreadsheet containing the data set we wish to import.
FIGURE 3.2 The Open Data dialog window.

THE IMPORTING PROCESS
17
FIGURE 3.3
The Opening Excel Data Source dialog window.
FIGURE 3.4 The imported data ﬁle needs to be named and saved.

18
IMPORTING DATA FROM EXCEL TO IBM SPSS
FIGURE 3.5 The acknowledgment of importing need not be saved.
importing of the Excel ﬁle; a good deal of speciﬁc information is shown, but the general
message is to deal with an .xls format to be found via the given path and capture all
of the data ﬁelds with their names. It is sufﬁcient to close that window without saving
(there will be a prompt requesting a decision about saving the ﬁle when we attempt to
close it).

P A R T 2
OBTAINING, EDITING,
AND SAVING STATISTICAL
OUTPUT


C H A P T E R
4
Performing Statistical Procedures
in IBM SPSS®
4.1
OVERVIEW
The Analyze menu located in the IBM SPSS main menu bar (see Figure 4.1) gives us
access to a range of statistical procedures (e.g., analysis of variance, linear regression),
and we will use it extensively, but we will also be working to a certain extent with some
of the other menus in the main menu (e.g., Data, Transform). All of this work will
involve interacting with dialog windows and, in most circumstances, obtaining output.
Performing statistical analyses is carried out in two stages: analysis setup and view-
ing/interpreting the output. Here, we treat these processes in a generic manner, without
being concerned about the details of the setup or the interpretation of the output; rather,
our purpose is to present the general process of what will be done for most analyses.
4.2
USING DIALOG WINDOWS TO SETUP THE ANALYSIS
In order to set up an analysis, it is necessary to have a data ﬁle open, as the analysis will
be performed on the active data ﬁle. We will use the data ﬁle named sales extraversion
that was created in Chapter 2 to illustrate our analysis setup, and we will call upon the
Bivariate Correlations procedure simply as a matter of convenience (we could have
chosen any other procedure as they are all structured similarly) to illustrate how to
perform a statistical analysis.
With the data ﬁle as the active window, select from the IBM SPSS menu Analyze ➔
Correlate ➔Bivariate to reach the main Bivariate Correlations dialog window shown
in Figure 4.2. The main window of a statistical procedure is the one where we identify
the variables to be included in the analysis; it is almost always the window that opens
when we invoke most procedures.
Although there are many elements contained in these main dialog windows, working
with them is pretty straightforward. Following are the major elements of a main dialog
window using Bivariate Correlations as our medium:
• The banner of the window provides the name of the procedure.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
21

22
PERFORMING STATISTICAL PROCEDURES IN IBM SPSS
a
FIGURE 4.1 The main menu of IBM SPSS.
FIGURE 4.2
The main Bivariate Correlations dialog
window.

USING DIALOG WINDOWS TO SETUP THE ANALYSIS
23
• The unnamed panel in the upper left quadrant of the window, which we will call
the Variable List panel, contains the names of the variables in the data ﬁle. They
are listed in the order that they appear in the data ﬁle but it is possible to have
them listed alphabetically (e.g., select Edit ➔Options ➔General tab and click
Alphabetical under Variable Lists as shown in Figure 4.3).
• The Variables panel identiﬁes the variables that are to be included in the analysis.
Variables may be moved into the Variables panel either by highlighting them in
the Variable Lists panel and clicking the arrow button or by double-clicking the
name of the variable.
• The pushbuttons on the far right of the main window ordinarily open subordinate
dialog screens in order to customize some aspect of the statistical analysis or to
instruct IBM SPSS to print certain information in the output.
• The pushbuttons on the bottom of the window enable some action to take place.
They must be active to be eligible for selection. For example, the OK pushbutton
(this enables the analysis to be performed) is not currently available (it is not
active) because no variables have yet been moved into the Variables panel (there
are no variables identiﬁed on which an analysis can be performed).
FIGURE 4.3 The General tab in editing the IBM SPSS system Options.

24
PERFORMING STATISTICAL PROCEDURES IN IBM SPSS
FIGURE 4.4
The main Bivariate Correlations dialog window with
extraversion and sales entered into the Variables panel.
FIGURE 4.5
The Options screen of Bivariate
Correlations.
Figure 4.4 shows the two variables of extraversion and sales after they have been moved
into the Variables panel. Note that the OK pushbutton is now active, but we will open
one of the subordinate windows to demonstrate this aspect of the setup before selecting
OK.
From the main dialog window, select the Options pushbutton. This opens the
Options dialog screen shown in Figure 4.5. By way of illustrating how to interact with
such screens, we have checked Means and standard deviations and retained the default
selection of Exclude cases pairwise (we will explain these and other options in the
context of the various analyses). Select Continue to return to the main dialog window
and select OK to perform the analysis.
4.3
THE OUTPUT
Figure 4.6 displays the results (usually called output) of the analysis. It is typical in
IBM SPSS output that much of the results of the analysis is contained in tables. Each
table is headed by a title above it. In the Bivariate Correlation procedure, the ﬁrst table
presents the means and standard deviations of the variables; these are provided in the
output because we requested in the Options dialog screen that these descriptive statistics
be displayed.

THE OUTPUT
25
FIGURE 4.6
Sample output from the Bivariate Correlations procedure.
The second table shows the Pearson correlation between extraversion and sales (a
rather exaggerated .936). In the Bivariate Correlation procedure, IBM SPSS footnotes
with asterisks show different probability levels. Here, there is only one correlation. Also
shown in the printout is Sig. (2-tailed); this is the exact probability of obtaining a
Pearson correlation of .936 based on an N of 6 cases assuming that the null hypothesis
(maintaining that the population correlation value is zero) is true.


C H A P T E R
5
Editing Output
5.1
OVERVIEW
As noted in Section 4.3, much of the output in IBM SPSS® is presented in the form of
tables. The tables produced by IBM SPSS in its output are “generic” and may be less
well formatted than we would prefer, especially if the table is to be included in a report.
However, tables in the output can be edited to a certain extent before copying them to
a word processing document or saving them to a PDF (Portable Document Format) ﬁle.
In this chapter, we illustrate some simple editing tasks that can be done.
To generate an output table so that we can illustrate these editing tasks, we have
performed a Bivariate Correlations analysis with four variables and have obtained the
table of Descriptive Statistics shown in Figure 5.1. Assume that we wish to show this
table in a presentation and, in that context we determine that (a) the Std. Deviation
column is disproportionally wide and (b) we would prefer to have the word “Standard”
written out rather than being abbreviated.
To edit this table, we double-click on it. The result is shown in Figure 5.2. We can
determine that it is editable because of three visual cues:
• It becomes outlined in a dashed line.
• It takes on a red arrow to its left.
• The table title format changes to white font on a black background.
5.2
CHANGING THE WORDING OF A COLUMN HEADING
Double-clicking the column heading Std. Deviation permits us to edit it (see Figure 5.3).
We then type out the full term (with a hard return after Standard) to replace the abbre-
viation as shown in Figure 5.4.
5.3
CHANGING THE WIDTH OF A COLUMN
We can also adjust the width of the columns. Double-clicking in the table and placing
the cursor on one of the vertical border lines of the Standard Deviation column gives
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
27

28
EDITING OUTPUT
FIGURE 5.1
A table of Descriptive Statistics obtained from the Bivariate
Correlations procedure.
FIGURE 5.2
The table is now editable.
FIGURE 5.3
Double-clicking the column heading Std. Deviation permits us
to edit it.
FIGURE 5.4
The full wording of Standard Deviation is now the column
heading.
us a double horizontal arrow as shown in Figure 5.5. Clicking and dragging this border
to the left allows us to narrow the column as shown in Figure 5.6. Clicking anyplace
outside of the table takes it out of edit mode. Having modiﬁed the ﬁle, we now save it.
5.4
VIEWING MORE DECIMAL VALUES
IBM SPSS carries out its computations to 16 decimal places, but the full range of these
decimal values is almost never displayed in the output tables, a strategy that makes a good
deal of sense, given the level of measurement precision in our research instrumentation.
Nevertheless, the fact that they are not displayed belies the fact that they are there—they
are present but hidden from view, as it is a rare occurrence when we wish to see all of
that information.

VIEWING MORE DECIMAL VALUES
29
FIGURE 5.5 In editing mode, it is possible to move the vertical column border in or out.
FIGURE 5.6
The Standard Deviation column is now narrower.
FIGURE 5.7
Double-clicking a numerical entry in a table yields the decimal
value to 16 places.
To view the full set of decimal values, we double-click the table to place it in edit
mode. Then we double-click the entry whose full decimal values we wish to see. This is
shown in Figure 5.7 where we have selected the entry for the standard deviation of social
whose value is displayed in the table as 1.36015. By double-clicking that entry, we can see
the full decimal value 1.3601470508735443 that was heretofore (and gratefully) hidden.
Note that the ordinarily displayed tabled entry is a properly rounded representation of
the full decimal value as computed by IBM SPSS.
It is common that SPSS will print a sig. value (the probability of the statistic occur-
ring by chance alone if the null hypothesis is true) to allow us to test the statistical
signiﬁcance of an obtained statistic (e.g., a Pearson r value) against the alpha level we
have established. In many of the statistical procedures, the sig. value is given to three
decimal places.
It is not uncommon for these probability values to be sufﬁciently low that the number
of zero digits well exceeds the three-decimal printing limitations in the IBM SPSS tables.
For example, the computed probability might be .000316. This conundrum is resolved
by IBM SPSS, sometimes to the dismay of students, by presenting in the output table a
sig. value of .000.
In presenting the results of any analysis in a report, probability values should never
be reported as .000; rather, they should be reported as p < .001 (American Psychological

30
EDITING OUTPUT
Association, 2009). This is because the probability is never zero, but just a very low
value. Double-clicking the displayed value of .000 will yield the longer decimal.
Some probability values may be sufﬁciently low that IBM SPSS will present them
in what is known as exponential notation. For example, the value of .000316 would be
displayed as 3.16E−4. This notation is interpreted as follows:
• 3.16 is the base nonzero numeral in the expression.
• E indicates that the value is written in exponential notation.
• The dash is a minus sign directing us to move the decimal to the left, adding zeros
as needed.
• 4 is the number of decimal places involved in the move.
Putting all this together, the exponential notation in this instance directs us to move the
decimal in 3.16 four places to the left. In order to comply, it is necessary to add three
zeros to the left of the 3; thus, the end result is .000316.
5.5
EDITING TEXT IN IBM SPSS OUTPUT FILES
Although much of the information in output ﬁles is contained in tables, text is also
produced. As an example, text in an output ﬁle will document the analysis setup by
displaying the underlying syntax. This will be the case even when IBM SPSS provides
an acknowledgment that it has carried out an instruction.
The output text can be edited. While we would not wish to change it, we may wish
to copy such text to a word processing or other document. Double-clicking on the text
gives us editing access to it, and we can, for example, copy and paste it where we wish.

C H A P T E R
6
Saving and Copying Output
6.1
OVERVIEW
The statistical analysis of our data is contained in IBM SPSS® output ﬁles. These results
need to be saved and often need to be copied into reports or other documents. In this
chapter, we show how to accomplish these operations.
6.2
SAVING AN OUTPUT FILE AS AN IBM SPSS OUTPUT FILE
In Section 1.3, we indicated that IBM SPSS output ﬁles are associated with an .spv
extension. The standard ﬁle-saving routine within IBM SPSS will save the output ﬁle in
that format. To save a newly generated output ﬁle, select File ➔Save As or select the
Save File icon (shown in Figure 2.9). This opens a standard ﬁle-saving dialog screen
in the operating system (e.g., Windows 7, Mac OS X). Navigate to the desired location
(e.g., a personal ﬂash drive), name the ﬁle, and save it.
It is possible to directly open the output ﬁle later. Double-clicking the ﬁle icon in
the directory will open the ﬁle provided that there exists on that computer the same or
a more recent version of IBM SPSS that created the ﬁle. Note that if for some reason
we are using a computer that does not have IBM SPSS (e.g., a home computer), or one
that contains an earlier version of the software, then trying to open the output ﬁle is not
possible.
6.3
SAVING AN OUTPUT FILE IN OTHER FORMATS
IBM SPSS allows for an output ﬁle to be saved in a myriad of formats including HTML,
Excel, PowerPoint, Microsoft Word/RTF, and PDF. We discuss how to do this for PDF,
but our description can also be generalized to the other formats.
A PDF document is a type of ﬁle that is in Portable Document Format. It is a
faithful copy of the original but it is not editable unless it is opened in the full version
of Adobe Acrobat or some comparable application. When PDF documents are printed
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
31

32
SAVING AND COPYING OUTPUT
or viewed on the screen, they mirror what was on the screen originally even though the
current computer may not have the fonts that were used in the document; that is what
makes them portable—the PDF contains within it all the information necessary for the
document to be displayed on the screen or printed.
IBM SPSS is capable of saving a PDF version of the output ﬁle. This is an ideal
way to view the full set of results (view a copy of the output ﬁle) when we cannot or
choose not to access IBM SPSS.
To instruct IBM SPSS to save an output ﬁle as a PDF document, the ﬁle should be
the active window (click its banner to make sure that it is active). Then select from the
main menu File ➔Export. This opens the Export Output screen shown in Figure 6.1.
Select Portable Document Format (*.pdf) from the File Type drop-down menu as
shown in Figure 6.2.
With the File Type speciﬁed as PDF, select Browse and navigate to the location
where the ﬁle is to be saved. Name the ﬁle. Figure 6.3 shows the result of this browsing
and naming process. Then click Save. This returns us to the Export Output screen.
Click OK and wait for the creation process to be ﬁnished.
FIGURE 6.1 The Export Output screen.

SAVING AN OUTPUT FILE IN OTHER FORMATS
33
FIGURE 6.2 Some of the choices in the File Type drop-down menu on the Export Output
screen.
FIGURE 6.3
Name the ﬁle in a way that suggests its
context and contents.

34
SAVING AND COPYING OUTPUT
6.4
USING OPERATING SYSTEM UTILITIES TO COPY AN IBM
SPSS TABLE TO A WORD PROCESSING DOCUMENT
As noted in Section 4.3, much of the output in IBM SPSS is presented in the form of
tables. It is not uncommon for users to save (copy) only certain output tables to a word
processing program (e.g., Microsoft Word). The two most useful forms to save such
copies are screenshots (images) and word processing tables. Both PCs and Macs have
utility programs as a part of their operating systems that can generate such copies.
Screenshots of the Active Window on a Windows-Based PC
There are utilities in the Windows operating systems that allow users to take screenshots
of windows. The quickest way to take a screenshot of the entire screen on most versions of
Windows is to tap the Print Screen key (on some keyboards this key may be designated
as Prnt Scrn). To take a screenshot of the one active window on the screen, hold down
the Alt key while tapping the Print Screen key. The screenshot in both instances is
placed on the clipboard and may be pasted into any word processing document.
Windows 7 and Windows 8 also have a user-friendly utility called the Snipping
Tool; it can be found through the path Start Button ➔All Programs ➔Accessories
➔Snipping Tool. Open the Snipping Tool. Click the Options button to conﬁrm that
Always copy snips to the Clipboard is checked. From the drop-down menu accessed
by clicking the New button, select Rectangular Snip; other choices include Free-form
Snip, Window Snip (for the active window), and Full-screen Snip. With the Snipping
Tool active (its window is visible on the screen), the screen will lighten to signal that it
is ready for the screenshot. Drag the cursor around the output table or any other area of
the screen that is to be copied and then release the cursor. The picture will be placed in
a new window (which can be saved), but it can also be placed on the clipboard. Simply
paste the picture in the desired location in the word processing document.
Screenshots of Any Part of the Output on a Mac
For Mac users, it is also possible to take a screenshot of a selected section of anything
that appears on the screen, of an open window that is visible on the screen, or of the entire
screen. The Mac OS X contains a utility called Grab (located in the Utilities folder in the
Applications folder). Open Grab and select from its Capture menu Selection, Window,
or Screen. Then follow these guidelines:
• If Selection was chosen, click and drag to create a rectangle around the portion
of the screen to be captured.
• If Window was chosen, click the inside of the window to be captured.
• If Screen was chosen, click on the screen.
After making a selection, the picture that has been taken will appear in its own window.
Save the screenshot (the Desktop may be the most convenient location) and drag its icon
into the word processing document.
An alternative utility that is even more user-friendly is SnapNDrag, available
as a free download from Yellow Mug Software (http://www.yellowmug.com). Open
SnapNDrag and select from the pushbuttons in its window Selection, Window, or
Screen. Then make the screen capture in the same way as described above for Grab.
The resulting picture appears in a miniwindow inside the SnapNDrag window. Drag
the minipicture directly into the word processing document or anyplace else (e.g., an
e-mail). It can also be dragged to the Desktop and saved.

USING THE COPY AND PASTE FUNCTIONS
35
6.5
USING THE COPY AND PASTE FUNCTIONS TO COPY
AN IBM SPSS OUTPUT TABLE TO A WORD PROCESSING
DOCUMENT
It is possible on either Macs or PCs to use the Copy and Paste commands to place a
copy of an output table into a word processing document. Several combinations of Copy
in the IBM SPSS menu and Paste in the Microsoft Word menu will accomplish the job,
and we identify one that will work for each platform:
• Move the cursor to a position inside the table that is to be copied from the IBM
SPSS output ﬁle.
• Click anyplace in the output table; this will produce a “box” around the table.
• In Mac OS X, select in the IBM SPSS menu Edit ➔Copy Special. This will open
the Copy Special dialog window with several choices all of which are checked.
Click OK. Open the Word document and set the cursor at the place where the
output table is to appear. Select in the Word menu Edit ➔Paste Special and
select Picture to obtain a screenshot or select Formatted Text (RTF) to produce
a rich text formatted word processing table.
• In Windows 7, select in the IBM SPSS menu Edit ➔Copy Special. This will
open the Copy Special dialog window with some of the several choices checked.
To produce a word processing table, select only Rich Text; to obtain a screenshot
of the table, select only Image. Click OK. Open the Word document and set the
cursor at the place where the output table is to appear. Right-click and select the
(leftmost) icon for Keep Source Formatting; whatever format was chosen in the
Copy Special dialog window will appear in the word processing document.
• Resize the screenshot as necessary or edit the word processing table for font and
size preferences as appropriate.
• Save the Word document.


P A R T 3
MANIPULATING DATA


C H A P T E R
7
Sorting and Selecting Cases
7.1
OVERVIEW
IBM SPSS® has procedures that enable us to display the data ﬁle in different ways and
to process selected portions of the data. These procedures are quite useful when there
are many more cases (rows) in the data ﬁle than we can easily view on the screen. We
discuss two of these procedures in this chapter.
7.2
SORTING CASES
Cases are arranged in the data ﬁle in the order that they were entered. Sometimes
researchers may wish to view the data ﬁle in a different case order. IBM SPSS pro-
vides a very simple procedure to sort cases in either an ascending or a descending order.
Figure 7.1 shows a portion of the data ﬁle named Personality ordered by a variable
named subid that represents an arbitrary identiﬁcation code unique to each case.
To sort the data ﬁle based on another variable (let us say age), from the IBM SPSS
main menu select Data ➔Sort Cases. This produces the Sort Cases dialog window
shown in Figure 7.2. Double-click age to move it from the Variable List to the Sort
by panel (see Figure 7.2). Ascending is the default option that is checked under Sort
Order, and we retain it. Click OK. The results of this are shown in Figure 7.3. IBM
SPSS acknowledges in an output window that it has executed the sorting by age; this
output ﬁle can be deleted without saving.
7.3
SELECTING CASES
There are occasions when researchers want to focus on a subset of the sample, for
example, one of the sexes or ethnicities or those who were coded as exhibiting symptoms
of a certain syndrome. It is possible to accomplish this using the Select Cases routine.
Cases selected by this routine are eligible to be included in any subsequent analysis;
cases excluded are not eligible to be included in any subsequent analysis.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
39

40
SORTING AND SELECTING CASES
FIGURE 7.1 The original data ﬁle.
FIGURE 7.2
The Sort Cases dialog window with age to be sorted in an
Ascending Sort Order.
We will demonstrate the selection routine with the data ﬁle shown in Figure 7.1 by
selecting the female cases in the data ﬁle. In this particular data ﬁle, males are coded
as 1; females, 2; and missing values, 9. From the IBM SPSS main menu select Data ➔
Select Cases. This produces the Select Cases dialog window shown in Figure 7.4. As
shown in the ﬁgure, we have checked the option If condition is satisﬁed; this condition
will be that the case is a female.
Selecting the if pushbutton under If condition is satisﬁed opens the Select Cases If
screen. It is possible to conﬁgure a complicated condition to be met, but it also allows
us to specify our simple condition. The steps to select females (the end result of which
is shown in Figure 7.5) are as follows:

SELECTING CASES
41
FIGURE 7.3 The data ﬁle is now sorted by age.
FIGURE 7.4
The Select Cases dialog window with If
condition is satisﬁed checked.

42
SORTING AND SELECTING CASES
FIGURE 7.5 The Select Cases: If dialog window conﬁgured to select females.
FIGURE 7.6 The data ﬁle with the female cases selected.

SELECTING CASES
43
• Double-click sex to move it from the Variable List to the panel at the top of the
screen.
• Click the equal sign on the keypad.
• Click the number 2 on the keypad (this is the code for females).
• Click the Continue pushbutton at the bottom of the screen to return to the Select
Cases dialog window.
• Click OK to perform the selection.
The result of the selection is shown in Figure 7.6. Note that cases 2, 6, 8, 9, and 10 (those
with sex code 1, indicating they are males) have a “cross-out” mark in their ﬁrst column.
This is a visual cue that these cases will be excluded from any subsequent data analysis.
Note also in the lower right corner of the data ﬁle the presence of the expression Filter
On to indicate that the data ﬁle is being ﬁltered on some basis.
The expression Filter On may be understood because the selection of a subset of
cases is mediated by a new variable named ﬁlter_$ that IBM SPSS has generated. The
new variable is placed as the last variable of the data ﬁle, but we have moved it next to
sex (in the Variable View, we highlighted the row for ﬁlter_$ and dragged it up to sex)
to make it easier to see how the two variables interface.
The sex and ﬁlter_$ variables are shown in Figure 7.7. The ﬁlter_$ variable is in
perfect synchrony with sex: where sex shows a code of 2, the ﬁlter_$ variable shows a
FIGURE 7.7 Data ﬁle screenshot showing that the sex and ﬁlter_$ variables are in synchrony;
sex code 2 is associated with x variable code 1 (select) and sex code 1 is associated with x variable
code 0 (do not select).

44
SORTING AND SELECTING CASES
FIGURE 7.8 An example of a more complex selection of cases where we are selecting for Asian-
American females.
code of 1 (representing that a case is selected), and where sex shows a code of 1, the
ﬁlter_$ variable shows a code of 0 (representing that a case is not selected).
It is important that when ﬁnished with analyses on the selected cases, researchers
should deactivate the selection. To include all cases in the analysis, select Data ➔Select
Cases, check the ﬁrst option for All cases, and click OK.
It is possible to conﬁgure more complicated selection conditions. For example, the
researchers may want to analyze data from females who are Asian-American (where
Asian-Americans are coded as 1 under the variable of ethnic). This additional speciﬁca-
tion can easily be accomplished by simply adding & ethnic=1 in the Select Cases: If
window as shown in Figure 7.8.

C H A P T E R
8
Splitting Data Files
8.1
OVERVIEW
Selecting cases in a single category of a variable (e.g., selecting females) to be more
extensively analyzed (see Section 7.3) is only one way to focus on a subset of the sample.
In other contexts, each of the categories (e.g., both males and females) may be worthy
of separate, individual examination. Using the Select Cases routine, we could select for
one category of Variable A, carry out our analyses, turn that selection off, select for a
second category of Variable A, carry out the same analyses, turn that selection off, select
for a third category of Variable A, and so on. If that is the goal, it is easier to accomplish
separate analyses using the Split File routine in IBM SPSS®.
8.2
THE GENERAL SPLITTING PROCESS
Splitting a data ﬁle in IBM SPSS has a very speciﬁc meaning. We identify a variable
whose values will represent separate groups. For example, in our Personality data ﬁle,
the variable for sex is coded 1, 2, and 9 for male, female, and missing, respectively.
If we split this variable, we would create three subordinate data ﬁles, that is, the main
data ﬁle would be ﬁrst sorted and then split (virtually rather than physically) into three
smaller data ﬁles. IBM SPSS would do this by sorting the ﬁle based on the splitting
variable so that all cases with the same sex code would be vertically adjacent to each
other in the main data ﬁle, and a ﬁlter variable, as we have seen in Section 7.3, would
be created to represent each code. We note that it is possible to create a split based on
the combination of two or more variables, but our goal here is to show the basics of the
procedure based on a single variable.
Once the split is in effect, we could perform all of the statistical analyses that were
on our research agenda. Any statistical analysis we invoked would automatically be
performed at the same time for all of the subordinate data sets. Thus, with respect to
the sex variable, we would obtain three analyses for each statistical procedure we used.
Once we had completed all of our work with the Split File, we should recombine the
subordinate ﬁles (turn off the split) and, if we wished, resort the ﬁle to match what it
was originally (assuming that was the preferred ordering of the cases).
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
45

46
SPLITTING DATA FILES
FIGURE 8.1
The Split File screen.
8.3
THE PROCEDURE TO SPLIT THE DATA FILE
We open our Personality data ﬁle and from the main menu select Data ➔Split File. As
shown in Figure 8.1, we have moved sex into the Groups Based on panel and clicked
the buttons for Compare groups (this will consolidate the output) and Sort the ﬁle by
grouping variables (the ﬁle, as may be seen in Figure 7.1, is currently sorted by subid).
Click OK. The ﬁle is now split as shown in a conﬁrmatory message in an output ﬁle.
8.4
THE DATA FILE AFTER THE SPLIT
The ﬁle is now sorted based on sex, with all cases whose code is 1 displayed ﬁrst
followed by those with the code 2 and those with the code 9. The message Split by sex
will appear in the lower right corner.
IBM SPSS has generated a new variable named ﬁlter_$ as the way it identiﬁes the
subordinate data ﬁles (we saw this in Section 7.3 when we selected cases). As we have
seen, the new variable is placed as the last variable of the data ﬁle, but we have moved it
next to sex in Figure 8.2 to make it easier to see that the two variables are in synchrony.
In IBM SPSS 20, the “ﬁlter_$” variable no longer appears; however, the lower right
corner “Split by” message will still be seen.
Figure 8.2a shows the data ﬁle as it transitions from sex code 1 to 2 (from line
142 to 143), and Figure 8.2b shows the data ﬁle as it transitions from sex code 2 to 9
(from line 421 to 422). The ﬁlter_$ variable is in perfect synchrony with sex: where sex
transitions from 1 to 2, ﬁlter_$ transitions from 0 to 1, and where sex transitions from
2 to 9, ﬁlter_$ transitions from 1 to empty cells (to represent missing values). Note that
the Split File based on the sex variable is indicated in the lower right corner of the data
ﬁle to remind users that it is in effect; in IBM SPSS 20, the lower right corner “Split
by” message will still be seen.
8.5
STATISTICAL ANALYSES UNDER SPLIT FILE
Assume that we performed a Bivariate Correlations procedure on two variables in the
Personality data ﬁle (openness as measured by the NEO Five-Factor Inventory named
neoopen and self-esteem as measured by the Coopersmith Self-Esteem Inventory named

STATISTICAL ANALYSES UNDER SPLIT FILE
47
(a)
(b)
FIGURE 8.2 Data ﬁle screenshots showing that the sex and ﬁlter_$ variables are in synchrony
for the transition from (a) sex code 1 to 2 and from (b) sex code 2 to 9.

48
SPLITTING DATA FILES
FIGURE 8.3 Correlations performed separately for males and females under Split File.
esteem) under the current Split File by sex. Figure 8.3 shows the results. For our current
purposes, note that only separate results are obtained for the males (sex code 1) and the
females (sex code 2). As there was only one case with a missing sex code, IBM SPSS
was unable to compute a correlation for that group (shown in footnote a in the table).
The potential usefulness of separate analyses is seen here, in that the Pearson r between
openness and self-esteem was somewhat different for males and females (.28 and .19,
respectively).
8.6
RESETTING THE DATA FILE
Once we have completed all of our analyses under Split File, we should reset the data
ﬁle. Select Data ➔Split File. Highlight the sex variable in the Groups Based on panel
and then click the arrow to its left (now pointing back toward the Variable List, as shown
in Figure 8.4). Then select the top radio (little round) button associated with Analyze all
cases, do not create groups (see Figure 8.5) and click OK.

RESETTING THE DATA FILE
49
FIGURE 8.4
As the ﬁrst step to resetting the data ﬁle, we highlight sex
and click the arrow pointing toward the Variable List to
return sex to that list.
FIGURE 8.5
As the last step to resetting the data ﬁle, we select
Analyze all cases, do not create groups and click OK.
Finally, we should delete the ﬁlter_$ variable (if we are in IBM SPSS 19) and resort
the data ﬁle. To accomplish the deletion, set the data ﬁle to Variable View, highlight
the row corresponding to the ﬁlter_$ variable, and select from the main menu Edit ➔
Clear to remove the variable. Then Sort Cases based on subid and save the data ﬁle in
that form.


C H A P T E R
9
Merging Data from Separate Files
9.1
OVERVIEW
It is not uncommon to have different sets of data each of which comprises a portion of
the data for a particular study. Two circumstances when this happens are as follows:
• Different members of the research team have collected and/or entered data into
different IBM SPSS® data ﬁles on all of the variables but for different cases. In
such a situation, we want to combine all of the cases together into a single data
ﬁle. IBM SPSS labels this as Add Cases.
• Information on different variables for the full set of cases is recorded or housed in
different IBM SPSS data ﬁles. In such a situation, we want to combine all of the
variables together into a single data ﬁle. IBM SPSS labels this as Add Variables.
9.2
ADDING CASES
We use the very simple (for illustration purposes) data ﬁle Self Regard, a screenshot of
which is shown in Figure 9.1. It contains an identiﬁcation variable named subid, two
demographic variables (sex and age), and one personality variable (regard). The ﬁve
cases in the ﬁle are sorted on subid.
Five additional cases, subid # 6 through subid # 10, are contained in the ﬁle Self
Regard adds currently residing in the ﬂash drive. This additional data ﬁle, shown in
Figure 9.2, contains the same variables as our Self Regard data ﬁle and is also sorted
on the subid variable.
Our goal is to bring the data from Self Regard adds into Self Regard. Although
we could copy and paste from one data ﬁle to another, very large data ﬁles are probably
best combined using the IBM SPSS Merge Files procedure that we demonstrate here.
We open our Self Regard data ﬁle and from the main menu select Data ➔Merge
Files ➔Add Cases. This opens the Add Cases dialog window (see Figure 9.3). We
have selected An external SPSS Statistics data ﬁle because Self Regard adds is not
currently open. We have navigated to the ﬁle on the ﬂash drive and selected it.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
51

52
MERGING DATA FROM SEPARATE FILES
FIGURE 9.1 Our primary Self Regard data ﬁle.
FIGURE 9.2 The Self Regard adds data ﬁle; the data here are to be added to the Self Regard
data ﬁle.
Clicking Continue opens the main Add Cases dialog window as presented in
Figure 9.4. The panel for Unpaired Variables contains variables that do not match
across the two data ﬁles (e.g., names do not match, variables with the same name are
deﬁned as numeric in one ﬁeld and as string in the other, sets of string variables have
different deﬁned lengths). As all of the variables match, this panel is empty.
The panel for Variables in New Active Dataset contains those variables that match
across the two data ﬁles and will therefore be combined. We can remove any variables
here that we wish; in our example, we retain all of the variables.
Clicking OK accomplishes the merge, as shown in Figure 9.5. We save this ﬁle
under a different name (Self Regard merged) to preserve the original ﬁle by selecting
File ➔Save As (see Figure 9.6).

ADDING CASES
53
FIGURE 9.3
The initial Add Cases dialog screen.
FIGURE 9.4
The main Add Cases dialog screen.
FIGURE 9.5
The additional cases have been added to Self
Regard.

54
MERGING DATA FROM SEPARATE FILES
FIGURE 9.6
We save the merged data ﬁle under a
new name.
FIGURE 9.7
The Self Control data ﬁle; the data
here are to be added to the Self
Regard merged data ﬁle.
9.3
ADDING VARIABLES
To illustrate how to combine variables, we will merge the variable selfcon contained in
the data ﬁle Self Control into the Self Regard merged data ﬁle we just built in Section
9.2. As shown in Figure 9.7, Self Control contains subid and selfcon with the same
cases as those in the Self Regard merged data ﬁle ordered on the subid variable; this
matches the ordering of the Self Regard merged data ﬁle (IBM SPSS requires that the
variables must be sorted on the same basis in both ﬁles).

ADDING VARIABLES
55
FIGURE 9.8 The initial Add Variables dialog screen.
FIGURE 9.9
The main Add Variables dialog screen.
We open our Self Regard merged data ﬁle and from the main menu select Data
➔Merge Files ➔Add Variables. This opens the Add Variables dialog window (see
Figure 9.8). We have selected An external SPSS Statistics data ﬁle because Self Control
is not currently open, navigated to the ﬁle on the ﬂash drive, and selected it.
Clicking Continue opens the main Add Variables dialog window as presented in
Figure 9.9. The panel for Excluded Variables contains the variable(s) in Self Control
that duplicate what we already have in Self Regard merged and therefore will not be
included in the merge. The variables in the New Active Dataset list the variables from
both ﬁles. Those marked with an asterisk (*) are already in Self Regard merged; the
variable selfcon is marked with a plus sign (+) to indicate that it will be added to the
set. The Key Variables panel is properly blank here, but if some cases were missing in
one of the data ﬁles, we could identify the variable whose value can be used to match
the cases so that the new variable(s) can be merged.

56
MERGING DATA FROM SEPARATE FILES
FIGURE 9.10 The additional variable selfcon has been added to Self Regard merged.
Clicking OK accomplishes the merge, as shown in Figure 9.10. Once again, this
new data ﬁle should be saved.

P A R T 4
DESCRIPTIVE STATISTICS
PROCEDURES


C H A P T E R
1 0
Frequencies
10.1
OVERVIEW
Once the data are veriﬁed as correctly entered, one of the ﬁrst steps researchers perform
as part of their data analysis is generating descriptive statistics on the variables in the
study. The Frequencies procedure in IBM SPSS® is one of the procedures available for
this purpose.
In generating such statistics, it is important to distinguish between variables assessed
on a nominal or categorical scale of measurement from those assessed on a quantita-
tive (summative response, interval, or ratio) scale of measurement (Meyers, Gamst, &
Guarino, 2013). For categorical variables, our only option is to determine the frequen-
cies of cases classiﬁed into each category (e.g., the number of cases in each ethnicity
category). Other descriptive statistics, such as the mean and standard deviation of such
a variable with more than two categories, are not interpretable values and so should not
be requested.
For quantitative variables, we often do have an interest in the number of cases
represented by each value of the variable, but our interest usually diminishes with greater
numbers of possible values. For example, we would be more interested in the number of
cases choosing 1, 2, 3, 4, and 5 on a 5-point response scale (e.g., to determine that all
scale points are being selected with reasonable frequency) than in the number of cases
whose score on a measure of extraversion was 31, 32, 33, and so on all the way to
70. But we would always want to obtain other descriptive statistics providing us with
information about the central tendency, variability, and shape of the distribution.
10.2
NUMERICAL EXAMPLE
The data we use for our example are extracted from a study of personality variables on
425 university students. Variables include demographics as well as personality measures.
Data are contained in the data ﬁle named Personality.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
59

60
FREQUENCIES
FIGURE 10.1
The main dialog window of Frequencies.
10.3
ANALYSIS SETUP: CATEGORICAL VARIABLES
We open the data ﬁle named Personality and from the main menu select Analyze ➔
Descriptive Statistics ➔Frequencies. The Frequencies procedure is appropriate to use
for both categorical and quantitative variables. To illustrate an analysis of categorical vari-
ables, we have moved sex and ethnic into the Variable(s) panel as shown in Figure 10.1.
We have also checked the checkbox Display frequency tables below the Variable List
panel. As the choices available in the Statistics screen apply to quantitative variables,
we just click OK to perform the analysis.
10.4
ANALYSIS OUTPUT: CATEGORICAL VARIABLES
The frequency tables for the sex and ethnic variables are shown in Figure 10.2. Following
is a brief description of the information contained in each column of the tables:
• The ﬁrst column (not labeled) lists the values (categories) associated with each
variable together with their labels (the ones that were typed in the Variable View
of the data ﬁle when the data were entered). Valid values appear ﬁrst in sequential
order followed by Missing values (in this data ﬁle, the code 9 is used to indicate
a missing value).
• Frequency provides a count of cases for each value. For example, there are 31
participants in the sample of Asian descent.
• Percent represents the percent of cases in each category with respect to the total
set of cases. For example, the 31 participants of Asian descent represent 7.3% of
the total sample of 425.
• Valid Percent represents the percent of cases in each category with respect to the
valid number of cases. For example, the 31 participants of Asian descent represent
7.4% of the 421 cases who have valid values on the ethnic variable.
• Cumulative Percent just continually adds in cases listed in order and computes the
growing percent with respect to the valid number of cases. Because the coding of
most categorical variables is arbitrary (as is true here where the groups are listed
in alphabetic order), this computation is ordinarily not particularly useful for such
nominal variables.

ANALYSIS SETUP: QUANTITATIVE VARIABLES
61
FIGURE 10.2 Frequency tables for two categorical variables.
10.5
ANALYSIS SETUP: QUANTITATIVE VARIABLES
We open the data ﬁle named Personality and from the main menu select Analyze ➔
Descriptive Statistics ➔Frequencies. To illustrate an analysis of quantitative variables,
we have moved neoopen (a measure of openness to new experiences from the NEO
Five-Factor Inventory; Costa & McCrae, 1992) and neoneuro (a measure of neuroticism
from the NEO Five-Factor Inventory) into the Variable(s) panel as shown in Figure 10.3.
We have also checked the checkbox Display frequency tables below the Variable List
panel to show such output.
Selecting the Statistics pushbutton opens the Statistics screen shown in Figure 10.4.
We select the following statistics to illustrate what is available in this procedure:
• Under Percentile Values, we select Quartiles.
• Under Central Tendency, we select Mean and Median.
• Under Dispersion, we select Std. deviation, Minimum (the lowest valid value in
the distribution), Maximum (the highest valid value in the distribution), and S.E.
mean (the standard error of the mean).
• Under Distribution, we select Skewness and Kurtosis.
We select Continue to return to the main dialog window, and click the Charts pushbut-
ton. In the Charts window shown in Figure 10.5, we select Histograms and opt not to

62
FREQUENCIES
FIGURE 10.3
The main dialog window of Frequencies.
FIGURE 10.4
The Statistics screen of Frequencies.
FIGURE 10.5
The Charts screen of Frequencies.

ANALYSIS OUTPUT: QUANTITATIVE VARIABLES
63
superimpose a normal curve on it (we do not check Show normal curve on histogram).
We select Continue to once again return to the main dialog window and click OK to
perform the analysis.
10.6
ANALYSIS OUTPUT: QUANTITATIVE VARIABLES
The descriptive statistics for neoopen and neoneuro is shown in Figure 10.6. The scores
for these two variables are standardized as linear T scores (mean of 50 and a standard
deviation of 10) based on the national norms. Given the means in the table, our sample
appears to be average in neuroticism and half a standard deviation more open than the
norm.
The standard error of the mean (Std. Error of Mean) is computed by dividing the
standard deviation of the scores by the square root of N, where N is the valid sample
size. Conceptually, the standard error of the mean represents the standard deviation of
the sample means that we would obtain if we were to draw an inﬁnite number of random
samples (based on a sample size N) from the population. We presume that this distribution
of (hypothetical) sample means is normally distributed. Given that we cannot actually
draw an inﬁnite number of samples and determine the mean of each so that we can
compute their standard deviation, we use the above formula to estimate the value of this
statistic.
One of main uses of the standard error of the mean is to serve as a basis for computing
a conﬁdence interval around the mean. To compute the 95% conﬁdence interval, for
example, we multiply the obtained value of the standard error of the mean (.53233 for
neoopen and .54396 for neoneuro) by 1.96 (a z value of ±1.96 subsumes 95% of the
area under a normal distribution) and subtract and add those values from and to the
FIGURE 10.6
The descriptive statistics output.

64
FREQUENCIES
mean to identify the lower and upper values of the conﬁdence interval, respectively. In
the present instance, using neoopen to illustrate this, our computations are as follows:
• 95% Band = 0.53233 * 1.96 = 1.04337
• Lower conﬁdence value = mean −95% band = 55.4607 −1.04337 = 54.42
• Upper conﬁdence value = mean + 95% band = 55.4607 + 1.04337 = 56.50
Thus, if hypothetically we were to repeatedly draw 420 cases randomly from the popu-
lation an inﬁnite number of times, then 95% of the time the sample mean for openness
is expected to be between 54.42 and 56.50. Another way to express this is to assert with
95% conﬁdence that the population mean is in the range 54.42–56.50.
The Minimum and Maximum values in Figure 10.6 are decimal values because the
scores are in standardized form based on the national norms (rounded to two decimal
places). Thus, for neoopen, at least one case obtained a standard score of 27.51 and at
least one case obtained a standard score of 79.05. Values corresponding to the 25th, 50th,
and 75th percentiles are shown in Figure 10.6 at the bottom of the table.
Skewness describes the degree of asymmetry exhibited by the distribution of scores.
The normal curve (being symmetric) has a skewness value of 0. Positive skewness
describes the situation where the bulk of scores is toward the relatively lower values of
FIGURE 10.7
Histograms for the two quantitative variables.

ANALYSIS OUTPUT: QUANTITATIVE VARIABLES
65
the variable (the “tail” of the distribution points toward the positive end of the X-axis);
negative skewness describes the situation where the bulk of scores is toward the relatively
higher values of the variable (the “tail” of the distribution points toward the negative end
of the X-axis).
Kurtosis describes the degree to which the distribution is compressed or ﬂattened
with respect to the normal curve. The normal curve has a kurtosis value of 0, known as
mesokurtosis. Positive kurtosis, known as leptokurtosis, indicates that, compared to the
normal curve, the distribution is more compressed toward the center; negative kurtosis,
known as platykurtosis, indicates that, compared to the normal curve, the distribution is
relatively ﬂattened.
The results shown in Figure 10.6 indicate that skewness and kurtosis are well within
±1.00 and thus suggest, respectively, that the distributions are (a) relatively symmet-
ric and (b) neither very compressed nor spread out. This can be most clearly seen in
the histograms presented in Figure 10.7. The distribution of openness exhibits a bit of
leptokurtosis (negative kurtosis) as its value of −.472 approaches the ±0.5 threshold of
what some researchers treat as suggestive of a bit of compression, but it really does not
give the appearance of anything untoward; however, with a standard error of .238, its
95% band (1.96 * 0.238 = 0.467) places the lower conﬁdence band at almost −1.00
(−0.938), a value that borders on what many would regard as at least mild compression
of the distribution. In comparison, neuroticism with its kurtosis value of −.263 is a bit
more balanced.
The ﬁrst and last parts of the frequency table for neoopen are shown in Figure 10.8,
as the full table is quite lengthy. Each value in the data ﬁle is represented in the table
together with the frequency with which that value occurred. For example, the value 30.95
occurred once but the value 37.82 occurred eight times (i.e., eight cases had that value).
Totally, there were 420 valid values and 5 missing values.
FIGURE 10.8
Frequency tables for the two quantitative
variables.


C H A P T E R
1 1
Descriptives
11.1
OVERVIEW
In addition to the Frequencies procedure described in Chapter 10, IBM SPSS® also has
the Descriptives procedure, which is applicable to only quantitative variables. It produces
most of the same statistics as the Frequencies procedure but neither contains percentiles
and nor provides graphic output. Its main virtues are that it allows us to transform raw
scores to standardized (z) scores (see Chapter 13) and provides the output in a form that
is convenient for researchers to compare variables.
11.2
NUMERICAL EXAMPLE
The data we use for our example are extracted from a study of personality variables on
425 university students. Variables include demographics as well as personality measures.
Data are contained in the data ﬁle named Personality.
11.3
ANALYSIS SETUP
We open the data ﬁle named Personality and from the main menu select Analyze ➔
Descriptive Statistics ➔Descriptives. We will generate descriptive statistics on the
same variables used in Chapter 10 to allow readers to compare Descriptives with Fre-
quencies. Thus, we move neoopen and neoneuro into the Variable(s) panel as shown
in Figure 11.1. We have not checked Save standardized values of variables below the
Variable List panel because we will do this as a part of Chapter 13.
Selecting the Options pushbutton opens the Options screen shown in Figure 11.2.
We select the following statistics to illustrate what is available in this procedure:
• In the ﬁrst (unlabeled) row in the window, we select Mean.
• Under Dispersion, we select Std. deviation, Minimum (the lowest valid value in
the distribution), Maximum (the highest valid value in the distribution), and S.E.
mean (the standard error of the mean).
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
67

68
DESCRIPTIVES
FIGURE 11.1
The main dialog window of Descriptives.
FIGURE 11.2
The Options screen of Descriptives.
FIGURE 11.3 The descriptive statistics output.
• Under Distribution, we select Skewness and Kurtosis.
• Under Display Order, we select Variable list (the variables could also be ordered
alphabetically or by the value of their means).
We select Continue to return to the main dialog window and click OK to perform the
analysis.

ANALYSIS OUTPUT
69
11.4
ANALYSIS OUTPUT
The results of the analysis are presented in Figure 11.3, which duplicate the values we
obtained from the Frequencies procedure (see Figure 10.6).


C H A P T E R
1 2
Explore
12.1
OVERVIEW
Explore is still another procedure in addition to the Frequencies and Descriptives pro-
cedures that is designed to generate descriptive statistics. Explore is also applicable to
only quantitative variables but is able to produce more types of output than either of the
other two procedures (except for generating standardized scores).
12.2
NUMERICAL EXAMPLE
The data we use for our example are extracted from a study of personality variables on
425 university students. Variables include demographics as well as personality measures.
Data are present in the data ﬁle named Personality.
12.3
ANALYSIS SETUP
We open the data ﬁle named Personality and from the main menu select Analyze
Descriptive Statistics Explore. We will generate descriptive statistics on the same vari-
ables used in Chapters 10 and 11 to allow readers to compare Explore with Descriptives
and Frequencies. Thus, we move neoopen and neoneuro into the Dependent List panel
of the main dialog window as shown in Figure 12.1.
We leave Factor List empty for our illustration, but this is a way to obtain separate
descriptive statistics for each category of a nominal variable. For example, if we placed
sex in that panel with neoopen and neoneuro in the Dependent List panel, we would
obtain separate analyses for males and females on those two variables; this would yield
the same result as executing a Split File by sex and running the Explore procedure on
neoopen and neoneuro.
Selecting the Statistics pushbutton opens the Statistics screen shown in Figure 12.2.
We select Descriptives (this displays a set of default descriptive statistics) and retain 95
in the panel for Conﬁdence Interval for Mean (this calculates the lower and upper 95%
conﬁdence boundaries of the mean). We also select Percentiles to obtain a variety of
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
71

72
EXPLORE
FIGURE 12.1
The main dialog window of Explore.
FIGURE 12.2
The Statistics screen of Explore.
percentile values in addition to the quartiles. We select neither M-estimators (these are
a set of alternative ways to weigh cases that we do not address in this book) and nor
Outliers (we address this in Chapter 18).
We select Continue to return to the main dialog window and select the Plots push-
button to reach the screen shown in Figure 12.3. In the Boxplots area, we select Factor
levels together. As we have not speciﬁed any factors in the main dialog window, this is
a moot point; if we had a factor, then we could group the boxplots by either factors or
the dependent variables. We also select Histogram in the Descriptive area so that we
can compare this plot to that produced by the Frequencies procedure.
We select Continue to return to the main dialog window and select the Options
pushbutton to reach the screen shown in Figure 12.4. We choose Exclude cases pairwise.
This choice will allow the computation of the descriptive statistics for each variable to
be based on different numbers of cases, that is, the number of valid values might differ
from variable to variable. In multivariate analyses (where multiple variables are analyzed
simultaneously), all cases must have valid values on all of the variables included in the
analysis and thus listwise selection is in effect in those analyses (any case with a missing
value on any of the variables in the analysis is removed from the analysis). We can
use the same set of cases as would be in the multivariate analysis by selecting Exclude
cases listwise. We select Continue to return to the main dialog window and click OK
to perform the analysis.

ANALYSIS OUTPUT
73
FIGURE 12.3
The Plots screen of Explore.
FIGURE 12.4
The Options screen of Explore.
12.4
ANALYSIS OUTPUT
The descriptive statistics are shown in Figure 12.5. The statistics duplicate the values we
obtained from the Frequencies and Descriptives procedures, but we obtain additional
statistics, which include the following.
• 95% Conﬁdence Interval for the Mean Lower and Upper Bounds. These values are
computed by multiplying the standard error of the mean (not part of the output)
by 1.96 and subtracting and adding those respective values to obtain the upper and
lower boundaries, respectively. These values match (within rounding error) those
we calculated in Chapter 10.
• 5% Trimmed Mean. This is the mean of the variable when the top and bottom
5% of the scores have been excluded from the computation. The trimmed mean
removes relatively extreme scores from both ends of the distribution. It is a very
rough way to determine if there are outliers at one of the ends of the distribution
based on the following reasoning:
– If the relatively extreme scores were symmetrically present in both the lower
and upper ends of the distribution, the trimmed mean should be very similar in
value to the full mean.
– If the relatively extreme scores were disproportional in the lower or higher end
of the distribution, this trimmed mean should differ sufﬁciently from the full
mean to be noticeable.
• Interquartile Range (IQR). The absolute difference between the scores correspond-
ing to the ﬁrst and third quartiles (the 25th and 75th percentiles).

74
EXPLORE
FIGURE 12.5 The descriptive statistics output.
FIGURE 12.6 Percentile output.
The Percentiles output is shown in Figure 12.6. The ﬁrst main row gives results for
the 5th, 10th, 25th, 50th, 75th, 90th, and 95th percentiles for each variable. The second
major row identiﬁes the quartiles. These are called Tukey’s Hinges after the eminent
statistician John Tukey who introduced boxplots (Tukey, 1977); the so-called hinges in
Figure 12.6 are the ﬁrst and third quartiles that are shown in the boxplot (Meyers et al.,
2013).

ANALYSIS OUTPUT
75
FIGURE 12.7
Boxplots for the two variables.
Tukey’s hinges may be seen in the boxplots presented in Figure 12.7. The vertical
axis in the IBM SPSS® plot depicts percentiles. Because percentiles indicate the propor-
tion of scores that are above the given score, higher scores and thus higher percentiles
are toward the top (a very high score would be in the 99th percentile and at the top of
the plot).
The rectangle in the approximate center of each boxplot represents the middle portion
of the distribution. The median (50th percentile) is marked by the heavy horizontal line
and the lower and upper boundaries of the rectangle are approximately the ﬁrst and third
quartiles (25th and 75th percentiles, respectively, although Tukey conceived of them as
the halfway points between the median and each end of the distribution); these three
percentiles are computed by IBM SPSS as Tukey’s hinges.
The vertical lines extending from the rectangle or “box” are the whiskers (hence
the name box and whiskers plot sometimes given to the display), and their ends are the
lower and upper inner fences, respectively. These fences are drawn at a distance of ±1.5
IQR, which is roughly about ±1.6 standard deviation units from the mean. Data points
exceeding ±1.5 IQR but less than ±3.0 IQR are designated by circles in the IBM SPSS
output; data points exceeding ±3.0 IQR are designated by the letter E. In Figure 12.7,
the neuroticism data point for Case 120 lies beyond the upper inner fence and is drawn
as a circle; we thus know that Case 120 scored quite high on this measure and may
qualify as an outlier.

76
EXPLORE
FIGURE 12.8
Histograms for the two variables.
The histograms are shown in Figure 12.8. These are identical to those produced by
the Frequencies procedure presented in Chapter 10.

P A R T 5
SIMPLE DATA
TRANSFORMATIONS


C H A P T E R
1 3
Standardizing Variables to z Scores
13.1
OVERVIEW
In Chapter 11, we indicated that the Descriptives procedure can be used to transform
the values of a variable to z scores. A z score is the basic standardized score and is a
direct index of how many standard deviation units a given raw score is from the mean
of the distribution (the distance from the mean in standard deviation units). It can be
hand-calculated by subtracting the mean from the raw score and dividing that difference
by the standard deviation: z = (X −M)/SD. Positive z scores indicate that the score is
greater than the mean; negative z scores indicate that the score is less than the mean.
Thus, a z score of 1.25 informs us that the raw score is greater than the mean by one
and a quarter standard deviations, and a z score of −0.75 informs us that the raw score
is less than the mean by three-quarters of a standard deviation.
We can compare these two cases in the sample in terms of how they performed on
the variable: assuming higher values represent more of the construct being measured,
we could say that the case whose z score is 1.25 appears to rate much higher on the
construct than the case whose z score is −0.75. We can also use z scores to compare
performance across measures within a single sample. For example, we can convert two
variables to their z score equivalents to determine immediately how consistent a given
case fared across the two variables.
13.2
NUMERICAL EXAMPLE
The data we use for our example are extracted from a study of personality variables on
425 university students. Data are present in the data ﬁle named Personality.
13.3
ANALYSIS SETUP
We open the data ﬁle named Personality and from the main menu select Analyze ➔
Descriptive Statistics ➔Descriptives. To illustrate how to obtain z scores, we move
neoneuro into the Variable(s) panel as shown in Figure 13.1. We have checked Save
standardized values of variables below the Variable List panel but do not speciﬁcally
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
79

80
STANDARDIZING VARIABLES TO Z SCORES
FIGURE 13.1
The main dialog window of Descriptives.
FIGURE 13.2 The output from the standardization operation.
ask for any descriptive statistics (the default statistics will be obtained). We then click
OK to perform the analysis.
13.4
ANALYSIS OUTPUT
There are two results of performing the transformation. One set of results is shown in
Figure 13.2. It is an output table providing some very limited descriptive information.
The Minimum, Maximum, Mean, and Std. Deviation are summaries of the variable
neoneuro; this is the default set of statistics preset in the Options dialog screen. On the
basis of this mean and standard deviation, the z scores were calculated; that is, the scores
are standardized with respect to their standing in the sample (they are not standardized
with respect to any outside normative population).
The other result of performing the transformation is the creation of a new variable
in the data ﬁle as shown in Figure 13.3. IBM SPSS® has generated the name of the new
variable by placing an uppercase Z in front of the name of the original variable—it may
not be elaborate but it is unambiguous. Hence, the new variable is named Zneoneuro,
but we can easily change it in the Variable View screen if we wish. Thus, Case 1 has a
standard score on neuroticism of 1.08192; Case 2, −1.80396; and so on. This variable
is now available for further analyses and saving the data ﬁle will preserve it.
13.5
DESCRIPTIVE STATISTICS ON ZNEONEURO
We illustrate the effect of a z score transformation by obtaining through the Descriptives
procedure a few descriptive statistics on both the original neoneuro and the standardized
Zneoneuro variables. The results are shown in Figure 13.4. We can see certain of the
score equivalents from these results:

OTHER STANDARD SCORES
81
FIGURE 13.3
A portion of the data ﬁle showing
the newly created standardized
variable.
FIGURE 13.4 Descriptive statistics on both neoneuro and Zneoneuro.
• The raw score mean of 50.5394 has a z score of zero.
• A raw score of 23.01 has a z score of −2.46953.
• A raw score of 84.77 has a z score of 3.07059.
One other feature worthy of mention is that a z score transformation does not alter
the shape of the distribution. Thus, the skewness and kurtosis of the original variable are
preserved in its z score transformation.
13.6
OTHER STANDARD SCORES
Although the Descriptives procedure can be used to perform the z score transforma-
tion, other standardized score systems are also commonly used. Examples of common

82
STANDARDIZING VARIABLES TO Z SCORES
standardized scales are the linear T scores (with a standardized mean of 50 and a stan-
dardized standard deviation of 10) and SAT scores (with a standardized mean of 500 and
a standardized standard deviation of 100). To transform our z scores to one of these other
standardized systems, we require a relatively simple computation that can be performed
in IBM SPSS, the details of which are shown in Chapter 16.

C H A P T E R
1 4
Recoding Variables
14.1
OVERVIEW
To recode a variable is to modify the values of a variable according to a rule speciﬁed
by the user as part of the recoding operation. Both nominal and quantitative variables
can be subjected to recoding to achieve various purposes.
Categorical variables are commonly recoded. In the process of data collection, we
often ask for very speciﬁc information, such as the ethnicity of the individual or the
region of the state or country in which a respondent resides. Because the responses
are so speciﬁc, the number of cases in certain categories may be too low to analyze
as a stand-alone group. We may also ﬁnd that some categories may be, if not exactly
the same, sufﬁciently similar to raise the possibility that they should not be treated as
different categories. We often deal with these issues by combining cases from two or more
categories into a larger group; that is, we recode this variable to create more expansive
subsets of cases.
It is also not uncommon to recode quantitative variables that are measured on a
summative response scale (e.g., a 5-point or 7-point scale). In certain analyses, it may be
important that each of the response categories is represented by some minimum number
of choices. Under those circumstances, response categories with very low endorsements
(usually the lowest or highest categories) may be recoded into an adjacent category. This
reduces the total number of categories but allows each remaining one to be associated
with an adequate sample size to support some analyses.
Quantitative variables approximating continuous measurement may occasionally be
recoded to eliminate outliers. In those instances, researchers will determine a minimum or
maximum value of the variable that is permitted and recode values beyond that bound-
ary to the minimum or maximum value allowed. For example, a response of 107 to
a question concerning the number of doctor visits for different medical problems over
the last 12 months might be considered out of range in a given data set. Researchers
could either deﬁne this value as missing (the most likely action they would take) or, if
they had reason to believe that the patient was responding in good faith by indicating
that many visits, recode this value to some arbitrary maximum that they had validly
established.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
83

84
RECODING VARIABLES
IBM SPSS® allows us to recode a variable into the same variable or recode a variable
into a different variable. Here is what this means together with our recommendations:
• Recoding into the same variable instructs IBM SPSS to overwrite the original
variable with the recoded values. Our recommendation to most users, but especially
to those who are not an expert in using IBM SPSS, is to never use this choice. We
most strongly recommend never overwriting an original variable; rather we urge
users to create another variable containing the recoded values, thus preserving the
original values in case they are ever needed for some future purpose.
• Recoding into a different variable instructs IBM SPSS to create a new variable
representing the recoded values under a name designated by the user. This choice
preserves the original variable and is our recommended strategy.
14.2
NUMERICAL EXAMPLE
The ﬁctional data we use for our example are in the data ﬁle Doctor Visits. The data
set represents a study of the number of medical visits patients have made to their family
physician during the previous 12 months for different medical issues. The two variables
of interest are as follows:
• The number of visits reported by patients is recorded under the variable
doctor_visits. Because they were patients, the minimum number of visits is
necessarily 1, as patients were selected from the records once they had at least
one visit.
• Patients from a wide range of states across the United States were sampled. Each
state was assigned an arbitrary code, and the value labels supplied in the Variable
View show these data. This variable is named state.
14.3
ANALYSIS STRATEGY
In this ﬁctional study, the goal of the researchers was to determine if there were differ-
ences in the number of visits by different places of residence. The most direct way to
address this is with a one-way between-subjects analysis of variance (ANOVA), which
we cover in Chapter 47. However, because the number of patients from each separate
state is too low to support the ANOVA, we need to combine patients from various
states together to form larger groups. For our present purposes, we adopt the following
sequential strategy:
1. We obtain a summary of the number of patients in each state using the Frequen-
cies procedure.
2. We recode the variable state into a different variable named region representing
geographic regions. We use the speciﬁcation for ranges of values in this analysis
and show recoding for speciﬁc values in Section 14.7.
3. We obtain a count of the number of patients in each newly created region using
Frequencies.
4. To generate descriptive statistics for each region, we could perform the analysis in
the Explore procedure placing doctor_visits in the Dependent List and region
in the Factor List. But because (a) the statistics produced by Explore are arrayed
vertically and (b) the output for the regions would be listed vertically as well, the
results would take up several pages and comparing the regions would be visually
somewhat inconvenient. We therefore opt to split the ﬁle by region.

FREQUENCIES ANALYSIS
85
5. We obtain descriptive statistics on doctor_visits for each region using Descrip-
tives. With Split File in effect, the output will be arrayed in a more condensed
manner, with the output for each region occupying a row.
14.4
FREQUENCIES ANALYSIS
We open the data ﬁle named Doctor Visits and from the main menu select Analyze ➔
Descriptive Statistics ➔Frequencies. Based on our discussion in Chapter 10, we have
moved state into the Variable(s) panel, have checked Display frequency table below
the Variable List panel, and we click OK to perform the analysis.
Figure 14.1 presents the frequency table for the state variable. Most states have
patient counts in the range 5–10. The states have been coded somewhat systematically
already, helping us to visualize the regions into which we can code them. The regions
do not need to have equal numbers of cases as long as the count of cases is sufﬁcient
to support the ANOVA (we will not perform that analysis here). The regions that seem
to capture the geographic grouping of the states we have represented in the data ﬁle are
New England, Southeast, Southwest, Midwest, West, and Far West.
FIGURE 14.1 Frequency table for state.

86
RECODING VARIABLES
14.5
RECODING AN ORIGINAL VARIABLE USING RANGES
From the main menu, selecting Transform ➔Recode into Different Variables brings
us to the main Recode dialog window as shown in Figure 14.2. We have moved state
into the Numeric Variable ➔Output Variable panel. The variable state is followed by
a “?” that needs to be replaced by the name of the newly recoded variable. We name the
recoded variable region in the name panel under Output Variable, and when we click
the Change pushbutton our name will replace the “?” this is shown in Figure 14.3.
Selecting the Old and New Values pushbutton below the Numeric Variable ➔
Output Variable panel brings us to the window where we specify our recoding (see
Figure 14.4). To recode Maine, Vermont, New Hampshire, and Rhode Island (codes 1
through 4), we select Range LOWEST through value because our lowest code is 1.
FIGURE 14.2 The main dialog window for Recode into Different Variables.
FIGURE 14.3 The name of the recoded variable is now in the Numeric Variable ➔Output
Variable panel, indicating that state will be recoded into region.

RECODING AN ORIGINAL VARIABLE USING RANGES
87
FIGURE 14.4 The ﬁrst recoding is speciﬁed: the lowest code (code number 1) through code 4
for state will be recoded into code number 1 for region.
FIGURE 14.5 The recode is now registered in the Old ➔New panel after clicking Add.
IBM SPSS will start with 1 and group the codes together into a new code. We type the
number 4 in the panel as shown in Figure 14.4. In the New Value panel in the upper
right portion of the window, we select Value and type the number 1. Thus, the codes
of 1, 2, 3, and 4 of state will become the code 1 for region. Clicking Add brings this
recode into the Old ➔New panel as shown in Figure 14.5. There is no opportunity to
add a label here, so we must wait until the recode is completed and then add the labels
in the Variable View.
Our next set of states to combine is Virginia, North Carolina, Georgia, and Florida
(codes 5 through 8). We select Range, which has two panels, the upper panel for the
lowest code of our set and the lower panel for the highest code of our set. We place
the values of 5 and 8 in the respective panels, select Value and type the number 2 in the

88
RECODING VARIABLES
New Value panel (all shown in Figure 14.6), and click Add to bring this recode into the
Old ➔New panel as shown in Figure 14.7.
We proceed with the recoding process grouping the following remaining states:
• Oklahoma, Texas, and New Mexico (codes 9 through 11) are assigned a code of 3.
• Ohio, Wisconsin, and Minnesota (codes 12 through 14) are assigned a code of 4.
• South Dakota, Montana, Wyoming, Colorado, Utah, and Idaho (codes 15 through
20) are assigned a code of 5.
• Washington, Oregon, California, and Nevada (codes 21 through 24) are assigned
a code of 6.
FIGURE 14.6 Recoding the second set of states using the Range panels.
FIGURE 14.7 Recoding for the second set of states has now been speciﬁed.

RECODING AN ORIGINAL VARIABLE USING INDIVIDUAL VALUES
89
FIGURE 14.8 Recoding for all of the states has now been speciﬁed.
For this last recode, we can use Range value through HIGHEST by entering 21 in
the panel. IBM SPSS will then incorporate into that recode the highest value code for
the variable state. The resulting full set of recodes is shown in Figure 14.8. We click
Continue to return to the main dialog window and click OK to perform the recode.
14.6
THE RESULTS OF THE RECODING
The case-by-case results of the recoding can be seen in the screenshot of the data ﬁle
shown in Figure 14.9. A new variable named region has been added to the end of the
data ﬁle. It shows as two decimal places because of the default in IBM SPSS. We switch
to Variable View, set the decimal values of region to zero by clicking in the cell and
toggling down (see Chapter 2), type in the labels of our regions (see Figure 14.10), and
click OK to register those changes. We then save the data ﬁle.
We have performed a Frequencies analysis on region, having selected Display
frequency tables, and the results are shown in Figure 14.11. The regions are relatively
evenly represented in our sample, with the Midwest providing the lowest number of
doctor visits and the West providing the highest.
To get a sense of any regional difference in visiting their physicians, we have per-
formed a Split File by region as described in Chapter 8. We have then performed a
Descriptives analysis on doctor_visits (placing that variable in the Variable(s) panel
and asking for a range of descriptive statistics). The results of that analysis are shown in
Figure 14.12. Generally, it appears from visual inspection that (in this ﬁctional data set)
patients in the Southeast visit their physicians almost twice as frequently as those in the
Southwest and that, generally, patients in the Southeast, Northeast, and West tend to visit
their physicians more frequently than those in the Southwest, Far West, and Midwest.
14.7
RECODING AN ORIGINAL VARIABLE USING
INDIVIDUAL VALUES
To illustrate recoding using individual values, we will recode the variable named ethnic
containing our code for ethnic categories. Obtaining a frequency table in the Frequencies

90
RECODING VARIABLES
FIGURE 14.9
The data ﬁle with the newly recoded
variable added.
FIGURE 14.10
The value labels for the newly created region variable.
procedure yields the results shown in Figure 14.13. The group codes with their labels
are displayed in the ﬁrst column. Assume that the researchers decide to collapse these
categories in the following manner to facilitate performing additional analyses on this
variable: Asian American and Paciﬁc Islander will be combined as will Mexican
American and Latino/a American. Such recoding will change the numerical codes
for all categories.
To recode the ethnic variable, we select Transform ➔Recode into Different Vari-
ables. This brings us to the main recoding dialog window as shown in Figure 14.14. We
have moved ethnic into the Numeric Variable ➔Output Variable panel, named the

RECODING AN ORIGINAL VARIABLE USING INDIVIDUAL VALUES
91
FIGURE 14.11 Frequency counts for the categories of region.
FIGURE 14.12 Descriptive statistics on the six regions after splitting the data ﬁle by region and
performing a Descriptives analysis.
FIGURE 14.13 A frequency table for ethnic.

92
RECODING VARIABLES
FIGURE 14.14 The variable newethnic has now been named.
FIGURE 14.15 Recoding the value of 1 to 1.
recoded variable newethnic in the name panel under Output Variable, and clicked the
Change pushbutton; the results of this are shown in Figure 14.14.
Selecting the Old and New Values brings us to the recoding speciﬁcation screen
(see Figure 14.15). In the Value panel in the Old Value area, we type 1, and in the
Value panel in the New Value area, we type 1 (see Figure 14.15); to ﬁnish we click
Add. To combine Paciﬁc Islander with Asian Americans, in the Value panel in the
Old Value area, we type 2, whereas in the Value panel in the New Value area, we type
1 (see Figure 14.16); to ﬁnish we click Add.

RECODING AN ORIGINAL VARIABLE USING INDIVIDUAL VALUES
93
FIGURE 14.16 Recoding the value of 2 to 1.
FIGURE 14.17 The recoding of ethnic to newethnic is ready to be executed.

94
RECODING VARIABLES
FIGURE 14.18 The recoded variable newethnic appears at the end of the data ﬁle.
FIGURE 14.19
The value labels for newethnic.
FIGURE 14.20 A frequency table for ethnic.

RECODING AN ORIGINAL VARIABLE USING INDIVIDUAL VALUES
95
We continue in this manner as follows:
• We recode 3 into 2.
• We recode 4 into 3.
• We recode 5 into 3.
• We recode 6 into 6.
The results of this typing are shown in Figure 14.17. We click Continue to return to the
main dialog window and click OK to perform the recode. The recoded variable newethnic
appears at the end of the data ﬁle (see Figure 14.18). We shift to Variable View to change
the decimal places and supply value labels to newethnic (see Figure 14.19). A frequency
table of newethnic from the Frequencies procedure (presented in Figure 14.20) shows
the number of cases in each of the recoded categories with the new value labels.


C H A P T E R
1 5
Visual Binning
15.1
OVERVIEW
Visual binning is an alternative recoding procedure that allows us to recode a quantitative
variable into a discrete number of categories (bins or groups of cases). It is “visual”
because IBM SPSS® displays a histogram (somewhat vertically compressed) and will
mark the boundaries of our to-be-generated bins with vertical lines superimposed on the
histogram to render it as a more visual process. Reducing a quantitative variable down to
a handful of bins or ordered categories disregards the rich information contained in the
data, and it is certainly not recommended as common practice, but there may be isolated
instances where it may be worthwhile to perform exploratory analyses on a variable that
has been reduced to four or ﬁve global ordered categories.
The Visual Binning procedure allows us to establish bins based on preset binning
choices available in the dialog window. In one preset binning choice, we can establish
categories based on standard deviation units. This choice would divide the distribution
into four categories whose boundaries would be −1 SD, 0 SD, and +1 SD (these corres-
pond to percentiles of 15.9, 50, and 84.1, respectively); such a strategy would result in
groups of unequal size with the two middle groups having more cases than the two end
groups. In another preset binning choice, we can create bins on the basis of percentiles to
keep the group sizes approximately equal. For example, we can create ﬁve bins spaced
20 percentile points apart, we could create four bins spaced 25 percentile points (one
quartile) apart, and so on.
15.2
NUMERICAL EXAMPLE
The data ﬁle we use for our example is Personality. We will subject the openness measure
(neoopen) to the Visual Binning procedure, dividing the distribution into four groups
based on quartiles. To set the stage, we have obtained the mean, standard deviation,
minimum and maximum scores, and the quartile demarcations of neoopen, as well as
its histogram, from the Frequencies procedure. This output is presented in Figure 15.1,
where it can be seen that neoopen scores range from 27.51 to 79.05, with the 25th,
50th, and 75th quartiles corresponding to values of 48.1271, 55.1448, and 63.5911,
respectively.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
97

98
VISUAL BINNING
FIGURE 15.1
Some descriptive statistics for neoopen
from the Frequencies procedure.
FIGURE 15.2
The Visual Binning initial dialog window.

ANALYSIS SETUP
99
15.3
ANALYSIS SETUP
We open the data ﬁle named Personality and from the main menu select Transform ➔
Visual Binning. This opens the initial dialog window shown in Figure 15.2. We move
neoopen into the Variables to Bin panel and click Continue to reach the main dialog
window.
FIGURE 15.3 The Visual Binning main dialog window.
FIGURE 15.4
The Make Cutpoints dialog window of Visual Binning
specifying quartile bins.

100
VISUAL BINNING
FIGURE 15.5 The Visual Binning main dialog window with the quartile values automatically
ﬁlled in.
FIGURE 15.6 The Visual Binning main dialog window with the quartile values automatically
ﬁlled in together with labels for the groups.
The main dialog window of Visual Binning is presented in Figure 15.3. Note the
representation of the histogram; it is vertically compressed but still roughly approxi-
mates the full histogram seen in Figure 15.1. The minimum and maximum scores appear
just above the histogram for easy reference. We must type in a name for the variable
we will create; in the Name area, we have named our to-be-generated binned variable
openness_quartiles.

ANALYSIS SETUP
101
FIGURE 15.7
Veriﬁcation screen for creating the new binned variable.
FIGURE 15.8 Data ﬁle with openness_quartiles placed at the end.
We could type in our boundaries in the rows of the Grid under the column labeled
as Value, but there is an easier way to accomplish the quartile categorization. We select
the Make Cutpoints pushbutton to open the Make Cutpoints dialog window (see
Figure 15.4). We select Equal Percentiles Based on Scanned Cases. In the Number of
Cutpoints panel, we type 3 (this will result in four bins); IBM SPSS automatically ﬁlls in
the Width(%) panel with 25.00 (which is the width of a quartile). The two-place decimal
value is used because 100 may not be evenly divisible by the number of boundaries we
specify. Click Apply to return to the main dialog window.
Upon returning to the main dialog window, we ﬁnd that the percentile values have
been automatically ﬁlled in the Grid under Value and that these quartile boundaries
are represented in the histogram by colored vertical lines (see Figure 15.5). Clicking
the Make Labels pushbutton creates unimaginative but very descriptive labels for our
quartile groups as shown in Figure 15.6; knowing the exact percentile boundaries removes
any ambiguity concerning how each bin is deﬁned.

102
VISUAL BINNING
FIGURE 15.9 The Variable View showing the newly created binned variable.
Click OK to create the new variable. We will be presented with a veriﬁcation
screen (see Figure 15.7). Clicking OK executes the procedure. The new variable (open-
ness_quartiles) has been placed at the end of the data ﬁle as seen in Figure 15.8. The
values of 1 through 4 represent the group codes; 1 indicates the cases that are at or
below the ﬁrst quartile, 2 indicates the cases that are between the ﬁrst quartile and the
median (the second quartile), and so on. The labels appear in the Variable View (see
Figure 15.9), and IBM SPSS has identiﬁed the new variable as an Ordinal Measure. It
is probably best to save the data ﬁle now that this new variable has been added to it.

C H A P T E R
1 6
Computing New Variables
16.1
OVERVIEW
When we compute a new variable, we identify one or more of the quantitative variables
in our data ﬁle and specify a computation we wish to apply. When working with a single
variable, for example, we could transform it to its natural log, compute its squared value,
or place it in an algebraic expression. When working with a set of variables, we could
compute, for example, a total score or a mean.
Whether working with single variables or sets of variables, the result of the computa-
tion is a new variable that is placed at the end of the data ﬁle once it has been computed.
Each case will have a value on the new variable, provided a valid computation was
possible; if the computation was not performed for a particular case for whatever reason
(e.g., a variable had a missing value), then a system missing marker (a blank cell) will
be seen in the data ﬁle for that case.
16.2
COMPUTING AN ALGEBRAIC EXPRESSION
In Chapter 13, we computed z scores for the variable neoneuro in the data ﬁle Personality
and concluded the chapter by indicating that other standardized scores can be computed
from z scores. We illustrate that computation here by obtaining the z scores through the
Descriptives procedure and then generating linear T scores based on Zneoneuro. To
transform a z score to a linear T score, we multiply the z score by 10 and add 50 to
that result. When it has been computed, the linear T score will have a mean of 50 and a
standard deviation of 10, and its distribution will mirror the z score distribution on which
it was based.
We have begun the procedure by making the data ﬁle named Personality the active
data ﬁle and generating the variable Zneoneuro as described in Chapter 13. To compute
linear T scores, from the main menu select Transform ➔Compute Variable. This opens
the Compute Variable dialog window shown in Figure 16.1.
The Target Variable panel is where we provide a name for our newly computed
variable. We name the variable Tneoneuro. Selecting the Type & Label pushbutton
under the panel brings us to the dialog screen shown in Figure 16.2. Although its name
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
103

104
COMPUTING NEW VARIABLES
FIGURE 16.1
The Compute Variable dialog
window with the name of our new
variable provided.
FIGURE 16.2
The Type and Label screen of the
Compute Variable dialog window.
may be self-explanatory, we illustrate how to interact with this dialog screen by pro-
viding the label linear T neoneuro and retaining the default Numeric in the Type
area of the window. Clicking Continue returns us to the Compute Variable dialog
window.
We are now ready to specify our algebraic expression to generate the linear T values.
The sequence needed to specify the computation is as follows:
1. Click the double parentheses on the symbol keypad. This will place a set of
parentheses in the Numeric Expression panel.
2. Place the cursor inside the set of parentheses.
3. Double-click Zneoneuro from the Variable List panel. This will place the vari-
able just before the cursor in the Numeric Expression panel.
4. We need to multiply the z score by 10. Click the single asterisk (this represents
multiplication) on the symbol keypad. This will place an asterisk in the Numeric
Expression panel immediately following Zneoneuro.

COMPUTING THE MEAN OF A SET OF VARIABLES
105
FIGURE 16.3 The algebraic expression to compute the linear T score is complete.
5. Click 1 and then 0 on the numeric keypad to place the number 10 after the asterisk
in the Numeric Expression panel.
6. Move the cursor to the right of the closing parenthesis.
7. Click the plus sign on the symbol keypad.
8. Click 5 and then 0 on the numeric keypad to place the number 50 after the plus
sign in the Numeric Expression panel.
The ﬁnished expression is shown in Figure 16.3. We click OK to perform the computa-
tion.
16.3
THE OUTCOME OF COMPUTING THE LINEAR T SCORES
To illustrate the outcome of our computation, we have analyzed neoneuro, Zneoneuro,
and Tneoneuro in the Descriptives procedure. The limited set of descriptive statistics
we generated is shown in Figure 16.4. As can be seen, the linear T score variable has
a mean of 50 and a standard deviation of 10. Neither the z score nor the linear T score
transformation has changed the shape of the distribution, as the skewness and kurtosis
values have remained constant throughout.
16.4
COMPUTING THE MEAN OF A SET OF VARIABLES
It is frequently the case that in the process of our data analysis, we need to combine
a set of variables together. The simplest way to do this is to compute a mean or total

106
COMPUTING NEW VARIABLES
FIGURE 16.4 Descriptive statistics for the original variable and its z score and linear T score
transformations.
score of the set assuming that they are all assessed on the same metric, for example, a
5-point scale or sales dollars per week (if they are assessed on different metrics, then
they should ﬁrst be converted to z scores and then combined).
Total scores and mean values are equivalent summaries of a set of variables. To
obtain a total, we sum all of the scores. But there can be disadvantages to total scores:
• If the total score is based on summative response scales (e.g., a 5-point scale),
then it is difﬁcult to quickly and intuitively interpret the total with respect to the
original rating scale.
• The total score must be based on the same number of variables for it to be
comparable across cases. Thus, every case must have the same number of valid
scores in order to participate in the analysis.
To obtain a mean value of a set of variables, we sum all of the scores on the variables
and then divide by the number of scores. Mean scores can have certain advantages:
• If the mean score is based on summative response scales (e.g., a 5-point scale),
then it is easy to quickly and intuitively interpret the mean with respect to the
original rating scale.
• If there are a sufﬁcient number of scores that are to be combined, then there
are occasions when having a missing value on a very small percentage of the
variables may still yield an interpretable and valid mean. IBM SPSS® has a way
to accomplish this, which we will demonstrate.
16.5
NUMERICAL EXAMPLE OF COMPUTING THE MEAN
OF A SET OF VARIABLES
The Aspirations Index (Kasser & Ryan, 1993, 1996) is an inventory that assesses the
value placed on achieving certain extrinsic and intrinsic goals. We will compute the
mean for one of the intrinsic subscales to illustrate the computation process. The 30-item
version of the inventory represented in the data ﬁle named Aspirations Index was part
of a larger project of one of our graduate students at the time (Leanne Williamson, now
at the Ohio State University). A total of 310 university students completed the inventory.
Respondents used a 9-point response scale to represent the degree to which the content
was applicable to them. Items in the data ﬁle are named aspire01 through aspire30.
The subscale we will generate is called Afﬁliation, and the items associated with
the subscale are aspire02, aspire08, aspire14, aspire20, and aspire26. Item content

THE COMPUTATION PROCESS
107
includes having committed and enduring relationships, sharing life with a loved one,
having good friends, and being loved by others and being loved in return. All of the
items are positively worded so that no recoding before combining them is required.
16.6
THE COMPUTATION PROCESS
We open the data ﬁle Aspirations Index and from the main menu select Transform
➔Compute Variable. This opens the Compute Variable dialog window shown in
Figure 16.5. IBM SPSS has a wide range of already structured functions to compute a
variety of things. We have scrolled down the Function group panel to select Statistical.
This action displays in the Functions and Special Variables panel the set of statistical
functions available. We selected Mean and double-clicked it (we could have clicked the
upward pointing arrow adjacent to the panel). These steps resulted in the generic Mean
function being displayed in the Numeric Expression panel as shown in Figure 16.5.
The question marks in the Mean function inside the parentheses are to be replaced
by variables whose mean is to be computed. The function is displayed in generic form
as Mean(?,?) but can be customized in two ways. First, only two question marks (place-
holders for variables) are shown in the generic display but more than two variables can
be included.
The second aspect of customization concerns the stand-alone word Mean. As it is
in the Numeric Expression panel, IBM SPSS requires that at least one of the variables
in the set has a valid value. If this basic condition is met by a case, then a mean will
be computed for that case. To use our Afﬁliation scale as an example, there are ﬁve
items comprising the subscale; however, if a case has a valid value on only one of the
variables (e.g., that case has missing values on four of the ﬁve items), IBM SPSS will
FIGURE 16.5 The generic form of the Mean function has been invoked.

108
COMPUTING NEW VARIABLES
still compute a mean for that case (the computed mean would be equal to the value on
the item with a valid score). In such an extreme situation, it would be difﬁcult to believe
that the computed mean is representative of the set of ﬁve items, and using it could
potentially compromise the internal and external validity of the whole data analysis.
This problem can be avoided in one of two ways. First, the researchers would have
examined the data set and presumably discovered and probably removed cases with large
amounts of missing data. But even then, an additional precaution is available in IBM
SPSS: we can add the .n speciﬁcation to the Mean. In such a speciﬁcation, the n is the
minimum number of valid values we require in order to permit a mean to be computed; if
there are fewer than n valid values, then IBM SPSS would return a system missing value
(a blank) rather than a mean. For example, if we were to specify Mean.4 in computing
the Afﬁliation subscale, there must be at least 4 valid (nonmissing) values in order to
proceed with the computation (some researchers might use Mean.5 here, as there are
only ﬁve items and four of ﬁve valid values might be considered too liberal a criterion).
We will use the speciﬁcation Mean.4 in our computation.
To create the speciﬁcation in the Compute Variable window, we have named
our subscale afﬁliation; as the variable name is sufﬁcient and the default is a numeric
variable, we have not accessed the Type & Label window. We have then moved our
cursor into the Numeric Expression panel. This panel can be edited, and we have typed
.4 next to the word Mean, deleted the question marks, and double-clicked our variables
associated with the subscale into the parentheses, being careful to separate them by (the
required) commas. We have separated the variables with spaces following the commas
to make it easier for humans to read the expression, but those spaces are optional for
the software. The ﬁnished expression is shown in Figure 16.6. We click OK to perform
the computation.
FIGURE 16.6 The Compute Variable dialog window with the name of our new variable, and
the Mean function speciﬁed to require at least four valid values for a mean to be computed.

THE OUTCOME OF COMPUTING THE AFFILIATION SUBSCALE
109
FIGURE 16.7 Descriptive statistics for the individual items and the subscale that they combine
to form.
16.7
THE OUTCOME OF COMPUTING THE AFFILIATION
SUBSCALE
To illustrate the outcome of our computation, we have analyzed the ﬁve individual items
comprising the subscale and the afﬁliation subscale itself in the Descriptives procedure.
The limited set of descriptive statistics we generated is shown in Figure 16.7. As can be
seen, the subscale mean is the average of the individual item means and is based on the
full set of 310 cases, even though there were only 309 valid cases for item aspire26; this
is because we speciﬁed a minimum of four valid values for the subscale to be computed,
a condition that was met for every case. Note also that the standard deviation of the
subscale mean is lower than the standard deviations of the raw scores for the individual
items (because the subscale mean is an average of the item means).


C H A P T E R
1 7
Transforming Dates to Age
17.1
OVERVIEW
It is not uncommon in behavioral, social, and medical research studies to capture dates
as one of the data elements. Perhaps the most familiar instance of this, and the one that
we treat in this chapter, is birth date, but other instances can include the date of sale of a
vehicle or an appliance, the date of the ﬁrst medical treatment of a patient, and the date
of the ﬁrst day of employment with a particular organization.
Raw dates cannot be used as variables in most of the statistical procedures we use,
but they can be transformed to a numeric variable representing the amount of elapsed
time between that date and some other date. This time variable resulting from such
a transformation represents ratio-level measurement that can appropriately be used in
virtually all statistical analyses. For example, the time elapsed (e.g., number of years)
between the date of birth and the current date is one’s age.
17.2
THE IBM SPSS® SYSTEM CLOCK
IBM SPSS tracks the current date and time with the variable $TIME. The $ symbol is
used to represent a system variable, that is, a variable already deﬁned by IBM SPSS and
reserved for the given purpose; it becomes available when the software is active to keep
track of information that may be required or may be potentially needed by the system.
In its Help screen under System Variables, IBM SPSS informs us that the $TIME
variable “ . . . represents the number of seconds from midnight, October 14, 1582, to the
date and time when the transformation command is executed . . . ” It thus serves as a
reference point for determining the current date and time. The date October 14, 1582, is
far from arbitrary, by the way. When Pope Gregory XIII in 1582 ordered replacing the
Julian calendar (created by Julius Caesar) with the improved one that we now call the
Gregorian calendar, he removed the days 5 through 14 of October in that year to help
with the transition from the old (and by then inaccurate calendar) to the new calendar;
thus, October 14, 1582, does not exist for those of us who use the Gregorian calendar.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
111

112
TRANSFORMING DATES TO AGE
17.3
DATE FORMATS
The data ﬁle containing our date variable is named Employee data, and a screenshot of
it is shown in Figure 17.1. Dates are associated with the bdate variable in the data ﬁle
and denote the date of birth of each case. The second case, for example, has the birth
date 05/23/1958, corresponding to May 23, 1958. In the Variable View screen, bdate is
speciﬁed under Type as Date (see Section 2.4), as shown in Figure 17.2, with its proper
American Date 10 Character Width format.
FIGURE 17.1 Data ﬁle.
FIGURE 17.2 Data ﬁle showing Variable View.

THE DATE AND TIME WIZARD
113
Dates (as well as time) can be represented in a variety of formats in IBM SPSS data
ﬁles. The two important points to bear in mind are as follows:
• The variable must be identiﬁed in the data ﬁle as a Date variable. This can be
seen in Figure 17.2 for bdate in the second column under Type.
• The variable must be in one of the formats recognizable by IBM SPSS as repre-
senting a date.
The format for bdate in this data ﬁle is American Date 10 Character Width
(mm/dd/yyyy) as can be seen in Figure 17.1. This format represents the fact that a total
of 10 characters (this includes the slashes) are used with two digits denoting the month
followed by two digits denoting the day followed by four digits denoting the year, with
each ﬁeld separated by a forward slash. Examples of other date formats are as follows:
• American Date 8 Character Width takes the form mm/dd/yy; an example is
05/23/58.
• International Date 9 Character Width takes the form dd-mmm-yy; an example
is 23-MAY-58.
• European Date 10 Character Width takes the form dd.mm.yyyy; an example is
23.05.1958.
17.4
THE DATE AND TIME WIZARD
Dates are transformed to time by invoking the Date and Time Wizard. Our plan is to
transform bdate into a variable representing age in years. We open Employee data and
from the main menu select Transform ➔Date and Time Wizard. This opens the Date
and Time Wizard window shown in Figure 17.3.
FIGURE 17.3 Opening screen of the Date and Time Wizard.

114
TRANSFORMING DATES TO AGE
There are six choices presented to us in the initial Date and Time Wizard window,
indicating the various operations that IBM SPSS can perform. Brieﬂy, these choices are
as follows:
• Learn how dates and times are represented in SPSS Statistics provides a brief
review of the Date and Time Wizard operations.
• Create a date/time variable from a string containing a date or time allows us
to convert a String Variable Type (e.g., a variable composed of words) to the
Date Variable Type similar to the format in bdate in our example date ﬁle.
• Create a date/time variable from variables holding parts of dates or times is
used to produce a date/time variable from a set of existing variables. For example,
the data ﬁle may contain three separate variables of month, day, and year. This
operation will combine the three variables into a single date/time variable.
• Calculate with dates and times provides two possible operations. The ﬁrst oper-
ation adds a constant to a date (e.g., a month to an age) or calculates the duration
of some process by subtracting a start time variable from an end time variable.
The second operation will calculate a number of time units between two dates; for
example, a birth date is subtracted from the present year to produce a chronological
age. The latter operation is illustrated in this chapter.
• Extract a part of a date or time variable allows the researcher to select a
single element from a date/time variable; for example, we can convert a variable
containing the month, day, and year to a new variable containing only the month
or the day or the year.
• Assign periodicity to a dataset (for time series data) creates a set of sequential
dates that can be used with time series data.
For our purposes in this chapter where we wish to compute an age from a birth
date, we select Calculate with dates and times and click the Next pushbutton (it may
be a Continue pushbutton on a Mac). This brings us to the ﬁrst of the three steps as
shown in Figure 17.4. We select Calculate the number of time units between two
dates (e.g., calculate an age in years from a birthdate and another date) and click
the Next pushbutton.
The second step in working with the Date and Time Wizard (see Figure 17.5)
allows us to specify our transformation. Note that there are only two variables in the
Variables panel, although there are several variables in the data ﬁle, and one of them is
not even in the data ﬁle. What has happened is as follows:
• The system variable $TIME is brought into the panel because we wish to perform
a date and time transformation, and IBM SPSS makes the current date and time
available to us because it is common to use it as a reference point.
• IBM SPSS displays only those variables in the data ﬁle that are in one of the
many date and time formats it recognizes; the only such variable in the data ﬁle
is bdate (it was speciﬁed as Date under Type in the Variable View).
We move $TIME into the panel under Date 1 and we move bdate into the panel
under Date 2 as shown in Figure 17.6. This subtracts bdate from $TIME (the present
time). This difference will be measured in units that is speciﬁed in the Units drop-down
menu. The default of Years is what we wish for in this illustration, but other units of
time (Months, Weeks, Days, Hours, Minutes, and Seconds) are available as needed for
other research purposes.
Under Result Treatment we selected Truncate to integer as a person is n years
old for the entire year until the day of their n + 1 birthday. Thus, fractional parts are

THE DATE AND TIME WIZARD
115
FIGURE 17.4 The ﬁrst step in working with the Date and Time Wizard.
FIGURE 17.5 The initial window in the second step in working with the Date and Time Wizard.

116
TRANSFORMING DATES TO AGE
FIGURE 17.6 The speciﬁed window in the second step in working with the Date and Time
Wizard.
ignored in reporting a truncated age. However, we could have chosen to round (cases
over half-way to their next birthday would be assigned their next year of age) or to retain
the decimal value of their age. When ﬁnished, we click the Next pushbutton.
The ﬁnal step in using the Date and Time Wizard allows us to name our transformed
variable. This is shown in Figure 17.7 where the calculation is displayed in upper left
portion of the screen. We have named our variable age and have supplied a Variable
Label just to illustrate it. We retain the default Create the variable now option and
click Finish to generate the new variable.
The newly created age variable is placed at the end of the data ﬁle as shown in
Figure 17.8. We have also performed a second transformation of bdate, repeating the
same steps as described above, but this time specifying Retain fractional part in the
second step. We named the variable that is based on this second transformation exact_age
to differentiate it from the truncated age variable and to illustrate what this speciﬁcation
will generate. This exact_age variable is shown in Figure 17.9. Note that the second case
whose birthday is May 23, 1958, shows a truncated age of 53 years and has a fractional
age of 53.80 years. These ages are based on transformations performed on March 10,
2012, at the time the ﬁrst draft of this chapter was written. Any readers who perform
this transformation on the data ﬁle will obtain age values dependent on the day and time
of their particular work session.
17.5
USING A DIFFERENT TIME REFERENT
Because $TIME sets the date and time reference to when the transformation is performed,
the resulting computation using the current date and time can become (no pun intended)
outdated. One way to avoid the problem is to do the following:

USING A DIFFERENT TIME REFERENT
117
FIGURE 17.7 The ﬁnal step in working with the Date and Time Wizard.
FIGURE 17.8 The newly created age variable is available at the end of the data ﬁle.

118
TRANSFORMING DATES TO AGE
FIGURE 17.9 The additional variable exact_age at the end of the data ﬁle.
• Select a ﬁxed reference date.
• Create a new date variable (being sure to establish it as a Date variable under
Type in the Variable View window).
• Assign the same reference date to all cases.
The variable would then appear in the Variables panel in the second step of the
Date and Time Wizard (see Figure 17.5) and it can be moved into the Date 1 panel
(see Figure 17.6) to represent the variable from which the variable date (e.g., birth date)
would be subtracted. This solution, of course, is not perfect either, as it computes an age
with respect to an arbitrary reference date rather than the current date.
FIGURE 17.10 Descriptive statistics for age.

USING AGE IN A STATISTICAL ANALYSIS
119
17.6
USING AGE IN A STATISTICAL ANALYSIS
To illustrate that the age variable is available for statistical analysis, we have generated
a small set of descriptive statistics using the Descriptives procedure. As can be seen in
Figure 17.10, ages of the cases in this sample ranged from 41 to 83 with a mean of 54.91
and a standard deviation of 11.79. The distribution tended to be somewhat positively
skewed and might be viewed as slightly platykurtic (a little ﬂattened compared to the
normal distribution).


P A R T 6
EVALUATING SCORE
DISTRIBUTION
ASSUMPTIONS


C H A P T E R
1 8
Detecting Univariate Outliers
18.1
OVERVIEW
Chapters 19 and 20 deal with detecting outliers—extreme or unusual values—that may
be found in the distribution of scores for a variable. These outliers are referred to as
univariate outliers (covered in this chapter) when they reside on a single variable, and
as multivariate outliers (covered in Chapter 19) when they are a function of multiple
variables.
The detection of outliers among variables in a data set is essential because the results
of many statistical procedures (e.g., multiple regression, analysis of variance) will become
distorted when extreme scores are included in the analysis. Outliers can occur because of
data entry errors, unusual circumstances (e.g., respondent fatigue or illness), and (more
frustratingly) for no known cause.
There are a number of approaches to univariate outlier detection, and we will illus-
trate a popular technique that uses boxplots (also known as box and whiskers plots) to
identify outliers. Figure 18.1 displays the generic structure of a box and whisker plot.
Scores are represented on the Y-axis; in the plot produced by IBM SPSS®, percentiles
rather than (as proxies for) raw scores are placed on the Y-axis.
Boxplots display a square or rectangle that represents the distribution of scores
between the ﬁrst quartile or 25th percentile and the third quartile or 75th percentile
(these points are also known as Tukey’s hinges). The distance between the ﬁrst and third
quartile is deﬁned as the Interquartile Range (IQR). A horizontal line or bar inside the
box represents the distribution’s midpoint or median (50th percentile).
Beyond the ﬁrst and third quartiles are vertical lines (called whiskers) that extend
1.5 IQR below and above and end at the lower and upper inner fences. Data points
beyond the fences are considered outliers. IBM SPSS marks values that fall between
±1.5 IQR and ±3.0 IQR with open circles; these may be considered outliers in some
contexts. Scores that are beyond ±3.0 IQR are marked with asterisks and are ordinarily
considered to be extreme scores.
18.2
NUMERICAL EXAMPLE
The present example is focused on Global Assessment of Functioning (GAF) scores for
mental health clients. GAF scale values can range from 1 (severe impairment) to 100
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
123

124
DETECTING UNIVARIATE OUTLIERS
FIGURE 18.1 Generic structure of an IBM SPSS box and whiskers plot. Source: Modiﬁed from
Meyers et al., 2013.
(good general functioning). Also included is the sex of the clients with females coded as
1 and males coded as 2. We will look for univariate outliers on the quantitative (metric)
variable GAF and again separately for each sex. The data can be found in the ﬁle named
GAF and sex.
18.3
ANALYSIS SETUP: SAMPLE AS A WHOLE
Open the ﬁle named GAF and sex and, from the main menu, select Analyze ➔
Descriptive Statistics ➔Explore, which produces the Explore main dialog window
seen in Figure 18.2. From the variable list we have highlighted and moved the GAF
variable to the Dependent List panel.
Selecting the Statistics pushbutton produces the Statistics dialog window (see
Figure 18.3), and we have checked Descriptives and Outliers. Click Continue to return
to the main dialog window and select the Plots pushbutton. This action produces the
Plots dialog window shown in Figure 18.4. We have removed the default checkmark
from Stem-and-leaf but have retained the Boxplot of Factor levels together. Click
Continue to return to the main dialog window and click OK to perform the procedure.
18.4
ANALYSIS OUTPUT: SAMPLE AS A WHOLE
We refer readers to Chapter 12 for a discussion of the descriptive statistics produced
by Explore, and will focus on the rest of the output here. The top table in Figure 18.5

ANALYSIS OUTPUT: SAMPLE AS A WHOLE
125
FIGURE 18.2
The main dialog window of Explore.
FIGURE 18.3
The Statistics dialog window of Explore.
FIGURE 18.4
The Plots dialog window of Explore.
presents the Case Processing Summary, shown here to report a count of cases (N = 25
with no missing cases), as we are not showing the descriptive statistics output.
The bottom table in Figure 18.5 presents the Extreme Values output, providing the
ﬁve highest and ﬁve lowest values on the GAF variable along with the case number
corresponding to each value. These values are not necessarily outliers; rather, they are
simply the highest and lowest values in the distribution. We see from the output, for

126
DETECTING UNIVARIATE OUTLIERS
FIGURE 18.5 Case Processing Summary and Extreme Values output.
example, that Case Number 20 had the highest GAF value of 70 and that Case Number
6 had the second highest GAF value of 60. Furthermore, Case Number 4 had the lowest
GAF value of 20 and Case Numbers 24 and 13 had the second lowest GAF value of 40.
Figure 18.6 displays the boxplot for the GAF variable. There are two univariate
outliers for this distribution of GAF scores. The score for Case Number 20 (with a
GAF score of 70) is marked with a circle. IBM SPSS identiﬁed it as worthy of note
because it is more than 1.5 IQR beyond the third quartile; however, the circle indicates
that it also lies inside 3.0 IQR. The score for Case Number 4 (with a GAF score of
20) is marked with an asterisk and indicates it is an extreme score that is more than 3.0
IQR beyond the ﬁrst quartile. Both of these cases on this variable would be possible
candidates for elimination from a subsequent statistical analysis.
18.5
ANALYSIS SETUP: CONSIDERING THE CATEGORICAL
VARIABLE OF SEX
It is often useful to examine univariate outliers with respect to the levels of a categorical
variable on which data have been collected. We illustrate this using the sex variable. From
the main menu select Analyze ➔Descriptive Statistics ➔Explore, which produces
the IBM SPSS Explore main dialog window, shown in Figure 18.7. We conﬁgure the
analysis exactly as just described, but with the following exception: from the variable
list we have moved the GAF variable to the Dependent List panel and we have moved
the sex variable to the Factor List panel. By including a Factor in our analysis, each
portion of the output will be provided separately for each level of the Factor (separately
for females and males in our example).

ANALYSIS OUTPUT: CONSIDERING THE CATEGORICAL VARIABLE OF SEX
127
FIGURE 18.6
The box and whiskers plot.
FIGURE 18.7
The main dialog window of Explore.
18.6
ANALYSIS OUTPUT: CONSIDERING THE CATEGORICAL
VARIABLE OF SEX
The top table in Figure 18.8 presents the Case Processing Summary separately for
females and males. We note that there are 11 females and 14 males with no missing
values. The bottom table in Figure 18.8 presents the Extreme Values output, providing
the ﬁve highest and ﬁve lowest values on the GAF variable separately for females and
males, along with the case number corresponding to each value. We see from the output
that the highest (a GAF score of 70) and lowest (a GAF score of 20) values in the
output for the sample as a whole were associated with females. IBM SPSS also informs
us by way of footnotes that some of the ﬁfth-ranked values in the table have a frequency
greater than 1 in the distribution.
Figure 18.9 presents the boxplots for GAF separately by the respondent sex. From
these boxplots, we see the visual depiction of what we learned from examining the
Extreme Values output, namely, both univariate outliers previously identiﬁed are found

128
DETECTING UNIVARIATE OUTLIERS
FIGURE 18.8 Case Processing Summary and Extreme Values output for each level of sex.

ANALYSIS OUTPUT: CONSIDERING THE CATEGORICAL VARIABLE OF SEX
129
FIGURE 18.9
The box and whiskers plot for each level of sex.
among the female respondents. A circle now represents the GAF score for Case Num-
ber 4. This is because the IQR that is its frame of reference for the GAF score of 20 is
based only on the female clients, and such a score falls inside the range 3.0 IQR for the
female group.
This output raises an important issue. Researchers may decide to remove cases with
outliers from subsequent analyses based, at least in part, on the boxplot results. If so,
we suggest that they do so on the results for the sample as a whole. It is our view that
researchers consider the results based on examining the separate levels of the categorical
variable as value-added information. Measures of variability based on only a portion of
the scores are less stable, and it is ill advised to make decisions about removing cases
taking only a subset of the data into account.


C H A P T E R
1 9
Detecting Multivariate Outliers
19.1
OVERVIEW
Multivariate outliers are cases with an extreme combination of values on two or more
variables. Detecting this sort of outlier is in some sense more “subtle” than detecting
univariate outliers because the combination of variables generating the multivariate out-
lier is not as readily apparent in the data set as recognizing a univariate outlier in the
listing of values for a single variable. Adding to the less obvious nature of identifying
multivariate outliers is that a case may not be a univariate outlier on any of the vari-
ables tied to the multivariate outlier. For example, we might have an age variable and
a variable representing the number of academic peer-reviewed journal articles the indi-
viduals have published over their lifetime. Age might range from 13 to 75 years, and the
number of publications might range from 0 to 210. The distribution of these individual
variables (e.g., their minimum and maximum values) may not be particularly noteworthy
or unusual, but a 14 year old with 147 publications would likely be a multivariate outlier
either because that case was a child prodigy or (more likely) because one of those values
represented a data entry (or data recording) error.
19.2
THE MAHALANOBIS DISTANCE
One common procedure used to identify multivariate outliers is to compute the Maha-
lanobis distance associated with each case. The Mahalanobis distance is the multivariate
distance between two points and can be most easily imagined by considering a scatterplot
of two variables. In a traditional scatterplot, the X- and Y-axes intersect at 90 degrees
and cases are depicted by data points corresponding to their X and Y coordinates. It is
possible to locate the X,Y coordinate of the center of that scatterplot. This central posi-
tion is known as a centroid and can be conceived of as a multivariate mean or average of
the multivariate distribution of coordinates. The distance between the centroid and any
of the other data points (e.g., using a ruler) is called the Euclidian distance.
When we apply the Mahalanobis distance strategy to measure distance, the process
takes into account the correlation between the variables. In the simpliﬁed situation of
two variables, if the two variables were correlated then the axes would not intersect at
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
131

132
DETECTING MULTIVARIATE OUTLIERS
90 degrees and the scatterplot would therefore take on a shape somewhat different from
that of our traditional scatterplot. The distance between the centroid and any of the other
data points is called the Mahalanobis distance. It is scaled analogously to a z score but
in a multivariate manner (Raykov & Marcoulides, 2008).
One very convenient property of the Mahalanobis distances is that their squared val-
ues can be approximately described by a chi-square distribution with degrees of freedom
equal to the number of variables on which the multivariate combination is based (Var-
muza & Filzmoser, 2009). The Linear Regression procedure can be used to compute
the squared Mahalanobis distance for each case based on a speciﬁed set of variables.
We then evaluate these values for statistical signiﬁcance against the appropriate chi-
square distribution using a stringent alpha level of .001 (Kline, 2011) because of the
large number of evaluations (one for each case) that are involved. Cases whose squared
Mahalanobis distance exceeds the critical chi-square value can be considered multivari-
ate outliers and can become possible candidates for exclusion from subsequent data
analyses.
19.3
NUMERICAL EXAMPLE
The present example examines mental health clients’ GAF scores, and their gender (sex)
and age. We generate the Mahalanobis distance values using the Linear Regression
procedure. We then determine if there are any multivariate outliers based on this combi-
nation of the three variables by viewing the distance values and attempting to diagnose
which subset of variables might be primarily generating the multivariate outliers. The
data can be found in the ﬁle named GAF sex age.
19.4
ANALYSIS SETUP: LINEAR REGRESSION
Open the IBM SPSS® save ﬁle named GAF sex age and from the main menu select
Analyze ➔Regression ➔Linear, which produces the main Linear Regression dialog
window shown in Figure 19.1. We have moved sex, GAF, and age to the Indepen-
dent(s) panel. The results of the Mahalanobis distance calculations are based only on the
three variables in the Independent(s) panel. Because the Linear Regression procedure
requires a Dependent variable in order to perform the full regression analysis (even
though our interest is exclusively in obtaining the squared Mahalanobis distances), we
have arbitrarily designated id as the dependent variable and moved it to the Dependent
panel; the dependent variable in this full regression analysis (id) is irrelevant, and the
main multiple regression results are meaningless and should be ignored.
Clicking the Save pushbutton produces the Save dialog window, where we have
activated the Mahalanobis checkbox in the Distances panel (see Figure 19.2). This
speciﬁcation will create a new variable in the data ﬁle that will be ﬁlled in with the
squared Mahalanobis distance for each case. Click Continue to return to the main dialog
window and click OK to perform the analysis.
19.5
ANALYSIS OUTPUT: LINEAR REGRESSION
Our focus is exclusively on the new variable that now appears at the end of the data
ﬁle (see Figure 19.3) and we will ignore the meaningless regression results. The new
variable is called MAH_1 (named generically by IBM SPSS as the ﬁrst set of saved
Mahalanobis distances in our analysis session). It represents the squared Mahalanobis

ANALYSIS OUTPUT: LINEAR REGRESSION
133
FIGURE 19.1
The main Linear Regression dialog
window.
FIGURE 19.2
The Save window of Linear Regression.

134
DETECTING MULTIVARIATE OUTLIERS
FIGURE 19.3 The data ﬁle with the saved MAH_1 variable.
distance for each case. For example, the ﬁrst case in the data ﬁle has a MAH_1 value
of 2.51111.
19.6
STRATEGIES TO EXAMINE THE RESULTS
To determine which cases, if any, are multivariate outliers, we evaluate these squared
Mahalanobis distance values against a chi-square distribution with degrees of freedom
equal to the number of variables we have clicked over to the Independent(s) panel in the
analysis setup (three variables in the present example). We therefore consult the Table
of Critical Values for chi-square shown in Table A.1 and use the stringent alpha level of
p = .001. In the present example with three variables, we have three degrees of freedom.
Consulting the table for df = 3, any case with a squared Mahalanobis distance value
equal to or greater than 16.266 can be considered a multivariate outlier.
There are alternative ways to determine which cases might be considered as multi-
variate outliers based on the chi-square evaluation of the squared Mahalanobis distances.
The IBM SPSS Explore procedure is one alternative but it provides only the ﬁve highest
(and lowest) values of a variable. With large data ﬁles or in case of data ﬁles with several
outliers, we may not be able to obtain all values beyond our critical Mahalanobis distance
value in the Explore output.
The squared Mahalanobis distance values cannot fall below zero; thus, we are inter-
ested in examining only the upper end of the distribution. Given this asymmetric nature
of our inquiry, there is a convenient alternative way to identify all of the outliers. We
can Sort Cases (see Chapter 7) in a descending order by the new MAH_1 variable; this
will place the highest squared Mahalanobis distance values at the top of the data ﬁle and
allow us to see them simply by viewing the data ﬁle in Data View (see Chapter 2).

EXAMINING THE DATA
135
19.7
EXAMINING THE DATA
From the main menu select Data ➔Sort Cases, move MAH_1 into the Sort by panel,
select Descending to place the highest value at the top of the spreadsheet (shown in
Figure 19.4), and click OK. The completed sorting can be seen in the Data View mode
of the data ﬁle in Figure 19.5, where it can be seen that only one case has a MAH_1
value exceeding our critical value of 16.266. That one case is ID # 20, whose MAH_1
value is 17.99652.
Looking across the row for ID # 20 provides us with a sense of why that case might
be an outlier—that client was female (sex code of 1), 21 years old (probably one of the
youngest cases in the data set), and had a GAF score of 90 (suggesting someone who
FIGURE 19.4
The main Sort Cases dialog window.
FIGURE 19.5 The data ﬁle after sorting the saved MAH_1 variable.

136
DETECTING MULTIVARIATE OUTLIERS
FIGURE 19.6
The main Descriptives dialog window.
FIGURE 19.7 The default descriptive statistics output.
FIGURE 19.8 The data ﬁle still sorted on the saved MAH_1 variable but showing the standardized
GAF and age variables.

EXAMINING THE DATA
137
is very high functioning). To see how far these two quantitative scores are from their
respective univariate means, we can standardize the age and GAF scores as described in
Chapter 13. Brieﬂy, we select Descriptive Statistics ➔Descriptives, move GAF and
age into the Variable(s) panel, check the box for Save standardized values as variables
(shown in Figure 19.6), and click OK.
The default descriptive statistics for the two variables are shown in Figure 19.7.
GAF has a mean of 49.43 with a standard deviation of 11.052, whereas age has a mean
of 29.95 with a standard deviation of 6.786. Thus, the GAF score of ID # 20 of 90 is
more than three standard deviation units higher than the mean, and the age of ID # 20
is more than one standard deviation unit lower than the mean.
These extreme scores can be seen very clearly and more precisely in Figure 19.8
where ID # 20 is still in the ﬁrst row in our sorted data ﬁle. We can now easily see
that her GAF score of 90 is 3.67088 standard deviation units higher than the mean,
and her age (z = −1.31930) is 1.31930 standard deviation units below the mean. This
combination (and not her sex, as there is almost an equal number of females and males)
of a very high GAF score and a somewhat, but not outrageously, young age produced a
multivariate outlier. Depending on the research context, this case might or might not be
excluded from subsequent data analysis.


C H A P T E R
2 0
Assessing Distribution Shape:
Normality, Skewness, and Kurtosis
20.1
OVERVIEW
A normal distribution is a frequency distribution that represents the relative number of
occurrences at each value of a variable. The shape of the distribution resembles a bell,
and so gets its nickname as the bell-shaped curve. We have drawn the normal distribution
in Figure 20.1. Many statistical procedures covered in this chapter assume that the errors
associated with the scores on the dependent variable are normally distributed (see Gamst
et al. (2008)); when this assumption is met, the dispersion of the scores themselves also
tends to be normally distributed. Most of the quantitative (metric) variables that we use
in our research (e.g., age, weight, blood pressure, monthly income) approximate this
ideal.
Normal distributions have a variety of properties, including the following:
• The normal distribution is horizontally symmetric around the mean of distribution
(each side is a mirror image of the other side).
• The middle value represents the mean, median, and mode of the distribution.
• The standard deviation is the distance between the mean and the inﬂection point
(change of the direction of slope) on the side of the curve (see Figure 20.1).
• Standard deviation is an interval-level scale of measurement. Knowing the distance
of one standard deviation allows us to ﬁll in the rest of the X-axis. The count of
standard deviation units in terms of distance from the mean is a z score scale.
• The mean has a z score of zero.
• Between ±1.00 standard deviation units, there is approximately 68.26% of the area
(or scores); this range corresponds to percentile scores of approximately 16–84.
• Between ±1.96 standard deviation units, there is approximately 95% of the area;
this range corresponds to percentile scores of approximately 2.5–97.5.
• Between ±3.00 standard deviation units, there is approximately 99% of the area;
this range corresponds to percentile scores of approximately 0.1–99.9.
• As the distance from the mean increases, the curve approaches but never reaches
the X-axis.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
139

140
ASSESSING DISTRIBUTION SHAPE: NORMALITY, SKEWNESS, AND KURTOSIS
FIGURE 20.1 The normal curve.
Normality can be assessed in three ways.
• We can use the Shapiro–Wilk and/or the Kolmogorov–Smirnov procedures to test
the null hypothesis that our distribution is normal in shape.
• We can assess the skewness and kurtosis of our distribution.
• We can assess normality graphically by visually examining histograms and normal
probability plots.
IBM SPSS® provides two statistical tests for a distribution’s normality: the Shapiro–Wilk
and the Kolmogorov–Smirnov tests. The Shapiro–Wilk test evaluates the null hypoth-
esis that a sample distribution comes from a normally distributed population. Thus,
a statistically signiﬁcant result (p < .05) is indicative of a normality violation. The
Kolmogorov–Smirnov test standardizes a sample distribution and compares it with a
reference standard normal distribution. A statistically signiﬁcant outcome (p < .05) indi-
cates a possible normality violation. However, these tests are quite powerful and with
large sample sizes will return a statistically signiﬁcant result even when the departure
from normality is not practically important.
When distributions of scores deviate from normality, it is useful to determine their
skewness and kurtosis. Skewness describes the degree of a distribution’s symmetry. A
normal curve has a skewness of zero. Skewness values close to zero (less than ±1.00)
are considered indicative of a relatively symmetric distribution (Meyers et al., 2013).
Values beyond this range may indicate either positive skewness, where the scores are
packed around the lower end of the distribution, or negative skewness, where the scores
are clustered around a distribution’s high end.
Kurtosis refers to whether the distribution of scores is compressed (peaked) or ﬂat
relative to the normal curve. A normal curve has a kurtosis of zero. Kurtosis values

ANALYSIS OUTPUT: FREQUENCIES
141
less than ±1.00 are typically considered to be in the range of the normal curve; values
outside that range may be considered either leptokurtic, they are relatively more peaked
distributions (positive kurtosis), or platykurtic, they are relatively ﬂatter distributions
(negative kurtosis).
Histograms are a graphical means of displaying the frequency distribution of a quan-
titative or metric variable’s distribution of scores. Values of the quantitative variable are
represented in the X-axis and the frequency of occurrence is represented in the Y-axis.
IBM SPSS can also superimpose a normal curve on the histogram for a quick visual
inspection. Typically, categorical (nonmetric) variables are proﬁled with bar charts rather
than histograms.
In a normal probability plot, the observed values of a variable are rank-ordered and
plotted against an expected normal distribution of values. A normal distribution produces
values that fall on a straight diagonal line, and the plotted data values are compared to
this diagonal. If the data values follow the diagonal line, then normality is assumed
(Meyers et al., 2013; Stevens, 2009).
20.2
NUMERICAL EXAMPLE
The present example examines mental health clients’ GAF scores and their age. We
will examine the distribution of scores for each variable and assess for any normality
violations of these two quantitative (metric) variables. The data can be found in the ﬁle
named GAF and age.
20.3
ANALYSIS STRATEGY
We make use of two of the three descriptive statistics procedures in IBM SPSS, Fre-
quencies and Explore. There is much the same output available in both. We use the
Frequencies procedure to generate the skewness and kurtosis information and to obtain
a histogram with the normal curve superimposed on it; we use the Explore procedure to
generate tests of normality and to produce the normal probability plot.
20.4
ANALYSIS SETUP: FREQUENCIES
We open the data ﬁle named GAF and age and from the main menu select Analyze ➔
Descriptive Statistics ➔Frequencies, which produces the Frequencies dialog window
shown in Figure 20.2. We have moved GAF and age to the Variable(s) panel and
disengaged the Display frequency tables checkbox.
Selecting the Statistics pushbutton produces the Statistics dialog window shown
in Figure 20.3, where we have activated only the Skewness and Kurtosis checkboxes.
Clicking Continue returns us to the main dialog window.
Selecting the Charts pushbutton produces the Charts dialog window (see
Figure 20.4). We have activated the Histograms and Show normal curve on histogram
options. Click Continue to return to the main dialog window and click OK to perform
the analysis.
20.5
ANALYSIS OUTPUT: FREQUENCIES
The skewness and kurtosis output is shown in Figure 20.5. For the GAF variable, both
skewness (.247) and kurtosis (−.331) values are well within the ±1.00 criterion; thus the

142
ASSESSING DISTRIBUTION SHAPE: NORMALITY, SKEWNESS, AND KURTOSIS
FIGURE 20.2
The main dialog window of Frequencies.
FIGURE 20.3
The Statistics dialog window of Frequencies.
FIGURE 20.4
The Charts dialog window of Frequencies.

ANALYSIS SET UP: EXPLORE
143
FIGURE 20.5
The skewness and kurtosis output.
distribution is relatively symmetric and appears to be neither particularly compressed nor
ﬂattened with respect to the normal curve. The kurtosis value for age (.379) is also within
the ±1.00 criterion, but age does appear to be positively skewed with a value of 1.075.
In examining the values of skewness and kurtosis, it is important to take into account
their standard errors. We use the standard error, in general, to establish a conﬁdence
interval around a statistic. For example, by multiplying the standard error by 1.96 and
subtracting that value from and adding that value to the value of the statistic, we can
identify the lower and upper limits of the 95% conﬁdence interval. We can then use that
95% conﬁdence interval to evaluate the obtained skewness and kurtosis values.
With respect to GAF, the standard error of skewness is .383, and 1.96 * .383 is .751.
Subtracting and adding that value to .247 yields a 95% conﬁdence interval of −.504 to
.998. It would appear that a skewness value of zero falls within the conﬁdence interval
and thus suggests that the obtained skewness value is not statistically different from zero.
The standard error of kurtosis for GAF is .750, and 1.96 * .750 is 1.470. Subtracting and
adding that value to −.331 yields a 95% conﬁdence interval of −1.801 to .419. It would
appear that a kurtosis value of zero also falls within its conﬁdence interval and thus
suggests that the obtained kurtosis value is not statistically different from zero.
With respect to age, the standard error of skewness is .383, and 1.96 * .383 is
.751. Subtracting and adding that value to 1.075 yields a 95% conﬁdence interval of
.324–1.826. It would appear that a skewness value of zero does not fall within the conﬁ-
dence interval, thus suggesting that the distribution of age is not symmetric. The standard
error of kurtosis for age is .750, and 1.96 * .750 is 1.470. Subtracting and adding that
value to .379 yields a 95% conﬁdence interval of −1.091 to.1.849. It would appear that
a kurtosis value of zero does fall within the conﬁdence interval and thus suggests that
the obtained kurtosis value is not statistically different from zero.
Figure 20.6 presents frequency histograms for the GAF and age variables. The
horizontal axis of each graph is drawn in 10-point increments of GAF scores and age
values, respectively. The vertical axis represents frequency of occurrence. The normal
curve superimposed on each histogram provides a convenient visual reference. It appears
from the histograms that GAF scores closely approximate a normal distribution, but, as
we noted earlier, the age variable is noticeably positively skewed.
20.6
ANALYSIS SET UP: EXPLORE
The next set of analyses utilizes the Explore procedure to produce the Kolmogorov–
Smirnov and Shapiro–Wilk normality tests and normal (Q-Q) or probability plots. From
the main menu selecting Analyze ➔Descriptive Statistics ➔Explore produces the
Explore dialog window shown in Figure 20.7, where we have moved GAF and age
over to the Dependent List panel.

144
ASSESSING DISTRIBUTION SHAPE: NORMALITY, SKEWNESS, AND KURTOSIS
FIGURE 20.6 Histograms for GAF and age.
FIGURE 20.7
The main dialog window of Explore.
Clicking the Plots pushbutton produces the Plots dialog window shown in
Figure 20.8. We have activated None in the Boxplots panel (as we have no need for
boxplots in this example) and have not selected either plot in the Descriptive panel.
However, we did activate the Normality plots with tests option. Click Continue to
return to the main dialog window and click OK to perform the analysis.
20.7
ANALYSIS OUTPUT: EXPLORE
Figure 20.9 displays the Kolmogorov–Smirnov and the Shapiro–Wilk normality tests.
Neither test indicated that the GAF distribution differed from normality (p = .200 and
.793, respectively). Conversely, for the age variable, both tests were statistically signiﬁ-
cant (p = .007 and .001, respectively), indicating a normality violation for that variable.
This conﬁrms our suspicions when we evaluated skewness and kurtosis with respect to
their standard errors.
Figure 20.10 displays the normal probability (or Q-Q) plots for the GAF and age
variables (IBM SPSS also produces a de-trended normal Q-Q plot that removes the linear

ANALYSIS OUTPUT: EXPLORE
145
FIGURE 20.8
The Plots dialog window of Frequencies.
FIGURE 20.9 Outcomes of the Kolmogorov–Smirnov and the Shapiro–Wilk normality tests.
FIGURE 20.10 Normal Q-Q plots for GAF and age.
trend in the data). This is a normal probability plot or Q-Q plot (the Q stands for quantiles,
which are points taken at regular intervals in a cumulative distribution). The diagonal
line angled from the lower left to the upper right represents an idealized normal distri-
bution. An observed distribution of scores (represented by circles) is superimposed on
the diagonal line. A completely normal distribution of scores would have its circles lined
up exactly along the diagonal line. As we can see in Figure 20.10, the GAF distribution
of scores lines up fairly close to the diagonal line, whereas the age distribution deviates
somewhat from the diagonal line, conﬁrming the previous normality test results violation.


C H A P T E R
2 1
Transforming Data to Remedy
Statistical Assumption Violations
21.1
OVERVIEW
A data transformation is a mathematical procedure that can be used to modify or adjust
variables that violate the statistical assumptions of normality, linearity, and homoscedas-
ticity, or have unusual patterns of univariate or multivariate outliers (Meyers, Gamst, &
Guarino, 2009, 2013; Osborne, 2008; Quinn & Keough, 2002). These transformations of
the raw data often help reduce distortions (i.e., skewness, kurtosis) found in the shape
or dispersion of a distribution of scores, but should be employed judiciously due to their
mixed beneﬁts. On the one hand, they can help reduce a distribution’s skewness (this
also brings outliers closer to the bulk of the scores) and kurtosis. On the other hand, the
resulting transformed data values may change the scale of measurement of the construct
and the values may be difﬁcult to interpret (see Gamst et al. (2008) and Meyers et al.
(2013)).
There are a variety of transformations that are commonly used by researchers to
bring the distribution closer to the underlying assumptions of their statistical analyses. If
we were to consider transforming the variable X, then some of these transformations are
as follows:
• Square root of X
• Base 10 logarithm of X
• Inverse of X (1 divided by X)
• Reﬂect X (multiply X by −1)
• Square X
• Cube X
Transforming the scores of a variable necessitates performing some mathematical
operation on those scores to create a new variable; that is, after the transformation, each
case would have a score on the original variable and another score on a newly computed
variable. These newly computed scores would be related to the original ones in that they
would be the square root, logarithm, or whatever other transformation was performed on
that original score.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
147

148
TRANSFORMING DATA TO REMEDY STATISTICAL ASSUMPTION VIOLATIONS
21.2
NUMERICAL EXAMPLE
The present example is from a hypothetical health care study. One interest motivating this
study is to examine the frequency of a sample of 173 clients using the available services
of a particular health care provider. One of the variables in that study was the number
of visits per year made by the members to their family physician. We have isolated this
variable named doc_visits in the data ﬁle named health care use. Our interest is in
adjusting its distribution in preparation for further data analysis.
21.3
ANALYSIS STRATEGY
Using transformations to correct skewness often succeeds in correcting kurtosis as well.
The variable doc_visits exhibits substantial positive skew (a preponderance of lower
scores) and positive kurtosis (it is leptokurtic, i.e., the distribution is compressed relative
to the normal curve). For our ﬁrst analysis, we will generate the descriptive statistics for
this distribution to serve as our baseline when we evaluate the transformations we use
in an effort to make the distribution more normal.
There are three types of transformations that are ordinarily applied to reduce the
magnitude of positive skew (Osborne, 2008; Meyers et al., 2009). These work by “draw-
ing in” the larger magnitude scores toward the bulk of the scores that are in the lower
magnitude area. It should be noted that different transformations (squaring and cubing
the values) targeted to correct negatively skewed distributions work by “pushing out” the
scores of somewhat greater magnitude.
We apply in turn the three transformations to ameliorate positive skew by using the
Compute Variable procedure to build new variables that will represent the transformed
values. We then perform descriptive statistics on the set of transformed variables to see
how our efforts to normalize the distribution fared. The three transformations we use
are as follows:
• Square Root Transformation. The effect of taking a square root of an original
value is obtaining a smaller value, and the size of this effect is proportional to
the magnitude of the original value. For example, the square root of 4 is 2, a
difference of two scale units, but the square root of 100 is 10, a difference of 90
scale units. Thus, larger scores are drawn closer to the lower scores.
• Log Transformation. Logarithms use a base that is raised to a power to represent
a positive number (the log cannot be calculated either for a negative number or
for 0). Among the common bases that are used is base 10, although other bases
(base 2, base e) are often used as well. Consider this example to see how it works.
The logarithm of 100 is the value that the base (we are using base 10 here) must
be raised to obtain 100; that is, 10 to what power equals 100? Here, the answer
is 2, and so the logarithm of 100 in base 10 is 2. This sort of transformation also
draws larger scores inward toward the lower scores.
• Reﬂected Inverse Transformation. As noted earlier, the inverse of X is 1/X. This
makes large values small and small values large, thus “ﬂipping” the scaling of
the variable. Because it is an effective transformation in many circumstances,
researchers like using it for severely skewed distributions but dislike the reversal
of the scaling metric. To deal with this reversal, we add one extra step when
using this transformation: we ﬁrst multiply the variable by −1 to “reﬂect” it.
By reﬂecting the variable, we are “ﬂipping” or reversing the scoring metric in
advance so that when we take the inverse and get the ﬂip or reverse, we will be
back to the original way in which the variable was scored. We know it sounds a

ANALYSIS SETUP: LOG BASE 10 TRANSFORMATION
149
little convoluted but the end result with all the ﬂipping is that the variable lands
right-side up.
21.4
ANALYSIS SETUP: FREQUENCIES OF THE ORIGINAL
doc_visits VARIABLE
Open the data ﬁle named health care use and from the main menu select Analyze ➔
Descriptive Statistics ➔Frequencies. This produces the main Frequencies dialog win-
dow (screenshots for this general setup can be found in Chapters 10 and 20), where we
have moved doc_visits to the Variable(s) panel and have also disengaged the Display
frequency tables checkbox. In the Statistics dialog window (not shown), we have acti-
vated Mean, Std. deviation, Skewness, and Kurtosis. We have also requested in the
Charts dialog window a Histogram but have not requested that the normal curve be
shown on it.
21.5
ANALYSIS OUTPUT: FREQUENCIES OF THE ORIGINAL
doc_visits VARIABLE
The output is shown in Figure 21.1. The mean of 4.63 visits each year is close to the
national average of the United States (about 3.6 visits per year), but both the skewness
(1.620) and kurtosis (2.408) are substantial. The skewness can be seen in the histogram
where most of the health care clients have a relatively few visits per year but there are
some who have quite a few. We can also see the kurtosis in the histogram in that the
distribution is relatively compressed in the region of less frequent visits. In general, the
distributional assumptions for most of our statistical procedures are not met here.
21.6
ANALYSIS SETUP: SQUARE ROOT TRANSFORMATION
From the main menu select Transform ➔Compute Variable, which produces the
Compute Variable dialog window (readers can review Chapter 16 for a refresher). We
have supplied the name square_root_visits for the new variables in the Target Variable
panel.
IBM SPSS® has a built-in square root function housed in the Function group of
Arithmetic. To access it, we highlight Arithmetic in the Function group panel and scroll
down to Sqrt in the Functions and Special Variables panel. This is shown in Figure 21.2.
Double-clicking Sqrt (or clicking the up arrow to the left of the panel) moves
the function into the Numeric Expression panel at the top of the dialog window. It
appears as SQRT(?), with the “?” highlighted, and double-clicking doc_visits replaces
the highlighted “?” with our doc_visits variable as shown in Figure 21.3. Clicking OK
performs the computation and places the new variable (square_root_visits) at the end
of the data ﬁle (see Figure 21.4).
21.7
ANALYSIS SETUP: LOG BASE 10 TRANSFORMATION
We engage in an analogous procedure to perform the log base 10 transformation.
We select Transform ➔Compute Variable, name the to-be-transformed variable
log10_visits, scroll down to Lg10 in the Arithmetic function set of the Function group,
double-click Lg10 into the Numeric Expression panel, and double-click doc_visits to
place it in the expression (see Figure 21.5). Clicking OK performs the computation.

150
TRANSFORMING DATA TO REMEDY STATISTICAL ASSUMPTION VIOLATIONS
FIGURE 21.1
Descriptive statistics and histogram for
doc_visits.
21.8
ANALYSIS SETUP: REFLECTED INVERSE
TRANSFORMATION
The reﬂected inverse transformation is accomplished in two successive computations:
• We ﬁrst multiply doc_visits by −1 to create a reﬂected variable.
• We then perform an inverse transformation on the reﬂected variable.
There is no established function to reﬂect a variable, so to reﬂect doc_visits, we select
Transform ➔Compute Variable, name the to-be-transformed variable reﬂected_visits,
double-click doc_visits into the Numeric Expression panel, and type * (–1) next to
it to multiply the variable by −1 as shown in Figure 21.6. Clicking OK performs the
computation and places the new variable (reﬂected_visits) at the end of the data ﬁle.
We now take the inverse of the reﬂected variable to create the reﬂected inverse
transformation. In the Compute Variable window, name the new variable reﬂected_in-
verse_visits, type 1/ in the Numeric Expression panel, highlight reﬂected_visits, and

ANALYSIS SETUP: FREQUENCIES OF ALL OF THE TRANSFORMED VARIABLES
151
FIGURE 21.2 Accessing the square root (Sqrt) function in Compute Variables.
then click the right-facing arrow to the left of the Numeric Expression panel to move
reﬂected_visits into it, as shown in Figure 21.7. Clicking OK performs the computation
and places the new variable (reﬂected_inverse_visits) at the end of the data ﬁle.
A screenshot of a portion of the data ﬁle is presented in Figure 21.8. For each case,
we see the original doc_visits variable together with the three transformed variables (as
well as the intermediate reﬂected_visits variable). As we indicated earlier, one of the
disadvantages of using transformations is the potential difﬁculty of directly interpreting
them in terms of the original variable. For example, the client with id code 175 registered
10 doctor visits, a value that is quite understandable. Our transformed values for that case
are less directly understandable, in that the 10 doctor visits show transformed values of
3.16, 1.00, and −.10 for square_root_visits, log10_visits, and reﬂected_inverse_visits,
respectively.
21.9
ANALYSIS SETUP: FREQUENCIES OF ALL OF THE
TRANSFORMED VARIABLES
We can now check our work to see if our transformations have reduced the original
skewness or kurtosis. From the main menu select Analyze ➔Descriptive Statistics ➔
Frequencies. This produces the main Frequencies dialog window (screenshots for this
general setup can be found in Chapters 10 and 20), where we have moved doc_visits,
square_root_visits, log10_visits, and reﬂected_inverse_visits to the Variable(s) panel.

152
TRANSFORMING DATA TO REMEDY STATISTICAL ASSUMPTION VIOLATIONS
FIGURE 21.3 We are ready to take the square root of doc_visits.
FIGURE 21.4
A portion of the data ﬁle with the inclusion of
square_root_visits.

ANALYSIS SETUP: FREQUENCIES OF ALL OF THE TRANSFORMED VARIABLES
153
FIGURE 21.5 We are ready to take the log base 10 of doc_visits.
FIGURE 21.6 Reﬂecting doc_visits.

154
TRANSFORMING DATA TO REMEDY STATISTICAL ASSUMPTION VIOLATIONS
FIGURE 21.7 Inversing the reﬂected variable to produce a reﬂected inverse transformation.
FIGURE 21.8 A portion of the data ﬁle with the inclusion of all of our computed variables.

ANALYSIS OUTPUT
155
FIGURE 21.9 Descriptive statistics for the original and transformed variables.
In the Statistics dialog window (not shown), we have requested Mean, Std. devia-
tion, Skewness, and Kurtosis. We have also requested in the Charts dialog window a
Histogram.
21.10
ANALYSIS OUTPUT
Figure 21.9 displays the Statistics results from our Frequencies analysis. The ﬁrst column
describes our original doc_visits variable, and the issue is how much improvement is
obtained with each transformation. The square root transformation certainly improved
the shape of the distribution; skewness was lowered from 1.620 to .924, and kurtosis
was lowered from 2.408 to .348. Thus, the skewness is close to the ±1.00 guideline but
the value for kurtosis is relatively close to that of the normal distribution.
The log base 10 transformation appeared to be a bit stronger than the square root
transformation; skewness was lowered from 1.620 to .168, and kurtosis was lowered from
2.408 to −.447. Taking the log of the variable reduced skewness down to a negligible
amount but actually overshot the zero mark for kurtosis, driving it negative or mildly
platykurtic (a little ﬂattened). Nonetheless, these values are also acceptable.
The reﬂected inverse transformation just was not effective in this context. It drove
the skewness all the way to −1.435 but only lowered the kurtosis to a level that was
still leptokurtic. Given a choice of these three transformations, we would be inclined to
go with the log transformation for this particular data set.


P A R T 7
BIVARIATE CORRELATION


C H A P T E R
2 2
Pearson Correlation
22.1
OVERVIEW
In its most general sense, a correlation indexes the extent to which the variables in the
analysis are related. There are several correlation coefﬁcients applicable to the research
conducted in the behavioral, social, and medical sciences, but the most widely used is the
Pearson Product Moment Correlation, usually referred to as the Pearson correlation or just
the Pearson r. It assumes that the variables represent approximately interval measurement
and that they are approximately normally distributed; outliers can seriously distort the
value of the correlation, and so should be appropriately handled before data analysis.
The Pearson r was developed by Karl Pearson (1896) based on the initial develop-
ment of the idea by Sir Francis Galton (1886, 1888). It assesses the degree to which two
variables are linearly related. To the extent that the relationship between the two variables
is not linear (e.g., a U-shaped function), the Pearson r will substantially underestimate
how strongly the two variables are associated (in case of a symmetric U-shaped function,
the Pearson r will be zero).
To say that two variables are related means that they covary. One way to think of
covariation is that variation in one variable is synchronous with that of the other variable.
For example, cases with higher values on one variable might tend to have lower values on
the other variable. A related way to think of covariation is that values of one variable are
predictable by some margin better than chance from the knowledge of the corresponding
values on the other. The Pearson r2 indexes the strength of the relationship, that is, the
amount of variance shared between the two variables.
22.2
NUMERICAL EXAMPLE
The data we use for our example are extracted from a study of personality variables on
425 university students. Data are present in the data ﬁle named Personality. We focus
on two of the personality variables in this chapter: beckdep as a measure of depression
and regard as a measure of self-regard.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
159

160
PEARSON CORRELATION
22.3
ANALYSIS SETUP: CHECKING FOR LINEARITY
Because the Pearson r assesses the degree to which a pair of variables is linearly related,
it is useful to make a rough determination of the degree to which the relation is linear
before calculating the Pearson r. This can be accomplished by examining the scatterplot
of the two variables.
To obtain a scatterplot, we open the data ﬁle named Personality and from the main
menu select Graphs ➔Legacy Dialogs ➔Scatter/Dot, which produces the window
shown in Figure 22.1. We select Simple Scatter and click Deﬁne to open the Simple
Scatter dialog screen presented in Figure 22.2. We will arbitrarily place beckdep in the
Y Axis panel and regard in the X Axis panel. Clicking OK produces the scatterplot.
FIGURE 22.1
Scatter/Dot selection screen.
FIGURE 22.2
The Simple Scatter dialog screen.

ANALYSIS OUTPUT: CORRELATING A SINGLE PAIR OF VARIABLES
161
FIGURE 22.3
Scatterplot of beckdep and regard.
22.4
ANALYSIS OUTPUT: CHECKING FOR LINEARITY
The scatterplot of beckdep and regard is presented in Figure 22.3. Visual inspection
suggests that the two variables are related linearly. It also appears that the line of best ﬁt
is angled from the upper left to the lower right, indicating a negative (inverse) relationship.
22.5
ANALYSIS SETUP: CORRELATING A SINGLE
PAIR OF VARIABLES
With the appearance of linearity, we can conﬁdently generate the Pearson correlation
between beckdep and regard. From the main menu select Analyze ➔Correlate ➔
Bivariate. This opens the main Bivariate Correlations window shown in Figure 22.4.
We move beckdep and regard into the Variables panel. We retain the default speci-
ﬁcations of asking only for Pearson in the Correlation Coefﬁcients area, requesting
Two-tailed under Test of Signiﬁcance, and ask that IBM SPSS® Flag signiﬁcant cor-
relations (it will use asterisks to represent statistical signiﬁcance).
Selecting the Options pushbutton allows us to obtain the Means and standard
deviations of the variables in the analysis (see Figure 22.5). We retain the Missing
Values speciﬁcation of Exclude cases pairwise. Click Continue to return to the main
dialog window and click OK to perform the analysis.
22.6
ANALYSIS OUTPUT: CORRELATING A SINGLE
PAIR OF VARIABLES
The output of the analysis is presented in Figure 22.6. Descriptive statistics are displayed
in the top table and the correlation in the bottom table. Correlation results are presented
in a “square” matrix, with a value of 1.00 on the diagonal. The three entries inside the
cell(s) showing the intersection of beckdep and regard provide the following information
in the following order:

162
PEARSON CORRELATION
FIGURE 22.4
The main Bivariate Correlations dialog window.
FIGURE 22.5
The Options window of Bivariate Correlations.
1. The Pearson correlation between the two variables is −.539.
2. The probability of that correlation value with an N of 421 (degrees of free-
dom = N −2 or 419) occurring by chance, given the null hypothesis is true, is
less than .001 (shown as .000 in the output but reported as p < .001). We can
see the exact probability by double-clicking .000 in the output table (see Section
5.4).
3. There were 421 cases with valid values on both variables.
22.7
CORRELATING SEVERAL PAIRS OF VARIABLES
To show the correlation output for several pairs of variables, we have added neoneuro
(neuroticism), posafect (positive affect), and neoconsc (consientiousness) to the set of
variables but otherwise speciﬁed the analysis as described. We show the correlation
matrix in Figure 22.7.
The diagonal of the matrix can be easily seen angled from upper left to lower right.
With a square matrix, values above and below the diagonal are redundant. All of the

CORRELATING SEVERAL PAIRS OF VARIABLES
163
FIGURE 22.6
Descriptive statistics and the Pearson
correlation output.
FIGURE 22.7 Descriptive statistics and the Pearson correlation output for a set of ﬁve variables.
correlations are statistically signiﬁcant given an alpha level of .05. These correlations
are based on sample sizes ranging from 419 to 422 due to the Exclude cases pairwise
speciﬁcation in the Options dialog window. Under pairwise exclusion, cases missing a
value on one of the two variables involved in one correlation calculation (and who are
thus excluded from that calculation) are eligible to be included in another calculation if
they have valid values on those two variables.


C H A P T E R
2 3
Spearman Rho and Kendall
Tau-b Rank-Order Correlations
23.1
OVERVIEW
When the assumptions underlying the Pearson correlation cannot be met, there are non-
parametric correlation procedures that can be implemented as an alternative. Generally,
nonparametric procedures do not compare a sample parameter (e.g., the sample mean) to
a population parameter (e.g., the population mean) and make fewer assumptions about the
distribution (e.g., they do not require a normal distribution). Furthermore, nonparametric
procedures are applicable to variables assessed on categorical (e.g., frequency) or ordinal
scales of measurement (Marascuilo & McSweeney, 1977). Two of the most commonly
used alternatives to the Pearson correlation are built into the Bivariate Correlation pro-
cedure in IBM SPSS®: the Spearman rho and the Kendall tau-b correlations. Both are
applicable to variables measured on ordinal scales and assume only monotonicity of the
relationship.
Ordinal measurement conveys only greater than or less than information and is rep-
resented by ranked data. In applying the Spearman rho or the Kendall tau-b correlations,
the values of the X and Y variables are ﬁrst independently rank-ordered (although the
Kendall tau-b can be applied to raw data as well). The ranks are then substituted for the
raw score values for each pair of scores, and the computation for each is then performed.
Neither correlation thus requires that the data approximate interval measurement, and
neither correlation requires that the variables be approximately normally distributed. By
tapping into only ordinal information, the inﬂuence of outliers that can seriously and
adversely affect the Pearson r is circumvented by the Spearman rho and the Kendall
tau-b.
A monotonic relationship between two variables exists when higher values of one
variable are consistently associated with higher values of the other or when higher values
of one variable are consistently associated lower values of the other. A linear relationship,
as assessed by the Pearson r, is a special case of a monotonic relationship, but some
other nonlinear relationships (e.g., a tilted J shaped function) are also monotonic and
can therefore be assessed by the Spearman rho and the Kendall tau-b correlations. The
key to monotonicity is that the association does not show a reversal someplace along the
continuum (e.g., higher values of X are associated with higher values on Y up to some
point and then still higher values of X are associated with lower values of Y).
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
165

166
SPEARMAN RHO AND KENDALL Tau-b RANK-ORDER CORRELATIONS
23.2
THE SPEARMAN RHO CORRELATION
The Spearman rho correlation was introduced by Sir Charles Spearman (1904b) primarily
but not exclusively as a way to minimize the adverse inﬂuence of outliers in computing a
Pearson r. If we were to transform two variables to their rank-order equivalents and apply
the computational procedure for the Pearson correlation, we would obtain the value of
the Spearman rho. Thus, many researchers think of the Spearman rho as a close relative
of the Pearson r.
In calculating the Spearman rho, we determine the difference between the ranks of
each data pair. These differences are then squared and summed and that value is entered
into the computational formula. Because large differences in ranks are magniﬁed when
squared, the presence of large rank differences will result in relatively lower values of
rho. Values of rho closer to ±1.00 indicate that the rankings on X and Y are relatively
similar to each other; values of rho closer to .00 indicate the rankings of X and Y are
relatively independent of each other.
23.3
THE KENDALL TAU-b CORRELATION
Maurice G. Kendall (1938, 1948) introduced the tau correlation as an alternative approach
to rank-order correlation. There are three variations of the statistic, tau-a, tau-b, and tau-c,
but in the IBM SPSS Bivariate Correlation procedure only tau-b (applicable for tied
ranks as well as rankings with no ties for two variables) is available.
As was true for the Spearman rho, the values of the two variables are ﬁrst indepen-
dently rank-ordered. Then the cases are listed based on their rank order on X, starting
with Rank 1.
To compute the Kendall tau-b, the number of concordant and discordant pairs below
each case is counted based on the Y variable. A concordant pair is one for which the
ranking on Y is lower than the given case. It is labeled as concordant because cases
lower in the listing are ranked lower on X (i.e., the basis of the listing), and if the Y
value is also ranked lower, then the relationship is concordant (consistent or consonant).
A discordant pair is one for which the ranking on Y is higher than the given case. It
is labeled as discordant because cases lower in the listing are ranked lower on X (that
is the basis of the listing), and if the Y value is ranked higher, then the relationship is
discordant (inconsistent or dissonant).
Values of tau-b closer to +1.00 indicate that the rankings of X and Y are ordered
in a very similar manner (i.e., they are mostly concordant); values of tau-b closer to
−1.00 indicate that the rankings of X and Y are ordered inversely (i.e., they are mostly
discordant); values of tau-b closer to .00 indicate the rankings of X and Y are relatively
independent of each other. There are some suggestions (e.g., Howell, 2010; Wilcox, 2012)
that Kendall’s correlation is a better population estimate than Spearman’s, and thus may
be somewhat preferred, but most textbooks discussing both usually do not indicate a
preference between them.
23.4
NUMERICAL EXAMPLE WITHOUT TIES
We use the data ﬁle named Spearman Kendall No Ties containing two generic variables
named X and Y. These variables have each independently been rank-ordered and have
been listed based on the X ranking. For example, the ﬁrst case is ranked ﬁrst on X but
second on Y. A screenshot of the data ﬁle is presented in Figure 23.1.

ANALYSIS OUTPUT
167
FIGURE 23.1 The Kendall No Ties data ﬁle.
23.5
ANALYSIS SETUP
The setup for the analysis is similar to how we obtained the Pearson correlation (which we
will also request here for comparison purposes). From the main menu, we select Analyze
➔Correlate ➔Bivariate. This opens the main Bivariate Correlations window as
shown in Figure 23.2. We move X and Y into the Variables panel requesting Pearson,
Kendall’s tau-b, and Spearman in the Correlation Coefﬁcients area. We retain the
default speciﬁcations requesting Two-tailed under Test of Signiﬁcance and asking that
IBM SPSS Flag signiﬁcant correlations. Click OK to perform the analysis.
23.6
ANALYSIS OUTPUT
The output of the analysis is shown in Figure 23.3, with the parametric (Pearson r) and
two nonparametric correlations provided in separate output tables. As indicated earlier, the
Pearson and Spearman correlations return the same value on the ranked data. Kendall’s
tau-b is noticeably lower than the Spearman rho; this is a very typical result as long
as the rank differences are not extreme. In both instances, however, it appears that the
variations in the ranks of X and Y are very much in synchrony (concordance) with each
other. All correlations are statistically signiﬁcant under an alpha level of .05; the Pearson
r of .958 and the Spearman rho of .958 are at p < .001 and the Kendall tau-b of .818 is
also at p < .001.

168
SPEARMAN RHO AND KENDALL Tau-b RANK-ORDER CORRELATIONS
FIGURE 23.2
The main Bivariate Correlations dialog window.
FIGURE 23.3
The Pearson, Kendall tau-b, and Spearman
correlations based on data with no ties.

ANALYSIS SETUP AND OUTPUT
169
FIGURE 23.4 The Kendall Ties data ﬁle.
23.7
NUMERICAL EXAMPLE WITH TIES
To demonstrate the differences among the three correlations when there are ties in the
rankings, we use the data ﬁle named Spearman Kendall Ties containing two generic
variables named A and B. These variables have each independently been rank-ordered
and listed based on the A ranking. Thus, Case number 1 is ranked ﬁrst on A but eighth
on B. A screenshot of the data ﬁle is presented in Figure 23.4.
Ties in ranks are handled by assigning each of the tied ranks an average of the ranks
that would have been used in the absence of a tie. For example, Case number 2 and Case
number 6 were tied on the B measure and were each given the average rank of 4.5. As
can be seen in the data ﬁle, 10 of the 14 cases were involved in tied scores on the B
variable.
23.8
ANALYSIS SETUP AND OUTPUT
The setup for the analysis is identical to our description in Section 23.5, and the output
of the analysis is shown in Figure 23.5. Once again the Pearson r and the Spearman rho
are equal in value, as the algorithm for calculating the Spearman used by IBM SPSS
accounts for the presence of ties. For these same data, the Kendall tau-b, also designed
to accommodate ties, returns a noticeably lower correlation value. All correlations are
statistically signiﬁcant under an alpha level of .05; the Pearson r of .632 and the Spearman
rho of .632 are at p = .015 and the Kendall tau-b of .475 is at p = .021. The choice

170
SPEARMAN RHO AND KENDALL Tau-b RANK-ORDER CORRELATIONS
FIGURE 23.5
The Pearson, Kendall tau-b, and Spearman
correlations based on data with ties.
between reporting the Spearman rho or the Kendall tau-b falls to the preferences of the
researchers, as each is reported in the journal literature. Our slight preference is for the
Kendall tau-b.

P A R T 8
REGRESSING (PREDICTING)
QUANTITATIVE VARIABLES


C H A P T E R
2 4
Simple Linear Regression
24.1
OVERVIEW
Pearson correlation and simple linear regression are two ways to express the same fun-
damental idea. Pearson correlation generally addresses the degree to which two variables
are associated or related, whereas simple linear regression addresses the prediction of
one variable based on the other (a speciﬁc way to characterize a relationship). Galton
(1886) ﬁrst discussed the idea of regression (regression toward the level of “mediocrity”
or what we now think of as regression toward the mean) and 2 years later (Galton, 1888)
broached the idea of covariation or what we now call correlation.
Unlike bivariate correlation where the two variables have the same status in the
analysis, the two variables play different roles in simple linear regression. The value
of one of the variables is being predicted; this variable is the dependent, criterion, or
outcome variable and is generically denoted as the Y variable. The other variable is used
to predict the dependent variable; this variable is the independent or predictor variable
and is generically denoted as the X variable.
Simple linear regression derives its name from the following reasons:
• It is regression because we are predicting the value of Y from one quantitative X
variable. Prediction is represented by a regression equation or model specifying
the predicted value of Y as a function of X.
• It is simple because we use a single predictor or independent variable (X) to predict
the value of outcome, criterion, or dependent variable (Y).
• It is linear because we are generating a linear regression model, that is, the equation
or model that results from the regression analysis represents a straight line.
The regression model is generated by a least squares algorithm; that is, the linear
prediction function describes where the sum of the squared deviations around the function
is minimal and thus produces the best ﬁt line relating X and Y. This is why the procedure
is sensitive to outliers; distances from the center of the distribution are squared and large
distances (which deﬁne outliers) that are squared are “disproportionally large” and thus
draw the regression function toward them.
The model generated by the regression procedure is provided in both raw score and
standardized score form. The raw score (unstandardized) model (Predicted Y = a + bX)
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
173

174
SIMPLE LINEAR REGRESSION
includes a value for the constant (a), which is the Y intercept of the function (the predicted
value of Y when X = 0), and a regression coefﬁcient (b), which is the weight given to X
to maximize the predictability of Y. The regression coefﬁcient is the slope of the linear
function and represents the amount of change expected in Y for each unit change in X.
The standardized model (Predicted Yz = beta Xz) applies to X and Y when each
has been transformed to z scores. It does not include a value for the constant, as the
standardized model passes through the origin (the Y intercept is zero). The standardized
regression coefﬁcient is labeled as beta. This model is automatically produced by IBM
SPSS® when the raw score model is being generated. In simple linear regression, the
beta coefﬁcient is the Pearson correlation coefﬁcient.
24.2
NUMERICAL EXAMPLE
We use the data ﬁle named Exercise containing ﬁctional data from a study predicting
the degree to which employees of a large ﬁrm are committed to following a physical
exercise regimen. In addition, the researchers also assessed the degree to which the
employees dieted as well as some personal variables (their need for social afﬁliation,
their self-acceptance, their self-esteem, and the esteem they felt in their own body). The
names of the variables are self-descriptive, and a screenshot of a portion of the data
ﬁle is presented in Figure 24.1. For the present example, we use selfesteem to predict
exercise_commitment.
FIGURE 24.1 The data ﬁle.

ANALYSIS SETUP
175
24.3
ANALYSIS SETUP
From the main menu, we select Analyze ➔Regression ➔Linear. This opens the main
Linear Regression window as shown in Figure 24.2. We move exercise_commitment
into the Dependent panel and selfesteem into the Independent(s) panel. We retain Enter
in the Method drop-down menu (this deals with how the variables are to be entered into
the model as it is built—the issue is not relevant with just a single predictor).
Selecting the Statistics pushbutton opens the Statistics screen (see Figure 24.3).
Most of these are more relevant to multiple linear regression, but it provides an
opportunity here to address them in a simpler manner than we do in our Chapter 26.
We check Estimates (to obtain the regression coefﬁcients), Model ﬁt (to obtain the
R2 and adjusted R2), Descriptives (to obtain the descriptive statistics), and Part and
partial correlations (to obtain the zero-order, partial, and semipartial correlations).
FIGURE 24.2
The main Linear Regression dialog window.
FIGURE 24.3
The Statistics screen of Linear Regression.

176
SIMPLE LINEAR REGRESSION
Click Continue to return to the main dialog window and click OK to perform the
analysis.
24.4
ANALYSIS OUTPUT
The descriptive statistics (means and standard deviations) for the two variables are shown
in the top table of Figure 24.4. As was seen in the screenshot of the data ﬁle and is appar-
ent from the descriptive statistics, selfesteem and exercise_commitment are assessed on
quite different scales, even though both are quantitatively measured. The variable of
exercise_commitment was computed by averaging across survey items that were asso-
ciated with a 5-point summative response scale, with higher values indicating greater
commitment; the mean of 3.3742 suggests somewhat more than neutral commitment to
exercise. The variable of selfesteem varied between 20 and 50, with higher values indi-
cating greater levels of self-esteem; the mean of 39.08 suggests a relatively high level
of self-esteem.
The square correlation matrix is shown in the bottom table of Figure 24.4. The
Pearson correlation of .262 based on a sample size of 415 is statistically signiﬁcant
at p < .001. IBM SPSS has used the more lenient 1-tailed test of signiﬁcance because
prediction is going to be in only one direction, but with the p value showing at .000, that
is not an issue here.
Figure 24.5 shows the result of testing the ﬁt of the model. In the top table, R
represents the multiple correlation; because there is only one predictor variable, R takes
on the same value (.262) as the Pearson r (the Pearson r is the limiting case of the multiple
correlation). R Square is the squared multiple correlation and represents the strength of
the predictive relationship; here, its value is .069 (the Pearson r2) and suggests that
about 7% of the variance of exercise_commitment is explained by selfesteem. We did
not request in the Statistics screen to obtain the change in R2 because with only a single
predictor we would go from zero (before the model was applied to the data) to .069.
IBM SPSS also provides a value for the Adjusted R Square. This is supplied by
most statistical software programs because R2 is a bit of an inﬂated estimate of how
much variance the model can really explain in that some of the prediction it subsumes
is taking advantage of chance (some of the error variance that coincidentally is in the
FIGURE 24.4
Descriptive statistics and the
correlation matrix.

ANALYSIS OUTPUT
177
FIGURE 24.5 Testing the ﬁt of the model.
direction of prediction). For this reason, an adjustment (based on sample size and the
number of predictor variables in the model) is made to R2 that provides a more realistic (a
lower) estimate (sometimes referred to as shrinkage) of how well the model is performing
(Meyers et al., 2013) and should always be reported in conjunction with R2. In the present
example, because of the high ratio of cases to predictors, R2 was expected to shrink by
a minimal amount (from .069 to .066).
The bottom table in Figure 24.5 provides a test of the statistical signiﬁcance of the
regression model using a one-way between-subjects ANOVA. The effect evaluated in the
ANOVA is the regression model (labeled Regression in the summary table), and it has
one degree of freedom because that is how many predictors are in the model. The total
degrees of freedom are equal to N −1 or 414, leaving 413 degrees of freedom for the
error term. The model accounts for a signiﬁcant amount of dependent variable variance,
F(1, 413) = 30.42, p < .001. If the Regression effect was not statistically signiﬁcant, we
would have concluded that we could predict no better than chance and thus we would
stop examining the results of the analysis at that point.
The eta square value in this ANOVA (a frequently used index of strength of effect
in the ANOVA family of designs) is equal to the sum of squares associated with the
Regression effect (this is the regression model) divided by the sum of squares associated
with the Total variance, which yields 21.593/314.732 or .069; this is the same value as
R2 because the ANOVA and linear regression are just different expressions of the general
linear model.
Figure 24.6 presents the Coefﬁcients of the variable in the regression model as well
as some other information. There are three coefﬁcients provided under the Correlations
heading in the table. These are brieﬂy noted as follows:
• Zero-order Correlation. This is the Pearson r between the predictor and the depen-
dent variable. It is labeled as zero-order to represent the fact that there are no
covariates taken into account. In a multiple regression analysis, the set of predic-
tor variables in the model serve as covariates in evaluating the effect of any given
predictor.
• Partial Correlation. This is the correlation between the predictor and the variance
not explained by the other predictors in the model (the residual variance). With
no other predictors in this model, the partial correlation is equal to the Pearson r.
• Part Correlation. This is a short name for the semipartial correlation. Squaring
this value to obtain the squared semipartial correlation informs us of how much

178
SIMPLE LINEAR REGRESSION
FIGURE 24.6 The regression coefﬁcients together with the correlations.
variance of the dependent variable is explained exclusively by the predictor given
that there are (usually) other predictors in the model; that is, the squared semipartial
correlation is said to assess the amount of dependent variable variance uniquely
explained by the given predictor (taking into account the other predictors in the
model). Again, with no other predictors in this model, the semipartial correlation
is equal to the Pearson r.
The Y intercept (labeled as the constant in the table) and the unstandardized regres-
sion coefﬁcient for selfesteem are shown under Unstandardized Coefﬁcients together
with the standard error for each estimate. The unstandardized (raw score) regression
coefﬁcient for selfesteem is .034 (this is the slope of the unstandardized model) and is
statistically signiﬁcant (p < .001) as assessed by a t test. Because it is positive, we can
say that an increase of one unit on the selfesteem measure is expected to be associated
with an increase in exercise_commitment of 0.034 units. This may seem like a very
small increase in exercise_commitment until we recall (see Figure 24.4) that selfesteem
scores average about 39 and exercise_commitment scores average about 3.4.
The beta (standardized) regression coefﬁcient for selfesteem is .262 (the slope of
the standardized model); this is also the value of the Pearson r between selfesteem and
exercise_commitment. On the basis of the value of this coefﬁcient, we can say that
an increase of 1.00 z score (one standard deviation unit) on the selfesteem measure is
expected to be associated with an increase in exercise_commitment of .262 z score
(standard deviation) units. The unstandardized and standardized coefﬁcients represent
alternative but comparable ways to express the slope of the regression function.
The Constant is the Y intercept of the unstandardized regression equation and rep-
resents the predicted value of Y for an X score of zero. Here, a selfesteem score of zero
(were such a score possible) would be associated with an exercise_commitment score
of 2.043.
We can summarize the two models by writing out the regression equations. The
unstandardized simple regression model is as follows:
exercise _ commitment = 2.043 + (. 034)(selfesteem)
The standardized simple regression model is as follows:
exercise _ commitmentz = (. 262)(selfesteemz)
24.5
THE Y INTERCEPT ISSUE
It is worthwhile noting that with a possible range on the selfesteem variable of 20–50, a
selfesteem score of zero is an out-of-range value. Because of this, we would not interpret
the Y intercept in this model in any empirical (meaningful) way. In other research studies,
it is possible that a score of zero on the predictor variable is a valid value and so an
empirical or meaningful interpretation of the Y intercept would then be possible.

THE Y INTERCEPT ISSUE
179
The lack of empirical meaning associated with the Y intercept does not often matter
to researchers because their focus is on the R2 and adjusted R2 values, as well as on the
regression coefﬁcients informing them of the slope of the regression function. But there
are situations in which this lack of empirical meaning is, if not exactly problematic, then
certainly not an efﬁcient use of all the information potentially contained in the regression
model. One very useful way to deal with this issue is to center the predictor, and we
address this in Chapter 25.


C H A P T E R
2 5
Centering the Predictor Variable
in Simple Linear Regression
25.1
OVERVIEW
As we indicated in Chapter 24, the Y intercept (the constant) in the unstandardized
regression model is the predicted value of the dependent (criterion) variable when the
score on the independent (predictor) variable is zero. However, it is not unusual for zero to
be an out-of-range value on the predictor variable. Examples include predictor variables
assessed on summative response scales (e.g., 5-point scales using response values of 1
through 5), test scores measured on standardized scales (e.g., intelligence, achievement
tests), and various biological medical/health measures (e.g., heart rate, body mass index).
Even when a zero score on the independent (predictor) variable is a valid value, it is often
the case that it is an unusual or an unlikely score, given the measurement scale or the
distribution of the predictor variable. When a zero value on the predictor is either not valid
or is an unusual score, the value of the Y intercept has no or very little empirical utility.
One approach to providing meaning or utility to the Y intercept without altering the
results of a simple regression analysis is to center the predictor variable. To center a
variable is to create from the original values of the variable a new variable with a mean
of zero. We accomplish this by transforming the raw score to a deviation score using the
Compute procedure to subtract a reference score, such as the mean of the distribution,
from the score recorded for each case (i.e., Deviation Score = Obtained Score −Mean).
Therefore, any case whose score is equal to the mean has a deviation score of zero.
A z score is an elaborate form of a centered score; it is centered on the mean, and
any z score represents the distance between a score and the mean of the distribution. It
is more elaborate than a simple deviation score because, by dividing the distance of the
score from the mean by the standard deviation, the distance between each data point and
the mean is expressed in standard deviation units.
Centering is a strategy that is commonly applied when dealing with interaction
effects in regression (Cohen, Cohen, West, & Aiken, 2003) and when performing an
analysis involving multilevel modeling (Meyers et al., 2013). In the context of simple
linear regression, there are two straightforward options for centering:
• Mean centering (more precisely called grand mean centering in the context of
multilevel modeling) transforms the raw scores of a variable into deviations (dis-
tances) from the overall mean of that variable. In such centering, the mean is the
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
181

182
CENTERING THE PREDICTOR VARIABLE IN SIMPLE LINEAR REGRESSION
center of the transformed distribution and becomes the new zero value. This is the
strategy we illustrate in this chapter.
• Reference-score centering requires us to select a particular score as the reference
value that presumably has a special meaning to the researchers. An example of
this might be the passing score on a written examination taken by applicants to
become licensed as an electrician; such an examination score is unlikely to be
the mean of the test score distribution but may actually be of greater use to a
state licensing agency as the reference score in a regression study predicting on-
the-job performance based on licensing examination performance. Such centering
transforms the raw scores into deviations (distances) from that reference value. In
such centering, the reference score is the center of the transformed distribution
and becomes the new zero value.
25.2
NUMERICAL EXAMPLE
We use the data ﬁle named BMI and pulse rate containing ﬁctional data from a medical
study. Body mass index can be used to judge whether someone is overweight. Adolphe
Quetelet originally developed this index in 1832 as the Quetelet Index, which was named
the body mass index in 1972 by Ancel Keys (Eknoyan, 2008). Quetelet, in his quest to
describe the average person, wished to determine if the normal curve could be applied
to characteristics of human beings (Stigler, 1986).
Body mass index is the ratio of weight in kilograms to squared height in meters
or, equivalently, weight in pounds multiplied by 703 to squared height in inches. Body
mass index categories are as follows: underweight is less than 18.5, normal is between
18.5 and 24.9, overweight is between 25 and 29.9, obese is between 30 and 39.9, and
morbidly obese is 40 or greater. Higher levels of body mass index pose a potentially
greater health risk. One standard gauge of health status is heart rate, and body mass index
may be related to (i.e., predict) that health factor. In the data ﬁle, we have recorded the
heart rate under the variable of pulse_rate and body mass index under the variable of
BMI for 40 medical patients.
25.3
ANALYSIS STRATEGY
Our analysis strategy is as follows:
• We perform a descriptive statistics procedure to determine the mean of BMI so
that we can perform the centering operation.
• We center the BMI scores using a Compute procedure to build (and save) to the
data ﬁle a new centered variable (subtracting the BMI mean from the BMI score
for each case).
• We use the original BMI variable to predict pulse_rate in a simple linear regres-
sion analysis so that we can compare these results to those of the centered analysis.
• We use the centered BMI variable to predict pulse_rate in a simple linear regres-
sion analysis to show what the difference is in a centered procedure.
25.4
OBTAINING DESCRIPTIVE STATISTICS
ON THE PREDICTOR VARIABLE
We open the data ﬁle named BMI and pulse rate, and from the main menu select
Analyze ➔Descriptive Statistics ➔Descriptives. We move BMI into the Variable(s)

COMPUTING THE CENTERED PREDICTOR VARIABLE
183
FIGURE 25.1
The main dialog window of Descriptives.
FIGURE 25.2
Descriptive statistics for BMI.
panel as shown in Figure 25.1. As the default statistics produced include the mean, we
click OK to perform the analysis.
The results of the analysis are shown in Figure 25.2. Our interest here is in the mean,
which is 25.31. In our centering computation, we will subtract this value from each BMI
score.
25.5
COMPUTING THE CENTERED PREDICTOR VARIABLE
As described in Chapter 16, from the main menu select Transform ➔Compute Vari-
able. This opens the Compute Variable dialog window shown in Figure 25.3. We have
named the new variable BMI_centered in the Target Variable panel. We have then
moved the variable BMI into the Numeric Expression panel, clicked the minus sign
(second row, leftmost key), and then typed 25.31 in the Numeric Expression panel. This
expression will subtract the value of 25.31 from each BMI score. Click OK to perform
the procedure.
A screenshot of a portion of the data ﬁle with the newly computed BMI_cen-
tered variable in place is shown in Figure 25.4. Patient number 3, for example, has a
BMI_centered value of −6.31. This value was obtained by subtracting the BMI mean
of 25.31 from patient 3’s BMI score of 19.00 (19.00 −25.31 = −6.31). The negative
value informs us that the score is below the mean, and the value of 6.31 informs us that
the score is 6.31 body mass units from the mean.
To illustrate the effects of a centering operation on the distribution of a variable, we
have analyzed both BMI and BMI_centered in the Descriptives procedure (where we
have requested more than the default statistics). The results of this analysis are shown in
Figure 25.5. The mean has shifted from 25.31 for BMI to .0000 for BMI_centered and
the accompanying minimum and maximum values have also changed (as the values for
BMI_centered are now deviations from the mean). More to the point, the fundamental
variability attributes of the distribution (standard deviation, skewness, and kurtosis) are
unchanged from the original variable as a result of this centering transformation; thus,
the centered distribution is simply shifted intact horizontally from the original location
around the original mean to a new location centered on zero but is otherwise identical.

184
CENTERING THE PREDICTOR VARIABLE IN SIMPLE LINEAR REGRESSION
FIGURE 25.3 The Compute dialog window conﬁgured to center the BMI variable.
FIGURE 25.4
A portion of the data ﬁle with the
newly computed BMI_centered
variable in place.

ANALYSIS SETUP: SIMPLE LINEAR REGRESSION USING BMI_CENTERED AS THE PREDICTOR
185
FIGURE 25.5 Descriptive statistics for BMI and BMI_centered.
FIGURE 25.6
The main dialog window of Linear
Regression.
25.6
ANALYSIS SETUP: SIMPLE LINEAR REGRESSION USING
BMI AS THE PREDICTOR
From the main menu, we select Analyze ➔Regression ➔Linear. This opens the main
Linear Regression window as shown in Figure 25.6. We move pulse_rate into the
Dependent panel and BMI into the Independent(s) panel.
In the Statistics window (see Figure 25.7) we check Estimates, Model ﬁt, R
squared change, Descriptives, and Part and partial correlations. Click Continue to
return to the main dialog window and click OK to perform the analysis.
25.7
ANALYSIS SETUP: SIMPLE LINEAR REGRESSION USING
BMI_CENTERED AS THE PREDICTOR
We set up the analysis exactly as described earlier but with one exception. In this analysis,
we use BMI_centered as the independent variable.

186
CENTERING THE PREDICTOR VARIABLE IN SIMPLE LINEAR REGRESSION
FIGURE 25.7
The Statistics dialog window of Linear Regression.
25.8
ANALYSIS OUTPUT FROM BOTH REGRESSION ANALYSES
The descriptive statistics (means and standard deviations) for the dependent and inde-
pendent variables in the regression analysis are shown in Figure 25.8, with the top table
representing the analysis using BMI as the predictor and the bottom table representing the
analysis using BMI_centered as the predictor. This matches the output of our previous
descriptive statistics analysis.
Figure 25.9 shows the correlation between pulse_rate and BMI (top table) and
between pulse_rate and BMI_centered (bottom table). Note that centering did not affect
the value of the Pearson correlation coefﬁcient, which was .866 in both analyses. Pearson
correlation represents the degree to which two variables covary, and because the vari-
ability of BMI did not change when we transformed it to BMI_centered, pulse_rate
covaries to the same degree with both of them.
The Model Summary tables are presented in Figure 25.10. Again, the effectiveness
of model is identical using the original (top table) and centered (bottom table) predictor
variables. The R2 is .749 and the adjusted R2 is .743. It therefore appears that body mass
index, whether centered (BMI_centered) or not (BMI), accounted for approximately
three-quarters of the variance of pulse_rate.
FIGURE 25.8
Descriptive statistics.

ANALYSIS OUTPUT FROM BOTH REGRESSION ANALYSES
187
FIGURE 25.9
The Pearson correlation of the
variables.
FIGURE 25.10
Model Summary table.
The results of ANOVA, testing the statistical signiﬁcance of the regression model are
shown in Figure 25.11. Still again, the original BMI variable and its centered counterpart
(BMI_centered) produced identical results. The regression model was associated with
an F ratio of 113.597 and, with 1 and 38 degrees of freedom, was statistically signiﬁcant
(p < .001). The eta square value was 3789.836/5057.600 or .749, the same value shown
earlier for R2.
The detailed results of the analyses containing the regression coefﬁcients are pre-
sented in Figure 25.12. The unstandardized simple regression model with BMI as the
predictor variable is as follows:
pulse _ rate = 30.781 + (2.039)(BMI)

188
CENTERING THE PREDICTOR VARIABLE IN SIMPLE LINEAR REGRESSION
FIGURE 25.11 ANOVA testing of the statistical signiﬁcance of the regression model.
FIGURE 25.12 Regression coefﬁcients table.
The standardized simple regression model is as follows:
pulse _ ratez = (.866)(BMIz)
The unstandardized simple regression model with BMI_centered as the predictor variable
is as follows:
pulse _ rate = 82.400 + (2.039)(BMI)
The standardized simple regression model is as follows:
pulse _ ratez = (.866)(BMIz)
The zero-order, partial, and semipartial correlations are the same in both analyses
because they all represent (in simple linear regression with a single predictor) the
Pearson r. The beta (standardized) regression coefﬁcient is also equal to the Pearson r
and thus takes on the same value in both analyses, and the unstandardized regression

ANALYSIS OUTPUT FROM BOTH REGRESSION ANALYSES
189
coefﬁcient is the same as well in both analyses, that is, 2.039. Thus, the slope of the
regression function is the same regardless of whether BMI or BMI_centered is used as
the predictor. This makes sense because the general nature of the relationship between
heart rate and body mass index is not affected by the centering operation. We may then
say (based on the unstandardized model) that every unit gain in body mass index is
associated on average with approximately 2 more heartbeats per minute.
The only value in these analyses that has been changed by the centering operation
is the Y intercept (the constant in the raw score regression model). Using the original
variable of BMI, a body mass index of zero corresponds to an expected heart rate of
30.781, values that have no real-world meaning (it is likely that someone with a body
mass index of zero, should that impossibility ever occur, would not likely have a pulse
at all). However, the Y intercept for the centered model is 82.40, not coincidentally the
mean of the pulse_rate variable. Translating that value into an empirical context simply
requires us to recall that BMI_centered is a mean centered variable; thus, a value of
zero on this predictor represents the mean of BMI (which we know from Figure 25.5
is 25.31). We thus immediately learn from this outcome that patients with a body mass
index of 25.31 are expected to have on average a heart rate of 82.40. It is this immediate
interpretation of the meaning of the Y intercept that is gained from the centering operation
(with no change in any of the other regression results) that makes centering so useful in
situations that warrant its use.


C H A P T E R
2 6
Multiple Linear Regression
26.1
OVERVIEW
Multiple linear regression is an extension of simple linear regression in that there are two
or more predictors that are included in the model. The raw score model thus contains
multiple bX terms, one for each predictor, and the standardized model contains multiple
beta Xz terms. The linear function is ﬁt by using the least-squares algorithm, and the
weights associated with the predictors are those that maximize the prediction of Y.
The most common way of performing a multiple regression analysis is using the stan-
dard (sometimes called the simultaneous) method in which all variables are entered into
the model in a single (albeit complicated) step. Each predictor is evaluated with all other
variables presumed to be in the model; thus, the other predictors act as covariates with
respect to the predictor that is being evaluated. The weights are known as partial regres-
sion coefﬁcients because they are computed with respect to the other predictors in the
model, and so even adding or subtracting a single variable from the set of predictors can
potentially change the value of the partial regression coefﬁcients by a substantial margin.
In some contexts, researchers may have reason to simplify a multiple regression
model by selecting only the “best” predictors, that is, only those predictors that are sig-
niﬁcantly predictive of the criterion variable when controlling for all the other predictors.
For example, certain predictors may be very resource intensive to use, may be uncom-
fortably intrusive, or in combination with the other predictors make less than optimal
theoretical sense. The idea of using a reduced predictor set is to perform virtually the
same amount of predictive work explaining the variance of the dependent variable as the
full set of predictor variables, but the outcome must have pragmatic or theoretical utility
for researchers to justify using the resulting model.
The Linear Regression procedure includes a set of step methods (forward, backward,
and stepwise) each used to generate a single parsimonious model. These methods retain
only those predictors of the set that are statistically signiﬁcant rather than including the
entire set in the model. Brieﬂy, the step methods are as follows:
• The forward method adds variables to the model one variable (step) at a time.
To be entered, a predictor must account for a statistically signiﬁcant amount of
dependent variable variance not already explained by other variables (the residual
variance) that are in the model. Once in the model, it remains in, even if its
predictive power is diminished in the presence of predictors entered later. The
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
191

192
MULTIPLE LINEAR REGRESSION
process ends when no remaining predictors can explain a signiﬁcant amount of
the residual variance.
• The backward method enters all of the variables at ﬁrst (just as in the standard
method), but then removes variables one (step) at a time. When all nonsigniﬁcant
predictors are removed from the model, the process stops.
• The stepwise method adds predictor variables to the model one step at a time. To be
entered, a predictor must account for a statistically signiﬁcant amount of dependent
variable variance not already explained by other variables (the residual variance)
that are in the model. Upon entering the third and all subsequent predictors, the
stepwise method removes any variable that no longer explains a signiﬁcant amount
of dependent variable variance in the presence of the other predictors. Removed
variables can ﬁnd their way back into the model in the later steps. The process
ends when no remaining predictors can explain a signiﬁcant amount of the residual
variance. Of the three step methods, this is the most frequently used.
Despite their appropriate use in certain limited situations, these step methods have
lost favor over the last quarter century as we increasingly became aware of their weak-
nesses, including applying the incorrect degrees of freedom and thus increasing the
chances of committing more Type I errors than the nominal alpha level would suggest,
capitalizing on sampling error and thus resulting in poor generalizability, and often failing
to identify the “best” set of predictors producing the largest explained variance (Cohen,
Cohen, West, & Aiken, 2003; Meyers et al., 2013; Tabachnick & Fidell, 2007).
An alternative to the traditional step methods was introduced in IBM SPSS® version
19, the Automatic Linear Modeling procedure within the Regression module. This
procedure performs an all-possible-subsets analysis. The all-possible-subsets method is
considered to be more sophisticated and credible than the step methods (e.g., Huberty,
1989; Thompson, 1995, 2006). While the step methods construct a single model by
adding or subtracting one predictor at a time, the all-possible-subsets method, as the
name implies, calculates all the possible combinations of the predictors and presents
competing models for researchers to consider.
Even with a small set of predictors, there are many possible subsets of predictors.
The number of possible combinations (i.e., all-possible-subsets) is (2p) −1, where p is the
number of predictors (we subtract 1 to eliminate the zero variable subset). For example,
if there are ﬁve predictors, the number of all possible subsets is equal to (25) −1 or
31. However, the output will be limited to the top 10 models, and, if the solution is
relatively straightforward, the best model will often match the one produced by the
stepwise method. As was true for the step procedures, researchers using this approach
need to exercise judgment in selecting the most appropriate model for their needs based
on both theoretical and pragmatic concerns.
26.2
NUMERICAL EXAMPLE
We use the data ﬁle named Exercise that was used in Chapter 24 in discussing simple
linear regression. For the present example, we will use all ﬁve of the variables (except
CaseID) to predict exercise_commitment.
26.3
ANALYSIS STRATEGY
We perform three analyses each using a different strategy of building the regression
model: the standard method, the stepwise method, and the all-possible-subsets method.
The ﬁrst two will be performed in the Linear Regression procedure and the third will
be performed in the Automatic Linear Modeling procedure.

ANALYSIS SETUP: STANDARD METHOD
193
26.4
ANALYSIS SETUP: STANDARD METHOD
From the main menu we select Analyze ➔Regression ➔Linear. This opens the main
Linear Regression window as shown in Figure 26.1. We move exercise_commitment
into the Dependent panel and all of the other variables into the Independent(s) panel.
We retain Enter in the Method drop-down menu, as this will enter all of the variables
on a single step as called for by the standard method.
Selecting the Statistics pushbutton opens the Statistics screen (see Figure 26.2).
We check Estimates (to obtain the regression coefﬁcients), Model ﬁt (to obtain the R2
and adjusted R2), R squared change (to show this output for illustration purposes),
Descriptives (to obtain the descriptive statistics), and Part and partial correlations (to
FIGURE 26.1
The main Linear Regression dialog
window.
FIGURE 26.2
The Statistics screen of Linear Regression.

194
MULTIPLE LINEAR REGRESSION
obtain the zero-order, partial, and semipartial correlations). Click Continue to return to
the main dialog window and click OK to perform the analysis.
26.5
ANALYSIS OUTPUT: STANDARD METHOD
The descriptive statistics (means and standard deviations) for the six variables (ﬁve
predictors and one dependent variable) and the square correlation matrix are shown in
Figure 26.3. The six variables are assessed on quite different scales, even though all
are quantitatively measured. The Pearson correlations are shown in the ﬁrst major row
of the Correlations table, and their corresponding probability levels are shown in the
second major row. Diet_Intensity and bodyesteem are more highly correlated with the
dependent variable of exercise_commitment than the other variables (.464 and .409,
respectively). Within the set of predictors, selfesteem and bodyesteem are more highly
correlated than the other pairs.
Figure 26.4 shows the result of testing the ﬁt of the model. In the Model Summary
table, we see that the multiple correlation (R) is .591, with a corresponding value of
R Square of .350, suggesting that 35% of the variance of exercise_commitment is
explained by the set of predictors. R Square Change is also .350, as we went from zero
FIGURE 26.3 Descriptive statistics and the correlation matrix.

ANALYSIS OUTPUT: STANDARD METHOD
195
FIGURE 26.4 Testing the ﬁt of the model.
before the model was generated to the full value because all predictors were entered into
the model in the ﬁrst step. The Adjusted R Square value is .342, and represents some
R2 shrinkage as a result of including ﬁve predictors in the model.
The ANOVA table in Figure 26.4 provides a test of the statistical signiﬁcance of the
regression model using a one-way between-subjects ANOVA. The regression model has
ﬁve degrees of freedom because that is the number of predictors in the model. The total
degrees of freedom are equal to N −1 or 414, leaving 409 degrees of freedom for the
error term. The model accounts for a signiﬁcant amount of dependent variable variance,
F(1, 409) = 43.97, p < .001. The eta square value is equal to Regression variance divided
by Total variance, which yields 110.038/314.732 or .350. Note that this is the same value
as the R2 because the ANOVA and linear regression are just different expressions of the
general linear model.
Figure 26.5 presents the Coefﬁcients table for the variables in regression model. The
three columns on the far right of the table show Correlations:
• The Zero-order correlations are the Pearson r values of the predictors with the
criterion variable. For example, the Pearson r between diet_intensity and exer-
cise_commitment is .464. They are being labeled by IBM SPSS as zero-order
partial correlations (with the term partial correlation omitted) because no covari-
ates are being used in evaluating the strength of those relationships (which thus
reduces to the limiting case of the Pearson r).
FIGURE 26.5 The regression coefﬁcients together with the correlations.

196
MULTIPLE LINEAR REGRESSION
• The Partial correlations are the correlations between each predictor and the resid-
ual variance of the dependent variable when statistically controlling for all of the
other predictors. For example, diet_intensity is correlated .454 with that portion
of exercise_commitment not explained by the other predictors (these other pre-
dictors are being used as covariates in assessing that relationship). These partial
correlations are fourth-order partial correlations in that the other four predictors
act as covariates when the statistic for each predictor is being computed.
• The Part correlations are the semipartial correlations representing the unique asso-
ciation between each predictor and the dependent variable. We typically square
these values for interpretation purposes. For example, the squared semipartial
correlation between diet_intensity and exercise_commitment is .4112 or approx-
imately .17. We can therefore say that controlling for all of the other weighted
predictors in the regression model, diet_intensity uniquely accounts for approxi-
mately 17% of the variance of exercise_commitment (note that some additional
variance of exercise_commitment associated with diet_intensity is also associ-
ated with some other predictor(s) and thus does not count toward the value of the
squared semipartial correlation of diet_intensity).
The columns on the left of the Coefﬁcients table in Figure 26.5 present information
concerning the regression coefﬁcients. It can be recalled from Chapter 24 that the raw
score (unstandardized) regression coefﬁcient for selfesteem is .034 and is statistically
signiﬁcant as the single predictor of exercise_commitment. In the companionship of
the other four predictors, however, selfesteem’s partial regression coefﬁcient is .008, a
weight not signiﬁcantly different from zero (p = .226). This is a good illustration of the
relativistic nature of the values of regression coefﬁcients. The weights of the predictors
in the model are determined to be those that, when joined in combination with the other
predictors, maximally predict the dependent variable; these weights often do not reﬂect
the predictive capability of each variable in isolation.
Only two of the predictor variables are statistically signiﬁcant in this model, diet_in-
tensity and bodyesteem, the two that, coincidentally, had the highest correlations with
exercise_commitment; this can also be seen in the relatively higher values of the unstan-
dardized and standardized (beta) coefﬁcients associated with these variables. We can thus
say the following:
• When controlling for the other predictors in the model, an increase of one unit
of diet_intensity is expected to be associated with a .495 unit gain in exercise_
commitment.
• When controlling for the other predictors in the model, an increase of one unit
of bodyesteem is expected to be associated with a .015 unit gain in exercise_
commitment.
We can also glean a sense of the dynamics underlying the weights associated with
these variables, two aspects of which are as follows:
• We noted that selfesteem and bodyesteem were relatively highly correlated, yet
bodyesteem was a signiﬁcant predictor in the model and selfesteem was not. It is
likely that much of the prediction work done by selfesteem was redundant, with
the predictive work done by bodyesteem, and only the latter was thus given credit
for that work.
• The semipartial correlations indicate that diet_intensity and bodyesteem were
able to contribute much more unique explanatory power than the other variables.
Based on the squared values, diet_intensity and bodyesteem uniquely explained

ANALYSIS SETUP: STEPWISE METHOD
197
about 17% (.4112 = .1689) and 8% (.2862 = .0818), respectively, of the variance
of exercise_commitment.
One statistic that is not provided in the IBM SPSS output is the structure coefﬁ-
cient for each predictor. This statistic is the correlation between the individual predictor
and the predictor variate (the weighted linear composite of the predictors) and can be
used, when the set of predictor variables warrant it, to help interpret the underlying latent
construct represented by the predictor model (e.g., a weighted linear composite of person-
ality variables is likely to represent some meaningful underlying dimension). Thompson
has particularly emphasized the importance of this statistic in regression (Courville &
Thompson, 2001; Thompson & Borrello, 1985), but it is commonly used to interpret other
latent constructs in, for example, principal components/factor analysis and discriminant
function analysis.
The structure coefﬁcients in regression can be computed by dividing the Pearson r
values for each predictor by the multiple correlation (R), which is .591 in our example.
Thus, the structure coefﬁcients for diet_intensity, social_afﬁliation_needs, selfaccep-
tance, selfesteem, and bodyesteem are .785, .299, .305, .443, and .692, respectively.
In this situation, the two statistically signiﬁcant predictors also had relatively substantial
structure coefﬁcients (and thus drive the interpretation). The latent construct underlying
the model thus appears to be valuing a trim and attractive body, a desire that appears to
drive a commitment to exercise.
26.6
ANALYSIS SETUP: STEPWISE METHOD
We will brieﬂy present the setup and output for the stepwise method, highlighting only
the differences between this and the standard method. In the main Linear Regression
window, shown in Figure 26.6, we move exercise_commitment into the Dependent
panel and all of the other variables into the Independent(s) panel and select Stepwise
from the Method drop-down menu.
FIGURE 26.6
The main Linear Regression dialog
window.

198
MULTIPLE LINEAR REGRESSION
FIGURE 26.7
The Options screen of Linear Regression.
We conﬁgure the Statistics screen as we did for the standard regression analysis,
but we open the Options screen (see Figure 26.7) just to show the Stepping Method
Criteria. We retain the default Entry and Removal criteria of .05 and .10, respectively
(having this sort of a difference between them prevents the analysis from getting caught
in an inﬁnite loop of entry and removals). Click Continue to return to the main dialog
window and click OK to perform the analysis.
26.7
ANALYSIS OUTPUT: STEPWISE METHOD
The Model Summary table is shown in Figure 26.8. There are two models, with the
second built on the ﬁrst. The variables contained in each model are shown in the footnotes
FIGURE 26.8 Testing the ﬁt of the model.

ANALYSIS OUTPUT: STEPWISE METHOD
199
to the Model Summary table. The ﬁrst model contained only diet_intensity as the best
predictor of exercise_commitment; the second model added bodyesteem. Because there
were no additional models, we learn that no additional predictors could signiﬁcantly
improve the R2 based on those two predictors.
The ﬁrst model explained close to 22% of the variance (R2 = .216). The second
model incremented the R2 by .132 (R Square Change) to give a ﬁnal R2 of .347 and an
adjusted R2 of .344, thus explaining about 34% of the variance with just two predictors.
Note that this is about the same percentage of variance explained by the full model
(because it contained three nonsigniﬁcant predictors) and may provide a sense of why
the stepwise method had such appeal. However, if the full set of variables had been
chosen with care based on existent theories and on the research literature, then removing
(not entering) variables from regression models strictly based on statistical decisions
made by the software should be done only for very carefully considered reasons (e.g.,
the need to predict in a limited resource context with as few variables as possible).
The ANOVA results are shown in the bottom table in Figure 26.8. The degrees of
freedom for Regression is a count of the predictors in the model; thus, the ﬁrst model has
one degree of freedom because there is only one predictor variable in it and the second
model has two degrees of freedom because there are two predictor variables in it. Both
models are statistically signiﬁcant, again not a surprise given that this is the stepwise
method (and only statistically signiﬁcant predictors are added to the model).
Figure 26.9 presents the Coefﬁcients of the variables in regression model. The
partial regression coefﬁcients for selfesteem and bodyesteem are very close in value
to what we saw in the standard regression output, and each uniquely accounts for a
FIGURE 26.9 The regression coefﬁcients together with the correlations.

200
MULTIPLE LINEAR REGRESSION
reasonable percentage of the variance of exercise_commitment. Variables excluded from
the analysis are shown in the bottom table.
26.8
ANALYSIS SETUP: AUTOMATIC LINEAR MODELING
From the main menu, we select Analyze ➔Regression ➔Automatic Linear Modeling.
This opens the main Automatic Linear Modeling window as shown in Figure 26.10,
with the default of Use predeﬁned roles activated. We change this to the Use custom
ﬁeld assignments button (see Figure 26.11). This shifts all the variables to the Fields
panel located on the left side of the Automatic Linear Modeling window. We then
move exercise_commitment into the Target panel and move all of the other variables
(except CaseID) to the Predictor (Inputs) panel as shown in Figure 26.12.
Selecting the Build Options tab changes the left panel to Select an item with
Objectives highlighted as shown in Figure 26.13. We retain the default of Create a
standard model (an explanation of the selected objective appears in the Description
panel below the model options).
Selecting the Basics choice in the Select an item panel opens the Basics window
with Automatically prepare data already checked as the default (see Figure 26.14).
FIGURE 26.10 The main Automatic Linear Modeling window.

ANALYSIS SETUP: AUTOMATIC LINEAR MODELING
201
FIGURE 26.11 The main Automatic Linear Modeling window with Use custom ﬁeld assign-
ments selected.
We retain that default, as we have no missing values in our data set and our quantitative
variables have lots of values (see later discussion). The operations that will be performed,
as needed, are as follows:
• Date and Time handling. Variables identiﬁed as a Date variable will be trans-
formed into continuous (scale) variables.
• Adjustment of measurement level. In the Variable View, there are three Mea-
sure designations—Scale for continuous variables, Ordinal for ranked data, and
Nominal for categorical variables. The adjustments made here are (a) if a pre-
dictor Scale variable has less than ﬁve distinct values, it will be changed to an
Ordinal Measure and (b) Ordinal predictors with more than 10 distinct values
are changed to Scale Measures.
• Outlier handling. Continuous (Scale) predictors with values beyond a designated
cutoff value (e.g., three standard deviations from the mean) are converted to the
cutoff value. Researchers should be sure that they want to handle outliers in this
manner, rather than using an alternative method (e.g., treat them as missing),
before accepting Automatically prepare data. For our data set, we accept the
cutoff replacement procedure.

202
MULTIPLE LINEAR REGRESSION
FIGURE 26.12 The main Automatic Linear Modeling window conﬁgured for our analysis.
• Missing value handling. For predictor variables that are Nominal, Ordinal, and
Scale, missing entries are replaced with the variable’s mode, median, and mean,
respectively. However, not only can data be missing in different patterns (not all of
which are completely at random), there are a variety of strategies for dealing with
missing data, with one of the least preferred methods being mean substitution
(e.g., Enders, 2010; Graham, 2009; McKnight, McKnight, Sidani, & Figuerdo,
2007; Meyers et al., 2013). Inserting the mean in place of missing scale values
reduces variability; this in turn inappropriately shrinks the standard error; lower
standard errors in turn increase the frequency of committing Type I errors. If there
are missing data in the data set, researchers need to determine if they do or do not
wish to accept Automatically prepare data with its mean substitution strategy.
As an alternative approach, researchers can ﬁrst perform one of the more preferred
strategies for handling missing data (assuming it is justiﬁed based on the missing
values patterns) and then, with no missing data issues, accept the Automatically
prepare data option, as mean substitution would not be invoked.
• Supervised merging. Categorical variables (Nominal) that have equivalent asso-
ciations on the Target (dependent) variable are merged to produce a more parsi-
monious model.

ANALYSIS SETUP: AUTOMATIC LINEAR MODELING
203
FIGURE 26.13 The Build Options window of Automatic Linear Modeling.
• Conﬁdence level. Computes the conﬁdence intervals of the coefﬁcients in the
models. The default is a 95% conﬁdence interval, and we retain that.
Highlight the Model Selection choice in the Select an item panel. As shown in
Figure 26.15, select from the drop-down menu Best subsets to replace the default of
Forward stepwise in the Model selection method panel. In the Best Subsets Selection
area, at the bottom of the screen, in the Criteria for entry/removal panel, we accept
the default of Information Criterion (AICC); this choice identiﬁes the subsets of the
predictors producing the “best” models and is useful in comparing competing models.
There are two other choices on the drop-down menu for Criteria for entry/removal.
Brieﬂy, the Adjusted R-squared criterion identiﬁes the “best” models based on the
largest Adjusted R-squared. The Overﬁt Prevention Criterion (ASE) is based on the
ﬁt (average squared error) of the overﬁt prevention set (a random subsample of ∼30%
of the original dataset).
We would move to the Ensembles choice only if we had selected one of the other
options in the Objectives window and we have no need to deal with the Advanced
choice. The Model Options tab allows us to save the predicted values to the data set and
export the model as a .zip ﬁle. As we do not intend to make use of either of these options,
we are ﬁnished conﬁguring the analysis and click the Run pushbutton to perform the
analysis.

204
MULTIPLE LINEAR REGRESSION
FIGURE 26.14 The Basics window of Automatic Linear Modeling.
26.9
ANALYSIS OUTPUT: AUTOMATIC LINEAR MODELING
The output opens in the Model Viewer window by displaying the Model Summary and
the Accuracy bar graph as shown in Figure 26.16. This format is different from most of
the other outputs we cover in this book but is more visually oriented. The left vertical
panel with the thumbnails is our access to different portions of the output. There is no
thumbnail for the Model Summary output, as it is literally an overview of the analysis
setup.
The ﬁrst thumbnail showing a horizontal bar graph is highlighted as a default. This
is the Accuracy bar graph and it is already displayed below the Model Summary in the
opening window. It is a visual depiction of the amount of variance explained by the best
ﬁtting model. Double-clicking the bar graph activates it, and moving the cursor across
the bar reveals this information: Adjusted R square = .344 (see Figure 26.17). This
number also appears to the left of the bar graph in the opening window but is unlabeled.
Also not displayed in the opening window is the model (recall that there are 31 possible
models) to which the adjusted R2 value applies.
Activating the Accuracy bar graph also activates the vertical list of thumbnails in the
leftmost panel. A scrollbar has been added to the panel (this can be seen in Figure 26.17),

ANALYSIS OUTPUT: AUTOMATIC LINEAR MODELING
205
FIGURE 26.15 The Model Selection window of Automatic Linear Modeling.
FIGURE 26.16
The Model Summary output of Automatic Linear
Modeling.

206
MULTIPLE LINEAR REGRESSION
FIGURE 26.17 The Accuracy output of Automatic Linear Modeling.
as there are more thumbnails than that can be viewed at once in the window, and placing
the cursor over a thumbnail provides a label for the output it represents.
The second thumbnail from the top, shown in Figure 26.18, reports the actions of
the Automatic Data Preparation function. In our example, Trim outliers was the only
transformation needed for all the Predictors. Scores greater than the absolute value of
a z score of 3.0 were replaced with 3.0 because we indicated in the analysis setup that
three standard deviations distance from the mean was the designated cutoff value.
We recommend to next select the next-to-last thumbnail (it resembles a little grid).
This is the Model Building Summary and gives us the needed overview of the results.
We show this output in Figure 26.19. Although there were 31 possible models to evaluate,
IBM SPSS thankfully shows us only the top 10 models (a set that is typically much larger
than we need to see).
All of the available predictor variables are listed in the ﬁrst column; they are named
as being transformed in that the evaluations of ﬁt are made on the standardized model
to place all variables on the same (z score) metric. Each column under Model is one of
the top 10 best ﬁtting models ordered by its value on the Information Criterion. Lower
(more negative) values indicate a better ﬁt.
The top-rated model (Model 1) has an Information Criterion value of −285.742.
The variables included as predictors in that model are checked (diet_intensity and

ANALYSIS OUTPUT: AUTOMATIC LINEAR MODELING
207
FIGURE 26.18
The Automatic Data Preparation output of Automatic Linear
Modeling.
FIGURE 26.19 The Model Building Summary output of Automatic Linear Modeling.
bodyesteem); this is the same model generated by the stepwise procedure. It is marked
by a rectangle to inform us that the numerical results in the other parts of the output are
keyed to this model. Thus, the adjusted R2 value seen in the Accuracy bar graph applies
to this model. Other models, with different combinations of predictors, are shown in the
Model Building Summary.
The Effects icon (a circle with two lines—because there are only two predictors
in the model—pointing to clock positions 7:00 and 11:00) and its window are shown
in Figure 26.20. By selecting Table from the drop-down Style menu at the lower left
portion of the window to get out of Diagram mode, we see the more familiar ANOVA
summary table shown in Figure 26.21. These are the same values as those produced by
the ﬁnal stepwise model.
The Coefﬁcients icon (a circle with three lines pointing to clock positions 7:00, 9:00,
and 11:00) is just below the Effects icon. Its Table mode is shown in Figure 26.22. The
unstandardized partial regression coefﬁcients are presented for each predictor together
with a test of the statistical signiﬁcance of each; these coefﬁcients are also the same as
those obtained in the stepwise solution. Both predictors contribute statistically signiﬁcant
amounts of prediction to the model.

208
MULTIPLE LINEAR REGRESSION
FIGURE 26.20 The Effects output of Automatic Linear Modeling in Diagram mode.
FIGURE 26.21
The Effects output of Automatic Linear Modeling in
Table mode.

ANALYSIS OUTPUT: AUTOMATIC LINEAR MODELING
209
FIGURE 26.22
The Coefﬁcients output of Automatic Linear Modeling in
Table mode.
Also reported in the Coefﬁcients table is a value called Importance. As described
in their historical overview, Johnson and LeBreton, (2004) suggest that researchers have
been engaged for more than three-quarters of a century in the search for ways to char-
acterize the relative importance of each predictor in a regression model. From the older
and simpler standbys, such as the values of the regression coefﬁcients, magnitudes of
the squared semipartial correlations, and change in R2, alternative indexes have emerged,
such as usefulness (Darlington, 1968), dominance analysis (Budescu, 1996), and relative
weight analysis (Johnson, 2000; Tonidandel & LeBreton, 2011).
The strategy adopted by IBM SPSS uses the sum of squares for the residual (the
error variance in the ANOVA table) as a signal regarding importance. To the extent
that a predictor is important in the model, leaving it out of the model should produce a
substantial increase in the residual sum of squares; to the extent that a predictor is not
important in the model, leaving it out of the model should produce a minor increase in
the residual sum of squares. Brieﬂy, in implementing this strategy, IBM SPSS computes
a series of models excluding each predictor in each successive model, records the sum
of squares associated with the residuals for each model, adds 1/p to each residual where
p is the total number of predictors, and then determines the ratio of each subtotal to the
grand total (these ratios will sum to 1.00).
The implementation described previously results in normalized predictor importance
values and appears to be what is reported in the Automatic Linear Modeling procedure
as the relative Importance of each predictor. Because the residual sums of squares
can be generated from the squared semipartial correlations, this method is related to
Darlington’s (1968) usefulness index (we are grateful to one of our colleagues, Professor
Greg Hurtz of the California State University, Sacramento, for his instruction on this
relative importance topic).
The last column of the Coefﬁcients table reports the results of this relative impor-
tance analysis: the relative importance of diet_intensity is .515 and the relative impor-
tance of bodyesteem is .485. Thus, in the top-rated model, diet_intensity was somewhat
more important a predictor than bodyesteem in predicting exercise_commitment.
Having a description of the model that was determined to best ﬁt the data, we can
view the analysis of the residuals. Residuals in this context are the differences between
the predicted and observed values of Y, and they are typically in a standardized form to
overcome differences in the metric of the different predictors. One of the assumptions
underlying regression is that the residuals are normally distributed. The Residuals icon is
a histogram, and clicking it gives us a histogram—a frequency distribution of standard-
ized residuals (see Figure 26.23). The normal curve is also superimposed on the histogram
so that we can visually determine how closely the histogram approximates it. In our

210
MULTIPLE LINEAR REGRESSION
FIGURE 26.23 The Residuals output of Automatic Linear Modeling in Diagram mode.
FIGURE 26.24 The Residuals output of Automatic Linear Modeling in P-P Plot mode.
example, the residuals are clearly normally distributed indicating no assumption violation.
Select the P-P Plot from the drop-down Style menu at the lower left portion of the
Residuals window to get out of Histogram mode and to obtain a P-P Plot. P stands
for “probability” in the name of the type of plot, and we are presented with the view of
the plot of the Expected Cumulative Probability on the Y-axis against the Observed
Cumulative Probability on the X-axis, shown in Figure 26.24, as another way to view
the information contained in the histogram. The diagonal line in the plot is a reference
line indicating how the data points should fall if the residuals were normally distributed;
the example plot, which conveys the same information as the histogram, conﬁrms that
the residuals are normally distributed.

C H A P T E R
2 7
Hierarchical Linear Regression
27.1
OVERVIEW
Hierarchical linear regression is an extension of standard multiple linear regression, with
a conceptual element resembling the step procedures. The key factor in hierarchical
regression is that, in contrast to the step regression procedures where the researchers
leave all decisions about entry to the software, the researchers play an active role in
structuring the analysis within the hierarchical strategy. In return for such an investment,
researchers are able to statistically control for the effect of predictors when it makes
theoretical, empirical, or common sense to do so.
In hierarchical regression, predictors are entered in order in subsets or blocks. A
subset can be composed of just a single variable or several variables. Each block of
variables can be controlled by any method available (simultaneously as deﬁned by the
standard method, stepwise, and so on). Because some variables are given “primacy” over
others in order of entry, the procedure is referred to as hierarchical linear regression.
The primary advantage of using such a blocking or hierarchical strategy is that
variables entered in earlier blocks serve as covariates for those entered later. For example,
we might wish to ask respondents to participate in a survey about their commitment to
an exercise schedule. But because it is a “socially correct” behavior, those respondents
sensitive to social expectations might be inclined to tell us that they are somewhat more
committed to such a schedule than they perhaps might truly be. It may also be the case
that those respondents highly committed to dieting to lose weight might also be inclined
to use exercise in such an effort.
One way to deal with the potential confounding of social desirability in this design
and to take account of the extent to which respondents were also dieting is to use measures
of social desirability and dieting as covariates in the analysis. The dynamics for doing
this in the context of hierarchical regression might be as follows:
• In the ﬁrst block, enter the measure of social desirability. This variable will thus
account for a certain amount of variance of the dependent variable of commitment
to exercise.
• In the second block, enter a measure of dieting intensity. With social desirability
already in the model, this newly entered variable must target the residual variance
of commitment to exercise (whatever social desirability did not explain). Thus,
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
211

212
HIERARCHICAL LINEAR REGRESSION
social desirability acts as a covariate in that it statistically accounts for whatever
variance of the dependent variable it can; the measure of dieting intensity is forced
to do predictive work not already under the auspices of the verbal ability variable.
At the same time, the dieting intensity variable acts as a covariate in the evaluation
of social desirability and thus could affect its regression weight in the model.
• In the third block, enter other personality variables that might explain the variance
of measure of commitment to exercise not explained by either social desirability or
dieting intensity. Each variable is evaluated, with the others acting as covariates.
27.2
NUMERICAL EXAMPLE AND ANALYSIS STRATEGY
We use the data ﬁle named Exercise that was used in Chapter 24, and we will continue
to predict exercise_commitment. However, we will enter the variables in the following
three ordered blocks, with each block using the simultaneous method of entry (assume
that these decisions were based on solid theoretical and empirical reasons):
1. In Block 1, we enter social_afﬁliation_needs to control for social desirability
effects.
2. In Block 2, we enter diet_intensity to control for the effects of diet on exercise.
It will be evaluated, with the effects of social desirability having been statistically
controlled. At the same time, social_afﬁliation_needs will be evaluated (actually,
reevaluated) controlling for the effects of diet_intensity.
3. In Block 3, we enter the remaining esteem/acceptance variables. Each of these
variables will be evaluated, with the effects of all of the other variables having
been statistically controlled.
27.3
ANALYSIS SETUP
From the main menu, we select Analyze ➔Regression ➔Linear. This opens the main
Linear Regression window as shown in Figure 27.1. We move exercise_commitment
into the Dependent panel and social_afﬁliation_needs into the Independent(s) panel.
Note that at the very top of the Independent(s) panel just below the Dependent panel
we see Block 1 of 1. IBM SPSS® will keep track of our blocks in this way and allow
us to toggle back and forth with the Next and Previous pushbuttons if we need to make
any changes before performing the analysis. We retain Enter in the Method drop-down
menu. This completes our speciﬁcation for the ﬁrst block. Click Next to conﬁgure the
second block.
We are now in Block 2 of 2. As seen in Figure 27.2, the dependent variable remains
in its panel but the Independent(s) panel has been emptied in preparation for our speciﬁ-
cation. We move diet_intensity into the Independent(s) panel, retain the Enter method,
and click Next to conﬁgure the third block.
We are now in Block 3 of 3. As shown in Figure 27.3, the dependent variable still
remains in its panel, but the Independent(s) panel has once again been emptied in prepa-
ration for our speciﬁcation. We now move selfacceptance, selfesteem, and bodyesteem
into the Independent(s) panel to complete our set of predictors. We retain the Enter
method as we wish to enter all three variables simultaneously into the model. We ask for
the same output in the Statistics window as in Chapter 24, and click OK in the main
dialog window to perform the analysis.

ANALYSIS SETUP
213
FIGURE 27.1
The ﬁrst block of the hierarchical
regression analysis is conﬁgured.
FIGURE 27.2
The second block of the hierarchical
regression analysis is conﬁgured.

214
HIERARCHICAL LINEAR REGRESSION
FIGURE 27.3
The third block of the hierarchical
regression analysis is conﬁgured.
27.4
ANALYSIS OUTPUT
The relevant output from the hierarchical regression analysis is presented in Figure 27.4.
There are three models in our analysis because IBM SPSS treated each block as a separate
model. Note that the models are cumulative, as we are adding variables in each block;
the ﬁnal model matches exactly the results from the standard multiple regression analysis
and is interpreted in the same manner.
What we gain with the hierarchical analysis is an opportunity to observe the dynamics
of the interplay of these variables. Our focus, in addition to interpreting the last model,
is on the contribution of our covariates and the value-added we obtain when including
more variables to the model; we therefore key on R Square and R Square Change (and
the tests of signiﬁcance) associated with the three blocks. We note the following:
• It appears from the Model Summary that in Block 1, social_afﬁliation_needs
accounted for a statistically signiﬁcant but quite a small proportion (about 3%) of
the variance of exercise_commitment. From the Coefﬁcients table, we see that
this variable was statistically signiﬁcant (p < .001).
• The addition of diet_intensity into the model in Block 2 made a relatively major
improvement in the prediction ability of the model, accounting for an additional
19% of the variance of exercise_commitment. The prediction contribution of
diet_intensity was statistically signiﬁcant (p < .001) controlling for social_afﬁlia-
tion_needs; at the same time, social_afﬁliation_needs became at best a marginal
predictor (p = .071), with diet_intensity in the model.
• Finally, the esteem/acceptance variables added in Block 3 contributed a statis-
tically signiﬁcant amount of prediction over and above the effects of social_
afﬁliation_needs and diet_intensity, adding another almost 13% in explaining

ANALYSIS OUTPUT
215
FIGURE 27.4 Output from the hierarchical regression analysis.
the variance of exercise_commitment. The effect of diet_intensity remained sta-
tistically signiﬁcant (p < .001) but the effect of social_afﬁliation_needs in the full
model became trivial (p = .707).


C H A P T E R
2 8
Polynomial Regression
28.1
OVERVIEW
Most of the statistical procedures we cover in this book assume that the relationship
between variables can be described by a completely linear function; that is, a straight
line (e.g., Y = a + bX) is the best ﬁtting model to represent the relationship between
two (or more) variables. There are situations, however, where the relationship between
variables may be somewhat more complex; we address in this chapter a situation in
which the variables are related in a polynomial manner.
Polynomial expressions involve variables that are raised to some positive whole-
number power. Our ordinary raw score measures represent variables that are raised to
the power of 1 (technically, a ﬁrst-order polynomial); any number raised to the ﬁrst
power has a value equal to that number (e.g., 91 = 9), and so we conventionally omit
the exponent when writing numbers. If we raise a variable to the power of 2 (X2),
then the squared value represents a quadratic expression (a second-order polynomial),
and if we raise a variable to the power of 3 (X3), then we would be talking about a
cubic function (a third-order polynomial); it is quite unusual in the behavioral, medical,
and biological research ﬁelds to deal with a fourth-order (quartic functions), ﬁfth-order
(quintic functions), or higher order polynomials.
Polynomial variables can be entered in a regression equation. For example, we could
envision some variable Y predicted to be a function of the square of X (e.g., Y = a +
bX + bX2). Such a model is an intrinsically linear model (Meyers et al., 2013) because
we add the terms together as required in a linear function but one (or more) of the terms
that we include in addition is not itself linear (e.g., it is quadratic). If we were to plot
such a function, the result would not be a straight line. For example, quadratic functions
have one “bend” or curvature and have the possibility of one inﬂection or turning point
in them; an idealized depiction of a quadratic function would be a U-shaped or inverted
U-shaped plot. A cubic function has an additional curvature and the possibility of having
up to two inﬂection points; an idealized depiction of a cubic function would be an
N-shaped or inverted N-shaped plot.
Polynomial functions sometimes very dramatically apply to the lives of human
beings. One well-known example of a polynomial function applied to daily life is the
relationship between the quality of performance and the level of stress, anxiety, or gen-
eral arousal that we experience as we perform some task. The general principle is that
increased arousal up to some point facilitates task performance but hinders performance
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
217

218
POLYNOMIAL REGRESSION
beyond that point (i.e., performance is an inverted U-shaped function of arousal). This
is generally known as the Yerkes–Dodson law, so named because Yerkes and Dodson
(1908) ﬁrst introduced it over a century ago when the two researchers attempted to teach
mice to select the lighter chamber and avoid the darker one on penalty of electric shock.
Over the more than hundred years since its introduction, this inverted U-shaped function
of performance and arousal or stress has become part of our common knowledge.
Polynomial regression analysis uses as predictors quadratic and (potentially) cubic or
higher polynomials in addition to the linear variable on which the polynomial predictors
are based. It is ordinarily performed as a hierarchical analysis, with the linear term entered
in the ﬁrst block, the quadratic term added to the model in the second block, the cubic
term added to the model in the third block, and so on. The polynomial function (model)
that best ﬁts the data, given the set of predictors, is based on the least-squares algorithm.
We evaluate the amount of R2 change (and its statistical signiﬁcance) associated with
each additional polynomial element to determine the viability of each model and to select
the model with the best combination of parsimony and utility.
28.2
NUMERICAL EXAMPLE
The data for our ﬁctional study of the Yerkes–Dodson law is present in the ﬁle named
performance and stress. College students were given a set of 20 puzzles that needed
to be solved. The number of puzzles correctly solved is provided under the variable of
performance. Test-taking conditions were such that the students experienced different
levels of stress; the amount of stress they were experiencing is recorded under the variable
of stress_level.
28.3
ANALYSIS STRATEGY
Our goal is to predict performance from stress_level and, given what we know of the
Yerkes–Dodson law, we would expect an inverted U-shaped function to relate these two
variables (assuming a sufﬁciently wide range of the level of stress experienced by the
participants and that the puzzles were of a moderate difﬁculty level). Thus, we anticipate
the need to include at least a quadratic component in our regression model. Our analysis
strategy follows this sequence:
• We obtain a scatterplot to visually evaluate the relationship between the two vari-
ables, expecting to see an inverted U-shaped array.
• We Compute any polynomial predictor variables (e.g., squared stress_level, cubic
stress_level) judged to be needed based on our visual examination of the scatter-
plot.
• We use the Linear Regression procedure to perform a hierarchical regression anal-
ysis using the linear variable (original stress_level) and the polynomial variables
in a sequence of models.
28.4
OBTAINING THE SCATTERPLOT
We open the data ﬁle named performance and stress and from the main menu select
Graphs ➔Legacy Dialogs ➔Scatter/Dot. As described in Chapter 22, this opens the
Scatter/Dot window shown in Figure 28.1. Select Simple Scatter and click Deﬁne to
open the Simple Scatter dialog screen shown in Figure 28.2. We place performance

OBTAINING THE SCATTERPLOT
219
FIGURE 28.1
Scatter/Dot selection screen.
FIGURE 28.2
The Simple Scatter dialog screen.
in the Y Axis panel and stress_level in the X Axis panel. Clicking OK produces the
scatterplot.
The scatterplot of performance and stress_level is presented in Figure 28.3. Visual
inspection suggests that the two variables are related in a mostly quadratic manner. A
straight line’s (a linear function) ﬁt to the data points (coordinates) would be roughly
parallel to the X-axis; thus, there appears to be no viable linear component to the rela-
tionship. Because the scatterplot is not perfectly symmetric (it “sticks out” a little toward
the right), suggesting a somewhat different rate of curvature in that part of the data array,
there is the possibility that there might be a weak cubic component to the relationship.
We will therefore use three predictors in our hierarchical analysis: the linear variable of

220
POLYNOMIAL REGRESSION
FIGURE 28.3
Scatterplot of test performance as a function of
level of stress.
stress_level, a squared stress_level variable, and a cubic stress_level variable. To use
them in the Linear Regression procedure, we will need to compute these latter two
variables.
28.5
COMPUTING THE POLYNOMIAL VARIABLES
From the main menu select Transform ➔Compute Variable. This opens the Compute
Variable dialog window shown in Figure 28.4. We will compute the squared value of
stress_level ﬁrst. In the Target Variable panel, we name the variable stress_square. We
now move stress_level into the Numeric Expression panel, click the double-asterisk
key (ﬁrst key in the last row on the key panel below the Numeric Expression panel) to
specify that we are computing an exponent or power function, and type (or select from
the key panel) the number 2 (this will accomplish the squaring). We click OK to perform
the computation and obtain the new variable stress_square at the end of the data ﬁle. We
then repeat the process to compute our cubic variable that we have named stress_cube
(be sure to type a 3 after the double asterisk). The results of these computations are
shown in the screenshot of a portion of the data ﬁle in Figure 28.5.
28.6
ANALYSIS SETUP: LINEAR REGRESSION
From the main menu, we select Analyze ➔Regression ➔Linear to open the main
Linear Regression window. We will perform a hierarchical regression analysis as
described in Chapter 27. Brieﬂy, we move performance into the Dependent panel
and stress_level into the Independent(s) panel as Block 1 of 1. Click Next and move
stress_square into the Independent(s) panel for the second block. Click Next and
move stress_cube into the Independent(s) panel for the third block. The setup for the
third block is shown in Figure 28.6. In the Statistics window, we ask for Regression
Coefﬁcients Estimates, Model ﬁt, R squared change, Descriptives, and Part and
partial correlations (see Figure 28.7). Click Continue to return to the main dialog
window and click OK to perform the analysis.

ANALYSIS SETUP: LINEAR REGRESSION
221
FIGURE 28.4 The Compute window setup to compute the squared value of stress_level.
FIGURE 28.5 A portion of the data ﬁle showing the newly computed polynomial variables.

222
POLYNOMIAL REGRESSION
FIGURE 28.6
The main dialog window of Linear
Regression in the third block of the
hierarchical analysis.
FIGURE 28.7
The Statistics dialog window of Linear Regression.
28.7
ANALYSIS OUTPUT: LINEAR REGRESSION
The Model Summary output from the hierarchical regression analysis is presented in
Figure 28.8. There are three models in our analysis, in that each block is treated as a
separate model, although the models are cumulative as we are adding variables in each
block. The ﬁrst model contains only the linear stress_level variable and is not statistically
signiﬁcant (p = .625); this is consistent with our expectations, given the scatterplot. The
R2 of .002 tells a vivid story.

ANALYSIS OUTPUT: LINEAR REGRESSION
223
FIGURE 28.8 The Model Summary output of the Linear Regression analysis.
FIGURE 28.9
ANOVA table for the Linear Regression analysis.
When the quadratic term (stress_square) was added to the model, the R2 changed
by .506 to reach a value of .508. This change in R2 was statistically signiﬁcant (p < .001).
The third model added the cubic term (stress_cube) to the model, and the R2 change
of .005 to boost the R2 to .512 was not statistically signiﬁcant (p = .260). Thus, the
relationship between performance and stress appears to be best described by a quadratic
function.
Figure 28.9 presents the ANOVA summary table for each model. Note that the model
with the cubic term is still statistically signiﬁcant, even though the cubic term added no
signiﬁcant enhancement to the amount of variance that was explained—it is statistically
signiﬁcant because the quadratic variable, which is doing all of the predictive work, is
contained in the third model.
The Coefﬁcients output is shown in Figure 28.10. Two points are worth noting here:
• The regression coefﬁcient for the linear term is statistically signiﬁcant in the second
model, even though there is no linear relationship between the two variables. This
is because the best ﬁtting curve is one that weights both the linear and quadratic
terms, with coefﬁcients whose values are signiﬁcantly different from zero so that
the quadratic function can optimally ﬁt the data.
• The beta coefﬁcients in the second and third models are well in excess of 1.00.
This is because the linear, quadratic, and cubic variables are, not surprisingly,

224
POLYNOMIAL REGRESSION
FIGURE 28.10
Coefﬁcients table for the Linear Regression analysis.
very highly correlated with each other (e.g., stress_level and stress_square are
correlated .983); standardized regression coefﬁcients can easily exceed 1.00 when
the predictors are relatively strongly correlated. This multicollinearity should likely
cause researchers to abandon analyses containing ordinary variables (they would
exclude one or more of the predictors from the analysis and then perform the
analysis again), but we accept this (disconcerting) feature in a polynomial analysis
by not interpreting the regression coefﬁcients as we usually do but instead we
focus on the overall nature of the relationship (e.g., the obtained relationship here
was a quadratic one) and the R2 information.

C H A P T E R
2 9
Multilevel Modeling
29.1
OVERVIEW
When we use a multiple regression procedure to predict the value of a quantitative
dependent variable, one of the presumptions underlying the treatment of the data is
that the scores of the cases on the dependent variable are independent of each other
(technically, their errors are independent); this is one of the assumptions of the general
linear model. However, there are many circumstances when that independence assumption
is violated. For example, we may focus on the self-assessment of quality of life by patients
in different hospitals as in the example we use for this chapter. In this context, the self-
reports of patients within a hospital may be more correlated than those from patients
selected at random from the sample. Other illustrations of this (based on some examples
from Snijders & Bosker, 2012) are as follows:
• The performance on some achievement test of school children within a school
may be more correlated than those from children enrolled in different schools.
• The political attitudes of residents within a neighborhood may be more correlated
than those from residents of different neighborhoods.
• The productiveness of employees working within a particular ofﬁce may be more
correlated than those from different ofﬁces.
In these examples, the patients, school children, neighborhood residents, and
employees are hierarchically structured (also called nested or clustered) within
groups, where each group in the sample is a hospital, school, neighborhood, or ofﬁce,
respectively.
The important point here is that the data supplied by the cases (e.g., personal assess-
ment of their quality of life) may be partly a function of the particular group (hospital)
with which they are associated. To the extent that this is true, the scores of the cases on
the dependent variable are not independent; that is, the scores of patients within the same
hospital may be more related than scores selected from patients from different hospitals.
This is what we label as a nesting or clustering effect, and to the extent that this effect
is observed, the research design needs to take this into effect in order to validly evaluate
the results of a study. One way to take this clustering effect into account in the statistical
analysis is by using a multilevel modeling procedure.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
225

226
MULTILEVEL MODELING
Several different ﬁelds have labeled the general technique we use here by many
different labels, partly depending on the way it is used (Norman & Streiner, 2008;
Vogt, 2007). Among these labels are hierarchical linear modeling, mixed modeling, nested
modeling, growth curve analysis, and contextual modeling. We follow the lead of Vogt
(2007) in using the term multilevel modeling, and we use the Linear Mixed Models
procedure in IBM SPSS® to perform the analysis. More complete recent coverage of this
complex topic can be found in Bickel (2007), Goldstein (2011), Hox (2010), Hox and
Roberts (2010), Raudenbush and Bryk (2002), and Stevens (2009).
Multilevel modeling takes into account the clustering or nesting effect of the
responses of individuals being more related on the dependent variable within groups
than between groups. We therefore use multilevel modeling to analyze the data at
multiple levels (i.e., from both the individual level and group level). If we consider the
individual cases as the most basic (microscopic) level of analysis (called Level 1 in
the context of multilevel modeling), then group membership is a higher order (more
macroscopic) level of analysis (called Level 2 in the context of multilevel modeling). It
is possible that these (Level 2) groups are themselves nested within an even higher level
of organization, although our example will be restricted to only a two-level hierarchy.
Multilevel modeling is a generalization of the multiple regression procedure allowing
for the assessment of individual observations while controlling for group effects as well
as assessing group effects while controlling for individual observations.
To estimate these variance components of the model, the IBM SPSS Linear Mixed
Models procedure uses a maximum likelihood algorithm; the default variation, which we
use in our analyses, is a restricted maximum likelihood method that takes into account
degrees of freedom associated with the ﬁxed effects in making these estimates. As one
illustration of a research context where the data might exhibit a hierarchical structure, and
the one we use for our numerical example, the assessment of quality of life by patients
in a hospital may be partially explained from their level of optimism concerning their
future (a Level 1 variable). Well over 100 hospitals were involved in this ﬁctional study,
and it is possible that, for a variety of reasons, patients in one hospital may have reported
a higher quality of life than those in another hospital. Thus, not only the scores of the
individual patients may be predictive of the degree of quality of life that is reported
but the particular hospital with which the patients are associated may also be a viable
predictor (e.g., patients in Hospitals A and B report on average a higher quality of life
than those in Hospitals C and D). It may therefore be necessary to incorporate hospital
as a Level 2 variable in the prediction model.
In this example, the quality-of-life assessments of patients within a given hospital
may on average be more alike (correlated) than those of patients selected at random from
the entire sample, that is, the Level 2 variable may be related to (predictive of) quality-
of-life judgments. This correlation is labeled as clustering (not to be confused with the
clustering procedures described in Chapters 59 and 60). To the extent that clustering is
obtained, the assumption of independence (that the patients in the study are independent
of each other with respect to the dependent variable) is violated.
Multilevel modeling allows us to accomplish two goals with respect to this cluster-
ing effect. The ﬁrst goal it allows us to accomplish is that it provides a convenient way
to determine the level of clustering in the data set. We do this by generating an intra-
class correlation (ICC); see Meyers et al. (2013) for a more complete discussion of this
coefﬁcient and see Chapter 37 for another application of this statistic. Using a two-level
hierarchy for our illustration, the ICC estimates the amount of the dependent variable
variance that can be explained by the Level 2 variable. An ICC of zero would indicate
that we observe no Level 2 effect on the dependent variable, with the implication that
we do not need to conduct the multilevel modeling procedure (i.e., ordinary regression
analysis would be sufﬁcient, as the assumption of independence would have been met).

OVERVIEW
227
However, as the value of the ICC increases, the more necessary multilevel modeling
becomes.
The ICC’s threshold to conduct a multilevel modeling is debated. Lee (2000) has
suggested that an ICC greater than .10 requires multilevel methods. Others have reported
that even very small levels of correlation can justify the use of multilevel modeling (Reise
& Duan, 2003), partly because the adverse consequences of violating the assumption of
independence are substantial (Meyers et al., 2013), including biased estimates of the
parameters of the model, alpha level inﬂation, and a greater risk of drawing an improper
conclusion.
The second goal multilevel modeling allows us to accomplish is to permit us to use
this clustering effect as a covariate in our analysis. By doing so, we can evaluate the
contributions of the other predictors in the data set when we statistically control for the
Level 2 effect.
Multilevel modeling is substantially more complex than multiple regression analysis.
Because there are at least two levels in the hierarchical structure, researchers can use
different combinations of predictors to address different issues; that is, researchers can
conﬁgure alternative models to address different research issues. As a consequence of
this ﬂexibility, each model addresses a somewhat different issue, and a common analysis
strategy used in multilevel modeling is to increase the complexity of the model in a
series of analyses.
We face two issues when performing a multilevel analysis: treating predictors as
either ﬁxed or random effects or both and if/how to center the predictor variables. We
deal with these two issues brieﬂy in turn.
Generally, treating an effect as a ﬁxed effect focuses the results of the analysis on
the particular values represented in the data ﬁle, whereas treating an effect as a random
effect allows the results of the analysis to be generalized to a wider range of values
than that is represented in the data ﬁle. In multilevel modeling, ﬁxed effects focus on
the differences in the intercepts (means) and slopes, whereas random effects focus on
variance that is explained by the variables. Furthermore, in IBM SPSS, only categorical
variables can be deﬁned as a Factor, and only Factors can be assigned as ﬁxed effects;
quantitative or dichotomously coded categorical variables can both qualify as Covariates,
and Covariates can be assigned as either ﬁxed or random effects.
In multilevel modeling, it is traditional to center the quantitative predictor variables.
In our numerical example, the only quantitative predictor variable in the data set is
optimism. As we discussed in Chapter 25, to center a variable involves subtracting a
mean from each score.
The two types of centering commonly used for Level 1 variables are grand mean
centering and group mean centering. For grand mean centering, we subtract the grand
(total sample) mean from the score for each case; thus, the centered mean for the sample
becomes zero and the scores for the cases represent their distances from the grand mean.
For group mean centering, we subtract each individual group mean from the score on
the variable for those cases within that particular group; thus, each group has a centered
mean of zero and the scores for the cases indicate their distances from their respective
group mean. Deciding on the type of centering to impose on a Level 1 variable is a
complex decision that has been discussed elsewhere (see Enders & Toﬁghi (2007) and
Meyers et al. (2013)). In our example, we center our Level 1 variable (patient optimism
score) on the grand mean.
In centering Level 2 (and higher level) quantitative predictors, we focus on the group
means of the quantitative predictors rather than the scores of the individual cases; that
is, the group means are treated as the “scores” from which a mean is subtracted. Each
case within a particular group is ﬁrst assigned the value of the mean of the group for a
speciﬁed quantitative predictor variable. We then center these group mean “scores” by
subtracting the grand mean from each. Differences among these centered “scores” thus

228
MULTILEVEL MODELING
reﬂect group differences. By using the optimism variable in our example to illustrate
this process, this Level 2 centering operation is accomplished as follows:
• We ﬁrst determine the mean of optimism for each hospital (group).
• In the data ﬁle, we create a new variable and assign the group mean to each patient
associated with the respective hospital, thus creating a Level 2 optimism variable
(e.g., all patients associated with Hospital A are assigned the mean optimism score
for Hospital A).
• We then center the Level 2 optimism variable by subtracting the grand mean of
optimism from each Level 2 optimism score. Thus, each patient associated with a
given hospital will have the same value for the centered Level 2 optimism variable.
To the extent that we observe differences among these scores, we would infer that
hospitals differ in the degree of optimism reported by their patients.
29.2
NUMERICAL EXAMPLE
This ﬁctional example provides the results of 4500 patients from 142 different hospitals,
and a screenshot of a portion of the data ﬁle named patient optimism is shown in
Figure 29.1. The outcome (dependent) variable is the score for these patients on the
Patient-Reported Outcome Measures (PROMs), an inventory that can range from 0 to
100, with higher scores indicating greater perceived health and quality of life; the variable
is named prom_dv to make it clear that it is the dependent variable. The 142 hospitals
FIGURE 29.1 A portion of the data ﬁle.

AGGREGATING THE OPTIMISM VARIABLE
229
represent the hierarchical structure of the data; these identiﬁcation codes are present in the
categorical variable of hospital. We have also dichotomously categorized these hospitals
under the variable type into either non-teaching (coded as 0 in the data ﬁle) or teaching
(coded as 1 in the data ﬁle) hospitals.
The one quantitative predictor variable in the data ﬁle is optimism, with higher
scores indicating more optimism. This variable has been grand-mean-centered at Level 1
under the variable name L1_optimism_C. Scores on this variable reﬂect the difference
between the individual patient optimism score and the total sample (grand) mean of
optimism (L1_optimism_C = optimism −grand mean). The mean of L1_optimism_C,
that is, the mean of the sample for the grand-mean-centered variable, is zero.
29.3
ANALYSIS STRATEGY
We start by showing how to build the Level 2 centered optimism variable. This entails
the following three steps:
• We need to assign each patient a score on the Level 2 variable equal to the mean
optimism score of the group. This is accomplished using the Aggregate function.
• We then calculate the grand mean (this had to be done for the Level 1 centering
operation but we generate it again to show it to readers).
• We subtract the grand mean from the aggregated scores for each patient generated
in the ﬁrst step.
We then perform the set of multilevel modeling analyses. It is common practice in
performing a multilevel modeling analysis to examine a set of models, starting with a
simpler model and generally building to more complex models (Hox, 2010). We conﬁgure
the following set of ﬁve models:
• The Unconditional Model. The ﬁrst step in any multilevel modeling analysis is to
assess the amount of variance of the dependent variable that is associated with the
Level 2 variable(s). In our example, the ICC describes the amount of prom_dv
score variance that is explained by differences among the 142 hospitals in the study.
This unconditional model serves as a baseline that we use to evaluate subsequent
models.
• A Mixed Level 1 Model. The ﬁrst predictive model includes the Fixed and Random
Effects of (individual) Level 1 centered optimism predicting prom_dv scores.
• A Mixed Level 2 Model. This model incorporates the Fixed and Random Effects
of the type of hospital together with the (individual) Level 1 optimism to predict
prom_dv scores.
• A Hierarchical Model. This model adds the optimism level of the hospital (the
Level 2 centered variable) to the mixed Level 2 model.
• An Interaction Model. This model adds the interaction effect of the type of hospital
by the Level 2 optimism level of the hospital to the hierarchical model.
29.4
AGGREGATING THE OPTIMISM VARIABLE
The most direct way to assign the mean value for a given hospital to all of the patients
associated with that hospital is to aggregate the optimism variable by hospital. To
aggregate a variable is to assign to each member of a group a particular value computed
by a designated function (e.g., a mean).

230
MULTILEVEL MODELING
FIGURE 29.2
The main Aggregate window.
We open patient optimism and from the main menu, we select Analyze ➔Data ➔
Aggregate to open the Aggregate window shown in Figure 29.2. We move hospital into
the Break Variable(s) panel. The Brake Variable is the basis for deﬁning the groups;
here, each hospital will be a group.
We move optimism into the Summaries of Variable(s) panel. This is the variable
whose values we wish to aggregate. When we move the variable into the panel, IBM
SPSS automatically does two things:
• It speciﬁes the mean as the default function. This is represented by the expression
MEAN(optimism) following the equal sign. The variable in parentheses is the
variable to be aggregated (and the one we moved into the panel).
• It assigns a generic name to the to-be-aggregated variable. Here, that name is
optimism_mean.
We select the Function pushbutton. This action opens the Aggregate Function
window (see Figure 29.3); we present this just to show readers what it looks like (the
default function—the mean—is what we wish to compute). There are several ways by
which we can aggregate our variable, including calculating the median, the sum, and the
standard deviation of the scores within each level of our Brake Variable. Here, we want
to calculate the mean optimism value for each hospital, and so we accept the default
function and select Continue.
Selecting the Name & Label pushbutton opens the Name and Label window shown
in Figure 29.4. We create the name optimism_groupmean to reﬂect the idea that it is

CENTERING THE LEVEL 2 OPTIMISM_GROUPMEAN VARIABLE
231
FIGURE 29.3
The Aggregate Function window with Mean
selected.
FIGURE 29.4
The Aggregate Name and Label window.
a Level 2 variable and click Continue. Our conﬁgured Aggregate window is shown in
Figure 29.5, where we click OK.
29.5
CENTERING THE LEVEL 2 optimism_groupmean VARIABLE
To center the Level 2 optimism_groupmean variable, we must subtract the grand
mean of optimism from each score. We therefore need to determine the value of
the grand mean, and we do so by analyzing the original optimism variable and
optimism_groupmean in the Descriptives procedure. The results, shown in Figure 29.6
for both the optimism_groupmean and the optimism variables, yielded a grand mean
for optimism_groupmean of 56.7567 and a grand mean for optimism of 56.76. The
differences in the Minimum, Maximum, and Std. Deviation between optimism_
groupmean and optimism are obtained because the former variable represents the
group means and the latter variable represents the individual scores.
We then Compute our centered Level 2 variable (to be named L2_optimism_C)
by subtracting the grand mean (56.7567) from the optimism_groupmean variable as
shown in Figure 29.7 (because the optimism_groupmean variable represents only an
intermediate step, we will not retain this variable in the variable list for the multilevel
modeling analyses in order to simplify the visual display).

232
MULTILEVEL MODELING
FIGURE 29.5
The main Aggregate window with the
to-be-aggregated variable named.
FIGURE 29.6 The Descriptive Statistics summary table.
29.6
ANALYSIS SETUP: UNCONDITIONAL MODEL
We are now ready to start evaluating our multilevel models, the ﬁrst of which is the
unconditional model. With the patient optimism data ﬁle open, we select from the main
menu Analyze ➔Mixed Models ➔Linear. This action displays the initial Specify
Subjects and Repeated window shown in Figure 29.8. In this window, we designate
the clustering variable. We move hospital to the Subjects panel to designate that our
patients are organized (clustered/nested) by being associated with different hospitals. This
speciﬁes that any differences among the hospitals will be statistically controlled in the
multilevel analysis (i.e., it will serve as a covariate). Because we do not have a longitu-
dinal variable, we leave the Repeated panel empty. Select the Continue pushbutton to
reach the main Linear Mixed Models window.

ANALYSIS SETUP: UNCONDITIONAL MODEL
233
FIGURE 29.7 Centering the Level 2 optimism variable by subtracting the grand mean from the
aggregated optimism_groupmean variable.
FIGURE 29.8
The initial Specify Subjects and Repeated window.

234
MULTILEVEL MODELING
FIGURE 29.9
The main Linear Mixed Models window set up for
the unconditional model.
The main Linear Mixed Models window is shown in Figure 29.9. We place
prom_dv in the Dependent Variable panel but do not specify any predictors (they
would be placed in the Factor(s) or Covariate(s) panel). The only effect in the model
will be the Subjects variable of hospital.
We presume that the hospitals in our data set have been randomly selected from all
hospitals in the country; given this presumption, we will deﬁne hospital as representing
a random effect. We retain the default Covariance Type of Variance Components from
the drop-down menu; once we start to add predictors to the model, we may opt to invoke
a different Covariance Type.
To designate hospital as a random effect, we select the Random pushbutton; this
selection opens the screen shown in Figure 29.10. In the Random Effects area toward
the top of the window, we check Include Intercept. In the Subject Groupings area in
the bottom part of the window, we move hospital into the Combinations panel. Click
Continue to return to the main window and select the Statistics pushbutton.
The Statistics window is shown in Figure 29.11. Check Parameter estimates and
Tests for covariance parameters under Model Statistics. Because the output is quite
lengthy, for our purposes, we do not request the Descriptive Statistics (which provides
for each hospital the number of patients and the mean and standard deviation of the
dependent variable) or the Case Processing Summary (which provides the number of
patients for each hospital). Click Continue to return to the main window and click OK
to perform the analysis.
29.7
ANALYSIS OUTPUT: UNCONDITIONAL MODEL
The Model Dimension table is shown in Figure 29.12. It reports the number of parameters
that were assessed in the analysis; in the present analysis, there were 3 parameters
(indicated in the bottom Total row). The Fixed Effect estimated was the Intercept
(which represents the grand mean), Random Effect (degree to which the intercepts vary
among the different hospitals in the Subject Variable named hospital), and Residual
(the unexplained variance of the dependent variable).
Figure 29.13 presents the Information Criteria. These are indexes reﬂecting how
well the model ﬁts the data. Smaller values represent a better ﬁtting model. These values
are useful in comparing one model to another. Norusis (2012) recommends the use of
the Akaike Information Criterion (AIC) and the Schwarz Bayesian Criterion (BIC)
to compare models, with lower scores indicating a more accurate model.

ANALYSIS OUTPUT: UNCONDITIONAL MODEL
235
FIGURE 29.10
The Random Effects window set up
for the unconditional model.
FIGURE 29.11
The Statistics window.
We can also use the −2 Restricted Log Likelihood value to determine if there is a
statistically signiﬁcantly improvement between two competing nested models (i.e., one
model is a subset of the other). This strategy will be applied as we build increasingly
more complex models in our series of analyses by adding another predictor in each step;
thus, the earlier model will be nested within the later model. To test whether the later
model has signiﬁcantly improved the ﬁt, a chi-square difference test is performed in the
following manner:

236
MULTILEVEL MODELING
FIGURE 29.12 The Model Dimension output for the unconditional model.
FIGURE 29.13
The Information Criteria for the unconditional model.
• We subtract the values of −2 Restricted Log Likelihood for the two models to
obtain an absolute difference score.
• We subtract the values for the total number of parameters for the two models to
obtain the absolute difference.
• Using the difference in parameters as our degrees of freedom, we test the statistical
signiﬁcance of the −2 Restricted Log Likelihood difference against the chi-square
distribution available in Table A.1.
The table of Estimates of Fixed Effects is presented in Figure 29.14. The only ﬁxed
effect in the unconditional model is the intercept. The value of approximately 56.06 is
the estimated population grand mean parameter for the dependent variable of prom_dv
based on the model, a value that is within the margin of error of the sample grand mean
of 56.46.
The table of Estimates of Covariance Parameters is presented in Figure 29.15.
These are variance estimates based on the random effect of hospital in the model.
The intercept represents the amount of variance explained by the presence of hospital,
which is 86.579037 and is statistically signiﬁcant (p < .001); the Residual represents the
FIGURE 29.14 The Estimates of Fixed Effects for the unconditional model.

ANALYSIS SETUP: MIXED LEVEL 1 MODEL
237
FIGURE 29.15 The Estimates of Covariance Parameters for the unconditional model.
remaining unexplained variance of 476.298593. The total variance is 562.87762, the sum
of those two values.
The ICC represents the percentage of variance explained by hospital and is computed
as 86.579037/562.87762. The resulting covariance value is 15.38156, indicating that
hospital accounted for approximately 15.38% of the variance of prom_dv. This value
for the ICC informs us that scores on prom_dv may be said to cluster within hospitals
(i.e., patients from the same hospital report more similar scores) to a sufﬁcient extent
for us to judge that the observations violate the assumption of independence. Because
there is a sufﬁciently strong Level 2 clustering effect, we are therefore well advised to
continue using hospital as a covariate in our series of analyses on prom_dv scores.
These results also inform us that there is still quite a bit of work remaining for us to
do. With only a little more than 15% of total variance explained by hospital, a little less
than 85% of the variance of prom_dv remains unexplained, and some of that variance
may be associated with the other variables in our data ﬁle. Thus, a series of additional
multilevel analyses appear warranted.
29.8
ANALYSIS SETUP: MIXED LEVEL 1 MODEL
The ﬁrst predictive model we examine is the mixed Level 1 model, where we will use as
a single predictor the (individual) Level 1 centered optimism variable. With the patient
optimism data ﬁle open, we select from the main menu Analyze ➔Mixed Models ➔
Linear. This action displays the initial Specify Subjects and Repeated window already
described in Section 29.6, which we conﬁgure in the same way to control for the effect
of hospital as our clustering variable. Select the Continue pushbutton to reach the main
Linear Mixed Models window.
The main Linear Mixed Models window is shown in Figure 29.16. We place
prom_dv in the Dependent Variable panel and place L1_optimism_C in the Covari-
ate(s) panel.
Selecting the Fixed pushbutton opens the Fixed Effects window presented in
Figure 29.17. We move L1_optimism_C from the Factors and Covariates panel
to the Model panel by highlighting it and clicking the Add pushbutton. This will
allow us to evaluate the average intercept and slope of the L1_optimism_C scores
predicting prom_dv. The Include intercept box is already checked, and we retain this
speciﬁcation. Click Continue to return to the main window and select the Random
pushbutton.
The Random Effects screen is shown in Figure 29.18. The top portion of the screen
concerns the Random Effects. In the drop-down menu, for Covariance Type, select
Unstructured. Covariance here deals with the relationship between the intercept and
slope parameters of L1_optimism_C, and we select Unstructured because the nature of
the relationship is unknown. We then check the Include intercept box. Finally, highlight
L1_optimism_C and select the Add pushbutton; this will move L1_optimism_C into the
Model window. In the bottom portion of the screen is the Subject Groupings, where

238
MULTILEVEL MODELING
FIGURE 29.16
The main Linear Mixed Models window set up for
the mixed Level 1 model.
FIGURE 29.17
The Fixed Effects window set up
for the mixed Level 1 model.
we have moved hospital from the Subjects panel to the Combinations panel. Click
Continue to return to the main window.
We conﬁgure the Statistics window as described in Section 29.6 (requesting Param-
eter estimates and Tests for covariance parameters under Model Statistics). Click OK
to perform the analysis.
29.9
ANALYSIS OUTPUT: MIXED LEVEL 1 MODEL
The Model Dimension table is shown in Figure 29.19. There were 6 parameters in
this model. Also documented in the table is the fact that we used an Unstructured
Covariance Structure.

ANALYSIS OUTPUT: MIXED LEVEL 1 MODEL
239
FIGURE 29.18 The Random Effects window set up for the mixed Level 1 model.
FIGURE 29.19 The Model Dimension output for the mixed Level 1 model.
Figure 29.20 presents the Information Criteria. Both the AIC and the BIC in the
Information Criteria table report smaller values than the previous analyses, indicating
a better model ﬁt.
We can also perform a chi-square difference test. The −2 Restricted Log Likelihood
values for the unconditional model and our mixed Level 1 model are 40778.149 and
40418.621, respectively, with a difference of 359.528. The degrees of freedom for these
two models are 3 and 6, respectively, with a difference of 3. We therefore evaluate the

240
MULTILEVEL MODELING
FIGURE 29.20
The Information Criteria for the mixed Level 1 model.
chi-square difference with three degrees of freedom. Based on Table A.1, a difference
of 359.528 is statistically signiﬁcant (p < .001). We therefore may infer that the mixed
Level 1 model improved prediction over the unconditional model.
Figure 29.21 presents the table of Estimates of Fixed Effects. The intercept still
represents the estimated population grand mean parameter for the dependent variable of
prom_dv based on the model. We note that it has slightly increased from the estimate of
the unconditional model to approximately 56.29 when controlling for L1_optimism_C.
We now have an additional entry in the table (the output for the unconditional
model reported only an intercept value). The parameter estimate for L1_optimism_C is
the slope of the function, and we interpret the slope of approximately .60 as follows:
every increase of one scale value in the L1_optimism_C score increases (because the
slope is positive) the prom_dv score by about .60. As can be seen in the Sig. column,
the slope is statistically signiﬁcant (p < .001); that is, it is statistically different from a
slope of zero.
The table of Estimates of Covariance Parameters is presented in Figure 29.22.
These are variance estimates based on the random effects of hospital in the model. Incor-
porating the patients’ individual optimism scores into the model reduced the Residual
FIGURE 29.21 The Estimates of Fixed Effects for the mixed Level 1 model.
FIGURE 29.22 The Estimates of Covariance Parameters for the mixed Level 1 model.

ANALYSIS SETUP: MIXED LEVEL 2 MODEL
241
from about 476.58 to about 442.44 (it became about 7.16% smaller), signifying that
L1_optimism_C explains more of the within-hospital variance than we could in the
unconditional model.
The next three rows are labeled as UN (1,1), UN (2,1), and UN (2,2), in which the
UN stands for the Unstructured Covariance Type that we speciﬁed in the Random
Effects dialog window. The numbers in parentheses after UN refer to coordinates in a
particular output table that we requested. The three speciﬁc parameters of the variance
in the order that we suggest examining these results are as follows:
• UN (1,1) represents the variance estimate (with a value of approximately 47.41)
of the prom_dv intercepts, with L1_optimism_C in the model. In this example,
the prom_dv intercepts vary signiﬁcantly (p < .001) among the hospitals.
• UN (2,2) represents the variance estimate (with a value of approximately 0.03) of
the prom_dv slopes, with L1_optimism_C in the model. This parameter failed
to achieve statistical signiﬁcance (p = .069), indicating that the hospitals have
roughly equivalent slopes.
• UN (2,1) represents the covariance estimate between the prom_dv slopes and
intercepts (with a value of approximately −0.45), with L1_optimism_C in the
model. This output assesses whether higher values of the intercept were associated
with steeper or ﬂatter slopes. This parameter also failed to achieve statistical
signiﬁcance (p = .131), indicating that there was no relationship between the values
of the intercepts and the values of the slopes. Because the random effect provides
no signiﬁcant relationship to the prom_dv score variance, we will simplify the
remaining models by not specifying the random effects of L1_optimism_C.
29.10
ANALYSIS SETUP: MIXED LEVEL 2 MODEL
With the exception of removing the random effects of L1_optimism_C, we are ready to
further raise the complexity level of our multilevel model. We do this by now taking into
consideration the effects of the type of hospital in the sample (recall that we have both
nonteaching and teaching hospitals represented). With the patient optimism data ﬁle
open, we select from the main menu Analyze ➔Mixed Models ➔Linear. This action
displays the initial Specify Subjects and Repeated window where we deﬁne hospital as
our clustering variable. Select the Continue pushbutton to reach the main Linear Mixed
Models window.
The main Linear Mixed Models window is shown in Figure 29.23. We place
prom_dv in the Dependent Variable panel and L1_optimism_C and type in the Covari-
ate(s) panel.
Selecting the Fixed pushbutton opens the Fixed Effects window (see Figure 29.24).
We move L1_optimism_C and type from the Factors and Covariates panel to the
Model panel and make sure that the Include intercept box is checked.
In the top portion of the Random Effects screen shown in Figure 29.25, we
select Unstructured as our Covariance Type and move type into the Model panel
(and do not specify L1_optimism_C as a random effect as indicated at the end of
Section 29.9). In the bottom portion of the screen, we have moved hospital from the
Subjects panel to the Combinations panel. Click Continue to return to the main
window.
We conﬁgure the Statistics window as described in Section 29.6 (requesting Param-
eter estimates and Tests for covariance parameters under Model Statistics). Click OK
to perform the analysis.

242
MULTILEVEL MODELING
FIGURE 29.23
The main Linear Mixed Models window setup
for the mixed Level 2 model.
FIGURE 29.24 The Fixed Effects window setup for the mixed Level 2 model.
29.11
ANALYSIS OUTPUT: MIXED LEVEL 2 MODEL
The Model Dimension table is shown in Figure 29.26. There were 7 parameters in this
mixed Level 2 model. Also documented in the table is our use of an Unstructured
Covariance Structure.
Figure 29.27 presents the Information Criteria, including both the AIC and the
BIC. These values are smaller than those in the mixed Level 1 model, suggesting a
better model ﬁt.

ANALYSIS OUTPUT: MIXED LEVEL 2 MODEL
243
FIGURE 29.25 The Fixed Effects window setup for the mixed Level 2 model.
FIGURE 29.26 The Model Dimension output for the mixed Level 2 model.
We can also perform a chi-square difference test. The −2 Restricted Log Likelihood
values for the mixed Level 1 model and our mixed Level 2 model are 40418.621 and
40342.084, respectively, with a difference of 76.537. The degrees of freedom for these
two models are 6 and 7, respectively, with a difference of 1. We therefore evaluate the
chi-square difference with one degree of freedom. Based on Table A.1, the difference of

244
MULTILEVEL MODELING
FIGURE 29.27
The Information Criteria for the mixed Level 2 model.
FIGURE 29.28 The Estimates of Fixed Effects for the mixed Level 2 model.
76.537 is statistically signiﬁcant (p < .001). We therefore may infer that the mixed Level
2 model improved prediction over the mixed Level 1 model.
Figure 29.28 presents the table of Estimates of Fixed Effects. The intercept rep-
resents the estimated population grand mean parameter for the dependent variable of
prom_dv based on the model now containing both L1_optimism_C and type. We note
that it has increased from the estimate of the mixed Level 1 model to approximately
52.91.
The parameter slope estimate for L1_optimism_C is approximately .57 and indi-
cates that every increase of one scale value in the L1_optimism_C score increases the
prom_dv score by about .57 when controlling for type. As can be seen in the Sig.
column, the slope is statistically signiﬁcant (p < .001).
Hospital type is associated with a parameter estimate of about 7.02. This is a dichoto-
mously coded variable with values of 0 for nonteaching hospitals and 1 for teaching
hospitals. Because the parameter is positive and because a value of 1 is greater than the
value of 0, we interpret the slope to indicate that patients in teaching hospitals report
prom_dv scores of about 7.02 (on average) that is higher than the L1_optimism_C
scores of patients in nonteaching hospitals.
The table of Estimates of Covariance Parameters is presented in Figure 29.29.
These are variance estimates based on the random effects of hospital in the model. Incor-
porating the patients’ individual optimism scores into this model reduced the Residual
very slightly from about 442.44 in the mixed Level 1 model to about 437.91 (it became
about 1% smaller), signifying that the model containing both L1_optimism_C and type
explains just a little more of the within-hospital variance than we could in the mixed
Level 1 model.
The Unstructured covariance components indicate the following:
• UN (1,1) represents the variance estimate (with a value of approximately 39.53)
of the prom_dv intercepts, with L1_optimism_C and type in the model. The
prom_dv intercepts vary signiﬁcantly (p < .001) among the hospitals.
• UN (2,2) represents the variance estimate (with a value of approximately 88.54) of
the prom_dv slopes, with L1_optimism_C and type in the model. This parameter

ANALYSIS OUTPUT: HIERARCHICAL MODEL
245
FIGURE 29.29 The Estimates of Covariance Parameters for the mixed Level 2 model.
was statistically signiﬁcant (p = .015), indicating that the slopes vary signiﬁcantly
among the hospitals.
• UN (2,1) represents the covariance estimate between the prom_dv slopes and
intercepts (with a value of approximately −40.53), with L1_optimism_C and
type in the model. This indicates that there is a signiﬁcant (p = .034) covariance
(correlation) between the slope and the intercept. Because the valence of this
parameter is negative, we interpret it as indicating that higher values of type (recall
that nonteaching and teaching hospitals were coded as 0 and 1, respectively) are
associated with smaller variances; thus, teaching hospitals have less steep slopes
than nonteaching hospitals.
29.12
ANALYSIS SETUP: HIERARCHICAL MODEL
We now add L2_optimism_C to the model to evaluate the average optimism level of
the hospital based on the possibility that this Level 2 variable, a characteristic of the
hospital, can enhance our prediction of prom_dv.
We open the patient optimism data ﬁle and select from the main menu Analyze
➔Mixed Models ➔Linear. We deﬁne hospital as our clustering variable in the initial
Specify Subjects and Repeated window and select the Continue pushbutton to reach
the main Linear Mixed Models window.
The main Linear Mixed Models window is shown in Figure 29.30. We
place prom_dv in the Dependent Variable panel and L1_optimism_C, type, and
L2_optimism_C in the Covariate(s) panel.
Selecting the Fixed pushbutton opens the Fixed Effects window shown in
Figure 29.31. We move L1_optimism_C, type, and L2_optimism_C from the Factors
and Covariates panel to the Model panel and make sure that the Include intercept
box is checked. We use the same speciﬁcations for Random Effects and Statistics as
we used in the mixed Level 2 model analysis.
29.13
ANALYSIS OUTPUT: HIERARCHICAL MODEL
The Model Dimension table is shown in Figure 29.32. There were 8 parameters in this
mixed Level 2 model.
Figure 29.33 presents the Information Criteria, including both the AIC and the
BIC. These values are smaller than in the mixed Level 2 model, suggesting a better
model ﬁt.
We can also perform a chi-square difference test. The −2 Restricted Log Likelihood
values for the mixed Level 2 model and this hierarchical model are 40342.084 and
40308.216, respectively, with a difference of 33.868. The degrees of freedom for these

246
MULTILEVEL MODELING
FIGURE 29.30
The main Linear Mixed Models window setup
for the hierarchical model.
FIGURE 29.31 The Fixed Effects window setup for the hierarchical model.
two models are 7 and 8, respectively, with a difference of 1. We therefore evaluate the
chi-square difference with one degree of freedom. Based on Table A.1, the difference of
33.868 is statistically signiﬁcant (p < .001). We therefore may infer that the hierarchical
model improved prediction over the mixed Level 2 model.
Figure 29.34 presents the table of Estimates of Fixed Effects. The intercept
represents the estimated population grand mean parameter for the dependent vari-
able of prom_dv based on the model now containing L1_optimism_C, type, and
L2_optimism_C. Its value is now approximately 54.12.

ANALYSIS OUTPUT: HIERARCHICAL MODEL
247
FIGURE 29.32 The Model Dimension output for the hierarchical model.
FIGURE 29.33
The Information Criteria for the hierarchical model.
FIGURE 29.34 The Estimates of Fixed Effects for the hierarchical model.
The parameter slope estimate for L1_optimism_C is approximately .52 and is sta-
tistically signiﬁcant (p < .001). It indicates that every increase of one scale value in the
L1_optimism_C score increases the prom_dv score by about .52 when controlling for
type and L2_optimism_C.
Hospital type is associated with a parameter estimate of about 4.55 (p < .001). We
interpret the slope to indicate that patients in teaching hospitals report prom_dv scores
of about 4.55 (on average) that is higher than those of patients in nonteaching hospitals,
controlling for L1_optimism_C and L2_optimism_C.

248
MULTILEVEL MODELING
FIGURE 29.35 The Estimates of Covariance Parameters for the hierarchical model.
The parameter slope estimate for L2_optimism_C is approximately .72 and is sta-
tistically signiﬁcant (p < .001). It indicates that every increase of one scale value in the
L2_optimism_C score increases the prom_dv score by about .72 when controlling for
type and L1_optimism_C.
The table of Estimates of Covariance Parameters is presented in Figure 29.35.
Adding the Hospital Level optimism scores into the model reduced the Residual only
slightly from about 437.91 to about 437.76, suggesting little variance is being explained
between hospitals. The values in UN (1,1), UN (2,2), and UN (2,1) have all been reduced
in value, reﬂecting the inclusion of L2_optimism_C. However, the results among the
covariances remain the same as in the previous model.
29.14
ANALYSIS SETUP: INTERACTION MODEL
The last model in our series builds on the hierarchical model with one additional predictor.
It is possible that the degree of optimism characterizing a given hospital (represented by
L2_optimism_C) may be moderated by hospital type; that is, the slope of the function
predicting prom_dv from L2_optimism_C may be different for nonteaching and teaching
hospitals. This would be represented by a statistically signiﬁcant interaction between type
and L2_optimism_C.
We open the patient optimism data ﬁle and select from the main menu Analyze
➔Mixed Models ➔Linear. We deﬁne hospital as our clustering variable in the initial
Specify Subjects and Repeated window and select the Continue pushbutton to reach
the main Linear Mixed Models window.
The main Linear Mixed Models window is shown in Figure 29.36. We
place prom_dv in the Dependent Variable panel and L1_optimism_C, type, and
L2_optimism_C in the Covariate(s) panel.
Selecting the Fixed pushbutton opens the Fixed Effects window is shown in
Figure 29.37. We move L1_optimism_C, type, and L2_optimism_C from the Factors
and Covariates panel to the Model panel and make sure that the Include intercept
box is checked.
To include the type*L2_optimism_C interaction in the model, we take the following
steps:
• From the drop-down menu between the Factors and Covariates panel and the
Model panel, we select Interaction. This is shown in Figure 29.37.
• While holding down the Control or Shift key, we highlight type and
L2_optimism_C (see Figure 29.38).
• We select the Add pushbutton to create the type*L2_optimism_C interaction
variable that will appear in the Model panel (see Figure 29.39). This will determine
the average intercept and slope of this interaction effect while controlling for
L1_optimism_C, type, and L2_optimism_C on prom_dv scores.

ANALYSIS OUTPUT: INTERACTION MODEL
249
FIGURE 29.36
The main Linear Mixed Models window setup for
the interaction model.
FIGURE 29.37 Select Interaction in the Fixed Effects.
We use the same speciﬁcations for Random Effects and Statistics as we used in
the mixed Level 2 model analysis.
29.15
ANALYSIS OUTPUT: INTERACTION MODEL
The Model Dimension table is shown in Figure 29.40. There were 9 parameters in this
mixed Level 2 model.

250
MULTILEVEL MODELING
FIGURE 29.38 Highlight both type and L2_optimism_C by selecting while depressing the Con-
trol or Shift key.
FIGURE 29.39 Selecting Add creates the type*L2_optimism_C term to appear in the Model
panel.

ANALYSIS OUTPUT: INTERACTION MODEL
251
FIGURE 29.40 The Model Dimension output for the interaction model.
FIGURE 29.41
The Information Criteria for the interaction model.
FIGURE 29.42 The Estimates of Fixed Effects for the interaction model.

252
MULTILEVEL MODELING
FIGURE 29.43 The Estimates of Covariance Parameters for the interaction model.
Figure 29.41 presents the Information Criteria, including both the AIC and the
BIC. These values increased very slightly over the hierarchical model, suggesting no
improvement in model ﬁt.
We can also perform a chi-square difference test. The −2 Restricted Log Likelihood
values for the mixed hierarchical model and the interaction model are 40308.216 and
40308.429, respectively, with a difference of 0.213. The degrees of freedom for these
two models are 8 and 9, respectively, with a difference of 1. We therefore evaluate the
chi-square difference with one degree of freedom, and a chi-square value of less than
1.00 is not statistically signiﬁcant. We therefore may infer that the interaction model
offers no improved prediction over the hierarchical model.
Figure 29.42 presents the table of Estimates of Fixed Effects. The results are vir-
tually the same as those for the hierarchical model, in that the type*L2_optimism_C
interaction failed to achieve statistical signiﬁcance (p = .324).
The table of Estimates of Covariance Parameters is presented in Figure 29.43.
Adding the type*L2_optimism_C interaction into the model barely affected the values
of the parameters shown in the table from those associated with the hierarchical model.

P A R T 9
REGRESSING (PREDICTING)
CATEGORICAL VARIABLES


C H A P T E R
3 0
Binary Logistic Regression
30.1
OVERVIEW
In contrast to ordinary least-squares linear regression where the dependent or outcome
variable is quantitative, we use logistic regression when we wish to predict the value of a
categorical dependent variable (e.g., success for failure in a training program or medical
regime). When only two outcome categories are assessed, the procedure is called binary
logistic regression; when three or more outcome categories are involved, the procedure
is called multinomial logistic regression. Predictor variables can be any combination of
quantitative and binary variables. Details regarding the nature of the logistic regression
model are described in other sources (e.g., Menard, 2010; Meyers et al., 2013).
The two possible outcomes in binary logistic regression represent two different
groups of cases, and the results of the analysis are framed in terms of the likelihood
of a case being in (coded in the data ﬁle as) one of the groups as opposed to the other.
As such, it is important to carefully formulate the group (categorical) coding schema for
the outcome variable in building the data ﬁle. The two groups (outcomes) and category
codes are as follows:
• The response or target group represents the desired or expected outcome (e.g.,
success). It is this category to which prediction is directed. This category should
be given a code of 1 for the outcome variable.
• The reference or control group represents the alternative outcome (e.g., failure).
This category should be given a code of 0 for the outcome variable.
It is not uncommon for binary variables to be used as predictors in logistic regression
analysis (e.g., sex, family history of a certain medical condition), and the results of the
analysis are framed in terms of one of the categories being more or less likely to achieve
the target outcome. The analysis describes the likelihood of achieving the target outcome
for the focus category with respect to the other category, which is the reference category.
For example, if the predictor variable is sex and we wished to describe the results in
terms of females being more (or less) likely to achieve the target outcome, then female
would be the focus category and male would be the reference category. If we wish to
describe the results in terms of males being more (or less) likely to achieve the target
outcome, then male would be the focus category and female would be the reference
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
255

256
BINARY LOGISTIC REGRESSION
category. The statistical outcome of one would be the inverse of the other. IBM SPSS®
provides us with a dialog window to select which category (the lower or higher code) is
to be designated as the reference category.
At a simple level, the result of interest to most researchers is the odds ratio associated
with each predictor. Assume that we have a target outcome (e.g., success). Odds represent
the ratio of number of successes to number of failures for any particular group of cases.
To illustrate the way in which we use odds ratios in logistic regression, let us assume
these two situations:
• The odds of females being successful (as opposed to failing) are 10 to 1.
• The odds of males being successful (as opposed to failing) are 5 to 1.
Now, if we focus on the females and compute the odds ratio, we place the odds
associated with females in the numerator and those associated with males in the denomi-
nator. For our example, this yields a ratio of 10 : 5, with the odds ratio for females being
2.00. We interpret this odds ratio to indicate that the odds for females being successful
are twice the odds of males being successful. If female is the focus category and male
is the reference category, the odds ratio for sex shown in the logistic regression results
would be 2.00.
By the same reasoning as just described, the odds ratio for males represents the
odds associated with males divided by the odds associated with females. Here, the ratio
is 5 : 10 or 0.50. We interpret this odds ratio to indicate that the odds for males being
successful are half of those for females. If male is the focus category and female is the
reference category, the odds ratio for sex shown in the logistic regression results would
be 0.50.
It is important to note that in many contexts, having the odds of 5 to 1 associated
with success in a program, as is the case with the males in our example, might be
considered to represent a wonderful opportunity for success. But this favorable situation
is overshadowed by the 10 to 1 odds of success associated with the females. Our point
is that the odds ratio does not convey information about the absolute level of the odds
but only how the odds associated with one group compare with those of another.
30.2
NUMERICAL EXAMPLE
The data for our ﬁctional study represent a set of at-risk junior high school children in a
special educational early intervention program. We use student sex as well as the degree
of encouragement students received from their families as predictors of program success.
Under the variable of graduated, success (the target category) is deﬁned as the children
having graduated and entering a high school (coded as 1 for graduated) and failure (the
reference category) is deﬁned as the children not having graduated (coded as 0 for not
graduated). In this example, sex of child is coded as 0 (male) and 1 (female), and we
will designate female (having the “higher” or “last” sex code) as the focus group and
male as the reference group (having the “lower” or “ﬁrst” sex code) in the analysis setup.
Also measured on a scale ranging from 0 through 45 is family_encouragement, with
higher values representing more encouragement. The data ﬁle is named graduation, and
a screenshot of a portion of the data ﬁle is shown in Figure 30.1.
30.3
ANALYSIS SETUP
From the main menu, we select Analyze ➔Regression ➔Binary Logistic to open
the main Logistic Regression window shown in Figure 30.2. We move graduated into

ANALYSIS SETUP
257
FIGURE 30.1 A portion of the data ﬁle.
FIGURE 30.2
The main Logistic Regression dialog
window.
the Dependent panel and sex and family_encouragement into the Covariates panel to
register them as predictor variables. We retain the default Method of Enter to perform a
standard logistic regression analysis where both variables are entered together in a single
block (various step procedures are available in the Method drop-down menu).
When variables are moved into the Covariates panel, the Categorical pushbutton
becomes available. Selecting the Categorical pushbutton opens the Deﬁne Categorical

258
BINARY LOGISTIC REGRESSION
FIGURE 30.3
The Deﬁne Categorical Variables dialog
window of Logistic Regression.
Variables window that allows us to specify our focus and reference groups for any
categorical predictor variable. This window is shown in Figure 30.3. We move sex into
the Categorical Covariates panel, and the expression (indicator) appears after the name
of the variable. Indicator is the type of contrast that we wish to use; it contrasts the
presence of a category with its absence, and this is the ﬁrst step in specifying how we
want the analysis to treat sex.
The second step in specifying how we want the analysis to treat sex is to identify the
focus and reference categories. IBM SPSS calls for the speciﬁcation of the Reference
Category (by implication, the other category of a binary variable will be the focus
category). Males and females are arbitrarily coded as 0 and 1, respectively. In our analysis,
we wish to have female as the focus category and male as the reference category; in
our coding schema, female is the last category (the highest numeric code) and male
is the ﬁrst category (the lowest numeric code). We therefore select ﬁrst to make the
code of 0 (male) the Reference Category, and click Change to register that with IBM
SPSS. Upon clicking Change, the speciﬁcation is added to the name of the variable
as shown in Figure 30.4, so that our categorical variable now appears in the panel as
sex(Indicator(ﬁrst)). Click Continue to return to the main dialog window.
FIGURE 30.4
The Deﬁne Categorical Variables dialog
window of Logistic Regression after specifying
the Reference Category as First.

ANALYSIS OUTPUT
259
FIGURE 30.5
The Options dialog window of Logistic
Regression.
In the Options window, we ask for the Hosmer-Lemeshow goodness-of-ﬁt statis-
tics as well as the 95% conﬁdence interval for the odds ratio (CI for exp(B)). The
Classiﬁcation cutoff is the probability we need to achieve in order to classify a case as
belonging to the target group. Probabilities can range between 0 and 1, and the default
in IBM SPSS is .5. This means that if a case has a .5 or greater probability of being
in the target group (based on the ﬁnal logistic model), it is classiﬁed (predicted to be)
in the target group; if a case has less than a .5 probability of being in the target group,
it is classiﬁed (predicted to be) in the reference group. For this analysis, we will keep
the default Classiﬁcation cutoff at .5, but we will deal explicitly with this speciﬁcation
in Chapter 31. Click Continue to return to the main dialog window and click OK to
perform the analysis (Figure 30.5).
30.4
ANALYSIS OUTPUT
Figure 30.6 shows what can be called administrative output. The Case Processing Sum-
mary table displays information on the number of cases in the analysis. The Dependent
Variable Encoding shows the “internal recoding” of our binary outcome variable. IBM
SPSS does this recoding because not all researchers may assign codes of 0 and 1 to
the outcomes. The software thus assigns the lower code (whatever was used by the
researchers) a value of 0 and the higher code a value of 1; IBM SPSS ignored the fact
that we had already anticipated the need to code as 0 and 1, and so it performed its
recoding anyway.
We also obtain a count of males and females in the Categorical Variables Codings
table together with their codes. The codes under Parameter coding reﬂect our speciﬁca-
tion of the reference group; the category shown as .000 (male) is treated as the reference
category in the analysis.
Figure 30.7 presents the results for the intercept-only model, computed with only the
constant in the equation but with none of the predictor variables; it is called the Step 0
model (the model as of Block 0), as there are no predictor variables in the equation yet.
The Classiﬁcation Table simply provides counts of the number of cases in each binary

260
BINARY LOGISTIC REGRESSION
FIGURE 30.6
Administrative output.
FIGURE 30.7 The Beginning Block output.

ANALYSIS OUTPUT
261
outcome. It is a prediction table, with the Observed cases in the rows and the Predicted
group membership represented by the columns.
With only the intercept in the model, our prediction is based exclusively on the
frequencies in that table: 138 cases did not graduate and 272 cases did graduate. Thus,
if we had no additional information, our best single guess is that a program participant
would have graduated (more graduated than did not), and our classiﬁcation (predictions)
would be correct 66.3% of the time.
The middle table in Figure 30.7 shows the Variables in the Equation; of course,
in the intercept-only model, we have no predictor variables, so the only factor in the
model is the intercept (shown as Constant). The odds ratio, shown as Exp(B), has a
value of 1.971. This is because 272 is 1.971 times (approximately twice) as large as 138
(two-thirds of the sample graduated and one-third did not). The odds ratio informs us that
a random program participant is 1.971 times (almost twice) as likely to have graduated
as not. The bottom table labeled as Variables not in the Equation reminds us that these
predictor variables have yet to be entered into the model.
Figure 30.8 shows the tables evaluating the model, with the predictors included.
This is Block 1 or Step 1 and is our only step because we entered both of our predictors
together. The Omnibus Tests of Model Coefﬁcients contains the model chi-square, a
statistical test of the null hypothesis that all the predictor coefﬁcients are zero. It is
equivalent to the overall F test in linear regression. The Model chi-square value (in the
last row) is 157.368, and with 2 degrees of freedom (there are two predictors in the
model), we have a statistically signiﬁcant amount of prediction (p < .001).
The Model Summary table provides three indexes of how well the logistic regression
model ﬁts the data. With all the variables in the model, the goodness-of-ﬁt −2 Log
likelihood statistic is 366.403. We do not usually interpret this statistic directly but use
it to compare different logistic models. The Cox and Snell pseudo R2 is .351 and the
Nagelkerke pseudo R Square, which is always the higher of the two, is .442. On the
basis of the Nagelkerke pseudo R Square, we would thus conclude that about 44% of
the variance associated with graduation is explained by our predictor variables.
The Hosmer and Lemeshow Test provides a formal test assessing whether the pre-
dicted probabilities match the observed probabilities. We hope to obtain a nonsigniﬁcant
p value for this test because the goal of the research is to derive predictors that will
accurately predict the actual probabilities. In this example, the goodness-of-ﬁt statistic
is 11.204; it is tested as a chi-square value and is associated with a p value of .190,
indicating an acceptable match between predicted and observed probabilities.
The Contingency Table for Hosmer and Lemeshow Test, shown in the bottom
table in Figure 30.8, demonstrates more details of the Hosmer and Lemeshow test. This
output has divided the data into 10 groups based on the outcome variable. These groups
are deﬁned by increasing rates of graduation (called “steps” in the table). For example,
the ﬁrst group (Step 1) represents those students least likely to graduate. The observed
frequencies were that 33 cases did not graduate and 9 cases did graduate. The observed
and the expected frequencies (based on the prediction model) match reasonably well for
all of the steps and is a desirable result.
The Classiﬁcation Table is presented in Figure 30.9. The overall predictive accuracy
is 80.0% now that we have our predictors in the model, although the model predicted
better for graduation (87.1%) than for not having graduated (65.9%). Recall from Block
0 that without considering any of our predictors, the likelihood or probability of a correct
prediction was 66.3%; thus, our predictors certainly contributed to successful prediction.
The ﬁne-tuned results of the analysis are presented in the Variables in the Equation
table shown in Figure 30.10. The table presents for each predictor the raw score partial
regression coefﬁcient (written as an uppercase B by IBM SPSS) and its standard error
(S.E.). These coefﬁcients indicate the amount of change expected in the log odds when
there is a one-unit change in the predictor variable, with all the other variables in the

262
BINARY LOGISTIC REGRESSION
FIGURE 30.8 The omnibus results and the model ﬁt information.
model held constant (see Meyers et al., 2013). A coefﬁcient close to 0 suggests that there
is no change in the outcome variable associated with the predictor variable.
The Sig. column represents the p value for testing whether a predictor is signiﬁcantly
associated with graduation controlling for the other predictor(s). The logistic coefﬁcients
can be used in a manner similar to linear regression coefﬁcients to generate predicted
values. In this example, the model is as follows:
graduation = −3.256 + 1.970 (sex) + 0.143 (family _ encouragement)
Because family_encouragement is a quantitative variable that can take a range of
values, the expression 0.143 (family_encouragement) can also take on a wide range of
values. But because sex is coded in a binary manner, there are only two values possible

ANALYSIS OUTPUT
263
FIGURE 30.9 The Classiﬁcation Table.
FIGURE 30.10 The Variables in the Equation.
for the expression 1.970 (sex): for females (coded as 1), the value of the expression will
be 1.970*1 or 1.970; for males (coded as 0), the value of the expression will be 1.970*0
or 0.
The Exp(B) column provides the odds ratios associated with each predictor (adjusting
for the other predictor), with the 95% conﬁdence interval associated with each provided in
the ﬁnal two columns. The adjusted odds ratio for sex is 7.170, with a 95% conﬁdence
interval of 4.207–12.221. This odds ratio indicates that in this sample, the odds of
females (because they were the focal group) graduating are 7.170 times the odds of
males graduating, controlling for family_encouragement.
The adjusted odds ratio for family_encouragement is 1.154, with a conﬁdence
interval of 1.111–1.198. This is a quantitatively measured variable, and so we interpret
this odds ratio of 1.154 to mean that an increase of 1 in the family_encouragement
measure increases the odds of graduation over the odds for not graduating by 1.154
times, controlling for sex.
We can apply the odds ratio to any two scores on the family_encouragement vari-
able. For example, we can say when controlling for sex that the odds of graduation
for a student with a family_encouragement score of 24 are 1.154 times greater than
the graduation odds for a student whose family_encouragement score is 23, as well as
2.308 (computed as 2*1.154) times greater than the graduation odds for a student whose
family_encouragement score is 22 and 3.462 (computed as 3*1.154) times greater than
the graduation odds for a student whose family_encouragement score is 21, and so on.


C H A P T E R
3 1
ROC Analysis
31.1
OVERVIEW
A receiver operator characteristic (ROC, pronounced “R-O-C” rather than “rock”) analy-
sis has been used for half a century in the context of signal detection and decision theory
(see Meyers et al. (2013) for a brief history). Generally, it allows us to evaluate a range
of decision rules that we may use to predict with which one of two possible outcomes
or groups of particular cases are associated. It is particularly well suited to partner with
binary logistic regression, and we treat the topic here as an extension of that topic.
ROC analysis requires that we have an underlying quantitative outcome measure,
and in binary logistic regression it is the predicted probability of a case being in the target
group. The decision to classify cases into the target and reference groups is made on the
basis of this predicted probability. In the analysis described in Chapter 30, we retained
the default Classiﬁcation cutoff of .5. Cases associated with a .5 or greater probability
of being in the target group were classiﬁed (predicted to be) in the target group, whereas
cases associated with less than .5 probability of being in the target group were classiﬁed
(predicted to be) in the reference group. However, a Classiﬁcation cutoff of .5 may
not necessarily be the most appropriate decision cutoff because different circumstances
might cause us to favor one type of decision strategy over another. Given a particular
circumstance, we can use an ROC analysis to determine if a more appropriate cutoff
should be established.
In an ROC analysis, the classiﬁcation results (see the Classiﬁcation Table in
Figure 31.1) are characterized in decision-making language. The 2 × 2 table shows the
rows as the observed outcomes and the columns as the predicted outcomes. The ﬁrst
row and column represent the reference (the negative) outcome (did not graduate), and
the second row and column represent the target (the positive) outcome (graduated).
Reading left to right for the ﬁrst and then the second row, the cells represent the
following:
• True Negatives (Observed, did not graduate; Predicted, did not graduate). These
are the cases who have been correctly predicted as having the negative (did not
graduate) outcome. The proportion of all reference group members that is repre-
sented by these cases is known as speciﬁcity.
• False Positives (Observed, did not graduate; Predicted, graduated). These are the
cases with the negative (did not graduate) outcome (reference group members)
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
265

266
ROC ANALYSIS
FIGURE 31.1 The Classiﬁcation Table with the default cut value at .5.
who have incorrectly been predicted to have the positive outcome. The proportion
of all reference group members that is represented by these cases is known as
(1 −speciﬁcity).
• False Negatives (Observed, graduated; Predicted, did not graduate). These are
members of the target (graduated) group who have been predicted to have a
negative (did not graduate) outcome.
• True Positives (Observed, graduated; Predicted, graduated). These are the cases
who have been correctly predicted as having the positive outcome (graduated).
The proportion of all target group members that is represented by these cases is
known as sensitivity.
The success of correctly predicting group membership depends on the location of
the decision criterion. But prediction will never be perfect (if it were, we would not
be doing this analysis), and there are trade-offs involved as in any decision process.
Generally, setting a higher probability as our decision criterion (Classiﬁcation cutoff)
will improve the rate of true negatives (predicting membership in the reference group)
but at the expense of generating more false negatives (predicting graduated outcomes
to be did not graduate outcomes). On the other hand, a lower decision criterion will
give us a better true positive rate but will increase the number of false positives.
The decision criterion turns out to be a coordinate on an ROC curve (we will generate
one in our analysis), a bow-shaped function with sensitivity (true positive rate) on the Y-
axis and 1 −speciﬁcity (false-positive rate) on the X-axis. The amount of the bow of the
curve (the area under the curve) is a gauge of how well the logistic model can differentiate
the groups; more bow (larger areas under the curve) reﬂects greater differentiation.
IBM SPSS® provides in its output a set of coordinates of the ROC curve together
with the probabilities associated with correctly predicting membership in the target group.
The default coordinate used by IBM SPSS is the one that corresponds most closely to .5.
The probabilities in the output are not continuous because they match to whole numbers
of cases rather than fractions of cases (e.g., we correctly classify 237 cases as having
graduated rather than 237.2 cases). The ROC output allows us to examine all available
coordinates and select a predicted probability as the potentially more appropriate decision
criterion.
31.2
NUMERICAL EXAMPLE
We use the example from Chapter 30 as our base, with the outcome variable of graduated
and the predictors of sex and family_encouragement. The data ﬁle is named graduation.

ROC ANALYSIS: SETUP
267
31.3
ANALYSIS STRATEGY
We perform the analysis in three stages. First, we run the identical binary logistic regres-
sion analysis described in Chapter 30, except that we now save the predicted probabilities
of target group membership to the data ﬁle; it is these predicted probabilities that the
decision criterion (Classiﬁcation cutoff) refers to. Second, we use these predicted prob-
abilities as a variable in an ROC analysis. Third, we select a different decision criterion
based on the results of the ROC analysis and perform another binary logistic regression
analysis using the revised Classiﬁcation cutoff.
31.4
BINARY LOGISTIC REGRESSION ANALYSIS: DEFAULT
CLASSIFICATION CUTOFF
From the main menu, we select Analyze ➔Regression ➔Binary Logistic to open the
main Logistic Regression window. We conﬁgure the analysis exactly as was done in
Chapter 30 with the following addition. In the Save dialog window (see Figure 31.2),
we check Probabilities in the Predicted Values area. Click Continue to return to the
main dialog window and click OK to perform the analysis.
This analysis resulted in the predicted probability of target group membership being
saved to the data ﬁle as can be seen in Figure 31.3 that shows a portion of the data
ﬁle. IBM SPSS has named this saved variable as PRE_1; this generic name translates to
“predicted values, ﬁrst analysis.”
Recall that the default Classiﬁcation cutoff is .5, and the relationship of PRE_1 to
.5 has driven predicted group membership of the cases. For example, subid 1 with a
predicted target (graduated) group membership of .96425 (a value greater than or equal
to .5) would be (correctly) classiﬁed in the target group, whereas subid 2 with a predicted
target group membership of .17678 (a value less than .5) would be (correctly) classiﬁed
in the reference (did not graduate) group.
The classiﬁcation table from the analysis is shown in Figure 31.1. It is exactly the
same as that obtained in the analysis described in Chapter 30.
31.5
ROC ANALYSIS: SETUP
From the main menu, we select Analyze ➔ROC Curve to open the main ROC Curve
dialog window as shown in Figure 31.4. We move PRE_1 into the Test Variable panel
FIGURE 31.2
The Save dialog window of Logistic Regression.

268
ROC ANALYSIS
FIGURE 31.3 A portion of the data ﬁle showing the saved PRE_1 variable.
FIGURE 31.4
The main ROC Curve dialog window.
(these are the probabilities of target group membership) and graduated into the State
Variable panel (this is the outcome variable). We indicate the code for the target group
by entering 1 in the Value of State Variable panel. In the Display area, we select ROC
Curve to display the curve, With diagonal reference line to include the reference line
that is both traditional and useful, Standard error and conﬁdence interval primarily to
obtain the area under the curve, and Coordinate points of the ROC Curve to obtain
our possible decision criteria. Click OK to perform the analysis.

ROC ANALYSIS: OUTPUT
269
31.6
ROC ANALYSIS: OUTPUT
The ROC Curve is presented in Figure 31.5. Sensitivity (true positive rate) is represented
in the Y-axis, and 1 −Speciﬁcity (false-positive rate) is represented in the X-axis. The
curve is bow shaped, indicating that the logistic function can differentiate the outcome
groups; the diagonal reference line visually depicts the situation where the logistic model
has no differentiating capability. The curve does not show the individual coordinates that
compose it (but those values are in a table provided in the output).
The table below the curve provides the value for the area under the curve. Larger
areas indicate better differentiation by the logistic model. Meyers et al. (2013) provide
interpretation guidelines of areas under the curve (.5’s indicate no discrimination; .6’s,
poor discrimination; .7’s, acceptable/good discrimination; .8’s, very good discrimination;
and .9’s, excellent discrimination); using these guidelines, an area of .851 with a conﬁ-
dence interval of .813–.889 would be considered to represent very good discrimination.
The coordinates of the ROC curve are shown in Figure 31.6. This information
interfaces with the Classiﬁcation Table (see Figure 31.1) in the following way. We
know from the Classiﬁcation Table that 87.1% of the cases in the target group were
correctly classiﬁed. This cell in the Classiﬁcation Table represents true positives and
is called Sensitivity in the ROC analysis. We have marked out the row containing this
value in the table in Figure 31.6 (Sensitivity = .871), and it corresponds to a predicted
FIGURE 31.5
The ROC Curve and the Area Under the Curve.

270
ROC ANALYSIS
FIGURE 31.6
The table presenting the coordinates
of the ROC curve.
probability value of .4912438, which is the actual decision criterion (the nominal cutoff
value of .5 did not correspond to a whole number in the classiﬁcation process).
The value of .871 is one of the coordinates (the contribution of Sensitivity). The
other coordinate is .341 and represents the contribution of 1 −Speciﬁcity (the false-
positive rate). This can also be derived from the Classiﬁcation Table. False positives
are cases in the reference (did not graduate) group who were misclassiﬁed as target
group (graduated) members. From Figure 31.1, we note that 47 of the 138 reference
cases (47 + 91 = 138) were false positives, and 47/138 is .341, the value appearing as
the 1 −Speciﬁcity coordinate.

BINARY LOGISTIC REGRESSION ANALYSIS: REVISED CLASSIFICATION CUTOFF
271
The coordinates table also allows us to evaluate alternative Classiﬁcation cutoffs.
As can be seen in the table, decision criteria near .2 and lower will result in a very high
true positive (Sensitivity) rate, but the rate of false positives (1 −Speciﬁcity) would be
extremely high as well; the reverse pattern may be seen with decision criteria near .94 or
higher. Where researchers place the decision criterion really depends on what the value
they place on the various outcomes. For example, when identifying true positives is a
matter of life and death (perhaps in certain medical research studies) and if a high rate
of false positives can be tolerated, then setting a very low decision criterion may be
appropriate.
For our ﬁctional data set, we are dealing with at-risk kids. Let us assume, even though
our true positive rate is .871, that we want to achieve approximately a .90 hit rate. It
appears from the coordinates table that a predicted probability of .3819184 is associated
with a Sensitivity (true positive) rate of .893 and that a predicted probability of .3527496
is associated with a Sensitivity (true positive) rate of .938; the corresponding rates of
false positives (1 −Speciﬁcity) would then be .464 and .609, respectively. Given the
substantial jump in the false-positive rate, and given that .893 is pretty close to our goal
of .9, we select a Classiﬁcation cutoff of .3819184 for a follow-up logistic regression
analysis.
31.7
BINARY LOGISTIC REGRESSION ANALYSIS: REVISED
CLASSIFICATION CUTOFF
From the main menu, we select Analyze ➔Regression ➔Binary Logistic to open
the main Logistic Regression window. We conﬁgure the analysis exactly as was
done in Chapter 30, with the following exception. In the Options dialog window (see
Figure 31.7), we now replace the default of .5 with a Classiﬁcation cutoff of .3819184
(only a part of which can be seen in the panel). Click Continue to return to the main
dialog window and click OK to perform the analysis.
The results of the analysis produced the identical logistic model to what was achieved
in Chapter 30 and in Section 31.4, and we will not display those results (readers can
FIGURE 31.7
The Options dialog window of Logistic
Regression.

272
ROC ANALYSIS
FIGURE 31.8 The Classiﬁcation Table with the default cut value at .382.
verify this for themselves). The one set of results affected by revising the Classiﬁcation
cutoff is the classiﬁcation portion of the analysis, and this is shown in Figure 31.8.
It can be seen in the table that our projected true positive rate of .893 is precisely
matched in our results. Furthermore, the false-positive rate of .464 is also matched: 64
of 138 is 46.4%. Interestingly, our Overall Percentage of correct classiﬁcation fell only
slightly from 80% previously to 77.3% here. Thus, using this alternative decision thresh-
old, we would be able to identify almost 90% of the students who will likely graduate
based on the prediction model containing sex and the level of family_encouragement
they receive.

C H A P T E R
3 2
Multinominal Logistic Regression
32.1
OVERVIEW
Multinominal logistic regression extends binary logistic regression as covered in Chapter
30 to the situation where there are three or more outcome categories. Predictor variables
can still be any combination of quantitative and binary variables.
Despite the presence of three or more outcome categories, logistic regression still
calls for a binary prediction of group membership (we predict target group membership
with respect to a reference group). This apparent contradiction is accommodated within
the multinomial analysis by designating one of the groups (in the analysis setup) as the
reference group. Each of the other groups serves as a target group and is compared to this
reference group. Thus, with three outcome categories, two separate (binary logistic) sets
of parameter estimates (the raw score coefﬁcients and the odds ratios) are generated, one
contrasting one of the outcomes to the reference category and another contrasting the other
of the outcomes to the reference category. However, in the classiﬁcation portion of the
analysis, all outcome categories are considered together in that classiﬁcation coefﬁcients
are generated for and applied to all groups, with the group achieving the highest score
for each case determining the group to which that case is predicted to belong.
32.2
NUMERICAL EXAMPLE
Our example represents a portion of a data set provided by one of our graduate students,
Kristine Christianson at the California State University, Sacramento; we have modiﬁed
the data set somewhat for the purposes of our example. The data ﬁle is named Kristine
troubled students.
Assume that we have identiﬁed three groups of students (under the outcome vari-
able of group): students who are functioning at a level that is lower than optimal (lower
functioning characterized by them experiencing relatively lower self-esteem and report-
ing relatively poorer interpersonal relationships), coded as 1 in the data ﬁle; students who
appear to be disorganized in their personal and academic lives, coded as 2 in the data
ﬁle; and students who appear to be functioning at a high level (high functioning), coded
as 3 in the data ﬁle. We will designate the high functioning group as the reference group
against which each of the other groups will be contrasted (serving as a target group).
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
273

274
MULTINOMINAL LOGISTIC REGRESSION
Two quantitative variables and one categorical variable will be used as predictors.
One quantitative predictor variable, parental criticism (parent_criticism), represents the
extent to which the students believed that their parents are overly critical of them; scores
range from 1 to 5, with higher scores reﬂecting more (perceived) parent criticism. The
other quantitative predictor variable, a suppressive coping style (suppressive_coping),
represents the tendency to deny problems and to avoid dealing with problems; scores
range from 1 to 5, with higher scores reﬂecting more suppression.
The one categorical predictor variable is a characterization of the students in terms
of the conﬁdence they appear to exhibit; it is named binary_conﬁdence in the data
ﬁle. Students are classiﬁed as either exhibiting high conﬁdence (coded as 0) or low
conﬁdence (coded as 1). This coding schema (intentionally chosen by us even if it seems
as though it should be the reverse) automatically causes the high conﬁdence group (the
group with the lower code, i.e., 0) to be the focus group and the low conﬁdence group
(the group with the higher code, i.e., 1) to be the reference group.
32.3
ANALYSIS SETUP
From the main menu, we select Analyze ➔Regression ➔Multinomial Logistic to
open the main Multinomial Logistic Regression window shown in Figure 32.1. We
move group to the Dependent panel. IBM SPSS® has automatically designated the
last category (ascending, i.e., from lower to higher values) in the coding schema as the
Reference Category.
Although we do want this last category (high functioning, coded as 3 in the data
ﬁle) to serve as our reference category, we select the Reference Category pushbutton
to show that dialog screen; this is presented in Figure 32.2. If we had wished the lowest
coded category to be the Reference Category, we would have selected First Category;
if we had wished any other category to be the Reference Category (the middle category
is the only other one in our example, but with more than three groups, there would be
several to choose from), we would have selected Custom and typed in the group code.
Click Continue to return to the main dialog window.
We now move binary_conﬁdence into the Factors panel because it is a categorical
variable, and we move parent_criticism and suppressive_coping into the Covariate(s)
FIGURE 32.1
The main Multinomial Logistic Regression window.

ANALYSIS OUTPUT
275
FIGURE 32.2
The Reference Category screen of Multinomial Logistic
Regression.
FIGURE 32.3
The main Multinomial Logistic Regression window
now conﬁgured.
panel, as they are quantitative variables. These three variables are now speciﬁed as the
predictors in our logistic regression analysis. This is shown in Figure 32.3.
Selecting the Statistics pushbutton gives us access to the Statistics window. As
shown in Figure 32.4, we have checked Case processing summary, under Model we
have marked the Pseudo R-square, Step summary, Model ﬁtting information, and
Classiﬁcation table. Under Parameters, we have checked Estimates and Likelihood
ratio tests. Click Continue to return to the main dialog window and click OK to perform
the analysis.
32.4
ANALYSIS OUTPUT
The Model Fitting Information table in Figure 32.5 shows the model ﬁt information. The
Final model has a −2 Log Likelihood value of 605.91, which is statistically signiﬁcant
(p < .001). This informs us that we can predict at a better than chance level using our
set of predictors. The Nagelkerke Pseudo R-Square value, as seen in the bottom table

276
MULTINOMINAL LOGISTIC REGRESSION
FIGURE 32.4
The Statistics window of Multinomial Logistic Regression.
FIGURE 32.5
The Model Fitting Information table and Pseudo
R-Square table.
in Figure 32.5, is .282. We interpret this to indicate that we are able to account for
approximately 28% of the variance associated with student group.
Figure 32.6 shows the Likelihood Ratio Tests, presenting the consequences of
removing one of the predictors from the ﬁnal model. The Likelihood Ratio Test is
what results when an effect is removed, and the reduced model is tested for statistical

ANALYSIS OUTPUT
277
FIGURE 32.6
The Likelihood Ratio Tests.
signiﬁcance with a chi-square procedure. Each of the last three rows considers removing
the particular predictor named in the Effect column; hence, each is associated with 2
degrees of freedom because there are two predictors remaining in the model once the
given effect is removed. For example, removing parent_criticism yields a −2 Log Like-
lihood value of 615.30 and with a corresponding chi-square value of 9.388, results in
a statistically signiﬁcant predictive model based on the other two predictors (p = .009).
As can be seen, the model remains statistically signiﬁcant when each of the predictors
is removed in turn.
The results of using the model to predict student group are shown in the Parameter
Estimates table in Figure 32.7. Recall that our reference group was the high functioning
group. Each of the major rows reports the results of a contrast between one of the other
groups and the high functioning group.
The B column provides the raw score coefﬁcients (adjusted for the presence of the
other predictors in the model) associated with each of the predictors, and the standard
FIGURE 32.7 The Parameter Estimates table.

278
MULTINOMINAL LOGISTIC REGRESSION
error (Std. Error) of these statistics is shown next to the coefﬁcients. These partial
regression coefﬁcients are tested for statistical signiﬁcance by means of the Wald test,
and the outcome of these tests is shown in the Sig. column. The odds ratio, which is the
primary part of the output that we ordinarily interpret, is shown as Exp(B).
The ﬁrst major row labeled lower functioning contrasts the lower functioning group
with the high functioning group. The raw score coefﬁcients associated with parent_
criticism and suppressive_coping are both positive and statistically signiﬁcant. The
direction of the effects are reasonable in that it makes sense that students who have
lower levels of self-esteem and who have interpersonal difﬁculties would tend to more
strongly believe that their parents were overly critical and would tend to deny problems
and avoid dealing with problems more so than students who are functioning well (these
coefﬁcients and odds ratios for a given group are interpreted with respect to the reference
group).
The odds ratios, adjusted for the other predictors in the model, yield an interpretation
of the dynamics of these predictors. For example, parent_criticism is associated with
an adjusted odds ratio of 1.544; this informs us that an increase of 1 scale point in the
parent_criticism measure increases the odds of a student being in the lower functioning
group by 1.544 versus the odds of being in the high functioning group, controlling
for the other predictors. By the same token, suppressive_coping is associated with an
adjusted odds ratio of 3.294; this informs us that an increase of 1 in the suppressive_
coping measure increases the odds of a student being in the lower functioning group by
3.294 versus the odds of being in the high functioning group, controlling for the other
predictors.
The results of our categorical binary_conﬁdence predictor are shown in the bottom
two rows of the lower functioning portion of the Parameter Estimates table. Two rows
are used, as IBM SPSS explicitly acknowledges both of the binary codes, but only the
row for binary_conﬁdence = .00 shows viable values. This reminds us that the code
of 0 represents the focus group and that the code of 1 represents the reference group;
values are associated with the focus group in the output.
We note that the partial raw regression coefﬁcient is negative, and this is one way
for researchers to reassure themselves that the coding schema is in accord with their
intentions. Here, we would expect that more conﬁdent students (coded as 0 to designate
them as the focus group) are less likely to be in the lower functioning group than in the
high functioning group; thus, the negative coefﬁcient is reasonable. The adjusted odds
ratio of .511 reinforces this assessment. We interpret the odds ratio as indicating that
the odds for high conﬁdence students (the focus group) to be in the lower functioning
group are about half the odds for them to be in the high functioning group, controlling
for the other predictors.
The second major row in Figure 32.7 labeled as disorganized contrasts the disor-
ganized group with the high functioning group. As was true for the previous analysis,
the raw score coefﬁcients associated with parent_criticism and suppressive_coping are
both positive and statistically signiﬁcant. The adjusted odds ratio for parent_criticism
is 1.162, informing us that an increase of 1 in the parent_criticism measure increases
the odds of a student being in the disorganized group by 1.162 versus the odds of being
in the high functioning group, controlling for the other predictors.
The adjusted odds ratio for suppressive_coping is 2.901. This informs us that an
increase in 1 in the suppressive_coping measure increases the odds of a student being in
the disorganized group by almost three times the odds of being in the high functioning
group, controlling for the other predictors.
The results for our categorical binary_conﬁdence predictor are shown in the bottom
two rows of the disorganized portion of the Parameter Estimates table. We note that
the partial raw regression coefﬁcient is negative here as well, an outcome that makes
sense in that we would probably expect that more conﬁdent students are less likely to

ANALYSIS OUTPUT
279
FIGURE 32.8 The Classiﬁcation table.
be in the disorganized group than in the high functioning group. The adjusted odds
ratio of .575 reinforces this assessment. The odds ratio indicates that the odds for high
conﬁdence students (the focus group) to be in the disorganized group are .575 the odds
for lower conﬁdence students to be in the high functioning group, controlling for the
other predictors.
The Classiﬁcation Table is also shown in Figure 32.8. It displays how well the model
classiﬁes cases into the three categories of the outcome variable. Overall, the predictive
accuracy is 56.3%, but that percentage differed considerably across the student groups.
Those in the high functioning group were most accurately predicted (84.3%), those in
the lower functioning group were next most accurately predicted (42.9%), and those in
the disorganized group were most poorly predicted (9.0%).


P A R T 10
SURVIVAL ANALYSIS


C H A P T E R
3 3
Survival Analysis: Life Tables
33.1
OVERVIEW
Chapters 33–35 examine a family of techniques variously labeled as survival analysis,
failure analysis, or event history analysis that present the analysis of Life Tables, the
Kaplan–Meier (or product-limit) method, and the Cox Regression method. Survival anal-
ysis (the generic term we will use throughout these chapters) examines the time interval
between two events. These events can range from mundane (e.g., the starting and stop-
ping of a newspaper subscription, purchasing and replacing a tire) to important events
(e.g., onset of a disease to death, birth to ﬁrst sexual experience, release from jail to
arrest or conviction for another crime).
At least two factors complicate the examination of these time intervals. First, the
event of primary interest (e.g., ﬁrst sexual experience, being convicted of another crime)
does not occur for all cases during the study period; for example, not all of the adolescents
under study become sexually active during the study time frame. Second, the period of
observation can vary from one case to another; for example, over a 3-year period, tire
purchases can vary from 1 to 36 months.
Owing to these complicating factors, simply calculating a mean or median time
between the occurrences of two events is not sufﬁcient to glean the useful information
from the study. Instead, two primary types of survival analyses have evolved to examine
duration data. One approach is to use actuarial (Merrell, 1947) or product-limit (i.e.,
Kaplan–Meier) life tables that partition a period into smaller time intervals, such as
weeks, months, or years (Kaplan & Meier, 1958; Kleinbaum, 1996; Lee & Wang, 2003;
Norusis, 2012). Probabilities can then be estimated for the occurrence of an event at a
given interval. The life table method is presented in this chapter, and the Kaplan–Meier
method is described in Chapter 34. A second approach, the Cox regression or Cox
proportional hazards model (Cox, 1972; Cox & Oakes, 1984; Singer & Willett, 1993,
2003; Tekle & Vermunt, 2012), attempts to predict survival time from covariates or
independent variables; this method is presented in Chapter 35.
33.2
NUMERICAL EXAMPLE
The present example examines a hypothetical data set for evaluating how long taekwondo
students continue to enroll (survive) in martial arts training as measured in months. Three
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
283

284
SURVIVAL ANALYSIS: LIFE TABLES
variables are provided in this data set. Students are each given an identiﬁcation number
under the variable student. The variable months_in_course represents the number of
months a student was enrolled in taekwondo training classes (until they dropped out)
during an 18-month block of time; thus, students will have been enrolled in the training
class for different amounts of time. Whether the students represented a drop out (coded
as 0) or were still attending (coded as 1) is provided by the variable survival_status;
such a variable is often thought of as a censorship variable. Observations are censored
when we have incomplete information about the situation, and this can happen under
several different circumstances. One such circumstance is when a case does not exhibit
or experience the event of interest. In the present example, the event of interest is dropping
out of the class. Thus, a code of 1 representing continuing enrollment is thought of as
indicating (perhaps anti-intuitively) a censored event or observation and a code of 0
representing withdrawal from the class is thought of as indicating an uncensored event
or observation. The data can be found in the ﬁle named taekwondo training.
33.3
ANALYSIS SETUP
Open taekwondo training and from the main menu select Analyze➔Survival➔Life
Tables; this produces the Life Tables main dialog window shown in Figure 33.1. We
have moved months_in_course to the Time panel at the top right portion of the dialog
window. It is also necessary to specify the period over which we wish to perform the
analysis. For the Display Time Intervals panel, we have entered 0 through 18 by 1 to
request that the ﬁrst 18 months are displayed one month at a time.
We have moved survival_status into the Status panel. Upon doing so, the expression
(? ?) appears next to the variable, as we need to supply to IBM SPSS® the code or codes
representing the occurrence of the event of primary interest. Selecting the Deﬁne Event
pushbutton produces the Deﬁne Event for Status Variable dialog window shown in
Figure 33.2. In this relatively simple data ﬁle, the code of 0 for the variable survival_
status identiﬁes drop out, the event of primary interest. We therefore select Single value
and type 0 in that panel. After clicking Continue, we see a (0) after the variable name
survival_status in the Status panel (Figure 33.3).
FIGURE 33.1
The main dialog window for Life Tables.

ANALYSIS SETUP
285
FIGURE 33.2
The Deﬁne Event for Status Variable window for Life Tables.
FIGURE 33.3
The main dialog window for Life Tables with
survival_status now deﬁned.
FIGURE 33.4
The Options window for Life Tables.
Selecting the Options pushbutton opens the Options dialog window shown in
Figure 33.4. We check Life table(s) and Survival to obtain the cumulative survival
function. We do not have different groups of cases, and so Compare Levels of First
Factor options are not available. Select Continue to return to the main dialog window
and click OK to perform the analysis.

FIGURE 33.5 The Life Table output.
286

ANALYSIS OUTPUT
287
33.4
ANALYSIS OUTPUT
The Life Table is shown in Figure 33.5. This table consists of 13 columns of summary
calculations. Each row is a 1-month period; the particular temporal period or interval
is shown under Interval Start Time. The intervals always start at Time 0 and include
the period up to 1 month. The second interval starts at 1 month but less than 2 months,
and so on for 18 months. In our analysis setup, we instructed IBM SPSS to increment
in 1-month intervals over the course of 18 months. With extended periods in their data,
researchers could opt to increment over longer time intervals.
The Number Entering Interval lists the number of cases who continue or survive
to the beginning of the current interval. We can see that 20 students (who comprised this
example) continued for at least 2 months. At the 3-month interval, this number dropped
to 18 indicating that two students did not survive till the end of the 3-month interval
because they stopped coming to class or had only been with the taekwondo studio for
less than 3 months.
The Number Withdrawing during Interval represents the number of cases who
enter the interval minus one-half of those who withdrew during the interval. The Number
Exposed to Risk estimates the effective sample size or how many cases were observed
during the entire interval. For example, at the start of the second month interval, we have
(20 −(0.5 × 1) = 19.5. An assumption is made that when a case withdraws, it does so
at the midpoint of the interval; hence, each case is weighted by one-half.
The Number of Terminal Events represents the number of cases for which the
alternative or nonsurvival event (dropout) occurs within the interval. For example, at the
start of the second month, we have one student dropping out of taekwondo training.
The Proportion Terminating represents the probability that nonsurvival (dropout)
will occur during the interval for any case surviving to the beginning of that interval.
For example, for the 16 cases entering the start of the seventh month of training, the
probability that a student who has completed 7 months of training will quit at 8 months
is .06, or 1/16 (the number of terminal events/number exposed to risk). The Propor-
tion Surviving is calculated as 1 −proportion terminating. This value represents the
probability that a person who enters an interval will leave the interval without the event
(dropout) occurring. For example, for the 10-month interval we have 1 −.06 = .94.
FIGURE 33.6
The Survival Function plot.

288
SURVIVAL ANALYSIS: LIFE TABLES
The Cumulative Proportion Surviving at End of Interval estimates the probability
of surviving till the end of an interval. For example, the cumulative proportion of students
“surviving” through the end of their twelfth month of martial arts instruction is .84
or 84%. This ﬁgure is often reported in survival analysis reports. The Std. Error of
Cumulative Proportion Surviving at End of Interval provides a basis to estimate a
conﬁdence interval for this cumulative proportion.
The Probability Density function estimates the probability of a case experienc-
ing nonsurvival (dropout) within the interval. The Std. Error of Probability Density
estimates the variability of the density function. The Hazard Rate measures the instan-
taneous rate of change (dropout) at a speciﬁc time for cases surviving to the start of the
interval. By multiplying the hazard rate by a small time increment, an approximation
of nonsurvival (dropout) is provided for that speciﬁc time duration. The hazard rate is
modeled in the Cox regression procedure (see Chapter 35). The Std. Error of Hazard
Rate estimates the variability of the hazard rate.
IBM SPSS reports the median survival time as a footnote to its Life Table. In the
present example, the median survival time is 17.50 months. This represents the point in
time at which half the cases fail to survive (dropout).
The Survival Function or life table plot is shown in Figure 33.6. This plot is a
visual depiction of the Cumulative Proportion Surviving at End of Interval column
from the Life Table for each time interval. The cumulative proportion is represented on
the Y-axis, and the time interval in months is represented on the X-axis.

C H A P T E R
3 4
The Kaplan–Meier Survival Analysis
34.1
OVERVIEW
This second chapter in our survival analysis trilogy examines the Kaplan–Meier (or
product-limit) method of producing life tables and survival functions. The Kaplan–Meier
procedure (similar to the actuarial Life Tables procedure) provides proportions of cases
surviving at various time intervals, survival functions, and tests of group differences.
While the Life Tables procedure is often used with large sample sizes, the Kaplan-
Meier procedure is typically the method of choice, particularly with smaller sample
sizes.
34.2
NUMERICAL EXAMPLE
The present example examines an extension of the hypothetical taekwondo student enroll-
ment data from Chapter 33. Five variables are provided in this data set contained in the
data ﬁle named taekwondo training K-M. Students are each given an identiﬁcation num-
ber under the variable student. The variable months_in_course represents the number
of months a student was enrolled in taekwondo training classes (until they dropped out)
during an 18-month period; thus, students will have been enrolled in the training class
for different amounts of time. The variable Survival_status is an indicator of censorship,
with students deﬁned as still attending (coded as 1, a censored event) or as a drop out
(coded as 0, an uncensored event). Student gender is coded as 1 for male and 2 for
female under sex_of_student. Three different levels of instructors are also represented
under instructor_level: black belt coded as 1, master coded as 2, and grand master coded
as 3.
34.3
ANALYSIS STRATEGY
In the previous Life Tables analysis (Chapter 33), we demonstrated how to estimate a
cumulative survival function for one group of taekwondo students. In this chapter, we
demonstrate how to compare survival functions for several groups (males and females)
of taekwondo students. An additional set of analyses will examine survival functions by
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
289

290
THE KAPLAN–MEIER SURVIVAL ANALYSIS
student gender, which is nested or stratiﬁed by the student’s type of instructor (black
belt, master, and grand master).
34.4
ANALYSIS SETUP: COMPARING MALES AND FEMALES
Open the data ﬁle taekwondo training K-M and from the main menu select Analyze
➔Survival ➔Kaplan-Meier; this produces the Kaplan-Meier main dialog window
shown in Figure 34.1.
We have moved months_in_course to the Time panel and survival_status to the
Status panel. When a variable is entered into the Status panel, the Deﬁne Event push-
button is activated. Clicking Deﬁne Event opens the Deﬁne Event For Status Variable
window (see Figure 34.2). We enter 0 in the Single value panel to indicate that drop
out is the event of interest.
Clicking Continue returns us to the main dialog window where we ﬁnd that IBM
SPSS® has placed a (0) after the variable survival_status in the Status panel (see
Figure 34.3). If we wish to compare survival functions of two or more groups, we spec-
ify that variable as a Factor; it often but not always represents some causal treatment
effect. For this example, we have moved the categorical variable sex_of_student into
FIGURE 34.1
The main Kaplan-Meier dialog window ready to
deﬁne survival_status.
FIGURE 34.2
The Deﬁne Event For Status Variable window.

ANALYSIS OUTPUT: COMPARING MALES AND FEMALES
291
FIGURE 34.3
The main Kaplan-Meier dialog window.
FIGURE 34.4
The Compare Factor Levels screen of Kaplan-Meier.
the Factor panel; this action has activated the Compare Factor pushbutton at the top
right of the dialog window as shown in Figure 34.3. Clicking this pushbutton produces
the Compare Factor Levels screen shown in Figure 34.4. We have activated the Log
rank, Breslow, and Tarone–Ware options in the Test Statistics panel and also selected
the Pooled over strata option in the lower panel. Clicking Continue returns us to the
main dialog window.
From the main dialog window, clicking the Options pushbutton produces the
Options dialog window as shown in Figure 34.5. We have retained the defaults in
the Statistics panel: Survival table(s) and Mean and median survival, and we have
activated Survival in the Plots panel. Click Continue to return to the main dialog
window and click OK to perform the analysis.
34.5
ANALYSIS OUTPUT: COMPARING MALES AND FEMALES
Figure 34.6 provides the Case Processing Summary table that depicts the total number
of cases for each sex_of_student group broken down by the observed number of events
(N of Events) and the censored count. The event of interest is drop out, and there
were six and seven of them for males and females, respectively; the censored event
was still attending, and there were 24 males and 13 females. The last column provides
the Percent of Censored observations showing that 80% of the males and 65% of the
females remained in the course, and the number of events (or dropouts) for each group.

292
THE KAPLAN–MEIER SURVIVAL ANALYSIS
FIGURE 34.5
The Options window of Kaplan-Meier.
FIGURE 34.6
Case Processing Summary.
Female students at 35% were more likely to be censored (still enrolled) than were males
at 20%.
Figure 34.7 presents the Kaplan–Meier Survival Table output. This table’s seven
columns of summary calculations are considerably more abbreviated than the 13-column
Life Table we encountered in Chapter 33. The ﬁrst column labeled Student Gender
partitions the male and female cases in ascending order based on their individual survival
time. Each case occupies its own row in the table. The second column labeled Time
provides each case’s survival time in months. The third column named Status indicates
whether a case is censored (still attending class) or uncensored (having experienced the
event of dropping out of the class).
Columns four and ﬁve provide an estimate and standard error of the Cumulative
Proportion Surviving at the Time. This value provides the proportion of cases that have
not reached the terminal event by the end of the interval. For example, approximately
96% of the males (1/30) remained in the course after the second month. The sixth column
labeled N of Cumulative Events reports the number of cases that have experienced the
terminal event (drop out) from time 1 until a particular time. The last column labeled
N of Remaining Cases indicates the number of cases at that particular time who have
not experienced the terminal event (drop out).
Figure 34.8 reports the mean and median survival time for male and female taek-
wondo students. We note that the median survival time for females is 17.
Figure 34.9 presents the Overall Comparisons table that provides three overall
(omnibus) tests of the equality of survival times across groups. The Log Rank
(Mantel–Cox) tests the equality of survival functions by weighting all time points
equally. The Breslow (Generalized Wilcoxon) tests the equality of survival functions
by weighting all the points by the number of cases at risk at each point in time. Lastly,
the Tarone–Ware tests the equality of survival functions by weighting all the points by
the square root of the number of cases at risk at each point in time.

ANALYSIS OUTPUT: COMPARING MALES AND FEMALES
293
FIGURE 34.7 The Survival Table for individual male and female students.

294
THE KAPLAN–MEIER SURVIVAL ANALYSIS
FIGURE 34.8
Means and Medians for Survival Time.
FIGURE 34.9
Overall Comparisons based on the three tests of
statistical signiﬁcance of the equality of survival
times across groups.
We recommend following the results of the log rank test, as it is associated with
greater statistical power under most circumstances (see Norusis (2012) and Prentice &
Marek (1979)), but in our example, the signiﬁcance values of all three tests are all greater
than .05. We therefore conclude that there is not a statistically signiﬁcant difference
between male and female survival time.
These ﬁndings are reinforced in the Survival Functions shown in Figure 34.10.
This plot gives a visual representation of the life tables. The horizontal axis represents
the number of months enrolled in a taekwondo class or time to event. The vertical axis
shows the probability of survival or not dropping out of taekwondo instruction.
34.6
ANALYSIS SETUP: COMPARING MALES AND FEMALES
WITH STRATIFICATION
In this example, we will perform the same analysis as before, except that we now
include a stratiﬁcation variable of instructor_level. This analysis will compare the two
sex_of_student groups over strata or each of the three levels of instructor_level. Open
the data ﬁle taekwondo training K-M and from the main menu select Analyze ➔
Survival ➔Kaplan-Meier; this produces the Kaplan-Meier main dialog window shown
in Figure 34.11. As we did for the previous analysis, we have moved months_in_course
to the Time panel, survival_status to the Status panel (and set Deﬁne Event For
Status Variable to 0), and sex_of_student to the Factor panel. In the Strata box, we
have moved instructor_level to enable us to examine the two genders within each of
the instructor types.
As shown in Figure 34.12, in the Compare Factor Levels screen, we have activated
the Log rank, Breslow, and Tarone-Ware options in the Test Statistics panel and
selected the Pooled over strata option in the lower panel. We will activate (in separate
runs) four of the factor level options: Pooled over strata, For each stratum, Pairwise
over strata, and Pairwise for each stratum to demonstrate the output for each option,
with this being the ﬁrst run. Note that each option must be run in separate IBM SPSS
Kaplan–Meier analyses.

ANALYSIS OUTPUT: COMPARING MALES AND FEMALES WITH STRATIFICATION
295
FIGURE 34.10
Survival Functions of male and female students.
In the Options dialog window, shown in Figure 34.13, we have activated only the
Survival checkbox in the Plots panel (to simplify the output). Clicking Continue returns
us to the main dialog window and clicking OK performs the analysis.
34.7
ANALYSIS OUTPUT: COMPARING MALES AND FEMALES
WITH STRATIFICATION
Figure 34.14 displays the Case Processing Summary that partitions the 50 cases among
the two levels of Sex and three levels of Instructor. The levels of the Factor vari-
able (sex_of_student) are tabulated within each level of the Strata variable (instruc-
tor_level), and so subtotals (in the Overall rows) are provided only for each instruc-
tor_level.
Figure 34.15, Figure 34.16, Figure 34.17, and Figure 34.18 are Test Statistics output
based on four separate IBM SPSS Kaplan-Meier runs where we activated (one at a time)
one of the four Compare Factor Levels options, namely, Pooled over strata, Pairwise
over strata, For each stratum, and Pairwise for each stratum, to give a sense of what
the output looks like.
Figure 34.15 presents the Overall Comparisons when the Compare Factor Lev-
els option is set at Pooled over strata. The three statistical tests (Log Rank, Breslow,
Tarone–Ware) test for the equality of survival functions for the two levels of sex_of_s-
tudent, but the results are different from our earlier analysis (see Figure 3.9) because they
have been adjusted for the Strata variable instructor_level. None of the tests reached

296
THE KAPLAN–MEIER SURVIVAL ANALYSIS
FIGURE 34.11
The main Kaplan-Meier dialog window
set up with a Strata variable.
FIGURE 34.12
The Compare Factor Levels screen of Kaplan-Meier.
FIGURE 34.13
The Options window of Kaplan-Meier.

ANALYSIS OUTPUT: COMPARING MALES AND FEMALES WITH STRATIFICATION
297
FIGURE 34.14
Case Processing Summary.
FIGURE 34.15
Overall Comparisons based on the three tests of
statistical signiﬁcance of the equality of survival
times across sex_of_student groups adjusted for
instructor_level.
FIGURE 34.16 The adjusted Pairwise Comparisons of males and females based on the three
tests of statistical signiﬁcance of the equality of survival times collapsed across instructor_level.

298
THE KAPLAN–MEIER SURVIVAL ANALYSIS
FIGURE 34.17
Overall Comparisons of sex_of_student based on three tests of statistical sig-
niﬁcance of the equality of survival times for each instructor_level.
FIGURE 34.18 Pairwise Comparisons of males and females based on the three tests of statistical
signiﬁcance of the equality of survival times for each instructor_level.

ANALYSIS OUTPUT: COMPARING MALES AND FEMALES WITH STRATIFICATION
299
FIGURE 34.19
Survival Functions of male and female students for each
instructor_level.

300
THE KAPLAN–MEIER SURVIVAL ANALYSIS
statistical signiﬁcance (p > .05), indicating overall equivalence of survival functions for
males and females.
Figure 34.16 presents the Pairwise Comparisons when the Compare Factor Levels
option is set at Pairwise over strata. Here the three statistical tests are divided separately
for males and females, none of which achieved statistical signiﬁcance (p > .05).
Figure 34.17 provides the Overall Comparisons when the Compare Factor Levels
option is set at For each stratum. Here the different levels of Sex survival functions
are compared within each instructor_level, none of which were statistically signiﬁcant
(p > .05).
Figure 34.18 presents the Pairwise Comparisons when the Compare Factor Levels
option is set at Pairwise for each stratum. Here Sex is compared at each level of
Instructor for the three statistical tests for male and female students. This is the most
comprehensive (microscopic) analysis of the set. Again, all tests were not statistically
signiﬁcant (p > .05).
Figure 34.19 presents the Kaplan–Meier Survival Functions for male and female
students at each instructor_level. The functions look somewhat different partially
because the students are at different proﬁciency levels but the relative small sample
sizes in some of the cells make those frequencies a good deal less stable bases for
projecting survival rates.

C H A P T E R
3 5
Cox Regression
35.1
OVERVIEW
This ﬁnal chapter in our survival analysis trilogy examines the Cox regression procedure
(or Cox proportional hazards model), as it pertains to time interval data that contain
censored observations. It was proposed by Cox (1972) to predict the occurrence of an
event of interest at a particular time. This procedure is useful for modeling the time to a
speciﬁed event (e.g., drop out of class). The Cox regression procedure assumes that the
independent or predictor variables (called covariates by IBM SPSS®) are time-constant
(i.e., they do not vary as a function of time). IBM SPSS also offers a separate procedure
for Cox regression with a Time-Dependent Covariate that we do not cover.
The model developed by Cox (1972) can be formulated in terms of a cumulative
survival function (i.e., the proportion of cases surviving to a particular time point) and
the hazard function or ratio (i.e., the rate per unit time a case will experience the event
given it survived to that point). Due in part to the pedagogical parsimony of the hazard
function, the Cox model is typically expressed in hazard form as the Cox proportional
hazards model (see Norusis (2012)).
The key ingredients for IBM SPSS Cox Regression are the following: a dichoto-
mously coded Status variable that serves as the dependent measure; a Time variable
(either continuous or categorical) that assesses the duration to the event deﬁned by the
Status variable; and Covariates, independent or predictor variables that may be either
categorical or continuous. Interaction terms can also be used. Categorical covariates with
three or more levels are automatically converted into a set of dummy variables, with one
level being used as a reference category.
35.2
NUMERICAL EXAMPLE
The present example further extends our hypothetical taekwondo student enrollment data
from Chapter 34. Six variables are provided in this data set contained in the data ﬁle
named taekwondo training CR. Students are each given an identiﬁcation number under
the variable student. The variable months_in_course represents the number of months
a student was enrolled in taekwondo training classes (until they dropped out) during
an 18-month period; thus, students will have been enrolled in the training class for
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
301

302
COX REGRESSION
different amounts of time. The variable Survival_status is an indicator of censorship,
with students deﬁned as still attending (coded as 1, a censored event) or as a drop
out (coded as 0, an uncensored event). Student gender is coded as 1 for male and 2 for
female under sex_of_student. Three different levels of instructors are also represented
under instructor_level: black belt coded as 1, master coded as 2, and grand master coded
as 3. The ﬁnal variable is age and refers to the age of the student in years.
35.3
ANALYSIS SETUP
Open the data ﬁle named taekwondo training CR and from the main menu select
Analyze ➔Survival ➔Cox Regression; this produces the Cox Regression main dialog
window as shown in Figure 35.1.
We have moved months_in_course to the Time panel and survival_status to the
Status panel. When a variable is entered into the Status panel, the Deﬁne Event push-
button is activated. Clicking Deﬁne Event opens the Deﬁne Event for Status Variable
window (see Figure 35.2). We enter 0 in the Single value panel to indicate that drop
out is the event of interest.
FIGURE 35.1
The main Cox Regression dialog window ready to
deﬁne survival_status.
FIGURE 35.2
The Deﬁne Event for Status Variable window.

ANALYSIS SETUP
303
FIGURE 35.3
The main Kaplan-Meier dialog window with
the Covariates speciﬁed.
Clicking Continue returns us to the main dialog window where we ﬁnd that IBM
SPSS has placed a (0) after the variable survival_status in the Status panel (see
Figure 35.3). In the Covariates panel, we have clicked over our independent or pre-
dictor variables (sex_of_student, instructor_level, and age). We retain the default of
Enter in the Method drop-down menu to produce a standard regression solution.
Selecting the Categorical pushbutton produces the Deﬁne Categorical Covariates
dialog window (see Figure 35.4). We have moved instructor_level and sex_of_student
to the Categorical Covariates panel because they are categorical variables. As can be
seen at the bottom of the window, IBM SPSS uses as a default the last (highest numeric
code) as the reference category in its dummy coding operation (relevant for variables
with three or more categories). We leave that as the default (highlighting the variable
activates the choice, thus allowing us to modify it). Age remains in the Covariates panel,
FIGURE 35.4
The Deﬁne Categorical Covariates window of Cox
Regression.

304
COX REGRESSION
FIGURE 35.5
The Plots window of Cox Regression.
as it is a continuous variable. Clicking Continue brings us back to the Cox Regression
main dialog window.
Clicking the Plots pushbutton produces the Plots dialog window shown in
Figure 35.5, where we have activated Survival in the Plot Type panel. In the panel
labeled Covariate Values Plotted at, we have moved Instructor_level (Cat) (Mean)
to the Separate Lines for panel, which will produce separate survival functions for
each level of Instructor. Clicking Continue returns us to the Cox Regression main
dialog window and clicking OK performs the analysis.
35.4
ANALYSIS OUTPUT
Figure 35.6 provides the Case Processing Summary table that provides the number
of cases that experienced the drop out event (N = 13), the number of censored (still
attending) cases (N = 35), and the number of cases with missing values (N = 0).
FIGURE 35.6
Case Processing Summary.

ANALYSIS OUTPUT
305
FIGURE 35.7
Categorical Variable Codings.
Figure 35.7 provides the Categorical Variable Codings table that depicts the dummy
coding schema used by IBM SPSS for all of the categorical variables. Brieﬂy (see Meyers
et al. (2013) for a more complete description), dummy coding involves assigning 1’s and
0’s to the categories of a variable, with the number of code sets equal to one less than
the number of categories. Dummy coding requires a reference category that is assigned
the code 0; we have accepted the default of Last category (see Figure 35.4). Categories
designated as the reference category have each of the other groups compared to it in the
regression analysis.
For sex_of_student (whose coding of 1 and 2 can be seen in the Categorical Vari-
able Codings table), female has the highest code value (with 2) and thus becomes the
reference category as shown in the column labeled (1) for the ﬁrst (and only) set of
dummy codes. Males will therefore be compared to females in the regression analysis.
Our instructor_level variable has three categories, also shown in the Categorical
Variable Codings table. With the code 3, grandmaster becomes the reference category;
this has caused it to be assigned 0 in both coding sets, with blackbelt and master each
assigned a dummy code 1 in one of the two dummy code sets. Because grandmaster is
the reference category, blackbelt and master will each be compared to it in the analysis.
Figure 35.8 presents the initial Block 0: Beginning Block analysis. This presents the
Omnibus Tests of Model Coefﬁcients, which is realized with the −2 Log Likelihood
statistic of 73.398. The −2 Log Likelihood assesses Block 0 where all covariates are yet
to be entered into the analysis; hence, it plays a similar role as the total sum of squares
does in the analysis of variance. All future blocks of covariates entered into the model
are assessed on the basis of the reduction of this statistic.
Figure 35.9 provides the output for Block 1: Method = Enter. The Method = Enter
heading reminds us that we have elected to enter our covariates (four in the present case)
simultaneously. The Omnibus Tests of Model Coefﬁcients table leads off with a −2
Log Likelihood value 63.018. This likelihood ratio statistic is compared to the previous
(Block 0) value, and its reduction is assessed with chi-square statistic with four degrees
of freedom (due to the four covariates). In the portion of the table labeled as Change
From Previous Step, the Chi-square value is 10.38. It is calculated as the difference
in −2 Log Likelihood from Block 0 to Block 1 (73.398 −63.018 = 10.38). The chi-
square value, which is statistically signiﬁcant (p = .034), indicates that our four covariates
signiﬁcantly affect the probability of dropping out of taekwondo class (the hazard rate).
FIGURE 35.8
Block 0 Omnibus Test output.

306
COX REGRESSION
FIGURE 35.9 The Omnibus regression model with the covariates entered.
FIGURE 35.10 The table of regression coefﬁcients.
Figure 35.10 provides the Variables in the Equation table. This table presents
the information for each covariate on separate rows. The column labeled B reports the
unstandardized regression coefﬁcients. Analogous to multiple regression, the unstandard-
ized coefﬁcients need be standardized so that we may compare the relative contributions
of the covariates; this standardization is accomplished in survival analysis as a log trans-
formation shown in the Exp(B) column.
The next four columns display the standard error of B (shown as SE), the outcome of
the Wald test of statistical signiﬁcance, the degrees of freedom (df), and the signiﬁcance
value of the coefﬁcient (Sig.). The test used to evaluate the statistical signiﬁcance of
each covariate is the Wald statistic; it is distributed as a chi-square distribution and is
computed as (B/SE)2 (Katz, 2006).
The only statistically signiﬁcant effect is the instructor_level covariate, a categor-
ical variable with three levels. The instructor_level row indicates there is a signiﬁcant
overall effect (p = .031), indicating that at least one of the instructor_level dummy
coded variables is statistically signiﬁcant (at least one signiﬁcantly predicted the event of
interest). Examining the two dummy instructor_level variables further informs us that
only instructor_level(1) was statistically signiﬁcant (p = .010). Referring to Figure 35.7,
we can see in the Categorical Variable Codings table that this dummy coded variable
is represented by blackbelt compared to grandmaster.
The column labeled Exp(B) presents the results of the log transformations of the
regression coefﬁcients. It is known as the hazard ratio. Values at or near 1.00 indicate no
signiﬁcant change in the chances of observing the event of interest, with changes in the
predictor variable. For a quantitative covariate as an example, the regression coefﬁcient
for age is .014. On the basis of the natural logarithm transformation, we compute Exp(B)
as 2.72.014 = 1.014. Exp(B) is the predicted change in the hazard ratio for every metric
increase in the covariate while controlling for all the other covariates in the model. With
the Exp(B) for age being 1.014, we can say that the hazard ratio increases 1.014 for
every increase in years; thus, for example, a 5-year increase equals a 5.07 hazard ratio
while controlling for all the other covariates.
The Exp(B) value associated with instructor_level(1) is 23.274. This was one part
of a dummy coded variable contrasting blackbelt with the reference group grandmaster.

ANALYSIS OUTPUT
307
We interpret this effect as follows: students taught by blackbelt instructors were 23.274
(95% conﬁdence interval: 2.12–255.10) times more likely to drop out than those taught
by grandmasters.
There were no signiﬁcant differences in drop out rate for instructor_level(2) (the
master vs. grandmaster dummy variable); thus, the dropout rates associated with these
two types of instructors were comparable. Note that if we computed the 95% conﬁdence
interval for the instructor_level(2) regression coefﬁcient, it would subsume the value of
1.00, thus suggesting that master and grandmaster instructors are comparable in terms
of their drop out rate.
Figure 35.11 provides the Covariate Means and Pattern Values table. This table
displays the average value of each covariate or predictor variable. instructor_level(1)
represents the ﬁrst dummy variable for instructor_level and has a mean of .354. This
variable was coded 1 for black belt instructor and 0 for all other instructors. The second
dummy variable, instructor_level(2), was coded 1 for master instructors and 0 for all
other instructors and its mean was .271. The mean age was 20.896, and the sex_of_
student had a mean of .583. The columns labeled Pattern 1, 2, 3 respect the means for
age and sex_of_student and provide the coding scheme for the dummy variables (1,0),
(0,1) (0,0). These are used to produce the graph in Figure 35.13.
Figure 35.12 provides a single survival graph for all covariates together. The function
is calculated on the basis of the mean of all covariates.
FIGURE 35.11
Means of the variables.
FIGURE 35.12
Survival Function at the mean of the
predictors.

308
COX REGRESSION
FIGURE 35.13
Survival functions by level of instructor.
Figure 35.13 depicts the survival functions for each instructor_level. Notice the
higher rates of drop out associated with black-belt-level instruction as compared to
master-level or grand-master-level instruction; this is why this variable was a statistically
signiﬁcant predictor in the regression analysis.

P A R T 11
RELIABILITY AS A GAUGE
OF MEASUREMENT
QUALITY


C H A P T E R
3 6
Reliability Analysis:
Internal Consistency
36.1
OVERVIEW
Over a century ago, Spearman (1904a, 1904b, 1907) suggested that the observed results
of our measurement operations contain a mixture of both the true value of the construct
and measurement error. Thus began an approach to the nature and quality of measurement
known as the Classical Test Theory.
In the prototypical testing situation, we have developed a set of test items to
measure some construct (e.g., an academic domain of knowledge, attitudes toward a
health care organization, a personality characteristic) and have collected responses to
these items from a sample of respondents. The issue facing us is to determine if these
items (or which subset of these items) can be combined to form a scale that measures
the construct with an acceptable degree of quality (i.e., with a minimally acceptable
amount of measurement error).
One result of the quest to develop procedures that assess measurement quality (see
Nunnally & Bernstein (1994) and Traub (1997)) has been the development of the reli-
ability coefﬁcient. A reliability coefﬁcient theoretically represents the proportion of true
score variance present in the total variance (true score plus error variance) of test scores
(Lord & Novick, 1968; Nunnally & Bernstein, 1994). Reliability ranges between .00
and 1.00, with .00 indicating that none of the observed variance is due to true score
variance (all of the observed variance is due to measurement error) and 1.00 indicating
that observed scores are composed only of true score variance (there is no measurement
error). All measurement operations ultimately serve the goal of achieving validity (allow-
ing us to draw appropriate inferences from our measures), and a critical requirement to
achieve validity is to measure our construct in a manner that is relatively free of mea-
surement error, that is, to have a relatively reliable measurement procedure (Pedhazur &
Schmelkin, 1991).
Because we cannot directly measure the true score (as it is a theoretical entity and
is presumably embedded in error variance), we need to gauge reliability indirectly. One
approach to this indirect approach has involved assessing the consistency of performance
on at least two measurement occasions, an approach subsuming both test–retest reliability
and parallel forms reliability. Test–retest involves cases tested at least twice over time,
and parallel forms reliability involves cases completing two or more scales. Both evaluate
consistency in part by examining the correlations of test scores between the testing
occasions.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
311

312
RELIABILITY ANALYSIS: INTERNAL CONSISTENCY
This chapter concerns those situations where we have just one measurement oppor-
tunity. We still need to assess consistency of performance, and our focus is thus on the
responses to the items. In simpliﬁed terms, we expect that if each of the items is an indi-
cator of the construct that is presumably being assessed by the scale, then respondents
should be internally consistent in their responses to (endorsement of) the items because
the items are at least partially measuring a common construct, that is, the responses to
the items should be correlated. Moderation and common sense are the keys to assess
item correlations and to combine them to form a scale assessing a particular construct.
• If the items are all correlated near 1.00, then we may conclude that we are asking
the same question over and over, and this is a poor measurement technique.
• If the items are not correlated at all, then they have nothing in common, and
because it is the construct underlying the scale that they are supposed to have in
common, there is no core construct that is being measured. This situation is to be
avoided.
• If some of the items are negatively (but modestly) correlated, then we must recode
them so that all are oriented in the same direction of the construct before we
combine items; that is, higher scores on the items should uniformly represent
more knowledge, more positive attitudes, more of a characteristic, and so on. This
recoding issue is often encountered because it is common practice to reverse-word
several scale items to thwart acquiescence bias (Robinson, Shaver, & Wrightsman,
1991).
Developing an effective reliability index has been an ongoing process. In the early
part of past century, it was common to use a split-half method where we divided the
items into two sets (for the purposes of calculation), scored each set for each respondent,
and correlated these subset scores; higher correlations were taken as indicating greater
reliability. However, different splits often resulted in different correlations, leading to
some ambiguity of interpretation. This procedure evolved into development by Kuder
and Richardson (1937) of a general reliability coefﬁcient known as the KR-20 coefﬁcient
because it was presented in the twentieth formula of their monograph. Its limiting feature
was that it applied only to dichotomously scored items.
Finally, Cronbach (1951) developed a generalized version of KR-20 known as coef-
ﬁcient alpha applicable to most item-scoring systems, and this has become the most
widely used reliability index; it yields the lower bound of the internal consistency of a
scale. But care needs to be taken in working with this coefﬁcient. Following are some
of the pitfalls of interpreting coefﬁcient alpha (Cortina, 1993):
• Although coefﬁcient alpha is intended to be used to describe the internal consis-
tency of homogeneous (unidimensional) scales, it is possible for multidimensional
scales to produce acceptable values of coefﬁcient alpha because there may be
present subsets of strongly related items; thus, a relatively high coefﬁcient alpha
does not necessarily signify unidimensionality.
• Coefﬁcient alpha is sensitive to the number of items on the scale. All else equal,
longer scales will drive up (potentially inﬂate) the reliability of the scale. Modern
psychometric researchers look to reduce an item set down to a focused subset
of items by removing those that are “weaker” indicators of the construct and by
countering item groups exhibiting local item dependence either by using a single
item from that group or by forming within the scale minitests known as testlets
(Steinberg & Thissen, 1996; Yen, 1993). To supplement coefﬁcient alpha, we can
also evaluate the average inter-item correlation (e.g., Clark & Watson, 1995), a
statistic that is not sensitive to the size of the item set.

ANALYSIS SETUP
313
A reliability coefﬁcient is a general index of the degree to which a scale is free of
measurement error. As such, we can establish broad guidelines for quality and compare
scales on the index. For example, a coefﬁcient alpha of .70 would be about as low as we
would want to see for a measure, and it would be desirable for scales to exceed .80 when
considering both the number of items and sample size (Clark & Watson, 1995; Nunnally
& Bernstein, 1994). However, if a measure is used to make a decision on an individual,
very high reliabilities (i.e., in the range .90 or higher) are mandated (Gay, 2010).
The amount of estimated measurement error can be converted to scale score
units by computing the standard error of measurement (multiply the square root of
1.00 −reliability coefﬁcient by the standard deviation of the test scores) and building a
(95%) conﬁdence interval around it. As the standard error of measurement varies across
the range of test scores (e.g., Feldt & Qualls, 1996; Lord, 1984), the conditional standard
error of measurement for speciﬁc scale scores should be calculated when making
decisions based on scale performance (i.e., whether a case has attained a criterion or
threshold score, such as passing or failing or being grouped into a particular category).
36.2
NUMERICAL EXAMPLE
To illustrate how to perform a reliability analysis, we use the 12 items comprising the
conscientiousness scale of the NEO Five-Factor Inventory (Costa & McCrae, 1992). The
items are named by their item number in the inventory (e.g., neo15), and we assess char-
acteristics such as being well organized, working hard to meet goals and responsibility,
and meeting responsibilities on time. Items are evaluated on a 5-point Strongly Disagree
to Strongly Agree response scale and are later converted to a 0–4 numerical score. Our
data reﬂect any recoding that was needed to accommodate reverse-worded items so that
the scoring of all the items is oriented toward higher levels of conscientiousness. Because
the construct is well understood and because of the care and expertise that went into the
development of the items, what we see is a scale that exhibits a high level of reliability
in this data set. The data are provided in the data ﬁle named conscientiousness items.
36.3
ANALYSIS SETUP
Open conscientiousness items and from the main menu select Analyze ➔Scale ➔Reli-
ability Analysis; this produces the Reliability main dialog window shown in Figure 36.1.
We have moved all 12 neo variables (neo5–neo60) to the Items panel and typed consci-
entiousness in the Scale label panel as a label for the output. It is important to recognize
that the Reliability procedure will not permanently compute a scale score and save it to
the data ﬁle (for which we need to use a Compute procedure) but only combines the
items for the duration of the analysis; it therefore is a projection of what the reliability
of the scale would be on the basis of this set of items. We retain Alpha in the Model
drop-down menu (split half and other computations are available as well in this menu).
Selecting the Statistics pushbutton opens the Statistics dialog window shown in
Figure 36.2. In the Descriptives area, we check Item (this generates descriptive statistics
for the items so that we can determine whether each has sufﬁcient variance to merit
inclusion on the scale) and Scale if item deleted (this generates several statistics we use
to evaluate the item set). In the Inter-item area, we check Correlations (this generates
a correlation matrix of the items), and in the Summaries area we check Correlations.
Select Continue to return to the main dialog window and click OK to perform the
analysis.

314
RELIABILITY ANALYSIS: INTERNAL CONSISTENCY
FIGURE 36.1
The main dialog window of Reliability.
FIGURE 36.2
The Statistics dialog window of Reliability.
36.4
ANALYSIS OUTPUT
The top (Case Processing) table in Figure 36.3 shows us that our sample size for the
analysis is 407. The bottom (Item Statistics) table presents the means and standard
deviations of the items. Ideally, items rated on a 5-point scale should yield a standard
deviation near 1.00, and for the most part, these items conform roughly to this expectation;
thus, each item demonstrates sufﬁcient variance to meaningfully correlate it with the other
items.
The middle (Reliability Statistics) table gives us the short answer we were seeking
at the outset of the analysis. Because these items are not in standardized form, we refer
to the column labeled Cronbach’s Alpha where we note that the coefﬁcient has a value
of .806. For a 12-item scale (shown under N of Items), this would be considered to
reﬂect good reliability.
Figure 36.4 presents the correlation matrix of the items and, below it, a summary
of the correlation statistics. We are looking primarily for two features in the correlation

ANALYSIS OUTPUT
315
FIGURE 36.3
Case Processing Summary, coefﬁcient alpha, and Item Statistics.
matrix: the correlations are neither too small to yield much common variance nor too
large (in the .70’s or greater) to indicate duplication of item and there are no negative
correlations (as they all need to be oriented in the same direction). As we would expect
from a well-respected scale with a coefﬁcient alpha of approximately .81, the correlations
in the matrix represent what we would hope to ﬁnd.
In the table of correlation summary statistics, our focus is on the Mean of .271;
this is the average correlation of the items on the scale. One of the criteria for a high-
quality scale assessing a broadly deﬁned construct recommended by Clark and Watson
(1995) is that the average inter-item correlation be in the range .15–.20 (more narrowly
deﬁned constructs should correlate to a somewhat greater extent). The NEO Five-Factor
conscientiousness scale appears to exceed this criterion, again suggesting a scale of high
reliability.
A set of useful statistics is presented in the table of Item-Total Statistics shown in
Figure 36.5. The table is used primarily for diagnostics to determine the contribution of
each item (mostly by showing the consequences of deleting the item from the set). Of
lesser interest are the ﬁrst two columns (Scale Mean if Item Deleted and Scale Variance
if Item Deleted); IBM SPSS® has totaled the item scores for each case and computed a
scale mean. It then shows the scale mean if the item on the row is deleted from the set.

316
RELIABILITY ANALYSIS: INTERNAL CONSISTENCY
FIGURE 36.4 Correlation matrix and a summary of the correlation statistics.
FIGURE 36.5 The table of Item-Total Statistics.
The Squared Multiple Correlation is the result of a multiple regression procedure
where the item on the row is being predicted by a weighted linear composite of the other
variables. For example, the Squared Multiple Correlation associated with predicting
neo5 from the other items is .204; thus, approximately 20% of the variance of neo5 is
explained by a weighted linear composite of the other items. This statistic is useful to
determine if there is sufﬁcient multicollinearity in the data set to warrant exclusion of a
variable. We ordinarily have the same goal here as when we examined the correlation
matrix for correlations in the .70’s; the difference is that the correlation matrix showed
us bivariate (pairwise) correlations, whereas the multivariate correlation statistic is a way
to discern multicollinearity.

ANALYSIS OUTPUT
317
Probably the column that attracts the primary interest of researchers is the Corrected
Item-Total Correlation. The Corrected Item-Total Correlation is a Pearson correlation
between the item score and the total of the remaining items in the set. It is “corrected”
in that the total score does not include the item in question. All else equal, corrected
item-total correlations can be interpreted as follows: .10’s are acceptable, .20’s are good,
.30’s are very good, and .40’s or better are considered to be extremely good (assuming
an average inter-item correlation in the .20’s or .30’s). We interpret such correlations to
indicate that higher scores on the item are associated with higher scale scores; as such,
the item is taken to be a good indicator of the construct. All of the obtained corrected
item-total correlations shown in Figure 36.5 are at least very good.
Corrected item-total correlations in the range .00–.10 indicate that there is not much
relationship between item performance and scale score. Unless its content was capturing
a unique and important aspect of the construct, such items are candidates for deletion or
rewriting.
Negative corrected item-total correlations are “red ﬂags.” They signify that the item
and the scale are oriented in different directions and that the analysis should be stopped.
The likelihood is that this is a reverse-worded item that was overlooked in the recoding
process, and this is an easy ﬁx (the reliability analysis should them be performed again
with the properly coded item replacing the one that was not properly coded). If the
problem is subtler, the item needs to be removed from the set.
Cronbach’s Alpha if Item Deleted indicates the reliability coefﬁcient consequences
of removing an item. This information needs to be used carefully by the researchers. In a
short inventory, removing an item can somewhat reduce the value of the alpha coefﬁcient,
as the statistic is sensitive to the number of items on the scale. With good quality items in
a short scale, as shown in Figure 36.5, the alpha coefﬁcient does not change dramatically,
although it tends to get slightly lowered for most of the items. This statistic will provide
a more dramatic result when the corrected item-total correlation is negative or even in
the zero range.


C H A P T E R
3 7
Reliability Analysis:
Assessing Rater Consistency
37.1
OVERVIEW
There are numerous situations where we call on raters or judges to evaluate or classify the
performance of individuals or other entities. For example, we may need to quantitatively
evaluate the interview performance of prospective job applicants, the seriousness of a set
of symptoms, if youthful offenders were or were not candidates to be placed in a certain
treatment program, or if someone passed or failed an oral examination for licensure as
a therapist. In all of these situations, we do not ordinarily rely on a single rater and run
the risk of obtaining idiosyncratic evaluations, even for someone who is carefully trained
in the rating system. Instead, we convene a panel of two or more raters and rely on the
judgments of such raters to the extent that they exhibit an acceptable level of consistency
in their assessments.
We focus on two procedures for assessing rater consistency. For ratings that are
made on a quantitative scale of measurement (often 5- or 7-point summative response
scales), we ordinarily use the intraclass correlation (ICC), a statistic that is available
in the Reliability procedure of IBM SPSS® and that we also make use of in multilevel
modeling (see Chapter 29). For a categorical rating system when we have only two raters,
we use the kappa coefﬁcient, a statistic that is available in the Crosstabs procedure of
IBM SPSS; with more than two raters, an extension of kappa has been provided by Fleiss
(1971) but this computation is not available in IBM SPSS. A couple of other variations
of kappa to handle the prevalence of ratings for one of the categories and substantial
differences between the raters have been reviewed by Di Eugenio and Glass (2004), but
these are also not available in IBM SPSS.
The ICC
The ICC can be dated back to Pearson (1901a) in describing roommates rating their
living quarters (Gonzalez & Grifﬁn, 2004), but Shrout and Fleiss (1979) popularized it
as a way to assess rater reliability. It can be calculated from a one-way within-subjects
(Treatment × Subjects) ANOVA where the raters represent the within-subjects variable
and the cases that are being evaluated represent the subjects (Guildford & Fruchter,
1978). There are no ﬁrm guidelines for interpreting the magnitude of the ICC in terms of
rater consistency, but generally, it is desirable to achieve an ICC value of .70 or higher.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
319

320
RELIABILITY ANALYSIS: ASSESSING RATER CONSISTENCY
The IBM SPSS data ﬁle needs to be structured with the raters as the variables
(columns or Treatment effect) and the cases evaluated as the rows. We could therefore
compute Pearson correlations between pairs of raters, but this would not be an adequate
way to evaluate rater consistency. Pearson correlations assess only covariation without
concern about any magnitude differences between the raters; thus, the ratings of two
raters may yield a substantial correlation but demonstrate large absolute differences in the
magnitudes of their ratings. The ICC takes into account the absolute levels of agreement
as well as the covariation of the ratings (IBM SPSS provides us with a choice on this
matter).
It is possible to use different ANOVA models in the computation of the ICC. Here
is a brief summary of these options:
• A one-way random effects model is used when different raters have evaluated the
cases in the data ﬁle. This model is relatively infrequently used; it usually comes
into play when there are large numbers of cases to evaluate and a single set of
raters is unable to perform all of the evaluations.
• A two-way random effects model is used when all raters have evaluated every case.
The decision to use this model hinges on the raters being considered a random
sample of all possible judges. Most authors (e.g., Berkman & Reise, 2012; Howell,
2010) suggest that this model is applicable to most situations. Theoretically, by
treating the raters as a random effect, we can generalize the ICC to a more general
population of raters.
• A two-way ﬁxed effect model is used when all raters have evaluated every case. The
decision to use this model hinges on the raters being considered to be unique or
that they are the only set of judges who are relevant. Theoretically, by treating the
raters as a ﬁxed effect, we cannot generalize the ICC to a more general population
of raters but are simply describing the behavior of the raters in the study.
Deciding between the two-way random and two-way ﬁxed effect is sometimes rel-
atively straightforward and sometimes not so straightforward. For example, in basic
research studies (e.g., rating videos of parent–child interactions) where the raters can
be students or staff members, it is clear that they comprise a (maybe not quite random)
sample and thus (sort of) satisﬁes the random effects model. In certain applied settings
(e.g., there are only three staff analysts in the Exam Development unit whose job is
to rate employment applications), the argument that the ICC needs to be descriptive of
the particular set of raters seems persuasive and thus this situation appears to satisfy
the ﬁxed effect model. Between these two extremes lies a range of situations requiring
careful consideration on the part the researchers in terms of which model to use. The
random effects model will occasionally yield somewhat lower values of the ICC than the
ﬁxed effect model and thus may represent the more conservative approach.
IBM SPSS produces two versions of the ICC, and the difference between these has,
in our personal experience, been the basis for some confusion. These two versions, and
when they should be used, are as follows:
• The single measures ICC should be used when the ratings in the data ﬁle represent
the scale value selected by the rater. This is a situation that will almost always be
in effect, and thus, the single measures ICC should almost always be used.
• The average measures ICC should be used when the ratings in the data ﬁle rep-
resent average values of a set of raters. This is a situation that will rarely be in
effect.
It has been suggested that if the average of the raters will be used to test hypotheses,
then the average measures ICC could be used (e.g., Hallgren, 2012). However, assessing

NUMERICAL EXAMPLE: ICC
321
the stability of the mean rating is quite different from describing the extent to which
the raters exhibit consistency. The value of the ICC for average measures will always
be higher than what is obtained for the single measures computation. This is because
the average measures ICC is computed by placing the single measures ICC into the
Spearman–Brown prophecy formula (Shrout & Fleiss, 1979) that was originally designed
by Brown (1910) and Spearman (1910) to project what the full-test correlation might be
based on a split-half reliability estimate.
It is also possible to select between a consistency and an absolute agreement compu-
tation. The consistency computation does not take into account the differences between
the raters and so provides a value of the ICC that is very close to the mean of the pairwise
correlations between the raters. The absolute agreement option does take into account
the differences between the raters. As one of the advantages of using an ICC is that it
takes rater differences as well as rater covariance into account, we almost always choose
the absolute agreement option.
Cohen’s Kappa Coefﬁcient
The kappa coefﬁcient was proposed by Jacob Cohen (1960) to index the degree of agree-
ment exhibited by two raters in their dichotomous categorization of the performance of
the cases. Because we are dealing with assignment of one of the two available cate-
gories to the cases (e.g., pass or fail), the data gleaned from the ratings are in the form
of frequencies. Although the frequency data are structured within a two-way chi-square
contingency table, contrasting observed frequencies with those expected on the basis of
the null hypothesis (see Chapter 63), the kappa coefﬁcient calculation controls for the
amount of agreement we would expect on the basis of chance. By controlling for chance
agreement, kappa is a more appropriate way to assess rater consistency than simply
computing the percentage of cases on which the two raters agreed in their categorization
(which does not correct for chance).
The value of the kappa coefﬁcient can vary from −1.00, indicating poorer than
chance agreement, to 0.00, indicating exactly chance agreement, to 1.00, indicating per-
fect agreement (Fleiss & Cohen, 1973). The decimal value can be interpreted as the
percent agreement between raters controlling for chance. Based partly on criteria sug-
gested by Krippendorff (1980), at least in applied situations, values below about .70
might be considered not acceptable, those between .70 and the low to middle .80’s might
be treated as borderline acceptable, and those higher than the middle .80’s might be
considered acceptable to good depending on where they lie in that range.
37.2
NUMERICAL EXAMPLE: ICC
The ﬁctional data for this example are provided in the data ﬁle named job candidate
1st round ratings. A total of 23 candidates in a city police force tested for a promotion
to Lieutenant. As one of the components of the selection process, they were asked to
study a complex scenario for 30 minutes and then present their resource deployment
and contingencies plan to a panel of three subject-matter experts. The panel members
were selected from a process that asked Police Captains from comparably sized cities
in nearby states to take part in this testing and were provided with extensive training
in this evaluation process before the testing began. After listening to each candidate’s
presentation and asking what questions were allowed by the testing process, each rater
made a rating of the candidate’s performance on a 7-point scale ranging from Relatively
Poor to Excellent. The ratings are provided under the variables rater1, rater2, and
rater3. It was anticipated that after the ratings were statistically analyzed, the panel

322
RELIABILITY ANALYSIS: ASSESSING RATER CONSISTENCY
members would be given the opportunity to discuss their ratings as a group and make
any changes they deemed appropriate before the ratings were ﬁnalized.
37.3
ANALYSIS SETUP: ICC
Open job candidate 1st round ratings and from the main menu select Analyze ➔
Scale ➔Reliability Analysis; this produces the Reliability main dialog window shown
in Figure 37.1. We have moved rater1, rater2, and rater3 to the Items panel.
Selecting the Statistics pushbutton opens the Statistics dialog window shown in
Figure 37.2. In the Inter-item area, we check Correlations (this generates a correlation
matrix of the items), and in the ANOVA Table area we check F test (this generates
a one-way ANOVA so that we can determine if there are rater differences). We check
the Intraclass correlation coefﬁcient checkbox. From the Model drop-down menu, we
select Two-Way Random (as these raters are a sample of those who were available
FIGURE 37.1
The main dialog window of Reliability.
FIGURE 37.2
The Statistics dialog window of Reliability.

ANALYSIS OUTPUT: ICC
323
and thus comprise a random effect), and from the Type drop-down menu, we select
Absolute Agreement (as we wish to have the ICC reﬂect rater differences as well as
their covariance). Select Continue to return to the main dialog window and click OK to
perform the analysis.
37.4
ANALYSIS OUTPUT: ICC
The top table in Figure 37.3 shows the correlation matrix for the raters. It is named
Inter-Item Correlation Matrix because often researchers use the IBM SPSS Reliability
procedure to determine the value of coefﬁcient alpha (see Chapter 36), and in such an
analysis, the items are the variables in the analysis. As can be seen from the matrix, there
is a good deal of covariation permeating the ratings.
The ANOVA summary table is also shown in Figure 37.3. This is a one-way within-
subjects design (see Chapter 51). The Between People effect represents the cases in
the analysis (the job candidates in our example), and IBM SPSS (and most researchers)
will not bother computing an F ratio for this effect (it ordinarily signiﬁes individual
differences).
The Between Items effect generally represents the within-subjects treatment effect
and here addresses the differences among our raters. The F ratio of 41.008 informs us
that we do have statistically signiﬁcant rater differences (p < .001). Were we sufﬁciently
interested in this, we would have performed the stand-alone ANOVA with multiple
comparison tests (as described in Chapter 51); here, we just note that there are rater
differences. Given that this is the ﬁrst round of ratings and that a discussion of the
ratings of these panel members is yet to take place with the opportunity (and likelihood)
that some ratings will be changed, this may not present a difﬁculty for the selection
process.
Figure 37.4 presents the two ICCs. The one in which we are interested is the Single
Measures coefﬁcient. Its value of .710 is suggestive that there is some work the raters
must do to get their ratings more in sync, as .710 may be at the lower end of acceptability
in some research contexts but is not likely to be judged as sufﬁcient in the selection
environment. Again, rater discussion should result in an improvement in the ICC.
FIGURE 37.3 Rater correlation matrix and the ANOVA summary table.

324
RELIABILITY ANALYSIS: ASSESSING RATER CONSISTENCY
FIGURE 37.4 The Single Measures and Average Measures ICCs.
37.5
NUMERICAL EXAMPLE: KAPPA
The ﬁctional data for this example are provided in the data ﬁle named juvenile offender
ratings. A total of 56 juveniles (ward in the data ﬁle) who have been incarcerated
for committing crimes were independently evaluated by two forensic psychologists for
placement in a community work program that would have the offenders leave the deten-
tion facility during the daytime hours. The evaluation was performed to determine if the
wards represented a sufﬁciently low risk (in terms of committing another crime or of
escaping while they were off-site) to be placed in the work program. The psychologists
each produced a binary rating for each youth under the variables rater1 and rater2 in
the data ﬁle. The categories in the rating system were remain on grounds (coded as 1
in the data ﬁle) and off-site work allowed (coded as 2 in the data ﬁle).
FIGURE 37.5
The main dialog window of Crosstabs.

ANALYSIS SETUP: KAPPA
325
FIGURE 37.6
The Statistics dialog window of Crosstabs.
37.6
ANALYSIS SETUP: KAPPA
Open juvenile offender ratings and from the main menu select Analyze ➔Descriptives
➔Crosstabs; this produces the Crosstabs main dialog window shown in Figure 37.5. We
have arbitrarily moved rater1 to the Row(s) panel and rater2 to the Column(s) panel.
A more extensive treatment of the Crosstabs procedure is discussed in Chapter
63. Nonetheless, we wish to obtain more than just the bare kappa statistic. Selecting
the Statistics pushbutton opens the Statistics dialog window shown in Figure 37.6. We
select Chi-square just to provide a test of the null hypothesis that there is no relationship
between the raters in their assignment of categories to the wards, and we select Kappa
because that is the point of this analysis.
FIGURE 37.7
The Cell Display dialog window of Crosstabs.

326
RELIABILITY ANALYSIS: ASSESSING RATER CONSISTENCY
Selecting the Cells pushbutton opens the Cell Display dialog window shown in
Figure 37.7. To obtain a sense of how the raters assigned wards to categories, we check
both Observed and Expected under Counts. These provide, respectively, the actual
frequencies of rater correspondence to the category assignments and the frequencies that
would be expected based on the null hypothesis. Select Continue to return to the main
dialog window and click OK to perform the analysis.
37.7
ANALYSIS OUTPUT: KAPPA
The frequency Crosstabulation is shown in the top table of Figure 37.8. Both raters
agreed on 20 wards who they believed should be kept on the grounds of the facility and
should not be allowed to work in the community; both raters also agreed on 30 wards
who could be allowed to leave the facility to work in the community. If we were using
the simple percentage of rater agreement as our gauge, then 50 of the 56 wards were
judged in the same way by these raters for a rater agreement percentage of 50/56, or
about 89.3%.
The chi-square test of the null hypothesis is shown in the bottom table in Figure 37.8.
Chi-square was computed to be 34.164, and with one degree of freedom (for the 2 × 2
array), it is statistically signiﬁcant (p < .001); thus, the null hypothesis of no relation in
the ratings gives way to the conclusion that the ratings provided by the raters are related
to each other.
FIGURE 37.8 Observed and expected cell frequencies (top) and chi-square test of null hypothesis.

ANALYSIS OUTPUT: KAPPA
327
FIGURE 37.9 The kappa coefﬁcient.
How strongly the judgments of the raters are related is shown by the kappa coefﬁ-
cient in Figure 37.9. The kappa value is .779, a value noticeably lower than the straight
percentage of agreement because its calculation has statistically controlled for the distri-
bution of frequencies we would expect by chance. It is statistically signiﬁcant (p < .001).
We interpret this value as representing a reasonable amount of agreement, but the two
psychologists did not evaluate the wards in complete synchrony, and some discussion
between them to resolve at least some of the discrepancies should be attempted if at all
possible.


P A R T 12
ANALYSIS OF STRUCTURE


C H A P T E R
3 8
Principal Components
and Factor Analysis
38.1
OVERVIEW OF PRINCIPAL COMPONENTS
AND FACTOR ANALYSIS
Principal components and factor analysis comprise a family of exploratory data analysis
procedures that are used to identify a relatively small number of dimensions or themes (the
components or factors) underlying a relatively larger set of variables. These procedures
represent ways to consolidate into a small number of synthesizing constructs information
that is dispersed among several variables. They can be applied to a set of items on
a survey (e.g., items assessing behavior on a range of health-related issues) or to a
set of already-developed measures (e.g., personality characteristics). We can trace the
beginnings of principal components analysis and factor analysis to Karl Pearson (1901b)
and Charles Spearman (1904a), respectively. Principal components analysis was more
fully developed by Harold Hotelling (1933, 1936), and factor analysis was transformed
into its more modern form primarily by Louis L. Thurstone (1931, 1935).
Principal components and factor analyses each begins with the Pearson correlation
matrix of the variables. Their mutual goal is to account for the variance in this matrix
by ﬁtting a series of weighted linear functions (components or factors) to the variables
within the multidimensional space they occupy. The analysis is composed of two major
phases: extraction and rotation.
Extraction can be accomplished through several methods. When that method is prin-
cipal components, we label the analysis as the principal components analysis. Principal
components is conceptually and statistically the simplest of the extraction techniques.
The other methods as a set are known as factor analysis techniques; they include prin-
cipal axis, maximum likelihood, and unweighted and generalized least-squares analyses.
When one of these methods is used, we label the technique as a factor analysis.
In both principal components and factor analysis, the dimensions are extracted
(straight line ﬁt functions to correlate maximally with the variables) sequentially and
are independent of each other (they are orthogonal). Thus, the ﬁrst component or fac-
tor will explain what variance it can. The second component or factor must target the
remaining (residual) variance to explain. The third component or factor must then address
the variance not explained by the ﬁrst two, and so on. Because they are orthogonal, we
can add the amount of explained variance across the components or factors to speak of
the cumulative amount of variance accounted for by the ﬁrst d number of dimensions.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
331

332
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
By virtue of the way components and factors are extracted, each successively
extracted component or factor will explain less variance than those extracted before
it. What usually happens is that the ﬁrst few components or factors will cumulatively
have accounted for a relatively large percentage of the to-be-explained variance, and
we often quickly reach a point of diminishing returns. If we were to plot the amount
of variance explained on the Y-axis as a function of components or factors extracted
on the X-axis we would see a backwards J-shaped function, popularized by Raymond
Cattell (1966) as a scree plot. The goal of the researchers in the extraction phase is to
select a relatively few components or factors that cumulatively explain a fair amount
of variance to bring into the rotation phase; the caveat to this is that, at the end of the
rotation process, the component or factor structure needs to be meaningfully interpreted.
As was true for extraction, rotation can also be accomplished through several meth-
ods. At the end of the extraction phase, the components or factors can be conceived as
intersecting at 90 degrees with respect to each other (they are orthogonal when inter-
secting at 90 degrees). To rotate the component or factor structure is to pivot it within
the multidimensional space while the variables remain in place. This pivoting results
in each component or factor being closer to (correlating more strongly with) some of
the variables in that space while at the same time being further from (correlating less
strongly with) some other variables. The pattern of correlations between the variables
and the components or factors resulting from striving to meet this goal is termed simple
structure after the original idea articulated by Louis L. Thurstone (1947).
One set of rotation procedures keeps the dimensions orthogonal; the most widely
used of these is a varimax rotation. Another set of rotation procedures allows the com-
ponents or factors to depart from 90 degrees; that is, it allows the angle between them to
become oblique. These procedures are known as oblique rotation strategies, and inter-
secting obliquely results in the components or factors being correlated once the rotation
is completed. One of the more commonly used oblique rotation strategies is a promax
rotation.
Extraction results in the ﬁrst component or factor as the strongest, the second next
strongest, and so on; that is, many or most of the variables will correlate most strongly
with the ﬁrst component or factor, with the other components or factors falling off quickly
in the order they were extracted. Because we interpret a component or factor based on
the pattern of the correlations of the variables with it, and because the “strength” of the
factors is a result of the algorithm used in the extraction, we do not ordinarily interpret
the components or factors at the end of the extraction phase. Instead, interpretation of the
components/factors resulting from the analysis is focused on the rotated factor structure.
This is the case because the associations between the variables and the components or
factors have been “equalized” to a large extent, providing each dimension with a viable
proﬁle of correlated variables of its own.
We will cover principal components and factor analysis in more detail as we go
through the worked examples. To learn more about these procedures, readers are referred
to Cudeck and MacCallum (2007), Gorsuch (2003), and Meyers et al. (2013) as the
places to start with.
38.2
NUMERICAL EXAMPLE
Here, we illustrate how to perform and interpret principal components and factor analysis
using a set of personality measures contained in the data ﬁle Personality. We will use
13 of the variables in the data ﬁle.

A STARTING PLACE
333
38.3
A STARTING PLACE
The starting place for principal components and factor analysis is the correlation matrix of
the variables, and these are presented in Figure 38.1. With a few exceptions, the variables
are correlated to a low or moderate extent. There are a couple of higher correlations that
might suggest removal of one of the variables in the pair (because they could form the
seed of what might turn out to be an artiﬁcial researcher-created factor), but we will retain
all of the variables in our illustration just to have enough to show how these analyses
work.
The correlation matrix shows the values of 1.000 on the diagonal of the matrix, and
these are preserved in the process of performing a principal components analysis (the
factor analytic procedures replace the 1’s with some measure representing the relationship
of each variable with the set of other variables). Principal components analysis focuses
on explaining the total variance of the set of variables; in this context, the total variance
is equal to the number of variables in the analysis (each variable contributes 1.000 units
to the total variance). For our present example where we are analyzing 13 variables, the
total variance achieves a value of 13.00. It is common practice to perform a preliminary
analysis involving only the extraction phase in order to assess how we should proceed in
the rest of the analysis, and this is what we do next using principal components analysis
because it is the simplest of the extraction methods.
FIGURE 38.1 Correlation matrix of the variables.

334
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
38.4
ANALYSIS SETUP: PRELIMINARY ANALYSIS
From the main menu we select Analyze ➔Dimension Reduction ➔Factor. This opens
the main Factor Analysis window as shown in Figure 38.2. We move the following
variables into the Variables panel: cntrlcpi (self-control), acceptnc (self-acceptance),
eminhib (emotional inhibition), bencntrl (benign control of self), aggcontrl (control of
one’s aggressive tendencies), neoextra (extraversion), neoagree (agreeableness), posa-
fect (positive affect), negafect (negative affect), neoneuro (neuroticism), depcon (control
of depressive tendencies), anxcon (control of anxious tendencies), and regard (self-
regard).
The Descriptives window is presented in Figure 38.3. Check both Univariate
descriptives (this will indicate the number of cases in the analysis as a check to ensure
that we have not lost too many cases due to missing values) and Initial solution (this
will display the amount of variance explained by each component) in the Statistics area
in the upper portion of the window.
In the Correlation Matrix area in the lower portion of the window shown in
Figure 38.3, select KMO and Bartlett’s test of sphericity. KMO stands for the
Kaiser–Meyer–Olkin indicator of how adequate the correlations are for factor analysis;
generally, a value of .70 or above is considered adequate (Kaiser, 1970, 1974). Bartlett’s
FIGURE 38.2
The main dialog window of Factor Analysis.
FIGURE 38.3
The Descriptives dialog window of Factor Analysis.

ANALYSIS OUTPUT: PRELIMINARY ANALYSIS
335
FIGURE 38.4
The Extraction dialog window of Factor Analysis.
test of sphericity tests the null hypothesis that the variables are not signiﬁcantly
correlated and should yield a statistically signiﬁcant outcome before proceeding with
the factor analysis. Click Continue to return to the main dialog window.
The Extraction window, shown in Figure 38.4, allows us to specify the method
we wish to use; Principal components is the default Method, and we retain it for this
analysis. We also retain the default of Correlation matrix as what we wish to Analyze.
Under Display, we check both Unrotated factor solution (to obtain the correlations
of the variables with the components) and Scree plot to obtain this classic graph. In
the Extract area, we retain (only for this preliminary analysis) the default Based on
Eigenvalues greater than 1. Click Continue to return to the main dialog window.
Because the default in the Rotation screen is None, and because we will not perform a
rotation in this preliminary analysis, there is no need to access this window. Click OK
to perform the analysis.
38.5
ANALYSIS OUTPUT: PRELIMINARY ANALYSIS
The Descriptive Statistics, shown in Figure 38.5, indicates that we have lost only six of
our original 425 cases, giving us a sample size of 419, and so our sample seems virtually
complete. With the Kaiser–Meyer–Olkin Measure of Sampling Adequacy exceeding
.70 and Bartlett’s Test of Sphericity being statistically signiﬁcant, we have conﬁdence
of the appropriateness of the analysis.
The Total Variance Explained table is shown in Figure 38.6. There are two major
vertical parts of the table. The Initial Eigenvalues portion of the table describes the
initial analysis, which is always a principal components analysis (even if we speciﬁed
one of the factor analysis extraction methods) and it is taken to completion. With 13
units of variance in this set of variables, it is possible to extract a maximum of 13
components. The third column under Initial Eigenvalues presents the Cumulative % of
the variance that is accounted for with increasingly more components extracted. When all
13 have been extracted, they will cumulatively have explained 100% of the variance. Note
that the largest chunks of explained variance are associated with the earliest extracted
components; by the time we have extracted the ﬁfth component, for example, we have

336
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
FIGURE 38.5
Descriptive statistics and the KMO and Bartlett’s
Test output.
explained almost 75% of the variance. The second column under Initial Eigenvalues
presents the % of Variance that is accounted for by each component; these are simply
added together to derive the Cumulative %.
The ﬁrst column under Initial Eigenvalues labeled Total presents the eigenvalues
associated with each component. Eigenvalues are one way to express the variance that
is explained. In this analysis, there is a total of 13 units of variance. The eigenvalue
associated with the ﬁrst component has a value of 4.106, and 4.106/13.00 = 31.587, the
percentage of variance explained by the ﬁrst component. Eigenvalues are additive here
because the components are orthogonal, and if we summed the column of eigenvalues,
we would achieve a total of 13.00.
The set of columns under Extraction Sums of Squared Loadings provides the
same information that we see in the Initial Eigenvalues columns but only for the ﬁrst
four components (because only these have eigenvalues of 1.00 or greater). It is the same
information as in the Initial Eigenvalues portion of the table because the extraction
procedure we speciﬁed was principal components; thus, the initial complete analysis,
which is always principal components, is the same as our chosen extraction method,

ANALYSIS OUTPUT: PRELIMINARY ANALYSIS
337
FIGURE 38.6 The Total Variance Explained table.
hence the outcome is identical. In summary, the ﬁrst four components cumulatively
accounted for 69% of the total variance.
There were only four components extracted because in the Extraction window, we
had retained the default extraction criterion of Based on Eigenvalues greater than 1.
Generally, researchers are very hesitant to extract (and thus subsequently rotate) com-
ponents whose eigenvalue is less than 1.000, in that, at least in a principal components
analysis, a single variable is worth that much variance and we would like a component to
do at least as well. Even then, researchers are equally as hesitant to take into the rotation
stage the full set of components whose eigenvalues exceed 1.000, as they are concerned
that the set of components would be inappropriately large.
Figure 38.7 displays the scree plot; it plots the eigenvalues that are contained in
the Initial Eigenvalues column against the components in the full principal components
solution. The function appears to start leveling out at approximately the fourth or ﬁfth
component, which indicates that we want fewer components in the solution. On the basis
of the values in the Initial Eigenvalues column and a visual inspection of them in the
scree plot, it would appear that
• we make reasonably substantial gains in explained variance through three compo-
nents, and thus probably want to retain at least (the ﬁrst) three components in the
ﬁnal solution;
• we may gain enough variance to warrant including the fourth component (depend-
ing on how the rotated components are to be interpreted), but it is not clear if this
will be one too many.
The Communalities of the variables are displayed in Figure 38.8. The column
labeled Initial represents the values on the diagonal of the correlation matrix when the
principal components method was applied and run to completion. These values are all
1.000. One way to interpret these 1’s is to think of each of the variables as being “fully
in” or “fully captured by” the dimensional structure; because they are fully captured by
the dimensional structure, principal components attempts to explain the total amount of
variance in the set of variables.

338
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
FIGURE 38.7
The Scree Plot.
FIGURE 38.8
Communalities of the variables.

ANALYSIS OUTPUT: PRELIMINARY ANALYSIS
339
The column labeled Extraction in the Communalities table describes the percentage
of variance of each variable subsumed in the number of factors that were ultimately
extracted (four in the present instance). The four extracted factors cumulatively accounted
for 69% of the total variance, and so there is still unexplained variance remaining.
The variable whose variance has been best captured in the four-component solution
is neoextra with a communality of .808, and the variable whose variance has been
least captured in the four-component solution is acceptnc with a communality of .518.
Despite these differences, however, we judge that all of the variables are “participating”
substantially in the four-component solution. Generally, we would hope that variables
would be associated with communality values at least in the high .4’s to be able to say
that we have captured enough of the variance of a variable to be worthy of inclusion in
the component or factor structure.
The Component Matrix is shown in Figure 38.9. The values in the table are the
Pearson correlations between each of the variables and each of the components. Squar-
ing the correlations and adding them provides some of the information we described
earlier. The sum of squared correlations down each column is equal to the eigenvalue of
that component. As is typical, the ﬁrst component is more strongly associated with the
variables as a set than any of the others because it accounts for more variance than any
of the other components; this pattern propagates across the columns (components). The
sum of squared correlations across each row is equal to the extracted communalities for
each variable.
FIGURE 38.9
The Component Matrix.

340
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
38.6
OUR ANALYSIS STRATEGY FOR THE MAIN ANALYSES
There are three separate issues that need to be resolved now that we have the results of
the preliminary analysis:
• whether an oblique or orthogonal rotation method should be used;
• whether we will ﬁnalize our solution using principal components or one of the
factor analytic methods for our extraction;
• whether we will retain for the rotation phase three or four components or factors
in the ﬁnal solution.
These three issues are intertwined, in that all three elements must be speciﬁed in any
single analysis that we perform. We use the following strategy to address these issues:
• We will perform an oblique (promax) rotation and examine the correlations
between the components or factors that is a standard part of the output. If the
correlations are in the low teens or less, we will judge the components or factors
sufﬁciently unrelated to revert to a varimax rotation; if some of the correlations
are close to .30 or higher, we will judge them sufﬁciently related to retain the
promax rotation; if the correlations fall between these two general criteria, our
bias will be to stay with the promax rotation.
• We will perform a set of analyses using principal components and, to simplify our
presentation, another set of analyses using principal axis factoring (we have exam-
ined all of the major factor analytic methods and they produce very similar results;
to illustrate our strategy, we limit ourselves to principal axis as representative of
the factor analysis methods).
• We will examine both the three- and the four-component/factor solutions. Based
on the outcome of these exploratory ventures, we will select a solution that
appears to be reasonable from both the statistical and research-oriented perspec-
tives. Because it will supply a bit more information by virtue of giving us a
potentially extra component/factor, we opt to obtain the four-dimensional solution
ﬁrst and back-step into the three-dimensional solution, but it would be equally
reasonable to perform these analyses in the reverse order. It should be pointed out
here that the three-component/factor solution is not the same as the ﬁrst three com-
ponents/factors of the four-component/factor rotated solution; rather, rotating three
components/factors in the multidimensional space will create a different correla-
tion pattern between the variables and the components/factors than that is obtained
when rotating four components/factors.
38.7
ANALYSIS SETUP FOR THE FOUR-FACTOR STRUCTURE
From the main menu we select Analyze ➔Dimension Reduction ➔Factor and move
the variables into the Variables panel, as was done in the preliminary analysis (see
Section 38.4).
In the Descriptives window presented in Figure 38.10, check only Initial solution
in the Statistics area (we have already obtained the Univariate descriptives and KMO
and Bartlett’s test of sphericity in the preliminary analysis and do not need to generate
them again). Click Continue to return to the main dialog window.
In the Extraction window, shown in Figure 38.11, we select Principal axis factoring
from the Method drop-down menu. The dimensions resulting from this analysis are
referred to as factors rather than as components. We will perform an identical analysis

ANALYSIS SETUP FOR THE FOUR-FACTOR STRUCTURE
341
FIGURE 38.10
The Descriptives dialog window of Factor Analysis.
FIGURE 38.11
The Extraction dialog window of Factor Analysis.
after this, except that we will have selected Principal components (without showing the
dialog screens). We request under Display the Unrotated factor solution but not the
Scree plot (we have already obtained it in the preliminary analysis). In the Extract area,
we select Fixed number of factors and in the panel labeled Factors to extract we type
4. This speciﬁes the number of factors that will be brought into the rotation phase. Click
Continue to return to the main dialog window.
The Rotation window is shown in Figure 38.12. We select Promax and retain the
Kappa value at the default of 4 (one of the steps in a promax rotation is to raise the
correlations of the variables with the factors to a power labeled as kappa and the default
is the fourth power—see Meyers et al. (2013) for a fuller description of this rotation
strategy). In the Display area, we wish to see the Rotated solution. Click Continue to
return to the main dialog window.
The Options window is shown in Figure 38.13. We retain the default Missing
Values procedure of Exclude cases listwise (all cases must have valid values on all of
the variables to be included in the analysis). Under Coefﬁcient Display Format, we
select Sorted by size; this will order the listing of the variables in the factor matrix by

342
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
FIGURE 38.12
The Rotation dialog window of Factor Analysis.
FIGURE 38.13
The Options dialog window of Factor Analysis.
the strength of their relationship to the factor, thereby making the output more convenient
to read (in our judgment). Click Continue to return to the main dialog window and click
OK to perform the analysis.
38.8
ANALYSIS OUTPUT FOR THE
FOUR-COMPONENT/FACTOR STRUCTURE
Analysis Output for the Four-Component/Factor Structure:
Total Variance Explained
The Total Variance Explained tables for the principal components analysis (top table)
and the principal axis factor analysis (bottom table) are presented in Figure 38.14. We
have already seen the principal components table except for the last column that now con-
tains information about the rotation phase. The last column (Rotation Sums of Squared
Loadings) displays the eigenvalues for the components after rotation, where the explained
variance is distributed more evenly across the components. In an orthogonal rotation,
these eigenvalues would add to the same value as the sum of the eigenvalues for the
four components at the end of the extraction phase (8.978); because we have used an

ANALYSIS OUTPUT FOR THE FOUR-COMPONENT/FACTOR STRUCTURE
343
FIGURE 38.14
Total Variance Explained for both the four-dimensional principal components
analysis and the principal axis factor analysis.

344
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
oblique rotation strategy (where the factors are permitted to correlate), the sum of the
eigenvalues based on the rotated components is a larger value (10.824).
The Total Variance Explained table for the principal axis factor analysis is new.
We can see that the ﬁrst major column labeled Initial Eigenvalues is identical to that
in the principal components analysis (except the dimensions are now labeled as factors)
because the ﬁrst pass through the analysis is a principal components analysis taken to
completion.
The Extraction Sums of Squared Loadings presents a different picture from the
principal components analysis results, and this highlights a major difference between
principal components analysis and the factor analysis procedures. In principal components
analysis, we retain the values of 1.000 on the diagonal of the correlation matrix and thus
attempt to explain the total variance. In factor analysis, the values of 1.000 are replaced
by some estimate of the amount of variance each variable shares (has in common) with
the other variables. The total variance is thus partitioned into variance that is common
to the set of variables in the analysis and variance that is unique or speciﬁc to each
variable, and the factor analysis methods attempt to explain only the common variance;
that is, they identify dimensions that are descriptive of the variance common to all of
the variables. As the common variance is less than the total variance, factor analysis
methods will explain less variance than principal components.
In Figure 38.14, we see that the ﬁrst four factors in the principal axis solution
account for 56.475% of the total variance, whereas the ﬁrst four components in the
principal components solution account for 69.058% of the total variance. Furthermore, the
fourth principal axis factor presents us with an eigenvalue substantially lower than most
researchers’ lower limit of 1.000, even though the fourth principal component squeaked
by with an eigenvalue of 1.095. To the extent that researchers are drawn toward factor
analysis over principal components (it is our view that the ﬁeld is pretty evenly divided
on this issue), this would probably be sufﬁcient for those researchers to abandon the
four-factor solution and examine the three-factor solution.
Analysis Output for the Four-Component/Factor Structure:
Communalities
Figure 38.15 presents the Communalities for the variables. We have already discussed
these for the principal components analysis, and we can see considerable differences in
the principal axis factor analysis. The Initial communalities shown in the ﬁrst column of
the principal axis factor analysis are not the 1.000’s we had in the principal components
analysis but rather the R2’s between the particular variable and the others (because
we focus here on the common variance). For example, if we predicted cntrlcpi from
the other variables in a multiple regression analysis, we would obtain an R2 value of
.515. This Initial estimate is reestimated through an iterative process that resulted in
a ﬁnal estimated strength of relationship provided in the Extraction column. Different
factor analysis procedures use somewhat different algorithms in these calculations, but
communalities derived from factor analysis procedures, such as principal axis factoring,
will generally be lower than those derived from principal components analysis.
Analysis Output for the Four-Component/Factor Structure:
Component/Factor Correlations
One of the issues that we hoped to resolve in this set of analyses was whether an oblique
rotation strategy was appropriate here. The component/factor correlation matrices shown
in Figure 38.16 provide the information we use to resolve this issue. The same correlation
pattern is exhibited in both tables, and so we highlight the correlations from the principal
axis factor analysis. Some pairs of components/factors are not correlated (e.g., Factors

ANALYSIS OUTPUT FOR THE FOUR-COMPONENT/FACTOR STRUCTURE
345
FIGURE 38.15
Communalities of the variables for both the four-dimensional principal compo-
nents analysis and the principal axis factor analysis.
2 and 3 show an r value of −.047), but other pairs are relatively highly correlated (e.g.,
Factors 1 and 2 show an r value of .517). The presence of any correlations in the
.3’s or greater supports the use of an oblique rotation; thus, we move forward with
the interpretation based on the obliquely rotated solution and do not pursue a varimax
(orthogonal) rotated solution (although the interpretations of the varimax and promax
rotations are likely to coincide).
Analysis Output for the Four-Component/Factor Structure:
Rotated Component/Factor Structure Coefﬁcients
We are ﬁnally ready to examine the rotated solutions, which are presented in a component
or a factor matrix. An oblique rotation strategy yields two such matrices, a pattern
matrix and a structure matrix, that present, respectively, pattern coefﬁcients and structure
coefﬁcients.
Recall that a component or factor is a weighted linear combination of the variables
in the analysis; the pattern coefﬁcients are those weights (analogous to beta coefﬁcients
in multiple regression). If the variables are highly correlated, it is possible for these to
exceed the value of 1.00. Structure coefﬁcients are the correlation between the particular
variable and the component or factor. They are true correlations and must range between
±1.00. In most analyses, these two matrices produce very similar values and yield the
same interpretation. Researchers appear to be divided on which of the two matrices to

346
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
FIGURE 38.16
Factor/Component Correlation Matrices of the
variables for both the four-dimensional principal
components analysis and the principal axis factor analysis.
present, but our preference is to interpret the structure matrix and these are presented for
the principal components and principal axis factor solutions in Figure 38.17 (note that an
orthogonal rotation strategy such as varimax yields only a single rotated component/factor
matrix because the pattern and structure coefﬁcients take on the same values.)
It is the results of the structure (or pattern) matrix that researchers interpret; that is,
we use the structure (or pattern) coefﬁcients to interpret in plain language the dimension
represented by the component or factor. As the results for the two analyses are similar,
we will work through the principal axis solution.
The Structure Matrix for the principal axis solution shown in Figure 38.17 contains
the variables as the rows and the factors as columns. Entries are the structure coefﬁcients,
that is, the correlation of each variable with each factor. Higher correlations indicate a
stronger relationship between the two. To the extent that the relationship between the
variable and the factor is reasonably strong, the variable can be conceived as an indicator
(or aspect) of the factor.
Looking across the rows (across the factors for each variable), we see that most of the
variables tend to correlate reasonably highly with (colloquially referred to as the variable
“loading on”) just one factor (this was the goal of simple structure). An exception is
bencntrl, as this variable correlates at least somewhat highly with (colloquially referred
to as the variable “cross-loading on”) both Factors 1 and 2.
With most of the variables relatively unambiguously “loading” on one of the factors,
we can proceed to interpret the factors. To interpret the factors, we work vertically down
each column separately, and this is why we sorted the coefﬁcients by size in the Options
dialog window. The variables are listed by factor and are sorted by size (the value of the
coefﬁcient) within each factor. Thus, the ﬁrst four variables are most highly correlated
with Factor 1, the next four variables are most highly correlated with Factor 2, the next
three variables are most highly correlated with Factor 3, and the last two are most highly
correlated with Factor 4.

ANALYSIS OUTPUT FOR THE FOUR-COMPONENT/FACTOR STRUCTURE
347
FIGURE 38.17
Factor/Component Structure Matrices of the variables for both the four-
dimensional principal components analysis and the principal axis factor analysis.
Researchers use the values of the correlations between the variables and the factor
as a cue to factor interpretation. As a general guideline, structure coefﬁcients of about
.80 or higher are taken as very strong indicators, coefﬁcients in the .70’s are taken as
relatively strong indicators, those in the .5’s and .6’s are taken as moderate indicators,
and those in the .3’s and .4’s are taken as very modest indicators.
Given these guidelines, we interpret the factors based on those variables correlating
most strongly with it. What those variables conceptually have in common with each other
should signify the core of the factor. Based on the correlations, we interpret the factors
as follows:
• Factor 1 is associated with (indicated most strongly by) lower levels of neoneuro
and negafect (these are negatively correlated with the factor) and higher levels
of regard and acceptnc (these are positively correlated with the factor). What
construct is indicated by less neuroticism, less negative affect, more self-regard,
and more acceptance of self? The answer is the interpretation of the factor, and
we subjectively label it as Mental Health.
• Factor 2 is indicated by higher values of cntrlcpi, bencntrl, neoagree, and aggcon-
trl and appears to represent Emotional Control.
• Factor 3 is indicated by higher levels of depcon, eminhib, and anxcon and appears
to represent Inhibition of Depression.
• Factor 4 is indicated by higher levels of neoextra and posafect and appears to
represent Positive Outgoingness.

348
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
Analysis Output for the Four-Component/Factor Structure:
Evaluation
The four-factor solution was interpretable, and the values of the largest structure coef-
ﬁcients for each variable were acceptable. This outcome therefore appears to be viable.
That said, the solution has four relative weaknesses to it:
• Using the four-component solution means accepting all four components whose
eigenvalues exceeded 1.00, something that researchers are reluctant to do; in the
principal axis factor analysis, the fourth factor has an uncomfortably low eigen-
value.
• One pair of factors correlated (in the principal axis analysis) in excess of .50; this
is a somewhat higher factor correlation than that would be preferred (correlations
in excess of the .4’s should cause some concerns), and at some point, perhaps in
the range .70, factors correlating too strongly must be judged as redundant.
• One variable (bencntrl) had a rather substantial “cross-loading.”
• One factor (Factor 4) had only two variables as indicators; even with only 13
variables in the analysis and using a four-component/factor solution, we would
like to have at least three indicator variables for a factor.
Some of these weaknesses may potentially be overcome in the three-component/
factor solutions. If this comes to pass and the interpretation is at least equally viable,
then the three-dimensional solution may be preferred.
38.9
THE THREE-COMPONENT/FACTOR STRUCTURE
We set up the analyses exactly as described in Section 38.8 except that we type 3 in the
panel labeled Factors to extract.
The Total Variance Explained tables for the two analyses are presented in
Figure 38.18. Consistent with the four-dimensional solution, the ﬁrst three principal
components cumulatively explain 60.637% of the total variance. The principal axis
factor solution has accounted for less variance because it was targeting the common
rather than the total variance; the ﬁrst three principal axis factors cumulatively explain
49.977% of the total variance. Furthermore, the third principal axis factor is associated
with an eigenvalue just under the general criterion of 1.00; however, that may be
acceptable if the factor structure lends itself to a reasonable interpretation.
Figure 38.19 presents the Communalities for the variables. In the principal compo-
nents analysis, most of the variables have Extraction communality values in excess of the
high .4’s, partly because the analysis explains more than 60% of the variance, whereas
in the principal axis factor analysis, several variables have relatively low Extraction
communality values.
The component/factor correlation matrices are shown in Figure 38.20. Compo-
nents/Factors 2 and 3 are uncorrelated but the two other pairs are sufﬁciently correlated
to warrant an oblique rotation but not sufﬁciently correlated so that we judge them to
represent the same underlying construct.
The structure matrices from the principal components and principal axis factor anal-
yses resulting from the promax rotations are shown in Figure 38.21. They present the
same dimensional structure, but the correlations from the principal components analy-
sis are substantially stronger; this should not come as a surprise because this analysis
accounted for more variance and, as a result, yielded larger communality values for the
variables. We focus on the principal axis factor structure (for explication purposes) and
interpret the factors as follows:

THE THREE-COMPONENT/FACTOR STRUCTURE
349
FIGURE 38.18
Total Variance Explained for both the three-dimensional principal components
analysis and the principal axis factor analysis.

350
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
FIGURE 38.19
Communalities of the variables for both the three-dimensional principal com-
ponents analysis and the principal axis factor analysis.
• Factor 1 is associated with (indicated most strongly by) lower levels of neoneuro
and negafect (these are negatively correlated with the factor) and higher levels of
regard, posafect, neoextra, and acceptnc (these are positively correlated with the
factor). Note that this factor is a combination of Factors 1 and 4 in the four-factor
solution; that is, the ﬁrst factor in the three-factor solution splits into two in the
four-factor solution. We can imagine people who have higher values on this factor
to be stable, conﬁdent, and positive in their attitude; if we had to provide a label
to this construct, one possibility might be Positive Life Adjustment.
• Factor 2 is indicated by higher values of cntrlcpi, bencntrl, aggcontrl, and neoa-
gree and appears to represent Emotional Control.
• Factor 3 is indicated by higher levels of depcon, eminhib, and anxcon and appears
to represent Inhibition of Depression.
38.10
DETERMINING WHICH SOLUTION TO ACCEPT
Overall, the three-component/factor structure has overcome most of the weaknesses of
the four-component/factor structure while supporting a viable interpretation of the dimen-
sions. The one remaining weakness is a little “cross-loading” for negafect in both analyses
and for neoagree in the principal axis factor solution, but there was some “cross-loading”
in the four-component/factor solution anyway. We therefore are very comfortable in
accepting the three-component/factor solution over the four-component/factor solution.

DETERMINING WHICH SOLUTION TO ACCEPT
351
FIGURE 38.20
Factor/Component Correlation Matrices of the variables for both the
three-dimensional principal components analysis and the principal axis
factor analysis.
FIGURE 38.21
Factor/Component Structure Matrices of the variables for both the three-
dimensional principal components analysis and the principal axis factor analysis.

352
PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
The researchers in this example would still need to decide which extraction method
to report in disseminating the results of the research. Because the principal components
solution provides for somewhat stronger structure coefﬁcients, less “cross-loading,” and
communality values that have achieved our general criterion of the high .4’s or better,
we would select the principal components solution. However, the apparent increment
in explained variance (about 60% for principal components compared to about 50% for
principal axis factoring) is really not a tangible gain, as principal axis factoring was aimed
at explaining only that portion of the variance common to the set of variables rather than
the total amount of variance targeted by principal components. Thus, other researchers
may very well opt to report the results of the principal axis analysis; ultimately, such
a decision will reﬂect the professional preferences of each research team. However, the
choice of which extraction procedure to report does not affect the interpretation of the
dimensional structure of the variables.

C H A P T E R
3 9
Conﬁrmatory Factor Analysis
39.1
OVERVIEW
Principal components and factor analysis, covered in Chapter 38, are considered to be
exploratory procedures in the sense that the components/factors are allowed to emerge
from the data analysis. Researchers thus approach such an analysis from an inductive or
bottom-up perspective even if they have some informal expectations about the nature of
the dimensional structure they expect to ﬁnd. Consistent with such an inductive approach,
it is not unusual for researchers using these procedures to examine a small set of possible
solutions (e.g., three, four, and ﬁve components/factors) in deciding on the one that they
will ﬁnally select.
Conﬁrmatory factor analysis represents a deductive or top-down approach. Here,
researchers hypothesize an explicit factor structure (a model) based on a theoretical or
an empirical framework. The analysis imposes the hypothesized model on the data and
assesses the extent to which that formulation ﬁts the data. Following are two of the major
differences between exploratory and conﬁrmatory techniques:
• In exploratory principal components and factor analyses, every variable is associ-
ated with every component or factor; the goal is to have each variable associated
strongly with only one component or factor and thus weakly associated with the
others. In conﬁrmatory factor analysis, researchers ordinarily specify each variable
to be associated with only one factor.
• Rotated components and factors in the exploratory technique are either all required
to be uncorrelated (resulting from an orthogonal rotation) or are all allowed to
correlate (resulting from an oblique rotation). In conﬁrmatory factor analysis,
researchers specify which factors, if any, are to be correlated; thus, in a single
analysis, researchers may hypothesize that some factors are correlated and that
other factors are orthogonal.
Conﬁrmatory factor analysis is carried out in specialized structural equation modeling
(SEM) software; in this book, we use the IBM SPSS® Amos add-on for that purpose.
Using the software, we conﬁgure the model in diagram form. An example of a simple,
generic conﬁrmatory factor analysis model is presented in Figure 39.1. The general
features of such a model are as follows:
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
353

354
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.1 A generic example of a conﬁrmatory factor analysis model.
• The two large ovals depict the hypothesized factors named Factor A and Factor B.
They are drawn as circles/ovals to represent them as latent (unobserved) variables
or constructs; latent variables are always drawn as circles/ovals. There can be
any number of factors in the analysis; we have chosen two factors for illustration
purposes.
• The double-headed arrow (path) between the factors represents a correlation
between them; if researchers believed the factors to be orthogonal (independent),
they would not draw such a path between the factors. In the ﬁnal model, the
value of the correlation between the factors derived from the analysis is written
on that path.
• The rectangles represent the observed or measured variables in the data ﬁle; mea-
sured variables are always drawn as squares/rectangles. Three of the measured
variables are hypothesized to be associated with one factor, and the other three
measured variables are hypothesized to be associated with the other factor.
• Single-headed arrows (paths) emerge from the factor pointing to each associated
measured variable. This represents the direction of “causal” or predictive ﬂow.
In this context, the measured variables are conceived of as indicators (minirep-
resentations) of their associated factor. In the ﬁnal model, it is common practice
to place on these paths the value of the standardized (path/regression) coefﬁcients
estimated from the data analysis.
• Because the latent variables require a scale or metric, the ﬁrst drawn of the indicator
(measured) variables for each factor is selected and the path from the latent variable
to that measured variable is assigned (constrained to) the value 1 (unity). This
constraining of the path to unity applies the scale from the measure to the latent
variable. These path coefﬁcients are the “loadings” of the variable of the factor and
are ordinarily the pattern/structure coefﬁcients resulting from the prediction of the
measured variables based on the latent variable (the factor). As part of the analysis,
the standardized values will be estimated from the data so that the constraint placed
on the ﬁrst indicator variable will not adversely impact the eventual solution.
• Each indicator variable is associated with its own latent variable, with the path
pointing from each latent variable to the respective measured variable. These latent
variables represent measurement error and are named e1 through e6 inclusive
in Figure 39.1. The error components are needed to specify the model because
an SEM analysis is a variation of a multiple regression analysis where, unlike

DRAWING THE MODEL
355
a regression analysis where the dependent variable is the predicted Y value, the
variable predicted in an SEM analysis is the actual Y value (here, Y is the measured
variable). Thus, the structural equation must include the unexplained (error or
residual) variance of Y, which is the error term; that is, whatever variance of a
given measured variable that is not explained by the factor is attributed to the error
term so that explained variance plus error variance is equal to the total variance
of the measured variable.
• Error terms are constrained to unity to apply the scale from the measure to the error
term. These path coefﬁcients will also be reestimated in the process of performing
the analysis.
• Researchers may choose to hypothesize that some error terms associated with the
indicator variables are correlated. In the model we have drawn in Figure 39.1,
the error terms are hypothesized to be uncorrelated. Assuming uncorrelated errors,
however, is often an oversimpliﬁcation that adversely affects the ﬁt of the model
to the data. Error variables may be related to each other for several reasons. For
example, two indicator variables may measure a common extraneous construct or,
if they are items on an inventory, the items with which they are associated may
be worded in a similar manner (e.g., reverse worded) or may share a common
reading level that is somewhat different from the other items (Brown & Moore,
2012; Kline, 2011; Wang & Wang, 2012). For such reasons, the errors would
be hypothesized as being correlated. If the researchers choose not to hypothe-
size any correlated errors at the outset of the analysis, they may wish to revise
that assumption based on the results of the analysis (IBM SPSS Amos provides
suggested modiﬁcations—additional paths—that can be added to the model to
improve model ﬁt) and then perform a more exploratory analysis on a revised
model that speciﬁes certain correlated errors. Our example will illustrate this.
39.2
NUMERICAL EXAMPLE
The purpose of this ﬁctitious study was to assess the internal structure of a potential
clinical assessment instrument that researchers hoped was able to differentiate grief from
depression. Grief is composed of three measured variables: no_energy, lonely, and past
(a focus on the past); depression is also composed of three measured variables: guilt,
pessimism, and self_critical. The data ﬁle is named grief and depression.
We note that there are no missing values associated with the variables that we
use in the analysis. IBM SPSS Amos is fussy about this; it will estimate means and
intercepts for those missing values before evaluating the model but will not produce
other important output (e.g., the Modiﬁcation indices) if there are missing values in
the data. We most strongly suggest that missing values be replaced by estimated values
(using a single imputation method such as regression or using more elaborate multiple
imputation procedures) through the Missing Values Analysis module (see Meyers et al.
(2013)) or that cases with missing values be removed from the data ﬁle before performing
any analysis in IBM SPSS Amos.
39.3
DRAWING THE MODEL
Drawing the Model: Some Useful Command Functions
We open the grief and depression data ﬁle and from the main menu we select Analyze
➔IBM SPSS Amos. The initial IBM SPSS Amos window is shown in Figure 39.2,
where the following portions of the screen are identiﬁed in our annotation:

356
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.2 The initial IBM SPSS Amos screen, with the different areas labeled.
• The icon toolbar from which we select various functions such as drawing cer-
tain objects (e.g., latent variables) or performing certain operations (e.g., erasing
objects).
• The output panel where we can select what portion of the output to display once
the analysis has been performed.
• The drawing area where we conﬁgure the diagram of our model.
• The main menu from which we can conﬁgure our analysis.
The commands on the icon toolbar are activated with a single click. Some helpful
functions from the middle of the icon toolbar are shown in Figure 39.3. To be used, some
object or objects need to have been drawn in the drawing area. Generally, we select the
icon (or select a function from the main Edit menu). This action changes our cursor to
that icon. We then place the cursor on the object and click or drag as appropriate. To
discontinue the function, we move the cursor to an empty portion of the drawing area
and right-click. Here is how to work with some of the functions:
• To select an object or objects, we select the Select one object at a time icon and
then click on our target objects one at a time.

DRAWING THE MODEL
357
FIGURE 39.3
Some useful command functions from
the icon toolbar.
• To select all objects in the drawing area, we select the Select all objects icon.
• To deselect all objects that have been selected, we select the Deselect all objects
icon.
• To move an object, we select the object, select the Move objects (truck icon),
click on our target object in the drawing area, and drag it elsewhere.
• To move a set of objects (e.g., a factor with its indicator variables and their
associated errors), we select all of the separate elements, select the Move objects
(truck icon), and then drag the set of objects to another location on the screen.
• To erase an object, we select the Erase (stylized X) icon and click on the target
object.
• To copy and paste an object or set of objects, we select our object or objects,
select Edit ➔Copy to clipboard from the main menu, and then select Edit ➔
Paste. The copied item or items will rest on top of the original and can simply be
moved by selecting the Move icon and dragging the object or objects.
Drawing the Model: the Drawing Tools
Because conﬁrmatory models usually take a fair amount of horizontal space to draw, we
opt to switch from the default portrait (long) orientation to landscape (wide) orientation.
To change the orientation, select View ➔Interface Properties from the main menu. This
action opens the Interface Properties window as seen in Figure 39.4. Select Landscape-
Legal under Paper Size, select Apply, and close the window.
From the icon toolbar, select the icon that represents Draw a latent variable or add
an indicator to a latent variable (the circle with squares and more circles), as shown in
Figure 39.5. We use this tool to draw our latent variables. After activating this function
by selecting it, place the cursor (which has now taken on the icon) toward the right center
of the drawing area (see Figure 39.6) and click to draw the factor. The drawn object
will take the shape dictated by the movement of the cursor and, with a little practice, the
desired shapes can be readily drawn. Then with the cursor inside the circle/oval, click
one more time to draw the ﬁrst indicator variable and its associated error; the size of the
rectangle will be proportional to the shape of the factor. Note that both paths have been
constrained to the value 1 to scale each latent variable (the factor and the error) to the
measured variable.
The results of the above actions are shown in Figure 39.6. In this illustration, we
have intentionally drawn the factor too small (to illustrate what can be done to remedy
the situation), and as a result, the rectangle for the indicator variables is a bit too small
to contain the names we will assign them. However, we can easily resize/reshape these
ﬁgures. With the cursor inside the oval for the factor we right-click and select Shape of
Object from the pop-up menu (also shown in Figure 39.6). We then place our cursor

358
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.4
The Interface Properties window where we select
Landscape-Legal page orientation.
FIGURE 39.5 Selecting the Draw a latent variable or add an indicator to a latent variable
function.

DRAWING THE MODEL
359
FIGURE 39.6 The ﬁrst factor with its ﬁrst indicator variable is drawn. We will modify the shape
of these ﬁgures before drawing the rest of the conﬁrmatory model.
inside each shape and stretch (drag) each to suit our aesthetic preferences. Once we
resize these shapes to meet our preferences, we deselect the Shape of Object function
by moving the cursor to a blank area of the screen and right-click. Then we again select
the Draw a latent variable or add an indicator to a latent variable icon, place the icon
inside the oval representing the factor, and click twice to generate two more indicator
variables (these additions will match our new shape). We then repeat the process to draw
the second factor. The ﬁnal product is shown in Figure 39.7.
Our next task is to associate each measured variable with a variable from our data
ﬁle. From the main menu, select View ➔Variables in Dataset. This opens the Variables
in Dataset window pictured in Figure 39.8. Select each variable in turn and drag it to
the appropriate rectangle so that all observed variables are properly conﬁgured as shown
in Figure 39.9. When ﬁnished, close the Variables in Dataset window.
We next draw the correlation (the covariance) path between the two factor vari-
ables because we hypothesize that the two factors are correlated. From the icon toolbar,
select the Draw covariances (double headed arrow) as shown in Figure 39.10. Covari-
ance is the (unstandardized) correlation between variables (Norman & Streiner, 2008)
whose value reﬂects the particular metric associated with the variables (and thus can take
any numeric value), whereas a correlation coefﬁcient such as the Pearson r represents
a standardized correlation value that is constrained to vary between ±1.00. With the
Covariance tool activated, place the cursor on the left border of the second factor and
click and drag to the ﬁrst factor to draw the covariance (correlation) path as shown in
Figure 39.11.
We also need to name the factors. Double-clicking inside a factor activates the Object
Properties window shown in Figure 39.12. We type grief in the Variable name panel

360
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.7 The latent variables in the conﬁrmatory model are now drawn.
FIGURE 39.8 The Variables in Dataset window.

DRAWING THE MODEL
361
FIGURE 39.9 All of the indicator variables have been associated with the proper variables in the
data ﬁle.
FIGURE 39.10 Select the Draw covariances (double headed arrow) from the icon toolbar.

362
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.11 The correlation between the factors is now drawn.
FIGURE 39.12 The covariance between the factors is drawn and, by double-clicking inside the
ﬁrst factor, we have activated the Object Properties window.

ANALYSIS SETUP
363
FIGURE 39.13 The ﬁrst factor is now named.
and (for aesthetic reasons given the size of our ovals) increase the font size to 36; this
name shows up in the diagram as shown in Figure 39.13. Repeating this process labels
the second factor as depression (see Figure 39.14); we then close the Object Properties
window.
We now need to name the latent error variables (called unique variables by IBM
SPSS Amos). IBM SPSS Amos will name these generically. From the main menu, select
Plugins ➔Name Unobserved Variables (see Figure 39.15). On selecting this command,
the unique variables will be labeled with an “e” (for error term) and given a sequential
number (see Figure 39.16) resulting in the variable names e1 through e6.
39.4
ANALYSIS SETUP
With the conﬁrmatory factor analysis diagram completed, our next task is to conﬁgure
the analysis. From the main menu we select View ➔Analysis Properties to open the
Analysis Properties window as shown in Figure 39.17. Select the Output tab. Mini-
mization history is checked by default. Check Standardized estimates (these produce
the standardized regression coefﬁcients for the prediction of the measured variables) and
Modiﬁcation indices (these produce IBM SPSS Amos “suggestions” on how to improve
the model ﬁt).
To execute the analysis, select Analyze ➔Calculate Estimates from the main menu.
As this is our ﬁrst analysis, we are presented with the operating system’s Save As dialog
window (see Figure 39.18). We select a location to save the ﬁle, provide a ﬁle name,
and Save.

364
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.14 Both factors are now named.
FIGURE 39.15 To name the latent error variables, we select Plugins ➔Name Unobserved
Variables from the main menu.

ANALYSIS SETUP
365
FIGURE 39.16 On selecting x, the latent error variables are labeled e1 through e6.
FIGURE 39.17
The Analysis Properties window.

366
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.18 Performing the Save As function.
39.5
ANALYSIS OUTPUT
Analysis Output: Initial Screens
To access the results of the analysis, select the View the output path diagram icon
(the one on the right) in the top output panel immediately to the left of the drawing
area as shown in Figure 39.19. This action causes the conﬁrmatory diagram with the
Unstandardized estimates to be displayed (as highlighted in the fourth output panel
from the top in the stack of panels immediately to the left of the drawing area). These
Unstandardized estimates include the unstandardized regression coefﬁcients and the
covariance (unstandardized correlation) between the two latent variables, as well as the
variances of the latent variables. These values also appear in tabular form as part of the
Text Output as described below.
To view the standardized coefﬁcients, select Standardized estimates in the fourth
output panel from the top just under Unstandardized estimates. These are shown in
Figure 39.20. In addition to the standardized regression coefﬁcients, the correlation (the
standardized covariance) of the two latent variables is also included in the display. All
of these values are available in output tables as well.
To obtain the output in tabular form, select from the main menu View ➔Text
Output. This action opens the Notes for Model screen of the Output window shown in
Figure 39.21. The Default model referred to in the ﬁrst line on the screen is the model
that was hypothesized by the researchers. The two most important pieces of information
we learn from this very global overview are as follows:
• The degrees of freedom are positive, indicating that the model is identiﬁed and
thus viable (Meyers et al., 2013). We discuss this in a little more detail in the
context of our SEM analysis in Section 43.8.

ANALYSIS OUTPUT
367
FIGURE 39.19 The Unstandardized estimates are displayed on selecting View the output path
diagram.
FIGURE 39.20 The Standardized estimates are displayed.

368
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.21 Having selected Text Output presents us with the Notes for Model screen.
• The chi-square value associated with the model is 31.765 and, with 8 degrees
of freedom, is statistically signiﬁcant (p < .001). This indicates that the expected
values based on the model differ signiﬁcantly from those represented by the data;
that is, the model does not appear to adequately ﬁt the data. That said, most
researchers rely on other ﬁt measures in addition to chi-square before drawing
conclusions about how well the model ﬁts the data.
Analysis Output: Model Fit
One of the very important aspects of the output of an SEM analysis is the set of indexes
of model ﬁt. Such indexes assess how well the hypothesized structure ﬁts the data; the
better the ﬁt, the more the support for the model. There are quite a few ﬁt indexes that
have been developed over the years, and there are some differing opinions that are more
appropriate to apply in any given research study (e.g., Hu & Bentler, 1999; Kline, 2011;
West, Taylor, & Wu, 2012).
Most writers, such as J¨oreskog and S¨orbom (1996) and Bentler (1990), advise against
the sole use of the chi-square value in judging the overall ﬁt of the model because
of the sensitivity of the chi-square to sample size. Larger sample sizes are associated
with increased power, and with large sample sizes, the chi-square test can detect small
unimportant discrepancies between the observed and predicted covariances and suggest

ANALYSIS OUTPUT
369
that the model does not ﬁt the data. Meyers et al. (2013) recommend reporting four ﬁt
measures in addition to chi-square. These are as follows:
• Goodness-of-Fit Index (GFI). The GFI is conceptually similar to the R2 in multiple
regression (Kline, 2011) and represents the proportion of variance in the sample
correlation/covariance accounted for by the predicted model.
• Normed Fit Index (NFI). The NFI, also known as the Bentler–Bonett normed ﬁt
index, equals the difference between the chi-square of the null model and the chi-
square of hypothesized model divided by the chi-square of the null model. For
example, an NFI of .95 indicates the model of interest improves the ﬁt by 95%
relative to the null or independence model.
• Comparative Fit Index (CFI). The CFI (Bentler, 1990) roughly represents the
extent to which the hypothesized model is a better ﬁt than the independence or
null model (where no correlations are assumed to exist between the variables).
• Root-Mean-Square Error of Approximation (RMSEA). The RMSEA is the average
of the residuals between the observed correlation/covariance from the sample and
the expected model estimated for the population (Browne & Cudeck, 1989, 1993).
Several sets of authors have proposed guidelines for interpretation of these ﬁt indexes
(e.g., Meyers et al., 2013; Schreiber, Nora, Stage, Barlow, & King, 2006; West et al.,
2012). Generally, the present consensus is that a value of .95 for the GFI, NFI, and CFI
would be taken to represent a good ﬁt of the model to the data, although blindly applying
these “lines in the sand” is ill-advised (Kline, 2011). For the RMSEA, values of .06, .08,
and greater than .10 are considered to represent a good, adequate, and poor ﬁt of the
model to the data, respectively.
Selecting Model Fit in the left panel displays the several ﬁt indexes produced by
IBM SPSS Amos; these are presented in Figure 39.22, Figure 39.23, and Figure 39.24
(we could not show them all in a single screenshot). The CMIN table shown at the top in
Figure 39.22 presents the minimum discrepancy value in the Default model row, which
is the chi-square goodness-of-ﬁt test that repeats the information we saw in the Model
Notes screen. The CMIN/DF, also shown in Figure 39.22, is the chi-square divided by
the degrees of freedom (31.765/8 = 3.971). Some researchers suggest values as high as
ﬁve for an acceptable ﬁt, while others maintain relative chi-square be two or less. Our
value falls between these two guidelines.
The GFI is shown in the middle table in Figure 39.22 where we see its value of
.967 exceeds our guideline value of .95, suggesting an acceptable level of model ﬁt. The
AGFI (adjusted goodness-of-ﬁt index) and the PGFI (parsimony goodness-of-ﬁt index)
represent adjustments to the GFI for degrees of freedom and number of parameters,
respectively; guidelines for acceptable model ﬁt are .95 for the AGFI and .50 or better
for the PGFI. The RMR (root-mean-square residual) is a measure of the average size of
the residuals between actual covariance and the proposed model covariance, with smaller
RMRs indicating a better ﬁt.
The NFI and the CFI are shown in the bottom table of Figure 39.22 where we see
that their values of .922 and .939 fall short of our guideline value of .95, suggesting
a less than acceptable level of model ﬁt. The RMSEA is presented in the top table of
Figure 39.24. Its value of .095 also suggests that the model does not ﬁt the data all that
well.
In summary, the chi-square, NFI, CFI, and RMSEA indexes paint a picture of a
model that falls sufﬁciently short of ﬁtting the data to meet our criteria of a good ﬁt. We
will examine the regression coefﬁcients to learn the details of the results, but we probably
want to modify the model to improve the ﬁt if the Modiﬁcation Indices suggest any
reasonable options.

370
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.22 Model ﬁt indexes (ﬁrst of three screenshots).
FIGURE 39.23 Model ﬁt indexes (second of three screenshots).

ANALYSIS OUTPUT
371
FIGURE 39.24 Model ﬁt indexes (third of three screenshots).
Analysis Output: Estimated Parameters
Selecting Estimates in the left panel displays the estimated parameters generated by the
model, the ﬁrst portion of which is shown in Figure 39.25. The Regression Weights
Table in the top table of Figure 39.25 presents the unstandardized regression coefﬁcients
in the Estimate column together with their standard errors (S.E.). These are the regression
weights estimated using the relevant latent variable to predict the value of the measured
variable, except for the two paths that were initially constrained in specifying the model.
For example, the unstandardized regression weight estimated for lonely as predicted by
grief is .948, with a standard error of .105.
Each estimated regression coefﬁcient is tested for statistical signiﬁcance, the result
of which is shown in the P column; a triple asterisk (***) indicates a probability level
of p < .001. All unconstrained regression coefﬁcients were statistically signiﬁcant.
The third column (C.R.) reports the critical ratio, a value obtained by dividing the
unstandardized regression coefﬁcient by its standard error. The C.R. operates as a z
statistic with values of 1.96 or greater indicating statistical signiﬁcance.
The second table in Figure 39.25 presents the standardized regression coefﬁcients
(often referred to informally as “factor loadings”) associated with each path. Note that
the two constrained paths whose unstandardized values were retained as 1.000 in the
Regression Weights Table do have estimated standardized values associated with them.
In reporting the results of the model in diagram form, these are the values that are

372
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.25 The ﬁrst portion of the Estimates screen.
ordinarily placed on the paths. All of these standardized regression coefﬁcients appear to
be of adequate value for us to judge that the measured variables appear to be reasonable
indicators of their latent construct.
Figure 39.26 presents the remaining estimated parameters. The top two tables display
the correlation between the factors in unstandardized form (Covariances) and standard-
ized form (Correlations). As may be seen, the correlation between the two factors is .723.
Such a correlation is quite high, suggesting that the two factors may not be independent
enough of each other to justify a two-factor structure.
Analysis Output: Modiﬁcation Indices
The suggested modiﬁcations offered by IBM SPSS Amos deal with adding parameters to
the model on the assumption that researchers can ﬁgure out for themselves if they wish to
remove any paths that were not statistically signiﬁcant. It must be emphasized that these
suggestions are generated purely statistically without any relationship to the theoretical,
empirical, and/or common sense world in which the researchers actually live; thus, these
suggested modiﬁcations need to be treated with more than a grain of salt as they may
make no sense either theoretically, empirically, or both. Furthermore, by making any
modiﬁcations to the model, the orientation of the analysis shifts from a conﬁrmatory
approach to more of an exploratory approach; by deﬁnition, the model ﬁt will improve
when any of these post hoc modiﬁcations are made to the model, so researchers should
not interpret a model ﬁt improvement as strong support for their modiﬁed model.
Given that the model appears to fall short of an acceptable ﬁt to the data and that the
two factors are correlated to an uncomfortable extent (i.e., the correlation in excess of .7
raises the possibility that the two factors are not able to be effectively distinguished), it is
probably worthwhile to examine the suggestions offered by IBM SPSS Amos to improve

ANALYSIS SETUP: MODIFIED MODEL
373
FIGURE 39.26 The last portion of the Estimates screen.
model ﬁt. We can view these suggested modiﬁcations by selecting Modiﬁcation Indices
in the left panel as shown in Figure 39.27.
The top table in Figure 39.27 labeled Covariances shows suggested additions of
Covariances (correlations). The ﬁrst column indicates the suggested parameter to add,
and the M.I. column indicates the improvement in the chi-square value should that single
change be made. The only two theoretically justiﬁable tweaks that appear in the table
are those that deal with correlating the errors of sets of indicator variables that are each
associated with the same factor. This is because it is possible that the errors may tap
into a common extraneous construct or that the items may be similarly worded in some
manner. These two suggested modiﬁcations are as follows:
• Adding a correlation between e1 and e2 (the errors associated with no_energy
and lonely indicators of grief).
• Adding a correlation between e5 and e6 (the errors associated with pessimism and
self_critical indicators of depression).
We therefore accept these two suggestions and implement the two modiﬁcations in
a modiﬁed model.
The bottom table in Figure 39.27 labeled Regression Weights shows suggested
additions of paths. As none suggest adding additional paths from the factors to indicator
variables, and because it makes no theoretical sense to tie the indicator variables together
in a causal chain, we do not accept any of these ideas.
39.6
ANALYSIS SETUP: MODIFIED MODEL
Select the window showing the model diagram to make it the active window. Then select
the View the output path diagram icon to the left in the top output panel next to the

374
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.27 The Modiﬁcation Indices.
FIGURE 39.28 Select the View the output path diagram icon in the top panel next to the
diagram.

ANALYSIS SETUP: MODIFIED MODEL
375
FIGURE 39.29 Select the Draw covariances (double headed arrow) from the icon toolbar.
FIGURE 39.30 The Standardized estimates for the modiﬁed model.

376
CONFIRMATORY FACTOR ANALYSIS
FIGURE 39.31 The model ﬁt windows showing the ﬁt indexes of interest to us.

ANALYSIS SETUP: MODIFIED MODEL
377
FIGURE 39.32 The Estimates windows.

378
CONFIRMATORY FACTOR ANALYSIS
diagram to reach the screen shown in Figure 39.28. Select the double-headed path icon
Draw covariances (double headed arrow) from the icon toolbar, and draw in the two
covariances needed, as has been done in Figure 39.29.
39.7
ANALYSIS OUTPUT: MODIFIED MODEL
From the main menu, select Analyze ➔Calculate Estimates. Click the View the output
path diagram icon. This action produces the Standardized estimates (see Figure 39.30).
Select from the main menu View ➔Text Output. This action opens the Notes for Model
screen of the Output window (not shown).
Select Model Fit from the panel on the right side of the screen. The two screens
showing the model ﬁt indexes of interest to us are presented in Figure 39.31. The chi-
square (CMIN) value dropped to a statistically nonsigniﬁcant value of 4.846 (p = .564),
indicating that the expected values based on the model were relatively closely matched
by the data. We can also see that the GFI improved to .995, the NFI went to .988,
the CFI topped out at 1.000, and the RMSEA bottomed out at .000. This model—not
surprisingly—ﬁts the data better, now that we have hypothesized that the two pairs of
errors are correlated; in fact, the model represents a superb (post hoc) ﬁt to the data.
Selecting Estimates from the left panel displays the parameter estimates shown in
Figure 39.32. All paths are associated with statistically signiﬁcant coefﬁcients, and the
standardized path coefﬁcients appear to be respectable.
Of much greater interest is the correlation between the factors. In this revised model,
the factors now correlate to .906. Such a very strong correlation is unacceptable, sig-
nifying that the factors lack a sufﬁcient individual identity to justify both of them. We
would therefore conclude that, despite the superb ﬁt of the model to the data, there is
not a viable two-factor structure describing these indicator variables; in fact, we would
guess that the six indicator variables are likely to represent a single latent construct (this
idea can be explored by constructing and testing a single-factor conﬁrmatory model).

P A R T 13
EVALUATING CAUSAL
(PREDICTIVE) MODELS


C H A P T E R
4 0
Simple Mediation
40.1
OVERVIEW
Simple mediation analysis is a conceptual extension of regression analysis. Instead of just
specifying a set of predictor variables, in a mediation analysis, the variables are arranged
in a predictive (sometimes referred to as causal) path model to assess the dynamics of
their interplay. The model represents the hypothesis of the researchers concerning how
the variables interrelate. Mediation analysis was popularized by Baron and Kenny (1986)
and is described in several more recent sources (e.g., MacKinnon, 2008; MacKinnon,
Fairchild, & Fritz, 2007; Preacher & Hayes, 2008).
Simple mediation analysis is the least complex type of path model and is shown
in generic form in the top portion of Figure 40.1. It relates just three variables to each
other: a dependent or outcome variable Y, an independent or predictor variable X, and a
potential mediator M. The issue addressed in such an analysis is whether the relationship
between X and Y (the prediction of Y based on X) is attenuated when M is included
together with X as a predictor of Y.
In the model presented in the top part of Figure 40.1, X is hypothesized to affect or
inﬂuence Y in two ways. One type of inﬂuence is direct and is represented by the path
(the single-headed arrow) leading from X directly to Y. The other type of inﬂuence is
indirect and is represented by the path leading from X through M to Y. The key issue in
a simple mediation analysis is the extent to which the presence of the mediator variable
M changes the degree of prediction we would otherwise observe for X predicting Y in
isolation.
To justify applying a mediation analysis, the following conditions should be met
(Meyers et al., 2013):
• X must signiﬁcantly predict Y in isolation.
• X must signiﬁcantly predict M in isolation.
• M must signiﬁcantly predict Y in the mediation model.
When these conditions are satisﬁed, we can determine the effect of M on the direct
relationship between X and Y. There are four logically possible effects that may be
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
381

382
SIMPLE MEDIATION
FIGURE 40.1
The general mediation model (top
path diagram) and the mediation
model that will be tested, with
regard mediating the effect of
beckdep on negafect (bottom path
diagram).
observed in the mediated model:
• The direct relationship between X and Y is fully attenuated (X does not signif-
icantly predict Y with M in the model). With this result, we say that we have
observed full mediation.
• The direct relationship between X and Y is less strong with M in the model than
it was in isolation, but is still statistically signiﬁcant. With this result, we say that
we have observed partial mediation.
• The direct relationship between X and Y is as strong with M in the model as it
was in isolation. With this result, we say that we have observed no mediated effect
of M.
• The direct relationship between X and Y is signiﬁcantly stronger with M in the
model than it was in isolation. With this result, we say that M has acted as a
suppressor variable; that is, the presence of M has enhanced the prediction of X
on Y. This suppression effect can occur because M has accounted for some of the
variance in Y not predictable by X (Darlington, 1990; Pedhazur, 1997); in some
sense, the presence of M helps “purify” the relationship between X and Y (Meyers
et al., 2013).
40.2
NUMERICAL EXAMPLE
The data we use for our example are extracted from a study of personality variables on
425 university students. Data are contained in the data ﬁle named Personality. We focus
on three of the personality variables: beckdep as a measure of depression, regard as
a measure of self-regard, and negafect as a measure of the amount of negative affect
generally experienced by respondents.
40.3
ANALYSIS STRATEGY
The mediation model is shown in the bottom portion of Figure 40.1 as a path diagram.
In this illustrative example, we have hypothesized that regard mediates the relationship
between beckdep and negafect. To fully evaluate this model, we will perform a total of
three linear regression analyses:

THE INDEPENDENT VARIABLE PREDICTING THE MEDIATOR VARIABLE
383
• We will use the independent variable beckdep to directly predict the mediator
variable regard.
• We will use beckdep and regard to predict the outcome variable of negafect.
• We will generate the unmediated model with beckdep predicting negafect in
isolation.
Our goal is to obtain the standardized (beta) and unstandardized (raw score) regres-
sion coefﬁcients as well as the SEs associated with the unstandardized regression coefﬁ-
cients. We will then use that information to test the statistical signiﬁcance of the indirect
effect (the mediated effect of beckdep acting through regard to affect negafect) by
performing the Aroian test. We will also evaluate the difference in the path coefﬁcients
between beckdep and negafect in the unmediated and mediated models by perform-
ing the Freedman–Schatzkin test. Finally, we will determine the relative strength of the
mediated effect.
40.4
THE INDEPENDENT VARIABLE PREDICTING
THE MEDIATOR VARIABLE
From the main menu, we select Analyze ➔Regression ➔Linear. This opens the main
Linear Regression window shown in Figure 40.2. We move regard into the Dependent
panel and beckdep into the Independent(s) panel.
In the Statistics dialog window shown in Figure 40.3, we check Estimates (to obtain
the regression coefﬁcients), Model ﬁt (to obtain the R2 and adjusted R2), R squared
change, Descriptives (to obtain the descriptive statistics), and Part and partial corre-
lations (to obtain the zero-order, partial, and semipartial correlations). Click Continue
to return to the main dialog window and click OK to perform the analysis.
The results of the analysis are shown in Figure 40.4. We have obtained a statistically
signiﬁcant amount of prediction; the unstandardized regression coefﬁcient for beckdep
FIGURE 40.2
The main Linear Regression dialog
window.

384
SIMPLE MEDIATION
FIGURE 40.3
The Statistics screen of Linear Regression.
FIGURE 40.4 Results of predicting regard with beckdep.
was −.814 with an SE of .062, and the beta (standardized) coefﬁcient (which is the
Pearson r) was −.539.
40.5
THE INDEPENDENT VARIABLE AND THE MEDIATOR
PREDICTING THE OUTCOME VARIABLE
We set up the analysis in the same way as described earlier except that we will place
beckdep and regard into the Independent(s) panel and negafect into the Dependent
panel.

CONSOLIDATING THE RESULTS OF THE MEDIATION MODEL
385
FIGURE 40.5 Results of predicting negafect with beckdep and regard.
The results of the analysis are shown in Figure 40.5. As can be seen in the output,
both the independent variable beckdep and the mediator variable regard are statistically
signiﬁcant predictors of negafect.
40.6
THE UNMEDIATED MODEL WITH THE INDEPENDENT
VARIABLE PREDICTING THE DEPENDENT VARIABLE
We set up the analysis such that beckdep is placed into the Independent(s) panel and
negafect into the Dependent panel. The results of the analysis are shown in Figure 40.6.
As can be seen in the output, the independent variable beckdep is a statistically signiﬁcant
predictor of negafect.
40.7
CONSOLIDATING THE RESULTS OF THE MEDIATION
MODEL
We have drawn in Figure 40.7 the results obtained for the mediation analysis. Each path
is associated with a letter because we will use these letters to represent values in the
equations to test the statistical signiﬁcance of certain aspects of the mediation model. Path
coefﬁcients are associated with each path; for example, the standardized (beta) coefﬁcient
for beckdep predicting regard was −.539, whereas the unstandardized path coefﬁcient
was −.814 with an SE of .062.
The primary issue in conﬁguring a mediation model is to determine if the hypoth-
esized mediator affected the direct relationship between the independent and outcome
variables. Visual examination of the results gives us a clue as to what our tests of signiﬁ-
cance are likely to verify. The unstandardized path coefﬁcient from beckdep to negafect

386
SIMPLE MEDIATION
FIGURE 40.6 Results of the unmediated model with beckdep predicting negafect.
FIGURE 40.7
Results of the path model.
in the unmediated model was .276 and was reduced to .186 in the mediated model (the
corresponding standardized coefﬁcients were .515 and .347). Because both these coefﬁ-
cients were statistically signiﬁcant, and because visual inspection informs us that the path
coefﬁcient is lower in the mediated model, we are led to guess that we have observed a
partial mediation effect. We now use a series of steps to statistically test that guess.
40.8
TESTING THE STATISTICAL SIGNIFICANCE
OF THE INDIRECT EFFECT
One aspect of evaluating statistical signiﬁcance in the mediation model concerns the
indirect effect of beckdep through regard on negafect. Probably, the most widely known
and most frequently used approach to address statistical signiﬁcance of the indirect effect

TESTING THE STATISTICAL SIGNIFICANCE
387
is the Sobel test (Sobel, 1982, 1986) and its variants, the Aroian test (Aroian, 1947) and
the Goodman test (Goodman, 1960). Partly because the sampling distribution of the
indirect effect tends to be somewhat skewed rather than meeting the assumption that
it is normal (causing the tests to lose some statistical power), these tests have received
some criticism in the professional community (e.g., Hayes, 2009; MacKinnon, Lockwood,
Hoffman, West, & Sheets, 2002; Preacher & Hayes, 2004). Nonetheless, these tests are
reported extensively in the research literature and we will apply one of the members of
this family, the Aroian test, here, in that it was the variation of the Sobel family of tests
popularized by Baron and Kenny (1986).
The equation for the Aroian test is presented in Figure 40.8 together with a summary
of the calculations. All three tests comprising the Sobel evaluations are computed in a
similar manner and differ only in how the expression (SEd
2 * SEe
2) at the end of the
square root expression in the denominator of the equation is treated:
• The Aroian test adds the expression to the other terms under the square root sign
in the denominator.
• The Goodman test subtracts the expression from the other terms under the square
root sign in the denominator.
• The Sobel test does not include the expression under the square root sign in the
denominator.
In the equation shown in Figure 40.8, the letters represent the unstandardized regres-
sion coefﬁcients associated with the paths shown in Figure 40.7, and the SE values of the
unstandardized regression weights are shown, with the subscripts indicating the reference
coefﬁcient. The outcome of the equation is a z value with an alpha level of .05 indicated
by a value of 1.96 or better. Here, we obtained a value of approximately 5.79 with a hand
calculator. It is also possible to use the calculator available on Kristopher J. Preacher’s
web site http://quantpsy.org/sobel/sobel.htm, which yields the same approximate value.
The value of 5.79 is sufﬁcient to reject the null hypothesis that the indirect path is not
different from zero; instead, we conclude that the indirect effect of beckdep through
regard on negafect is statistically signiﬁcant.
40.9
TESTING THE STATISTICAL SIGNIFICANCE
OF THE DIFFERENCE BETWEEN THE DIRECT PATHS
IN THE UNMEDIATED AND THE MEDIATED MODELS
The Freedman–Schatzkin test (Freedman & Schatzkin, 1992) compares the relative
strengths of the paths from the independent variable to the outcome variable in the
unmediated model versus the mediated model. The equation is shown in Figure 40.9
together with a summary of the calculations; it produces a t value tested against a Student
FIGURE 40.8
The Aroian test.

388
SIMPLE MEDIATION
FIGURE 40.9
The Freedman–Schatzkin test.
t distribution with N −2 degrees of freedom. Here, we obtained a value of approximately
6.66 with a hand calculator. With 419 degrees of freedom (our N was 421), it is sufﬁcient
to reject the null hypothesis that the coefﬁcients are not signiﬁcantly different (p < .001).
We therefore conclude that the mediated path coefﬁcient is signiﬁcantly lower than the
unmediated path coefﬁcient, indicating that we have obtained a partial mediation effect.
40.10
DETERMINING THE RELATIVE STRENGTH
OF THE MEDIATED EFFECT
We now calculate the relative strength of the mediated effect using the beta coefﬁcients
that are associated with the paths in our mediation model. This is computed as the ratio
of the strength of the indirect effect to the strength of the direct effect and is calculated
as follows:
• The strength of the indirect effect is the product of the beta coefﬁcients associated
with paths beckdep to regard and regard to negafect (paths d and e) in the
mediated model. Here it is equal to (−.539) * (−.312), or .168.
• The strength of the isolated direct effect is the beta coefﬁcient in the unmediated
model (with a value of .515) where beckdep is the single predictor of negafect.
It can also be calculated as the sum of the indirect effect and the beta coefﬁcient
for beckdep predicting negafect in the mediated model (path f): .168 + .347, or
.515.
• The relative strength of the mediated effect is equal to the indirect effect divided
by the direct effect. Here it is equal to .168/.515, or .326.
We may then conclude that about a third (32.6%) of the effect of beckdep on
negafect is mediated through regard.

C H A P T E R
4 1
Path Analysis Using Multiple
Regression
41.1
OVERVIEW
In a multiple regression analysis, we assess only the direct effects of the predictors on the
dependent variable. However, variables can, and often do, have an indirect (mediated)
effect as well as, or instead of having, a direct effect on an outcome. Indirect or medi-
ated effects can be analyzed by conﬁguring the variables into a path structure. Simple
mediation, described in Chapter 40, is the simplest form of path analysis. We extend that
treatment in this chapter to a somewhat more complex illustration.
Sewall Wright (1920) introduced path analysis in the context of examining phylo-
genetic models (Lleras, 2005). The procedure achieved popularity in the 1960s (e.g.,
Blau & Duncan, 1967) in the ﬁeld of sociology but has since steadily gained in research
applications. Path analysis, sometimes called causal modeling, typically involves more
than three variables and therefore subsumes more interrelationships than we see in simple
mediation. The path diagram represents the model that the researchers have hypothesized
weaving the measured variables together in a theoretically or empirically meaningfully
way to explain the variance of the outcome variable. Thus, path analysis is used in a
conﬁrmatory manner to evaluate the structure hypothesized by the researchers rather than
in an exploratory manner such as is done in multiple regression (where the variables are
weighted in a linear or “ﬂat” conﬁguration to statistically maximize prediction).
Because there are more interconnections between variables, and because some vari-
ables take on both roles (in different portions of the analysis), the terms “independent
variable” and “dependent variable” as used in multiple regression are less precise.
This dual-role possibility is illustrated by considering the path model diagrammed in
Figure 41.1. The variables are represented by rectangles to signify that they are observed
or measured variables (as opposed to latent variables that are represented by circles or
ovals). Arrows are used to depict causal or explanatory ﬂow and are referred to as paths.
In Figure 41.1, age and socioeconomic status (SES) are hypothesized to predict the
mediating variable of optimism, and optimism together with age is proposed to predict
quality of life. Thus, optimism takes the role of a dependent variable in one portion
of the analysis (it is predicted by age and SES) but takes the role of an independent
variable in another portion of the analysis when it is one of the predictors of quality of
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
389

390
PATH ANALYSIS USING MULTIPLE REGRESSION
life. The terms that are used to better characterize the roles of the variables in the path
analysis context are as follows:
• Exogenous variables are exclusive predictors, never serving as dependent vari-
ables. These variables are the primary or ﬁrst-order predictors behind the explana-
tory ability of the model, in that there are no variables in the model that are
presumed to “cause” or inﬂuence them; that is, no attempt is made to explain
exogenous variables within the context of the path structure. In the path diagram,
the causal arrows always emanate from but never point to exogenous variables. A
double-arrowed curved line indicates the correlation between exogenous variables.
Age and SES are exogenous variables in the model shown in Figure 41.1.
• Endogenous variables are predicted (explained) by other variables. The explana-
tory variables may be either exogenous variables (e.g., the endogenous variable of
optimism is hypothesized as being explained by the exogenous variables of age
and SES) or other endogenous variables (e.g., the endogenous variable of quality
of life is hypothesized as being partially explained by the endogenous variable of
optimism). In the path diagram, arrows will always point to endogenous variables;
it is also possible for arrows to emanate from endogenous variables. Optimism
and quality of life are endogenous variables in Figure 41.1. Each endogenous
variable will serve as a dependent variable in a separate regression analysis.
• The outcome variable (quality of life in the example in Figure 41.1) is the ultimate
focus of what the model is hypothesized to predict. It is always an endogenous
variable.
There are two separate statistical techniques available to perform a path analysis in
IBM SPSS®, multiple regression and SEM. We describe the multiple regression approach
to path analysis in this chapter and treat path analysis using SEM in Chapter 42.
Using multiple regression, we perform a series of multiple or simple regression anal-
yses, one analysis for each endogenous variable. For the model drawn in Figure 41.1, for
example, we have two endogenous variables and therefore would require two separate
multiple regression analyses—one analysis involves age and SES predicting optimism
and the other analysis involves age and optimism predicting quality of life. The out-
put from these regression analyses is limited to the variance explained (R2) for each
endogenous variable as well as the unstandardized and standardized path (regression)
coefﬁcients. This output may be sufﬁcient to assess those relationships of theoretical
interest, but regression analyses do not supply any estimate of the model’s ﬁt to the data.
FIGURE 41.1
An example path model.

THE ‘‘FLAT’’ MULTIPLE REGRESSION ANALYSIS: OUTPUT
391
41.2
NUMERICAL EXAMPLE
The ﬁctional data we use for our example represent a medical study of 244 patients faced
with challenging medical conditions. The goal was to explain how satisﬁed they were
with the quality of their lives (quality_of_life); values on quality_of_life could range
between 0 and 20, with higher values signifying more positive judgments. Age and SES
(categorized in seven steps with higher values indicating higher SES) serve as exogenous
variables in the model, and optimism (scored between 30 and 90, with higher values
indicating greater optimism) serves as a mediator variable in the model. Data are present
in the data ﬁle named quality of life.
41.3
ANALYSIS STRATEGY
We ﬁrst perform and brieﬂy present the results of a standard (“ﬂat”) multiple regression
analysis using age, SES, and optimism to predict the outcome variable of quality_of_life;
this will serve as a baseline for the path analysis. We then perform a path analysis using
two multiple regression procedures to generate all of the path coefﬁcients; within that
context we evaluate the simple mediation role played by optimism.
41.4
THE ‘‘FLAT’’ MULTIPLE REGRESSION ANALYSIS: SETUP
Open the quality of life data ﬁle and from the main menu, we select Analyze ➔
Regression ➔Linear. Brieﬂy, we move quality_of_life into the Dependent panel and
all of the other variables into the Independent(s) panel (see Figure 41.2). We conceive
of this analysis as “ﬂat,” in the sense that all of the predictors are of equal status in the
model; that is, all of the predictor variables are treated as correlated exogenous variables.
In the Statistics screen shown in Figure 41.3, we check Estimates (to obtain the
regression coefﬁcients), Model ﬁt (to obtain the R2 and adjusted R2), R squared change
(to show this output for illustration purposes), Descriptives (to obtain the descriptive
statistics including the correlation matrix), and Part and partial correlations (to obtain
the zero-order, partial, and semipartial correlations). Click Continue to return to the main
dialog window and click OK to perform the analysis.
41.5
THE ‘‘FLAT’’ MULTIPLE REGRESSION ANALYSIS: OUTPUT
The Correlations between the pairs of variables are shown in Figure 41.4. quality_of_life
is substantially correlated with all three predictors. With the predictors all correlated
as well, the possibility of a viable path structure might be worth considering if it is
theoretically justiﬁed. The correlation between the two exogenous variables (in the path
model) age and SES is −.29, a value that we will place in the ﬁnal version of the path
diagram.
The Model Summary presented in Figure 41.5 informs us that approximately 33%
of the variance (adjusted R2 = .334) of quality_of_life was explained by the model.
The Coefﬁcients associated with the individual predictors are shown in Figure 41.6.
Only optimism was statistically signiﬁcant in the model; its squared semipartial correla-
tion of approximately .15 (.3892 = .151321) suggests that about 15% of the variance of
quality_of_life was uniquely explained by optimism.
From these results, some researchers may be inclined to believe that age and SES
are irrelevant to the explanation (are expendable in any account of the prediction) of
quality_of_life. In fact, these two variables would be excluded from a model based on

392
PATH ANALYSIS USING MULTIPLE REGRESSION
FIGURE 41.2
The main Linear Regression dialog window.
FIGURE 41.3
The Statistics screen of Linear Regression.
a step method (e.g., stepwise multiple regression). But the story may be more compli-
cated (i.e., theoretically relevant and interesting) if something other than a “ﬂat” analytic
strategy is used to structure the variables. The opportunity to use a more complex con-
ﬁguration of the variables is the reason why path modeling has gained popularity in the
past couple of decades.
41.6
PATH ANALYSIS USING MULTIPLE
REGRESSION: ANALYSIS 1
We have presented the path model in Figure 41.1. Because there are two endogenous
variables, it is necessary to perform two multiple regression analyses to obtain the full
set of path coefﬁcients, each predicting one of the endogenous variables. In the ﬁrst
regression analysis, we use age and optimism to predict quality_of_life. From the main
menu, we select Analyze ➔Regression ➔Linear. Brieﬂy (without showing the dialog

PATH ANALYSIS USING MULTIPLE REGRESSION: ANALYSIS 1
393
FIGURE 41.4
Correlations between the variables.
FIGURE 41.5 The Model Summary table.
FIGURE 41.6 The Coefﬁcients table.
screens), we move quality_of_life into the Dependent panel and age and optimism into
the Independent(s) panel, retaining the above setup for the rest of the analysis.
The main results are presented in Figure 41.7. Age and optimism together accounted
for approximately 33% of the variance of quality_of_life. Only optimism was associ-
ated with a statistically signiﬁcant partial regression coefﬁcient. The standardized (beta)
coefﬁcients associated with age and optimism were −.105 and .528, respectively; these
values are to be placed into the path diagram as path coefﬁcients for these variables.

394
PATH ANALYSIS USING MULTIPLE REGRESSION
FIGURE 41.7 The Model Summary and Coefﬁcients tables when predicting quality_of_life.
FIGURE 41.8 The Model Summary and Coefﬁcients tables when predicting optimism.
41.7
PATH ANALYSIS USING MULTIPLE
REGRESSION: ANALYSIS 2
In the second analysis, age and SES are used to predict optimism. From the main menu,
we select Analyze ➔Regression ➔Linear. Without showing the dialog screens, we
move optimism into the Dependent panel and age and SES into the Independent(s)
panel, retaining the statistical setup for the rest of the analysis that was already described
in Section 41.4.
The main results are presented in Figure 41.8. Age and SES together accounted for
approximately 35% of the variance of optimism. Each predictor was associated with a
statistically signiﬁcant partial regression coefﬁcient. The standardized (beta) coefﬁcients
associated with age and SES were −.306 and .431, respectively; these values are to be
placed into the path diagram as path coefﬁcients for these variables.

ASSESSING THE STATISTICAL SIGNIFICANCE OF THE INDIRECT EFFECTS
395
FIGURE 41.9
The path model with the path
coefﬁcients and the R2 values in
round-edged rectangles to the upper
right part of each endogenous variable.
41.8
PATH ANALYSIS USING MULTIPLE
REGRESSION: SYNTHESIS
The completed path model is shown in Figure 41.9. The correlation between age and
SES as well as the standardized path coefﬁcients are placed in their respective paths,
and the adjusted R2 values for the outcome variable quality_of_life and the endogenous
variable optimism are shown in round-edged rectangles near their upper right corners.
Based on the multiple regression results, researchers might be inclined to trim this model
by removing the nonsigniﬁcant path from age to quality_of_life should it be theoretically
reasonable to do so. Such an action stretches the use of path analysis from conﬁrmatory
to that borders on exploratory, but some researchers would take that action based on the
principle of parsimony.
41.9
ASSESSING THE STATISTICAL SIGNIFICANCE
OF THE INDIRECT EFFECTS
Our visual inspection of the path coefﬁcients suggests that both age and SES exerted
an indirect effect on quality_of_life through optimism as a mediating variable. The
statistical signiﬁcance of the indirect effects can be evaluated by the Aroian (1947) test
as described in Chapter 40:
• Using the raw partial regression coefﬁcients and SEs associated with the paths
age to optimism (b = −.634, SE = .112) and optimism to quality_of_life
(b = .190, SE = .021), we obtain (based on Kristopher J. Preacher’s web site
http://quantpsy.org/sobel/sobel.htm) an Aroian z value of approximately −4.78,
p < .001.
• Using the raw partial regression coefﬁcients and SEs associated with the paths
SES to optimism (b = 3.187, SE = .399) and optimism to quality_of_life
(b = .190, SE = .021), we obtain (based on Kristopher J. Preacher’s web site
http://quantpsy.org/sobel/sobel.htm) an Aroian z value of approximately 5.97,
p < .001.
We may therefore conclude that the indirect effects of age and SES through optimism
in explaining quality_of_life are statistically signiﬁcant.

396
PATH ANALYSIS USING MULTIPLE REGRESSION
41.10
ASSESSING THE STRENGTH OF EACH INDIRECT EFFECT
We can assess the strength of each indirect effect by multiplying the standardized (beta)
coefﬁcients associated with each segment of each path. Thus, the absolute value of the
indirect effect of age through optimism on quality_of_life is −.31 * .53 = .16, and the
value of the indirect effect of age through optimism on quality_of_life is .43 * .53 = .23.
Both of these values are quite substantial for indirect effects and should be treated as
being of practical worth. Our general conclusion from this analysis is that age and SES
indirectly inﬂuence quality_of_life through the mediated inﬂuence of optimism.
41.11
EVALUATING THE POSSIBILITY OF MEDIATION
Although the path from age to quality_of_life did not achieve statistical signiﬁcance
in the model, it does raise the interesting question of whether age can predict quali-
ty_of_life in isolation. If that is the case, then we may have observed full mediation
with the inclusion of optimism in the model. To test that possibility, we have performed
an additional regression analysis using age in isolation to predict quality_of_life. The
results, shown in Figure 41.10, indicate that age on its own does predict quality_of_life
(p < .001); the unstandardized regression coefﬁcient is −.248 (with SE = .045) and the
standardized regression coefﬁcient is −.332.
41.12
TESTING THE STATISTICAL SIGNIFICANCE
OF THE DIFFERENCE BETWEEN THE DIRECT PATHS
IN THE UNMEDIATED AND THE MEDIATED MODELS
Although it may be obvious what the result will be, it is instructive to perform the
Freedman–Schatzkin test, as described in Chapter 40, to compare the relative strengths
of the paths from age to quality_of_life in the unmediated and the mediated models.
When we solve the Freedman–Schatzkin equation, we obtain a t value of 8.699. Evaluated
against a Student t distribution with N −2 degrees of freedom (here we have 244 −2,
or 242, degrees of freedom), our result indicates that the coefﬁcients are signiﬁcantly
different (p < .001). We may therefore conclude that we have obtained a full mediation
effect (given that the path in the mediated model is not statistically signiﬁcant).
FIGURE 41.10 The
Model
Summary
and
Coefﬁcients
tables
using
age
to
predict
quality_of_life.

C H A P T E R
4 2
Path Analysis Using Structural
Equation Modeling
42.1
OVERVIEW
The IBM SPSS® Amos module that we used in Chapter 39 to perform a conﬁrmatory
factor analysis can also be used to perform a path analysis. As was the case for the
multiple regression approach to path analysis (discussed in Chapter 41), we obtain in
an SEM analysis the path (regression) coefﬁcients and R2 for the endogenous variables.
However, because it uses SEM, IBM SPSS Amos also provides ﬁt indexes informing us
how well the proposed model ﬁts the data. These ﬁt indexes allow us to compare differ-
ent plausible models to help determine which theoretical approach has more empirical
support.
Some of the differences between the multiple regression and SEM approaches to
path analysis are as follows:
• The SEM analysis is completed as a whole rather than in portions. Thus, the
path coefﬁcients are estimated simultaneously based on all of the hypothesized
interrelationships among the variables rather than based only on those involved in
separate portions of the analysis as is done in the multiple regression approach.
• By performing the analysis as a whole using SEM, we obtain ﬁt indexes that
inform us how well the hypothesized model ﬁts the data; multiple regression does
not supply these ﬁt indexes.
• The multiple regression approach involves a weighted linear composite of the
predictors as the variate and the predicted value of Y as the dependent variable. In
SEM, the predictor side of the model includes an error term as a latent variable (in
an SEM model diagram, each endogenous variable will have an associated error
term as an explanatory variable); this difference means that the dependent variable
in a structural model is the actual Y value rather than the predicted Y value.
We analyze the same path model evaluated by multiple regression analysis in Chapter
41. As indicated in Chapter 39, the data used in an SEM analysis should contain no
missing values, and our data meet this criterion.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
397

398
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
42.2
PATH ANALYSIS BASED ON SEM: DRAWING THE MODEL
We open the quality of life data ﬁle and from the main menu, we select Analyze ➔IBM
SPSS Amos. The initial IBM SPSS Amos window is shown in Figure 42.1 where the
model is to be drawn in the large pane on the right and frequently used commands are
located in the icon toolbar on the left. Readers are referred to Section 39.3 for additional
details on some commonly used tools on the icon toolbar.
As we have done in Chapter 39, we opt to switch from the default portrait (long)
orientation to landscape (wide) orientation. To change the orientation, select View ➔
Interface properties from the main menu. This action opens the Interface Properties
window as seen in Figure 42.2. Select Landscape-Legal under Paper Size, select Apply,
and close the window.
From the icon toolbar menu, we select the Draw observed variables icon (the
rectangle in the upper left corner) as shown in Figure 42.3 and draw our ﬁrst measured
variable for the path model (shown in Figure 42.4). If any drawn object looks less than
aesthetically pleasing or if the drawn object is not what is desired, either Erase the object
and draw it again or right-click to select change the Shape of object.
We then continue this to draw the other three measured variables of SES, optimism,
and quality of life as shown in Figure 42.5. These will represent our measured variables.
Our next task is to associate each depiction of a measured variable with the proper
variable in the data ﬁle. From the main menu, we select View ➔Variables in Dataset.
This opens the Variables in Dataset window pictured in Figure 42.6. Select each variable
FIGURE 42.1 The initial IBM SPSS Amos screen.

PATH ANALYSIS BASED ON SEM: DRAWING THE MODEL
399
FIGURE 42.2
The Interface Properties window where we select
Landscape-Legal page orientation.
FIGURE 42.3 Selecting the Draw observed variables function.
in turn and drag it to the appropriate ﬁgure so that all observed variables are properly
conﬁgured (see Figure 42.7). When ﬁnished, close the Variables in Dataset window.
We now need to draw the paths. From the icon toolbar, select the Draw paths (single
headed arrow) as shown in Figure 42.8. With this tool activated, place the cursor on the
right border of the age rectangle and click and drag to the optimism rectangle. Use this

400
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.4 The ﬁrst measured variable is now drawn.
FIGURE 42.5 The observed variables are now drawn.

PATH ANALYSIS BASED ON SEM: DRAWING THE MODEL
401
FIGURE 42.6 The Variables in Dataset window.
FIGURE 42.7 All of the rectangles have been associated with the proper observed variables.

402
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.8 Select the Draw paths (single headed arrow) from the icon toolbar.
process to draw each of the other paths in the model so that the structure mirrors what
is in Figure 42.9.
We next draw the correlation (the covariance) path between the two exogenous
variables, as we hypothesize that they are correlated. From the icon toolbar, select the
Draw covariances (double headed arrow) as shown in Figure 42.10. With this tool
activated, place the cursor on the left border of the SES rectangle and click and drag to
the age rectangle to draw the covariance path as shown in Figure 42.11.
In the SEM approach to path analysis, and even with all of the variables as measured
(observed) variables, it is necessary to explicitly include latent error variables (called
unique variables by IBM SPSS Amos) that are to be associated with each endogenous
variable (because the dependent variables in the structural equation are the actual rather
than the predicted Y values). These unique variables are not speciﬁcally measured and
are thus represented as latent variables, to be depicted by circles or ovals. To add these
latent variables to the model, we select from the icon toolbar the Add a unique variable
to an existing variable as shown in Figure 42.12.
With this tool activated, place the cursor inside the optimism ﬁgure and click. This
results in a latent variable (with its path initially constrained to a value of 1) being placed
above the optimism rectangle as shown in Figure 42.13. The standardized value of this
path coefﬁcient will be reestimated as a part of the main analysis. Because the latent
error variable is in a diagrammatically awkward position, it is best to shift its position.
Keeping the cursor inside the optimism rectangle and clicking repeatedly rotates the
unique variables clockwise. Sufﬁcient clicking ﬁxes it in a more aesthetically pleasing
place (see Figure 42.14). Applying this same procedure to quality_of_life yields the
conﬁguration shown in Figure 42.15.
We now need to name our unique variables, although IBM SPSS Amos will do
so rather generically. From the main menu, we select Plugins ➔Name Unobserved
Variables (see Figure 42.16). On selecting this command, the unique variables will be

PATH ANALYSIS BASED ON SEM: DRAWING THE MODEL
403
FIGURE 42.9 The paths are now drawn.
FIGURE 42.10 Select the Draw covariances (double headed arrow) from the icon toolbar.

404
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.11 The covariance path is now drawn.
FIGURE 42.12 Select the Add a unique variable to an existing variable from the icon toolbar.

PATH ANALYSIS BASED ON SEM: DRAWING THE MODEL
405
FIGURE 42.13 The unique variable is drawn on top of the optimism rectangle.
FIGURE 42.14 The unique variable has been shifted to a better spatial position.

406
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.15 Both unique variables for the endogenous variables are now in place.
FIGURE 42.16 Select Plugins ➔Name Unobserved Variables to name the unique variables.

PATH ANALYSIS BASED ON SEM: ANALYSIS OUTPUT
407
FIGURE 42.17 The unique variables have now been labeled as “e” for error term, using sequen-
tial numbers.
labeled as “e” (for error term) and given the sequential numbers e1 and e2 as shown in
Figure 42.17.
42.3
PATH ANALYSIS BASED ON SEM: ANALYSIS SETUP
With the path diagram completed, we next conﬁgure the path analysis. From the main
menu, we select View ➔Analysis Properties to open the Analysis Properties window
as shown in Figure 42.18. Select the Output tab and check Standardized estimates,
Squared multiple correlations, and Indirect, direct & total effects. Because of the
simplicity of our model, we do not request Modiﬁcation indices to have IBM SPSS
Amos offer “suggestions” on how to improve the model ﬁt.
To execute the analysis, select Analyze ➔Calculate Estimates from the main menu.
As this is our ﬁrst analysis, we are presented with the operating system’s Save As dialog
window (see Figure 42.19). We select a location to save the ﬁle, provide a ﬁle name,
and Save.
42.4
PATH ANALYSIS BASED ON SEM: ANALYSIS OUTPUT
To access the results of the analysis, select the View the output path diagram icon
(the one on the right) in the top output panel immediately to the left of the drawing area
as shown in Figure 42.20. What is displayed is the path diagram with the Unstandard-
ized estimates (as highlighted in the fourth output panel from the top). These values,
the unstandardized regression coefﬁcients and the covariance between the exogenous
variables of age and SES, also appear in tabular form (we show these shortly).

408
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.18
The conﬁgured Output screen in the Analysis
Properties window.
FIGURE 42.19 The Save As window.

PATH ANALYSIS BASED ON SEM: ANALYSIS OUTPUT
409
FIGURE 42.20 To access the results, select the View the output path diagram icon (the one on
the right) in the top panel immediately to the left of the drawing area.
To view the standardized path coefﬁcients in the path diagram, select Standardized
estimates in the fourth output panel from the top just under Unstandardized estimates.
These are shown in Figure 42.21. In addition to the unstandardized regression coefﬁcients,
the R2 for each endogenous variable and the correlation of the two exogenous variables
are also included in the display. All of these values are available in output tables as well.
To obtain the output in tabular form, select from the main menu View ➔Text
Output. This action opens the Notes for Model screen shown in Figure 42.22. The
Default model referred to in the ﬁrst line on the screen is the model that we conﬁgured.
The chi-square value associated with the model is 2.094 and, with 1 degree of freedom,
is not statistically signiﬁcant (p = .148). This indicates that the expected values based
on the model do not differ signiﬁcantly from those represented by the data; that is, the
model ﬁts the data. This is what researchers hope to ﬁnd.
The model ﬁt indexes, accessed by selecting Model Fit from the left panel, are
shown in Figure 42.23, Figure 42.24, and Figure 42.25. As described in Chapter 39, we
focus on a subset of the indexes that are presented:
• The CMIN index (minimal discrepancy as assessed by chi-square) is a statistically
nonsigniﬁcant value of 2.094 (see top table of Figure 42.23), indicating that the

410
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.21 The Standardized estimates are shown in the path diagram.
model ﬁts the data. We have already seen this result in the Notes for Model
screen.
• The GFI is .996 (see middle table of Figure 42.23) and indicates an excellent ﬁt
(the goal is to exceed .95).
• The NFI is .991 (see bottom table of Figure 42.23) and indicates an excellent ﬁt
(the goal is to exceed .95).
• The CFI is .995 (see bottom table of Figure 42.23) and indicates an excellent ﬁt
(the goal is to exceed .95).
• The RMSEA is .067 (see top table of Figure 42.25) and indicates a reason-
ably good ﬁt (we exceeded the benchmark of .06 indicating a very good ﬁt but
succeeded in achieving a value below .08 indicating an adequate ﬁt).
Selecting Estimates in the output panel changes the display to a set of tables, the ﬁrst
screenshot of which is shown in Figure 42.26. The top table presents the unstandardized
regression coefﬁcients (labeled as Regression Weights), the standard error associated
with the coefﬁcients (S.E.), and the critical ratio (C.R.). The CR is the coefﬁcient divided
by the standard error and operates something as a z statistic with absolute values of 1.96
or greater indicating statistical signiﬁcance based on an alpha level of .05.

PATH ANALYSIS BASED ON SEM: ANALYSIS OUTPUT
411
FIGURE 42.22 Selecting Text Output from the main menu opens the Notes for Model screen.
Each coefﬁcient is tested for statistical signiﬁcance (reported under the column
labeled P). The triple asterisks (***) represent a probability level of p < .001. Three
of the four regression coefﬁcients reached statistical signiﬁcance (p < .001); the excep-
tion was the path from age to quality_of_life that yielded a trend toward statistical
signiﬁcance (p = .071) but fell short of an alpha level of .05. It is worth noting that these
unstandardized regression coefﬁcients produced by maximum likelihood estimation have
achieved the same values as those produced by the ordinary least-squares multiple regres-
sion procedure, thanks in part to the stability of the model based on our very large sample
size.
The middle table of Figure 42.26 presents the standardized regression coefﬁcients.
Not surprisingly, given the match of the unstandardized coefﬁcients, these also match the
values obtained by the multiple regression analyses. The bottom table of Figure 42.26
presents the covariance between age and SES. Covariance is a different way to express
correlation; it is based on the original measurement units in the computation rather than
being standardized to produce values (that we ordinarily see in reporting correlations)
between ±1.00 (Norman & Streiner, 2008).
Figure 42.27 presents the next portion of the Estimates display. The top table shows
the correlation between age and SES and is the translation of the covariance into the
standardized form. The correlation of −.29 is what we had obtained in the multiple
regression analysis. Variances of the predictor variables are shown in the middle table

412
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.23
Model Fit indexes (ﬁrst of three screenshots).
and the R2 (Squared Multiple Correlations) values for optimism and quality_of_life
are shown in the bottom table of Figure 42.27. The R2 value of .337 for quality_of_life
matches exactly what was obtained from the multiple regression analysis; the R2 value
of .356 for optimism is close to but not exactly the same as that we had obtained from
the multiple regression analysis.
Figure 42.28 shows the results of the calculations for the indirect effects. The bottom
table presents the standardized results; the values of .227 for SES through optimism
and −.161 for age through optimism are equivalent to those computed (by hand) from
the multiple regression analyses.
The substantive conclusion based on the SEM analysis did not change from what
resulted from the series of multiple regression analyses, but we were able to obtain ﬁt
measures informing us that the model was an excellent ﬁt to the data. Such an outcome
strongly supports the structure of the variables hypothesized by the researchers with the
one possible exception of the hypothesized causal path of age predicting quality_of_life
that did not reach the .05 level of statistical signiﬁcance.
Although we will not repeat the calculations here because we are dealing with the
same values and therefore would achieve the same outcomes, it would be appropriate
to engage in the following procedures at this point in our explication of the model
results:
• We would want to perform the Aroian tests to evaluate the statistical signiﬁcance
of the indirect effects.

PATH ANALYSIS BASED ON SEM: MODIFIED MODEL OUTPUT
413
FIGURE 42.24
Model Fit indexes (second of three screenshots).
• We would want to examine the possibility of mediation by creating a model with
only age predicting quality_of_life.
• Having determined that age predicted quality_of_life, we would want to
perform the Freedman–Schatzkin test to compare the relative strengths of the
paths from age to quality_of_life in the unmediated model and the mediated
models.
The remaining issue is how to treat the nonsigniﬁcant path between age and qual-
ity_of_life. One option open to researchers, if they believed that it is theoretically
defensible, is to consider removing the path between age and quality_of_life. In a multi-
ple regression approach, the researchers would have to make a decision about removing
the path based on no more information than its failure to achieve a statistically signif-
icant outcome. Within the context of SEM, researchers have one additional avenue of
investigation open to them: they can perform a new analysis excluding the path between
age and quality_of_life to determine how much the model ﬁt is changed by the omission
of that path. We choose that option here.
42.5
PATH ANALYSIS BASED ON SEM: MODIFIED
MODEL OUTPUT
Without showing the screenshots, we have returned to the path drawing by selecting
the icon just to the left of the View the output path diagram icon at the top of the
output panel, removed the path between age and quality_of_life by activating the Erase

414
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.25
Model Fit indexes (third of three screenshots).
function and clicking on the path, and performed a new analysis mirroring the setup of
the original. The results of the modiﬁed model (which should be treated as an exploratory
analysis) are brieﬂy summarized in the following.
The Model Fit screens of relevance are presented in Figure 42.29. It is interesting
that the chi-square value for the modiﬁed model of 5.343, while still not statistically
signiﬁcant (p = .069), is greater than the value of 2.094 obtained in the original analysis.
To determine if this model represents a statistically poorer ﬁt than the initial model, a
chi-square difference test can be performed. To conduct a chi-square difference test, we
simply subtract the smaller chi-square value from the larger chi-square value. This chi-
square difference is evaluated with degrees of freedom equal to the difference in degrees
of freedom between the two models.
In this example, the chi-square difference is 3.249 (5.343 −2.094 = 3.249). The
degrees of freedom for the original and modiﬁed models were 1 and 2, respectively,
giving us a difference of one degree of freedom. Based on the chi-square critical values
table in Appendix A, we require a chi-square value of 3.841 to be statistically signiﬁcant
at a .05 alpha level for one degree of freedom (a value of 2.706 is required to meet an
alpha level of .10). It therefore appears that our modiﬁed model does not represent a
statistically worse ﬁt than the original model based on a .05 alpha level. That said, the

PATH ANALYSIS BASED ON SEM: MODIFIED MODEL OUTPUT
415
FIGURE 42.26 The ﬁrst portion of the Estimates screen.
obtained chi-square value for this trimmed model does suggest a somewhat poorer ﬁt
than what we obtained for the original model.
The picture painted by the chi-square analysis is reinforced when we examine the
other ﬁt indexes.
• The GFI was .996 in the original analysis; it has dropped to .989 in the new
analysis.
• The NFI was .991 in the original analysis; it has dropped to .977 in the new
analysis.
• The CFI was .995 in the original analysis; it has dropped to .985 in the new
analysis.
• The RMSEA was .067 in the original analysis; it has dropped to .083 (suggesting
not quite an adequate ﬁt) in the new analysis.
The Estimates resulting from the modiﬁed model are shown in Figure 42.30. These
values changed minimally from the original analysis.

416
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.27 The second portion of the Estimates screen.
FIGURE 42.28 The last portion of the Estimates screen.

PATH ANALYSIS BASED ON SEM: MODIFIED MODEL OUTPUT
417
FIGURE 42.29 The Model Fit indexes.

418
PATH ANALYSIS USING STRUCTURAL EQUATION MODELING
FIGURE 42.30 The Estimates screens.
In summary, researchers might be inclined to remove the nonsigniﬁcant path from the
model based on multiple regression analyses; however, the SEM analysis suggests that the
original model should be retained, as removing the path between age and quality_of_life
results in a somewhat (although not a statistically signiﬁcant) poorer ﬁt of the model to
the data. Thus, our conclusion is that the original model is as good a description of the
data as we have, even with the inclusion of a path that does not quite reach the .05 alpha
level of statistical signiﬁcance.

C H A P T E R
4 3
Structural Equation Modeling
43.1
OVERVIEW
SEM extends what we have discussed in Chapter 42 with respect to path analysis—where
only measured variables are included in the model—to causal or predictive models that
incorporate latent variables. We broached the concept of latent variables in Chapter 39
in the context of conﬁrmatory factor analysis, where the factors were presented as latent
variables. We treat the idea of latent variables in the same way in this chapter—latent
variables are factors or constructs that are not directly observed but rather are indi-
cated or reﬂected (Edwards & Bagozzi, 2000) by measured (observed) variables; a much
richer discussion of latent variables may be found in Bollen (2002). Because latent vari-
ables (and their accompanying measured variables) are included in the model, SEM thus
subsumes conﬁrmatory factor analysis.
An example of a structural model is presented in Figure 43.1. Latent variables A and
B are the exogenous variables in the model; they are hypothesized to be correlated as
indicated by the double-headed arrow between them. The outcome variable in the model
is the latent variable D; it is hypothesized to be driven by both exogenous variables in
both directly and indirectly through the latent variable C. As endogenous variables, latent
variables C and D have associated error terms. Each of the latent variables is indicated
by two measured variables (for illustration purposes). Structural models always contain
latent variables; these models can also include measured variables as either exogenous
or endogenous variables (we use only latent variables in our example).
SEM is composed of two major and sequentially dependent components. These com-
ponents are both contained within the SEM analysis. In our example, we will treat them
separately in an effort to make these components clear to readers. The two components
are as follows:
• The measurement model represents the degree to which the indicator variables
relate to their respective latent variables, given the hypothesized relationships
among the latent variables. This measurement model is assessed by performing a
conﬁrmatory factor analysis as part of the analysis. The presumption is that there
is sufﬁcient internal validity evidence associated with the measurement model
to support (act as the foundation for) the structural (predictive) model; if the
measurement model does not have sufﬁcient integrity, then there is little point in
even evaluating the structural model.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
419

420
STRUCTURAL EQUATION MODELING
• The structural model represents the causal or predictive relationships between the
exogenous and endogenous variables as speciﬁed by the researchers. It is concep-
tually akin to a path analysis in terms of both analysis setup and statistical output
but includes latent variables that are weaved together in a causal conﬁguration.
43.2
NUMERICAL EXAMPLE
The ﬁctional data we use for our example are present in the data ﬁle named statistics
achievement and represent a study of 196 students who were evaluated on their
achievement in a statistics class (statistics). The model that integrates these variables is
shown in Figure 43.2. The latent constructs of anxiety in academic contexts (anxiety)
and academic self-efﬁcacy (efﬁcacy) are hypothesized to directly predict both science
FIGURE 43.1
An illustration of a generic structural model.
FIGURE 43.2
The hypothesized model.

EVALUATING THE MEASUREMENT MODEL: DRAWING THE MODEL
421
aptitude (science) and statistics achievement (statistics), with science placed as a
mediator variable between the two exogenous variables and the outcome variable.
The two indicator variables (associated measured variables) of statistics (stat1 and
stat2) could vary from 40 (low ability) to 100 (high ability). The indicators of anxiety
(anx1 and anx2) could vary from 0 (low anxiety) to 12 (high anxiety), and the indicators
of efﬁcacy could vary from 10 (low efﬁcacy) to 30 (high efﬁcacy). Finally, sci1 and sci2
as indicators of science could vary from 20 (low aptitude) to 60 (high aptitude).
43.3
ANALYSIS STRATEGY
When the structural model is speciﬁed and executed in IBM SPSS® Amos, results pertain-
ing to the measurement model and to the structural model components are both contained
in the output. To make the two components clear to readers, we perform the analysis in
two phases corresponding to the two components of the overall model. First, we con-
ﬁgure the conﬁrmatory analysis to evaluate the measurement model. Assuming that the
measurement model is acceptable, we then evaluate the causal (structural) model. The
output for the structural model contains the results reﬂecting the measurement model;
we show where in the output these results are located when discussing the output of the
second analysis. Readers are also referred to Section 39.2 where we have discussed the
need to have no missing values, a condition that is met by our data set.
43.4
EVALUATING THE MEASUREMENT MODEL:
DRAWING THE MODEL
Much of what is covered here has already been described in Chapters 39 and 42, and so
we will go into a little less detail in this section; readers are encouraged to consult those
two chapters for additional details if needed.
We open the statistics achievement data ﬁle and from the main menu we select
Analyze ➔IBM SPSS Amos. As we have done in the previous chapters, we opt to
switch from the default portrait (long) orientation (opened by default in IBM SPSS
Amos) to landscape (wide) orientation. To change the orientation, we select View ➔
Interface properties from the main menu. This action opens the Interface Properties
window as seen in Figure 43.3. Select Landscape-Legal under Paper Size, select Apply,
and close the window.
The measurement model is evaluated by performing a conﬁrmatory factor analysis.
From the icon toolbar menu, select the icon (the circle with squares and more circles)
representing Draw a latent variable or add an indicator to a latent variable as shown
in Figure 43.4. We use this tool to draw our conﬁrmatory model. After activating this
function by selecting it, click to draw the ﬁrst factor toward the left of the drawing
area. Place the cursor inside the circle/oval. Then click twice to draw the two indicator
variables and their associated errors (changing the shape of the objects if necessary as
described in Section 39.3.2). Draw the next three latent variables down the line resulting
in the conﬁguration shown in Figure 43.5. Note that all the paths from the errors and the
path from each factor to its ﬁrst indicator variable have been automatically constrained
to the value of 1 to scale each latent variable (the factor and the error) to the measured
variables. Right-click in a blank portion of the drawing area to end that function (and
return to the operating system cursor).
We next draw the correlation (the covariance) paths between the factors based on the
hypothesized structural model shown in Figure 43.2. Each latent variable in the model has
a path from itself to every other latent variable; most of these paths are causal (predictive)
paths (single-headed arrows) but there is also one covariance (correlation) path between

422
STRUCTURAL EQUATION MODELING
FIGURE 43.3
The Interface Properties window where we select
Landscape-Legal page orientation.
FIGURE 43.4 Selecting the Draw a latent variable or add an indicator to a latent variable
function.

EVALUATING THE MEASUREMENT MODEL: ANALYSIS SETUP
423
FIGURE 43.5 The latent variables in the conﬁrmatory model are now drawn.
anxiety and efﬁcacy; these relationships need to be captured in our conﬁrmatory factor
analysis. From the icon toolbar, we select the Draw covariances (double headed arrow)
and, working from right to left, draw all of the covariances to create a structure mirroring
that shown in Figure 43.6.
Our next task is to associate each measured (indicator) variable with a variable from
our data ﬁle. From the main menu, we select View Variables in Dataset. This opens the
Variables in Dataset window pictured in Figure 43.7. Select each variable in turn and
drag it to the appropriate rectangle so that all observed variables are properly conﬁgured
as shown in Figure 43.8. When ﬁnished, close the Variables in Dataset window.
To name the factors, we double-click inside a factor to activate the Object Properties
window shown in Figure 43.9. We type the name we wish to use for each latent variable
in turn; we use the names anxiety, efﬁcacy, science, and statistics. The result of this
activity is shown in Figure 43.10.
The ﬁnal task in drawing the model is to name the latent error variables (called
unique variables by IBM SPSS Amos). From the main menu, we select Plugins ➔
Name Unobserved Variables (see Figure 43.11). On selecting this command, the unique
variables will be labeled as “e” (for error term) and given a sequential number (see
Figure 43.12), resulting in the variable names e1 through e8.
43.5
EVALUATING THE MEASUREMENT MODEL:
ANALYSIS SETUP
With the conﬁrmatory factor analysis diagram completed, our next job is to conﬁgure
the analysis. From the main menu, we select View ➔Analysis Properties to open the

424
STRUCTURAL EQUATION MODELING
FIGURE 43.6 The correlations between the factors are now drawn.
FIGURE 43.7 The Variables in Dataset window.

EVALUATING THE MEASUREMENT MODEL: ANALYSIS SETUP
425
FIGURE 43.8 All of the indicator variables have now been associated with the proper variables
in the data ﬁle.
FIGURE 43.9 By double-clicking inside the ﬁrst factor, we have activated the Object Properties
window.

426
STRUCTURAL EQUATION MODELING
FIGURE 43.10 All of the factors are now named.
FIGURE 43.11 To name the latent error variables, we select Plugins ➔Name Unobserved Vari-
ables from the main menu.

EVALUATING THE MEASUREMENT MODEL: ANALYSIS OUTPUT
427
FIGURE 43.12 On selecting Plugins ➔Name Unobserved Variables, the latent error variables
are labeled e1 through e8.
Analysis Properties window as shown in Figure 43.13. Select the Output tab. Check
Standardized estimates (these produce the standardized regression coefﬁcients for the
prediction of the measured variables) and Modiﬁcation indices (these produce IBM
SPSS Amos “suggestions” on how to improve the model ﬁt). To execute the analysis,
select Analyze ➔Calculate Estimates from the main menu, saving the output ﬁle as
needed to an appropriate location.
43.6
EVALUATING THE MEASUREMENT MODEL:
ANALYSIS OUTPUT
To access the results of the analysis, select the View the output path diagram icon
(the one on the right) in the top output panel immediately to the left of the drawing
area as shown in Figure 43.14. What is displayed is the conﬁrmatory diagram with the
Unstandardized estimates displayed.
To obtain the output in tabular form, select from the main menu View ➔Text
Output. This action opens the Notes for Model screen of the Output window shown
in Figure 43.15. The Default model referred to in the ﬁrst line on the screen is the
conﬁrmatory model shown in Figure 43.12. It may be seen that the chi-square value
associated with the model is 24.695 and, with 14 degrees of freedom, is statistically
signiﬁcant (p = .038). This suggests that the expected values based on the model differ
signiﬁcantly from those represented by the data, but we use other ﬁt measures as well,
in that the chi-square test is overly powerful and can detect small discrepancies between
the observed and predicted covariances, suggesting (sometimes inappropriately) that the
model does not ﬁt the data (Bentler, 1990; J¨oreskog & S¨orbom 1996).

428
STRUCTURAL EQUATION MODELING
FIGURE 43.13
The Analysis Properties window.
FIGURE 43.14 The Unstandardized estimates are displayed on selecting View the output path
diagram.

EVALUATING THE MEASUREMENT MODEL: ANALYSIS OUTPUT
429
FIGURE 43.15 Selecting Text Output presents us with the Notes for Model screen.
Selecting Model Fit in the left panel displays the several ﬁt indexes produced by
IBM SPSS Amos; the ones on which we focus are presented in Figure 43.16. The
CMIN table shown at the top in Figure 43.16 presents the minimum discrepancy value
in the Default model row, which is the chi-square goodness-of-ﬁt test that repeats the
information we saw in the Model Notes screen. The GFI is shown in the next table in
Figure 43.16 where we see its value of .973 exceeds our guideline value of .95, suggesting
an acceptable level of model ﬁt. The NFI and the CFI are shown in the bottom table of
the top screenshot in Figure 43.16 where we see their respective values of .962 and .983
also meet our guideline value of .95, suggesting an acceptable level of model ﬁt. The
RMSEA table is presented in the bottom screenshot of Figure 43.16. Its value of .063
also suggests that the model ﬁts the data fairly well.
In summary, the NFI, CFI, and RMSEA indexes suggest that our conﬁrmatory model
is a reasonably good ﬁt to the data despite an associated chi-square that was statistically
signiﬁcant at an alpha level of .05.
Selecting Estimates in the left panel displays the estimated parameters generated by
the model, the factor coefﬁcients portion of which is shown in Figure 43.17. The Regres-
sion Weights Table Figure 43.17 presents the unstandardized regression coefﬁcients and
the bottom table presents the same information in the standardized form (which is eas-
ier to read). All coefﬁcients that were estimated were statistically signiﬁcant, and the

430
STRUCTURAL EQUATION MODELING
FIGURE 43.16 The model ﬁt indexes.
standardized coefﬁcients suggest that the measured variables are good indicators of their
respective factors.
Figure 43.18 presents the Covariances in the top table (unstandardized correlations)
and the Correlations between the factors in the bottom table. The factors are indeed
correlated, but the strengths of those relationships are at most in the moderate range and
are thus perfectly acceptable for a conﬁrmatory model (we would not want the factors
correlated to such an extent that they were indistinguishable from one another, as was
obtained in the example for Chapter 39).
The Modiﬁcation Indices are shown in Figure 43.19. There appear to be no theo-
retically viable suggestions here, and so they can be ignored.
As an overall conclusion with respect to this component of the analysis, it appears
that the measurement model is acceptable; that is, we take the hypothesized structure to
be a sufﬁciently good ﬁt to the data to accept the viability of the measurement model.

EVALUATING THE STRUCTURAL MODEL: DRAWING THE MODEL
431
FIGURE 43.17 The factor coefﬁcients portion of the Estimates screen.
Given this conclusion, we can now proceed with specifying and evaluating the structural
model proper.
43.7
EVALUATING THE STRUCTURAL MODEL: DRAWING
THE MODEL
We start drawing the structural model in exactly the same way described in Section 43.4,
in that we wish to draw the four latent variables and their indicators. Brieﬂy, we
open the statistics achievement data ﬁle, select Analyze ➔IBM SPSS Amos from
the main menu, select View ➔Interface properties from the main menu, choose

432
STRUCTURAL EQUATION MODELING
FIGURE 43.18 Covariances and correlations.
FIGURE 43.19 The Modiﬁcation Indices.

EVALUATING THE STRUCTURAL MODEL: DRAWING THE MODEL
433
FIGURE 43.20 The ﬁrst pass at drawing the model.
Landscape-Legal under Paper Size to switch to landscape orientation, and select the
icon (the circle with squares and more circles) representing Draw a latent variable or
add an indicator to a latent variable from the icon toolbar menu. Conﬁgure the factors
and their indicator variables in a two-row by two-column array as shown in Figure 43.20.
On our screen, the second row drawings interfere with the ﬁrst row drawings, but
even if they did not (on a larger monitor), we would want to have the indicator variables
in the second row facing down rather than up (for aesthetic reasons). To accomplish
this, place the cursor inside one of the ovals in the second row and right-click. From
the pop-up menu, select Rotate (see Figure 43.21). The cursor now includes the word
rotate. Click twice. This changes the orientation of the indicator variables. With the
Rotate function still in effect, repeat the operation for the next latent variable to achieve
the result shown in Figure 43.22. Place the cursor in a blank area and right-click to end
the Rotate function.
Drawing the details of the structural model uses the same procedures as described
in Section 42.2. Select the Draw paths (single headed arrow) from the icon toolbar as
shown in Figure 43.23. With this tool activated, draw each of the paths in the model so
that the structure mirrors that in Figure 43.24.
We next draw the correlation (the covariance) between the two exogenous latent
variables. From the icon toolbar, select the Draw covariances (double headed arrow)
as shown in Figure 43.25. With this function activated, place the cursor on the left border
of the lower left latent variable and click and drag to the upper latent variable as shown
in Figure 43.26.
Each endogenous variable needs to be associated with an error term. We select
from the icon toolbar the Add a unique variable to an existing variable, as shown in
Figure 43.27. With this tool activated, place the cursor inside one of the latent variables in
the second column and click thrice (once to create the unique variable and twice more

434
STRUCTURAL EQUATION MODELING
FIGURE 43.21 By rotating the indicator variables and their error terms in the second row, we
can make the drawing much clearer.
FIGURE 43.22 After the rotation, the diagram is much clearer.

EVALUATING THE STRUCTURAL MODEL: DRAWING THE MODEL
435
FIGURE 43.23 Select the Draw paths (single headed arrow) from the icon toolbar.
FIGURE 43.24 The paths are now drawn.

436
STRUCTURAL EQUATION MODELING
FIGURE 43.25 Select the Draw covariances (double headed arrow) from the icon toolbar.
FIGURE 43.26 The covariance path between the two latent variables is now drawn.

EVALUATING THE STRUCTURAL MODEL: ANALYSIS SETUP
437
FIGURE 43.27 Select the Add a unique variable to an existing variable from the icon toolbar.
to rotate it out of the way). Doing the same for the other endogenous latent variable
produces the conﬁguration shown in Figure 43.28.
Our next task is to associate each measured variable with the proper variable in the
data ﬁle. From the main menu, select View ➔Variables in Dataset. This opens the
Variables in Dataset window. Select each variable in turn and drag it to the appropriate
ﬁgure so that all observed variables are properly named as in Figure 43.29. When ﬁnished,
close the Variables in Dataset window.
We name the factors by double-clicking inside each. This activates the Object
Properties window. After typing each name in turn, we obtain the result shown in
Figure 43.30.
Naming the unique variables is the last step in drawing the model. It is accom-
plished by selecting Plugins ➔Name Unobserved Variables from the main menu
(see Figure 43.31). On selecting this command, the unique variables will be given the
sequential numbers e1 through e10, as shown in Figure 43.32.
43.8
EVALUATING THE STRUCTURAL MODEL:
ANALYSIS SETUP
From the main menu, we select View ➔Analysis Properties to open the Analysis Prop-
erties window, as shown in Figure 43.33. Select the Output tab and check Standardized
estimates, Squared multiple correlations, Modiﬁcation indices, and Indirect, direct
& total effects. To execute the analysis, select Analyze ➔Calculate Estimates from
the main menu and Save as needed.
In order for IBM SPSS Amos to perform the analysis, the model must be identiﬁed
(Bollen, 1989; Byrne, 2010; Meyers et al., 2013; Raykov & Marcoulides, 2008). Brieﬂy,
there must be more known or nonredundant elements (e.g., covariances and variances of

438
STRUCTURAL EQUATION MODELING
FIGURE 43.28 Both unique variables for the endogenous latent variables are now in place.
FIGURE 43.29 All of the measured variables in the model have been associated with the proper
observed variables.

EVALUATING THE STRUCTURAL MODEL: ANALYSIS SETUP
439
FIGURE 43.30 All of the latent variables are now named.
FIGURE 43.31 To name the latent error variables, we select Plugins
➔Name Unobserved
Variables from the main menu.

440
STRUCTURAL EQUATION MODELING
FIGURE 43.32 On selecting Plugins ➔Name Unobserved Variables, the latent error variables
are labeled e1 through e10.
FIGURE 43.33
The Analysis Properties window.

EVALUATING THE STRUCTURAL MODEL: ANALYSIS OUTPUT
441
FIGURE 43.34 To access the results, select the View the output path diagram icon (the one on
the right) in the top panel immediately to the left of the drawing area.
the measured variables) rather than parameters to be estimated (e.g., variances of the latent
variables, pattern/structure coefﬁcients from the latent variable to the indicator variables)
associated with the model. Subtracting the number of parameters to be estimated from
the number of known elements yields the degrees of freedom of the model. The goal is
for the model to be overidentiﬁed, that is, for its degrees of freedom to be greater than
zero (i.e., have a positive value).
If the model is not properly identiﬁed, IBM SPSS Amos will request that one or
more additional paths be constrained (thereby reducing the number of parameters to be
estimated). If researchers receive such a message, they should determine which path
they wish to constrain, place the cursor on that path, double-click to produce the Object
Properties pop-up window, select the Parameters tab, and in the Regression weight
panel type in the value 1. Finally, close the dialog pop-up window and attempt to execute
the analysis again.
43.9
EVALUATING THE STRUCTURAL MODEL:
ANALYSIS OUTPUT
To access the results of the analysis, select the View the output path diagram icon (the
one on the right) in the top output panel immediately to the left of the drawing area as
shown in Figure 43.34. What is displayed is the path diagram with the Unstandardized
estimates.
To obtain the output in tabular form, select from the main menu View ➔Text
Output. This action opens the Notes for Model screen shown in Figure 43.35. The
chi-square results shown in this screen are identical to those obtained in the conﬁrmatory
factor analysis, as the model ﬁt output addresses the measurement model. We show
the other ﬁt indexes of relevance in Figure 43.36 (they duplicate our conﬁrmatory factor

442
STRUCTURAL EQUATION MODELING
FIGURE 43.35 Selecting Text Output from the main menu opens the Notes for Model screen.
analysis results; we performed the two separate analyses to make it clear that the structural
model was dependent on the integrity of the measurement model).
Selecting Estimates in the left panel presents the unstandardized regression coefﬁ-
cients (labeled as Regression Weights), the SE associated with the coefﬁcients (S.E.),
and the C.R., as shown in Figure 43.37. The critical ratio is the coefﬁcient divided by the
SE and operates something as a z statistic with absolute values of 1.96 or greater, indi-
cating statistical signiﬁcance based on an alpha level of .05. The standardized regression
coefﬁcients are shown in Figure 43.38.
Each coefﬁcient is tested for statistical signiﬁcance (reported under the column
labeled P in Figure 43.36). The triple asterisks (***) represent a probability level of
p < .001. The lower set of coefﬁcients in both the unstandardized and standardized tables
duplicate what we obtained in the conﬁrmatory analysis. What is new in the present anal-
ysis is the upper set of coefﬁcients relating the latent variables to each other. Although
the ﬁt indexes, with the exception of chi-square, suggested a good measurement model
ﬁt, only two of the ﬁve hypothesized paths were associated with statistically signiﬁcant
coefﬁcients, neither of which involved anxiety.
• The path from efﬁcacy to science yielded an unstandardized coefﬁcient of .425
and a standardized coefﬁcient of .276 (p = .001).

EVALUATING THE STRUCTURAL MODEL: ANALYSIS OUTPUT
443
FIGURE 43.36 The Model Fit indexes of relevance.
• The path from science to statistics yielded an unstandardized coefﬁcient of .499
and a standardized coefﬁcient of .317 (p = .003).
Figure 43.39 presents the Covariance and Correlation between the two exogenous
latent variables of anxiety and efﬁcacy. The two were correlated to −.371.
Figure 43.40 presents the Squared Multiple Correlations for all endogenous vari-
ables. Of relevance are the two latent variables, science and statistics, shown in the ﬁrst
two rows. The model could explain approximately 7% and 15% of the variance of these
two variables, respectively.
Figure 43.41 shows the results of the calculations for the indirect effects based on the
standardized coefﬁcients. Given the statistical signiﬁcance of the set of paths, the only
relevant indirect path is from efﬁcacy through science to statistics. Hand-calculating

444
STRUCTURAL EQUATION MODELING
FIGURE 43.37 The unstandardized regression coefﬁcients.
this value (multiplying the two path coefﬁcients) yields .276 * .317 or .087 (on our hand
calculator). The more precise value of .088 is found in the table at the coordinate of
efﬁcacy and statistics (the ﬁrst entry in the second row).
The Modiﬁcation Indices are shown in Figure 43.42. None of these additions to the
model appear theoretically justiﬁable and so we will not act on them.
43.10
EVALUATING THE STRUCTURAL MODEL: SYNTHESIS
Although the measurement model appeared to represent an acceptable ﬁt to the data,
three of the ﬁve hypothesized paths weaving the latent variables together into a coherent
structure were not statistically signiﬁcant. Anxiety was not associated with any statis-
tically signiﬁcant path (at least with efﬁcacy in the model), and efﬁcacy appeared to
inﬂuence statistics only indirectly through science (at least with anxiety in the model).
Most researchers would likely take two steps at this juncture to productively move
forward in their research program: (a) they would probably trim the model down in a
thoughtful manner and rerun the procedure as an exploratory analysis and (b) then they
might rethink the dynamics underlying the relationships between the variables, perhaps
consider other variables as predictors, collect new data on a revised model that they
might envision, and test the ﬁt of their new model on the newly collected data. We take
the ﬁrst of these two steps here.

THE STRATEGY TO CONFIGURE AND ANALYZE A TRIMMED MODEL
445
FIGURE 43.38 The standardized regression coefﬁcients.
FIGURE 43.39 The Covariance and Correlation between the two exogenous latent variables
anxiety and efﬁcacy.
43.11
THE STRATEGY TO CONFIGURE AND ANALYZE
A TRIMMED MODEL
With no statistically signiﬁcant path connecting anxiety to either endogenous latent vari-
able, we make the decision to remove it from the model. But a model is a holistic entity
and removing one of its parts can change the nature of the relationships that remain in
a reconﬁgured model. When anxiety was in the model, the only statistically signiﬁcant
path leading from efﬁcacy was to science—its direct effect on statistics was not statis-
tically signiﬁcant. Let us presume at this point that the indirect effect of efﬁcacy through
science on statistics will remain viable, with anxiety removed from the model.

446
STRUCTURAL EQUATION MODELING
FIGURE 43.40 The Squared Multiple Correlations for all endogenous variables.
FIGURE 43.41 The Standardized Indirect Effects.
With the removal of anxiety in a revised model, it is possible that the direct effect of
efﬁcacy on statistics could now be worth hypothesizing. The extent to which this option is
reasonable hinges on whether, in isolation, efﬁcacy is signiﬁcantly predictive of statistics.
If efﬁcacy signiﬁcantly predicts statistics in isolation, then we may have a situation where
the effect of efﬁcacy on statistics is mediated either partially or completely by science.
This line of thought suggests a strategy to use in exploring a trimmed model that is
analogous to the strategy described in Chapter 40 for simple mediation and Chapter 41
for path analysis. First, we will perform a simple SEM analysis (assuming that any SEM
analysis can be called simple) with efﬁcacy hypothesized to predict statistics in isolation

EXAMINING THE DIRECT EFFECT OF EFFICACY ON STATISTICS IN ISOLATION
447
FIGURE 43.42 The Modiﬁcation Indices.
(i.e., we will have just two latent variables in the model). We would then follow one of
these two scenarios depending on the outcome of that analysis:
• If efﬁcacy is a statistically signiﬁcant predictor of statistics in isolation, then we
will perform a mediation analysis akin to the path structure described in Chapter
40; that is, we will include in the trimmed model the direct path from efﬁcacy to
statistics in addition to the indirect path from efﬁcacy through science to statistics.
• If efﬁcacy does not signiﬁcantly predict statistics in isolation, then we will specify
the trimmed model with only the indirect path from efﬁcacy through science to
statistics.
43.12
EXAMINING THE DIRECT EFFECT OF EFFICACY
ON STATISTICS IN ISOLATION
With the statistics achievement data ﬁle open and IBM SPSS Amos in landscape ori-
entation, we draw the model in which efﬁcacy directly predicts statistics as shown in
Figure 43.43. Then from the main menu, we select View ➔Analysis Properties to
open the Analysis Properties window, and in the window of the Output tab, we check
Standardized estimates and Squared multiple correlations. To execute the analysis,
select Analyze ➔Calculate Estimates from the main menu.
To access the results of the analysis, select the View the output path diagram icon
(the one on the right) in the top output panel immediately to the left of the drawing area.
The path diagram with the Unstandardized estimates is shown in the top screenshot

448
STRUCTURAL EQUATION MODELING
FIGURE 43.43 The simple model with efﬁcacy predicting statistics.
in Figure 43.44; we also show the Standardized estimates in the bottom screenshot in
Figure 43.44.
To obtain the output in tabular form, select from the main menu View ➔Text
Output and select Model Fit in the left panel. The ﬁt indexes of relevance are shown
in Figure 43.45. The chi-square value is 1.342; with 1 degree of freedom, it is not
statistically signiﬁcant (p = .247), suggesting concordance between the observed and
expected values. The GFI, NFI, CFI, and RMSEA yielded values of .997, .997, .999,
and .042, respectively, indicating a very good ﬁt of the measurement model.
The Estimates screenshots are presented in Figure 43.46. The top screenshot informs
us that the path from efﬁcacy to statistics was statistically signiﬁcant (p = .006), yielding
an unstandardized coefﬁcient of .501 (with an SE of .181) and a standardized coefﬁcient
of .222. The bottom screenshot in Figure 43.46 shows us that efﬁcacy explained almost
5% of the variance of statistics (a Squared Multiple Correlation of .049). Thus, with
efﬁcacy signiﬁcantly predicting statistics in isolation, we can proceed to conﬁgure a
mediation structure as our trimmed model in the next analysis.
43.13
EXAMINING THE MEDIATED EFFECT OF EFFICACY
ON STATISTICS THROUGH SCIENCE
The mediation model with efﬁcacy hypothesized as exerting both a direct effect on statis-
tics as well as an indirect effect on statistics through science is shown in Figure 43.47.
After performing the analysis, the Unstandardized estimates and the Standardized
estimates are presented in the top and bottom screenshots, respectively, of Figure 43.48.
Figure 43.49 presents the relevant information from the Model Fit Summary. The
chi-square value is 4.017; with 6 degrees of freedom, it is not statistically signiﬁcant
(p = .674), suggesting close agreement between the observed and the expected values

EXAMINING THE MEDIATED EFFECT OF EFFICACY ON STATISTICS THROUGH SCIENCE
449
FIGURE 43.44 The unstandardized and standardized coefﬁcients for the model.

450
STRUCTURAL EQUATION MODELING
FIGURE 43.45 The model ﬁt indexes for the simple model.
based on the model. The GFI, NFI, CFI, and RMSEA yielded values of .993, .993,
1.000, and .000, respectively, indicating an exceptionally good ﬁt of the measurement
model to the data.
The Estimates screenshots are presented in Figure 43.50. From the top screenshot,
we learn that all three paths in the mediation model are statistically signiﬁcant:
• The path from efﬁcacy to science yielded an unstandardized coefﬁcient of .442
(with an SE of .127) and a standardized coefﬁcient of .268 (p < .001).
• The path from efﬁcacy to statistics yielded an unstandardized coefﬁcient of .350
(with an SE of .176) and a standardized coefﬁcient of .156 (p = .047).

EXAMINING THE MEDIATED EFFECT OF EFFICACY ON STATISTICS THROUGH SCIENCE
451
FIGURE 43.46 The Unstandardized and Standardized Regression Weights, as well as the
Squared Multiple Correlations for the simple model.
• The path from science to statistics yielded an unstandardized coefﬁcient of .430
(with an SE of .142) and a standardized coefﬁcient of .316 (p = .003).
The bottom screenshot shows us that efﬁcacy explained approximately 7% of the
variance of science (a Squared Multiple Correlation of .072) and that efﬁcacy in
combination with science explained approximately 15% of the variance of statistics (a
Squared Multiple Correlation of .151).

452
STRUCTURAL EQUATION MODELING
FIGURE 43.47 The mediation model with efﬁcacy hypothesized to directly inﬂuence statistics
as well as indirectly inﬂuence statistics through science.
The Standardized Indirect Effects are displayed in Figure 43.51. The only indirect
effect in the model is efﬁcacy through science to statistics, and its value is a nontrivial
.085.
43.14
SYNTHESIS OF THE TRIMMED (MEDIATED)
MODEL RESULTS
Based on visual inspection of the results, it appears that the direct effect of efﬁcacy on
statistics is mediated by science. Furthermore, because the direct regression coefﬁcient
from efﬁcacy to statistics in the mediated model is lower than that in the simple model
(but still statistically signiﬁcant), it would appear that we have observed a partial media-
tion effect. These impressions based on the visual inspection of the results now need to
be evaluated statistically so that we can draw our conclusions from the data analysis.
43.15
STATISTICAL SIGNIFICANCE OF THE INDIRECT EFFECT:
THE AROIAN TEST
As described in Section 40.8, we use the Aroian test (Aroian, 1947) as a representative
of the Sobel test family to evaluate the statistical signiﬁcance of the indirect effect.
Based on the unstandardized partial regression coefﬁcients and SEs associated with the
paths efﬁcacy to science (b = .442, SE = .127) and science to statistics (b = .430,
SE = .142), we obtain (using the calculator available on Kristopher J. Preacher’s web site
http://quantpsy.org/sobel/sobel.htm) an Aroian z value of approximately 2.23, p = .023.

STATISTICAL SIGNIFICANCE OF THE INDIRECT EFFECT: THE AROIAN TEST
453
FIGURE 43.48 The unstandardized and standardized coefﬁcients for the mediation model.

454
STRUCTURAL EQUATION MODELING
FIGURE 43.49 The model ﬁt indexes for the mediation model.
We therefore conclude that the indirect path from efﬁcacy through science to statistics
is statistically signiﬁcant.
43.16
COMPARING THE DIRECT EFFECTS OF EFFICACY
ON STATISTICS IN THE SIMPLE MODEL AND THE MEDIATED
MODEL: THE FREEDMAN-SCHATZKIN TEST
In the simple model, the unstandardized regression coefﬁcient from efﬁcacy to statistics
was .501, with an SE of .181; in the mediated model, the unstandardized regression
coefﬁcient from efﬁcacy to statistics was .350, with an SE of .176. Based on the
Freedman–Schatzkin test (Freedman & Schatzkin, 1992), by comparing the relative

COMPARING THE DIRECT EFFECTS OF EFFICACY
455
FIGURE 43.50 The Unstandardized and Standardized Regression Weights as well as the
Squared Multiple Correlations for the mediation model.

456
STRUCTURAL EQUATION MODELING
FIGURE 43.51 The Standardized Indirect Effects.
strengths of the paths from the exogenous variable to the outcome variable in the unmedi-
ated model and the mediated model as described in Section 40.9, we obtain a t value of
approximately 3.76. With a sample size of 196 cases, the degrees of freedom are 194 and
the obtained t value is statistically signiﬁcant (p < .001). We therefore conclude that the
direct path from efﬁcacy to statistics is less signiﬁcant (but still statistically signiﬁcant)
in the mediated model, indicating that we have observed a partial mediation effect.
43.17
THE RELATIVE STRENGTH OF THE MEDIATED EFFECT
We now calculate the relative strength of the mediation effect using the standardized
coefﬁcients that are associated with the paths in our mediation model (as described in
Section 40.10). This is computed as the ratio of the strength of the indirect effect to the
strength of the direct effect and is calculated as follows:
• The strength of the indirect effect is indexed by the product of the standardized
coefﬁcients associated with paths efﬁcacy to science and science to statistics in
the mediated model. As seen in Figure 43.51, this value is .085.
• The strength of the isolated direct effect is indexed by the value of the standardized
coefﬁcient in the unmediated model; its value is .222.
• The relative strength of the mediated effect is equal to the indirect effect divided
by the direct effect. Here it is equal to .085/.222 or approximately .383.
We may then conclude that about 38% (38.3%) of the effect of efﬁcacy on statistics
is mediated through science.

P A R T 14
t TEST


C H A P T E R
4 4
One-Sample t Test
44.1
OVERVIEW
William Gosset was a chemist and mathematician who worked for the Guinness Brewing
Company and developed the t test in an effort to track the quality of the beer that was
produced. When Gosset wished to publish a description of this statistical technique, he
had to do so under a pseudonym to protect proprietary trade secrets. With the help of his
colleague Karl Pearson, he published under the name of Student in Biometrika in 1908
(Student, 1908). The letter t was selected from the last letter in Student to depict the new
procedure and its distribution (Salsburg, 2001).
There are three types of t test procedures: one-sample t test, independent-samples
t test, and the paired-samples t test. The one-sample t test described in this chapter is
used when we have data on a dependent variable from a single group or sample and
wish to determine whether the sample mean is statistically different from the known (or
assumed) population mean. The general strategy is as follows:
• The standard error of the sample mean is computed.
• A conﬁdence interval that corresponds to a particular alpha level (e.g., .05, or 95%
conﬁdence interval) is computed from the standard error.
• We determine where the population parameter falls with respect to the conﬁdence
interval.
If the estimate of the population parameter falls inside the conﬁdence interval or
range, we would judge the sample mean and population to be not signiﬁcantly different; if
the population parameter falls outside the conﬁdence interval, we would judge the sample
mean and population parameter to be signiﬁcantly different. The statistical assessment is
made by means of a t test, with the null hypothesis assuming equivalence between the
sample mean and the population parameter.
44.2
NUMERICAL EXAMPLE
To illustrate the one-sample t test, we use the hypothetical mental health data set, a portion
of which is shown in Figure 44.1. A total of 25 mental health consumers were given
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
459

460
ONE-SAMPLE T TEST
FIGURE 44.1 A portion of the data ﬁle.
intake evaluation interviews at a new mental health facility. The assessment included
evaluating each consumer on their GAF Axis V rating of the Diagnostic and Statistical
Manual of Mental Disorders, IV-TR (DSM-IV-TR; American Psychiatric Association,
2000). The GAF scale values can range from 1 (severe impairment) to 100 (good general
functioning). The question being investigated is whether the consumer GAF sample mean
is comparable to the consumer GAF statewide average of 55. The data can be found in
the ﬁle named One Group t Test.
44.3
ANALYSIS SETUP
Open the IBM SPSS® save ﬁle named One Group t Test and from the main menu select
Analyze ➔Compare Means ➔One-Sample T Test. This produces the One-Sample
T Test dialog window shown in Figure 44.2. We have moved the GAF variable to the
Test Variable(s) panel and typed 55 in the Test Value panel.
Clicking the Options pushbutton produces the Options dialog window in
Figure 44.3. This dialog box allows us to set the Conﬁdence Interval Percentage; in
this example, we have retained the default value of 95%. The Missing Values option
has also been left at its default setting of Exclude cases analysis by analysis. Clicking
Continue takes us back to the main dialog window, and clicking OK generates the
analysis.

ANALYSIS OUTPUT
461
FIGURE 44.2
The main One-Sample T Test dialog window.
FIGURE 44.3
The Options dialog window of the One-Sample T Test.
FIGURE 44.4 Output of the One-Sample T Test.
44.4
ANALYSIS OUTPUT
The output for the analysis is presented in Figure 44.4. The top table, One-Sample
Statistics, provides the sample size, the sample mean and standard deviation, and the
standard error of the mean. The bottom table, One-Sample Test, contains the presumed
population mean (labeled Test Value at the top of the table), the t statistic, its degrees
of freedom, signiﬁcance level, mean difference, and the 95% lower and upper bounds of
the conﬁdence interval of the difference.
From the output, we can see that the value of the t statistic is −4.907. It is negative
for the following reason: the mean difference was calculated as the sample mean (47.44)

462
ONE-SAMPLE T TEST
minus the population mean (55.00), resulting in the mean difference of −7.56, and thus
the t value will be negative. With 24 degrees of freedom (number of cases −1), the t
value was statistically signiﬁcant (signiﬁcantly lower than zero) based on an alpha level
of .05 (IBM SPSS displays a Sig. (2-tailed) probability value of .000, which is treated
as p < .001). The signiﬁcance level is labeled “2-tailed” because it sums the areas of
both tails of the t distribution. The present data indicate that the sample mean of 47.75
is signiﬁcantly lower than the population mean of 55.

C H A P T E R
4 5
Independent-Samples t Test
45.1
OVERVIEW
The independent-samples t test assesses whether the means of two independent groups
(measured as a nonmetric or categorical independent variable) signiﬁcantly differ on
a metric or quantitatively measured dependent variable. A one-way between-subjects
ANOVA, as treated in Chapter 47, is the general case of the independent-samples t test
and is often used in its stead.
The sampling distribution of t is leptokurtic (a bit compressed toward the center with
slightly raised tails compared to the normal distribution). It is symmetrically distributed
around a mean difference of zero, which is the null hypothesis. With increasingly larger
sample sizes, the t distribution becomes more normal, and by the time we reach Ns of
triple ﬁgures, it is very close to a normal distribution.
Interpreting the results of a t test rests on meeting the following assumptions:
• The observations are independent. This assumption is generally achieved through
explicit randomization procedures, where participants are randomly and inde-
pendently assigned to treatment conditions or levels of the independent variable
(Gamst et al., 2008).
• The dependent variable is normally distributed. Normality can be assessed by a
graphical examination of the distribution of dependent variable scores or through
special tests of univariate normality (see Chapter 20).
• The variance of the dependent variable is comparable across levels of the indepen-
dent variable. This assumption of homogeneity of variance is addressed by default
directly within the IBM SPSS® t test procedure, and we discuss it later in more
detail.
The equality of variance assumption, known as homogeneity of variance, is that the
two groups have comparable variability on the dependent variable. This assumption is
tested in IBM SPSS by the Levene test; if the test is statistically signiﬁcant, then homo-
geneity of variance has been violated. To the extent that this homogeneity assumption
is violated, use of the standard t test is not appropriate, as it will generally increase the
chances of making a Type I error; instead, the alternative t test procedure introduced by
Welch (1937) and modiﬁed by Satterthwaite (1946), provided in the default IBM SPSS
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
463

464
INDEPENDENT-SAMPLES t TEST
output (Moser & Stevens, 1992), is to be used instead. We present two examples to
demonstrate how to work with homogeneity of variances; the ﬁrst is an instance where
the assumption is met by the data set and the second is an instance where there is a
violation of the assumption.
45.2
NUMERICAL EXAMPLE: MEETING THE HOMOGENEITY
OF VARIANCE ASSUMPTION
Our example concerns performance in a history test (history_test) by male (sex coded
as 1) and female (sex coded as 2) middle-school students. We are interested in whether
or not there is a reliable (statistically different) test score difference between males and
females. The data can be found in the data ﬁle history test, and a screenshot of part of
the data ﬁle can be seen in Figure 45.1.
45.3
ANALYSIS SETUP: MEETING THE HOMOGENEITY
OF VARIANCE ASSUMPTION
Open the ﬁle named history test and from the main menu select Analyze ➔Compare
Means ➔Independent-Samples T Test. This produces the main Independent-Samples
T Test dialog window shown in Figure 45.2. We have moved the history_test variable
to the Test Variable(s) panel; this will be the dependent variable in the analysis.
We next move sex into the Grouping Variable panel to represent our independent
variable. Note in Figure 45.2 the question marks in parentheses in the panel next to
sex. This is a prompt from IBM SPSS asking us to indicate the values (codes) of the
Grouping Variable. We accomplish this by clicking the Deﬁne Groups pushbutton,
which displays the Deﬁne Groups dialog window seen in Figure 45.3. The two group
FIGURE 45.1
A portion of the data ﬁle.

ANALYSIS OUTPUT: MEETING THE HOMOGENEITY OF VARIANCE ASSUMPTION
465
FIGURE 45.2
The main Independent-Samples T Test dialog window.
FIGURE 45.3
The Deﬁne Groups screen of the Independent-Samples
T Test dialog window.
FIGURE 45.4
The options window of the Independent-Samples T Test.
codes are 1 and 2 and we have ﬁlled them in. Clicking Continue brings us back to the
main Independent-Samples T Test dialog window.
Clicking the Options pushbutton produces the Options dialog window shown in
Figure 45.4. We can set the Conﬁdence Interval Percentage to whatever level we wish,
but we have retained the default value of 95%. The Missing Values box has also been
left at its default setting of Exclude cases analysis by analysis. Clicking Continue takes
us back to the main dialog window, and clicking OK generates the analysis.
45.4
ANALYSIS OUTPUT: MEETING THE HOMOGENEITY
OF VARIANCE ASSUMPTION
The output for the analysis is presented in Figure 45.5. The Group Statistics table
provides the group size, mean, standard deviation, and standard error of the mean for

466
INDEPENDENT-SAMPLES t TEST
FIGURE 45.5 Output from the Independent-Samples T Test.
each level of the categorical independent variable. Males and females averaged 75.58
and 83.46, respectively, in the history test.
The Independent Samples Test table provides several pieces of information. The
ﬁrst two columns of the statistical output show the results for Levene’s equality of
variances test, tested with an F ratio (see Chapter 47). The Levene test evaluates the ratio
of the group variances (larger variance divided by smaller variance), and a statistically
signiﬁcant result indicates unequal variance across groups. In the present example, the F
of .141 yielded a probability of occurrence (Sig.) of .711, given the null hypothesis (of
no difference in variances) is true. The F ratio was therefore not statistically signiﬁcant,
indicating that the assumption of equal variances was met.
To the right of the Levene test results, the table splits into two rows, and the one we
use depends on whether the assumption of homogeneity of variances was met (Equal
variances assumed in the top row) or was violated (Equal variances not assumed in
the bottom row). Because the assumption holds for the data set we are using here, we
examine the output in the ﬁrst row.
The Mean Difference of −7.878 is computed by subtracting the mean of Group 2
(83.46) from the mean of Group 1 (75.58), and the Std. Error Difference (2.286) is the
denominator of the t ratio. The 95% Conﬁdence Interval of the Difference is centered
on the mean difference and is based on the Std. Error Difference.
The t test represented for Equal variances assumed is the standard t test. The t
statistic of −3.446 was negative, informing us that the group deﬁned as 1 in the Deﬁne
Groups dialog window had a smaller mean than of the group deﬁned as 2 (the mean
difference is computed as mean of Group 1 minus mean of Group 2). Had these codes
been reversed (the coding in a two-group t test is arbitrary), the t value would have been
positive. Note that whether positive or negative, the probability indicates the distance in
the sampling distribution between zero and this absolute value. Here, with 23 degrees of
freedom (total degrees of freedom is N −1, which is 25 −1, or 24, and we subtract from
that one additional degree of freedom for the two-group independent variable), the t value
of −3.446 was statistically signiﬁcant; its probability of occurrence if the null hypothesis
is true (Sig. (2-tailed)) is .002, which is less than our (presumed) alpha level of .05.
Viewing the means allows us to interpret this result to indicate that females performed
signiﬁcantly better than males in the history test.

NUMERICAL EXAMPLE: VIOLATING THE HOMOGENEITY OF VARIANCE ASSUMPTION
467
45.5
MAGNITUDE OF THE MEAN DIFFERENCE
With a statistically signiﬁcant mean difference, it is appropriate to assess the magnitude
of the effect. There are two related and not mutually exclusive ways to make such a
determination:
• Strength of Effect. This approach is quite general and applies to most procedures
based on the general linear model (e.g., t tests, ANOVA, least-squares regression);
its index is the squared correlation coefﬁcient known as eta square.
• Effect Size. This approach converts the mean difference between two conditions
into standard deviation units; its index is Cohen’s d statistic.
Strength of Effect
Strength of effect is indexed by eta square and represents the amount of variance of
the dependent variable that is explained by the independent variable (Meyers et al.,
2013); it is akin to R2 in ordinary least-squares regression (Cohen et al., 2003). In the
absence of any context, the general guideline suggested by Cohen (1988, pp. 285–288)
for interpreting the strength of effect is that eta square values of .01, .06, and .14 can
be considered to be small, medium, and large, respectively. For the two-group t test, eta
square can be computed as follows (Hays, 1981):
eta square = t2/(t2 + degrees of freedom)
In the present example, t = −3.446, t2 = 11.875, and degrees of freedom = 23;
eta square thus computes to .043. We would therefore consider the superior performance
of the females to represent a medium effect (with no context within which to frame the
importance of this difference); that is, test-taker sex explained about 4% of the variance
in history test scores.
Effect Size
Effect size is a concept rather strongly identiﬁed by Jacob Cohen (1988) and is symbolized
by d. It provides a criterion for interpreting the separation of two means by determining
the distance between them in standard deviation units. Cohen’s d is computed by dividing
the mean difference by the (weighted) average standard deviation of the groups. For
example, a d value of .50 informs us that the means are half a standard deviation apart.
In the absence of any context, effect sizes of .20, .50, and .80 are considered to be small,
medium, and large, respectively (Cohen, 1988, pp. 24–27).
In the present example, the weighted average of the standard deviations is computed
as follows: ((5.484 * 12) + (5.911 * 13))/25 = 5.706. With a mean difference of −7.878,
Cohen’s d = −7.878/5.706 = −1.38. Thus, males scored approximately one and a third
standard deviation units lower than females; generally, a mean difference exceeding one
standard deviation would be considered a relatively large effect size.
45.6
NUMERICAL EXAMPLE: VIOLATING THE HOMOGENEITY
OF VARIANCE ASSUMPTION
The present example concerns performance in a test of aptitude for science (science_
aptitude) by male (sex coded as 1) and female (sex coded as 2) elementary-school
students. We are interested in whether or not there are reliable (statistically different) test

468
INDEPENDENT-SAMPLES t TEST
FIGURE 45.6
A portion of the data ﬁle.
score differences between males and females. The data can be found in the ﬁle named
science aptitude, and a screenshot of a part of the data ﬁle can be seen in Figure 45.6.
45.7
ANALYSIS SETUP: VIOLATING THE HOMOGENEITY
OF VARIANCE ASSUMPTION
Open the IBM SPSS ﬁle named science aptitude and from the main menu select
Analyze ➔Compare Means ➔Independent-Samples T Test. This produces the
Independent-Samples T Test dialog window shown in Figure 45.7. We have moved
the science_aptitude variable to the Test Variable(s) panel to serve as the dependent
variable in the analysis.
We move sex into the Grouping Variable panel to represent our independent variable
and click the Deﬁne Groups pushbutton where we supply the two group codes 1 and
2. Clicking Continue brings us back to the main Independent-Samples T Test dialog
window. As we do not wish to change any of the defaults in the Options dialog window,
we click OK to generate the analysis.
45.8
ANALYSIS OUTPUT: VIOLATING THE HOMOGENEITY
OF VARIANCE ASSUMPTION
The output for the analysis is presented in Figure 45.8. The Group Statistics table
shows us that males and females averaged 510.77 and 584.29, respectively, in the aptitude
assessment. The Independent Samples Test table provides the results for Levene’s equal-
ity of variances test. In the present example, the F of 11.024 yielded a probability of

ANALYSIS OUTPUT: VIOLATING THE HOMOGENEITY OF VARIANCE ASSUMPTION
469
FIGURE 45.7
The main Independent-Samples T Test dialog
window.
FIGURE 45.8 Output from the Independent-Samples T Test.
occurrence (Sig.) of .004, given the null hypothesis (of no difference in variances) is true.
In this instance, the F ratio was statistically signiﬁcant, indicating that the assumption of
equal variances was violated.
To the right of the Levene test results, the table splits into two rows, and because
the assumption of equal variances was not met for the data set we are using here, we
examine the output in the second row (Equal variances not assumed).
The Mean Difference of −73.516 is computed by subtracting the mean of Group
2 (584.29) from the mean of Group 1 (510.77), and the Std. Error Difference (33.778)
is the denominator of the t ratio. The 95% Conﬁdence Interval of the Difference is
centered on the mean difference and is based on the Std. Error Difference.
The t test represented for Equal variances not assumed is the Welch–Satterthwaite
t test. The t statistic of −2.176 is not statistically signiﬁcant (p = .067) based on an
adjusted degrees of freedom value of 6.856. Had we used the standard t test (Equal
variances assumed), our t value of −2.800 would have been based on 18 degrees of
freedom and with an expected probability of occurrence of .012 would have reached an
alpha level of .05. Had we taken the standard t test as our result, we would have probably
committed a Type I error (claiming a difference when the group means are not really
different).


C H A P T E R
4 6
Paired-Samples t Test
46.1
OVERVIEW
The paired-samples t test is a somewhat different way to analyze data arrayed in a
Pearson correlation (see Chapter 22) or simple regression (see Chapter 24) design. Each
case is associated with two scores that may represent either different variables or the
same variable measured at two different points in time. We invoke the t test when we are
interested in the differences between the means of the two measures. Such an approach
uses a paired-samples t test because the two measures are linked or paired for each case.
Probably the more common application of the paired-samples design is the
pretest–posttest design to assess any meaningful change that may have occurred in the
dependent variable between the two time periods. This is known as a repeated measures
or within-subject design, and the one-way within-subjects ANOVA is the more general
procedure of this design (see Chapter 51). The paired-samples t test explicitly makes
use of the Pearson correlation between the measures.
46.2
NUMERICAL EXAMPLE
Our present example examines the degree of change in mental health of clients following
6 months of mental health treatment. The dependent variable is performance on the GAF
described in Chapter 44. Clients were assessed at intake (pretest named GAF_time1) and
again after 6 months (posttest named GAF_time2). The question of interest is whether a
statistically signiﬁcant change occurred between the two time periods. The data can be
found in the ﬁle GAF time1 time2. A portion of the data ﬁle is presented in Figure 46.1,
and as can be seen, the data structure is identical to that of a correlation design.
46.3
ANALYSIS SETUP
Open the ﬁle named GAF time1 time2. From the main menu select Analyze➔Compare
Means➔Paired-Samples T Test, which produces the main Paired-Samples T Test
dialog window shown in Figure 46.2. From the variable list on the left side of the dialog
window, we have moved GAF_time1 to the Variable 1 panel and GAF_time2 to the
Variable 2 panel.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
471

472
PAIRED-SAMPLES t TEST
FIGURE 46.1
A part of the data ﬁle.
FIGURE 46.2 The main dialog window of Paired-Samples T Test.
The Options dialog window is the same as we have seen in Chapter 45; we can set
the Conﬁdence Interval Percentage to whatever level we wish and deal with missing
values in a couple of different ways. We retain the defaults of 95% Conﬁdence Interval
Percentage and Exclude cases analysis by analysis (see Figure 46.3). Clicking Continue
takes us back to the main dialog window, and clicking OK generates the analysis.
46.4
ANALYSIS OUTPUT
The output for the analysis is presented in Figure 46.4. The top table (Paired Samples
Statistics) provides descriptive statistics for each measure. It appears that the mean for

ANALYSIS OUTPUT
473
FIGURE 46.3
The Options dialog window of Paired-Samples T Test.
FIGURE 46.4 Output from the Paired-Samples T Test.
GAF_time2 (65.52) is higher than the mean for GAF_time1 (46.08); as higher scores
on the GAF represent healthier functioning, it appears that the treatment might have been
effective.
The middle table of Figure 46.4 (Paired Samples Correlations) informs us that the
Pearson correlation between the GAF scores is .801. The Sig. value of .000 indicates
that the chances of this correlation occurring by chance if the null hypothesis is true is
less than .001, and we would take this to be statistically signiﬁcant.
The bottom table of Figure 46.4 shows the results of the t test. The mean difference
−19.440 (mean of GAF_time1 −mean of GAF_time2) is negative because the posttest

474
PAIRED-SAMPLES t TEST
scores are higher than the pretest scores. The standard deviation of the differences between
pairs of variables is 3.831, and the correlated difference standard error of the mean is
.766. The value of the t statistic is −25.375; with 24 degrees of freedom (N −1), the
Sig. value of .000 indicates that the chances of this correlation occurring by chance if
the null hypothesis is true is less than .001. We therefore conclude that the mental health
treatment signiﬁcantly improved the GAF score of the clients.
46.5
MAGNITUDE OF THE MEAN DIFFERENCE
As was true for the independent-groups t test, we can evaluate the magnitude of the
mean difference as a squared correlation coefﬁcient; in this context, it is indexed by
computing the squared Pearson r. The IBM SPSS® output has provided the Pearson r of
.801; squaring this value yields .64. We may then say that the pretest and posttest scores
share 64% of their variance.
Cohen’s d is computed as the mean difference divided by the average standard
deviation. The mean difference is −19.440 and the average standard deviation is 6.042
((6.232 + 5.852)/2) = 6.042); Cohen’s d thus computes to 3.217. With the mean differ-
ence spanning more than three standard deviation units, we would judge the effect size
to be very large.

P A R T 15
UNIVARIATE GROUP
DIFFERENCES:
ANOVA AND ANCOVA


C H A P T E R
4 7
One-Way Between-Subjects
ANOVA
47.1
OVERVIEW
A one-way between-subjects ANOVA is the generalized form of an independent-groups
t test. The t test involves the comparison of just two groups representing a single inde-
pendent variable; the ANOVA permits us to compare two or more groups. The name of
the ANOVA design is derived from the following considerations:
• It is a one-way design in that there is only one independent variable, although
any number of groups or levels representing that independent variable can be
subsumed.
• It is a between-subjects design in that the cases in each group must be independent
of each other, that is, they must be different entities.
The origins of ANOVA can be traced to Sir Ronald Aylmer Fisher when he worked
at the Rothamsted Agricultural Experimental Station from 1919 to 1933. The staff
researchers at the station in evaluating the effectiveness of different fertilizers were using
only one fertilizer each year, by comparing them across the years despite differences in
temperature and rainfall. Fisher (1921a) showed this methodology to be ﬂawed. Instead,
he started using multiple fertilizers each year by randomly assigning them to different
plots within the ﬁeld to control for a host of environmental conditions and developed the
family of ANOVA techniques to analyze the data (Fisher, 1921b, 1925, 1935a; Fisher &
Eden, 1927; Fisher & Mackenzie, 1923).
The statistic that is computed and tested for statistical signiﬁcance in the ANOVA
procedure is an F ratio. The letter F was ﬁrst used by George W. Snedecor at the Iowa
State University in the ﬁrst edition of his book Statistical Methods (Snedecor, 1934) as a
way to honor Fisher who he knew personally and very much respected. A more complete
history can be found in Salsburg (2001), and a more comprehensive treatment of a wide
range of ANOVA designs can be found in Gamst et al. (2008).
The statistical strategy underlying the ANOVA is to partition (analyze) the total
variance of the dependent variance into its constituent sources of variance. This total
variance is deﬁned in terms of the difference between the grand or overall mean of the
entire sample and each score associated with each case.
Different ANOVA designs partition the variance of the dependent variable into some-
what different sources of variance. In a one-way between-subjects design, the two sources
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
477

478
ONE-WAY BETWEEN-SUBJECTS ANOVA
of variance into which the total variance of the dependent variable is partitioned are as
follows:
• Between-Groups Variance. This reﬂects the differences in means between the
groups and is deﬁned in terms of the differences between the group means and
the grand mean. It represents the effect of the independent variable. However, if
there is any systematic experimental error (confounding) resulting from some other
source covarying with the treatment effect, the mean differences will also repre-
sent the source of experimental error (Keppel & Wickens, 2004). The variance
associated with the between-groups variance is the numerator of the F ratio.
• Within-Groups Variance. This reﬂects the variability within each group and is
deﬁned in terms of the difference between the group mean and each score
within that group. It represents the error of measurement in the study. The
variance associated with this source of variance is the denominator of the F
ratio.
47.2
NUMERICAL EXAMPLE
The data we use for our example are ﬁctional. The Vehicle Testing Corporation has
compared three types of vehicles (vehicle_type), namely, mid-sized cars (coded as 1
in the data ﬁle), full-sized cars (coded as 2 in the data ﬁle), and cross-over SUVs
(coded as 3 in the data ﬁle), based on their risk to drivers in case of an accident.
Using crash-test dummies, the researchers have estimated the level of injury that may
be expected (expected_injury) from a 40-mph collision with a solid and immovable
object. Higher expected_injury scores reﬂect a greater amount of anticipated injury. The
data ﬁle is named vehicle safety, and a screenshot of part of the data ﬁle is shown in
Figure 47.1.
FIGURE 47.1
A portion of the data ﬁle.

ANALYSIS SETUP
479
47.3
ANALYSIS STRATEGY
IBM SPSS® has several procedures available that we can use to perform an ANOVA.
We use the General Linear Model in our ANOVA chapters in this book, as this module
can also be applied to the range of designs that we discuss in the subsequent chapters;
the one exception to this will be in Chapter 48 where the analysis of polynomial trends
is conveniently performed with the One-Way ANOVA procedure.
If the effect of the independent variable is statistically signiﬁcant (in what is called
the omnibus or overall analysis), then it is necessary to perform additional statistical tests
to determine which pairs of means are statistically signiﬁcant (assuming that there are
more than two groups in the analysis); these procedures are generically called multiple
comparison tests or tests of simple effects. There are many types of multiple comparison
tests available as described in detail in Gamst et al. (2008), but we will illustrate the
easiest to apply of the procedures here: the post hoc test. Of those post hoc tests that
appear to be a good compromise for statistical power and also protect against alpha level
inﬂation, we will use the Studentized Range variation of the Ryan–Enoit–Gabriel–Welsch
test. For convenience, we will perform this test in the setup of the omnibus analysis, but
not examine its outcome unless the between-groups F ratio is statistically signiﬁcant.
47.4
ANALYSIS SETUP
We open the data ﬁle vehicle safety and from the main menu, we select Analyze ➔
General Linear Model ➔Univariate (because we have just a single dependent variable
in the analysis). This opens the main Univariate window as shown in Figure 47.2. We
move expected_injury into the Dependent Variable panel and vehicle_type into the
Fixed Factor(s) panel.
The Options dialog window shown in Figure 47.3 is divided into two major areas,
the upper Estimated Marginal Means area and the lower Display area. For our one-way
between-groups design, we do not work with Estimated Marginal Means. In the Display
area, we check Descriptive statistics and Homogeneity tests (to obtain the Levine test
of equal group variances). Click Continue to return to the main dialog window.
FIGURE 47.2
The main Univariate dialog window in the
General Linear Model.

480
ONE-WAY BETWEEN-SUBJECTS ANOVA
FIGURE 47.3
The Options dialog window of Univariate.
The Post Hoc dialog window is shown in Figure 47.4. In the upper portion of the
window, we move vehicle_type into the Post Hoc Tests for panel and check R-E-G-W-
Q to obtain the Ryan–Enoit–Gabriel–Welsch Studentized Range test; this test assumes
equal variances and, if the Levene test returned a statistically signiﬁcant result (indicating
that we failed to meet this assumption), we would perform the analysis again with the
Tamhane’s T2 test. Click Continue to return to the main dialog window and click OK
to perform the analysis.
47.5
ANALYSIS OUTPUT
The Descriptive Statistics table in Figure 47.5 shows the mean, standard deviation, and
group sizes. Groups are represented by the rows and are displayed in the order of their
codes. The bottom table in Figure 47.5 shows Levene’s statistic for homogeneity of
variances. The F ratio of .545 was not statistically signiﬁcant (p = .586) when evaluated
with 2 and 30 degrees of freedom; we may therefore conclude that the assumption of
equal group variances was not violated.
Figure 47.6 displays the ANOVA summary table. Because we performed this analysis
in the General Linear Model procedure, IBM SPSS presents all of the output from
the general linear model analysis. Three of the rows include the output from the full
regression model that we will not use (Corrected Model, Intercept, and Total), whereas
other rows are comparable to what other (simpler) ANOVA procedures ordinarily yield
(a corrected, partial, or reduced model). The sums of squares for the rows represent the
following:
• Corrected Model. This is the sum of all between-subjects effects. As there is only
one between-subjects effect in a one-way between-subjects design (vehicle_type),
this sum of squares is equal to that of the effect of vehicle_type. It is called

ANALYSIS OUTPUT
481
FIGURE 47.4
The Post Hoc dialog window of Univariate.
FIGURE 47.5
Descriptive statistics and the results of the
Levene test.

482
ONE-WAY BETWEEN-SUBJECTS ANOVA
FIGURE 47.6 Summary table for the omnibus one-way between-subjects ANOVA.
“Corrected Model” because it excludes the effect of the intercept that is speciﬁc
to the regression analysis.
• Intercept. This is a regression-speciﬁc effect representing the value of the Y
intercept in the model.
• vehicle_type. This is the effect of the independent variable and our focus of
interest.
• Error. This is the within-groups effect and is an estimate of the measurement
error.
• Total. This is the total sum of squares including the regression-speciﬁc Intercept
effect.
• Corrected Total. This is the total for the reduced model including only the effects
of the independent variable and the error source of variance. When computing the
value of eta square as an estimate of the strength of effect for the independent
variable, this is the proper denominator of that ratio.
We focus on the reduced model. The F ratio is computed as the Mean Square (the
variance) associated with the effect for the independent variable (computed as sum of
squares for vehicle_type divided by its degrees of freedom shown under df) divided by
the Mean Square (the variance) associated with the Error effect (computed as sum of
squares for Error divided by its degrees of freedom). With 2 and 30 degrees of freedom,
the F ratio for the effect of the independent variable of 10.929 is statistically signiﬁcant
(p < .001). We note that attributing the effect to the independent variable presumes that
there is no systematic error component covarying with the treatment effect (Keppel &
Wickens, 2004).
The eta square value associated with the effect is computed as the sum of squares
associated with vehicle_type divided by the sum of squares associated with Corrected
Total. In the present example, we have 1188.424/2819.515 or .421; this value is shown
in the footnote for the table as R Squared together with a value for the adjusted R2 (it
is traditional in the context of ANOVA to report R2 rather than the adjusted R2 and to
refer to it as an eta square value).
With the effect of the independent variable statistically signiﬁcant, we are conﬁdent
that some group means (or some combination of group means) differ from some others
(or some combination of group means). The Ryan–Enoit–Gabriel–Welsch post hoc test
addresses this issue, and its results are shown in Figure 47.7. The groups, shown in the

ANALYSIS OUTPUT
483
FIGURE 47.7
Results of the Ryan–Enoit–Gabriel–Welsch post hoc test.
rows, are ordered by the magnitude of their means; the columns represent subsets of
means whose values do not signiﬁcantly differ under an alpha level of .05.
In the present instance, there are two Homogeneous Subsets of means. Means within
a given Subset column do not statistically differ; means in different Subset columns do
statistically differ. The interpretation of the output is therefore as follows:
• The cross-over SUV mean of 9.64 differs signiﬁcantly from (it is signiﬁcantly
lower than) the other two means, as it is in its own Subset column.
• The full-size car mean of 19.82 and the mid-size car mean of 23.91 do not
signiﬁcantly differ (p = .203), as they are together in a Subset column.
We may therefore conclude that crossover SUVs are projected by the Vehicle Testing
Corporation to result in less serious injury following a 40-mph crash than either full-sized
or mid-sized cars.
We can also compute Cohen’s d value for each signiﬁcant mean difference. For cross-
over SUVs and full-size cars, the mean difference is 10.18 and the average standard
deviation is 6.825; Cohen’s d thus computes to approximately 1.49. For cross-over SUVs
and mid-size cars, the mean difference is 14.27 and the average standard deviation is
7.269; Cohen’s d thus computes to approximately 1.96. Both of these represent substantial
effect sizes.


C H A P T E R
4 8
Polynomial Trend Analysis
48.1
OVERVIEW
A polynomial trend analysis is a specialized application of a one-way between-subjects
ANOVA. It examines the shape of the function (in terms of polynomials) relating three or
more groups where the groups are represented on the X-axis and the value of the means
on the dependent variable are represented on the Y-axis. The data points are connected by
lines to show the pattern. The types of possible polynomial relationships are a function
of the number of groups.
• With two groups, we are limited to a linear relationship. Thus, the dependent
variable is predictable only from a linear function: Y is a function of X.
• With three groups, we may have up to a quadratic relationship. Thus, the dependent
variable is potentially predictable from a linear and quadratic function: Y is a
function of X and X2.
• With four groups, we may have up to a cubic relationship. Thus, the dependent
variable is potentially predictable from a linear, quadratic, and cubic function: Y
is a function of X, X2, and X3.
• With ﬁve groups, we may have up to a quartic relationship. Thus, the depen-
dent variable is potentially predictable from a linear, quadratic, cubic, and quartic
function: Y is a function of X, X2, X3, and X4.
The primary condition to be met in a polynomial trend analysis is that the groups are
able to be ordered in magnitude based on at least an interval-level measurement variable.
This condition is necessary because the “spacing” between the groups on the X-axis must
be ﬁxed in order for the shape of the function to make sense (if we could place the groups
anywhere on the axis, then there could be no “true” shape to the function). Examples of
groups that would meet this requirement include:
• different exam preparation times (e.g., 1 week, 2 weeks, and 3 weeks);
• different drug dosages (50, 100, 150 mg).
Just because a relationship is possible does not mean that it will be exhibited in the
data of every study. Some of the polynomial functions may not be statistically signiﬁcant,
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
485

486
POLYNOMIAL TREND ANALYSIS
and more than one can sometimes describe the data. For example, with three groups, the
means can be related in a linear and/or quadratic manner:
• Fully Linear. The means can fall on a straight line; the quadratic component will
not be signiﬁcant.
• Fully Quadratic. The means can be arrayed in a “V” shape or an inverted “V”
shape; the linear component will not be signiﬁcant.
• Partially Linear and Quadratic. The means can show an increase or a decrease
and then level off; both the linear and quadratic components may be statistically
signiﬁcant.
48.2
NUMERICAL EXAMPLE
The data we use for our example are ﬁctional. The Good Food Market has a large
number of stores across the country selling a variety of food and drink products, including
wine. Management was interested in determining the effect on wine sales when different
numbers of wineries were represented on the shelves of the stores. A total of 56 of their
stores participated in this study, 14 in each of 4 groups. Stores in each group stocked
vintages from a different number of wineries, 20, 40, 60, or 80 different wineries; this
was the independent variable named n_wineries in the data ﬁle. We note here that
number of wineries qualiﬁes as an interval-level variable, thus meeting our requirement
for performing a polynomial trend analysis. The dependent variable named sales_dollars
in the data ﬁle was the sales volume on average each day measured in hundreds of dollars
of wine sales. The data ﬁle is named Wine Sales.
48.3
ANALYSIS STRATEGY
A polynomial trend analysis is a one-way between-subjects ANOVA with one extra
piece: the sum of squares associated with the between-groups variance (the effect of
the independent variable) is partitioned into polynomial components. The analysis thus
entails two levels of partitioning of the variance of the dependent variable.
• At the highest level, the variance of the dependent variable is partitioned into
between-groups variance and error or within-groups variance. The F ratio (mean
square associated with between-groups variance divided by mean square associated
with error variance) assesses the effect of the independent variable (number of
different wineries stocked). With a statistically signiﬁcant effect, we can perform
our multiple comparison tests.
• At a more microscopic level, the between-groups variance is itself partitioned into
polynomial components. Each polynomial component will be associated with (will
explain) a certain amount of variance and will thus be associated with its own F
ratio. We can examine a plot of the means and interpret each of the statistically
signiﬁcant polynomial components.
48.4
ANALYSIS SETUP
We open the data ﬁle Wine Sales and from the main menu, we select Analyze ➔Com-
pare Means ➔One-Way ANOVA. This opens the main One-Way ANOVA window as
shown in Figure 48.1. The One-Way ANOVA is a specialized procedure that allows us

ANALYSIS SETUP
487
FIGURE 48.1
The main One-Way ANOVA dialog window
to perform several types of mean contrasts (see Gamst et al. (2008)) including polyno-
mial contrasts. We move sales_dollars into the Dependent List panel and n_wineries
into the Factor panel.
In the Options dialog window shown in Figure 48.2, we select Descriptives, Homo-
geneity of variance test, and Means plot (to obtain a visual representation of the
function, a very useful part of the output for a polynomial trend analysis), retaining
the default of Exclude cases analysis by analysis.
The One-Way ANOVA procedure also has options for us to perform ANOVA when
the assumption of equal variances among the groups is violated. Both the Brown–Forsythe
and the Welch tests accommodate for unequal group variances. We do not select these
now because the polynomial partitioning is based on the ordinary ANOVA; if our data
violated the equal variance assumption, we would evaluate the F ratio using a more
stringent alpha level. Click Continue to return to the main dialog window.
In the Post Hoc dialog window shown in Figure 48.3, we check R-E-G-W Q to
obtain the Ryan–Enoit–Gabriel–Welsch Studentized Range test; this test assumes equal
variances and, if the Levene test returned a statistically signiﬁcant result (indicating that
we failed to meet this assumption), we would perform the analysis again with Tamhane’s
T2 test. Click Continue to return to the main dialog window.
FIGURE 48.2
The Options dialog window in One-Way ANOVA.

488
POLYNOMIAL TREND ANALYSIS
FIGURE 48.3
The Post Hoc dialog window in
One-Way ANOVA.
FIGURE 48.4
The Contrasts dialog window in
One-Way ANOVA.
In the Contrasts dialog window shown in Figure 48.4, checking Polynomial acti-
vates the Degree drop-down menu next to it. Select Cubic from the menu, as this is
the highest polynomial contrast we can obtain with our four groups. Click Continue to
return to the main dialog window and click OK to perform the analysis.
48.5
ANALYSIS OUTPUT
The descriptive statistics and the results of the Levene test are shown in Figure 48.5. The
Levene Statistic was computed to be .795 and, with 3 and 52 degrees of freedom, was
not statistically signiﬁcant (p = .502). It thus appears that we have met the homogeneity
of variance assumption.
Figure 48.6 displays the ANOVA summary table. The Between Groups (combined)
source of variance represents the effect of the independent variable, the number of
wineries stocked by the stores. It speciﬁes (combined) because this is the effect of

ANALYSIS OUTPUT
489
FIGURE 48.5 The descriptive statistics and Levene’s test of homogeneity of variances.
FIGURE 48.6 The summary table for ANOVA.
the independent variable as a whole (without considering the polynomial components).
The F ratio is 37.232, which, with 3 (number of groups −1) and 52 degrees of freedom,
is statistically signiﬁcant (p < .001). The eta square value associated with this effect can
be computed as the sum of squares for the between-groups variance divided by the total
variance (2118.054/3104.125), or .68; thus, the differences between the groups explained
about 68% of the variance of the dependent variable.
The evaluation of the group differences is seen in Figure 48.7. The results of the
Ryan–Enoit–Gabriel–Welsch Studentized Range test inform us (in order of the magnitude
of the means) that there is no reliable wine sales difference when the stores stocked 20
or 40 different wineries, but that stocking 80 different wineries produced a higher sales
volume from the 20–40 winery level, and that stocking 60 different wineries produced
the highest sales volume of all the groups.
Thus far, we have described the outcome of an ordinary one-way ANOVA. But this
was a polynomial trend analysis, and we can now address this portion of the analysis.

490
POLYNOMIAL TREND ANALYSIS
FIGURE 48.7
Results of the Ryan–Enoit–Gabriel–
Welsch Studentized Range test.
FIGURE 48.8
The plot of the group means.
In the summary table (Figure 48.6) we have the polynomial results. Under the row for
Between Groups (combined) is a set of rows that document the polynomial components
of the between-groups variance. We treat each in order.
A polynomial trend analysis is as much a statistical as a visual analytic enterprise,
and it is useful to view the plot of means as we discuss each polynomial component of
the variance. The means are plotted in Figure 48.8. It appears, generally, that the function
increases to a certain extent from 20 to 40 wineries, sharply increases to 60 wineries,
and then sharply drops to 80 wineries.
In the summary table, the row labeled Linear Term has two parts to it: the row
labeled Contrast and the row labeled Deviation. The Contrast row assesses the linear

ANALYSIS OUTPUT
491
component. Its Sum of Squares is 1100.089, and the F ratio of 58.013 is statistically
signiﬁcant. This can be seen in the plot of means shown in Figure 48.8. There is a general
trend of the function to rise from 20 to 80, even though it is rather “bumpy”; this general
rise in the function is the linear component in that we could ﬁt a straight line through
the data points that would angle upward from 20 to 80.
The Deviation row under the Linear Contrast in the summary table in Figure 48.6
assesses the remaining nonlinear components—whatever is left after the linear compo-
nent of the variance has been explained (which in the present example is the combination
of the quadratic and cubic components). Its Sum of Squares is 1017.964, and the F ratio
of 26.841 is statistically signiﬁcant. Thus, there is a statistically signiﬁcant nonlinear com-
ponent to the means. This can be seen in the plot of means shown in Figure 48.8—it is
the “bumpiness” in the function that was not explained by the linear component. Note
that the Sums of Squares of the Linear Contrast (1100.089) and the Linear Deviation
(1017.964) sum to the total Between Groups (combined) Sum of Squares (2118.054).
We can compute the eta square value associated with the linear component (or
any polynomial component) with respect to either the between-subjects variance or the
total variance. With respect to the between-subject variance, the eta square would be
1100.089/2118.054, or about .519; with respect to the total variance, the eta square
would be 1100.089/3104.125, or about .354. Thus, the linear component accounted for
approximately 52% of the between-subject variance and approximately 35% of the total
variance.
The row labeled Quadratic Term in the summary table in Figure 48.6 also has
two parts to it: the row labeled Contrast and the row labeled Deviation. The Con-
trast row assesses the quadratic component. Its Sum of Squares is 212.161, and the
F ratio of 11.188 is statistically signiﬁcant. Thus, there is a statistically signiﬁcant
quadratic component to the means; its eta square with respect to the between-groups
effect is about 10% (212.161/2118.054) and with respect to the total variance is about
7% (212.161/3104.125). This can be seen in the plot of means shown in Figure 48.8.
Although far from symmetric (its eta square value is much less than that associated with
the linear component), the function does appear to somewhat resemble an inverted “V”
indicative of a quadratic relationship.
The Deviation row under the Quadratic Contrast in the summary table in
Figure 48.6 assesses the remaining component when the linear and quadratic compo-
nents of the variance have been explained. Its Sum of Squares is 805.804, and the F
ratio of 42.494 is statistically signiﬁcant. Thus, there is a statistically signiﬁcant amount
of variance yet to be explained by the remaining polynomial component(s); in the present
example, there is only one such component remaining, and it is the cubic component.
The row labeled Cubic Term in the summary table in Figure 48.6 has only one
part to it, as it is the last polynomial component possible to be assessed in our four-
group example. Its Sum of Squares is 805.804, and the F ratio of 42.494 is statistically
signiﬁcant. This is equivalent to the row above it in that only the cubic contrast remaining
after the quadratic component was assessed. Thus, there is a statistically signiﬁcant
cubic component to the means; its eta square with respect to the between-groups effect
is about 38% (805.804/2118.054) and with respect to the total variance is about 26%
(805.80/3104.125). This can be seen in the plot of means shown in Figure 48.8. There
appear to be two distinct inﬂexion points in the function, one at 40 and another at 60,
and it is these slope changes that are likely driving the cubic component.
Overall, although there is a major linear component in the results, suggesting at
a general level that stocking more wineries is associated with more sales volume, the
cubic function combined with the post hoc test results indicate that the optimal level of
wineries carried by the Good Food Market appears to be 60. Increasing the number of
wineries on their shelves beyond that optimal level appears to actually adversely affect
wine sales.


C H A P T E R
4 9
One-Way Between-Subjects
ANCOVA
49.1
OVERVIEW
Imagine the following scenario. We have k levels of an independent variable that we
would like to associate with differences in dependent variable scores. But there is another
(quantitative) variable that could also potentially affect the performance of the depen-
dent variable, and so any differences we observe between the k groups may be at least
partly attributable to the other variable. One procedure that is available to deal with this
possible confounding issue is to use a one-way between-subjects analysis of covariance
(ANCOVA) design in this situation in an attempt to “purge” the scores of the dependent
variable of this other quantitative variable (by statistically controlling for it) so that we
can evaluate a “purer” effect of the independent variable.
We can statistically control for any number of quantitatively measured variables,
and these controlled-for variables are assigned the role of covariates in the ANCOVA;
we will restrict our coverage here to a single quantitative covariate. We can also apply
ANCOVA to ANOVA designs subsuming any number of independent variables, but we
will restrict our coverage here to the one-way between-subjects design.
In the ﬁrst stage of a one-way between-subjects ANCOVA design, the relationship
(correlation) between the covariate and the dependent variable is determined for the
sample as a whole. As a result of this evaluation, the values of the dependent measure
are adjusted (in much the same way as is done in a multiple regression analysis) such that
the variance associated with the covariate is removed from the dependent variable scores.
These adjusted values (representing the residual variance of the dependent variable)
become the “new” or “revised” scores of the dependent variable and are then evaluated by
ANOVA to assess the effect of the independent variable. If the effect of the independent
variable is statistically signiﬁcant, the k groups differ in the means of the adjusted scores
(and not necessarily in the raw or observed means).
All of the assumptions underlying ANOVA (normal distribution, independence, and
homogeneity of variance) apply to ANCOVA, but ANCOVA adds two additional assump-
tions:
• Linearity of Regression. It is assumed that there is a linear relationship between
the covariate variable and the dependent variable. This is important because
the adjustment process is based on an ordinary least-squares (linear) regression
procedure.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
493

494
ONE-WAY BETWEEN-SUBJECTS ANCOVA
• Homogeneity of Regression. It is assumed that the slope of the regression function,
with the covariate predicting the dependent variable, is equal across the k groups.
This is important because the adjustment procedure is done once on the sample as
a whole, and thus we need to have the relationship between the covariate and the
dependent variable comparable for each group in order for the score adjustment
procedure to be interpretable.
49.2
NUMERICAL EXAMPLE
The ﬁctional data we use for our example are in the data ﬁle named math teaching
methods. A school district wished to explore alternative methods of teaching elementary-
grade math. The researchers established three instructional groups of 12 classes each
under the independent variable of teaching_method: the currently used method (the
standard method is coded as 1), a social method where two students worked in teams to
solve the problems (the social method is coded as 2), and computer assisted instruction
(the CAI method is coded as 3). Believing that children with greater levels of math ability
would probably do better under any teaching method, and wishing to statistically control
for such ability, the researchers obtained the standardized math ability scores (based on
a statewide testing program) from the district ﬁles to be used as a covariate in this study
(math_ability_cov). The dependent variable was the average ﬁnal examination scores
of each class of students reﬂecting course performance over the period of instruction
(exam_grade_dv). A screenshot of part of the data ﬁle is shown in Figure 49.1.
49.3
ANALYSIS STRATEGY
The analysis strategy is as follows:
FIGURE 49.1 A portion of the data ﬁle.

ANALYSIS OUTPUT: ANOVA
495
• First, we perform an ANOVA on the observed scores to show the results when
the covariate is not included in the analysis.
• Second, we assess the degree to which the data set meets the extra assumptions
of ANCOVA.
• Third, we perform the ANCOVA, making sure to carry out the multiple compar-
ison tests (assuming we obtain a statistically signiﬁcant effect of the independent
variable) on the adjusted rather than the raw (observed) means.
49.4
ANALYSIS SETUP: ANOVA
We open the data ﬁle math teaching methods and from the main menu, we select
Analyze ➔General Linear Model ➔Univariate. This opens the main Univariate win-
dow as shown in Figure 49.2. We move exam_grade_dv into the Dependent Variable
panel and teaching_method into the Fixed Factor(s) panel.
In the Options dialog window (see Figure 49.3), we check Descriptive statistics and
Homogeneity tests (to obtain the Levene test of equal group variances). Click Continue
to return to the main dialog window.
We will also request a post hoc test, but will examine its results only if the effect of
the independent variable is statistically signiﬁcant. In the upper portion of the Post Hoc
dialog window (see Figure 49.4), we move teaching_method into the Post Hoc Tests
for panel and check R-E-G-W-Q to obtain the Ryan–Enoit–Gabriel–Welsch Studentized
Range test. Click Continue to return to the main dialog window and click OK to perform
the analysis.
49.5
ANALYSIS OUTPUT: ANOVA
The Descriptive Statistics table in Figure 49.5 shows the mean, standard deviation,
and group sizes. As can be seen, the group means are very close. The middle table in
Figure 49.5 shows Levene’s statistic for homogeneity of variances. The F ratio of 2.202
was not statistically signiﬁcant (p = .127) when evaluated with 2 and 33 degrees of
FIGURE 49.2
The main dialog window of the Univariate procedure of
the General Linear Model.

496
ONE-WAY BETWEEN-SUBJECTS ANCOVA
FIGURE 49.3
The Options dialog window of the Univariate procedure
of the General Linear Model.
Univariate
FIGURE 49.4
The Post Hoc dialog window of the Univariate
procedure of the General Linear Model.
freedom; we may therefore conclude that the assumption of equal group variances was
not violated. The bottom table presents the summary table for the omnibus analysis, and
the F ratio for teaching_method of .168, with 2 and 33 degrees of freedom was not
statistically signiﬁcant (p = .846). It therefore appears that the three teaching methods
were equally effective.

EVALUATING THE ANCOVA ASSUMPTIONS
497
FIGURE 49.5 Descriptive statistics, Levene’s Test and Omnibus Analysis Results.
49.6
EVALUATING THE ANCOVA ASSUMPTIONS
Linearity of Regression
We ordinarily evaluate linearity of regression by visually examining a scatterplot of the
covariate and the dependent variables. From the main menu select Graphs ➔Legacy
Dialogs ➔Scatter/Dot to open the Scatter/Dot window shown in Figure 49.6. Select
Simple Scatter and click Deﬁne. This opens the Simple Scatterplot dialog window
(see Figure 49.7). Click exam_grade_dv to the Y Axis panel (the dependent variable is
represented on the Y-axis) and click math_ability_cov to the X Axis panel. Click OK.
The scatterplot of the dependent variable (exam_grade_dv) as a function of the
covariate (math_ability_cov) result of this setup is shown in Figure 49.8, and it appears
that the two variables are linearly related. To display the regression line, double-click

498
ONE-WAY BETWEEN-SUBJECTS ANCOVA
FIGURE 49.6
Selecting the simple scatterplot.
FIGURE 49.7
Conﬁguring the scatterplot.
inside the scatterplot in the IBM SPSS® output. This gives us access to the Chart Editor.
Select Elements ➔Fit Line at Total. As soon as this is selected, the line of best ﬁt will
appear superimposed on the scatterplot. This is shown in Figure 49.9.
Homogeneity of Regression
What we are looking for when we evaluate the assumption of homogeneity of regression is
whether the individual group regression functions predicting the dependent variable from
the covariate are the same (they have slopes comparable to the total sample regression line
shown in the scatterplot of Figure 49.8). Obtaining the Independent Variable × Covariate
interaction effect allows us to test this assumption; we presume that we have conformed

EVALUATING THE ANCOVA ASSUMPTIONS
499
FIGURE 49.8
Output of the scatterplot.
FIGURE 49.9 The scatterplot with the least-squares regression line ﬁt.

500
ONE-WAY BETWEEN-SUBJECTS ANCOVA
FIGURE 49.10
The main dialog window of the Univariate procedure of the
General Linear Model.
to the homogeneity of regression assumption if the interaction effect is not statistically
signiﬁcant.
From the main IBM SPSS menu select Analyze ➔General Linear Model ➔
Univariate. This opens the main Univariate dialog window shown in Figure 49.10. We
have conﬁgured it with teaching_method as the Fixed Factor, exam_grade_dv as the
dependent variable, and math_ability_cov as the Covariate.
Select the Model pushbutton to reach the dialog screen shown in Figure 49.11. Select
Custom in the area where we Specify Model. This selection opens the two panels on
either side of the window and activates the Build Term(s) drop-down menu. Select Main
FIGURE 49.11 The Model dialog window of the Univariate procedure of the General Linear
Model conﬁgured for main effects.

ANALYSIS SETUP: ANCOVA
501
FIGURE 49.12 The Model dialog window of the Univariate procedure of the General Linear
Model conﬁgured for main effects and the interaction.
effects from the pull-down menu under Build Term(s) and click teaching_method and
math_ability_cov to the Model panel. This is shown in Figure 49.11.
Now select Interaction from the Build Term(s) pull-down menu (replacing the
Main effects choice). Select both teaching_method and math_ability_cov (by holding
down the Ctrl or Shift key while selecting the variables one at a time) and use the
arrow button to click them over to the Model panel. The result of this is shown in
Figure 49.12. Click Continue to return to the main dialog window and click OK to
perform the analysis.
The only output that interests us is the test of statistical signiﬁcance of the teach-
ing_method*math_ability_cov interaction shown in the summary table in Figure 49.13.
As can be seen in the summary table, the effect is not statistically signiﬁcant with an
F ratio of .548 (p = .584). We thus presume that the assumption of homogeneity of
regression has not been violated and proceed with the ANCOVA.
49.7
ANALYSIS SETUP: ANCOVA
Select Analyze ➔General Linear Model ➔Univariate. Conﬁgure the main dia-
log window as we did in testing the assumption of homogeneity of regression, with
teaching_method as the Fixed Factor, exam_grade_dv as the dependent variable, and
math_ability_cov as the Covariate. In the Model window (see Figure 49.10), set the
Specify Model to Full factorial.
In the Options dialog window displayed in Figure 49.14, we check Homogeneity
tests (to obtain the Levene test of equal group variances on the adjusted scores); we do
not request the Descriptive statistics because (a) we already have them on the observed
scores from our initial ANOVA and (b) the covariance analysis is performed on the
adjusted and not the observed scores.
We obtain the adjusted means from the Options dialog window. As shown in
Figure 49.14, in the Estimated Marginal Means area in the top half of the Options

502
ONE-WAY BETWEEN-SUBJECTS ANCOVA
FIGURE 49.13 Tests of Between-Subjects Effects.
FIGURE 49.14
The Options dialog window of the Univariate
procedure of the General Linear Model conﬁgured
for obtaining the estimated marginal means and the
Bonferroni corrected t tests on the group means.
window, we select teaching_method from the Factor(s) and Factor Interactions panel
and click it over to the Display Means for panel. These means are labeled by IBM
SPSS as estimated marginal means and are also known as least-squares means: they are
unweighted means of the values (the adjusted values here) associated with the groups
and are accompanied by standard errors rather than standard deviations.

ANALYSIS OUTPUT: ANCOVA
503
We also click the checkbox under the Display Means for panel for Compare main
effects (see Figure 49.14). This is how we obtain multiple comparison tests in a one-way
covariance design because the Post Hoc tests we have used up to this point are not
available for adjusted scores (the post hoc tests can only be used on raw or observed
data). There are three multiple comparison tests available in the Conﬁdence interval
adjustment drop-down menu, all of which use a t test to assess the mean differences for
each pair of means. The primary difference among them is how they control for alpha
level (Type I error) inﬂation:
• LSD. This is the Least Signiﬁcant Difference test and does not control for alpha
level inﬂation. As such, it is the most powerful of the three (it will detect more
“signiﬁcant” differences than the other two) but the comparisons are really being
evaluated at less stringent alpha levels. It may be appropriate to use when testing
one or two a priori predictions but is generally not recommended for exploratory
purposes.
• Bonferroni. Named after the mathematician Carlo Emilio Bonferroni, this set of
pairwise t tests controls for alpha inﬂation by dividing the ordinarily used .05
level by the number of comparisons—what is called the Bonferroni correction to
the alpha level. It is the most conservative of the three available methods and is
probably the one most frequently used.
• Sidak. Named after the mathematician Zbynek Sidak, this set of pairwise t tests is
a variation of the Bonferroni method that adds a tad more power (it is slightly less
conservative than the Bonferroni correction) but is still relatively conservative.
We select the Bonferroni procedure from the drop-down menu. Click Continue to
return to the main dialog window and click OK to perform the analysis.
49.8
ANALYSIS OUTPUT: ANCOVA
The top table in Figure 49.15 shows the results of Levene’s test of the equality of
error variances. The F ratio is .105 (p = .901), indicating that we appear to meet the
homogeneity of variance assumption. Note that this Levene’s F value is based on the
adjusted scores and yields a different result from that computed on the observed scores
(see Figure 49.5).
The bottom table in Figure 49.15 presents the estimated marginal means together
with their standard errors. Note that these means are different from the observed means
shown in Figure 49.5; the estimated marginal means reﬂect the statistical “removal” of
variance due to math ability and exhibit greater differences than did the observed means.
We show the summary table for the omnibus ANCOVA in Figure 49.16. Both
the covariate of math_ability_cov and the effect of the independent variable of teach-
ing_method are statistically signiﬁcant. The eta square value for teaching_method is
computed by dividing its sum of squares (469.055) by the Corrected Total sum of
squares (6155.000) to yield a value of .076. The eta square value for math_ability_cov
is computed by dividing its sum of squares (4460.331) by the Corrected Total sum of
squares (6155.000) to yield a value of .725.
The Bonferroni corrected pairwise comparisons of the estimated marginal means
are shown in Figure 49.17. The Pairwise Comparisons table contains some redun-
dancy. Each major row focuses on one of the three groups and compares the other two
to it. Consider the ﬁrst major row focusing on the standard method. The difference
between the estimated marginal mean for that method and the estimated marginal mean
for the social method is 61.211 −67.167, or −5.956. This difference is not statisti-
cally signiﬁcant (p = .161). However, the difference between the estimated marginal

504
ONE-WAY BETWEEN-SUBJECTS ANCOVA
FIGURE 49.15 ANCOVA homogeneity of variance test and estimated marginal means.
FIGURE 49.16 ANCOVA summary table.
mean for the standard method and the estimated marginal mean for the CAI method
is 61.211 −70.122 or −8.911 and is statistically signiﬁcant (p = .016). Examining the
other rows suggests that this is the only reliable difference.
Based on the ANCOVA, we would conclude that, when controlling for effects of
math ability, the social method is not more effective than the standard method but the
CAI method appeared to be better than the standard method. Note that this outcome

ANALYSIS OUTPUT: ANCOVA
505
FIGURE 49.17 ANCOVA multiple comparison tests.
is different from what we obtained in our original ANOVA; without taking math ability
into account, the researchers might have erroneously concluded that the two alternative
teaching methods were no more effective than the one they had been using all along,
whereas, once we take math ability into account, the computer-based method appears to
be better than the one currently being used by the school district.


C H A P T E R
5 0
Two-Way Between-Subjects ANOVA
50.1
OVERVIEW
A two-way between-subjects ANOVA involves two between-subjects independent vari-
ables. The levels of these two variables are combined in all possible combinations
(factorially combined), with each combination representing an independent group of
cases. For example, with two levels of independent variable A (a1 and a2) and two lev-
els of independent variable B (b1 and b2), there are four separate conditions (separate
groups of cases) in the design (a1b1, a1b2, a2b1, and a2b2).
Evolving the design from a one-way between-subjects ANOVA to a two-way
between-subjects ANOVA is both a quantitative and a qualitative advancement. It is
a quantitative advancement in that in a one-way design we evaluate only the effect
of the single independent variable in the study, whereas in a two-way design we can
evaluate the effects of each of the independent variables (as though we implemented
two one-way designs). In the two-way design, the effects of the independent variables
are called main effects.
The two-way design is also a qualitative advancement over the one-way design
because we have two independent variables combined factorially. Such a factorial combi-
nation yields a “bonus” effect that is extremely valuable—the omnibus (overall) interac-
tion effect. In a two-way design, this interaction effect is known as the two-way interaction
and is ordinarily referred to as the A × B interaction (in a research study A and B take
on the actual names of the independent variables).
These three effects, the two main effects and the two-way interaction, represent
separate partitions of the variance of the dependent variable, and each is separately tested
for statistical signiﬁcance. The null hypothesis in each test is that the effect accounts for
none of the variance of the dependent variable (i.e., the eta square value for each effect
is zero).
Probably, the primary reason why researchers are drawn to two-way designs is that
they can examine the interaction effect of the two independent variables. Interactions
are higher order effects than main effects (they carry more information), and if they are
statistically signiﬁcant, they supercede (take priority in explicating over) main effects.
This idea of an interaction being a higher order effect is based on the following points:
• The omnibus interaction effect explicitly deals with the means of the separate
conditions (e.g., a1b1, a1b2, a2b1, and a2b2), thus carrying considerable detailed
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
507

508
TWO-WAY BETWEEN-SUBJECTS ANOVA
information. The main effects deal only with their own respective levels; for
example, with two levels of A, the main effect of A addresses the difference
between a1 and a2 (collapsing across the levels of B).
• Because the interaction effect carries more information, the information contained
in the main effects can be explicitly seen in the interaction; because each main
effect averages across the effect of the other, information contained in the inter-
action is “lost” when looking at the main effects.
• Given the above two points, the strategy that researchers typically adopt, all else
equal, is to focus their attention on the omnibus interaction effect if it is statisti-
cally signiﬁcant; they then perform multiple comparison tests on the means of the
conditions (called tests of simple effects in this context) to “simplify” or explicate
the omnibus effect.
• Only when the interaction is not signiﬁcant will most researchers examine in detail
the main effects by comparing the means of the levels within each independent
variable (e.g., using post hoc tests).
A statistically signiﬁcant omnibus interaction effect will be obtained when different
levels of one independent variable produce a different pattern of results across the other
independent variable. Under these conditions, it may be said that one of the independent
variables moderates the effect of the other, in that the pattern across levels is not parallel.
This verbal characterization is illustrated relatively simply in Figure 50.1.
The two plots on the left in Figure 50.1 depict two (of many possible) interaction
effects. The top plot shows the lines representing a1 and a2 crossing and the bottom plot
shows the lines representing a1 and a2 not crossing. We can recognize that both depict
omnibus interaction effects because the functions for a1 and a2 are not parallel (they bear
different relationships across the levels of the independent variable B).
Tests of simple effects compare each pair of means. In the bottom plot, for example,
we would want to determine whether a1 differed from a2 at b1, whether a1 differed from
a2 at b2, whether b1 differed from b2 under condition a1, and whether b1 differed from
b2 under condition a2.
We can also glean some information about the main effects by examining the inter-
action. For example, in the bottom plot, we can reasonably guess that there is probably
a signiﬁcant main effect of A; we can make this guess because the function for a2 is, on
average, higher than a1 (the mean of a2 is reasonably higher than the mean of a1). On
the other hand, in the top plot, the mean of a1 (the midpoint of the a1 line) appears to
be very close to the mean of a2 (the midpoint of the a2 line), suggesting that the main
effect of A (assessing the mean difference between a1 and a2) is probably not statistically
signiﬁcant.
The single plot to the right in Figure 50.1 shows the absence of an interaction effect.
The key in diagnosing the absence of a statistically signiﬁcant interaction is that the
functions for a1 and a2 are parallel (they bear the same relationship across the levels of
the independent variable B). Graphed in this manner, the likely signiﬁcance of the main
effect of A also becomes apparent.
50.2
NUMERICAL EXAMPLE
The data we use for our example is present in the data ﬁle named swelling treatment
are ﬁctional. A drug for a certain type of muscle swelling has just been developed
and is undergoing initial limited testing on 54 patients with the symptom. Swelling is
assessed on a 25-point measure, with higher scores indicating higher levels of swelling;
the dependent variable is named swelling in the data ﬁle. Patients with comparable levels

ANALYSIS SETUP
509
FIGURE 50.1 Two examples of an interaction and an illustration of no statistically signiﬁcant
interaction effect.
of swelling at the outset of the study were randomly assigned to one of three conditions
under the independent variable named treatment: no treatment (coded as 1 in the data
ﬁle), placebo (coded as 2 in the data ﬁle), or medication (coded as 3 in the data ﬁle).
Patient sex is coded in the data ﬁle as 1 for females and 2 for males. A screenshot of
part of the data ﬁle so that the coding may be seen is displayed in Figure 50.2.
50.3
ANALYSIS SETUP
We open the data ﬁle swelling treatment and from the main menu, we select Analyze
➔General Linear Model ➔Univariate. This opens the main Univariate window
as shown in Figure 50.3. We move swelling into the Dependent Variable panel and
treatment and sex into the Fixed Factor(s) panel.
The Post Hoc dialog window is shown in Figure 50.4. In the upper portion of the
window, we move treatment into the Post Hoc Tests for panel, as this independent
variable has three levels. If this main effect is statistically signiﬁcant, and assuming
we met the assumption of homogeneity of variance, we would want to compare the
treatment means if the interaction is not statistically signiﬁcant. We check R-E-G-W-
Q to obtain the Ryan–Enoit–Gabriel–Welsch Studentized Range test but are prepared

510
TWO-WAY BETWEEN-SUBJECTS ANOVA
FIGURE 50.2
A portion of the data ﬁle.
FIGURE 50.3
The main Univariate dialog
window in General Linear Model.
to perform the analysis again with Tamhane’s T2 test if necessary. Click Continue to
return to the main dialog window.
Anticipating the possibility of a statistically signiﬁcant interaction effect, it is useful
to obtain a plot of the means for two reasons: (a) interactions can best be understood
by examining their visual depiction and (b) mapping our simple effects tests to a visual
display facilitates our interpretation. Selecting the Plots pushbutton opens the Proﬁle
Plots dialog window (see Figure 50.5). As both independent variables are categorical, it
is arbitrary how we structure the plot. We have therefore placed sex in the Horizontal

ANALYSIS SETUP
511
FIGURE 50.4
The Post Hoc dialog window of Univariate.
FIGURE 50.5
The Proﬁle Plots dialog window of Univariate to set up the plot.
axis and will have Separate Lines for each treatment. Click the Add pushbutton to
place the plot in the Plots panel (see Figure 50.6) and click Continue to return to the
main dialog window.
The Options dialog window shown in Figure 50.7. In the Display area, we check
Descriptive statistics and Homogeneity tests (to obtain the Levene test of equal group
variances). We also begin the setup for the simple effects tests but will examine those
results only if the interaction effect is statistically signiﬁcant. The ﬁrst step in setting
up the simple effects tests is to focus on the Estimated Marginal Means portion of
the Options window (the upper half of the window). We move the interaction shown
as sex*treatment from the Factor(s) and Factor Interactions panel into the Display
Means for panel, and click Continue to return to the main dialog window.
The next step in setting up the simple effects tests is to click Paste. This opens the
syntax window shown in Figure 50.8 that presents the analysis setup (this is what our
pointing and clicking generated and what actually drives the IBM SPSS® analysis). The

512
TWO-WAY BETWEEN-SUBJECTS ANOVA
FIGURE 50.6
The Proﬁle Plots dialog window of Univariate with the plot
registered with IBM SPSS.
FIGURE 50.7
The Options dialog window of Univariate requesting a
table of estimated marginal means for the interaction
effect.
syntax in a syntax window can be edited in a word processing manner (but with a lot
less power), and we will need to add a few words to this underlying syntax.
Our focus is on the subcommand /EMMEANS = TABLES(sex*treatment). The
translation of this syntax is roughly “create a table of estimated marginal means for
the sex*treatment interaction.” Our simple effects tests will be based on the estimated
marginal means. We take the following steps to perform the full set of simple effects
tests:
• Copy and paste the subcommand directly below the original making sure that it
is indented as the original (see Figure 50.9).
• At the end of the original line, type compare (sex) adj (Bonferroni). Do not type
a period. IBM SPSS will anticipate what is being typed and will offer prompts

ANALYSIS SETUP
513
FIGURE 50.8 The Pasted syntax in a syntax window.
FIGURE 50.9 The syntax with the subcommand /EMMEANS = TABLES(sex*treatment)
duplicated.

514
TWO-WAY BETWEEN-SUBJECTS ANOVA
FIGURE 50.10 The syntax with the subcommand /EMMEANS = TABLES(sex*treatment)
duplicated and the simple effects tests conﬁgured.
that are useful to accept (the screenshot in Figure 50.10 reﬂects the acceptance
of the prompts). This additional syntax translates to “compare the levels of sex
(females vs. males) separately for each level of the other independent variable
(compare females with males for the no treatment condition, again for the placebo
condition, and still again for the medication condition) using a Bonferroni alpha
level correction.”
• At the end of the line that we copied and pasted, type compare (treatment)
adj (Bonferroni). Do not type a period. This addition translates to “compare the
levels of treatment (no treatment vs. placebo vs. medication) separately for
each level of the other independent variable (compare these three groups for the
female patients and compare them again for the male patients) using a Bonferroni
alpha level correction” (see Figure 50.10).
• From the main menu, we select Run ➔All to obtain the output.
50.4
ANALYSIS OUTPUT: OMNIBUS ANALYSIS
The Descriptive Statistics table in Figure 50.11 shows the observed mean, standard
deviation, and group sizes for each sex by treatment condition. The bottom table in
Figure 50.10 shows Levene’s statistic for homogeneity of variances. The F ratio of
1.507 was not statistically signiﬁcant (p = .205) when evaluated with 5 and 48 degrees
of freedom; we may therefore conclude that the assumption of equal group variances
was not violated.
Figure 50.12 displays the ANOVA summary table. Because we performed this anal-
ysis in the General Linear Model procedure, IBM SPSS presents all of the output
from the general linear model analysis, as described in Chapter 47. We focus on the
reduced model as described in Section 47.5. The F ratio is generally computed as the

ANALYSIS OUTPUT: OMNIBUS ANALYSIS
515
FIGURE 50.11
Descriptive statistics and the results of the
Levene test.
FIGURE 50.12 Summary table for the omnibus two-way between-subjects ANOVA.

516
TWO-WAY BETWEEN-SUBJECTS ANOVA
FIGURE 50.13
Plot of the interaction.
Mean Square (the variance) associated with the effect divided by the Mean Square (the
variance) associated with the Error effect.
There are three effects of interest. In the order that they appear in the summary table,
these effects are as follows:
• The main effect of sex with an F ratio of 4.268 and degrees of freedom of 1 and
48 is statistically signiﬁcant (p = .044); its eta square value is 37.500/1313.500,
or approximately .03.
• The main effect of treatment with an F ratio of 34.622 and degrees of free-
dom of 2 and 48 is statistically signiﬁcant (p < .001); its eta square value is
608.444/1313.500, or approximately .46.
• The interaction of sex and treatment with an F ratio of 13.985 and degrees of
freedom of 2 and 48 is statistically signiﬁcant (p < .001); its eta square value is
245.778/1313.500, or approximately .19.
Although the main effect of treatment appears to be a highly potent effect, the
statistically signiﬁcant interaction informs us that the treatment effect (the differences
among the means of the no treatment vs. placebo vs. medication groups) is moderated
(affected) by the sex of the patient. This can be seen in the plot shown in Figure 50.13.
Based on visual inspection of the plot, it appears that
• the no treatment condition (solid line) appears to exhibit the most swelling, and
the difference between females and males is small and not likely to be statistically
signiﬁcant;
• for the placebo condition (dashed–dotted line), females may not be signiﬁcantly
different from the no treatment control, whereas males appear to have less
swelling than the no treatment control;

ANALYSIS OUTPUT: SIMPLE EFFECTS TESTS
517
• for the medication condition (dashed line), females appear to have very little
swelling, whereas males are not very different from the males in the placebo
condition.
50.5
ANALYSIS OUTPUT: SIMPLE EFFECTS TESTS
The syntax we added at the end of the analysis setup generated two sets of simple effects,
one for each subcommand line. The ﬁrst set of simple effects was designed to compare
sex (compare females and males) for each of the treatment levels. The results of this
analysis are shown in Figure 50.14.
The Estimates table shows the estimated marginal means, standard deviations, and
95% conﬁdence intervals for each of the groups. Because we are focused on the individual
groups (and are not combining groups of unequal sample sizes), these estimated marginal
means are equal to the observed means.
The Pairwise Comparisons table shows the Bonferroni corrected t test results on
each pair of means. We described the structure of such a table in Chapter 49. Brieﬂy,
we read across the three major rows to learn that
FIGURE 50.14 Simple effects tests comparing females and males.

518
TWO-WAY BETWEEN-SUBJECTS ANOVA
• for the no treatment condition, females and males do not differ (p = .306);
• for the placebo condition, females have signiﬁcantly more swelling than males
(p = .017);
• for the medication condition, females have signiﬁcantly less swelling than males
(p < .001).
The second set of simple effects was designed to compare treatment (compare no
treatment vs. placebo vs. medication) for each of the sex levels. The results of this
analysis are shown in Figure 50.15. Brieﬂy, we read across the two major rows to learn
the following.
FIGURE 50.15 Simple effects tests comparing treatment levels.

ANALYSIS OUTPUT: SIMPLE EFFECTS TESTS
519
• For females, the medication condition resulted in signiﬁcantly less swelling than
both the placebo (p < .001) and the no treatment (p < .001) conditions; however,
the placebo and no treatment conditions did not differ signiﬁcantly in the amount
of swelling (p = .717).
• For males, both the placebo (p < .001) and the medication conditions resulted in
signiﬁcantly less swelling than the no treatment (p < .001) conditions; however,
the placebo and medication conditions did not differ signiﬁcantly in the amount
of swelling (p = 1.000).
Overall, the results is that the new medication appeared to work for females but not
for males; the placebo worked as well as the medication for males but females obtained
no signiﬁcant relief from the placebo.


C H A P T E R
5 1
One-Way Within-Subjects
ANOVA
51.1
OVERVIEW
A one-way within-subjects ANOVA design represents the generalized form of a paired-
samples t test, with the ANOVA permitting us to compare two or more conditions. It is
also known as a one-way repeated measures design. The name of the ANOVA design is
derived from the following considerations:
• It is a one-way design in that there is only one independent variable, although any
number of levels representing that independent variable can be used in the research.
Usually, this single independent variable is a time marker where measurement of
the same dependent variable is made at various points in time, that is, cases are
repeatedly assessed on a given dependent variable.
• It is a within-subjects or repeated measures design in that the same cases are
repeatedly measured on the same dependent variable; in this sense, the cases serve
as their own control. It is often referred to as a pretest–posttest design when the
within-subjects factor is time related.
• Although it is common to envision a repeated measure design to assess the same
variable on two or more different occasions (a pretest–posttest design), a one-
way within-subjects ANOVA is also capable of assessing two or more different
conditions experienced by the participants. For example, an instructor could use
three different types of graded requirements for a class, such as a class presentation,
a term paper, and a ﬁnal exam. The scores on these three components could
comprise three levels of a within-subjects factor and could be statistically analyzed
by a one-way within-subjects ANOVA.
Using a time-related independent variable generally involves administering one or
more pretest measurements to establish a baseline level of performance on the dependent
measure and then administering a “treatment” of some kind. Following the completion
of the treatment, one or more posttests are made to determine the effect of the treatment
and, potentially, the longevity of its effects.
Substantial drawbacks to the one-way within-subjects design are associated with the
substantial threats to internal and external validity (Shadish, Cook, & Campbell, 2002).
Within-subjects designs that are time-based have no control group; thus, changes in per-
formance over time in the dependent measure may be due in part to the treatment but may
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
521

522
ONE-WAY WITHIN-SUBJECTS ANOVA
also be due to factors that covary with time during which the treatment was administered
but are, by deﬁnition, neither controlled nor measured. Despite the substantial drawback
of no control group being present in such a study, the design is used in situations where
a set of cases is unique (no appropriate control group can be identiﬁed) and subdividing
the set of cases into a “holdout” (control) subsample and a treatment subsample is not
feasible.
The statistical procedure in designs containing a within-subjects variable is to parti-
tion the variance of the dependent variable into two major portions—between-subjects
variance and within-subjects variance—and then subpartition those portions when pos-
sible. In a one-way within-subjects design, the partitioning is as follows:
• The between-subjects variance is not able to be partitioned further; the variance
represents the overall individual differences between the cases and is of little
interest to researchers (IBM SPSS® actually labels this portion of the variance as
between-subjects error).
• The within-subjects variance is further partitioned into the overall effect of the
independent variable—the main effect of the treatment—and within-subjects
error. The error represents the differences in patterns of performance of the cases
across the levels of the treatment and is often called the Treatment × Subjects
interaction. The F ratio for the treatment effect is computed by dividing the Mean
Square associated with the treatment by the Mean Square associated with the
within-subjects error.
51.2
NUMERICAL EXAMPLE
The ﬁctional data we use for our example depict an evaluation of an educational compo-
nent in connection with a mandatory drug rehabilitation program for offenders younger
than 21 years. The data ﬁle is named drug rehab. A total of 11 youth offenders were
assessed on their attitude toward using illegal drugs before the start of the program
(pretest), after completing the 2-week program (posttest1), and 1 month following pro-
gram completion (posttest2). Scores on the attitude measure could range between 5 and
20, with higher scores representing more favorable attitudes toward drug use.
A screenshot of the data ﬁle is shown in Figure 51.1. Note that the levels of the
within-subjects variable are variables and that the values recorded under those vari-
ables are the scores from the attitude survey. Thus, the independent variable in this
within-subjects design—the treatment—is represented by multiple variables (three in
this example); it is for this reason that researchers can conceptualize within-subjects
(repeated measures) designs as a form of a multivariate (multiple dependent measures)
design.
51.3
ANALYSIS SETUP
We open the data ﬁle drug rehab and from the main menu, we select Analyze ➔
General Linear Model ➔Repeated Measures. This opens the Deﬁne Factor(s) window
of Repeated Measures as shown in Figure 51.2. The Within-Subject Factor Name
panel opens with the default of factor1; we change that to drug_education as the
name we created for our independent variable, as shown in Figure 51.3. We also type
3 in the Number of Levels panel. This action sets drug_education as our within-
subjects independent variable. Click Add to register this information with IBM SPSS
(see Figure 51.3) and click Deﬁne to open the main dialog window.

ANALYSIS SETUP
523
FIGURE 51.1
The data ﬁle.
FIGURE 51.2
The Deﬁne Factors window of General
Linear Model Repeated Measures.
The main General Linear Model Repeated Measures dialog window (see
Figure 51.4) opens with three question-marked lines in the Within-Subjects Variables
panel. Our name for the treatment effect, drug_education, appears just above the
panel. There are three question marks because we deﬁned the Within-Subject Factor
as having three levels in the previous Deﬁne Factors window. Note that the question
marks are numbered to accommodate a time-based set of variables. We move the
variables in order into the Within-Subjects Variables panel as shown in Figure 51.5.
In the Options dialog window shown in Figure 51.6, we select Descriptive statistics
in the Display area. We do not select Homogeneity tests because it applies only to

524
ONE-WAY WITHIN-SUBJECTS ANOVA
FIGURE 51.3
The Deﬁne Factors window of
General Linear Model Repeated
Measures with the treatment effect
deﬁned.
FIGURE 51.4
The main dialog window of General Linear Model
Repeated Measures.
between-subjects independent variables; instead, the Repeated Measures procedure will
automatically perform a sphericity test (the repeated measures rough analog of Levene’s
test) so long as we have at least three levels of the within-subjects variable.
Recall in the one-way between-subjects ANOVA, there was the assumption of homo-
geneity of variances (i.e., that all levels of the independent variable produced outcomes
with equivalent variances). In the within-subjects ANOVA, the general sphericity assump-
tion requires that the variances of the differences between each level are equivalent. It
also assumes that each pair of levels (pretest and posttest1, pretest and posttest2, and
posttest1 and posttest2) is correlated to the same extent. These assumptions are tested
together as sphericity. The general method of assessing these assumptions is to evaluate
a variance/covariance structure known as compound symmetry, which speciﬁes that both

ANALYSIS SETUP
525
FIGURE 51.5
The main dialog window of General Linear Model
Repeated Measures conﬁgured with the three
levels of our treatment variable.
FIGURE 51.6
The Options window of Repeated Measures.
the variances and the covariances (correlations) between each level are comparable (see
Chapter 52 for a more complete discussion of this topic).
As was the case with the ANCOVA (see Chapter 49), we can request our multiple
comparison tests in the Estimated Marginal Means area. We therefore move drug_
education from the Factor(s) and Factor Interactions panel to the Display Means for
panel, check Compare main effects, and select Bonferroni in the Conﬁdence interval

526
ONE-WAY WITHIN-SUBJECTS ANOVA
FIGURE 51.7
Descriptive Statistics and Mauchly’s Test of Sphericity.
adjustment drop-down menu. Click Continue to return to the main dialog window and
click OK to perform the analysis.
51.4
ANALYSIS OUTPUT
The table labeled as Descriptive Statistics in Figure 51.7 shows the means, standard
deviations, and sample sizes (even though these are the same 11 cases in each condition).
The bottom table in Figure 51.7 shows the results of Mauchly’s sphericity test. Mauchly’s
test evaluates the following two assumptions simultaneously:
• The levels of the within-subjects variable have equal variances.
• Each pair of levels of the within-subjects variable is correlated to the same extent
(i.e., pretest with posttest1, pretest with posttest2, and posttest1 with posttest2).
In order for the sphericity test to be enabled, there must be at least three levels of the
independent variable (with only two levels of the within-subjects variable, there would
be only one correlation that could not be compared to anything). If the Mauchly test
returns a statistically signiﬁcant outcome, indicating that the assumption of sphericity
has not been met, then we would use one of the corrections automatically provided by
IBM SPSS, the Greenhouse–Geisser, Huynh–Feldt, or the Lower Bound. All three
corrections work by multiplying the df for the F ratio associated with the within-subjects
treatment effect by the respective epsilon value shown in the table and evaluating the
signiﬁcance of F against these adjusted degrees of freedom.
Mauchly’s W statistic is evaluated against a chi-square distribution with degrees of
freedom of one less than the number of levels of our treatment effect. In the present
case, Mauchly’s W of .643 corresponds to a chi-square value of 3.972. With 2 degrees
of freedom, this chi-square value is not statistically signiﬁcant (p = .137), and so we
judge that we have not violated the assumption of sphericity.

ANALYSIS OUTPUT
527
FIGURE 51.8 Summary tables showing the omnibus ANOVA results.
Figure 51.8 displays the ANOVA summary tables. These correspond to the major
partitions of the variance of the dependent variable. The Tests of Between-Subjects
Effects table takes account of the individual differences in overall performance across
the three testing occasions. In this analysis, IBM SPSS treats these individual differences
as Error in the table (with a Sum of Squares of 123.576) and is of little interest to us
in this context.
The Tests of Within-Subjects Effects table presents the results for our treatment
variable drug_education and its associated within-subject error term. We focus on the
rows for Sphericity Assumed because the Mauchly test returned a nonsigniﬁcant out-
come. The F ratio for the effect is 30.876 and, with 2 and 20 degrees of freedom, is
statistically signiﬁcant (p < .001). It therefore appears that there are differences in drug
attitudes across the three measurements.
In a design containing a within-subjects variable, eta square is ordinarily computed
separately for within-subjects effects and (if there are any) between-subjects effects. Our
single treatment variable is a within-subjects effect and so the reference sum of squares
is the total within-subjects sum of squares; in the example, it is 213.515 + 69.152, or
282.667. We may then say that the drug_education effect explained 213.515/282.667 or
approximately 76% of the within-subjects variance.
IBM SPSS also provides another set of results evaluating the effect of the within-
subjects variable, and these are shown in Figure 51.9. As we indicated in describing
the structure of the data ﬁle, the three levels of the within-subjects factor are structured
as separate variables (assembled under the umbrella of a within-subjects factor in the
Deﬁne Factor(s) dialog window). These are treated by IBM SPSS as separate dependent
variables in a multivariate ANOVA. Using this language, our one-way repeated measures
ANOVA would be called a univariate design, in that it conceives of the repeated factor
as a single dependent variable measured at three different times.

528
ONE-WAY WITHIN-SUBJECTS ANOVA
FIGURE 51.9
Multivariate tests of statistical signiﬁcance.
FIGURE 51.10 Polynomial contrasts for the three levels of the within-subjects variable.
We will discuss more about the general multivariate approach in Chapter 54. For
now, it is sufﬁcient to say that this multivariate analysis tests the null hypothesis that
the dependent variables have the same mean; as such, it is the same null hypothesis
evaluated in the univariate ANOVA. However, the multivariate tests do not rest on the
assumption of sphericity (because the levels of the within-subjects factor are treated as
separate dependent variables).
The four Multivariate Tests each occupy a row in the table shown in Figure 51.9.
Although they are calculated somewhat differently (see Meyers et al. (2013)), they are
translated here into an identical F value of 36.256. Based on 2 and 9 degrees of freedom,
all are statistically signiﬁcant (p < .001). The most commonly used of these multivariate
tests is Wilks’ lambda. Its value is an estimate of the unexplained variance and so
subtracting that value from 1.00 yields an estimated multivariate eta square value; here,
1.00 −.110 is .89, and we estimate that 89% of the variance is explained by the within-
subjects factor.
IBM SPSS also performs polynomial contrasts (see Chapter 48) on the levels of
the within-subjects factor, and the results of this portion of the analysis are shown in
Figure 51.10. With only three levels in our current example, we can obtain only linear
and quadratic components, and each of these is tested for statistical signiﬁcance. The
major row of interest in the table is labeled drug_education. Both the linear (p = .002)
and the quadratic (p < .001) components are signiﬁcant.
These contrasts may be understood by examining the means of the levels. As shown
in Figure 51.7, the means of the pretest, posttest1, and posttest2 are 15.27, 9.18, and
11.09, respectively. The linear trend is seen by the overall drop in positive attitudes
toward drugs between the pretest and the posttest2; the quadratic trend is seen because
drug attitudes dropped sharply from the pretest to the ﬁrst posttest (posttest1) but showed
an unfortunate (from the standpoint of the rehabilitation program) resurgence from the
ﬁrst to the second posttest.

ANALYSIS OUTPUT
529
FIGURE 51.11 Results of the Bonferroni-corrected paired comparison t tests.
The estimated marginal means together with their standard errors and 95% conﬁdence
intervals are shown in the top table in Figure 51.11. These means are identical to the
observed means.
The Bonferroni-corrected pairwise comparisons of the estimated marginal means are
shown in the bottom table in Figure 51.11. Each major row focuses on one of the three
conditions and compares it to the other two. The ﬁrst major row focuses on the pretest
(shown simply as 1 in the table, forcing us to remember our coding of the variable).
The difference between the estimated marginal mean for that condition and the estimated
marginal mean for posttest1 (shown as 2 in the table) is 6.091 (pretest −posttest1).
The difference is positive and statistically signiﬁcant (p < .001), indicating that attitudes
toward drugs were less favorable at posttest1 than at pretest. The difference between
the estimated marginal mean for the pretest (shown as 1 in the table) and the estimated
marginal mean for posttest2 (shown as 3 in the table) is 4.182 (posttest1 −posttest2)
and is also positive and statistically signiﬁcant (p = .005); thus, drug attitudes were still
less favorable at posttest2 than at pretest. Examining the other rows indicates that the
difference between posttest1 and posttest2 is statistically signiﬁcant as well (p = .024);

530
ONE-WAY WITHIN-SUBJECTS ANOVA
examining the means informs us that attitudes toward drugs became somewhat more
favorable from posttest1 to posttest2.
Given the means of the three conditions and the results of the polynomial contrasts,
we would conclude that the drug education component of the program was effective
in lowering positive attitudes toward illegal drug use by the time it was completed.
However, the offenders did regain some favorable attitude toward drug use after a month
following the end of the program, although they were still less favorable at that time
than they were at the start of the program. This resurgence of favorable attitude toward
drugs should cause the researchers to consider what could be done to revise the program
so that this effect could be reduced or eliminated.

C H A P T E R
5 2
Repeated Measures Using
Linear Mixed Models
52.1
OVERVIEW
A repeated measures ANOVA was applied in Chapter 51 to assess differences at three
time points of youth offenders’ attitude toward using drugs. The analysis was performed
within the General Linear Model that requires the data to be normally distributed, have
homogeneous variance across the levels of the within-subjects variable, and represent
independent observations. However, the assumption of independence is automatically
violated in repeated measures analyses in that the same cases are contributing multiple
data points; therefore, to the extent that there are any individual differences, scores within
any given case are going to be more related (correlated) than scores between cases.
In the Linear Mixed Models module of IBM SPSS®, the independent observations
assumption is not necessary to be made in that the correlations among the scores tied to
each individual case can be taken into account. In addition, the Linear Mixed Models
is more adaptable. For example, the General Linear Model procedure for repeated
measures requires participants to be observed at constant time points (e.g., every week),
that we have complete data on all cases, and that all cases are observed at the same time
points (e.g., if the schedule calls for measurements to be made every month at a speciﬁed
time, observations cannot occur for some participants at a 6-week interval); the Linear
Mixed Models is robust to these restrictions.
In the General Linear Model, individual differences are assumed to be ﬁxed effects
in the sense that all participants have the same intercept (initial score) and slope (changes
in scores over time). In the Linear Mixed Models, individual differences are treated as
a random effect in that cases can differ in their intercepts and slopes; such treatment of
the variable tends to provide a better model of the data.
The Linear Mixed Models approach allows for alternative variance/covariance
structures, whereas the General Linear Model procedure uses only a Compound Sym-
metry structure; alternative variance/covariance structures can lead to the Linear Mixed
Models approach being statistically more powerful than the General Linear Model
procedure.
The variance/covariance structure is contained in what is known as an R matrix.
We have generated such a matrix for the analysis described in Chapter 51 by requesting
the Residual SSCP matrix in the Options dialog window. The portion of the Residual
SSCP matrix related to our current discussion is presented in the major Covariance row
(the middle row) of the table shown in Figure 52.1. Such a matrix places the variances
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
531

532
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.1 The Residual SSCP Matrix with Covariance as the middle row.
of the measures on the diagonal. The largest variance was at pretest (9.618). Following
that there was a reduction of the variance in posttest1 (4.364), with a slight increase at
posttest2 (5.291).
The entries off the diagonal (the upper and lower elements are redundant) are
the covariances. Covariances are akin to correlations but use the original measurement
units in the computation; that is, they are not standardized to produce values between
±1.00 (Norman & Streiner, 2008). Inspecting the covariances shown in Figure 52.1
reveals that the largest covariance occurs between pretest and posttest1 (3.845), fol-
lowed by posttest1 and posttest2 (2.982) with the smallest covariance between pretest
and posttest2 (2.073). It appears that the covariances have become smaller after the
pretest; as the intervals between time points increase, the covariances appear to decrease.
The structure of the R matrix is essential to the repeated measures analysis. The
aim of the Linear Mixed Models procedure is to apply an R matrix that best ﬁts the
observed R matrix (i.e., the differences between the two matrices are minimized). In
the General Linear Model approach, the R matrix is imposed on the data, whereas the
Linear Mixed Models allows for alternative R matrices to be invoked. Obtaining an
accurate R matrix is essential in calculating valid contrasts between the levels of the
within-subjects factor.
In the General Linear Model Repeated Measures ANOVA, the observed R matrix
is compared to the Compound Symmetry R matrix; this is how the assumption of
sphericity (i.e., the variances and correlations between each level are equivalent) is tested
in the General Linear Model ANOVA procedure. Compound Symmetry mandates
that each of the three variances is equal and that all three covariances (correlations) are
likewise equal. Although there are six parameters (three variances and three covariances)
in the R matrix, the Compound Symmetry covariance structure needs to estimate only
two of them (the variance and the covariance), as it proposes that within each set of
three (variances and covariances), the values are equal, and this affords it relatively
greater statistical power. The Compound Symmetry R matrix is, however, often not

RESTRUCTURING THE DATA FILE
533
very realistic, and the assumption of sphericity in the General Linear Model ANOVA
is frequently violated because of the statistical power associated with it.
An alternative to the ANOVA procedure in the General Linear Model approach
is the multivariate technique that is automatically reported. This approach utilizes the
Unstructured R matrix where there are no assumptions concerning the variances and
the covariances of the elements. However, the Unstructured matrix is extremely complex
because the variances and covariances need to be estimated at every time point (in this
example, the three variances and the three covariances for a total of six parameters as
opposed to only two parameters in the Compound Symmetry matrix). The inclusion
of these additional parameters attenuates statistical power (Norman & Streiner, 2008).
The Unstructured matrix may be applied if researchers are unable to determine the R
matrix.
An alternative to both the Compound Symmetry and the Unstructured matrices—a
viable and reasonable middle ground between the two—is to assume that the variances
are either homogeneous (i.e., equivalent) or heterogeneous (i.e., different) but to consider
that the correlation of each successive measurement with respect to the ﬁrst time point
would attenuate with each successive time point. This attenuation over time (that the
covariances—correlations—between adjacent pairs decrease across the time points) is
assessed by the ﬁrst-order autoregressive covariance structure. We use here the variant
of the autoregressive covariance structure that assumes homogeneity of variances. It has
less statistical power than the Compound Symmetry covariance structure but is perhaps
the approach most descriptive of the majority of such data structures, including the one
shown in Figure 52.1.
52.2
NUMERICAL EXAMPLE
We use the data contained in the ﬁle named drug rehab that we used in Chapter 51.
A total of 11 youth offenders were assessed on their attitude toward using drugs before
the start of the program (pretest), after completing the 2-week program (posttest1), and
1 month following program completion (posttest2). Scores on the attitude measure could
range between 5 and 20, with higher scores representing more favorable attitudes toward
drug use.
52.3
ANALYSIS STRATEGY
The Linear Mixed Models procedure requires a different data structure in order to
analyze data in a repeated measures design. We therefore ﬁrst convert the data ﬁle
from Chapter 51 into the necessary structure using the Restructure Data Wizard. We
then perform the Linear Mixed Models analysis using the ﬁrst-order autoregressive
covariance structure. Finally, we repeat the Linear Mixed Models analysis two additional
times, ﬁrst using the Compound Symmetry covariance structure and then using the
Unstructured covariance structure.
52.4
RESTRUCTURING THE DATA FILE
We open the data ﬁle drug rehab and from the main menu, we select Data ➔Restruc-
ture. This opens the Restructure Data Wizard window as shown in Figure 52.2. Our
data ﬁle is currently in the multivariate or wide format, where repeated measures on
the same dependent variable (e.g., attitudes toward drugs) are in separate columns on a
single row for each case. The Linear Mixed Models procedure requires the data to be

534
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.2 The initial Restructure Data Wizard window.
in the univariate or long format, where only one repeated measure on any one dependent
variable is allowed in a single row. Thus, in our restructuring process, each case will
occupy three rows (ﬁrst for pretest, second for posttest1, and third for posttest2). The
ﬁrst choice of Restructure selected variables into cases speciﬁes this format and we
select it. Click Next.
Step 2 of the Restructure Data Wizard is to specify the Number of Variable
Groups we have in our data set (this is the number of within-subjects variables in our
study). This is shown in Figure 52.3. We select One (for example, w1, w2, and w3)
and click Next.
Figure 52.4 presents the initial Select Variables screen, with default information
ﬁlled in that we need to replace. In the Case Group Identiﬁcation area, we select from
the drop-down menu Use selected variable and move id into the panel. This variable
identiﬁes each case and will appear in three successive rows in the restructured data ﬁle
(because there are three measures and one measure is placed on each row). This is shown
in Figure 52.5.
In the area of the Select Variables screen labeled as Variables to be Transposed, we
replace the default trans1 in the Target Variable panel with some descriptive name for
our measure. As we have assessed attitudes toward drugs, we have created the variable

RESTRUCTURING THE DATA FILE
535
FIGURE 52.3 Here we specify the Number of Variable Groups (within-subjects variables) we
have in our data set.

536
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.4 The Select Variables screen.

RESTRUCTURING THE DATA FILE
537
FIGURE 52.5 The Select Variables screen with the variables now speciﬁed.

538
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.6 The Create Index Variables screen.
name attitude and typed it in the panel. We have then moved the three measures, pretest,
posttest1, and posttest2, to Target Variable panel below the name. This represents the
measurements made on our dependent variable and is also shown in Figure 52.5. Click
Next.
The Create Index Variables screen is shown in Figure 52.6. An index variable
tracks the level of the repeated measure. With our three levels, each case will have a
score named attitude for pretest, posttest1, and posttest2. The index variable will code
these levels as 1, 2, and 3 for the ﬁrst case, 1, 2, and 3 for the second case, and so on.
We have only one such variable and so we select One. Click Next.
We can now name our index variable in the Create One Index Variable screen (see
Figure 52.7). We choose Sequential numbers under What kind of index values? We
then replace the default Index1 shown in Figure 52.7 with a name we create for this
purpose. As shown in Figure 52.8, we have named our index variable time. Click Next.
The Options window is conﬁgured by default in the way we wish (see Figure 52.9)
and so we simply click Next.
The Finish screen is shown in Figure 52.10. We wish to Restructure the data now.
Clicking Finish opens a conﬁrmation screen (see Figure 52.11) to which we click OK.

RESTRUCTURING THE DATA FILE
539
FIGURE 52.7 The Create One Index Variable screen.

540
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.8 The Create One Index Variable screen with the index variable named.

RESTRUCTURING THE DATA FILE
541
FIGURE 52.9 The Options screen.

542
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.10 The Finish screen.
FIGURE 52.11
Accept this information and click OK.
A portion of the restructured data ﬁle is shown in Figure 52.12. This univariate or
long format can be read as follows. The ﬁrst case (id 1) is represented in the ﬁrst three
rows. The index variable of time represents the set of variables originally named pretest,
posttest1, and posttest2. Under attitude, we see the values corresponding to pretest,
posttest1, and posttest2. Each case is structured similarly. We save this ﬁle under the
name drug rehab long format to preserve our original data ﬁle that is in the multivariate
format.

ANALYSIS SETUP: AUTOREGRESSIVE COVARIANCE STRUCTURE
543
FIGURE 52.12 A portion of the restructured data ﬁle.
52.5
ANALYSIS SETUP: AUTOREGRESSIVE COVARIANCE
STRUCTURE
We open the data ﬁle drug rehab long format and from the main menu, we select
Analyze ➔Mixed Models ➔Linear. This opens the initial Specify Subjects and
Repeated window shown in Figure 52.13. We move id into the Subjects panel (this
variable represents the different cases) and time (this is our index variable that tracks the
levels of the within-subjects factor) into the Repeated panel. From the drop-down menu
for Repeated Covariance Type, we select AR(1); this is the autoregressive structure
that assumes homogeneous variances. Click Continue to reach the main Linear Mixed
Models dialog window.
The main Linear Mixed Models window is shown in Figure 52.14. We move
attitude into the Dependent Variable panel and time into the Factor(s) panel.
Selecting the Fixed pushbutton opens the Fixed Effects dialog window shown in
Figure 52.15. Highlight time and select the Add button that becomes active once it is
selected; this action places time into the Model panel. Click Continue to return to the
main Linear Mixed Models dialog window.
Selecting the Random pushbutton opens the Random Effects dialog window shown
in Figure 52.16. In the Subject Groupings area at the bottom of the screen, double-
click id from the Subjects panel into the Combinations panel. We do not check the
box associated with Include intercept (including the intercept yields a Warning about

544
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.13
The initial Specify Subjects and Repeated window of
Linear Mixed Models.
FIGURE 52.14
The main dialog window of Linear Mixed Models.
iteration being terminated and a footnote in the Estimates of Covariance Parameters
table indicates that its covariance parameter is redundant and cannot be computed). Click
Continue to return to the main Linear Mixed Models dialog window.
Selecting the Statistics pushbutton opens the Statistics dialog window shown in
Figure 52.17. Under Model Statistics, check the boxes associated with Parameter esti-
mates, Tests for covariance parameters, and Covariances of residuals. Click Continue
to return to the main Linear Mixed Models dialog window.
Select the EM Means pushbutton to obtain the estimated marginal means. In the
dialog window shown in Figure 52.18, we move time from the Factors and Factor
Interactions panel to the Display Means for panel. To obtain the pairwise comparisons
of the means of the three time measurements, we check Compare main effects and select
Bonferroni from the drop-down menu, retaining None (all pairwise) under Reference

ANALYSIS SETUP: AUTOREGRESSIVE COVARIANCE STRUCTURE
545
FIGURE 52.15 The Fixed Effects window of Linear Mixed Models.
FIGURE 52.16 The Random Effects window of Linear Mixed Models.

546
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.17
The Statistics window of Linear Mixed Models.
FIGURE 52.18 The EM Means window of Linear Mixed Models.

ANALYSIS OUTPUT: AUTOREGRESSIVE COVARIANCE STRUCTURE
547
Category. Click Continue to return to the main Linear Mixed Models dialog window
and click OK to perform the analysis.
52.6
ANALYSIS OUTPUT: AUTOREGRESSIVE COVARIANCE
STRUCTURE
The Model Dimension table shown in Figure 52.19 provides a review of the number
of parameters to be determined by the analysis. The First-Order Autoregressive is dis-
played as the Covariance Structure and is associated with 2 parameters to be estimated.
These two parameters are as follows: (a) a single variance estimate for all three levels
as we are assuming homogeneity of variances and (b) one covariance estimate that is
exponentially smaller over the time measurements.
The Information Criteria table shown in Figure 52.20 presents how well the model
ﬁts the data. We will compare the Autoregressive model to the Compound Symmetry
and Unstructured models. To assess which model best ﬁts the data, Norusis (2008,
2012) recommends the use of the Akaike information criterion (AIC) and the Bayesian
information criterion (BIC) with lower scores as being indicative of a more accurate
model. Both indexes penalize the estimation of more parameters and thus increase, all
else equal, with additional parameters in the model (Hox, 2010).
The Type III Tests of Fixed Effects table is presented in Figure 52.21. Our focus
is on the time row where the omnibus or overall effect of the within-subjects factor is
evaluated. The F ratio of 40.110 assessed with 2 and 18.673 degrees of freedom indicates
that there are signiﬁcant differences (p < .001) in the dependent variable means across
the three time measurements.
FIGURE 52.19 The Model Dimension table.
FIGURE 52.20
The Information Criteria table.

548
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.21
Omnibus assessment of the effects of the
within-subjects factor.
FIGURE 52.22 The Estimates of Fixed Effects table.
The Estimates of Fixed Effects table is shown in Figure 52.22. These estimates
focus on the differences among the three test time points. Referring back to the results
of the repeated measures ANOVA discussed in Chapter 51, we may recall that the means
(shown in Figure 51.1) of the pretest, posttest1, and posttest2 conditions were 15.273,
9.182, and 11.091, respectively.
The Intercept in the Estimates of Fixed Effects table is the mean score at posttest2
(taken to six decimal places), with the remaining rows presenting deviations between
posttest2 and each level of the within-subjects factor. The row labeled as [time = 1]
displays the deviation score between the posttest2 and the pretest; by adding 4.18 to
11.09, we obtain the pretest mean score of 15.27. Similarly, for the posttest1 labeled as
[time = 2], the deviation score is 11.09 −1.90, or 9.19. The row labeled as [time = 3]
displays the deviation score between posttest2 and the posttest2, which is 0.
These mean differences are tested for statistical signiﬁcance, and the results indicate
that each difference suggests a signiﬁcant difference. However, these comparisons nei-
ther represent the full set of pairwise comparisons nor have they been subjected to the
Bonferroni correction.
In the Estimates of Covariance Parameters table in Figure 52.23, the AR1 diago-
nal row deals with the estimated variances. This refers to the diagonal of the R matrix.
There is only one such estimate because this procedure assumes homogeneity of variance
(i.e., all three variances are equal). The estimated variance is 6.722444.
FIGURE 52.23 The Estimates of Covariance Parameters table.

ANALYSIS: COMPOUND SYMMETRY
549
FIGURE 52.24
The Residual Covariance (R) Matrix.
The second parameter that was estimated is the correlation between successive mea-
surements, and this is shown in the row labeled as AR1 rho. Although there are three
pairs of correlations (pretest and posttest1, posttest1 and posttest2, and pretest and
posttest2), the way in which the procedure is associated with only a single parameter is
to estimate a single value for the correlation between adjacent pairs (pairs that are “next
to” each other in the design; here this is the correlation between pretest and posttest1
and between posttest1 and posttest2). This correlation was estimated to be .615842 and
was determined by the Wald statistic to be statistically signiﬁcant (p < .001).
The correlation between the pair of time measurements that are two steps removed
from each other (pretest and posttest2) is estimated by squaring the parameter (because
the covariance is assumed to become exponentially smaller as intervals increase). Thus,
the correlation between pretest and posttest2 is estimated as approximately .6152 or
approximately .378. These results indicate that the time points are statistically and prac-
tically signiﬁcantly correlated with score change.
The Residual Covariance (R) Matrix is presented in Figure 52.24. These are the
parameters that have been estimated under the First-Order Autoregressive procedure.
As we have just described, the estimated variance of 6.722444 is placed on each diag-
onal element. The covariances are simply unstandardized variations of the correlations
described earlier.
The Estimated Marginal Means and the Pairwise Comparisons are presented in
Figure 52.25. The pairwise comparisons results lead to the same conclusion that we
drew in the repeated measures ANOVA in Chapter 51, but there are a couple of subtle
differences in the probability levels. Speciﬁcally, the comparison between pretest and
posttest1 is still signiﬁcant at .000, the comparison between pretest and posttest2 yielded
a probability of .005 in the ANOVA but here the probability is .000, and the comparison
between posttest1 and posttest2 yielded a probability of .024 in the ANOVA but here
the probability is .036.
52.7
ANALYSIS: COMPOUND SYMMETRY
We repeat the analysis exactly as described earlier with the following exception: in the
initial Specify Subjects and Repeated window, we select Compound Symmetry from
the drop-down menu for Repeated Covariance Type. This is shown in Figure 52.26.
The Model Dimension table in Figure 52.27 indicates that only two parameters
are being estimated: the variance (assumed to be equivalent) and the correlations (also
assumed to be equivalent) between all pairs of time measures. The bottom table in
Figure 52.27 shows the Information Criteria associated with this analysis. Comparison
to Figure 52.20 reveals that the AIC and BIC indexes are higher for the Compound Sym-
metry structure than for the Autoregressive structure, suggesting that the Autoregressive
structure is a better ﬁt for the data.

550
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.25 The Estimated Marginal Means and the Pairwise Comparisons.
The Estimates of Fixed Effects table is presented in Figure 52.28. The F ratio of
30.876 duplicates the result obtained from the repeated measures ANOVA described in
Chapter 51 (see Figure 51.8) as both are based on Compound Symmetry.
Compound Symmetry estimates just two parameters as shown in the Residual
Covariance (R) Matrix in the bottom table of Figure 52.28. The estimated variance of
6.424242 in this analysis is placed on each diagonal element, and the estimated covariance
for each pair of time measures, irrespective of the number of steps removed from each
other, is 2.966667.
Figure 52.29 presents the Pairwise Comparisons based on Compound Symmetry.
In this analysis, unlike the results for either the repeated measures ANOVA or the Autore-
gressive structure, the difference between posttest1 and posttest2 was not statistically
signiﬁcant (p = .077).

ANALYSIS: COMPOUND SYMMETRY
551
FIGURE 52.26
The initial Specify Subjects and Repeated window of
Linear Mixed Models, with Compound Symmetry
selected as the Repeated Covariance Type.
FIGURE 52.27 The Model Dimension and Information Criteria tables based on Compound Symmetry.

552
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.28 The Estimates of Fixed Effects and Residual Covariance (R) Matrix tables
based on Compound Symmetry.
FIGURE 52.29 The Pairwise Comparisons based on Compound Symmetry.
52.8
ANALYSIS: UNSTRUCTURED COVARIANCE
We repeat the analysis one more time but here we select Unstructured from the drop-
down menu for Repeated Covariance Type in the initial Specify Subjects and Repeated
window This is shown in Figure 52.30.
The Model Dimension table shown in Figure 52.31 indicates that six parameters are
being estimated. These are the three variances and the three correlations between all pairs
of time measures. The bottom table in Figure 52.31 shows the Information Criteria
associated with this analysis. Comparison to Figure 52.20 and Figure 52.27 reveals that

ANALYSIS: UNSTRUCTURED COVARIANCE
553
FIGURE 52.30
The initial Specify Subjects and Repeated window of the
Linear Mixed Models, with Unstructured selected as the
Repeated Covariance Type.
FIGURE 52.31 The Model Dimension and Information Criteria tables based on Unstructured Covariance.

554
REPEATED MEASURES USING LINEAR MIXED MODELS
FIGURE 52.32 The Estimates of Fixed Effects, the Residual Covariance (R) Matrix, and the
Pairwise Comparisons tables based on Unstructured Covariance.
the AIC and BIC indexes are higher for the Unstructured covariance than for the other
two, reinforcing our earlier supposition that the Autoregressive structure is a best ﬁt of
the three approaches for the data.
The Estimates of Fixed Effects table, the Residual Covariance (R) Matrix, and the
Pairwise Comparisons based on Unstructured Covariance are shown in Figure 52.32.
The F ratio is somewhat higher from what we have seen earlier, every parameter takes
on a different value in the Residual Covariance (R) Matrix, and the probability levels
for the Pairwise Comparisons are the same as those obtained in the repeated measures
ANOVA.

C H A P T E R
5 3
Two-Way Mixed ANOVA
53.1
OVERVIEW
A mixed ANOVA design contains at least one between-subjects variable and at least
one within-subjects variable. A two-way mixed ANOVA must, by deﬁnition, contain one
independent variable of each type and is often called a simple mixed design because
it is the minimal (or simplest) mixed ANOVA design that is possible; an overview of
complex mixed designs is presented in Gamst et al. (2008). From an ANOVA standpoint,
a two-way mixed ANOVA is a repeated measures design with the addition of a between-
subjects factor. We thus partition the variance in the same overall way as we did in
the one-way within-subjects design: into between-subjects and within-subjects variance.
However, both major partitions can be further partitioned in the following way in a
two-way mixed design:
• The between-subjects variance is further partitioned into variance attributed to the
between-subjects independent variable and error variance. The effect associated
with the independent variable is evaluated for statistical signiﬁcance against this
between-subjects error (the F ratio is computed by dividing the mean square asso-
ciated with the independent variable by the mean square associated with the error
term).
• The within-subjects variance is further partitioned into the overall effect of the
within-subjects independent variable, the interaction of the between-subjects and
within-subjects variables, and the within-subjects error. As was true for the one-
way within-subjects design, the error represents the differences in patterns of
performance of the cases across the levels of the within-subjects independent
variable. The F ratios for both the within-subjects independent variable effect and
the interaction are computed by dividing the Mean Square associated with each
of these by the Mean Square associated with the within-subjects error.
The two-way mixed design allows us to track the performance of two or more
independent groups over time on a dependent measure or to compare the performance
of two or more independent groups on different measures. By plotting the performance
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
555

556
TWO-WAY MIXED ANOVA
of the groups as separate lines across the set of measures represented on the X-axis,
we can immediately discern any differences in the pattern of performance among the
groups. Examining these pattern differences has often been referred to as a form of
proﬁle analysis (Fitzmaurice, Laird, & Ware, 2011; Kim & Neil, 2007).
53.2
NUMERICAL EXAMPLE
The ﬁctional data we use for our example track the effectiveness of an advertising
campaign for a new widget. There are two levels of the between-subjects variable we
have named ad_campaign. The 20 sample markets serving as cases for this study were
randomly assigned to one of the two conditions: 10 were assigned to a media blitz
advertisement campaign (coded as 1) and 10 were assigned to a targeted marketing
advertisement campaign (coded as 2). The dependent variable is thousands of sales dollars
and was measured before the start of the campaigns (pretest) and 1 (post1), 2 (post2),
and 3 (post3) weeks after the start of the advertisement campaign. The data ﬁle is named
advertising campaign.
A screenshot of a portion of the data ﬁle is shown in Figure 53.1. Note that the values
of ad_campaign in the data ﬁle show the codes 1 and 2 for the two types of marketing
strategies because it is a between-subjects variable. The levels of the within-subjects
variable measure the sales dollars at four points in time, and each level is a variable in
the data ﬁle. These levels will be linked together in the Deﬁne Factors dialog window
to form the within-subjects variable with four levels.
FIGURE 53.1 A portion of the data ﬁle.

ANALYSIS SETUP
557
53.3
ANALYSIS SETUP
We open the data ﬁle advertising campaign and from the main menu, we select Analyze
➔General Linear Model ➔Repeated Measures. This opens the Deﬁne Factor(s)
window of Repeated Measures as shown in Figure 53.2. The Within-Subject Factor
Name panel opens with the default of factor1; we change that to sales as the name we
create for our within-subjects independent variable. We also type 4 in the Number of
Levels panel. Click Add to register this information with IBM SPSS® and click Deﬁne
to open the main dialog window.
The main General Linear Model Repeated Measures dialog window opens with
four question-marked lines in the Within-Subjects Variables panel. Our name for the
treatment effect, sales, appears just above the panel. As described in Chapter 51, we
move the variables pretest, post1, post2, and post3 in order into the Within-Subjects
Variables panel as shown in Figure 53.3. We also move ad_campaign into the Between-
Subjects Factor(s) panel.
Anticipating the possibility of a statistically signiﬁcant interaction effect, we will
generate a plot. Selecting the Plots pushbutton opens the Proﬁle Plots dialog window
(see Figure 53.4). Because the within-subjects variable of sales represents a time-related
factor, it almost always facilitates interpretation to place such a variable on the horizontal
axis; this also simpliﬁes the ﬁgure in that there will be only two functions (one for each
level of ad_campaign). We therefore place sales on the Horizontal axis and will have
Separate Lines for each level of ad_campaign. Click the Add pushbutton to place the
plot in the Plots panel and click Continue to return to the main dialog window.
In the Options dialog window shown already conﬁgured in Figure 53.5, we select
Descriptive statistics and Homogeneity tests (because we have a between-subjects
variable in our design) in the Display area; the Repeated Measures procedure will
automatically perform a sphericity test for the levels of the sales variable.
As was the case with the one-way within-subjects design (see Chapter 51), we can
request our multiple comparison tests for the main effect of the within-subjects variable
in the Estimated Marginal Means area should we need it. We therefore move sales
from the Factor(s) and Factor Interactions panel to the Display Means for panel,
FIGURE 53.2
The Deﬁne Factors window of General Linear
Model Repeated Measures.

558
TWO-WAY MIXED ANOVA
FIGURE 53.3
The main dialog window of General Linear Model
Repeated Measures.
FIGURE 53.4
The Proﬁle Plots dialog window of Univariate to set up the plot
of the interaction.
check Compare main effects, and select Bonferroni from the Conﬁdence interval
adjustment drop-down menu.
Remaining in the Options window, we can also set up for the simple effects tests
of the interaction should that prove needed by moving the interaction term shown as
ad_campaign*sales from the Factor(s) and Factor Interactions panel into the Display
Means for panel.
Click Continue to return to the main dialog window, and click Paste to
copy the underlying syntax to a syntax window (see Figure 53.6). Note that the
/EMMEANS = TABLES(sales) is already conﬁgured for the Bonferroni-corrected t
tests of the main effect of sales (it does not mention the variable sales in parentheses
after Compare as this is the only variable in the TABLES(sales) portion of the
syntax and IBM SPSS recognizes this fact). This syntax resulted from us having
checked Compare main effects and selected Bonferroni from the Conﬁdence interval

ANALYSIS SETUP
559
FIGURE 53.5
The Options window of Repeated Measures.
FIGURE 53.6 The Pasted syntax in a syntax window.

560
TWO-WAY MIXED ANOVA
FIGURE 53.7 The syntax with the full set of simple effects tests conﬁgured.
adjustment drop-down menu. We now need to conﬁgure the simple effects tests for the
interaction effect.
We focus on the subcommand /EMMEANS = TABLES(ad_campaign*sales). As
described in Chapter 50, we duplicate this subcommand line, and add COMPARE (ad_-
campaign) ADJ (BONFERRONI) to the ﬁrst /EMMEANS = TABLES line (to com-
pare the two different types of advertisement campaigns for each of the four time periods)
and COMPARE (sales) ADJ (BONFERRONI) to the second /EMMEANS = TABLES
line (to compare the four different time periods once for the blitz campaign and again for
the targeted campaign). The completed syntax is shown in Figure 53.7. From the main
menu select Run ➔All to obtain the output.
53.4
ANALYSIS OUTPUT
The Descriptive Statistics with the means, standard deviations, and sample sizes are
shown in Figure 53.8. Figure 53.9 shows the results of Box’s test, Mauchly’s spheric-
ity test, and Levene’s test. Box’s Test of Equality of Covariance Matrices tests the
assumption that the dependent variable covariance matrices are equal across the levels of
the independent variable (ad_campaign); in this data set, the assumption has been met
(Box’s M = 11.245, p = .582). The data also appear to meet the sphericity assumption
(Mauchly’s W = 11.245, p = .464).
Levene’s test of homogeneity of variance returned statistically nonsigniﬁcant results
for pretest, post1, and post2, but showed a statistically signiﬁcant result for post3
(p = .041). However, even with just four evaluations, there is a risk of alpha inﬂation
here; a more stringent Bonferroni-corrected alpha level (.05/number of statistical tests)
would give us a corrected alpha level of .05/4, or .0125, and this would render the

ANALYSIS OUTPUT
561
FIGURE 53.8 The Descriptive Statistics output.
FIGURE 53.9 Tests of homogeneity and sphericity.

562
TWO-WAY MIXED ANOVA
FIGURE 53.10 Within-subjects and between-subjects summary tables.
Levene test for post3 nonsigniﬁcant. Thus, we should probably be cautious concerning
any within-subjects effect whose signiﬁcance levels hover around the .05 mark.
Figure 53.10 displays the ANOVA summary tables; the Repeated Measures proce-
dure of IBM SPSS reports only the reduced model, thus simplifying the output. All three
effects are statistically signiﬁcant (p < .001). Because interaction effects supercede main
effects, we focus on that effect. The total sum of squares for the within-subjects partition
of the variance is 5566.050 + 2343.250 + 839.200, or 8748.500. Thus, the eta square
value for the interaction is 2343.250/8748.500, or .268, indicating that the interaction
effect explained about 27% of the within-subjects variance.
The plot of the interaction effect is presented in Figure 53.11, and a visual inspection
of the difference in the pattern or group proﬁles tells much of the general story. Both
advertisement campaigns appeared to work early but the media blitz (solid line) was
losing ground to the targeted campaign by the second week (sales3 on the X-axis) and had
pretty much run its course while the targeted campaign continued to generate increased
sales.
We can also see the main effect of sales in the plot, as both functions rise from the
pretest (sales1) to the ﬁnal measure at sales4, but it is clear that the rise shows a different
rate or pattern for each group and reinforces the idea that the interaction contains more
detailed information in it than the main effect.

ANALYSIS OUTPUT
563
FIGURE 53.11
Plot of the interaction effect.
FIGURE 53.12 Simple effects tests comparing the two levels of ad_campaign for each time period.

564
TWO-WAY MIXED ANOVA
FIGURE 53.13 Simple effects tests comparing the four levels of sales for each level of
ad_campaign.

ANALYSIS OUTPUT
565
The details of what we were able to see in the plot of the means are revealed in the
simple effects tests. The ﬁrst set of simple effects tests (the ﬁrst /EMMEANS = TABLES
subcommand in the syntax) called for the two levels of ad_campaign to be compared for
each time period, and these results are shown in Figure 53.12. As can be seen from the
Pairwise Comparisons table (see Chapter 49 for how to read the Pairwise Comparisons
table), the two sets of widget markets were comparable in sales during the pretest (sales1
in the table) period (p = .847); thus, the random assignment was effective in generating
two comparable groups. At the end of the ﬁrst week (sales2 in the table), the groups
were still comparable in widget sales (p = .518). However, at the ﬁnal two measurement
times, the targeted marketing campaign resulted in signiﬁcantly more widget sales than
the media blitz campaign.
The other half of the story suggested by the plot of the interaction is conveyed in the
second set of simple effects analyses shown in Figure 53.13 where the four time periods
are compared for each type of advertisement campaign. For the media blitz campaign,
sales increased from the pretest (sales1 in the table) to post2 (sales3 in the table) but
leveled off at a point where post2 and post3 (sales3 and sales4, respectively, in the
table) do not differ signiﬁcantly. However, for the targeted marketing campaign, sales
of widgets signiﬁcantly increased at every measurement period. When all is said and
done, we may conclude that the targeted marketing campaign was the better of the two
in that its effect extended over a longer time period than did the one associated with the
media blitz campaign.


P A R T 16
MULTIVARIATE GROUP
DIFFERENCES: MANOVA
AND DISCRIMINANT
FUNCTION ANALYSIS


C H A P T E R
5 4
One-Way Between-Subjects
MANOVA
54.1
OVERVIEW
ANOVA is deﬁned as a univariate procedure in that there is only one dependent variable
in the analysis. But in many research contexts, the cases in the study are affected in more
than one manner by the independent variables, and when this is a possibility, it is useful
to assess the cases on multiple dependent variables. When we compare the cases in each
independent variable level on their performance on two or more dependent variables,
the design that we use is a multivariate ANOVA (MANOVA). The key feature of a
MANOVA is that, at least in the initial portion of the analysis, the dependent variables
are treated together as a set rather than analyzed in isolation from each other. We consider
in this chapter the one-way between-subjects design, where we have a single independent
variable with k levels and two or more dependent variables.
The strategy we use in a MANOVA is intimately tied to discriminant function
analysis, a topic that is covered in Chapter 55. The ﬁrst step in the strategy is typi-
cally to determine if there are multivariate differences between the groups by examining
the omnibus MANOVA results. If there are, then researchers ordinarily engage in two
additional analyses in either order depending on the research focus. One analysis is
to determine the latent dimensions along which the groups differ; such a multivariate
approach is performed with discriminant function analysis. Another analysis is to per-
form univariate ANOVAs on each dependent variable, examining the effects as justiﬁed
from the MANOVA results.
The statistical objective in a MANOVA is to form a weighted linear composite (a
variate) of the dependent variables analogous to what takes place in multiple regression.
In MANOVA, however, the weights are generated such that the groups are maximally
differentiated. The value that results from solving the discriminant function (the weighted
linear composite of the dependent variables) is a discriminant score (i.e., a discriminant
score is computed for each case), and the weights assigned to each dependent variable
achieves the goal of maximally separating (differentiating) the groups based on their
average discriminant score. The groups are then compared using an ANOVA but with
the discriminant score (representing the variate or “super variable”) as the dependent
variable in the analysis (Meyers et al., 2013).
To most effectively use MANOVA, it is best that the dependent variables are mod-
estly to moderately correlated. Variables very highly correlated are redundant, and it is
pointless (and perhaps a bit distorting) to claim they are separate, stand-alone measures.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
569

570
ONE-WAY BETWEEN-SUBJECTS MANOVA
Variables that are barely correlated can best be directly analyzed in separate univariate
ANOVAs, as joining them together into a variate makes little theoretical sense given that
they are not related to each other. Generally, it is preferable to have correlations between
the dependent variables in the approximate range .30–.60 (Stevens, 2009; Tabachnick &
Fidell, 2007).
There are four multivariate tests of the effect of the independent variable (Pillai’s
Trace, Wilks’ lambda, Hotelling’s Trace, and Roy’s Largest Root) produced by IBM
SPSS®. Each is converted to a multivariate F ratio and tested for statistical signiﬁcance.
If the focus is on comparing the means of the groups (as it is in this chapter)
rather than on identifying the latent dimensions underlying the group differences and
the multivariate tests indicate that the groups differ signiﬁcantly on their discriminant
scores, we then examine the results of the univariate ANOVAs where each dependent
variable is analyzed separately; these univariate ANOVAs are conveniently produced by
IBM SPSS as part of the default output. Researchers will often apply an alpha level
inﬂation correction when examining the ANOVAs, such as the Bonferroni correction
where .05 is divided by the number of dependent variables, to establish a revised alpha
level. Each statistically signiﬁcant effect of the independent variable (using the corrected
alpha level) is then subjected to a multiple comparison test (if there are more than two
levels) to determine which groups differ signiﬁcantly from which others.
54.2
NUMERICAL EXAMPLE
The ﬁctional study we use for our example examines some personality characteristics
of employees in companies that are experiencing one of three types of business cycles.
The between-subjects independent variable is named business_cycle, and its levels are
bankruptcy (coded as 1 in the data ﬁle), steady state (coded as 2 in the data ﬁle),
and rapid expansion (coded as 3 in the data ﬁle). The dependent variables are the Big-
5 personality dimensions of neuroticism (neoneuro), extraversion (neoextra), openness
(neoopen), agreeableness (neoagree), and conscientiousness (neoconsc) and represent
scores of employees sampled from each type of business_cycle. The data ﬁle is named
business climate.
54.3
CORRELATION ANALYSIS
We will perform a correlation analysis on the dependent variables as a preliminary to
our MANOVA. We open the data ﬁle business climate and from the main menu, we
select Analyze ➔Correlate ➔Bivariate. This opens the main Bivariate Correlation
window (the general dialog window can be seen in Chapter 22). We move neoneuro,
neoextra, neoopen, neoagree, and neoconsc into the Variables panel, and click OK to
perform the analysis.
The Correlations output is shown in Figure 54.1. Correlations ranged between abso-
lute values of approximately .01 and approximately .38 and would be considered quite
modest in the present context. We can proceed directly to the MANOVA.
54.4
ANALYSIS SETUP: MANOVA
We select Analyze ➔General Linear Model ➔Multivariate. This opens the main
Multivariate window shown in Figure 54.2. We move business_cycle into the Fixed
Factor(s) panel and neoneuro, neoextra, neoopen, neoagree, and neoconsc into the
Dependent Variables panel.

ANALYSIS SETUP: MANOVA
571
FIGURE 54.1
Correlations of the dependent variables.
FIGURE 54.2
The main dialog window of General Linear Model
Multivariate.
Selecting the Options pushbutton opens the Options dialog window shown in
Figure 54.3. In the Display area, check Descriptive statistics, Residual SSCP Matrix
(to obtain Bartlett’s test of sphericity evaluating whether or not there is statistically sig-
niﬁcant correlation between the dependent variables), and Homogeneity tests (to obtain
Box’s M and the Levene tests). Click Continue to return to the main dialog window.
We will anticipate the possibility of a statistically signiﬁcant main effect and thus
select the Post Hoc pushbutton. In the Post Hoc window (see Figure 54.4), we move

572
ONE-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 54.3
The Options dialog window of General Linear Model
Multivariate.
FIGURE 54.4
The Post Hoc dialog window of General Linear Model
Multivariate.
business_cycle into the panel labeled Post Hoc Tests for. It is possible that we may have
unequal variances, and so we select R-E-G-W-Q under Equal Variances Assumed and
Tamhane’s T2 under Equal Variances Not Assumed, planning to use whichever is the
more appropriate with respect to each dependent variable. Click Continue to return to
the main dialog window and click OK to perform the analysis.

ANALYSIS OUTPUT: MANOVA
573
54.5
ANALYSIS OUTPUT: MANOVA
The Descriptive Statistics with the mean, standard deviation, and sample sizes are shown
in Figure 54.5. Figure 54.6 shows the results of Box’s test and Bartlett’s test. Box’s Test
of Equality of Covariance Matrices tests the assumption that the dependent variable
covariance matrices are equal across the levels of the independent variable (business_-
cycle); in this data set, the assumption has apparently not been met (Box’s M = 166.018,
p < .001). However, Box’s M statistic has a good deal of statistical power when the
sample size is large (Norman & Streiner, 2008), and so we will still evaluate our results
but exercise caution; because Box’s M was statistically signiﬁcant, we use Pillai’s Trace
rather than Wilks’ lambda. Bartlett’s Test of Sphericity determines if the correlations
between the dependent variables are sufﬁciently strong to support the MANOVA. A sta-
tistically signiﬁcant outcome indicates that the correlations are sufﬁcient and that is what
was obtained (approximate chi-square = 275.796, p < .001).
Figure 54.7 displays the results of the four multivariate tests of statistical signiﬁ-
cance. Wilks’ lambda and Pillai’s Trace are the two most commonly used criteria, and
with the statistical signiﬁcance of Box’s M, we should be focused on and should report
the outcome based on Pillai’s Trace, but the effect for business_cycle is statistically
signiﬁcant (p < .001) based on all four indexes. Wilks’ lambda represents the amount
FIGURE 54.5 The Descriptive Statistics output.

574
ONE-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 54.6
Results of Box’s and Bartlett’s tests.
FIGURE 54.7 Multivariate tests of statistical signiﬁcance.
of multivariate variance not explained by the effect and subtracting that value from
1.00 gives a sense of the strength of the business_cycle effect (a multivariate analog of
eta square). In this analysis, 1.00 −.373 = .627, and so it appears that business_cycle
explained approximately 63% of the variance of the discriminant score variable. With
the multivariate effect of business_cycle statistically signiﬁcant, we can examine the
univariate results.

ANALYSIS OUTPUT: MANOVA
575
FIGURE 54.8
Levene’s test results for each dependent variable.
Levene’s test of homogeneity of variance, presented in Figure 54.8, returned statisti-
cally signiﬁcant results for neoopen and neoconsc, borderline signiﬁcance for neoagree,
and nonsigniﬁcant results for neoneuro and neoextra; we should therefore increase
the stringency of our revised alpha level beyond the Bonferroni level when evaluating
neoopen and neoconsc, and even possibly neoagree.
The summary table for the univariate ANOVAs can been seen in Figure 54.9. Because
we are in the General Linear Model module, we obtain the full model solution (see
Chapter 47). Our interest is in the reduced model and so we focus on the rows for
business_cycle, Error, and Corrected Total. Each dependent variable is given its own
row within each of these portions of the summary table; even though they are displayed
together, these represent the results of ﬁve separate univariate ANOVAs.
We are well advised to impose the Bonferroni correction on our nominal .05 alpha
level. Dividing .05 by 5 (the number of dependent variables) brings our corrected alpha
level to .01. For variables that violated the homogeneity assumption, we should make
the alpha level even more stringent (perhaps .005), but even then the effects of all ﬁve
dependent variables are statistically signiﬁcant (p < .001).
Eta square values are computed with reference to the Corrected Total. For neoneuro,
we thus divide its sum of squares (49350.149) by its Corrected Total (82665.373). The
result is .597. We note that this is also shown in the footnote at the bottom of the table,
where the eta square values (called R Squared in the footnote) for all of the dependent
variables may be found.
Levene’s test indicated that neoneuro and neoextra yielded homogeneous variances.
We can therefore examine the results of the R-E-G-W-Q post hoc tests for these variables
(see Figure 54.10). These tests revealed that all three groups differed from each other
on each of these measures. Thus, the rapid expansion group was most neurotic and
extraverted, the steady state group was less neurotic and extraverted, and the rapid
expansion group was least neurotic and extraverted.
Levene’s test further indicated that neoopen and neoconsc demonstrated hetero-
geneity of variances and that neoagree was suggestive of heterogeneity of variances.
We can therefore examine the results of Tamhane’s T2 post hoc tests for these
variables (see Figure 54.11). These tests revealed that all three groups differed
signiﬁcantly on neoagree. Thus, the rapid expansion group was most agreeable, the
steady state group was less agreeable, and the rapid expansion group was least
agreeable.

576
ONE-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 54.9 Summary table for the ﬁve ANOVAs.

FIGURE 54.10 The results of the R-E-G-W-Q post hoc tests.
577

578
ONE-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 54.11 The results of Tamhane’s T2 post hoc tests.
For neoopen, the bankruptcy and steady state groups did not differ but the rapid
expansion group was signiﬁcantly more open than either of the other two groups. Thus,
the rapid expansion group was more open than either the steady state or the rapid
expansion group.
For neoconsc, the steady state and rapid expansion groups did not differ but
the bankruptcy group was signiﬁcantly less conscientious than either of the other two
groups.

C H A P T E R
5 5
Discriminant Function Analysis
55.1
OVERVIEW
Discriminant function analysis is an alternative (and not incompatible) way to concep-
tualize a one-way between-subjects MANOVA. As we discussed in Chapter 54, the
quantitative dependent variables in the MANOVA analysis are synthesized together to
form a variate or weighted linear composite, the value of which is a discriminant score
when solved for each case. These variates are discriminant functions. In discriminant
function analysis, our interest is less on group differences per se and more on interpreting
the discriminant functions that characterize the group differences. Interpreting a discrim-
inant function is a matter of identifying the latent construct or dimension underlying it.
The orientation in discriminant function analysis toward interpreting latent dimen-
sions of variates shifts the nominal roles of the variables in the statistical analysis (but
not in the data collection procedure). The quantitative variables that were the dependent
variables in the MANOVA are treated as the predictors or independent variables in dis-
criminant function analysis (akin to multiple regression analysis); group membership is
the independent variable in the MANOVA but it is the dependent variable in discrimi-
nant function analysis. Thus, in discriminant function analysis, we use the quantitative
variables to predict group membership. Regardless of such role differences, however, the
same dynamics underlie both MANOVA and discriminant function analysis if we think
in terms of quantitative variables and a grouping variable.
The focus on the latent dimensions representing the quantitative variables emerges
in two aspects of the discriminant function data analysis: description or explanation and
prediction or classiﬁcation. The differences between groups can be relatively complex,
falling along several dimensions (e.g., university professors, corporate executives, and
politicians differ in a host of ways). In discriminant function analysis, the number of
dimensions (discriminant functions) along which we can describe the group differences
(and which together comprise the discriminant function model) is the smaller of the
following two quantities:
• the number of quantitative predictor variables in the analysis;
• the number of groups −1 (the degrees of freedom of the group variable)—this
quantity is almost always smaller than the number of quantitative variables and
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
579

580
DISCRIMINANT FUNCTION ANALYSIS
so almost always determines the number of discriminant functions that can be
extracted from the data set.
Each discriminant function represents one latent dimension along which the groups
differ. The variables in each function have their predictive weights (unstandardized and
standardized discriminant coefﬁcients analogous to those in multiple regression), but they
also have associated structure coefﬁcients. These structure coefﬁcients are the same as
we have seen in multiple regression and factor analysis; they represent the correlation
between the variable and weighted linear composite and are used to interpret the latent
dimension represented by the variate. If there are three groups in the analysis, for example,
then there are two discriminant functions; if both are statistically signiﬁcant, then the
groups may be said to differ along both the latent dimensions. That said, because the
discriminant functions are extracted sequentially and are orthogonal to each other, as we
have seen in principal components analysis (see Chapter 38), the ﬁrst function typically
accounts for the bulk of the explained variance and should therefore be given appropriate
emphasis in the interpretation.
Another major aspect of discriminant function analysis deals with prediction or
classiﬁcation. In addition to the discriminant weights and structure coefﬁcients produced
by the analysis, IBM SPSS® also produces a set of classiﬁcation coefﬁcients or weights.
These classiﬁcation weights are applied to the quantitative variables to predict group
membership; that is, they are used to classify cases based on their quantitative variable
scores into the group to which they most likely belong. Each group is associated with
a unique set of classiﬁcation weights. When these weights are applied to a given case,
the case is classiﬁed into the group for which the total classiﬁcation score is highest.
Prediction will ordinarily not be perfect, and the percentage of correct predictions can
be used to gauge the effectiveness of the model. Although no test is provided in the
IBM SPSS output of whether cases are classiﬁed signiﬁcantly better than chance, such
as Press’ Q statistic (Press, 1972), readers can consult other sources (e.g., Meyers et al.,
2013) to perform such tests on their own.
IBM SPSS provides two different procedures to perform the classiﬁcation analysis.
The default analysis uses all cases to derive the discriminant model and then applies the
model to each case. This procedure maximizes the rate of successful classiﬁcation of the
cases in the data set but would not be as successful if applied to a new set of cases.
IBM SPSS also provides us with a jackknife (Quenouille, 1956; Tukey, 1958) clas-
siﬁcation procedure called the Leave-one-out method (see Meyers et al. (2013)). In this
method, one case is omitted from the analysis and the discriminant function model is
generated and applied to classify that case. The case is then reabsorbed into the sample,
another case is selected to be left out of the analysis, and so on until all cases have been
treated in this way. This attempt at cross-validation provides a more realistic picture
of classiﬁcation effectiveness and has much greater external validity than the default
method.
55.2
NUMERICAL EXAMPLE
We use the same business climate data ﬁle used in Chapter 54 and will also use busi-
ness_cycle as our group variable with its levels of bankruptcy (coded as 1 in the data
ﬁle), steady state (coded as 2 in the data ﬁle), and rapid expansion (coded as 3 in
the data ﬁle). For this analysis, we use the following quantitative variables as predictors
of group membership: beckdep (depression), regard (self-regard), selfcon (self-control),
neoneuro (neuroticism), neoextra (extraversion), neoopen (openness), neoagree (agree-
ableness), neoconsc (conscientiousness), posafect (positive affect), negafect (negative
affect), sanx (state anxiety), tanx (trait anxiety), and acceptnc (self-acceptance).

ANALYSIS SETUP
581
55.3
ANALYSIS SETUP
We open the data ﬁle business climate and from the main menu, we select Analyze ➔
Classify ➔Discriminant. This opens the main Discriminant Analysis window shown in
Figure 55.1. We move the quantitative variables of beckdep, regard, selfcon, neoneuro,
neoextra, neoopen, neoagree, neoconsc, posafect, negafect, sanx, tanx, and acceptnc
into the Independents panel,
We next move business_cycle into the Grouping Variable panel. Upon doing so,
we see two question marks in parentheses next to the variable name; this is because
IBM SPSS must be informed of the group codes we have used. To accomplish this,
we select the Deﬁne Range pushbutton to reach the Deﬁne Range dialog window (see
Figure 55.2), type 1 in the Minimum panel and 3 in the Maximum panel, and select
Continue to return to the main dialog window where our codes have been speciﬁed in
the parentheses next to business_cycle as shown in Figure 55.3.
Selecting the Statistics pushbutton opens the Statistics dialog window shown in
Figure 55.4. In the Descriptives area, check Means and Univariate ANOVAs (to obtain
tests of statistical signiﬁcance for each predictor). Under Function Coefﬁcients, check
both Fisher’s (to obtain the classiﬁcation coefﬁcients) and Unstandardized (to obtain
the discriminant coefﬁcients for each predictor). Selecting Continue returns us to the
main dialog window.
Selecting the Classify pushbutton opens the Classiﬁcation dialog window shown in
Figure 55.5. In the Prior Probabilities area, select All groups equal. This requires the
prediction of group membership to be based exclusively on the discriminant model (the
alternative option Compute from group sizes allows the software to take advantage of
differences in group sizes to “improve” prediction). Retain the default of Within-groups
in the Use Covariance Matrix area.
In the Display area, we select Summary table and Leave-one-out classiﬁcation
to obtain the standard classiﬁcation results as well as the cross-validation results. In
FIGURE 55.1
The main Discriminant Analysis window.
FIGURE 55.2
The Deﬁne Range dialog window.

582
DISCRIMINANT FUNCTION ANALYSIS
FIGURE 55.3
The main Discriminant Analysis window with the
ranges deﬁned.
FIGURE 55.4
The Statistics dialog window of Discriminant Analysis.
FIGURE 55.5
The Classiﬁcation dialog window of Discriminant
Analysis.

ANALYSIS OUTPUT
583
the Plots ﬁeld, we select Combined-groups to obtain a pictorial representation of the
multivariate group differences. Click Continue to return to the main dialog window and
click OK to perform the analysis.
55.4
ANALYSIS OUTPUT
The Descriptive Statistics with the mean, standard deviation, and sample sizes is shown
in Figure 55.6. As all groups are equally weighted (we speciﬁed All groups equal in the
Prior Probabilities area), the Unweighted and the Weighted Valid Ns are equal.
Figure 55.7 presents the results of the univariate ANOVAs for each quantitative
predictor. Group differences are tested using Wilks’ lambda statistic. As may be seen in
the table, the three groups (that is why df1 is given as 2) were signiﬁcantly different on
all of the predictors (p < .001).
The results of the overall (omnibus) multivariate analysis are shown in Figure 55.8.
The bottom table provides the Wilks’ lambda output. Had we performed MANOVA
using these variables, the Wilks’ lambda test of multivariate statistical signiﬁcance would
have corresponded to the ﬁrst row of the table labeled 1 through 2.
The Wilks’ lambda table is a dimension reduction analysis. The ﬁrst row evalu-
ates the entire discriminant model, which in our example consists of two discriminant
functions (because there are three groups). Functions 1 through 2 generated a Wilks’
lambda of .145. Evaluated against a chi-square distribution with 26 degrees of freedom
(13 predictors in each of functions), functions 1 through 2 as a set are statistically sig-
niﬁcant (p < .001). Subtracting the value of Wilks’ lambda (the amount of unexplained
multivariate variance) from 1.00 yields .855. Thus, the discriminant model with its two
functions accounted for almost 86% of the multivariate group difference variance.
The dimension reduction analysis continues to completion. Having examined all of
the functions as a set, the ﬁrst function is removed and the remaining functions as a
set are evaluated. Here, the remaining set contains only discriminant function 2. It is
statistically signiﬁcant as well.
To achieve a sense of the effectiveness of each function in differentiating the groups,
we examine the Eigenvalues table. As we have seen in factor analysis, eigenvalues are
interpretable in terms of explained variance and, although the numerical calculations are
a bit different here, the meaning is essentially the same. The values in the Eigenvalues
table are taken with respect to the total amount of explained variance (not the total
variance). Of the explained variance, the ﬁrst function accounts for the vast majority
(97%), with an Eigenvalue of 4.968. The canonical correlation associated with the ﬁrst
function is .912; squaring this value yields approximately .83. We can thus say the ﬁrst
discriminant function accounts for about 83% of the variance in the discriminant score
differences of the groups.
The second function is much less potent than the ﬁrst. Its Eigenvalue is .152, and
it explains the remaining 3% of the explained variance. With a canonical correlation of
.364, it appears that it accounts for about 13% of the variance in the discriminant score
differences of the groups.
Figure 55.9 presents the standardized and unstandardized discriminant function coef-
ﬁcients. These weights are akin to those in multiple regression; the raw score coefﬁcients
are associated with a constant to be included in the equation.
The Structure Matrix is presented in Figure 55.10 and is analogous to what we
have seen in principal components and factor analysis (already sorted by magnitude per
function) in Chapter 38. We interpret these structure coefﬁcients as though they were
factors, although it is not unusual that the magnitudes of the values are often lower in

584
DISCRIMINANT FUNCTION ANALYSIS
FIGURE 55.6 Descriptive statistics.

ANALYSIS OUTPUT
585
FIGURE 55.7 Results of the univariate ANOVAs for each quantitative predictor.
FIGURE 55.8
Eigenvalues and Wilks’ lambda results.
discriminant function analysis. The latent dimensions represented here describe the way
the groups differ. The ﬁrst function appears to represent higher levels of self-regard and
lower levels of neuroticism and trait anxiety; this could be interpreted as representing
emotional stability and positive feelings toward oneself. The second function appears to
represent higher levels of self-acceptance and depression; this could be interpreted as
representing an acceptance of feelings of melancholy.
Figure 55.11 presents the centroids for each function in table (Functions at
Group Centroids) and graphic (Canonical Discriminant Functions) forms. The
centroids—think of them as multivariate means—are in units of the Mahalanobis
distance and are standardized akin to z scores (see Chapter 19).

586
DISCRIMINANT FUNCTION ANALYSIS
FIGURE 55.9 Standardized and unstandardized discriminant function coefﬁcients.
FIGURE 55.10
The structure coefﬁcients.

ANALYSIS OUTPUT
587
FIGURE 55.11
Group centroids in the table and plot forms.
We determined from the eigenvalues that the ﬁrst function was overwhelmingly more
potent than the second in terms of explaining differences between the groups, and the
potency of the ﬁrst function can be seen in both the tabled and the plotted displays. In the
table labeled Functions at Group Centroids, the groups are very much differentiated
in the ﬁrst function, spanning a wide range −3.378 to 2.348, whereas their centroids are
much closer (less differentiated) in the second function where they span the range −.507
to .394. This differentiation can also be seen in the Combined-groups plot. The groups
are substantially spaced along the horizontal axis representing Function 1 but are not
that far apart vertically (representing Function 2).
The Classiﬁcation Function Coefﬁcients are shown in Figure 55.12. These are
used to predict the group membership of each case (we use them to classify the cases
into predicted groups). The scores of the quantitative variables for each case would be
entered into a classiﬁcation function using the weights in this table. Thus, in our example
,we would compute three classiﬁcation functions for each case. These are unstandard-
ized weights and so the appropriate constant would be included in the solution for each
function. The case would be classiﬁed into (predicted to be a member of) the group
whose value of the function is highest. Because the true group membership of the
case is also known, it is possible to track the success rate of classiﬁcation across the
sample.
The results of the success rate of the classiﬁcation procedure are presented in
Figure 55.13 in the Classiﬁcation Results table. Rows represent actual group mem-
bership, and the columns represent the group membership predictions based on the dis-
criminant model. Within each major portion of the table (Original and Cross-validated),

588
DISCRIMINANT FUNCTION ANALYSIS
FIGURE 55.12
The classiﬁcation coefﬁcients.
FIGURE 55.13 The classiﬁcation table for Original and Leave-one-out (Cross-validated)
methods.

ANALYSIS OUTPUT
589
correct classiﬁcations occupy the upper left to lower right diagonal. The Original clas-
siﬁcations are based on all of the cases, whereas the Cross-validated classiﬁcations are
based on the Leave-one-out method. With our huge sample size and the substantial
differences between the groups, both methods achieved remarkable overall classiﬁcation
success as shown in the footnotes to the table (95.1% for the Original method and 94.4%
for the Cross-validated method).


C H A P T E R
5 6
Two-Way Between-Subjects
MANOVA
56.1
OVERVIEW
A two-way between-subjects MANOVA is an extension of the one-way MANOVA design
discussed in Chapter 54, and everything we discussed in that chapter applies here as
well. The difference, and it is an important one, is that we have the addition of a second
independent variable, and by including it, we can evaluate not only the main effects of
the independent variables but also their interaction.
The two-way between-subjects MANOVA design is analogous to the univariate two-
way design but is considered from a multivariate standpoint. The dependent variables are
still combined into a weighted linear composite but there are three multivariate effects of
interest (the two main effects and the interaction). If our focus is on group differences,
and it almost always is if we structure the design as a two-way, then after we assess
the multivariate interaction and the main effects, we end up interpreting the univariate
effects.
For each multivariate effect, there are v univariate effects, where v is the number
of dependent variables. We saw this in Chapter 54 where we evaluated each of the
ﬁve univariate effects after obtaining a statistically signiﬁcant multivariate effect. The
same general strategy works for the two-way design, but we must also remember that
interaction effects supercede main effects. Given this context, we process the results of
the analysis using the strategy that is presented as a ﬂowchart in Figure 56.1.
As shown in the top portion of Figure 56.1, we ﬁrst examine the multivariate inter-
action effect. If the multivariate interaction is not statistically signiﬁcant, we go on to
the multivariate main effects of both independent variables for all dependent variables.
If the multivariate interaction effect is statistically signiﬁcant, then we examine the uni-
variate interaction for each dependent variable using the Bonferroni-corrected alpha level
(or a more stringent one if homogeneity of variance has been violated). For any depen-
dent variables whose univariate interaction effect has reached statistical signiﬁcance, we
graph the estimated marginal means, perform simple effects analyses, and interpret the
results. For any dependent variables whose interaction effect has not reached statistical
signiﬁcance, we examine (for those dependent variables only) each of the main effects.
For those dependent variables not involved in a statistically signiﬁcant interaction,
we start with the multivariate main effect of one of the independent variables and
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
591

592
TWO-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 56.1 Flowchart for processing multivariate and univariate interactions and main effects.
Source: Modiﬁed from Meyers et al. (2013).
then examine the other multivariate main effect; this is shown in the bottom portion
of Figure 56.1. If the multivariate main effect is not statistically signiﬁcant, we go on to
the other multivariate main effect, repeating the process we describe here (because the
main effects are independent of each other). If the multivariate main effect is statistically
signiﬁcant, then we examine the univariate main effects for each dependent variable using
the Bonferroni-corrected alpha level (or a more stringent one if homogeneity of variance
has been violated). We evaluate only those dependent variables (if any) that were not
involved in a statistically signiﬁcant univariate interaction. If there are more than two
levels of the independent variable, we perform multiple comparison tests and interpret
the results; if there are only two levels, we interpret the mean difference directly.

ANALYSIS SETUP
593
56.2
NUMERICAL EXAMPLE
We use the same business climate data ﬁle used in Chapter 54 and will also use busi-
ness_cycle as one of our independent variables; this factor has the following three levels:
bankruptcy (coded as 1 in the data ﬁle), steady state (coded as 2 in the data ﬁle), and
rapid expansion (coded as 3 in the data ﬁle). The other independent variable we use
here is organization_structure with its two levels hierarchical (coded as 1 in the data
ﬁle) and egalitarian (coded as 2 in the data ﬁle). The three dependent variables we
use in this example are extraversion (neoextra), openness (neoopen), and the level of
self-regard the employees feel for themselves (regard).
56.3
ANALYSIS SETUP
We open the business climate data ﬁle and from the main menu select Analyze ➔Gen-
eral Linear Model ➔Multivariate. This opens the main Multivariate dialog window
as shown in Figure 56.2. We move business_cycle and organization_structure into the
Fixed Factor(s) panel and move neoextra, neoopen, and regard into the Dependent
Variables panel.
Selecting the Options pushbutton opens the Options dialog window shown in
Figure 56.3. In the Display area, check Descriptive statistics, Residual SSCP Matrix
(to obtain Bartlett’s test of sphericity evaluating whether or not there is sufﬁcient corre-
lation between the dependent variables), and Homogeneity tests (to obtain Box’s M and
the Levene tests). On the possibility that we will obtain a statistically signiﬁcant interac-
tion effect, in the Estimated Marginal Means area of the window, we move business_
cycle*organization_structure from the Factor(s) and Factor interactions panel to the
Display Means for panel.
We will anticipate the possibility of a statistically signiﬁcant main effect and thus
select the Post Hoc pushbutton. In the Post Hoc window (see Figure 56.4), we move busi-
ness_cycle (because it has more than two levels) into the panel labeled Post Hoc Tests
for and select R-E-G-W-Q under Equal Variances Assumed (the tests for violation of
homogeneity of variance are not available in the two-way design).
Continuing to anticipate the possibility of a statistically signiﬁcant interaction effect,
we will generate a plot of the interaction for each dependent variable. Selecting the
FIGURE 56.2
The main dialog window of the Multivariate module of
General Linear Model.

594
TWO-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 56.3
The Options dialog window of the Multivariate module
of General Linear Model.
FIGURE 56.4
The Post Hoc dialog window of the Multivariate
module of General Linear Model.
Plots pushbutton opens the Proﬁle Plots dialog window. With both independent vari-
ables as categorical, it is arbitrary as to which goes on the X-axis. We therefore place
business_cycle on the Horizontal axis and will have Separate Lines for each level of
organization_structure; this setup is shown in Figure 56.5. Click the Add pushbutton to
place the plot in the Plots panel and click Continue to return to the main dialog window.
Given that we wish to anticipate the possibility of a statistically signiﬁcant interaction
effect, we continue to conﬁgure the simple effects analysis. We select Paste to obtain the

ANALYSIS SETUP
595
FIGURE 56.5
The Proﬁle Plots dialog window of the Multivariate module of
General Linear Model.
FIGURE 56.6 Syntax underpinning the analysis as conﬁgured.
window containing the syntax that represents our analysis setup shown in Figure 56.6.
We conﬁgure our simple effects in a manner analogous to that described in Chapter 50;
this is shown in Figure 56.7. From the main menu select Run ➔All to perform the
analysis.

596
TWO-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 56.7 Simple effects comparisons added to the syntax.
56.4
ANALYSIS OUTPUT
The Descriptive Statistics and sample sizes are shown in Figure 56.8. It is worth noting
that our very large sample size has provided us with more statistical power than is usual
for analyses of group differences. Thus, to be statistically signiﬁcant, an effect need not
be especially potent.
Figure 56.9 shows the results of Box’s test and Bartlett’s test. The results from the
Box’s Test of Equality of Covariance Matrices suggests that the assumption of equal
dependent variable covariance matrices has apparently not been met (Box’s M = 154.479,
p < .001). However, Box’s M statistic has a good deal of statistical power when the sam-
ple size is large (Norman & Streiner, 2008), and so we will still evaluate our results
(using Pillai’s Trace) but exercise caution. The statistically signiﬁcant outcome of Bar-
lett’s Test of Sphericity indicates that there the correlations are sufﬁcient for our analysis
(approximate chi-square = 240.982, p < .001).
Figure 56.10 displays the results of the four multivariate tests of statistical signiﬁ-
cance; Pillai’s Trace (as well as the other multivariate tests) yielded all three multivariate
effects as statistically signiﬁcant (p < .001). Wilks’ lambda represents the amount of mul-
tivariate variance not explained by the effect and subtracting that value from 1.00 gives a
sense of the strength of each multivariate effect. For business_cycle, organization_struc-
ture, and business_cycle*organization_structure, the subtraction yields .614, .104, and
.078, respectively. Even with the strong effect of business_cycle, it is still worthwhile
to evaluate the interaction, as the main effect of business_cycle will still be evident at a
richer information level.

ANALYSIS OUTPUT
597
FIGURE 56.8 The group sizes and Descriptive Statistics.

598
TWO-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 56.9
Results of Box’s and Bartlett’s tests.
Levene’s test of homogeneity of variance, presented in Figure 56.11, returned sta-
tistically signiﬁcant results for all three dependent variables. Thus, we will increase the
stringency of our revised alpha level beyond the Bonferroni level when evaluating the
univariate effects.
The summary table for the univariate ANOVAs showing the full model solution
can be seen in Figure 56.12. Because we are using the General Linear Model mod-
ule, we obtain the results of the full general linear model (see Chapter 47). Our interest
is in the reduced model, and so we focus on the rows for business_cycle, organiza-
tion_structure, business_cycle*organization_structure, Error, and Corrected Total.
Each dependent variable is given its own row within each of these portions of the
summary table, representing the results of three separate univariate ANOVAs.
We choose to impose the Bonferroni correction on our nominal .05 alpha level.
Dividing .05 by 3 (the number of dependent variables) brings our corrected alpha level
to .017. Because all three dependent variables violated the homogeneity assumption, we
should make the alpha level even more stringent; we will use an alpha level of .005 in
this example.
When we ﬁrst examine the univariate interaction effects against our stringent .005
alpha level, we determine that regard is statistically signiﬁcant (p < .001) but neoextra

ANALYSIS OUTPUT
599
FIGURE 56.10 Multivariate tests of statistical signiﬁcance.
FIGURE 56.11
Levene’s test results for each dependent variable.
and neoopen were not (p = .012 and .034, respectively). Thus, we examine the plot
and the simple effects for regard and then examine the main effects for neoextra and
neoopen. Eta square values are computed with reference to the Corrected Total. For
regard, we divide its interaction sum of squares (1166.410) by its Corrected Total
(86421.142). The result is .014.
The plot of the interaction is shown in Figure 56.13. Self-regard appears to be
greater for steady state than for bankruptcy and greater still for rapid expansion than
for steady state (this is the main effect of business_cycle showing), but employees of
hierarchical organizations who seemed to have higher self-regard if their businesses were
in the bankruptcy or steady state cycle appear to lose that advantage in businesses that
are in rapid expansion (this crossover accounts for the interaction effect).
The tests of simple effects can ﬁne-tune our visual examination, and these are
contained in two separate portions of the output. Figure 56.14 presents the Pairwise

600
TWO-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 56.12 Summary table for the univariate effects.
Comparisons (see Chapters 49 and 50 for guidance on reading the table) for the three lev-
els of business_cycle for each type of organization_structure. The estimated marginal
mean differences here are equal to the observed mean differences that can be gleaned
from Figure 56.8. For example, the ﬁrst mean difference value shown in the Pairwise
Comparisons table is −13.429 and represents the difference between bankruptcy and
steady state for hierarchical structures. The regard means for these two groups are
40.12 and 53.55, respectively, and subtracting the second from the ﬁrst gives us a value
of −13.43.
We learn from these Pairwise Comparisons that there are statistically signiﬁcant dif-
ferences between all three business_cycle groups for both types of organization_struc-
ture. Translated to the plot in Figure 56.13, employees in both hierarchical (solid line)
and egalitarian (dashed–dotted line) organizations exhibited the highest level of self-
regard in the rapid expansion cycle, next highest level of self-regard in the steady state
cycle, and the lowest level of self-regard in the bankruptcy cycle.
The other half of the simple effects results is shown in Figure 56.15. Here we have
the Pairwise Comparisons for the two levels of organization_structure for each type

ANALYSIS OUTPUT
601
FIGURE 56.13
Plot of the interaction effect for regard.
FIGURE 56.14 Simple effects tests comparing the three levels of business_cycle for each type
of organization_structure.
of business_cycle. Although the Bonferroni correction is taken into account in reporting
the probability (Sig.) levels, the variances are not homogeneous and we should probably
make the alpha level even more stringent. We will use .01 for our evaluation.
We learn from these Pairwise Comparisons that there are statistically signiﬁcant
differences between hierarchical and egalitarian organizations under bankruptcy and
steady state but not under rapid expansion cycles. Translated to the plot in Figure 56.13,
employees in both hierarchical (solid line) organizations exhibited higher levels of self-
regard under bankruptcy and steady state cycles but (even though the egalitarian

602
TWO-WAY BETWEEN-SUBJECTS MANOVA
FIGURE 56.15 Simple effects tests comparing the two levels of organization_structure for each
type of business_cycle.
FIGURE 56.16
The results of the R-E-G-W-Q post
hoc tests for neoextra.
employees seem to have a higher average score) were not signiﬁcantly different in their
feelings of self-regard in businesses that were under rapid expansion.
Having ﬁnished with the interaction effect associated with regard, we can look into
the main effects for the other two dependent variables (in the summary table shown
in Figure 56.12). Against our stringent alpha level of .005, only neoextra yields a sta-
tistically signiﬁcant main effect of business_cycle. The eta square associated with this
effect is computed by dividing its sum of squares (5272.344) by its Corrected Total
(100119.778), which is .053. The results of the R-E-G-W-Q post hoc tests for neoextra
are presented in Figure 56.16. All groups appear to be signiﬁcantly different from each
other.
For the main effect of organization_structure (see Figure 56.12), only one (neoex-
tra) of the two dependent variables not subsumed in the interaction yielded a statistically
signiﬁcant effect. With only two levels of organization_structure, we can directly inter-
pret the mean difference. The Descriptive Statistics table shown in Figure 56.8 provides
us with these two means in the fourth main row labeled Total in the neoextra (top
third) of the table. On average, employees in hierarchical structures (M = 57.2991,
SD = 10.47856) exhibited higher levels of self-regard than did those in egalitarian
structures (M = 52.8115, SD = 12.63648).

P A R T 17
MULTIDIMENSIONAL
SCALING


C H A P T E R
5 7
Multidimensional Scaling:
Classical Metric
57.1
OVERVIEW
Chapters 57 and 59 cover multidimensional scaling (MDS), a statistical technique that
is used to describe the dimensional structure underlying a set of objects or stimuli.
The technique was introduced, developed, and popularized by Kruskal (1964), Shepard
(1962), and Torgerson (1952, 1958). Readable descriptions of MDS can be found in
Davison (1992), Gigu`ere (2007), Kruskal and Wish (1978), Meyers et al. (2013), and
Young and Harris (2012).
The primary focus of MDS is to assess the ways in which objects or stimuli are dis-
similar to each other. Dissimilarity can be thought of in terms of the relative distance (the
proximity) of objects to each other. Objects that are further apart are more dissimilar. In
MDS, objects are arranged in multidimensional space as a function of their dissimilarity.
This space is deﬁned by the number of dimensions it is presumed by the researchers to
contain (usually two or three).
The MDS procedure is typically performed on (very roughly) about a dozen or so
objects or stimuli that are evaluated in pairs. For each pair, the distance between the
objects is determined. In some contexts, distance can be assessed on some objective
metric (e.g., mileage between cities). In other contexts, and the one we illustrate in
this chapter, ratings are made on a summative response scale indicating the degree of
perceived dissimilarity between pairs of stimuli. For example, respondents could be asked
to rate the similarity or dissimilarity of a group of automobile brands.
In practice, MDS places more similar stimuli closer together in a multidimensional
space conﬁguration. MDS maps each object as a point in two (or three) multidimensional
space. These dimensionality data are typically arrayed in the form of a proximity or dis-
similarity matrix. Once these distances between objects are determined, IBM SPSS® uses
a special MDS algorithm called ALSCAL (Alternating Least Squares Scaling) developed
by Takane, Young, and de Leeuw (1977). This procedure locates the coordinates for each
object and depicts them in the multidimensional space such that the distance between
points resembles respondents’ original dissimilarity judgments.
We focus here on classical metric MDS (CMDS). This approach uses a single matrix
of metric dissimilarities (or a single matrix that reﬂects averages across participants) to
perform the analysis. Using this single matrix, IBM SPSS initiates the ALSCAL algorithm
to create a multidimensional space based on the number of dimensions speciﬁed by the
researchers.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
605

606
MULTIDIMENSIONAL SCALING: CLASSICAL METRIC
FIGURE 57.1 The proximity data.
57.2
NUMERICAL EXAMPLE
The present data set represents the average ratings of 15 respondents who compared
the similarity/dissimilarity of 10 automobile brands on a 1 (very similar) to 9 (very
dissimilar) summative response scale. The data can be found in the ﬁle named auto
maker proximities and can be seen in Figure 57.1.
Note that the data are structured quite differently from what we have seen in the
previous chapters. Because MDS operates on proximity data between pairs of objects,
the data must be entered in that form. Thus, we directly enter a proximity matrix into the
data ﬁle. For example, the average rating for the pair Chevrolet–Bentley was 7; thus,
respondents viewed these auto brands as relatively dissimilar on average. On the other
hand, the pair Toyota–Honda yielded an average of 2; thus, respondents viewed these
auto brands as relatively similar. Because the data are depicted by means of a symmetric
matrix, ratings above the matrix diagonal do not need to be entered, as they are mirror
images of the lower diagonal counterparts.
57.3
ANALYSIS SETUP
Open the ﬁle named auto maker proximities and from the main menu select Analyze ➔
Scale ➔Multidimensional Scaling (ALSCAL). This produces the Multidimensional
Scaling main dialog window shown in Figure 57.2.
We have moved the automobile brands to the Variables panel. In the Distances
panel, we have kept the default Data are distances with its default setting at Square
symmetric for Shape because our data are distances between objects (auto brands) and
the shape of the distances matrix is square and symmetric (see Meyers et al. (2013) for
more details on this topic).

ANALYSIS SETUP
607
FIGURE 57.2
The main Multidimensional Scaling dialog window.
FIGURE 57.3
The Model window of Multidimensional Scaling.
Selecting the Model pushbutton produces the Model dialog window shown in
Figure 57.3. This dialog screen contains four separate panels. The Level of Measure-
ment panel (upper left of screen) indicates the measurement level of the data: Ordinal,
Interval, or Ratio. For the present example, we have activated the Interval level for
our summative response measure, as we will presume that our ratings approximate this
level of measurement.
The Conditionality panel provides three options: Matrix (for data measured on the
same measurement scale), Row (for rectangular data matrices where the data cannot be
compared with data in other matrices), and Unconditional (unconditional data matrices
can be compared with each other). The default Matrix option is selected in the present
example.
The Dimensions panel allows researchers to specify the Minimum and Maximum
number of dimensions (solutions) they wish to obtain. IBM SPSS allows for one to

608
MULTIDIMENSIONAL SCALING: CLASSICAL METRIC
FIGURE 57.4
The Options window of
Multidimensional Scaling.
six dimensions to be speciﬁed. In the present example, we have requested the default
Minimum and Maximum of 2 dimensions.
Finally, the Scaling Model panel offers either a Euclidean distance (appropriate for
CMDS analyses) or an Individual differences Euclidean distance option (appropriate
for replicated MDS and weighted MDS analyses). The Euclidean distance option is
activated for the present analysis. Clicking Continue brings us back to the main dialog
window.
Selecting the Options pushbutton produces the Options dialog window shown in
Figure 57.4. In the Display panel, we have activated two options: (a) the Group plots,
which produces the CMDS perceptual map or Derived Stimulus Conﬁguration and
the Scatterplot of Linear Fit, and (b) Data matrix, which reproduces the original
data matrix. The Individual subject plots (not activated) option provides perceptual
maps for each participant during a replicated or weighted MDS analysis. The Model
and options summary (not activated) documents the various IBM SPSS Data Options,
Model Options, Output Options, and Algorithm Options requested by the researchers.
The Criteria panel has the following three options: S-stress convergence, Minimum
S-stress value, and Maximum iterations. We recommend for most situations leaving
these values in their default state as in the present example. The Treat distances less
than [a to-be-ﬁlled-in value] as missing option was also kept at its default value of
0. Clicking Continue returns us to the main window and clicking OK performs the
analysis.
57.4
ANALYSIS OUTPUT
Figure 57.5 displays the Raw (unscaled) Data for Subject 1. Note that these data are
the original dissimilarity data averaged across our 15 respondents, and so Subject 1 here
(in the title of the table) is the only proximity matrix in the data ﬁle.
Figure 57.6 provides the Iteration history for the 2 dimensional solution (in
squared distances). This table provides two measures of model ﬁt known as Young’s
S-stress and Kruskal’s Stress Index. Stress, as developed by Kruskal (1964), is the
difference between the input raw disparities and the output distances in the multidi-
mensional map and is an index of the ﬁt of solution to the data. Stress varies from a
minimum value of 0 (the dimensional structure perfectly ﬁts the data) to a maximum
value of 1 (the dimensional structure does not ﬁt the data). According to Kruskal and

ANALYSIS OUTPUT
609
FIGURE 57.5 The proximity matrix from the data ﬁle.
FIGURE 57.6
Iteration history for the 2 dimensional solution and Stress levels.
Wish (1978) and Gigu`ere (2007), Stress values less than .05 are considered excellent,
.05 to less than.10 are considered good, .10 to less than .20 are fair, and values greater
than .20 are considered a poor ﬁt. The Stress value found at the bottom of Figure 57.6
is Stress = .15922 indicating a fair model ﬁt to the original proximity data.
At the top of Figure 57.6 is a second ﬁt index known as S-stress (developed by
Takane et al. (1977)). S-stress is derived from the Stress measure and differs only in
that it is deﬁned by squared distances and disparities. IBM SPSS provides an Iteration
history that depicts for each iteration (up to a maximum of 30) the S-stress value and its
improvement over the previous iteration. From Figure 57.6, we note S-stress began with
a value of .17068 and improved slightly after 4 iterations to a value of .15434, indicating
a fair or modest model ﬁt (it is interpreted in the same way as Kruskal’s Stress Index).
One last measure of model ﬁt, the squared correlation index (labeled by IBM
SPSS as RSQ) is also provided at the bottom of Figure 57.6. RSQ indicates the amount
of input data variance accounted for by the MDS model. An acceptable ﬁt is typically
indicated by RSQ values of .60 or greater. The present RSQ value of .88048 indicates a
reasonable ﬁt to our data.
Figure 57.7 displays the Optimally scaled data (disparities) for subject 1. Recall
that subject 1 refers here to the data ﬁle consisting of only one matrix. This matrix of

610
MULTIDIMENSIONAL SCALING: CLASSICAL METRIC
FIGURE 57.7
Optimally scaled data matrix.
FIGURE 57.8
Stimulus Coordinates used for the plot for the perceptual map.
data represents the raw data that was transformed by means of a least-squares algorithm
employed by the IBM SPSS MDS procedure.
Figure 57.8 provides the Stimulus Coordinates for Dimensions 1 and 2 for each
automobile manufacturer. These coordinates are used by the IBM SPSS MDS procedure
to locate or position each object (auto brand) in a two-dimensional space conﬁguration.
The Derived Stimulus Conﬁguration or perceptual map of the auto manufactur-
ers/models can be seen in Figure 57.9. Objects that are closer to each other are perceived
as more similar. For example, we can see that Maserati and Lamborghini vehicles and
likewise Toyota and Honda vehicles are both plotted relatively close together, indicating
a substantial amount of perceived similarity between Maserati and Lamborghini and also
between Toyota and Honda. Conversely, there is considerable distance between Maserati
and Mercedes, indicating a considerable perceived dissimilarity between these two makes
of cars.
Researchers are tasked with the challenge of imposing meaning to the dimensional
conﬁguration by naming the separate dimensions. Dimension 1, the horizontal dimension,
separates Toyota, Honda, Ford, and Chevrolet (on the left side of the map) from a
grouping of Maserati, Lamborghini, Bentley, Ferrari, Porsche, and Mercedes. Dimension
1 appears to represent a dimension of value or the level of exoticness, with relatively
inexpensive (or less exotic) vehicles at the negative (left) end of Dimension 1 and
expensive (or more exotic) autos at the positive (right) end.
The interpretation of Dimension 2, the vertical dimension, is less clear. One possi-
bility is that it may represent a performance dimension, with expensive high performance
auto brands at the positive end of the dimension and other manufacturers clustered around
the low or negative end of the dimension. Another possibility is that this dimension
may represent the perceived level of excitement involved in driving such vehicles. For

ANALYSIS OUTPUT
611
FIGURE 57.9
The perceptual map showing the dimensions
representing the solution.
FIGURE 57.10
The Scatterplot of Linear Fit.
example, Maserati and Lamborghini brands may be seen as more exciting vehicles or that
driving them may result in a more exciting experience, whereas Mercedes and Porches
are much less exciting (with our apologies to Porsche owners).
Figure 57.10 presents the Scatterplot of Linear Fit. As indicated in its title, it is
based on Euclidean distance (as we speciﬁed in the Model dialog window). This ﬁgure
is sometimes referred to as the Shepard diagram. It has on the horizontal axis the original
raw distance data (labeled as Disparities) and on the vertical axis the Euclidean distances
between pairs of objects. Ideally, we would like to see a strong linear relationship (from
lower left to upper right of the ﬁgure). The present scatterplot indicates some degree of
error for this model (and is to be expected, given our Stress level of .15922 indicated
that our model represented only a fair ﬁt to the original proximity data).


C H A P T E R
5 8
Multidimensional Scaling:
Metric Weighted
58.1
OVERVIEW
This chapter provides a brief overview of weighted multidimensional scaling (WMDS),
also known as individual differences scaling (INDSCAL). WMDS extends MDS to
multiple matrices of dissimilarity data (i.e., one for each respondent) under the assump-
tion that the multidimensional space (perceptual map) is different for each matrix (or
respondent). WMDS allows for differences (typically cognitive or perceptual) among the
respondents (exhibited in their dissimilarity matrices) through the IBM SPSS® ALSCAL
algorithm (see Carroll & Chang (1970) and Takane et al. (1977)), which builds the usual
(unweighted) stimulus space conﬁguration as well as a weighted stimulus space.
The unweighted space is shared or is common across individuals or matrices (Young
& Harris, 2012). The weighted or personal stimulus space contains information that
is unique to the individual. Weights vary from 0.0 to 1.0 for each individual on each
dimension of the stimulus; larger weights indicate greater importance of the dimension,
and smaller weights indicate less importance to the individual (Meyers et al., 2013; Young
& Harris, 2012).
58.2
NUMERICAL EXAMPLE
The present data set represents 12 students from one of our universities who were asked
to make dissimilarity judgments on 10 pairs of the following automobile manufacturers:
Chevrolet, Ferrari, Ford, Honda, Lamborghini, Maserati, Mercedes, Porsche, Rolls Royce,
and Toyota. Judgments were made on a summative response scale with anchors of 1 (very
similar) to 9 (very dissimilar). Respondents were given separate randomized lists of 45
(n (n −1)/2 or 10 (10 −1)/2 = 90/2 = 45) paired automobile manufacturers on which
to make dissimilarity judgments.
The data can be found in the ﬁle named auto maker proximities 12 raters and can
be partially seen in Figure 58.1. This data ﬁle includes 12 individual symmetric matrices
embedded in one ﬁle (only the ﬁrst of which is fully displayed in Figure 58.1). Data must
be submitted to IBM SPSS in matrix form to run a WMDS. Only the bottom portion of
each matrix and the diagonal elements are entered.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
613

614
MULTIDIMENSIONAL SCALING: METRIC WEIGHTED
FIGURE 58.1 A portion of the data ﬁle.
Sub_ID, the ﬁrst variable in the ﬁle, provides a subject number identiﬁer for each
line of each dissimilarity matrix. Thus, Subject 1 has a 1 at the beginning of the 10
lines of data that comprise his or her symmetric matrix. This Sub_ID variable serves no
functional purpose because IBM SPSS already “knows” the beginning and end points of
each of the 12 symmetric matrices (Gigu`ere, 2007), but it does serve a useful purpose
in terms of data entry veriﬁcation.
The remaining 10 variables in the columns of the ﬁle represent the names of the
auto manufacturers. The data represent paired comparisons. For example, for Subject 1,
the ﬁrst comparison is left blank because it represents a comparison between Chevrolet
and itself. The next comparison is between Chevrolet and Ferrari and received a 9
(indicating very dissimilar). The next ratings were between Chevrolet and Ford and
Ferrari and Ford and were rated 1 and 9, respectively. This indicates an appraisal of
very similar in the ﬁrst comparison and very dissimilar in the second comparison.
58.3
ANALYSIS SETUP
Open the ﬁle named auto maker proximities 12 raters and from the main menu select
Analyze ➔Scale ➔Multidimensional Scaling (ALSCAL). This produces the Multi-
dimensional Scaling main dialog window seen in Figure 58.2.
We have moved the automobile brands to the Variables panel. In the Distances
panel, we have kept the default Data are distances with its default setting at Square
symmetric for Shape because our data are distances between objects (auto brands) and
the shape of the distances matrix is square and symmetric.

ANALYSIS SETUP
615
FIGURE 58.2
The main Multidimensional Scaling dialog window.
FIGURE 58.3
The Model window of Multidimensional Scaling.
Selecting the Model pushbutton produces the Model dialog window shown in
Figure 58.3. This dialog screen contains four separate panels. The Level of Measure-
ment panel (upper left of screen) indicates the measurement level of the data: Ordinal,
Interval, or Ratio. For the present example, we have activated the Interval level for
our summative response measure based on the presumption that our ratings approximate
this level of measurement.
The Conditionality panel provides three options: Matrix (for data measured on the
same measurement scale), Row (for rectangular data matrices where the data cannot be
compared with data in other matrices), and Unconditional (unconditional data matrices
can be compared with each other). The default Matrix option is selected in the present
example.
The Dimensions panel allows researchers to specify the Minimum and Maximum
number of dimensions (solutions) they wish to obtain. IBM SPSS allows for one to
six dimensions to be speciﬁed. In the present example, we have requested the default
Minimum and Maximum of 2 dimensions.

616
MULTIDIMENSIONAL SCALING: METRIC WEIGHTED
FIGURE 58.4
The Options window of
Multidimensional Scaling.
Finally, the Scaling Model panel offers either a Euclidean distance (appropri-
ate for CMDS analyses) or an option for Individual differences Euclidean distance
(appropriate for replicated MDS and weighted MDS analyses. The Individual differences
Euclidean distance option is activated for the present analysis. Clicking Continue brings
us back to the main dialog window.
Selecting the Options pushbutton produces the Options dialog window shown in
Figure 58.4. In the Display panel, we have activated two options: (a) the Group plots,
which produces the WMDS perceptual map or Derived Stimulus Conﬁguration and
the Scatterplot of Linear Fit, and (b) Data matrix, which reproduces the original data
matrix. The Individual subject plots (not activated) provide perceptual maps for each
participant during a replicated or weighted MDS analysis. The Model and options sum-
mary (not activated) documents the various IBM SPSS Data Options, Model Options,
Output Options, and Algorithm Options requested by the researchers.
The Criteria panel has the following three options: S-stress convergence, Minimum
S-stress value, and Maximum iterations. We recommend for most situations leaving
these values in their default state as in the present example. The Treat distances less
than [a to-be-ﬁlled-in value] as missing option was also kept at its default value of
0. Clicking Continue returns us to the main window and clicking OK performs the
analysis.
58.4
ANALYSIS OUTPUT
The output begins with the Raw (unscaled) Data for Subject 1-12 (not shown as it is
a transcription of the values in the data ﬁle). Figure 58.5 provides the Iteration history
where convergence occurred after ﬁve iterations. The analysis yielded a Young’s S-
stress value of .29889, indicating a relatively poor ﬁt. The result is followed toward the
bottom of the ﬁgure by separate Stress and RSQ values for each of the 12 respondents
or matrices (see Chapter 57 for details on these ﬁt indexes). For example, Subject 1
(represented in the output as Matrix 1) had a Stress value of .240 and an RSQ value
of .865, etc. The Stress values range from .156 to .381 and the RSQ values range from
.162 to .917. The average Kruskal’s Stress value (collapsed across all 12 matrices) was
.25572 and the average RSQ was .69504, both of which indicate a fair to poor model ﬁt.
Figure 58.6 presents the Subject Weights table and the plot of those weights. The
values in the table indicate the relative importance of each dimension to each respondent.

ANALYSIS OUTPUT
617
FIGURE 58.5
Iteration history for the 2 dimensional solution and Stress levels.
Also displayed is the Weirdness index for each respondent. The Overall importance of
each dimension across the 12 respondents is shown at the bottom of the table. In the
present example, the overall importance values are .6509 and .0441 for Dimensions 1
and 2, respectively. Thus, in terms of the dimensionality, the respondents gave consid-
erably more subjective importance to Dimension 1 than they did to Dimension 2. When
the individual Subject Weights are fairly proportional to the overall weights for each
dimension, the Weirdness index is closer to 0.00. Larger Weirdness index values suggest
that respondents are investing somewhat different importance weights to the dimensions,
and Weirdness index values greater than .50 may be considered atypical or outliers
rendering them as possible candidates for elimination from subsequent analyses. In the
present example, Subjects 3, 4, and 6 meet this consideration criterion; for illustration
purposes, we will retain them in the analysis.
The Derived Subject Weights plot depicts how each respondent’s assessments of
the auto brand pairings were weighted on the two dimensions of the Euclidean distance
model. These are simply the coordinates for the two dimensions shown in the table
above the plot. We note a small grouping of respondents (e.g., 3, 4, and 12) who weight
Dimension 1 fairly low and Dimension 2 relatively high.
The Optimally scaled data (disparities) for each respondent are not shown due to
space considerations. These disparities represent the transformed raw data based on the
ALSCAL algorithm used by IBM SPSS.
Figure 58.7 presents the Scatterplot of Linear Fit Individual differences
(weighted) Euclidean distance model. This plot shows some scatter and departure
from linearity, indicating a modest but not an ideal model ﬁt.
Figure 58.8 provides the Flattened Subject Weights and a plot of the Flattened
Subject Weights Individual differences (weighted) Euclidean distance model. IBM
SPSS calculates ﬂattened weights. These coefﬁcients transform raw subject weights into
distance coordinates. The ﬂattening algorithm reduces d dimensions to d −1 dimensions;
in the present case, this computes to 2 −1, or 1 dimension. It has been labeled as
Variable 1 on the Y-axis in Figure 58.8. From this plot, we note that respondents with
relatively low Weirdness scores and symmetrical subject weights on the two dimensions

618
MULTIDIMENSIONAL SCALING: METRIC WEIGHTED
FIGURE 58.6 Importance weights for the two dimensions.
will appear in the middle of the plot (e.g., Subjects 8, 2, 9, and 7) and subjects with
relatively high Weirdness scores and high weights on one or the other dimensions will
appear near the top or bottom of the Y-axis (e.g., Subjects 1, 6, 4, and 3).
Figure 58.9 provides the Stimulus Coordinates for Dimensions 1 and 2 for each
automobile manufacturer. These coordinates are used by the IBM SPSS MDS procedure
to locate or position each object (manufacturer) in a two-dimensional space conﬁguration.
Figure 58.10 displays the perceptual map or Derived Stimulus Conﬁguration Indi-
vidual differences (weighted) Euclidean distance model. Four obvious sets of auto
manufacturer clusters (one in each quadrant) can be discerned in this two-dimensional
conﬁguration. Dimension 1 (horizontal axis) appears to represent a “Cost or Value”
dimension; lower perceived cost/value is toward the right and higher perceived cost/value

ANALYSIS OUTPUT
619
FIGURE 58.7
Scatterplot of Linear Fit Individual differences
(weighted) Euclidean distance model.
FIGURE 58.8
The Flattened Subject Weights and a plot of the
Flattened Subject Weights Individual differences
(weighted) Euclidean distance model.

620
MULTIDIMENSIONAL SCALING: METRIC WEIGHTED
FIGURE 58.9
Stimulus Coordinates used for the plot for the perceptual map.
FIGURE 58.10
The perceptual map showing the dimensions
representing the solution.
is on the left. Dimension 2 (vertical axis) is less easy to interpret and is similar to the
solution we obtained in Chapter 57. One possibility is that it may represent the perceived
level of excitement involved in driving such vehicles or a level of performance that is
associated with those vehicles. For example, the Maserati and Lamborghini brands (and
the Toyotas and Hondas) may be seen as more exciting or higher performing vehicles
or that driving them may result in a more exciting experience, whereas the Mercedes
and Porsches (as well as the Chevrolets and Fords) are perceived to represent a lower
performance level and to be much less exciting to drive.
With the dimensions identiﬁed, we can now identify the four auto groupings. Lam-
borghini, Ferrari, and Maserati are clustered together and appear to be perceived as high
excitement or performance vehicles and are high cost/value autos. Porsche, Rolls Royce,
and Mercedes appear to be viewed as high on perceived cost/value and lower on per-
ceived excitement or performance. Toyota and Honda are viewed as low on perceived
cost/value and high on perceived excitement or performance. Lastly, Ford and Chevrolet
are seen as low on both the cost/value and excitement or performance dimensions. We
should put in a disclaimer here that this small sample of psychology students is not
necessarily representative of the general population of marketplace consumers.

P A R T 18
CLUSTER ANALYSIS


C H A P T E R
5 9
Hierarchical Cluster Analysis
59.1
OVERVIEW
As is true for MDS covered in Chapters 57 and 58, cluster analysis also uses as its base
the proximities or distances between cases in the process of organizing them. But unlike
MDS where we attempt to align the cases along two or sometimes more dimensions, in
hierarchical cluster analysis, we place the cases into groupings or clusters based on their
proximities. Although hierarchical cluster analysis can be applied to large sets of cases
as part of larger analyses (e.g., IBM SPSS® has a TwoStep Cluster procedure that uses
a hierarchical analysis as its ﬁrst stage), when used as a stand-alone analysis, it is almost
always applied to a relatively small set of cases (e.g., one or maybe two dozen).
Cases are measured on a set of quantitative variables, and the proximities of the
cases are computed with respect to those variables; if a different set of variables was
used, the distances between the cases, and thus the groupings or clusters, could be
quite different. Cases can be any entities that have characteristics that can be measured.
Examples include people belonging to a given group, such as military personnel in
a training program or female college students in a sorority; manufacturers of certain
products, such as automobiles or refrigerators; and geopolitical entities, such as cities or
countries.
A hierarchical clustering solution starts with a given number of cases and establishes
one joining or combining of entities (called linkage) at each step until all cases are
ultimately combined into a single cluster. Consider an example in which we cluster 12
large US cities. Each city is associated with several quantitative variables that have been
gleaned from public records; these variables might include such measures as median
household income, population density, and the percentage of residents with a graduate
or professional degree. At the start of the process, we have 12 cities each comprising
a cluster. Proximities (similarities) between each pair of cities, assessed in terms of the
measured variables, are computed. In the ﬁrst pass or step through the cases, the two
cities that are most similar (most proximal) on the measured variables are joined or linked
together to form a cluster. The clustering process is hierarchical in that once joined, these
cities remain joined for the duration of the analysis.
After the ﬁrst two cities have been placed into a cluster, we have 11 entities remain-
ing; that is, we have 10 stand-alone cities and one cluster composed of two cities. The
next step in the hierarchical cluster analysis evaluates the proximities (similarities) of
these 11 entities, and the two entities most similar are joined together. Thus, the two-city
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
623

624
HIERARCHICAL CLUSTER ANALYSIS
cluster may be joined with some other city or two stand-alone cities may be joined into
their own cluster. At the end of this step, there are 10 entities, and the process is repeated,
one step at a time, until all of the cities are part of a single cluster.
By virtue of the clustering algorithm, regardless of how dissimilar the entities may
be, they are ultimately going to be linked together by the end of the analysis. The job
of the researchers is to determine how many “viable” clusters there are. As there is no
statistical test of any null hypothesis in cluster analysis, researchers must be guided by
the pattern of clustering over the course of the analysis displayed in both tabular and
visual forms, showing the joining process at each step as well as their knowledge of the
entities in the analysis.
The clustering process is computationally complex, involving two separate types
of calculations, one of distance and another of linkage, with several choices of method
available for each; these are more fully described in Meyers et al. (2013). Proximities are
measures of distance, and in hierarchical cluster analysis, there are several such distance
measures available (the squared Euclidean distance, the Euclidean distance, city-block
or the Manhattan distance, the Chebyshev distance, the Minkowski distance, and power
metric distance). Distance is assessed in terms of the difference between all corresponding
measures between all of the entities (e.g., calculating the difference between each pair
on median household income, population density), although each method makes this
determination somewhat differently.
There are several methods available to link the entities together into clusters; these
include the unweighted pair-group method with arithmetic averages (also called the aver-
age linkage between-groups) method, the average linkage within-groups method, the
nearest neighbor (also called the single linkage) method, the furthest neighbor (also
called the complete linkage) method, the unweighted pair-group centroid method (often
called the centroid method), the weighted pair-group centroid method (often called the
median method), and Ward’s method.
Because we calculate differences and perform other arithmetical operations on the
variables, it is desirable for all of the variables to be set to the same metric before
data analysis, and the most convenient way to do that is to transform the values to z
scores. The Hierarchical Clustering procedure in IBM SPSS has a built-in procedure
to accomplish this standardization if we request so.
59.2
NUMERICAL EXAMPLE
For this example, we use the demographics 12 US cities data ﬁle. In addition to the City
variable containing the name of the city, the data ﬁle contains the following quantitative
variables for each city: age (median resident age), income (median household income in
thousands of dollars), house_price (mean detached home price in thousands of dollars),
rent (median cost of rental), cost (cost of living index), African_Amer (percentage of
African-Americans residing in a city), Latino_Amer (percentage of Latino-Americans
residing in a city), density (population density), higher_degree (percentage with a grad-
uate or professional degree), never_marry (percentage who have never been married),
divorced (percentage who have been divorced), foreign_born (percentage who were born
outside of the United States), same_sex (percentage of same sex households), obesity
(adult obesity rate), and offenders (number of registered sex offenders).
59.3
ANALYSIS SETUP
We open the demographics 12 US cities data ﬁle and from the main menu select Analyze
➔Classify ➔Hierarchical Cluster. This opens the main dialog window shown in

ANALYSIS SETUP
625
FIGURE 59.1
The main dialog window of Hierarchical Cluster
Analysis.
Figure 59.1. We move City into the Label Cases by panel so that we will see the city
names in our output. We then move all of the other variables into the Variable(s) panel
and retain the defaults of Cases under Cluster and of Statistics and Plots under Display.
Selecting the Method pushbutton opens the Method dialog window shown in
Figure 59.2. It is common practice to perform a hierarchical cluster analysis using
various combinations of clustering methods and distance measures (to determine which
one will be used to represent the clustering solution). One of the solutions that yielded
a relatively parsimonious and interpretable outcome is illustrated here. For the Cluster
Method, we select Ward’s method from the drop-down menu, and for our distance
Measure, we select the default of Squared Euclidean distance. In the Transform
Values area of the window, we choose Z scores from the Standardize drop-down menu
and retain the default By variable. Clicking Continue returns us to the main dialog
window.
In the Statistics window (see Figure 59.3), we select Agglomeration schedule (to
show progression of the clustering process in tabular form) and Proximity matrix to
FIGURE 59.2
The Method dialog window of Hierarchical Cluster
Analysis.

626
HIERARCHICAL CLUSTER ANALYSIS
FIGURE 59.3
The Statistics dialog window of
Hierarchical Cluster Analysis.
FIGURE 59.4
The Plots dialog window of
Hierarchical Cluster Analysis.
see the initial distances between the cities. Click Continue to return to the main dialog
window.
In the Plots window shown in Figure 59.4, we select Dendrogram and an Icicle
plot for All clusters. In the Orientation area, we select Vertical to present the icicle
plot in its most traditional form. Click Continue to return to the main dialog window
and select OK to perform the analysis.
59.4
ANALYSIS OUTPUT
The Proximity Matrix for the cities is shown in Figure 59.5. These are the squared
Euclidean distances between each pair of cities, and the two cities that are closest will
be joined together in a cluster at the ﬁrst clustering step. Larger values are indicative
of greater distances (hence the footnote indicating that this is really a dissimilarity
matrix). The pair of cities with the smallest value (the two closest cities as measured by
the variables in our analysis) is Austin and Phoenix; their squared Euclidean distance is
6.053.
Figure 59.6 shows the Agglomeration Schedule for the clusters. It tracks the clus-
tering process using shorthand that is not necessarily intuitive, but the most important

FIGURE 59.5
Proximity Matrix for the cities.
627

628
HIERARCHICAL CLUSTER ANALYSIS
FIGURE 59.6
Agglomeration Schedule for the clusters.
information in the table is contained under Coefﬁcients. This column represents the
distance at which clusters were linked (with Ward’s method, it is the within-cluster sum
of squares) at each step in the analysis.
It may be computationally complex but we interpret the Coefﬁcients relatively sim-
ply and subjectively: larger jumps in the value of the coefﬁcients across steps (Stages)
suggest that, in overly simpliﬁed terms, the joining was rather “forced” (clusters that are
relatively dissimilar are being joined) and that we might want to identify the number of
clusters in the solution we wish to accept as falling just before a relatively large jump. In
our results, we have a jump of 26.502 between Stage 9 and Stage 10 (114.548 −88.046)
and a much larger jump of 50.452 between Stage 10 and Stage 11 (165.000 −114.548).
With 12 cities in the analysis, all 12 will be joined into one cluster at Stage 12. Based on
the increments in the coefﬁcient values (and without peeking at which cities are linked
together at each stage), it would appear that we probably would be inclined to accept the
solution at the end of Stage 11 with an outside possibility of accepting the solution at
the end of Stage 10.
The classical output of a hierarchical cluster analysis is the vertical icicle plot as
presented in Figure 59.7. It derives its name from the appearance of icicles having formed
on the roof of a house, and it is read from bottom to top. A full (dark) vertical bar from
the top (analysis complete) to the bottom (11 clusters identiﬁed) represents each city. For
example, in the 11-cluster step (the location where the vertical bars begin at the bottom
of the plot), we have 10 stand-alone cities and one cluster where two cities have been
linked.
We know from the Proximity Matrix that the ﬁrst joining is between Austin and
Phoenix. Note that Austin and Phoenix occupy adjacent bars, as IBM SPSS used the
information of how these cities will be sequentially linked to align them across the top
of the icicle plot. Also note that to signify their joining, a vertical bar has been drawn
between these two cities down to the 11-cluster mark.
Moving up to the 10-cluster position, we see that Los Angeles and Chicago have
been linked. Thus, we have eight stand-alone cities and two clusters each containing two
cities. Again moving up one notch to the nine-cluster position, we see that Seattle and
Atlanta have been joined together.

ANALYSIS OUTPUT
629
FIGURE 59.7
The vertical icicle plot.
FIGURE 59.8
Dendrogram plot of cities.
The light spacing in the icicle plot helps us to distinguish the clusters as they grow.
Just before we reach the top row, we can distinguish the jump in coefﬁcient values
that caught our attention in the Coefﬁcients column of the Agglomeration Schedule. It
appears to present us with two major clusters. One cluster (on the far right of the plot)
appears to contain Seattle, Atlanta, San Francisco, Boston, and New York; the other
contains the remaining cities.

630
HIERARCHICAL CLUSTER ANALYSIS
Examining the next lowest position allows us to discern the alternative and less
appealing solution that we highlighted in the Agglomeration Schedule. Here, we appear
to have a set of three clusters. Again, Seattle, Atlanta, San Francisco, Boston, and New
York represent one of the clusters. Another cluster appears to be on the far left of the
plot and consists of Memphis and Jacksonville. The third cluster lies between these two
in the plot and consists of Miami, Austin, Phoenix, Los Angeles, and Chicago.
An alternative way to view the clustering process is the Dendrogram plot shown in
Figure 59.8. It may be somewhat more difﬁcult to discern the three-cluster solution in
this diagram but the two-cluster solution stands out strikingly clear, being almost pointed
to by the rightmost pair of horizontal lines. Given the results of the Agglomeration
Schedule, together with the Icicle and Dendrogram plots, we would conclude that the
two-cluster solution best characterizes the similarities among the studied cities.

C H A P T E R
6 0
k-Means Cluster Analysis
60.1
OVERVIEW
While hierarchical clustering discussed in Chapter 59 is most often used to group a
relatively few entities on the basis of several variables, k-means clustering is used to
form a small set of groups or clusters from relatively many entities based on a small
subset of variables. As was true for hierarchical clustering, the variables should be in
z score form at the start of the analysis; however, this must be done in advance for
k-means clustering in the Descriptives procedure of IBM SPSS®. The algorithm used
for the k-means clustering procedure was named by J. MacQueen (1967), but the version
of the algorithm that is currently used by most software programs was put forward by
Hartigan and Wong (1979).
The k-means procedure begins with the researchers specifying the number of clusters
they wish to have formed; the k in k-means is the number of clusters. Because we do
not know the “best” (most interpretable, most sensible, most appealing) solution at the
outset, researchers almost always perform a set of analyses to generate a small range of
solutions (e.g., three to ﬁve clusters); they will then choose the one that works best for
their needs.
The k-means analysis is an iterative process. The ﬁrst step in the analysis is the
identiﬁcation of k cases in the data ﬁle that have quite different values (are far apart) on the
clustering variables. These cases will serve as seed points or initial cluster centers (outliers
must have been screened out, as they are otherwise strong candidates to be selected as
seed points and thus distort the results). In this ﬁrst phase, clusters are built with a case
at a time being assigned to the cluster to which it is closest. A modiﬁed centroid method
uses the Euclidean distance between the centroid or center of the cluster (an average of
the clustering variables) and the nonclustered entities. The smallest distance determines
which entity is to be assigned to which cluster.
Once all of the cases have been assigned cluster membership, the center of the
cluster (its centroid) is determined. This centroid is, simplistically, the multivariate mean
of the cases assigned to the cluster based on the variables that were used in the clustering
process. These centroids take the role of new seed points, the cases are “freed” from their
prior cluster assignment, and a reassignment phase is initiated akin to what was done at
the start of the procedure.
After all of the cases have been reassigned to the new clusters, centroids based
on this reconﬁguration are computed. IBM SPSS then determines the largest change in
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
631

632
K-MEANS CLUSTER ANALYSIS
distance between clusters that has taken place. If that change is greater than a preset
threshold, it starts again (iterates) using the current cluster centers as the initial centers
(seed points) of another reclassiﬁcation phase. Iteration continues until either the change
in distance reaches its criterion threshold or IBM SPSS reaches the number of iterations
that was speciﬁed by the researchers. Cluster membership can be saved to the data ﬁle
and can then be used as a categorical grouping variable in subsequent analyses.
60.2
NUMERICAL EXAMPLE: k -MEANS CLUSTERING
For this example, we use the personality data ﬁle with 425 cases. We will cluster
the cases based on three variables: neoconsc (conscientiousness), negafect (amount of
negative affect, such as distress, hostility, and fear respondent have reported as having
been recently experienced), and regard (a measure of self-regard).
60.3
ANALYSIS STRATEGY
We ﬁrst perform a z score transformation of the three clustering variables in the Descrip-
tives procedure. We then use those transformed variables in a k-means procedure. For
illustration purposes, we obtain the four-cluster solution, as we believe it presents a viable
and interpretable solution, although a three-cluster solution is not unreasonable. Finally,
we will use the groups generated by the k-means procedure as an independent variable
in a one-way between-subjects ANOVA to illustrate the use of the cluster variable in a
subsequent (and very simple) analysis.
60.4
TRANSFORMING CLUSTER VARIABLES TO Z SCORES
We open the personality data ﬁle and from the main menu select Analyze ➔Descriptive
Statistics ➔Descriptives. This opens the main Descriptives dialog window shown in
Figure 60.1. We move neoconsc, negafect, and regard into the Variable(s) panel, check
the box for Save standardized values as variables, and select OK to perform the
analysis.
The result of our procedure can be seen in Figure 60.2. The newly transformed
variables, named with an upper case Z in front of the original name, appear at the end
of the data ﬁle.
FIGURE 60.1
Using Descriptives to create z score transformations of
the three variables for the k-means cluster analysis.

ANALYSIS SETUP: k-MEANS CLUSTERING
633
FIGURE 60.2 A portion of the data ﬁle showing the transformed variables.
60.5
ANALYSIS SETUP: k -MEANS CLUSTERING
From the main menu select Analyze ➔Classify ➔k-Means Cluster. This opens the
main dialog window shown in Figure 60.3. We move Zneoconsc, Znegafect, and Zre-
gard into the Variable(s) panel. We specify 4 in the Number of Clusters panel and
retain the default of Iterate and classify under Method.
Selecting the Iterate pushbutton opens the Iterate dialog window shown in
Figure 60.4. The default number of iterations established by IBM SPSS is 10, a
carryover from long-ago days when too many iterations would stress the limited
processing power of the ancient machines. We bump the iterations to 100 to ensure that
our analysis will go to completion. Clicking Continue returns us to the main dialog
window.
In the Options window shown in Figure 60.5, we select in the Statistics area Initial
cluster centers (to show the initial seed points) and ANOVA table to compare the groups
on the clustering variables. We also retain the option of Exclude cases listwise in the
Missing Values portion of the window. Click Continue to return to the main dialog
window.
The Save window is shown in Figure 60.6. We select Cluster membership. This
will create a new categorical variable at the end of the data ﬁle, with numeric codes
indicating the cluster to which each case has been assigned in the ﬁnal solution. Click
Continue to return to the main dialog window and select OK to perform the analysis.

634
K-MEANS CLUSTER ANALYSIS
FIGURE 60.3 The main dialog window of k-Means Cluster Analysis.
FIGURE 60.4
The Iterate dialog window of
k-Means Cluster Analysis.
FIGURE 60.5
The Options dialog window of
k-Means Cluster Analysis.

ANALYSIS OUTPUT: k-MEANS CLUSTERING
635
FIGURE 60.6
The Save dialog window of
k-Means Cluster Analysis.
60.6
ANALYSIS OUTPUT: k -MEANS CLUSTERING
Figure 60.7 shows a screenshot of a portion of the data ﬁle, with the cluster membership
of each case designated under the variable QCL_1. The name of the variable is shorthand
for “Quick Cluster, First Save Command of this Session.” We can see that the case in
the ﬁrst row is assigned to Cluster 3, the case in the second row is assigned to Cluster
1, and so on.
The initial seed points for the analysis are shown in the Initial Cluster Centers
table in Figure 60.8. The values are in z score form, with a sample mean of zero and
standard deviation of 1.00. Most of the seed point z scores are well in excess of one
standard deviation distance from their respective means, and many are somewhat over
the distance of two standard deviations. We note two points. First, we do not see a true
outlier (a score beyond three standard deviation units), and that is reassuring. Second,
these seed points are very distant from the sample mean, but the iterative process will
abate these distances to a large extent by the time we obtain the ﬁnal cluster centers.
Figure 60.9 displays the Iteration History. Our modiﬁcation of the default appeared
to be unnecessary here, as the convergence threshold was achieved by the tenth iteration.
One concern of researchers in performing a k-means cluster analysis is making sure
that the number of cases assigned to each cluster in the ﬁnal solution is acceptable. This
information is shown in Figure 60.10, where we see that sample sizes ranged from 66
FIGURE 60.7 A portion of the data ﬁle showing the saved cluster membership variable.

636
K-MEANS CLUSTER ANALYSIS
FIGURE 60.8
The table of Initial Cluster Centers.
FIGURE 60.9
The Iteration History table.
FIGURE 60.10
The sample size falling into each of the four clusters.
(Cluster 2) to 125 (Cluster 1). We judge this as quite good. To be avoided is the situation
where one cluster has very few cases (maybe a dozen or fewer cases), rendering such a
group as not viable in subsequent analyses.
The ANOVA summary table is shown in Figure 60.11. The footnote to the table is
important, which informs us that the analysis needs to be treated as strictly exploratory.
Here, there are statistically signiﬁcant group differences in all of the clustering variables.
The primary use of this table to researchers is to recognize when a clustering variable
does not yield a statistically signiﬁcant effect; this would indicate that the variable is not
effective for its chosen purpose and might suggest to researchers that they may wish to
repeat the analysis swapping it out for another variable if one is reasonable and available.
The table of Final Cluster Centers is shown in Figure 60.12. This table shows
the heart of the results and is used to characterize the clusters. Note that the relatively
extreme values of the seed points have been substantially moderated. These z scores are
based on the sample statistics and so must be interpreted relative to this sample rather
than in any population sense.

FOLLOW-UP ONE-WAY BETWEEN-SUBJECTS ANALYSIS
637
FIGURE 60.11 Results of an exploratory ANOVA examining group differences for each of the
three clustering variables.
FIGURE 60.12
The table of Final Cluster Centers.
Because the standardization is based on a relatively large sample size (N = 420),
we can interpret the magnitude of the z scores in a functional way. Thus, we can judge a
z score in the range about ±.50 to about ±.75 as substantial, a z score of approximately
±1.00 as relatively considerable, and a z score of anything greater than about ±1.30
as fairly extreme with respect to the sample mean. In our characterization, we look for
these higher z scores to guide our interpretation. We characterize these clusters (groups)
as follows:
• Cluster 1. The only distinguishing feature of Cluster 1 is a substantial (negative)
conscientiousness mean, suggesting that the cases in this cluster can on average
be generally characterized as being relatively more careless and irresponsible than
average for the sample.
• Cluster 2. The cases on average in this cluster appear to be characterized by a
relative lack of conscientiousness and self-regard but appear to experience consid-
erable negative affect; they may be characterized as being relatively more hopeless
and lost than average for the sample.
• Cluster 3. The cases on average in this cluster appear to be characterized by sub-
stantial conscientiousness and self-regard while experiencing very little negative
affect; they may be characterized as being relatively more psychologically healthy
than average for the sample.
• Cluster 4. The cases on average in this cluster appear to be relatively conscien-
tiousness but experience substantial negative affect; they may be characterized as
being more likely than average for the sample to brood while completing the tasks
of life without taking much pleasure in their actions.
60.7
FOLLOW-UP ONE-WAY BETWEEN-SUBJECTS ANALYSIS
To illustrate the use of the cluster membership variable in a subsequent analysis, we
perform a one-way between-subjects ANOVA using the One-Way ANOVA procedure
in IBM SPSS. We discussed this procedure in Chapter 48 and opt for it here because
it is not unusual for groups formed by a clustering procedure to differ quite a bit in

638
K-MEANS CLUSTER ANALYSIS
variability in the dependent variable, and in the One-Way ANOVA procedure, we can
request specialized tests of signiﬁcance of the omnibus effect (an overall group difference)
that take into account a violation of the homogeneity of variance assumption.
60.8
NUMERICAL EXAMPLE: ONE-WAY ANOVA
In the personality data ﬁle now containing our QCL_1 variable that will be used as the
independent variable representing our groups, we will use selfcon (self-control) as our
dependent variable. This dependent variable is the score on the Self-Control Schedule
(Rosenbaum, 1980) and represents ways of coping with life by taking positive control
of one’s thoughts and feelings.
60.9
ANALYSIS SETUP: ONE-WAY ANOVA
From the main menu select Analyze ➔Compare Means One-Way ANOVA. This opens
the main dialog window shown in Figure 60.13. We move selfcon into the Dependent
List panel and QCL_1 into the Factor panel.
In the Options dialog window presented in Figure 60.14, we check Descriptive
(to obtain several descriptive statistics for the dependent variable) and Homogeneity
FIGURE 60.13
The main dialog window of One-Way ANOVA.
FIGURE 60.14
The Options dialog window of One-Way ANOVA.

ANALYSIS OUTPUT: ONE-WAY ANOVA
639
FIGURE 60.15
The Post Hoc dialog window of One-Way
ANOVA.
of variance test (to obtain Levene’s test). To obtain statistical signiﬁcance tests of the
between-subjects effect if the assumption of homogeneity of variance has not been met,
we also check Brown-Forsythe and Welch. Click Continue to return to the main dialog
window.
We will optimistically anticipate a statistically signiﬁcant omnibus group difference
and request in the Post Hoc dialog window (see Figure 60.15) the R-E-G-W-Q test (to
be used if we have homogeneity of variance) and Tamhane’s T2 test (to be used if we
have violated the homogeneity of variance assumption). Click Continue to return to the
main dialog window and click OK to perform the analysis.
60.10
ANALYSIS OUTPUT: ONE-WAY ANOVA
The descriptive statistics shown in the top table of Figure 60.16 provide relatively com-
plete information describing each group. Based on visual inspection of the differences in
means with respect to their standard errors, it appears likely that the analysis will yield
a statistically signiﬁcant omnibus effect.
The bottom table in Figure 60.16 displays the results of the Levene test. With 3 and
416 degrees of freedom, the value of the test statistic (1.205) is not statistically signiﬁcant
(p = .307). This outcome indicates that we have not violated the assumption of equal
group variances and can use the ordinary F ratio as our test of statistical signiﬁcance for
the overall effect.
The ANOVA summary table is presented in the top table of Figure 60.17. The
omnibus effect of the independent variable (Between Groups in the table) yielded an F
ratio of 54.490. With 3 and 416 degrees of freedom, the effect is statistically signiﬁcant
(p < .001). The value of eta square is computed as the ratio of Between Groups Sum
of Squares to Total Sum of Squares; here, it is 30.685/108.772, or .282. Thus, approx-
imately 28% of the total variance of selfcon is explained by QCL_1 (the cluster group
membership).
The bottom table of Figure 60.17 shows the results for the Robust Tests of Equality
of Means. Had we violated the assumption of equal variance, we would have used one
of these tests to evaluate the statistical signiﬁcance of the omnibus effect.
With the omnibus F ratio statistically signiﬁcant, we can examine the results for our
post hoc tests. We focus on the R-E-G-W-Q test in that we have observed comparable
variances across our four groups, and these results are presented in Figure 60.18. We

640
K-MEANS CLUSTER ANALYSIS
FIGURE 60.16 The Descriptives and Test of Homogeneity of Variances output.
FIGURE 60.17 The ANOVA summary table and the Robust Tests of Equality of Means (used
if homogeneity of variance has been violated).
described how to read this output in Section 47.5; brieﬂy, means within a subset do
not statistically differ, whereas means that are in different subsets do signiﬁcantly differ.
Thus, cases in Cluster 2 (relatively more hopeless and lost than average) reported the
lowest level of self-control, those in Clusters 1 (relatively more careless and irresponsible
than average) and 4 (somewhat conscientious but more likely than average to brood)
reported the second highest degree of self-control, and those in Cluster 3 (relatively more
psychologically healthy than average) reported the ﬁrst highest level of self-control.
We can also evaluate the effect size using Cohen’s d statistic. For example, the
largest mean difference was between Cluster 3 and Cluster 2. From the descriptive statis-
tics output in Figure 60.16, we can compute the (weighted) average standard deviation of

ANALYSIS OUTPUT: ONE-WAY ANOVA
641
FIGURE 60.18 The results of the R-E-G-W-Q post hoc test.
these two groups as ((119 * .41563) + (66 * .43524))/(119 + 66) = 78.1858/185 = .423.
Cohen’s d is the ratio of the mean difference to the average standard devia-
tion; for the difference between Cluster 3 and Cluster 2, it can be computed as
4.4458 −3.6366/.423 = .8092/.423 = 1.913. This would be interpreted as a large effect
size. However, because these groupings were created statistically with an algorithm
biased toward separating the clusters on at least the clustering variables, and because
selfcon is likely correlated with the clustering variables to a certain extent, we would
use Cohen’s d effect size index in this instance with considerable caution.


P A R T 19
NONPARAMETRIC
PROCEDURES FOR
ANALYZING FREQUENCY
DATA


C H A P T E R
6 1
Single-Sample Binomial
and Chi-Square Tests:
Binary Categories
61.1
OVERVIEW
The procedures we have described in most parts of this book are applicable when we
are working with variables that are assessed on quantitative scales of measurement, but
it is not uncommon for researchers to also collect frequency data. Frequency data can be
collected on nominal or categorical variables where we count the number of occurrences
observed for each mutually exclusive category of a qualitative variable. Examples of such
categorical variables are the number of students enrolled in a particular university who
are on ﬁnancial aid or not; the number of patrons of the local coffee house purchasing a
caf´e latte or a caf´e mocha on a given day; the number of red, silver, or black vehicles that
pull in for gas at a particular service station; and the number of patients in a particular
health provider facility given a ﬂu shot during past October.
In the most general sense, our research focus in such qualitative studies is on whether
the pattern of observed frequencies meets our expectations. To address this focus, we
apply a single-sample nonparametric test, either the binomial test or the single-sample
chi-square test developed by Karl Pearson (1900b). The binomial test is based on the
binomial distribution and is applied only to variables that have binary (exactly two)
categories; we use a z statistic to compare the observed and expected frequencies. The
single-sample chi-square can be applied to two or more categories; it will provide the
same result as the binomial test for a binary variable.
To qualify as a single-sample design, we process only one piece of information
regarding the cases—the category into which each case is classiﬁed. Using binary or
dichotomous categorical variables, cases are associated with either one arbitrary numer-
ical code (e.g., 1 representing “yes”) or another (e.g., 0 representing “no”). For each
case, a student is on ﬁnancial aid or is not, a patron purchased either a caf´e latte or a
caf´e mocha, and so on. If we had collected frequency data on a variable that had three
or more categories, cases are still associated with only one of the arbitrarily assigned
multiple codes (e.g., 1, 2, and 3 represent red, silver, and black vehicles, respectively),
but this schema does not meet the requirement of a binary categorical variable; how to
handle such variables is treated in Chapter 62.
The frequencies we expect to observe in the study must be made explicit in setting
up these nonparametric tests. Often, our expectations will be based entirely on chance,
that is, a 50-50 split for dichotomous variables. For example, we might suppose that half
of the students sampled will indicate that they are on ﬁnancial aid and half will not, or
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
645

646
SINGLE-SAMPLE BINOMIAL AND CHI-SQUARE TESTS: BINARY CATEGORIES
that half of the patrons who buy one of the two drinks will purchase a caf´e latte and that
the other half will purchase a caf´e mocha. In other circumstances, on either a theoretical
or an actuarial basis, we might expect some different breakdown. For example, we might
know that on a national basis, about 67% of all college students received some ﬁnancial
aid during the previous year and so we might expect a two-thirds to one-third showing
in the frequencies on the particular campus where the study is being conducted.
61.2
NUMERICAL EXAMPLE
We use the data ﬁle named ﬁnancial aid containing the variable ﬁnancial_aid. This
is a Nominal variable (deﬁned as such in the Measure column of the Variable View)
assessing whether the student who was sampled was on ﬁnancial aid or not. Financial_aid
is a binary variable with the two categories of No (coded as 0 in the data ﬁle) and Yes
(coded as 1 in the data ﬁle). A screenshot of a portion of the data ﬁle is presented in
Figure 61.1 to illustrate the data structure of a one-sample design.
61.3
ANALYSIS STRATEGY
IBM SPSS® has revamped the Nonparametric Tests module in recent versions but has
retained the older procedures under Legacy Dialogs within the Nonparametric Tests
FIGURE 61.1
A portion of the data ﬁle.

ANALYSIS SETUP
647
submenu. The Legacy procedures have some features that, in our opinion, make user
errors more likely to occur. For example, if the proportions for the categories need to
be user-deﬁned rather than having equal probabilities in the Binomial test, the speciﬁed
proportion will be applied to the ﬁrst value in the data ﬁle, thus forcing users to either
carefully select the ﬁrst case in data entry or to sort the ﬁle before performing the analysis.
These features are much improved in the revamped Nonparametric Tests module, and
so we describe this module in this chapter. Both the binomial and chi-square tests can
be used for a binary coded variable, and we perform both in a single data analysis run
to illustrate how each works.
61.4
FREQUENCIES ANALYSIS
We have performed a Frequencies analysis on the variable ﬁnancial_aid to determine its
distribution and to provide some context, the results of which are shown in Figure 61.2.
As can be seen, the breakdown of students in the sample is as evenly divided as possible,
given an N of 35.
61.5
ANALYSIS SETUP
We open ﬁnancial aid and from the main menu select Analyze ➔Nonparametric Tests
➔One Sample. This opens the Objective screen in the initial One-Sample Nonpara-
metric Tests window shown in Figure 61.3. We select Customize analysis to gain full
control over the speciﬁcations for our analysis and click the Fields tab.
Despite our selection of Customize analysis in the previous screen, the Fields win-
dow opens with the Use predeﬁned roles button selected and all of our variables already
placed in the Test Fields panel (see Figure 61.4). We want only ﬁnancial_aid in the Test
Fields panel, and the easiest way to accomplish this is to highlight id and move it into
the Fields panel by clicking the horizontal arrow. This action not only moves id across
the panels but also switches the choice at the top left portion of the screen to Use custom
ﬁeld assignments as shown in Figure 61.5.
Selecting the Settings tab opens the Settings window (see Figure 61.6). The choices
in this window allow us to specify our statistical analysis, but again we must wrest control
from IBM SPSS by selecting in the Choose Tests screen the option of Customize tests
and thereby negate the default of Automatically choose the tests based on the data
(an option that allows users to blindly perform analyses without necessarily knowing
what they are doing). The only two tests applicable to our frequency data are the Bino-
mial test and the Chi-Square test (the ﬁrst two checkboxes). In Figure 61.6, we have
selected Customize tests and have checked Compare observed binary probability to
hypothesized (Binomial test).
FIGURE 61.2 Output from Frequencies analysis.

648
SINGLE-SAMPLE BINOMIAL AND CHI-SQUARE TESTS: BINARY CATEGORIES
FIGURE 61.3 Initial screen of One-Sample Nonparametric Tests where we have selected Cus-
tomize analysis.
FIGURE 61.4 The initial window on the Fields tab.

ANALYSIS SETUP
649
FIGURE 61.5 The Fields window conﬁgured with ﬁnancial_aid in the Test Fields panel.
FIGURE 61.6 In the Choose Tests screen of Settings, we have chosen to Customize the Binomial test.

650
SINGLE-SAMPLE BINOMIAL AND CHI-SQUARE TESTS: BINARY CATEGORIES
FIGURE 61.7 The Binomial Options are now speciﬁed.
Selecting the Options pushbutton under the Binomial test opens the Binomial
Options window shown in Figure 61.7. In the Hypothesized proportion panel, we
replace the default of 0.5 with 0.67 by typing it in (we could also use the arrow buttons
to modify the value), as we wish to evaluate the proportion of students in our sample
who are receiving ﬁnancial aid against the national proportion of 67%.
In the Deﬁne Success for Categorical Fields, we wish to reject the default option
(carried over from the Legacy procedures) of Use ﬁrst category found in data and
instead select Specify success values. The term success is used in a generic manner
to designate the category that is our focal interest (here it is the proportion of students
on ﬁnancial aid). Opting for Specify success values places the user in control of the
assignment of the 0.67 Hypothesized proportion. Selecting Specify success values
opens the Value dialog panel where we type in 1 to indicate the category assigned to
students in the sample who are on ﬁnancial aid. All this is also shown in Figure 61.7.
Click OK to return to the Choose Tests screen. We need not modify the Test Options
screen (it speciﬁes an alpha of .05) or the User-Missing Values (which speciﬁes that we
exclude missing values from the analysis).
In the Choose Tests screen shown in Figure 61.8, we now also check Compare
observed probabilities to hypothesized (Chi-Square test), as we can perform this test
at the same time. Selecting the Options pushbutton under the Chi-Square test opens
the Chi Square Test Options window shown in Figure 61.9. There is no Hypothesized
proportion panel because this test can be (and usually is) applied to more than two
categories.
We select Customize expected probability, thereby deactivating the default All cat-
egories have equal probability option. On selecting Customize expected probability,
the Expected probabilities dialog area becomes available. We have two categories and
will specify probabilities for both in whatever order we prefer. Thus, we type 1 in the
cell in the ﬁrst row under Category, click the panel in the ﬁrst row under Relative
Frequency, and type 67. Then we click the second row under Category that has now
become available, type 0, click the panel under Relative Frequency, and type 33. We
strongly recommend that these relative frequencies add to 100 because IBM SPSS will

ANALYSIS OUTPUT
651
FIGURE 61.8 In the Choose Tests screen of Settings, we have chosen to Customize the Chi-
Square test.
FIGURE 61.9
Chi Square Options are now speciﬁed.
total them to determine the percent of total represented by each of the relative frequen-
cies. The ﬁnished speciﬁcations are shown in Figure 61.9. Click OK to return to the
Choose Tests screen, and select Run to perform the analyses.
61.6
ANALYSIS OUTPUT
An overview of the results is shown in the summary table in Figure 61.10. The top row
provides output from the chi-square test and informs us (in the ﬁrst column) that we had
speciﬁed the probabilities; the bottom row addresses the binomial test and provides us
with the proportions we speciﬁed. Both tests are statistically signiﬁcant (p = .020 for

652
SINGLE-SAMPLE BINOMIAL AND CHI-SQUARE TESTS: BINARY CATEGORIES
FIGURE 61.10
The basic output that can be expanded by
double-clicking.
chi-square and p = .016 for the binomial test), and in both cases, based on an alpha level
of .05, we would reject the null hypothesis.
The footnote to the table indicates that Asymptotic signiﬁcances are displayed.
Brieﬂy, asymptotic signiﬁcance levels are approximations that may falter with very small
samples, especially where several cells have frequencies of fewer than ﬁve cases. In the
Legacy procedures, we could override the default of asymptotic results and instead
request exact probabilities (assuming we had the Exact Probabilities software as part
of our IBM SPSS package), but this option is gone in the renovated procedures where
asymptotic results are always shown.
Double-clicking on a given row expands the output. Figure 61.11 presents the
expanded view of the binomial test. The bar chart visually displays the observed and
FIGURE 61.11 Expanded view of binomial test results.

ANALYSIS OUTPUT
653
FIGURE 61.12
Expanded view of chi-square results.
hypothesized frequencies. In the statistical table below the graph, we see that the z value
is −2.139 and that the chances of obtaining that value or one displaced even further from
the mean of a normal curve is .016. We therefore conclude that the frequencies were not
distributed as hypothesized; rather, there were proportionally fewer students on ﬁnancial
aid in this sample than was true for the national average.
Figure 61.12 presents the expanded view of the chi-square test. The chi-square value
is shown in the table as 5.376. Evaluated against a chi-square distribution with one degree
of freedom, the chances of obtaining that value or greater are .020. The conclusion we
draw is identical to the outcome of the binomial test.


C H A P T E R
6 2
Single-Sample (One-Way)
Multinominal Chi-Square Tests
62.1
OVERVIEW
The one-way chi-square test described in Chapter 61 is applicable not only to binary
variables but also to multinomial (more than two categories) categorical variables. The
analysis setup is a simple extension of the two-category design. It is called a one-way
chi-square analysis, in that there is a single variable under study. Each category within
that variable represents a different group of cases, and our question is whether the sample
sizes of the groups (the frequencies) are in accord with our expectations.
62.2
NUMERICAL EXAMPLE
The ﬁctional data we use for this example are derived from clients of a stress-reduction
clinic serving many companies in its metropolitan area. The 110 clients in this study
worked in high stress jobs for a large technology ﬁrm and were referred to the clinic
during the past ﬁscal year by the Human Resources manager based on recommenda-
tions from supervisors. These clients were presenting distress symptoms or signals and
were categorized by the clinic according to the strongest set of signals they presented:
symptoms of cognitive decline included lessened ability to make decisions, lowered con-
centration, and less likelihood of remembering important things (coded a 1); symptoms
of emotional deterioration included general irritability, loss of trust in people, and pro-
crastination (coded a 2); and symptoms of physical changes included increase in alcohol
consumption, use of prescription or illegal drugs, and weight change (coded a 3) under
the variable of distress_signal. The data ﬁle is named distress signals.
62.3
ANALYSIS STRATEGY
We use the revamped Nonparametric Tests module as described in Chapter 61 to perform
the analysis. Assume for our example that it was determined, based on clients from other
types of industries seen by the clinic, that the strongest set of symptoms presented by
clients would be equally likely across the three categories. We therefore perform the
omnibus (three-category) analysis under the expectation that the categories should have
equal frequencies. If we obtain a statistically signiﬁcant chi-square (indicating that the
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
655

656
SINGLE-SAMPLE (ONE-WAY) MULTINOMINAL CHI-SQUARE TESTS
frequencies are not equally divided among the three categories), we will then perform
post hoc comparisons analogous to what we would do in a one-way ANOVA design.
As there is no ofﬁcial post hoc option in the module, we perform additional chi-square
analyses on the pairs of the categories in order to pinpoint the group differences.
62.4
FREQUENCIES ANALYSIS
We have performed a Frequencies analysis on the distress_signal variable to determine
its distribution, the results of which are shown in Figure 62.1. As can be seen, the most
prevalent category is cognitive decline with 48 (43.6%) of the cases, the next most
prevalent category is emotional deterioration with 38 (34.5%) of the cases, and the
least frequently seen category is physical changes with 24 (21.8%) of the cases.
62.5
ANALYSIS SETUP: OMNIBUS ANALYSIS
We open distress signals and from the main menu select Analyze ➔Nonparametric
Tests ➔One Sample. In the Objective screen in the initial One-Sample Nonparametric
Tests window as shown in Figure 62.2, we select Customize analysis.
In the Fields screen, we highlight id and move it into the Fields panel by clicking
the horizontal arrow. This action leaves distress_signal in the Test Fields panel and
switches the choice at the top left portion of the screen to Use custom ﬁeld assignments
as shown in Figure 62.3.
Selecting the Settings tab allows us to specify our statistical analysis. We select
Customize tests in the Choose Tests screen and check Compare observed probabilities
to hypothesized (Chi-Square test) as shown in Figure 62.4.
Selecting the Options pushbutton under the Chi-Square test opens the Chi Square
Test Options window shown in Figure 62.5. In the Choose Test Options panel, we
select All categories have equal probability for this omnibus analysis, with all three
categories included. Click OK to return to the Settings window and click Run to perform
the analysis.
62.6
ANALYSIS OUTPUT: OMNIBUS ANALYSIS
The summary output for the omnibus analysis is shown in Figure 62.6. Under Null
Hypothesis, we are reminded in the ﬁrst column that we opted for expecting equal
frequencies. The chi-square value is statistically signiﬁcant (p = .019), indicating that
we should reject the null hypothesis.
FIGURE 62.1 Output from Frequencies analysis.

ANALYSIS OUTPUT: OMNIBUS ANALYSIS
657
FIGURE 62.2 Initial screen of One-Sample Nonparametric Tests where we have selected
Customize analysis.
FIGURE 62.3 The Fields window conﬁgured with distress_signal in the Test Fields panel.

658
SINGLE-SAMPLE (ONE-WAY) MULTINOMINAL CHI-SQUARE TESTS
FIGURE 62.4 In the Choose Tests screen of Settings, we have chosen to Customize the Chi-
Square test.
FIGURE 62.5
We have selected All categories have equal
probability in the Options window.
Double-clicking on the summary table expands the output as shown in Figure 62.7.
The chi-square value is shown in the table as 7.927. Evaluated against a chi-square
distribution with 2 degrees of freedom (because there are three categories in the analysis),
the chances of obtaining that value or greater are .019. The omnibus analysis has thus
informed us that the frequencies across the categories are not distributed equally. While

ANALYSIS SETUP: COMPARISON OF CATEGORIES 1 AND 2
659
FIGURE 62.6
The summary of the results for the omnibus analysis.
FIGURE 62.7 Detailed view of the results for the omnibus analysis.
this result may be sufﬁcient for some research purposes, it is often useful to engage in
ﬁner-grain analyses by comparing pairs of categories or groups.
62.7
ANALYSIS SETUP: COMPARISON OF CATEGORIES
1 AND 2
We set up the analysis exactly as described earlier up to the point where we conﬁgure the
Chi-Square Options window. As shown in Figure 62.8, we select Customize expected
probability. In the Expected probabilities panel, we click the cell under Category in
the ﬁrst row, type 1, click the cell next to it under Relative Frequency, and type 50. We
then do the same for the second row but now specify Category 2. This conﬁguration
accomplishes two goals at once:
• It restricts the chi-square analysis to only Categories 1 and 2, thereby automatically
excluding Category 3. The total frequency is the sum of Categories 1 and 2.
• It sets the expectation that the two categories in the analysis should have equal
frequencies. We make sure that the values in the Relative Frequency column sum

660
SINGLE-SAMPLE (ONE-WAY) MULTINOMINAL CHI-SQUARE TESTS
FIGURE 62.8
We have conﬁgured the Options to compare only the ﬁrst
and second categories.
to 100, in that IBM SPSS® will sum the values and determine what proportion of
the total each value represents.
62.8
ANALYSIS OUTPUT: COMPARISON OF CATEGORIES
1 AND 2
Figure 62.9 presents the summary of the chi-square test comparing Categories 1 and 2.
Under the Null Hypothesis, we see that we have speciﬁed the probabilities but those
speciﬁcations do not appear in the summary. Another element not included in the sum-
mary table is exactly what categories were included in the analysis. However, we do learn
that the included categories were not signiﬁcantly different in frequency (p = .281).
Double-clicking on the summary table expands the output as shown in Figure 62.10.
The bar graph above the statistical table shows us that the categories in the analysis were
cognitive decline and emotional deterioration, but the choice of scale on the Y-axis,
starting at 38, is unable to clearly display the observed frequency for emotional deterio-
ration (it is 38). The chi-square value is shown in the table as 1.163. Evaluated against a
chi-square distribution with 1 degree of freedom (because there are two categories in the
analysis), the chances of obtaining that value or greater are .281. We therefore conclude
that the frequencies of cognitive and emotional distress signals in the sample were not
statistically different.
62.9
ANALYSIS SETUP: COMPARISON OF CATEGORIES
1 AND 3
We set up the analysis exactly as described earlier up to comparing the ﬁrst two categories,
except that in the Chi-Square Options window, we specify 1 and 3 in the Category
column, each still at a Relative Frequency of 50. This is shown in Figure 62.11.
FIGURE 62.9
The summary of the results comparing
Categories 1 and 2.

ANALYSIS OUTPUT: COMPARISON OF CATEGORIES 1 AND 3
661
FIGURE 62.10 Detailed view of the results comparing Categories 1 and 2.
FIGURE 62.11
We have conﬁgured the Options to compare only the ﬁrst
and third categories.
62.10
ANALYSIS OUTPUT: COMPARISON OF CATEGORIES
1 AND 3
Figure 62.12 presents the summary of the chi-square test comparing Categories 1 and 3.
The results indicate that the included categories were signiﬁcantly different in frequency
(p = .005).
Double-clicking on the summary table expands the output as shown in Figure 62.13.
The bar graph above the statistical table identiﬁes the categories in the analysis as cogni-
tive decline and physical changes. The chi-square value is shown in the table as 8.000.

662
SINGLE-SAMPLE (ONE-WAY) MULTINOMINAL CHI-SQUARE TESTS
FIGURE 62.12
The summary of the results comparing
Categories 1 and 3.
FIGURE 62.13 Detailed view of the results comparing Categories 1 and 3.
Evaluated against a chi-square distribution with 1 degree of freedom (because there are
two categories in the analysis), the chances of obtaining that value or greater are .005.
On the basis of this analysis, we therefore conclude that there were signiﬁcantly more
clients presenting cognitive signals of distress than those presenting physical signals of
distress.
62.11
ANALYSIS SETUP: COMPARISON OF CATEGORIES
2 AND 3
We set up the analysis exactly as described earlier up to comparing the ﬁrst two categories,
except that in the Chi-Square Options window, we specify 2 and 3 in the Category
column. This is shown in Figure 62.14.

ANALYSIS SETUP: COMPARISON OF CATEGORIES 2 AND 3
663
FIGURE 62.14
We have conﬁgured the Options to compare only the second
and third categories.
FIGURE 62.15
The summary of the results comparing
Categories 2 and 3.
FIGURE 62.16 Detailed view of the results comparing Categories 2 and 3.

664
SINGLE-SAMPLE (ONE-WAY) MULTINOMINAL CHI-SQUARE TESTS
62.12
ANALYSIS OUTPUT: COMPARISON OF CATEGORIES
2 AND 3
Figure 62.15 presents the summary of the chi-square test comparing Categories 2 and
3. The results indicate that the included categories were not signiﬁcantly different in
frequency (p = .075).
Double-clicking on the summary table expands the output as shown in Figure 62.16.
The bar graph above the statistical table identiﬁes the categories in the analysis as emo-
tional deterioration and physical changes. The chi-square value is shown in the table as
3.161. Evaluated against a chi-square distribution with 1 degree of freedom, the chances
of obtaining that value or greater are .075. On the basis of this analysis, we conclude
that there were not signiﬁcantly more clients presenting emotional signals of distress than
those presenting physical signals of distress.
In conclusion, the post hoc analyses indicated that there were signiﬁcantly more
clients presenting symptoms of cognitive decline than symptoms of physical changes
or symptoms of emotional deterioration.

C H A P T E R
6 3
Two-Way Chi-Square Test
of Independence
63.1
OVERVIEW
The two-way chi-square design is an extension of the one-way design covered in Chapters
61 and 62; the difference, and it is a substantial one, is that we include a second categorical
variable in the analysis. The frequencies in a two-way design are placed in a table where
one variable is represented by the rows (e.g., females and males) and another by the
columns (e.g., whether the patron ordered a caf´e latte or a caf´e mocha at a local coffee
house). Such a conﬁguration is called a two-way contingency table in the sense that the
frequencies are contingent on the levels of two categorical variables (e.g., females who
ordered a caf´e latte).
To distinguish between the two variables in the research study, some researchers
place the variable representing the groups of interest on the rows and have the other
variable (analogous to an outcome or focus of the study) represented by the columns, but
such placement is arbitrary and other researchers suggest the reverse arrangement. For
example, if we were studying the drink preferences of the two sexes in a local coffee
house, we would envision the rows representing the different sexes and each column
representing one of the coffee drinks. Such a conﬁguration is less relevant when we are
just comparing the observed with the expected frequencies as we do in this chapter but
becomes somewhat more relevant in the analysis of risk that we address in Chapter 64.
The null hypothesis in a two-way design is that the two variables are independent.
Consider the coffee house study where females and males are ordering drinks. If the two
variables are independent, then roughly the same proportion of females as males would
order each drink. For example, if 25% of each sex ordered a caf´e latte (and thus 75% of
each ordered a caf´e mocha), then we would obtain a nonsigniﬁcant chi-square value and
we would conclude that the two variables are independent; that is, if the two variables are
independent, the same pattern of drink ordering would apply to each sex, and knowing
the sex of the patron would not enhance our prediction of what drink is ordered.
If the two variables are related (if they are not independent), then the opposite of
what we just described will occur. For example, if 10% of the females but 75% of the
males ordered a caf´e latte, then we would obtain a statistically signiﬁcant chi-square value
and we would conclude that the two variables are not independent (i.e., they are related).
In this situation, sex of patron is related to the ordering preferences for the drinks, and
sex becomes a predictor of ordering behavior; given the above scenario, males would be
more likely than females to order a caf´e latte.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
665

666
TWO-WAY CHI-SQUARE TEST OF INDEPENDENCE
Another difference between the one-way and two-way chi-square design is the man-
ner in which the expected frequencies are generated. As we have seen in Chapter 62,
in the one-way design, they are generated theoretically, actuarially, or on the basis of
chance; in any case, it is the researchers who must decide on the strategy to generate
the values of the expected frequencies. In the two-way design, the expected frequencies
are generated empirically based on the observed frequencies, and they represent the null
hypothesis (that there is no relationship between the variables).
In our coffee house study, with patron sex as the rows and the type of drink as
the columns, we can establish the expected frequencies by ﬁrst determining the column
totals, that is, how many patrons overall ordered a caf´e latte and how many overall
ordered a caf´e mocha. Suppose that 40 patrons ordered caf´e latte and 60 ordered caf´e
mocha. The total, N, is thus 100, and of those 100 patrons, 40% overall ordered a caf´e
latte and 60% overall ordered a caf´e mocha.
The expected frequencies for each sex are based on these observed overall (marginal)
frequencies; thus, we would hypothesize that if the variables are independent (i.e., the
null hypothesis), then 40% of the female patrons and 40% of male patrons would order
a caf´e latte and 60% of the female patrons and 60% of male patrons would order a
caf´e mocha. These expected frequencies are compared to the observed frequencies, and a
chi-square test is used to determine if these two sets of frequencies are comparable (yield-
ing nonsigniﬁcant chi-square value) or not (yielding statistically signiﬁcant chi-square
value).
To say that two variables are related is to say that they are correlated, and there
are several indexes that assess the degree of relationship between categorical variables.
Two indexes that are not applicable here are the tetrachoric (introduced by Pearson
(1900a)) and polychoric correlations; these are used when underlying continuous or
latent variables have been dichotomized or divided into three or more ordered categories,
respectively. The three indexes that are applicable to a two-way contingency table are as
follows:
• Phi Coefﬁcient. Phi is a correlation coefﬁcient describing the relationship between
two truly categorical variables when each variable has only two levels (it can be
applied only to 2 × 2 contingency tables). Karl Pearson (1900a) derived it from his
Pearson r formula. Phi is computed by taking the square root of (chi-square/N),
where N is the total frequency. We typically square phi and interpret it analogously
to r2 as the amount of variance shared by the two variables.
• Contingency Coefﬁcient. This is a variant of phi coefﬁcient used when the number
of rows and columns is equal and exceeds 2 × 2. It is computed similarly to phi
except that the denominator is N + chi-square.
• Cramer’s V. This is another variant of phi used when the number of rows or
columns exceeds two levels. It is computed similarly to phi except that the denomi-
nator is N * (k −1), where k is the smaller value of the number of rows or columns.
Out of a concern that the chi-square test is less appropriate when applied to 2 × 2
contingency tables containing cells with low frequencies (because the chi-square distribu-
tion is continuous but frequencies are discontinuous and small frequencies accentuate the
discontinuity), Yates (1934) offered a correction for this lack of continuity. One step in
computing chi-square is to determine the difference between the observed and expected
frequencies in each cell. Yates suggested reducing the difference by 0.5 to correct for the
accentuated discontinuity in low observed frequencies (this correction has increasingly
less effect for increasingly larger frequencies). For many years following the suggestion
by Yates, this correction was routinely recommended for analyses, with observed cell
frequencies ranging between 5 and 10 or so (see Meyers et al. (2009)). However, using
this correction has become the subject of controversy over the past several decades, with

Numerical Example: 2 × 2 Chi-Square
667
several authors suggesting that it should not be used (e.g., Haviland, 1990; Jaccard &
Becker, 1990) because it is too conservative a correction to the computed chi-square
value.
63.2
ANALYSIS STRATEGY
A chi-square analysis can be performed on any (reasonable) sized two-way contingency
table. However, each of the cells must have an expected frequency greater than 1 and
most (e.g., 80% or better) of the cells should have expected frequencies greater than 5.
We will illustrate an analysis of a 2 × 2 table ﬁrst, and then extend that to an analysis of
a 4 × 2 table.
63.3
NUMERICAL EXAMPLE: 2 × 2 CHI-SQUARE
The data we use for this example (selected from IBM SPSS® sample data ﬁles) are risk
factors for 1048 patients at a large HMO. We deal with two of the variables here. For the
variable physically_active, clients were dichotomized into those who were not active
(coded as 0 in the data ﬁle) and those who were active (coded as 1 in the data ﬁle). For
the variable obesity, clients were dichotomized into those whose were not diagnosed as
obese (no, coded as 0 in the data ﬁle) and those who were (yes, coded as 1 in the data
ﬁle). The data ﬁle is named lifestyle medical study. A screenshot of a portion of the
data ﬁle is shown in Figure 63.1.
FIGURE 63.1 A portion of the data ﬁle.

668
TWO-WAY CHI-SQUARE TEST OF INDEPENDENCE
63.4
ANALYSIS SETUP: 2 × 2 CHI-SQUARE
We open lifestyle medical study and from the main menu select Analyze ➔Descriptive
Statistics ➔Crosstabs. This produces the main Crosstabs dialog window shown in
Figure 63.2. We move physically_active into the Row(s) panel to represent our groups
for the purpose of this analysis, and we move obesity into the Column(s) panel to
represent our outcome variable for the purpose of this analysis.
Selecting the Exact tab (some readers may not have this optional module, in which
case the analysis defaults to the Asymptotic solution) opens the Exact Tests screen (see
Figure 63.3) where we have selected Exact. The default time limit to perform these
more memory-intensive computations (compared to the Asymptotic estimate) is set at 5
minutes, but it would take a huge data ﬁle for most personal computers to exceed this
default. Click Continue to return to the main dialog window.
FIGURE 63.2
The main dialog window of Crosstabs.
FIGURE 63.3
The Exact dialog window of Crosstabs.

Analysis Setup: 2 × 2 Chi-Square
669
FIGURE 63.4
The Statistics dialog window of Crosstabs.
FIGURE 63.5
The Cell Display dialog window of Crosstabs.
Selecting the Statistics pushbutton produces the Statistics dialog window shown in
Figure 63.4. We select Chi-square in the upper left area of the screen, and Phi and
Cramer’s V in the Nominal area (for use with categorical variables). Click Continue
to return to the main dialog window.
Selecting the Cells pushbutton produces the Cell Display dialog window shown in
Figure 63.5. In the Counts area, Observed is already checked; we also select Expected.
In the Percentages area, we select Row (this will facilitate in understanding how the
expected frequencies were generated).
In the Residuals area, we select Adjusted standardized. Residuals are the differ-
ences between the observed and expected values. The Unstandardized residuals are the
differences based on the original metric and may not be interpretable if the metrics are
different (e.g., weight in pounds and heart beats in beats per minute). To correct this
problem, the residuals can be Standardized and interpreted as any standard score on
the normal curve with a mean of 0 and a standard deviation of 1. If the Standardized

670
TWO-WAY CHI-SQUARE TEST OF INDEPENDENCE
residual for a particular cell exceeds the absolute value of 1.96, then that cell is associated
with a statistically signiﬁcant difference between the expected and the observed count.
The Adjusted Standardized residual takes into account both the number of compar-
isons made and sample size and reports a more accurate difference between the observed
and expected counts. In a 2 × 2 contingency table, there is only one degree of freedom,
and so the Adjusted Standardized residuals for each of the four cells will have equal
absolute values. If the Adjusted Standardized residual is greater than 1.96, then the
observed frequency is signiﬁcantly greater than the expected frequency; if the Adjusted
Standardized residual is less than −1.96, then the observed frequency is signiﬁcantly
less than the expected frequency.
In designs with more cells, the chi-square test is an omnibus test, and examination
of the Adjusted Standardized residual can inform us of which cells are associated with
statistically signiﬁcant differences between the observed and expected frequencies. Select
Adjusted Standardized, click Continue to return to the main dialog window, and click
OK to perform the analysis.
63.5
ANALYSIS OUTPUT: 2 × 2 CHI-SQUARE
The overall results of the analysis are shown in the Chi-Square Tests table in Figure 63.6.
The Pearson Chi-Square was the original test employed for Crosstabs and is still
commonly reported. The chi-square value is 26.398 and labeled Pearson Chi-Square
by IBM SPSS in honor of its developer, Karl Pearson (1900b). A 2 × 2 table has 1
degree of freedom and, evaluated against its degrees of freedom, the chi-square value
is statistically signiﬁcant (p < .001) whether tested asymptotically (the default output) or
exactly (as we requested). Thus, we would conclude that being physically active and
presenting symptoms of obesity are related to each other (we reject the null hypothesis
of independence).
The results of other tests are also shown in the table. The Continuity Correction
results from applying the Yates correction to the frequencies. A more recently developed
test is the Likelihood Ratio, which for more complex designs (e.g., three-way analyses) is
almost exclusively applied. Fisher’s Exact Test is appropriate for small samples or large
samples where some cells have small or empty counts; here it can be ignored because
none of the cells had small or empty counts. The Linear-by-Linear Association test can
also be ignored here because it assumes the variables are on a continuous scale.
The estimates of the strength of relationship between the variables are shown in
Figure 63.7. With a 2 × 2 design and a statistically signiﬁcant chi-square value indicating
FIGURE 63.6 Chi-square values from the analysis.

ANALYSIS OUTPUT: 2 × 2 CHI-SQUARE
671
FIGURE 63.7 Estimates of the strength of relationship between the variables.
that the two variables are related, we focus on the Phi value of −.159, ignoring the
negative sign, to address the strength of this relationship. This is the correlation between
the two variables, and squaring this value yields .025. We would therefore infer that
the two variables share about 2.5% common variance (or that physical activity accounts
for approximately 2.5% of the variance of obesity). This may be judged to be a weak
relationship in many academic laboratory studies, but it may be clinically important
in applied medical research as the amount of increased risk of obesity from a lack of
physical activity may be worthy of attention (see Chapter 64).
Figure 63.8 presents the observed and expected frequencies together with row per-
centages and adjusted residual values. The bottom row labeled as Total gives us the
strategy to compute our expected frequencies. Of the 1048 clients, 800 (76.3%) of
them were not obese and 248 (23.7%) of them were evaluated as obese. These over-
all or marginal percentages were then applied to the observed frequencies to generate the
expectations. For example, of the 531 not active clients, it was expected based on the
null hypothesis that 76.3% of them would not be obese; this translates to 531 * .763, or
approximately 405, clients.
FIGURE 63.8 The observed and expected frequencies together with row percentages and adjusted
residual values.

672
TWO-WAY CHI-SQUARE TEST OF INDEPENDENCE
With a statistically signiﬁcant chi-square, we anticipate that the Adjusted Stan-
dardized Residuals for each cell (they will be equal in a 2 × 2 table) are statistically
signiﬁcant. In this example, the absolute value of 5.1 can be interpreted as a z score and,
in exceeding an absolute value of 1.96, would be considered statistically signiﬁcant; thus,
(as is true by deﬁnition in a 2 × 2 table when the chi-square is signiﬁcant), the observed
frequency for each cell signiﬁcantly departs from its expected cell frequency. Thus, those
who were not active were more frequently diagnosed as obese than expected, whereas
those who were active were less frequently diagnosed as obese.
63.6
NUMERICAL EXAMPLE: 4 × 2 CHI-SQUARE
We use the same lifestyle medical study data ﬁle but select two different variables for
the present analysis. The variable of age_category codes clients into four categories
based on their age: 45–54 years (coded as 1 in the data ﬁle), 55–64 years (coded as 2
in the data ﬁle), 65–74 years (coded as 3 in the data ﬁle), and 75+ years (coded as 4 in
the data ﬁle). The variable of myocardial_infarction indicates whether clients have not
(no, coded as 0 in the data ﬁle) or have (yes, coded as 1 in the data ﬁle) had a history
of a heart attack.
63.7
ANALYSIS SETUP: 4 × 2 CHI-SQUARE
We open lifestyle medical study and from the main menu select Analyze ➔Descriptive
Statistics ➔Crosstabs. This produces the main Crosstabs dialog window shown in
Figure 63.9. We move age_category into the Row(s) panel to represent our groups for
the purpose of this analysis, and we move myocardial_infarction into the Column(s)
panel to represent our outcome variable for the purpose of this analysis. The analysis is
otherwise conﬁgured in exactly the same way as described in Section 63.4.
FIGURE 63.9
The main dialog window of Crosstabs.

ANALYSIS OUTPUT: 4 × 2 CHI-SQUARE
673
FIGURE 63.10 Omnibus chi-square output.
63.8
ANALYSIS OUTPUT: 4 × 2 CHI-SQUARE
The results of the omnibus analysis are shown in the Chi-Square Tests table in
Figure 63.10. The Pearson Chi-Square value is 36.277. Degrees of freedom for a
two-way contingency table are equal to (number of rows −1) * (number of columns −1);
here that is (4 −1) * (2 −1), or 3 * 1 or 3. Evaluated against 3 degrees of freedom, the
chi-square value is statistically signiﬁcant (p < .001) whether tested asymptotically (the
default output) or exactly (as we requested). Thus, we would conclude that having
a history of a heart attack is related to age. Cramer’s V is .186. Squaring .186
yields a value of approximately .035; it therefore appears that the two variables share
approximately 3.5% of their variance.
Figure 63.11 presents the observed and expected frequencies together with row per-
centages and adjusted residual values. The bottom row labeled as Total gives us the
strategy to compute our expected frequencies. Of the 1048 clients, 765 (73%) of them
had no heart attack history and 283 (27%) of them did have a history of heart attack. This
was then applied to the observed frequencies to generate the expectations. For example,
of the 240 clients who were aged between 45 and 54 years, it was expected based on
the null hypothesis that 73% of them would not have had a heart attack; this translates
to 240 * .73, or approximately 175, clients.
With a statistically signiﬁcant chi-square, we can examine the Adjusted Standard-
ized Residuals for each cell (to “simplify” the omnibus analysis analogous to multiple
comparison tests following an omnibus ANOVA). As these are interpreted as z scores,
we are looking for absolute residuals values of 1.96 or greater to meet an alpha level of
.05. Only two cells do not meet this threshold, those for the age group of 55–64 years;
thus, there were no statistically signiﬁcant differences between the observed and expected
frequencies for these cases. For the other groups, all of the standardized residuals are
greater than 1.96 in absolute value and indicate that the observed and expected frequen-
cies differ signiﬁcantly. Thus, those in the 45–54 years age group had signiﬁcantly fewer
heart attacks than expected, whereas those in the 65–74 and 75+ years age groups had
signiﬁcantly more heart attacks than expected.

674
TWO-WAY CHI-SQUARE TEST OF INDEPENDENCE
FIGURE 63.11 The observed and expected frequencies together with row percentages and
adjusted residual values.
Note that in our example, our expected frequencies were based on the observed
frequencies of the sample as a whole. This served to illustrate how we would perform
the analysis on a contingency table larger than 2 × 2. However, if we were interested
in comparing our sample of those in the 75+ years age group (for example) to the per-
centage based on a national average, we would perform a one-way chi-square analysis
as described in Chapter 62. Such a restructuring implies that we would be examining a
different research question than that was dealt with in our present example.

C H A P T E R
6 4
Risk Analysis
64.1
OVERVIEW
Used extensively in medical and health behavior research but applicable in a variety of
ﬁelds, the 2 × 2 contingency tables provide us with the opportunity to compute indexes
of risk assessment. The two indexes we address here are the risk ratio (also called
relative risk) and the odds ratio. These ratios provide a comparison of two groups in
terms of risk. Because they are both ratios, a risk ratio of 1.00 and an odds ratio of
1.00 both indicate that there is no difference in risk between the groups involved in
the comparison. However, these two indexes have different bases and are interpreted
differently (e.g., Cohen, 2000; Holcomb, Chaiworapongsa, Luke, & Burgdorf, 2001).
Risk represents the likelihood or probability that a condition or event of interest
(positive or negative) central to the main issue of the study will happen or will be
observed (e.g., a patient will experience a heart attack or become free of a certain
debilitating symptom). Risk is a rate of occurrence that is assessed on two groups of
cases. One of the groups can generally be thought of as representing a factor of interest
in the study; for example, the group can represent a “risk” condition or a treatment
group. The other group can be thought of as the comparison, base, or reference condition
representing, for example, a “less risky” condition or a no-treatment group.
A risk ratio is the ratio of two probabilities: (a) the probability of displaying the
condition of interest for the (higher) risk condition and (b) the probability of not display-
ing the condition of interest for the lower risk condition. We often use the probability
associated with the high risk group in the numerator, but this decision depends on the
context of the research study.
Odds themselves are ratios that are not translatable into probabilities. They are deter-
mined by dividing the number of occurrences of an event by the number of times the
event is not observed. For example, if the event occurs thrice and does not occur twice
out of ﬁve possibilities, then the odds are 3 to 2 (a ratio of 1.5) in favor of the event
occurring. We determine the odds for the same two groups that we described for the risk
ratio.
An odds ratio (discussed in Chapters 30 and 32) is the ratio of the odds for each
group (which makes it a ratio of two ratios and rendering it a little more difﬁcult to
conceptualize). We often use the odds associated with the risk group in the numerator,
but again this depends on the context of the research study.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
675

676
RISK ANALYSIS
An odds ratio greater than 1.00 indicates that the odds for the condition of
interest occurring in the risk group are higher relative to the odds for the base or
comparison group. An odds ratio less than 1.00 indicates that the odds for the condition
of interest occurring in the risk group are lower relative to the odds for the base or
comparison group (Harris & Taylor, 2009).
To illustrate how these ratios are generated, consider the table of frequencies shown
in Figure 63.8 (and reproduced in Figure 64.3) where we have crossed physical activity
with obesity. Let us assume that our interest is in the risk of becoming obese. Let us
further assume that one possible factor responsible for obesity is lack of physical activity.
Our most basic question concerns the obesity consequences of physical activity (the risk
of obesity for each physical activity group), given the frequency data collected from the
sample.
The risk ratio is a ratio of the obesity rate or probability of occurrence of obesity for
each physical activity group. The rate of obesity is computed by dividing the number of
observed obese cases by the number of cases in total. Of the 531 cases in the not active
group, 161 were diagnosed as obese. The rate of occurrence (the probability) of obesity
for this group is 161/531, or approximately .303. Of the 517 cases in the active group,
87 were deﬁned as obese. The rate of occurrence of obesity for this group is 87/517,
or approximately .168. Dividing the risk for the “higher risk” (not active) condition
by the risk for the “lower risk” (active) condition yields a risk ratio of .303/.168, or
approximately 1.80. Thus, the risk of becoming obese is approximately 1.8 times higher
for those who are not physically active relative to those who are physically active.
IBM SPSS® will actually calculate the risk for the other outcome as well, which is
in this case, the outcome of not being diagnosed as obese. The analogous reasoning is
as follows. Of the 531 cases in the not active group, 370 were diagnosed as not obese.
The rate of occurrence (the probability) of not being obese for this group is 370/531, or
approximately .697. Of the 517 cases in the active group, 430 were deﬁned as not obese.
The rate of occurrence of not being obese for this group is 430/517, or approximately
.832. Dividing the risk for the “higher risk” (not active) condition by the risk for the
“lower risk” (active) condition yields a risk ratio of .697/.832, or approximately .83.
Thus, the “risk” of not being obese is approximately .83 times as great (it is lower)
for those who are not physically active than for those who are physically active. This
outcome, with its verbalization as a double negative, is probably a less elegant and clear
but an equally valid way to communicate the results of the study.
The odds ratio is a ratio of the odds of being diagnosed as obese for each physical
activity group. Odds are computed by dividing the number of cases with the target
outcome (those who are obese) by the number of cases with the alternative outcome.
For the not active group, 161 were diagnosed as obese and 370 were diagnosed as not
being obese. The odds of obesity in this not active group are 161/370, or .435. For the
active group, 87 were diagnosed as obese and 430 were diagnosed as not being obese.
The odds of obesity in this active group are 87/430, or .202.
Dividing the odds for the “higher risk” condition by the odds for the “lower risk”
condition yields an odds ratio of 2.153. Thus, the odds of being diagnosed as obese for
those who are not physically active is over twice as great as the odds of being diagnosed
as obese for those who are physically active. IBM SPSS actually calculates the other
side of this coin by dividing the group coded as 0 (our “lower risk” condition) by the
group coded as 1 (our “higher risk” condition). This computation yields an odds ratio of
0.464 (it is the inverse of 2.153 in that 1/2.153 = .464) and indicates that the odds of
being diagnosed as obese for those who are physically active are about half as great as
the odds for those who are not physically active being diagnosed as obese.
The differences between how risk ratios and odds ratios are computed link to their
application in different types of research designs (MacDonald, 2012; Schlesselman,
1992). Generally, studies that are prospectively (future) oriented are conducive to the

ANALYSIS OUTPUT
677
use of the risk ratio in that the risk ratio (as an index of incidence or probability of
an event occurring) makes sense when we are projecting what the future might hold in
store. In a cohort study, for example, a sample of cases is selected and measured or
followed over time often to determine if a certain medical condition surfaces, and a risk
ratio would be an appropriate index of risk in such a study.
On the other hand, studies that are retrospective (past) oriented are not conducive
to the use of the risk ratio, and we must revert to the odds ratio instead. For example,
in a case control study, cases with the condition of interest are selected and compared
to those who do not have the condition. Often, such a study is carried out on archival
records (an existing data ﬁle). The incidence rate of the condition is not applicable here
in that the cases were selected because they either had the condition or not. With the
incidence rate not applicable in such studies because the cases were selected on the basis
of having or not having the particular condition, we cannot meaningfully interpret the
risk ratio; instead, we use the odds ratio as our gauge of risk.
The risk ratio and the odds ratio calculated on the same 2 × 2 contingency table
will both be either greater than or less than 1.00, although their absolute values and
interpretation will differ. As pointed out by Merrill (2013), the odds ratio will generally
overestimate the strength of the relationship between the variables. More speciﬁcally,
the odds ratio will approximate the value of the risk ratio when the condition of interest
occurs relatively infrequently (in the range 10% or less), but the values of the two ratios
will tend to be different for relatively more frequent events (Harris & Taylor, 2009).
64.2
NUMERICAL EXAMPLE
We use two of the variables here as we did in the ﬁrst analysis presented in Chapter 63.
For the variable physically_active, clients were dichotomized into those who were not
active (coded as 0 in the data ﬁle) and those who were active (coded as 1 in the data
ﬁle). For the variable obesity, clients were dichotomized into those who were not obese
(no, coded as 0 in the data ﬁle) and those whose were (yes, coded as 1 in the data ﬁle).
The data ﬁle is named lifestyle medical study.
64.3
ANALYSIS SETUP
We open lifestyle medical study and from the main menu select Analyze ➔Descriptive
Statistics ➔Crosstabs. This produces the main Crosstabs dialog window shown in
Figure 64.1. We conﬁgure this analysis exactly as we did in Chapter 63 but with one
exception: in the Statistics window, in addition to selecting Chi-square and Phi and
Cramer’s V, we select Risk. This is shown in Figure 64.2.
64.4
ANALYSIS OUTPUT
The overall results of the analysis are identical to those shown in Chapter 63, and so we
show only the table of observed and expected frequencies in Figure 64.3. What is new
with this analysis is the Risk Estimate table shown in Figure 64.4.
The two risk ratios are provided in the second and third rows of the table. We have
already described how these values were calculated and how they are interpreted.
• The row labeled For cohort obesity = 0 no focuses on the absence of obesity.
Dividing the risk for the “higher risk” (not active) condition by the risk for the
“lower risk” (active) condition yields a risk ratio of .697/.832, or approximately
.838.

678
RISK ANALYSIS
FIGURE 64.1
The main dialog window of Crosstabs.
FIGURE 64.2
The Statistics dialog window of Crosstabs.
• The row labeled For cohort obesity = 1 yes focuses on the occurrence of obesity.
Dividing the risk for the “higher risk” (not active) condition by the risk for the
“lower risk” (active) condition yields a risk ratio of .303/.168, or approximately
1.802.
The output also provides the lower and upper values for the 95% conﬁdence interval
for each ratio, as it is appropriate for researchers to provide these when reporting the
risk ratio.
Only one odds ratio is computed with the row label of Odds Ratio for physical-
ly_active (0 not active / 1 active). Given the label, it may be clear that this computation
uses odds for the not active group in the numerator, focusing on the lack of obesity (the
0 code) rather than the occurrence of obesity. We can calculate this odds ratio in two
ways.

ANALYSIS OUTPUT
679
FIGURE 64.3 The observed and expected frequencies together with row percentages and adjusted
residual values.
FIGURE 64.4
Risk analysis results.
• We can say that the odds of not being obese for the not active group are 370/161,
or approximately 2.298, and the odds for not being obese for the active group are
430/87, or approximately 4.943. Dividing 2.298 by 4.943 yields an odds ratio of
approximately .465.
• We can say that the odds of being obese for the not active group are 161/370, or
approximately .435, and the odds for being obese for the active group are 87/430,
or approximately .202. Dividing .202 by .435 yields an odds ratio of approximately
.465.
Whichever way we view it, the odds of developing obesity for those who are phys-
ically active are a little less than half the odds of developing obesity for those who are
not physically active. Again, the table provides the lower and upper values for the 95%
conﬁdence interval, as it is appropriate for researchers to provide these when reporting
the odds ratio.


C H A P T E R
6 5
Chi-Square Layers
65.1
OVERVIEW
One way to enrich the information we obtain in a two-way chi-square design is to
superimpose the presence of a third categorical variable, referred to as a layer in IBM
SPSS®. This third variable serves as a stratifying variable; that is, we examine our
ordinary two-way contingency table for each category of the layer variable. For example,
we could use a variable indicating whether the person was a smoker or not in our
obesity and physical activity study. In such a situation, we would obtain separate chi-
square analyses (physical activity by obesity) for smokers, nonsmokers, and the combined
sample.
65.2
NUMERICAL EXAMPLE
We use the same two variables here as we did in the analysis presented in Chapter 64.
For the variable physically_active, clients were dichotomized into those who were not
active (coded as 0 in the data ﬁle) and those who were active (coded as 1 in the data
ﬁle). For the variable obesity, clients were dichotomized into those who were not obese
(no, coded as 0 in the data ﬁle) and those who were (yes, coded as 1 in the data ﬁle).
For this analysis, we now add the binary coded variable of smoker, coded as 0 for no
and 1 for yes. The data ﬁle is named lifestyle medical study.
65.3
ANALYSIS SETUP
We open lifestyle medical study and from the main menu select Analyze ➔Descriptive
Statistics ➔Crosstabs. This produces the main Crosstabs dialog window shown in
Figure 65.1. We move physically_active into the Row(s) panel and obesity into the
Column(s) panel as we have done in previous analyses. However, we also move smoker
into the Layer panel as shown in Figure 65.1.
The remaining setup is the same as we described in Chapter 64. Brieﬂy, choose the
Exact pushbutton (if it is available) and select Exact (see Figure 65.2). In the Statistics
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
681

682
CHI-SQUARE LAYERS
FIGURE 65.1
The main dialog window of Crosstabs including
a Layer variable.
FIGURE 65.2
The Exact dialog window of Crosstabs.
dialog window shown in Figure 65.3, we select Chi-square and Phi and Cramer’s V in
the Nominal area, and Risk. In the Cell Display dialog window shown in Figure 65.4,
we select Observed, Expected, Row Percentages, and Adjusted standardized. Click
OK to perform the analysis.
65.4
ANALYSIS OUTPUT
Figure 65.5 presents the chi-square values for the layer analysis. The bottom major row
of the Chi-Square Tests table is a full-sample result (labeled as Total) and is identical to
the results described in Chapter 63; the Pearson Chi-Square is 26.398 and is statistically
signiﬁcant (p < .001). But in the layer analysis, this overall analysis is partitioned into
the two levels of smoker, and these results are shown in the ﬁrst two major rows of the
table.

ANALYSIS OUTPUT
683
FIGURE 65.3
The Statistics dialog window of Crosstabs.
FIGURE 65.4
The Cell Display dialog window of Crosstabs.
The ﬁrst major row provides the results for those cases (N = 826) who indicated
that they did not smoke (coded as 0 in the data ﬁle). For this group, the Pearson Chi-
Square was 29.051 and with one degree of freedom (this is still a 2 × 2 contingency
table) it is statistically signiﬁcant (p < .001) based on either an exact two-tail test of
signiﬁcance or an asymptotic test. Thus, there is a relationship between the variables of
physically_active and obesity for nonsmokers.
The second major row provides the results for those cases (N = 222) who indicated
that they did smoke (coded as 1 in the data ﬁle). For this group, the Pearson Chi-Square
was 0.718 and with one degree of freedom it is not statistically signiﬁcant based on either
an exact two-tail test of signiﬁcance (p = .444) or an asymptotic test (p = .397). Thus,
the variables of physically_active and obesity appear to be independent of each other
for nonsmokers.
Estimates of the strength of the relationship between the variables are shown in
Figure 65.6, with the same major partitions in the table. The phi coefﬁcient for the

684
CHI-SQUARE LAYERS
FIGURE 65.5 Chi-square values from the analysis.
FIGURE 65.6 Estimates of the strength of relationship between the variables.

ANALYSIS OUTPUT
685
FIGURE 65.7 The observed and expected frequencies together with row percentages and adjusted
residual values.

686
CHI-SQUARE LAYERS
overall (Total) analysis is again identical to what we presented in Chapter 63, but we
also obtain separate phi coefﬁcients for nonsmokers and smokers. For nonsmokers, we
know that the chi-square value was statistically signiﬁcant and so we can move forward
with these results. Squaring the value of .188 (and ignoring the negative valence) yields
a phi square value of approximately .035. We may thus conclude that for nonsmokers,
physical activity accounts for approximately 3.5% of the variance of obesity. For smokers,
we learned that the chi-square value was not statistically signiﬁcant, and so we know that
the phi coefﬁcient (computed to be −.057) is not statistically different from zero (there
is no relationship between the variables).
The observed and expected frequencies together with row percentages and adjusted
residual values in Figure 65.7, again structured by the Total analysis (duplicating what
we have seen in Chapter 63) and by levels of smoker. All expected frequencies are based
on the Total percentages, as they represent a common frame of reference.
The nonsmoker frequencies were associated with a statistically signiﬁcant chi-square,
and we can see in those cells in what way the two variables are related for this group.
For those who are not active, approximately two-thirds are not obese and one-third are
obese, but for the cases who are physically active, approximately 85% are not obese,
whereas only approximately 15% are obese.
We obtained a nonsigniﬁcant chi-square for the smokers, indicating that physical
activity was not related to obesity for these cases. The cell frequencies show us that,
regardless of whether the cases are or are not physically active, about three-quarters of
the cases are not obese and about one-quarter of the cases are obese.
Because each of the partitions is still based on a 2 × 2 contingency table, we are
able to perform a risk analysis, the results of which are presented in Figure 65.8. Again,
FIGURE 65.8
Risk analysis results.

ANALYSIS OUTPUT
687
the outcome for the Total sample is exactly what we obtained earlier. But in this layer
analysis, we also obtain risk estimates for each level of smoker.
The chi-square analysis indicated that there was no relationship between the variables
for smokers. Thus, with respect to obesity, it did not matter whether cases were physically
active or not. It then follows that both the risk ratio and the odds ratio should be effectively
1.00. As can be seen from the Risk Estimate table, these values are not exactly 1.00,
and there is no test of statistical signiﬁcance for these indexes. However, IBM SPSS
has provided us with an invaluable piece of information, namely, the lower and upper
limits of the 95% conﬁdence interval. Note that the conﬁdence interval for all three of
the smoker risk estimates subsumes the value 1.00, and so we can indirectly assert that
the obtained ratios could reasonably have been 1.00.
The results for nonsmokers were statistically signiﬁcant, and so the risk ratios and
odds ratio are viable (they are not effectively 1.00). The values shown in the table are
not markedly different from those representing the Total sample and suggest that the
risk of becoming obese is better than two times higher for those who are not physically
active than for those who are physically active.


C H A P T E R
6 6
Hierarchical Loglinear Analysis
66.1
OVERVIEW
Loglinear analysis is an extension of a two-way Crosstabs analysis in that we cross
three (or more) categorical variables together (e.g., Variables A, B, and C), but the focus
is still on the differences between the observed and expected frequencies. In the two-
way Crosstabs analysis, a statistically signiﬁcant chi-square value indicates that the two
variables are not independent of each other; that is, they are related in the sense that
it is the particular combinations of their levels that are associated with (predictive of)
different frequencies. This is tantamount to saying that there is a statistically signiﬁcant
interaction between the two variables. We tend not to verbalize the relationship in this
way because there are just two variables in the analysis and it is ordinarily sufﬁcient to
say that they are related.
With three or more variables, however, there are many combinations of (relationships
among) the variables that could be associated with differences between the observed and
expected frequencies, and so it becomes convenient to use the ANOVA terminology and
refer to these combinations as interactions. With three variables in our study, we therefore
can speak of a three-way interaction or of two-way interactions. We use loglinear analysis
to assess these relationships among the variables (Fienberg, 1994; Knoke, & Burke, 1980).
Loglinear analysis thus bears a resemblance to factorial ANOVA; it can also be (under
certain circumstances) transformed into a logistic regression design. Brieﬂy, the ties to
these other designs are as follows:
• Loglinear analysis resembles factorial ANOVA designs in that the various combi-
nations of variables (e.g., AB, ABC) are thought of as interactions. In a three-way
loglinear design, for example, we can have one three-way interaction, three two-
way interactions, and three main effects. In a loglinear design, a statistically
signiﬁcant interaction indicates that the observed frequencies result from particular
combinations of the levels of the variables involved. But unlike ANOVA, there
is no dependent variable per se (in an ANOVA sense), as it is the frequency of
occurrence of various combinations of the levels of the variables that is the target
of the analysis.
• Logistic regression can be invoked if one of the variables does indeed appear to
better assume the role of dependent variable. As would be true for any regression
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
689

690
HIERARCHICAL LOGLINEAR ANALYSIS
analysis, we could evaluate the effectiveness of the main effects and interactions
of the other variables to predict the occurrence of the dependent variable (e.g.,
coded as 0 for absent and 1 for observed).
The general approach of a hierarchical loglinear analysis is to evaluate the ﬁt of
models containing all or some of the effects (interactions and main effects) to predict
the observed frequencies. Assessing model differences focuses on the deviations (differ-
ences) between the observed frequencies and the expected frequencies. Because of the
complexity of the calculations in dealing with the increased number of relationships over
the two-way classiﬁcation design, loglinear analysis transforms the frequency value in
the cells (count data) to its natural log.
A natural logarithm uses the base e (with a value of approximately 2.72) in its
computation, and the natural log of a number is the power to which 2.72 needs to be
raised to achieve it. For example, the natural log of 7.398 is 2 because 2.72 raised to the
second power produces a value of 7.398. If a cell had a frequency of 44, then its natural
log is 3.78 (computed as 2.723.78 = 44). This transformation from frequency to natural
logs produces interval-level data that are appropriate for linear regression analysis (thus
“log” and “linear” are joined in the name “loglinear” analysis).
As a simpliﬁed description, the full loglinear model is an equation (analogous to
a multiple regression equation) that predicts expected cell frequencies as a function of
terms representing the observed frequencies associated with the interactions and main
effect terms. The algorithm attempts to calculate expected scores that are as close to the
observed scores as possible; that is, the residuals or differences between the predicted
(expected) score and the actual (observed) score are as small as possible. The smaller
the residuals, the better the model ﬁts the data.
Model ﬁt is assessed by the chi-square test. If a model ﬁts the data well, the results
of the chi-square will be nonsigniﬁcant. In the context of hierarchical loglinear analysis,
it is recommended that the p values exceed .10 to be acceptable (Knoke & Burke, 1980).
The important point is that a nonsigniﬁcant chi-square supports the plausibility of the
model. The proposed model should predict scores that are not statistically signiﬁcantly
different from the observed data, thus ﬁtting the model to the data.
The hierarchical strategy is the most popular variation of loglinear analysis (Fienberg,
1994). The process begins with the full or saturated model containing all of the effects.
In a three-way design, which is what we cover here, this model subsumes the three-way
interaction, all of the two-way interactions, and all of the main effects. We then proceed
in an exploratory fashion to search for the best ﬁtting model. The analysis is hierarchical
in the sense that if an interaction term is included in a model, then all of the lower
order interactions and main effects involving those variables must also be included. For
example, if the model contains the A × B × C term, then all two-way terms (A × B,
A × C, and B × C) and all main effects (A, B, and C) must also remain in the model.
Including these lower order associations is necessary because in most applications, it is
not meaningful to include higher order interactions without including the lower order
interactions (Agresti, 2002).
To identify which of the hierarchical associations are statistically related, a system-
atic backward deletion process is applied to the saturated model with the highest order
association removed ﬁrst. The saturated model will predict the cell frequencies perfectly
(there will be no residuals), and so the issue becomes one of determining if a simpliﬁed
model can do virtually as well, that is, determining if a simpliﬁed model can ﬁt the data
as indicated by a statistically nonsigniﬁcant chi-square value. The strategy used in a hier-
archical loglinear analysis is one of systematically deleting the highest order association
and proceeding to the lower order effects; such a strategy is referred to as Generating
Class.

ANALYSIS SETUP
691
The backward deletion process starts by removing the three-way interaction from
the model and comparing that model to the saturated model using a chi-square statistic
to assess if the two models are statistically signiﬁcantly different. If the result of the
chi-square is statistically signiﬁcant, then the three-way interaction is necessary in ﬁtting
the model and the analysis is completed. However, if the result of the chi-square is not
signiﬁcant (i.e., the reduced model ﬁts the data virtually as well as the saturated model),
then the three-way interaction is not necessary in ﬁtting the model and can be removed.
If the three-way interaction is removed, then the model remaining contains all of
the two-way interactions along with the main effects. Because there are three two-way
interactions, each of the two-way interactions is considered for deletion. The two-way
interactions that achieve statistical signiﬁcance remain in the model (because their dele-
tion would cause a signiﬁcant drop in ﬁt), as well as the main effects of individual
variables comprising the interactions. The two-way interactions that are not signiﬁcant
are deleted from the model, as well as the main effects of the individual variables.
66.2
NUMERICAL EXAMPLE
The data ﬁle is named lifestyle medical study, a data ﬁle we have used in previous
chapters. For the variable physically_active, clients were dichotomized into those who
were not active (coded as 0 in the data ﬁle) and those who were active (coded as 1 in
the data ﬁle). For the variable obesity, clients were dichotomized into those who were
not obese (no, coded as 0 in the data ﬁle) and those who were (yes, coded as 1 in the
data ﬁle). The variable hist_myocardial_infarction represents clients who did not have
(no, coded as 0 in the data ﬁle) and did have (yes, coded as 1 in the data ﬁle) a history
of heart attack. Thus, we have a 2 × 2 × 2 design.
66.3
ANALYSIS SETUP
We open lifestyle medical study and from the main menu select Analyze ➔Loglinear
➔Model Selection. This produces the main Loglinear Model Selection dialog win-
dow shown in Figure 66.1. We select the three variables to be included in the analysis
FIGURE 66.1
The main Loglinear Model Selection dialog
window.

692
HIERARCHICAL LOGLINEAR ANALYSIS
FIGURE 66.2
The Deﬁne Range window of Loglinear Model Selection.
FIGURE 66.3
The main Loglinear Model Selection dialog
window with the ranges now deﬁned.
(physically_active, obesity, and hist_myocardial_infarction) and move them to the
Factor(s) panel. When they are placed in the Factor(s) panel, the expression (? ?)
appears after each variable; this serves as a visual reminder that we must deﬁne the
range of codes for each. To facilitate the deﬁning of range for our variables, we have
highlighted all three (because they are coded identically).
Selecting the Deﬁne Range pushbutton opens the Deﬁne Range dialog window. As
seen in Figure 66.2, we are asked to specify the Minimum and Maximum code values.
What we type in will apply to all highlighted variables in the Factor(s) panel of the main
dialog window. As all three are coded in the same way, and as all three are highlighted,
we can supply the codes 0 and 1 as seen in Figure 66.2. Selecting Continue returns
us to the main dialog window where the codes are now associated with each variable
(see Figure 66.3). We also retain the default of Use backward elimination under Model
Building.
Selecting the Options pushbutton opens the Options dialog window shown in
Figure 66.4. We retain the Display defaults of Frequencies and Residuals but change
the Delta value (a value added to the cell frequencies for saturated models) from the
default of .5–0.
66.4
ANALYSIS OUTPUT
The Cell Counts and Residuals output in Figure 66.5 presents the results of the saturated
model. This model contains all of the effects (the three-way interaction, all two-way

ANALYSIS OUTPUT
693
FIGURE 66.4
The Options window of Loglinear Model Selection.
FIGURE 66.5
Observed and Expected cell frequencies and Residuals.
FIGURE 66.6
The Goodness-of-Fit Tests output.
interactions, and the main effects). It perfectly ﬁts the data, as shown by the fact that the
Residuals (the differences between expectations based on the model and the observed
frequencies), are equal to zero. It is this model that will be subjected to backward
elimination to detect any signiﬁcant hierarchical associations.
Figure 66.6 shows the Goodness-of-Fit Tests for the saturated model. Because there
are no residuals, the Likelihood Ratio and the chi-square value (shown as Pearson) are
both zero; neither can be meaningfully evaluated for statistical signiﬁcance.
The K-Way and Higher-Order Effects table is shown in Figure 66.7 and provides
a global view of the results of the analysis. The value of K in the rows of the table
represents main effects (shown as 1), two-way interactions (shown as 2), and three-way
interactions (shown as 3), and the degrees of freedom (df) provides a count of the number
of effects involved.

694
HIERARCHICAL LOGLINEAR ANALYSIS
FIGURE 66.7 The K-Way and Higher-Order Effects table.
The ﬁrst major row carries the same name as provided for the entire table and is
divided into a set of three rows. The following is the message conveyed in the upper half
of the table: If these effects and all higher order effects are removed from the model, then
here are the consequences. For example, the ﬁrst row labeled as 1 shows the consequences
of removing all of the main effects and all higher order effects (i.e., everything) from
the model; as seen by the df, we are talking about removing 7 effects (three main
effects + three two-way interactions + one three-way interaction). Not surprisingly, the
consequences are a statistically signiﬁcant chi-square (745.313), informing us that the
degree of predicting the cell frequencies is signiﬁcantly worse than that in the saturated
model. This suggests that at least one of those seven effects is sufﬁciently important that
it needs to be included in the model.
As we can see from the table, we would draw the same conclusion regarding K = 2;
removing all two-way interactions and the three-way interaction (removing 4 effects)
from the model also signiﬁcantly deteriorates our prediction prowess. Thus, at least one
of these four effects is needed for prediction.
Finally, the results for row 3 indicate that removing the single three-way interaction
from the model yields a nonsigniﬁcant chi-square (p = .684); thus, removing this single
effect does not signiﬁcantly lower our prediction of cell frequencies and it therefore
becomes expendable. We may therefore infer that at least one of the two-way interactions
was driving the results in K = 2, as it is now clear that the three-way effect is nonessential.
The second major row of the K-Way and Higher-Order Effects table examines
the same global scenarios but without packaging the higher order effects. Thus, the row
for K = 1 deals with removing the main effects only, the row for K = 2 deals with
removing the two-way interaction effects only, and row for K = 3 deals with removing
the single three-way interaction effect from the model. The results here are similar to
those in the ﬁrst row. Speciﬁcally, removing from the model the main effects and the
two-way interactions signiﬁcantly decreases our prediction ability but removing the three-
way interaction does not signiﬁcantly decrease prediction (this third ﬁnding duplicates
the information we obtained in the ﬁrst major row because the three-way interaction is
the highest order effect in the model).
With the global results now understood, we are in a position to examine the con-
sequences of removing the speciﬁc effects. Even with the three-way interaction able
to be removed, retaining one or more of the two-way interactions in this hierarchical
approach will automatically retain the main effects associated with them as well. The
detailed analysis dealing with this is shown in the Step Summary table in Figure 66.8.
The Generating Class in each step is the set of effects under consideration for deletion.

FIGURE 66.8 The Step Summary table.
695

696
HIERARCHICAL LOGLINEAR ANALYSIS
FIGURE 66.9
The Convergence Information table.
FIGURE 66.10
Cell Counts and Residuals output.
At the start of the process (Step 0), we consider only the three-way interaction. As
we have already seen, removing that effect from the model does not signiﬁcantly reduce
prediction (p = .684), and so it is deleted.
The Generating Class for the next step (Step 1) is the set of two-way interactions.
We begin this step with them all in the model (Generating Class in the top portion of
the Step 1 row), and the Sig. column just repeats the information from the row above it,
thus letting us know that prediction is not signiﬁcantly reduced (p = .684). Now each
individual two-way effect is evaluated. As can be seen, removing any of them results
in a statistically signiﬁcant chi-square (p < .001), indicating that they are all making a
statistically signiﬁcant contribution. Thus, there is no basis to remove them from the
model.
Step 2 would ordinarily consider the main effects not subsumed in the signiﬁcant
interactions. But with all two-way effects in the model, and with the model being struc-
tured in a hierarchical manner, the main effects are all subsumed in the interactions and
so are not eligible for removal. Therefore, the backward elimination process ends with
just the three-way effect removed.
In summary, the ﬁnal model includes all the main effects and all the two-way inter-
actions. This means that the combinations of (a) physically_active and obesity, (b)
physically_active and myocardial_infarction, and (c) obesity and myocardial_infarc-
tion are signiﬁcant predictors of the frequencies.
Figure 66.9 displays the Convergence Information. This is a summary of the Gen-
erating Class consisting of the three two-way interaction effects.
The Cell Counts and Residuals are shown in Figure 66.10. A residual is the dif-
ference between the observed and expected frequencies. In the table, the residuals are
relatively small because the model ﬁts the data.

THE NEXT STEPS
697
FIGURE 66.11
The Goodness-of-Fit Tests output.
The Goodness-of-Fit Tests table presented in Figure 66.11 informs us whether there
are signiﬁcant differences between the expected and the observed frequencies. IBM
SPSS® generates both the Likelihood Ratio and the Pearson. If the model ﬁts the data,
the Goodness-of-Fit Tests will be nonsigniﬁcant. Because of the complicated calculations
involved with assessing model ﬁt, researchers generally prefer to report the Likelihood
Ratio rather than the traditional Pearson chi-square (Read & Cressie, 1988). In our
example, both tests indicated nonsigniﬁcance (p = .684), suggesting that ﬁnal model
adequately ﬁts the data.
66.5
THE NEXT STEPS
If the three-way interaction signiﬁcantly contributed to prediction, the table of frequencies
shown in Figure 66.10 would represent the ﬁnal solution. However, with the three-way
interaction unnecessary but with all of the two-way interactions as signiﬁcant predictors,
it would be a reasonable next step to perform a series of two-way chi-square analyses
on each interaction effect. These would mirror the analyses described in Chapter 63.
Furthermore, if any of the two-way interactions represented a 2 × 2 array and if the
research context and variables warranted, then an analysis of risk as described in Chapter
64 should also be performed.


A P P E N D I X
Statistics Tables
TABLE A.1
Selected Critical Values of Chi-Square Distribution with Degrees
of Freedom
df
p = .10
p = .05
p = .025
p = .01
p = .001
1
2.706
3.841
5.024
6.635
10.828
2
4.605
5.991
7.378
9.210
13.816
3
6.251
7.815
9.348
11.345
16.266
4
7.779
9.488
11.143
13.277
18.467
5
9.236
11.070
12.833
15.086
20.515
6
10.645
12.592
14.449
16.812
22.458
7
12.017
14.067
16.013
18.475
24.322
8
13.362
15.507
17.535
20.090
26.125
9
14.684
16.919
19.023
21.666
27.877
10
15.987
18.307
20.483
23.209
29.588
11
17.275
19.675
21.920
24.725
31.264
12
18.549
21.026
23.337
26.217
32.910
13
19.812
22.362
24.736
27.688
34.528
14
21.064
23.685
26.119
29.141
36.123
15
22.307
24.996
27.488
30.578
37.697
16
23.542
26.296
28.845
32.000
39.252
17
24.769
27.587
30.191
33.409
40.790
18
25.989
28.869
31.526
34.805
42.312
19
27.204
30.144
32.852
36.191
43.820
20
28.412
31.410
34.170
37.566
45.315
21
29.615
32.671
35.479
38.932
46.797
22
30.813
33.924
36.781
40.289
48.268
23
32.007
35.172
38.076
41.638
49.728
24
33.196
36.415
39.364
42.980
51.179
25
34.382
37.652
40.646
44.314
52.620
26
35.563
38.885
41.923
45.642
54.052
27
36.741
40.113
43.195
46.963
55.476
28
37.916
41.337
44.461
48.278
56.892
29
39.087
42.557
45.722
49.588
58.301
30
40.256
43.773
46.979
50.892
59.703
(continued)
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
699

700
STATISTICS TABLES
TABLE A.1
(Continued)
df
p = .10
p = .05
p = .025
p = .01
p = .001
31
41.422
44.985
48.232
52.191
61.098
32
42.585
46.194
49.480
53.486
62.487
33
43.745
47.400
50.725
54.776
63.870
34
44.903
48.602
51.966
56.061
65.247
35
46.059
49.802
53.203
57.342
66.619
36
47.212
50.998
54.437
58.619
67.985
37
48.363
52.192
55.668
59.893
69.347
38
49.513
53.384
56.896
61.162
70.703
39
50.660
54.572
58.120
62.428
72.055
40
51.805
55.758
59.342
63.691
73.402
41
52.949
56.942
60.561
64.950
74.745
42
54.090
58.124
61.777
66.206
76.084
43
55.230
59.304
62.990
67.459
77.419
44
56.369
60.481
64.201
68.710
78.750
45
57.505
61.656
65.410
69.957
80.077
46
58.641
62.830
66.617
71.201
81.400
47
59.774
64.001
67.821
72.443
82.720
48
60.907
65.171
69.023
73.683
84.037
49
62.038
66.339
70.222
74.919
85.351
50
63.167
67.505
71.420
76.154
86.661
51
64.295
68.669
72.616
77.386
87.968
52
65.422
69.832
73.810
78.616
89.272
53
66.548
70.993
75.002
79.843
90.573
54
67.673
72.153
76.192
81.069
91.872
55
68.796
73.311
77.380
82.292
93.168
56
69.919
74.468
78.567
83.513
94.461
57
71.040
75.624
79.752
84.733
95.751
65
79.973
84.821
89.177
94.422
105.988
66
81.085
85.965
90.349
95.626
107.258
67
82.197
87.108
91.519
96.828
108.526
68
83.308
88.250
92.689
98.028
109.791
69
84.418
89.391
93.856
99.228
111.055
70
85.527
90.531
95.023
100.425
112.317
71
86.635
91.670
96.189
101.621
113.577
72
87.743
92.808
97.353
102.816
114.835
73
88.850
93.945
98.516
104.010
116.092
74
89.956
95.081
99.678
105.202
117.346
75
91.061
96.217
100.839
106.393
118.599
76
92.166
97.351
101.999
107.583
119.850
77
93.270
98.484
103.158
108.771
121.100
78
94.374
99.617
104.316
109.958
122.348
79
95.476
100.749
105.473
111.144
123.594
80
96.578
101.879
106.629
112.329
124.839
81
97.680
103.010
107.783
113.512
126.083
82
98.780
104.139
108.937
114.695
127.324
83
99.880
105.267
110.090
115.876
128.565
84
100.980
106.395
111.242
117.057
129.804
85
102.079
107.522
112.393
118.236
131.041
86
103.177
108.648
113.544
119.414
132.277
87
104.275
109.773
114.693
120.591
133.512
88
105.372
110.898
115.841
121.767
134.746
89
106.469
112.022
116.989
122.942
135.978
90
107.565
113.145
118.136
124.116
137.208

APPENDIX
701
TABLE A.1
(Continued)
df
p = .10
p = .05
p = .025
p = .01
p = .001
91
108.661
114.268
119.282
125.289
138.438
92
109.756
115.390
120.427
126.462
139.666
93
110.850
116.511
121.571
127.633
140.893
94
111.944
117.632
122.715
128.803
142.119
95
113.038
118.752
123.858
129.973
143.344
96
114.131
119.871
125.000
131.141
144.567
97
115.223
120.990
126.141
132.309
145.789
98
116.315
122.108
127.282
133.476
147.010
99
117.407
123.225
128.422
134.642
148.230
100
118.498
124.342
129.561
135.807
149.449
http://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm.
National Institute of Standards and Technology. The NIST/SEMATECH Engineering Statistics Handbook, Web
Based statistics handbook.
Developed as a joint partnership between the Statistical Engineering Division of NIST and the Statistical
Methods Group of SEMATECH.
http://www.itl.nist.gov/div898/handbook/ for complete handbook.


REFERENCES
Agresti, A. (2002). Categorical data analysis (2nd ed.). New York, NY: John Wiley &
Sons, Inc.
American Psychiatric Association. (2000). Diagnostic and statistical manual of mental
disorders (4th ed., Text Revision). Washington, DC: Author.
American Psychological Association. (2009). Publication manual (6th ed.). Washington,
DC: Author.
Aroian, L. A. (1947). The probability function of the product of two normally distributed
variables. Annals of Mathematical Statistics, 18, 265–271.
Baron, R. M., & Kenny, D. A. (1986). The moderator-mediator variable distinction
in social psychological research: Conceptual, strategic, and statistical considerations.
Journal of Personality and Social Psychology, 51, 1173–1182.
Bentler, P. M. (1990). Comparative ﬁt indexes in structural models. Psychological
Bulletin, 107, 238–246.
Berkman, E. T., & Reise, S. P. (2012). A conceptual guide to statistics using SPSS .
Thousand Oaks, CA: Sage.
Bickel, R. (2007). Multilevel analysis for applied research. New York, NY: Guilford
Press.
Blau, P., & Duncan, O. (1967). The American occupational structure. New York, NY:
John Wiley & Sons, Inc.
Bollen, K. A. (1989). Structural equations with latent variables. New York, NY: John
Wiley & Sons, Inc.
Bollen, K. A. (2002). Latent variables in psychology and the social sciences. Annual
Review of Psychology, 53, 605–634.
Brown, T. A. (2006). Conﬁrmatory factor analysis for applied research. New York, NY:
Guilford Press.
Brown, T. A., & Moore, M. T. (2012). Conﬁrmatory factor analysis. In R. H. Hoyle (Ed.),
Handbook of structural equation modeling (pp. 361–379). New York, NY: Guilford
Press.
Brown, W. (1910). Some experimental results in the correlation of mental abilities. British
Journal of Psychology, 3, 296–322.
Browne, M. W., & Cudeck, R. (1989). Single sample cross-validation indices for covari-
ation structures. Multivariate Behavioral Research, 24, 445–455.
Browne, M. W., & Cudeck, R. (1993). Alternative ways of assessing model ﬁt. In
K. A. Bollen & J. S. Long (Eds.), Testing structural equation models (pp. 136–162).
Newbury Park, CA: Sage.
Budescu, D. V. (1996). Dominance analysis: A new approach to the problem of relative
importance of predictors in multiple regression. Psychological Bulletin, 114, 542–551.
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
703

704
REFERENCES
Byrne, B. M. (2010). Structural equation modeling with AMOS: Basic concepts, applica-
tions and programming (2nd ed.). New York, NY: Routledge.
Carroll, J. D., & Chang, J. J. (1970). Analysis of individual differences in multidi-
mensional scaling via an N-way generalization of “Eckart-Young” decomposition.
Psychometrika, 35, 238–319.
Cattell, R. B. (1966). The scree test for the number of factors. Multivariate Behavioral
Research, 1, 245–276.
Clark, L. A., & Watson, D. (1995). Construction validity: Basic issues in objective scale
development. Psychological Assessment, 7(3), 309–319.
Cohen, J. (1960). A coefﬁcient of agreement for nominal scales. Educational and
Psychological Measurement, 20, 37–46.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hills-
dale, NJ: Lawrence Erlbaum Associates.
Cohen, J., Cohen, P., West, S. G., & Aiken, L. (2003). Applied multiple regres-
sion/correlation analysis for the behavioral sciences (3rd ed.). Hillsdale, NJ: Lawrence
Erlbaum Associates.
Cohen, M. P. (2000). Note on the odds ratio and the probability ratio. Journal of Edu-
cational and Behavioral Statistics, 25, 249–252.
Cortina, J. M. (1993). What is coefﬁcient alpha? An examination of theory and applica-
tion. Journal of Applied Psychology, 78, 98–104.
Costa, P. T., Jr., & McCrae, R. R. (1992). The NEO PI-R professional manual. Odessa,
FL: Psychological Assessment Resources.
Courville, T., & Thompson, B. (2001). Use of structure coefﬁcients in published multiple
regression articles: x is not enough. Educational and Psychological Measurement, 61,
229–248.
Cox, D. R. (1972). Regression models and life tables. Journal of the Royal Statistical
Society, Series B: Methodological, 34, 187–220.
Cox, D. R., & Oakes, D. (1984). Analysis of survival data. New York, NY: Chapman &
Hall.
Cronbach, L. J. (1951). Coefﬁcient alpha and the internal structure of tests. Psychome-
trika, 16, 297–334.
Cudeck, R., & MacCallum, R. C. (Eds.) (2007). Factor analysis at 100: Historical devel-
opments and future directions. Mahwah, NJ: Lawrence Erlbaum Associates.
Darlington, R. B. (1968). Multiple regression in psychological research and practice.
Psychological Bulletin, 69, 161–182.
Darlington, R. B. (1990). Regression and linear models. New York, NY: McGraw-Hill.
Davison, M. L. (1992). Multidimensional scaling. New York, NY: Krieger.
Di Eugenio, B., & Glass, M. (2004). The kappa statistic: A second look. Computational
Linguistics, 30, 95–101.
Enders, C. K. (2010). Applied missing data analysis. New York, NY: Guilford Press.
Eknoyan, G. (2008). Adolphe Quetelet (1796–1874)—The average man and indices of
obesity. Nephrology, dialysis, transplantation: Ofﬁcial publication of the European
Dialysis and Transplant Association—European Renal Association, 23, 47–51.
Edwards, J. R., & Bagozzi, R. P. (2000). On the nature and direction of relationships
between constructs and measures. Psychological Methods, 5, 155–174.
Enders, C. K., & Toﬁghi, D. (2007). Centering predictor variables in cross-sectional
multilevel models: A new look at an old issue. Psychological Methods, 12, 121–138.
Feldt, L. S., & Qualls, A. L. (1996). Estimation of measurement error variance at speciﬁc
score levels. Journal of Educational Measurement, 33, 141–156.

REFERENCES
705
Fienberg, S. E. (1994). The analysis of cross-classiﬁed categorical data (2nd ed.).
Cambridge, MA: MIT Press.
Fisher, R. A. (1921a). Some remarks on the methods formulated in a recent arti-
cle on the qualitative analysis of plant growth. Annals of Applied Biology, 7,
367–372.
Fisher, R. A. (1921b). Studies in crop variation. I. An examination of the yield of dressed
grain from broadbalk. Journal of Agricultural Science, 11, 107–135.
Fisher, R. A. (1925). Statistical methods for research workers. Edinburgh, England:
Oliver & Boyd.
Fisher, R. A. (1935a). The design of experiments. Edinburgh, England: Oliver & Boyd.
Fisher, R. A. (1935b). The logic of inductive inference. Journal of the Royal Statistical
Society, 98, 39–54.
Fisher, R. A. (1950). Statistical methods for research workers (11th ed.). New York, NY:
Hafner.
Fisher, R. A., & Eden, T. (1927). Studies in crop variation. IV. The experimental deter-
mination of the value of top dressings with cereals. Journal of Agricultural Science,
17, 548–562.
Fisher, R. A., & Mackenzie, W. A. (1923). Studies in crop variation. II. The manorial
responses of different potato varieties. Journal of Agricultural Science, 13, 311–320.
Fitzmaurice, G., Laird, N. M., & Ware, J. H. (2011). Applied longitudinal analysis (2nd
ed.). Hoboken, NJ: John Wiley & Sons, Inc.
Fleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psycho-
logical Bulletin, 76, 378–382.
Fleiss, J. L., & Cohen, J. (1973). The equivalence of weighted kappa and the intra-
class correlation coefﬁcient as measures of reliability. Educational and Psychological
Measurement, 33, 613–619.
Freedman, L. S., & Schatzkin, A. (1992). Sample size for studying intermediate endpoints
within intervention trials of observational studies. American Journal of Epidemiology,
136, 1148–1159.
Galton, F. (1886). Heredity stature. Journal of the Anthropological Institute, 15, 489–499.
Galton, F. (December 13, 1888). Co-relations and their measurement, chieﬂy from anthro-
pometric data. Proceedings of the Royal Society, 45, 135–145.
Gamst, G., Meyers, L. S., & Guarino, A. J. (2008). Analysis of variance designs: A con-
ceptual and computational approach with SPSS and SAS. New York, NY: Cambridge
University Press.
Gay, L. R. (2010). Educational research: Competencies for analysis and applications
(8th ed.). Upper Saddle River, NJ: Pearson Prentice Hall.
Gigu`ere, G. (2007). Collecting and analyzing data in multidimensional scaling experi-
ments: A guide for psychologists using SPSS. Tutorials in Quantitative Methods for
Psychology, 2, 26–37.
Goldstein, H. (2011). Multilevel statistical models (4th ed.). Chichester, West Sussex:
John Wiley & Sons, Ltd.
Goodman, L. A. (1960). On the exact variance of products. Journal of the American
Statistical Association, 55, 708–713.
Gonzalez, R., & Grifﬁn, D. (2004). Measuring individuals in a social environment: Con-
ceptualizing dyadic and group interaction. In C. Sansone, C. C. Morf, & A. T. Panter
(Eds.), The Sage handbook of methods in social psychology (pp. 313–334). Thousand
Oaks, CA: Sage.

706
REFERENCES
Gorsuch, R. L. (2003). Factor analysis. In I. B. Weiner, J. A. Schinka, & W. F.
Velicer (Eds.), Handbook of psychology: Volume 2 Research methods in psychology
(pp. 143–164). Hoboken, NJ: John Wiley & Sons, Inc.
Graham, J. W. (2009). Missing data analysis: Making it work in the real world. Annual
Review of Psychology, 60, 549–576.
Guilford, J. P., & Fruchter, B. (1978). Fundamental statistics in psychology and education
(6th ed.). New York, NY: McGraw-Hill.
Hallgren, K. A. (2012). Computing inter-rater reliability for observational data: An
overview and tutorial. Tutorial in Quantitative Methods for Psychology, 8, 23–34.
Harris, M. J., & Taylor, G. (2009). Medical and health science statistics made easy (2nd
ed.). Sudbury, MA: Jones & Bartlett.
Hartigan, J. A., & Wong, M. A. (1979). A K-means clustering algorithm. Applied Statis-
tics, 28, 100–108.
Haviland, M. G. (1990). Yates’s correction for continuity and the analysis of 2 x 2
contingency tables. Statistics in Medicine, 9, 363–367.
Hayes, A. F. (2009). Beyond Baron and Kenny: Statistical mediation analysis in the new
millennium. Communication Monographs, 76, 408–420.
Hays, W. L. (1981). Statistics (3rd ed.). New York, NY: Holt, Rinehart & Winston.
Holcomb, W. L., Chaiworapongsa, T., Luke, D. A., & Burgdorf, K. D. (2001). An odd
measure of risk: Use and misuse of the odds ratio. Obstetrics and Gynecology, 84,
685–688.
Hotelling, H. (1933). Analysis of a complex of statistical variables into principal com-
ponents. Journal of Educational Psychology, 24, 498–520.
Hotelling, H. (1936). Simpliﬁed calculation of principal components. Psychometrika, 1,
27–35.
Howell, D. (2010). Statistical methods for psychology
(7th ed.). Belmont, CA:
Wadsworth.
Hox, J. J. (2010). Multilevel analysis: Techniques and applications (2nd ed.). New York,
NY: Routledge.
Hox, J. J., & Roberts, J. K. (2010). Handbook of advanced multilevel analysis. New
York, NY: Psychology Press.
Hu, L.-T., & Bentler, P. M. (1999). Cutoff criteria for ﬁt indices in covariance structure
analysis: Conventional criteria versus new alternatives. Structural Equation Modeling,
6, 1–55.
Huberty, C. J. (1989). Problems with stepwise methods—better alternatives. In
B. Thompson (Ed.), Advances in social science methodology (Vol. 1, pp. 43–70).
Greenwich, CT: JAI Press.
Jaccard, J., & Becker, M. A. (1990). Statistics for the behavioral sciences (2nd ed.).
Belmont, CA: Wadsworth.
J¨oreskog, K. G., & S¨orbom, D. (1996). LISREL 8 user’s reference guide. Chicago, IL:
Scientiﬁc Software International.
Johnson, J. W. (2000). A heuristic method for estimating the relative weight of predictor
variables in multiple regression. Multivariate Behavioral Research, 35, 1–19.
Johnson, J. W., & LeBreton, J. M. (2004). History and relative use of importance indices
in organizational research. Organizational Research Methods, 7, 238–257.
Kaiser, H. F. (1970). A second-generation Little Jiffy. Psychometrika, 35, 401–415.
Kaiser, H. F. (1974). An index of factorial simplicity. Psychometrika, 39, 31–36.
Kaplan, E. L., & Meier, P. (1958). Nonparametric estimation from incomplete observa-
tions. Journal of the American Statistical Association, 53, 457–481.

REFERENCES
707
Kasser, T., & Ryan, R. M. (1993). A dark side of the American dream: Correlates
of ﬁnancial success as a central life aspiration. Journal of Personality and Social
Psychology, 65, 410–422.
Kasser, T., & Ryan, R. M. (1996). Further examining the American dream: Differential
correlates of intrinsic and extrinsic goals. Personality and Social Psychology Bulletin,
22, 280–287.
Katz, M. H. (2006). Multivariable analysis: A practical guide for clinicians (2nd ed.).
New York, NY: Cambridge University Press.
Kleinbaum, D. G. (1996). Survival analysis. New York, NY: Springer-Verlag.
Keppel, G., & Wickens, T. D. (2004). Design and analysis: A researcher’s handbook
(4th ed.). Upper Saddle River, NJ: Pearson Prentice Hall.
Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30, 81–89.
Kendall, M. G. (1948). Rank correlation methods. London, England: Charles Grifﬁn &
Company.
Kim, K., & Neil, T. (2007). Univariate and multivariate general linear models (2nd ed.).
Boca Raton, FL: Chapman & Hall/CRC.
Kline, R. B. (2011). Principles and practice of structural equation modeling (3rd ed.).
New York, NY: Guilford Press.
Knoke, D., & Burke, P. J. (1980). Log-linear models. Thousand Oaks, CA: Sage.
Krippendorff, K. (1980). Content analysis: An introduction to its methodology. Beverley
Hills, CA: Sage.
Kruskal, J. B. (1964). Multidimensional scaling by optimizing goodness of ﬁt to a non-
metric hypothesis. Psychometrika, 29, 1–28.
Kruskal, J. B., & Wish, M. (1978). Multidimensional scaling. Newbury Park, CA: Sage.
Kuder, G. F., & Richardson, M. W. (1937). The theory of the estimation of test reliability.
Psychometrika, 2, 151–160.
Lee, E. T., & Wang, J. W. (2003). Statistical methods for survival data analysis (3rd
ed.). Hoboken, NJ: John Wiley & Sons, Inc.
Lee, V. E. (2000). Using hierarchical linear modeling to study social contexts: The case
of school effects. Educational Psychologist, 35, 125–141.
Lleras, C. (2005). Path analysis. In K. Kempf-Leonard (Ed.), Encyclopedia of social
measurement, (Vol. 3, pp. 25–30). Amsterdam, The Netherlands: Elsevier.
Lord, F. M. (1984). Standard errors of measurement at different ability levels. Journal
of Educational Measurement, 21, 239–243.
Lord, F. M., & Norick, M. R. (1968). Statistical theories of mental test scores. Reading,
MA: Addison-Wesley.
MacDonald, P. D. M. (2012). Methods in ﬁeld epidemiology. Burlington, MA: Jones &
Bartlett.
MacKinnon, D. P. (2008). Introduction to statistical mediation analysis. Mahwah, NJ:
Lawrence Erlbaum Associates.
MacKinnon, D. P., Fairchild, A. J., & Fritz, M. S. (2007). Mediation analysis. Annual
Review of Psychology, 58, 593–614.
MacKinnon, D. P., Lockwood, C. M., Hoffman, J. M., West, S. G., & Sheets, V. (2002).
A comparison of methods to test mediation and other intervening variable effects.
Psychological Methods, 7, 83–104.
MacQueen, J. (1967). Some methods for classiﬁcation and analysis of multivariate obser-
vations. In L. M. LeCam, & J. Neyman (Eds.), Proceedings of the 5th Berkeley
Symposium on Mathematical Statistics and Probability (Vol. 5, pp. 281–297). Berke-
ley, CA: University of California Press.

708
REFERENCES
Marascuilo, L. A., & McSweeney, M. (1977). Nonparametric and distribution-free meth-
ods for the social sciences. Monterey, CA: Brooks/Cole.
McKnight, P. E., McKnight, K. M., Sidani, S., & Figuerdo, A. J. (2007). Missing data:
A gentle introduction. New York, NY: Guilford Press.
Menard, S. (2010). Logistic regression: From introductory to advanced concepts and
applications. Thousand Oaks, CA: Sage.
Merrell, M. (1947). Time-speciﬁc life tables contrasted with observed survivorship. Bio-
metrics, 3, 129–136.
Merrill, R. M., (2013). Fundamentals of epidemiology and biostatistics: Combining the
basics. Burlington, MA: Jones & Bartlett.
Meyers, L. S., Gamst, G., & Guarino, A. J. (2009). Data analysis using SAS enterprise
guide. New York, NY: Cambridge University Press.
Meyers, L. S., Gamst, G., & Guarino, A. J. (2013). Applied multivariate research: Design
and interpretation (2nd ed.). Thousand Oaks, CA: Sage.
Moser, B. K., & Stevens, G. R. (1992). Homogeneity of variance in the two-sample
means test. The American Statistician, 46, 19–21.
Norman, G. R., & Streiner, D. L. (2008). Biostatistics: The bare essentials (3rd ed.).
Hamilton, Ontario: BC Decker.
Norusis, M. J. (2008). SPSS statistics 17.0: Advanced statistical procedures companion.
Upper Saddle River, NJ: Pearson Prentice Hall.
Norusis, M. J. (2012). IBM SPSS statistics 19 advanced statistical procedures companion.
Upper Saddle River, NJ: Pearson Prentice Hall.
Nunnally, J. C., & Bernstein, I. H. (1994). Psychometric theory (3rd ed.). New York,
NY: McGraw-Hill.
Osborne, J. W. (2008). Best practices in data transformation: The overlooked effect of
minimum values. In J. Osborne (Ed.), Best practices in quantitative methods (pp.
197–204). Thousand Oaks, CA: Sage.
Pearson, K. (1896). Mathematical contributions to the mathematical theory of evolution.
III Regression, heredity, and panmixia. Philosophical Transactions of the Royal Society
of London, 187, 253–318.
Pearson, K. (1900a). Mathematical contributions to the theory of evolution. VII. On the
correlation of characters not quantitatively measurable. Philosophical Transactions of
the Royal Society of London, Series A, 195, 1–47.
Pearson, K. (1900b). On the criterion that a given system of deviations from the probable
in the case of correlated system of variables is such that it can be reasonably supposed
to have arisen from random sampling. Philosophical Magazine, 50, 157–175.
Pearson, K. (1901). Mathematical distributions to the theory of evolution IX. On the
principle of homotyposis and its relation to heredity, to the variability of the individual,
and to that of the race. Part I: Homotyposis in the vegetable kingdom. Philosophical
Transactions of the Royal Society of London: Series A, 197, 285–379.
Pearson, K. (1901). On lines and planes of closest ﬁt to systems of points in space.
Philosophical Magazine, 2, 559–572.
Pedhazur, E. J. (1997). Multiple regression in behavioral research: Explanation and pre-
diction (3rd ed.). Orlando, FL: Harcourt Brace.
Pedhazur, E. J., & Schmelkin, L. P. (1991). Measurement, design, and analysis: An
integrated approach. Hillsdale, NJ: Lawrence Erlbaum Associates.
Preacher, K. J., & Hayes, A. F. (2004). SPSS and SAS procedures for estimating indirect
effects in simple mediation models. Behavior Research Methods, Instruments, and
Computers, 36, 717–731.

REFERENCES
709
Preacher, K. J., & Hayes, A. F. (2008). Asymptotic and resampling strategies for assess-
ing and comparing indirect effects in multiple mediator models. Behavior Research
Methods, 40, 879–891.
Prentice, R. L., & Marek, P. (1979). A qualitative discrepancy between censored data
rank tests. Biometrics, 35, 861–867.
Press, S. J. (1972). Applied multivariate analysis. New York, NY: Holt, Rinehart &
Winston.
Quenouille, M. H. (1956). Note on bias in estimation. Biometrika, 43, 353–360.
Quinn, G. P., & Keough, M. J. (2002). Experimental design and data analysis for biolo-
gists. New York, NY: Cambridge University Press.
Raudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: Applications and
data analysis methods (2nd ed.). Thousand Oaks, CA: Sage.
Raykov, T., & Marcoulides, G. A. (2008). An introduction to applied multivariate anal-
ysis. New York, NY: Routledge.
Read, T. R. C., & Cressie, N. A. C. (1988). Goodness-of-ﬁt statistics for discrete multi-
variate data. New York, NY: Springer-Verlag.
Reise, S. F., & Duan, N. (2003). Multilevel modeling: Methodological advances, issues,
and applications. Mahwah, NJ: Lawrence Erlbaum Associates.
Robinson, J. P., Shaver, P. R., & Wrightsman, L. S. (1991). Measures of personality and
social psychological attitudes. San Diego, CA: Academic Press.
Rosenbaum, M. (1980). A schedule for assessing self-control behaviors: Preliminary
ﬁndings. Behavior Therapy, 11, 109–121.
Salsburg, D. (2001). The lady tasting tea: How statistics revolutionized science in the
twentieth century. New York, NY: W. H. Freeman.
Satterthwaite, F. W. (1946). An approximate distribution of estimates of variance com-
ponents. Biometrics Bulletin, 65, 110–114.
Schlesselman, J. J. (1992). Case-control studies: Design, conduct, analysis. New York,
NY: Oxford University Press.
Schreiber, J. B., Nora, A., Stage, F. K., Barlow, E. A., & King, J. (2006). Reporting struc-
tural equation modeling and conﬁrmatory factor analysis results: A review. Journal
of Educational Research, 99, 323–337.
Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-
experimental designs for generalized causal inference. Boston, MA: Houghton-Mifﬂin.
Shepard, R. N. (1962). The analysis of proximities: Multidimensional scaling with an
unknown distance function. I. Psychometrika, 27, 125–140.
Shrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater
reliability. Psychological Bulletin, 86, 420–428.
Singer, J. D., & Willett, J. B. (1993). It’s about time: Using discrete-time survival analysis
to study duration and the timing of events. Journal of Educational Statistics, 18,
155–195.
Singer, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis. New York, NY:
Oxford University Press.
Snedecor, G. W. (1934). Analysis of variance and covariance. Ames, IA: Collegiate
Press.
Snijders, T. A. B., & Bosker, R. J. (2012). Multilevel analysis: An introduction to basic
and advanced multilevel modeling (2nd ed.). Thousand Oaks, CA: Sage.
Sobel, M. E. (1982). Aysmptotic conﬁdence intervals for indirect effects and their standard
errors in structural equation models. In N. Tuma (Ed.), Sociological methodology (pp.
159–186). San Francisco, CA: Jossey-Bass.

710
REFERENCES
Sobel, M. E. (1986). Some new results on indirect effects in structural equation models.
In S. Leinhardt (Ed.), Sociological methodology (pp. 290–312). Washington, DC:
American Sociological Association.
Spearman, C. (1904a). “General intelligence,” objectively determined and measured.
American Journal of Psychology, 15, 201–293.
Spearman, C. (1904b). The proof and measurement of association between two things.
American Journal of Psychology, 15, 72–101.
Spearman, C. (1907). Demonstration of formulae for true measurement of correlation.
American Journal of Psychology, 18, 161–169.
Spearman, C. (1910). Correlation calculated from faulty data. British Journal of Psychol-
ogy, 3, 271–295.
Steinberg, L., & Thissen, D. (1996). Uses of item response theory and the testlet concept
in the measurement of psychopathology. Psychological Methods, 1, 81–97.
Stigler, S. M. (1986). The history of statistics: The measurement of uncertainty before
1900. Cambridge, MA: Harvard University Press.
Stevens, J. P. (2009). Applied multivariate statistics: A modern approach (3rd ed.). Mah-
wah, NJ: Lawrence Erlbaum Associates.
Student . (1908). The probable error of a mean. Biometrika, 6, 1–25.
Tabachnick, B. G., & Fidell, L. S. (2007).Using multivariate statistics (5th ed.). Boston,
MA: Pearson.
Takane, Y. Young, F. W., & de Leeuw, J. (1977). Nonmetric individual differences
multidimensional scaling: An alternating least squares method with optimal scaling
features. Psychometrika, 42, 7–67.
Tekle, F. B., & Vermunt, J. K. (2012). Event history analysis. In H. Cooper, P. M.
Camic, D. L. Long, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA handbook of
research methods in psychology: Data analysis and research publication (Vol. 3, pp.
267–290). Washington, DC: American Psychological Association.
Thompson, B. (1995). Stepwise regression and stepwise discriminant analysis need not
apply here: A guidelines editorial. Educational and Psychological Measurement, 55,
525–534.
Thompson, B. (2006). Foundations of behavioral statistics: An insight-based approach.
New York, NY: Guilford Press.
Thompson, B., & Borrello, G. M. (1985). The importance of structure coefﬁcients in
regression research. Educational and Psychological Measurement, 56, 203–209.
Thurstone, L. L. (1931). Multiple factor analysis. Psychological Review, 38, 406–427.
Thurstone, L. L. (1935). Vectors of the mind. Chicago, IL: University of Chicago Press.
Thurstone, L. L. (1947). Multiple factor analysis. Chicago, IL: University of Chicago
Press.
Tonidandel, S., & LeBreton, J. M. (2011). Relative importance analysis: A useful sup-
plement to regression analysis. Journal of Business Psychology, 26, 1–9.
Torgerson, W. S. (1952). Multidimensional scaling: I. Theory and method. Psychome-
trika, 17, 401–419.
Torgerson, W. S. (1958), Theory and method of scaling. New York, NY: John Wiley &
Sons, Inc.
Traub, R. E. (1997). Classical test theory in historical perspective. Educational Measure-
ment: Issues and Practice, 16, 8–14.
Tukey, J. W. (1958). Bias and conﬁdence in not quite large samples (Abstract). Annals
of Mathematical Statistics, 29, 614.
Tukey, J. W. (1977). Exploratory data analysis. Reading, MA: Addison-Wesley.

REFERENCES
711
Varmuza, K., & Filzmoser, P. (2009). Introduction to multivariate statistical analysis in
chemometrics. Boca Raton, FL: CRC Press.
Vogt, P. W. (2007). Quantitative research methods for professionals. Boston, MA: Allyn
and Bacon.
Wang, J., & Wang, X. (2012). Structural equation modeling: Applications using Mplus.
Chichester, West Sussex: John Wiley & Sons, Ltd.
Welch, B. L. (1937). The signiﬁcance of the difference between two means when the
population variances are unequal. Biometrika, 29, 350–362.
West, S. G., Taylor, A. B., & Wu, W. (2012). Model ﬁt and model selection in structural
equation modeling. In R. H. Hoyle, (Ed.), Handbook of structural equation modeling
(pp. 209–231). New York, NY: Guilford Press.
Wilcox, R. (2012). Introduction to robust estimation & hypothesis testing (3rd ed.).
Waltham, MA: Elsevier.
Wright, S. (1920). The relative importance of heredity and environment in determining
the piebald pattern of guinea-pigs. Proceedings of the National Academy of sciences,
6, 320–332.
Yates, F. (1934). Contingency tables involving small numbers and the χ2 test. Supplement
to the Journal of the Royal Statistical Society, 1, 217–235.
Yen, W. M. (1993). Scaling performance assessments: Strategies for managing local item
dependence. Journal of Educational Measurement, 30, 187–213.
Yerkes, R. M., & Dodson, J. D. (1908). The relation of strength of stimulus to rapidity
of habit-formation. Journal of Comparative Neurology and Psychology, 18, 459–482.
Young, F. W., & Harris, D. F. (2012). Multidimensional scaling. In M. J. Norusis (Ed.),
SPSS Statistics 19.0: Advanced statistics procedures companion (pp. 361–430). Upper
Saddle River, NJ: Pearson Prentice Hall.


AUTHOR INDEX
Aiken, L., 181, 467
Agresti, A., 690
American Psychological Association, 30
Aroian, L. A., 387, 395, 452
Bagozzi, R. P., 419
Barlow, E. A., 369
Baron, R. M., 387
Becker, M. A., 667
Bentler, P. M., 368, 369, 427
Berkman, E. T., 320
Bernstein, I. H., 311, 313
Bickel, R., 226
Blau, P., 389
Bollen, K. A., 419, 437
Bosker, R. J., 225
Brown, T. A., 355
Brown, W., 321
Browne, M. W., 369
Bryk, A. S., 226
Budescu, D. V., 209
Burgdorf, K. D., 675
Burke, P. J., 689–690
Byrne, B. M., 337
Campbell, D. T., 521
Carroll, J. D., 613
Cattell, R. B., 332
Chaiworapongsa, T., 675
Chang, J. J., 613
Clark, L. A., 312–313, 315
Cohen, J. 181, 321, 467
Cohen, M. P., 675
Cohen, P., 181, 467
Cook, T. D., 521
Cortina, J. M., 312
Costa, P. T. Jr., 61, 313
Courville, T., 197
Cox, D. R., 283, 301
Cressie, N. A. C., 697
Cronbach, L. J., 312
Cudeck, R., 332, 369
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
Darlington, R. B., 209, 382
Davison, M. L., 605
de Leeuw, J., 605, 609, 613
Di Eugenio, B., 319
Dodson, J. D., 218
Duan, N., 227
Duncan, O., 389
Edwards, J. R., 419
Eknoyan, G., 182
Eden, T., 477
Enders, C. K., 202, 227
Fairchild, A. J., 381
Feldt, L. S., 313
Fidell, L. S., 570
Fienberg, S. E., 689–690
Figuerdo, A. J., 202
Filzomoser, P., 132
Fisher, R. A., 477
Fitzmaurice, G., 556
Fleiss, J. L., 319, 321
Freedman, L. S., 337, 387
Fritz, M. S., 381
Fruchter, B., 319
Galton, F., 159, 173
Gamst, G., 3, 59, 74, 139–141, 147, 181,
202, 217, 226, 227, 255, 332, 337,
341, 355, 366, 369, 381, 382, 463,
467–477, 479, 555, 569, 580, 592,
605, 613, 624, 666
Gay, L. R., 313
Gigu`ere, G., 605, 609, 614
Glass, M., 319
Goldstein, H., 226
Goodman, L. A., 387
Gonzalez, R., 319
Gorsuch, R. L., 332
Graham, J. W., 202
Grifﬁn, D., 319
Guarino, A. J., 3, 59, 74, 139–141, 147,
181, 202, 217, 226–227, 255, 332,
337, 341, 355, 366, 369, 381–382,
463, 467, 477, 479, 555, 569, 580,
592, 605, 613, 624, 666
Guilford, J. P., 319
Hallgren, K. A., 320
Harris, D. F., 605, 613
Harris, M. J., 676–677
Hartigan, J. A., 631
Haviland, M. G., 667
Hays, A. F., 381, 387
Hoffman, J. M., 387
Holcomb, W. L., 675
Howell, D., 166, 320
Hotelling, H., 331
Hox, J. J., 226, 547
Hu, L.-T., 368
Huberty, C. J., 192
Jaccard, J., 667
Johnson, J. W., 209
J¨oreskog, K. G., 368, 427
Kaiser, H. F., 334
Kaplan, E. L., 283
Kasser, T. 106
Katz, M. H., 306
Kenny, D. M., 387
Kendall, M. C., 166
Keough, M. J., 147
Keppel, G., 478
Kim, K., 556
King, J., 369
Kleinbaum, D. G., 283
Kline, R. B., 132, 355, 368–369
Knoke, D., 689–690
Krippendorff, K., 321
Kruskal, J. B., 605, 608
Kuder, G. F., 312
713

714
AUTHOR INDEX
Laird, N. M., 556
LeBreton, J. M., 209
Lee, E. T., 283
Lee, V. E., 227
Lleras, C., 389
Lockwood, C. M., 387
Lord, F. M., 311, 313
Luke, D. A., 675
MacCallum, R. C., 332
MacDonald, P. D. M., 676
Mackenzie, W. A., 477
MacKinnon, D. P., 381, 387
MacQueen, J., 631
Marcoulides, G. A., 132, 337
Marascuilo, L. A., 165
Marek, P., 294
McCrea, R. R., 61, 313
McKnight, K. M., 202
McKnight, P. E., 202
McSweeney, M., 165
Meier, P. 283
Menard, S., 255
Merrell, M., 283
Merrill, R. M., 677
Meyers, L. S., 3, 59, 74, 139–141, 147,
181, 202, 217, 226, 227, 255, 332,
337, 341, 355, 366, 369, 381–382,
463, 467, 477, 479, 555, 569, 580,
592, 605, 613, 624, 666
Moore, M. T., 355
Moser, B. K., 464
Neil, T., 556
Nora, A., 369
Norman, G. R., 226, 359, 532, 573
Norusis, M. J., 283, 294, 301, 547
Novick, M. R., 311
Nunnally, J. C., 311, 313
Oakes, D., 283
Osborne, J. W., 147
Pearson, K., 159, 319, 331, 645, 666,
670
Pedhazur, E. J., 311, 382
Preacher, K. J., 381, 387
Prentice, R. L., 294
Press, S. J., 580
Qualls, A. L., 313
Quenouille, M. H., 580
Quinn G. P., 147
Raudenbush, S. W., 226
Raykov, T., 132, 337
Read, T. R. C., 697
Reise, S. F., 227, 320
Richardson, M. W., 312
Roberts, J. K., 226
Robinson, J. P., 312
Rosenbaum, M., 638
Ryan, R. M., 106
Salsburg, D., 459, 477
Satterthwaite, F. W., 463
Schatzkin, A., 387, 337
Schlesselman, J. J., 676
Schmelkin, L. P., 311
Schreiber, J. B., 369
Shadish, W. R., 521
Shaver, P. R., 312
Sheets, V., 387
Shepard, R. N., 605
Shrout, P. E., 319, 321
Sidani, S., 202
Singer, J. D., 283
Sndeecor, G. W., 477
Snijders, T. A. B., 225
Sobel, M. E., 387
S¨orbom, D., 368, 427
Spearman, C., 166, 311, 321, 331
Stage, F. K., 369
Steinberg, L., 312
Stevens, G. R., 464
Stevens, J. P., 141, 226, 570
Streiner, D. L., 226, 359, 532, 573
Stigler, S. M., 182
Student, 459
Tabachnick, B. G., 570
Takane, Y., 605, 609, 613
Taylor, A. B., 368, 369
Taylor, G., 676, 677
Tekle, F. B., 283
Thissen, D. 312
Thompson, B., 192, 197
Thurstone, L. L., 331, 332
Togighi, D., 227
Tonidandel, S. 209
Torgerson, W., 605
Traub, R. E., 311
Tukey, J. W., 74, 580
Varmuza, K., 132
Vermunt, J. K., 283
Vogt, P. W., 226
Wang, J., 355
Wang, J. W., 283
Wang, X., 355
Ware, J. H., 556
Watson, D., 312–313, 315
Welch, B. L, 463
West, S. G., 181, 368–369, 387, 467
Wickens, T. D., 478
Wilcox, R., 166
Willett, J. B., 283
Wish, M., 605, 609
Wong, M. A., 631
Wright, S. 389
Wrightsman, L. S., 312
Wu, W., 368–369
Yates, F., 666
Yen, W. M., 312
Yerkes, R. M., 218
Young, F. W., 605, 609, 613

SUBJECT INDEX
Acronym for SPSS, 3
Adjusted R2
linear regression, 175–177, 179, 186,
187, 193–195, 199, 204. 207
mediation analysis, 383
one-way between-subjects ANOVA, 482
path analysis, 391, 395
Adjusted standardized residuals, 669–670,
672, 673–674, 682
Adjusted values based on covariate, 493,
495, 501–503
Agglomeration schedule, 625–631
Aggregating variables, 229–231
Akaike information criterion (AIC)
linear regression, 203, 206–207
multilevel modeling, 234, 239–240,
242, 245, 252, 547
one-way within-subjects ANOVA, 549,
552–554
All possible subsets regression method,
192, 200–210
Alpha level
chi square, 652, 673
editing output, 29
Kendall tau-b, 167
linear regression, 192
mediation analysis, 387
multilevel modeling, 227
one-way between-subjects ANCOVA,
503
one-way between-subjects ANOVA,
479, 483
one-way between-subjects MANOVA,
570, 575
outliers, 132, 134
path analysis, 410, 411, 414, 418
Pearson correlation, 163, 169,
polynomial trend analysis, 487
Spearman rho, 167
structural equation modeling, 429, 442
t test, 459, 462, 466, 469,
two-way between-subjects ANOVA,
514
two-way mixed ANOVA, 560
Performing Data Analysis Using IBM SPSS®, First Edition.
Lawrence S. Meyers, Glenn C. Gamst, and A. J. Guarino.
© 2013 John Wiley & Sons, Inc. Published 2013 by John Wiley & Sons, Inc.
two-way between-subjects MANOVA,
591–592, 598, 601–602
Alpha level correction. See Bonferroni
corrected alpha level
Alpha level inﬂation, 227, 479, 503, 560,
570
Alternating least squares scaling
(ASCAL), 605, 606, 614
Analysis of variance (ANOVA) designs
one-way analysis of covariance,
493–505
one-way between-subjects, 477–483,
637–641
one-way within-subjects using GLM,
531–554
one-way within-subjects using Linear
Mixed Models, 521–530
polynomial trend analysis, 485–491
two-way between-subjects, 507–519
two-way mixed, 555–565
Aroian test, 387, 395, 412, 452–454
Automatic Linear Modeling procedure,
200–210
Autoregressive structure. See
Variance/covariance structures
Average inter-item correlation, 312, 315,
317, 322, 323
Backward regression method, 192
Bartlett’s test of sphericity, 334–335, 340,
571, 573, 593, 596
Bayesian information criterion (BIC), 547,
549, 552–554
Beta coefﬁcient. See Standardized
regression coefﬁcient
Between-groups variance deﬁned,
478
Binomial test, 645–653
Body mass index, 182
Bonferroni corrected alpha level,
one-way between-subjects ANCOVA,
503
one-way between-subjects MANOVA,
570, 575
two-way between-subjects ANOVA,
512, 514, 517
two-way between-subjects MANOVA,
591–592, 598, 610
two-way mixed ANOVA, 558–560
one-way within-subjects ANOVA, 525,
529, 544, 548
Box and whiskers plot, 72, 73, 75,
123–124
Box’s test of equality of covariance
matrices, 560, 571, 573, 593, 596
Breslow Generalized Wilcoxon test, 292,
295
Brown-Forsythe test, 487, 639
Canonical correlation, 583–585
Centering
grand mean centering, 181, 227–228,
231–232
group mean centering, 227–228
predictor variable centering, 181–189
reference score centering, 181–182
Centroid, 131–132, 585–587, 624, 631
Chi square
conﬁrmatory factor analysis, 368, 378,
427
layers analysis, 681–687
loglinear analysis, 689–697
path analysis, 409, 414
risk analysis, 675–679, 686–687
single sample test for binary categories,
645–653
single sample test for multiple
categories, 655–664
structural model, 448
two-way test of independence, 665–674
Chi square difference test, 235–236,
242–243, 245–246, 252, 414–415
Classiﬁcation
discriminant function analysis, 579–580,
581, 587–589
k-means clustering, 632
logistic regression, 259–261, 273, 275,
279
715

716
SUBJECT INDEX
Classiﬁcation (continued)
ROC analysis, 265–272
Classiﬁcation cutoff, 259, 265–267,
271–272
Classiﬁcation function coefﬁcients, 587
Cluster analysis
hierarchical, 623–630
k-means, 631–641
Clustering in multilevel modeling,
226–227, 232, 237, 241, 245, 248
Coefﬁcient alpha, 312–317
Cohen’s d, 467, 474, 483, 640–641
Cohen’s Kappa coefﬁcient, 321, 324–327
Communalities, 337–338, 339, 344–345,
348, 352
Comparative Fit Index (CFI), 369, 378,
410, 415, 429, 448, 450
Compound symmetry. See
Variance/covariance structures
Computing new variables
algebraic expression, 103–106
mean of a set of variables, 106–109
Conﬁdence interval
chi square, 687
descriptive statistics, 63–65, 71, 73–74,
143
linear regression, 203
logistic regression, 259, 263
one-way between-subjects ANCOVA,
503
one-way within-subjects ANCOVA, 525,
529
risk analysis, 678–679
reliability analysis, 313
ROC analysis, 269
survival analysis, 288, 307
t test, 459–461, 465, 466, 469, 472
two-way between-subjects ANOVA, 517
two-way mixed ANOVA, 558, 568
Contingency coefﬁcient, 666
Copying output, 34–35
Corrected ANOVA model, 480–482, 575,
598–599
Corrected item-total correlation, 317
Correlated error terms, 355
Correlation paths drawn with
double-headed arrows, 354
Covariance structures. See
Variance/covariance structures
Covariates
ANCOVA, 493–495, 497–498, 500–502
linear regression, 177, 191, 196,
211–212, 214
logistic regression, 257, 258, 274
multilevel modeling, 227, 234, 237, 241,
245, 248
survival analysis, 283, 301, 303–307
Cox and Snell pseudo R2, 261
Cramer’s V, 666, 669, 673, 677, 682
Cross validation, 580, 587–589
Data View Display, 4, 11–13
Date formats, 112–113
Date and Time Wizard, 113–117
Decimal values in output, 28–30
Degrees of freedom
ANOVA, 480, 482, 488, 489, 496, 514,
516, 526, 527, 528, 547
chi square, 132, 134, 368, 369, 414,
427, 448, 658, 670, 673
conﬁrmatory factor analysis, 366, 369
discriminant function analysis, 579, 583
Freedman-Schatzkin test, 388
k-means cluster analysis, 639
linear regression, 177, 187, 192, 195,
199
logistic regression, 261, 277
loglinear analysis, 693
mediation models, 396
multilevel modeling, 226, 236, 239, 240,
243, 245, 252
Pearson correlation, 162
structural equation modeling, 414, 441,
456
survival analysis, 305, 306
t test, 461, 462, 466, 467, 469, 474
Dendrogram, 630
Descriptives procedure, z scores, 79–81
Dialog windows, 21–24
Dimension reduction analysis, 583
Discriminant function analysis, 579–589
canonical correlation, 583–585
latent construct, 583
number of, 579–580
overview, 569, 579–580
Discriminant score, 569–570, 574, 579,
583
Dissimilarity matrix. See Proximity matrix
Double-headed arrow paths, 354, 359, 399,
402
Drawing models in Amos
conﬁrmatory models, 355–363
path models, 398–407
measurement model, 421–423
structural model, 431–437
Editing output
column width, 27–28
text editing, 29
viewing more decimal values, 28–29
Effect size. See Cohen’s d
Eigenvalues, 335–337, 339, 340, 344, 348,
583, 587
Endogenous variables, 390, 419, 433–437
Error latent variables, 354, 359
Estimated marginal means, 502, 503, 511,
517, 529, 544, 549, 593
Eta square
k-means cluster analysis, 639
linear regression, 177, 187, 195
one-way between-subjects ANCOVA,
503
one-way between-subjects ANOVA, 482
one-way between-subjects MANOVA,
574, 575
one-way within-subjects ANOVA, 527,
528
polynomial trend analysis, 489, 491
t test, 467
two-way between-subjects ANOVA,
507, 516
two-way between-subjects MANOVA,
599, 602
two-way mixed ANOVA, 562
Euclidian distance, 608, 611, 616
Excel. See Importing data from Excel
Exogenous variables, 390, 419
Expected frequencies, 645–646, 650, 656,
659–660, 666–667
Extensions for ﬁle names, 3
Extraction of components/factors,
331–344, 348, 352
maximum likelihood, 331
principal axis, 331, 340–352
principal components, 331, 335–352
unweighted least squares, 331
Factor analysis
conﬁrmatory, 353–378
exploratory, 331–352
False negatives, 266
False positives, 265–266
File name extensions, 3
File types, 3
Final cluster centers, 636–637
Fisher exact test, 670
Fisher, R. A., 477
Fixed effects, 531, 547–548, 550
Flattened subject weights, 617
Forward regression method, 191–192
Freedman-Schatzkin test, 387–388, 396,
413, 454–456
Frequencies procedure
categorical variables, 60–61
quantitative variables, 61–65
Generating class, 690–691, 694–696
Goodman test, 387
Goodness of Fit Index (GFI), 369, 378,
410, 415, 429, 448, 450
Gosset, W., 459
Greenhouse-Geisser correction, 526

SUBJECT INDEX
717
Hazard ratio, 306–307
Heterogeneity of variance, 467–469
Hierarchical multilevel model, 245–248
Hierarchical multiple regression, 211–215
Hierarchically structured data, 225
Histogram
descriptive statistics, 61–62, 63–65, 72,
76, 140–141, 143–144
linear regression, 209–210
transforming variables, 149, 155
visual binning, 97, 100–101
History of IBM SPSS, 3–4
Homogeneity of regression, 494, 498–501
Homogeneity of variance, 463, 464–467
Hosmer-Lemeshow goodness of ﬁt, 259,
261
Hotelling’s Trace. See Multivariate tests of
statistical signiﬁcance
Huynh-Feldt correction, 526
Icicle plot, 628–629
Importing data, 15–18
Independence assumption, 225–227
Independent-samples t test, 463–469
Indirect effect
mediation analysis, 383, 386–388,
395–396
path analysis, 412, 395–396
structural equation modeling, 443–446,
448, 452, 456
Initial cluster centers, 635
Item-total correlation. See Corrected
item-total correlation
Interaction effects
linear regression, 181
loglinear analysis, 689–697
multilevel modeling, 229, 248–252
one-way between-subjects ANCOVA,
501–502
one-way between-subjects ANOVA, 498
one-way within-subjects ANOVA, 522,
525, 545
survival analysis, 301
two-way between-subjects ANOVA,
507–508, 509–512, 516
two-way between-subjects MANOVA,
591–592, 593–594, 596, 598–599,
602
two-way mixed ANOVA, 555, 557–558,
560, 562, 565
Interaction multilevel model, 248–252
Intercept
ANOVA, 480, 482, 531, 543, 548
conﬁrmatory factor analysis, 355
linear regression, 174, 178–179,
181–182, 189
logistic regression, 259, 261
multilevel modeling, 227, 234, 236, 237,
240, 241, 244, 245, 246, 248
Inter-item correlations, 312, 313, 317, 322,
323
Intraclass correlation (ICC), 226–227, 237,
319–324
Importing data from Excel, 15–18
Interquartile range, 73, 123
Intrinsically linear regression model, 217
Iteration history, 635
Jackniife procedure, 580
Kaiser–Meyer–Olkin indicator, 334, 335,
340
Kappa coefﬁcient. See Cohen’s Kappa
coefﬁcient
Kendall tau-b coefﬁcient, 166–170
with ties, 169–170
without ties, 166–168
Kolmogorov-Smirnov test, 140
K-R 20 coefﬁcient, 312
Kurtosis, 61, 65, 140–141
Latent variables
conﬁrmatory factor analysis, 354, 356,
357, 359, 363, 366, 371–372, 378
discriminant function analysis, 579–580,
585
linear regression, 197
one-way between-subjects MANOVA,
569–570
path analysis, 389, 397, 402
structural equation modeling, 419–423,
427, 431, 433, 437, 441–447
Latent variables drawn as circles, 354,
357–359
Least signiﬁcant difference (LSD) test, 503
Least squares algorithm, 173, 191, 218,
255, 411, 467, 493, 610
Least squares means. See Estimated
marginal means
Leave-one-out procedure, 580, 581, 589
Leptokurtosis, 463
Levene test of homogeneity of variance
k-means cluster analysis, 639
one-way between-subjects ANCOVA,
495, 501, 503
one-way between-subjects ANOVA, 480
one-way between-subjects MANOVA,
571, 575
one-way within-subjects ANOVA, 524
polynomial trend analysis, 487–488
t test, 463, 466, 468–469
two-way between-subjects ANOVA,
511, 514
two-way between-subjects MANOVA,
593, 598
two-way mixed ANOVA, 560, 562
Likelihood Ratio Test, 275–277, 305, 670,
693, 697
Linear function, 486, 490–491, 528
Linearity, 147, 160–161, 617
Linearity of regression, 493, 497–498
Linkage, 623
Log likelihood value
logistic regression, 261, 277
multilevel modeling, 235–236, 238,
243, 245, 252
survival analysis, 305
Log transformation, 147–155
Logistic regression
binary, 255–263
multinominal, 273–279
Mahalanobis distance, 131–134, 585
Main effects
loglinear analysis, 689–691, 693–694,
696
one-way between-subjects ANCOVA,
501, 503
one-way between-subjects MANOVA,
571
one-way within-subjects ANOVA, 522,
525, 544
two-way between-subjects ANOVA,
507–509, 516
two-way between-subjects MANOVA,
591–593, 596, 599, 602
two-way mixed ANOVA, 557–558, 562
Mantel-Cox test, 292
Mauchly sphericity test, 526, 527, 560
Maximum likelihood algorithm, 226, 331,
411
Maximum likelihood extraction, 331
Measured variables drawn as rectangles,
354. 399
Measurement model, 419, 430–431
Mediation. See Simple Mediation
Merging data
adding cases, 51–54
adding variables, 54–56
Missing values
chi square, 650
computing variables, 103, 106–108
conﬁrmatory factor analysis, 355
descriptive statistics, 60, 65, 72
entering data, 8
exploratory factor analysis, 334, 341
k-means cluster analysis, 633
linear regression, 201–202
outliers, 127
path analysis, 397
Pearson correlation, 161
sorting and selecting cases, 40
splitting data ﬁles, 46

718
SUBJECT INDEX
Missing values (continued)
structural equation modeling, 421
survival analysis, 304
t test, 460, 465, 472
Mixed level 1 multilevel model, 237–241
Mixed level 2 multilevel model, 241–245
Model identiﬁcation, 437–441
Moderation effect, 508
Modiﬁcation Indices, 372–372, 430
Monotonic relationship, 165
Multicollinearity, 224, 316
Multidimensional scaling
classic metric, 605–611
metric weighted, 613–620
Multilevel modeling
hierarchical model, 245–248
interaction model, 248–252
mixed level 1 model, 237–241
mixed level 2 model, 241–245
unconditional model, 232–237
Multiple regression, standard method, 191,
192–197
Multivariate analysis of variance
(MANOVA) designs
one-way between-subjects, 569–578
two-way between-subjects, 591–602
Multivariate data ﬁle format, 533–534
Multivariate outliers, 123, 131–137
Multivariate tests of statistical signiﬁcance
Hotelling’s Trace, 570
Pillai’s Trace, 570, 573, 596
Roy’s Largest Root, 570
Wilks’ Lambda, 528, 570, 573–574,
583, 596
Nagelkerke pseudo R2, 261, 275
Natural logarithm, 690
Nesting, 225
Normal distribution
assessing normality, 140–145
assumption of normality, 63, 147–148,
159, 165, 209–210, 387, 463, 493,
531
properties, 63–65, 139
residuals, 210, 669
superimposed on histogram, 63, 209
t distribution approximating normality,
463
transformations to achieve normality,
147–155
Normed Fit Index (NFI), 369, 378, 410,
415, 429, 448, 450
Oblique rotation, 332, 340–348
October 14, 1582 as reference date, 111
Odds ratio
logistic regression, 256, 259, 261, 263,
273, 278–279
risk analysis, 675–679, 687
Omnibus ANOVA, 479
One-sample t test, 459–462
One-way random effects model, 320
Ordinal measurement, 165
Orthogonal rotation, 332, 340, 342
Outliers
multivariate, 123, 131–137
univariate, 123–129
Output
editing. See Editing output
displays, 24–25
Output ﬁle, 3
P-P plot, 201
Paired-samples t test, 471–474
Parallel forms reliability, 311
Part correlation. See semipartial correlation
Partial correlation
linear regression, 176, 177, 185,
193–196, 209, 220
mediation analysis, 383
path analysis, 391
Path analysis
linear regression, 389–396
structural equation modeling, 397–418
Pattern coefﬁcients
conﬁrmatory factor analysis, 354, 441
exploratory factor analysis, 345–346
Pearson correlation, 159–163
beta coefﬁcient in simple linear
regression, 174
chi square, 666
compared to rank-order correlation,
165–169
conﬁrmatory factor analysis, 359
linear regression, 173, 176–178,
186–188, 194–195
mediation analysis, 384
paired-samples t test, 471, 473–474
principal components/factor analysis,
331, 339
split ﬁles, 48
reliability analysis, 317, 319
Pearson, K., 159, 319, 331 459, 645, 666,
670
Percentiles, 61–64, 71–75, 97, 101, 123,
139
Perceptual map, 610, 618–620
Personal stimulus space, 613
Phi coefﬁcient, 666, 669, 671, 677, 682,
683–686
Polynomial regression, 217–224
Pillai’s Trace. See Multivariate tests of
statistical signiﬁcance
Predictive paths drawn with double-headed
arrows, 354
Predicted variable
linear regression, 173–174, 191, 355
structural equation modeling, 355
Press’ Q statistic, 580
Pretest-posttest design, 521–522
Principal components analysis, 335–352
Principal axis factor analysis, 341–352
Proﬁle plot, 557, 594
Polynomial functions, 217–218, 485–486
Polynomial multiple regression, 217–224
Polynomial variables, computing 220
Principal axis extraction, 331, 340–352
Principal components extraction, 331,
335–352
Promax rotation, 341, 345–352
Proximities, 624
Proximity matrix, 605, 625–626, 628
Q-Q plot, 144–145
Quadratic function, 486, 491, 528
Qu´etelet index, 182
R matrix, 531–533
R2
chi square, 666
exploratory factor analysis, 344, 369
linear regression, 176, 177,179, 186,
193–195, 199, 214–215, 222–223
mediation analysis, 383
path analysis, 390, 391, 397, 409, 412
Pearson correlation, 159
one-way between-subjects ANOVA, 482
t test, 467
R2 change
linear regression, 176, 185, 193–195,
199, 209, 214–215, 218, 220,
223–224
mediation analysis, 383
path analysis, 391
Random effects, 531
Ranked data, 165–170
Recoding variables
individual values, 89–95
ranges, 86–89
Reﬂected inverse transformation, 147–155
Regression methods
all possible subsets, 192, 200–210
backward, 192
forward, 191–192
hierarchical, 211–215
polynomial, 217–224
simple, 173–179, 181–189
standard method, 191, 192–197
stepwise, 192, 197–200
Regression toward the mean, 173
Reliability
coefﬁcient alpha, 312–317
internal consistency, 311–317
K-R 20 coefﬁcient, 312

SUBJECT INDEX
719
rater consistency, 319–327
split-half reliability, 312
Reliability coefﬁcient, 311
Repeated measures, 471, 521–522, 524,
527, 531–554, 555–562
Restructuring the data ﬁle, 533–543
Risk ratio, 675–679, 687
ROC curve, 266, 268–271
Root Mean Square Error of Approximation
(RMSEA), 369, 378, 410, 415, 429,
448, 450
Rotation of components/factors, 331–332,
335, 337, 340, 353
oblique, 332, 340–348
orthogonal, 332, 340, 342
Rothamsted Agricultural Station, 477
Roy’s Largest Root. See Multivariate tests
of statistical signiﬁcance
Ryan–Enoit–Gabriel–Welsch (R-E-G-W)
test
k-means cluster analysis, 639–641
one-way between-subjects ANCOVA,
495
one-way between-subjects ANOVA,
479, 480, 482–483
one-way between-subjects MANOVA,
572, 575
polynomial trend analysis, 487, 489
two-way between-subjects ANOVA, 509
two-way between-subjects MANOVA,
593, 602
S-Stress index, 608–609, 616
Saving data ﬁles, 10–11, 31–33
Scatterplots, 218–220. 497–498, 611
Schwarz Bayesian criterion (BIC), 234,
239–240, 242, 245, 252
Scree plot, 332
Seed point, 631
Selecting cases, 39–44
Semipartial correlation
linear regression, 176–178, 188, 194,
196. 209
mediation analysis, 383
path analysis, 391
Sensitivity, 269–271
Shepard diagram, 611
Sidak corrected alpha level, 503
Simple effects tests
one-way between-subjects ANOVA, 479
one-way between-subjects MANOVA,
591, 594–595, 599–602
two-way between-subjects ANOVA,
508, 510–514, 517–519
two-way mixed ANOVA, 558, 560, 565
Simple mediation, 381–388
as a path model, 381
conditions to be met, 381–382
full mediation, , 382
partial mediation, 382
strength of mediated effect, 388, 456
suppressor variable, 382
Simple structure, 332
Skewness, 61, 65, 140–141
Slope
ANCOVA, 494, 498
linear regression, 174, 178–179, 189
multilevel modeling, 227, 237,
240–241, 244–245, 247–248
normal curve,139
polynomial trend analysis, 491
repeated measures ANOVA, 531
Sobel test, 387
Sorting cases, 39, 135–137
Spearman rho coefﬁcient, 166–170
with ties, 169–170
without ties, 166–168
Speciﬁcity, 269–271
Sphericity
exploratory factor analysis, 334, 335,
340
one-way between-subjects MANOVA,
571, 573, 593
one-way within-subjects ANOVA, 524,
526, 527, 528, 532–533
two-way mixed ANOVA, 557, 560
two-way between-subjects MANOVA,
596
Split-half reliability, 312
Splitting data ﬁles
how to split, 45–46
resetting the split ﬁle, 48–49
statistical analyses under split, 46–48
Square root transformation, 147–155
Squared correlation index (RSQ), 609, 616
Squared Euclidian distance, 625
Squared multiple correlation, 316
Standard error of the difference, 466, 469
Standard error of the mean, 3, 61, 63–64,
67, 73, 461, 465, 474
Standardized discriminant coefﬁcient, 580,
583, 585
Standardized regression coefﬁcient
conﬁrmatory factor analysis, 354, 363,
366, 371–372, 378
linear regression, 174, 178, 188–189,
191, 196, 206, 209, 221, 223–224
mediation analysis, 383–386, 388
path analysis, 390, 393–396, 402, 407,
409, 411–412
principal components/factor analysis,
345
structural equation modeling, 427,
429–430 437, 442–443, 447–448,
450–452, 456
survival analysis, 306
Standardizing variables
z scores, 79–81, 625, 632
T scores, 81–82
Stepwise regression, 192, 197–200
Strength of effect. See eta square
Strength of mediated effect, 388, 456
Stress index, 608–609, 611
Structure coefﬁcients
conﬁrmatory factor analysis, 354, 441
exploratory factor analysis, 345–352
discriminant function analysis, 580,
583–585
linear regression, 197
Suppressor variable, 382
Survival analysis
Life tables, 283–288
Kaplan-Meier, 289–300
Cox regression, 301–308
Syntax ﬁle, 3
System clock, 111
T scores, 81–82
t test
independent-samples, 463–469
one-sample, 459–462
paired-samples, 471–474
Table, copy to word processing document,
34–35
Tarone-Ware test, 292, 295
Tamhane’s T2 test, 487, 509, 572, 575,
639
Test-retest reliability, 311
Transforming variables
cube, 147
dates to age, 111–118
inverse, 147–155
log, 147–155
reﬂect, 147–155
remedy statistical assumption violations,
147–155
square, 147
square root, 147–155
Trend analysis. See Analysis of variance
(ANOVA), polynomial trend analysis
Trimmed mean, 73–74
True negatives, 265
True positives, 266
Tukey’s hinges, 74–75, 123
Two-way ﬁxed effects model, 320
Two-way random effects model, 320,
322–324
Type I error, 192, 202, 463, 469, 503
Unconditional multilevel model, 232–237
Univariate data ﬁle format, 533–534
Univariate outliers, 123–129

720
SUBJECT INDEX
Unstandardized discriminant coefﬁcient,
581, 583
Unstandardized regression coefﬁcient
conﬁrmatory factor analysis, 359, 366,
371
linear regression, 173, 174, 178, 181
187–189, 196, 207, 223
mediation analysis, 383, 385–387
path analysis, 390, 396, 393–396, 407,
409–411
structural equation modeling, 427, 429,
441–443, 447, 448, 450–454
survival analysis, 306
Unstructured covariance structure. See
Variance/covariance structures
Unweighted least squares extraction, 331
Variable View Display, 4, 7–10
Variance/covariance structures, 531
autoregressive covariance structure, 533,
543–549
compound symmetry, 524–525,
531–533, 549–552
R matrix, 531–533
unstructured covariance structure,
237–238, 241, 242, 244–245, 533,
552–554
Varimax rotation. See Rotation of
components/factors
Visual binning procedure, 97–102
Wald test, 306
Ward’s method, 625
Weighted stimulus space. See Personal
stimulus space
Weirdness index, 617–618
Welch test, 487, 639
Wilks’ Lambda. See Multivariate tests of
statistical signiﬁcance
Within-groups variance deﬁned, 478
Y intercept. See Intercept
Yates’ correction, 666–667, 670
z scores, 79–81
Zero-order correlation
linear regression, 176 177, 188, 194, 195
mediation analysis, 383
path analysis, 391


A
FIRST COURSE
IN
LINEAR MODEL
THEORY
Nalini Ravishanker and Dipak K. Dey
Texts in Statistical Science
CRC Press
Taylor &.Francis Croup
A C H A P M A N & H A L L B O O K

FIRSTCOURSE
LINEAR MODEL
THEORY

CHAPMAN & HALL/CRC
Texts in Statistical Science Series
Series Editors
C. Chatfield, University of Bath, UK
Tim Lindsey, University of Liege, Belgium
Martin Tanner, Northwestern University, USA
J. Zidek, University of British Columbia, Canada
TheAnalysis ofTime Series—
An Introduction, Fifth Edition
C. Chatfield
Applied Bayesian ForecastingandTime
SeriesAnalysis
A. Pole, M. West andJ. Harrison
Applied Nonparametric Statistical
Methods,Third Edition
P.Sprent and N.C. Smeeton
Applied Statistics— Principles and
Examples
D.R. Cox and E.J. Snell
Bayesian DataAnalysis
A. Gelman,J.Carlin,H.Stern and D. Rubin
BeyondANOVA— BasicsofApplied
Statistics
R.G.Miller,Jr.
Computer-Aided MultivariateAnalysis,
Third Edition
A.A. Afifi and V.A. Clark
A Course in Categorical DataAnalysis
T. Leonard
A Course in Large SampleTheory
T.S. Ferguson
Data Driven Statistical Methods
P Sprent
DecisionAnalysis— ABayesianApproach
J.QiSmith
ElementaryApplicationsof Probability
Theory, Second Edition
H.C.Tuckwell
Elements of Simulation
B.J.T. Morgan
Epidemiology— Study Design and
DataAnalysis
M. Woodward
An Introduction to Generalized
LinearModels,Second Edition
AJ. Dobson
Introduction to MultivariateAnalysis
C. Chatfield and A.J.Collins
Introduction to Optimization Methods
and theirApplications in Statistics
B.S. Everitt
Large Sample Methods in Statistics
PK. Sen andJ. da Motta Singer
MarkovChainMonteCarlo— Stochastic
Simulationfor Bayesian Inference
D. Gamerman
Mathematical Statistics
K. Knight
ModelingandAnalysis ofStochastic
Systems
V. Kulkarni
Modelling Binary Data
D. Collett
ModellingSurvival Data in Medical
Research
D. Collett
MultivariateAnalysisof Varianceand
Repeated Measures— A Practical
Approachfor Behavioural Scientists
D.J. Hand and C.C.Taylor
Multivariate Statistics—
A PracticalApproach
B. Flury and H. Riedwyl
Practical DataAnalysisfor Designed
Experiments
B.S.Yandell
Practical Longitudinal DataAnalysis
D.J. Hand and M. Crowder
PracticalStatisticsforMedical Research
D.G. Altman
Probability— Methodsand Measurement
A.O’Hagan
ProblemSolving— AStatistician’sGuide,
Second Edition
C.Chatfield
Essential Statistics, Fourth Edition
D.G. Rees
Interpreting Data— A First Course
in Statistics
A.J.B. Anderson

Randomization, Bootstrap and
Monte Carlo Methods in Biology,
Second Edition
B.F.J. Manly
Readings in DecisionAnalysis
S. French
Sampling Methodologies with
Applications
P. Rao
StatisticalAnalysis of Reliability Data
M.J.Crowder, A.C. Kimber,
T.J. Sweeting and R.L.Smith
Statistical Methodsfor SPCandTQM
D. Bissell
Statistical MethodsinAgriculture and
Experimental Biology, Second Edition
R.Mead, R.N. Cumow and A.M. Hasted
StatisticalProcessControl— Theoryand
Practice,Third Edition
G.B. Wetherill and D.W. Brown
StatisticalTheory, Fourth Edition
B.W. Lindgren
StatisticsforAccountants,FourthEdition
S. Letchford
StatisticsforTechnology—
A Course inApplied Statistics,
Third Edition
C. Chatfield
Statisticsin Engineering—
A Practical Approach
A.V.Metcalfe
Statistics in Research and Development,
Second Edition
R. Caulcutt
TheTheoryof LinearModels
B.Jorgensen
A First Course in LinearModelTheory
Nalini Ravishanker and Dipak K. Dey


A
FIRST COURSE
IN
LINEAR MODEL
THEORY
Nalini Ravishanker and Dipak K. Dey
Department of Statistics
University of Connecticut
Storrs, Connecticut
AUChapman & Hall/CRC
T3nl Taylor & FrancisGroup
Boca Raton
London
New York
Chapman & Hall/CRC is an imprint of the
Taylor Si Francis Group, an informa business

Library of Congress Cataloging-in-Publication Data
Ravishanker, Nalini.
A first course in linear model theory / Nalini Ravishanker and Dipak K. Dey
p.
cm. —
(Texts in statistical science series)
Includes bibliographical references and index.
ISBN 1-58488-247-6 (alk. paper)
1. Linear models (Statistics) I. Dey, Dipak. II. Title. III. Texts in statistical science.
QA276.R38 2001
519.5'35— dc21
2001053726
CIP
This book contains information obtained from authentic and highly regarded sources. Reprinted material
is quoted with permission, and sources are indicated. A wide variety of references are listed. Reasonable
efforts have been made to publish reliable data and information, but the authors and the publisher cannot
assume responsibility for the validity of all materials or for the consequences of their use.
Neither this book nor any part may be reproduced or transmitted in any form or by any means, electronic
or mechanical, including photocopying, microfilming, and recording, or by any information storage or
retrieval system, without prior permission in writing from the publisher.
The consent of CRC Press LLC does not extend to copying for general distribution, for promotion, for
creating new works, or for resale. Specific permission must be obtained in writing from CRC Press LLC
for such copying.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are
used only for identification and explanation, without intent to infringe.
Visit the CRC Press Web site at www.crcpress.com
© 2002 by Chapman & Hall/CRC
No claim to original U.S. Government works
International Standard Book Number 1-58488-247-6
Library of Congress Card Number 2001053726
3 4 5 6 7 8 9 0
Printed in the United States of America
Printed on acid-free paper

To Ravi, Vivek and Varan.
N.R.
To Rita and Debosri
D.K.D.
Vll


Preface
Linear Model theory plays a fundamental role in the foundation of mathemat-
ical and applied statistics. It has a base in distribution theory and statistical
inference, and finds application in many advanced areas in statistics including
univariate and multivariate regression, analysis of designed experiments, longi-
tudinal and time series analysis, spatial analysis, multivariate analysis, wavelet
methods, etc. Most statistics departments offer at least one course on linear
model theory at the graduate level. There are several excellent books on the
subject, such as “Linear Statistical Inference and its Applications” by C.R.
Rao, “Linear Models” by S.R. Searle, “Theory and Applications of the Linear
Model” by F.A. Graybill, “ Plane Answers to Complex Questions: The Theory
of Linear Models” by R. Christiansen and “The Theory of Linear Models” by
B. Jorgensen.
Our motivation has been to incorporate general principles of inference in
linear models to the fundamental statistical education of students at the grad-
uate level, while our treatment of contemporary topics in a systematic way will
serve the needs of professionals in various industries. The three salient features
of this book are: (1) developing standard theory of linear models with numer-
ous applications in simple and multiple regression, as well as fixed, random
and mixed-effects models, (2) introducing generalized linear models with exam-
ples, and (3) presenting some current topics including Bayesian linear models,
general additive models, dynamic linear models and longitudinal models. The
first two chapters introduce to the reader requisite linear and matrix algebra.
This book is therefore a self-contained exposition of the theory of linear mod-
els, including motivational and practical aspects. We have tried to achieve a
healthy compromise between theory and practice, by providing a sound theo-
retical basis, and indicating how the theory works in important special cases
in practice. There are several examples throughout the text. In addition, we
provide summaries of many numerical examples in different chapters, while a
more comprehensive description of these is available in the first author’s web
site (http://www.stat.uconn.edu/~nalini). There are several exercises at the
end of each chapter that should serve to reinforce the methods.
Our entire book is intended for a two semester graduate course in linear
models. For a one semester course, we recommend essentially the first eight
chapters, omitting a few subsections, if necessary, and supplementing a few
selected topics from chapters 9-11, if time permits. For instance, section 5.5,
section 6.4, sections 7.5.2-7.5.4, and sections 8.5, 8.7 and 8.8 may be omitted
in a one semester course. The first two chapters, which present a review on
vectors and matrices specifically as they pertain to linear model theory, may
also be assigned as background reading if the students had previous exposure
IX

X
to these topics. Our book requires some knowledge of statistics; in particular,
a knowledge of elementary sampling distributions, basic estimation theory and
hypothesis testing at an undergraduate level is definitely required. Occasionally,
more advanced concepts of statistical inference are invoked in this book, for
which suitable references are provided.
The plan of this book follows. The first two chapters develop basic con-
cepts of linear and matrix algebra with a view towards application in linear
models. Chapter 3 describes generalized inverses and solutions to systems of
linear equations. We develop the notion of a general linear model in Chapter 4.
An attractive feature of our book is that we unify full-rank and non full-rank
models in the development of least squares inference and optimality via the
Gauss-Markov theorem. Results for the full-rank (regression) case are provided
as special cases. We also introduce via examples, balanced ANOVA models
that are widely used in practice. Chapter 5 deals with multivariate normal and
related distributions, as well as distributions of quadratic forms that are at the
heart of inference. We also introduce the class of elliptical distributions that
can serve as error distributions for linear models. Sampling from multivari-
ate normal distributions is the topic of Chapter 6, together with assessment of
and transformations to multivariate normality. This is followed by inference for
the general linear model in Chapter 7. Inference under normal and elliptical
errors is developed and illustrated on examples from regression and balanced
ANOVA models. In Chapter 8, topics in multiple regression models such as
model checking, variable selection, regression diagnostics, robust regression and
nonparametric regression are presented. Chapter 9 is devoted to the study of
unbalanced designs in fixed-effects ANOVA models, the analysis of covariance
(ANACOVA) and some nonparametric test procedures. Random-effects mod-
els and mixed-effects models are discussed in detail in Chapter 10. Finally in
Chapter 11, we introduce several special topics including Bayesian linear mod-
els, dynamic linear models, linear longitudinal models and generalized linear
models (GLIM). The purpose of this chapter is to introduce to the reader some
new frontiers of linear models theory; several references are provided so that the
reader may explore further in these directions. Given the exploding nature of
our subject area, it is impossible to be exhaustive in a text, and cover every-
thing that should ideally be covered. We hope that our judgment in choice of
material is appropriate and useful.
Most of our book was developed in the form of lecture notes for a sequence
of two courses on linear models which both of us have taught for several years in
the Department of Statistics at the University of Connecticut. The numerical
examples in the text and in the web site were developed by NR over many years.
In the text, we have acknowledged published work, wherever appropriate, for
the use of data in our numerical examples, as well as for some of the exercise
problems. We are indeed grateful for their use, and apologize for any inadvertent
omission in this regard.

XI
In writing this text, discussions with many colleagues were invaluable. In
particular, we thank Malay Ghosh, for several suggestions that vastly improved
the structure and content of this book.
We deeply appreciate his time and
goodwill. We thank Chris Chatfield and Jim Lindsey for their review and for
the suggestion about including numerical examples in the text. We are also
very grateful for the support and encouragement of our statistical colleagues, in
particular Joe Glaz, Bani Mallick, Alan Gelfand and Yazhen Wang. We thank
Ming-Hui Chen for all his technical help with Latex.
Many graduate students helped in proof reading the typed manuscript; we
are especially grateful to Junfeng Liu, Madhuja Mallick and Prashni Paliwal.
We also thank Karen Houle, a graduate student in Statistics, who helped with
“ polishing-up” the numerical examples in NR’s web site. We appreciate all the
help we received from people at Chapman & Hall/CRC - Bob Stern, Helena
Redshaw, Gail Renard and Sean Davey.
Ravi, what can I say, except thanks for the warm smiles and hot dinners! N.R.
Rita and Debosri, without your sacrifice, the project wouldn’t be completed on
time. D.K.D.
Nalini Ravishanker and Dipak K. Dey
Department of Statistics
University of Connecticut
Storrs, CT


Contents
1
A Review of Vector and Matrix Algebra
1.1
Notation
1.2
Basic definitions and properties
Exercises
1
1
3
28
2
Properties of Special Matrices
2.1
Partitioned matrices
2.2
Algorithms for matrix factorization
2.3
Symmetric and idempotent matrices
2.4
Nonnegative definite quadratic forms and matrices
2.5
Simultaneous diagonalization of matrices
2.6
Geometrical perspectives
2.7
Vector and matrix differentiation
2.8
Special operations on matrices
2.9
Linear optimization
Exercises
33
33
40
45
51
57
58
63
66
69
70
3
Generalized Inverses and Solutions to Linear Systems
3.1
Generalized inverses
3.2
Solutions to linear systems
Exercises
73
73
82
88
4 The General Linear Model
4.1
Model definition and examples
4.2
The least squares approach
4.3
Estimable functions
4.4
Gauss-Markov theorem
4.5
Generalized least squares
4.6
Estimation subject to linear restrictions
4.6.1
Method of Lagrangian multipliers
4.6.2
Method of orthogonal projections
Exercises
91
91
96
113
118
122
129
. . 129
131
133
xm

CONTENTS
xiv
5
Multivariate Normal and Related Distributions
5.1
Multivariate probability distributions
5.2
Multivariate normal distribution and properties . .
5.3
Some noncentral distributions
5.4
Distributions of quadratic forms
5.5
Alternatives to the multivariate normal distribution
5.5.1
Mixture of normals distribution
5.5.2
Spherical distributions
5.5.3
Elliptical distributions
Exercises
137
137
145
164
172
181
181
184
185
190
6
Sampling from the Multivariate Normal Distribution
6.1
Distribution of the sample mean and covariance matrix
6.2
Distributions related to correlation coefficients . . . .
6.3
Assessing the normality assumption
6.4
Transformations to approximate normality
6.4.1
Univariate transformations
6.4.2
Multivariate transformations
Exercises
195
195
200
204
209
209
211
212
7
Inference for the General Linear Model
7.1
Properties of least squares estimates
7.2
General linear hypotheses
7.2.1
Derivation of and motivation for the F-test . .
,
7.2.2
Power of the F-test
7.2.3
Testing independent and orthogonal contrasts .
7.3
Confidence intervals and multiple comparisons
7.3.1
Joint and marginal confidence intervals
7.3.2
Simultaneous confidence intervals
7.3.3
Multiple comparison procedures
7.4
Restricted and reduced models
7.4.1
Nested sequence of hypotheses
7.4.2
Lack of fit test
7.4.3
Non-testable hypotheses
7.5
Likelihood based approaches
7.5.1
Maximum likelihood estimation under normality
7.5.2
Elliptically contoured linear model
7.5.3
Model selection criteria
7.5.4
Other types of likelihood analyses
Exercises
215
215
219
219
231
232
233
233
236
239
246
246
263
266
266
267
269
270
271
277
8
Multiple Regression Models
8.1
Departures from model assumptions
8.1.1
Graphical procedures
8.1.2
Sequential and partial F-tests
8.1.3
Heteroscedasticity
281
281
282
285
287

CONTENTS
xv
8.1.4
Serial correlation
8.1.5
Stochastic X matrix
Model selection in regression
Orthogonal and collinear predictors
8.3.1
Orthogonality in regression
8.3.2
Multicollinearity
8.3.3
Ridge regression
8.3.4
Principal components regression
Prediction intervals and calibration
Regression diagnostics
8.5.1
Further properties of the projection matrix . . .
8.5.2
Types of residuals
8.5.3
Outliers and high leverage observations
8.5.4
Diagnostic measures based on influence functions
Dummy variables in regression
Robust regression
8.7.1
Least absolute deviations ( LAD ) regression . . .
8.7.2
M-regression
Nonparametric regression methods
8.8.1
Additive models
8.8.2
Projection pursuit regression
8.8.3
Neural networks regression
8.8.4
Curve estimation based on wavelet methods . . .
Exercises
291
295
8.2
296
8.3
304
304
307
309
313
8.4
314
8.5
319
320
321
325
326
8.6
336
8.7
339
340
343
8.8
344
345
347
348
350
353
9
Fixed Effects Linear Models
9.1
Checking model assumptions
9.2
Inference for unbalanced ANOVA models . .
9.2.1
One-way cell means model
9.2.2
Higher-order overparametrized models
9.3
Analysis of covariance
9.4
Nonparametric procedures
9.4.1
Kruskal-Wallis procedure
9.4.2
Friedman’s procedure
Exercises
357
357
359
361
363
371
378
379
381
381
10 Random-Effects and Mixed-Effects Models
10.1 One-factor random-effects model
10.1.1 ANOVA method
10.1.2 Maximum likelihood estimation . . .
10.1.3 Restricted maximum likelihood (REML) estimation
. . . 395
10.2 Mixed-effects linear models
10.2.1 Extended Gauss-Markov theorem
10.2.2 Estimation procedures
Exercises
385
385
388
392
395
396
398
404

CONTENTS
xvi
11 Special Topics
11.1 Bayesian linear models
11.2 Dynamic linear models
11.2.1 Kalman filter equations
11.2.2 Kalman smoothing equations . .
11.3 Longitudinal models
11.3.1 Multivariate models
11.3.2 Two-stage random-effects models
11.4 Generalized linear models
11.4.1 Components of GLIM
11.4.2 Estimation approaches
11.4.3 Residuals and model checking . .
11.4.4 Generalized additive models . . .
Exercises
407
407
411
412
415
416
417
420
422
422
424
428
430
. 431
A Review of Probability Distributions
433
Solutions to Selected Exercises
441
References
449
Author Index
465
Subject Index
469

Chapter 1
A Review of Vector and
Matrix Algebra
In this chapter, we introduce basic results dealing with vector spaces and ma-
trices, which are essential for an understanding of univariate and multivariate
linear statistical methods. We provide several numerical and geometrical illus-
trations of these concepts. The material presented in this chapter will be found
in most textbooks that deal with matrix theory pertaining to linear models, in-
cluding Graybill (1983), Harville (1997), Rao (1973a) and Searle (1982). Unless
stated otherwise, all vectors and matrices are assumed to be real, i.e., they have
real numbers as elements.
1.1
Notation
An m x n matrix A is a rectangular array of real numbers of the form
an
ai2
&21
&22
&ln
Q'2n
— { CLij }
A =
\®m l
^m2
*
* *
O'mn/
with row dimension m, column dimension n, and (i, j)th element a*j. For ex-
ample,
5
4
1
-3
2
6
A =
is a 2 x 3 matrix. An n-dimensional column vector
aA
a2
a =
\an/
1

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
2
can be thought of as a matrix with n rows and one column. For example,
A).25\
3
0.50
b =
1 I , and c =
a =
0.75
5
\i -°oy
are respectively 2-dimensional, 3-dimensional and 4- dimensional vectors. An
n-dimensional column vector with each of its n elements equal to unity is called
the unit vector, and is denoted by ln, while a column vector whose elements are
all zero is called the null vector and is denoted by 0n. For any integer n > 1,
we can write an n-dimensional column vector as a = (ai, • • • , an )', i.e., as the
transpose of the n-dimensional (row) vector with components ai, • • • ,an.
An m x n matrix A with the same row and column dimensions, i.e., with
m = n, is called a square matrix of order n. An nxn identity matrix is denoted
by In; each of its n diagonal elements is unity while each off-diagonal element
is zero. An m x n unit matrix J
has each element equal to unity. An nxn
mn
unit matrix is denoted by Jn. For example, we have
1
0
0\
0
1
0 j and J3 =
0
0
1/
1 1 1
1
1
1
1 1 1
I3 =
An nxn matrix whose elements are zero except on the diagonal, where the
elements are nonzero, is called a diagonal matrix. We will denote a diagonal
matrix by D =diag(di,
• • • , dn ). Note that In is an n x n diagonal matrix,
written as In =diag(l,1, • • • ,1). An m x n matrix C all of whose elements are
equal to the same constant c is called a constant matrix. If c = 0, the resulting
matrix is the null matrix 0. An nxn matrix is said to be an upper triangular
matrix if all the elements below and to the left of the main diagonal are zero.
Similarly, if all the elements located above and to the right of the main diagonal
are zero, then the nxn matrix is said to be lower triangular. For example,
5
0
0
0
2 -6 1 and L =|4
2
0
3 -6
5
5
4
3
U =
0
0
5
are respectively upper triangular and lower triangular matrices. A square matrix
is triangular if it is either upper triangular or lower triangular. A triangular
matrix is said to be a unit triangular matrix if aij = 1 whenever i = j. Unless
explicitly stated, we assume that vectors and matrices are non null.
A submatrix of a matrix A is obtained by deleting certain rows and/or
columns of A. For example, let
1
3
5
5
4
1
— 9| a n d B =
-3
2
6
7
5
4
1\
-3
2
6
'
A =
4

1.2. BASIC DEFINITIONS AND PROPERTIES
3
The 2 x 3 submatrix B has been obtained by deleting Row 1 and Column 4 of
the 3 x 4 matrix A. Any matrix can be considered to be a submatrix of itself.
An r x r principal submatrix B of an n x n matrix A is obtained by deleting the
same rows and columns from A. For r = 1, 2, • • • , n, the r xr leading principal
submatrix of A is obtained by deleting the last (n — r) rows and columns from
A. The 2 x 2 leading principal submatrix of the matrix A shown above is
1
3
B =
5
4 /
*
It may be easily verified that a principal submatrix of a diagonal, upper tri-
angular or lower triangular matrix is respectively diagonal, upper triangular or
lower triangular.
Some elementary properties of vectors and matrices are given in the next
section. Familiarity with this material is recommended before a further study
of properties of special matrices that are described in the following two chapters.
1.2
Basic definitions and properties
An n-dimensional vector a is an ordered set of measurements, which can be rep-
resented geometrically as a directed line in n-dimensional space with component
a\ along the first axis, component 02 along the second axis, • • •, and compo-
nent an along the nth axis. We can represent 2-dimensional and 3-dimensional
vectors respectively as points in the plane and in 3-dimensional space.
R 2
R 3
x3
x2
b.2
b
(-2,3) \
(U)
\
*3
\
b
” 7
1
xi
X2
X1
(b)
(a)
Figure 1.2.1.
Geometric representation of two and three-dimensional vectors.
Any 2-dimensional vector a = (01, 02)' can be graphically represented by the
point with coordinates (ai, 02) in the Cartesian coordinate plane, or as the arrow
starting from the origin (0, 0), whose tip is the point with coordinates (01, 02).

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
4
For n= 2, Figure 1.2.1 (a) shows the vectors (1,1) and (— 2, 3) as arrows starting
from the origin. For n = 3, Figure 1.2.1 (b) shows a vector b = (61 , 62, 63)' in
U 3.
Two vectors can be added (or subtracted) only if they have the same dimen-
sion, in which case the sum (or difference) of the two vectors is the vector of
sums (or differences) of their elements, i.e.,
a ± b = (ai ± 61, a2 ± 62, • • • ,an ± 6n)'.
The sum of two vectors emanating from the origin is the diagonal of the par-
allelogram which has the vectors a and b as adjacent sides. Vector addition is
commutative and associative, i.e., a 4- b = b+a, and a 4- (b + c) = (a + b) + c.
The scalar multiple ca of a vector a is obtained by multiplying each element
of a by the scalar c, i.e.,
ca = (cai, • • • , can )'.
Scalar multiplication has the effect of expanding or contracting a given vector.
Scalar multiplication of a vector obeys the distributive law for vectors, the
distributive law for scalars, and the associative law, i.e., c(a 4- b) = ca -I- cb,
(ci + c2)a = Cia 4- c2a, and ci(c2a) = (cic2)a.
Also, a + 0 = 0 + a = a,
la = a, and for every a, there exists a corresponding vector a such that a -fa =
0. A collection of n-dimensional vectors (with the associated field of scalars)
satisfying the above properties is a linear vector space and is denoted by Vn.
The product of two vectors can be formed only if one of them is a row vector
and the other is a column vector and the result is called their inner product or
dot product.
The inner product of two
Definition 1.2.1. Inner product of vectors.
n-dimensional vectors a and b is denoted by a•b or a'b and is the scalar
/61\
62
= EILi a*bi-
a b — (fli , &2,
? &n)
—
ci\b\ 4- a262 4 • • • 4- cinbn
b\un/
The inner product of a vector a with itself is a'a. The positive square root
of this quantity is called the Euclidean norm, or length, or magnitude of the
vector, and is
II a ll= (al + a2 + "' + an)1/2
•
The Euclidean distance between two vectors a and b is defined by
Geometrically, the length of a vector a = (ai, a2) in two dimensions may be
viewed as the hypotenuse of a right triangle, whose other two sides are given by
the vector components, ai and a2. Scalar multiplication of a vector a changes
its length,

1.2. BASIC DEFINITIONS AND PROPERTIES
5
|| ca ||= (c2a2 + c2a2 H
b c2a2 )1/2 = |c|(a2 + a2 H
b a2 ) x/2 =|c||| a || .
If |c| > 1, a is expanded by scalar multiplication, while if|c| < 1, a is contracted.
If c = 1/ || a ||, the resulting vector is defined to be b = a/ || a
dimensional unit vector with length 1. A vector has both length and direction.
If c > 0, scalar multiplication does not change the direction of a vector a.
However, if c < 0, the direction of the vector ca is in opposite direction to the
vector a. The unit vector a/ || a || has the same direction as a. The angle 0
between two vectors a and b is defined in terms of their inner product as
the n-
cos 9 = a'b/ || a HI! b ||= a'b/v/a7a\/b'b
(see Figure 1.2.2.). Since cos # = 0 only if a'b = 0, a and b are perpendicular
(or orthogonal) when a'b = 0.
Figure 1.2.2. Inner product of two vectors.
Result 1.2.1.
dimensional vectors and let d be a scalar. Then
Properties of inner product.
Let a, b, and c be n-
1. a•b = b•a
2. a « (b -f c) = a » b + a t c
3. d(a•b) = (da) •b = a•(db )
4. a•a > 0, with equality if and only if a = 0
5. || a ± b ||2=|| a ||2 + || b ||2 ±2a•b
6. |a•b| <|| a |||| b
7. || a -b b ||<|| a || -f- || b ||.

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
6
The last two inequalities in Result 1.2.1 are respectively the Cauchy-Schwarz
inequality and the triangle inequality, which we ask the reader to verify in
Exercise 1.2. Equality holds in property 6 if and only if a = 0, or b = 0, or a
and b are scalar multiples of each other. In property 7, equality holds if and
only if a = 0, or b = 0, or b = ca for some constant c > 0. Geometrically,
the triangle inequality states that the length of one side of a triangle does not
exceed the sum of the lengths of the other two sides.
Definition 1.2.2. Outer product of vectors. The outer product of two
vectors a and b is denoted by a A b or ab' and is obtained by post-multiplying
the column vector a by the row vector b'. There is no restriction on the dimen-
sions of a and b; if a is an m x 1 vector and b is an n x 1 vector, the outer
product ab' is an m x n matrix.
Example 1.2.1. We illustrate all these vector operations by an example. Let
2
6
10
3 ,
b =
7
and d =
a =
207 '
4
9
Then,
8
-4
60
a — b =
a + b =
10
10b =
— 4
70 1 ,
13
-5
90
a'b = 2 x 6 + 3 x 7 + 4 x 9 = 69
2
12
14
18
ab' = ( 3| (6
7
9) = ( 18
21
27
24
28
36
and
4
2
20
40\
30
60 .
40
80/
ad' =
3
(10
20)
4
However, a + d, a'd, and b'd are undefined.
The set of all linear combinations of the n-dimensional
Definition 1.2.3.
vectors {v* = (v*i,
• • , u*n),
Vij G K, i = 1, 2, • • • , 1} is called their span and
is denoted by Span{vi, •
• , v*}. For example, the vectors 0, vi, V2, vi 4- V2,
10vi, 5vi - 3V2 all belong to Span{vi, v2}. A vector u is in Span{vx, • • • , v*}
if and only if there are scalars ci, • • • , Q such that u = CiVi H
+ QVJ .
Definition 1.2.4. Vector space.
A vector space is a set
Vn = {Vj = (Uu, * * * , Vin),
Vij
11, i - 1, 2,
, /}

1.2. BASIC DEFINITIONS AND PROPERTIES
7
which is closed under addition and multiplication by a scalar, and contains the
vector 0. For example, 7Zn is a vector space for any positive integer n — 1, 2, •
•.
As another example, consider k linear equations in n variables x\, • • • ,xn:
F ‘
Cin^Ti — 0>
2 =
where Cij are real constants. Then the totality of solutions (xi, • • • , xn ) consid-
ered as vectors is a subspace of 1Zn. We discuss solutions of linear equations in
Chapter 3.
Definition 1.2.5.
vector 0 and a subset of vectors in Vn. If Sn is also a vector space, it is called a
subspace of Vn. For example, {0} and Vn are (trivially) subspaces of Vn. Any
plane through the origin is a subspace of 1ZZ.
Vector subspace. Let Sn be a space consisting of the
Definition 1.2.6. Linear dependence and independence of vectors. Let
{vx , • • • , vm} denote n-dimensional vectors in Vn. These m vectors are said to
be linearly dependent if and only if there exist scalars ci, • • • , Cm, not all zero,
such that 53^
! fyVi = 0. If all the c% are zero, then vi , • • • , vm are said to be
linearly independent (LIN) vectors. For example, the null vector 0 is a linearly
dependent set, as is any set of vectors containing 0.
Example 1.2.2. Letvi = (l
— 1
3)
/ andv2 = (l
1
l)
;. Now,
c*v* =
0
C\ -I- C2 = 0, -Ci 4- C2 = 0, and 3ci + C2 = 0, for which the only solution is
cx = c2 = 0. Hence, vx and v2 are LIN vectors.
Example 1.2.3. The vectors
Vi = (1
-1)
linearly dependent, which is verified by setting
we see that £i=1
= 0.
(l
2)', and v3 = (2
l)'
ci = 1, C2 = 1, and c3 = — 1;
v2 =
are
The following properties hold:
1. If m > 1 vectors vi , - - - , vm are linearly dependent, we can express at
least one of them as a linear combination of the others.
Result 1.2.2.
2. If s of the m vectors are linearly dependent, where s < m, then all m
vectors are linearly dependent.
3. If m > n, then vi , • • • , vm are linearly dependent.
4. There can be at most n LIN n-dimensional vectors.
5. The totality of all vectors which are linearly dependent on the n-dimensional
vectors vi , • • • , vm is a vector space. The dimension of this vector space
is the maximum number of LIN vectors in the space. A formal definition
follows.

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
8
Definition 1.2.7. Basis of a vector space.
Let Vi , • • • , vm be a set of m
LIN vectors in Vn that span Vn, i.e., each vector in Vn is obtained as a linear
combination of these m vectors. Then, {vi , • • • , vm} is called a basis (Hamel
basis) for Vn and the dimension of Vn is dim(Vn) = m.
Every vector space Vn has a basis, which is seen as follows. Sequentially
choose non null vectors vi, V2, * • • in Vn so that no v* is linearly dependent on
the preceeding vectors. Suppose that after the choice of m vectors, there is no
other LIN vector in Vn, we say that vi, • • • , vm is a basis of the m-dimensional
vector space Vn. Although a vector space Vn may be infinite dimensional, we
only focus on finite dimensional vector spaces. Every vector in Vn has a unique
representation in terms of a given basis. If we can represent a vector x in Vn as
CjVi and^
then,^2(ci — di )vi = 0, which, by Definition 1.2.6 is possible
only if Ci — di for all i. Note that a basis of a vector space is not unique. How-
ever, if vi, • • • , vm and uq, • • • ,
are two choices for a basis of Vn, then m= k.
The cardinal number m which is common to all bases of Vn is the maximum
number of LIN vectors in Vn (or, the minimum number of vectors that span Vn).
To verify that fc = m, let us suppose that on the contrary, k > m and consider
the linearly dependent set of vectors ui, vi, * •• , vm. Suppose v; depends on the
preceeding vectors, then Vn can be generated by ui, vi, • • • , v*_i, Vi+i, • • • , vm.
Then, the set of vectors 112, ui, vi, • • • , v*_i, Vi+i,
• , vm are linearly depen-
dent, which in turn implies that we may discard one more v from this set. We
continue this process of discarding a vector v and including a vector u until we
have the set of vectors Ui, •
, um, which spans Vn. Hence, ( k — m) of the u
vectors are redundant. The matrix whose columns are the basis of Vn is called
the basis matrix of Vn.
A vector space V is said to be the direct sum of subspaces Vi, • • • ,14, i.e.,
V = Vi0 • • •0Vfc, if a vector v e V can be uniquely written as v = vi H
l-v*,
where v* G Vj. Also, Vi HVj = {0}, and dim(V) = dim(Vi) H
+ dim(Vfc).
The m-dimensional vector with 1 for its ith component and zeroes elsewhere
is denoted by e*. The vectors ei, • • • ,em are called the standard basis vectors of
the vector space 7Zm, of dimension m. For example, the standard basis vectors
in 1Z2 are ei = (1,0)' and e2 = (0,1)', while those in 7Z? are ei = (1,0,0)',
e2 = (0, 1,0)', and e3 = (0, 0,1)'. Any n-dimensional vector x can be written as
x = a:iei +
\- xnen. Any vector space of dimension m is isomorphic to 7£m,
so the study of m-dimensional vector spaces is equivalent to the study of 7Zm.
Further, there exists an isomorphism between two vector spaces that have the
same dimension. Useful notions of distance or angle between vectors in a space
Vn were given in Definitions 1.2.1, Result 1.2.1, and the associated discussion.
Definition 1.2.8. Orthogonal vectors. Two vectors vi and V2 in Vn are
orthogonal if and only if vi •V2 = v'jV2 = V2V1 = 0; we write vx 1 V2.
Pythagoras’s Theorem states that the n-dimensional vectors vi and V2 are
orthogonal if and only if
II V! + v2 ||2=|| vi ||2 + || v2 ||2;

1.2. BASIC DEFINITIONS AND PROPERTIES
9
this is illustrated in Figure 1.2.3.
Result 1.2.3. Ifvi , V2, * “
, vn are nonzero vectors which are mutually orthog-
onal, i.e., v
^
Vj = 0, i^
j, i, j = 1, • • • , n, then these vectors are LIN.
Definition 1.2.9. Normal vector. A vector Vi is said to be a normal vector
if vi •vi = v'jVi = 1.
V|+ V2
V2
II Vl+ V2|[
II v2||
Vi
llv.ii
Figure 1.2.3. Pythagoras’s theorem.
Definition 1.2.10. Orthonormal basis of Vn. A basis {vi , - • • , vm} of a
vector space Vn such that v - Vj = 0, for all i ^
j, i, j = 1, • • • , m is called
an orthogonal basis.
If further, v
^
v* = 1 for i = 1,
• , m, it is called an
orthonormal basis of Vn .
Let {v i , • • • , vm}
Gram-Schmidt orthogonalization.
Result 1.2.4.
denote an arbitrary basis of Vn . To construct an orthonormal basis of Vn starting
from {v i , * • • , vm}, we define
yi
vi
fc-i yjvfc
-E
Yi ,
/c = 2, • • • , m,
yk
^
II 2
2=1
yfc
k
Yk il
5
= !, - • • , m.
zk
It may be easily verified that {zi , • • • , zm} is an orthonormal basis of Vn. The
stages in this process for a basis {v i , V2, V3} are shown in Figure 1.2.4.

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
10
Example 1.2.4.
from the basis vectors Vi = (1, — 1,1)', v2 = (-2,3, — 1)', and V3 = (1, 2,-4)'.
Let yi = vi. We compute y/
1v2 = -6, and y
^
yi = 3, so that y2 = (0,1,1)'.
Next, yiv3 = -5,
= -2, and y'2y2 = 2, so that y3 = (8/3, 4/3, — 4/3)'.
It is easily verified that {yi, y2, y3} is an orthogonal basis and also that zi =
(1/A-1/A,1/A)', z2 = (0,1/A,1/A)', and z3 = (2/A,1/A,-1/A)'
form a set of orthonormal basis vectors.
We use Result 1.2.4 to find an orthonormal basis starting
V3
Y j
V3
V3
,.W
YT^vr
YI
Yi
Yi
'\r*
V*
y2
#2
Yz
Figure 1.2.4. Gram-Schmidt orthogonalization.
We next describe some elementary properties of matrices and provide illus-
trations. More detailed properties of special matrices that are relevant to linear
model theory are given in Chapter 2.
Definition 1.2.11.
m x n matrices A and B, each of the same dimension, C = A ± B is an mxn
matrix whose
element is
± bij. For example,
Matrix addition and subtraction.
For arbitrary
-5
4
1
-3
2
6
7 -9
10
"
*
“ 1 2
6
-1
2 -5
11\
-1
8
5 / '
Multiplication of a matrix by a scalar.
Definition 1.2.12.
arbitrary mxn matrix A, and an arbitrary real scalar c, B = cA = Ac is an
mxn matrix whose (i, j)th element is btj = caij. For example,
For an
-25
20
5 \
-15
10
30/ '
When c = — 1, we denote (-l)A as — A, the negative of the matrix A.
Result 1.2.5. Laws of addition and scalar multiplication. Let A, B, C
be any m x n matrices and let a, 6,c be any scalars. The following results hold:

1.2. BASIC DEFINITIONS AND PROPERTIES
11
1. (A + B) + C = A + (B + C)
6. (a + b)C = aC + 6C
7. (afc)C = a(6C) = b(aC )
8. OA = 0
9. 1A = A.
2. A + B = B -f A
3. A 4- (-A) = (— A) + A = 0
4. A + 0 = 0 + A = A
5. c(A + B) =cA + cB
Definition 1.2.13. Matrix multiplication.
B of respective dimensions mxn and n x p, C = AB is an m x p matrix whose
(iyj )th element is Cij = XIILi aubij- The product AB is undefined when the
column dimension of A is not equal to the row dimension of B. For example,
For arbitrary matrices A and
7
5
4
1
-3
2
6
25
-3
V-15 /
2
In referring to the matrix product AB, we say that B is pre-multiplied by
A, and A is post-multiplied by B. Provided all the matrices are conformal
under multiplication, the following properties hold.
Result 1.2.6. Laws of matrix multiplication.
Let a be a scalar, let A
be an m x n matrix and let matrices B and C have appropriate dimensions so
that the operations below are defined. Then,
1. (AB)C = A(BC)
2. A(B + C) = AB + AC
5. ImA = AIn= A
3. (A + B)C = AC + BC
6. OA = 0 and AO = 0.
In general, matrix multiplication is not commutative, i.e., AB is not neces-
sarily equal to BA. Note that depending on the row and column dimensions of
A and B, it is possible that (i) only AB is defined and BA is not, or (ii) both
AB and BA are defined, but do not have the same dimensions, or (iii) AB and
BA are defined and have the same dimensions, but AB ^
BA. Two n x n
matrices A and B are said to commute under multiplication if AB = BA. A
collection of n x n matrices Ai, • • • , A* is said to be pairwise commutative if
AiAj = AjAi for j > i, i, j = 1, • • • , k. Note that the product Ak = A • • •A
( k times) is defined only if A is a square matrix. It is easy to verify that
JmnJnp
4. a(BC) = (aB)C = B(aC)
= nJ771p •
Example 1.2.5.
is upper triangular. Let
We show that the product of two upper triangular matrices
bn
bn
613
0
622 ^23
0
0
633
an
a12
^13
0
a22
&23
0
0
A =
and B =
^33
be 3 x 3 upper triangular matrices. By Definition 1.2.13, their product is
flll&ll ^11^
12 T Ul2^
>22 &11&13+&12&23 + U13&33
^22^22
&22^23 + ^23633
^33^33
AB =
0
0
0

CHAPTER 1 . VECTOR AND MATRIX ALGEBRA
12
which is upper-triangular.
Definition 1.2.14. Matrix transpose. The transpose of an m x n matrix
A is an n x m matrix whose columns are the rows of A in the same order. The
transpose of A is denoted by A'. For example,
2
4
2
1
6
4
3
5
6
7
6
8
A' =
B' =
A =
1
3
B =
8
9
7
9 /
*
6
5
As we saw earlier, the transpose of an n-dimensional column vector with
components ai, -
* - ,an is the row vector (ai, * * * ,an). It is often convenient
to write a column vector in this transposed form. The transpose of an upper
(lower) triangular matrix is a lower (upper) triangular matrix. It may be easily
verified that the n x n unit matrix may be written as Jn = lnl^
. For any
matrix A, each diagonal element of A'A is nonnegative.
Result 1.2.7. Laws of transposition.
Let A and B conform under ad-
dition, and let A and C conform under multiplication. Let a, b and c denote
scalars and let k > 2 denote a positive integer. Then,
1. (A')'= A
2. (aA + 6B)' = aA'+frB'
3. (cA)' = cA'
Definition 1.2.15. Symmetric matrix. A matrix A is said to be symmetric
if A' = A. For example,
4. A'= B' if and only if A = B
5. (AC)'= C'A'
6. (Ai-A/O' = A'^ - Ai.
1
2 -3
2
4
5
-3
5
9
A =
is a symmetric matrix. Note that a symmetric matrix is always a square matrix.
Any diagonal matrix, written as D — diag(di, -
« - ,dn), is symmetric. Other
examples of symmetric matrices include the variance-covariance matrix and the
correlation matrix of any random vector, the identity matrix In and the unit
matrix Jn. A matrix A is said to be skew-symmetric if A' = — A.
Definition 1.2.16. Trace of a matrix.
trace of A is a scalar given by the sum of the diagonal elements of A, i.e.,
£r(A) = Y^
i=\au • For example, if
Let A be an n x n matrix. The
2 -4
5
6 -7
0
3
9
7
A =
then tr( A) = 2- 7 + 7 = 2.
Result 1.2.8. Properties of trace. Provided the matrices are conformable,
and given scalars a and 6,

1.2. BASIC DEFINITIONS AND PROPERTIES
13
1. tr(In) = n
2. tr(aA ± &B) = atr( A) ± btr(B)
3. tr(AB) = £r(BA)
5. *r(A) = 0 if A = 0
6. tr(A') = tr( A)
7. tr( AA' ) = tr( A' A) = £ aij
i j=i
4. tr(ABC) = tr(CAB) = fr(BCA)
8. tr(aa') = a'a =|| a ||2= J2ai -
i=i
The trace operation in property 4 is valid under cyclic permutations only.
Definition 1.2.17. Determinant of a matrix.
The determinant of A is a scalar given by
Let A be an n x n matrix.
n
dij (— 1)t+J
~|Mjj|, for any fixed i,
or
|A|
3=1
n
=
djj (— 1)*+J|Mjj|, for any fixed j.
A
2=1
We call |Mjj| the minor corresponding to a^. The minor |M^| is the determi-
nant of the (n— 1) x (n — 1) submatrix of A after deleting the zth row and the jth
column from A. The cofactor of aij is the signed minor, i.e., Fij = (— l)*+J
'|Mij|.
We consider two special cases:
1. Suppose n = 2. Then|A| = ana22- «i2«2i -
2. Suppose n= 3. Fix i = 1 (row 1). Then
«22
«23
«32
«33
«21
«23 .
«31
«33
1+1
;
F12 = (-i)1+2
Fn = (-1)
«21
«22
«31
«32
1+3
*13 = (“ I)
and
|A| — «iiFn -1- «12^12 + «13-^13 *
For example, if
2 -4
5
6 -7
0
3
9
7
A =
then,
-7
0
6 -7
3
9
6
0
1+3
- 4(— 1)1+2
2(— 49) + 4(42) + 5(75) = 445.
1+1
+ 5(— 1)
|A| =
2(— 1)
9
7
3
7
Result 1.2.9. Properties of determinants.
and k be any integer. Then
Let A be an n x n matrix

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
14
1. |A| = |A'|.
2. |cA|= cn|A|.
3. AB| = |A||B|;
IAJ - . - A*! = flLi|At|; |A2| = |A|2; and |Afc| =
A|fc .
4. If A is a diagonal matrix or an upper (or lower) triangular matrix, the
determinant of A is equal to the product of its diagonal elements, i.e.,
iAi _
5. If two rows (or columns) of a matrix A are equal, then |A| = 0.
6. If A has a row (or column) of zeroes, then |A|= 0.
7. If A has rows (or columns) that are multiples of each other, then|A| = 0.
8. If a row (or column) of A is the sum of multiples of two other rows (or
columns), then |A| = 0.
9. Let B be obtained from A by multiplying one of its rows (or columns) by
a nonzero constant c. Then,|B| = c|A|.
10. Let B be obtained from A by interchanging any two rows (or columns).
Then, |B| = -|A|.
11. Let B be obtained from A by adding a multiple of one row (or column)
to another row (or column). Then, |B| = |A|.
12. If A is an ra x n matrix and B is an n x ra matrix, then |Im + AB| =
|In+BA|.
Example 1.2.6. Let A be a k x k nonsingular matrix, and let B and C be
any k x n and n x k matrices respectively. Since we can write A 4- BC =
A(Ifc -1- A-1BC), we see from property 3 of Result 1.2.9 that |A + BC| =
|A||I/c + A-1BC|.
Example 1.2.7. Vandermonde matrix.
monde matrix if there are scalars ai, • • • , an, such that
A n n x n matrix A is a Vander-
(
1
1 \
1
1
Q>2
an
al
«3
a2n
al
<4
a3
A =
71— 1
n— 1
or1
«rl
a2
\al

1.2. BASIC DEFINITIONS AND PROPERTIES
15
The determinant of A has a simple form:
n
IAI = n^
D
i,j=l
i< j
—
(an
Q"n-l )( Q'n
^n-2) * * * (&n
a2)(an
&l )
x
(&n— 1 ” an-2)(an-l “ an-3) * ' ‘(an-1 “ al )
X
• * • x (a2 - aa ).
It is easily seen that |A| ^
0 if and only if ai ^
aj for i < j = 1, • • • , n, i.e.
ai, • • • , an are distinct. An example of a Vandermonde matrix is
1
1
1
1
-1
2
1
1
4
A =
with a\ = l, a2 = — 1, cis = 2, and |A| = — 6.
Example 1.2.8. Intra-class correlation matrix.
intra-class correlation matrix, which is also called an equicorrelation matrix, by
/i
p
p\
p
i
p
We define an n x n
= d[(i - p)i + pj],
C — d
\P
P
1/
where — 1 < p < 1 and d > 0 is a constant. In an intra-class correlation matrix,
all the diagonal elements have the same positive value, and all the off-diagonal
elements have the same value which lies between — 1 and 1. The determinant of
C is easily computed by seeing that
l + (n- l ) p
p
p
1+ (n -\)p
1
• • •
p
1
P
...
p
...
p
P
1
1 + (n- l)p
p
•
•
1
I
P
• • •
P
II
P
1
P
P
[1+ (n- l)p]
1
P
1
1
P
. ..
p
0
1- p
• • •
0
[l + (n- l)p]
0
0
. . .
i - p
n— 1
[1 + (n — l)p](l — p)

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
16
< P < 1. So |C| = d"[l +
n— 1
and [1 + (n- l)p](l - p)
(n - l)p](l - p)” -1.
> 0 implies that
Definition 1.2.18. Nonsingular and singular matrices.
If |A|^
0, then
A is said to be a nonsingular matrix. Otherwise, A is singular. For example,
A is a nonsingular matrix and B is a singular matrix, where
1
6 )
1/2 31
1
6
A =
and B =
0
3
Definition 1.2.19. Inverse of a matrix. Let A be an n x n matrix. If there
exists a n n x n matrix B such that AB = In (and BA = In ), then B is called
the (regular) inverse of A, and is denoted by A-1. A matrix A is invertible if
and only if |A|^
0.
Example 1.2.9. We compute the inverse of a matrix A using the formula
A
"1 = jxf Adj(A),
where Adj(A) denotes the adjoint of A, and is defined to be the transpose of
the matrix of cofactors of A. Suppose the matrix A, the matrix of cofactors F
and the matrix Adj(A) are given by
-1 2
2
4
3 -2
-5
0
3
9 -2
15
-6
7 -10
-10
6 -11
A =
F =
and
9
-6 -10\
-2
7
6
,
15 -10 -11 /
Adj(A) =
then, |A| = 17, and
/ 9/17
- 6/17 -10/17
A
” 1 =^
Adj(A) = -2/17
7/17
6/17
V 15/17 -10/17 -11/17
is the inverse of A.
An m x n matrix A is
Definition 1.2.20. Reduced row echelon form,
said to be in reduced row echelon form (RREF) if the following conditions
are met:
Cl. all zero rows are at the bottom of the matrix
C2. the leading entry of each nonzero row after the first occurs to the right of
the leading entry of the previous row,
C3. the leading entry in any nonzero row is 1, and

1.2. BASIC DEFINITIONS AND PROPERTIES
17
C4. all entries in the column above and below a leading 1 are zero.
If only Cl and C2 hold, the matrix has row echelon form. For example,
among the following matrices,
/1
0
2
3
0
^
0 14
5
0
0
0
0
0
0
0
0
0
0
0 J
/0
1
2
0
3N
0
0
4
0
5
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0 12
0
4
0
0
0
0
0
0
0
0
1
5
VO o o o oy
A =
B =
>
.<
/0
1
2
3
4
C =
0
0
0 1
5
\ 0
0
0
0
0
D =
and
0
0
0
1
5
0 12
0
4
E =
the matrix A is in RREF, whereas none of the other matrices is in RREF. In
matrix B, row 2 violates C3; matrix C violates C4, matrix D violates Cl, while
matrix E violates C2.
To verify invertibility and find the inverse (if it exists) of a square matrix A,
(a) Perform elementary row operations on the augmented matrix (A
I) until
A is in RREF.
(b) If RREF(A) ^
I, then A is not invertible.
(c) If RREF(A) = I, then the row operations that transformed A into RREF(A)
will have changed I into A"1.
Example 1.2.10.
We describe an algorithm used to test whether an n x n
matrix A is invertible, and if it is, to compute its inverse. The first step is
to express the matrix (A : I) in reduced row echelon form, which we denote
by RREF(A : I) = (B : C), say. If B has a row of zeroes, the matrix A is
singular and is not invertible. Otherwise, the reduced matrix is now in the form
(I : A-1). We use this approach to find the inverse, if it exists, of the matrix
1
0 -1\
3
4 -2 .
3
5 -2/
A =
We row reduce (A : I):
1
0 -1
3
4 -2
3
5 -2
1
0
0\
0
1
0 I -
0
0 1
10
0
-3
1
0
-3
0
1
1
0 -1
0
4
1
0
5
1
10
- 1
1
0
0
1
0 I ~
1
0 -1
1
0
0\
-3 1
0
,
-3
5 -4/
0
4
1
-3
0
4
1
0
0
1
0
0 -1/4
3/4 -5/4
1

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
18
1 0
0
0
4
0
0
0
1
-2
5
-4
1
0
0
-2
5 -4\
0 -1
1
,
-3
5 -4/
0 -4
4| ~|0
1
0
-3
5 -4
0
0
1
so that
-2
5
— 4\
0 -1
1
.
-3
5 -4/
A"1 =
Result 1.2.10. Properties of inverse.
1. A-1 is unique.
Provided all the inverses exist
= B-1A-1;
(A1 - -.A*)1 = Afc
1 .. - Ar
1.
3. (cA)-1 = (Ac)-1 = ^
A-1.
4. If |A| ^
0, then A' and A-1 are nonsingular matrices and (A')
(A"1)'.
5. (A + BCD)-1 = A-1 - A-1B(C"1H-DA‘1B)
“ 1DA-1, where A, B, C,
and D are respectively raxrn, 7nxn, nxn and nxm matrices [Sherman-
Morrison-Woodbury theorem].
-l
2. (AB)
-l _
(A ~1a)(b/A ~1)
l i b'A-i a
*
i.e., the determinant of the inverse of A is equal to the
6. Provided 1 ± b'A xa / 0, we have (A ± ab
7)
-1
> :
reciprocal of |A| .
-l = A-1 T
7. IA-1! = |A|
oo
=1+ £ (-I)VA\
-1
8. (I -h uA)
i— 1
Each of these properties is obtained by verifying that the product of the given
matrix and its inverse is the identity matrix (see Exercise 1.21). The inverse
in property 5 does not exist in some cases. For example, when we set A = In,
B = X, which is an n x k matrix of rank h, D = X', and C = — (X'X)-1, we
see that (A -f BCD) = (In — P), where P = X(X'X)-1X/, which is a singular
matrix of rank (n— k ). Hence, its (regular) inverse does not exist. The matrix P
is the familiar projection matrix, or hat matrix of linear model theory. One may
however, interpret this property in terms of a generalized inverse of a matrix,
which always exists (see Chapter 3).
Example 1.2.11. Inverse of an intra-class correlation matrix.
continue with Example 1.2.8. The cofactor of any diagonal element is based on
the determinant of an (n — 1) x (n — 1) submatrix:
1
P
• • •
P
P
1
• ' *
P
We
dn-1
n—2
— dn 1[l + (n- 2)p](l- /9)
P
P
•
•
1

1.2. BASIC DEFINITIONS AND PROPERTIES
19
while the cofactor of any off-diagonal element is
P
P
*
* •
P
1 ...
p
p
-dn~ l
= — dn lp( l — p)n
2.
1
P
P
Letting D = d(1 — p) [1+ (n — 1)p],
/1+ (n- 2)p
“ P
-P
1 + (n - 2 ) p
• • •
-P
“ P
1
A
"1
D
• • '
1+ (n- 2 )pJ
\
~P
4 (l1+ (n ~ l)p]I- pJ)
~P
D
('-TTWrrr/ )
1
d{1- p)
An alternate way to obtain C-1 is using property 6 of Result 1.2.10. Suppose
first that p > 0, and C = d [(1 — p)I + pJ], so that C_1 = d_1[(l — p)I + pJ]_1.
In property 6, set A = (1 — p)I, and a = b =-y/pln; then,
1 !
(1 - P)-2P
j,
1- /3
1 + (1- p)~lnp
[I -
d{ l - py
1+ (n — l)p
c-1 =
d"1!
P
J],
If p < 0, it follows that since
C = d[(1- p)I- (Vi:pln)(^=plri)']
we have
!- 1
j
(1 ~ P)
2P
j,
d v l - p
1 + (1- p)_1np
1
[/ -
d(1- p) 1
C"1
P
J].
1+ (n-1)p
Consider the n x n Toeplitz matrix
Example 1.2.12. Toeplitz matrix.
A which has the form
P2
Pn
l\
P
• • •
P
(
1
P
n-2
1
P
A =
n— 2
^n— 3
n— 1
1
P
VP
p
In general, the elements of a Toeplitz matrix satisfy the condition that all the
elements on the jth subdiagonal and the jfth superdiagonal coincide, for j > 1.
It is easy to verify that, for \ p\ < 1, the inverse of A is

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
20
/ 1
• •
0
0
0
^
•
0
0
0
•
0
0
0
• • -p
l + p2
• •
0
— p
1
0
0
-p
l + p2 -p
0
- p
A-1 = -i.
0
0
0
0
0
0
0
-p
V O
which has a simple form.
Definition 1.2.21. Orthogonal matrix.
if AA' = A'A = In. For example,
An nxn matrix A is orthogonal
cos6
— sin6
sin0
cos6
is a 2 x 2 orthogonal matrix.
A direct consequence of Definition 1.2.21 is that, for an orthogonal matrix
A, A' = A-1 . Suppose a
^
denotes the ith row of A, then, AA; = In implies
that a -a* = 1, and a
^
aj = 0 for i ^
j; so the rows of A have unit length and
are mutually perpendicular (or orthogonal). Since A'A = In also, the columns
of A have this property as well. If A is orthogonal, clearly, |A| = ±1. It is
also easy to show that the product of two orthogonal matrices A and B is itself
orthogonal. Usually, orthogonal matrices are used to represent a change of basis,
or rotation.
Example 1.2.13. Helmert matrix,
example of an orthogonal matrix and is defined by
/— 1'
Hn = I
y/
n
An nxn Helmert matrix Hn is an
Ho
where the 2th row of the (n — 1) x n matrix Ho is defined as follows for i =
n - 1 : ( 1'
example, when n — 4, we have
) / VK
5 where, A* = i(i -I- 1).
For
~i\
0
In—
Z— 1
/ l /Vl
\/ s/i
l/ v/4
1 / y/i }
1/v^
- I /V2
0
1 / y/ E
l / y/6
-2/Vd
0
\l/\/l2
1/ y/tt
1/V12
-3/V 12)
0
H4 =
Definition 1.2.22. Linear space of matrices.
matrices denoted by V is called a linear space if
A nonempty set o f n x n
1. for every matrix A in V, and B in V, the sum A -j- B is in V, and
2. for every matrix A in V, and every scalar c, the product cA is in V.

1.2. BASIC DEFINITIONS AND PROPERTIES
21
Note that if A
ciAi -1
b CkA*; is in V. Examples of linear spaces include the set of all n x n
symmetric matrices, and the set {0} containing only the null matrix.
• , Afc are in V, and Ci, • • ,
are scalar constants, then
i >
* *
Definition 1.2.23.
of V if it is itself a linear space. For example, {0} and V are both subspaces of
V, and the set Vn of all n x n diagonal matrices is a subspace of the set of all
n x n matrices.
A subset Vi of a linear space V is said to be a subspace
Definition 1.2.24. A basis for a linear space V is a finite set of linearly inde-
pendent matrices in V that span V.
Definition 1.2.25. Let V be a linear space of matrices and let Vi be a subspace
of V. A matrix Y in V which is orthogonal to every matrix in Vi is said to be
orthogonal to Vi and we write Y ± Vi.
If every matrix in a subspace Vi is orthogonal to every matrix in a subspace
V2, then we say Vi is orthogonal to V2 and write Vi ± V2.
Definition 1.2.26. Let V be a linear space of matrices, let Vi and V2 be sub-
spaces of V, let Y be a matrix in V, let {Xi, • • • , XS} span Vi, and {Zi , • • • , Zt }
span V2. We say that Y _LVi if and only if Y•X* = 0, i.e., Y is orthogonal
to Xi for i = 1, • • • , s. We say that Vi _L V2 if and only if X* •Zj = 0 for
i = 1, • * * , s and j = 1, • • • ,t.
Definition 1.2.27. Orthogonal complement of Vi - The set of all matri-
ces in a linear space V that is orthogonal to a subspace Vi of V is called the
orthogonal complement of Vi relative to V. The orthogonal complement is also
a subspace of V, and is denoted by Vf1.
Let Vi be a subspace of V and let {Bi , • • • , B*
;} span Vi. Then, A is in
Vf
1- if and only if A•Bj = 0 for j = 1, • • • , k. We next define three important
vector spaces associated with any matrix, viz., the null space, the column space
and the row space. These concepts are closely related to properties of systems
of linear equations, which are discussed in detail in Chapter 3. A system of
homogeneous linear equations is denoted by Ax = 0, while Ax — b denotes a
system of nonhomogeneous linear equations.
The null space, Af ( A), of an
Definition 1.2.28. Null space of a matrix.
m x n matrix A consists of all n-dimensional vectors x such that Ax — 0, i.e.,
Af ( A) = {x elZ71 such that Ax — 0}.
That is, the null space is the set of all solutions to the homogeneous linear
system Ax = 0.
J\f(A) is a subspace of Rn, and its dimension is called the
nullity of A. For example, the vector x = (1, 2)' belongs to the null space of
2 -1
-4
2
2
-1
1
0
the matrix A =
since
or
— 4
2
2

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
22
We may use RREF(A) to find a basis of the null space of A. We add or
delete zero rows until RREF(A) is square. We then rearrange the rows to place
the leading ones on the main diagonal to obtain H, which is the Hermite form
of RREF(A). The nonzero columns of H - I are a basis for Af (A). In general,
an n x n matrix H is in Hermite form (i) if each diagonal element is either 0 or
1; (ii) if ha = 1, the rest of column i is all zeroes; and (iii) if ha = 0, i.e., the
ith row of H is a vector of zeroes.
Example 1.2.14. We first find a basis for the null space of the matrix
/1
0 -5
1\
0
1
2 -3
0
0
0
0
0
0
0
0/
which is in RREF, as we can verify.
The augmented matrix of the system
Ax = 0 will be
A =
(\
0
— 5
1
0 1
2 -3 0
0
0
0
0
0
0
0
0
0
0 J
RREF(A) =
where x'= (21, 22, 23, 24). We can choose 23 and 24 freely; we set 23 = s, and
24 = t. It is clear that the general solution vector is
/ 5\
/-l\
-2
+t
3
1
0
’
1/
( x\\
X 2
= S
X 3
0/
W
so that the vectors (5, — 2,1, 0)' and (-1,3,0,1)' form a basis for Af ( A). This
basis can also be obtained in an alternate way from RREF(A), which in this
example coincides with A. Computing
(0
0
5 -1\
0
0 -2
3
0
0
1
0
(0
0
0
I- RREF(A) =
V
we see that the last two nonzero columns form a basis for J\f( A).
Definition 1.2.29. Column space of a matrix.
whose columns are the m-dimensional vectors ai ,a2,
• • ,an. The vector space
spanned by the n columns of A is called the column space (or range space) of A,
and is denoted by C(A). The dimension of the column space of A is the number
of LIN columns of A, and is called the column rank of A. For example, given
= (— 2, 2)' is not in C(A), whereas the vector
Let A be an m x n matrix
( l
-2\
\ 2
4 / ’
vector Xl
A =
X2 = (3,6)' is, because

1.2. BASIC DEFINITIONS AND PROPERTIES
2 3
1
-2 -2
2 -4
2
1
-2 -2
0
0
6
1 -2
3
2 -4
6
1 -2
3\
0
0
0 / ’
a n d
rs*>
T h e row space 7£(A) and row rank are defined similarly.
The column space C(A) and the row space 7£(A) of any m x n matrix A
are subspaces of lZm and 1Zn respectively. The symbol C±(A) or {C(A)}
represents the orthogonal complement of C(A).
To find a basis of the column space of A, we first find RREF(A). We select
the columns of A which correspond to the columns of RREF(A) with leading
ones. These are called the leading columns of A and form a basis for C(A). The
nonzero rows of RREF(A) are a basis for TZ( A).
_L
Example 1.2.15. We find a basis for C(A), where the matrix A and B =
RREF(A) are shown below:
/ 1 -2
2
l
0\
-1
2 -1 0 0
2 -4
640
3 -6
851
/1 -2
0 -1
0\
0
1
1
0
0
0
0
1
*
0
0
0
0/
We see that columns 1, 3 and 5 are pivot columns and they are linearly inde-
pendent; they form a basis for C(A).
and
B =
A =
\o
Result 1.2.11.
Let C(A) and Af (A) respectively denote the column and null
space of an m x n matrix A. Then,
1. dim[C(A)] = n- dimpV’(A)].
2. V(A) = {C(A)}X.
3. C(A'A) = C(A'), and IZ( A' A) = 11( A).
4. For any A and B, C(AB) C C(A).
5. C {ACB) = C{AC) if r(CB) = r(C).
Definition 1.2.30. Rank of a matrix.
that A has full row rank if r(A) = m (which is possible only if m < n), and has
full column rank if r(A) = n (which is possible only if n < m). A nonsingular
matrix has full row rank and full column rank. We say A has rank r (denoted
by r(A) = r ) if its column rank (which is equal to its row rank) is equal to r.
To find the rank of A, we find RREF(A). We count the number of leading
ones, which is then equal to r(A).
Let A be an m x n matrix. We say
Example 1.2.16.
Consider the matrices
/12
2 -1\
1 3
1
-2
113
0
0 1 -1 -1
1 2
2 -1
/1 2
2 -l\
0
1 -1 -1
0
0
0
0
0
0
0
0
\0
0
0
o )
A =
a n d
B =

CHAPTERl . VECTOR AND MATRIX ALGEBRA
24
where B, which is the reduced row echelon form of A, has two nonzero rows.
Hence, r(A) = 2.
Result 1.2.12. Properties of rank.
1. An m x n matrix A has rank r if the largest nonsingular square submatrix
of A has size r.
2. For an m x n matrix A, r(A) < min(m, n).
3. r(A 4- B) < r(A) + r(B).
4. r(AB) < min{r(A), r(B)}, where A and B are conformal under multipli-
cation.
5. For nonsingular matrices A, B, and an arbitrary matrix C,
r(C) = r(AC) = r(CB) = r(ACB).
6. r(A) = r( A' ) = r(A'A) = r(AA').
7. For any n x n matrix A, |A| = 0 if and only if r(A) < n.
8. r(A, b) > r(A), i.e., inclusion of a column vector cannot decrease the rank
of a matrix.
Definition 1.2.31. Equivalent matrices.
same dimension and the same rank are said to be equivalent matrices.
Two matrices that have the
An m x
Result 1.2.13.
Equivalent canonical form of a matrix.
n matrix A with r(A) = r is equivalent to PAQ =
Q are respectively m x m and n x n matrices, and are obtained as products
of elementary matrices, i.e., matrices obtained from the identity matrix using
elementary transformations. The matrices P and Q always exist, but need not
be unique. Elementary transformations include
( Oi
o) w^ere ^ anc*
1. interchange of two rows (columns) of I, or
2. multiplication of elements of a row (column) of I by a nonzero scalar c, or
3. adding to row j (column j ) of I, c times row i (column i ).
Definition 1.2.32. Eigenvalues and eigenvectors of a matrix.
eigenvalues (or characteristic roots) Ai > A2 > • • • > An and the corresponding
eigenvectors Vi, V2, • • • , vn of an n x n matrix A satisfy the relationship
The
(A - AJ
-I)VJ = 0,
j = 1, • • • , n.

1.2. BASIC DEFINITIONS AND PROPERTIES
25
The eigenvalues of A are solutions to the characteristic polynomial equation
P( A) = |(A — AI)| = 0, which is a polynomial in A of degree n. Note that
the n eigenvalues of A are not necessarily all distinct or real-valued. Since
|(A - Ajl)| = 0, A - AjI is a singular matrix, for j = 1,
, n, and there
exists a nonzero n-dimensional vector Vj which satisfies (A — \jl )v j = 0, i.e.,
AVj = Aj\j. The eigenvectors of A are thus obtained by substituting each
A j into A\j = AjVj, j = 1, •
• , n, and solving the resulting n equations. We
say that an eigenvector Vj is a normalized eigenvector if v'-Vj = 1. If A j is
complex-valued, then Vj may have complex elements. If some of the eigenvalues
of the real matrix A are complex, then they must clearly be conjugate complex
(a conjugate complex pair is defined as (a 4 ib), (a — ib) ). Suppose v^
i and
Vj2 are nonzero eigenvectors of A corresponding to A j , it is easy to see that
aiVji 4- 0:2Vj2 is also an eigenvector corresponding to Aj, where ai and (*2 are
real numbers. That is, we must have A(aiVji 4 o^v.^
) — \j (aiVji 4 <*2Vj2)-
The eigenvectors corresponding to any eigenvalue A j span a vector space, called
the eigenspace of A for A j.
Definition 1.2.33. Eigenspace of a matrix.
The eigenspace corresponding
to an eigenvalue A of a matrix A is the set obtained from AT( A - AIn) by
excluding 0. The dimension of this eigenspace is g = n - r(A - AIn), and is
called the geometric multiplicity of A.
To find the eigenvectors of A corresponding to an eigenvalue A, we find the
basis of the null space of (A— AI). The nonzero columns of H — I are a basis for
Af (A— AI), where H denotes the Hermite form of RREF(A— AI) (see Example
1.2.19).
Definition 1.2.34. Spectrum of a matrix.
eigenvalues {Ai , A2, • • • , A*} of A is called the spectrum of A.
The characteristic polynomial may be written as
P( A) = ( ~l)n(A- Ai )ai • • • (A- A*)a‘*(A),
where ai, - * * , cik are positive integers, dj is called the algebraic multiplicity
of Ajy j — 1, • • • , fc, and q( A) is a polynomial of degree n — Ylj=i aj
no
real roots. The geometric multiplicity of A is at most equal to its algebraic
multiplicity. This gives a bound on the dimension of the eigenspace of A. In
general, if an nxn matrix A has k distinct eigenvalues Ai, * • • , Ak with respective
algebraic multiplicities and geometric multiplicities ai, * * * , afc and pi, -
*
* ,9k )
then, Ylj=i 9 j
SjLi aj — n- The geometric multiplicity of an eigenvalue A of
a symmetric matrix is equal to its algebraic multiplicity (see Exercise 2.13).
The set of distinct (real)
1
1 . Then, P( A) = (A - l )2, with solutions
Example 1.2.17. Let A =
0
1
0 1
Ai = 1 (repeated twice), so that ai = 2. Since A — A1I2 =
1, the geometric multiplicity ofAi is pi = 2 — 1 = 1 < ai.
, with rank
0
0

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
26
Example 1.2.18. Let A =^
. This matrix has no real eigenvalues
P(A) = A2 4- 1, with solutions Ai = i and A2 = — z, where i =
The
since
corresponding eigenvectors are complex, and are (z,1)' and (— z,1)'.
Example 1.2.19. Let
-1
2
0\
1
2
1 .
0
2
-1/
Then |A — AI| = — (A 4- 1)(A - 3)(A + 2) = 0, yielding solutions Ai = — 1,
A2 = 3, and A3 = -2, which are the distinct eigenvalues of A. To obtain the
eigenvectors corresponding to A*, we must solve the homogeneous linear system
(A— Ajl)v^ = 0, or in other words, identify the null space of the matrix (A-A^I),
by completely reducing the augmented matrix (A — A*1: 0) . Corresponding to
Ai — -1, we see that
A =
0
2
0
1
0
1\
0
1
0 ,
0
0
0 /
A - (-1)1 =
1
3
1
,
RREF(A + I) =
0
2
0
which is in Hermite form, i.e., RREF(A + 1) = H. The nonzero column of
/0
0
1
H — I = I 0
0
0
\0
0 -1
i.e., (1,0,-1)' is a basis of M{A - (— 1)1) and therefore of the eigenspace cor-
responding to Ai = — 1.
Using a similar approach, we find that a basis of
the eigenspace corresponding to A2 = 3 is (— 1, — 2, — 1)' and corresponding to
A3 = -2, it is (— 1,1/2, —!)'. These three vectors are the eigenvectors of the
matrix A.
Let A be an eigenvalue of A and let c be a real scalar. Then,
1. Afc is an eigenvalue of Afc , for any integer k.
2. cA is an eigenvalue of cA.
3. A 4- c is an eigenvalue of A 4- cl, while the eigenvectors of A + cl coincide
with the eigenvectors of A.
4. /(A) is an eigenvalue of /(A), where /(.) is any polynomial.
To see that property 3 holds, it is easy to verify that |(A + cl) — (A 4- c)I|=
|A — AI| = 0, so that A 4- c is an eigenvalue of A + cl.
Also, if Av = Av,
then (A 4- cl)v = (A 4- c)v, which shows that A and A 4- cl have the same
eigenvectors.
Result 1.2.14.
Result 1.2.15. Sum and product of eigenvalues.
matrix with eigenvalues Ai, A2, • • • , An. Then,
Let A be an n x n

1.2. BASIC DEFINITIONS AND PROPERTIES
27
1. tr{A) = £ Ai.
1=1
2. |A| = IlAi.
1=1
3. IIn ± A| = n (1 ±^
) -
2=1
Let
represent the (i, j)th element of an m x n matrix A*, & = 1, 2, * • •.
For every i = 1, • • • ,m and j — 1, • • • , n, suppose there exists a scalar aij which
is the limit of the sequence of numbers
• • •, and suppose A = { a2 j } .
We say that the mxn matrix A is the limit of the sequence of matrices A*, k =
1, 2, • • •, or that the sequence A*, k = 1, 2, • • • converges to the matrix A, which
we denote by limfc_
*ooAfc = A. If this limit exists, the sequence of matrices
converges, otherwise it diverges. The following infinite series representation of
the inverse of the matrix (I — A) is used in Chapter 5.
For a n n x n matrix A, the infinite series
QA*, with
Result 1.2.16.
A0 = I, converges if and only if lim^-^ooA^ = 0. Then, (I — A) is nonsingular
and
(I - Ar^JXoA*.
Definition 1.2.35. Exponential matrix. For any nxn matrix A, we define
the matrix eA to be the n x n matrix given by:
oo
2=0
where the expression on the right is a convergent series, i.e., all the nxn series
!, * • • , n are convergent, afy being the (j, k )th
V°° a{i ) i -
2si=Q ajle ’ J
~
element of A1 and e° — I.
We conclude this section with some definitions of vector and matrix norms.
Definition 1.2.36. Vector norm. A vector norm on lZn is a function / :
1Zn —
7Z, denoted by || v ||, such that for every n-dimensional vector v G R n,
and every a
11, we have
1. /(v) > 0, with equality if and only if v = 0,
2. f (av ) =|a|/(v), and
3. /(u + v) < /(u) + /(v) for every u
1Zn.
Specifically, the p-norm of a vector v = (vi, • • * ,vn )' E 7?.n, which is also known
in the literature as the Minkowski metric is defined by

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
28
II v llp= {MP + K|P + ' • • + K|P}1/p, P > 1.
We mention two special cases. The Li-norm is defined by
v l|l= X>i|
1=1
and forms the basis for the definition of LAD regression (see Chapter 8). The
L2-norm, which is also known as the Euclidean norm or the spectral norm is
defined by
II V ||2= (M2 + M2 +
t-|^n|2}1/2 = (v'v)1/2,
and is the basis for least squares techniques. In this book, we will denote|| v||2
simply as || v ||.
An extension is to define the I^-norm with respect to a
nonsingular matrix A by
V||A= (v'Av)1/2.
Definition 1.2.37. Matrix norm.
matrix norm on 7£mxn, denoted by || A ||, if
1. /(A) > 0 for all m x n real matrices A, with equality if and only if A = 0,
2. /(aA) =|a|/(A) for all a
71, and for all m x n matrices A, and
3. /(A + B) < /(A) + /(B) for all m x n matrices A and B.
A function / : 7Zm x n
71 is called a
The norm of an m x n matrix A = { a,ij } with respect to the usual inner
product is defined by
A ||- [tr(A'A)]1/2 = [J2iLi
2.U/2
i=i
It is easy to verify that || A ||> 0, with equality holding only if A = 0. Also,
I! cA ||= |c.
A II.
Exercises
1.1. Verify the Cauchy-Schwarz inequality for a = (— 1, 2, 0, — 1) and b =
(4,-2,-1,1).
1.2. Verify Properties 6 and 7 in Result 1.2.1.
1.3. Suppose x, y and z are orthonormal vectors.
Let u = ax -f by and
v = ax+ bz. Find a and b such that the vectors u and v are of unit length
and the angle between them is 60°.
1.4. Show that v[ = (l
1
0
— l),V2 = (2
0
1
l), and V3 =
(0
— 2
1
l) are linearly dependent vectors. Find a set of two linearly
independent vectors and express the third as a function of these two.

Exercises
29
1.5. Show that the set of vectors v[ = (2
3
2), v'2
V3 = (— 4
3
l) are linearly independent.
1.6. Verify whether the columns of A are linearly independent given
(8
— 6
5), and
-3
3 3\
2
2
2
.
0
1
0/
A =
1.7. Verify whether the vector u = (2,3)' is in the span of the vectors
=
(1, 2)' and v2 = (3, 5)'.
1.8. Verify Result 1.2.3.
1.9. Find all matrices that commute with the matrix
b
1
0\
0
b
1 .
0
0
6/
B =
a
b , find Afc, for all k > 2.
1.10. Given A =
0
1
1.11. If AB = BA, show that, for any given positive integer A;, there exists a
matrix C such that Afe — Bfc = (A — B)C.
1.12. For any n x n matrix A, show that the matrices A'A and AA' are sym-
metric.
1.13. For any m x n matrix A, show that A = 0 if and only if A'A = 0.
1.14. Let A be an n x n matrix and let x* be an n x 1 vector, i = 1, • • • , &.
(a) Show that £r(A £^
=1 X*XD = Yli=1
(b) Show that £r(B-1AB) = tr(A).
1.15. Verify Result 1.2.8.
1.16. Find the determinant of the matrix
x'Axi.
(\
3 -3
1\
5
9 -10
3
10
5
-2
2
1 -3
1
A =

CHAPTER 1. VECTOR AND MATRIX ALGEBRA
30
1.17. Let
(1 4 a2
0 \
0
0
a
1 4 a2
0
0
a
a
1 + a2
0
0
0
a
An = det
1 4- a2
0
0
0
a
1 4 a2
0
0
0
a
Show that An — An_i = a2(A
— An_ 2), and hence find An.
1.18. If the row vectors of a square matrix are linearly dependent, show that
the determinant of the matrix is zero.
71—1
1.19. Evaluate the determinant of
^
ai + 1
a2
d\
d2 4 1
dn
dn
dyi 4 1J
\ dl
d2
1.20. By reducing the matrix
1
2 -1
1
-2
-1
1
6
-1
A =
show that it is singular.
1.21. Verify Result 1.2.10.
1.22.
(a) Show that (I 4 AB)
_ 1= I — A(I 4 BA)
_ 1B, provided AB and BA
exist.
(b) Using (a), show that (al* 4 6Jjt)-1 = Ik / d- bJk / { d(a 4 kb) }.
1.23. Let A be an n x n orthogonal matrix.
(a) Show that |A| = 41.
(b) Show that r2r' = <5^
, and c'cj = dij, where ri is the ith row of A,
ci is the zth column of A and 8ij denotes the Kronecker delta, i.e.,
Sij = 1 for i = jf, and 8ij = 0 otherwise.
(c) Show that AB is orthogonal, where B is an n x n orthogonal matrix.
1.24. Let A be an n x n symmetric matrix and let r(A) = 1.
Show that
|I 4 A| = 1 4 tr(A).
1.25. Find the dimension of the column space, null space and row space of the
matrix

Exercises
31
(
i
1
1\
2
2
2
-1
1 -3
A =
1
2
V
1.26. Prove Result 1.2.11.
/1
24
3\
1.27. Let A —
I 3
— 1
2
— 2 I . Find the rank of A.
\5 -4
0
-7
1.28. Verify Result 1.2.16.
CL + 1
1
1
1
a -1- 1
1
I .
Show that A — al3 has a nonzero
1
a + l j
eigenvalue of 3. Find the corresponding eigenvector.
1.30. If A and B conform under multiplication, show that the nonzero eigen-
values of AB coincide with the nonzero eigenvalues of BA.
1.31. Let A be a fc x /c matrix and B be a k x k nonsingular matrix. Show that
A and BAB-1 have the same eigenvalues. If Avj = AjVj, i.e.,\j is an
eigenvector corresponding to the eigenvalue Aj of A, show that Bvj is an
eigenvector of BAB-1 for A j.
1.32. Let x and y be real vectors and A > 0. Verify the following results:
(a) (x'Ay)2 < (x/Ax)(y/Ay).
(b) (x'y)2 < (x'Ax)(y'A-1y).
1.29. Let A =
1


Chapter 2
Properties of Special
Matrices
In this chapter, we define special matrices that find direct use in the theory
of linear models and present some properties of such matrices. We illustrate
salient properties of these matrices using several examples. The concepts and
results discussed here will be used in subsequent chapters for the development
of matrix results and distribution theory.
2.1
Partitioned matrices
Definition 2.1.1. An m x n partitioned matrix A is expressed as an array of
submatrices (or blocks):
/An
A12
• • •
Aic\
A21
A22
* ' •
A2C
(2.1.1)
A-
\Ari
Ar2
*
Arc )
where Aij is an m* x rij submatrix for i = 1, • • * , r, j = 1, • • • , c; mi , • • • , mr
and ni, • • • , nc are positive integers such that ]T^
-i m* = m> an<i
nj = n-
Note that each submatrix A^
, j = 1, • • • , c has the same number of rows for
any
and similarly that each submatrix Aij ,i = 1, • • • , r has the same number
of columns for any j. For example,
1
3
5
7
1
3
5
7
1
-9
and
5
4
5
4
1
-3
2
6
-9
4
-3
2
6
4
are two different partitions of the same 3 x 4 matrix.
33

CHAPTER 2.
PROPERTIES OF SPECIAL MATRICES
34
Definition 2.1.2. An m x n matrix A partitioned as
/An
A12
• • •
Air\
A21
A22
* • •
A2r
(2.1.2)
A =
^Arj
Ar2
* *
Arrj
is said to be a block-diagonal matrix if Aij = 0 for i^
j , and is written as
A = diag(Au , A22,
* • , Arr).
In (2.1.2), if A^ = 0 for j < i = 1, • • • , r, A is called an upper block-triangular
matrix, while if A^ = 0 for j > i = 1, • • • , r, then A is called a lower block-
triangular matrix.
An m x n matrix A partitioned only by rows is written as
/Ai\
A2
(2.1.3)
A =
\Ar/
and if it is partitioned only by columns, we write
/A'A
A'
A2
A' =
(2.1.4)
\KJ
or A = (Ai, A2, * * • , AC). A partitioned n-dimensional column vector is de-
noted by
/ai\
a2
(2.1.5)
a =
\ar/
where a* is an n^-dimensional vector, and n*,i = 1, • • • , r are positive integers
such that 5^1= j ni — n. A partitioned n-dimensional row vector is of the form
a' = (a;, -
Consider a p* q matrix B which is partitioned as
B21
B22
* * *
B2h
(2.1.6)
• • •
Blh\
B12
(2.1.7)
B =
\B;i
B;2
•
•
Bih )

2.1. PARTITIONED MATRICES
35
where the dimension of B^
is
x </j, for i = 1, • • • , Z, j — 1, •
• , h. We define
the following elementary operations on partitioned matrices.
The matrices A and B defined in
Addition of partitioned matrices.
(2.1.1) and (2.1.7) are conformal under addition if p = m, q = n, l = r, h = c,
Pi = rrii , for i = !, • • • , r, and Qj ~ rij , for j —
1, • • • , c. Then, the (z, j)th
submatrix of C — A ± B is given by
for
i = 1, • • • , r
and
j = 1, • • • , c.
Cjj — Ajji By
(2.1.8)
For example, when r = c = l — h — 2, and the submatrices have conformal
dimensions, (2.1.8) becomes
An i Bn
A12 i BX2^
A21 dh B21
A22i B22/
(2.1.9)
A ± B =
Multiplication of partitioned matrices.
n = p, l = c, and rij —
pj for j —
1, • • • , c, in which case the submatrix of
C = AB is given by
The product AB is defined if
^ij — ^^
A^fcBfcj.
(2.1.10)
fc=i
When r = c = l = h= 2, (2.1.10) simplifies as
AnBn + A12B21
AnBi2 + A12B22 \
A21B11 + A22B21
A21B12 4- A22B22 /
(2.1.11)
AB-
Result 2.1.1. Determinant of a lower block-triangular matrix.
For
nonsingular square matrices A, An, and A22, where
An
0
A21
A22
A =
we have
(2.1.12)
|A| = |An 11A22 I .
Proof. It is easily verified that A can be written as
A = ( 1
0
VO
A22
1
An
0\
0
I
'
0
(2.1.13)
-^22 -A-21An
I
Evaluating the determinant of each of the matrices on the right side of (2.1.13)
using the cofactor expansion (see Definition 1.2.17), the result follows.
Similarly, we can show that if A is an upper block-triangular matrix, then
|A| = |An||A22|.

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
36
Result 2.1.2. Determinant of a partitioned matrix.
matrix A is partitioned as
Suppose an n x n
An
Ai2 \
A21
A22/ ’
A =
(2.1.14)
where An, A12, A21, and An are respectively n\ x m, n\ x 77-2, n2 x ni, and
n2 x ri2 dimensional submatrices, with ni + n2 = n. Suppose |A221 7^ 0. Then,
|A| = |A22 [|An - AI 2A£ A2I \ .
(2.1.15)
Proof. From Result 2.1.1, we see that
-A2-2IA21 Ayi -|A22| iA-
i
1
lA22|
= 1.
Hence,
I
An
AI2
A21
A22
0
|A| = |A22|
A22 A21
A22*
An — Ai2A221A2i
A12A221
A21 — A22A22 A21
IA22 I
|A22 I All — Ai2A221A2l
1
using the result on the determinant of an upper block-triangular matrix.
The matrix An - A12A221A21 is called the Schur complement of A22. Note
that if |An|^
0, then,
|A| = |An||A22
the matrix A22 — A2IA^
11A12 is called the Schur complement of An.
A2IA1^ AI2|;
(2.1.16)
Example 2.1.1. (a)
We will find |A|, where
0
1
2
2
5
A =
0
4
6
5
has been partitioned into
An
A12
A2I
A22
Al2 = (0) ,
A2I = (4
6)
and A22 = (5).
Also, |An| = 1, and |A22| = 5.
From Result 2.1.1, we see that |A| =
IAn11A22| = 5.
(b) We compute|A| where
1
2
,
with An =
2
5

2.1. PARTITIONED MATRICES
37
1 \
1
2
A =
2
5
7
4
6
5 )
1
Here, A12 =
, while the other submatrices remain the same as in (a). Using
Result 2.1.2, |A|= IA22 I|Aii — Ai2A22lA2i = 11.
D
We next show a result on the inverse of A partitioned as in (2.1.14). It is
straightforward to verify that if A22 is nonsingular, then
-
r (Al1 ~ A12A22 A21
0
V
A21
=
r(An - A12A22lA2i) + r(A22)
which is used in the proof of the following result.
r ( AH M
V-A-21
A22/
A22
Result 2.1.3. Inverse of a partitioned matrix,
matrix A is partitioned as in (2.1.14), and suppose that B = A"1 is partitioned
similar to A.
1. Suppose |A22 I 7^ 0. Then, the submatrices of the inverse of A are given
Suppose a nonsingular
by
=
(An - AI2A22 A2I )
= -BiiAi2A22
=
-A221A2iBn
~
A22
X -f A22 A2IBIIAI2A22 .
-1
Bn
B12
B21
B22
(2.1.17)
2. Suppose |An| 7^ 0. Then, the submatrices of the inverse of A are
An
1 + A111Ai2B22A2iA111
— AnxA12B22
B22A21Ax^
(A22 - A2iA^
11Ai2)_1.
Bn
B12
B21
B22
(2.1.18)
Proof. We prove property 1. The proof of property 2 is similar and is left to the
reader. The matrix A is nonsingular if and only if the matrix An — Ai2A22 A21
is nonsingular. Since B is the regular inverse of A, we must have
AiiBn + A12B21
A11B12 + A12B22
A21B11 4- A22B21
A21B12 + A22B22
Since A22 is nonsingular, A21B11+A22B21 = 0 implies that B21 = — A^
1A2iBn-
Substituting for B21 into AnBn + A12B21 = 1, we get
AnBn + Ai2( A22^A21B11) = I,
I
0
AB =
0
I

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
38
so that Bn = (An - Ai2A221A2i )
1. Then,
B21 = -A22 A2iBn = — A22 A2i(An — Ai2A221A2i)
“ 1.
Again, since A21B12 + A22B22 = I, we see that B22 = A^
I- A21B12).
Substituting this expression into A11B12 + A12B22 = 0, and solving for B12,
we get B12 = -B11A12A22 , and then
B22 = A22 (I- A21B12) — AJ2 4- A221A2iBnA12A
completing the proof.
Example 2.1.1.
(continued).
We find the inverse of the matrix in (a).
Since|A22 I = 5 / 0, using (2 . 1.17) we see that Bn =
B21 = (— 8/5
2/5), and B22 = 1/5, so that
/
5
-2 0
A-1 =
-2
1
0
8/5
2/5
1/5
Since|An|= 1, we can also use (2 . 1.1 8) to get the same result.
Example 2.1.2. Suppose an n x k matrix A with r(A) = k is partitioned into
A = (Ax
A2) where A\ is an n x r matrix of rank r and A2 is an n x ( k — r)
matrix of rank ( k — r). Let
-1
22 *
P = A(A'A)
_1A', Pi= A1(A,
1Ai)-1A,
1,
B = (I
PJ )A2,
and
P2 = B(B B)
_1B' = (I-PJA^
A' (I- P1)A2]-1A'2(I - Pj).
It is easy to verify that P = Pi -f P2. Using (2 . 1.1 1), we first write
7A;
A;A2
A;A2
A'JAJ.
A' Aj
P = (Ai
A2)
(2 . 1.1 9)
K )
We use Result 2 . 1.3 to evaluate (A^A)-1 in partitioned form as
E
F
G
N
(A'A)
"1 =
(2.1.20)
where
E = (A'jA!)-1 + (A;AI )_1A,
1A2NA
/
2A1 (A,
1A1)-1,
F- -(A;A1)-1A'1A2N,
G =
NA;A1(A,
1AJ)
1, and
N- (A'2A2 - A;A1 (A;A1)-1A;A2)-1
= (A;[I - A1(A;A1)-1A;]A2)-1
— [A2(I - Pi)A2]-1.
(2.1.21)

2.1. PARTITIONED MATRICES
39
Substituting (2.1.20) into (2.1.19), we see that
P =
P1+P1A2NA^
P1-PIA2NA
^-A2NA
^
P1+A2NA2
=
Pj+(I- P1)A2[Ai(I- P1)A2]~1A2(I- Pi)
—
Pi “ I" P2 *
This example is useful to show that the projection matrix in a linear model can
be decomposed into the sum of two (or more) projection matrices.
Example 2.1.3. Let X be an n x k matrix, let X(^
denote the (n - 1) x k
matrix which is obtained by deleting the zth row x*. We can write the inverse
of the symmetric k x k matrix X'X as
=
(X'(i)Xw + xpc')-1
-
(X'(i)Xw)
(X'X)-1
(X' .)X(i))-1xix'(X'(i)X(i))-1
-1
(2.1.22)
l + x{(X'<)X(i))-ixi
which follows directly by using property 5 of Result 1.2.10, setting A = X'^
X(i),
B = Xi, C = 1, and D = x'. Similarly, if we use the same property and now set
A — X'X, B = — Xi, C = 1, and D = x', we obtain
(XJ,)^
)-1
(X'X — x^x')-1
(X'X)"1 +
(X'X)-1Xix'(X'X)
1 — x -(X'X)-1Xj
'
-1
(2.1.23)
These results are useful in studying the effect of deleting an observation in a
linear regression model (see Chapter 8).
Example 2.1.4. Let A be a k x k nonsingular matrix, while B and C are
k x m matrices. We will show that
|A — BC'| = |A||I- C,A-1B|.
(2.1.24)
From (2.1.16), we see that
A
B
C'
I
=|A||I — C
/A-1B|
(2.1.25)
while from (2.1.15), we get
A
B
C' I
= |A - BC'|.
(2.1.26)
The required result follows by equating (2.1.26) with (2.1.25).

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
40
Algorithms for matrix factorization
2.2
In this section, we present factorization methods that enable efficient comput-
ing in linear model theory and multivariate analysis. Although this is less a
statistical problem than it is a problem in numerical computing, it is useful
for a practicing statistician to have at least a rudimentary knowledge of such
techniques. These methods are especially valuable for an appreciation of the
development and computation of estimates and diagnostic measures in linear
models and are employed by most statistical software in order to produce nu-
merically stable results. The decomposition of a matrix A into a product of
two or more matrices is useful in computing properties such as the rank, the
determinant or inverse of A. We do not give an exhaustive presentation, and the
reader is referred to Golub and Van Loan (1989) or Stewart (1973) for details.
Result 2.2.1. Full-rank factorization.
r(A) = r.
There exists an m x r matrix B and an r x n matrix C with
r(B) = r(C) = r such that
Let A be an m x n matrix with
(2.2.1)
A = BC.
We show the existence of such matrices B and C. Since r(A) =r,
Proof.
the matrix A has r LIN rows and columns. Assume that the first r rows and
the first r columns are LIN (otherwise, permutation matrices may be used as
described at the end of the proof). The matrix A can be partitioned as
X
Y
Z
W
A =
where X, Y, Z, and W have respective dimensions r x r, r x (n — r ), (m — r ) x r,
and (m — r ) x (n — r), and X is nonsingular. The first r rows in A are those of
(X
Y) and are LIN, so that the rows of (Z
W) are linear combinations of
those of (X
Y) . Hence, for some matrix F,
(Z
W) = F (X
Y) .
Applying a similar reasoning to the columns of A, we have for a given matrix
(2.2.2)
H
Y
X
(2.2.3)
H
W
~ \ Z
From (2.2.2), Z = FX and W = FY, while from (2.2.3), Y = XH.
Hence,
W = FXH, and
) = (') x (I
H)
X
Y
Z
W
X
XH
FX
FXH
(2.2.4)
A =
Also, since X is nonsingular, X 1 exists, and hence Z = FX implies that
F = ZX-1, and Y = XH implies that H = X
_ 1Y. It follows that

2.2. ALGORITHMS FOR MATRIX FACTORIZATION
41
W = FY = ZX_1Y.
We can write (2.2.4) as
) = (zx-) x f1 X"Y>
X
Y
Z
ZX_
1Y
X
Y
Z
W
A =
(2.2.5)
Note that (2.2.4) can be further rewritten in two equivalent ways as
(FX) I1 h> “
d
A = (F) (x
XH)
(2.2.6)
A =
each of which is of the form A = BC, where B is an m x r matrix with full
column rank r = r(A), and C is an r x n matrix with full row rank r = r(A).
Suppose that the first r rows and columns of A are not LIN. Let M = PAQ,
where P and Q are permutation matrices. We obtain the full-rank factoriza-
tion M = BC. By the orthogonality of P and Q, we have A = P^MQ-1 =
P BCQ' = (P'B)(CQ'), where P'B and CQ' play the same roles as B and C.
This completes the proof. If A is a symmetric matrix of rank r, it has full-rank
factorization A = LL' with r(L) = r.
Result 2.2.2. Triangular decomposition. Let A be an m x m positive
definite symmetric matrix (see Definition 2.4.5). There exists a unique unit
lower triangular matrix L and a unique diagonal matrix D with positive diagonal
elements such that
1
^
1
L"XDL
,
or, equivalently,
or, equivalently,
A
LAL'
(2.2.7)
D
A” 1
L'D_1L.
In the literature, there are three different, but mathematically equivalent ver-
sions of the triangular decomposition that go under different names.
The
first involves L” 1 and D, and is known as the Crout decomposition: A =
(L“ 1D)L
tion and combines matrices D and L_
1/: A = L-1(DL -1) = L-1U', say. The
third version is the Cholesky decomposition of A, and is obtained by factor-
ing D into D = D1/2Dx/2, so that we get A = (L'1D1/2)(D1/2L'“ 1) = VV',
say, where V = L_1D1/2 is a lower triangular matrix. We show a proof of the
Cholesky decomposition here.
/-i
/-i
say. The second form is called the Doolittle decomposi-
= UL
Proof. We show a proof by induction on the dimension m. When m = 1,
A corresponds to a real scalar, and its Cholesky factorization is A =a2, where
a =
y/A. Assume the Cholesky decomposition for dimension m — 1, where
m > 1. Suppose we partition A as
An
a12
a21
0,22
A =

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
42
By the induction hypothesis, there exists a unique lower triangular (m — 1) x
(m - 1) matrix Ci with positive diagonal entries such that An = CiC^
. Let
Ci
0
c'
c
C =
where c is an (m — l)-dimensional vector, and c > 0 is a scalar, both yet
unknown. We obtain these from the requirement that A = CCr, i.e., CiCi =
It follows that c = C1
"1ai2, and c =
An, Cic = ai2, and c'c + c2
(«22 — c'c)1/2, where «22 — c'c > 0, since A is positive definite.
= «22-
Result 2.2.3. QR decomposition,
n. Then there exists an m x n matrix Q and an n x n upper triangular matrix
R such that
Let A be an m x n matrix with r(A) =
(2.2.8)
where Q is an orthogonal basis for the column space C(A) of the matrix A.
Proof. Let ai, • • • ,an denote the columns of A. The Gram-Schmidt orthog-
onalization (see Result 1.2.4) can be used to construct an orthogonal set of
vectors bi, • • • , bn which are defined recursively by
A = QR,
bi
ai
i— 1
ai ^
]cjjbj > i
hi
= 2, -
« - , n,
3=1
where Cij = a'b^/b'bi, i < j = l, - - * , n.
By construction, we define Q =
(bi, • • • , bn) to be the required m x n orthogonal matrix, while R denotes the
n x n upper-triangular matrix whose (2, j)th element is given by dj, i < j =
1,
• • , n. Then, the QR decomposition of A has the form A = QR.
The QR decomposition is useful for computing numerically stable estimates
of coefficients in a linear model.
Various orthogonalization algorithms have
been employed in the literature (see Golub and Van Loan, 1989 or Stewart,
1973) which operate directly on the matrix of explanatory variables in a linear
model. The QR decomposition also enables us to factor the projection matrix
in linear model theory into the product of two orthogonal matrices, which is
useful in the study of regression diagnostics (see Chapter 8).
Example 2.2.1.
Let A = LL' denote a full-rank factorization of a symmetric
matrix A of rank r, where L is an n x r matrix of full column rank. We will
show that L'L is nonsingular. Let M be a leading nonsingular submatrix of L,
with r(M) = r, so that
J , L' = M' (I
(M')-1N') = M' (I
S) , say, and
L
L

2.2. ALGORITHMS FOR MATRIX FACTORIZATION
43
so that L'L = M'(I 4- SS
;)M. Since M has full rank, both M and M' are non-
singular. We show that I 4- SS' is also nonsingular. Suppose, on the contrary,
it is not. Then, there exists a nonzero vector u such that (I 4- SS
7)u = 0, i.e.,
u'(I 4- SSr)u = 0, i.e., u'u 4- u'S(u'S)' = 0, which is possible only if u = 0.
This contradicts our assumption, so I 4- SS; is nonsingular, and so is L'L.
In the next three sections, we present results that are crucial for the devel-
opment of linear model theory and are related to the diagonalization of general
matrices, and in particular to symmetric and p.d. matrices. We first give some
definitions and basic ideas.
Definition 2.2.1. Diagonability of a matrix.
to be diagonalizable (or diagonable) if there exists a n n x n nonsingular matrix
Q such that
A n n x n matrix A is said
Q
_1AQ = D
(2.2.9)
The matrix Q diagonalizes A and further,
where D is a diagonal matrix.
Q-1AQ = D if and only if AQ = QD, i.e., if and only if A = QDQ-1.
The process of constructing a matrix Q which diagonalizes A is referred to
as the diagonalization of A; in many cases, we can relate this to the eigensystem
of A. In Result 2.2.5, we show how to diagonalize an arbitrary nxn matrix A.
In section 2.3, we show that a symmetric matrix A is orthogonally diagonable.
A n n x n matrix A is said
Definition 2.2.2. Orthogonal diagonability.
to be orthogonally diagonable if and only if there exists an n x n orthogonal
matrix P such that P'AP is a diagonal matrix.
Result 2.2.4. Let A be an nxn matrix. Suppose there exists an nxn nonsingu-
lar matrix Q such that Q“ 1AQ = D = diag(Ai, • • • , An). Let Q = (qx , • • • , qn).
Then,
1. r(A) is equal to the number of nonzero diagonal elements in D.
2. |A| = fiA* = |D|.
i=1
3. tr(A) = £ Ai = tr( D ).
t=I
n
4. The characteristic polynomial of A is P(A) = (— l)n[I (A - A*).
i=1
5. The eigenvalues of A are Ai, • • • An, which are not necessarily all nonzero,
nor are they necessarily distinct.

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
44
6. The columns of Q are the LIN eigenvectors of A, where q* corresponds to
the eigenvalue A*.
Proof. Since Q is a nonsingular matrix, r(A) = r(Q-1AQ) =r(D), which is
clearly equal to the number of nonzero diagonal elements A*, i = 1, • • • , n, which
proves property 1. The proof of property 2 follows from seeing that
|A| = |Q_1Q||A| = |Q_1AQ| = |D| = riV
1=1
Similarly, tr( A) = £r(QQ
_1A) = tr(Q_1AQ) = tr(D) — Yli=i Ai, which
proves property 3. By property 2, for any scalar A,|D- AI| = |Q_1AQ— AI| =
|Q_1(A— AI)Q| = |A — AI|, so that the characteristic polynomials of D and A
coincide, which proves property 4, of which property 5 is a direct consequence.
Now, Q_1AQ = D implies that AQ = QD, i.e., Aq
^ = A^q*, i = 1, -
Since A*, i = 1, • • • , n are the eigenvalues of A, property 6 follows.
• , n.
Result 2.2.5.
Diagonability theorem.
eigenvalues Xk with algebraic multiplicities ak ,k = 1,
• • , s, with ]P£=1 ak = n >
has n LIN eigenvectors if and only if r(A — A^I) = n — ak , k = 1, * • • , s. Then
the matrix of eigenvectors U = (tq, * • • ,un ) is nonsingular and A is diagonable
as U-1AU = D = diag(Ai, • • • , An).
An n x n matrix A having
Proof.
Sufficiency. Suppose r(A - A*I) = n — ak , k — 1
This
implies that (A — Afcl)x = 0 has exactly n - (n — ak ) = ak LIN nonzero
solutions which are the eigenvectors of A.
Corresponding to each A&, there
exists a set of ak LIN eigenvectors.
We must show that these sets are lin-
early independent of each other.
Suppose that, on the contrary, they are
not LIN. Let (zi, - *
« ,,zai ) and (y i , - * - , ya2) denote the two sets of vectors
and suppose y2 is a linear combination of zi, - - - , zai , i.e., y2 = X^
=i°izii
where not all the c*’s are zero. Now, Ay2 =
c^Az*, which implies that
A2y2 = YliLi CiAiZi = Ai J2iLi cizi = Aiy2, which is impossible, since Ai ^
A2.
Hence, our supposition is incorrect and all the s sets of ak eigenvectors must be
LIN and hence the matrix U is nonsingular.
Necessity. Suppose U~1AU = D exists. Given D = diag(Ai , - - - , An), the
matrix D — A*J has exactly ak zero values on its diagonal since r( D — X^ I ) =
n — ak. Now, U_1AU — D, which implies that A = UDU-1. So, A — A*J =
UDU"1 -\kI = U(D - AfcI)U-1, from which it follows that r(A - XkI ) =
r(D - Afcl) = n- a k.
A general result on the decomposition of an m x n matrix A is given by the
singular-value decomposition, which is shown in the next result. We leave its
proof to the reader.
Let A be an mxn matrix of rank r. Let P be an m xm orthog-
Result 2.2.6.
onal matrix, let Q be an n x n orthogonal matrix, and Di = diag(di, • • • , dr ) be
a n r x r diagonal matrix with di > 0, i = 1, * • • ,r. Suppose we partition P and

2.3. SYMMETRIC AND IDEMPOTENT MATRICES
45
Q as P = (Pi
P2) and Q = (Qi
C$2)- The singular-value decomposition of
A is
(D„ S) Q' = PiD,Q;, or
A
P
A
2=1
where pi, • • • , pr are the r columns of Pi , and qi , •" , Qr are the r columns of
QL
The scalars di, -
* - ,dr, which are called the singular values of A, are the
positive square roots of the (not necessarily distinct) nonzero eigenvalues of
A'A, which do not vary with the choice of P and Q. The m columns of P
are eigenvectors of AA', with the first r columns corresponding to the nonzero
eigenvalues d\, • • •
while the remaining m — r columns correspond to the
zero eigenvalues. Similarly, the n columns of Q are eigenvectors of A'A, with
the first r columns corresponding to the nonzero eigenvalues d\, • • • , dj!, and the
remaining n — r columns corresponding to the zero eigenvalues. Once the first
r columns of P are specified, the first r columns of Q are uniquely determined,
and vice versa (see Harville, 1997, section 21.12 for more details).
2.3
Symmetric and idempotent matrices
Recall from Definition 1.2.15 that a n n x n matrix A is symmetric if A' = A.
We now give several results on symmetric matrices that are useful in the theory
of linear models.
Result 2.3.1. Let A be an n x n symmetric matrix. There exist vectors xi
and X2 such that
x'xAxx ^ x'Ax ^ X2AX2
x'x
(2.3.1)
<
<
X'2X2
X'XXI
for every nonzero vector x £1ZU. Here, x
^
Axi/x
^
xi and x2Ax2/x2X2 are re-
spectively the smallest and largest eigenvalues of A, while xi and X2 are the
eigenvectors corresponding to these eigenvalues.
Proof. Define S — {x : x'x =1}. The quadratic form x'Ax is a continuous
function of x, and 5 is a closed and bounded set. Therefore, x'Ax attains a
maximum and a minimum value over S, i.e., there exist xi and X2 in S such
that, for every x 6 5,
X'LAXI < x'Ax < X2AX2-
Clearly, u = x/vx'x £ 5, and u'Au = x'Ax/x'x. Therefore, for every XGS,

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
46
x
^
Axi/x
^
xi = x
^
Axi < x'Ax/x'x < x^
Ax2 = X2AX2/X2X2.
By property 3 in Result 2.7.1
d (x'Ax/x'x)
(x'x)
2[(X'X)2AX- (xAx)2xj.
(2.3.2)
Sx
Since x'Ax/x'x attains its minimum at Xi and its maximum at x2, we set the
expression in (2.3.2) to zero, which gives
x'Ax
Ax =
(2.3.3)
x
x'x
so that, by Definition 1.2.32, we conclude that x
^
Axi/x
^
xi and X2AX2/X2X2
are real eigenvalues of A, with corresponding eigenvectors xi and x2.
Result 2.3.2. The eigenvalues of every real symmetric matrix are real-valued.
That is, if a symmetric matrix has all real-valued elements, its eigenvalues can-
not be complex-valued.
Proof.
A proof follows directly from Result 2.3.1. An alternate proof implic-
itly uses the fundamental theorem of algebra which states that every polynomial
equation of the form
ao + CLIX + a^x2 H
{- anxn = 0
where ao, ai, a2,
• • * , an are arbitrary real numbers, an ^
0, has a solution
among the complex numbers if n > 1. This statement is true even if the coeffi-
cients ao, ai, a2, • • •, an are complex-valued. The idea is that, in order to solve
polynomial equations with possibly complex coefficients, it is not necessary to
construct numbers more general than complex numbers. The proof follows. Let
A be an n x n real symmetric matrix. If possible, let A = a+i(3, where i = >/— I,
be a complex eigenvalue of A. Since A is real, any complex eigenvalues must
occur in conjugate pairs. Let A* = a - i(3 denote the complex conjugate of
A. Let x = (a?i, * • • , £n)' = a 4* ib and x* = a — zb denote the eigenvectors
corresponding to A and A* respectively. By Definition 1.2.32, Ax = Ax, so that
x*'Ax = x*'Ax = Ax*'x.
(2.3.4)
We also have Ax* = A*x*, so that
x*'Ax = (Ax*)'x = (A*x*)'x = A*x*'x.
(2.3.5)
Equating (2.3.4) and (2.3.5), we get
Ax*'x = A*x*'x.
Since x*'x is nonzero, being the sum of squares of elements of a nonzero real
vector, we must have that A = A*, i.e., a + i(3 = a — i(3, so that A must be
real-valued.

2.3. SYMMETRIC AND IDEMPOTENT MATRICES
47
Result 2.3.3. Let xi and X2 be two eigenvectors corresponding to two distinct
eigenvalues Ai and A2 of an n x n symmetric matrix A. Then, xi and X2 are
orthogonal.
We are given that A = A', and Axj = A^
Xj, j = 1, 2. Since x' AXj is
Proof.
a scalar for j — 1, 2, we have
xiA2X2 = A2XJX2 = A2X2X1.
Since Ai ^
A2, we must have X2X1 = 0. By Definition 1.2.8, xi and x2 are
orthogonal. For a more general alternate proof which is applicable when the
eigenvalues are not necessarily distinct, see Corollary 21.5.9 in Harville (1997).
A1X2X1 = X2A1X1 = x^
Axi = X [ A X 2
2
2
2 -1
Example 2.3.1. Let A
. Then
2 - A
2
|A - AI|=
= — (2 — A)(l + A) — 4 = 0;
2 -1- A
the solutions are A = 3, and A = — 2, which are the eigenvalues of A. It is easy
to verify that the corresponding eigenvectors are (2,1)' and (1,-2)', which are
clearly orthogonal.
Result 2.3.4. Spectral decomposition of symmetric matrices.
n x n matrix A with eigenvalues A^ and corresponding eigenvectors p/c, k =
1, • • * , n, is diagonable by an orthogonal matrix P = (pi , • • * , pn) such that
An
P'AP = D = diag(A1,... , An)
(2.3.6)
if and only if A is symmetric. In other words, every symmetric matrix is or-
thogonally diagonable. The spectral decomposition of A is
A =^2^kPkP'k -
(2.3.7)
k=1
Proof. Necessity.
n x n orthogonal matrix P such that P'AP = D, where D is a diagonal matrix
(and hence symmetric). It follows that A = PDP', so that
A' = (PDP )' = PD'P' = PDP'
so that A is symmetric (see Definition 1.2.15).
Sufficiency. We prove this by induction. Clearly every lxl symmetric matrix
A (which corresponds to a scalar) is orthogonally diagonable. Suppose that
every (n — 1) x (n — 1) symmetric matrix A is orthogonally diagonable, n > 2.
Now consider the symmetric n x n matrix A with an eigenvalue equal to A,
Let A be any nxn matrix, and suppose there exists an

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
48
and corresponding normal eigenvector equal to u (so that u'u = 1). By Gram-
Schmidt orthogonalization (see Result 1.2.4), there exists a n n x (n-l) matrix
V such that the n x n matrix (u, V) is orthogonal, and
u'Au
u'AV
V'Au
V'AV
Since Au = Au (by Definition 1.2.32), u'Au = Au'u =A; also
V'Au = V'Au = AV'u = 0
(by the orthogonal construction), and u'AV = 0 (transposing V'Au). Hence
(2.3.8) becomes
(u, V)'A(u, V) =
(2.3.8)
A
0
0
V'AV
(u, V)'A(u, V) =
= diag(A, V'AV).
Since V'AV is symmetric, it is orthogonally diagonable by the induction hy-
pothesis; i.e., there exists an (n — 1) x (n — 1) orthogonal matrix R, and a
diagonal matrix F such that
R'V'AVR = F.
Define
S = diag(l, R) and P = (u, V)S.
Using the orthogonality of R, it may be verified that
S'S = diag(l, R'R) = diag(l, I„_ i ) = I„,
so that S is orthogonal. The matrix P, being the product of two orthogonal
matrices is also orthogonal. Further,
P'AP = S'(u, V)'A(u, V)S
= S'diag(A, V'AV)S
= diag(A, R'V'AVR)
= diag(A,F),
so that P'AP is a diagonal matrix. To summarize, using the eigenvectors of
A, we have constructed an orthogonal matrix P such that P'AP is a diagonal
matrix.
Result 2.3.5. Let A be an n x n nonsingular symmetric matrix. Then A and
A-1 have the same eigenvectors, while the eigenvalues of A"1 are the reciprocals
of the eigenvalues of A.
Proof. By Result 2.3.4, we have P'AP = D. Since P is an orthogonal matrix,
this implies A = PDP'. Let B = PD_1P', where D"1 denotes the regular
inverse of the diagonal matrix D, which is itself diagonal with elements that are
the reciprocals of the diagonal elements of D, which in turn are the eigenvalues
of A. The matrix B is clearly symmetric and its eigenvalues are the reciprocals
of the eigenvalues of A, while its eigenvector matrix is P. Now,

2.3. SYMMETRIC AND IDEMPOTENT MATRICES
49
AB = PDP
/PD_1p/— PDD-1P'= PP'= In
and similarly, BA = In. Hence, B = A-1.
Result 2.3.6. Let A be a symmetric nxn matrix with eigenvalues Ai, • • • , An.
Then
1. tr(A) = i=l
2. tr( A*) = £ A?.
1=1
3. tr(A
l ) = J^
l/Ai, provided A is nonsingular.
i=\
Proof. By Result 2.3.4, there exists an orthogonal matrix P such that P'AP = D,
where D — diag(Ai, * • * , An). Since P is orthogonal, P'P = PP'= In and
ET=1 A* = MD) = tr(P'AP) = tr(PP'A) = tr(A),
proving property1. Note that property 1 holds for all square matrices. To prove
property 2, once again, from the orthogonality of P, it follows that
Ds = (P'AP)(P'AP) • • -(P'AP) = (P'ASP)
and so
Af = ^r(Ds) = tr(P'A5P) = tr( As ). To show property 3, note
that
D-1 = (PAP)-1 = P'A-1P
from which the result follows directly.
Result 2.3.7. Let A and B be m x n matrices. Let C b e a p x m matrix with
r(C) — m, and let D be an n x p matrix with r(D) = n.
1. If CA = CB, then A- B.
2. If AD = BD, then A = B.
3. If CAD = CBD, then A — B.
Proof. We prove only property 3 here; the proofs of the first two properties
follow as special cases. Since C and D have respectively full row and column
ranks, let L and R denote their respective left and right inverses (which exist).
Then, CAD = CBD implies
A = IAI = LCADR = LCBDR = IBI = B
which proves the result.
Result 2.3.8. Let A be an m x n matrix.

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
50
1. For n x p matrices B and C, AB = AC if and only if A'AB = A'AC.
2. For p x n matrices E and F, EA'= FA' if and only if EA'A = FA'A.
Proof. To prove property 1, note that if AB = AC, then A'AB = A'AC
holds. Now suppose that A'AB = A'AC holds. We must show that this implies
AB- AC. We have 0- (A'AB - A'AC) = (B'- C')(A'AB - A'AC) =
(AB — AC)'(AB — AC), which implies that AB — AC = 0 (see Exercise 2.11).
The proof of property 2 follows directly by transposing relevant matrices in
property 1.
Definition 2.3.1. An nxn matrix A is said to be idempotent if A2 = A. We
say that A is symmetric and idempotent if A' = A and A2 = A.
Examples of symmetric and idempotent matrices include the identity matrix In,
the matrix Jn = ^
Jn and the centering matrix Cn = In — Jn. We complete
To
this section with some properties of idempotent matrices.
Result 2.3.9. Properties of idempotent matrices.
1. A' is idempotent if and only if A is idempotent.
2. I — A is idempotent if and only if A is idempotent.
3. If A is an n x n idempotent matrix, then r(A) = tr( A) and r(In - A) =
n — tr(A).
4. If r(A) = n for an n x n idempotent matrix A, then we must have A = In.
Proof. To prove property 1, assume first that A'A' = A'. That A is idempo-
tent follows by transposing both sides. The proof that A' is idempotent if A is
idempotent is similar. To prove property 2, assume first that I — A is idempo-
tent, which implies that (I- A) = (I- A)(I — A), from which idempotency of
A follows immediately. The converse is similarly proved. To show property 3,
let r = r(A). By the full-rank factorization in Result 2.2.1, there exists an nxr
matrix P and an r x n matrix Q, each of rank r, such that A = PQ. Now,
PQPQ = A2 = A = PQ,
so that, by property 3 of Result 2.3.7, QP = Ir. Using properties of trace (see
Result 1.2.8), tr(A) = tr(PQ) = tr(QP) = £r(Ir) = r. A simple extension of
this idea proves the second part, i.e., r(In — A) = n — tr(A) = n — r. To prove
property 4, suppose that the nxn idempotent matrix A has rank n, so that
A-1 exists. Then,
A = InA = A' 1AA= A'1A2 = A"1A = In

2.4. N.N.D. QUADRATIC FORMS
51
which proves the result. The only nonsingular idempotent matrix is the identity
matrix.
Result 2.3.10. Let A be an n x n symmetric matrix. A is idempotent of rank
m if and only if m of its eigenvalues are equal to 1 and the remaining (n — m)
eigenvalues are equal to 0.
Proof. Let A' = A, and let Ax, • • • , An denote the eigenvalues of A, which are
not necessarily all distinct. By Result 2.3.4, there exists an orthogonal matrix
P such that A = PDP', where D = diag(Ai, • • • , An). Also,
A2 = PDP'PDP' = PD2P',
where D2 = diag(A2, - - - , A2 ). Suppose A2 = A, i.e., A is idempotent. This
must imply that D2 = D, or A2 —
A^ = 0, which in turn implies that each
eigenvalue is either 0 or 1. Conversely, let us suppose that each eigenvalue of A
is either 0 or 1. This implies that A2 = Aj for all j, i.e., A2 = A, so that A is
idempotent. Clearly, r(D) is equal to the number of nonzero eigenvalues in A,
which is also equal to r(A) (since P is a nonsingular matrix).
2.4
Nonnegative definite quadratic forms and
matrices
We introduce quadratic forms and matrices of quadratic forms, and describe
their properties.
First, we define a linear form in a vector x, as well as a
bilinear form in x and y.
Given an arbitrary vector a =
a linear form in x = (xi, • • • ,xn)' is a function that assigns to
Linear form in x.
Definition 2.4.1.
(&1 >
* * *
> ®n)
each vector x lln the value
n
a'x =
a,iXi = d\Xx + • • • + anxn.
(2.4.1)
Note that the linear form a'x can also be written as x'a and is a homogeneous
polynomial of degree 1 with coefficient vector a. For example, 4xi + 5x2 — 3x3
is a linear form in x = (XI, X2 J X3)' with coefficient vector a = (4, 5, — 3)'. Two
linear forms a'x and b'x are identically equal for all x if and only if a=b.
Definition 2.4.2. Bilinear form in x and y.
matrix A = {a^}, a bilinear form is a function that assigns to each pair of
vectors x = (xi, • • • , xm)' e 7lm and y = (2/1, • • • , yn )' e 7Zn, the value
m
n
X
1Ay =Y2J2 aijxiyj
1=1 j=1
Given an arbitrary m x n
(2.4.2)

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
52
and A is the matrix of the bilinear form. The form in (2.4.2) can also be written
as y'A'x.
Two bilinear forms x'Ay and x'By are identically equal if and only if A = B.
A bilinear form x'Ay is symmetric if x'Ay = y'A'x for all x and y, i.e., if and
only if the matrix of the bilinear form is (square) symmetric, i.e., A = A'.
Example 2.4.1.
is a bilinear form in x = ( xi ,x2, x3)' and y = (2/1, 2/2)', with the matrix of the
bilinear form given by
The expression x\y\+2x\y2+4x2 yi +7^22/2+2x3yi-2x3y2
A = G i
An example of a symmetric bilinear form in x = ( x± , x2,x3 )' and y = (t/i , y2, 2/3)'
is xiyi + 2xiy2 - 3xiy3 + 2x2 yi + 7x2yi + 6x22/3 - 3X32/I + §x3y2 + 5x32/3, the
matrix of the bilinear form being
1
2
—3\
2
7
6 .
-3
6
5 /
A =
Definition 2.4.3. Quadratic form in x.
Given an arbitrary n x n matrix
A = {flij}, a quadratic form is a function that assigns to each vector x =
(xi, • • • , xn )
f
Tln, the value
x'Ax =^^
dijXiXj
i=1 j=1
which is a homogeneous polynomial of degree two.
(2.4.3)
The expression x\ 4- 7x\+ 4x§ + 4XIX2 + 10xix3 — 4x2x3
Example 2.4.2.
is a quadratic form in x = (xx, x2, x3)', the matrix of the quadratic form being
r
2
5\
A = I 2
7
— 2 1 . When x = 0, then x'Ax = 0 for all A.
\5 -2
4/
Let A = { dij } and B = { bij } be two arbitrary n x n matrices. We say
x'Ax and x'Bx are identically equal if and only if A + A' = B+ B;. If A
and B are symmetric matrices, then x'Ax and x'Bx are identically equal if
and only if A = B. For any matrix A, note that C = (A + A')/2 is always
symmetric and x'Ax = x'Cx. Hence, we may assume without loss of generality
that corresponding to a given quadratic form, there exists a unique symmetric
matrix A which is the matrix of that quadratic form. Let x'Ax be a quadratic
form in x and let y = C“ xx, where C is an n x n nonsingular matrix. Then,
x'Ax = y'C'ACy = y'By, say. We refer to A and B as congruent matrices.

2.4. N.N.D. QUADRATIC FORMS
53
Definition 2.4.4. Nonnegative definite (n.n.d.) quadratic form.
An
arbitrary quadratic form x'Ax is said to be nonnegative definite if x'Ax > 0
for every vector x G 7Zn. The matrix A is called a nonnegative definite matrix.
Definition 2.4.5. Positive definite (p.d.) quadratic form.
A nonnega-
tive definite quadratic form x'Ax is said to be positive definite if x'Ax > 0 for
all nonnull vectors x G lZn and x'Ax = 0 only when x is the null vector, i.e.,
when x = 0. The matrix A is called a positive definite matrix.
Definition 2.4.6. Positive semidefinite (p.s.d.) quadratic form,
nonnegative definite quadratic form x'Ax is said to be positive semidefinite if
x'Ax > 0 for every x G lZn and x'Ax = 0 for some nonnull x. The matrix A
is called a positive semidefinite matrix.
A
The quadratic form x\ + • • • + x\ — x'Inx > 0 for every
Example 2.4.3.
nonnull x G 1Zn and is p.d. The quadratic form ( x\ +
* • • -f xn)2 = x'll'x
— x'Jnx > 0 for every x £lZn and is equal to 0 when x = (1 — n,1, • • * ,1)'; it
is a p.s.d. quadratic form.
A quadratic form x'Ax is respectively nonpositive definite, or negative defi-
nite or negative semidefinite if — x'Ax is nonnegative definite, or positive definite
or positive semidefinite. The only symmetric n x n matrix which is both non-
negative definite and nonpositive definite is the null matrix. A quadratic form
is said to be indefinite if x'Ax > 0 for some vectors x in lZn and x'Ax < 0
for some other vectors x in 7Zn. The matrices of such quadratic forms have the
corresponding names as well. In general, we will assume p.d. matrices to be
symmetric.
Result 2.4.1. Let P be an n x m matrix and let A be an n x n n.n.d. matrix.
Then the matrix P'AP is n.n.d. If r(P) < m, then P'AP is p.s.d. If A is p.d.
and r(P) = m, then P'AP is p.d.
Proof. Since A is n.n.d., by Definition 2.4.4, x'Ax > 0 for every x £7Zn.
Suppose x = Py, y G TZm. Then,
y'(P'AP)y = (Py)'A(Py)- x'Ax > 0
which implies, by Definition 2.4.4 that P'AP is n.n.d.
If r(P) < m, then
by property 4 of Result 1.2.12, we see that r(P'AP) < r(P) < m, so that
P'AP is p.s.d. Further, if A is p.d., the quadratic form (Py)'A(Py) — 0 only
when Py = 0, which implies that y = 0 (since r(P) = m). Thus, in (2.4.4),
y'(P'AP)y = 0 only when y = 0, i.e., P'AP is p.d.
(2.4.4)
Result 2.4.2. Properties of nonnegative definite matrices.
1. If an n x n matrix A is p.d. (or p.s.d.), and c > 0 is a positive scalar, then
cA is also p.d. (or p.s.d.).

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
54
2. If two n x n matrices A and B are both n.n.d., then A 4- B is n.n.d. If,
in addition, either A or B is p.d., then A + B is also p.d.
3. Any principal submatrix of a n.n.d. matrix is n.n.d. Any principal sub-
matrix of a p.d. (or p.s.d.) matrix is p.d. (or p.s.d).
Proof. To prove property 1, we see that by Definitions 2.4.5 and 2.4.6, the
matrix A is p.d.
(or p.s.d.)
if the quadratic form x'Ax is p.d. (or p.s.d.),
or since c > 0, if cx'Ax = x'cAx is p.d. (or p.s.d.). This implies that cA is
p.d. (or p.s.d.). Property 2 follows since A and B are both n.n.d., so that we
have by Definition 2.4.4 that for every nonnull vector x
7Zn, x'Ax > 0 and
x'Bx > 0. Hence, x'Ax -I- x'Bx = x'(A + B)x > 0, which implies that the
matrix A+B is n.n.d. In addition, suppose that A is p.d. Then, we must have
by Definition 2.4.6 that x'Ax > 0, while x'Bx > 0 for every nonnull x 6 Rn.
Hence, x'(A+B)x = x'Ax + x'Bx > 0, so that AH-B is p.d. To prove property
3, consider the principal submatrix of an n x n matrix A obtained by deleting
all its rows and columns except its ii,i2*
* • *
>imth, where i\ < %2 <
* • * < im•
We can write the resulting submatrix as P'AP, where P is the nxm matrix of
rank m, whose columns are the ii, Z2, • • • ,imth columns of In. If A is n.n.d., it
follows from Result 2.4.1 that P'AP is too. In particular, the principal minors
of a p.d. matrix are all positive.
Result 2.4.3.
1. An n x n p.d. matrix A is nonsingular and its inverse is also a p.d. matrix.
2. If a p.s.d. matrix A is nonsingular, then it is invertible, and its inverse is
p.s.d.
Proof. To prove property 1, suppose that, on the contrary, the p.d. matrix A
is singular, with r(A) < n. The columns of A are linearly dependent and hence
there exists a vector v / 0 such that Av = 0, which implies that v'Av = 0,
which is a contradiction to our assumption that A is p.d. Hence A must be
nonsingular, and let A-1 denote the regular inverse of A. Since A is p.d., by
Result 2.4.1, (A“ 1)'AA_1 is p.d. But (A-1)' = (A“ 1)'AA~1, implying that
(A-1)' is p.d. and so is A-i. The proof of property 2 is similar, and follows
from Result 2.4.1.
Result 2.4.4. Let A be an nxn symmetric matrix and let D = diag(Ai, • • • , An)
be an n x n diagonal matrix such that P'AP = D. Then,
1. A is p.s.d. if and only if Xj > 0, j = 1, • • • , n, with equality holding for
at least one j, and
2. A is p.d. if and only if X j > 0, j = 1, • • • , n.
Proof. From Result 2.3.4, we know that there exists an orthogonal matrix P
which diagonalizes A. Since P is orthogonal, P' = P” 1, and hence

2.4. N.N.D. QUADRATIC FORMS
55
A = (P
_ 1)'DP
We can also show that A is p.d. (or p.s.d.) if and only if A' is p.d. (or p.s.d.).
Together with Result 2.4.1, this completes the proof.
Result 2.4.5. Diagonability of p.d. (p.s.d.) matrices. An nxn symmet-
ric matrix A is diagonable by an n x n matrix P with r(P) — n (or r(P) < n)
such that A = PP' if and only if A is p.d. (or p.s.d.).
Proof. Let A = PP'. Then
-l = PDP'.
x'Ax = x'PP'x = (P'x)'(P'x) > 0
(2.4.5)
for every nonnull x G lZn. If r(P) = n, then the columns of P form a basis for
Hn, so that P'x = 0 only if x = 0. Hence, A is p.d. If r(P) < n, there exists
some nonnull x G 7£n, such that P'x = 0, so that (2.4.5) holds with equality
for some nonnull x, so that A is p.s.d.
To prove the converse, since A is
symmetric, by Result 2.3.4, we have A = QDQ', where D = diag(Ai, • • • , An),
Aj > 0, j = 1, •
• , n if A is p.d. (or A^ > 0, j — 1, • • • , n if A is p.s.d). Define
y/ Xj
if Aj > 0
0
if A j = 0.
D1/2 — diag(di, • • • ,dn )
where
dj =
Then, A = QD1/2D1/2Q'- PP', where P- QD1/2.
Result 2.4.5 can be used to define the square root of a positive definite (or
positive semidefinite) symmetric matrix A. We may write
A = QD1/2D1/2Q' = QD1/2Q'QD1/2Q',
where Q is orthogonal. Suppose we set B = QD1 //2Q', we see that A = BB = B2
i.e., the matrix B is the square root of the matrix A, and we can write B = A1/2.
For all p.d. k x k matrices A, we show that
exp{— ^
tr(A_1B)}/|A|
f> < (26)fcfeexp(— /c6)/|B|6,
with equality holding only when A = B/26, where B is a k x k symmetric p.d.
matrix, and b > 0 is a scalar. If B1/2 denotes the symmetric square root of B,
then B1/2B1/2 = B, and tr( A^B) = tr{(A~1B1/2)B1/2}- tr(B1/2A-1B1/2).
Since
Example 2.4.4.
x'B1/2A“ 1B1/2x = (B1/2x)'A-1(B1/2x) > 0
if x ^
0, the matrix B1/2A_1B1/2 is p.d. Let Xj > 0, j = 1, • • • , k denote the
eigenvalues of this matrix. Then
k
tr(A XB)
—
tr(B^
2A 1B1^
2) =^
Aj,
and
j=i
k
IA-11|B1^
2||B1/2|= |B|/|A| = TTA
|B1/2A“ 1B1/2|
3 1
j-1

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
56
so that|A| = |B|/ nj=1
From these results, we see that
b
k
k
exP( ~ 2 E Aj)
n A,
exp{-|ir(A JB)}
j=i
j=i
|A|6
|B|6
n^
exp(-iAj)
3=1
Bl6
It can be verified that the function A
^
exp(-|Aj) attains a maximum value of
(2b)b exp(— b) at Aj = 26, j = 1, • • • , k, from which the result follows.
Let P be a p.d. matrix. For any vector b,
Result 2.4.6.
(h'b)2
= b'P_ 1b.
(2.4.6)
sup
h^
O h'Ph
Proof.
For every constant a
7£,
0
<
|| (v — au) ||2
a2 || u||2 — 2au'v+ || v ||2
u'v . o
2
(U'V)2
{a|| u
u ||2 '
For nonzero u, the Cauchy-Schwarz inequality implies that
(uV)2
v'v
(2.4.7)
sup <
v^o .
= u u.
Since P is p.d., there exists a nonsingular matrix R such that P = RR'. Set
v = R'h, and u = R“ 1b. Then, (2.4.7) yields (2.4.6) after simplification.
The next example shows a useful matrix inequality called the extended
Cauchy-Schwarz inequality.
Let b and d be two n-dimensional vectors, and let B be
Example 2.4.5.
an n x n p.d. matrix. We will show that
(b'd)2 < (b'Bb)(d'B“ 1d),
(2.4.8)
with equality if and only if b — aB-id, or if d = aBb, for some constant
a. Since (2.4.8) holds trivially when b = 0 or d = 0, let us consider nonzero
vectors. Let Ai, • • • , An denote the eigenvalues of B, and let Vi, • • • , vn denote
the corresponding normalized eigenvectors. Since B1/2 —
\AiviV^
anc*
B-1/2-ELi Viv'/\/A7, we see that

2.5. SIMULTANEOUS DIAGONALIZATION OF MATRICES
57
b'd = b'ld = b,B1/2B
_ 1/2d = (B1/2b)'(B-1/2d).
Apply the Cauchy-Schwarz inequality (see Result 1.2.1) to the vectors Bx/2b
and B-1/2d to obtain the inequality in (2.4.8).
We end this section with a result on the spectral decomposition of a sym-
metric, n.n.d. matrix.
Let A be an n x n symmetric n.n.d. matrix. We can write A
Result 2.4.7.
in the form
^Q (DO
Z)*
(2.4.9)
where Q is an n x n orthogonal matrix and Di is a diagonal matrix with positive
diagonal elements.
Proof.
The proof follows directly from Result 2.3.4 and the nonnegativity of
eigenvalues of a n.n.d. matrix.
2.5
Simultaneous diagonalization of matrices
We present results that deal with finding a matrix P that will simultaneously
diagonalize two n x n matrices with different properties in terms of symmetry
and nonnegative definiteness.
Result 2.5.1. Let A and B be two n x n symmetric matrices. There exists an
orthogonal matrix P such that P'AP and P'BP are both diagonal if and only
if AB = BA.
Proof. Sufficiency. Suppose that AB = BA. Since A is symmetric, there
exists an orthogonal matrix R such that R'AR = D = diag(A*ImJ, where A*
is a distinct eigenvalue of A with multiplicity m*, i = 1, • • •
say. Suppose
further that R'BR = C = {C^
}, where the matrix C has been partitioned
conformably with D. Then,
CD = R'BRRAR = R'BAR = R'ABR = R'ARR'BR = DC,
or CijAj = AiCij , i ^
j. For i ^
j, since A* ^
Aj, we must have Cij = 0.
That is, the matrix C must be block-diagonal with C = diag{C^}. Since C
is symmetric, Cu must be symmetric.
Hence, for i = 1, * • * ,$, there exist
orthogonal matrices Q* such that Q -C^Qi = A*, which is diagonal. Let Q =
diag{Qj}; then Q'Q = I. Define P = RQ; we have P'P = I and
Q'R'ARQ = Q'DQ = diag{AjQ'Qi} = diag{AiIm .}, and
Q'R'BRQ = Q'CQ = diagJA*} = A.
Necessity. Let P'AP = D, P'BP = A, where D and A are diagonal matrices.
Now, DA = AD, which implies that
P'AP
P'BP

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
58
AB = PP'APP'BPP'= PDAP'= PADP'= PP'BPPAPP'= BA,
and completes the proof.
This result extends to n x n symmetric matrices Ai , • • • , A£, k > 2; these
matrices are simultaneously diagonable by an orthogonal matrix P if and only
if they commute under multiplication in pairs.
Let A be an n x n p.d.
Result 2.5.2.
symmetric matrix. There exists a nonsingular matrix P such that P'AP = I
and P'BP = A = diag(Ai , • • • , An) , where A* are solutions to |B - AA| = 0.
matrix and let B be an n x n
Proof. Since A is p.d., there exists a nonsingular matrix R such that R'AR = I
so that A = (R') _ 1R_ 1 . Also, since B is symmetric, R'BR is symmetric. By
Result 2.3.4, there exists an orthogonal matrix Q such that
Q'R'BRQ = D = diag(Ai , • • • , An),
where A^’s are solutions to the characteristic equation |R'BR — AI| = 0. Note
that|R'BR— AI| = |R'BR-AR'AR| = |R'BR-R'AAR| = |R'||B-AA||R| =
0. Hence, the A*’s are also solutions of |B— AA| = 0. Let P = RQ. Then,
P'BP = Q'R'BRQ = D
and
PAP = Q'R'ARQ = Q'Q = I,
which proves the result.
The problem of finding the solutions for A to the equation |B — AA| = 0 is
called the generalized eigenvalue problem, and reduces to the problem of finding
the eigenvalues of B when A = I.
Since |B — AA| = |R|2|R'BR — AI| =
|A_ 1B — AI| = |BA_1 — AI|, the generalized eigenvalue problem is equivalent
to that of finding the eigenvalues of R'BR or A-1B or BA-1.
Geometrical perspectives
2.6
We discuss orthogonal projections and projection matrices and their relevance
to linear model theory. Recall that if u, v £ lZn, we say that u _L v if u'v = 0. If
u £ Rn and V is a subspace of 7£n , u X V if u'v = 0 for every v £ V. Likewise,
if U and V are two subspaces of 7Zn , then ULV if u'v = 0 for every u £ U and
for every v £ V. The vector space TZn is said to be the direct sum of subspaces
U and V if any vector y £7£n can be uniquely expressed as y = yx + y2, where
yi £ U and y2 G V. We denote this by
Kn = U © V.
We now build upon the basic ideas that we introduced in Chapter 1. We begin
with the definition of the projection of an n-dimensional vector.
The orthogonal projection of a vector vi onto another
Definition 2.6.1.
vector V2 is given by

2.6. GEOMETRICAL PERSPECTIVES
59
(viv2/v
^
v2)v2 = (viv2/ II v2 ||)(1/ || v2 ||)v2.
Since the length of V2/ || V2 || is unity, the length of the projection is
KV2|/ || V2 ||=|| V! II |viv2|/(|| Vi
v2 ||) =11 Vi
cos(0)|
where 9 is the angle between v* and V2.
v>r
/
//
C
v
t5
Figure 2.6.1, Projection of three vectors onto a two-dimensional subspace V
of a three-dimensional space.
Figure 2.6.1 illustrates the projection of three vectors x, y, and z onto vectors
u, v, and w in a two-dimensional subspace V of a three-dimensional space. We
next discuss the notion of a projection of a vector onto a subspace of the 71-
dimensional Euclidean space. This concept is basic to an understanding of the
geometry of the least squares approach which is a classical estimation tool in
linear model theory. We show that such a projection exists, it is unique and the
corresponding matrices of the projection are unique as well. This is graphically
represented in Figure 2.6.2, which illustrates the projection of a 2-dimensional
vector y onto a vector u which belongs to a subspace V, and a vector v which
belongs to Vx, the orthogonal complement of V. The orthogonal complement
of any subspace V of 7Zn is defined below.
Definition 2.6.2. Orthogonal complement.
For any subspace V c 7Zn,
the orthogonal complement of V, written as V1 is the subspace of 1Zn which
consists of vectors in lZn that are orthogonal to every vector in V. Then, VfiV1
is empty.
The null space of any n x k matrix X is the orthogonal complement of
the column space of X', i.e., A/"(X) = C(X')-1-. The next result discusses the
orthogonal decomposition of an n-dimensional vector.

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
60
Result 2.6.1. Every vector y G 1Zn can be expressed uniquely as
±
(2.6.1)
y = u + v,
u
V, v
V
where V is a subspace of IZn. Further, if X is an n x k basis matrix for V, i.e.,
V = C(X), and the columns of X are LIN, then the projection of y onto V is
given by X(X'X)-1X'y. We can write 7ln = V ® V±.
Proof.
y = ui +vi and y = U2 + V2, where Ui, U2 G V, and vi, v2 G Vx. It follows that
ui - U2 + vi - V2 = 0. However, ui - U2 G V, while Vi - V2 G V1. Therefore,
we must have ui = U2, and vi = V2, i.e., the decomposition of y is unique. To
prove the second part of the result, suppose that v = X(X
/X)
_ 1X'y. Clearly,
v G V. We must now show that y — v
Vx. Let u G V. We can write u = Xc,
for some vector c. Hence,
(y _ v)'u = (y - X(X,X)
"1X,y)'Xc = y'Xc - y,X(X,X)'1X/Xc = 0,
so that y — v G V-1. That is, the projection of y onto V is X(X
/X)
_ 1X'y, which
is a linear function of y. That 7Zn = V 0 V1 follows directly from the definition
of the direct sum of a vector space (see the discussion following Definition 1.2.7).
If possible, let there be two such decompositions of y, i.e., suppose
Definition 2.6.3. Projection matrix. The matrix Py = X(X X)
_ 1X
/ is
called the projection matrix, since premultiplying y
TZn by this matrix gives
the projection of the vector y onto V. The matrix Py (which is simply denoted
by P when it is clear which subspace we are projecting onto) is the unique linear
function which assigns to each y its projection onto the subspace V, which is
itself a vector.
Result 2.6.2. The projection matrix P and the matrix is In — P are symmetric
and idempotent, and further PX = X.
Proof. We see that
P' = [XtX'X)-1^
]^ X(X
/X)
“ 1X', and
P2 = PP = X(X'X)-1X'X(X,X)-1X
/ = xtx'x^
x'
so that symmetry and idempotency of P follow directly from Definition 1.2.15
and Definition 2.3.1. To show this in another way, observe that Pc G V, and
(In — P)d G VL for some vectors c and d, so that by Definition 1.2.8, c'P'(In —
P)d = 0, which in turn implies that P'(In — P) = 0, that is, P' = P'P. Then,
p = (P')' = (P'P)' = P'P = P',
which implies that P is symmetric. Since P2 = P, it is also idempotent. That
r(P) = k follows directly from property 3of Result 2.3.9. It is easy to verify that

2.6. GEOMETRICAL PERSPECTIVES
61
PX = X. The proof of the symmetry and idempotency of In — P is obtained
in a similar manner.
Result 2.6.3. The column space C(P) of P is V, and the column space C( In -
P) is Vx. If dim(V) = k, then tr(P) = r(P) = k and tr(In - P) = r(In - P) =
n — k.
Proof. Since Py = u
V, it follows that C(P) C V. Also, if x 6 V, then
by Result 2.6.1, the unique orthogonal decomposition of x is x = x 4- 0, which
implies that x = Px
C(P) . The two spaces therefore coincide, and dim(V) =
r(P). Since the projection matrix P is symmetric and idempotent, it follows
from Result 2.3.9 that
r(P) = tr(P) = trlXiX'xy' X' } = ^[X'XCX'X)-1] = tr(lk ) = k.
That r(In - P) = n — k follows immediately.
/
/
/
/
/
u
t
/
/
/
/
/v
/
/
Figure 2.6.2. Projection of a 2-dimensional vector y onto a subspace V and
its orthogonal complement V 1-.
Result 2.6.4. The matrix In — P represents the orthogonal projection onto
Vx.
Proof. Prom Result 2.6.2, recall that P is the n x n symmetric, idempotent
projection matrix of y. Using the identity y = Py + (In - P)y, it follows from
Result 2.6.1 that v = (In — P)y, so that In — P represents the matrix of the
orthogonal projection onto Vx. Then,
(Py)'(In - P)y = y'(P - P2)y = 0,
which implies orthogonality of the components of y that belong respectively to
V and V-1-. But, P and In — P respectively span the orthogonal subspaces V
and Vx (see Result 2.6.3), from which the result follows.

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
62
Result 2.6.5. If v S V, then,
1- II y- v ||2=||y - Py||2 +|| Py- v ||2.
2. || y - Py ||2<|| y - v ||2 for all v
V with equality holding if and only if
v- Py.
Proof. To prove property 1, we note that since Py is the projection of y onto
V, y — Py G
and Py- v G V. Hence, y — Py _L Py — v, so that the cross
term is zero in
l| y - v ||2=|l (y - Py) + (Py - V) ||2.
2
2
2
We therefore get || y — v|| =|| y — Py|| + ||Py — v || . To prove property 2,
first note that since|| y — v ||2 =|| y — Py||2+ ||Py- v||2, and || Py — v||2 >
0, we must have || y — Py || <|| y — v|| for all v GV. It is also obvious that
II Py “ v || = 0, and we get equality if and only if v = Py, i.e., v G V is the
vector closest to y.
Result 2.6.6. Let X be an n x k matrix of rank k and with column space C(X).
There exists an (n- k ) x n matrix Z such that ZX = 0, and C(X) = A/
*(Z).
Proof. Let Q. = C(X). By Definition 2.6.3 and Result 2.6.3, we see that P^ =
X(X,X)_ 1X' has rank k, while the n x n matrix I„-Pn = I„- X(X'X)
_1X'
has rank n — k.
Let Z consist of any n - k LIN rows of In
PQ.
Since
(In — PQ)PQ = 0 by Result 2.6.4, we have ZX = 0. Further, by Definition
2.6.2, M{ Z) = C(Z')X = C(I - PQ)X =a
Result 2.6.7. Let fl C 1Zn, Oi C O, fi]1 C ft, and let P^, Pfix and Pfix
denote the corresponding projection operators. Then,
1- POPQ!
PHJPO = P«i -
2. PnPfii = PQJ- PQ = Pnx.
3. Pfii PQ-L = PQX PQJ = 0.
4- pn = Pni d- PnfnQ *
Proof. Since by Result 2.6.3, fti = C( PQ1 ), we see that P^P^ = Pnx. By
symmetry of these projection matrices, property 1 follows. This implies that
a vector v projected first onto ft and then onto fti stays in fti. The proof
of property 2 is similar. To prove property 3, write PQ-LP^ = PQ± ( PQ -
pOj0 = P^
pn - P?
states property 3 in an alternate way, consider P^y = Pfily + (PQ — Pox )y,
where P^y Gft, and P^
y G f2. We also see that (PQ- Pnx )y Gft. Thus,
P^y = Pfily + (PQ — Pni )y corresponds to the unique orthogonal decomposi-
tion described in Result 2.6.1, so that PQ = P^ -1- Pn± nQ.
= V
Pn± = 0.
To prove property 4, which

2.7. VECTOR AND MATRIX DIFFERENTIATION
63
2.7
Vector and matrix differentiation
We first define derivatives of vectors, matrices, products of vectors and matrices,
as well as scalar functions of vectors and matrices, results which commonly ap-
pear in linear model theory (see Magnus and Neudecker, 1988 for more details).
We assume throughout that all the derivatives exist and are continuous.
Let /3 = (f t, - - * ,A)' be a A;-dimensional vector and let
Definition 2.7.1.
f ( /3) denote a scalar function of (3. The first partial differential of / with respect
to (3 is defined to be the A:-dimensional vector of partial differentials d f / dfii:
(df /dt3A
df / dfo
Qf (P )/op = d f / d p =
(2.7.1)
\Of /O0k )
Also,
df /d0
; = {df / dpu
df /dfo,
df /d(3k ) .
(2.7.2)
Definition 2.7.2.
defined to be the k x k matrix of partial differentials 32 f /3(3id(3j:
The second partial differential of / with respect to (3 is
f
O2 f /O0\
O'2 f /O0,O02
O2 f /O01d0k\
02 f /00,002
02 f /002
02 f /00200k
O2 f /O0O0' =
.
(2.7.3)
\02 f /00,00k
O2 f /O02O0k
• • •
O2 f /O02 )
Example 2.7.1.
Let 0 — (01,02 )' and let f (0) = (02 — 20,02 ). Then,
Of /00' = (20,- 202, -20,)
and
2
— 2\
-2
o j
*
d2 f /3(33(3'-
Let / and g represent scalar functions of a A>dimensional
Result 2.7.1.
vector /3, and let a and b be real constants. Then
0(af + bg )/O0j =
aOf /O0j + bOg/OPj
O( fg )/O0j =
fOg/OPj + gOf /OPj
0( f / g )/00j =
(l/ g^
igOf /OPj - fOg/OPj }.
(2.7.4)
Let A = { ciij } be an m x n matrix and let /(A) be a real
Definition 2.7.3.
function of A. The first partial differential of / with respect to A is defined as

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
64
the m x n matrix of partial differentials df / daij:
{df / da,ij } ,i = l, -
- ,m, j = 1,
-
( df /dan
df /dai2
df /daXn\
df { A)/dA
, n
•
(2.7.5)
\df /dami
df /dam2
• • •
df /damn/
The results that follow give rules for finding partial derivatives of vector
or matrix functions of matrices and vectors and are useful in the first step in
linear model theory, viz., obtaining a solution to the least squares minimization
problem.
Let 0 denote an n-dimensional vector and let A be an m x n
Result 2.7.2.
matrix. Then
dA0/d0' = A, and 90' A' /80 = A'.
(2.7.6)
Proof.
We may write
^ 0-1101 +
* • •+ O\n0n ^
02101 +
* * * + 02n0n
A0 =
\amlA “{
* * * “h Omn0nf
so that by Definition 2.7.2, dA0/d0' is given by
/ 9(ou0i -1
1- O\n0n )/d0i
• • •
d(an0i +
f O\n0n )/d0n
N
d(O2l01 +
1" O2n0n )/d01
*
*
*
d( d2l01 +
1"^2n/?n)/90n
\9{Omi0i
“H
*
* * "h ornn0n )/90\
• • •
9{(lml01 "h
4 * * ”h ornn0n ) j90nJ
Oln
On
(2.7.7)
= A .
\Oml
That 90' A' /90 = A' follows by transposing both sides of (2.7.7).
Result 2.7.3.
Let 0 be an n-dimensional vector and let A be an n x n matrix.
Then
O-mnJ
dP' Ap/dp =
(A + A')/3
dp’ Ap/ dp1 =
p' ( A+ A' )
d2 p' Ap/ dpdp'
=
A + A'.
(2.7.8)
Further, if A is a symmetric matrix,
dp' Ap/dp =
2Ap
dp' Ap/dp'
=
2/3' A
d2P' Ap/dp dp'
=
2A.
(2.7.9)

2.7. VECTOR AND MATRIX DIFFERENTIATION
65
Proof.
We will prove the result for a symmetric matrix A. Clearly,
/3'A/3 = £ ciijPipj
M=1
so that
n
^^
Q'rjPj “t"
2CLrrPr
d/3' AP/ dpr
3=1
i=l
n
=
2^
drj(3j
(by symmetry of A)
j=i
=
2a'r/?
where a'r denotes the rth row vector of A. By Definition 2.7.3, we get
( a'i \
( dfi' Af3/d0i\
d(3' A0/dp2
<9/3'A/3
a2
(2.7.10)
.
/3- 2A/3
= 2
5/3
VnJ
The second result follows by transposing both sides of (2.7.10). To show the
last result, we again take the first partial derivative of d(3rA(3/d(3
f = 2flrA, and
use Result 2.7.2.
WA0/d(3nJ
Let C be an rrt x n matrix, a be an m-dimensional vector and
Result 2.7.4.
13 be an n-dimensional vector. Then
da'C/3 = a(3'.
(2.7.11)
dC
Proof.
Since
a'c/3 = n i l E;=I V V
CijCXi(3j,
we have
d(a'C0)/dcki = aicPi
which is the (k,l)th element of
from which the result follows.
We next give a few useful results without proof.
Let A be an n x n matrix. Then
Result 2.7.5.
dtr( A) _
dA
n’
(2.7.12)
and
0|A|
aA = [Adj(A)|
where Adj(A) denotes the adjoint of A.

CHAPTER 2.
PROPERTIES OF SPECIAL MATRICES
66
Suppose A is an n x n matrix with|A| > 0. Then
Result 2.7.6.
31n|A|
aA
(2.7.13)
Result 2.7.7
Let A be an m x n matrix and let B be an n x m matrix. Then
9£r(AB) = B'.
(2.7.14)
dA
Let
be a symmetric matrix, let y be an n-dimensional
Result 2.7.8.
vector, let 0 a /c-dimensional vector, and let X be an n x k matrix. Then
d( y- X/J)'n(y- X0 ) / d0
d2{ y - Xpyn{ y - XP ) / d(3d(3' =
2X'fiX.
-2X'fl(y- X/3), and
(2.7.15)
The next definition deals with partial derivatives of a matrix (or a vector) with
respect to some scalar 0. We see that in this case, the partial differential is
itself a matrix or vector of the same dimension whose elements are the partial
derivatives with respect to 6 of each element of that matrix or vector.
Definition 2.7.4.
6, then
Let A be an m x n matrix which is a function of a scalar
{ dciij / dO } , i = l, - - - , m, j = ! ,
,n
/ dan/86
da^/ dO
da\n / dd\
8A/86
(2.7.16)
\dami /d0
dam2 / d9
• • •
8amn/d6
2.8
Special operations on matrices
Let A = {a^j} be
Definition 2.8.1. Kronecker product of matrices.
an m x n matrix and B = { bij } be a p x q matrix. The Kronecker product of
A and B is denoted by A <g) B and is defined to be the rap x nq matrix
'
anB
CI12B
• • •
ainB
N
a2lB
<222®
• • •
Ct2nB
(2.8.1)
A ® B =
\
am2B
* • •
amnB J
The matrix in (2.8.1) is a partitioned matrix whose (i , j)th entry is a p x q
submatrix a^B. The Kronecker product A
B can be defined regardless of
the dimensions of A and B. The Kronecker product is also referred to in the
literature as the direct product or the tensor product
Example 2.8.1.
Consider two matrices A and B where

2.8. SPECIAL OPERATIONS ON MATRICES
67
3
4
-J).
A =
2
0
Then,
(15 -3
20 -4 -5
1\
9
12
12 -3 -3
0
0
0
0
0
0
A ® B =
9
10 -2
0
6
6
0
and
^
15
20 -5 -3 -4
1
10
0
0 -2
0
0
12 -3
9
12 -3
^
6
0
0
6
0
0
B ® A =
In general, A®B is not equal to B®A. The elements in these two products
are the same, except that they are in different positions. The definition A ® B
extends naturally to more than two matrices:
k
(2.8.2)
A ® B ® C = A ® (B ® C)
and
® A* = Ai ® A2 ® •
• ® A&.
i-1
Result 2.8.1. Properties of Kronecker product,
matrix. Then
1. For a positive scalar c, we have c ® A = A ® c = cA.
2. For any diagonal matrix D = diag(dj, • • , dk ), we have
Let A be an m x n
A ® D = diag(di A, • • • , dkA).
3. I ® A = diag(A, A, * • • , A).
4. 1772 ® Ip ~ Imp•
5. For a p x q matrix B, we have (A ® B)' = A' ® B'.
6. (A®B)(C®D) = (AC) ® (BD), where we assume that relevant matrices
are conformal for multiplication.
7. r(A ® B) = r(A)r(B).
8. (A + B) ® (C + D) = (A ® C) + (A ® D) + (B ® C) + (B ® D).
9. Suppose A is an n x n matrix, and B is an m x m matrix. The nm
eigenvalues of A ® B are products of the n eigenvalues Ai,i = 1, • • • , n of
A and the m eigenvalues 7j, j =!, * • • ,m of B.
71
m
n
m
10. |A <8> B| = |A|m |Br = r[ Ai
Li=l
.
U i j
j-l

CHAPTER 2.
PROPERTIES OF SPECIAL MATRICES
68
-l = A-1 0 B_ 1.
11. Provided all the inverses exist, (A 0 B)
Moser and Sawyer (1998) present algorithms for sums of squares and covariance
matrices using Kronecker products.
Definition 2.8.2. Vectorization of matrices.
A with columns ai, a2, • • • ,an, we define vec(A) = (a'1 ? a
^
, • • • ,aJJ' to be an
ran-dimensional column vector.
Given an m x n matrix
Result 2.8.2. Properties of the vec operator.
1. Given m x n matrices A and B, vec( A + B) = vec( A) 4- vec(B).
2. If A, B and C are respectively m x n, n x p and p x q matrices, then
(i) vec(AB) = (Ip 0 A)vec(B ) = (B' 0 Im )vec( A).
(ii) vec(ABC) = (C' 0 A)vec(B ).
(iii) vec( ABC) = (I, <8> AB )vec{C ) = (C'B'0 In)vec(A).
3. If A is m x n and B is n x m,
vec(B' )'vec( A)= vec( A' )'vec(B ) = tr(AB).
4. If A, B and C are respectively m x n, n x p and pxm matrices,
vec( A' y(C' 0In)t;ec(B)
uec(A')'(Im 0 B)uec(C)
vec(B')'(A 0 Ip)t'ec(C)
vec(B' y( In 0 C)vec(A)
t>ec(C')/(B/ 0 Im)i;ec(A)
vec(C' y( Ip 0 A)vec(B)
tr(ABC)
Definition 2.8.3.
matrices A and B (which can be of any dimension) is defined as
Direct sum of matrices.
The direct sum of two
A
0
0
B
(2.8.3)
A 0 B =
This operation extends naturally to more than two matrices:
/ Ai
0
•
•
0
0
A2
0
k
k
^—
>+
® Ai = >
Ai = Ai ® A2 0 • •
2=1
“
. (2.8.4)
• ® Afc =
2=1
0
0
0
Afc
/
This definition applies to vectors as well.

2.9. LINEAR OPTIMIZATION
69
2.9
Linear optimization
The technique of Lagrange multipliers (sometimes called undetermined multi-
pliers) is used to find the stationary points of a function of several variables
subject to one or more constraints. Consider the problem of finding the mini-
mum of a function f ( x1, 0:2) subject to a constraint relating xi and X 2 which is
written as
g{ xux2 ) = 0.
One approach to the minimization is, of course, (a) to express x2 as a function
h( x1) of X\ by solving (2.9.1), (b) to substitute x2 = h( x1) into f ( x\, x2 ) to
obtain /(x1, h( x1)), and (c) to minimize this function of a single variable x\ in
the “ usual” way using differential calculus. A difficulty with this approach is
that explicitly obtaining h( x1) may be difficult in some cases. A simpler, and
more elegant method is the Lagrange multiplier approach, which incorporates
a parameter A into the minimization problem. Suppose x = (xi, * * * , x^)'
V C TZd\ the constraint equation g(x ) = 0 geometrically represents a surface S
in V. We denote the gradient of the function /(x) at any point P on S by V/
and wish to find the stationary point of /(x) within the surface. To do this, we
compute the component V5/ of V/ which lies in S and set V5/ = 0. Consider
the Taylor expansion of g( x.)
(2.9.1)
P(x + e) = 5(x) + e'Vg(x)
for some small e. If the point x + e lies within the surface
<S, then we have
<?(x -f* £) =
<?(x), and e'V^
(x) —
0, i.e., the vector X7g is orthogonal to the
surface p(x) = 0. We obtain V5/ by adding to V/ some multiple of Vg:
v5/ = V/ + AVg.
(2.9.2)
Let
£(X >A) = /(x) + A^
(x)
denote the Lagrangian function. Note that dL/ dX — 0 leads to the constraint
condition #(x) = 0. The stationarity condition for minimizing L is VL = 0,
where VL is given by the right side of (2.9.2). We find the stationary point of
L(x,A) with respect to both x and A, using d+1 equations, leading to stationary
solutions x and A. If we are not interested in A, we can eliminate it from the
stationarity equations without the necessity of finding its value (hence A is
called the “ undetermined multiplier” ). This technique can be extended to the
situation where there are K constraints, Pj(x) = 0, j = 1, • • , K. In this case,
the Lagrangian function becomes
(2.9.3)
K
L(x, A) = /(x) +
^
TA^
CX)
(2.9.4)
j=1
where A = (Ax, • • • , XK )' denotes the vector of Lagrangian multipliers. We min-
imize (2.9.4) with respect to x and A (see Dixon, 1972).

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
70
Exercises
2.1. Let P = (A
B), where P is an n x n orthogonal matrix. Show that A'A
is an idempotent matrix.
An
AI2
A21
A22
2.2. Suppose an m x n matrix A is partitioned as A =
An,
(n — r ) x (n — r ) submatrices, such that r(An ) = r(A) = r > 0. Show
that A22 — A2iA111Ai2.
, where
A12, A21, and A22 are respectively r xr, r x (n — r ), (n-r) xr, and
P
x
2.3. Suppose an n x n matrix A is partitioned as
(n — l) x (n — 1) dimensional nonsingular matrix, and x is an (n — 1)-
dimensional vector. Show that x'P_ 1x = 1 — |P — xx'|/|P|.
, where P is an
1
2.4. Let A, B, C, and D b e m x p, p x g, n x m, and nxq matrices respectively,
and let E = D — CAB.
0
B
A
0
A
AB
CA
D
= rr °
\ 0
A = r(A) + r(B).
(a) Show that r
fD
CA
VAB
A
(b) Show that r
= r(A) + r(E).
2.5. Consider the partition in Example 2.1.2, and suppose that A2 contains
only one column, i.e., r = k — 1. Show that
(I-PQA.A^
I-P, )
P = Pl4
A'2(1-P1 )A2
2.6. [Rao and Toutenberg, 1995, p. 295]. Let A be an n x k matrix and B be
a k x n matrix with n > k.
-Ain -A
n— k
(a) Show that
(b) Hence, show that the n eigenvalues of AB are equal to the k eigenval-
ues of BA, together with the eigenvalue 0, which has a multiplicity
of (n — k ).
(c) Let v denote a nonzero eigenvector corresponding to a nonzero eigen-
value A of AB. Show that u = Bv is a nonzero eigenvector of BA
corresponding to this A.
2.7. Let A = aa', where a is a nonzero n-dimensional vector. Show that the
only nonzero eigenvalue of A is A = a'a with corresponding eigenvector a.
2.8. Let A be a k x k nonsingular matrix, and let B and C be respectively
k x n and nx k matrices. Show that |A + BC|= |A||In + CA_1B|.
2.9. Let B and C be k x n and n x k matrices respectively. Show that \lk +
BC|=|In 4- CB|.
= (-A)
|BA-AIfc| = |AB- AI„|.
B
Ik

Exercises
71
2.10. Show that |A + aa'| = (1 + a'A 1a)|A|, where A is a k x k nonsingular
matrix and a is a ^-dimensional vector.
2.11. If (AB — AC)'(AB — AC) = 0, show that we must have AB — AC = 0.
2.12. Let A be an n x n symmetric matrix with eigenvalues Ai, • • • , An.
(a) Show that r(A) is equal to the number of nonzero eigenvalues of A.
(b) Show that || A||= (£”=1 A?)1/2.
2.13. Let A be an n x n symmetric matrix which has k distinct eigenvalues
Ai, *
* , A/c with geometric multiplicities
<?!,
• • ,9k and algebraic multi-
plicities ai, - - * , afc. Show that J2 j=i 9j ~ XIjLi aj —
and that gj = aj
(= rrij ), for j = 1, - - - ,h.
2.14. Let x be an n-dimensional nonzero vector. Is
((a) -2“ symmetric and idempotent ?
(b) xx' symmetric and idempotent ?
2.15. Let A be an n x n symmetric matrix, let B and U be k x k and n x k
matrices such that U'U = I/-, and AU — UB. Show that there exists an
n x (n — k ) matrix V such that the n x n matrix (U
V) is orthogonal
B
0 \
0
V'AV ) •
and (U
V)' A (U
V) =
^
2.16. Show that every symmetric idempotent matrix is nonnegative definite.
2.17. Let A be an n x n matrix. Show that the nonzero eigenvalues of AA'
coincide with the nonzero eigenvalues of A'A.
2.18. Let A and B be n x k and k x n matrices respectively, with n > k.
(a) Show that the n eigenvalues of AB are equal to the p eigenvalues of
BA, together with the eigenvalue 0 with multiplicity (n — k ).
(b) If x is a nonzero eigenvector of AB corresponding to a nonzero eigen-
value A, show that y = Bx is a nonzero eigenvector of BA corre-
sponding to A.
2.19. Let Q be an n x n nonsingular matrix which diagonalizes a nonsingular
matrix A. Show that A-1 is also diagonalized by Q.
2.20. Let A be an n x n symmetric matrix of rank r. Suppose tr(A) = tr(A2) =
P-
(a) Show that 0 < p < r.
(b) If tr(A) = r, show that A must be idempotent of rank r.

CHAPTER 2. PROPERTIES OF SPECIAL MATRICES
72
2.21. Suppose Ai and A2 are respectively n x p and n x q matrices of ranks p
and q, and suppose the columns of Ax are LIN of the columns of A2. Let
B = I - Ai (A'1Ai)_1A'1. Show that C = A2BA2 is nonsingular.
2.22. Let P be an n x n orthogonal matrix and let A be an n x n symmetric
and idempotent matrix. Show that the matrix P'AP is symmetric and
idempotent.
2 -1 -1
2.23. Let A = I -1
1
1
\~1
1
4
B such that A = BB'.
. Verify that A is a p.d. matrix. Find a matrix
2.24. Let A = (1 — a)In 4- aJn. For what values of a is the matrix A p.d. ?
2.25. Let B be an n x n p.d. matrix and let A be an n x n symmetric matrix.
Show that there exists a positive real number A such that B — AA is p.d.
2.26. Let A be an n x n symmetric matrix and x be an n-dimensional vector.
(a) If x'Ax = 0 for all x, show that A = 0.
(b) Let A be p.s.d. Show that if x'Ax = 0, then Ax = 0.
2.27. Let A and B b e n x n symmetric matrices such that A 4- B is nonsingular.
Let x, a and b be n-dimensional vectors. If c = (A 4- B)_1(Aa 4- Bb),
show that (x- a)'A(x — a)4-(x- b)'B(x — b) = (x — c)'(A 4- B)(x- c)4-
(a - b)'A(A 4- B)"1B(a - b).
2.28. Let A, B, and C b e m x m, n x n and n x m matrices respectively, and
suppose A and B are p.d. Show that
(a) (A"1 4- C'B-1C)-1 = A - AC^
CAC' 4- B)~XCA,
(b) (A"1 4- C'B_
1C) ~1C/B_1 = AC'(CAC' 4- B)_1.
• , Afc be n x n matrices.
(a) Show that a necessary condition for Ai , • • • , Ak to be simultaneously
diagonalizable (by an n x n nonsingular matrix P) is that A^AZ —
AiAj ) j > i ~ 1, * * • , k , i.e., Ai, • • • , A^ commute in pairs.
(b)
If Ai , • • • , Ak are symmetric, show that the condition in (a) is nec-
essary and sufficient.
2.30. Let A and B be n x n symmetric n.n.d. matrices. Show that there exists
a nonsingular matrix P such that P'AP and P'BP are both diagonal.
2.31. For square matrices A and B, show that tr( A0 B) = tr(A)tr(B).
2.32. If u — Pvy, y 6 7£n, show that Pv must be unique.
2.29. Let Ai > • •

Chapter 3
Generalized Inverses and
Solutions to Linear Systems
The notion of a generalized inverse of a matrix has its origin in the theory of si-
multaneous linear equations. A generalized inverse of a matrix A is some matrix
G such that Gb is a solution to a set of consistent linear equations Ax = b (Rao
and Mitra, 1971). We give the definition and properties of generalized inverses
of matrices in section 3.1, while in section 3.2, we discuss solutions to systems
of linear equations. Both topics play a fundamental role in the development of
linear model theory.
3.1
Generalized inverses
Definition 3.1.1. A generalized inverse (g-inverse) of an m x n matrix A is
any n x m matrix G which satisfies the relation
(3.1.1)
AGA = A.
The matrix G is also referred to in the literature as “conditional inverse” or
“ pseudo-inverse” . We will refer to G as g-inverse and denote it by A ~ (pro-
nounced A minus).
Result 3.1.1. A g-inverse G of a real matrix A always exists. In general, G is
not unique except in the special case where A is a square nonsingular matrix.
Proof. To show the existence of G, we recall the full-rank factorization of an
m x n matrix A, with r(A) = r given by A = BC where B and C are m x r and
r x n matrices respectively, each with rank r, and B B and CC' are nonsingular
(see Result 2.2.1). It is easy to verify that the n x m matrix defined by
A} = C'(CC,)“ 1(B'B)-1B'
(3.1.2)
73

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
74
satisfies (3.1.1) and is a g-inverse of A. For arbitrary nxm matrices E and F,
let
A2 — Ax AAL 4- (In — Ax A)E 4- F(Im — AAj ).
(3.1.3)
By direct multiplication, it is easy to verify that A2 satisfies (3.1.1). Inserting
different matrices E and F into (3.1.3) generates an infinite number of g-inverses
of A, starting from AJ
". The only matrix A which has a unique g-inverse is
a square matrix with |A ^
0.
Use the notation A- for G in (3.1.1), and
pre-multiply and post-multiply both sides by A"1 to get
= A-1AA-1,
i.e.,
A “ = A_1,
-l
A l A A A A
and completes the proof.
Result 3.1.2. Algorithm to compute A” . Let Bn denote a submatrix
of A with r(Bn) = r(A) = r. Let R and S denote elementary permutation
matrices that bring Bn to the leading position, i.e.,
Bn
B12
B21
B22
RAS =
= B, say.
Then,
A
" = SB-R = {R'(B-),S
/}
(3.1.4)
is a g-inverse of A, where
Bn
1
0\
0
0/
*
(3.1.5)
B" =
-1
Proof. Since R and S are orthogonal, R' = R-1 and S' = S
A = R'BS'. It is easy to verify that B
“ satisfies (3.1.1), so that it is a g-inverse
of B (see Exercise 2.2). Now,
AA~A = R'BS'SB-RR'BS' = R'BB
“ BS' = R'BS' = A
since RR' = Im and S'S — In.
so that
To summarize the algorithm, we find a submatrix Bn of order r of A, and
compute (Bn
1)'. In A, we replace each element of Bn by the corresponding
element of (Bn1)', and replace all other elements of A by 0. We transpose the
resulting matrix to obtain A“ .
Example 3.1.1. We will find a g-inverse of the rectangular matrix
4 12
0
1 1
5
15
3 13
5
A =

3.1 . GENERALIZED INVERSES
75
4
1
using Result 3.1.2. We first verify that r(A) = 2. Next, suppose Bn =
then |Bn| = 3. Application of Result 3.1.2 gives the corresponding g-inverse as
1
1
1/3
-1/3
0\
1/3
4/3
0
0
0
0
'
0
0
0/
Suppose, we now choose another nonsingular submatrix of A of rank 2, i.e.,
we let Bn = (j
, with |Bn| —
— 10. Using the algorithm, we find the
corresponding g-inverse to be
A-
-
/0
0
0 \
0
-1/2
3/2
0
0
^
0
1/10 -1/10/
In this example, we see that a unique g-inverse of A does not exist. Result
3.1.3 shows this formally.
A
” =
0
Example 3.1.2.
Let A be an m x n matrix with a g-inverse G. The following
results can be verified using (3.1.1).
1. For nonsingular matrices P and Q, a g-inverse of PAQ is the matrix
Q_iGp-i This is clear from PAQQ
“ 1GP~ 1PAQ = PAQ, since PP
1 =
Im, QQ
-1 = In, and AGA = A.
2. A g-inverse of GA is GA since GAGAGA = GAGA = GA.
3. Let c be a scalar. A g-inverse of cA is G/c, which is verified by seeing
that cA(G/c)cA = cAGA = cA.
4. A g-inverse of the unit matrix J„is I„/n, which is verified by Jn(In/n)J„=
TlJn / fl — Jn.
Example 3.1.3.
Let
2
2
6
2
3
8
6
8
22
A =
2
6
2
8
Result 3.1.2, we find the g-inverse corresponding to Bn to be
with |A| = 0.
Let Bn —
, with |Bn| = 4.
Using the algorithm in
2
-3/2
0\
0
0
0
.
-1/2
1/2
0/
A- =

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
76
2
2
2
3
corresponding g-inverse
, with |Bn| —
2. Using the algorithm, we find the
Again, let Bn =
3/2 -1
0
-1
1
0
0
0
0
A" =
which is symmetric.
The previous example demonstrates that even if A is an n x n symmetric
matrix, its g-inverse A- is not necessarily symmetric. However, we can always
construct a symmetric g-inverse of a symmetric matrix. To see this, note that if
A- is any g-inverse of a symmetric matrix A, so is A
(s e e Result 3.1.3), and
therefore, the symmetric matrix|(A~ 4- A-') is a g-inverse of A. In general,
we may also obtain a symmetric g-inverse by applying the same permutation in
the algorithm of Result 3.1.2 to the rows and to the columns. This would result
in a symmetric Bn and therefore a symmetric g-inverse.
Let A
be a g-inverse of a symmetric matrix A. Then (A )'
Result 3.1.3.
is also a g-inverse of A.
Proof.
By (3.1.1), we have AA A = A. Transposing both sides, and using
A' = A, we get the result.
If A
is a g-inverse of A, then
Result 3.1.4.
r(A) <r(A ) < min(n, m).
(3.1.6)
Proof. The proof follows directly from property 4 of Result 1.2.12.
Let A be an m x n matrix of rank r. Then
Result 3.1.5.
1. A A and AA
are idempotent.
2. (I — A-A) and (I — AA
_
) are idempotent.
3. r(A“ A) = r(A) = r and r(I — A"A) = n — r(A) = n — r.
4. £r(A
“ A) = tr( AA
“ ) = r.
Proof. Using (3.1.1), we see that (A~A)(A“ A) = A
” A and (AA
*~)(AA
“ ) =
AA” , proving property 1. We can prove property 2 by a similar application
of (3.1.1). To prove property 3, we see that since (I- A-A) is idempotent,
it follows from property 3 of Result 2.3.9 that r(I — A-A) = tr(I — A-A) =
n- r(A) = n — r. The result for A-A follows immediately. Property 4 follows
from property 3 and the idempotency of AA” and A"A.

3.1. GENERALIZED INVERSES
77
Definition 3.1.2. If A A = AA , then A
is called a commuting g-inverse
of A, where A is a square matrix (Englefield, 1966).
Result 3.1.6. Let D — diag(di , • • • , dn). Then, D
“ is a diagonal matrix with
the zth diagonal element given by 1/d* if di ^
0 and by 0 if di = 0.
Proof. It is easy to verify that D~ satisfies (3.1.1).
Result 3.1.7. Let A be a symmetric matrix of rank r. Let Ai, • • • , Xn denote
the eigenvalues of A and let xi, • • • , xn denote the corresponding eigenvectors
of A. A g-inverse of A is given by A" = XD~X', where X = (xi, - - - , xn)'
and D- — diag(di, • • • , dn ), where di = 1/A* if Xi / 0 and di = 0 if A* = 0.
Proof.
generality, we can label them as the first r values, viz., Aj, • • • , Ar. Since A is
symmetric, we have by Result 2.3.4 that
A- XDX' with XX' = X'X = In.
Note that there are exactly r nonzero eigenvalues, and without loss of
Since D
is a g-inverse of D,
AA
“ A = XDX'XD~X'XDX' = XDD” DX' = XDX' = A,
so that XD~X' is a g-inverse of A.
Result 3.1.8. Let G be a g-inverse of the symmetric matrix A'A, where A is
any m x n matrix. Then,
1. G' is also a g-inverse of A'A.
2. GA' is a g-inverse of A, so that
AGA'A = A.
(3.1.7)
3. AGA' is invariant to G, i.e.,
AGiA'= AG2A',
(3.1.8)
for any two g-inverses Gi and G2 of A'A.
4. Whether or not G is symmetric, AGA' is.
Proof. To prove property 1, we see that since G is a g-inverse of A'A, we have
from (3.1.1) that A'AGA'A = A'A. Transposing both sides
A'AG'A'A = A'A,
(3.1.9)
so G' satisfies (3.1.1) and is a g-inverse of A'A. Using property 1 of Result 2.3.8,
we see that the result A'AGA'A = A'A implies (3.1.7), proving property 2.
To prove property 3, let Gi and G2 be two distinct g-inverses of A'A. From
(3.1.7),

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
78
AGiA'A = A
and
AG2A'A = A,
i.e., A'AG
^
A' — A'AG2A'. By property1of Result 2.3.8, this implies AG'jA' =
AG'2A'; transposing both sides, AGiA' = AG2A', i.e., AGA' is invariant to
the choice of a g-inverse. This is an important result. It was mentioned earlier
that every symmetric matrix must have a symmetric g-inverse. Let Gi denote
a symmetric g-inverse and let G denote any other g-inverse of A'A. Since
(AGiA')' = AGiA' = AGiA'
(since Gi = G)), AGiA' is symmetric when Gi is symmetric.
Consider
(AGA')' = AG'A'. By property 1 and property 3, we have
AG'A' = AGA',
so that AGA' is symmetric for any G, proving property 4.
We will see later that Result 3.1.8, especially property 3, is very important for
the discussion of inference for the less than full-rank linear model.
Suppose
Example 3.1.4.
(1
1
0
0\
1
1
0
0
110
0
10 10
1
0
1
0
1
0
0
1
x =
It is easy to verify that
/6
3
2 1\
3
3
0
0
2
0
2
0
1
0
0
1
A = X'X =
with rank 3. Two distinct g-inverses of X'X are
/0
0
0
0\
0 1/3
0
0
1
0
0
1/2
0
\0
0
0
1
a n d
/0
1/3
0
0\
0
0
0
0
0 -1/3
1/2
0
\0 -1/3
0
1
which correspond to the full rank submatrices

3.1. GENERALIZED INVERSES
79
/3
0
0
An =
0
2
0
VO
0
1
and
3
0
0
2
2
0
1
0
1
An =
respectively. It is easy to verify that
/1/3 1/3 1/3
0
0
0\
1/3 1/3 1/3
0
0
0
1/3 1/3 1/3
0
0
0
0
0
0
1/2
1/2
0
0
0
0
1/2
1/2
0
0
0
0
0
0
1/
XGiX' = XG2X' =
Let A
• , A*; and xi, • • • , Xk denote the eigenvalues and
Example 3.1.5.
corresponding eigenvectors of a symmetric matrix B = A'A, where A is an
n x k matrix. Suppose the first r eigenvalues are nonzero, while the last (A; —
r) are zero. We first write down the spectral decomposition of B. Let A =
diag(Ai, - * - , Ar, 0, • • • ,0) and let X = (xi, - * - ,xr,xr+i, - - - , x^).
By Result
i >
*
*
2.3.4,
B = XAX' = £AiXix'.
i— 1
Let C = ELI^
X'/A*, and let A
= diag(l/Ai, • • • ,1/Ar,0, • • • , 0). We can
verify that AA
“ A = A, so that A- is a g-inverse of A, and that C = XA~X'.
Since XX' = X'X = I,
BCB = (XAX')(XA"X')(XAX') = XAA~AX' = XAX' = B,
so that C is a g-inverse of B. This result is useful in the discussion of estimable
functions in the less than full rank linear model.
Let B be an m x n matrix and let A and C respectively
Example 3.1.6.
denote nonsingular m x m and n x n matrices. We will show that G is a g-
inverse of ABC if and only if G = C_ 1HA
“ 1, H being any g-inverse of B . By
Definition 3.1.1, G is a g-inverse of ABC if and only if ABCGABC = ABC,
i.e., if and only if BCGAB = B, i.e., if and only if G = C
” 1HA
“ 1 . Two
special cases of this condition are given in Exercise 3.13.
Result 3.1.9. An m x p matrix B is in the column space of an m x n matrix
A, i.e., C(B) C C(A) if and only if AA
“ B = B, or equivalently, if and only
if (I — AA
“ )B = 0. Similarly, any q x n matrix C is in the row space of A,

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
8 0
i.e., 11(C ) C R( A) if and only if C = CA A, or equivalently, if and only if
C(I- A’A) = 0.
Proof. The proof of the sufficiency is obvious. To prove necessity, C(B) C C(A)
implies that there exists a matrix M such that B = AM. Then,
AA“ B = AA"AM = AM = B.
The proof of the other result is similar.
An
A12
A21
A22
Result 3.1.10. G-inverse of a partitioned matrix.
be an m x n partitioned matrix, Aij having dimension ra* x rij, i, j = 1, 2,
and with r(A) < m.
Let E —
A22 — A2iAf1Ai2, C(Ai2) C C(An), and
R( A2\ ) C 1Z(An). A g-inverse of A is
Let A =
An + AnA12E A2iAu
— AnAi2E
— E'A2iAj
"
1
— Aj“1Ai2
In2
E“
) E (-AaiAn ,
Au
0
0
0
Im2) •
Proof. By Result 3.1.9, A12 = AnAuAi2 and A21 = A2iA11An. Hence,
An
A12
A21
A22
An
A2iAnAu
AnAnAi2
A22
The result follows by using Exercise 3.14, setting A — An , D — A22, B = AnAi2,
and C = A2IA^
, so that CAnB = A21Aj
“
xAnA^Ai2 = A2iA^
1Ai2-
Example 3.1.7.
Let
4
4
1
1
0
0
0
0
An
0
A =
A22 )
0
0
0
3
5
Let
1/4
0
0 I ,
and
An-
A22 —
0
Using Result 3.1.10,
( l / A
0
0 \
0
0
0
0
0
0
0
0 1/5/
An
0
0
A22
A“ =

3.1. GENERALIZED INVERSES
81
is a g-inverse of A
.
Result 3.1.11. Let D = C(A), where A is an m x n matrix of rank r, and
let (A'A)
” denote any g-inverse of the n x n symmetric matrix A'A. Then,
P = A(A'A)-A' represents the matrix of the orthogonal projection of the
n-dimensional vector y onto D .
Proof. Let c = A'y.
Then (A'A) c is a solution of A'Ab = c. Since by
Definition 3.1.1,
A'A[(A'A)-c] = A'A(A'A)
"A'Ab = A'Ab ,
we see that (A'A)-c must be a solution of A'Ab = A'y. Let t = A(A'A)-c.
Then, we can write y = t + (y — t) where
A'(y - t) = A'y - A'A(A'A)-A'Ab = Ay - A'Ab = 0.
Hence, we have an orthogonal decomposition of the vector y, with t
C(A), and
y — t
C(A)-l , the orthogonal complement of C(A). The matrix P = A(A'A)-A'
is therefore the matrix of this unique orthogonal projection (see Definition 2.6.3),
and the proof is complete.
Note that if the columns of A are LIN, then P = A(A'A) XA', since the
unique g-inverse of A'A is (A'A)-1 (see Result 3.1.1).
Result 3.1.12. The matrix of the orthogonal projection of y onto M( A) is
In - P = I„ — A(A'A)-A'.
Proof.
Let us denote D = -Af(A), so that by Definition 2.6.2, fi-1* = C(A').
By Definition 2.6.3, P^
J. = A(A'A)
“ A', from which it follows that PQ =
In “ Pfij- = In ~ A(A'A)-A'. If the columns of A' are LIN, then the unique
g-inverse of A'A is (A'A)-1 and hence the matrix of the orthogonal projection
in this case is In — A(A'A)-1A'.
Example 3.1.8.
For an m x n matrix A, and a n m x p matrix B, we show
^
is a g-inverse of (A
B) if and only if, by Definition 3.1.1,
that
B-
<A
B) (B:) (A
b)
(A
B)
(A
B) f
A-A
A B
B A
B B
=
(A A A + B B A,
A A B + B B B)
i.e., if and only if AA"B = 0, and BB-A = 0.
Definition 3.1.3. Moore-Penrose inverse.
A+ of an m x n matrix A satisfies the following conditions:
The Moore-Penrose inverse

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
82
1. AA+A = A, i.e., A+ is a g-inverse of A,
i.e., A is a g-inverse of A+
+
.
2. A+AA+ = A
3. (AA+)' = AA+, and
4. (A+A)' = A+A.
If a g-inverse G of A satisfies only conditions 1 and 3, it is called a reflexive
g-inverse of A. We will not discuss these alternative inverses, except to mention
that A+ is unique. The reader may refer to Rao and Mitra (1971) or to Pringle
and Rayner (1971) for details.
3.2
Solutions to linear systems
Least squares estimation of the parameters in a linear statistical model starts
with the mathematical problem of solving a system of linear equations involving
those parameters and the data. These equations are called normal equations.
In this chapter, we discuss systems of linear equations and some properties
of solutions to such systems. A linear system of m equations in n unknown
variables x = (xi, • • • , xn ) is written as
bi
L\lX\-\-
ai2#2+
PO'lnXn
^21^1+
U22^2+
* * •
+&2nZrn
t>2
bm
Gm2£2+
* * *
or in matrix form as
(3.2.1)
Ax = b
where A = {a^} is an m x n coefficient matrix, b = (&i, • • • , 6m)' is called the
right side and
*s and b^
s are fixed scalars. Solving (3.2.1) is the process of
finding a solution, provided it exists, i.e., a value of x which satisfies (3.2.1).
The matrix BA = ( A
b ) is called the augmented matrix. In multivariate
problems, we will find it necessary to solve each of L linear systems
(3.2.2)
Ax; = b^
/ =!, - • • , L.
Let X be an n x L matrix with columns X*, l = 1, • • * , L, and let B be an m x L
matrix with columns bi, l = 1, *
* , L; then (3.2.2) can be written collectively
as a linear system in X as
(3.2.3)
AX = B.
A solution is any n x L matrix X that satisfies (3.2.3). Note that (3.2.1) is a
special case of (3.2.3) when L = 1. We present definitions and results for the

3.2. SOLUTIONS TO LINEAR SYSTEMS
83
system (3.2.3). They will be directly valid for (3.2.1) with L = 1. The linear
system AX = B is said to be homogeneous if B = 0, i.e., the system is
AX = 0,
(3.2.4)
and is said to be nonhomogeneous if B ^
0. The collection of solutions to
(3.2.3) is the set of all n x L matrices X that satisfy AX = B and is called the
solution set of the system.
Definition 3.2.1. Consistent linear system.
A linear system AX = B
is said to be consistent if it has at least one solution; otherwise, if no solution
exists, the system is inconsistent.
Example 3.2.1.
When L = 1, the system
i S) (2)- 1
5
10
which may be written out as
X\
+
3x2
2x\
4-
6x2
has a solution given by X\ = 2, x2 = 1, and is consistent. Note that row 2 in
the coefficient matrix is twice row 1 and the same relationship exists between
the corresponding elements of the right side. This linear system is said to be
compatible. It is easy to see that the system
5
10
1
3 M
2
6 ) U2
5
19
is not consistent. Note that in this example, |A| = 0.
A linear system AX = B is said to be compatible if
Definition 3.2.2.
every linear relationship that exists among the rows of A also exists among the
corresponding rows of B.
We state two results without proof.
A linear system AX = B is consistent if and only if it is
Result 3.2.1.
compatible.
For consistency of the system AX = B, we do not require existence of linear
relationships among the rows of A. For instance, when the rows of A are LIN, so
that A-1 exists, (3.2.3) is consistent. However, Result 3.2.1 shows that should
there exist linear relationships among the rows of A, the same relationships
should exist among the rows of B. Solutions to linear equations exist if and
only if the equations are consistent; hereafter, we assume consistency.
Result 3.2.2. The linear system AX = B is consistent

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
84
1. if and only if C(A, B) = C(A); or
2. if and only if r(A, B) = r(A); or
3. if A has full row rank m.
A homogeneous linear system AX = 0 is always consistent; its (nonempty)
solution set is a linear space called the solution space. For L —
1 and for an
n x n matrix A , the solution space corresponding to Ax = 0 is
M{ A) = {x : (I - A)x = x} C C(I - A).
(3.2.5)
Also, N ( I — A) = {x : Ax = x} C C(A). The solution set of a nonhomogeneous
system (3.2.1) is not a linear space. This is seen directly by noting that the null
matrix 0 is not a solution to the system.
Result 3.2.3.
GB is a solution to the system if and only if the n x m matrix G is a g-inverse
of A.
For every m x L matrix B for which AX = B is consistent
Proof.
Let G be a g-inverse of A and suppose X° is any solution to AX = B.
Then, by (3.1.1)
A(GB) = (AG)B = AGAX0 = AX° = B,
so GB is a solution.
Conversely, suppose that GB is a solution to AX =
B. Suppose that B = (a j, 0,
, 0), where aj denotes the jth column of A.
Then, one solution to AX = B is the matrix (lj, 0, • • • , 0), where 1j is the
n-dimensional unit vector. Hence,
AG(aj,0, • • • , 0) = (aj, 0, • • • , 0)
or AGaj = aJ } j = 1, • • • , n, i.e. , AGA = A. By (3.1.1), G is a g-inverse of A.
Let A be any n x n nonsingular matrix, and G be any n x n matrix. GB is
a solution to AX = B for every n x L matrix B if and only if G = A-1.
1
3
for
A g-inverse of the coefficient matrix A =
Example 3.2.2.
2
6
(l/3 S) “ dA_b =
the consistent linear system in Example 3.2.1 is A
=
(0
5/3)' is a solution.
For convenience of notation, we use Z° to denote a solution to the homoge-
neous system AZ = 0 (in Z) and X° todenote asolution to the nonhomogeneous
system AX = B (in X).

3.2. SOLUTIONS TO LINEAR SYSTEMS
85
An n x L matrix Z° is a solution to the homogeneous system
Result 3.2.4.
AZ = 0 (in Z) if and only if, for some matrix Y,
Z° = (I- A"A)Y.
When L = 1, we see that for some column vector y
z° = (I — A-A)y
is a solution to Az = 0 (in z), Af { A) = C(I — A-A) and
dim[A/
"(A)] — n — r(A).
Proof. Let Z° = (I — A
~A)Y for some n x L matrix Y; using (3.1.1),
AZ° = (A- AA’A)Y = (A- A)Y = 0,
so that Z° solves AZ = 0. Conversely, if Z° is a solution to AZ = 0, then
Z° = Z° - A“ (AZ°) = (I- A“ A)Z°,
i.e., there exists a matrix Y = Z° satisfying (3.2.6). As a special case, when
L = 1, we have (3.2.7), from which it directly follows that J\f( A) =C(I — A-A).
From property 1 and property 2 of Result 1.2.11, we have
dim[A/
*(A)] = dim[C(I — A“ A)] = r(I — A“ A) = n — r(A) .
(3.2.6)
(3.2.7)
(3.2.8)
If r( A) = n, i.e., if A has full column rank, then dim[A7(A)] == 0, and the
system Az = 0 has a unique solution, viz., 0. If r(A) < n, then the homoge-
neous system Az = 0 has an infinite number of solutions.
Result 3.2.5.
Let AZ = 0 denote a system of homogeneous linear equations.
The dimension of its solution space is L{n — r(A)}.
Clearly, a matrix Z° is a solution to AZ = 0 if and only if each
Proof.
column of Z° is a solution to AZ = 0. We consider two cases. First, suppose
that r(A) = n. From the discussion below Result 3.2.4, the only solution to
AZ = 0 is the nxL null matrix. Next, suppose that r(A) <n, let s = n — r(A),
and let Zi , Z2, • • • , Zs be any s LIN solutions to AZ = 0.
Clearly, the Ls
matrices (Zi,0, • • • ,0), • • •(Zs,0, • • • , ()),
• • , (0,
• • ,0, Z^
, - * - , (0, • • • , 0, Zs),
each of which has dimension nxL, form a basis of the solution space of AZ = 0.
The dimension of this solution space is Ls — L{n — r(A)}.
Result 3.2.6. Let X* be any particular solution to a consistent linear system
AX = B. A matrix X° is a solution to this system if and only if
X° = X* + Z°,
for some Z° which is a solution to the homogeneous system AZ = 0.
Proof.
is a solution to (3.2.3). To prove the converse, if X° solves the system AX = B
defining Z° = X° - X*, we see that X° = X* + Z°. Since
(3.2.9)
If X° = X*+ Z°, then AX° = AX*+ AZ° = B + 0 = B, so that X°

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
86
AZ° = AX°- AX* = B- B = 0
we see that Z° solves AZ = 0.
Hence, all matrices in the solution set AX = B can be generated from
X = X* + z
where X* is any particular solution of (3.2.3) and Z ranges over all matrices in
the solution space of AZ = 0. When L = 1, we replace matrices X, X°, X*, Z,
and Z° respectively by vectors x, x°, x*, z, and z° .
A matrix X° is a solution to the system AX = B if and only
Result 3.2.7,
if, for some matrix Y,
X° = A“ B + (I- A~A)Y.
(3.2.10)
Proof.
The proof follows directly from Result 3.2.3, Result 3.2.4 and Result
3.2.6.
A special case of Result 3.2.7 for L = 1 states that x° is a solution to Ax = b
if and only if, for some column vector y,
x° = A“ b + (I- A~ A)y.
Example 3.2.2 (continued). Let y = (2/1, 2/2);
* We see that
(3.2.11)
Vi
(3.2.12)
A“ b + (I- A
“ A)y =
5/3- 1/3y i J -
The solution set then consists of all vectors of the general form (3.2.12).
Consider the system of equations Ax = b where
( xi\
Example 3.2.3.
/1 -2
3
2\
A = I 1
0
1
— 3 1 ,
\ l
2 -3
0 /
2
X 2
and b = -4 .
x =
Xs
-4
\X4 /
This system is consistent (see Exercise 3.15). A g-inverse of A is
1/2
0
l/2\
-1
3/2 -1/2
-1/2
1
-1/2 ’
V
0
0
and a solution to the system of equations is x = A~b = (— 1, — 6, — 3,0)'.
Another solution is xi = A“ b + (I- A“ A)y, for an arbitrary 4-dimensional
vector y.
A“ =
0 /

3.2. SOLUTIONS TO LINEAR SYSTEMS
87
Result 3.2.8. If A is nonsingular, then AX = B has a unique solution given
by A-1B.
Proof. By property 3 of Result 3.2.2, the system AX = B is consistent and a
solution is given by (3.2.10).
Let AX = B denote a consistent linear system. For any n x q
Result 3.2.9.
matrix C, the value of C'X is the same for every solution to the system AX = B
if and only if 1Z(C' ) C 7£(A), i.e., if and only if every row of C' belongs to 11( A).
If 1Z(C' ) C 1Z( A) , there exists a matrix F such that C' = FA.
Proof.
Let X* and X° respectively denote any solution and a particular solution to
AX = B. Then, C'X* = FAX* = FB = FAX0 = C'X0, i.e., C'X is the same
for each solution to AX = B. To prove the converse, it follows by Result 3.2.7
that for every Y, the value of C'[A
“ B 4* (I — A
“ A)Y] remains the same. Let
Y = (y, 0, • • • , 0). It follows that C'(I — A
“ A)y = 0 for every n-dimensional
vector y, i.e., C'A~ A = C', i.e., R(C' ) C 71( A).
When L — 1, and c is an n-dimensional vector, the value of c'x is the same
for every solution to Ax = b if and only if c' e 7£(A) (see Exercise 3.18). We
will recall this result in the discussion of estimability in the less than full rank
linear model theory in Chapter 4.
We next state without proof (see Harville, 1997, p.
155) a result on ab-
sorption, which enables us to solve a linear system with an arbitrary number
of equations in an arbitrary number of unknown variables. Consider the linear
system AX = B, where A, B and X are respectively m x n, m x L and n x L
matrices partitioned as
An
AI2
A21
A22
B =
and X =
A =
where An is mi x m, A12 is mi x n2, A21 is m2 x ni , A22 is m2 x 712, Xi is
711 x L, X2 is 712 x L, Bi is mi x L, and B2 is m2 x L. We can express AX = B
as
AnXi -j- A12X2 = Bi ,
A21X1 + A22X2 = B2.
The following result implicitly requires solving the first mi equations in AX = B
for Xi in terms of X2 , substituting this solution for Xi into the last m2 equar
tions, thereby absorbing the first mi equations into the last m2 equations, solv-
ing the resulting reduced linear system, and finally back-solving for Xi . This
procedure is called absorption, and is useful, for example, in the context of solv-
ing the system of normal equations in a two-way fixed-effects ANOVA model
without interaction (see Example 4.2.6).

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
88
Consider the linear system AX = B, where
Result 3.2.10. Absorption.
A, B and X are partitioned as above. Suppose C(A12) C C(AU), C(Bi) C
C(An ), and TZ(A21) C 1Z( An ). The matrix X* =
AX = B if and only if
(i) X2 is a solution to the linear system (in X2)
is a solution to
(A22 — A2IE))X2 = B2 — A21C, and
(ii) XJ and X2 form a solution to the system (in Xi and X2)
AnXi + A12X2 = Bi ,
where C = AnBi and D = AnA12.
Exercises
3.1. Find a g-inverse of
1
2
/1
2
4
3\
and (b) A =
3 -1
2 -2 .
\ 5 -4
0 -7/
(a) A =
1
1
-1
0
AiAi
A'2A!
A'XA2
A
^
A2
3.2. Suppose an mx n matrix B is partitioned as B =
that a g-inverse of B is given by
. Show
(A;AO-
o
— (A
/
1A1) ~A'1A2) S- (-A^
A^
A
^
AO-
I)
B =
+
I
0
0
where S = A'2A2 - A
^
A^
A
^
Ai) A1A2.
3.3. Let K be an idempotent matrix, let B = KAK, and let B~ denote a
g-inverse of B. Show that KB~K is also a g-inverse of B.
3.4. Let G be a g-inverse of a matrix A'A. Show that
(a) AG'A'A = A.
(b) A'AGA'= A'.
(c) A'AG'A'= A'.
(d) AG'A'= AGA'
(e) AG'A' is symmetric.
3.5. Show that B“ A“ is a g-inverse of AB if and only if A” ABB” is idem-
potent.

89
Exercises
3.6. Let A be an n x n symmetric matrix of rank r < n, and let A = CC^
where r(C) = r. If A “ denotes a symmetric g-inverse of A, show that
(A~C)(A~C)' is also a g-inverse of A.
3.7. Let A have rank r, and let Pi and P2 be nonsingular matrices such that
Ir
(A
0 or
PiAP2 = Q =
Show that G is a g-inverse of A if and only if it can be written as G =
P2Q_PI , where
Ir
U
V w
Q- =
for arbitrary matrices U, V, and W.
3.8. [Englefield, 1966]. Let A be an n x n matrix. If r(A) = r < n, show
that any one of the following conditions is necessary and sufficient for the
existence of a commuting g-inverse of A:
(a) r(A) = r(A2),
Ai
0
0
0
(b) there exists a nonsingular matrix B such that BAB
1 =
where Ai is an r x r nonsingular matrix.
3.9. Show that a symmetric matrix A always has a commuting g-inverse and
that it is also possible to construct a symmetric commuting g-inverse of
A.
3.10. Let Gi and G2 be two g-inverses of an m x n matrix A. Let x be any
vector such that AGix = x. Show that AG2x = x.
3.11. In Example 3.1.5, let D = CB. Show that x'D = xj, i = 1, • • • ,r, while
x'D = 0, i = r +
, k.
3.12. Show that PX = X, where P = X(X'X)_X' denotes the projection
matrix onto C(X).
3.13. Let H be any g-inverse of an m x n matrix B and let A and C respectively
denote nonsingular m x m and n x n matrices. Show that
(a) G is a g-inverse of AB if and only if G = HA-1; and
(b) G is a g-inverse of BC if and only if G = C_ 1H.
3.14. Given the matrices defined in Exercise 2.4, show that a g-inverse of the
matrix
A
AB\ .is
CA
D
) “ (Ao
~
S) +
E‘(-C' u) -
A + B E
“ C
-BE
"
-E-C
E
"

CHAPTER 3. G-INVERSES AND LINEAR EQUATIONS
90
3.15. Show that the system of equations Ax = b in Example 3.2.3 is consistent.
3.16. Solve the system of linear equations Ax = b where
/ 2
— 2
0
-10
3
6 -6
1
4 \
/
2 \
1
6
and b =
A =
8
12
2 -7 -16
V i
-7
3.17. Show that c'x has a unique value for each solution to Ax = b if and only
if c'A-A = c', where A- is any g-inverse of the matrix A.
A
b
QJ where A is an m x n matrix, b is an Tri-
dimensional vector which belongs to the row space of A, and c is an
n-dimensional vector which belongs to the column space of A. Show that
the value of c'x is the same at all solutions of Ax = b and is given by
c'A” b where A- is a g-inverse of A.
3.18. Consider the matrix
c'

Chapter 4
The General Linear Model
The theory of linear statistical models underlies several important and widely
used procedures such as univariate and multivariate regression analysis, analysis
of variance, analysis of covariance, random-effects modeling, time series analysis,
spatial analysis, etc. In this chapter, we introduce the general linear model to
explain an unknown response vector as a linear function of known predictors
and an unknown vector of model parameters. We discuss general results for
the situation where the matrix of predictors need not have full rank. We define
the notion of estimability of linear functions of the vector of model parameters,
and derive the least squares estimates of such functions. The Gauss-Markov
theorem guarantees an important optimality property for these estimates. The
results in this chapter are distribution free, so that we do not need to specify a
probability distribution for the model errors.
4.1
Model definition and examples
Given data (T*, Xu, X {2, • • • , Xik ), i — 1, • • • , N , the general linear model has
the form
(4.1.1)
y = X0+ e
where y = (Ti, • • • ,YN )' is an TV-dimensional vector of observed responses, /3 =
(An /3i, - - • , /3k )' is a ( k + l)-dimensional vector of unknown parameters, X is an
N x ( k T 1) matrix of rank r of known predictors, and e —
( £\, • • • ,e;v)' is an
iV-dimensional random vector of unobserved errors. The matrix X is written as
/1
Xu
• • •
Xlk\
1
X 21
• • •
X 2k
x =
\l
XN 1
• • •
Xivk /
For convenience of notation, we let p = k -f 1. Unless specified otherwise,
we assume that the first column of X is the vector 1# = (1, • • • ,1)', so that
91

CHAPTER 4. THE GENERAL LINEAR MODEL
92
the first coefficient 0o is the intercept. We can write X = (l;v, X), where X
is an N x k matrix. There are k coefficients (3\, • • • , /?* which correspond to
the explanatory variables X\,• • • , Xfc. Note that we may also write the model
(4.1.1) as
(4.1.2)
Yi =0o + PiXn 4-
1-0kXik + 6i, i — 1, • • • , N,
or as
Yi = x •/?+£*, i = 1, • • • , iV,
where x* = (1,
• • • , Xik )' denotes a p x 1 vector corresponding to the ex-
planatory variables on the ith subject. Suppose
E{e ) = 0,
and
Cov(e ) =
i.e., the errors are uncorrelated, each with zero mean and the same variance a2.
In Chapter 7, where we discuss detailed inference for the linear model, we will
assume some probability distribution for the error vector, usually the normal
distribution. Here, we see that it follows from (4.1.4) and the properties of the
expectation and covariance operators that
(4.1.3)
(4.1.4)
E( X0 4 e ) = X/3 + E(e) = X/3,
and
Cov(y)
=
Cov(X(3+e )= Cov(e ) = a2!^.
E( y )
(4.1.5)
Multiple regression models, fixed-effects and random-effects analysis of vari-
ance (ANOVA) models, and analysis of covariance (ANACOVA) models that
are frequently encountered in applied statistics all fall under this umbrella. In
regression models, the Xj’s may be continuous or categorical observed vari-
ables, whereas in the family of ANOVA models, the explanatory variables gen-
erally correspond to levels of different factors of interest in designed experi-
ments. The symmetric matrix X'X, and the symmetric, idempotent matrices
P — X(X'X)~X' (the projection matrix, or hat matrix, or prediction matrix),
and I — P play a central role in the development of statistical theory for linear
models. If r(X'X) = p, we have the full rank model, while r(X'X) = r < p
corresponds to the non-full rank model, or the design model. The multiple re-
gression model is a “full rank linear model” , whereas the ANOVA models are
examples of “less than full rank linear models” . In this chapter, we describe the
form of the general linear model, derive the least squares estimates of relevant
parameters and prove an important theorem called the Gauss-Markov Theorem.
This theorem states a useful optimality result for the least squares estimator
of linear parametric functions of the parameter vector /3. We begin with some
examples of the linear model.
Example 4.1.1. Simple linear regression.
continuous-valued response (dependent variable) Y , and a single continuous-
valued predictor (independent variable) X, and we wish to explain the variability
in Y due to X . The simple regression model has the form (4.1.2) with k = 1:
Yi = (3o 4-
Xi 4- Si , i = 1, • • • , AT.
Suppose there is a single
(4.1.6)

4.1. MODEL DEFINITION AND EXAMPLES
93
We have postulated a straight line relationship between X and Y, with intercept
Po and slope Pi. Both po and Pi are unknown model parameters, which must
be estimated together with the error variance <r2, based on data pairs (X*, Y*),
i = 1, • • • , N. The usual interpretation for Po is that it is the value assumed byY
when X = 0. When Po = 0, (4.1.6) reduces to a simple regression model without
intercept, represented by a straight line through the origin. The coefficient
Pi is the change in Y for a unit increase in X .
When Pi > 0, there is a
positive association between X and Y, so that Y is expected to increase as X
increases. When pi < 0, there is an inverse or negative relationship between
the two variables, while if Pi — 0, Y is unaffected by changes in X . When pi is
very close to zero, we interpret it as a weak, or virtually nonexistent regression
relationship. A scatterplot of Y versus X is one of the first exploratory data
analysis (EDA) steps that enables a quick assessment of the validity of a linear
model fit to the data.
Suppose we transform both the response and predictor variables as follows:
Y* = ci + c2Y, and X * — di + d2X. We will investigate the effect of this data
transformation on the regression coefficients. Suppose the regression model on
the transformed variables is
Y* = PZ + PIX * .
After some algebra, we can see that
Pi = c2 pi/ d2, and 0Q = ci - {c2di0i / d2 } + c2 p0.
The slope is unaffected by a shift in the location but is affected by a scale change,
while the intercept is affected by both a location shift and a scale change.
Example 4.1.2. Multiple linear regression.
We relate a single continuous-
valued response Y to multiple predictors X \, • • • ,X*, k > 1, using a linear
model of the form (4.1.2), which represents a hyperplane in W . The response
surface has a linear functional form whose parameters are the coefficients Pj ,
j = l, - * * ,k, which are called partial regression coefficients. We interpret Pj
as the amount by which Y changes when Xj is increased by one unit, while all
other predictor variables are held at fixed values. The coefficient Po corresponds
to the constant term. If a particular pj is zero, the interpretation is that the
corresponding predictor Xj is unrelated to Y (in a linear model), and may be
dropped from the regression. A multivariate scatterplot is recommended as a
preliminary step which enables us to assess the usefulness of a linear model
fit to such data. Let Y — YliLi
and Xj =
Xij / N , j = 1, * • • , &.
By expressing each observation on the response and the predictors in terms of
deviations from their respective means, we may write (4.1.2) in deviations form
as
Vi = PlXn + • • • + PkXik 4- £u
where yi — Yi — Y, and Xij = Xij — Xj, j = 1, • • • , k.
The centered and scaled form of the multiple regression model is

CHAPTER 4. THE GENERAL LINEAR MODEL
94
Y* = ftXn + 0*2*i2 + • • + P*kX?k +*
where /3* =0j( Sjj / Syy )1/ 2, j = 1, • • • , k and
Xtj = ( Xij - Xj )/ yS~, and Y * = (Yt -Y )/ y/S^
,
for i = 1, •
• , TV, j = 1, • • • , k. Let X* denote the regression matrix of centered
and scaled variables, i.e.,
x*lk\
V*
V*
yV 22
* *
*
x* 2k
( X1*1
X{2
X%i
X* =
V^Jvi
X i
so that X*'X* denotes the k x k correlation matrix of the explanatory variables,
excluding the intercept column, which becomes zero by the centering.
Standardized coefficients describe the relative importance of the explanatory
variables in the model. They are the partial regression coefficients in the model
where each variable is normalized by subtracting its sample mean and dividing
by its sample standard deviation. A comparison of the different forms yields
the following relationship between the partial regression coefficients and the
corresponding standardized coefficients:
P* =0jSXj / sY , j = l, • • • ,£.
We see that the standardized coefficients have the same sign as the /?/s. A
standardized coefficient of 0.6 means that an increase of one standard deviation
in the independent variable is expected to cause a change of 0.6 standard de-
viations in the response variable. An elasticity measures the percent change in
the response variable Y corresponding to an increase of 1% in the explanatory
variable. We end this example with the notion of column-equilibrating the ma-
trix X. This consists of dividing each column of X by the sum of squares of its
elements. The sum of squares of the elements in each column of the resulting
matrix X#, say, should be 1.
• • •
XIN k /
N 2
Example 4.1.3. One-way fixed-effects ANOVA model,
model
Consider the
(4.1.7)
Yij
n -j- Ti H- Sij ,
j
1, * * * ,
i — 1, * • • , u,
which can be written in the form (4.1.1) with
(Yiu - - -
i^
i, * * * ,Ya%nay ,
(^11 > ' * *
1 £l,ni 1 * *
*
5^al 5 * * *
5 £a,na )
5
y
£
P
a +
[ 1N £ In* 1
X
i=1

4.1. MODEL DEFINITION AND EXAMPLES
95
where N = YLi=ini > E(e ) = 0’ Cov(e ) =
CJ2IN , and the direct sum of vec-
tors was introduced in Definition 2.8.3. In general, Yij represents the observed
response from the jth subject in the ith group, where j = l, * * * ,n», and
i = 1, • • • ,a. The design or incidence matrix X consists of l’s and 0’s and
p = a + 1. The ith column of X has l’s in its X^
=i(nfc + l)th row to its
rifcth row, and zeroes elsewhere. It is easy to verify that r(X) = r = a,
since the last a columns of the design matrix add up to the first column, im-
posing one dependence. For example, when a = 3, n\ = 3, 712 = 2, and
= 1,
we write
E;k— \
V + Ti + £ij ,
j — 1, 2, 3
fi A 72 A £2h,
h = 1, 2
[i A T3 -f- 631,
so that E(Yij ) = (1 A TI , j = 1, 2, 3, £(> 2/1) = M + T2, h — 1, 2, and £(>31) =
[i
73. In this case, the response vector is y = (Yi, Y2 > • * • ,l^
)', the parame-
ter vector is /3 = (^
, ri , r2, T3)', the error vector is e = (£1, £2, • * *
5 £6/ , and the
design matrix is
Yrj
Y2h
Y31
(1
1
0
0\
1
1
0
0
110
0
1
0
1
0
1
0
1
0
1
0
0
1
x =
\
with r(X) = 3. When rii = n for i = 1, • • • , a, we say the model is balanced,
otherwise it is an unbalanced model. Using Kronecker product notation (see
Definition 2.8.1), it is easy to verify that the design matrix in a balanced model
can be written as
X = [(la, Ia) ® ln].
Example 4.1.4. One-way random-effects ANOVA model.
Consider the
balanced version of the model in (4.1.7):
Yij —
(1 A Ti A £{ j ,
j — 1, • * * , 71, i — 1,
where the errors are independently distributed with E(£ij ) = 0, and Var(eij ) =
a2 . Unlike the fixed-effects model, the Ti s are no longer fixed unknown con-
stants, but are themselves assumed to be independent random variables, with
E(ji ) = 0, and Var{Ti ) = a2 .
We also assume that the T» S and e^
-’s are
independently distributed. In this case, it is straightforward to verify that
°T +
i = i' , j = j' ,
i = i'J # j'.
i^
i' .
crl
Cov(Yij ,Yi’ tj' ) =
(4.1.8)
T »
0

CHAPTER 4. THE GENERAL LINEAR MODEL
96
Concisely, we can write
(10 <g> ln)/i + (Ia
<g>1„)r + e, and
Cou(y)
=
la ®
+ (Tgln).
y
(4.1.9)
The quantity
(4.1.10)
P
Cl +
is called the intra-class correlation. We will discuss inference for random-effects
models in Chapter 10. We will also study mixed-effects models which are useful,
for instance, in designed experiments where some factors are fixed, while others
are random.
In the general linear model (4.1.1), the parameter vector (3 and the error
variance a2 are usually unknown. The least squares approach, which is described
in the next section, is a simple and elegant procedure that enables estimation
of functions of the parameters in a linear model.
4.2
The least squares approach
Given data on the response variable and predictors, either from an observational
study or from a designed experiment, the objective is inference on the model
parameters, or functions of the model parameters, as well as predictions for the
response variable based on the general linear model (4.1.1). The method of least
squares, which was introduced in the early 19th century, enables such inference
using minimal assumptions. In particular, we need not specify any parametric
form for the probability distribution of the errors £*. In the full rank linear
model, i.e., when r(X) = p, the least squares approach enables us to construct
the best linear unbiased estimator of the parameter vector /?, “ best” in the sense
of having minimum variance in the class of all linear unbiased estimators. When
r(X) = r < p, we will obtain least squares estimates of certain linear functions
of /3, although, as we shall see, there does not exist a unique estimator for /3
itself. In order to proceed with inference beyond point estimation and prediction
for the linear model, i.e., in order to construct confidence interval estimates or
to do hypothesis tests, it is usual to assume some parametric form for the
error distribution. The simplest and most popular distributional assumption
is the assumption of normality of the linear model errors. In Chapter 5, we
introduce suitable families of multivariate probability distributions, including
the multivariate normal distribution, and return to classical inference for linear
models in Chapter 7. We now describe the least squares principle.
Geometrically, the response y = (Yi, • • • , Y}v) represents a point (or a vec-
tor from the origin 0) in the N-dimensional Euclidean space HN. Let Xj —
{ X i j r- I X N J Y , j =
so that X = (1N, X) = (l^, x i , - - - , xfc), and
let C(X) denote the vector subspace of TZN defined by these columns of X (see

4.2. THE LEAST SQUARES APPROACH
97
Definition 1.2.29). The vector X/3 = POIN + PA + • • • + Pk*k is in C(X).
When r(X) = p, the p LIN columns of X span the p-dimensional estimation
space
= C(X). In Figure 4.2.1 (a), the points 0, y, and X/3 form a triangle in
7ZN , whose sides are the vectors y, X/3, and e = y- X/3. The method of least
squares consists of minimizing e'e =|| y — X/3|| with respect to /3, or equiva-
lently minimizing || y — 9 ||
where 9 = X/3
C(X). As 9 varies in C(X), the
square of the length of the vector y — 9 will be minimum when 9 =9, for 9 in
C(X). Then, X'(y - 9) = 0 or X'0 = X'y- The vector 9 denotes the unique
orthogonal projection of y onto C(X), so X/3 (which is equal to 9 ) is unique. In
other words, when X has full rank p, the least squares estimate of /3 is the vec-
tor /3 which uniquely minimizes the quadratic form (y — X/3) (y - X/3), which
denotes the squared length of the vector joining y and X/3 in Figure 4.2.1 (a).
From Figure 4.2.1 (b), we see that this quadratic form is minimum when X/3
joins the origin 0 to the foot of the perpendicular (denoted by y) from y onto
the estimation space Q. The fitted vector y = X/3 is orthogonal to
y — X/3.
The residual vector e lies in the subspace fl1- = Cx(X) of HN , called the error
space. Any vector in the estimation space is orthogonal to any vector in the
error space, i.e., the estimation space is orthogonal to the estimation space. The
dimensions of the estimation and error spaces are respectively r and N — r.
y
y
/
X
Estimation space
(b)
(a)
Figure 4.2.1.
Geometry of least squares.
Starting from the general linear model (4.1.1), consider the function of /3
N
S(0) = e'e =£(Yi - x'/3)2
(y- X/J)'(y - X/J)
1=1
y'y— 2/3'X'y+/3'X,X/3.
(4.2.1)
The least squares solution of /3 is chosen to minimize (4.2.1). Differentiating
(4.2.1) with respect to the vector /3 (see Result 2.7.8) yields the set of p normal

CHAPTER 4. THE GENERAL LINEAR MODEL
98
equations
X' X0°= X' y,
(4.2.2)
which admits a solution if r(X'X, X'y) = r(X'X) (see Result 3.2.2). We ask the
reader to verify this (Exercise 4.1). The set of normal equations has a unique
solution if and only if the matrix X'X has full rank p, i.e., (X'X)-
Otherwise, when r(X'X) = r < p, there are infinitely many solutions to (4.2.2),
one of which is
exists.
pP = GX'y,
(4.2.3)
G = (X'X)~ being a generalized inverse of the symmetric p x p matrix X'X.
We saw in Result 3.1.1 that G is not unique. What a solution 0° “estimates”
depends on which g-inverse of X'X we use! It is clear that 0° cannot be regarded
as an estimator of /?, but merely as one possible solution to the set of normal
equations (4.2.2). On the other hand, when r(X'X) = p, the solution to (4.2.2)
is the unique ordinary least squares (OLS) estimator of 0 denoted by
0 = (X'X^
X'y-
The solution 0° indeed minimizes the sum of squares function S(0 ) since
(4.2.4)
S(0) =
(y — X0° + X0° - X0 )' ( y-X0°+X0°— X0)
=
(y- X0°Y ( y-X0° ) + (y- X0° ) fX(0° -0 )
+(0°-0 )' X' ( y - X0° )+(0° -0)'X'X(0° -0 )
=
S(0° ) + (0° -0)' X' X (0° -0),
(4.2.5)
since (0o— 0)' X' ( y - X0° ) = (y- X0° yX {0° - 0)- 0 by (4.2.2). The last
term on the right side of (4.2.5) is non-negative, so that S(0 )-S(0° ) >0 for all
0
7?.p. The minimum value S(0° ) is denoted by SSEy and provides the basis
for an estimate of the error variance <r2.
Letting y = X/3°, the least squares estimate of the error variance is
^^
(y - y^
y - y)
a2 =
(4.2.6)
for which a computationally simple form is obtained after some algebraic sim-
plification as
1
a2 =
-(y'y- /?0'X'y).
r
(4.2.7)
N -
Example 4.2.1.
lation with mean 9 and variance cr2. In the framework of (4.1.1),
Yi ~ 9 + Si,
Let Yi,... , Y}v be a random sample from a normal popu-
AT

4.2. THE LEAST SQUARES APPROACH
99
where £* are independently distributed with E{ei ) = 0, and Var(ei ) = a2 . In
matrix notation, lety = (Yi, - - * , Y}v)', £ = (si, - - *
X = (l, - - - ,1)' = ljv,
and /3 = 9. Since X'X = iV, and X'y = YliLi
the least squares estimate of
the scalar parameter 6 and its variance are
N
0 = Zyi / N = Y , and Var(9 ) =
N )' 1 =
i=l
The vector of fitted values y is uniquely
Definition 4.2.1. Fitted vector.
defined as a function of the solution vector /3°:
y = X/3° — XGX'y.
For the full rank model, the fitted vector has the form X(X'X)“ 1X'y.
(4.2.8)
Definition 4.2.2. Residual vector.
e is also uniquely defined as a function of the solution vector /3° :
The vector of least squares residuals
e = y - y = ( IN ~ XGX')y.
(4.2.9)
For the full rank model, the residual vector has the form [I/v — X(XxX) 1X']y.
By property 3 of Result 3.1.8, XGX' is invariant to the choice of G, so that
the fitted vector y and the residual vector e are unique. In other words, each of
the infinite solution vectors to the normal equations yields the same predictions
and the same residual summaries. We denote by P the matrix XGX' in the
situation when r(X) < p, and the matrix X(X'X)_1X' in the full rank case.
P = XGX; is the linear transformation matrix representing the orthogonal
projection from the iV-dimensional space 1ZN onto the estimation space C(X),
while I — P = (I-XGX') represents the orthogonal projection of 1ZN onto the
error space (^
(X). The iV-dimensional vector y can be uniquely decomposed
as
y = Py + (I - P)y,
(4.2.10)
where Py G C(X) and (I — P)y G ^^(X) (see section 2.6). Result 4.2.1 gives
some properties of the matrices P, I- P, and H = GX'X, while Result 4.2.2
follows as an immediate consequence.
Suppose
Result 4.2.1. Properties of the matrices P, I — P and H.
r(X) = r. Then,
1. P and I — P are symmetric and idempotent matrices.
2. r(P) = r, and r(I — P) = N — r.
3. P(I- P) = 0.

CHAPTER 4. THE GENERAL LINEAR MODEL
100
4. H is idempotent, with r(H) = r. Farther, X'XH = X'X.
Proof. Using Definitions 1.2.15 and 2.3.1, it is straightforward to verify that
P'= p, p2 = P, (I- P)' = I- P, and (I- P)2 = I- P, proving property
1. Together with property 3 of Result 2.3.9, this gives the results in property
2. An algebraic verification of property 3 again follows from the symmetry and
idempotency of P:
P(I- P) = P- P2 = P- P = 0.
Geometrically, we see that P(I - P) must be 0 by Definition 1.2.8, since P G
C(X) and I — P G Cx(X), and these are orthogonal subspaces of 1ZN. Idempo-
tency of H is a direct consequence of the fact that X'XGX' = X' (see Exercise
3.4), so that H2 = GX'XGX'X = GX'X = H. Hence, r(H) = r(X'X) =
r(X) = r. Now, X'XH = X'XGX'X = X'X, which also follows immediately.
Note that the matrix H need not be symmetric, since G is not necessarily
symmetric.
Corollary 4.2.1. When r(X) = p, P and I— P are symmetric and idempotent
matrices, such that P(I — P) = 0, with r(P) = p, and r(I — P) = N — p.
Proof.
Symmetry and idempotency have been verified in Result 4.2.1. From
property 3 of Result 2.3.9,
r(P) = tr(P) = irfXfX'X^
X'] = tr[X'X(X'X)-1] = tr(Ip) = p,
(also see property 4 in Result 1.2.8). That r(I - P) = N — p can be shown in
the same way.
Result 4.2.2.
For the general linear model (4.1.1),
?' ( y - y ) = 'tYi(Yi -Yi )= o.
i-1
Clearly, y G C(X), while e = (y — y)
C
_L(X), so that their
Proof.
inner product must be zero (see Definition 1.2.8). Since P is symmetric, and
P(I- P) = 0, it follows that
=
y'(y - y) = (Py)'(y - Py)
i-1
y'P'(I — P)y = 0.
In Figure 4.2.1 (a), y, y and e are the sides of a right triangle. An application
of Pythagoras’s Theorem gives
y'y = y'y +?e.

4.2. THE LEAST SQUARES APPROACH
101
We next characterize these squared distances in terms of quadratic forms in y.
Definition 4.2.3. Sums of squares.
Let SST, SSR and SSE respectively
denote the total variation in Y , the variation explained by the fitted model, and
the unexplained (residual) variation. We define these sums of squares by
N
y'y =YXi '
i=1
y'y- y'Py = /?°'x'X/30 , and
= y'(IN - P)y = y'y- /?°'X'X/3°.
SST
SSR
(4.2.11)
SSE
SST refers to the total sum of squares, and the ANOVA decomposition repre-
sents a partition of SST as
(4.2.12)
SST = SSR + SSE,
where SSR is the model sum of squares and SSE is the error sum of squares.
The ANOVA decomposition in (4.2.12) can also be written as
y'y = y'XGX'y + y' (lN - XGX')y.
Orthogonality of the fitted vector y and the residual vector e is necessary for an
unambiguous partition of SST as the sum of SSR and SSE. The dimensions
of the linear spaces 1ZN , C(X) and C“L(X) also split as N = r + ( N - r ). In
some cases, it is useful to express the ANOVA decomposition in terms of mean
corrected sums of squares:
(4.2.13)
SSTC = SSRC 4- SSE
where
N
530S - y f = y'y- MY
2, and
2=1
/?0,X'y - NY
2.
We can also express the ANOVA decomposition in terms of the sum of squares
due to the mean
SSTC
(4.2.14)
SSRC
SSM = NY
2
(4.2.15)
as
(4.2.16)
SST = SSM + SSRC + SSE.
Note that the error sum of squares can also be written as
SSE = e>e= (y- y)'(y - y),
(4.2.17)

CHAPTER 4. THE GENERAL LINEAR MODEL
102
so that the least squares estimate of the error variance is
SSE
S2 =
(4.2.18)
N - r
'
Example 4.2.2.
regression model, y = (Yi, • • • , Yjy )' and x = ( Xi , - • • , X^ )' are vectors in TZN .
We continue with Example 4.1.1. In the simple linear
Let
S(0o , Pi ) = e'e
EW - Po - PiXi )2.
i=1
i= 1
Minimizing 5(/?o, /?i) with respect to /?o and /?i, and setting the normal equa-
tions equal to zero,
N
%rS(Po ,0i )
2Y,(Yi ~ 0o ~ PiXi ) = 0
dPo
1=1
N0o +0xY,X' = I2Yi
=>
1=1
1=1
N
^
- S( fa, fa )
2 j2*i(Yi - Po -0iXi ) = 0
d(3i
1=1
N
N
N
PoY/ Xi + 0lJ2x? ='£/ XiYi .
i=1
i=l
i= l
Solving these equations simultaneously for the parameters, we obtain
N
N
Z ( Xi - X )(Yi - Y )
Y.XiYi - NXY
1=1
i=l
Pi
N
—2 ’
EX? - NX
2
N
^
( Xi - xy
t=i
<=i
(4.2.19)
0o
Y - faX ,
and
3a =
't (Yi - 0o - PiXi )2 / (N - 2).
i=l
Consider the vector e
—
(Yi
—
fio —
We give a geometric interpretation.
PiXi , - - - ,YN - Po ~ PIXNY for arbitrary coefficients 0Q and 0\. The least
squares criterion chooses 0o and 0\ such that the squared length
N
e ll
2= ll y - A> IN - Ax ||2= £Oi -0o -0\Xi )2
i= l
is a minimum (see Figure 4.2.2). Varying (0o ,0i ) varies 0O1N +0I* through all
the vectors in the space spanned by 1# and x (point A). The choice of (0o ,0i )

4.2. THE LEAST SQUARES APPROACH
103
minimizing \\ £ \\ corresponds to the projection of the response vector y onto
this plane, such that the vector e is orthogonal to the plane (point B). The
least squares estimates 0o and 0\ arejshese minimizing values. The vector of
“fitted” ^or “ predicted” values is y = 0Q1N 4- ftx, i.e., the zth predicted value
is Yi -0o + Xi01, i = 1, • • • , N.
X
Figure 4.2.2.
Geometry of simple linear regression.
The leastsquares residual vector is e = y — 0Q1N - /?ix, i.e., the ith residual
is £i = Yi - Yi, i — 1, • • • , N. The least squares predictor y is the vector with
the smallest error, i.e., the vector corresponding to the shortest error vector, e.
This happens when y J_ e.
Example 4.2.3.
Consider the linear model
0o
Yi
1 -1
1
1
0 -2
1
1
1
£i
0i
+ \ e2
•
Y2
02
Yz
ez
We can verify that the columns of X are mutually orthogonal. The least squares
estimate of 0 = {0o,0i,02 )1 is
§(Vi + ^2 + Ya )
-5(yi - y3)
i(n - 2Y2 +Y3 )
Suppose we set 02 = 0 in the above model. The resulting model is
0 —
Yi
1 -1
1
0
1
1
£i
Y2
+
^2
,
Yz
^3

CHAPTER 4. THE GENERAL LINEAR MODEL
104
and the least squares estimate of 0o and 0\ are 0Q =|(Yi + Y2 + ¥3), and
0i = -\(Y\ — I3), which are unchanged by omitting the parameter 02- It is
not difficult to see that this is a consequence of the orthogonality of X, which
leads to a diagonal X'X matrix.
Measuring the adequacy of the least squares fit is an important practical
problem. Recall that the vectors y, y and e form a right triangle, with the
latter two vectors containing the right angle. Either the angle between the
vectors y and y (which lies between zero and 90°) or their relative lengths can
be used to assess adequacy of fit (see Figure 4.2.3). When the model provides
a good fit (as in (a)), the angle between y and y is small, and the two vectors
have nearly the same length. On the contrary, the angle is large and y is much
shorter than y if there is a poor fit (as in (b)). If y X C(X), the projection of y
onto C(X) is the null vector, i.e., y = 0. The square of the cosine of the angle
between y and y, which is also equal to the square of the relative lengths of y
and y is called the coefficient of determination, i.e., R2 =|| y ||2 / || y ||2. In
other words, we can interpret R2 as the proportion by which the fitted vector
y is shorter than the observed response vector y. We give a formal definition.
/
/
y /
/
£
j
y
£
L
A
A
y
y
(b)
(a)
Figure 4.2.3.
Adequacy of least squares.
We define the coefficient of determination of
Definition 4.2.4. R-square.
the linear model as the proportion of the total variation in Y explained by the
explanatory variables, i.e.,
SSR
SSE
R2 =
(4.2.20)
= 1- SST '
SST
Clearly, 0 < R2 < 1.
This measure, which is also the multiple correlation coefficient between Y
and the set of predictors (Xi,
• • , X k ) (see Definition 5.2.8), is widely used to

4.2. THE LEAST SQUARES APPROACH
105
assess goodness of fit of the linear regression. A value of R2 close to 0 indicates
a poor linear regression fit, while a value close to 1 indicates a good fit. An R2
of 0 says that the linear model does not explain any variation in the response Y.
In a simple regression model, it is possible that (a) the values of Y lie randomly
around the horizontal line Y = Y , or (b) the observations lie on a circle. An R2
of 1 can occur only when all the Y values lie on the estimated regression line. In
this case, it is easily verified that R2 is the square of the simple product-moment
correlation between X and Y . One disadvantage with using R2 as a measure of
goodness of fit is that it does not account for the number of degrees of freedom
associated with SSR. We define a measure that makes this degree of freedom
correction in the context of a full rank model.
The adjusted i?2, or the corrected
Definition 4.2.5. Adjusted R-square.
R2 is defined as
Var{e ) __
Var(y)
Ef=in/ ( N - p)
Rl
1-
adj .
E£IW-W(tf - l)
SSE
N -k l - R2 )
V
(4.2.21)
1-
= 1-
SSTC
N -
where p is the total number of parameters in the fitted model including the
intercept, SSTC is the total mean corrected sum of squares and SSE denotes
the error sum of squares with ( N — p) degrees of freedom.
Since an “ adjustment” has been made for the corresponding degrees of free-
dom in the relevant sums of squares, J?2
rfj- is useful for comparing different
regression fits to the same data as well as for comparing different data sets,
although in the latter situation, its usefulness is rather limited. It is a gross
initial indicator, and one is better off using other model comparison criteria.
The adjusted R2 is closely related to Mallows’ Cp statistic, which we describe in
Chapter 8. We next derive a general formula for the mean of a quadratic form
which is useful for obtaining the covariance matrix of /3°.
Result 4.2.3. Suppose E( x ) = /z, and Var( x.) = E, then,
E(x'Ax) = tr(AE)+/i;A/i.
(4.2.22)
Proof. Similar to the well known univariate expression E( X 2 ) = Var( X ) +
{ E( X ) }2, we can write in the vector case
£^(xx') = E + up' ,i.e.,
E(x — p)( x — p )' = E.
Since tr(x'Ax) = tr(Axx') from property 4 of Result 1.2.8,
E[tr(Axx')] = £r[A.E(xx/)] = tr [AE + A-pp ]
tr(AE) + tr( p' Ap) = tr(AE) + p' Ap,
£(x'Ax)

CHAPTER 4. THE GENERAL LINEAR MODEL
106
which completes the proof.
Result 4.2.4.
The following properties hold.
1. We have
E( /3° ) =Up and
Cav{0° )= a2GX'XG',
(4.2.23)
where H = GX'X.
2. The expectation and covariance of the vector of fitted values and the
residual vector are
E(y) = X/3, and Cov(y) = a2P,
E(e ) = 0,
and Cov(e) = a2( I - P).
(4.2.24)
3. The expectation of the mean square error is
E(a2 ) = <T2.
(4.2.25)
Proof. The expected value of the solution vector /3° is
E(0° ) = GX'E(y) = GX'X/3 = H/3,
so that /3° is an unbiased estimate of H/3. Note that H is a function of the
design matrix X and of G, a g-inverse of X'X. The variance-covariance matrix
of /3° is
Cov((3° ) = GX,Cov{ y )XG
/ = GX'((j2IiV)XG
/ = <r2GX'XG'.
By a suitable choice of g-inverse, we would obtain Cov( f3° ) = cr2G. To see this,
recall from Result 2.3.4 that there exists a p x p orthogonal matrix P such that
P'X'XP is a diagonal matrix whose r nonzero elements are the eigenvalues of
X'X. Let An denote the leading r x r principal submatrix of P'X'XP. Then,
An
1
0
^
p'
0
0 )
is a symmetric g-inverse of X'X and we can verify that G*X'XG*/ = G*.
If /3° = G*X'y, then Cov( j3° ) =
<r2G*. To prove property 2, we see that
E( y ) = XGX' E( y ) = XGX'X/3 = X/3, and E(e ) = E(y-y) = 0. Now,
SSE =
(y- X/3°)'(y — X/3°)
=
y'(IN - XGX')'(IN - XGX')y
=
y'(Ijv - XGX')y
since IN " XGX; is a symmetric and idempotent matrix (see Exercise 4.2).
Since XGX' is unique, so is SSE. Using Result 4.2.3,
=
E [y'(lN - XGX')y]
=
<r[a2(IN - XGX')] + 0' X' ( IN - XGX')X/3
=
a2( N — r ).
G* = P
(4.2.26)
E( SSE )
(4.2.27)

4.2. THE LEAST SQUARES APPROACH
107
Therefore, the least squares estimator a2 is an unbiased estimate of cr2, and is
again invariant to the choice of G.
Corollary 4.2.2.
estimator of /3 follow as a special case of Result 4.2.4 and are given below:
In the full rank model, the properties of the least squares
E(P ) = 0 and
Cov( p)= a2( X' Xy\
(4.2.28)
Example 4.2.4.
E(Yi ) = iO, and Var(Yi ) = <72, for i = 1, 2,3. We first set this up as a full rank
linear model of the form (4.1.1), with
Let Yi,Y2, and Y3 be independently distributed with
Yi
1
£1
y = *2
0 = 0.
x =
2
£ —
£2
Y3
3
£3
The least squares estimator of 0 is 6 = (X'X) *X'y =^
(Yi + 2Y2 + 3Y3),
with variance Var(9 ) = a2/14. The estimator is unbiased for 6 since E{6 ) =
j^
{6 + 2(26 ) + 3(36 ) } = 6. The fitted vector y and the residual vector e
respectively given by
/ A\
are
Yi - e\
Y2 - 29
.
\Ys - 36 j
MYX + 2y2 + 3Y3 )\
\{Yi + 2Y2 + 3Y3 )
, e — y-y =
£(Yi + 2Y2 + 3Y3 )J
e
2e
y =
36
The error sum of squares is
= (Y?+Y 2 + Y32)-^
(FI + 2Y2 + 3Y3)
=
(Y2 + Y22 + Y32)-L(y, + 2Y2 + 3Y3)2,
and the mean square error (MSE) is a2 = SSE/ ( N — p) = SSE/ 2.
y-y
£ £
SSE
Numerical Example 4.1. Simple linear regression.
A company which
markets and repairs small computers needs to forecast the number of service
engineers required over the next few years. This requires consideration of the
length of service calls, which in turn depends on the number of components
that need to be repaired or replaced. The data set consists of the number of
components repaired, X, and the length of the service call in minutes, Y, for a
random sample of 20 calls (Chatterjee and Price, 1991). We illustrate fitting a
simple linear regression model to explain the relationship between Y and X.
The estimated simple correlation between Y and X is p= 0.965, indicating
a high, positive linear relationship between the two variables. The fitted simple
linear regression model is
Y = 37.21+ 9.97X

CHAPTER 4. THE GENERAL LINEAR MODEL
108
with d = V M S E == 18.75. The estimated standard errors of the least squares
estimates are s.e.( Po ) = 7.99, and s.e.( fii ) = 0.72. Both coefficients are sig-
nificantly different from zero at the 5% level of significance. The coefficient of
determination, R2 = 0.90, which is the square of the simple correlation coeffi-
cient, while R2
= 0.89.
A
adj.
Numerical Example 4.2. Multiple linear regression.
of health records of 30 employees who were regular members of a company’s
health club. The variables are Y, the time in a one-mile run (in seconds), Xi,
the weight (in pounds), X2, the resting pulse rate per minute, X3, the arm and
leg strength (no. of pounds a subject was able to lift), and X4, the time in a
1/4-mile run (in seconds) (see Chatterjee and Hadi, 1988). The following matrix
shows the pairwise correlations between the variables (Y, Xi, X2, X3, X4):
/1.000
0.798
0.501
0.452
0.848\
0.798
1.000
0.420
0.734
0.643
0.501
0.420
1.000
0.063
0.539 .
0.452
0.734
0.063
1.000
0.410
\0.848
0.643
0.539
0.410
1.000/
The data consists
The fitted multiple regression model is
Y--8.863 + 1.243Xi- 0.502X2- 0.476X3 + 3.938X4.
Least squares estimates of the parameters are shown below, and R2 = 0.85.
Least squares estimates for Numerical Example 4.2.
Parameter
d.f.
Estimate
s.e.
t-value
Pr> t
Intercept
1
-8.863
55.520
-0.16
0.875
4.37
0.0002
-0.58
0.568
-1.98
0.059
5.23
< .0001
Xx
1.243
0.284
1
-0.502
0.867
1
-0.476
0.241
3.938
0.752
1
X2
X3
X4
1
Entries in the last two columns pertain to inference based on normality of errors,
which will be discussed in Chapter 7. A
Although the normal equations in (4.2.2) presumably consist of p equations
in p unknowns, there are only r LIN equations, since the remaining (p - r )
equations are linear combinations of these. To obtain a solution of /3, ( p - r )
additional consistent equations, say, a'/? = bj, j = 1, • • • , (p- r), are required,
satisfying the condition that aj is not a linear combination of the rows of X'X.
Suppose that, on the contrary, a' is such a linear combination. Then, we can
either obtain the corresponding additional equation from (4.2.2), or we will face
inconsistency, i.e., we will obtain two different values for a' /3°. In other words,
this condition requires that a'/3 is a non-estimable function (see section 4.3).
Suppose we represent the (p — r) additional equations in the form A/3 = b;

4.2. THE LEAST SQUARES APPROACH
109
each equation (row) must correspond to a non-estimable function.
We will
refer to these additional equations as constraints; they are generally chosen by
inspection, and are included for the purpose of solving the normal equations.
In section 4.6, we will talk about linear restrictions on the parameter vector /3
which arise as an integral part of the model specification.
Example 4.2.5.
effects ANOVA model, we have seen that r(X) — a < p, so that the least
squares solution /3° is not unique. Let Y{. and Yrespectively denote the total
and average of the sample observations under the ith treatment. Let K. and Ir-
respectively denote the grand total and average of all the sample observations.
We have used the “dot” subscript notation which implies summation over the
subscript that it replaces. Symbolically, Yi. =
Yij, Yi- = Yi./ rii, i =
1, • • • ,a,Y..=
]Cj=i Yij, and Y .. = Y../ N, where N = Yli=i n* 1S the total
number of observations. The method of least squares consists of minimizing the
sum of squares
the (a 4- 1) normal equations are
We continue with Example 4.1.3. In the one-way fixed-
4 =H=i
(Y i j -n-n f ;
Ta ) =T.U1E”=l
dS
dS
= 0, and — —
= 0, i = 1,— ,a,
dp
dri
M°»Tf
M°,r?
which have the form
Np° +^2niTi
~
and
i=i
Tliifl0 +4)
Yi., i — 1, * ” , fl.
Note that the sum of the last a normal equations is equal to the first; the a -I-1
normal equations are linearly dependent and hence no unique solution exists.
In other words, there are p = a + 1 parameters, and r(X) = a, so we can
set one element of /3° equal to zero, and solve for the remaining a parameters
uniquely. In general, we may obtain a solution to the normal equations subject
to a constraint such as p° = 0, or r® = 0.
If we set p° = 0, we obtain the solution /3° = (0,y!., • • • ,Ya Y - The g-
inverse that corresponds to this solution is obtained by deleting the first row
and column from X'X in the algorithm in Result 3.1.2 as
0
0 \
o
D r
G =
where D = diag(l/ni, • • • ,1/na ) is an a x a diagonal matrix. The projection
matrix P has the block diagonal form
diag (^
Jni , - - * ,^
Jna)
P =

CHAPTER 4. THE GENERAL LINEAR MODEL
110
and the fitted vector y is
y = Py = (y1.i;i , . . . ,Ya.l'nJ .
The ANOVA decomposition can be written as
fl
Tli
CL
r\
CL
71j
E EY% = E«*n + E E {Yu -Yi
i=l j= l
i— 1
i=lj=l
Suppose we impose an alternate constraint
niT? = 0 on the normal
equations, we obtain the solution vector
(3° = (F..,Fi.-F.., - - - ,Ffl.-F.).
Subject to this constraint, the estimate of the overall mean /x is the grand sample
average of all the N observations, while the estimate of the zth treatment effect
Ti is the difference between the average of the sample observations under the zth
treatment and the grand average. The estimate of the error variance in both
cases is
SSE
a2 =
(4.2.29)
( N - a )
where
a
rii
&
£X>d - X>F*-
(4.2.30)
SSE =
i=ij=i
Neither of these solutions for /3 is unique; a solution depends on the constraint
under which we choose to solve the normal equations.
i=i
Although this might seem like an unfortunate event, since we would be afraid
that it might lead two different data analysts to draw two completely different
inferences, we show that estimation and inference is indeed unique for certain
functions of the parameters called “estimable functions” which are described in
the next section. We conclude this section with an example of another fixed-
effects linear model.
Suppose an exper-
Example 4.2.6. Two-way cross-classified model.
iment has two factors, Factor A at a levels, and Factor B at b levels. In a
cross-classified model, every level of Factor A can be studied with every level of
Factor B. We introduce two relevant models.
This model involves only main effects due to each factor
Additive model,
and has the form
Yijk
ft
Ti ~h (3j -f- £% jk i ^
Ij * * *
7 Tt ) i
1, *
y CLy j
1, *
5 ^
7
(4.2.31)
where Yijk denotes the kth replicate in the (z, j)th cell, fj, is the overall mean, Ti
denotes the effect due to the zth level of Factor A, (3j is the effect due to the jth
level of Factor B, and the errors Sijk are iid with zero means and variance cr2.
If n = 1, we can write (4.2.31) in the form (4.1.1) with N = ab ) p = a -f- 6 + l,
and

4.2. THE LEAST SQUARES APPROACH
111
fib
0
I6\
1&
0
• •
1b
0
lb
• • •
0
Ib
x =
•
:
16
16 Ib )
\lb
0
0
y = (Yu
...
Ylb ...
F a l
• • •
with r(X) = a+6 — 1. Let TV = nab. In the balanced case, X is an iV x p design
matrix, with p= (a+ 6+1), while r(X) = (a -F b — 1), since there are two linear
dependencies. The normal equations X'X/?° = X'y have the form
nabp0 + nbJ2 ri + na^
fij
<=i
j=i
nbp° + n6rf + n53Pj
j=i
na/i° + n
rf + na/3j
Y...
Yi..
2 = 1,. *. , a
Y.4
j =
,6.
2=1
We impose two constraints 53?=i^
= 0, and 53$=I
= 0 ? i*e., A'/3 = 0, with
0
1
1
0
• • •
0
0
0
0
1 ...
_
the resulting system of normal equations to obtain
A' =
1
1, P = (M,TI , • • • ,ra,/?i • • • , fa )' , and solve
M°
Y ...
r°
Yi .. - Y ...
Y .j . - Y ...
i = 1, *
• ,a
j = 1,... ,6.
Searle (1971, p. 266) has described the use of absorbing equations for solution
of the normal equations (see Result 3.2.10). The fitted values are
Yijk = M° +
+ (% = Yt .. + Y .j . - Y . . .
$
In some experiments, we find that the difference
Model with interaction.
in response between the levels of Factor A is not the same at all levels of Factor
B. In this case, there is interaction between the two factors. A general model
which involves main effects due to each factor as well as interaction between
them is
Yijk — p
Ti ~\~ Pj
-ijki
& — 1»
* * * •>
i
1? * *
»
j —
* * *
?
(4.2.32)
where (r/?)ij denotes the interaction effect between the zth level of Factor A
and jth level of Factor B, and the other terms have been defined earlier. The
interaction term is sometimes denoted in the literature by 7^
. We can write
Pij = P+ Ti + /3j + 7ijy so that 7ij — pij - p — Ti — Pj is what is left over from
additivity. If n = 1, i.e., there is no replication, we can show that we cannot
separate interaction from the error (see Exercise 4.10). Let N = nab, and let
V
—
yr
yr
_
V-
V
—
V'71
V
,
~ Z^k= l z^ j=l 2^i=l J i j k ,
— z_>fc=l L^ j=\ * i j k i * - j - —
2-jk= l
1 i j k ,
and Yij. ~ 53£=1Y^
k - The normal equations obtained by minimizing

CHAPTER 4. THE GENERAL LINEAR MODEL
112
n
b
a
E E E (Yijk - M -
— Pj ~ lij )2
Ti
k=lj=li=l
are given by
abn/ j.0 + bn^
jr? + an^
Pj +
Y...
i— 1
3=1
i=1 j=1
b
b
bnn° + bnr? + n^
/3° + n^
7° ,
i
Yi..
= !, • • • , a
j=i
j=i
ann° +
+ on/3° + n^
y?-,
j = 1, • • • ,6
i=l
rifjP + nrf 4- n/?® 4- n7°-, i
K,.
3
i=i
%
i = 1, • • • ,a, j = 1, • • • ,6.
Only the last set of ab equations is LIN, and the remaining equations can be
derived from these, by adding over z, or over j, or both. In matrix form, the
design matrix X has dimension nab x (l + a + 6 + ab), but its rank is only a&,
since there are (a -f b 4- 1) linearly independent constraints. Of the constraints
E“=i Ti = 0, Ej=i Pj = 0, E“=i 7,° = 0, j = 1, • • • , b, and E$=i 7% = 0, i =
• , a, only (a+&+1) are LIN, sinceYli=i
l?j ~ 0» so that r(X'X) = ab.
1,
•
Imposing these constraints, the normal equations yield the following solutions:
Y ...
=
Yi..-Y ..., i =
a
Y .j. — y...,
j = 1, • • , 6, and
- V'
t- - y.j. +
i =!, - • • , a, j =!, - • • ,6.
(TP )ij
(4.2.33)
A numerical example illustrating these and further inference is described in
Chapter 7.
Example 4.2.7. Two-factor nested model.
ments, the levels of one factor, say Factor B, are similar, but not identical for
different levels of another factor, Factor A. Then, the levels of Factor B may be
considered to be nested within (under) the levels of Factor A, and we have a
nested or hierarchical model. The two-way nested model has the form
In some multi-factor experi-
(4.2.34)
Yijk —
M + ri + Pj(i ) + £(ij )k
for k = 1, • • • ,Uij, j = 1, • • • , bi, and i = 1, • • • ,a. There are a levels of Factor
A, and bi levels of Factor B nested within each of the levels of Factor A in
this unbalanced model. For example, suppose that a clinical trial involves a
counties, and there are bi hospitals in the ith county, and that data is collected
on a sample of nij patients from the jth hospital in the ith county. The analysis

4.3. ESTIMABLE FUNCTIONS
113
of such data requires a nested or hierarchical model. Notice that we cannot
include an interaction term between Factor A and Factor B since every level of
Factor B does not appear with every level of Factor A. If
= n, and bi = b
for all 2, j, we have a balanced model with N — nab observations. The total
number of model parameters in this case is p — 1 -f a + ab corresponding to /2,
Tj, and (3j(i), j = 1, • • • , 6, i — 1, • •
In the unbalanced case, the normal equations are obtained by minimizing
the function
• , a.
a
bi
n i j
i— l j— l k— l
and have the form
NP° +^
n2.r° + X^
ny /3°(l),
i=lj=l
bi
=
rii /x0 + m-T? +
,
t = 1, • •
j-1
=
+ r? + /$(*)) >
where ni. =
n^
, and A/- =
nij - We may verify that the first
equation is obtained by summing the last set of equations over both 2 and j,
while the next set of a equations are obtained by summing the last set over
j. Although the total number of parameters in the unbalanced model is p =
1 + a +
ft*, the number of LIN normal equations is only Yli=i
We
impose the following (a + 1) constraints. We assume that
1 niT? — 0 » and
nijPj(i ) = 0, i = 1, • • • , a. The least squares solutions of the parameters
under these constraints are
Y...
i=1
Yi..
• , a,
%
J
1 >
* ’ *
> bi > ^ — I >
* * *
1
y..„
— y..., 2 = 1, • • • , a, and
Yij .
Yi.., ji — 1, • • • , biy
% — 1, • *
,u.
$*>
The fitted values are Yi3k = y
*j., and the residuals are £ijk = ly* — yij.• For
the balanced model, the least squares solutions are obtained by setting Uij — n,
and bi — b in the above formulas. In Example 7.4.6, we generalize the two-stage
nested model to an m-stage nested model.
Numerical examples for these ANOVA models will be described in Chapter
7, where we discuss inference for the balanced models. In Chapter 9, we discuss
unbalanced ANOVA models.
4.3
Estimable functions
In the previous section, we saw that unless r(X) = p, /3° is not unique. Although
in the full rank model, we can estimate any function of /3, we must restrict our-

CHAPTER 4. THE GENERAL LINEAR MODEL
114
selves to estimating only certain linear functions of (3 when r(X) < p. Such a
linear function of /3 is called an estimable function. In other words, a linear func-
tion of (3 for which a (unique) estimator based on (3° exists, which is invariant
to the solution /3°, is called an estimable function. A more precise definition,
which also provides an approach for the identification of an estimable function
of /3, is given below.
Definition 4.3.1. A linear parametric function cf (3 is said to be an estimable
function of (3 if there exists an iV-dimensional vector t = (ti, * - • , t^ )' such that
the expectation (with respect to the distribution of y) of the linear combination
t'y = t\Y\ -f
f tNYN is equal to c'/3, i.e.,
E(t' y ) = c'(3.
In other words, c'/3 is estimable if there exists a linear function of y whose
expected value is identically equal to c'/3.
Note that for a particular linear
function c'/3, the vector t is not required to be uniquely defined in any sense;
the existence of such a vector t suffices. Usually, there may exist several linear
functions of y, each of whose expectations is equal to c'/3. For establishing
estimability, it is sufficient to verify the existence of any one of these functions.
If r(X) = p, i.e., if we have a full rank linear model, then any linear function of
/3 is a linear estimable function, which implies that we may estimate and carry
out inference for any linear function of (3. This, however, is not the case for
the less than full rank model, and we must first check whether a function of
interest is an estimable function. Only then, we can estimate it and proceed
with further inference. Note also that the definition of estimability does not
depend on the error variance specification. We state and prove some properties
of estimable functions of /3; these results are trivially true for the full rank case.
(4.3.1)
Result 4.3.1.
1. The expected value of any observation is estimable.
2. Any linear combination of estimable functions is estimable.
3. The function c'/3 is estimable if and only if
c' = t'X
(4.3.2)
for some vector t.
4. The function c'/3 is estimable if and only if
c' = c'H
(4.3.3)
where H = GXX
5. Given an estimable function c'/3, the quantity c'/3° is invariant to the least
squares solution (3°. That is, if (and only if) c'/3 is estimable, c'/3° is the
(unique) least squares estimator of c'/3.

4.3. ESTIMABLE FUNCTIONS
115
Proof. To prove property 1 for the zth response, consider an ^-dimensional
vector t = (t\, • • • , £;v )' where, for j = 1, • • • , AT, tj = 1 if j = 2, and tj = 0
otherwise. By Definition 4.3.1, t'£’(y) is an estimable function of 0 and is the 2th
element of the vector .E(y). To prove property 2, we see that by Definition 4.3.1,
every estimable function of /3 is a linear combination of the elements of £(y),
and so is any linear combination of such estimable functions. This is therefore
estimable. Alternatively, let c[0 and c2/3 be any two estimable functions of /3.
This implies that for some AT-dimensional vectors t* and t2,
ci/3 = E(t [ y )
and
c'2/3 = E(t'2 y ).
For constants a\ and <22
aiC
/
1/3+a2c'2/3 = (ait'14-a2t2)£(y)
is a linear combination of E(y), which proves property 2. Next, since E( y ) =
X/3, we see from Definition 4.3.1. that
c'/3 = t' E( y ) = t' X /3
which holds for all values of /3. This in turn implies that
c' = t'X
for some iV-dimensional vector t if and only if c'/3 is an estimable function,
proving property 3. Property 3 thus gives a necessary and sufficient condition
for estimability. We may also state the necessary and sufficient condition for
estimability in terms of (4.3.3) . To prove this, first assume that c'/3 is estimable.
Then, from (4.3.2) and property 2 of Result 3.1.8,
c'H = t'XH = t'XGX'X = t'X = c'.
To show the converse, suppose c'H = c' holds. Then,
c' = c'GX'X = t'X,
say, where t' = c'GX', so that by (4.3.2), c'/3 is estimable. To prove property 5,
we must show that c'/?° is invariant to the solution /3° we obtain to the normal
equations. We see that
c'/3° = t'X/?0 = t'XGX'y,
which is invariant to the choice of g-inverse G (by property 3 of Result 3.1.8)
and hence to the choice of /3° .
Example 4.3.1.
In the model
E{Yi )
E(Y2 )
E(Ys )
Pi + @2
Pi +0s
0i 4- 02

CHAPTER 4. THE GENERAL LINEAR MODEL
116
we find a necessary and sufficient condition for estimability of ci/?i+C2P2 + C3/I3,
for real constants c*, i = 1, • • • , 3. J2i=l Ci/3i is estimable if and only if there
3
3
exist U , i = 1, • • • ,3 such that E(%2i=1UYi ) =
c*/?*, i.e., if and only if ci,
C2 and C3 satisfy the conditions c\ = Y^
i=i^
c2 = £i + £3, and C3 = £2, he.,
if and only if Ci = 02 + 03. While /3i + #2/2 -f 03/ 2 is estimable, the function
Pi — 02/ 2 — 03/ 2 is not estimable.
If c'0 is estimable, then c lies in the estimation space, and is orthogonal
to any vector d, where d'y is in C-^
X). The following result is an immediate
consequence of the previous result, and we leave its proof as an exercise.
Result 4.3.2.
1. X/? is estimable, and any linear function of X/3 is also estimable.
2. X'X/3 is estimable, and any linear function of X'X/3 is also estimable.
Note that in general, there could exist an infinite number of linear parametric
estimable functions of 0 corresponding to a given linear model. The following
result characterizes the exact number of LIN estimable functions.
Result 4.3.3. There exist exactly r LIN estimable functions of /3, where r =
r(X).
Proof. By (4.3.2), c'0 is estimable if c' = t'X for some vector t. Let T be an
N x N matrix of rank N , and let C' — T'X, so that C'0 denotes N estimable
functions. By property 5 of Result 1.2.12, r(C) = r, so that there are exactly r
LIN rows in C', which implies that exactly r of the N estimable functions are
LIN.
Although T need not be unique, its orthogonal projection onto C(X) is
unique. For, if Ti and T2 are such that C' = T
^
X = T2X, then
PTi = X(X'X)"X'Ti = X(X'X)~C = X(X/X)-X'T2 = PT2.
The only linear functions of 0 with unique least squares estimates are estimable
functions.
The least squares estimate of c'0 is unique if and only if c'0
Result 4.3.4.
is estimable.
Suppose 0i and 0% satisfy X/3? = X/3^
= Py. To prove sufficiency,
Proof.
suppose c' = t'X. Then
c'/3? = t'X/?? = t'Py = t'X/?? = o'/??.
Consider the model y = X/3 + e, where
Example 4.3.2.

4.3. ESTIMABLE FUNCTIONS
1 1 7
{Yu\
( 1
1
0
0\
1
1
0
0
1
1
0
0
1
0
1
0
1
0
1
0
U
o o
i
/en\
( A
Y12
£l2
*13
T\
£13
0=
X =
—
>21
T2
£21
\T3/
> 22
^22
\£31/
y-
\ 31/
Since E(Yij ) = /x + 7
*
*, for j = 1, • • • , n*, z = 1, • • • , 3, we have
i?(*ij -*2*) = M + n - /x - r2 = ri - r2 = ( 0
1
so that there exists a vector t = ( 1
0
0
— 1
0
0 )' such that t'i5(y) =
7
*1 —
7
*2, which implies that ri —
7
*2 is an estimable function of 0. Similarly,
since E(t'y) = /x -f n for t — ( 1
0
0
0
0
0 )', we see that /i + ri is
an estimable function. To generalize, \x -f n, i — 1, - - • ,3, and 7* - r^, z /
fe,i, fc =1, • • • , 3 are estimable, as are all linear combinations of such functions.
In this model, r(X) = 3, so that one set of LIN estimable functions consists of
{n - 7
*2, ri - 73, r2- 73}.
-1
0 )/?,
Definition 4.3.2. Contrast.
A contrast in the p-dimensional parameter
vector 0 is a linear function c'0 such that c'lp =
ci = 0.
Two contrasts c' /? and c[(3 are
Definition 4.3.3. Orthogonal contrasts.
orthogonal if c'Gc/ = 0 in an unbalanced model. In the balanced case, the
condition for orthogonality is c'c/ = 0.
The normal equations that lead to the least squares solu-
Example 4.3.3.
tions in the one-way model were given in Example 4.1.3. From these, it follows
that /x + Ti, z —
form a set of LIN estimable functions, so that all
estimable functions must be linear combinations of these functions. Estimable
functions of the parameters /x, and r^,i = 1, •
, a are given below:
1. For i = 1, • • • ,a, /x-f-7* is estimable; its estimate isY i., with variancea2/rn.
The set of functions{/x + r*, z = 1, • • • , a} is a basis of the estimation space.
2. The function
1
Ti ) is estimable; its estimate isYlt
variance
xc
2a2/n*.
=1 CiY^
., with
3. The function
CiTi is estimable if and only if Yli=1
contrasts in r^’s are estimable.
Cj = 0; i.e., only
4. /x is not estimable, and neither is 7*, for any z.
Verification of these results follows directly from the definition of estimability.
Since the expected value of any observation is estimable (see Definition 4.2.1),
it follows that /x + 7* is estimable, being the expectation of Y^
-. Since 00 =
(y..,Y 1. — y.., • • • ,yo. — y..)' under the constraint
1 Ti ~ 0, it follows that
i = 1, • • • , <2
/x + 7* = y<. ,

CHAPTER 4. THE GENERAL LINEAR MODEL
118
with variance a2 jn^ To obtain these results, we may write /i+ r* ascJ/3, where
ci is an (a -f l)-dimensional vector with 1 in the first and (i+ l)th positions, and
zero elsewhere. That { /J, + r^, i = 1, • • • , a ] is a set of LIN estimable functions
follows directly from the form of the normal equations in Example 4.1.3, so
that all estimable functions must be linear combinations of this basis. This
proves properties 1 and 2. Clearly, the estimate of Yli=i C*(A* + T*) is equal to
Ci(/x -F Ti ) =
I
and its variance is Yli=i cia
2/ni- Property 3 is a
special case of property 2. We prove property 4 by contradiction. Suppose that
[i is in fact estimable. Since{/z + 77,i = 1, • • • ,a} is a basis set, there must exist
constants c*,i = 1, •
• , a such that
ci(M+ 7i) =
Ci+J2i=x c<rt 5
this implies that
= 1» andY^
i=i
= 0 for all 7*, which in turn implies
that Q must be equal to zero for i = 1, • • • ,a, which is a contradiction! Hence,
/z is not estimable. We leave the proof of the property that rt is not estimable
for any i as an exercise (Exercise 4.13).
In the one-way ANOVA model, the function 7 =
Cin such thatYli=1
= 0 is a contrast in the treatment effects r^, i = 1, • • • ,a. The functions Tj — 77,
for j ^
l are elementary contrasts since they represent simple comparisons of
two effects or groups. Other functions like r\ + 72 — 273 or Yli=iciT
/ii with
= 0 are called general contrasts. General contrasts can be expressed as
linear combinations of elementary contrasts. For instance, we can write 77 4-r2 —
2T3 = (TI-7-3)+(72-7-3), and CITI + C2T2+
hcara = Ci(n-ra)+c2(r2-ra) -h
b ca_i(ra_ i — ra), since XIi=i c* = 0* The vector c' = (ci, • • • , ca) such that
53?=1 Q = c'l = 0 is called a contrast vector. There exists at most a — 1 linearly
independent (LIN) contrasts and a — 1 corresponding contrast vectors. One
such contrast vector set is (1,-1, 0, • • • , 0), (1,0,-1, • • ,0), • • • , (1, 0,0, • • • ,-1)
which corresponds to contrasts T\ — r2, r\ — r3, • • • , r% — ra.
Another set is
defined by c\ =
> 1
(1,1, • • • ,1, — Z, 0, • • • ,0), for l = 1, • • * ,a — 1, and the
v *0+i)
corresponding contrasts are (TI 4-T2H
1-77— Z77+1) jy/l(l + 1)
Note that cjc* = 1 and
= 0, l ± /1, i.e., these are a-1 mutually orthogonal
contrasts. In general, two contrasts
ciTi an(i
are said to be
orthogonal if
= 0 for a balanced design, and if
UiCidi = 0 for an
unbalanced design.
£*=1
l times
/ =!, • • • , a— 1.
4.4
Gauss-Markov theorem
We now state one of the most fundamental results in linear model theory, which
gives an optimality result for linear estimates of estimable functions of /3 without
any distributional assumption.
Let c'/3 be an estimable function of /3, and let /3° denote any
Result 4.4.1.
solution to the normal equations (4.2.2). Then, c' P° is the best linear unbiased
estimator (b.l.u.e.) of c'/3, with variance Var(c'0° ) = c'Gccr2.

4.4. GAUSS-MARKOV THEOREM
119
Proof. Clearly, c'/3° = c'GXy is a linear function of y. As we mentioned
earlier, c'(3° is invariant to G, and hence to /3°, and can therefore be regarded
as a unique estimator of c'/3. Now,
E(c' /3° ) = c'£(/?°) = c'GX'X/3
where H — GX'X.
Estimability of c'/3 implies that c' = t'X for some N-
dimensional vector t, so that, using property 2 of Result 3.1.8
E(c'(3° ) = t'XGX'X/3 = t'X/3 = c' /3.
c'H/3,
(4.4.1)
(4.4.2)
This proves that c' /3° is an unbiased estimator of c'/3. Further, since c' = t'X,
and X'XG'X' = X' (see Exercise 3.4), we have
Var(cf /3° )
a2c'GX'XG'c = oVGX'XG'X't
<r2c'GX't = <J2C'GC.
(4.4.3)
We now show that c'/3° has minimum variance among the class of linear unbiased
estimators of c'/3. Let d'y denote any linear unbiased estimator of c'/3 other
than c'/3°. Then, we must have .E(d'y) = d
;X/3 = c
7/3, which implies that
d'X = c'.
The covariance between c'/3° and d'y is
Co-y(c'/3°, d'y)
=
Cou^
'GX'y,d'y)
= ^
c'GX'd = (T2C'GC,
(4.4.4)
(4.4.5)
where the last equality follows from (4.4.4). Using (4.4.3) and (4.4.5), we see
that
0
<
Var{c'/3° — d'y) = Var(c!/3° ) + Var(d' y ) — 2Cov(c'(3°,d'y)
=
a2c'Gc + Uar(d'y)
2<T2C'GC
=
Uar(d'y) - cr2c'Gc = Uar(d'y) -Var(c'(3° ),
with equality holding if and only if d'y = c'/3°. We have shown that c'(3° has the
smallest variance among all linear unbiased estimators of c'/3; it is the b.l.u.e.
of c' /3.
m
(4.4.6)
Consider a projection approach for constructing the b.l.u.e. of an estimable
function c'(3. Since c'(3 is estimable, there exists at least one vector t'y whose
expectation is c'(3. We can decompose the vector t into two orthogonal compo-
nents as t' = t'P + t'(I - P). Hence,
t'y = t'Py + t'(I - P)y,
where, from (4.2.2) and (4.3.2)
t'Py
=
t'XGX'y
=
t'X/?0 = c'(3°.

CHAPTER 4. THE GENERAL LINEAR MODEL
120
So, t'Py is the b.l.u.e. of c'/3, while t'(I — P)y belongs to the error space. Thus,
t'P is the projection of t' onto the estimation space.
Corollary 4.4.1. In the full rank model, the Gauss-Markov theorem states
that the b.l.u.e. of c'P is c'0 = c'(X'X)“ 1X'y, with variance
2 „r(X'X)-1^
<T C
Result 4.4.2. Let c[ j3 and c'2P be two estimable functions of (3. Then
Cov^
P0,c'2PQ ) = cr2c/
1Gc2.
Proof. By definition,
Cov(Ci /3°, C2/30) = <J 2c'lCov(0° )C2 = 0-2C'1GC2.
Example 4.4.1.
For the simple linear regression in Example 4.1.1, we have
E0o ) = 0o, and Var(0o ) = a2ZL X?/ N Zti ( Xi - X )2
E(01) = 0i and Varifo ) = a2 / ztiiXi - X )2 .
These results follow directly from Corollary 4.2.2. Further,
Cov(0o,0i ) = -a2 X / zL( Xi ~ X )2 .
^
/-s
The estimators po and p\ are clearly unbiased. When X = 0, Po and Pi are
uncorrelated, intuitively, we may see that this is so because when X = 0,
Po = Y, while pi is unaffected by a shift in location of the Y variable, i.e., it is
uncorrelated with Y . That E(Yi ) = Po -f- P\X %, and
Var(0o ) + XfVar(Pi ) + 2XtCovtfo,0i )
a2 / N + cr2( Xi - X)2 /
-X)2
Var{Y«)
2=1
follow directly from property 2 of Result 4.2.4.
We will show in Chapter 7 that the vector of fitted values y = (Yi, • • • , Y)v)
has a (singular) multivariate normal distribution. It may be verified that the
residuals are uncorrelated with the fitted values, whereas they are correlated
with the observed responses; the slope of the straight line fit of Si on Yi is
zero, while that of the fit on Yi is equal to 1 — r2, where r denotes the sample
correlation coefficient between X and Y defined by
r2 = [£f=1(Xi - X )(Yi - Y)l2/Ef=i ( Xi - X )2Ef=1E - 5^)2
•
There are several situations that arise in practice where it is known that PQ = 0,
corresponding to a straight line through the origin, i.e., a simple linear regression
without intercept. In this case, it is easily shown that

4.4. GAUSS-MARKOV THEOREM
121
Pi = EWM
andl/ar(A ) = a2/£^
2.
•
t=i
2=1
2=1
Example 4.4.2. In the two-way fixed-effects cross-classified additive model
with Ti — l, the following functions are estimable.
1. /i + Ti + (3j is estimable with b.l.u.e. Y ± +Y .j — Y ..;
2. Ti — Th is estimable, for i ^
h, with b.l.u.e. Y
— Yh \
3. Pj — Pi is estimable, for j / /, with b.l.u.e. Y .j — Y .i ;
estimability of these functions is directly verified since they are in turn E(Yij ),
EiYijY-EiYHj ), and E^
-EiYu ). The b.l.u.e. of n+n+ pj is A*
0+T?+/?? =
Y_.+Y±-Y ..+Y .j -Y ..=Yi. +Y .j -Y ... The b.l.u.e. of r<- rh is r?-r£ =
(Yi. — Y ..) — (yh. — Y ..), with estimated variance 2a2/ b. To verify the variance
estimate, we may see that Var(Yi.) = Var(Y^
=i Yij/ b) = Var(^
=1Yij )/ b2 =
ba2/ b2 = d2/ b. The b.l.u.e. of pj - pt is 0$ -$ = {Y .j - ¥ .. ) - (Y .t -F..),
with estimated variance 2d2/ a.
We conclude this section with a result on optimal estimation of the error
variance a2.
Optimal estimator of a2.
Result 4.4.3.
(4.1.1), where the second, third and fourth central moments of Yi exist and are
respectively equal to cr2, /X3, and (14. Then (N — r )d2 is the unique nonnegative
quadratic unbiased estimator of ( N - r)cr2, with minimum variance when fi4 =
3a4, or when the nonzero diagonal elements of the projection matrix P are equal
(Atiqullah, 1962).
Consider the linear model
Since a2 > 0, Rao (1952) proposed the class A of nonnegative
Proof.
quadratic unbiased estimates y'Ay of (N - r )a2. Let a denote the vector of
diagonal elements of A. We will state sufficient conditions for y'(I— P)y to have
minimum variance in A. If y'Ay
A, by Result 4.2.3, { N — r )a2 = E'(y'Ay) =
a2tr{ A) + /3'X'AX/? for all p. In particular, setting P — 0, tr{A) = {N — r),
then /3'X'AX/3 = 0 for all /3, implying that X'AX = 0, which in turn implies
that AX = 0 (see Exercise 2.26). It follows that AX(X'X)"X/ = AP = 0.
Then
(M4 - 3<r4)a'a + 2a4tr{ A2 ) + 4a2p' X' A2Xp + 4/i3/?'X'Aa
(4.4.7)
(for proof see Seber, 1977, Theorem 1.8). From (4.2.27), ( N - r )a2 = y'(I -
P)y
Ay and
Far(y'Ay)
{ /.14 — 3a4)a'a+ 2cr4tr( A2 )
yar[y'(I - P)y] = (/i4 - 3a4)q'q + 2cr4( N - r),
where q is the vector of diagonal elements of I— P. Suppose A = (I— P)+R, and
similarly, a = q+r. It follows that R is symmetric, with tr{R) = £r(A) — tr{I —
(4.4.8)

CHAPTER 4. THE GENERAL LINEAR MODEL
122
P) = 0. Since 0 = AP = (I — P)P+RP = RP, we get R(I— P) = R; it follows
using Result 4.2.1 that tr(A2) = tr(I-P)+tr(R2)-}- 2£r(R) = (A^-r)-ftr(R2).
Substitute a = q + r in (4.4.7) and use (4.4.8) to get
=
- 3<r4)a'a + 2a4 [( N — r ) -h £r(R2)]
=
(/x4 - 3<r4)(q'q + 2q'r + r'r) + 2a4 [( N - r ) + £r(R2)]
=
(/i4 — 3a4)q'q + 2a4( N - r )
+
2fr4[(/Li4 - 3(j4)(q'r + ^r'r) + tr(R2)]
=
Var( y' (l — P)y) +
2cr4[(/x4 -3(T^C^
qura +
i=i j=\
Var(y'Ay)
i-1
i~1
Although minimization of Var(y'Ay) subject to ir(R) = 0 and R(I- P) = R
is difficult in general, we can consider two special cases. First, when ^
4 = 3<r4,
Var(y'Ay) = Far(y;(I — P)y) + 2a4YiLiYjL1 r\y which is minimum when
nj = 0 for z, j = 1, •
• , N , i.e., when A = I — P. As a second case, suppose that
each nonzero diagonal element of (I-P) is equal to ( N -r )/ N. The proof that
Var(y'Ay) attains a minimum is left as an exercise (see Exercise 4.15).
4.5
Generalized least squares
Consider the linear model
y = X(3 4- e, Var(e ) = <J2V,
(4.5.1)
where V is a known N x N p.d. matrix. The other assumptions of the general
linear model still hold.
We show that the least squares procedure produces
optimal estimates (in the sense of the Gauss-Markov theorem). Since V is p.d.,
it follows from Result 2.4.5 that there exists aniVxiV matrix K with r(K) = A,
such that V = KK;. Let
z = K-1y, B = K~1X,
and
7] = K' l £.
(4.5.2)
Since r(X) = r < p, it follows from property 5 of Result 1.2.12 that r(B) = r.
It may also be verified that
E{rj ) = 0, and Var( rj ) = K‘1(a2V)K
Consider the “ transformed” linear model
<72IJV.
-1/ = <72K _ 1KK,K ~ 1'
z = B/3 + 77, Var(r} ) = V 2IN ,
(4.5.3)
which resembles the general linear model in (4.1.1) with r(B) = r < p. Mini-
mizing 77'7] = (z — B/3)'(z — B/3) with respect to /?, we obtain the generalized

4.5. GENERALIZED LEAST SQUARES
123
least squares (GLS) solution to the normal equations as
(B'B)-Bz
(X'K_1,K_1X)_
X,K_1/K ~1y
[X,(KK,)-1X]-X,(KK')~1y
(X,V_1X)“ X'V~1y,
where (X'V“ 1X)_
is a g-inverse of the p x p matrix X'V_1X (since V is
p.d., and r(X) = r, it follows that r(X'V“ 1X) = r).
Notice that X/3£,L5
denotes a projection of y onto C(X), but it is not an orthogonal projection.
Let W = X(X'V-1X)-X'V-1; then Wy = X(3°GLS, and C (W) = C( X ). The
matrixW is a projection matrix onto C(X), and if the inner product between two
vectors u and v is defined as u'V_1v, then W denotes the orthogonal projection
onto C(X). If r = p, i.e., if we have a full rank model, then (X/V“ 1X)
and the solution vector (4.5.4) is the unique generalized least squares (GLS)
estimator of f3 given by
&GLS
(4.5.4)
-l exists
0GLS = (X,V“ 1X)” 1X/V” 1y.
There is an alternate way to obtain the solution (3GLS
=
e'V -'e
=
(y — X^
)'V_1(y — X/3)
=
y'y-S/S'X'V-V + /S'X'V^X/J.
Differentiating with respect to f3 and setting equal to zero, i.e.,
drj' p/dp= -2X'V-1y+2X/V-1X/3 = 0
gives /3GLS in (4.5.4). The unbiased GLS estimator of a2 is given by
(4.5.5)
. Write
T j r j
1
(* - B0GLS )' (* - BP°GLS )
^2
aGLS
( N - r )
^b) (y - X/?GI(S),(KK,)-1(y- XP°GLS )
- X/3^
s)'Y-1(y- XP°GLS )
( N^
V ) SSEGLS-
(4.5.6)
A function c'(3 is estimable in model (4.5.1) if and only if it is estimable in the
transformed model (4.5.3) (see Exercise 4.18); also see Exercise 4.19.
Result 4.5.1. Suppose r(X) = r < p. Provided crf3 is an estimable function
of /3, the statistic c' /3GLS is the unique best linear unbiased estimator (b.l.u.e.)
of c'/3.
Proof. The proof mimics the proof of Result 4.4.1 for the transformed model
(4.5.3). If c' /3 is estimable, then

CHAPTER 4. THE GENERAL LINEAR MODEL
124
Var(c'0°GLS ) = <j2c'(X,V_
1X)_
c.
For two estimable functions c\ f3 and c2/?,
Cw{c[(PGLS,c'2tPGLS ) = cr2^
(X'V ~1X)-c2.
Corollary 4.5.1.
For the full rank model with r(X) = p, the GLS estimator
0GLS has the following properties:
1. PGLS is an unbiased estimator of 0.
2. The variance-covariance matrix of0GLS is Var(0GLS ) = a2(X'V“ 1X)-1.
3. The error sum of squares is SSEQLS = (y- X^
GLS)/V 1(y - X0GLS)•
4. c'0GLS is the best linear unbiased estimator (b.l.u.e.) of c'/3, with variance
aVfX'V^X)-1c.
Proof.
£(3GLS)- (X'V-1X)
1X'V
1X/3 = 0,
which proves property 1. Property 2 follows directly by seeing that
Var0GLS )
=
VarKX'V^X^
X'V-V]
=
<72(X'V~1X) “ 1X'V
” 1VV~1X(X'V-1X)-1
= ^
(x'v^x)-1.
The error sum of squares is first computed for the transformed model; substi-
tuting the original variables gives
SSEGLS
=
rfp = (z - B0GLS)'{Z - B0GLS)
=
(y - X^
is)'(KK')_1(y - X(3GLS )
=
(y-x3bLs)'V-1(y- X^
OLS).
To prove property 4, we see that
c'0GLS = c'(X'V
” 1X)_1X/V_1y = b'lY,
say, is a linear function of y which is an unbiased estimator of c'0 since
E(C'0GLS ) =^
'(X'V-'X^
X'V^X/?] = c0.
Suppose b^
y denotes any other linear unbiased estimator of c'0. In terms of
the transformed variables, we can write
c'0GLS = c'(B'B)"1B'z
and b2y = b2KK
_1y = (K'ba)'*.
Then,
Var [c'0GLS } < Var[(K'b2)/z,

4.5. GENERALIZED LEAST SQUARES
125
using reasoning similar to that in the proof of Result 4.4.1.
Example 4.5.1.
Consider the model
Yi =0Xi + £i, i = 1, • • • , AT,
where Si are normal random variables with E(ei ) = 0, and Cov(ei,£ j ) =
i, j = l, - * - , iV, \ p\ < 1. We can write this in the form (4.5.1) with
y = (Vi, • • * , YJv)', e = (ei, • • •
x = ( X\, • • • ,XN )\ (3 is a scalar parame-
ter, and
P2
* * *
P
P
P
TV — 1^
TV-2
/
1
P
1
P
V =
TV-1 .TV— 2 .TV-3
1
VP
p
p
This is a linear regression model with autoregressive order 1 (AR(1)) errors, i.e.
a serially correlated simple regression model. We can verify that
i/a - P2),
(i + P2)/(w2),
-P/(I - P2),
i
if i = j = 1, AT
if z = j = 2, •
* , AT
1
if|j-i\ = 1
otherwise ,
{V-1}^ =
10,
^
XX + P
2I2 X? - 2PEX^- I
P
2=1
2=2
2=2
1
TV
TV-1
TV
—
J2XiYi + P
2J2 Xiyi - 2p£(Xi*i-i +^
- i )
>
^
2=1
2=2
2=2
TV
(x'V_1x)
and
1-
x'VV
1-
Then, POLS = (x/V_1x) “ 1x'V ~1y is the b.l.u.e. of /3 with variance given by
a2(x'V_1x)-1 (see Corollary 4.5.1). By substituting an estimate of cr2, viz.,
(y ~ x/?GLs)/V_1(y —
X^
GLS) 7 we obtain the estimated variance of
the GLS estimator of /3. The variance of the OLS estimator of /3 is Var{(3) =
cr2(x/x)
_1x/Vx(x'x)
_1. Therefore,
Var{P ) -Var(PcLs )
<7?GLS
a2(x'x)'1x'Vx(x'x)
-
(72(X'V_
1X)_1X,V_1VV_1X(X,V_1X)
_ I
=
a2sVs/, say,
where s = (x/x)~1x/ — (x/V_1x)
_1x/V“ 1. This expression is p.s.d. since V is
p.d., verifying that Var( /3) > Var{PcLs )\ equality holds when V — Iyy.
-l
Under certain conditions, the OLS estimator and the GLS estimator of 0
coincide, asshown in the next example. We then present a result due to McElroy

CHAPTER 4. THE GENERAL LINEAR MODEL
126
(1967) which gives a necessary and sufficient condition for this to happen in the
full rank linear model.
Let y = (Yi, • • • ,YN )' be a random vector with mean 91^
Example 4.5.2.
and variance covariance matrix cr2V, where V is the equicorrelation matrix
with Va = 1,Vij = p, i 7^
i, j = 1, • • * , 7V (see Example 1.2.8). We write
this in the form (4.5.1) with x = ljv, /3= 9, and e = (ei, • • ,£ N )' - Using the
form of the inverse of cr2V derived in Example 1.2.8, we obtain (x'V~1x)‘_1 =
{1+ ( N — 1)p}/7V, and x'V_1y = NY / { l 4- ( N — 1)p}. From (4.5.5), the GLS
estimate of 9 is 9QLS = Y. The GLS estimate of 9 coincides with its OLS
estimate.
In the full rank linear model, (3 and (3GLS are identical if and
Result 4.5.2.
only ifCfV^X) =C(X).
Proof.
f3 and 0GLS are identical if and only if
(X'V^X^
X'V^y = (X'X^
X'y,
or, equivalently, if and only if
X'V^y = (X,V-1X)(X'X)-1X'y
for all y E 1ZN. Recall that we can write y = yi 4- y2, where yi E C(X), and
y2 E C± (X). This completes the proof.
The GLS estimate of /3 also coincides with its OLS estimate if y is in C(X),
i.e., if y = Xz for some vector z.
Another situation when these estimates
coincide is when X'y = 0 and Var( y ) = (1- p)I 4- pJ (see Exercise 4.17).
Consider the linear model (4.5.1) where V = diag(cr2, • • • ,<r% ) is an N x N
diagonal matrix. We refer to (3QLS as the weighted least squares solution vector
in the less than full rank case, and to (3GLS as the WLS estimator of (3 in the full
rank situation. When V is diagonal, the effect of the generalized least squares
analysis is to “ weight” the original observations by the corresponding diagonal
entries of V.
Example 4.5.3. Let Yi, • • • ,YN denote independent observations, where Yi ~
N (i9,i2a2 ), i = 1, • • • , N. We will estimate 0 by the method of least squares
and obtain the variance of this estimate. First, write this in the form (4.5.1)
with y = (Yi, • * • , Yn);, x = (1, 2, • - - , 7V)', (3 = 0, e = (ei, * * * ^ NY and
V = Var(e ) —
cr2diag(l2, 22, • • • , 7V2).
It follows that the weighted least
squares (WLS) estimate of 9 and its variance are 9WLS = (x/V“ 1x)_ 1x/V
“ 1y =
jj Eili Yi/ii and Var(0WLs ) = v2/N.
Numerical Example 4.3.
Weighted least squares.
industrial establishments of varying size, the number of supervised workers, X,
and the number of supervisors, Y, were recorded for 27 firms (Chatterjee and
In a study of

4.5. GENERALIZED LEAST SQUARES
127
Price, 1991). A plot of Y versus X in the following figure suggests that the
variance of Y tends to increase with the magnitude of X . We show results from
an OLS estimation as well as a WLS estimation, the latter with weights 1/ Xf .
220 H
>-
120 —
%
%
•••
20 A
700
1200
200
1700
X
Weighted least squares.
OLS estimates for Numerical Example 4.3.
Predictor
Estimate
s.e.
t-value
Pr > T
Constant
14.448
0.105
9.562
1.51
0.011
9.30
0.143
< 0.0001
X
WLS estimates for Numerical Example 4.3.
Predictor
Estimate
s.e.
lvalue
Pr > T
Constant
3.803
0.121
4.570
0.832
0.413
0.009
13.45
< 0.0001
X
Note that the R2 value is respectively 0.767 and 0.879 in the two fits, while a
is 21.73 and 0.023 respectively. As mentioned in the earlier numerical example,
the last two columns in each table will become clear after we discuss inference
based on normality in Chapter 7.
A
So far, we assumed that the matrix V is p.d. In some models, however, this
assumption is untenable (see section 10.2). For situations where V is a known
symmetric n.n.d. matrix, the following result, called the generalized Gauss-
Markov theorem, describes optimal estimation of estimable functions of /3. Let

CHAPTER 4. THE GENERAL LINEAR MODEL
128
N be any matrix whose column space C(N) is the same as the null space of X'.
Let D,y denote the class of generalized inverses V# of V that satisfy
r(X'Vt,X)
=
r(X), and
X'VttVN
=
0.
(4.5.7)
Let /3° be any solution to the general normal equations
Result 4.5.3.
X'yttx/?0 = X'V^y.
(4.5.8)
Let c'/3 be estimable. A generalized version of the Gauss-Markov theorem states
that Dy is nonempty and the linear system (4.5.8) is consistent. Further, c'/3°
is an essentially-unique b.l.u.e. of c'/3 in the sense that if do + d'y is any other
linear, unbiased estimator of c'/?, then Var(c'(3° ) < Var(do+d'y), with equality
holding if and only if do + d'y = c' /3° with probability 1.
Suppose r(X) = r, r(V) = v and dim{C(V) nC(X)} = s. Then,
Proof.
ft’ 2) p'
V = P
where Dv is a diagonal matrix of positive eigenvalues, P = (Pi , P2) is an
orthogonal matrix of eigenvectors of V such that the columns of Pi form an
orthonormal basis of C(V) and the columns of P2 form an orthonormal basis of
C±(V). V
” is a g-inverse of V if and only if for arbitrary M and W,
D-1
L
M
W
P'.
V- = P
Also
= pft DoL P';
vv-
if L = 0, the idempotent matrix VV”
is a projection operator onto C(V),
with the path of projection solely determined by L. The construction of f2v
requires a choice of L by which any vector in C(X) is projected by VV- onto
C(X) flC(V).
Let the N x s matrix S be a basis for C(X) n C(V), and the N x (r — s)
matrix R be any extension of S to a basis of C(X), i.e., for any v x s matrix A,
A
QA
0 Qj -
S = PIA, and (S, R) = P
Qi and Q2 are respectively v x (r — s ) and ( N — v ) x (r — s) matrices, and
r(Q2) — r - s. VV«' is a projection along C(X) if VV^'R = 0, which after
some simplification implies Qi = KQ2. This system of equations always has a
solution since the columns of Q2 are LIN. Then fiv consists of g-inverses of V
of the form

4.6. ESTIMATION SUBJECT TO LINEAR RESTRICTIONS
129
1 -D^
K
V#/ = P
P
w
where M and W are arbitrary, and is therefore nonempty. Estimability of c!0
follows from the condition in (4.5.7) (for further details, see Zyskind and Martin,
1969; or Rao, 1973b).
4.6
Estimation subject to linear restrictions
Suppose we wish to estimate the parameters in the linear model (4.1.1) subject
to the linear restrictions imposed by A'0 = b, where A is a known pxq matrix
of rank q, and b is a known ^-dimensional vector. There are many examples
where a “ natural restriction” on the parameters is an integral part of the model
specification; we do not refer to the constraints we had to impose in order to
solve the normal equations when r(X) < p, as for instance, in the one-way
ANOVA example in section 4.2. The linear model without the imposed restric-
tion will be referred to as the unrestricted model We discuss two approaches,
viz., the method of Lagrangian multipliers, which is an algebraic approach, and
the method of orthogonal projections, a geometrical approach.
Method of Lagrangian multipliers
We obtain the least squares solution of 0 under the restriction A'0 = b using
the method of Lagrangian multipliers (see section 2.9).
Let 2A denote a q-
dimensional vector of Lagrange multipliers. We minimize
4.6.1
Sr (0, A)- (y - X0 )' ( y - X0 )+2X' ( A'0 - b)
(4.6.1)
with respect to 0 and A. Setting equal to zero the first partial derivatives of
Sr{0,A) with respect to0 and A results in the following set of normal equations:
§gSr{P,A)
=
0 => X'X/?° + AA° = X'y,
(4.6.2)
0 => A'/3° = b.
(4.6.3)
We will distinguish between the cases when A'/3 is estimable, and when it is not.
First, assume that A'/3 is estimable. We solve (4.6.2) and (4.6.3) simultaneously
for (3® and A°. From (4.6.2) we see that
it
G(X'y - AA°)
GX'y - GAA°
/3°- GAA°,
(4.6.4)
while from (4.6.3) and (4.6.4) we get
A! ft = b = A'0° - A'GAA?,

CHAPTER 4. THE GENERAL LINEAR MODEL
130
so that
A° = (A'GA)“ 1(A,
/0°- b)
(see Exercise 4.20). It follows from (4.6.4) and (4.6.5) that
/3° = /3°- GA( A'GA) ~ l ( A'(3° - b).
We refer to (3® as the restricted least squares solution of (3 in the linear model
y = X/3+ e subject to the linear restriction A'(3 = b. Let yr = X/3}?.
If A'(3 is not estimable, it can be shown that (3® = /3° + (H — I)z, where z
satisfies A'(H — I)z = b-A'GX'y, i.e.,0% is merely one solution to the normal
equations X'X/3 = X'y. Further, SSEr = SSE in this case (see section 7.4.3
for more details).
(4.6.5)
(4.6.6)
The properties of the restricted least squares solution are
Result 4.6.1.
shown below.
1. E((3°r ) =m
2. Cov( /3° ) = <T2G[X'X- A(A'GA)"1A'jG'.
3. E(SSEr ) = { N — r + </)cr2, where SSEr is the restricted error sum of
squares.
Since A'(3 is estimable, A'H = A' (see (4.3.3)). Further E({3° ) =
Proof.
H/3, so that,
E($ ) = H(3- GA(A'GA)-1(A'H/3- b) = H/3,
proving property 1. Next
Cau($ ) = Cov{[I- GA(A/GA)“ 1A']y30}
which, after some simplification, and using (4.2.23) gives property 2. Property
3 follows directly by using Result 4.2.3, since
(y - X#)'(y- X0°r )
[y - X/?° + X(/3° - /3«)]'[y- X0° + X ((3° -#)]
(y - X/3°)'(y - XP° ) + (0° - /3r°)'X'X(/3°- ft )
SSE + { A'0° - b)'(A'GA)-1(A'/?°- b),
X'(y- X0° ) = 0, and A' = T'X (since A'0 is estimable).
SSEr
Suppose Yi = 9i + Ei, i =
,5,
are iid N (0,a2 )
Example 4.6.1.
variables, and 0* are unknown real values subject to the restriction X^
=i
=
100.
We write y = X/3 + e, where, y = (Yi, - - * ,1s)', X = I5, and (3 =
(0i , - - - , 05)', subject to the restriction A'(3 = b, with A = I5, and b = 100.
The unrestricted least squares estimate of (3 is y, while the restricted estimate

4.6. ESTIMATION SUBJECT TO LINEAR RESTRICTIONS
131
(see (4.6.6)) is% = (Yi-Y +20, • • • ,Y5 — Y + 20) with Cov((3r ) = <r2(I-J/5)
and SSEr = (£f=1Yi ~ 100)2/5.
Let Yij =
+ Ti -1- £ij , with E(cij ) = 0, i = 1, • •
Example 4.6.2.
j — 1, • • • , n, and suppose that we impose the constraint Yli=iciri = 0> with
ci 7^ 0 * We obtain the least squares solutions for p, and r = (n, • • • , ra)'
using the method of Lagrangian multipliers. Minimizing
SrM = E“=i
M- n)2 + 2A'E -=1cm
with respect to /z, r and A leads to the solutions /z° = Yli=I ciYi tYli=I cii and
rf =r*. - #A°, i =
, o-
Method of orthogonal projections
We describe the method of orthogonal projections for the full rank case. Under
the restriction A'/3 = b, let LJ denote the ( p — g)-dimensional subspace of the
p-dimensional estimation space fi. Let us denote the projection matrix P —
X(X'X) ~1X/ in fl by PQ, SO that Pay = X/3. It is useful to temporarily
eliminate b which represents an origin choice in w. To do this, suppose /3*
satisfies A'/? = b. Since the projection onto
of a vector already in Q leaves it
untouched,
4.6.2
Pnx/r- x/r- b.
Subtracting X/3* from both sides of (4.1.1), we can write
y - X/r = X(/3 - /3*) + e = X9 + e.
The vector 9 =0 — 0* represents the new parameter vector for which
A'0- A'/3 - A' /3* = A'/3 - b = 0.
The columns of the N x q matrix V — X(X'X)-1A span a subspaceu1- — f1
LJ
of the estimation space Q, which is orthogonal to LJ. This is obvious because
A'9 = 0 implies that A/(X'X) ~ 1X'(X0) = 0. The unique projection matrix in
LJ -1 is
(4.6.7)
= VfV'V^
V' = X(X/X)-1A[A/(X/X) ~iA]“ 1A/(X'X)-1X',
and the projection matrix for LJ is therefore
Pw = Pft - PW-L -
Pw
Then,
Pu,y - P.xr- Pay - PaX/3*- P^ (y- X/3*)
denotes the projection of y - X/?* onto u>.
If (3r denotes the least squares
estimate of (J in the restricted space u>, it is clear that
Pwy = X0r.
(4.6.8)
(4.6.9)

CHAPTER 4. THE GENERAL LINEAR MODEL
132
Similar to (4.6.7), we have
pwx/r = x/?* = b,
(4.6.10)
and further,
Pa,x (y- X/F) = X(X'X)-1A[A'(X'X)-1A]-1(A^- b).
Simplifying (4.6.8) using (4.6.7), (4.6.10), and (4.6.11), we see that the re-
stricted least squares estimate of (3 adjusts (3 by an amount that depends on X,
A, and the distance ||A'/? — b|| and has the form
f3r = (3 - (X'X)"1A(A/(X'X)_
1A]“ 1(A'y5 — b).
This is illustrated geometrically in Figure 4.6.1. We end this chapter with an
example.
(4.6.11)
(4.6.12)
A
/
( Y-Yr ) ' ( Y-Yr )
//
/
\
/
/
/
/
"CO /
——
•" _ A
X $r
/
/
Q
Figure 4.6.1.
Geometry of restricted least squares.
Let A and B denote two n x n n.n.d. matrices. The inequality A < B
implies the usual nonnegative partial ordering of matrices, i.e., B — A is n.n.d.
The following example shows that projection (under the usual inner product in
Definition 1.2.1) onto any subspace of S C 7Zn is at most the projection onto S
(Mathew and Nordstrom, 1997).
Let X be an n x m matrix, and let P = X(X;X) X'
Example 4.6.3.
denote the orthogonal projection matrix onto the column space C(X). For a
k x n matrix A such that AX = 0, we will show that
A'(AA')-A < (In - P).
From Result 2.6.3, C(In - P) is orthogonal to C(X). By assumption, C(A') is
orthogonal to C(X) as well. Hence, C(A') C C(In — P). The matrix A/(AA/)~A
(4.6.13)

Exercises
133
denotes the orthogonal projection onto C(A'), so that (In — P) — A/(AA
/)” A
is also a projection matrix, which is n.n.d. (see Result 2.6.7), yielding (4.6.13)).
Note that equality is achieved in (4.6.13) if and only if C(A') = C(In — P).
Notice also that for k x n matrices Ai and A2 satisfying AiX = A2X, we have
(Ax - A2)'[(A1 - A2)(AI - A2)/]-(A1- A2) < (In - P).
This is seen by setting A — A\ — A2 in (4.6.13).
Exercises
4.1. Verify the consistency condition for existence of a least squares solution,
i.e., r(X'X, X'y) = r(X'X).
4.2. Show that
(a) XXH = X'X, where H = GX'X.
(b) IN
XGX' is symmetric and idempotent.
4.3. When an object of unit mass is subjected to an unknown force 0 for
a length of time t, its position changes to t29 / 2.
Observed positions
Y\, • * * , YN are measured at known times £1, • • • , tjsj. Assume the linear
model Yi = t26 /2 + £*, i = 1, • • • , iV and that the measurement errors
have E(ei ) = 0 and Var(ei ) = a2. Obtain the least squares estimator of
the unknown force 9, and derive the variance of the estimator.
4.4. The displacement Si of the ith object at time ti is given by the formula
Si = vti + £*,
z = 1, - • * , TV,
where all the N objects are subjected to an equal velocity. Assume that
E(£i ) = 0, and Var{ei ) = a2. Find the least squares estimate of the
unknown velocity v and estimate the variance of this estimate.
4.5. Let y = X/? -I- e, where e ~ N ( Q, CT2IN ), and /3 is a p-dimensional vector,
with N = 10 and p = 3. Given y'y = 58, and given the following normal
equations,
0i + 2/?2 - 2/?3 =
2(3i + 2^
2 +
=
— 2/5i + 02 + 6/33
=
9,
4
7
find the least squares estimates of 0 and o2. What are the estimates of
Pi, 02, 0s, 0i ~ 02, and 0i + /?3 ?

CHAPTER 4. THE GENERAL LINEAR MODEL
134
4.6. Suppose we have Yij = Pi + Sij, j = 1, - • • , ni, F2j = /?i + 02 +
j = 1, - • • ,ni, and y3j = /?i — 2/32 +
j = l, - - - , n2, where
are
iid 7V(0,a2) variables. Under what condition on ni and n2 are the least
squares estimates of
and /?2 uncorrelated?
4.7. (Rao, 1973a, p. 236). Let X = [xo, xi, - - - , Xfc_i, Xfc] = [W, Xfc], and let
r(X) = Jfc + 1.
(a) Show that |X'X| = |W'W|(x
^
xfc-x'fcW(W'W)"1W/xfc).
(b) From (a), deduce that |W'W|/|X'X| > 1/x^
x*. Use this to show
that in the usual linear model y = X/3+ e, Var(0k )
<x2(x'fcx/c)_1,
with equality holding if and only if xJ^Xj = 0, j = 0, 1, • • • , k — 1.
4.8. Consider least squares estimation in the simple linear regression model
(4.1.6).
(a) Prove that Var( /3o ) is a minimum if the observations on the predictor
variable
i = 1, • • • , N are chosen such that X = 0.
(b) If the Xi s can be selected anywhere in the interval [a, 6], and if N
is an even integer, show that Var( /3i ) is minimized if N/ 2 values of
the predictor are selected equal to a and the remaining N/ 2 values
are chosen to equal b.
4.9. Let Yi, • • • ,1a denote the yield of a production process on six consecutive
days. Machine A was used on days 1, 3, and 5, while Machine B was used
on days 2, 4, and 6. Consider the models
(i) Yi = 0o +
+ £i,
(ii) Yi =0o + (— l)l/?i + Z/32 4- Si.
In each case, assume that E(£{ ) = 0, and Var(ei ) = a2. Obtain the
least squares estimate of (3\ under^each model, and show that the ratio of
Var( /31) under Model (i) to Var( fii ) under Model (ii) is 32/35.
4.10. In (4.2.32) with n = 1, show that it is not possible to carry out inference
on the effects, although it is possible to obtain point estimates.
4.11. Prove Result 4.3.2.
4.12. In the general linear model (4.1.1), prove that all linear functions c'/3 are
estimable if and only if the columns of X are LIN.
4.13. In the model (4.1.7), show that rx is not estimable for any i.
4.14. In the model (4.2.31), show that Y^
=i
= o-
4.15. In Result 4.4.3, show that ( N — r )a2 is the unique nonnegative quadratic
unbiased estimator of (X-r)cr2, with minimum variance when the nonzero
diagonal elements of the projection matrix P are equal.
CiTi is estimable if and only if

Exercises
135
4.16. SupposeYu denotes a pre-treatment score and Y*2 denotes a post-treatment
score on the 2th individual, i = 1, • • • , N. For i = 1, • • • , N and j = 1, 2,
let E(Yij ) = Tj , Var(Yij ) = cr2, and Corr(Yu,Ykz ) = p if i = fc, and 0 if
i ^
k.
(a) Estimate the parameters ri and r2 in this linear model.
(b) How will you obtain the estimated standard errors of the estimates
in (a)?
4.17. In the model (4.5.1), show that PGLS and /3 coincide when
(a) y
C(X), or
(b) V = (1 — p)I 4- pJ and y is orthogonal to X, the first column of X
being 1N.
4.18. Show that c'/3 is estimable in the model (4.5.1) if and only if it is estimable
in the model (4.5.3).
4.19. In the model (4.5.1), show that
(a) (X'V_ 1X)/3 is estimable.
(b) c' f3 is estimable if c(X,V-1X)"(X'V-1X) = c'.
4.20. In (4.6.5), show that (A'GA)-1 exists.
4.21. Suppose Yi, Y2, and Y3 are measurements of the angles of a trianglesubject
to error. The information is given as a linear model Yi = 6i 4-
where
0i are the true angles, i — 1, 2, 3. Assume that E(el ) = 0, and Var(ei ) =
a2. Obtain the least squares estimates of 0* (subject to the constraint
0i + 02 + 03 = 180°).
4.22. Consider the model Yij = p, + ri + Sij, j = 1, • • • , n, i = 1, • • • ,a. Con-
struct a set of (a — 1) orthogonal contrasts in (7*1, • • • , ra). What is the
relationship between SSRC = n
— Y..)2 and the sums of squares
due to these contrasts?
4.23. In the model Yi = /?o + (3\Xi + e*, 2 = 1, • • • , N, suppose the X^
s are
restricted to lie in the interval [— 10,15]. Choose the Xi s to minimize (a)
Var( f30), and (b) Var((31). Repeat the exercise assuming that the XiS
are restricted to lie in the interval [— 10, 10].
4.24. Consider the model y = (3-f e, e ~ N (0, <J2V), where V is an N x N p.d.
matrix. If z = By, and 77 = Be:, where B = V-1/2, show that the linear
model can be expressed as z = B/3 -b 77, with 77 ~ iV(0, (72Iiv). Obtain
the least squares estimate of /3. Also obtain the estimate of /3 under the
restriction C'/3 = 0, where C is a q x N matrix of rank q < N.
4.25. Consider the model y = X/3 + e, where

CHAPTER 4. THE GENERAL LINEAR MODEL
136
(
0
1
0\
1
0
1
-1
1 -1
1 -1
1
/YA
01
y2
, X =
0 =
02
y =
Y3
03
w
and e ~ N (0,a2I ). If possible, obtain the b.l.u.e.’s of (i) Pi +03, and (ii)
02, and compute their variances.
4.26. Show that || y - yr ||2=|| y- y ||2 + || y -
||2, where y = X/?° and
yr = X/?°.

Chapter 5
Multivariate Normal and
Related Distributions
This chapter describes distributions that are at the heart of linear model theory.
We begin with the definition and properties of the multivariate normal distri-
bution, and then define various distributions such as the chi-square, F > and t
distributions that are derived from the normal distribution. Using these, we
are able to describe the distributions of functions of quadratic forms in normal
random vectors, which form the backbone of statistical inference in linear model
theory. In section 5.5, we highlight some distributions that are useful as alter-
natives to the multivariate normal distribution in a contemporary treatment of
linear models. These distributions accommodate anomalous observations better
than the normal distributions. We begin with a brief review of basic concepts
of distributions of random vectors in section 5.1.
5.1
Multivariate probability distributions
Consider n random variables Xi , X2, • • • , Xn with realized values x\, #2, • • • , xn.
The cumulative distribution function (cdf ) of the n-dimensional random vector
x = (Xi, X2, -.- ,*»)' is
F(x) = F(xi,x2, -
,xn ) = P( X 1 < X i , X2 < x2 ,
,xn < xn )
(5.1.1)
and the joint probability density function (pdf), provided it exists, is
dn
(5.1.2)
/(x) = f ( xi , x2 , - - - , x„) =
F( X l , X 2,- - - , X n )
dx\ • • •dxn
where /(#1, 0:2, * * * ,^n)
0, for — 00 < X { < 00, i — 1, • • , n, and
00
00
f
f
f ( xi , x2,
, xn )dx1 • • - dxn = 1.
—
OO
— OO
137

CHAPTER 5.
MULTIVARIATE NORMAL DISTRIBUTION
138
The marginal pdf of the subvector (X^+i, • • • , Xn)' of x is obtained from the
joint pdf f (xi, • • • , xn ) by integrating out ( Xx , • • • , Xk ) as
oo
oo
fk+1,•••,n(®f c+l > ’ * *
> ^n) —
, Xfc,
? ' ' ' 1
• * • dXfc.
— oo
— oo
(5.1.3)
The marginal pdf of the subvector (Xi, - - * , Xfc) is defined similarly and de-
noted by /i,...
• , rcfc) * Note that we can use this definition to obtain the
marginal pdf of single components of the vector x; we will use the notation
fi( xi ), i = 1,
• • , n to denote these marginal densities. The conditional pdf of
( Xi , • • • , Xk ) given (Xfc+i = Xk+I, " " " , Xn = xn ) is defined as
f { X u X 2,-' - , X n )
.
(5.1.4)
9( 1,—
i ‘
* i x n )
/fc+1, * * * ,n{ xk+l •>
* *
*
i #n)
The conditional density function of (ttfc+i, * * *
> xnl^i, * * * ,%k ) is defined simi-
larly.
The moment generating function (mgf ) Mx(t) of a random vector x with pdf
/(x) is defined as
oo
oo
/
' 7
e x p(t
Mx(t)- £[exp(t'x)] =
' x ) f ( x i , - - ‘ , xn )d x f - dxn
(5.1.5)
—
OO
—
OO
for t'= (ti , • • • , in), — h < ti < h, i = 1, • • • , n, and for some positive real number
ft, provided the integral on the right side of (5.1.5) exists. For two cdfs all of
whose moments exist, we have FXl (u) = FX2(u) if and only if MXl (t) = MX2(t),
which shows how a distribution can be characterized. This leads to the well-
known moment generating function technique for determining the distribution
of a “ new” random variable, which we use in various places throughout this
chapter. Given the mgf about the origin of x, the cumulant generating function
(cgf ) is defined as
£x(t) = log[Afx(t)].
We say that (Xi , • • • , Xn ) are mutually independent if and only if
F(x) «n*(«.)
(5.1.6)
2=1
for all x e H n , where Ft ( x i ) denotes the zth marginal cdf, or equivalently, if
Mx(t) = JjMx.fr)
(5.1.7)
2=1

5.1. MULTIVARIATE PROBABILITY DISTRIBUTIONS
139
for all t in an open rectangle in 7Zn which includes the origin. Note that functions
of independent random vectors are themselves independent.
If Mx(t) exists, then moments of all orders exist. The rth moment of Xi
whose marginal pdf is fi ( xi ) is given by
oo/
oo
oo
A fi { xi )dxi = j
J
(r) = E{ Xl ) =
%i f (*^i » * * *
} Xjijdxi • • •dx-ft .
Pi
— oo
— oo
— oo
(5.1.8)
The mean of Xi is the first moment,
(i ) = E{ Xi ).
(5.1.9)
Pi = Pi
We let n' = ( JJ,i, • • • , /xn) denote the mean of the random vector x, i.e.,\x = E( x ).
The variance of Xi is
<j\= /42
) - n\ = Var( Xi )
(5.1.10)
and the standard deviation of Xi is the positive square root of the variance and
is denoted by a*. The covariance Cov( X{, Xj ) between Xi and Xj, for i ^
j is
/ / (^z ~~ Pi ){ x j
~~ Hj ) fij ( x l , X j )dxidxj
&ij
oo
oo
(5.1.11)
—oo—oo oo
oo
f
’
S
Pi )^S^ j
Pj )/
"(*^i j ' * *
? XTI)C£XI • • •dxn.
— oo
— oo
The variance-covariance matrix of x = (Xi, • • • , Xn)' is defined by E = { ctij }
where the elements Oij for i , j = 1, • • • ,n are given by (5.1.11). The ith diagonal
element of E is Var( Xi ), and the (i, j)th off-diagonal element is Cov( Xi.Xj ).
The variance-covariance matrix E is a nonnegative definite matrix, since for
nonzero t, t'Et =
i UjViVj
t'x = 0. Clearly,
— Far(t'x) > 0, with equality only if
#(x)][x- E'(x)]'}= E(xx') - E( x)E{x' ).
E — Cov( x) = I£{[x
(5.1.12)
The correlation between Xi and Xj is c%j / OiOj for i, j = 1, • • • , n; the corre-
lation matrix of x is
r^j
i
J = D-1ED_1,
(5.1.13)
R =
where D = diag(cr1, •
• , an ) . The correlation matrix R is a symmetric, non-
negative definite matrix with unit diagonal elements. Suppose x is partitioned
as
x'= (x;, x'2, - - - , x'M)

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
140
where x* is a ^-dimensional vector, qi > 0, and YliLi Qi = 71• Suppose the mean
vector fi and the variance-covariance matrix E of x are partitioned conformably,
so that E( xi ) is the ^-dimensional vector\Xi, and Cov( xi, Xj ) = E^, which is a
(<lixQj ) matrix. We say x* and Xj are uncorrelated if and only if Covfa, Xj ) = 0.
Result 5.1.1. Transformation. Let x = ( X\, • • • ,Jn)' be an n-dimensional
continuous random vector with pdf /(x), which is positive in a domain X>x C Rn.
Let y = (Yi, • • • , Yn)' denote another n-dimensional vector. Let
Y = 9i{ Xi,
i = 1, • • • , n
denote n real-valued one-one transformations of the n variables Xx , • • • , Xn with
inverse transformation
Xi ~ g*{Yi, • • • , Yn),
i = 1, * “ , n.
Assuming that the functions are differentiable, the pdf of y is given by
/(y) = /(x)J(y)
where J(y) = |det(J)| denotes the absolute value of the Jacobian of the trans-
formation, with
dXi
d X i
dYx
9Y2
d X x v
ay„
ax
ax
ax
J =
dYy
QY2
9Yn
dxn
axn
axn
\ avi
av2
“
*
dYn /
and det(J) denotes the determinant of the matrix J.
Proof. Let A and B be some subsets of 1Zn. We know that P{x e A) =
fA f ( x )dx. Suppose that each x eA is mapped by the transformation into a
vector y GB, and the inverse transformation maps each y GB back into X G A
Using results on multiple integrals, we see that
//(x)dx = //[#i(Yi, • • • ,Yn ), • • •
* * * ^
n)R(y)^y.
.4
S
This holds for all A C 7Zn\ the right side of the above equation is the pdf of y,
which completes the proof.
The class of nonsingular linear transformations, which is a simple case of
Result 5.1.1, plays a special role in the theory of linear models. Next, let us
define linear transformations of a random vector x with pdf /(x). Let T be a
nonsingular matrix and suppose
(5.1.14)
y = Tx

5.1. MULTIVARIATE PROBABILITY DISTRIBUTIONS
141
where y = (Yi, • • • , yn)'. Then, x = T xy, and the Jacobian of the transfor-
mation is |J| — 1/| det(T)|. The pdf of y is
My) = /(T-1y)|J|.
(5.1.15)
Now, let x and y be respectively n-dimensional and m-dimensional random
vectors. The covariance matrix of x and y is defined as
Cou(x, y) = E{[x- £(x)][y- £(y)]'} = {Cou(y, x)}'.
(5.1.16)
Let A and B be p x n and q x m matrices respectively. Then,
E{ [Ax - E( Ax )]\By - E( By )]' }
AE{ [x - E( x )} [y - E( y )]' }B'
ACcw(x, y)B'.
Cov(Ax, By)
(5.1.17)
For further details and examples, the reader is referred to Casella and Berger
(1990) or Mukhopadhyay (2000).
If xi, •
• , xn are independent Ar-dimensional random vectors with a common
variance-covariance matrix E, then the ” rolled out” vector of the x*’s, viz.,
y = (x'x, • • • , x^
)' — uec(xi , - - - , xn) (see section 2.8) has variance-covariance
matrix
/E
0
• • •
0
E
•
•
0
Cov( y ) =
\0
0
• • •
E
This vectorized version is useful in a discussion of multivariate linear model
theory, including multivariate time series.
We conclude this section with two results useful for evaluating integrals that
arise in the context of multivariate normal distributions (see section 5.2). From
integral calculus, we recall that for a scalar variable x,
oo
J
exp{ ~
^
x2 }dx
VTK ,
— oo
oo
J xexp{ —
^
x2 }dx =
0, and
— oo
oo
J x2 exp{—
^
x2 }dx =
\JTK.
(5.1.18)
— oo

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
142
Result 5.1.2. Aitken’s integral.
and let x = ( X\, • • • , Xn )' be an n-dimensional vector. Then
Let A be an n x n p.d. symmetric matrix
oo
oo
/ -/
exp{--x'Ax}dx = (27r)n/2|A|-1/2.
(5.1.19)
—
OO
—
OO
Proof. Since A is p.d., by Result 2.4.5, there exists a nonsingular matrix
P satisfying A = PP'.
Letting Q = P~ x , we see that QAQ' = In, so that
IQAQ' I = |Q|2|A| = 1, and hence |Q| = |A|-1/2. Under the transformation
y = P'x, we have x'Ax = y'QAQ'y = y'y and
oo
oo
/
•
•/
oo
oo
J
• • • J IQ|exp{-iy'y}dy
1
exp{— -x Ax}dx
— oo
— oo
— oo
— oo
OO
OO
n
J
J
exp{-i]fj/2}<*/i
- dyn
-1/2
|A|
— OO
— OO
OO
\y2
i )dyi
|A|-V2
eXP{~
2
OO
(27r)"/2|A|-1/2
using (5.1.18).
Result 5.1.3. General integral evaluation theorem. Let ao and 6o denote
scalars, let a and b denote n-dimensional vectors of constants, let A be an n x n
symmetric matrix of constants, and let B be a p.d. matrix of constants. For an
n-dimensional vector x = ( Xi , • • • , Xn )' , the integral
OO
OO
1=/
. . . J
(x'Ax + x'a -1- a0 ) exp{-(x'Bx -f- x'b 4- 6o)}dx
(5.1.20)
—
OO
— OO
is evaluated as
=
^
7r"/2|B|-1//2{£r(AB-1) - b'B^a + ib,B
_ 1AB
“ 1b + 2a0}
x
exp{
^
b'B
_ 1b - bo }.
I
(5.1.21)
It is easily verified (by expanding the right side in each case and
Proof.
simplifying to the expression on the left) that the exponent in (5.1.20) can be
written as
x'Bx + x'b + 60 = ~ (x +^
B-1b)'(2B)(x +^
B
_ 1b) -
^
b'B^+ b0
1
l
l
(5.1.22)

5.1. MULTIVARIATE PROBABILITY DISTRIBUTIONS
143
while the nonexponent can be written as
(x +
^
B-VAfx + iB^b)
+
x'(a- AB-1b)-ib'B_
1AB-1b + o0.(5.1.23)
Substituting (5.1.22) and (5.1.23) into (5.1.20), we see that
x'Ax 4- x'a 4- ao
[exp{ jb'B 1b- 60}]
I
oo
oo
x [/
.../ (x
1
— c)'A(x — c) exp{— -(x — c)'R(x — c )dx\dx2 • • - dxn
4' j
• • * J x'dexp{— i(x — c)'R(x — c) }dxidx2
* • •dxn
-ib/B~1AB'"1b 4- ao) exp{-i(x- c)'R(x- c ) }dx\dx2 • • •dxn]
v
v
Tt
Z
—
OO
— oo
[exp{ib'B-1b- &0}][A + h + /3], say
— OO
— OO
00
00
— 00
— 00
00
00
/ / <
+
(5.1.24)
where we define c = —|B *b, d = a — AB *b, and R = 2B; note that R is
p.d., since B is. Then,
00
00
f
f (x — c)'A(x — c) exp{—|(x — c)'R(x — c)}dx.
/1 =
— 00
— 00
Since R is p.d., there exists an nxn nonsingular matrix P such that R — PP;.
Let z = P'(x — c), so that x = (P')"^ 4 c, and the Jacobian is
J = |p-i| = ]p|~i = |R|-1/2.
We see that
oo
oo
J
* * • J {z'P_1A(P/)_1z}exp[ 1
/
~-zz]|J|dz
iz'zjdz,
2
J
h
— oo
— oo
oo
oo
i R r1/2E w^/
• •/
zzZj exp{—
* >3
— oo
— oo

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
144
and further simplification yields
oo
X>/^
2
oo
exp{-
^
}dzlU f exp{-\zl }dzs
s^-oo
|R|-1/2
/1
Z
— OO
00
E,wv /
00
i }^i J z
1
j6Xp{ — Zj^
dZj
-1/ 2
+
|R|
z« exp{--2;
—
OO
— 00
00
x n / exp{~
SytiJZ.00
Finally, we simplify this expression to get
= |Rr
1/2^
ioii(27r)1/2(27r)(n-1)/2 = (27r)n/2|Rr1/2tr{P-1A(P')-1}
i
=
(27r)n/2|R|_1/2fr{A(P')
""1P
_1} = (27r)n/2|Rr1/2tr{AR“ 1}
= i 7rn/2|Br1/2<r(AB
~ 1).
ml
Since x'd = YA=1 Xidi > ^2 can be written as the sum of n integrals, so that,
h
(5.1.25)
OO
OO
J
• • • J exp{— “ (x — c)'R(x- c)}dx
— OO
OO
J
J d'(x — c) exp{-
^
(x- c)'R(x- c)}dx
d'c
/2
— OO
OO
+
— 00
— 00
Simplifying further, we see that
00
00
(d'c)/
"/ expt-
^
— 00
— 00
00
00
J
•
• J {d,(P,)-1z}exp{-iz'z}|./|<iz
1 ,
-zz][ J\dz
h
+
— 00
— 00
„
00
n/
i=1-oo
exP{-\zi )dzi
-1/2
(d'c)|R|
n
°°
5> /
z— 1
00
«p{-^
}*Jl/
exp{
s^
z-oo
i7r"/2|Br1/2b'B-1(AB-1b -
-1/2
+
1*1
•?»
— OO
-1/2(27T)n/2
(d'c)|R|
a).

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 145
We evaluate
(— ib/B-1AB-1b + a0)(27r)n/2|Rr1/2
7rn/2|B|_
1/2(a0-ib'B-1AB-1b).
Substituting these expressions for /1,/2, and /3 into (5.1.24), we get the required
expression for I.
/3
5.2
Multivariate normal distribution and prop-
erties
The multivariate normal distribution is the most widely used distribution to
characterize the probabilistic behavior of a fc-dimensional random vector x =
(Xi, - * - , Xk )' > In this section, we define this distribution, and describe some
important properties associated with this distribution. The normal distribution
forms the basis for the development of the classical theory of linear models and
multivariate analysis. We begin with the definition of the standard multivariate
normal distribution.
A random vector z = [Z1, • • * , Z^ )' defined on
Definition 5.2.1. iVfc(0,I).
!Zh has a multivariate standard normal distribution if and only if
1
/
exp --z z
1
/(*)
(5.2.1)
(27r)*/2
2
and we say z ~Nk (0,1).
The mean of this distribution is E( z ) = 0, and its variance-covariance matrix
is Cov(z) = Ik. It is easily verified that /(z) > 0 for all z
7Zk , and using
Aitken’s integral in Result 5.1.2, we can easily see that f^
/(z)dz =1,
so (5.2.1) is a proper pdf.
Result 5.2.1. Let z ~ Nk (0,1). The mgf of z is
exp{it't}
Mz(t) =
(5.2.2)
for t = (f j, •
• ,t/c ) in TZk.
Proof. From (5.1.5),
00
00
/
• •/ “
p(*
•E{exp(t'z)} =
'z)/(z)dz
M.(t)
— OO
— OO
OO
OO
<2»W/
"7
exp{*'*
1
/
-zzjdz.
— OO
— OO

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
146
By application of Aitken’s integral (see (5.1.19))
oo
oo
/
• / “ p(
1
-
^
[(z - t)
/(z - t)] +^t't}dz
M,(t)
(27r)fc/2
exp{
^
t't}, t e K k
&
— oo
— oo
(5.2.3)
follows directly.
It can be shown that if Xi, • * • ,Xfc are mutually independent normal random
variables with respective means /21, - • • , /2^, and a common variance cr2, then
x = (Ai, - • • , Xfc) ~ Nk (n, (T 2I ), where n= (MI, - " "
Result 5.2.2. Let z ~iVfc(0,I), let 7 = (71, - • • ,7*)' be a nonzero vector of
constants and let 70 be a scalar constant. Then the univariate linear function
of z defined by X = 7'z 4* 70 = ^2i=i747o has a X(7o, 7
/7) distribution.
That is, any linear function of a standard multivariate normal vector itself has
a normal distribution.
Proof. The mgf of X is given by Mx {t ) = E [exp{tX }] = £
,[exp{£(7'z47o)}]
which we evaluate using Aitken’s integral as
00
00
/
•/
exp{70f}
1
/
exp{£7 z --z z}dz
Mx(t )
(27r)fc/2
— OO
— OO
=
exp{70« 4-
From (A4), this is seen to be the mgf of a N (7o,7
/7) random variable.
(5.2.4)
In general, let x = (X i, - - * , X*)', /2' = E(X i, - - - , Xk ) = (/xi, • • • , /2*), and
the n.n.d.
matrix E —
iF{(x — /2)(x — /2) } = Cov( x) with r(E) = r < k.
Suppose E is factored as E = TT, where F' is a fc x r matrix of rank r (see
Result 2.2.1). Then x ~ iV*
;(/2, E) if and only if the cdf of x is the same as the cdf
of r'z 4- /2, where z ~iVr(0,1). This result holds since, if x is to have the same
cdf as T'z 4 /2, then E(T' z 4 fi ) must coincide with 2£(x) and Cov(T' z 4 /2) with
Cov( x). We can verify that these are in fact true, i.e.,
£(r'z + /x) = r' E( z ) + n= li=E(x)
(5.2.5)
and
Cov(T' z + /2)
=
E [r'z +
* jt2-JS(r'z 4 /2)][r'z + ^-#(r'z 4 ^)]
7
=
E[(r'z)(r,z)
/] = r'£?(zz')r = r'r = E =C<W(X).
(5.2.6)

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 147
Let x ~ Nk { fJ>,E) with r(E) = r < k. Then
Result 5.2.3.
Mx(t) = expft'/Li+it'Et}, t G lZk.
Cd
Proof. From the discussion above this result, we see that both x and T'z -1- /z
have the same cdf, and hence the same mgf. Therefore, Mx(t) = Mr'Z+M(t) =
E[exp{t'(r'z -f /z)}], so that
(5.2.7)
OO
oo
exp{t'/z}
1
'
exp{t'r'z — ^z'z}dz
-hz - rt)'(z - rt)+h'rTt}dz
z
z
Mx(t)
(5.2.8)
fc/2
4o ^
— OO
oo
oo
/ /
e x p{
exp{t'/i}
fc/2
(2TT)
— OO
— OO
from which the result follows using Aitken’s integral.
A random vector x = (Xi , *
* * , X*); in 7lk
Definition 5.2.2. iVfc(/z, E).
is said to have a multivariate normal distribution if and only if, for a p.d.
covariance matrix E, and for /z G lZk, the pdf of x (with respect to Lebesgue
measure) is given by
1
exP{-
^
(x ~ M)'S
X(x — /z)},
x
TZk.
(5.2.9)
/(x;^, E) = (27r)fc/2|E|1/2
The mean of this distribution is E( -x.) = /z, and its variance-covariance matrix
is Cov( x.) = E. It is easy to verify that /(x; /z, E) > 0 for all x G7Zk, and that
fZ • • •JZ /(x5
£)dx = 1 (using Result 5.1.2). We directly derive the mgf
Mx(t) from /(x; /z, E) as follows:
OO
oo
f
f exp{t'x}
1
1
exP{~ 2^
x
Hx-^ldx
(27r)
“ fc/2|E|
_ 1/2 J
• • • J exp{— “ X/E
~'1x + (E
_ 1/z -h t)
;x
i /z'E
"V}dx
oo
oo
(27r)-fe/2|E|
... J exp{
Mx(t)
(27T)fc/2|E|1/2
— OO
— OO
oo
oo
— oo
— oo
1
“ [g (x - M - S t/E 1(x — /Z — Et)]
-oo
— oo
+
t'/z + it'Et}dx.
Z
Let y — x- /z- Et. From Result 5.1.2, we see that
Mx(t) = expjt'/H-lt'Et}, t G
(5.2.10)

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
148
The cumulant generating function (cgf ) /Cx(t) of x is
i
fe
^
k
k
iog{Mx(t)} = t'n + -t'Et
(5.2.11)
i=1
i=lj=l
= 0, when J2i=iri > 2. In particular, /Ci,o,... ,o
so that /Cfj t
/Co,... ,o,i = Mfc > /C2,o,- ,0 = tfiir
*
* 5 /Co,... ,0,2 = 0**, etc.
= Mi, - - - ,
—
Let x = (Xi, X2)', p' =
where — oo < X* < oo, — oo < pi < oo,
Example 5.2.1. Bivariate normal distribution.
o\
paxa2
P&1&2
<j\
(Ml » M2), and S =
Gi > 0, for 2 = 1, 2, and \ p\ < 1. Then,|E| = <T2(T2 (1 _ P2 ) and
l
P
*2
— pv\o2
1
1
E'1 =
*1
1
- 1-p^
P
<T?<7$ ( 1-p2)
— p<J\(J2
&1&2
We will obtain the form of the pdf of x starting from two iid iV(0, 1) variables Zi
and Z2. Note that E is p.d. since p ^
1. Hence, by Result 2.4.5, there exists a
nonsingular matrix P such that PP
7 = E. The matrix P is not unique, however.
P?l + P?2
P21P11 + M22P12
P11P21 + P12P22
pii + P2 2
P11
M12
P2i
P22
choice of P is: p\x = cr2, P12 = 0, P21 = P&2, P22 = &2 —
P2&2-
Another
choice of P is: P22 =
P21 = 0, pi2 = p<J1, p\i = o\- pVf . Consider the
transformation x = p + Pz, i.e.,
, so that PP
7 =
. One
Let P =
Zi
+ P z2 y
The inverse transformation is z = P
x (x — /x), with Jacobian J — |P
1
|E|-1/2. The joint pdf of Z\ and Z2 is for z G 7£fc,
/(*1^
2) =^
exp{-$(*? + ag)}=^
exp{—|z'z}
so that the joint pdf of X\ and X2 for — 00 < X\ < 00,
00 < X2 < 00 is
• **-*)e-(:;=:)}
1
f ( xi, X 2 )
exp
- Mi
27T|E|i/2
1
1
#2 — M2 s 2
*^1
Ml \ 2
exp[
{(
r + (
27ro-lo-2(l — p2)1/2
£1 — Mi\ / x2 - M2
2(1- M2)
^2
)}]•
(5.2.12)
2p(
)(
<y\
°2
We sometimes denote this bivariate normal distribution for (Xi, X2) by
^2(pi, P2,^i , cr|, p).
The multivariate normal pdf is constant on ellipsoids in fc-dimensional Euclid-
ean space, i.e., (x — p)
/E“ 1(x — p) =c2, for all c > 0. The center of this ellipsoid

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 149
is f.i, its shape and orientation are determined by E, while c determines its size.
Prom the expression in (5.2.12) for the bivariate case, it is evident that the
paths of x values yielding a constant height for the density (called contours) are
ellipsoids. In general, the multivariate normal pdf assumes a constant value on
surfaces where the squared distance (x — /i)'E-1(x — p) is constant, i.e.,
(x — n)'E
x (x — p) = c2.
(5.2.13)
The ellipsoid can be characterized in terms of the eigenvalues and eigenvectors
of E-i, or in terms of the eigenvalues and eigenvectors of the p.d. matrix E
(see Result 2.3.5). Let Ai >
• • • >
> 0 denote the eigenvalues of E and let
vi , • • • , Vfc denote the corresponding normalized eigenvectors. If the multiplicity
rrij of Aj is 1, then Vj determines the direction of the jth principal axis, i.e., the
jth principal axis is in the direction of the jth eigenvector of E, and its length is
2cy/\j, j = 1, • • • , k. However, if rrij > 1, then the corresponding eigenvectors
and hence the corresponding principal axes are not uniquely defined. The center
of this ellipsoid is at p and its volume is proportional to cfc|E~1|. For a fixed
c, if E-1 is a constant diagonal matrix, i.e., E-1 = diag(l/cr,
• • , l/<x), then
(5.2.13) represents a fc-dimensional sphere with radius c/ y/a, while if E_1= I*.,
(5.2.13) reduces to (x — pY ( x — \T) = c2, so that c =|| x — /x ||. If we choose
c2 = xl,oc -> which is the upper (100a)th percentile from a chi-square distribution
with k degrees of freedom (d.f.), we obtain contours that contain 100(1 — a)%
of the probability.
Example 5.2.2. We discuss contours of the bivariate normal density defined
in (5.2.12), assuming that a\ — a\ (Johnson and Wichern, 1988). It is easy to
verify that the eigenvalues and corresponding eigenvectors of E are
=
CTj(l + p),
A2 = af ( l - p), and
1 /V2 \
-1/V2 J
'
Ai
l / y/2
Vl
V2 =
1 /V2 J ’
When p > 0, Ai is the largest eigenvalue and Vi lies along the 45° line through
the point (pi,P2); the major axis is associated with Ai. When p < 0, then A2
is the largest eigenvalue and the major axis of the constant density ellipse will
lie along a straight line at right angles to the 45° line through (/ii, /i2).
Example 5.2.3. Suppose x has a bivariate normal distribution with mean and
covariance respectively given by
2
1
and E =
M =
1
1
so that we identify p,\ = 2, P2 = 5,
—
\/2, <?2 = 1, and p = l/ \/2. We will
find an ellipse which contains x with probability a, 0 < a < 1. For the bivariate
normal distribution, we can write (x ~ /x)'E_1(x — p) as

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
150
(x^)
2 _ 2p
(*£*) + (^
)l /(1- p2)
so that, in this example, contours of equal density have the form
2{§(X1- 2)2- ( X 1 - 2 )( X 2 - 5) + ( X 2 - 5)2} = c2.
The value of c corresponds to the upper (100a)th percentile of the chi-square
distribution with 2 degrees of freedom (see (A6)) which is given as the value of
u such that the chi-square cdf F (u) is equal to a, i.e.,
c= — 21n(l - a).
By computing the contour for different a values, we generate the elliptical con-
tours of the bivariate normal pdf.
The pdf in (5.2.9) involves k parameters of location, and k( k 4-1)/2 distinct
parameters that characterize E. Thus, for k > 2, tabulation of its distribution
function is a formidable task! Tables corresponding to selected values of p for
a bivariate normal distribution are found in Pearson’s Tables for Statisticians
and Biometricians, Part 2. The evaluation of the A;-variate normal integral
when X j > 0, j = 1, • • • , k is of interest in some problems (see Gupta, 1963).
Although simple analytical results do not exist when k > 3, it is possible to
obtain a simple form of the solution in the special case when all the correlations
are equal.
The existence of the pdf f ( x; p,E) of x with respect to Lebesgue measure
in 1Zk depends on the nature of E. In general, we distinguish between two cases
with respect to the multivariate normal distribution. If E is p.d., then r(E) = k,
and the density of x exists in lZk. Note that x ~ Nk ( p,E) if and only if x =
T'z 4- /x, where z ~ 7Vfc(0,I) and where r(r') = k. The inverse transformation,
viz., z = (r')-1(x - p) exists, the Jacobian of the transformation being |J| =
|r'| = |E|-1/2 (since E = rr'). In this case, /(x; /x,E) which we defined in
(5.2.9) is the pdf of x with respect to Lebesgue measure in lZk. If, on the other
hand, E is not p.d., so that r(E) — r < k, then the inverse transformation z =
(r')_1(x — p) does not exist and no explicit determination of the pdf of x with
respect to Lebesgue measure in 1Zk exists. However, the pdf of x is concentrated
on an r-dimensional hyperplane, and we say x has a singular normal distribution.
We describe this in the following result and then give some examples of singular
multivariate normal distributions.
Result 5.2.4. Let E{x ) = /x, and Var(x) — E, where E is a n.n.d. matrix
of rank r < k. The following definitions of a singular normal distribution are
equivalent.
(i) x is said to have a (singular) Nk ( p,E) distribution of rank r if it can be
expressed as x = /x + Bz, where B is a k x r matrix satisfying BB' = E,
and z ~ Nr(0,1).
(ii) x is said to have a Nk ( p,E) distribution of rank r if

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 151
Mx(t) = exp{t' fi 4-|t'Et}.
(iii) x is said to have a jV*.(/z, E) distribution of rank r if every linear combi-
nation c'x, (c 7^ 0), has a 7V(c'/i,c'Ec) distribution.
Proof. We first show that (i) implies (ii). Letting a' = t'B, we see that
= £{exp(t'x)} = £[exp{t'(/i -1- Bz)}]
—
exp(t'/z) jF{exp(a'z)}= exp{t'/i4-ia'a}
=
expjt'/i-h
^
t'BB't} = exp{t'\x 4- t'Et}.
Next, it follows that (ii) implies (iii) since, setting d' — 6a', we see that
—
£{exp(d'x)} — exp(d'/x 4-
^
d'Ed)
—
exp{b(a.' p) 4-
^
62(a'Ea)}
Z
Mx(t)
#{exp(6a'x)}
which implies that the univariate linear combination a'x has a AT(a'/x,a'Ea)
distribution. We see that (iii) implies (ii) since
-Efexplt'x}] = J5[exp{l.t'x}] — exp{t'^
4-|t'Et}.
Again, (ii) implies (i) since
£
,{exp(t'x)} = exp(t'/i 4-|t'Et) = exp(t' p 4-
^
t'BB't).
Letting a' — t'B, and since z ~ 7Vr(0,1), we see that E{exp(t'x)} = exp{t'/x}x
exp{|a'a} — exp{t' fj,}E{exp(t' z ) } = E{exp(t' p 4- a'z)} = E{exp(t' p 4- t'Bt)}
— E{exp[t' ( fj, 4- Bz)]}, which implies that x = p 4- Bz. For example, the bi-
variate normal distribution with p = 1 is singular. In this case, X\ = a 4- bX 2,
and the distribution is concentrated on a straight line.
Example 5.2.4.
(univariate) normal distribution is obtained only if the variance is zero. The
mgf of the normal random variable X is Mx(t ) = exp( pt ), which is the mgf of
a random variable Y which is degenerate at /x, i.e., P(Y = p) = 1. That is, the
singular normal distribution when k — 1 is a discrete distribution degenerate at
> 0 if and
When k = 1, E corresponds to a scalar, and a singular
#11
042
&12
22
a single value. When k = 2, we can verify that E =
only if an > 0, (722 > 0, and crncr22 — o\2 > 0 (see the proof of Result 2.4.2).
Bivariate singular normal distributions occur under the following four scenarios:
(i) G\\ —
CT22 = 0 (so that (j\2 = 0). Then, x ^A^
MJO), so that x is degen-
erate at p —
( p u P2/ •

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
152
(ii) G\\ — 0, (722 > 0 (so that G\2 = 0). Then, X\ ~ N ( /JI,0), and is degen-
erate, while X 2 ~ N ( 112,022 ) and is nondegenerate. Since a degenerate
random variable is independent of any other random variable, X\ and X 2
are independent.
(iii) an > 0, (722 = 0 (so that <ri2 = 0). This case is similar to (ii), with the
roles of Xx and X 2 reversed.
(iv)
<7n > 0, (j22 > 0, and
<7n <722 = 0i2 - In this case, both X\ and X 2 have
nondegenerate distributions. However, we can verify that all the proba-
bility lies on a line 022^1 —
<712 *^2 = ^22/^1 - 0121*2, since Far(<722^i —
^12^2) = 0, from which it follows that P( c722^1 - ^12-^2 = ^22/^1 —
^12^
2) = 1.
An extension of the discussion for the bivariate case to k = 3 implies that all
the probability lies either at a point, or on a line, or a plane, each of which is a
subset of 71s. For the general
variate case, all the probability lies in any one
of the lower-dimensional subspaces (sometimes called affine subspaces) of 7Zk.
In these cases, it is not possible to write down the pdf of x.
In the proofs of many of the following results, we will use the mgf of the
normal vector x.
There are two reasons for this, the first being that if £
is not p.d., then there is no density function defined on 1Zk , while the mgf
exists. The second reason is that the mgf is a natural tool to use for derivation
of the distributional results that we require.
The next result states that a
linear combination of multivariate normal vectors also has a multivariate normal
distribution.
Result 5.2.5. Reproductive property. Suppose Xj, j = 1, • • • ,p are inde-
pendently distributed as
£j). Then
Yfj=1 aj* j ~
s
2^
)’
where aj, j = 1, • • • ,p are constants.
Proof. The mgf of y = Yjrj— 1 djX - j is
v
V
= n^texp { (ijt' xij } }
=
E
exp < t'Y
My(t)
ajXj
3=1
3=1
Vr
j
= nexp(tajMj +
P
i
p
exp{t'
+ -^
V(a*£j)t}
3=1
3=1
3=1
from which the result follows by noting the form of the mgf of a normal vector
in (5.2.7).

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 153
The following two results are widely used in linear model theory. They state
that if a A;-variate normal random vector is subject to a linear transformation,
then the resulting vector also has a multivariate normal distribution of appropri-
ate dimension. The usefulness of these results may be appreciated by recalling
that in linear model theory, the least squares estimates (which correspond to the
maximum likelihood estimates under normality) of the model parameters are
linear functions of the response vector y. The distributions of such estimators
will be derived in Chapter 7 using the following results.
Result 5.2.6. Let x ~ iVfc(/z, E) where r(E) — r < k. For a given q x k matrix
B of constants and a g-dimensional vector b of constants,
y = Bx + b ~ Nq( Bfi + b, BEB')-
Proof. Let t' = (£i , • • • ,tq ) e 7Zq. Then
—
E{exp(t'y)} = £[exp{t'(Bx + b)}]
=
exp{t'b}E’[exp{t'Bx}] = exp{t'b}Mx(B't)
—
exp{t'(B/z + b)-f|t'BEB't},
which is the mgf of a normal random vector with mean B/z + b and variance-
covariance matrix BEB'.
(5.2.14)
My (t)
(5.2.15)
An immediate consequence is that if x ~ jV*(/z, E), then x — /z ~ 7V*(0, E).
Result 5.2.7. The random vector x ~ iVfc(/z, E) if and only if every linear
combination c'x has a univariate N(c'/z,c'Ec) distribution, where c / 0 is a
real vector. This technique of reducing a multivariate normal distribution into
a univariate distribution is known as the Cramer-Wold technique.
Proof. Let c' = (ci, -
* - , c^), PP' — E, z ~ iVfc(0,I), and d' = tc' P. Then
F^
expftc'x}] = E [exp{tc' ( fi + Pz)}] = exp{tc'/z}E
,[exp{d'z}] = exp{tc'/z-f|d'd}
= exp{tc'(i+^
c' PP'c} = exp{tc'/z+^
t2c'Ec} so that c'x ~ iV(c'/z,c'Ec). To
show the converse, we assume that every linear combination of x has a univariate
normal distribution; in particular, let c' — t' — (ti, • • • , t^). Then,
E[exp{t'x}] = E[exp{l.t'x}] — exp{t'/z+^
t'Et},
which completes the proof.
Example 5.2.5. Let x —
(Xi , X2, X3)' ~ iV3(/z, E). We use Result 5.2.6 to
find the distribution of
X\ — X 2
x2-x3
1 -1
0
0
1 -1
x2 \ .
Bx =
*3
This vector clearly has a bivariate normal distribution with mean

CHAPTER 5.
MULTIVARIATE NORMAL DISTRIBUTION
154
Mi
B"= (J
-1
0
1
-1
_ [ Mi ~ M2
\M2 - M3
M2
M3
and covariance matrix
1
0
-1
1
0 -1
<7ll
<?12
<713
0
*12
<722
0
*23
0
"l3
023
033
BEB' = (J
-1
0
1 -1
Example 5.2.6. Let x = (Xi , • • • , Xn)' ~ iVn(/xln, cr2In) . Note that x repre-
sents a “ random sample” of size n from a normal population with mean fi and
variance a2. Then X =
Xi / n = c'x, wh&e c' = l'/n. It is easily verified
that c' fx = J27=i M/n = M > and c'Ec = 0'2/n * By Result 5.2.7, X is a linear
function of x, and has a iV(/i, a2 jn) distribution.
Definition 5.2.3.
of a random vector x ~ JVfc(/z, S). The covariance between yi and y2 is given
Let yi = Aix and y2 = A2X denote two linear functions
by
Cov {yi , y2) = AIEA'2.
Definition 5.2.3 enables us to obtain conditions under which linear forms
in a normal random vector are independently distributed.
For example, let
x ~ iVfc(/z, <72I).
We can verify that a necessary and sufficient condition for
nonzero linear forms a'x and b'x to be independently distributed is a'b = 0.
Example 5.2.7. Let x ~ 7Vj^(0, cr2I).
We show that yi = Ax and y2 =
(I — A ~ A)'x are independently normally distributed, where A- denotes a g-
inverse of the q x k matrix A. That yi and y2 have normal distributions is
immediate from Result 5.2.6.
We see that Ax ~ Ag(0, CJ2AA
/), while y2 ~
iVfc(0,a2(I - A“ A)'(I — A“ A)). Also,
Cov{ Ax, (I - A-A)'x} = cr2A(I — A-A) = 0
using Definition 3.1.1.
Result 5.2.8. Let x ~
E). Suppose we partition x as x' = (x^
x^
)
7
where x
^
= (Xi , • • • , Xq ) is a ^
-dimensional vector, and x2 = (Xg+i , • • • , X*)
is a ( k — g)-dimensional vector.
We partition
ji and E conformably.
That
is, let
fi'= (M11M2) where
= (f i i , - - - , /xg) is a ^
-dimensional vector, and
EH
E12
£21
£22
M2 = (Mg+i > *
* *
> Mfc) is a ( k — #)-dimensional vector. We let E =
where E
and (k — q ) x ( k — q) submatrices. The marginal distribution of the subvec-
tor xi is Nq( fii , En), while the marginal distribution of the subvector X2 is
^-<?(M2 ) £22) -
E12, E21, and E22 are respectively q x q, q x ( k - q ), ( k - q ) x q,
11 ?

5.2.
MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 155
Proof. We first derive the marginal distribution of xi. Since x has a
E)
distribution, applying Result 5.2.6 with the q x k matrix B = (1^
0) and the
^-dimensional vector b = 0, we see that B/x — /x1? and BEB' = En. It directly
follows that xi has a Nq ( /i i, E n ) distribution. The marginal distribution of
X2 follows similarly by setting the ( k — q ) x k matrix B = (0
I/c-g) and the
( k - <?)-dimensional vector b = 0.
It is clear from Result 5.2.8 that all subsets of a multivariate normal random
vector themselves have normal distributions. The mean and variance of the
normal distribution of a particular subvector of x is, in fact, obtained by simply
selecting appropriate elements from /x and E.
Example 5.2.8. Let x ~ N^
fi, E), where
< 4
1
0
-1\
1
3
0
0
0
2
-12
0
We first note that E is p.d., since all its principal minors are positive. We will
find the distribution of xi = (.X^ X*)'. By Result 5.2.8, we see that xi has a
bivariate normal distribution with /x'x = ( 3
— 1 ) and En =
the off-diagonal elements of En are nonzero, the random variables X 2 and X4
are dependent.
Result 5.2.9. Suppose x ~ A^
/x, E), which we partition as x'= (x
^
, • • • , xfm )
f
Xj being a ^
-dimensional subvector, for j = Im -
pose we partition /x and E conformably. Then, xi,
•
dently distributed if and only if Eij = 0, for i =£ j , i , j = 1, • •
Proof. First, suppose E^ = 0, for i ^
j, x, j — 1, •
• ,ra. Then,
exp-Jt'/x+it'Et}
U4
m
^
m
m
exp
i=l j=l
m
1
= n expWjH +
m
nM*A)’
2
ji' = (2
3
0
— l) and E =
0
V
3
2 . Since
2
5
and
1 Qj =
Sup-
xm are jointly indepen-
• , m
• , m.
Mx(t)
3— 1
3=1
(5.2.16)
3=1
so that, by (5.1.7), xi, • • • , xmare jointly independently distributed. To prove
the converse, if we assume that xi, • • • , xm are jointly independently distributed,
then
= E [{ xi -
- Hj )’ ) = [E( xi -
- Hj )’ ) = 0
(5.2.17)

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
156
which proves the result.
(continued).
We will verify that xi = ( Xi ^ X^ )' and
Example 5.2.8.
x2 = X3 are independently distributed. Once again, from Result 5.2.8, we see
that the distribution of X(i ) = ( Xi , X 2, Xs )
f is a trivariate normal with mean
and covariance given by
2
4
1
0\
1
3
0
.
0
0
2/
3 1 , and £(i) =
A*(i) -
0
, so that xi and x2 are
In the notation of Result 5.2.9, this gives £12 =
independent.
Result 5.2.10. Let x ~ ATfc(/x, £) with r(£) = k. Suppose we partition x
(x
^
, x2), where xi is a g-dimensional subvector, 0 < q < h, X2 is
as x
( k —
<j)-dimensional, and suppose that
f i and £ are partitioned conformably.
The conditional distribution of xi given that x2 = c2 is multivariate normal
with mean vector
Ml.2 = Ml + ^12^22
X(c2 ” M2)
(5.2.18)
and variance-covariance matrix
E11.2 = En — £I2E221E2I.
Proof. Let /(xi , x2) denote the joint pdf of x, and /i (xi) and /2(x2) denote
the marginal pdfs of xi and x2 respectively. The conditional pdf of xx given
x2 = c2, provided /2(c2) > 0 is
(5.2.19)
/(xi, c2)
M(XI|X2 = c2)
/2(^2)
S^
1
Vs
(27r)-fc/2|S|-1/2 exp
^
Xi - MI
c2 - M2
Xi ~ Mi
C2- M2
11
E2I
E22
(27r)-< fc-9)/2|£22|~1/2 exp[-i(c2- /n2)'£22 (c2-
2)]
Substituting for £
“"x using Result 2.1.3 as
Si2\
1
V-1
^11.2
— E22*E2I£1112
^ll!2
E22.IE2I£H
1
where £11.2 = £11
£I2£221£2I and £22.1
S(xi|x2 = c2) = (27r)"g/2|£22|1/2|E|_1/2 exp{-|(<3- Q2 ) }
^11.2^12^22
^22
X
1
“ Eii EI2£22.I
E22V
En
E2I
E22
E22 — E2I£111EI2. We obtain

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 157
where
(Xi - Mi )'S1112(xi - m ) - (xi - /ii)'E1112Ei2E221(c2 - H2 )
-(c2 - /i2)'E221E2iEf112(xi - m ) + (c2 - M2),EJ211(C2 - /i2)
Q
and
Q2 = (C2 -^
2)/S221(C2 - M2)
so that
Q - Q2 = [(xi - Ml ) - Si2Ej21(C2 — M2)]/S]
"112[(XI - Ml ) - Si2S221(c2 ~ M2)]-
From Result 2.1.2,|E| =|E22||£II.2|> so that
5(XI|X2 = C2) = (27r)_
9/2|En.2|_1/2 exp{-'i(xi- Mi.2),Ef112(xi - /uL2)}.
By (5.2.9), this is a normal pdf with mean M1.2 and covariance En.2, which
completes the proof.
Example 5.2.9. Let x = (Xi , X2, X3)' ~ iV3(0, E), where
4
1
0\
1
2
1 .
0
1
3/
E =
We illustrate the computation of marginal and conditional distributions as well
as distributions of a linear combination of the components of x. By Result 5.2.8,
we see that X\ ~ TV(0, 4), X 2 ~ iV(0, 2), and X3 ~ N (0, 3). The marginal
distribution of the vector (X2, X3)' is bivariate normal with mean vector and
covariance matrix given respectively by
From Result 5.2.10, we see that the conditional distribution of (Xi|X2 =
X3 =
x3) is univariate normal with mean and variance given respectively by (see
(5.2.18) and (5.2.19)):
(3x2 — x3)/5, and
17/5.
To find the distribution of a linear function of x, viz., Y = 4Xi — 6X2 +X3 — 18,
we use Result 5.2.6 with q — 1, B = (4, — 6,1), and b = — 18. Then, y ~
TV(— 18, 79).
E( Xx\X 2 = X 2, X 3 = X3 )
Var( X l\X 2 = x2,X3 = x3 )
Suppose the random
Definition 5.2.4. Simple correlation coefficient.
vector x = (Xi, • • • , X^)' has a TV^(M, E) distribution. The simple correlation
between any two random variables Xi and Xj is defined by
Cov( Xi, Xj )
[V ar( Xi )V ar( Xj)]1/2
y/a
G j j
(5.2.20)
Pij —
ii&jj

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
158
provided an > 0 and ajj > 0. If either term in the denominator is zero, then
Pij is undefined.
For a collection of algebraic, geometric, and trigonometric interpretations of
the simple correlation coefficient, see Rodgers and Nicewander (1988).
Result 5.2.11. The simple correlation coefficient satisfies the inequality — 1 <
Pij
1, with \ pij \ = 1 if and only if there exist constants a ^
0 and b such that
P( X j = aXi + b) = 1. If p^ = 1, then a > 0, and if pij = — 1, then a < 0.
Proof. For every real constant c, we have E [( Xj —
pj ) — c( Xi —
pi )]2 >
0, which implies that the discriminant of the nonnegative quadratic function
Var( Xj ) — 2cCov( Xi, Xj ) + c2Var( Xi ) is nonpositive, i.e., 4 [Cov( Xi, Xj )]2 —
4Var( Xi )Var( Xj ) < 0. This implies that [Cov( Xi, Xj )}2 / Var( Xi )Var( Xj ) <
1, or p2
j < 1, from which it follows that — 1 < pij < 1. This result may also
be proved as a direct consequence of the Cauchy-Schwarz inequality which we
defined in Result 1.2.1. Also, \ pij \ = 1 if and only if the discriminant is equal to
zero, i.e., the quadratic function E [( X j — P j )— c( Xi ~ pi )]2 has a single root. Since
[( X j - p j )- c( Xi - p^
] > 0, the expected value E [( X j - p j )- c( Xz -^
i)]2 = 0
if and only if
P{ [( X j - p j ) - c( Xi - pi )]2 = 0}= 1,
which is equivalent to
P{ [( X j - p j ) - c( Xi - p i )\ = 0} = 1.
That is, P( X j = aXi + b) = 1 with a = c, and b = p j — c p i. The single root of
the quadratic function is c = C o v( Xi, X j ) / V a r( Xi) , so that if p^ = 1, then
a > 0, while if p^ —
— 1, then a < 0.
Result 5.2.12. If Xi and X j are independent random variables, then
Pij — 0.
Proof. If Xi and X j are independent, E( X i X j ) = E( Xi )E( X j ). Hence,
C o v( X u X j ) = E( X i X j ) - E( Xi )E( X j ) = 0,
and using (5.2.20), it follows that pij = 0.
Note that if Xi and X j are uncorrelated, they need not necessarily be in-
dependent. The covariance and correlation measure only a particular kind of
linear relationship between Xi and X j. In Example 5.2.10, we present two de-
pendent random variables with zero correlation. In Example 5.2.1, we defined
the bivariate normal distribution of x = ( X\, X 2 ) with correlation p between
X\ and X <i- In Example 5.2.11, we show that for this normal case, p = 0 does
indeed imply independence of X\ and X^.

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 159
Example 5.2.10. Let the joint pdf of X\ and X 2 be uniform on the square
with vertices (— 1,0), (0,1), (1, 0), and (0,-1). By symmetry of the marginal
distributions, we see that E(Xi ) = E( X 2 ) = 0. Since the contributions from
the domains with XiX 2 > 0 and xix2 < 0 cancel each other in the integral
/^
o x1x2 f ( x1,x2 )dxidx2, we see that E( XIX 2 ) = 0, and hence p\2 — 0.
Yet, these two random variables are dependent.
(a) In the definition of the bivariate normal distribution
o \
. 0 °V '
Example 5.2.11.
for x = ( Xi , X 2y given in Example 5.2.1, let p = 0. Then, E =
1/a2^
’
S0
becomes
|E| = erf a2, and E 1 =
f { xi ,x2 ) = (27r(Jicr2) 1exp{— (xa - pi )2/ 2a\}exp {-{x2 - p2 )2/ 2al )
for — oo < X\ < oo, and — oo < x2 < oo, which is the product of two univariate
normal pdfs (see (Al)), indicating independence of X\ and X 2.
(b) Let x havea k-variate normal distribution with mean vector p and covariance
matrix 1^
.
Consider Yi — a'x, Y2 — b'x, with a'b —
0.
By Result 5.2.7,
y = (YuY2 y has a bivariate normal distribution with mean vector (a.' p,b' p)' ,
a'a
0
0
b'b
since a'b = 0. The joint distribution
of y factors into the marginal distributions of Y\ and Y2, so that Y\ and Y2 are
independent.
and covariance matrix E =
Result 5.2.13.
means pi and pj and correlation pij,
Var(aXi -f bXj ) — a2Var( Xi ) + b2Var( Xj ) + 2abpij^
Var( Xi )Var( Xj ).
(5.2.21)
For any two random variables X { and X3 with respective
If Xi and X3 are independent, then
Var{aXi + bXj ) = a2Var( Xi ) + b2Var( Xj ).
Proof. Since E(aXi + bXj ) = api + bpj , we see that
Var(aXi + bXj ) =
E [{ {aXi + bXj ) - ( api + bpj ) }2 }
=
E [{ a( Xi -m )+ b( Xj - fij )}2]
=
a2E( Xi - m )2 + b2E( Xj - fij )2 + 2abE( Xi - m){ Xj -m )
=
a2Var( Xi ) + b2Var( Xj ) + 2abCov{ Xu Xj )
and we get (5.2.21) using the definition in (5.2.20). Equation (5.2.22) follows
directly from Result 5.2.12.
(5.2.22)
Let x — ( Xi , - • • , XfcY be a random
Definition 5.2.5. Correlation matrix.
vector with componentwise means pi,i = 1, • • • , fc, and pairwise correlations pij

CHAPTER 5.
MULTIVARIATE NORMAL DISTRIBUTION
160
i, j = 1, • • • , k. The k x k symmetric matrix R = { p i j } is called the correlation
matrix of x. Each diagonal element of R is equal to 1, while each off-diagonal
element lies between — 1 and 1.
Result 5.2.14. The correlation matrix R is nonnegative definite.
Proof. The variance covariance matrix E of x is nonnegative definite. Let
D be a diagonal matrix with ith diagonal element
, i = 1, • • •&, where
on = Var( Xi ). We can write R = D'ED, from which it follows that R too is
n.n.d.
Let x = (Xi, X2 , X3, X 4 )' ~ iV4(/x, E) where
/
1\
Example 5.2.12.
(
2
0
1
0
3
0
1
0
5
-1 2
0
2
2
and E =
-1
0
v 3
3
It is easy to verify that pn = p22 - /933 = P44 = 1, P12 = P23 = P34 = 0,
P13 = 1/VW , P14 = -l/\/6, and P24 = 2/3. The correlation matrix is
0
1 / y/ l O
-1/V6\
1
2/3
0
1
0
R =
.
l/ \/H)
0
2/3
0
1
0
1
Definition 5.2.6. Fisher’s ^-transformation of pij .
or the arctanh transformation of p^
is
Szj = i loge [(1+ P i j )/ {l - p i j )} = arctanh(pij).
In Chapter 6, we describe sampling from a multivariate normal distribu-
tion and properties of various sample statistics that arise in this context. In
particular, we will consider Fisher’s ^-transformation of the sample correlation
coefficient and derive its distribution. This finds use in linear model theory for
conducting inference on pi j.
The z-transformation
Suppose the random
Definition 5.2.7. Partial correlation coefficient.
vector x = (Xi, • • • , X/-)' has a Nk ( p,E) distribution, and suppose we partition
x as x' = (x^
x
^
)' where
= (Xi, - - * , Xg) is a ^-dimensional vector, and
x2 = (X9+i, - - - , Xfc) is a (k — #)-dimensional vector. We partition p and E
conformably, as in Result 5.2.8. Let Xj and Xi be components of the subvector
x\. The partial correlation coefficient of Xj and Xi given X2 = C2 is defined by
=
provided the expressions in the denominator are nonzero, and where 0jZ|(9+i,... ,fc)
is the (j, Z)th element in En.2 = En - Ei2EJ2 E2i.
(5.2.23)

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 161
Result 5.2.15.
A partial correlation coefficient defined by (5.2.23) satisfies
— 1
Pij\( q+1,—
,*)
1-
Proof. This follows directly from the Cauchy-Schwarz inequality (see Result
1.2.1).
(5.2.24)
Continuing with Example 5.2.12, we compute the partial
Example 5.2.13.
correlations between components of x. To compute Pi2(3,4) given E, we first
compute the conditional covariance matrix of (Xi, X2IX3, X4) as
-1
22/15
2/3\
2/3
5/3/
*
The other partial correlations may be
2
0
1 -1
0
2
5
0
1
0
0
3
0
3
-1
2
2/3
Hence, Pi2(3,4) =
y/ 22/ lby/bJZ
computed similarly.
Definition 5.2.8. Multiple correlation coefficient.
vector (XQ, Xj,
* • , X*)' has a Nk+i (/i, E) distribution, and suppose
Suppose the random
*0
Mo
000
001
E(!)
, and E =
x =
X(D
M(1)
010
where x^
1) — (Xj, • • • , X*)', fi^ = (/i1, • • • , /i^)', E^
1
^ is the lower kxk subma-
trix of E, a10 = (0'io > • 4
4 0'/co)/ is a fc-dimensional vector consisting of covariances
between Xo and Xi, •
• , X^, and croi = 0’io- The multiple correlation coefficient
of Xo and x^
1
^ is
Cov(W, X0)
{Var(W )Var(Xo)}1/2’
where W = /io + oroi [E^
1
^]_1(x^
1
^ —
fi^ ). A computationally simple formula is
given by
(5.2.25)
Po(i , ~ . ,*) =:
cr0i [E(1)] V10}l/2
A>(i,-,*) = {
(5.2.26)
000
Example 5.2.14.
the multiple correlation coefficient of X\ and (X2, Xa) using (5.2.26). First,
we find the marginal distribution of (Xi, X2, Xa).
From Result 5.2.8, this
distribution is normal with mean and variance given by
We continue with Example 5.2.12, and compute Pi(2,3)
1
2
0
1\
0
3
0 .
1
0
5/
2
and
-1
3
0
Further, an = Var(Xi) = 2, ai2 — (0’i2, 0'i3)/ = (0,1), and E(1) =
Therefore, Mi(2,3) = 0. Next, using (5.2.26), we compute
0
5 /
*

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
162
Pl(2,3,4) — \/3/5.
Result 5.2.16. Suppose (XQ , Xi , • • • , X&) has a iVfc+i(/i, E) distribution, and
suppose it is partitioned as in Definition 5.2.8. Let a2 = cr0o _
c r0i
1
<x i o
denote the conditional variance Var( Xo\Xi = xi, * * * , Xfc = Xfc). Then, the
multiple correlation coefficient can be computed as
= (cr00 - cr2)1/2/0OO
2
-
Po(i,-,fc)
Proof. By definition,
Far(Xo)
=
£[(X0 - MO)2]- aoo
Var(W)
=
- /x(1))}2
=
(Toi ]^
1)]-^^
1) - M
(1))(x( J) - M
(1))'][E(1)]-VIO
=
<Toi[E(1)]_1(Tio, and
Ccw( X0,W )
=
£[(Xo - Mo){/io + ^oi[E(1)]-1(x(1) - M(1)) - M}']
=
£{(*o - Mo)(x(1) - M
(1) )'[E(1)rVio} = a0i[E(1)]-V10.
The proof follows from substituting into (5.2.25) and simplifying.
Result 5.2.17. The multiple correlation coefficient in Definition 5.2.8 satisfies
0 < Po(i,-,fc)
L
Proof. The expression Cov(Xo,W ) in the proof of Result 5.2.16 is nonnegative,
from which it follows that po(i,2,- ,fc)
0- That Po(i,2,... ,fc)
1 follows from
observing that a2 < <roo (since
<7oo - o2 —
croi [E^]-1^ > 0, being equal to
Var(W ) ).
Result 5.2.18. Let y = (XQ , XI , - *
« , X^) ~ Nk+i ( fj,y,Ey).
The following
equation relates the multiple and partial correlation coefficients and holds for
q = 2,3, • • , fc:
2
2
f
2
1 2
P0(l,2,- ,q ) = Po( l ,2t- ,q- 1) + l1 “ A)(l,2,- ,g-l)]P0g|(lf 2,
Proof.
Let y* = (XQ, Xi, • • • , Xg )' denote a (g + l)-dimensional subvector of
y. By Result 5.2.8, y* has a (q+ l)-variate normal distribution with mean and
covariance given by
(5.2.27)
*01
000
M* = (MOiMii • ’ ' iMg)7! and £* =
£*(!)
where cr\0 = (trio, • * •VqoY * *oi — ^lV ) and 2*^ is the q x q covariance matrix
of (Xi , • • • , XQ)'. We partition E* and its inverse A* as
^00
$01
$0q\
A
6iq I ,
3q0
3ql
fiqq /
*00
<?01
CT0q
*10
2
(Tig
*g0
0ql
&qq
^10
E* =
and A
3*' =

5.2. MULTIVARIATE NORMAL DISTRIBUTION AND PROPERTIES 163
where aoo, <x0g, 0go 5 and aqq are scalars, a01 and aq\ have dimension 1 x (q — 1),
<7io > and cr\q have dimension (q-1) x1, and E is a (<?— 1) x (q — 1) matrix. Entries
in corresponding locations in A* have dimensions similar to components of E*.
We consider the conditional distribution of (Xo, Xq\Xi = x\, • • • , Xq _\ = xq-\);
by Result 5.2.10, this distribution is bivariate normal with covariance
£oo
fiqO
fiqqj
1
5qq
— SqQ
( SooSqq - 5lq )\-Soq
^00
^00|(1,2,-,g— 1)
^0g|(l,2,- ,g-1)
0gO|(l,2, " ,g— 1)
agg|(l,2,--- ,g-1)
, say.
By Definition 5.2.7,
2
^0g|(l,2,-
al
)/KO|(1,2,-,g-l )^gg|(l,2,- ,g-l)]
“
"0g|(l,2,- >g-l
=
(- f>0q )2/ [SooSqq}-
q- l )
From Result 5.2.16, we see that
^0(1,2,-
(tfoo — ^
OQ1)/ <J00
-
> <?)
and that
^0(1,2,-
[&00 — 0OO|(1,--- ,g— 1)]/^OO
[^00
&qq(3oO&qq
^0g)
]/^00 *
> <?-!)
Hence
)] t1
Po9|(l,2,- ,g-l )]
—
[ fiqq(.$003qq
^0<j)
1
<700 ]
x [l-
[1 ~ ^0(1,2
• ,9-1
tig
(5.2.28)
^00^gg
= 1 “ Po(l,2,.. ,g)-
1
0oo<5oo
After simplification of (5.2.28), we get the required result.
Result 5.2.19. Given any positive integer q, 1 < q < k, the following inequal-
ity holds:
/>0(1) — />0(1,2)
“
*
/>0(1,2- > <?) *
Proof. From the proof of Result 5.2.18, we see that l” Po(i,2,
(-So*)2
— ^gg(^00^
gg
••' ,g-l)
&lq ) ^oo
1 °-
and p^
|(li2
^0(1,2, -.g)
^0(1,2,-
> 0. From (5.2.27), it follows that
QQ
• • , <?-!)
, for q — 2, 3, • • • , k.
S006
,9-1)
We conclude this section with a look at another geometrical property asso-
ciated with the multivariate normal distribution.

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
164
Let x = ( Xi , • • • , XkY be a k-
Result 5.2.20. Concentration ellipse.
variate random vector with E( x ) = /x, and Var(x) = E. Suppose the vector
£ — (£i , • * *
»CkY has a uniform distribution on the ellipsoid (x — /x)'A(x — /x) =
c2, where A is a p.d. matrix. Such an ellipse with E(^
) — /x, and Var(£) = E
is called the concentration ellipse and is written as
(x — p)fE 1(x — p) = k 4- 2.
Proof. The joint distribution of £ = (£i, • • • , £&)' is /(£i , • • • , £&) = const, if
(x — /x)'A(x — /x) < c2. Consider the transformation y = B(x — /x)/c, where
A = B'B is the decomposition of the p.d. matrix A. Then,
I/ {k 4- 2) = Var(y ) = BEB'/c2,
so that BEB' = c2I/(/c 4- 2), and E = c2A~ l / ( k 4- 2), which implies that
A = c2E-1/(fc 4- 2). The equation of the concentration ellipse is therefore the
expression in (5.2.29).
(5.2.29)
5.3
Some noncentral distributions
The noncentral chi-square, noncentral F, and noncentral ^-distributions are de-
rived from the multivariate normal distribution and are useful for a discussion of
inference for linear models. In general, these distributions arise as the sampling
distributions of statistics when a null hypothesis of interest is not true. The pdf
of the central chi-square and the noncentral chi-square distributions have been
variously derived in the literature. For instance, Kendall and Stuart (1958, Sec.
11.2) give a geometrical derivation of the central chi-square pdf using spherical
(or polar) coordinates while Guenther (1964) extended this approach to the non-
central chi-square distribution. In this section, we present derivations that are
based on the moment generating function method. We begin with the derivation
of the central chi-square distribution, starting from the iV*.(0,1) distribution.
Result 5.3.1. Let z ~ Nk (0,1), and let U
T!T* — J2i=I
Then, U ~
i.e., U has a (central) chi-square distribution with k degrees of freedom (d.f.)
and pdf given by
/W = 2^T ( k/ 2 )u( k
exP(“ u/2)’
u > °-
(5.3.1)
Proof. Since U is a function of a multivariate standard normal random vector
z, we have
oo
oo
1
2=1
^
(1- 2i)E2^
dz =
“ 2<)
£[exp{*£/}] = J
J
1
Mv{t )
(2n)k/ 2
i— 1
— oo
~oo
oo
oo
/ /
exp{
1
-k / 2
(27T)fc/2
i=l
— oo
— oo
(5.3.2)

5.3.
SOME NONCENTRAL DISTRIBUTIONS
165
which follows from the use of an integral evaluation theorem, either Result 5.1.2
or Result 5.1.3. Comparing (5.3.2) with (A7), we see that (5.3.2) is the mgf of
a xl random variable. The pdf of U follows from (A6).
In particular, if Z ~ iV(0, 1), then U = Z\ ~ xl - However, U ~ Xk does
not imply that Y = \/U has a standard normal distribution.
Properties of
the central chi-square distribution are given in the appendix. It is useful to
recall that the xi distribution, where k is an integer, is a special case of the
Gamma(a, (3) distribution with a = k/ 2 and (3 — 2. The chi-square distribution
plays an important role in statistical inference, especially when sampling from
a normal population. In tests for the variance of a single normal population
using information from a random sample of size n, it is well known that under
the null hypothesis, the test statistic follows a central chi-square distribution
with (n — 1) degrees of freedom. Critical values from the corresponding central
chi-square distribution enables us to construct the rejection region for the test.
When the mean of a normal random vector x is nonzero, the distribution of the
quadratic form x'x no longer has a central chi-square distribution. Result 5.3.2
generalizes this to the case where E( x ) = /x ^
0. The next example shows a
property of the central chi-square distribution.
If m > n, both being integers, we show that P( Xm > c) >
Example 5.3.1.
P{ Xn > c), where c is a known constant. This result is obvious since clearly
P(E%i x] > c) > P(E;=1 Xj > c) where X^
s are iid iV(0, 1) variables.
Let x ~
ATfc(/^, I), where \s! —
(/xi , -
* - , Hk ) 7^ 0, and let
Result 5.3.2.
U = x'x. The pdf of U is
^
exp(-A)A^ u(fe+2j-2)/2 exp{,^/2|
2;+tr(i(fc + 2j))
/(«) =T,
(5.3.3)
u > 0
f-
j=o
where A =
fi =\
We say U ~
U has a noncentral
chi-square distribution with k degrees of freedom and noncentrality parameter
equal to A .
Proof.
We derive the mgf of U = x'x, where x ~ iVfc(/x, I), and show that this
coincides with the mgf of a random variable with pdf (5.3.3). Since x ~ Nk( fJ>,I),
we have
oo
oo
/ / (27r)&/2
=
(1 - 2tyk / 2 exp{2tA/(l - 2t)},
t < 1/2,
1
exp{ tx'x — -(x — M)
7(X - fJ,) }dx
£
Mx' x{t )
— oo
— oo
(5.3.4)
which follows directly from Result 5.1.3, with B = diag{(l — 2t)/2}, b = — /x,
60 =
A = 0, a = 0, and ao = 1. Alternately, we can write the mgf in
the form Mx/x(t) = (1 — 2t )~k^
2 exp{— A[1 — (1 — 2£)-1]}, for t < 1/2. Next, we

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
166
evaluate the mgf of a random variableU with pdf (5.3.3) as Mjj(t ) = E [exp(Ut )]
i.e.
/“ exp{„(}f;-V
-” /2l
Jo
j— o
J
MV (t )
du
2*+ir($(*+ 2j))
,'l
^
e-AA^
(1- 2*)-fc/a53j!(i - 2ty
3=0
=
(1- 2t )~k / 2 exp{ 2 Xt/ {l - 2t ) }, t < 1/2
=
(1 — 2i) ~ fc/2 exp{— A[1 — (1 — 2f )-1]}, t < 1/2.
(5.3.5)
Therefore, t/ = x'x ~ x2{k , X ) distribution.
Note that A = 0 if and only if fj, = 0, in which case, the distribution of U is
a central chi-square. The noncentral chi-square distribution is the distribution
of a quadratic form x'x when x has a nonzero mean. The jth term in (5.3.3) is
the product of the Poisson pmf with mean A at count j and a central chi-square
density with k + 2j degrees of freedom. In other words, the noncentral chi-
square distribution defined in Result 5.3.2 is an example of a mixture distribution
involving central chi-square and Poisson distributions. The hierarchy is (see
Casella and Berger, 1990, Chapter 4):
U \Y ~ Xk+2Y an(i ^ ~ Poisson( X ).
(5.3.6)
Suppose
• • • , Xk are independently distributed random
Example 5.3.2.
variables, with Xj ~ iV(^j, «jJ), j =!, • • • , &. We obtain the distribution of
Xj jo?. Note that if we set Zj = Xj joj , j = 1, • • • , k ) then Z\ , • • * , Zk
are independently distributed, with Zj ~ N ( fij joj ,1), j = 1, * • * , &. Hence,
by Result 5.3.2, Ylj=i %j = Ej=i Xj / aj ^as a X2(& > ^) distribution, where
E -3=1
k
A = EM/2<’1-
3=1
Suppose U ~ x2(&, A) and V ~ x2- We show that
Example 5.3.3.
P(U > c) > P(V > c), for some constant c. We have
oo~
\
P(U
>
c) =^2,P{U > c\ J = j)P(J = j),
where J ~ Poisson{ — )
3=0
oo
=
> C)P(J = j)
J — o
oo
> ^
P(x2 > C )P( J = j )
(see Example 5.3.1)
3=0
oo
=
P{xl > c)Y,P{ J = j )
j=0
= P(X^
> c).

5.3. SOME NONCENTRAL DISTRIBUTIONS
167
Let U ~ x2(^
> A). For a given constant c, we show that
Example 5.3.4.
a property of the noncentral chi-square distribution is that P(U > c) is an
increasing function of A. Again,
CO
h( A) = P(U > c) = J2 p( xlj+k > c) jtexp(-|
)(|y,
3=0
so that the first derivative of h( A) is
oo
exp(-
^
)(
^
)j
1
h' {A) - £p04-+* > c)[-2j\
j=l
exp(“ )(
^
)j l } ~\exp(-
^
)P(Xi > c)
1
+
2( j ~ 1)!
^^W >4e x P(-^
y
3=0
\j2 p(xi j+k > c)A exp( ~\)(\y
3=0
>
0,
which shows that P(U > c) is an increasing function of A.
Let x ~ ./Vfc (/z, E), with r(E) = k. Then
(x- /x)'S_1(x- M) ~ Xk , and
x'E_1x ~ x
2( k , A),
Result 5.3.3.
C/i
C/2
where A =
lfi.
Proof. Let E = IT', with r(r) = k, and let zi = (r') * (x — /z). From Result
5.2.6, zi ~^
(0,1). Using Result 5.3.1, we see that z
^
zi ~\k • But
z'lZi = (x - M),r-1r,-1(x - n) = (x -^
)'S_ 1(x - M) = U i ,
which proves the first result. To derive the distribution of C/2, we see again
that Z2 = (r')-1
Z'2Z2 ~ X2(^
i A). Again
Nk (Tf V,I), so that using Result 5.3.2, we see that
x ~
Z2Z2 = xT 1r/ *x = x'E *x = [/2,
which proves the second result.
Result 5.3.4. Let U ~ x2(fc, A). Then E(U ) = k+2\and Var(U ) = 2(fc+4A).
Proof. Using the hierarchical setup given in (5.3.6), we have
E [E{U \Y )} = E [k + 2Y ] = k + 2A, and
E {Var {U\Y )] + Var [E(U \Y )\
E( 2k + AY ) + Uar(A; 4- 2y)
2/c H~ 4A -j- 4A
2( k + 4A).
(5.3.7)
E([7)
Var(17)
(5.3.8)

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
168
Alternatively, we can derive these expressions using
E(W ) = Q-Mu(t )|t=0
(5.3.9)
3 = 1, 2
and Var(U ) = E(U 2 ) —
[E(U )]2. Note that all moments of the pdf (5.3.3) exist
and we may interchange summation and integration in order to compute these
moments.
Result 5.3.5. Let Ui ~ x
2fe A*), i = 1, • • * , K , and suppose the Ui s are all
independent. Then
K
/c
^
^ =£> ~ x2£>,I>).
(5.3.10)
i=1
i=l
2=1
For t < 1/2, the mgf of U is
Proof.
K
=
(t )
(by independence)
Mv(t )
2=1
K
ri(!- 21)-ki / 2 exp{— Aj[l — (1 — 22) *]},
=
(1- 21)
~
fc< /2 exp{— [1- (1- 2t)
"1]
M
2=1
which is the mgf of a X 2(YliLi
A*) distribution, thus establishing the
additive property of the noncentral chi-square distribution.
Result 5.3.6.
independently distributed. Then
Let Ui ~ x2(&i > A), let U2 ~
an<l let f/i and U2 be
Ux / kx
(5.3.11)
~ F ( kx , k2,\)
F = U2 / k2
i.e., a noncentral F-distribution with numerator and denominator degrees of
freedom equal to kx and k2 respectively, noncentrality parameter A, and with
pdf
'
e~ x\j
+2j)/2 fc22 /2r(fc!/ 2+k2/ 2+j )vkl /2+J ~1
3=0
j\
r(k7/ 2+ j )rjk2/2)( fc2+
12;)
1 /2+ fc2 /2+ j
f {v; ki ,k2, X ) =
u > 0.
Proof. Since [/1 and U2 are independently distributed, we have from (5.3.1)
and (5.3.3) that for ux > 0, u2 > 0,
/(Wl, u2)
f {ui ) f (u2 )
y^ exp(-A)AJ u
(
1
fcl +2-!~ 2) /,2 exp{-ui/2} it2
fc2_2^
2 exp(-u2/2)
^
^
2(^+2J )/2r((fci + 2j)/2)
2^/2r(fc2/2)
j=0
(5.3.12)
Let

5.3. SOME NONCENTRAL DISTRIBUTIONS
169
ctj = exp(-A)AV{j!2(fcl + fc2+2^/2r((A:1 + 2j)/2)r(fc2/2)}.
Consider the following transformation of variables:
F = k2Ul / klU2, and Z = UX +U2
with Jacobian
Ml = k2{Ui +U2 )/ (kiU$ )
so that
oo/
0
/(v; ki ,k2, X )
\J \
/
'V'
/
fciuz
A * kl+ j 1 /
k2z \
J
\kiv + k2 j
\k1v + k2 )
0
\^2 1
k\k2z
2 exp(— z/ 2 )dz
x
(fciu + fo)
2^1 "t"j
1
OO
(felt; 4- fc2)£fcl+£*2+J’
oo/
0
^
ifc,+ifc2+ j-l exp(_
2/2)d2.
X
Since
oo
2^2+j“ l
0
exp(-z/2)d^ = 22 /ci + 2 /c2+.7r( ifc1 4. I/j2 +
the result follows directly.
The noncentral F-distribution is an extension of the central F-distribution to
the case when the normal random vector x has nonzero mean, i.e., the quadratic
form in the numerator has a noncentral chi-square distribution (see the appen-
dix). The noncentral F-distribution is useful in order to evaluate the power of
tests of hypotheses, which requires the evaluation of
00
G W = j f (v\ fci, k2, A )dv,
(5.3.13)
where g —
Fkuk2,a{ A) denotes the lOOath percentile point of the noncentral
F( k\,k2,\) distribution, and a denotes the level of significance of the test.
Tang’s (1938) tables may be used for evaluating G(A); see also Pearson and
Hartley (1970). The noncentral Beta distribution, which is discussed in Result
5.3.7 is also related to a noncentral chi-square and an independent central chi-
square random variable.

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
170
Result 5.3.7. Let U\ ~ X
2(^ii ^) > let U2 ~ Xfc2 > and let CA. and [/2 be inde-
pendently distributed. Then
G = C/i/(C/i + t/2)- Beto(ibi, fc2, A)
i.e., a noncentral Beta distribution with numerator and denominator degrees of
freedom equal to ki and k <i respectively, noncentrality parameter A, and with
pdf
f (w\ ki , k2, A) = exp(-^
)
jrB(w;
),
0 < w < 1,
+ 3
’ 2
» 2
where B(w;a, /3) corresponds to the (central) Beta pdf with shape parameter a
and scale parameter /3 (see the appendix).
Proof. We begin with the joint pdf of Ui and t/2 which was shown in (5.3.12).
Consider the change of variable
G = U\/ (Ui 4-1/2);
the derivation of the noncentral Beta pdf is similar to that of the noncentral F
distribution.
Two other noncentral distributions that we define are the noncentral t-
distribution and the doubly noncentral F-distribution. Scheffe (1959) presented
an application of the doubly noncentral F-distribution and also showed a pro-
cedure for approximating it via the noncentral F-distribution.
If X ~ N ( fi1a2 ) 1 U/a2 ~ xh and X
IS distributed indepen-
Result 5.3.8.
dently of t/, then
AT
(5.3.14)
~ *(M)
T = VU/ k
has a noncentral ^-distribution with pdf (Rao 1973a, p. 172)
kk/ 2
exp(— A2/2) ^
/
r( Jfe/2) {k + i2)(fc+D/2 ZJ \
s/ 2
212
6s
k + s 4- 1
f {t; k,6 ) =
k + t2
s!
2
(5.3.15)
for — 00 < t < 00, where
<5 = /i/cr.
Proof. The joint distribution of X and U is
ciexp{
(*-M)2
272-}eXP{-^
}ufc/2 1
where cf 1 =\/2TTak+i2k^
2r( k/ 2 ). Consider the transformation
X = Fsin 0,
U = R2 cos2 0;

5.3. SOME NONCENTRAL DISTRIBUTIONS
171
the joint density of R and 9 is equal to
C2r*(cos0)
where c2 = c\ exp{ — fi2/ 2a2 } = C\ exp{-<52/2}. The joint pdf does not factor
into two expressions, one involving r alone, and the other involving only 0, so R
and 9 are not independent. By expanding exp{/xrsin 0/ <r2}, we write the joint
density as an infinite series
k-1 exp{-(r2 - 2 fir sin0)/2a2},
( fi sin6 yrk+ j
C2 exp{-r2/2(72}(cos0)/c
(5.3.16)
j\a2 j
3
We obtain the marginal density of 9 by integrating out term by term with
respect to r in (5.3.16), to get
(c2/2)(cos6>)fc-1
^
r (*±i±±) 20
'+fc+D/2
Now, transforming from 0 to T = \/fctan 0, we obtain the pdf in (5.3.15).
Note also that if Z ~ iV(0, 1), f/ ~ xL ^ *s a constant, and Z and U are
independently distributed, then
T = ( Z + 6 )/ y/U / k ~ t(k ,6 )
has a noncentral ^-distribution. The noncentral ^-distribution plays the follow-
ing role in statistical inference. Suppose we consider a test Ho : fi = 0 versus
Hi :
fi # 0 on the basis of observations x = (Xi, - * - , Xn) ~ N ( filn,cr2In),
where both
fi and a2 are unknown.
The test statistic is
y/nX / S where
X = EILI*</n, and S2- E2
n
=i (*i “*)7(n-1). Under tf0, X- X(0,a2/n),
(n— 1)52/ <T2 ~ Xn— 1’ and they are distributed independently. Hence, y/nX / 5 ~
£n_ i, which is the central i-distribution with (n — 1) degrees of freedom. Under
the alternative hypothesis H\ however, X ~ iV(/i, cr2/n), and (n— l )S2/a2 is still
distributed independently as a Xn-i variable, so that
y/nX / 5 has a noncen-
tral ^-distribution with (n — 1) degrees of freedom and noncentrality parameter
A = fi2/ 2a2. The noncentral ^-distribution is useful for power calculations.
Result 5.3.9. Let U\ and U2 have independent noncentral chi-square distrib-
utions; U\ ~ x
2(ki > ^i) > and U2 ~ X
2(fe, A2). Then
_ U l / fe!
“
U 2 / k2
where F"(fci, fc2, Ai, A2) refers to the doubly noncentral F-distribution with de-
grees of freedom k\ and &2, and noncentrality parameters Ai and A2. The doubly
noncentral F-distribution is based on the ratio of two independent noncentral
chi-square random variables, and for v > 0 has pdf
~ F"(/ti, /c2, Ai, A2)
F*
00
00
f {v ) = E E N ( ki , k2 ) / D( k1 , k2 ) ,
k2— 0 k\=0

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
172
where
N (kuka ) =
exp(-A1- A2)AflA^
r(in1 + in2 + fe1 + fc2)
x
njni+fc,n|
na+*at;in‘+k*-1
= fciiwlm + fcijrcina + fca )
x
(mv + n2)^
ni+^
n2+fcl+/c2.
Proof. Recall that we can write U\ ~
an<^ ^2 ~ X2 j2+fc2 > where Ji
and J2 are independent Poisson random variables with respective means Ai/2
and A2/2. Given Ji = j\ and J2 = j2, U\ and f/2 are independently distributed
and
and
D(ki ,k2 )
U - U1/U2 ~ { (2ji + fci )/(2j2 + k2 ) }F2 jl+ki >2 j2+k2
i.e., conditional on Ji and J2, the ratio of U\ and U2 is proportional to a central
F-distribution. The pdf of U is then
00
00
1
>
= EEp-
p<-|
>
J2=0
x
+^i) > 2^
*
2 +
+ u)
32
Ji
A2
1
/(«)
77 exp(
J 2!
2 '
— (2jfi+2j2+fci+A:2)/2
X
^
(2J'i+fci )/2~l^
(2j;24-A;2)/2-l
^
which after some simplification yields the required result.
Distributions of quadratic forms
There is a rich literature on the distribution of quadratic forms (see Gray-
bill, 1961 and Rao, 1973a). The basic theorems dealing with distributions of
quadratic forms in normal random vectors are due to Cochran (1934), Craig
(1943), and Rao (1973a), while Shanbhag (1966) dealt with independence of
quadratic forms. Although the discussion that we present is by no means com-
plete, it gives the basic results that are needed for the development of linear
model theory. Recent references are Driscoll (1999), Hayes and Haslett (1999),
Khuri (1999) and Seely, Birkes, and Lee (1997). As before, we assume that the
matrix of a quadratic form is symmetric.
Result 5.4.1. Let x ~ Nk (0, 1). The quadratic form x'Ax ~
^ and only
if the matrix of the quadratic form A is idempotent with r(A) = m.
5.4
Proof.
show that x'Ax ~
distribution.
By Result 2.3.4, there exists a k x k
P'x. Partition
Let A be a symmetric and idempotent matrix of rank ra. We will
(l
0\
orthogonal matrix P such that P'AP = (
1. Define z =

5.4. DISTRIBUTIONS OF QUADRATIC FORMS
173
P as P = (Pi
P2), where P
^
Pi = Im, and partition z' = (z'x
zi = P'jX ~ Nm(0,1) and
x'Ax
z'2 ). Then
(Pz)'A(Pz) = z'P'APz
« >( 0 °o) (;)
Z'IZI -
( zi
(5.4.1)
Since zi~ Nm(0,1), the proof follows from Result 5.3.1. To prove the converse,
assume that x'Ax ~ Xm * We must show that this implies that the symmetric
matrix A is idempotent of rank m. Since x'Ax ~ xL we have by (A7),
MX'Ax(t) = (1 -
t < 1/2.
Since x ~ Nk (0,1), we can also write down the mgf of x'Ax using an integral
evaluation theorem, as
(5.4.2)
OO
OO
/ -/
1
(2^
2 exp{*(x'Ax)-ix'x}dx
Afx'Ax(^)
—
OO
—
OO
k
-1/2 _ H (i - 2ui)-i/2,
life - 2tA\
(5.4.3)
i=i
where A*, i = 1, • • • , k are the eigenvalues of the symmetric matrix A. Com-
paring the two expressions in (5.4.2) and (5.4.3) which must be equal to one
another, we should have
k[J(l - 2tAi)
"1/2 = (1 - 2t )-m/ 2
(5.4.4)
i=l
for every t in some neighborhood of zero. This will be true if m of the eigenvalues
of A are equal to 1, and the remaining k — m eigenvalues are equal to 0. By
Result 2.3.10, this implies that A is an idempotent matrix of rank m.
Let x ~
£). The rth cumulant of the quadratic form
Result 5.4.2.
x'Ax is
/cr(x'Ax) = 2r
x (r — l)![£r(A£)r + r\j!A(£A)r
1
f j}.
Proof. The mgf of x'Ax is
(5.4.5)
OO
OO
/ /
1
1
exp{£(x'Ax)— -(x —
f i ) £
l ( x- fj,) }dx
£
•Wx'AxW
(27r)fc/2|£|1/2
exp{—
^
f/ [I - (I - 2tAE)-1]S
_V}.
— OO
— OO
-1/2
(5.4.6)
|Ifc - 2£A£|
The cumulant generating function of x'Ax is

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
174
KX’ Ax(t ) =
i Krtr/r\ = log[Mx-Ax(£)]
so that
00
1
Es
1log|I- 2(AE|-ifi'II- (I- 2tAE)-‘|E-V-
KrtT —
(5.4.7)
r=l
For sufficiently small|t|, suppose Aj and 5j, j = 1, • • • , fc, denote the eigenvalues
of (I — 2LAE) and AS respectively. Then
k
k
-|log|I- 2iAE| =
3-1
3=1
--EE
~(2<<5jT
^ j=l r=1
°° or — 1+r
^
°° or-
- EM-ES-EH
for |2t (5j| < 1
^r(A£)r. (5.4.8)
r=l
J=1
r=l
Also, by direct binomial expansion, we see that for sufficiently small £,
oo
I- (I- 2tAE)-1 = -E 2rtr(AE)r.
(5.4.9)
r=1
Substituting from (5.4.8) and (5.4.9) into the right side of (5.4.7), and equating
coefficients of like powers of tr on both sides of that equation gives the required
result for Kr(x'Ax).
From Result 5.4.2, we can show directly (see Exercise 5.23) that
£(x'Ax)
=
tr(AE) + /x'A/z,
Var(x'Ax) =
2tr(AE)2 + 4/z'AEA/z, and
Cov^
x'Ax)
=
2£A^
z.
These properties are useful to characterize the first two moments of the distrib-
ution of x'Ax. We sometimes encounter the need to employ the following result
on the moments of quadratic forms, which we state without proof (see Magnus
and Neudecker, 1988).
Let A, B, and C be symmetric k x k matrices and let x ~
Result 5.4.3.
Nk {0,£). Then
£(x'Ax. x'Bx)
£?(x'Ax •x'Bx • x'Cx)
*r(AE)tr(BE) + 2*r(A£B£),
fr(A£)tr(BE)tr(CE) 4- 2[tr(AE)][tr(B£CE)]
+2[tr(BE)][tr(AECE)] + 2[tr(CE)][tr(AEBE)]
+8tr(AEBECE).

5A. DISTRIBUTIONS OF QUADRATIC FORMS
175
We next state and prove an important result which gives a condition under
which a quadratic form in a normal random vector has a noncentral chi-square
distribution. This result is fundamental for a discussion of linear model inference
under normality.
Let x ~ Affc(/x, E), E being p.d., and let A be a symmetric
Result 5.4.4.
matrix of rank r. The necessary and sufficient condition for the quadratic form
U = x'Ax to have a noncentral chi-square distribution, i.e., U ~ x2(r, A), with
r d.f. and noncentrality parameter A = /x'A/i/2, is that AE is idempotent of
rank r.
Proof.
We present a proof which only uses simple calculus and matrix algebra
(see Khuri, 1999; Driscoll, 1999).
Necessity.
AE is idempotent of rank r. By Result 2.4.5, we can write E = PP', for
nonsingular P.
Let y = P-1x.
Clearly, x'Ax = y'P'APy.
Since P'AP
is symmetric, by Result 2.3.4, there exists an orthogonal matrix Q such that
P'AP = QDQ', the elements of the diagonal matrix D being the eigenvalues
of P'AP, i.e., of AE. Let di denote the nonzero elements of D, with di having
multiplicity m*, i ~ 1, • • • ,p. Let Q* denote a matrix with rrii columns of Q
with the orthonormal eigenvectors of P'AP corresponding to d*. Then, z = Q'y
has a 7V(Q'P_
1/i,I) distribution. Also,
We assume that U ~ x2(ri A), and must show that this implies
p
z'Dz =^
(1^
x'Ax =
(5.4.10)
1=1
where the WVs have independent x2(m* > 0*) distributions, the noncentrality pa-
rameter being 0i = /i'P'_1QiQ'P_1/i > 0, i = 1, • • • ,p.
From (5.3.5), the mgf of Wi is
MWi (t ) = (1- 2i) ~mi/2 exp{-0i[l - (1- 2i) ~1]}, i =1,—
,p,
while the mgf of U = x'Ax is
MX' Ax (t ) = (1- 2*)-r/2 exp{— A[1- (1- 2i)-1]}.
Equating the mgf’s of the variables on both sides of (5.4.10), we obtain
p
(1 — 2t )~rt 2 exp{— A[1 — (1 — 2f)-1]} = JJ(l - 2dji)
exp{-0j[l- (1- 2dit ) ~ 1 } } ,
-m i / 2
2=1
X
and taking natural logarithms of both sides, we get
log(l - 21)- A[1 - (1 - 2t)
_1]-^
{-^
1log(l - 2dit ) - <M1 - (1 - 2dj )-1} }
(5.4.11)

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
176
Using the expansions
CO
log(l — u) = —
and
j=i
CO
Euj
i- (i-1*)-1 =
(5.4.12)
3=1
in (5.4.11), we see that
CO
OC
P
OO
oo
(r/2)£W/j} + AX)(2«y =E{(m,/2)i;[(2diOJ/j] + «<D2d‘
t)j}.
(5.4.13)
i=i
J=I
2=1
j=l
j=l
Equating coefficients of (2t ) j on both sides on (5.4.13), we get
(r/ 2j )+\='Yj{ (mil2j )+0i }d\,
j
(5.4.14)
J = 1, 2,
• -
2=1
which implies that \di\ < 1, i = 1, • • • ,p. We see this must be true, since, if, on
the contrary, \di\ > 1, then by letting j — > oo, the right side of (5.4.14) tends to
infinity, while the left side is equal to A < oo.
In fact, \di\ = 1, i — 1, • • • ,p, as we now show. If, on the contrary, \di\ < 1,
by summing terms on both sides of (5.4.14) over j, we get
E£i{(r/2j) + A} = E?=i(mi/2){£~ iK/.7)} +£?=i »i ET=i4
Using (5.4.12), this implies
£~ i((r/2i) + A} =-EJLito/2) log(l - d*) -£f=1041- (1- di)"1],
which is impossible since the right side is finite, while the left side is infinite.
Hence, we must have \di\ = 1, i = 1, • • • ,p. The values of di can only be one of
the following three cases. Case (i): p = 2, di = 1, cfe = — 1; Case (ii): p = 1,
di = — 1; and Case (iii): p= 1, d\ — 1. We show that Case (iii) is the only valid
case.
Under Case (i), choose j to be an even integer in (5.4.14). Then,
(r/2j) + A = £ -=i{(mi/2j) + 0*},
j = 2, 4, • • •;
oo implies A = 6\ + 62, so that r = mi + m2. However, choosing j
letting j
to be an odd integer in (5.4.14) gives
(r/ 2 j )+ A = (m l / 2 j ) +6i - (m2/2j) + d2;
substituting r = mi + m2 and A = 9\+6%, gives (m2/2j) + 02 = -(m2/2j)- <92,
which is not possible since rrii > 0 and 0i > 0, £ = 1, 2. Therefore, Case (i) is
not valid. To show that Case (ii) is invalid as well, set j to be an odd integer in
(5.4.14); we get

5.4. DISTRIBUTIONS OF QUADRATIC FORMS
177
(r/ 2 j ) + A = ~ (m1/ 2 j ) - 6\,
which is invalid since the right side is negative, while the left side is positive.
Hence, Case (iii) must be valid. In this case,
(r/ 2 j ) + A = (mi / 2 j ) + 6
j = 1, 2,...,
which implies that mi = r and 0\ = A. Therefore, AE has r eigenvalues equal
to one, and must be an idempotent matrix of rank r.
Sufficiency.
has a x2(r, A) distribution. Since AE is idempotent, in (5.4.10), p = 1, d\ = 1,
and mi = r. Hence, x'Ax will have a x2(r> A) distribution, where A =
An
alternate proof of sufficiency amounts to showing that the mgf of x'Ax coincides
with the mgf of a x2(r^
) random variable, which is obvious from (5.3.5) and
(5.4.6).
i >
We must show that if AE is idempotent of rank r, then U
We state the following result without proof.
Result 5.4.5. Let x ~ JVfc(/z, E) with r(E) = k. The quadratic form U =
x'Ax ~ X2(m > A) with A = /z'A/x/2 if and only if any one of the following three
conditions are met:
1. AE is an idempotent matrix of rank m,
2. EA is an idempotent matrix of rank m,
3. E is a g-inverse of A with r(A) = m.
Suppose x ~
_/Vfc(/z, E), where E is p.d. We show that
Example 5.4.1.
x'Ax is distributed as a linear combination of independent noncentral chi-square
variables.
First, since E is p.d, there is a nonsingular matrix P such that
E = PPr. Since P'AP is symmetric, there exists an orthogonal matrix Q
such that Q'P'APQ = D = diag(Ai, • • • , Ar,0, • • • , 0), these being the distinct
eigenvalues of P'AP. Let x = PQz. From Result 5.2.6, z ~ N (Q'P-1/z, I);
also x'Ax = z'Dz = Yli=i^
Z?• The required result follows from Result 5.3.2.
Let x ~ Nk ( p, E). The linear form Bx and the quadratic form
Result 5.4.6.
x'Ax are independently distributed if and only if BEA = 0.
Proof.
By Result 2.2.1, A = LL' with r(A) = r(L). Let us assume first that
BEA = 0. This implies that BELL' = 0, and since (L'L)-1 exists (see Ex-
ample 2.2.1), this implies that BELL/L(L'L)
_1 = 0, i.e., BEL = 0. However,
BEL = Cou(Bx, x'L), and since x ~ jV/c(/z, E), this implies that Bx and x'L
are independently distributed, and therefore Bx and x'LL'x = x'Ax are inde-
pendently distributed, which proves sufficiency. To prove necessity, note that
independence of Bx and x'Ax implies that Cou(Bx, x'Ax) = 0. Using Result

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
178
5.4.2, we can show that this implies (see Exercise 5.23) 2B£A/^ = 0 for all //,
i.e., B£A = 0.
The following result is a generalization of Craig’s theorem (Craig, 1943) and
gives a necessary and sufficient condition for the independence of two quadratic
forms in a normal random vector. Two random variables W\ and W2 that have
cgf’s are independent if and only if their cumulants satisfy
Kr (aWx + bW2 ) = Kr (aWi ) + Kr (bW2 )
for all real a and b and for all positive integers r.
Let x ~ iVfc(^
, £), where £ is p.d. The two quadratic forms
Result 5.4.7.
x'Ax and x'Bx are independently distributed if and only if
(5.4.15)
A£B = 0, or equivalently, B£A = 0.
Since £ is p.d., we can write £ = TT' (see Result 2.4.5). The
Proof.
rth cumulant of the quadratic form x'Cx, where C = aA + 6B, has the form
in (5.4.5):
Kr(x'Cx) = 2r-1(r - l)!{£r(C£)r + r/i'C(£C)r-V}-
Using (5.4.15) in «r(x'Cx), we get
ttr(x'Cx) = «r(x'(aA)x) + Kr(x'(6B)x),
Sufficiency.
(5.4.16)
for all r and real a and 6, which implies that Q\ and Q2 are distributed inde-
pendently.
Necessity.
Q\ and Q2 are assumed to be distributed independently, so that
(5.4.16) holds. For fixed, but arbitrary real values a and 6, let Ai , • • • , Ap denote
the elements in the union of three sets of eigenvalues, viz., those of aA£, those
of 6B£, and those of (aA -j- 6B)£ (so that the Ai are nonzero and distinct). Let
C represent one of the matrices aA, 6B, or aA 4- 6B. In the spectral decompo-
sition of C£ (see Result 2.3.4), let Ai be an eigenvalue of C£ with multiplicity
mi 7c-> and let Pi,c denote the matrix associated with Ai in the decomposition.
If Ai is not an eigenvalue of aA, take mi ?AA to be 0, and Pi,aA to be the zero
matrix. Let fii 7c denote /z'CP^cM- Then, (C£)r =
A[Pi,c, and
«r(x'Cx) = 2r ~ 1(r - 1)! YlPi=i{ Ximi,c + rApVi.c} -
Using (5.4.16), this implies that for any positive integer r,
pV.K {mi,aA+6B ^i,aA
i=l
V
^ ^
{Mi,aA+6B
Mi,aA
Mi,6B} — 0-
+
i=1

5.4. DISTRIBUTIONS OF QUADRATIC FORMS
179
Write the first 2p of these equations in the matrix form
Av = 0
(5.4.17)
where A is a 2p x 2p matrix with elements
= AJ, and A^+j = zA* 1
for z = !, •
• , 2p and j = 1, • • • ,p. Also, v is a 2p-dimensional vector with
elements
—
ZTZ^aA+bB ^i ,aA ^i,6B 7 S-Ild
—
£Zz,aA+fcB
Mi,aA
f^i ,bB
for z = 1, • • • ,p. Since the matrix A is nonsingular (see Exercise 5.34), (5.4.17)
implies that for z = 1, • • • ,p,
ftt-i,aA+6B ^i,aA
^i,6B = 0,
Therefore,
and
Mi,aA+6B
M«,oA
Mz,bB — 0*
£r{(aA 4- 6B)£}r
fj,' (aA 4- 6B){£(aA + 6B)}r"V
ar*r(A£)r + 5r*r(B£)r
aVA(EA)r"V + 6VB(EB)r"V
(5.4.19)
for all positive integers r and for all real a and b (since these were arbitrary
choices). Setting r = 4 in (5.4.18), expanding the left side and equating coeffi-
cients of a262, and using property 4 of Result 1.2.8, we get
£r(T'A£BT + T'BEAT)2 4- 2£r(T'AEBT)(T'A£BT)' = 0.
Since each of these terms is of the form ^r(CC;), it is nonnegative. This im-
plies then that 2tr(T/AEBT)(T'AEBT)/ = 0, so that T'AEBT = 0, i.e.,
EAEBE = 0. This together with (5.4.19) when r = 4 gives
(T'AEB/Z)'(T/AEB/LZ) + (T/BEA^
)/(T'BEA/z) = 0,
which implies that EAEB/z and EBEA/z must be zero vectors. If we set r = 2
in (5.4.19) and equate like coefficients in a&, we obtain fj!AEB/z = 0.
(5.4.18)
For more details, see Ogawa (1950, 1993), Laha (1956), and Reid and Driscoll
(1988). Using only linear algebra and calculus, Driscoll and Krasnicka (1995)
proved the general case where E may be singular. In this case, they showed that
a necessary and sufficient condition for x'Ax and x'Bx to be independently
distributed is that
EAEBE = 0, EAEB/z = 0, EBEA/z = 0, and /z'AEB/z = 0.
We show independence between the mean and sum of
Example 5.4.2.
squares. Let x = (Xi, • • • , Xn )' ~ ATn(0,1) so that we can think of Xi, • • • , Xn
as a random sample from a AT(0,1) population. The sample mean X = l'x/n
has a N(0,1/n) distribution (using Result 5.2.7), and the sample sum of squares
Hr=i(Xi — A
")2 has a central chi-square distribution with (n — 1) degrees of
freedom (by Result 5.4.1). It is easily verified by expressing X and J^
L^
X*-
X )2 as quadratic forms in x that X , and hence X is independent of X)"=i(Xi-
X)2.

CHAPTER 5.
MULTIVARIATE NORMAL DISTRIBUTION
180
Example 5.4.3.
Let x ~ A/c(/i,I) and suppose a k x k orthogonal matrix
T is partitioned as T = (T^
T^
)', where T\ is a k x ki matrix, i — 1, 2, such
that k\ + &2 = k. It is easy to verify that TiT^ = I/Cl , T2T2
I*
;2, TITJ> = 0,
T2T
1; — 0, and T'T = 1^. Also,
is idempotent of rank /c*, i = 1, 2. By
Result 5.4.5, x'T
^
T^x ~ X 2{ki,//TjTj/z), i — 1, 2. By Result 5.4.7, these two
quadratic forms are independently distributed.
Note that Result 5.4.6 and Result 5.4.7 apply whether or not the quadratic
forms x'Ax and x'Bx have chi-square distributions. A related result, called
Craig’s Theorem (Shanbhag, 1966) is given in Exercise 5.31. We next discuss
without proof a more general theorem dealing with quadratic forms in normal
random vectors. The basic result is due to Cochran (1934), which was later
modified by James (1952).
Result 5.4.8 Let x ~ Nfc(/i, E), A* be k x k symmetric matrices with r(A*) =
r*, i = 1, • • • , n, and let A = Y^
i-i A* with r(A) = r. Then
(i) x'A*x ~ x2(n, 5^'Aifi),
(ii) x'A*x are pairwise independent, and
(iii) x'Ax ~ x2{r,|
M'A/X)
if and only if
I. any two of the following statements are true:
(a) A*E is idempotent for i = 1, • •
(b) AjEAj = 0 for all i < j,
(c) AE is idempotent, or
II. (c) is true and (d) r =
holds, or
III. (c) is true and so is (e) each of the matrices AjE, • • • , An_iE is idempotent
and AnE is n.n.d.
• , n,
There are many other results in the literature that summarize properties of
quadratic forms in normal random vectors. We present two more results. For a
proof of these results, see Rao (1973a, section 3b.4).
Result 5.4.9.
Fisher-Cochran theorem.
Q1, • • • , QL denote L quadratic forms in x with respective ranks r\ , • • ,ri such
that x'x =
Then, Q/s are independently distributed as X 2(rj^
j )
variables if and only if ^2j=1 r j = k\ then A j = p' Ap, if Qj — x'A^x.
Let x ~ A^fc(/i, I).
Let
Let x ~ Afc(/z, E). The quadratic form (x - p )' A(x - p) has
Result 5.4.10.
a Xp distribution if and only if EAEAE = EAE, where p= r( AE) (Ogasawara
and Takahashi, 1951).

5.5. ALTERNATIVES TO NORMAL DISTRIBUTION
181
Let x ~ JVfc (/z, E), where E is p.d. Then Qi = x'Ax + a'x
Result 5.4.11,
and Q2 = x'Bx+ b'x are independently distributed if and only if (i) AEB = 0,
(ii) a'EB — 0, (iii) b'EA — 0, and (iv) a'Eb = 0. This is often referred to as
Laha’s theorem (Laha, 1956).
5.5
Alternatives to multivariate normal distrib-
ution
The multivariate normal distributions constitute a very useful family of sym-
metric distributions that have found widespread use in the classical theory of
linear models and multivariate analysis. However, the normal distribution is
not the only choice to characterize the response variable in many situations.
Particularly, in robustness studies, where interest lies in assessing sensitivity
of procedures to the assumption of normality, interest has centered on a more
general class of multivariate distributions (see Johnson and Kotz, 1972). Of
special interest are distributions whose contours of equal density have elliptical
shapes (see section 5.2), and whose tail behavior differs from that of the normal.
It is also possible to incorporate skewness to obtain a richer family of distrib-
utions. We begin the discussion by introducing a finite mixture distribution
of multivariate normal distributions, as well as scale mixtures of multivariate
normals. We then extend these to a more general class of spherically symmetric
distributions, and finally to the class of elliptically symmetric distributions.
5.5.1
Mixture of normals distribution
A finite parametric mixture of normal distributions is useful in several practical
applications. We give a definition and some examples.
Definition 5.5.1.
normal distributions if its pdf is
We say that x has an L-component mixture of fc-variate
XL,PL ) = 2>(27r)-fc/2|Ej|
j~1
-1/2
/(x j A t i. S i.P i, - - -
» PL
x
Tlk.
x exp
»3
This pdf exhibits multimodality with up to L distinct peaks.
We saw the form of the bivariate normal pdf in Example
Example 5.5.1.
5.2.1. Mixtures of L bivariate normal distributions enable us to generate a rich
class of bivariate densities which have up to L distinct peaks. Let p,\ = M2 = 0,
o\ j — a\ j — 1, for j — 1, 2, pi = 1/2, and P2 = — 1/2. With mixing proportions
Pi = p2 = 1/2, the mixture pdf of x = (Ai, X 2 ) is

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
182
2
/(X,^1, EI, PI, M2,S2, P2) = E 5^
75 exp{-§(*1 - X i X 2 + x% ) }, x e K2.
i=i
A plot of this pdf reveals regions where one of the two components dominates,
and there are also regions of transition where the pdf does not appear to be
“ normal” .
A well known property of a bivariate normal mixture is that all
conditional and marginal distributions are univariate normal mixtures.
Consider the following e-contaminated normal distribution
Example 5.5.2.
which is a mixture of a N (0, 1) and a N (0, a2!), with 0 < e < 1. Its pdf is
/(x; <T2, e) = (1 — £)(27r)
k / 2 exp{-±x'x} + e(27r)
k/ 2a
k expf-^
x'x}.
The mixture of normals accommodates modeling in situations where the
data exhibits multimodality. Suppose A denotes a discrete random variable,
assuming two distinct positive values Ai and A2, with respective probabilities
pi and p2, where Pi + P2 = 1. Let x be a fc-dimensional random vector which
is defined as follows: conditionally on A = Aj, x ~ Affc(0, AjI), j = 1, 2. The
“conditional” pdf of x (conditional on A) is
/(x|Aj) = ( 2n)~kt2\
~kt2 exp{— x'x/2Aj}, x G lZk.
The unconditional distribution of x has the mixture pdf
/(x) = (27r)_
fc/2{piAJ
"fc/2 exp(-x,x/2Ai) + p2A^
fc/2 exp(-x'x/2A2)} .
This distribution is called a scale mixture of multivariate normals. In general,
we can include L mixands, L > 2. By varying the mixing proportions Pj and
the values Aj, we can generate a flexible class of distributions that are useful
in modeling a variety of multivariate data. It can be shown that all marginal
distributions of this scale distribution mixture are themselves scale mixtures of
normals of appropriate dimensions, a property which this distribution shares
with the multivariate normal (see Result 5.2.8). Suppose we wish to maintain
unimodality while allowing for heavy-tailed behavior, we would assume that
the mixing random variable A has a continuous distribution with pdf TT(A). We
define the resulting flexible class of distributions and show several examples
which have useful applications in modeling multivariate data.
Definition 5.5.2. Multivariate scale mixture of normals (SMN) dis-
tribution.
A fc-dimensional random vector x has a multivariate SMN distri-
bution with mean vector 0 and covariance matrix E if its pdf has a “ mixture”
form
j Nk ( x;0,x(A)E)7r(A)dA
(5.5.1)
/(x; 0, E) =
n+
where K(.) is a positive function defined on 7£+, and 7r(.) is a probability function,
which may be either discrete or continuous. The scalar A is called the mixing
parameter, and 7r(.) is the mixing density.

5.5. ALTERNATIVES TO NORMAL DISTRIBUTION
183
Suppose we set «(A) = 1/A in (5.5.1), and assume that the
Example 5.5.3.
parameter A ~Gamma(i//2, i//2), i.e.
{u/ 2 )^
2 X^
2-1 exp{-u\/ 2 }
l
7r(A) =
— 00 < A < oo.
I>/2)
The resulting multivariate ^-distribution is a special example of the scale mix-
tures of normals family with v degrees of freedom and pdf
-( i/+fe)/2
r{£(* + »/)}
1
/(x; 0, E, i/) =
1/2
r(i//2)(i/7r)*/2|E|
(5.5.2)
for x E 7£fc. When z/ — > oo, the multivariate ^-distribution approaches the multi-
variate normal distribution. By setting 9 = 0, and E = I& in (5.5.2), we get the
standard distribution, usually denoted by /(z). When z/ = 1, the distribution
corresponds to a fc-variate Cauchy distribution. In particular, when k = 2, let
z = (Zi, Z2 ) denote a random vector with a Cauchy distribution. The pdf of z
is
/(Z) =^
-(1 + Z'z)-3/2, zen\
and corresponds to a (standard) bivariate Cauchy distribution, which is a simple
example of a bivariate scale mixture of normals distribution.
Example 5.5.4. If we assume K( A) = 4A2, where A follows an asymptotic
Kolmogorov distribution with pdf
00
?r(A) = 8£ (-l)j+1J 2Aexp{-2j2A2}
3=1
the resulting multivariate logistic distribution is a special case of the scale mix-
ture of normals family. This distribution finds use in modeling multivariate
binary data.
Example 5.5.5. If we set
K ( A) = 2A, and assume that
7r(A) is a positive
stable pdf Sp(a,1) (see the appendix) whose polar form of the pdf is given by
(Samorodnitsky and Taqqu, 1994)
7r5p (A;a,1) = {a/(l - a)}A
a
^+1J/s(u) exp{-s(u)/Aa //^
a
^ }du
0
for 0 < OL < 1, and
s(u) = {sin(o:7m)/ sin(7ru)}a^
1 a
^{sin[(l — a)ixu\/ sin(7m)}.
The resulting scale mixture of normals distributions is called the multivariate
symmetric stable distribution.

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
184
Spherical distributions
In this section, we define the class of spherical (or radial) distributions.
5.5.2
A A>dimensional random vector z = (Zi, • • • , Zk )
f is said
Definition 5.5.3.
to have a spherical (or spherically symmetric) distribution if its distribution does
not change under rotations of the coordinate system, i.e., if the distribution of
the vector Az is the same as the distribution of z for any orthogonal k x k matrix
A. If the pdf of z exists in 7Zk 1 it depends on z only through z'z =
Zf ;
for any function h (called the density generator function),
/(z) a h(z'z) = Cfcft(z'z),
(5.5.3)
where Ck is a constant. The mean and covariance of z, provided they exist, are
E(z) = 0,
and Cov(z) = elk
where c > 0 is some constant.
Different choices of the function h gives rise to different examples of the
spherical distributions (Muirhead, 1982; Fang, Kotz and Ng, 1990). Contours
of constant density of a spherical random vector z are circles when k = 2,
or spheres for k > 2, which are centered at the origin. The spherical normal
distribution shown in the following example is a popular member of this class.
Let z have a /c-variate normal distribution with mean 0
Example 5.5.6.
and covariance cr2Ik > We say z has a spherical normal distribution with pdf
1
1
Z II
2}, zeRfc
/(z; <72)
exp{
(27r)fc/2<7fc
2o2
1
1 z'z},
z 6 Rk'
exp{
(27T )k / 2crk
The density generator function is clearly h(u )
2a2
cexp{— u/ 2},u > 0.
The e-contaminated normal distribution shown in Example 5.5.2. is also an
example of a spherical distribution, as is the standard multivariate^distribution
defined in Example 5.5.3. The following example generalizes the well-known
double-exponential (Laplace) distribution to the multivariate case; this distrib-
ution is useful for modeling data with outliers.
Consider the bivariate generalization of the standard
Example 5.5.7.
double-exponential distribution to a vector z = (Zi, Z2) with pdf
/(z) = In exP{-(Z'Z)1/2}.
Z n2.
This is an example of a spherical distribution; notice the similarity of this pdf
to that of the bivariate standard normal vector.

5.5. ALTERNATIVES TO NORMAL DISTRIBUTION
185
The squared radial random variable T =|| z|| has pdf
7T*/2
Definition 5.5.4.
t( k/ 2 )-l h( f y
t > Q
(5.5.4)
T ( k/ 2 )
We say T has a radial-squared distribution with k d.f. and density generator h,
T ~ Rl (h).
i.e.
The main appeal of spherical distributions lies in the fact that many re-
sults that we have seen for the multivariate normal hold for the general class of
spherical distributions. For example, if z = ( Z^ Z^ )* is a bivariate spherically
distributed random vector, the ratio V = Zi/Z2 has a Cauchy distribution pro-
vided P{Z2 = 0) = 0. If z = ( Z i , • • • , ZkY has a k-variate spherical distribution,
k > 2, with P(z = 0) =0, we can show that
V = Z1 / { ( Zl +
• • + 2% )W / (k-1)}~ 4-1.
In many cases, we wish to extend the definition of a spherical distribution to
include random vectors with a nonzero mean /1 and a general covariance matrix
E. This generalization leads us from spherical distributions to elliptical (or
elliptically contoured distributions), which form the topic of the next subsection.
Elliptical distributions
The family of elliptical or elliptically contoured distributions is the most gen-
eral family that we will consider as alternatives to the multivariate normal
distribution.
We derive results on the forms of the corresponding marginal
and conditional distributions, and also give some useful results on distributions
of quadratic forms in elliptical random vectors. There is a vast literature on
spherical and elliptical distributions (Kelker, 1970; Devlin, Gnanadesikan and
Ketternring, 1976; Chmielewski, 1981; Fang, Kotz and Ng, 1990; and Fang and
Anderson, 1990) and the reader is referred to these for more details on this
interesting and useful class of distributions.
5.5.3
Let the /c-dimensiona! random vector z follow a spheri-
Definition 5.5.5.
cal distribution, (i be a fixed A;-dimensional vector, and T be a k x k matrix.
The random vector x =
+ Tz is said to have an elliptical, or elliptically con-
toured, or elliptically symmetric distribution. Provided they exist, the mean
and covariance of x are
E( x ) = //,
and Cov( x ) = cfr' = cV
where c > 0 is a constant. The mgf of the distribution, if it exists, has the form
Mx(t) = ^
(t'Vt) exp{t'//}
for some function ip. In case the mgf does not exist, we invoke the characteristic
function of the distribution for proof of distributional properties.

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
186
In order for an elliptically contoured random vector to admit a density (with
respect to Lebesgue measure), the matrix V must be p.d, and the density gen-
erator function h(.) in (5.5.3) must satisfy the condition
00
t / 0
/ rffc72) ^
(fc/2) ~1/l(^)^ =!> t > 0.
If the pdf of x exists, it will be a function only of the norm|| x ||= (x'x)1//2 (see
Definition 1.2.36). We denote the class of elliptical distributions by £*(/i, V,h).
If a random vector x ~ £*(0,1
/i), then x has a spherical distribution. Suppose
[i is a fixed fc-dimensional vector, and y =
4- Px, where P is a nonsingular
k x k matrix. Then, y ~ £fc(/x, V, /i), with V = PP\
Result 5.5.1. Let z denote a spherically distributed random vector with pdf
/(z), and let x = \x + Tz have an elliptical distribution, where T is a k x k
nonsingular matrix. Let V = IT', and note that z = T
_ 1(x- fi ). Then the pdf
of x has the form
/(x) = cfc|V|
1/2/i[(x - /i)'V
1(x
/x)], x e 7lfc
for some function h(.) which can be independent of fc, and such that rk~lh(r2 )
is integrable over [0, oo).
Proof. The transformation from z to x = /i + Tz has Jacobian J =|r_1|. By
Result 5.1.1, we have for x
llk ,
/(x) =
fj,) }
=
cfc[|r|-1|r|-1]1/2/i[(x-
- M)]
=
cfc|Vr1/2/i[(x- /x)'V1(x- /i)].
Note that the same steps are used in the derivation of the multivariate nor-
mal pdf in section 5.2. The relation between the spherical distribution and the
(corresponding) elliptical distribution is the same as the relationship between
the multivariate standard normal distribution (Definition 5.2.1) and the corre-
sponding normal distribution with nonzero mean and covariance E (Definition
5.2.2).
The distribution of x will have m moments provided the function rm+k ~1h(r2 )
is integrable on [0, oo). We show two examples.
(5.5.5)
Example 5.5.8. Let x have a A;-variate normal distribution with mean / j, and
covariance a21. Then x has an elliptical distribution. A rotation about /2 is
given by y = P(x —
/1) + /2, where P is an orthogonal matrix.
We see that
y ~Nk (n,a2I ) (see Exercise 5.9), so that the distribution is unchanged under
rotations about 11. We say that the distribution is spherically symmetric about
/2. In fact, the normal distribution is the only multivariate distribution with
independent components Xj, j = 1, • • , fc, that is spherically symmetric.

5.5. ALTERNATIVES TO NORMAL DISTRIBUTION
187
Example 5.5.9.
Suppose x — /4 + Tz, where z was defined in Example 5.5.3,
M = {^
1,^
2 )' is a fixed vector, and T is a nonsingular 2 x 2 matrix. Let A = IT';
the pdf of x = ( Xi , X2 y is
-3/2
A) = (2ir ) 11A|
l/2 [1+ (x- M)'A ^
X- M)]
,
x e f t2.
This is the multivariate Cauchy distribution, which is a special case of the mul-
tivariate t-distribution. The density generator for the k-variate Cauchy distrib-
ution is h(u) — c{l + w}~^+1)/2, while the density generator for the k-variate
^-distribution with v degrees of freedom is h(u) = c{l -f- u/ v }~
^
kJtU^
2. In terms
of its use in linear model theory, the Cauchy distribution and the Student’s
^-distribution with small v are considered useful as robust alternatives to the
multivariate normal distribution in terms of error distribution specification.
Example 5.5.10.
Let z be the standard double exponential variable specified
in Example 5.5.7, and suppose we define x = fi -f Fz where fi — (/x1,^
2 )' is a
fixed vector, and T is a nonsingular 2 x 2 matrix. Let A = IT
7; the pdf of
x = (xux2 y is
/(x;
A) = (27r)-1|A|-1/2 exp{-[(x- /i)'A-1(x - /x)]1/2}, x
'll2.
A comparison of the contours of this distribution with those of a bivariate normal
distribution having the same location and spread shows that this distribution
is more peaked at the center and has heavier tails.
The next result specifies the marginal distributions and the conditional dis-
tributions. Result 5.5.3 characterizes the class of normal distributions within
the family of elliptically symmetric distributions. Let x —
(Xi, * - - , X^)' ~
Suppose we partition x a s x = (xj,x
^
)
7, where xi and X2 are
Result 5.5.2.
respectively g-dimensional and ( k - </)-dimensional vectors. Suppose \x and V
are partitioned conformably (similar to Result 5.2.8).
1. The marginal distribution of x* is elliptical, i.e., xi ~ Eq(fii,Vn ) and
X2 ~ Ek-q{ fi2,V 22)- Unless /(x) has an atom of weight at the origin, the
pdf of each marginal distribution exists.
2. The conditional distribution of xi given X2 = C2 is ^-variate elliptical with
mean
£(XI|X2 = c2) = Ml + Vi2v221(c2 - M2)
while the conditional covariance of xi given X2 = C2 only depends on C2
through the quadratic form (X2 —
C2)'V221(x2 —
C2). The distribution of
X2 given Xi = ci is derived similarly.
(5.5.6)

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
188
The mgf of Xi is^
(t
^Vnti) expft^
i}, and that of X2 is ^
>(t2V22t2)
Proof.
exp{t'2^
2}, so that xi ~ Eq( /J,i,Vn) and x2 ~ Ek-q(ii2, V22). The pdf of xi
if it exists, has the form
/l (xi ) = Cq|Vn| 1/2/i«j[(X!- MiyVli^
Xl - Ml )].
where the function hq depends only on h and q, and is independent of\x and V.
To show property 2, we see that by definition, the conditional mean is
£(xi|x2 = c2) = / xi <iFXl|C2 (xi ).
Substituting y = xi - Mi -
- M2) and simplifying, we get
£(XI|X2 = c2) = / ydFy|C2 (y) + MI + VI2V221(C2- M2)-
Since it can be verified that the joint mgf of y and X2, when it exists, satisfies
•My,x2(-ti,t2) = My,X2(ti,t2), we see that / y<fFy|C2(y) = 0, proving (5.5.6).
The conditional covariance is
/[xi - E( xi|x2 = c2)][xi - £
,(XI|X2 = c2)]7x,|x2(xi )dxi
,h^
V 1T2Z + (c2 - M2)'VJ21(C2 - M2)]
M
/x,(C2)
Cov( xi|x2 = c2)
/
Ck
dz,
IVIV2
where Vn .2 = Vn — Vi2V221V2i. The result follows since /X2(c2) is a function
of the quadratic form (x2 —
C2)'V221 (X2 - C2).
Result 5.5.3.
Suppose x,
and V are partitioned as in Result 5.5.2.
1. If any marginal pdf of a random vector x which has an^
(/i, V, h) distri-
bution is normal, then x must have a normal distribution.
2. If the conditional distribution of xi given x2 = c2 is normal for any q,
q — 1, •
• , k — 1, then x has a normal distribution.
3. Let k > 2, and assume that the pdf of x exists. The conditional covariance
of xi given X2 = C2 is independent of X2 only if x has a normal distribution.
4. If x ~ Efc(/i,V, h), and V is diagonal, then the components of x are
independent only if the distribution of x is normal.
By Result 5.5.2, the mgf (or characteristic function) of x has the
Proof.
same form as the mgf (or characteristic function) of xi, from which property 1
follows. Without loss of generality, let /i — 0 and V = I, so that by Result 5.5.2,
the conditional mean of xi given X2 = C2 = 0, and its conditional covariance
has the form
</>(c2)Ik-9 * Also, (?(xi|x2 = C2) = Cfch(x'1x1 + x'2X2)//2^
2) is a
function of x
^
xi. If the conditional distribution is normal,
Cfc/^
x'iXi + X2X2) = {27r<7(c2)}
{k
9)/2/2(c2) exp{-ixixi/o-(c2)},

5.5. ALTERNATIVES TO NORMAL DISTRIBUTION
189
so that
Ckh( x[ xi ) = {27T(j(c2)}_ ( A:_9)/2/2(c2) exp{ix2X2/cr(c2)} exp{-ixixi/CT(c2)}.
This implies that /(x) = Cfc/i(x'x), i.e., x has a normal distribution, proving
property 2. The proof of properties 3 and 4 is left as an exercise.
We end this subsection with results on distributions of quadratic forms in
elliptical random vectors, some of which are analogous to the theory for normal
distributions. Note that if a random vector y has pdf /(y) = ckh(y'y), then
the pdf of || y|| is
2Cfc 7Tfc/2
rw2) wk-1h(w2 )
/||y||M =
(5.5.7)
(see Kelker, 1970).
Let x ~ Ek (0,V, h) with pdf /(x) = c/c|V| 1/2/i(x/V xx),
Result 5.5.4.
and let Q = x'V-1x. Then
7Tfc/2
ckwkt2
1h(w ).
/QM =
(5.5.8)
T ( k/ 2 )
Let y = V
x/2x, and /(y) = ckh(y'y). The distributions of Q and
Proof.
y'y coincide.
For q < /c,
q/ 2
9<,M = Y{^
jT)CqWq/ 2~lhq^
is the pdf of J2U 1
for y = (Fi, • • • ,Yk )'.
(5.5.9)
Let x ~ Ek (0, V, h) with finite fourth moment, and let A be
Result 5.5.5.
a symmetric matrix of rank r. Then x'Ax ~ gr(.) if and only if
1. r + r(V 1
2. A = AVA.
A) = fc, or
To prove property 1, assume first that r + r(V 1 — A) = k. By
Proof.
Result 2.4.5, there exists a nonsingular matrix P such that V = PP'. Let
y = P-1x; /(y) = ckh{y'y).
Now, x'V^x = x'Ax + x^
V"1 - A)x, i.e.,
y'y = y'P'APy + y'(I — P'AP)y. The rank condition ensures the existence
of an orthogonal transformation z = By such that /(z) = /(Zi, * * * ,Zk ) =
Cfc/i(z'z). Now, x'Ax = y'P'APy = Yli=i
~ 9r (w )- To prove the converse,
suppose x'Ax ^ 9r { ,w ) ) where r = r(A). There exists a nonsingular matrix
P such that V = PP', and an orthogonal matrix Q such that Q'AQ = D =
diag(Ai , • • • , Ar,0, • • • ,0). If x = QPz, it follows that x'Ax = z'P'Q'AQPz =

CHAPTER 5. MULTIVARIATE NORMAL DISTRIBUTION
190
Yli=i
Using the second and fourth moments of z, it can be shown that
21=1^ = 2[=i
= r > so ^at
= 1, i = 1, • • • , r. We can therefore write
x'V_1x = x'Ax 4- x'(V_1- A)x as z'z = Yli=i
+2i=r+i
This implies
that r(A) 4- r(V_1 — A ) = k, proving property 1. The proof of property 2 is
similar to the proof of property 2 of Result 5.4.5.
Anderson and Fang (1987) discuss Cochran’s theorem for elliptical distrib-
utions. Matrix variate elliptically contoured distributions extend the elliptical
distributions from the vector to the matrix case and an excellent discussion
can be seen in Gupta and Varga (1993). Azzalini and Dalla Valle (1996) have
discussed the multivariate skew-normal distributions which extend the class of
normal distributions by the inclusion of a shape parameter. They described dif-
ferent methods to generate skew-normal distributions. Branco and Dey (2001)
used a conditioning method to generate multivariate skew-elliptical distribu-
tions.
Exercises
5.1. Let x = ( Xi > • • • , Xk ) ~ -ATfc(/i, E), with r(E) = k.
(a) Show that
oo
oo
J
J exp{-
^
(x- /i)'E\K -n) }dx1
/
• • •d x k
— OO
— OO
=
(27r) fc/2|E|1/2.
(b) Evaluate
exp{— ( x\ 4- 2x\X 2 4- 4x2 ) }dxidx2-
5.2. [Graybill, 1961]. Let x = ( X\, X 2 ) have a bivariate normal distribution
with pdf
/(x; /x, E) = ± exp(-Q/2)
where Q = 2xj —
X 1X 2 4- 4^2 — llx\ — 5x2 4- 19, and k is a constant. Find
a constant a such that P(3X\ — X 2 < a) = 0.01.
5.3. Let X\ and X 2 be random variables such that X\ 4- X 2 and X\ — X 2 have
independent standard normal distributions. Show that x = ( X 1 1 X 2 ) has
a bivariate normal distribution.
5.4. The logarithm of the mgf of a trivariate random vector x = (Xi, X2, Xs )
f
is given by
lnMx(t) — 5t\ 4- 3££> 4" 6^3 — 2t\t2
+ 2^2^3 4* 4£i — 2^2 4* £3.

Exercises
191
Show that x has a trivariate normal distribution. Identify the mean and
variance-covariance matrix of the distribution.
(a) Show that ( X\, X 2 ) has a bivariate normal distribution with means
pi, P2, variances o\ and <r2, and correlation coefficient p if and only if
every linear combination C\X\ -\- C2 X 2 has a univariate normal distri-
bution with mean cipi -f C2P2 ? and variance c\cr\+c2
<72 -t- 2ciC2pai2,
where C\ and C2 are real constants, not both equal to zero.
(b) Let Yi — Xijoi, i — 1, 2. Show that Var(Yi — Y2 ) — 2(1 — p).
5.6.
(a) Let ( X\, X 2 ) ~ N2 ( pi , p-2^ 1^2 ^ P ) where pi = P2 = 0, and p ^
1.
The polar coordinate transformation is defined by X\ = Rcos©,
X 2 —
Rsin ©. Show that the joint pdf of R and © is given by
2(i^
Tr
2(1 - Psin 261)}'
0 < r < 00, and 0 < 6 < 27r, and that the marginal pdf of © is
(27r)-1(l — p2)1/2(l — psin 20)_1,
0 < 6 < 2n.
(b) Suppose (Xi, X 2 ) has a bivariate normal distribution iV2(0,0, <r2,a\, p),
|p| ^
1. Show that
5.5.
r(27r)
X (1 — p2)
1/2 exp|
P(X1 > 0, X2 > 0) = i +^
sin-1(p).
5.7. The random vector x = (Xi , X2, - *
*
Is said to have a symmetric
multivariate normal distribution if x ~ ATfc(p, E) where p = pi*, i.e., the
mean of each Xj is equal to the same constant p, and E is the equicorre-
lation dispersion matrix, i.e.,
( 1
P
*
*
P\
p
1
• • •
p
E —
cr2
\P
P
• • •
V
When A; = 3, p = 0,
<T2 = 2 and p — 1/2, find the probability that
X3 = min(X1, X2, X3).
Hint: Recall that if x = (Xi, • • • , X^)' has a continuous symmetric dis-
tribution, then all possible permutations of Xi, • • • , X^ are equally likely,
each having probability P( Xix <
• • • < X*fc ) = l/ k\ for any permutation
(ii, • • • , ik ) of the first k positive integers.
5.8. Let x ~ Nk {0, E) with pdf /(x) where E = {£27}- The entropy h( x) is
defined as
Mx) = -//(x) ln/(x)
(a) Show that h( x.) =|ln(27re)*|E|.

CHAPTER 5.
MULTIVARIATE NORMAL DISTRIBUTION
192
(b) Hence, or otherwise, show that |E| < Yli=i
with equality holding
if and only if Eij = 0, for i ± j [Hadamard’s inequality].
5.9. Let x ~ ATfc(0, <J2I) and let y = Px, where P is a k x k orthogonal matrix.
Show that y has a ATfc(0, cr2I) distribution.
5.10. Let x ~7Vfc(/x, E), where r(E) = r < k. Show that there exists an r x k
matrix B and an r-dimensional vector b such that the vector z = Bx -f b
has a iVr(0,I) distribution.
5.11. Let x ~ Nk { fiy E), where p = (/ii, • • • , /ifc)' and E =
i, j = 1, • • • , k.
Show that Xi ~ N ( pi ^ au ).
5.12. Let x ~
E) and suppose that x = (x'^x^
Xg)', where x* is a ki-
dimensional vector, and
i K ~ k. Assume that p and E are par-
titioned conformably.
Derive the conditional distribution of X3 given
Xi = ci, and X2 = C2.
5.13. Let x = ( XiiX 2, X $ y ~ AT3(/i, E), where p = ( pi , p2,P3 )' and E =
1
p
0
a2
p
1
P \ -
0
p
1
(a) What are the marginal distributions of X 2 and X3?
(b) Write down the conditional distribution of X\ given X 2 and X3. Un-
der what condition does this distribution coincide with the marginal
distribution of Xi?
(c) For what value of p are the two random variables X\ + X 2 + X$ and
X\ — X 2 — Xs independently distributed?
1
p
p\
p
1
p J . Show that
P
P
V
5.14. Let x = ( X\, X 2, X3)' ~ AT3(0, E), where E =
(a) Corr( X %, X$ ) = p2
(b) Pi2|3 = P/(1 + p)
(c) Pi(2,3) = 2p2/(l 4* p).
5.15. For any distribution, let E( X 2\X\ = Xi ) = a +fixShow that
Corr( X 1 ^ X 2 ) = fio\ j<J 2, where o\ and 02 denote the respective standard
deviations of X\ and X2.
5.16. Show that y2(fc, A) = y2(l, A) H- y2_r
5.17. Let U-
A). Show that P{U < u) = P{ X 1 - X 2 > k/ 2 ) where Xx
and X 2 are independent Poisson random variables with respective means
uf 2 and A.

Exercises
193
5.18. Suppose Xi, - -
* , X& are iid x2(ljA/k) * Show that Y = Yli=1
X2(&, A) distribution.
5.19. Let (Xi, X2) ~
|p|^
1. Show that
X* has a
(x^)
2 _ 2p
(*£*) + (^
)
2
T =
> h_
has a x2 distribution. What are its parameters?
5.20.
(a) Let x ~iVfc(/x,I). Show that £/ — x'x has a x2(^
i^) distribution,
with A — p'/i/2.
ATfc(p,cr2I). Show that x'x/ <72 ~ x2(&i fi' IA/ 2(J 2 ).
(b) Let x ~
iVfc(/i, D), where D = diag(<r2, • • • , cr|), r(D) = k. Find the
mean and variance of the random variable t/ = x'D_1x.
NidfJ,) E), with r(E) = fc. What is the distribution of U =
(x- pyE-Hx- p)?
(c) Suppose A = D_1 — (D-111'D~1)/1'D~11.
Assume that x ~
Xfc(p, D) distribution. Find the distribution of x'Ax.
5.22. Let V ~ F(fci, h2,A). Show that
5.21.
(a) Let x ~
(b) Let x ~
hi (i + ft )
fc2 — 2
and
(fci +2A)2
k\{k2-2) [(/c2 — 2)(/c2— 4)
fei+4A
Var^
) =
fc2 — 4
5.23. If x ~
E), show that
(a) E(x'Ax) — tr(AE) + p'A/z.
(b) Far(x'Ax) = 2tr(AE)2 + 4/i'AEA/x.
(c) Co?;(x, x'Ax) = 2EAp.
5.24. Let x ~ Nfc(/i,I). Show that x'Ax
if and only if A is
idempotent of rank m.
5.25. Let x ~ Nk (0, E). Show that x'Ax ~ Xm if and only if AE is an idempo-
tent matrix with r(A) = m.
5.26. Let x ~
(/i, QA), where A is a positive definite matrix, and Q is
symmetric and idempotent with tr(Q ) = m. What is the distribution of
U = x'A-1x?
5.27. Suppose x = (Xi, X2, Xs)' is distributed as iV3(0,I). Let Q =|(X2 +
4X|+ Xf - 4X1X2 + 2X1X3- 4X2X3). Find the distribution of Q.

CHAPTER 5.
MULTIVARIATE NORMAL DISTRIBUTION
194
5.28. Let x ~ Nk (0, E), where E =
<r2[(l — p)Ik + pJfc], 0 < p < 1.
(a) Show that the distinct eigenvalues of E are Ai = 1 — p, with multi-
plicity gi = k - 1, and A2 = 1 + (k - l)p with multiplicity p2 = L
(b) Define Ax = Ik — Jk / k, and A2 = Jfc/fc. Show that Ai and A2 are
idempotent, A1A2 = 0, and that E = AiAi + A2A2.
(c) Let Qi = x'Aix/Ai , i = 1, 2. Show that Qi and Q2 have independent
chi-square distributions. Find the parameters of these distributions.
5.29. Let x ~ Nk ( p, E), where p = plk and E = cr2[(1 — p)Ik + plfciy, 0 < p <
1.
(a) Derive the distributions of X =
Xi / k and Q =
~
X)2/[a2(l - p)].
(b) Verify that X is distributed independently of Q.
5.30. Let x ~ ATfc(Ep> <r2E), where E is a symmetric matrix of rank A:, a2 > 0,
and p is a fixed vector. Let B = E_ 1 — E_1l /c(l
^
.E-1lfc) _ 1l^
E_ 1.
(a) Derive the distribution of y = Bx.
(b) Derive the distribution of y'Ey when (i) p = 0, and (ii) p^
0.
Hint: Show that B is symmetric and BE is idempotent.
5.31.
(a) Show that two quadratic forms x'Ax and x'Bx in x ~ Nk ( fb,a2I) are
independently distributed if and only if AB = 0 (Craig’s Thoerem).
(b) If further, A and B are idempotent matrices, show that x'Ax/<r2
and X'BX/<72 are independent chi-square variables.
5.32. Let x = (Xi , X <i )' ~ V2(pl, E), where E = (1 — p)l2 + PJ2 - Let Q\ —
( Xi - X 2 )2 and Q2 = { Xx + X2)2.
Show that Qi/2(1 - p) has a x2
distribution and that Q1 and Q2 are distributed independently.
5.33. Let x ~ Nk ( p, E).
Show that a necessary and sufficient condition for
(x — p)'A(x — p) and (x — p)'B(x — p) to be independently distributed is
EAEBE = 0.
5.34. [Driscoll and Krasnicka, 1995].
Show that the matrix A in (5.4.17) is
nonsingular.
5.35. Prove properties 3 and 4 in Result 5.5.3.

Chapter 6
Sampling from the
Multivariate Normal
Distribution
Let Xi, * * * , XJV denote a random sample from a
variate normal distribu-
tion with mean vector \x and variance-covariance matrix E, where we write
Xi = pG,i, • • •
i = 1, • • • , N . Let X' — (xi, • • • ,x^)- The joint density
of this random sample is obtained as the product of N multivariate normal den-
sities. Let us denote the parameter space by
= {/i, E; //
7Jfc, E p.d.}. In the
following sections, we describe estimation of the parameters from a multivariate
normal distribution and describe properties of these estimates. We also describe
inference for the simple, multiple, and partial correlation coefficients based on
the normal random sample. In practice, we must of course assess whether the
random sample does indeed come from a normal distribution. We present some
methods for assessing normality and also describe suitable transformations to
normality.
6.1
Distribution of sample mean and covariance
The mean of the random sample x i, - - - , XJV is defined as the fc-dimensional
vector
i "
TVE
(6-1.1)
xW =
Xi ,
i=1
and the sample covariance matrix is the k x k matrix S N / ( N - 1), where
N
N
sN = E](Xi ~ Xw)(Xi - XA,)' = Ex,x'- NXNX'n.
(6.1.2)
2=1
2=1
195

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
196
We can write SAR/ ( N — 1) = { Sji }, j j = 1, * • • ,
with
N
Sjl = Z ( X u - X j )( Xi,t - Xl ) / ( N -1)
i=l
where X j = 5Zi=i X i j / N.
The sample covariance matrix has k variances
Sjj, j = 1, •
• , k and k( k — l)/2 possibly distinct covariances, 5^7, j < l, j,l =
1, • • • , /c. The generalized sample variance is the scalar quantity \SN / ( N — 1)|,
which determines the degree of “ peakedness” of the joint density of xj, • • • , Xjv,
and is a natural summary measure of variability in the sample.
Given a univariate random sample X\, • • • , X^ from a N ( f i,a2 ) population,
we know that the sample mean X has a normal distribution with mean f i and
variance a2/ N , the statistic ( N — l )S2/a2 is distributed as a XN-I variable, and
the two distributions are independent (Casella and Berger, 1990; Mukhopad-
hyay, 2000). The corresponding distributional result for the
variate situation
is given in Result 6.1.1. We first define a multivariate distribution called the
Wishart distribution, which is derived from the multivariate normal distribution
as the sampling distribution of the sample statistic J^^
x* - x /v)(x* - XAT)'.
The Wishart distribution is a multivariate extension of the chi-square distribu-
tion.
Definition 6.1.1. Wishart distribution.
to follow a fc-dimensional (noncentral) Wishart distribution, (VT^
E, ? , A)), with
m degrees of freedom, parameter E, and noncentrality parameter A = /x'E_1^/2
m£x,x', where x,, j
A random k x k matrix W is said
if W can be represented as W —
Nk (ii,E) vectors. Provided m > &, the density function of W is
= 1, • *
* , m are iid
3=1
|W|(m-fc-i)/2 exp{i7’(— S_1W/2)}
/(W; M, E) =
(6.1.3)
2fcm/2|E|m/2rfc(m/2)
where Tk {m/ 2) =
n£=i r(|(m + 1 — j ) ) is the multivariate Gamma
function.
If A = 0, we say that W follows a central Wishart distribution
Wfc(E, m); since E is p.d., it is clear that A = 0 if and only if /x = 0. The additiv-
ity property of Wishart matrices states that if W^
l~ Wk {E, rrij), j = 1, • • • ,J,
then J2 j=i Wi ~ W^
(E, Ej=i mj ).
Result 6.1.1. Distribution of
x^ and SN -
random sample from a jVfc(/A, E) population. Then
1. the distribution of x/v is
E/iV),
2. for N > 2, Sjv follows a Wishart Wfc(E, N — 1) distribution, and
3. xyv and Spj are independently distributed.
Proof. Prom (5.2.7), MXj. (t) = exp{t'/7+^
t'Et}, so that
Let xi, • • • , xw denote a

6.1. DISTRIBUTION OF MEAN AND COVARIANCE
197
n M
(t / N ) = exp{t'n + ±t'(E/iV)t}
j= i
t) =
which proves property 1. We prove property 2 by induction on N (see Ghosh,
1996). For N = 2, it follows from (6.1.2) that
[xi - i(xi + x2)][xi - l(xi + x2)]'
l(xi - x2)(xi - x2)',
S2
while from Result 5.2.5 and Result 5.2.6, we see that (xi - X2)/\/2 ~ Nk(0, E).
By Definition 6.1.1, S2 ~ W*.(E,1). To use the induction argument, assume
that property 2 holds for N = m. We leave as an exercise the verification of the
following identities:
1
(mxm + xm+i ), and
(^m+1
Xm)(xm_|_i
xm) •
Xm+1
m -1- 1
Sm +
m
(6.1.4)
Sm+1
771 + 1
Since x$, i = 1, • • • , m + l are mutually independent, and since (xm,Sm) is a
function only of x*, i = 1, • • • ,ra, it follows that (xm+i,Sm) is independent
of xm+i. By the induction hypothesis, we also have independence of xm and
Sm. It follows that xm, Sm, and xm+i are mutually independent. Thus, Sm is
independent of (xm, xm+i ), and hence independent of xm+i — xm which follows
aiVfc(0,^
E). By Definition 6.1.1,^
_ (xm+1-xm )(xm+i-xm)' ~ H4(E,1).
By assumption, Sm ~ W4(E, ra - 1). The proof of property 2 follows from the
additivity property of Wishart matrices. To prove property 3, note that
Cov( xl - XAf , XJV) = ifE- JJT, = 0,
i = 1,—
, iV,
which implies that [(xi — xjv ) /,
, (x^ — x^)']' is uncorrelated with, and there-
fore independent of, XAT. It follows directly that SJV is distributed independently
ofxiv -
For any vector a = (ai, • • • , a/-)', we show that the statistic
a'(xi -
xjv)'a. That a'Sjva is a scaled chi-square is an immediate consequence.
Example 6.1,1.
a'S^a/a'Ea ~ XN ~I - Decomposing SN as in (6.1.2), a'S^a = ]TV=
xyv)(xi
Anderson (1984) gives an alternate proof using the Helmert transformation,
while Mardia, Kent and Bibby (1979) use a multivariate version of Cochran’s
theorem. A sufficient statistic for a parameter is a function of the sample ob-
servations that captures all the information about that parameter. Additional
information in the sample, beyond that contained in the sufficient statistic,
provides no information about the parameter of interest. The conditional dis-
tribution of the sufficient statistic given the sample data does not depend on

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
198
the parameter. The factorization theorem (Halmos and Savage, 1949) enables
us to identify a sufficient statistic by inspecting the form of the pdf or pmf of
the random sample. We give a statement of the factorization theorem and use it
to identify the sufficient statistics in a random sample from a k-variate normal
distribution. Let f ( x\0) denote the joint pdf or pmf of a random sample X. A
statistic T(X) is a sufficient statistic for 0 if and only if there exist functions
g(t\6 ) and h( x ) such that f ( x\6 ) = g(T ( x )\6 )h( x ), for all sample points x and
all parameter values 6.
The sample mean and sample variance are sufficient statistics
Result 6.1.2.
for /i and E in a k-variate normal random sample xi, • • • , x^.
Proof.
The joint density function of N random vectors Xi, •
• ,
is a prod-
uct of the marginal A;-variate normal densities:
rrexp{— ± (xi - f j )' E
1( x i - f i) }
/(xj, - - - , xw; /x, E)
= JJ
1/2
(27r) fc/2|E|
N
exP{~5E (xi -
- M)}
i— 1
z=l
.
(6.1.5)
(27r)JVfc/2|E|W/2
N
N
Since £) (xj- xN )( xN -n)' = 0, and
~ n)(xi
XTV )' = 0, using Result
1.2.8, we can write Ejli(xi ~ M/E-1^ - A4) = Eiliir(xi ~ ^
),S_1(xi -
M) = EiIiMs
_1(xi - M)(xi - M)'] = tr[E-1Eili((x» - M)(x* - /*)'}] =
trp
*1E!Ii{(xi - xw + xN - ja)( xi - xN + xN - At)'}] = ir[£-1Eili{(x» ~
Xiv)(xi — xyv)'}] + iV(xjv — /i)
/E ~1(xiv “ /x). The joint density function can be
written as
i— 1
2=1
Y(XN - JU)'S ^
XN - P)}
exp{—
[( N — 1)E 1STV]
/(X I, - - - , xN \n, E)
N / 2
(27r)'
Vfc/2|E|
(6.1.6)
and depends on the sample observations xi, • • • , x;v only through x.^ and SN .
By the factorization theorem, these two sample statistics are sufficient statistics
for /i and E.
Result 6.1.2 states a special property of a multivariate normal sample. Fur-
ther, by Basu’s theorem (Basu, 1964), x/v and S/v are independently distributed.
In general, the sample mean and variance are not sufficient statistics in nonnor-
mal samples. Given observed values of xi, • • • , xjv, (6.1.6) viewed as a function
of \i and E is the likelihood function, maximizing which yields the MLE’s of
these parameters. The MLE’s of the mean vector and covariance matrix of a
k-variate normal population are derived in the following result.

6.1. DISTRIBUTION OF MEAN AND COVARIANCE
199
Result 6.1.3. Maximum likelihood estimation of
/z and E.
on a random sample xi, - -
* , XJV from a Nk ( /i,E) distribution, the maximum
likelihood estimates of [i and E are
Based
1
N
l— 1
l
N
£A/ L
=
- x/v)(xi - x N y =
i = x;v, and
(6.1.7)
jUML
AT — -SN
<
(6.1.8)
2=1
Proof. The likelihood function L(^
z, E; xx, • • • , x^) has the form shown on the
right side of (6.1.5). The MLE’s of /z and E are denoted by 9 M L and E^L,
and are the values that maximize L(/z, E; Xj, • • • , x^). Since E-i is p.d. (see
Definition 2.4.5), the distance (XN
lA' E
(XN
9 ) in the exponent of the
likelihood function is positive unless 9 = XN. The MLE of /z is then XN, since
it is the value of 9 that maximizes the likelihood function. We next maximize
the following function with respect to E:
TV
-\tr E
x£ (xj - xjv)(xi - XJV)'
exp
2=1
L{9ML,E) =
(6.1.9)
TV/2
(2TT)7V^/2|E|
TV
Using Example 2.4.4 with A = E, B =
(x2 “ x/v )(xj — x// )', and b — N/ 2,
2=1
TV
we can show that EML —
XTV)(x2 — XTV); maximizes L( J1ML, E). The
maximized likelihood is
2=1
exp(-^
)
(6.1.10)
(27T)^/2|EML|iV/2
which completes the proof.
Corollary 6.1.1. Using the invariance property of MLE’s, which states that
the MLE of a function g{6 ) of a parameter 6 is 9(9M L ), we see that
1. the MLE of the function 9' E_1/z is
2. the MLE of y/tfjj, the (/, j)th element of E is given by yfaij, the square-
root of the MLE of the (/, j)th entry in EML-
Although the estimator 9ML is a unbiased estimator of /x, EML is a biased
estimator of E. Analogous to the univariate case, an unbiased estimator of E
is E =
respectively are complete sufficient statistics (Seber, 1984).
i
Sjsr . The unbiased estimators of 9 and E denoted by 9 and E
TV — 1

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
200
Distributions related to correlation coeffi-
cients
6.2
In Chapter 5, we defined theoretical versions of simple, multiple, and partial
correlations; we described properties of each type of correlation, and discussed
relationships between them. In this section, we discuss estimation of these cor-
relation coefficients, as well as distributional properties of these estimators that
enable inference. We begin with the simple correlation based on iid bivariate
normal samples ( Xij.Xij ), i = 1, -
• , iV, where Xj and Xi denote the jth
and Ith. components of the /c-dimensional vector x. Suppose E( Xij ) = pj,
E( Xij ) = nu Var( Xij ) = a], Var{ Xiti ) = erf , and Corr
^
X^^ Xi^
) = pjt.
Suppose we have a
Result 6.2.1. Estimation of simple correlations.
random multivariate normal sample xi, • • • , x#.
1. The maximum likelihood estimator of the simple correlation between Xj
and Xi is
Pji
(6.2.1)
Pjl —
y/ ojjau
where the quantities on the right side have been defined in property 2 of
Corollary 6.1.1. The estimator pji can be expressed as a function of the
complete, sufficient statistics XAT and S/v.
2. When pji = 0, the pdf of pji is
1
(1 _ r2)( N-4)/2
/(* ) =
(6.2.2)
-1 < r < 1.
B(l/2, ( N - 2)/2)
3. When pji = 0, the random variable
N — 2
Tji = Pji
(6.2.3)
1-3*
has a Student’s ^-distribution with ( N — 2) degrees of freedom.
Proof. The form of pji follows from the invariance of the MLE (see Corollary
6.1.1). Since the correlation coefficient is invariant under change of location
and scale, we can assume without loss of generality that pj = pi = 0, and
<7 jj = afi = 1, and for i =1, • • • , TV, the pairs ( Xij ^ X^i ) are iid samples from a
bivariate normal distribution with these means and variances, and correlation
Pji. When pji = 0,
f(Xiyir" ,XNJ\XXJ ,-
" " , Xjyj)
= /(XU > . . . , XAU)
i=i
Consider the transformation

6.2. CORRELATION COEFFICIENTS
201
N
Ui = EXi ,i /VN , u2 = z h
- XJ )/E4=I( Xij -*,)2]1/2 ,
2—1
and Ui, i — 3, • • • , iV orthogonal to each other, and to U\, t/2, such that the
entire transformation is orthogonal, and let
rji = U2/(Zl2Ui )1/ 2.
Then,
N
N
i=1
i=1
N
N
and
N
- NX* =Y,( Xiti - Xt )2
i=2
2=1
i=l
so that
• , XN , j ) cxexp{i^
f/2}-
/(t/x, • • •.l/jvl-XTu, - -
2=1
Let V = YliLi U?- Computing the joint distribution of Vji and V , and integrat-
ing out the V , we obtain,
f (r\Xld , • • , Xftj ) = {5(1/2, ( N - 2)/2)}-1(l- r3)<"-4>/2, -1 < r < 1,
which does not involve the conditioning variables, and is therefore the uncon-
ditional pdf of pji. This proves property 2. To prove property 3, note that
W = U2{ N - 2)1/2/(£*3U? )1/ 2 is an increasing function of p3i. Conditional
on Xij, i = 1, • • • , N, Ui, i = 1, • • • , N are iid N (0,1) variables, and W has a
i/v— 2 distribution. Further, since the distribution of W does not involve these
conditioning variables, the unconditional distribution of W is also t/v-2- Since
N
N
U2( N - 2)1/ 2/C£ U? )
=
{u2( N - 2Y^ / (J2u! )1/ 2 }
1/2
2=3
i— 2
N
*- {i -ul/ J2u2 }1/ 2
2=2
=
( N - 2)1/ 2 pjt / (l - Pji )1^
2 — R*,
say, which is increasing in pji, it follows that when pji = 0, R* ~ £/v_2 *
For an alternate proof of this result, as well as for a proof of the sampling
distribution of pji when pji ^
0, see Rao (1973a). Here, we present two alternate
forms for the nonnull distribution of pji. Starting from the joint distribution of
Sjj , Su and rji (see Exercise 6.5), it can be shown that this has the form
(1 _ r2yiV-4)/2^
(2r^
)
2W-3(l- p2 )( JV-D/2
t
N +t -1))2
-
^
2
f (r ) =
TTT ( N - 2)
1!
t=0
(6.2.4)

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
202
Another form of the distribution of p3i is
(i-4)(*-i)/2
( N - 2)(1- r-2) ( N_4)/2
f (r )
7r
- hr-
0
yv-2
1
(6.2.5)
du,
w2)1/2 (1 — pjiru )N-l
which is obtained by verifying that
l
N-3
(1- u2 )
l/2(\- rupji)
= rfjv^I)
JV-2
f U
0
t=0
It would be natural to test Ho : pp = 0 versus ifi : pp / 0 using property
2 in Result 6.2.1. Unfortunately, the critical values under this null distribution
of pji have not been widely tabulated. The null distribution of the transformed
variable in property 3 is generally used instead. The two-sided test procedure
rejects Ho at level of significance a if |( N — 2 )1^
2 pji / y/ 1- pji \ > t^
_ 2,a/2- In
practice, Fisher’s ^-transformation of pji leads to simpler inference, as shown
in the next result. For a proof of this result, see Kendall and Stuart (1958),
section 16.33.
Result 6.2.2.
Let
5 ln(jVfO = *anh~'<ft,)’
Zji
± ln
2
VI - P j i )
tanh l ( pji ), and
1
v2
(6.2.6)
JV- 3
then, Zji has an approximate N (5ji,v2 ) distribution (Fisher, 1921).
We now consider estimation of the partial correlations (see Definition 5.2.7)
based on the random sample xi, • • • , x/v. Recall that we partitioned the random
vector Xj=(Xi>i, • • • , XiykY as x-= (x'fl,x^
2)' where x' x =
, Xi %q ) is
a ^-dimensional vector, and x' 2 = ( Xi >q+\, • • • , Xiyk ) is a ( k — g)-dimensional
vector. We partition p, E, x#, and Sw conformably (as in Result 5.2.8). In
particular, suppose
Sn
S12
S21
S22
Syv =
where Sn is a q x q matrix. Define
= Su — SI2S221S2I 5
S11.2

6.2. CORRELATION COEFFICIENTS
203
and let Xij and Xij denote components of the subvector x^i.
The quantity Sn.2 has a^
(En.2, N-1- k+q ) distribution.
Result 6.2.3.
From property 2 of Result 6.1.1,
~ Wk ( E, N — 1 ), which implies
N-1
Proof,
that SN =
where zn is a ^-dimensional vector. For a fixed g-dimensional vector h,
where z* are independent JVfc(0, E). Let z
^ — (z
^ <2)'.
Z2Zj,
i,l’
2=1
h'Snh
h'Si2
E21I1
E22
h'z2,1 ~ Nk-q+i(0, E*), with E* =
and
=
Z2,2
N-1
s* = £ z* z*' =
h'Suh
h'Si2
S2ih
S22
It may be verified that the first diagonal elements of E* and S* are respec-
tively |E22|/|£*| and IS22 I/IS*|, and also that
Result 2.1.2, |S*|/|S22| = h'tSn-S^S^
ijh = h'Sn .2h, and |E*|/|E22| =
h'[En — Ei2E221E2i]h = h'En^h. Since h^n^h ~ h^En^hx?
quired result follows directly.
~ Wk-q+i (Tl* , N -1).
2=1
/lS ~ XN-l-*+g- ^0m
IS* 1
IS22T
, the re-
N — 1— k+q
Result 6.2.4. Estimation of partial correlation coefficients.
1. The maximum likelihood estimator of Pji|(9+i,... ,k ) is
[Su.2]j,J
(6.2.7)
pjl\( q+l , - ,
[Sll.2]jj
2[Su.2]1/2
l ,l
where [Sn.2]
denotes the (m, n)th element of the q x q matrix Sn.2-
2. If Pj/|(g+i, * *- ,fc) = 0, then
m,n
[N - ( k - q) - 2}1/ 2 pjl [ {q+1 >...
<k )
(6.2.8)
~ tN -( k-q )-2-
Proof.
The form of the estimator follows from the invariance property of the
MLE. When Pji|(<H-i,— ,fc) = 0, the pdf of Pji\(q+i,— ,k ) has the form in (6.2.2),
replacing the empirical and theoretical simple correlations by corresponding
partial correlations, and replacing N by N — ( k — q).
We next discuss estimation of the multiple correlation coefficient based on
N samples from a ( k + 1)-variate normal distribution. Using notation similar
to Definition 5.2.8, let
Soo
soi
S(1)
*0
(6.2.9)
and S =
XJV =
S10
denote the sample mean and sample variance-covariance matrix of the random
sample ( Xiy
,Jt*,*.), i =!, - • • ,N.

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
204
Result 6.2.5. Estimation of the multiple correlation coefficient.
1. The maximum likelihood estimator of po(i, ••• ,&) is
{s0i[S(1)]_1sio}1/2
(6.2.10)
A)(i,- ,fc) =
1/2
s.00
2. When p0(i,- ,fc) = 0,
( N - k - l)p§(i,...,fc)
(6.2.11)
~ Fk ,N-k-l -
^[1
PQ(I,.- ,f e)
Proof. The proof of property 1 follows directly from the form of the multiple
correlation coefficient and invariance of the maximum likelihood estimator. Let
Soo.i = Soo ~ s0i[S(1)]_1Sio. Then, 1-
k ) = Soo.i/Soo, so that
,«/(1 -
,.)> = fs«.[S>1>l-IS.o}/Soo., .
0 (see Definition 5.2.8), so that Soo.i ~ Wi (T,oo,N —
When p0(lr..ik ) = 0, a0\
k — 1), Soi[S^
1
^]” 1Sio ~ Wi ( Eoo, k ), and they have independent distributions.
The proof of property 2 follows from the definition of the F-distribution (see
(A9)).
When p0(i,-,fc) = 0, the statistic B —
P o(i,...
“ Po(i,- ,fc)l f°U°ws a
Beta(fc — 1, N - k ) distribution, as we ask the reader to show in Exercise 6.8.
The non-null distribution of B is a non-central Beta (see Result 5.3.7).
6.3
Assessing the normality assumption
In this section, we discuss a few procedures that enable us to detect situations
where the observed data depart, to a small or large extent, from the assumption
of a normal parent population. There exist several procedures in the literature
for assessing the hypothesis of univariate normality, such as (a) skewness and
kurtosis tests, (b) omnibus tests such as Shapiro and Wilk’s W-test, (c) likeli-
hood ratio tests with specific nonnormal alternatives, (d) goodness of fit tests
such as the x2-test and the Kolmogorov-Smirnov test, and (e) graphical meth-
ods such as normal probability plots. Assume that we have a random sample
Xx, *
* , XN from some continuous distribution with cdf F(.). We denote the
empirical cdf by
FA(X) = -^
(Number of observations < x ).
The Kolmogorov-Smirnoff test is useful for testing Ho : F ( x ) = Fo(x) V x
versus H\ : F( x) ^
FQ( X ) for some x, where FQ(.) corresponds to a N(p,a2 )
distribution. The Kolmogorov-Smirnoff test statistic is
(6.3.1)
DN = sup|FN ( X ) - F0(:r)|

6.3. ASSESSING THE NORMALITY ASSUMPTION
205
which will be large if the data are not consistent with HQ. The p-values can be
computed based on the asymptotic null distribution of
given by
lim P{\/NDjsf < z } — Q( z ),
N — >oo
exp(— 2k2z2 )
oo
k— 1
Q( z ) = i - 2E (-i)
1
for every z > 0. The function Q( z ) is the cdf of a continuous distribution called
the Kolmogorov distribution. In general, the parameters p and <r2 are unknown,
and may be replaced by their sample counterparts.
Another test for normality is due to Shapiro and Wilk and is based on a
comparison of ordered sample values with their expected locations under the
null hypothesis of normality. Let Z(i) < • • • < Z( jv) be an ordered sample from
a standard normal distribution, and let
—
1E( Z^
) ) i = l, - - - , iV be the
normal scores. Under the hypothesis of normality,
E( X ( i ) ) = p + a E( Z(i ) ) - f i + crmi,
i.e., we expect that the ordered observations X^’s are linearly related to the
s. Shapiro and Wilk (1965) proposed the statistic
SW = (m,r2-1m)-l(m'Q-1u)2/ El=1(^
~*)2,
where m and Q are the mean and covariance of z = (Z^
), * * * , Z^
)/ * The
statistic SW may be interpreted as the ratio of an estimate of variability based
on ordered statistics and normality to the usual residual mean square. Shapiro
and Wilk provided extensive tables of percentage points of SW .
The well known normal probability plot, which is a plot of the empirical
quantiles versus the theoretical quantiles from a standard normal distribution,
is available with most software packages. When the points in the normal prob-
ability plot lie very nearly along a straight line, the normality assumption is
reasonable. The pattern of possible deviations of the scatter points from a
straight line will indicate the nature of departure from normality, such as skew-
ness, kurtosis, outliers or multimodality.
Next, we discuss some approaches for assessing multivariate normality, in-
cluding tests based on multivariate measures of skewness and kurtosis, as well
as graphical procedures (Gnanadesikan, 1977). In Chapter 5, we have seen that
(a) contours of the multivariate normal distribution are ellipsoids, and (b) linear
combinations of normal random variables also have a normal distribution. As a
first step in assessing multivariate normality, we may check to see whether the
univariate marginal empirical distributions are approximately univariate normal
using any of the approaches described earlier.
To assess bivariate normality, scatterplots of pairs of variables, Xj and Xi
are useful. By Result 5.2.6, the vector X2 = (Xj, Xi ) has a bivariate normal
distribution, i.e., Nz ( p2,£2) » s&y. In Example 5.2.3, we showed that contours
of constant density would be ellipses defined by (X2-^
2 )^21(x2 - P2 ) — X2,<* >
where, x!,a 1S
uPPer (100a)th percentile from a chi-square distribution with

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
206
k = 2 degrees of freedom. If the data follows a multivariate normal distribu-
tion, the scatterplot of X^
j versus Xi,u i = 1, • • • , AT should exhibit a pattern
that is (nearly) elliptical and roughly 100a% of the points should lie inside
the estimated ellipse (x2 —
X2)'Sj1(x2 - X2) < x!,a > where X2 is the mean of
the bivariate sample, and S2 is the sample variance covariance matrix. This
comparison of the empirical proportion of points within the estimated ellipse
to the theoretical probability gives a rough assessment of bivariate normality.
Alternately, for any k > 2, one can compute the squared generalized distances
d2
j = {N - 1)(xj- xjvys^
x.,- - xN ),
j =!, •
, N.
Provided the data follows a k-variate normal distribution, and both N > 30 and
N — k > 30, xjy converges in probability to /4, while SN converges in probability
to E; then, d? behaves like a chi-square random variable. The following plot,
called the chi-square plot, or the gamma plot enables us to assess multivariate
normality. First, order the squared generalized distances, d?
<
• • • < d?W
Plot the pairs of points (d'fjyXk ,
where j* = jj ( j-1/2), and xl, j* denotes
the upper (100j*)th percentile of the chi-square distribution with k degrees of
freedom. If the points lie approximately on a straight line, normality may be
assumed, but not otherwise. Once again, the pattern of points may suggest the
nature of departure from normality (Andrews et al., 1971; and Gnanadesikan,
1977).
(i) -
Mardia (1970) proposed a test for normality based on multivariate skewness
and kurtosis, arguing that these quantities provide direct measures of departure
from normality. Let x follow a fc-variate distribution with mean // and covariance
E, and suppose E-1 = {<7rr*}.
A multivariate measure of skewness which is invariant
Definition 6.3.1.
under nonsingular transformations (such as x = Ay + b) is defined by
Px,k = E
E a"V*VVmVm 'n,
r,s, £
r* s* t*
where Mi11^ — E{ { Xr — pr )( Xs — ps )( Xt — Mt)}- F°r any symmetric distribution
about fi, /?i,fc = 0.
When k = 1, /?i 5i = /?i, the usual univariate measure of skewness. When
k = 2, with (r,s)th central moment
Var( X1) = 1, Var( X 2) = 1, and
Corr( X 1 ^ X 2 ) = p = 0, the bivariate skewness measure is
2 = M§,o + Mo,3 + 3M1,2 +
which is identically zero if and only if /430 * MO,3> MI,2 and M2,i all vanish.
1’
Definition 6.3.2.
singular transformations is
A measure of multivariate kurtosis invariant under non-
02 ,k = £{(X- jU)'S
J (x —
(J,) }2

6.3. ASSESSING THE NORMALITY ASSUMPTION
207
When fi= 0, and E =1^, the measure of kurtosis is invariant under orthogonal
transformations and is 02,k = E{ ( x' x )2 }.
Let xi, * * - , x/v denote a random sample from a /c-dimensional population
with mean /i and covariance matrix E.
Let xjv and S^ / ( N — 1) = {S^
}
respectively denote the sample mean and sample covariance matrix, and let
{ S'i } denote the inverse. A sample measure of skewness corresponding to 0yyk
is
bitk = ( N — l)3^
(6.3.2)
r,s,t
r* s* t *
where M^
f = ZZi ( Xi,r ~ Xr )( Xi<s - X 3 )( Xi <t - Xt )/N. An alternate ex-
pression for b\^
is (see Mardia, 1975)
N
N
= j^EEH
i=l j— 1
(6.3.3)
where we define
rij = ( N - l)(x, - x
N)'
- xN).
The sample skewness b\^ is also invariant under nonsingular transformations of
the data. A sample measure of kurtosis is
i
N
N -
- xw )}2,
(6.3.4)
£>2,fc =
N
i=i
which is also invariant under nonsingular transformations such as x = Ay -I- b.
For large samples, we can test for A;-variate normality based on these measures
as shown in the following result. For a proof of the result, the reader is referred
to Mardia (1970).
The following tests for multivariate normality hold in large
Result 6.3.1.
samples.
1. The statistic A = ^
Nb\tk has a x2 distribution with k( k + l)(fc -1- 2)/6
degrees of freedom under the null hypothesis Ho :0i,k = 0. We reject the
null hypothesis of normality for large values of A. We can also use the
approximation that for k > 7, y/2A ~ N (
^
[k( k + 1)(& + 2) — 3],1).
2. Under the null hypothesis HQ : 02,k = k( k -f 2), the statistic
B =^
N (b2 ,k - (32' k )/ {8k( k + 2 ) } 1/ 2
has a standard normal distribution.
Geometrically, the Mahalanobis distance between x* and Xj is

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
208
d'ij = ( N - l)(xi- xJ )'SA,1(xi - xj )
and let ra be the square of the Mahalanobis distance between x* and x/v. It
may be verified that rij =\{ra + rjj — d?-). The Mahalanobis angle between
the vectors X* — XN and Xj — XJV is defined by
rij / irur-jj }112 .
COS
j
We can express the multivariate skewness and kurtosis in terms of these quan-
tities:
N
N
1
=
775>:>:!
}3/2{cos%}3,
bl ,k
riirjj
i=lj=l
i "
t>2 ,k
i=1
If the sample points x*, i = 1,
• • , N are uniformly distributed on a fc-dimensional
hypersphere or ellipsoid, then b\^ — 0. If, however, the data departs from spher-
ical symmetry, then bi^
will be large; this might occur for instance when there
is an abnormal clustering of points. In such cases, the value of 62,k will also
be abnormally large.
Figure 6.3.1 (a) and (b) represent the symmetric and
abnormal clustering cases respectively.
\
/
/ X
\
\
/
/ X
/
/
\
01
*7
x
x
/
\\
\
X
/
/
\
\
/
«
(b)
(a)
Figure 6.3.1.
Graphical representation of skewness.

6.4. TRANSFORMATIONS TO APPROXIMATE NORMALITY
209
6.4
Transformations to approximate normality
The normal distribution plays a central role in the classical theory of linear
models. The normal general linear model is attractive because it has closed
form expressions for estimators and test statistics and the distribution theory
is elegant. When the assumptions of homoscedasticity, independence, or nor-
mality are violated, as often happens in practice, there are several choices. A
usual remedy for the problem of heteroscedasticity is the use of generalized least
squares for estimation and inference. This procedure may be carried out using
the matrix and distributional tools we have described so far. When the normal-
ity assumption is violated, the data can be transformed, if possible, so that the
transformed variables are approximately normal. A general family of univariate
and multivariate transformations is introduced in this section. It should be kept
in mind, however, that there are some drawbacks associated with these transfor-
mations. In particular, the parameters of the transformed data model may not
be as meaningful as those in the original model. Alternatively, one can consider
general linear models where the errors have an alternate distribution (than the
normal) chosen from the general family of elliptically contoured distributions
which were described in section 5.5.
Univariate transformations
A transformation of A is a function T which replaces A by a new ’’transformed”
variable T(A). The simplest and most widely used transformations belong to
the family of power transformations, which are defined below.
6.4.1
Definition 6.4.1. Power transformation.
The family of power transfor-
mations have the form
aXp+ b
if
clogA + d
if
p = 0
where a, 6, c, d, and p are arbitrary real scalars.
(6.4.1)
T P( X ) =
The power transformation is useful for bringing skewed distributions of ran-
dom variables closer to symmetry, and thence, to normality. The square root
transformation or the logarithmic transformation, for instance, have the effect
of “ pulling in ” one tail of the distribution. Any power transformation is either
concave or convex throughout its domain of positive numbers, i.e., there is no
point of inflection. This implies that a power transformation either compresses
the scale for larger A values more than it does for smaller A values (for ex-
ample, Tp(A) = log A), or it does the reverse (for example, Tp(A) = A2).
We cannot however, use a power transformation to expand the scale of A for
large and small values, while compressing it for values in between! Tukey (1957)
considered the family of transformations
AA
if
log A
A ^
O
if
A = 0, and A > 0
A<A) =
(6.4.2)

CHAPTER 6. SAMPLING FROM NORMAL DISTRIBUTION
210
which is a special case of (6.4.1). This family of transformations, indexed by
A, includes the well-known square root (when A = 1/2), logarithmic (when
A = 0), and reciprocal (when A = -1) transformations. Notice that (6.4.2) has
a discontinuity at A = 0. The Box-Cox transformation offers a remedy to this
problem and is defined below.
Definition 6.4.2. Box-Cox transformation.
the transformation
Box and Cox (1964) defined
[XA - 1]/A
if
log*
which has been widely used in practice to achieve transformation to normality.
If some of the Xi s assume negative values, a positive constant u may be added
to all the variables to make them positive. Box and Cox also proposed the
shifted power transformation which is defined below.
A / 0
if
A = 0, and X > 0
X w =
(6.4.3)
Definition 6.4.3. Box-Cox shifted power transformation. Box and Cox
also proposed the shifted power transformation
[( X + S )x - 1 / A
if
A ^
0
logpf -1- 5]
if
A = 0
where the parameter <5 is chosen such that X > — 8.
Xw =
(6.4.4)
Several modifications of the Box-Cox transformation exist in the literature,
of which a few are mentioned here. Manly (1976) proposed a modification of
the Box-Cox transformation which allows the incorporation of negative obser-
vations and is an effective tool to transform skewed unimodal distributions to
approximate normality. This is given by
[exp(AX)-1] / A
if
A ^
0
if
A = 0.
x ( A) =
(6.4.5)
Bickel and Doksum (1981) proposed a modification which incorporates un-
bounded support for X
X^ = [\ X \xsign( X ) - l } / X .
(6.4.6)
With any of these transformations, it is important to note that very often the
range of the transformed variable X^ is restricted based on the sign of A, in
turn implying that the transformed values may not cover the entire real line.
Consequently, only approximate normality may result from the transformation.
Definition 6.4.4. Modulus transformation.
The following transformation
was suggested by John and Draper (1980):
sign[(|X|+ 1) A - 1]/A
sign[log(|X|+ 1)
if
A ^
0
if
A = 0,
=
(6.4.7)

6.4. TRANSFORMATIONS TO APPROXIMATE NORMALITY
211
where the sign of X^ is that corresponding to the observation X . The modulus
transformation works best to achieve approximate normality when the distrib-
ution of X is already approximately symmetric about some location. It alters
each half of the distribution about this central value via the same power trans-
formation in order to bring the distribution closer to a normal distribution.
It is not difficult to see that when X > 0, (6.4.7) is equivalent to the power
transformation. Given a random sample X\, * • • , X /v, estimation of A using the
maximum likelihood approach and the Bayesian framework was proposed by
Box and Cox (1964), while Carroll and Ruppert (1988) discussed several robust
adaptations. The maximum likelihood approach for estimating A in the con-
text of linear regression is described in section 8.1.1. A generalization of the
Box-Cox transformation to symmetric distributions was considered by Hinkley
(1975), while Solomon (1985) extended it to random-effects models.
6.4.2
Multivariate transformations
Andrews et al. (1971) proposed a multivariate generalization of the Box-Cox
transformation.
A transformation of a A>dimensional random vector x may
be defined either with the objective of transforming each component Xj, j =
1, - • •
marginally to normality, or to achieve joint normality. They defined
the simple family of “ marginal” transformations as follows.
Definition 6.4.5. Transformation to marginal normality The transfor-
mation for the jth component Xj is defined for j = 1, • • • , k by
[x? -m,
if
log Xj
where the Xj are chosen to improve the marginal normality of X
^
X^ for j —
1, • • • , k, via maximum likelihood estimation.
It is well known that marginal normality of the components does not imply
multivariate normality; in using the marginal transformations in the previous
section, it is hoped that the marginal normality of XjXj ^ might lead to a trans-
formed x vector which is more amenable to procedures assuming multivariate
normality. The following set of marginal transformations was proposed by An-
drews et al.
(1971) in order to achieve joint normality for the transformed
data.
A j 7^ 0
if
A j = 0, and Xj > 0
(6.4.8)
Definition 6.4.6. Transformation to joint normality.
Suppose XJXJ\
j = 1, • • • , k denote the marginal transformations, and suppose the vector A =
(Ai, - - - , Afc) denotes the set of parameters that yields joint normality of the
vector x^
A) —
( X [ Xl\- - ' ,x£
Afc
^ )'. Suppose the mean and covariance of this
multivariate normal distribution are\i and E. The joint density function of x is
/(x; /u, E, A) = (27r)-fc/2|Sr
1/V|exp{-i(x( A)
(6.4.9)

CHAPTER 6.
SAMPLING FROM NORMAL DISTRIBUTION
212
where the Jacobian of the transformation is given by
k
J = n*jAi
) .
j=1
Given N independent observations xi , • • • , xjv, the estimate of A, along with\i
and E is obtained by numerically maximizing the likelihood function which has
the form in (6.4.9). There are some situations where nonnormality is manifest
only in some directions in the /^-dimensional space.
Andrews et al.
(1971)
suggested an approach (a) to identify these directions, and (b) to then estimate
a power transformation of the projections of xj, - - - , x;y onto these selected
directions in order to improve the normal approximation.
Exercises
6.1. Express the maximized likelihood function in (6.1.10) as a function of the
generalized variance |S/v| of the random sample xi , • • • , XAT -
6.2. Prove the identities (6.1.4).
6.3. If SN ~ W*(I!, N —
1), and a = (ai , - - * ,a*
;)' is an arbitrary vector,
show that maxa(a'Siva/a/Ea) and mina(a'S;va/a'Ea) are respectively the
largest and smallest roots of the determinantal equation |Sjv — cE| = 0.
6.4. Supposes^ ~ Wfc(E, N\— 1) and is independent of S/v2 ~ Wife(£, A2-1).
For any a = (ai , • • • , a^)', find the distribution of a'S^a/a'S^a.
6.5. Derive the joint distribution of Sjj , Su and r.
6.6. The following table gives the means and the variance-covariance matrix
of four variables Ao, Ai , A2, and A3:
Xo
X!
A2
X3
Means
A0
60.516
0.998
3.511
21.122
18.3
15.129
23.860
1.793
14.9
54.756
3.633
30.5
18.225
Ai
A2
A3
7.8
(a) Compute the multiple correlation coefficient of AQ on Ai , A2 and
AS.
(b) Compute the partial correlation coefficient between Ao and A3, elim-
inating the effects of Ai and A2.
6.7. Suppose S is partitioned as in (6.2.9). Evaluate
_
k
^
in terms of |S|,
IS^ I and s00.

213
Exercises
6.8. When p0(i, - ,fc) = 0, show that the statistic Po(i , -- ,fe) f°U°ws a Beta dis-
tribution. What are the parameters of this distribution?
6.9. Find E(dj ), where dj are the squared generalized distances.
6.10. Let x ~ iV(/x, E). Show that bitk and 62,k defined in (6.3.3) and (6.3.4) are
invariant under a nonsingular linear transformation y = Px + q (where
P is nonsingular). Discuss whether these coefficients depend on /z and E.


Chapter 7
Inference for the General
Linear Model
In this chapter, we describe classical inference for the general linear model that
we introduced in Chapter 4. The least squares estimation did not require any
distributional assumptions on the model errors. However, in many cases, one
is interested in constructing interval estimates for the parameters of interest, as
well as in testing hypotheses about these parameters or functions of these para-
meters. Parametric inference proceeds by assuming that the errors are generated
by some probability model. The most widely used assumption is that of normal-
ity, and the multivariate normal distribution was defined in Chapter 5. We also
introduced some alternate distributions that enable us to incorporate skewness
and heavy tailed behavior in the distribution of the model errors. In Chapter
6, we saw how it is possible to assess the multivariate normal assumption in
practice, and introduced transformations to approximate normality. This has
the advantage of allowing for greater modeling flexibility, thereby enhancing the
scope of these models. The properties of the multivariate normal distribution
ensures that the least squares estimates of the general linear model parameters
are also multivariate normal, as Result 7.1.1 shows. Using results from distrib-
utions of quadratic forms that were discussed in section 5.4, we derive tests of
hypotheses.
7.1
Properties of least squares estimates
Suppose the error vector e in the general linear model (4.1.1) follows a multi-
variate normal distribution, i.e., e ~ iV(0, <72Iw) with pdf
e'e }
i "
i
i
f (e; cr2IN )
exp{
(27rcr2)N/2
2a2
1
&
2}
e e nN .
exp{
(27TCT2) 7V/2
2<72
2— 1
215

216
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
In Chapter 4, we derived the least squares solutions 0° and a2. For the general
linear model, we show that the least squares estimate of any estimable function
of (3 has a multivariate normal distribution. We derive the mean and variance
of this estimator, and discuss the full rank model as a special case. Based on
the point estimates and precision estimates for estimable functions of /3, we
construct joint confidence region and marginal confidence intervals. We develop
test procedures for various hypotheses of interest about estimable functions of
0. In (4.2.12), we introduced the ANOVA decomposition of the total sum of
squares (SST ) into the model sum of squares (SSR) and the error sum of squares
(SSE ). Using results from Chapter 5, we now derive the distributions of these
quadratic forms in normal random vectors. We also derive the distribution of
the fitted vector and the residual vector.
Result 7.1.1. Let e ~ N (0, <J 2IN )- Then,
1. /3° has a singular normal distribution
(3
Q ~ S N (H/3, <T2GX'XG'),
(7.1.1)
where H = GXX.
2. y ~ SW(X/3, <T2P), and e ~ SN [0,a2(I — P)].
3. SSE has a scaled chi-square distribution:
SSE ~ XiV-r *
(7.1.2)
<T2
4. 0° and SSE are distributed independently.
5. The model sum of squares has a scaled noncentral chi-square distribution:
SSR
1
~ *
2 (r, A),
where\= — 0' X' X0.
(7.1.3)
a2
6. SSR and SSE are independently distributed.
7.
(l'X/3)2
SSM ~ x
2d
(7.1.4)
) •
cr2
2N <72
8. SSM is distributed independently of SSE.
Proof. The model function is y = X/? + £, and y ~7V(X/3, (T21N )-
Since
0° = GX'y is a linear function of
y, the proof of property 1 follows from
Result 5.2.6.
We prove property 2 by applying Result 5.2.6 as well.
Since
SSE = y'(I- XGX')y, property 3 follows from Result 5.4.4 since (I- XGX')
is idempotent with rank N — r. Since0° = GX'y, and G(X/ — X'XGX
/)(T2 = 0

7.1. PROPERTIES OF LEAST SQUARES ESTIMATES
217
(see Exercise 3.4), property 4 follows from Result 5.4.6. Property 5 follows di-
rectly by observing that XGX' is an idempotent matrix of rank r, so that
SSR/a2 has a noncentral chi-square distribution with noncentrality parameter
A = /3'X'X/?/2cr2. Independence of SSR and SSE is proved by invoking Result
5.4.7 since (XGX
/)[I — XGX'] = 0. From properties 2, 4 and 5, and Result
5.3.9, it follows that the ratio ( N — r )SSR/rSSE has a noncentral F distrib-
ution with noncentrality parameter A. Recall that JN = 11' /N is symmetric
and idempotent of rank 1. We can write
SSM/a2 = y'll' y/a2
so that property 8 is a direct consequence of Result 5.4.7. Similarly, indepen-
dence of SSM and SSE follows from Result 5.4.7, using the fact that
( l l f / N )(l — XGX') = 0.
Ansley (1985) derived these and related distributions via the Q R algorithm.
Result 7.1.2.
1. Let cj /3, j = 1, • • • ,s denote s estimable functions of /3 (see Definition
4.3.1). Then
C'(3° ~ N (C' f3,<T2C'GC ),
where C = (ci, • • • ,cs) is a p x s dimensional matrix of known coefficients
which satisfies C'H = C'. Also, C'GC is nonsingular provided ci, • • •
are LIN.
2. For a given 5-dimensional vector of constants d,
Q/a2 = (C'(3° - d),(C'GC)_1(C,
/3°- d) ~*
2(s, A)
distribution, with A = (C'/3- d)'(C,GC)_1(C'/3-d) /2a2 .
cS
Proof. From property 1of Result 7.1.1and Result 4.2.6, it follows that C'(3° ~
Ar(C'H/?,o-2C'GX'XG'C). Since C'- T'X, using Exercise 3.4, we see that
C'GX'XG'C = C'GX'XG'X'T = C'GX'T = C'GC, and the result follows.
We state some results for the full rank case in the following corollary. In the
full rank linear model, (3 is estimable, as is any linear function of (3. Further, the
least squares estimate of /?, in addition to being BLUE, is also the minimum
variance unbiased estimate ( M V U E ).
Corollary 7.1.1. When r(X)
1. 0 ~ Np(P,tT2(X'X)_1).

218
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
apy , a'/? ~ N (a.'(3,a2a.' ( X ,X )-1a).
2. For any given vector a = (oi , •
•
3. /3'X'X/?/ <72 ~ X 2{P, A), where A = 0' X' X{3/ 2a2.
4. Cov( /3,e) — 0, so that (3 is independent of S'S'is’.
5. SSE /a2
{N - p )a2 jo2 ~ x^
_p.
Proof. The proof is left as an exercise.
Example 7.1.1. We continue with Example 4.1.1. For the simple regression
model, it may be shown that
N
A- N^^/Y^
i X i - X )2 )
2=1
N
N
A ~ N ((30,J2 X?a2 /Nl>2( Xi ~ X)2)’
2=1
2=1
The distributions of Po and Pi involve the unknown error variance a2. The
estimate of o2 is the sample variance of the residuals, i.e.,
- Ehn/ ( N - 2) = Et=i (yi - 00 - AX > )2/ ( N - 2),
where £* = Yi — Y* is the 2th residual. The residual variance d2 is often called
the residual mean square and provided the model assumptions are met, it is an
unbiased and consistent estimator of a2. The residual degree of freedom (d.f.)
is the sample size N minus the number of estimated model parameters, here 2.
It may be shown that ( N — 2)cr2 / a2 has a chi-square distribution with N — 2
degrees of freedom. Substituting the value of a2 for the unknown quantity a2,
we obtain estimated variances of Po and pi as well as their estimated covariance.
/s
Let us denote the estimated standard errors of Po and p\ respectively by s^
o
and
. The estimates of Po and Pi and their associated standard errors will
be used in the derivation of confidence intervals and hypotheses tests.
Example 7.1.2.
Suppose y = X/3 + e, X is an TV x p matrix with
r(X) = p, and s ~ iV(0, <J2V), where V is a known p.d. matrix. Let PGLS
denote the GLS estimator of /3. It is easy to verify that the matrix PGLS =
X(X'V- 1X)-1X' is symmetric, but not idempotent. By Result 5.2.6, PGLS ~
N (P, <J2(X/V“ 1X)~1). We verify that
Q = ( y - X0GLs ),y ~ 1( y - X0GLS )/V 2
has a Xjv-p distribution, and that Q/ ( N - p) is the MINQUE of a2. Now,
Q/a2 = [y- X(X'V-1X)-1X'V-1y],[V-1/^
2][y - X(X'V-1X)-1X'V-1y] =
y'[I - XfX'V-^X^
X'V-YtV-1/^]!!- X(X'V-1X)-1X/V-1]y = y'Ay,
say. We can verify that cr2AV is idempotent, with r [a2AV) = tr(a2AV) =

7.2. GENERAL LINEAR HYPOTHESES
219
tr(I N ) - £r[(X'V_1X)(X'V_1X)-1] = ( N - p) , and p' X' AXp = 0, so that
Q/a2 ~ x2( N
= 0), i.e., Q/cr2 ~ xh-p- Then- E [Q/ ( N ~ P )] = &2 , and
since y ~ N ( Xp,a2V ), Q/ ( N —
p) is the MINQUE of a2. Further, YQLS =
X(X'V_1X)_1X/V_1y = PGz,sy is the fitted response vector.
7.2
General linear hypotheses
In this section, we derive tests for hypotheses about certain parametric linear
functions of (3. The hypothesis
H : C'/3- d
(7.2.1)
is called a general linear hypothesis, where C is a p x s matrix of rank s with
known coefficients and d = (d\, • • • , ds )
f is a vector of known constants. This
can also be written as
H : c[ p = duC20 = d2," - ,c'5/3 = ds.
(7.2.2)
We assume that r(C) = s, since otherwise, some of the relations in H are
redundant, and may be obtained from the others. In practice, we expect that
such redundant hypotheses about parameters have been eliminated.
Unless
r(X) = p, not all such linear hypotheses are in general, testable. A hypothesis
is said to be testable if C' fi can be written in terms of estimable functions of
/3; otherwise, it is a non-testable hypothesis. The matrix C must satisfy the
condition T'X = C' or C'H = C' in order that the hypothesis H is testable
(see section 4.3). To obtain a solution for /3 under the restriction imposed by
H , we use the approach in section 4.6, and minimize the function S((3) subject
to C' /3= d. The equations corresponding to (4.6.2) and (4.6.3) are
X' X /3% + CA = X'y,
C'/& = d,
(7.2.3)
where 2A denotes the vector of Lagrangian multipliers. Provided C'(3 — d is
estimable, the restricted (under H ) least squares solution of (3 has the form (see
(4.6.6))
(3°H = (3° ~ GC(C'GC)“ 1(C'/3°- d).
(7.2.4)
7.2.1
Derivation of and motivation for the F-test
We develop a statistic based on the F-distribution to test the (testable) null
hypothesis H : C' /3 — d. We first define testable hypotheses.
Definition 7.2.1. Consider the normal general linear model y = X/3+e, where
r(X) = r < p. A hypothesis H : C'/3 = d is testable if the s rows of C' are
linearly dependent on the rows of X; i.e.,
— t'X, or equivalently, if there
exists an N x s matrix T such that C' = T'X. Equivalently, a hypothesis
H : C'(3 = d is testable if and only if C'H = C'.

220
CHAPTER 7.
INFERENCE FOR THE GENERAL LINEAR MODEL
Assume that (7.2.1) is a consistent system of equations. When
Result 7.2.1.
y ~AT(X/?,cr2I;v), the test statistic
Q/ s
F ( H ) =
~ F ( s, N - r, A),
(7.2.5)
SSE/ ( N - r)
where the terms in this expression are defined below.
Proof. When y ~7V(X/3,<T2I), 0° ~ SN ( H0,a2GX' XG' ) (see Result 7.1.1),
and further,
C'/3° - d- NS {C'(3 - d, C'GCcr2).
It is easy to verify that the matrix C'GC is nonsingular. Consider the quadratic
form
Q = (C'/3° - d),
[C/GC]
_1(C'/5°- d).
(7.2.6)
By Result 5.3.3,
Q/ CT2 ~ X 2( s,A),
A =^
j(C'0- d)'[C/GC]-1(C'/?- d),
a noncentral chi-square distribution with s degrees of freedom and noncentrality
parameter A. Note that under the null hypothesis, A = 0 since C'/3 = d, and
the null distribution of Q/a2 is a central chi-square with s degrees of freedom.
We have already seen that SSE/ a2 ~
now show that Q and
SSE are independently distributed. Substituting /3° = GX'y, using C; = T'X
and simplifying, we can write
=
[y - XC(C'C)-1d]'XG'C[C,GC]
_1C'GX'[y - XCtC'C^
d]
=
[y- XC(C'C)-1d)
/A1[y- XC(C
/C)-1d],
say. We can also write
(7.2.7)
Q
(7.2.8)
(y - X/3°)'(y- X/?°)
y'[I- XGX']y
[y- XC(C
/C)-1d]'[I - XGX'][y - XC(C'C)-1d]
[y- XC(C'C)-1d],A2[y- XC(C'C)-1d].
It is easily verified, either algebraically, or geometrically that At A2 = 0, so
that by Result 5.4.7, Q is distributed independently of SSE. Hence, by Result
5.3.6, the statistic
SSE
(7.2.9)
{Q/ sa2 } / { SSE/ ( N - r )a2 }
{Q/ s } / { SSE/ ( N - r)} ~ F(s, N - r, A)
F( H )
i.e., a noncentral F-distribution with numerator degrees of freedom $, denom-
inator degrees of freedom N — r and noncentrality parameter A. Since A = 0

7.2. GENERAL LINEAR HYPOTHESES
221
under the null hypothesis, the statistic F( H ) has a central F$^-r distribution
under H.
Let CIH denote the estimation space Cl reduced by the hypothesis H. Fig-
ure 7.2.1 illustrates the F-test geometrically. The F-test amounts to checking
whether d belongs to the 100(1 — a)% confidence ellipsoid for C'/?, which is
centered at C'/?°, i.e., the ellipsoid
{C' p : (C' f3- C,
/3°)'[C'(X,X)
_C]-1(C'/3 - C'/3°) < sa2Fs,N-r }.
(7.2.10)
(V -
- v H )
/ .
/
Y
K
z
/
^
(3
/
Figure 7.2.1.
Geometry of the F-test.
In the simple regression model, we have seen that the
Example 7.2.1.
estimation space Cl is a plane spanned by IN and x = (Xi, - - - , X/v)'. The
estimation space CIH reduced by the null hypothesis H : C'0 — d, i.e., H :
c0/?o + ci fa
~ d is a straight line formed by combining the vectors dx/ci (of
constant length), and 0O{ IN
cox/ci} (of variable length). This straight line
is parallel to ljv — cox/ci, but displaced by a distance of dx/ci. Clearly, CIH
is a subset of Cl, while Cl — CIH is the space spanned by all vectors in Cl that
are orthogonal to CIH-
When p = 2, and 5 = 1, two vectors span Cl, while
CIH and Cl — CIH are each spanned by a single vector denoted respectively by
v = 0Q1N 4- (d — coA>)x/ci, and (v'x)l^ - (v'l^v)x.
Consider the model (4.1.2). We derive the F-test for
Example 7.2.2.
H : 0j —
dj, where dj is a fixed constant, 1 < j < k. We can write the
hypothesis as H : C'0 = d, where C' is a p-dimensional vector with 1 in the
( j + l)th position and zero elsewhere. Straightforward calculation yields
so that under H ,
F( H ) = Q/ { SSE/ ( N - p) } ~ Flf Ar-p.
Let A\ and A2 denote objects of unknown weights 0\ and
Example 7.2.3.
02 respectively. The weights of A\ and A2 are measured on a balance using the
following scheme, all of these actions being repeated twice:

222
CHAPTER 7.
INFERENCE FOR THE GENERAL LINEAR MODEL
(1) both objects on the balance, resulting in weights Y^i and
(2) A\ on the balance, resulting in weights i^.i and >2,2 > and
(3) ^2 on the balance, resulting in weights Y34 and
2-
We assume that Y^
’s are independent, normally distributed variables, with
common variance a2 . We also assume that the balance has an unknown sys-
tematic error 0Q . We wish to test the hypothesis that both objects have the
same weight. The model can be written as
Yl .l = Yl ,2
*2,1 = *2,2
y3,i = *3,2
where Si are iid N (0 > a2 ) variables, i = 1, 2, 3. Let Y
= (i*,1 + Yi^
)/ 2. It is
easy to verify that
00 +0i + 02 + £ 1
00 A0i + £2
0o + 02 + £3,
-Yi_+ y2_+ y3.
y>
. - y3.
yi . - y2.
with
3/2 -1
-1
Cov(/3) = a2
1/2
,
-1
1
-1
1/2
1
and that 0 has a normal distribution with mean 0 and this covariance. Also,
The test of H \ 0\ ~ 02 can be written as H : C'0 = d, with C = (0,1, — 1)
and d = 0. The test statistic is
F( H ) = (y2.- y3.)2/*
2- FI,3
under if.
An alternative derivation of the F-statistic is shown below. The least squares
solution for 0 in the linear model y = X/? + e subject to the linear restriction
H : G0 = d was given in (7.2.4). We see that
(y- X/J°,)'(y- X/3° )
[y - X/3° + X(/3° - /^
)]'[y - X/3° + X(/3° - /&)]
(y- X/3°)'(y - X/3°) + (0°-0° )'X'X(/3° - (3°H )
SSE + (C'/3°- d),(C'GC)_1C'G,X,XGC(C'GC)_1(C'/3°- d)
(7.2.11)
SSEH

7.2. GENERAL LINEAR HYPOTHESES
223
since X'(y - X/?°) = 0. Estimability of C' fi implies that C' = T'X, so that
C'G'X'XGC(C'GC)-1 = I. It follows that SSEH = SSE + Q, where Q was
defined in (7.2.6). The statistic for testing H : &(3 = d may be written as
( .SSEH - SSE )/ s
F ( H ) =
(7.2.12)
SSE/ ( N - r )
which has an FSIN-T distribution when H is true. Golub and Styan (1973) have
suggested a numerically efficient procedure for computing Q.
Here is a motivation for the F-test statistic. Let us consider the quantity
Q/ s.
We have seen that Q/a2 has a noncentral y2(s, A) distribution, with
A = (C'(i - d)'[C'(X'X)-1C]
_1(C,/3 - d)/2<r2. By Result 5.3.4, we have
E(Q/ a2 ) = a + 2A,
so that
2 + (C' p - d)
/[C'(X/X)_ 1C]-1(C'/3 - d)/s.
We know that E(a2 ) ~ a2.
Under H : C' /3 — d = 0, the second term in
E(Q/ s ) becomes zero, and E(Q/ s) — a2. We therefore expect that under H )
the statistic F ( H ) is approximately equal to 1. When the null hypothesis is
false, E(Q/ s ) > cr2 = E(a2 ). Now,
E(Q/ s ) = a
E{ F ( H ) }- E(Q/ s )E(l /a2 )
and we know from the Markov inequality that E(\/d2 ) >\/ E(a2 ). Therefore,
E{ F ( H ) } > E(Q/ s ) / E(a2 ) > 1,
and H is rejected if F( H ) is significantly large.
Example 7.2.4. Sets of regression lines.
Consider L regression lines
Yi,i = 0i ,o + PitiXi,i
i — I, - - * , n/, l =!, • • • ,i,
(7.2.13)
which can be written in the form (4.1.1) with
( £lil ^
/ Yltl\
( @1’°\
Yhni
Y2 , I
£l ,ni
£2,1
0L,0
Pi ,i
,
P =
£ =
y = ^2,n2
£2 ,Tl2
\PL,IJ
YL,i
i
\YL,UJ
\£L,nL/

224
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
( 1
0
• • •
0
Xxj
0
0 \
1
0
• • •
0
Xlini
0
1
• • •
0
0
0
*2,1
' ’ '
0
0
X =
0
*2,712
•
0
1
• • •
0
0
0
*L,1
0
0
• • •
1
0
*L,71L/
where E(eiti ) — 0, Var{ei,i ) = a2, and ei/ s are uncorrelated, for i = 1, - • • , n;,
l = 1, • • • ,L. Here, N —
nh P — 2L, and r( X ) = 2L. The least squares
estimate of the parameter vector /3 is obtained by minimizing the function
S( /3) = (y- X0 )' ( y- X/3) and the coefficients are
«i
~ni
H
E (*u -*i )2
i=1
Yt.- ft ,i*|.
where Yu = E"=i Yi,i/nu and Xt. = YliL\ Xl ,i/ni > for l = 1, • • • , L. Then
\0
0
0
1
0
0
i=l
(7.2.14)
0i,i
(7.2.15)
01,o
L
rii
ni
= E Efti-^
i )2-©^-^
)2
/=1 U=1
2=1
L
Til
L
Til
= E D y u -n)2 -K I E^
M - X,.)2.
(7.2.16)
i=l i=l
Suppose we impose the linear restriction that the slopes of the L lines are the
same, i.e., we hypothesize that
/=i
i=1
H : pi
%1 = 02%l = :.=0Lti = P.
The least squares estimates of (3ito, l = 1, * • • , L and (3 are obtained by mini-
mizing the function
L
ni
S(/Jlf 0, • • • ,0L.O,« =
(«.< - A,o - /J*w)2
i=ii=i
with respect to
/ = 1, • • • , L, and /?. The normal equations are
ni
X>,i - Pi
— 0H^
l ,i ) =
/ = 1, • • • ,1/,
,0,Z/
i=l
L
rij
EE(yoi-0t ,O,H ~ 0HXi,i )X^
i = 0;
/=1 2=1

7.2. GENERAL LINEAR HYPOTHESES
225
solving these simultaneously, we obtain the least squares estimates of the para-
meters under the reduction imposed by H as
Pl,0,H
Yt.- faXi.,
L
Til
E E XiAYi'i - Yt . )
l=li=l
(7.2.17)
PH
(7.2.18)
EE Xi,i { Xiti - Xi. )
l=li=l
Clearly, SSE is the error sum of squares in (7.2.16), while
L
ni
SSEH
=
-^
(XM -Xt.)}2
i=l i=l
L
ni
= fY {Yl,i -Yl.)2 - faYTi{xl,i - xl.)2
L
Til
(7.2.19)
1=1 i=1
l=li=1
so that
L
ni
L ni
SSEH - SSE = £/??£(xM - Xi )2 -^EE (^
i- Xi.)2.
1=1
2=1
/=lt=l
The test statistic is
{SSEH - SSE )/ ( L-1)
F ( H ) =
(7.2.20)
SSE/ ( N - 2L)
which follows an i7L-i,N-2L distribution under H.
In Chapter 8, we will further explore inference for sets of regression lines.
We next discuss the relation between the F-test statistic and the coefficient of
determinant R2 in the full-rank model.
In the model (4.1.2) where e* are iid 7V(0, a2 ) variables,
Result 7.2.2.
consider the test of H : f3\ = 02 —
* * * = Pk = 0. The F-test statistic can be
written in terms of the coefficient of determination as
F ( H ) = ( N - k - l )R2/ [k( l - R2 )} ~ FkiN
under H. The null distribution of R2 is Beta(k/ 2, ( N — k — l)/2).
-/c-l
Under /7, the model is V* = /3Q 4-
with /J# = Y and SSEH =
Proof.
SST —
~ Y )2- From (7.2.12) and Definition 4.2.4, the form of F( H )
in terms of R2 follows. Since F ( H ) ~ Fk ,N
of R2 follows directly from (A14).
under //, the null distribution
— k— 1
In Chapter 4, we introduced sums of squares associated with the general
linear model. In particular, we distinguished two ways of writing the ANOVA

226
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
decomposition, SST = SSR+ SSE, or SSTC = SSRC 4- SSE, where SST , and
SSR denote the total and model sums of squares respectively, SSE denotes the
residual sum of squares, while SSTC and SSRC denote the mean corrected total
and model sums of squares. SSM is the sum of squares due to fitting the mean.
We now present the ANOVA table in special cases.
Example 7.2.5. Suppose we wish to test the hypothesis H : X/3 = 0 in the
general linear model (4.1.1). Since X/3 includes all estimable parametric func-
tions of (3, this hypothesis requires that all estimable parametric functions are
null. Clearly this is a testable hypothesis. The F-statistic has the form
F( H ) ={SSF/r} /{ SSE/ { N - r)}- F(r, N - r, /3'X'X/3/2cr2);
under H , the noncentrality parameter is zero since X/? = 0, so that the null
distribution of the test statistic F( H ) is an Frjjv~r distribution. It is not sur-
prising that the numerator degrees of freedom is r, since there are exactly r
LIN estimable functions in X/3 (see Result 4.3.3). It is usual to represent this
information via an ANOVA table as shown in Table 7.2.1.
Table 7.2.1. ANOVA table for Example 7.2.5
MS
Source
d.f.
SS
SSR = /3u'X'y
SSE = y'y - /?°'X'y
SST = y'y
MSR= SSR/ r
MSE = SSE/ {N - r)
Model
Residual
Total
r
N - r
N
It is important to keep in mind that this statistic does not provide a test of
(3= 0; in fact, unless r(X) = p, (3 is not even estimable, so that (3 = 0 is not a
testable hypothesis. In the full rank case when (X'X)-1 exists, the hypotheses
X/3 = 0 and (3= 0 are equivalent, since X/3= 0 implies that X'X/3 = 0. It is
left as an exercise to the reader to verify that SSR, and hence the F-test is
unchanged if we replace X/3 = 0 by L/3 = 0, where L/3 is a different set of r
LIN estimable parametric functions of /3. The ANOVA table separating out the
mean fitting is shown in Table 7.2.2.
Table 7.2.2. ANOVA table separating out the mean
SS
MS
Source
d.f.
SSM = NY
SSRC = P0,X' y-NY
2
SSE = y'y- /3^
X'y
SSTC = y' y-NY
2
MSM = SSM/1
MSRC = SSR/r
MSE = SSE/ ( N - r )
Mean
Mean Corrected Model
Residual
Total
1
r — 1
N - r
N

7.2. GENERAL LINEAR HYPOTHESES
227
Under the null hypothesis H : X0 = 0, FM = MSM /MSE ~ Fi
Fc = MSRc/ MSE ~ Fr- xyN-r. While FM tests whether E(Y ) = 0, Fc tests
for effects apart from the mean.
while
N ~r t
Example 7.2.6.
Consider the linear model
ft + 3ft + \
201 — 02 + £2
3ft - 4ft + e3,
where e* are iid N (0,a2 ) variables, i — 1, 2,3. We derive the F-statistic for
testing H : 0\ — ft, which can be written as C'/3 = d, with C' = (1,-1),
0 = (ft,ft)', and d = 0. We can verify that 0 and I — P are
> 2
Yz
25
— 65
35 \
-65
169 -91 ,
35 -91
49 /
i ( 59Yi + 41F2 4
* 34F3
53^ + 8F2 “ 23Fs
I _ p —
JL_
,
i
JT
243
0 = 243
C'/?-d = (6Yi+33F2+57y3)/243, C' { X' X )~lC = 2/27. Also, a2 = y'(I - P)y.
The F-test statistic has the form in (7.2.5), and has an F^i distribution under
H .
Example 7.2.7.
effects ANOVA model, our primary interest is in testing equality of the a treat-
ment means, viz.,
We continue with Example 4.1.3. In the one-way fixed-
H0
:
(i\ = 1x2 =
* • • = Ma, versus
Hi
:
/.^ ^
fij for at least one pair (z, j),
or equivalently, the hypothesis that the a treatment effects are equal, viz.,
Ho
: n = r2 = • • • = ra, versus
:
T*
~f~ Tj f°r
least one pair (z, j).
The ANOVA identity leads to the orthogonal partitioning of the total variability
in the response variable Y into its component parts:
£“=i E]U (Yij -F..)2-£“=1 MYi.-F..)2 +H=1
-F )2
•
The term on the left is the total corrected sum of squares SSTC. The first term
on the right side is a sum of squares of the differences between treatment means
and the grand mean and is called the treatment sum of squares, SSTr, while the
second term on the right is the sum of squares of the differences of observations
within a treatment from the treatment mean and is the error sum of squares,
SSE. Symbolically, we can write the decomposition as
SSTC = SSTr + SSE.

228
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
This algebraic identity is easily verified by seeing that NY .. = Y..=
n/Yi..
Each of these sums of squares is a quadratic form in an N-dimensional nor-
mal random vector y = (Yn,Yi2>,
• * ,Ylni, • • • , Yai, Ya2, • • • , Ya >rla )'. It may
be shown using standard linear model theory for normal random variables that
SST / a2 and SSTr / a2 have chi-square distributions with N — 1 and a— 1de-
grees of freedom respectively when Ho is true, while SSE / cr2 has a chi-square
distribution with N — a degrees of freedom. Moreover, SSTr and SSE are in-
dependently distributed. We define the mean squares, MSTr = SSTr/ (a — 1)
and MSE = SSE/ ( N — a); it is easily seen that E( MSE ) = a2, while
E( MSTr ) = G2 4-
n* r2/ {a-1).
The ANOVA identity provides us with two alternate estimates of
<r2, one
based on the variability within treatments and the other based on the variabil-
ity between treatments. Specifically, MSE is an estimate of a2 and MSTr
estimates a2 under Ho. If there are significant differences in the treatment
means, then E( MSTr ) exceeds a2. The F-statistic for testing Ho compares
MSTr with MSE. Prom the independent chi-square distributions of SSTr/a2
and SSE/a2 under i?o, it follows that the ratio
F( Ho ) = { SSTr/ {a- 1)} /{ SSE/ { N - a) } = MSTr/ MSE
is distributed as a Snedecor’s F-distribution with a — 1 and N — a degrees of
freedom. F( HQ ) is the test statistic for testing HQ. These details are summarized
in Table 7.2.3.
Table 7.2.3. ANOVA table for the one-factor model
d.f.
SS
MS
Source
SSTr = J2ni(Yi.- Y ..)2
MSTr = SSTr/ {a- 1)
Treatment
a — 1
i=1
a
ni
S S E = Z Z (Y i j -Y i )2
i=lj=l
MSE = SSE/ ( N - a)
Residual
N - a
Corrected
rii
SSTC = EZ (Yij -Y ..)2
Total
N - 1
We saw that under
both MSTr and MSE are unbiased estimates of a2.
Under the alternative hypothesis however, E( MSTr ) > a2. Hence, if the null
hypothesis were false, the expected value of the numerator would be significantly
greater than the expectation of the denominator, and consequently, we would
reject Ho for a large value of the test statistic F(if0). That is, we reject Ho
at a% level of significance if F( Ho ) > Fa-i ,N
the upper a% critical point from an Fa-i , jv-a distribution. Alternatively, in
practice, we may compute the p-value of the test as P{ F > F ( Ho )}-
Most
statistical software give the p-value of this upper-tailed test. If the p-value is
less than a, we reject Ho.
where, Fa_i,^_
aia denotes
-a,or,

7.2. GENERAL LINEAR HYPOTHESES
229
Numerical Example 7.1. Fixed-effects one-factor model. Four speci-
mens (n) of each of five brands (a) of a synthetic wood veneer material are
subjected to a friction test. A measure of wear, Y, is determined for each spec-
imen. All tests are made on the same machine in completely random order.
The five brands are ACME, CHAMP, AJAX, TUFFY, and XTRA. The brands
ACME and AJAX are produced by a U.S. company A-line while CHAMP is
produced by a U.S. company C-line. TUFFY and XTRA are produced by a
foreign company (see Littell, Freund and Spector, 1991). The ANOVA table is
shown below.
ANOVA table for Numerical Example 7.1.
Source
d.f.
SS
MS
F-value
Pr > F
Model
4
0.617
0.154
Error
15
0.313
0.021
Corrected
Total
19
0.930
7.40
0.0017
The F-test rejects H : r^
s are equal, i = 1, • • • ,5. We will continue with this
example in section 7.3.
Example 7.2.8.
We continue with Example 4.2.6. The ANOVA decompo-
sition for the main effects model is
SSTC = SSA + SSB + SSE
where
6
n
a
- Y...)2 with N — l = nab-1 d.f.
k=ii=ij=i
a
nby^(Yj.. — Y...)2 with a — 1 d.f.
SSTC
SSA
i=1
6
^^
(Y.j. — Y...)2 with b — 1 d.f.
na
3=1
b
nEEE(% - y
— Y .j. -h Y...)2 with nab — a — b + 1 d.f.
SSE
i -
k=li=lj=l
The ANOVA table is shown in Table 7.2.4. The test statistic for the hypothesis
HA : TI = • • • = Ta, is FA = MSA/MSE, which has an Fa_ iiTia6_a_6+i distrib-
ution under HA- The test statistic for HB : Pi = • • • = /3& is FB = MSB/MSE,
which has an Fs-i ^nab-a-b+i distribution under HB > For the model with no
replication, we set n = 1 in the above formulas. In the balanced model, it can
be shown that the r effects and (3 effects are orthogonal, so that it is irrelevant
which effect we test first. This unfortunately does not hold in the unbalanced
case which is discussed in Chapter 9.

230
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
Table 7.2.4. ANOVA table for the two-factor main effects model
d.f.
SS
Source
SSM = oibnY...
SSA = nbZ (Yi~ - Y ..' )2
Y ...)2
Mean \i
T after /i
/3 after ( fi,r )
1
a — 1
SSB = naYXY+
SSE = ZiXijk -Yi.. -Y .j. +F...)2
ijk
SST = YJijk
6- 1
nab — a — b 4- 1
Error
nab
Total
ijk
The ANOVA table for the model with interaction between Factor A and
Factor B model is shown in Table 7.2.5.
Table 7.2.5. ANOVA table for two-factor model with interaction
SS
d.f.
Source
SSM = abnYt
SSA = nbJ2(Yi..
Mean //
r after fi
(3 after ( fi,r)
(T/3) after ( fi,r, /3)
Error
1
Y...)2
SSB = naJ2(Y .j.-F...)2
SSAB = ntiYij.-Yi.. -Y .j.+F..)2
a - l
i
6 - 1
(a — 1)(6-1)
ab(n - 1)
ij
SSE = UYijk -Yij.)2
ijk
SST = £Y,2
fc
anb
Total
ijk
Let 71. = £*=17ij, 7 j = E“=i 7»j and 7.. = Ej=i E<=i 7y - Although 7i/s
are not estimable, 7*,- = 7^ - 7*. - 7.j 4- 7.., z = 1, • • • , a, j — 1, • • • , 6 are
estimable. We first test for no interaction effects, i.e., HAB
** 7*j — 0 for all z, j,
using the test statistic
= {SSUs/fa- 1)(6 -1)}/ { SSE/ ab(n- 1)} ~ F(a-i ) (6-i),a&(n-1)
under HAB- We next test the hypothesis of no difference among the levels of
Factor B. This does not lead to testing /3i = • • • = /?&, since this has no meaning
in the model with interaction. We test that the effects due to the levels of Factor
B are equal, averaged over all levels of Factor A, i.e., HB - fij 4-Y^
ilij /a equal
for all j, using the test statistic
FB = { SSB/ ( b- 1)}/{ SSE/ ab(n-1)}- F( b_ lhab{ n_
1 }
under HB. Similarly, we test that effects due to the levels of Factor A are equal,
averaged over all levels of Factor B; i.e., we test the hypothesis HA : 7z+£T lij / b
equal for all z, using the test statistic

7.2. GENERAL LINEAR HYPOTHESES
231
FA = { SSA/ (a - 1)} /{ SSE/ ab(n-1)}- F(a_1)}a6(n_1}
under HA.
Numerical Example 7.2. Fixed-effects two-factor model.
An engineer
is designing a battery for use in a device that would be subjected to extreme
variations in temperature. Three choices of the plate material are possible, and
the engineer tests these at three temperature levels. Four batteries are tested at
each plate material/temperature combination, all 36 tests being run in random
order. We make inference based on fitting a two-factor model to lifetimes of the
batteries (in hours) (Montgomery, 1991). The ANOVA table is shown below.
ANOVA table for Numerical Example 7.2.
d.f.
SS
MS
Source
F-value
Pr > F
Model
Plate
Temp
Plate x temp
4
8
59416.222
7427.028
11.00
2
10683.722
5341.861
7.91
2
39118.722
19559.361
28.97
9613.778
2403.444
3.56
0.0001
0.0020
0.0001
0.0186
27
18230.750
675.213
Error
Corrected
Total
35
77646.972
All the effects are significant.
A
7.2.2
Power of the F-test
By definition, the power of the F-test for the general linear hypothesis H is
(SSEH - SSE )/ s
P(RejectFj# is false)
> FS )iv-r,a|F is false
P
SSE/ ( N - r )
oo/
FS ,N-T.O
where the random variable
{ {SSEH - SSE )/ s } /{ SSE/ ( N - r)}
{ {SSEH - SSE )/ {sa2 ) } / { SSE/ { N - r )a2 }
has pdf
<?(£), which we know has a noncentral F(s, N — r, A) distribution (see
Chapter 5). We may derive an explicit expression for g(£\s, N — r, A) using the
distribution of (SSEH — SSE ) and SSF, and transforming to £ (see Kendall
and Stuart, 1963 for details). In order to evaluate the integral involved in the
power calculation, we may use Tang’s tables (1938), which are given in terms of

232
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
a random variable E2 = s£ / ( N — r + s£) ; see Pearson and Hartley (1970) for
charts of the power function.
It is clear that an evaluation of the power function requires an estimate of
the noncentrality parameter A. If U ~ x2(&, A), it is easy to see that (U — k )/ 2
is an unbiased estimator of A. However, since it may assume negative values,
it is not a useful estimator, and we prefer to use (U —
fc)+/2. The MLE of A
does not have a closed form and must be solved numerically (Saxena and Alam,
1982).
7.2.3
Testing independent and orthogonal contrasts
Let c - and c'• denote two distinct rows of the k x s matrix C. The quadratic
forms
Qi
=
/30,
Ct(c£Gci)-1c -/3° = y'XG'c^
c'Gcj)"‘c'GX'y, and
0°'cj(cjGcj)“ 1c' /3° = y'XG'cj(c' GCj) “ 1c'GX'y
Qj
appear as the numerator sums of squares in the F-statistics for testing Hi :
c'/? = 0, and Hj : c' /3= 0. That is,
F( Hi ) = Qi / { SSE/ ( N - r ) } ~ FUN.r
under Hi, so that we reject Hi at level of significance a if F( Hi ) > F\^
can be verified that Qi and Qj are independently distributed if and only if
XG'c*(c •Gcj)
“ 1
GX'XG'cj(c'Gcj)” 1c'-GX' = 0,
which in turn is true if and only if c'GX'XG'cj — 0. Since c' = t
^
X, the
condition is equivalent to c'GX'XG'X'tj = 0. Since X'XG'X' = X', this in
turn is equivalent to c -GX'tj = 0, i.e.,
. It
— r.a
c'Gcj = 0.
(7.2.21)
(7.2.21) gives a necessary and sufficient condition for independence of Qi and
Qj. If (7.2.21) holds, we refer to
and c'•/? as orthogonal contrasts. It can
be verified that when c'GX'tj = 0, for i, j = 1, • • • , a, i ^
jr, then (C'GC)-1
reduces to a diagonal matrix. The statistic for testing the general linear hy-
pothesis H : C'/3 = d, can then be written as
- dl),(c'Gcl)-1(c'/3'
d i )
Q
i=i-(c
= E
=E«<-
c'Gcj
i=1
2=1

7.3. CONFIDENCE INTERVALS AND MULTIPLE COMPARISONS
233
Confidence intervals and multiple compar-
isons
7.3
The least squares approach and the method of maximum likelihood (see section
7.5.1) provide point estimates for the vector /3. Under the assumption of normal
errors, we now construct the joint confidence region for an estimable function
C'/3 as well as marginal confidence intervals for each estimable function c'/3,
j =!> • • • , 5.
Joint and marginal confidence intervals
Let C'/3 denote a vector of estimable functions of /3, where C is a p x s matrix
of known coefficients. We have seen that the least squares estimator of C' /3 is
C'/3°, with covariance
<72C'GC.
7.3.1
The 100(1- a )% joint confidence region for C'/3 is
Result 7.3.1.
(C'/3° - C'(3)' (C
1GO )-1(C
1/3° - C'0) < sa2Fs,N
(7.3.1)
— r,a
denotes the upper a% critical point from an F5^-r distribution.
where FSIN — r,a
Proof.
This result directly follows from inverting the F-statistic in (7.2.5).
In the full rank case, the 100(1 - a )% joint confidence
Corollary 7.3.1.
region for the parameter vector (3 is
(0 - (3)' ( X' X )(0 - (3) < p52Fp,N_p,Q.
(7.3.2)
In Figure 7.3.1, for k = 2, the ellipsoid represents the joint confidence region for
/3, while the marginals intervals for /3i and 02 form the rectangle ABCD.
The marginal 100(1— a)% confidence intervals for s estimable functions c
^
/3,
i = 1, • • • ,s of /3 are given by
C -/J° ± tN ^
(c'Gci )1'
2, i
While each marginal interval contains the true parameter c'/3 with probability
(1- a), the probability that “simultaneously” all s functions c
^
/3, i = 1, • • • , $
are contained in the intervals in (7.3.3) is certainly much less than (1 — a).
We may, of course, construct simultaneous confidence intervals for estimable
functions of /3, as described in section 7.3.2.
(7.3.3)
— r,a
Continuing with Example 4.1.1, we construct confidence
Example 7.3.1.
intervals for the model parameters and functions of the parameters in a simple
regression model by inverting t-tests (or, equivalently, F-tests having one nu-
merator degree of freedom). We can test the null hypothesis HQ : (3\
/3i,o

234
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
using the ^-statistic t —
{0\ ~0\,o )/ s§x
(TV — 2) degrees of freedom under HQ. For a two-sided alternative Hi : (3\ ^
/?i ?o,
the test rejects the null hypothesis at level of significance a if |£0&fl| > t^-2,a/2 >
i.e., the observed test statistic is greater in absolute value than the upper a/2
percent critical value from a t-distribution with (TV — 2) degrees of freedom.
The 100(1 — a)% confidence interval for fix is given by 0i ±
2,a/ 2- Us-
ing a similar procedure, the 100(1 — a )% confidence interval for fio is given by
00 d: S^
tN — 2,a/ 2'
which has a Student ^-distribution with
02
B
C
\
\
X
A
Xk
0
\
^
Confidence region
A
*1
0
Figure 7.3.1.
Joint confidence region for (3.
We continue with Example 4.1.2. From the distributional
Example 7.3.2.
properties of the multiple regression model, it follows that
<T2(X'X)_
1 is an
unbiased estimator of Cov(0 ). Since (i) 0j — 0j , j — 0, • • • ,k has a normal
distribution with mean 0 and variance a2(X'X)~1 (which is the jth diagonal
element of Cov(0)), (ii) (TV — k — l )a2/ a2 is distributed as a chi- square with (TV—
k — 1) degrees of freedom, and (iii) ( N — k — l )a2/a2 is distributed independently
XV
of 0j — 0j, j = 0, • • • , k, it follows that the statistic
^ —
(0j
0j )/ spj
(7.3.4)
has a ^-distribution with (TV-k — 1) degrees of freedom. This framework enables
us to carry out hypothesis tests on the components of 0 as well as to construct
confidence intervals for
f i j . To test the null hypothesis HQ : f i j = 0j ,o , we
substitute (3j$ for (3j in (7.3.4); if the observed value of the statistic is greater
than the upper a/ 2 critical value from a ^-distribution with (TV — A;-1) degrees of
freedom, we reject the null hypothesis at level of significance a. The 100(1— a)%
confidence interval for 0j is

7.3. CONFIDENCE INTERVALS AND MULTIPLE COMPARISONS
235
0j 4:^ p^
N — k— l ,a/ 2‘
D
Example 7.3.3. We continue with Example 4.5.1, where we looked at GLS
estimation of 0 in a model with AR(1) errors. We see that
PGLS ~ N ( p,a
,2(x!
‘V ~1x )~1 ),
and
( N - 1)$GLS ~ XN-I
(see Example 7.1.2). Hence,
( PGLS - P )/ {9GLS ( K'V
1x) 4/2 ~ tN
The 100(1 — a )% confidence interval for 0 is
-i *
\/?GLs(x'V-lx)
1
0GLS ± tjV-ifo/2
where £/v-i,a/2 denotes the upper 100a/2th percentile from a Student’s t-
distribution with ( N — 1) degrees of freedom.
Example 7.3.4.
Suppose
0i 4- £i,
20i - 02 4* 62,
01 4- 202 4 63,
where £* are iid 7V(0, <T2) variables, i = 1, 2,3. We construct a 95% confidence
interval for 0 = /?i 4 3/?2- By Corollary 7.1.1, the b.l.u.e. of 6 is 6 — /?i 4 3/?2 =
(5Fi-8Y2+ 41Ys)/30, while a2 = E -=i *7-|(*i + 2T2+Y3 )2 + ±(-F2 + 2F3)2-
We can verify that _E(0) = 9 and est.Var(6 ) = 5952/30. The 95% confidence
interval for 6 is the f-interval
Yi
Y2
Y3
6 ± t K O25(59a2/30)1/2.
Example 7.3.5.
Consider the linear model
1 1
0
1
0
1
1 1
0
01
Yi
£i
02 I 4
62
,
Y2n
03
\S3
where e ~ iV(0, cr2I). We construct a 95% confidence interval for 0\ 4 02/ 3 4
2/?3/3. First, see that since r(X) = 2, we must verify that the linear function of
0 is estimable. Given c' = (ci, 02, 03), it is easy to verify using (4.3.3) that cf0
is estimable if and only if c\ = c2 4 c3 (see Example 4.3.1). Based on
(Yi 4 y3)/2
/0
1/2
0\
G = (X'X)- =
0
0
0 ) , and 0° =
\1 -3/2
0/
the least squares estimate of 0\ 4 /32/3 4 2/?3/3 is c'/?° = (Yi 4 4Y2 4 Y3)/6.
Using (7.3.3), the 95% confidence interval of o!0 is
c' P° ± (2l.o25ff(c'Gc)1/2.
0
Y2- (Yi + Y3)/2

236
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
Simultaneous confidence intervals
The marginal 100(1- a )% confidence intervals for s contrasts c£/3, i = 1, • • • , s
given in (7.3.3) provide coverage probability of (1- a) for each dfi. For simul-
taneous inference, we wish to claim in general that P[c'/3 e ICi , c*
S ] = 1 — a,
where S is a finite or infinite collection of such vectors c* ; then, {/Ci, Cj
S }
is called a “family” of confidence intervals with confidence coefficient (1 — a).
Let T = { Sj } = {5i, • • •
denote a family of statements in a confi-
dence interval estimation or a hypothesis testing problem, where N (T ) is the
number of statements in the family. Let NW {T ) denote the number of incorrect
statements in T.
7.3.2
For N(T ) < oo, the error rate
Definition 7.3.1.
Er{T ) = NW (P )/N {P )
(7.3.5)
is a random variable.
The probability of a nonzero error rate is P{ Er(P) > 0}. The probability
that at least one statement in T is incorrect is
P(J=) = P{ NW {P )/N (T ) > 0}= P{ NW (P) > 0}
so that 1 — P(P) denotes the probability that all statements in T are correct,
and provides a simultaneous probability statement for T. If N (P) < oo, the
expected error rate is
E(P ) = E{ NW (P )/N (P )}.
Let I(Sf ) = 1 if Sf is an incorrect statement in T , and I {Sf ) = 0 if S/ is a
correct statement. Let us denote the level of significance by a/ = P{ I ( Sf ) =
1} = P( Sf is incorrect ) = E{ I ( Sf )}, / =!, - • • ,N (P). Now, E{ Nw( Jr) } =
E{ I (S\ ) } -f
• • • + E{ I (SN ( F ) ) }, and, since N (P ) is not a random variable,
E(P) — {ai -f
• • • + Q^VOF)}/N{P). For the assessment and comparison of
simultaneous inference procedures, including multiple comparisons of means,
P(T ) or E(P) is a useful criterion. The following result may be interpreted as
a statement that the use of P{P ) is an adequate criterion.
If N (F ) < oo, then E{F ) < P(T ) < N ( f )E(F ).
Result 7.3.2.
By definition, P(T ) > a/, / = l, * *
* ,N (P). Clearly, E{ I (Sf ) } =
a/, / = 1, • • • , N ( JF ), so that E{ NW (T ) } =
‘)
<*/, and
Proof.
N (n
E(D = £ af!N(^)
•
(7.3.6)
/=i

7.3. CONFIDENCE INTERVALS AND MULTIPLE COMPARISONS
237
The probability that at least one statement is incorrect is
P{F ) = P(U^
if
){5/ is incorrect }) = P( NW {F ) > 0)
P( Sf is incorrect ) =
aj
<
/=i
/=i
by Boole’s inequality. That is, P{T ) <Y *f^!P af = E(P)N ( Jr). Since by defi-
nition, P{T ) > <*/, / = 1, • • • , N (T ), it follows that Y
i.e., N (T )P{T ) > Y
1}
proves the result.
PiF )
Y.NfLV <Xf ^
af , i.e., P(T )
YffiV <* f /N {T ) = E(F), which
/=1
N (T )
=1
We describe the construction of simultaneous confidence intervals for s con-
trasts c'/3. We first describe a procedure due to Scheffe, and then discuss Bon-
ferroni ^-intervals.
Scheffe intervals
Scheffe (1953) proposed a method for comparing all possible contrasts in /? such
that the probability of type I error is at most a for any of the possible compar-
isons. In other words, the probability is 1 — a that the associated confidence
intervals simultaneously contain the true values of all the contrasts in /3. Let
£ = {c = (ci, • • • , cp)'} C C(X), with dim(£) = s, say. That is, £ is a fixed
s-dimensional linear subspace of C(X), s < r. The objective is to construct con-
fidence intervals for c' /3 =Yi=i cifli for all c
£ with simultaneous 100(1— a)%
coverage. Here, T = £, which can denote a family of contrasts, or arbitrary
estimable linear functions of /3, or individual coordinates of dimension s, etc.
For example, in the fixed-effects one-way ANOVA model, we are often interested
in estimating contrasts. In simple linear regression, we might be interested in
confidence intervals on all linear combinations co/?o + c\0\. If dim(£) = 1, we
would obtain a single confidence interval. Consider s LIN combinations with
coefficients c* = (c*i, • • • , CjP)', i = 1, * • • ,s. Consider the s x p matrix
cn
* * •
cip
L' =
\C8\
* * *
Csp
with r(L') = s, so that the rows of L' form a basis of £. From Result 5.2.6,
L'/3° ~ iVs(L'/3, <J2L'GL). A test of H : L' fi — b*, say, is based on
(L'/3° - b*),(L'GL)_
1(L'/3°- b*) / so2 ~ Fs,w_r
(7.3.7)
under the null hypothesis.
The 100(1 — a)% simultaneous confidence intervals via the
Result 7.3.3.
UF- projections” approach for estimable functions c' /3 for all c
£ are given by
c' f3° ± 5{s(c'Gc)Fs,w_r,Q}1/2.
(7.3.8)

238
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
The simultaneous confidence interval for any c'/3, c e C is the projection of the
confidence ellipsoid (7.2.10) onto the one-dimensional subspace of £ generated
by c.
Let P = C'GC, 7 = sd2Fs,N-r,a, and C' /3° - O0 = b. Using
Proof.
Result 2.4.6
—
P( Ps,N — r — PstN — r,a)
=
P{ {C'(3° -CmCGCrHC/P - C’ p)
sa2Fs,N^ a }
(h'b)2
h'Ph
1 — a
= P{b'P-1b < 7} = P{sup[
(h'b)2
h'Ph
|h'C'/?° - h'C'/?|
{
S(h'Ph)1/2
Setting c = Lh, gives 1- a = P{|c'/3°- c'/3| < {sc'GcP^w-,.^}1/2 for all c}
which proves the result.
] < 7}
h^
O
P{
< 7 for all h / 0}
< [sFSyN — r,a
In Exercise 7.9, we ask the reader to derive the Scheffe F-intervals for the
linear model with a p.d. covariance structure. For ANOVA models, Scheffe’s
method is generally recommended when (a) the sample sizes n*, i = 1, * • • , a are
unequal, and (b) we are interested in more complicated comparisons between
treatment means than simple pairwise comparisons.
Example 7.3.6.
We apply Scheffe’s simultaneous procedure to simple linear
regression to construct simultaneous confidence intervals for co/?o + Ci/?i, which
are given by
{ (coPo + ciPi ) ± [2 est. Var{coPo + CI3I)F2
where, Var(co/3o 4- C\0\ ) = c^Var^
) + c{Var((3{) + 2coC\Cov(@o,(3\ ) can be
computed from (4.2.28).
Letting Co = 1, and C\ = A, we can obtain the
simultaneous confidence intervals for /3Q +
X .
i1/2}
, N ~ 2,a
Suppose r(X)
p. To obtain simultaneous confidence surfaces for the entire
regression, we have from Scheffe’s procedure that the unknown Y corresponding
to all values of x lies in
[x(X,X)_1x]}1^
2.
x'/3 ± d { pFPtN-
(7.3.9)
p,a
Bonferroni t-intervals
Let us consider the problem of constructing simultaneous 100(1— a)% confidence
intervals for L pairwise treatment differences T*
i ^
k, z, A: = 1, • • • , a, such
that the probability that the intervals jointly contain the true differences is at
least 1 — a. This goal can be achieved by using the Bonferroni inequality. For
any events A\ y • • • , AL, we have

7.3. CONFIDENCE INTERVALS AND MULTIPLE COMPARISONS
239
P{ A1 n • • •n Al ) = 1- P{ Ai U • • U Al )
where Aj denotes the complement of the event Aj, and
L
P( A\ U • • • U AL ) < ^
P(Ai ).
i=1
The first-order Bonferroni inequality gives
P( At n • • • n AL ) > l - EP(Ai).
i=1
Suppose we denote the confidence intervals by Ji, • • • , JL, and suppose P(0* e
Ii ) = 1 - a*, i = 1
where 0* denotes a difference between any two
treatment means. Let A* denote the event {0*
A). Then
L
L
P(O1 e /1, • • • ,eL e iL )
1-
£ /») = i - E^i-
i=1
2=1
If a* = a for all i = 1, - — , L, then the simultaneous confidence coefficient
could be as small as (1 — La ) < (1 — a) when L > 1. For example, if L = 10
and a = 0.05, the joint confidence coefficient is 0.50! We achieve an overall
confidence of 1 — a by setting Ei=i ai = a• ^ne way
do this is to have each
individual confidence coefficient to be 1 — (a/ L). The Bonferroni procedure can
be conservative.
For constructing Bonferroni intervals, we see that
Ti = (c'/?°- c'/3) /{<?(c'Gcj)1/2} , i = 1, • • • ,s
has a Student’s ^-distribution with ( N — r ) degrees of freedom. Let ta/ 2s denote
upper a/2sth percentile points from this distribution. Suppose E{ denotes the
event that the interval c
^
/3° ± t0c/ 2<j [c'iGci\
1/ 2 contains c'/3, for i = 1, • •- s, so
that P( Ei ) = 1 — a/ s. Hence, the probability that c\ fi lies in the interval
c'/3° ± tQl / 2s^
[c'iGci ] 1^
2
^ for allz = 1, • • • , s is at least (1- a).
Multiple comparison procedures
We describe multiple comparison procedures which are widely used for com-
paring fixed-effect means in ANOVA procedures. In the one-way fixed-effects
ANOVA model with a treatments, suppose the F-test rejects the null hypoth-
esis of equal treatment effects; this does not give the user any information on
how the test was rejected, i.e., which of the (£) pairs of means were significantly
different. A naive approach would be to compare all pairs of treatment means
based on the usual t-tests. If we do this, we run into a problem that although
each individual comparison would have a Type I error rate of a, simultaneous
comparison of all pairs would have a rate which may be considerably less than
a. The solution to this problem is offered by a variety of multiple comparison
procedures, ranging from simple and elegant graphical techniques, to sophisti-
cated significance tests, which were pioneered by Tukey and Scheffe (see Miller,
1981; or Hochberg and Tamhane, 1987).
7-3.3

240
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
The effects to be compared must be planned in advance. To illustrate, sup-
pose we wish to test Ho : pi = P2 = M3 = M4 = ^
5, and suppose we reject
the null hypothesis. Assume that during the ANOVA analysis, we had observed
that Y 1. was the largest sample mean and F3. was the smallest and then decided
to test for the difference between p\ and p$. Since this is not a predetermined
comparison, the level of significance of this single test will not be a. In general,
comparisons must be planned before looking at the sample information. Oth-
erwise, an analyst will be guilty of indulging in data dredging or data snooping.
These are the alternate names given to the practice of first looking at the sam-
ple information and then choosing to analyze those comparisons that appear to
be interesting. However, it is perfectly acceptable to use a multiple comparison
procedure with a given confidence coefficient to cover all comparisons that could
be carried out after observing the sample data. It is important that users have
protection against drawing erroneous conclusions. While the error rate with a
single comparison is the Type I error rate, the notion of error rate in multiple
comparisons can be complicated. We give two definitions.
Definition 7.3.2.
ratio of the number of comparisons incorrectly declared significant to the total
number of nonsignificant comparisons tested.
The comparisonwise Type I error rate is defined as the
Definition 7.3.3.
ratio of the number of experiments with one or more comparisons incorrectly de-
clared significant to the total number of experiments with at least two treatment
means.
The experimentwise Type I error rate is defined as the
The experimentwise error rate is more conservative. Although the choice
of error rates is subjective, the comparisonwise error rate should be used if one
incorrect inference from an experiment does not affect other inferences from that
experiment. The following results describe methods for the pairwise comparison
of treatment means using the LSD method, Duncan’s multiple range procedure,
the Newman-Keuls method, and Tukey’s procedure.
We use a numerical example (Montgomery,
Numerical Example 7.3.
1991) to clarify the procedures. Consider a balanced one-way ANOVA setup
with a = 5, n = 5, (so that N —
25), Y\. = 9.8, Y 2. = 15.4, Y $. = 17.6,
Y 4. = 21.6, Y 5. = 10.8, Y .. — 15.04, and MSE = 8.06. The overall F-statistic
is 14.76, which is significant at the 5% level of significance.
LSD procedure
Fisher’s least significance difference (LSD) procedure is useful for making pair-
wise comparisons among a set of a population means.
We define the least
significance difference to be the observed difference between two sample means
that is required in order to decide that the corresponding population means are
distinct. For a given level a, the LSD for comparing pi and pj in an unbalanced
one-factor fixed-effects ANOVA model is

7.3. CONFIDENCE INTERVALS AND MULTIPLE COMPARISONS
241
LSD =
a,a/2\/52 (1M + l/nj ),
while for a balanced model with m = n,i = 1, • • • ,a, it is
L5Z) = tjv_a,a/2 \/252/n.
All pairs of sample means are compared.
If
— Yj. > L5Z), we decide
that /ii is significantly different from /Zj, with a Type
error probability of
a. Note that the LSD procedure is similar to a two-sample test for any two
population means, the difference being that we use the estimate a2 instead of
the pooled variances from the zth and jth samples. To implement this procedure,
we compute the LSD for all pairs of sample means.
In general, the LSD procedure is criticized for inflating the Type I error rate.
That is, the overall probability of incorrectly declaring some pair of means sig-
nificantly different, when they are in fact equal, is substantially higher than the
specified a value. The a level of the LSD procedure is valid for a given pairwise
comparison only if the procedure is used for LIN and orthogonal contrasts or
for preplanned comparisons. In practice however, the procedure is more loosely
used because of its simplicity and ease of implementation. For this reason, it has
been recommended that Fisher’s LSD procedure be used only after the F-test
for treatments is seen to be significant. This revised procedure is sometimes
called Fisher’s protected LSD, and some simulation studies show that the error
rate for the protected LSD procedure is controlled on an experimentwise basis
at a level that is approximately equal to a.
Numerical Example 7.3. (continued).
To illustrate, the LSD at level of
significance a = 0.05 is
LSD = *20,0.025x/2(8.06)/5 = 3.75.
The differences in the five treatment averages areY
— Y 2- = — 5.6; Y
— Y 3. —
-7.8;Fi .-Y 4.=-11.8; Yh-Y 5.= — 1.0; Y± -Fs. = -2.2;Y 2. -F4. = -6.2;
Y 2. - Y 5. = 4.6; Y 3. - V 4. = -4.0; Y 3. - Y 5. = 6.8; and Y 4. - Y 3. = 10.8.
Treatment means whose absolute difference is bigger than 3.75 are significantly
different. Thus, the only pairs that do not differ significantly are (1,5), and
(2,3).
A
Duncan’s multiple range procedure
Duncan’s multiple range procedure is widely used for comparing all pairs of
means. We arrange the treatment means Yin ascending order. If rii = n,
i = I,
* * ,a, the standard error of each Yis a/ yjn. If not all the ni’s are
equal, we compute their harmonic mean TIH = a/^JL1(l/rii), and obtain the
standard error of Y *. to be Syi = ojy/nj{. Duncan (1955) produced tables of
significant ranges ra (p,/), for p = 2,3, • * • ,a, where / denotes the error degrees
of freedom. We convert these significant ranges into a set of a-1least significant
ranges
Rp = ra( p, f )s y . , for p = 2,3, • • • ,a.

242
CHAPTER 7.
INFERENCE FOR THE GENERAL LINEAR MODEL
We test the observed differences between treatment means as follows. First, we
compute the difference between the largest and the smallest means, which is
compared to the least significant range Ra. Next, we compute the difference
between the largest and the second smallest mean and compare it to Ra-i -
We continue until we have exhausted differences between the largest mean and
all other treatment means. If an observed difference is greater than the corre-
sponding least significant range, we conclude that there is a significant difference
between that pair of means. Next, we compute the difference between the sec-
ond largest and the smallest mean and compare it to Ra-1, etc. We continue
this process of comparisons until we have exhausted differences between all Q)
pairs.
Duncan’s test is powerful, i.e., it is effective in detecting real differences
between treatment means, and is therefore widely used.
As the number of
treatment means to be compared increases, this test requires a greater observed
difference between the means to detect significance. When we compare two
means, the critical value R2 is exactly equal to the LSD value. To a lesser
extent than the LSD procedure, Duncan’s multiple range procedure may also
inflate the Type I error rate.
Numerical Example 7.3.
(continued).
treatment means in ascending order: Y 1. = 9.8; Y 5. = 10.8; Y 2.
Y 3. = 17.6; and Y 4. = 21.6.
Each has standard error yjMSEjh = 1.27.
From the tables of significant ranges for Duncan’s test, we obtain correspond-
ing to a — 0.05: ro.o5(2, 20) = 2.95, r0.05(3, 20) = 3.10, ro.os(4, 20) = 3.18,
and ro.o5(5, 20) = 3.25. The least significant ranges are computed as R2 =
(2.95)(1.27) = 3.75; R3 = (3.10)(1.27) = 3.94; R4 = (3.18)(1.27)- 4.04; and
f?5 = (3.25)(1.27) = 4.13. These comparisons show that all pairs of treatment
means except (3,2) and (5,1) are significantly different. This conclusion is iden-
tical to the one given by the LSD procedure (although in general, results from
the two procedures need not coincide).
A
To illustrate, we rank the
= 15.4;
Tukey’s procedure
Tukey’s method is based on the distribution of the Studentized range statistic
(see the appendix). Assume that m = n, i = 1, * • • ,a. Since Si are iid 7V(0, <r2)
variables, it follows that Y
— /i* are independently distributed as iV(0, <72/n)
variables. Since all the treatment difference pairs are less than some number if
and only if the largest difference^
, we have P{|(5/t1. —
fx^
) — (Yi 2.-^
i2 )\ < Ca,
for all i\ and 22} = P{maxilii2|(F<1.-/ii1)-(y<2.-/ii2)| < Ca}. Tukey derived
the distribution of the random variable
{|(*V - Mi, )- (*v
max
where the maximum is taken over all pairs i\ and %2 from 1 to a. This distribu-
tion is called the Studentized range distribution with parameters a and a(n-1).
Let qa(a,N — a) denote the upper 100a percentage point of this distribution.

7.3. CONFIDENCE INTERVALS AND MULTIPLE COMPARISONS
243
Then 100(1 — a )% set of confidence intervals that hold simultaneously for all
pairwise treatment mean differences /z^
— pi2 is (Y^ . — Yi2.)±qa(a, N -a) sy. .
If the intervals do not include zero, we reject the null hypothesis that there is
no difference between
and pi2 for all i\ and %2 at level a. We can compute
Tot = <?a(a, N — a) sy. , and decide that two means are significantly different if
the absolute difference between them exceeds Ta. Note that, unlike the Duncan
and the Newman-Keuls procedures, a single critical value is used in all com-
parisons. Tukey’s test procedure has Type I error rate of a for all pairwise
comparisons. It has a smaller Type I error rate than Duncan or Newman-Keuls
procedures, and is therefore more conservative. It is less powerful than either
of those procedures.
Numerical Example 7,3.
(continued).
From the table of the Studen-
tized range distribution, we compute the value 40.05(5, 20) = 4.24, and then
To.05 = (4.24)(1.27) = 5.38. The comparisons indicate that all pairs of treat-
ment means except (4,3), (3,2) and (5,1) are significantly different (similar to
the LSD procedure and Duncan’s procedure).
Newman-Keuls procedure
We next discuss the Newman-Keuls procedure. Newman (1939) first derived
this test, which got its name due to the revival of the method by Keuls (1954).
The overall nature of this test is very similar to Duncan’s multiple range test, ex-
cept for an alternate approach for computing differences between the treatment
means. For the Newman-Keuls test, we compute a set of critical values
Kp = q« ( pj )sy. , p = 2,3, • • • , a,
where <7a (p,/) denotes the upper 100a percentage point of the Studentized range
for groups of means of size p and error degrees of freedom /. We now compare
extreme pairs of means in groups of size p with the value of Kv, just like we did
for Duncan’s test. The Studentized range is defined as the random variable
q —
(Y max
Y min
where Kmax and Kmin are respectively the largest and the smallest treatment
means out of a group of p means.
This test is more conservative than Duncan’s test, i.e., it has a smaller type
I error probability, and is less powerful. It is therefore easier to decide that
a pair of means are significantly different using Duncan’s procedure than it is
. using Newman-Keuls procedure.
Numerical Example 7.3. (continued).
We compute K2 = <70.05(2, 20)(1.27)
= (2.95)(1.27) = 3.75; Ks =
<70.05(3, 20)(1.27) = (3.58)(1.27) = 4.55; K4 =
40.05(4, 20)(1.27) = (3.96)(1.27) = 5.03; andK5 = 4o.o5(5, 20)(1.27) = (4.24)(1.27)
= 5.39. The comparisons are similar to Duncan’s procedure, and all pairs of
treatment means except (3,2) and (5,1) are significantly different (similar to the
LSD and Duncan’s procedures).

244
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
Dunnett’s procedure
In many experiments, one treatment is the control, and there is interest in
comparing each of the other a - 1 treatments with this control (using (a - 1)
comparisons). To apply Dunnett’s (1964) procedure for this purpose, suppose
that treatment a is the control, and we wish to test the hypothesis Ho : T\ — ra
versus Ha : Ti ^
ra for i = 1, •• • ,a — 1. For each of the (a — 1) hypotheses,
compute observed differences in the sample means|YV — Ya.
At the joint significance level a, H0 is rejected if
|Yi.- Ya.| > da(a - 1, 7V- a){ d2{l /rn + l/na)}1/2,
where ( N — a ) is the error degrees of freedom, and the values of da(a — 1, N — a )
have been tabulated for different values of (a - 1) and N - a. It is usually
recommended that more observations be used for the control than for other
treatments. A rule of thumb is to choose these numbers such that na/n = y/a,
where na is the number of observations for the control and n denotes the (same)
number of observations for each of the other (a — 1) treatments.
i =!
» • • • a — 1.
Numerical Example 7.3. (continued).
To illustrate, let treatment 5 be
the control. The table value is do.O5(4, 20) = 2.65, yielding a critical difference
of (2.65) \/(2)(8 06)/5 = 4.76. Thus, any treatment mean that differs from the
control by more than 4.76 is significantly different. The comparisons indicate
that treatment 3 and treatment 4 are significantly different from the control
treatment 5.
A
Choice of procedure
Which multiple comparison procedure should one use?
Unfortunately, there
is no precise answer to this question, and there are varied opinions about the
relative merits of these procedures. There have been some simulation studies
that compare these procedures for different situations. The LSD and Duncan’s
multiple range test appear to be most powerful for detecting true differences
in treatment means. The LSD procedure can be carried out using the widely
available tables of t-values, whereas the other pairwise comparison procedures
require specialized tables. The Newman-Keuls test is more conservative than
Duncan’s test in that the Type I error rate is smaller, and the test is less
powerful. Hence, it is more difficult to declare that two treatment means are
significantly different using the Newman-Keuls test than it is using Duncan’s
test. Scheffe’s procedure is used for all possible comparisons and may therefore
be extremely conservative when applied to a finite set of treatment differences.
Certainly, Dunnett’s procedure, which is a modification of the usual t-test is
useful when treatment means are compared to a control mean.
Numerical Example 7.1. (continued),
and the F-test for this example in section 7.1. The means and standard errors
of the treatment means are shown in the next table, while t-tests from the LSD
We obtained the ANOVA table

7.3. CONFIDENCE INTERVALS AND MULTIPLE COMPARISONS
245
procedure at the 5% level of significance are given in the following table. This
test controls the Type I comparisonwise error rate, and not the experimentwise
error rate. The critical f-value is 2.131, while the least significant difference is
0.218. In the table, means with the same letter are not significantly different.
Least squares estimates of treatment means
BRAND
N
Mean
s.e.
ACME
4
2.325
0.171
AJAX
4
2.050
0.129
4
2.375
0.171
4
2.600
0.141
4
2.375
0.096
CHAMP
TUFFY
XTRA
Comparison of effects using the LSD procedure
t grouping
Mean
N
BRAND
A
2.600
4
TUFFY
B
2.375
4
XTRA
B
B
2.375
4
CHAMP
B
B
2.325
4
ACME
C
2.050
4
AJAX
It is possible to test hypotheses on various contrasts of interest, as results in the
next table show.
Tests of selected contrasts
Contrast
d.f.
Contrast SS
MS
F-value
Pr > F
US VS FOREIGN
US VS FOREIGN
ALINE VS CLINE
ACME VS AJAX
TUFFY VS XTRA
US VS FOREIGN
ALINE VS CLINE
ACME VS AJAX
TUFFY VS XTRA
US BRANDS
0.271
0.271
0.271
0.271
0.094
0.094
0.151
0.151
0.101
0.101
0.271
0.271
0.094
0.094
0.151
0.151
0.101
0.101
0.245
0.123
13.00
13.00
0.0026
0.0026
0.0510
0.0166
0.0435
0.0026
0.0510
0.0166
0.0435
0.0130
1
1
1
4.50
1
7.26
4.86
1
13.00
1
1
4.50
7.26
1
1
4.86
2
5.88
The least squares estimate for the contrast ACME versus AJAX is 0.275, with
s.e. 0.102, so that the f-value is 2.69, and the p-value is 0.017. For the contrast
US AVERAGE, the estimate and s.e. are 2.250 and 0.042 respectively, while
the f-value is 54.00, with p-value < 0.0001. The reader is referred to the first
author’s website for more details.

246
CHAPTER 7.
INFERENCE FOR THE GENERAL LINEAR MODEL
Restricted and reduced models
7.4
In Chapter 4, we discussed estimation subject to linear restrictions on the pa-
rameters.
We now describe partitioning of the sums of squares under such
restrictions. We will also discuss reduced models, and restricted, reduced mod-
els in detail. These ideas are extremely useful for building a general theory of
hypothesis testing in a linear model. We now describe the situation where the
linear model y = X/?+e is subject to the “ natural” restriction A'/? = b, and we
wish to test the hypothesis H : C'(3 = d. We develop inference in the framework
of a nested sequence of hypotheses.
7.4.1
Nested sequence of hypotheses
Let C(X) denote the column space of X in (4.1.1); since r(X) = r, dim[C(X)] =r.
Let us introduce the following notation. Let SHQ = M { X ) be the space spanned
by X corresponding to the full model, viz., E( y ) = X/3. For l = 1, 2, • • * , let
SHI be the space spanned by X/3 subject to C'-/3 = dj, j = 1, * • • , Z, and let SHL
correspond to y = e . Suppose that C'- is an ( Sj x p) matrix with r(C') = Sj ,
j = 1, • • • , L. Therefore, L sets of restrictions takes us from the full model down
to random error. Clearly,
SH0
D
SHl = { X(i : C'1/? = d1}
D S*2 = {X/3 : C [ f3 = dx , C'/3 = d2}
=>
SHl = { X(3 : C'j(3 = dj , j = l , . . . , /}
3
SHL - {o}.
r ~Ei=i si >
Note that dim(iSH0 ) = r > dim^
nj = r-si > • • • > dim
• • > dim(SHL ) = r —
si = 0- We can also see this from
- r(C' ).
dim(SHl ) = r
i.e., PHl = X
^QX',
Geometrically, PHL denotes the projection of y onto SH
where X* is the design matrix under the Zth hypothesis, and G/ —
(X{X/ )~ .
Then, yHi
PHIY ^ and Q( Hi ) = y' PHiY , l = 0, • • • , L. The reduction in the
model sum of squares due to Hi , l = 0, • • • , L is
i ’
Q( Hi ) = y' PHly = y' XiGiX'iy.
Q{ H0 ) = y'P//0y corresponds to the full model sum of squares SSR, while
Q{ Hi ) = 0. At the Ith stage, Q( Hi ) corresponds to the reduced SSR due to
fitting the model y = X /3 + e subject to C'/? = dj, j — 1, • • • , L Consider the

7.4. RESTRICTED AND REDUCED MODELS
247
Zth and (l + l)th stages of the nesting. We denote by Q( Hi\ Hi+i ) the reduced
model sum of squares due to the fit under Hi beyond that obtained by fitting
(4.1.1) under Hi+\. In other words, Q{ Hi \ Hi+\ ) is the sum of squares for Hi
adjusted for Hi+1, and is defined to be
Q{Hi\Hi+1 ) = Q{ Ht )-Q( Ht+i ).
/
y
/
/
/
//
/
Q (Ht)
/
/ A
SH,
A/
yu,
/
0 (Hm )
A
^\
B
/
\
I
Hi I H1+1)
/
/
/
\
Figure 7.4.1.
Nested sequence of hypotheses.
Note that the fitted model has more parameters under Hi than under Hi+\.
In the triangle in Figure 7.4.1 with vertices A, B,C, we see that || AB || denotes
the length of the vector y//z , so that
II AB IN ( y'n. yH, )1'
2 = y'P//,y =Q( Ht ).
which is ( y’Hl+1 yHl+l )1/ 2
denotes the length of yn
Similarly, || AC
y'P«1+Iy =Q( Hi+1). Hence,
1+1 5
||CB||=|| AB || - || AC ||= Q( Hi ) - Q( Hl+i ) = Q( Hl \ Hl+1 ).
The quantity Q( Hi\Hi+i ) tests whether SH 1+1 is acceptable, given that we are
in SHr Consider the following partition of SST :
Q( H0 ) + SSEHO
Q( H!) + Q( HQ \ HI ) + SSEHO == Q( HX )+ SSEHl
SST
L-1
J2Q( HI \HI+1 ) + SSEH0 = Q( HL ) + SSEHl .
1=0

248
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
The statistic Q( H[\Hi+i ) has a noncentral x2(sz+i,^+1) distribution, where the
noncentrality parameter is A/+i =
— PHt+iYiPHi “ Mtfi+i) > where
SHI , and fXHl+i
<SHJ+1. When Cj+1/3 = d/+1, the noncentrality parameter is
zero, and
{Q(H,|fr,+i)/«,+1}/{55^
0/(iV- r)}- FSi+1 )iV_r
under CJ+1/T = dj+1, which is the usual “ partial F-test” in linear model appli-
cations. We may compute these sums of squares as follows:
Qm
Q( Hi\Hl+1)
yHlyH,
^1/y»,y»i - yjf,+iy»i+i
(y^i - yHi+J^
yH
*
Consider the model y = /3 -f £, where e ~ TV4(0,a2I), and
Example 7.4.1.
Yli=i Pi = ^P — 0* We derive the test statistic for H : f3\ = 02- The estimate
of 0 subject to the restriction 1'0 = 0 is|i'y.
The hypothesis H : (31 = P2 can be written as C'0 = d with C' = (1, — 1, 0, 0),
and d = 0. It is a straightforward exercise to verify that SSE = (X^
=i^
)2/4>
and Q = (Y1 -Y2 )2/2, so that F = {2(yx -T2)2}/{(£ y<)2}-*1,1 under
/? = y
The reduced model setup enables us to discuss consequences of model mis-
specification on solution vectors and predictions. Consider a partition of the
general linear model y = X/? + e as
(7.4.1)
y — Xx/?i 4- X.2P2 + s
where y and e are both TV-dimensional vectors, Xi and X2 are respectively
TV x pi and TV x p2 matrices with ranks r\ and r2, r(X) = r = r\ 4- r2, Pi
and 02 are pi and p2-dimensional vectors, E(e ) = 0, and Cov(f:) = a I//. Let
Pi = Xi(X'Xi)” X', z = 1, 2. The least squares solutions of 0\ and /?2 under
(7.4.1) are obtained by solving simultaneously the normal equations
XiX1/3? + X'1X2/^
=
Xly
X'Xi/?? + X' X2/?°
=
X' y.
Let M2 = X'2(I- Pi )X2. Then,
/3° =
(XiXO-X^
y- Xa^
), and,
[X' (I- P1)X2]-X' (I - Pi)y = M2-X' (I-Pi)y,
P2
while the corresponding error mean square is
sL(y- Xi/3?- X2$)'(y- Xj/3?- X2/?°).
MSE =

7.4. RESTRICTED AND REDUCED MODELS
249
In the full rank case with r(X) = p, r(X*) = p*, i = 1, 2, and pi 4- P2 = P,
it can be shown that for testing H : (3\ = 0, E(Q ) = E( SSEH — SSE ) =
a2Pl + (3[ X'1 ( IN - P2 )X 1l31.
Example 7.4.2.
respond to imposing a restriction on the (unrestricted) one-way ANOVA model
(4.1.7). We show that if a parametric function of (3 is estimable under the un-
restricted model, it must be estimable under the restricted model. When the
natural restriction is not an estimable function, an interesting situation devel-
ops. We see that under different restrictions, the same parameter p is estimable,
whereas it is non-estimable in the unrestricted model. However, its b.l.u.e. is
different under each restriction; in fact, its b.l.u.e. under a particular restriction
is equal to the b.l.u.e. of that estimable function in the unrestricted model from
which we obtain p under the restriction. In the following result, we use the solu-
tion vector /3° = (0,Y
where D = diag(l/ni, • • • , l/na).
Consider the unrestricted one-way ANOVA model in (4.1.7).
1. The test for H : p = 0 in the model (4.1.7) subject to the constraint
We describe hypothesis tests in three situations that cor-
0
0
• • ,Ya. )\ and corresponding g-inverse G ~
I- )
1
0
D
niTi = 0
equivalent to testing H : p 4
Yli=i niTi = 0
the
unrestricted model, and the test statistic SSM /a2 follows an F\^-a
distribution under H.
2. The test for H : p = 0 in the model (4.1.7) subject to the constraint
= 0 such that Yli=i c* = 1 is equivalent to testing H : p 4
Zti CiTi /X)<=i°i — 0 in the unrestricted model, and the test statistic
[Ei=iYi ]* /\°
2 El=i^
-lni )\ follows an F1>N
3. The test for H : p = 0 in the model (4.1.7) subject to the constraint
i Ti — 0 is equivalent to testing H : p 4 ^
Ti — 0 in the un-
restricted model, and the test statistic [X^
=i
follows an F\^-a distribution under H.
These results are easily verified. The function p4^
Yli=i niTiestimable under
the unrestricted model, and yields p upon imposing the restriction
niri ~
0. Therefore testing H : p 4 jj
niT% —
or equivalently, H : Np 4
Yli=i niTi —
0 m the unrestricted model is the same as testing H : p = 0 in
the model with the constraint Yli-i n%Ti — 0- In the notation of this section,
the hypothesis H : iVp 4 Y2i=i n*T* = 0 can he written as H : c'(3 = 0, where
c = (TV,m , - - - , na)' is an (a 4 l)-dimensional vector. By Result 7.4.1, Fo =
Q/d2 ~ FiyN ~a under H , where Q — (30,c(c Gc) ~lc' /3° = NY
2 = SSM. This
proves property 1. To show property 2 and property 3, we again write the
hypothesis under the unrestricted model as H : c'(3 =0, where c' is the vector
(1,1/a,
• • ,1/a) for property 2, and the vector (1, Ci/c., • • • , ca/c.) for property
3, where c.
7.4.1.
El— 1
-a distribution under H.
*Yi.]
2 / [v2 Zl=1(cl/ni )\
Yli=x ci* In
case, the F-test statistic follows from Result

2 5 0
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
Consider the fixed-effects two-way ANOVA model for the
Example 7.4.3.
balanced case shown in (4.2.31). Writing as a sequence of nested hypotheses,
b
a
SH0 —
M 4" 7~i
fij + £%j,^ ^
T%
2=1
3— 1
6
a
D
= {Yij = fi + Ti + Pj -
= 0,^
/3,- = O,0j = 0 V j}
2=1
i=i
a
= {>ij = M + n + £i j ,Y n = 0}
2=1
6
a
D
= {*ij = M + Ti + 0j + £i j ,Y Ti = 0.Y Pi =°
> 03 = 0 V j
n = o v «}
2=1
J=1
"f” ^2j}
D Sff3 ={lij = M + Tj + /?j
Ti
6
a
o>5> = °
2=1
j=l
0j = 0 V j, Ti = 0 V i,\i = 0}
=
=
}•
As mentioned earlier, the constraints are represented in matrix notation by
A'/? = 0. In SH1 y we have in addition to this constraint, the reduction imposed
by the null hypothesis H\ : /3j’s equal, j = 1, • • • ,6, viz., C [ fi = 0, say, where
the (b — 1) x p matrix C'x has the form
/1 -1
0
0
• • •
1 0
-1 0
• • •
0 \
0
C i =
\1
o
o
0 ... -1
Similarly, in SH 2 , we have the constraint A'0 = 0, the reduction C[ (3= 0, and
additionally, the reduction by Hi : r,’s equal, i = 1, -
,o, viz. C'2/3 = 0, say,
where the (a- 1) x p matrix C2 has the form
/1 -1
0
0
1
0
-1
0
0 \
0
C'2 =
1
0
0
0
-1

7.4. RESTRICTED AND REDUCED MODELS
251
Note that dim[5//0] = a + 6+1— 2 = a + 6 — 1, dim[5^
] = p — r( A'
C'x )' =
a-f 6+1— (6+1) = a, and dim[<S//2] = p— r (A'
Cx
C
^
)
7 = a+6+1— (a+6) = 1.
Here, Suy corresponds to the one-way ANOVA model, while SH 2 corresponds
to the model with overall mean only. The numerator degrees of freedom in
the F-test for Hi is dim[tS#0] — dim[5^] = 6 — 1, and in the F-test for
is
dim^
j/J — dim[<S//2] = a — 1. We have,
9H0 =
where Yij was defined under Example 4.2.6. Also,
yHi
,¥* )'
and yH2 = (Y .., -
,Y
, while yn3 = 0.
In a balanced model, it does not matter in what order we test for the r
effects and the (3 effects. This is possible as long as the “ manifolds” of the
X columns corresponding to these effects are orthogonal. To make this more
precise, let us write X = (Xi
X2
X3), where Xi has dimension N x 1,
and contains the first column of X (corresponding to the parameter /z), X2
is an N x a matrix consisting of the next a columns of X (corresponding to
T*, i = 1, • • • , a), and X3 is an N x 6 matrix consisting of the last 6 columns
of X (corresponding to /3j , j — 1, • * • , 6). Letting Ad(X) denote the manifold
of X, we can see that M { X ) = M(Xi) 0 Ad(X2) 0 Af (X3), and M ( X i ) 1
A4( Xj ), i
7^ jf, z, j = 1, 2, 3. The manifolds are orthogonal for the following
reasons. An arbitrary member in Ad (X1) is (c, • • • ,c)'; an arbitrary member in
the manifold Ad(X2) is (cfo, • • • , di, • • • , da, * * • ,da)'; and an arbitrary member
in Ad(X3) is (ei, - -
*
, e*,)'.
Let x/ be an arbitrary column in
Ad (Xj), l = 1, 2, 3. Then, x'xx2 = c6^=i di = 0, since in Ad(X2) YTi=\Ti = 0;
x'xX3 = caY^
i=i ^ j —
0 since in Ad (X3), 5Z*LIPJ = 0; and then, X2X3 =
Y'sj-i di yZj-i ej =
We present this in an alternate way in the following
result.
(ftl , —
Xu - ' - Xb)'
Consider the model (4.2.31) with riij = 1, i — 1, • • • ,a, j =
Result 7.4.1.
1, • * • A
1. The function
1S estimable if and only if Yli=i ci ~
ie* > the
function is a contrast in T*.
2. Contrasts in {T*} are orthogonal to contrasts in { f l j }-
Proof.
Suppose
1 c»ri 1S estimable. We must have
= y
^
/i(n - T i ) = TiY^
fi -
i=1
2=1
2=1
2=1
(E-f c -hVi -E/2^
~2 ?
2=2
2=1

252
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
which implies that C\ =
Si ~ fi = Yli=2
that
contrasts in the effects are estimable. This proves property 1. To prove property
2, let YA=1°iri
a contrast in {r*} and let
dj0j be a contrast in { /3j }.
Since
x
/i > C2 —
/2 ) ‘ * *
» Ca —
/ai so
c* = 0. Conversely, if
c* = 0, then £3? , CjTi is a contrast, and
cir° = Ei=lCi(y i -y-) =
= 0, the b.l.u.e. of Yli=1
1S E,=l
Ci
E“;=1 CjTj. with variance
f>!Far(Yi. ) = Ecf^aKE^/6) = E (c?/ b2 )ba2 = Y,CW!b-
i=1
2=1
i=l
2=1
2=1
Substituting a2 = 5S£'/{(a — 1)(6 — 1)} into the above expression gives the
estimated variance of the b.l.u.e. Similarly, since^
-=1 dj = 0, it follows that
the b.l.u.e. of
1 djPj is Sj=i djY .j , with estimated varianceY?j=i d2d2/a.
Also,
b
b
a
Cov^
an,T,djPj )
YTECoviY'.^
.i )
i=1
2=lj=l
2=1
a
b
rl
ciaj
2
afe ^
1=1.7=1
2=1
J=1
using the formula Cov(yi., y. j) =
proves the result.
Y j / b,
1y./a} = a2 jab, which
j=i
One consequence of this orthogonality is that in a balanced two-way model,
we have
R(T\n) = i?(r|/x, /3) and
/?(,%) = J?C%, r) .
In other words, it does not matter in what order we test for the r effects and
the /3 effects. This result does not hold for unbalanced models which we will
discuss in Chapter 9.

7.4.
RESTRICTED AND REDUCED MODELS
253
Example 7.4.4.
In the two-way fixed-effects model with interaction effects,
consider a sequence of nested hypotheses,
$H0
3
b
—
ijk —
Ti
(3j
Sijki^ ^
t% - y
= o}
i— 1
3= 1
a
{Yijk = H + Ti + £ijk ,^2,Ti = 0}
{Yijk = /i- + £i j k }
{Yijk = £i j k}
3 <S//2
2=1
3 SH 3
35H4
The vectors of fitted values under the models are
{M° + Ti +0% + 7ij} =
ij-}»
{M° + rP + /3?} = {Fi.. + F..
-. -F...},
y//0
y//x
yn2
y//3
Also,
6
n
a
SST
i=lj=lk=l
y'HcYHa =
’ and
i=lk=l
a
b
n
SST - R( H0 ) =
~Yv )2
-
2=l j— \k~1
R( HQ ) ~ Xab under Ho , while SSE ~ Xab{ n-1)- We ^rst
^he hypothesis
that there are no interaction effects. Here,
R( H0 )
SSE
b
R{ H1) = y^
y*,. = E E (n. + Y . j . - Y ... )\
i=lj=l
and it follows that the interaction sum of squares is
2=1 j=l
R( H0 \ Hi )
2=1J=1

254
CHAPTER 7.
INFERENCE FOR THE GENERAL LINEAR MODEL
which follows a X
^
a-i )(6-i ) un<^er #i - The statistic for testing Hi is
R( H0\Hi )/ (a- l )( b- l )
F( Hi) =
~ ^(a-i)(6-i),a6(n-i ) under H\.
(7.4.2)
SSE/ab(n- 1)
The model <S#2 corresponds to a hypothesis of no difference among the b levels
of Factor B. This does not refer to a test of H : fii = • •- =
under the full
model, since the (3j \s are not even estimable functions, nor are
djPj - Also,
in a model with interaction, it is meaningless to test whether (3j's are equal. For
instance, suppose Factor A denotes operators of type i, i = 1, • • • , a, and Factor
B denotes machines of type j, j = 1, • • • ,6. If there are interaction effects jij,
it is meaningless to compare two machine effects fij and f3y unless we consider
their performance averaged over all levels of Factor A (operators), i.e., we test
the hypothesis that (3j 4-Y^
i=i lij /a are equal, j = 1, •
• , b. Notice that under
the constraint
i 7ij ~ 0» Pj +
I 7ij /a becomes (3j. Then,
R( Hi \H2 )/ (b ~ l )
F( H2 ) =
(7.4.3)
~ F( b-l ) }ab( n- l )
SSE/ ab(n — 1)
under H2 , where
(yHi - YH 2 )'(9H 1 - 9H2 )
R( HX \H2 )
a
b
n
= EEEo7r -Y ...)2
i=1 j— ih— 1
£<?*
. - Y ... )2 ~ xl-i under H2 .
an
j=i
Therefore
6
anYJ {Y .].-Y ... )2/ {b-\)
j=i
F( H2 ) =
(7.4.4)
~ F( b-l ),ab(n-l )
SSE/ab(n-1)
under H2.
SH3 corresponds to the hypothesis of no difference among a levels of Factor
A, i.e., H3 : Ti +2j=i 7r?/& are equal for i = 1, • • • , a. Here,
R( H2\H3 ) = (yH, -9H3Y (9H2 -9H 3 )
where 9H 2 ~ 9H 3 = (
"Yi- - Y ... ), so that
R( H2\H3 )/ (a-1)
F( H3 )
SSE/ ab(n-1)
nbt (Yi..-Y ... )2/ (a- l )
i — l
^ ^a— l ,a6(n— 1)
SSE/ ab(n — 1)

7.4. RESTRICTED AND REDUCED MODELS
255
under i/3. Finally, i/4 corresponds to testing \x = 0 in the restricted model.
Then,
(y//3 - yw4 )'(y//3 - y//J
a
b
n
= na&y2. ~ Xi under i/4
Z=1j=lfc=l
WIH4)
so that
= nabY
2.. / { SSE/ ab(n-1)} ~ Flia6(B_1}
under i/4.
Example 7.4.5. Two-way nested or hierarchical model.
4.2.7, we derived the least squares solutions of the parameters in the balanced
model. The ANOVA decomposition is
In Example
SST = SSA + SSB( A ) + SSE
where
a
b
n
EEEfe - y ...)2 with (abn — 1) d.f.
SST
i=ij=ik=i
a
r*Y,(Yi~ ~ Y ...)2 with (a — 1) d.f.
55^
i=1
b
a
nV
^
y
~\y
*j. — Yi.. )2 with a(b — 1) d.f.
SSB( A )
i=lj=l
b
a
n
~ Yij.)2 with ab(n-1) d.f.
SSE
i=ij=ik=i
The ANOVA table is shown in Table 7.4.1.
Table 7.4.1. ANOVA table for the two-way balanced nested model
MS
Source
SS
d.f.
Factor A
Factor B
(within Factor A)
SSB( A )
a( b — 1)
SSE
SST
SSA
MSA
a — 1
MSB( A )
ab(n — 1)
MSE
abn — 1
Error
Total

256
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
The expected mean squares are
bn± rf
i=1
E( MSa ) =
a2 + a — 1
b
i=lj=l
a2 +
E(MSB{A))
a( b — 1)
a1.
E( M S E )
The hypothesis HQ : T£S equal, i = 1, • • • , a is tested by
F = MSA/ MSE ~ Fa _i ,ab(n-l )
under Ho. The test statistic for Ho :0j(i )’s equal, j = 1, • • • ,6, 2 = 1, • • • , a is
F = MSB( A ) / M S E ~ Fa(6_1) )a6(n_
1)
under #o-
Example 7.4.6. Higher-order nested models.
two-stage nested model to an m-stage nested model. For instance, a three-stage
completely nested model (balanced case) is given by
Y{ jkl
M
T% H“ fij(i )
“b'yk(ij )
£(ijk )h
for i = 1, • • • ,a, j = 1, • • • , 6, A; = 1, • • • , c, and l = 1, •
• ,n. For example,
consider an experiment where a factory wishes to investigate the hardness of two
different formulations of a metal alloy. Three heats of each alloy formulation
are made, two ingots are selected at random from each heat, and two hardness
measurements are made on each ingot. Thus, heats are nested within the levels
of the factor alloy formulation, while ingots are nested within the levels of the
factor heats. There are two replicates in this three-stage nested model. The
ANOVA table for the three-stage nested model is shown in Table 7.4.2, based
on which we can develop hypotheses tests as in the two-stage case.
We can generalize the
Table 7.4.2. ANOVA table for a three-way balanced nested model
Source
SS
d.f.
MS
i
1 y‘2
abcn
Factor A
MSA
a -1
Factor B
(within A)
Factor C
(within B)
Error
j_
_
1
v2
cnL*'2-*fXij ~
ben
a(b — 1)
MSB( A )
*
j
i
j
k
ab(c-1)
abc(n — 1)
abcn- 1
MSC ( B )
MSE
*
3
EEEEVSH - iEEEift.
i
j
k
l
i
j
k
l _ y2
abcn 1
EEEEY?jU
Total
i
j
k
l

7.4. RESTRICTED AND REDUCED MODELS
257
This may be extended to higher order models in a similar way.
Consider the balanced model with nested and crossed
Example 7.4.7.
factors:
yijkl — M + ri + 0j + 7k ( j ) + (TP )ij + (r7)ik( j ) + e(ijk )h
(7.4.5)
i = 1, • • • ,a; j = 1, • • • , b\ k = 1, • • • , c; and l = 1, • • • , n. It may be verified
that the ANOVA decomposition is
SST = SSA + 55B + SSc( B ) + SSAB + 55,4(7(5) + 55E,
(7.4.6)
where
6
SST = EEEE(^
« - y--)2
With (often-1) d.f.
i=lj=U=H=l
Y 2
ben
—
with (a — 1) d.f.
55^
aben
i=1
, yYk.Xl
acn
aben—
with (5- 1) d.f.
J=i
6
C yr2
jk -
= EE
SSC ( B )
with 5(c — 1) d.f.
' an
acn
j=ik=i
a
b y2
V2
55** - YYiit.- L-
AB
cn
aben
'—
SSA
SSB
with (a — 1)(6 — 1) d.f.
i=lj=1
b
C y2
EEETT
6
c Y 2
b
b y2
EE
EEE
*55^0(5)
/ an
* acn
i=lj=lk=l
j=lk=l
with 5(a — l)(c — 1) d.f.
completing this example.
It is possible to extend
Example 7.4.8. Higher-order crossed models.
the methods of the two-factor models to deal with models involving several
factors. We illustrate the ideas using three factors, Factor A, Factor B and
Factor C with levels a, 6, and c respectively. We only consider the balanced
case. The three-way cross-classified model with all possible interactions is
Yijkl = /it + n+ (3j + 7fc 4- (r(3)ij + (T~/ )lk + ( P j ) j k + (T/?7)^ + eijki

258
CHAPTER 7.
INFERENCE FOR THE GENERAL LINEAR MODEL
for i = 1, • • • ,a, j = 1, * • • , 6, k = 1, • • • , c, and l = 1, • • • , n, with n > 1. We
assume that Sijki ~ N ( Q,a2 ). The effects (r/?)^
-, (7-7)** and (^
) jk are called
two-factor interactions, while (r/3y )ijk is a three-factor interaction. Suppose all
three factors are fixed effects. We obtain least squares solutions of the main ef-
fects, two-way and three-way interactions by imposing the following constraints
on the normal equations:
6
E«
Efii =
= 0
k=1
i=1
3=1
6
a
X^
/% = y
=
= 0
i=l
fc=l
2=1
j= l
6
5^
(07)ifc = 0,
j=l
fc= l
6
a
=^
(r07 )ijk = 0.
22(T07 )ijk
2=1
j=l
fc=l
6, A: = 1, • • • , c, and l = 1, • • • , n, the solutions of
For z = 1, • • • ,a, j = 1, • • •
the parameters are
y ...
F,..- F...
F.J.. -Y ....J = 1, • - • ,6,
F .fc. -Y ....,k = 1, • • • , c,
i — 1,
i <2 >
# =
7° =
(nO?*
Fij- — Fi... -Y.j..+Y ....,i = l, -
- ,a; j = l ,
- - , 6,
Fi t. - Fi...- F..fe. + Y ....,i — 1, • • • , a; fe = 1, • • • , c,
(/?7)°fc
=
(T07)yfc
=
F.jfe. - F.j.. - F.. fc . + F ...., j = 1, -
• ,b; k = 1, - • • , c,
Fyfc. - Fjj.. -Yi.k. -Y .jk. + F,... + Y .j.. + Y ..k. -7
The ANOVA decomposition is
SSTC = 55A + 55B + SSC + SSAB + SSAc + 55BC + 55ABC + SSE,

7.4. RESTRICTED AND REDUCED MODELS
259
where
a
b
c
n
~ Y
- )2 with (abcn -!) d.f.
SSTC
i=i j=ifc=i /=i
° v2
y2
^
with (a — 1) d.f.
ben
abcn
E
55,
i=l
6 y?
K?
E
3 -
with (6 — 1) d.f.
acn
abcn
SSB
3=1
y^
_I
2
^ abn
abcn
a
b y2
V2
cn
abcn
—
with (c — 1) d.f.
55c
fc=i
1— 55>i — 55B with (a — 1)(6 — 1) d.f.
SSAB
I= I j=i
a
c v2
bn
V 2
ik •
:— 55,4 — 55c w^h (a — l)(c — 1) d.f.
SSAC
abcn
i=lk=l
b
c y2
Y 2
jk -
yy
-- SSB - SSC with ( b - l)(c-1) d.f.
SSBC
' an
abcn
a
b
c y2
i=l j=lk=l
Y 2
SSABC
'—
SSA - 55B ~ 55c - SSAB
abcn
-SSAC ~ SSBC with (a — 1)(6- l)(c- 1) d.f. ,
while SSE is obtained by subtracting these sums of squares from SSTC and has
a x2 distribution with abc(n— 1) d.f. These may be used to construct confidence
interval estimates of estimable functions of the parameter vector as well as for
carrying out various tests of hypotheses.
The model simplifies considerably when there are no interactions:
Yijfcl —
fJ' H“ T~i ~h ftj + 'Y/e “t“ £ijkl't
for i = 1, • • • ,a, j = 1, • • • ,6, k = 1, • * * , c, and l = 1, • • • , n. We assume that
£ijki ~ iV(0, <72). Suppose all three factors are fixed effects. We obtain least
squares solutions of the main effects by imposing the following constraints on
the normal equations:
fin = E& = E 7* = o.
i=l
3=1
/c=l
The ANOVA decomposition in this case is
SSTC = SSA + SSB + 55c + SSE,

260
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
where
a
b
c
n
Y ....)2 with (abcn - 1) d.f.
SSTC
i=l j= lfc= l i=l
a y2
y2
Y ~~
7—
with (a — 1) d.f.
' ben
abcn
SSA
2=1
6 y^
y2
—
3—
— with (6 — 1) d.f.
acn
abcn
E
SSB
3=1
^ abn
abcn with (c — 1) d.f.,
S5C
fc=i
while SSE is obtained by subtracting these sums of squares from SSTC and has
a x2 distribution with abcn — a — b — c+ 1 d.f. Based on this decomposition, we
can test for the significance of these main effect differences.
If all three factors have a levels each, and n= 1, the design corresponds to
an a x a Latin Square Design (LSD):
Yijk —
M + Ti + (3j + 7fc 4- £ijk >
for i,j, fc = 1, • • • ,a. We assume that Sijki ~ AT(0, cr2). This corresponds to a
square with a rows (Factor A) and a columns (Factor B); each of the resulting
a2 cells contains one of the a levels of Factor C occurring once and only once
in each row and each column. Note that in an LSD, N = a2. The ANOVA
decomposition is
SSTC = SSA + SSB + SSC + SSE,
where
with (a2 — 1) d.f.
EEE^*
N
i=i j=i k=i
a Y 2
Y 2
Eir - AT with (a ~ 1} di-
SSTC
SSA
2=1
a y2
y2
E — ^
with (a — 1) d.f. and
5S*
3=1
a Y 2
y2
Ef - AT with (a ~ 1} di-
SSC
fc=1
To test the null hypothesis of no difference in effects due to Factor C, the test
statistic is F = MSc/MSE which has an F(a-i),(a-i)(a-2) distribution under
the null hypothesis.
n

7.4. RESTRICTED AND REDUCED MODELS
261
Example 7.4.9.
Consider the two-phase regression model
E(Y ) =ft*+
E(Y ) = 02,0+02,iX ,
if X < 5
if X > 6,
(7.4.7)
with Pi# +0i ,i5 = 02,o + 02,iS = r), say. The parameters 5 and ri respectively
denote the changeover point and the changeover value. Given data (Y), JG),
i — 1, • • • ,N , we derive the F-test of H : 8 = SQ, where
<5o 6 ( Xn, Xn+i ). This
is a test for concurrence of the two regression lines at X = So The least squares
estimates of $4 and 0iy0, l = 1, 2 have the form (7.2.14) and (7.2.15), while
SSE has the form (7.2.16) with L = 2. To compute 0i,itH and /3^
0, //, we must
minimize the expression
S = Z)LiY,i=i (Yu ~
-0i,iXu )2 4- 2A{/?2,O- /3i ,o + ^o(/?2
where -2A is the Lagrange multiplier. Let N* = nirt2/ {ni 4- ri2), and let
,1 “ A,1)}
an =£(*« ~
+ N* (xi- ~ 5o)2
’
1 =
2’
i
O12 = ^21 ~ — JV*(Xi. — ^o)(^2- —
<fo), and
-n)(XH - Xj.) + (-1)‘JV*(F2. -F!.)(X,. -*
>),
I =1, 2.
«Z3 =
After some algebra, we see that /3/,!,#, Z = 1, 2 are solutions to
<11101,1,H + &12@2,1,H
a2l/3l , l ,H + 0,2202,1,H
Ol3
a23-
We solve for A as
XH = N* {Y 2 . - Yv 4-0I , I , H ( XI . - S0 ) -
(*2. - 50)}
while
0z,o,# — Y i. -0I % I
^HX {. 4- (— l)z 1A//n/
1,
l = 1, 2.
It may be verified that (Sprent, 1961)
SSEH - S S E = E?=i(A,i - A.I.H) E?=IW< -YI. )( XU - Xl.)+ X(Y 2.-TV)
and
JP = { SSEH - SSE } /{ SSE/ ( N - 4)} ~ FUN-4
under H.
The following result summarizes the consequences of underfitting, and sug-
gests that deleting variables corresponding to small coefficients (relative to their

262
CHAPTER 7.
INFERENCE FOR THE GENERAL LINEAR MODEL
standard errors) will lead to higher precision in the estimates of coefficients cor-
responding to the retained variables.
Result 7.4.2. Underfitting.
Suppose the true model is given by (7.4.1),
but the model fitted to the data is
(7.4.8)
y = XiPi+e.
Let P\ H = (X,
1Xi)-X'1y be the solution vector for pi (possibly after imposing
some restriction A'/?i = b), and let a\ H = {y'(I- Pi)y}/(iV- ri) denote the
estimate of cr2. Let MSEH denote the error mean squares under (7.4.8). Then,
1. E{ p\ H ) = Hi/?! + (X'jXi)-XiX2/?2, where Hi = (X,
1X1)-X,
1X1. In
the full rank case, E( p^
H ) = Pi + (XiX1)-1X'1X2/32.
2. E(alH ) = a2 + J^
P'2X'2( I - Px)X2/?2.
3. Let YQ,H = Xi,o/3i denote the prediction corresponding to X^o- Then
E(Y0tH ) = Xi ,0[/3i -I- (X'1X1)-X'1X2/?2].
4. If the matrix Cov^
S® ) — P2P2 is p.s.d, MSE > MSEH -
Proof.
0 on the model (7.4.1). The solution vector under the reduced fitted model has
covariance Cov((3f ) = (72(X'1XI ) ~. Now,
The model (7.4.8) corresponds to imposing the reduction H : X2/?2 =
E( P% )
(x;xx)-x;2?(y)
(X'1X1)-X'1( X 101 + X 2 p2 )
(X'xXO’XiXx/?! + (X'xXx )“ X'x X 2P2
Hx/?x + (X'XXX )-X,IX2/32.
(X'JXI) X'xX2. In the full rank case, j3\
= (X
^
Xi) 1X'1y, and
Let Mi
E(0itH ) = Pi + (X/1Xi)_1X'1X2/?2. Thus,
is an unbiased estimate for /?i
only if either (a) /?2 = 0, i.e., (7.4.8) is the “correct model” , or (b) X'xX2 = 0,
i.e., Xi is orthogonal to X2, or both (a) and (b) hold. This proves property
1. To prove property 2, we use Result 4.2.3, setting x = y, and A = (I — Pi).
Since X^
I — Pi) = 0 and tr(I — Pi) = N — 7*1, we see that
ohr(l - Px ) + ( X 1 p1 + X 2 p2 )\I- Px )(Xi/?i + X 2P2 )
( N - ri )a2 + P'2X'2( I - P1 )X 2p2,
E(y'(l - Px )y)
so that
E(KH ) =
+ xvM2X'2(I- Pi )X 2 p2? a2

7.4. RESTRICTED AND REDUCED MODELS
263
unless /?2 = 0. Property 3 follows directly since E(YotH ) = £(Xi,o/3? H ). Hence
y0,H is biased unless #2 = 0, or Xi is orthogonal to X2, or both. To prove
property 4, we verify that
a2 ( X [ X 1 )- + M 1 p2/3,
2M [ , and
cr2(X;Xi)- + M1C'ou(/3^
)M
/
1;
hence, MSE
MSEH = Mi{Ccw(/?£) — /?2/?2}M'i, and the result follows.
MSEH
MSE
While the true model is E(Y ) —
/30 + (3\ X\ + faX*,
Example 7.4.10.
suppose we fit the model Y* = /30 + fi\X\ + £*, i = 1, 2, 3, using data (Xj,Y\ ) =
(-2,15), (X2, Y2) = (0, 21), and (X3, Ya) = (2,55). Using Result 7.4.2, we can
verify that /?o has bias 8/?2/3, while
is unbiased.
Suppose the true model is (7.4.8), but we fit
Result 7.4.3. Overfitting,
the full model (7.4.1) to the data. Using the earlier notation,
1. £(/??) = Hi/?!.
2. MSE is an unbiased estimate of a2.
Since (I— Pj)Xi = 0, it follows that E( /3$ ) = M2 X^
I— Pi)Xi/?i =
Proof.
0. Then, E($) = E{^
H ) =^
(X
^
X^
-X'^
y-X2/320)} = H^
. This proves
property 1. The proof of property 2 is left as an exercise (see Exercise 7.16).
7.4.2
Lack of fit test
In some data sets, we observe the response variable repeatedly at the same
settings of the predictor variables. Let Y^i denote the Ith observation of the
response variable at the zth setting for X j y l = 1, • • • , n*, j = 1, * • • , &, i =
1, • • • ,m. The total number of observations is N =
1 ni • Suppose r(X) = p.
Unless p < m, the parameters cannot be estimated. In order to test for lack of
fit, we require p < m. Suppose Xe denotes a n A^ x m matrix whose jth column
contains rii ones (in locations ni + n2 + • • • -f n*_1 + 1 up to n\ + • • • + rij)
and N — rii zeroes (in the remaining locations). An explanatory variable X j
with only one setting (no repeats) would imply that the jth column of Xe has
a single value equal to 1. Consider the linear model
(7.4.9)
y = X e j U + £
where, we can express the m-dimensional vector fi = (pi,
• • , /xm)' in terms of /3
whenp < m. Let Pe = Xe(X'Xe)-X'e; thenC(X) C C(Xe), i.e., C(P) C C (Pe),
and X'eXe = diag(ni, • • • , nm). Let ye = Pey.
Then, y-y = (ye-y)+(y-ye), i.e., (I-P)y = (Pe-P)y+(I-Pe)y, which
represents an orthogonal decomposition of the residual vector into the lack of fit

264
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
vector and the pure error vector, since (Pe — P)'(I — Pe) = 0 (see Figure 7.4.2).
Note that the pure errors denote deviations of the individual observations from
their own group averages (for the same setting of the explanatory variables).
The corresponding breakup of the residual sum of squares is
y'(i - P)y = y'(Pe - P)y + y'(i - Pe)y,
(7.4.10)
y,
d-Pe)y
{N-m}
//////
!
/
0
Py
(p)
Q
Figure 7.4.2.
Geometry of lack of fit and pure error.
while the degrees of freedom decompose correspondingly as N — p= (m — p) -\-
( N — m). Table 7.4.3 shows the ANOVA decomposition corresponding to the
lack of fit and pure error.
Table 7.4.3. ANOVA table for lack of fit and pure error decomposition
Source
d.f.
SS
MS
SSRC
SSE = y'(I- P)y
SSLF = y'(Pe - P)y
SSPE = y'(I — Pe)y
MSRC = SSRc/ p
MSE = SSE/ ( N — p)
MSLF = SSLF/ (m- p)
MS PE = SSPE/ ( N - m)
Model
Residual
Lack of Fit
Pure Error
Corrected
Total
p- 1
N — p
m — p
N — m
N - 1
SSTC
The following example provides a simple illustration.
Consider the data in Table 7.4.4. Suppose X refers to
Example 7.4.11.
a level of light intensity, and Y denotes a measure of plant growth. The data
are genuine repeats, i.e., observations on two different plants at the same light

7.4. RESTRICTED AND REDUCED MODELS
265
intensity were obtained, and not measurements on the same plant twice. In this
example, N = 10, and m = 6.
Table 7.4.4. Illustration for lack of fit and pure error
Obs.
Xi
Y
1
2.0
1.4
2
2.0
3.1
3
3.1
1.9
4
3.1
2.5
5
3.1
3.8
6
3.5
1.6
7
4.6
1.8
8
4.6
2.2
9
5.7
3.5
10
5.9
3.1
The residual for the 1th response at the ith setting of the explanatory variables
is
eiti = Yitl - Yi = (Yi,i -Yi ) + (Yi -% ),
since the repeated responses at the setting Xi have the same predicted value Yi.
The residual sum of squares in this case is
SSE = £ £(>5,i-?oa >
1=1 1=1
which is decomposed into the pure error sum of squares, and the lack of fit sum
of squares. The overall pure error sum of squares is
771
71 j
SSPE = EE(^^i)2
i=ll=l
with
ni ~ m) degrees of freedom. The residual sum of squares is parti-
tioned as
m
U i
m
7ij
771
EE(*M -Yi?
EE(r*.< -Y?2 +
- Yi? > Le-
i= l 1= 1
SSPE + SSLF,
i=i /=i
i=i
SSE
where SSLF denotes the lack of fit sum of squares. The pure error mean square
is obtained by dividing SSPE by its degrees of freedom, and is an estimate of
a2. The lack of fit mean square is obtained by dividing SSLF by its degrees
of freedom, viz., m — r. The reader is referred to Draper and Smith (1998) for
more details.

266
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
7.4.3
Non-testable hypotheses
Suppose that C' /3 is not estimable, i.e., C' ^
T'X for any T' (see (4.3.2)). This
in turn implies that C' ^
(T'XG)X'X for any T' (see Exercise 3.4 (a)), so that
the rows of C' are LIN of the rows of X'X. Since r(X'X) = r, this requires
that C contains no more than ( p — r) LIN rows. Now,
(3° = GX'y + (H — I)z
is a solution to the unrestricted normal equations X'X/?0 = X'y. Consider the
equations
(7.4.11)
C'(H — I)z = d - C'GX'y
(7.4.12)
Since r(H - I) = p — r, for arbitrary z, there are only ( p - r ) arbitrary
in z.
elements in (H-I)z, of which the remaining r elements are linear combinations.
Hence, (7.4.12) represents a set of at most ( p — r ) consistent equations in the
( p — r) unknowns (H — I)z; suppose that z is a solution. Then, z is a solution
to (7.2.3). Substituting z for z in (7.4.11), we see that
= GX'y + (H-1)2
satisfies (7.2.3) in which A = 0.
Therefore, (7.4.13) is just a subset of the
solutions /?° to the unrestricted normal equations, so that
(7.4.13)
SSE„= (y- X(3°HY ( y - Xp°H ) = SSE
(7.4.14)
and no test for H : C'/? = d can be derived. Searle (1973, sec. 5.5.d-e) has
argued that if we compute Q for a non-testable hypothesis, the hypothesis being
tested is actually H : C'H/? = d, and not H : C'/? = d. Note that C' Hfi is
always estimable.
7.5
Likelihood based approaches
Least squares estimation does not require specification of a probability dis-
tribution for the errors e.
In this section, we assume that e follows a spe-
cific probability distribution, derive the maximum likelihood estimate of /? and
study its properties.
We describe two classes of distributions - the normal
family, and the family of elliptically contoured distributions.
In general, if
z = (Zi, • • • , Zpj ) is a random sample from a population with pdf or pmf f ( z\0),
where 9 = (0i, • • • ,^9);, the likelihood function is defined by
N
1,(0; z) =n /(*t; 0).
i— l
The maximum likelihood estimator of 8 is a point estimator, denoted by OML ,
and is the value in the parameter space 0 that maximizes L(6; z), i.e., the value
in © for which the observed sample z is most likely. In practice, it is conve-
nient to obtain OML by maximizing logL(0; z), which is a monotonic function
of L(0\ z). That is, we find OML such that

7.5. LIKELIHOOD BASED APPROACHES
267
log L(9ML',* ) = suplog L(9\ z).
eee
There are situations where this supremum is attained at an interior point of ©,
and other situations where the supremum is attained on the boundary of ©.
In the former case, if logL(0; z) is a differentiable function of 6, 8M L may be
obtained as a solution to the equations
d\ogL(0; z )/ dOi = 0, i = 1, • • • ,q,
which satisfies logL(0;z) > log L(8ML,Z) for all 9 G©. The MLE of 6 is unique
if f ( z\ 6 ) is unimodal, which can be verified in practice by the log concavity
of f ( z; 9 ). For instance, suppose f ( z; 8 ) corresponds to the mixture of normals
distribution (see section 5.5.1), then 8ML need not be unique. Consider a situa-
tion where we are interested in estimating some function /i(0), although f ( z; 9 )
is itself indexed by 8. By the invariance property of the maximum likelihood
estimator, II(0ML ) is the MLE of h(9 ). This property is useful in linear model
theory, as the following result shows. Let 0 = G0 denote an estimable function
of 0.
Maximum likelihood estimation under normality
Assuming that e ~ AT(0, <72Iyv ) in (4.1.1), we obtain the maximum likelihood
estimates of parameters.
7.5.1
Result 7.5.1.
Consider the general linear model y = X/3 + e with normal
errors. The maximum likelihood estimate of an estimable function 6 = G0
of (3 coincides with its least squares estimate, while o\lL is a scalar multiple
of a2. In the full-rank case, by setting C = Ijv, we obtain the MLE of /? as
0ML = (X'X)_1X'y, while d 2ML = ( N - p)a2/N.
Proof. When y ~A
”(X/3, c72Iyv ), the logarithm of likelihood function is (see
(Al))
N
“ o (y ~ X/?)'(y — X/3).
/(y; X, /3lCT2) =
2 l0g(27T(72)-
(7.5.1)
2<T2
We set the first partial derivatives of l( y; X, /3,a2 ) with respect to (3 and a2
equal to zero, and solve the resulting equations simultaneously. Specifically,
using Result 2.7.2 and Result 2.7.3, we obtain the equations
^
/(y; X, /?,cr2)
which we solve to obtain 0° = (X'X) ~X'y = GX'y, and o2ML — (y— X^°)
/(y—
'K0° )/N . When G0 is estimable,
^
(2X'y- 2X'X/3°) = 0,
2a2
N
1
(y - X/?°)'(y- X/3°) = 0
O/T2
251
M L
M L

268
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
C'0° = C'GX'y = T'XGX'y
using the definition of estimability. Since XGX' is invariant to the choice of G,
it follows that C'0 = C'0° is unique, and is the MLE of C' /3. In the full-rank
case, by setting C =1^, we obtain the MLE of /3 as /3ML = (X'X)“ 1X'y, while
Z 2ML = (y-
y -W M L )/N.
In the full-rank case, 0ML is an unbiased estimate of /3, with covariance
<72(X'X)-1 (obtained as the inverse of the Fisher information matrix). The
minimal sufficient statistics for /3 and a2 are respectively /3 and a2, and (3 is the
MVUE of /3 (Rao, 1973a). Also, E(d2ML ) = ( N — p)a2/N , so that
is a
biased estimator of <r2, with the bias decreasing as N increases (relative to p).
Let Yi = f30 + (3iXi + 02X 2 + eu i = 1, • * * , N , { N > 3)
Example 7.5.1.
whereY^iLi
=
5ZiLi X? = 0, and
{ are iid 3V(0, a2 ) variables. We assume
that /32
7^ 0. We wish to find the value X*, (— oo < X * < oo), for which the
quadratic model function attains an extremum (maximum for a profit function,
and minimum for a cost function). Setting the first derivative of the function
f ( x ) = (3o+(3\x+ fox2 equal to zero, and solving, we get x* = — 0\/ 202. Under
normality, the maximum likelihood estimator of /3 = (/3Q, fix ,02); coincides with
its least squares estimator 0 = (X'X) LX'y, with
o
E^
2
o
E^
2
o
Ex?
o
E
N
and X'y =
EXiYi
E X?Yi
X'X =
i.e.
i=1
i=l
{Y,YiXi }/(Exi }' and
Po
Pi
i= 1
1=1
N
N
N
02 =
{Y,Yix? - PoJ2 x? }/Y,x?-
i=i
i=i
i=i
By the invariance property of the MLE, X *ML = -0I /202*
In model (4.5.1), suppose e ~ iV(0, <72V), then c'/3QLS is
Example 7.5.2.
the maximum likelihood estimator of the function c'0 provided the function is
estimable.
We can also motivate the F-test statistic based on the likelihood ratio test,
as the following result shows.

7.5. LIKELIHOOD BASED APPROACHES
269
Result 7.5.2. The statistic
(NZPl {A-W/2 _ 1]
s
is a monotonically decreasing function of the likelihood ratio test statistic A.
(7.5.2)
F0 =
For the full rank case, the maximum likelihood estimates of 0 and a2
Proof.
obtained by maximizing the likelihood function over the entire (unrestricted)
parameter space 7Zp x
are
0ML = (X'X^
X'y and S2ML = (y - X0ML )' ( y - XpML )/ N.
The corresponding (unrestricted) maximized likelihood function is
L0ML,S2ml ) = ( 2Trd2ML ) ~ N / 2 exp{-N/ 2 }.
The maximum likelihood estimates of 0 and a2 subject to the restriction C' /3=
d under H are obtained by using the method of Lagrangian multipliers for
maximizing the log-likelihood function subject to the linear constraint. It may
be verified that (3ML = 0, the restricted least squares estimate of /3, while
O2ML = (y- X0MLy(y - X0ML )/N.
The corresponding (restricted under H ) maximized likelihood function is
L0ML,a2ML )= ( 2na2ML )~ N/ 2 exp{-N/ 2 }.
The likelihood ratio test statistic is
A = L(0ML,&ML )/ L(0ML ,
=
and we reject if if A is too small. We can see that FQ defined in (7.5.2) is a
monotonically decreasing function of A, and we will reject Ho if the statistic Fo
is too large.
7.5.2
Elliptically contoured linear model
Consider the elliptical linear model
y = X/3 + e,
e ~ En(0,a2lN
) ft),
and r(X) = p. Then, y ~ EN (X.0, CT21N ,ft) ? and the likelihood function is
a~Nh{a-2(y- X0 )' (y - X/3)}.
The least squares estimator of 0 and SSE have the form in (4.2.4) and (4.2.17)
respectively. It is easily verified that the maximum likelihood estimator of 0
coincides with its least squares estimator, i.e., 0 = (X'X)_1X'y for any density
generator ft(.) and the fitted vector and residual vector are the same too. In
fact, the distribution of the MLE of 0 is the same for all ft(.) within the elliptical
family. However, the distribution of the MLE of a2 is affected by departures
from normality within the family of elliptical distributions.
(7.5.3)
(7.5.4)
Result 7.5.3.
Under the model (7.5.3),

270
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
1. 0 ~ Ep{(3,<j2( X' X )-\h), and
2. a2 = y'{I — X(X/X)_1X/}y/(W - p) is an unbiased estimator of a2.
Let y ~
_E/v(X./3, <r2lAr > ft), and suppose My(t) exists and has the
Proof.
form given in Definition 5.5.5. The mgf of (X'X)~1X/y is
My(X(X'X)” 1t) =^
(t,(X'X)-1X'((j2IN)X(X'X)-1t) exp{t'(X'X)-1X/X/3}
(see Exercise 7.30), from which property 1 follows. The proof of property 2 is
left to the reader.
Also
{X'X/<72}1/,2(/?-0) ~ tp(0, Ip, N - p),
where tp(6,ft, v ) is a p-variate Student’s t-distribution with v degrees of freedom,
location vector S and scale matrix Q (see (A25)), and
(7.5.5)
{P — /?)'X'X(/3 — 0 )/o-2 ~ pFVtN-
(7.5.6)
P-
Further,
( N - p)d2/o2 ~ R2
N_
p(h)
(7.5.7)
is the radial-squared distribution with ( N — p) degrees of freedom and density
generator h (see (5.5.4)). Consider testing the hypothesis H : C'0 = d where C
is a p x s matrix of rank s, and d is a known vector. By using the relationship
between the likelihood ratio test statistic and the F-statistic, it can be shown
that
( N - p) (C'/J- d)'{C/(X,X)-1C}-1(C'^- d)
(7.5.8)
F0 =
( N - p)d2
8
has an Fs^- p distribution under H (see Result 7.5.2). These distributional re-
sults facilitate inference in linear models with elliptical errors. For more details,
the reader is referred to Fang and Anderson (1990) and references given there.
7.5.3
Model selection criteria
In this section, we describe criteria based on the likelihood function, such as the
Akaike Information Criterion (AIC), and the Bayesian Information Criterion
(BIC), that are widely used in model selection.
In the linear model, these
measures have simple forms, as we will show. Suppose the observed response
y = (Y i , - - - ,YN )' ~ N (X{3, cr2IN ). Let 0ML and d2ML denote the maximum
likelihood estimators of 0 and a2 respectively, and let
l(y; X,0ML , Z2
ML ) = - log(27Td2ML )/N - SSE(y,0ML )/ 2a2ML

7.5. LIKELIHOOD BASED APPROACHES
271
denote the maximized log-likelihood function, where SSE(Y ,0ML ) = (y -
X/?ML)'(y- X0ML ) is the usual error sum of squares. Let y* = (Y{ , • • • , Y^
)'
be an independent random vector, with the same distribution as y. The defini-
tion of AIC is based on an estimate of -2 log(likelihood) for y*, which can be
written as
— 2i(y*;X,
$M L ) = -2l( y,XJML,32ML ) + { SSE( yJML )
-
SSE( y*JML ) }/ 2S2
ML.
While the first term on the right side of (7.5.9) can be computed from the ob-
served data, we must estimate the second term. The distribution of SSE( y* ,0ML )
is the same as the distribution of SSE( y,0ML ), where 0ML is the MLE of 0
based on y*, and
E{ SSE( y\0ML ) - SSE( yJML )} =
E{ SSE( y* ,0ML ) ~ SSE( y* ,0ML ) }
=
O2E{0ML - pyx'x(sML - /?)}
+
a2E{ {(3- pML )' X' X (0- PML ) }
=
using property 3 of Corollary 7.1.1. The estimate of — 2/(y*;X, /3ML >5^
/L) is
then -2l( y; X ,0ML > &ML ) + 2p, which leads to the following definition.
Definition 7.5.1.
Information Criterion (AIC) (Akaike, 1974) is defined by
AIC( p) =
~2l( y,XJML,a2ML ) + 2p
= -N\og{d2
ML )/ 2- N/ 2+ 2V.
The model order p must be chosen so as to minimize (7.5.10). The AIC crite-
rion tends to overestimate p, and the following modified criteria were proposed
by Akaike (1978) and Schwarz (1978).
(7.5.9)
A criterion for model order selection, called the Akaike
(7.5.10)
The Bayesian Information Criterion (BIC) is defined by
BIC = Nlog(a2
ML )+ 2( p+ 2 ).
The Schwarz Bayesian Criterion (SBC) is defined by
SBC = N \og(S2
ML )+ p\og( N ).
Definition 7.5.2.
(7.5.11)
Definition 7.5.3.
(7.5.12)
Other types of likelihood analyses
In section 7.5.1, we introduced the likelihood function of the parameters 0 and
cr2. In some problems, we may think of a subset of the parameter vector to be
nuisance parameters. One way of handling nuisance parameters is to maximize
7.5.4

272
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
the marginal likelihood, also called the restricted likelihood. The general frame-
work is discussed below. Applications to the random-effects and mixed-effects
models are given in Chapter 10. We also discuss the notion of quasi-likelihood,
and invoke it in Chapter 11 for analyzing longitudinal data, and for generalized
linear models. Finally, we look at likelihood based inference for incomplete data
problems due to missingness.
Restricted maximum likelihood
The method of restricted maximum likelihood (REML) was introduced as a tech-
nique for estimating variance components in a random-effects or mixed-effects
general linear model (Patterson and Thompson, 1971). Let y ~ N(X/3,0),
where fi = f2( <p), for some unknown parameter vector
<p. Suppose p is the
parameter of interest, and (3 is a nuisance parameter. The marginal likelihood
approach eliminates (3 from the likelihood function, and obtains the REML es-
timator of tp as the MLE based on a linear transformation, y* = Ay, which has
zero expectation, and whose distribution is independent of the location para-
meter (3. Let P be the usual projection matrix X(X'X)” 1X/, when r(X) = p.
The REML estimator PRM maximizes the function
Result 7.5.4.
IRM ( <P ) = -1log|fi|-\log|X
#n-1X|-l(y- XpGLsYtl-Hy- XPGLS ),
(7.5.13)
where (3GLS — (X'£2 lX ) 1X/f2 ly = Ay, say, is the GLS estimator of (3.
Let B denote the N x ( N — p) matrix such that BB' = I — P, and
Consider the transformation z = B'y. and 0GLS = Ay, with
Proof.
B'B = Ijv-p.
Jacobian |X'X|-1/2. By Result 5.2.6, z and PGLS have normal distributions.
Further, they are independent (irrespective of the value of 0 ) because
E{ Z(0GLS ~ P )' } = E{B'yCy'A'-0' ) }
B' {Var( y ) + £(y)£(y)'}A' - B' E( y )0'
B'{0 + X00' Xf }A' - B' X00' = 0,
since it may be verified that X'A' = I, and B'fSA' = 0. Also, E(z) = B'E(y) =
B' X0 = B' BB' X0= B'(I - P )X0 = 0, since (I- P) _L C(X). The pdf of z is
singular normal and is proportional to
COV ( Z,PGLS )
/(y)
(27r)-( N-p)/2inri/2|x,fi-ixri/2
x
exp{-i(y- XpGLsY^
~ l { y ~ X /3CLS ) } -
The REML estimator of p then maximizes the (restricted) log likelihood in
(7.5.13) (see Harville, 1974, 1977). This estimator is invariant to A; any choice
9( PGLS )

7.5. LIKELIHOOD BASED APPROACHES
273
of A for which E(z) = 0 and Cov( z, PGLS ) = 0 will suffice. It is instructive to
observe that the MLE of
<p maximizes
i(wy) =
log|n|-|(y- XPGLSYV-HY - xpGLS ).
u
In estimating random effects, maximum likelihood estimation does not ac-
count for the degrees of freedom that are involved in estimating fixed effects.
REML (sometimes called residual maximum likelihood) corrects this defect, and
obtains estimates of variance components in a general mixed-effects linear model
based on residuals from an OLS fit to the fixed-effects portion of the model.
Quasi-likelihood
In the linear model (4.1.1), supposed it is specified for each i = 1, • • • , iV that
Var(Yi ) = 'yV ( fii ), where pi = E(Yi ) = x'/?, I^(.) is some known function,
and 7 is a possibly unknown constant of proportionality. The quasi-likelihood
function K (Yi , p,i ) for the ith response is defined by the relation (Wedderburn,
1974)
dK {Yum )
(Yi — m )
(7.5.14)
V (m )
d\ix
The quasi-likelihood function may be used for estimation in much the same
way as a likelihood function. For i — 1, • • • , iV, let ut denote the p-dimensional
vector with components dK (Yi , pi )/dfij. The following result is central to the
notion of quasi-likelihood estimation (for proof, see section 5 in Wedderburn,
1974).
Result 7.5.5.
1. E(ui ) = 0, while the (j, fc)th element of its variance-covariance matrix is
-E{d2K {Yl^
i )/dP3dpk }.
2.
u0 = 0? while the ( jyk )th element of its variance-covariance ma-
trix is -E{d2
K {Yi,nP/ dfadfik }.
The maximum quasi-likelihood estimate 0Q is obtained by setting
ui
equal to its expectation of 0 and solving the resulting pequationssimultaneously.
This holds irrespective of whether 7 = 1, or 7 is an unknown parameter. If
Yi are independent, homoscedastic normal variables, then the maximum quasi-
likelihood estimator (3Q coincides with the least squares (or maximum likelihood)
estimator.
Result 7.5.6.
{^(H)}-1, where H denotes the matrix of second derivatives of
When 7 = 1, the approximate covariance matrix of (3Q is
Ui .
We have, up to first order in ({3 — /3Q),
Proof.

274
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
Eh u,* H(/J - PQ )
>*v.
ATV
so that 0 - (3Q « H_
1
and approximating H by its expectation of
-D, we get (3Q « /3 4- D_1
ui- The result follows since the covariance of
D-^
iix isD-1.
When 7 is unknown, the following estimate enables us to approximate the
covariance matrix of /3 — (3Q:
7 = {Eh(Yi - fii )2/V (Jli ) } / { N - p).
When V ( f i i ) = 1, maximum quasi-likelihood estimation reduces to least squares.
For a one-parameter exponential family, the log likelihood coincides with the
quasi-likelihood. For a discussion of generalized estimating equations (GEE), a
multivariate analogue of quasi-likelihood, see Liang and Zeger (1986).
Missing data analysis
In many situations, a portion of the data might be missing; these missing ob-
servations may have a significant impact on the statistical analysis of the non-
missing data. Suppose y = (y0bs 5 ymis) denotes the complete data, where y0bs
and ymiS respectively denote the N\ and .^
-dimensional vectors of observed
data and the missing data. The missing values are said to be missing at random
(MAR) if the observed units are a random subsample of the sampled units; in
this case, the missing data mechanism is ignorable. On the other hand, if the
probability that Yi is observed depends on the value of Yi, then the missing
values are not MAR, and the missing data mechanism is nonignorable (Lit-
tle and Rubin, 1987). Suppose we partition the N x p predictor matrix as
X = (Xobs, Xmis), where XQbS is an N\ x p matrix and Xmis is an N2 x p
matrix. In general, there are three approaches for handling missing data, viz.,
(i) complete case analysis, (ii) imputation for missing values, and (iii) model
based techniques via the likelihood function. We take a brief look at (i) and
(ii), before discussing (iii) in more detail.
As the name suggests, complete case analysis is usual statistical analyses
using only the cases where all observations are available. Standard software
may be used, which is attractive. The disadvantage is that if the percentage
of missingness (which is IOON2/ N ) is large, and if the data is not MAR, such
analyses tend to be inefficient. An alternative approach is imputation of missing
data, usually in the X matrix. Standard procedures are then used to analyze
the data with X* = (X0bs > X^
is). The following methods of imputation are
commonly used in practice. Hot deck imputation consists of selecting imputed
values from the sample distribution.
In cold deck imputation, we replace a
missing value by a constant, usually obtained extraneously. Mean imputation
consists of replacing missing values by the averages of available observations.
Sometimes, we may use model based imputation. For instance, regression (or
correlation) imputation consists of fitting a regression model of the missing data

7.5. LIKELIHOOD BASED APPROACHES
275
on observed values, and substituting the resulting predicted values for the miss-
ing values. We next discuss model based techniques which use a factorization
of the complete data likelihood.
Assume that the data are MAR. Let /(y|0) = /(yQbs > ymis|#) denote the
joint pdf of yGbs and ymiS, from which we obtain the marginal pdf of yQbs as
/(yobs) — J /(yobs ? ymis|^)dymis *
The likelihood of 9 ignoring the missing-data mechanism, and based only on
/(yobsl#) is called the incomplete data likelihood and is
£(%obs) OC /(yObs|0).
(7.5.15)
The objective is to maximize L(0|yobs) with respect to 9. If L(0|yobs) is uni-
modal and differentiable, the MLE of 9 is obtained either as the closed-form
solution to the ML equations
d\ogL(9\ yobs )/ d9 - 0,
or by using iterative methods such as the Newton-Raphson algorithm. In many
cases, it is cumbersome to maximize the incomplete data likelihood £(0|yobs)-
An alternative, widely used iterative approach is the Expectation - Maxi-
mization (EM) algorithm where the MLE of 9 is related to the complete data
likelihood L{9|y). This algorithm has a wide range of applications, beyond in-
complete data problems (Dempster, Laird and Rubin, 1977). Each iteration
of the EM algorithm consists of an E step (expectation step) and an M step
(maximization step). The E step computes the conditional expectation of ymjs
given yQbs and the current estimate of 9 and then substitutes these expecta-
tions for functions of ymiS that appear in L(9|y) (or usually, the logarithm of
this function). The M step maximizes the resulting expected log likelihood.
Let 9
denote the estimate at the mth iteration. In the E step, we compute
Q(0|0(m)) = j /(%)/(ymis|yobs, 0(m))dy
(7.5.16)
mis-
The M step obtains 0(m+1) by maximizing the expected log-likelihood in (7.5.16)
with respect to 8:
Q(0(m+l )|fl(m)) > Q(010
(”
»))5
for all e.
A generalized EM (GEM) algorithm chooses 0(m+1) such that
Q(0(m+1)|0(m)) > Q(6>(m )|0(m)).
(7.5.17)
(7.5.18)
Result 7.5.7.
1. In an EM (or GEM) algorithm, the change from 9^ to 0(m+1) cannot
decrease the log-likelihood, i.e.,
Z(0(m+1)|yobs) - /(6>(m)|y0bS) > 0
(7.5.19)
with equality if and only if
Q(0(m+1)|0(m)) = Q(0Cm )|0(m))-
(7.5.20)

276
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
2. The observed information matrix evaluated at the MLE 6 is given by
I(%obs) = -D2oQ(0|0) -£{S(%obs, ymis)S'(%obs, ymiS)|yobs, 0} \e=$
(7.5.21)
where S(0|yobs > ymis) denotes the score function, and
V 20Qm= (d2/d6d6' )Q(6|fl) \ B=S.
Proof.
Since we can write
/(y|0) = /(yobs, ymis|^) = /(yobs|0)/(ym »s|yobs, 0)
it follows that
i(0\ y ) = i(0lyobs) + iog/(ymis|yobS,^)-
(7.5.22)
If we define
H (0\0{rn ) ) = y
'
{log/(ymis|yobs,^)}/(ymis|yobs, 0(m))rfymis, then
(7.5.23)
by Jensen’s inequality. Then,
Q(e\e^ ) - H (e\e^ ), and
i(6{m+l )|y0bs) - i(0{m) lyobs)
=
{Q(9(m+i )\e^ )- Q(e^ \e^ )}
-
[ H (e(m+1)|6>(m)) - H (d {m)|6>(m))}.
^(^|y0bs)
(7.5.24)
That the left side of (7.5.24) is nonnegative follows from (7.5.18) and (7.5.23)
proving property 1. The proof of property 2 is left as an exercise.
Dempster, Laird and Rubin (1977) showed that if /(0|yobs) is bounded, then
Z(0(m)|y0bs) converges to some value P, while if f (Y \0 ) is a regular exponential
family (see (A22)), and /(0|yobs) is bounded, then 6^ converges to some sta-
tionary point 0*. In general, the rate of convergence is inversely proportional to
the proportion of missing information. When the distribution of the complete
data y belongs to the exponential family, the EM algorithm is especially simple.
Then, the E step consists of estimating the complete data sufficient statistics
T(y) by
T( +!) =
_£{T(y)|y0bs >^
m)})
while the M step obtains the new iterate 0(m+1) as the solution to the likelihood
equations
£{T(y)|0} =: T(m+1).

Exercises
277
Exercises
7.1. Prove Corollary 7.1.1.
7.2. Consider the model in Example 7.5.1.
(a) How will you test H : /?o = 0?
(b) Derive an F-statistic to test H : (3\ = /?2-
(c) For N = 12, how would you test H0 : X * = 0 versus Hx : X * ± 0 at
the 5% level of significance?
7.3. Let Yi =
+
+ £$, i = 1, • • • , AT, £^ ~ ./V(0, <J2). Derive
the F-statistic for testing H : /?3 = /?4 = /?5 = 0. By what amount is this
test statistic changed if mean subtracted responses Yi — Y are used instead
of the original responses?
7.4. Consider the measurement error model in Exercise 4.3. Assuming that
£i are iid 7V(0,a2) variables, derive the distribution of the least squares
estimator of the unknown force 0, and the level (1— a) confidence interval
for 9.
7.5. In the model (4.1.7) with a = 3, assume that Sij are iid N (0, <r2) variables.
Derive a suitable test for H :\x + T\ = /x + T2 = fx + 73. Characterize the
power function of this test.
7.6. In an experiment where several treatments are compared with a control,
it may be desirable to replicate the control more than the experimen-
tal treatments, since the control enters into every difference investigated.
Suppose each of m experimental treatments is replicated t times while the
control is replicated c times. Let Y% j denote the jfth observation on the zth
experimental treatment, j = 1, • • • ,t, i = 1, • • • , m, and let Y0 j denote the
jth observation on the control, j = 1, • • • ,c. Assume that Yij = T* 4- Sij,
i = 0, • • • , m, where £ij are iid N(0, cr2) variables. Find the distribution
of the least squares estimates of 0* = r* — To, i= 1, • • • , m.
7.7. Suppose £1, " " " , £JV are iid random variables following a normal distribu-
tion with mean 0 and variance 1. Suppose YQ = 0, and Y{ — OYi-1 -F £*,
i = 1, • • • , N , and |0| < 1. Find the maximum likelihood estimate of 9.
7.8. In Exercise 4.5, assume that the errors have a normal distribution. Con-
struct 95% confidence intervals for /?i, /?2 > /?3» Pi — /?2, and /?i +^
3.
7.9. Consider the regression model y = X/? + £, £ ~N(0,a2V), where V is a
known p.d. matrix. Obtain a simultaneous confidence set for c'/3 for all c
in £, using Scheffe’s projection method.
7.10. Derive the F-statistic to test the hypothesis that two straight lines Y\^ =
A,o +
+ £ j,<, i = 1, •
, nz, Z = 1, 2, £/ti ~ iV(0, (T2) intersect at a
point (x0, yo) «

278
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
7.11. Consider two parallel regression lines
i = 1, • • • , n/,
l = 1, 2, ei :i ~ 7V(0, a2). Obtain a point estimate and an interval estimate
for the horizontal distance 5 = { p2,o ~ Pi ,o )/ P between the two lines.
7.12. In the less than full rank linear model, suppose we wish to test H : C'(3 =
d. Let G = (X'X)-. Show that C'GC is nonsingular whenever H is a
testable hypothesis.
7.13. Obtain E( R2) in the multiple regression model with normal errors when
Pi =
* • • = Pk = 0.
7.14. [Wu, Hosking and Ravishanker, 1993]. For i = 1, • • • , iV, let
Si ,
i
/c, k \, h
2,
Ai + £h
— c\\ + A2 + £k+1
— CA2 + £k+2 >
Vi
Yk
Yk+1
n+2
2, |c| < 1 is a known constant,
where A: is a fixed integer, 1 < k < N
and £i’s are iid N (0 ya2 ) variables. Let p = (Ai, A2)/, and suppose a2 is
known.
(a) Derive the least squares estimate of P, and the variance of the esti-
mate.
(b) Derive the least squares estimate of p subject to the restriction Ai +
A2 = 0. What is the variance of this estimate ?
(c) Derive a statistic for testing H : Ai + A2 = 0 versus the alternative
hypothesis that Ai and A2 are unrestricted.
7.15. Consider the model y = p + e, where e ~ AT4(0, cr2I), and
P* =
Derive the F-statistic for testing H : p\ = p$ .
7.16. Prove property 2 in Result 7.4.3.
7.17. In Exercise 4.3, derive the distribution of the maximum likelihood estima-
tor of 0, and use it to construct a 100(1 — a)% confidence interval for 0,
assuming normal errors.
7.18. Based on N observations (Y*, X^
), j = 1, • • , p, i = 1, • • • , iV following
the model (4.1.1) with Si ~ AT(0, cr2), suppose we wish to predict the value
of a future observation, Fo, say. Let P and d 2 denote the least squares
estimates of p and a2 respectively.
(a) Show that the mean of Yo and Var(Yo — Yo) are respectively x.'0P
and oi
d2{1 + xb(X'X) 1XQ}. Find the distribution of (Yo —
x'0 p)/a{1+ xb(X'X)“'1xo}1/2.
Vo
(b) Find the 95% symmetric two-sided prediction interval for YQ .

Exercises
279
(c) Let 7] 6 (0,0.5], and suppose the lOOr/th percentile of the distribution
of YQ is 7(77) = XQ/3 -h ZrjCTYo' Find a 100(1 — a)% lower confidence
bound for 7(77).
7.19. In Example 7.5.1, test Ho : X * = 0 versus H\ : X * ^
0 at the 5% level of
significance when N — 40.
7.20. Consider the balanced incomplete block design (BIBD) with a levels of
treatment, each replicated r times, b blocks, and k treatments per block.
The total number of observations is N — ar = bk. Let A = r( k — l )/ (a — 1)
denote the number of times each pair of treatments appears in the same
block. Test the equality of treatment effects T*, i = 1, • •
7.21. Let Yij = fj,+ Ti +
and Sij ~ N (0,cr2 ), j = 1, * • • , n, i = 1,
• • ,a. Let
0 = (/X, T1,—
,Tay . For / =!, *
• , (a — 1), define
• , a.
C'IP = (E\=1n - ITI+I )/ [1{I +1)]1/2.
Verify that c[ p is estimable. Construct marginal and Tukey simultaneous
confidence intervals for these functions of /3.
7.22. In the regression model
Yi =0o+ faXi + fo&X?- 2) + ei 9
i — 1, 2, 3,
with X\ = — 1, X 2 — 0, and ^3 = 1, what happens to the least squares
estimates of 0Q and 0\ when 02 = 0? Why?
7.23. Consider a two-way ANOVA model with no interaction and n > 1 repli-
cations per cell:
Yijk — \x H- Ti T 0j
£ijkt
, 6, and k = 1, - • • ,n, where eijk are iid N (0,a2 )
i = 1, • • • ,a, j = 1, * • *
random variables.
(a) Show that the MVUE of a2 is
a2 = (T1 +T2 )/N\
where
= ±£ «<7«
1=1 j=1
= EED%-%)2
- y.^
. + y...)2,
Ti
.- y,..
T2
i=l j=l k=1
and AT* = nab — a — b + 1.

280
CHAPTER 7. INFERENCE FOR THE GENERAL LINEAR MODEL
(b) Let Zx = J2i=i°iTi , and Z2 = Yli
U = N*a2/ <72 are jointly independent.
7.24. Let Yi, - -
* ,YN be independent normal random variables with E(Yi ) =
P\Xn + P2Xi2, and Var(Yi ) = a2XnXi2. Assume that X^
s are positive
and the regression matrix X has full rank. Obtain the MVUE’s
and 02
of p\ and P2. If Pi and /?2 denote the OLS estimates of these parameters,
obtain the efficiency of p\ relative to
i.e., obtain Var(Pi )/Var( pi ).
7.25 Consider the model
j=i djPj • Show that Zi, Z2, and
E(Yi )
E(Yi )
Pi,0 + Pi,iXu i = 1, • • • ,n
P2,o + p2,\Xi'> i = n + 1, • * • , n + m,
which corresponds to a two-phase regression in which the straight line
changes phase between Xn and Xn+i. Suppose IVs are independent nor-
mal random variables with variance a2, i = 1, • • • , n + m. Let 7 denote
the value of X at which the lines intersect.
(a) Find the MLE’s of P\
.0, Plyi , /?2,0, /02,1»
and 7.
(b) Construct a 100(1 — a)% confidence interval for 7. Does such an
interval always exist?
7.26. Iri the model Yij = /Z+ T* + £»j, j = 1, • • • ,
i = 1, • • • , 3, £ij ~ N (0, cr2),
derive a test statistic for H : (/LZ+ TI )/5 = (^+ r2)/10 = (/i -f r3)/15. HOW
will you obtain the power of this test ?
7.27. In the one-way ANOVA model Y^ = fi+Ti+eij , j = 1, • • • , n, i = 1, • • • ,a,
with 7V(0, (T2) errors, derive a test of H :
fi -I- T\ = 2(/i + T2) =
• • * =
a(/x + ra).
7.28. Let Y^ = JJ,+ n -b £*j, j = 1,
• • , n, i = 1, • • • , 3, and £*7 ~ JV(0, (72).
Derive a test for H : r2 = (T\ -f 73)/2.
7.29. Consider the three factor model with normal errors, viz., Y^ = /z + T* +
Pj
“b (r/?)y -f 7/c "b (/?7)jfc
“b £ijki i ~ 1) * * *
j
J ^ 1? *
* * »
&
I ) '
» £»
with the constraints Eir* = 0, £7 Pj ~ 0,
= 0, ^2 j(rP )ij = 0,
Ei(r^
)u =
Ej(/37)j/c = 0, and J2k ( p7 ) jk = 0-
Develop a suitable
sequence of nested hypotheses, giving the relevant sums of squares. Com-
plete the ANOVA table.
7.30. Let y ~ £V(/z,V, h), and let B be a q x k matrix of constants, with
r(B) = q < k. Show that the distribution of By ~ Eq(Bjj,,BVB
7).
Hint: Derive the mgf of By.
7.31. Prove property 2 of Result 7.5.3.
7.32. Prove property 2 of Result 7.5.7.

Chapter 8
Multiple Regression Models
A regression model is a mechanism that enables us to describe the relationship
between a response variable and explanatory variables via a functional form
that involves some unknown parameters called the regression coefficients. We
introduced these models in Chapter 4, and discussed inference in Chapter 7.
Regression analysis consists of fitting the model to a set of observed data on
the response and explanatory variables, that is, estimation of the regression
coefficients in order to obtain a fitted regression model and then, assessing the
goodness of the fit. The fitted model is then usually used for predicting the
response at “ new” values of the explanatory variables (see Draper and Smith,
1998).
The general regression model has the form (4.1.1), i.e., y = X/3 -he, subject
to the assumptions (4.1.4) on the errors, i.e., E(e ) = 0 and Cov(e ) = (J 2IN •
The N x p matrix X of predictors usually includes the vector 1^ as the first
column, and we may write X = (1^ : X). We have seen in Chapter 4 that the
least squares estimate of (3 is (3 = (X'X)"1X/y, with Cov( /3) —
cr2(X'X) _1.
The symmetric and idempotent projection matrix P = { pij } = X(X'X)
""1X/
orthogonally projects the response y onto the p-dimensional subspace spanned
by the columns of X. Let x* denote the 2th row of X. Then, pn — x'(X'X)-1Xi,
and p^ = x'(X'X)_ 1Xj. The matrix (I-P) is the projection onto the ( N - p)-
dimensional orthogonal subspace of 7ZN .
Departures from model assumptions
8.1
Residual analysis is a crucial step in assessing the adequacy of a fitted regres-
sion model.
The ordinary least squares residuals, defined by e* = Y* —
Y*,
2 = 1, • • • , AT, represent the difference between the observed responses and the
corresponding predictions from the fitted model. If the fitted model is accurate,
the behavior of the residuals, which may be viewed as “estimates” of the errors,
should be similar to the underlying errors. A careful perusal of the residuals
should therefore enable us to conclude either that the fitting procedure has not
281

CHAPTER 8. MULTIPLE REGRESSION MODELS
282
violated any assumptions, or that some or all of the assumptions have indeed
been violated and there is merit in revising the model fit. Recall from Chapter 4
that the assumptions include (a) normality of the residuals, (b) homoscedastic-
ity, (c) independence and (d) linearity of the model. Regression residuals provide
graphical and nongraphical summaries that enable us to verify departures from
the usual assumptions. In the first subsection, we describe graphical procedures,
while a few significance tests based on the F-distribution are described in the
following subsection. Subsequent sections discuss heteroscedasticity and serial
correlation in regression.
Graphical procedures
Three basic residual plots that enable verification of the regression assumptions
are included routinely in almost all statistical software packages. These are
described in the next subsection and include (i) a plot of residuals versus each
predictor, (ii) a plot of residuals versus fitted values Y*, and (iii) a normal
probability plot of residuals. The following subsections describe more enhanced
plots such as added-variable plots and partial residual plots.
8.1.1
Basic residual plots
In any model including an intercept, we must have £V=1^ = 0, so that an
ideal plot of residuals versus a predictor variable, or residuals versus fitted val-
ues should contain a random scatter of points in an approximate horizontal
band centered at zero. The residuals are always correlated with the actual re-
sponses even when the model gives a perfect fit; therefore, a plot of residuals
versus observed responses would be useless! On the contrary, when the model
fits perfectly, the residuals are expected to be uncorrelated with the fitted val-
ues, so that presence of some correlation would indicate inadequate model fit.
Departures from the “ideal” plot can occur in several ways, and correspond to
violation of specific assumptions. A funnel shaped plot, which widens (becomes
narrow) to the right indicates that the error variance increases (decreases) with
increasing values of the predictor (or fit). This is a departure from the assump-
tion of homoscedasticity of the errors and may be corrected by an appropriate
(variance stabilizing) transformation or by using weighted (instead of ordinary)
least squares for model fitting (see section 4.5). A departure from the linearity
assumption will be indicated by a tendency of the scatter to exhibit curvature.
In such cases, a remedy could be to include polynomial powers of the X/s of
suitable orders as additional predictors in the multiple regression model. In
some cases, especially when the data is observed over time, the independence
assumption on the errors is violated. A plot of residuals versus order (or time)
would indicate this by a pattern of runs, i.e., high (low) values followed by high
(low) values.
The normality assumption is checked via a histogram, or a stem-and-leaf
plot, or a normal probability plot of the residuals. If the histogram or the stem-
and-leaf plot exhibit skewness, heavy-tailed behavior, or multimodality, then

8.1. DEPARTURES FROM MODEL ASSUMPTIONS
283
the normality assumption is suspect. These plots also indicate the presence
of outlying residuals, i.e., large discrepancies between the actual and fitted re-
sponse values. The normal probability plot is a plot of the empirical residual
quantiles versus quantiles from a standard normal distribution; departure from
a straight line would indicate violation of the normality assumption. All these
basic plots are now routinely available with many statistical software.
If normality is suspect, a possible remedy is a transformation of the data (see
section 6.4). In general, the parameter of the transformation A is unknown and
must be estimated from the data. We give a brief description of the maximum
likelihood approach for estimating A (Box and Cox, 1964). Suppose we fit the
model (4.1.2) to the data (x*, ]^
), i = l, - - , N. The estimation procedure
consists of the following steps. We first choose a set of A values in a preselected
real interval, such as (— 5, 5).
For each chosen A, we compute the vector of
transformed variables
= (Y}X\ • • • ,Y^
) using (6.4.3), say. We then fit the
normal linear model (4.1.2) to (xj, y/
A
^ ), and compute SSE( X ) based on the
maximum likelihood estimates (which coincide with the OLS estimates under
normality). In the plot of SSE( X ) versus A, we locate the value of A which
corresponds to the minimum value of SSE( A). This is the MLE of A.
Added-variable plots
Added-variable plots, which are also called partial regression plots are useful
for understanding the role of a single predictor variable in a multiple regression
model (Cook and Weisberg, 1982). Suppose we partition X = (Xi, X2) where
Xi is an N x k matrix which usually includes the constant vector 1/v, with
corresponding projection matrix Pi, and x2 corresponds to a single predictor,
i.e., X 2 corresponds to one of the predictor variables Xj, j = 1, • • • , fc.
An
added-variable plot shows the contribution made by X 2 to the variability in Y
in the model
(8.1.1)
y = Xi/?i + x2/?2 + £,
over and beyond the portion explained by Xi alone. Let e denote the vector of
residuals from fitting the model (8.1.1).
Let e(Y |XI) = (I-Pi)y and e(x2|Xi) — (I- Px )x2 respec-
Result 8.1.1.
tively denote the ordinary residuals from a fit of Y on Xi and that of X 2 on
Xi. Then,
(8.1.2)
£?[e(y|X1)] = Ae(x2|X1).
Further, the OLS estimate of 02 in the model (8.1.1) is
e'(x2|Xi )e(y|Xi )
(8.1.3)
02 = ^(xalXO^xalXO
'

CHAPTER 8. MULTIPLE REGRESSION MODELS
284
Since Pi = Xi(X,
1Xi) 1X'1? premultiplying both sides of (8.1.1) by
Proof.
I- Pi, we see that
(I - Pi )y — (I - PJX^
+AP - Pi)x2+(I - PJe.
(8.1.4)
The first term on the right side of (8.1.4) is zero; taking expectations on both
sides of (8.1.4),
E [e(y|Xi )]- A(I - PJx2 = Ae(x2|Xi) f
proving (8.1.2). Using results on partitioned matrices from section 2.1, and the
idempotency of the matrix (I - Pi), it is easily verified that the OLS estimate
of 02 in the model (8.1.1) is
02 = {x'2(I - Pl )y}/{x2(I - Pl)x2}
which leads directly to (8.1.3).
This result implies that a plot of e(y|Xi) versus £(X2|XI) is expected to be
a straight line through the origin, with estimated slope /32, which incidentally, is
also the GLS estimate in (8.1.1), and the correct estimate to use (see Kruskal,
1968). This is the added-variable plot, which is a visual summary of the t-
statistic (or extra sum of squares F-statistic) for testing HQ : 02 = 0. If all
the points in the added-variable plot lie exactly on a straight line with slope
02, 0 < 02 < oo, i.e., the residuals from the fit of ?(y[Xi ) versus ?(x2|Xi) are
all zero, then X2 is a useful addition to the model. If, on the other hand, the
points lie exactly on a horizontal line, the regression y = Xi/?i explains all the
variation in y, and there is no need to include X2 as a predictor. If the points
in the added-variable plot lie on a vertical line, then e(x2|Xi) = 0, i.e., X2 is an
exact linear combination of the components of Xj, and is therefore superfluous
to the model. This is a situation that we recognize as collinearity and discuss
in detail in section 8.3.
Partial residual plots
Partial residual plots, also called residual plus component plots (Larsen and
McCleary, 1972 and Wood, 1973), are widely used in practice, because they are
computationally more convenient than added-variable plots. Let X = (Xi, x2)
as before. The vector of partial residuals corresponding to X2 is defined by
£2 = y- xi/3i = £ + X2#2-
In other words, these partial residuals are residuals that have not been adjusted
for the predictor variable X2.
A plot of £2 versus X 2 has estimated slope 02 and is called a partial residual
plot. In general, X2 could correspond to any predictor variable Xj, j = 1, • • • , k\
the corresponding partial residuals, which we denote by e* are residuals from a
regression that includes all other variables except Xj in the model. Thus, partial

8.1. DEPARTURES FROM MODEL ASSUMPTIONS
285
residuals contain the remnant variability in the response V, after accounting for
relationships between Y and X [ , l ^
j, l = I, -
* - , /c, and therefore constitute
the portion of the data used for estimating /3j. A plot of these partial residuals
versus X j will show the partial relationship between Y and X j.
Although
they look very different, it may be verified that the partial residual plot and
the added-variable plot for Xj have the same slope and the same residuals.
Cook and Weisberg (1994) have devoted an entire text to the area of regression
graphics.
Sequential and partial F-tests
The ANOVA identity was introduced in section 4.2. In the full-rank regression
case, the identity represents a partition of the total variation in Y into two
orthogonal components, one due to the fitted regression, and the other due to
unexplained error. It is written as y'y = /3'X'y + e'e, or, by subtracting out
the effect due to the intercept from the total and regression sum of squares, as
y'y- NY
2 = (/3'X'y - NY
2 ) + e'e. The ANOVA table corresponding to each
of these forms is shown below. In each case, the mean squares in column 4 are
obtained by dividing the sums of squares (in column 3) by the corresponding
degrees of freedom (in column 2).
8.1.2
Table 8.1.1. ANOVA table for the multiple regression model
Source
d.f.
SS
MS
SSR= /3'X'y
SSE =?e= y'y-0'X'y
SST = y'y
Regression
Residual
Total
MSR
MSE
P
N - p
N
The F-statistic for the joint hypothesis Ho : 0j = 0, j — 0, • • • , k is given by
MSR
( N - p)R2
SSE /(AT- p)
~ MSE ~ p(1- R? ) ’
SSR / p
(8.1.5)
F0 =
which has an FPyN- p distribution under Ho. We reject the null hypothesis at
level of significance a if Fo > Fp,iv-
The ANOVA table corresponding to separating the intercept from the re-
maining k predictor variables is shown in Table 8.1.2, where SSM denotes the
sum of squares due to the intercept term (or mean) and SSRC = SSR- SSM
denotes the corrected sum of squares due to regression. The ANOVA decompo-
sition in Table 8.1.3 commonly appears in many standard statistical software.
The F-statistic for the joint hypothesis Ho : (3j = 0, j = 1, • • • , k is given by
p,a -
MSRC _ { N - k - 1)R2
k {1- R2 )
’
SSRC / k
(8.1.6)
F0 = SSE / { N - k -1)
M S E

CHAPTER 8. MULTIPLE REGRESSION MODELS
286
which has an Fk ,N-k-i distribution under this null hypothesis. We reject the
null hypothesis at level of significance a if FQ > FktN-k-i,<*.
Table 8.1.2. ANOVA table for multiple regression with intercept separated
d.f.
SS
Source
MS
SSM = NY
SSRC = 0' X' y-NY 2
SSE = £*£ = y' y — /3'X'y
SST = y' y
Intercept po
Regression \p0
Residual
Total
MSM
MSRC
MSE
1
k
N - k - l
N
Table 8.1.3. Mean corrected ANOVA table for multiple regression
SS
MS
Source
d.f.
p' X' y-NY
2
Regression|/?o
Residual
Total
MSRC
M S E
SSRC
SSE = e'£ = y' y - /3'X'y
SSTC = y' y-NY
2
k
N - k - l
N - 1
The regression sum of squares SSR or SSRC can be partitioned into mean-
ingful components that enable us to assess the effect of a single explanatory
variable. The partition of SSRC = R(0i, * * * ,0k\/3o ) into sequential regression
sums of squares is given by
SSRC = R((3I\PQ ) + R( fo\0uPo) + • • • 4- R(0k\0k
*
4
> /?i , /3o)
(8.1.7)
-i >
*
where R(9\v ) refers to the “ regression explained by 0 in the presence of
For
example, i?(/?3|/?2, /3i , /?o) denotes the increase in the regression sum of squares
when X3 is included in a model that has the intercept, X\ and X <i. Equation
(8.1.7) represents a partition of SSRC into single degree of freedom contributions
from explanatory variables that are added sequentially one at a time to a model
with just an intercept. Each component represents the incremental increase
in the variability in Y explained by a particular explanatory variable included
into the model. Alternately, we can view R((3s\ f32,Pi ,0o ) as the reduction in
the residual sum of squares by the inclusion of Xz into a model that had the
intercept, X\ and X 2. This sequential sum of squares partitioning enables us
to assess the contribution of each explanatory variable individually.
We can also implement a subset partitioning of SSRC- Suppose we subdivide
X = (lN, Xi, X2), where Xi is N x ki , X2 is N x /c2, such that k = k\+ /c2; and
we correspondingly subdivide (3 = ( fio, /3[ , fyY • The linear regression model in
(4.1.2) can be written as
y - A) + Xi/?i 4* X2/32 + e

8.1 . DEPARTURES FROM MODEL ASSUMPTIONS
287
and SSRC = R( Pup2 I Po ) • We partition
SSRc = R( pl \ p0 ) + R( p2\PuPo ) ,
where R( /32|/?i , /?o) is the extra sum of squares due to regression and represents
the increase in the regression sum of squares by adding X2 to a model that
already has the intercept and Xi . This partition enables us to test whether any
subset of regression coefficients is zero. Suppose we wish to test HQ : /?2 = 0
versus Hi : @2 7^ 0- Under HQ , R( p2 \ PuPo ) / a2 ~ x|
2 > so that the partial
F-statistic
R(02 \ PuPo ) / k2
F* =
(8.1.8)
MSE
has an Fk2 ,N -k-1 distribution under Ho - If F* > Fk2 ,N - k-i ,a > we reject the
null hypothesis at level of significance a, since the extra variation explained by
including X2 in the model in the presence of an intercept and Xi is greater
than what we would attribute to chance. Some authors refer to F* as a par-
tial F-statistic only when a single regression coefficient is tested, i.e., when
fc2 = 1 and the resulting statistic has an F^
jv-fc-i distribution under Ho-
In general, the quantities R(0i|/?o, /?2, /?3, * *
* , Pk ), R{ p2\Po, Pu /%, * * * , /?*) >
•
R{Pk \Poi Pu P2 , * * * , Pk-1) are partial regressionsums of squares. However, these
sums of squares need not add up to SSRC . Hence, these sums of squares and
the resulting test statistics are not independent.
Note however that (8.1.7) corresponds to a complete partitioning of SSRC
and the sums of squares on the right side are independent. The resulting F-test
statistics are called sequential F-statistics. Sequential F-test statistics enable
us to test the significance of the contribution of an explanatory variable in a
model containing the preceeding variables. Clearly, the order of entry of the
variables into the model will affect the results. For example, R(Po\Po, Pi , P2 ) 7^
R{Ps\PoyPi ) and the contribution of X3 to SSRC will depend on which other
variables were included in the model previously. Therefore, an objective and
complete variable screening cannot be accomplished using only sequential F-
tests, unless the selection is implemented in several stages.
Many software
packages contain both sequential and partial F-tests.
8.1.3
Heteroscedasticity
A linear regression model is heteroscedastic if Var(e ) = diag( <rj* , • • • , <rjy), where
not all the a2 are necessarily the same. That is, the errors are uncorrelated,
but do not have identical distributions. Heteroscedasticity is common in many
applications. For example, in a cross-sectional study of firms within an industry,
revenues of large firms might be more variable than revenues of smaller firms.
Consider the simple regression model
Example 8.1.1.
Yi = piXi + Si ,
i
N ,

CHAPTER 8.
MULTIPLE REGRESSION MODELS
288
where £* ~ N (0,a2Xf ), Xi 7^ 0. Using (4.5.5) and (4.5.6), the WLS estimates
of /?i and <r2 are
N 2^
PI ,WLS
— , and
i=1
- PiXi )2
1 rE
-2
aWLS
X2
iV-
2
2=1
Recall that under heteroscedasticity, the OLS estimates of the regression pa-
rameters are unbiased, but no longer efficient. The estimated variances of the
regression estimates are also biased estimates of the corresponding true vari-
ances, possibly resulting in misleading conclusions from hypothesis tests about
regression parameters.
Example 8.1.2. Consider the simple regression model
Yi = pi Xi 4- Si, i = 1, • • • , iV,
where Yi =
Uj / ni, and Xi =
Vj/ni are aggregated variables, and
N (0, cr2/nj), i = 1, • • • , AT. The WLS estimate of Pi in this case is
01,W L S ={Eill
with Var(0i ,WLs ) = <r2/ (T,iLiniXi )-
UiXiYi
The graphical methods of section 8.1.1 help us to diagnose heteroscedasticity.
We can also carry out formal tests for departures from homoscedasticity, viz., a
test of the null hypothesis
H0 : al = (rl =
= <!% = a2
versus specific alternatives for the nature of nonconstant variance, such as the
ones in the previous examples (see Judge et al., 1985, section 11.2). A detailed
discussion of modeling and diagnostics under heterogeneity is given in Carroll
and Ruppert (1988). The following results describe, without proof, some tests
for heteroscedasticity without actually specifying its functional form. Each test
procedure requires a (subjective) grouping of the data based on the heteroge-
neous errors. The first test is based on Bartlett’s (1937) likelihood ratio test for
the equality of a variances from normal populations.
(8.1.9)
Consider the model
Result 8.1.2.
(8.1.10)
yi = Xi/? + ei, i =!, • • • ,a,
where for i = 1, • • • ,a, yi and £i are n^-dimensional vectors, X* is an rii x k
matrix, jE(e<eJ) = ofI, and E(ei£ j ) = 0, i ^
j. The likelihood ratio test statistic

8.1. DEPARTURES FROM MODEL ASSUMPTIONS
289
of the null hypothesis Ho : o\ — o\ = • • • = G2
N = a2 versus the alternative that
the error variance is constant within a subgroups of the observations, but varies
between the groups, is
A = X>'/?2)”
</2
(8.1.11)
i=l
where a2 denotes the MSE from the fitted regression in the ith group, i =
1, • • • ,a, and a2 is the MSE from fitting the same model to all N = £2i=i ni
observations. The statistic -2 In A has an approximate Xa-i distribution.
To improve the x
2 approximation to this test statistic, Bartlett (1937) sug-
gested the following statistic which consists of replacing n* by n*— 1, and dividing
by a scaling constant c:
— a) ln <r2 -
— l)lnSf
2=1
where (m - 1)3? =
~~Yi )2
i ( N ~ a)$2 = XXi(n* “ 1)<?h and
c = l + (V
N-a
i=1
The statistic B has an approximate Xa-i distribution under Ho-
Dyer and
Keating (1980) provided exact critical values for Bartlett’s test statistic for the
balanced case, i.e., n* = n, i = 1, •
• ,a. It is possible that the grouping occurs
in one of the following ways.
First, replications are available at each level
of X and constitute natural grouping. Second, observations may be grouped
together according to geographic region, size, or, as in the case of time series
data, according to whether they correspond to a period before or after a fixed
time to- Note that when the number of subgroups a is equal to 2, we may carry
out the usual F-test for comparing two variances from normal populations. The
usefulness of this test is limited to situations where the grouping of data into
subgroups with approximately equal variances is possible. The test also requires
an assumption of normal populations.
Another test procedure is the Goldfeld-Quandt test (Goldfeld and Quandt,
1965) which is useful in situations where the alternative hypothesis states that
the error variance is an increasing function of the magnitude of the predictor
variable, i.e., Hi : cr2 = CX 2, i = 1, * • • , iV. The test procedure consists of
(subjectively) calculating two least squares regressions, one using data associ-
ated with the low variance errors and the other based on data corresponding to
the high variance errors. A significant disparity in the residual variances from
these two regressions would lead us to reject the null hypothesis. The following
steps facilitate the testing procedure. We first order the data by the magnitude
of one of the explanatory variables Xj. We omit the middle m observations
(with m approximately equal to N/5). Next, we fit two regressions, one asso-
ciated with ( N — m)/ 2 observations corresponding to smaller Xj values, with

CHAPTER 8.
MULTIPLE REGRESSION MODELS
290
residual sum of squares SSEs and the other with the same number of obser-
vations corresponding to the larger X j values and with residual sum of squares
SSEl.
Let MSEs = 2SSEs/ { N - m) and MSEL = 2SSEL/ ( N -
Result 8.1.3.
m) be the residual mean squares corresponding to the two groups. Under the
assumption that the errors are independent and have normal distributions,
MSEL/ MSES ~ E{ N -m-2p)/ 2,( N-m-2p )/ 2
distribution under HQ.
Dropping the central observations with approximately equal error variances
is known to improve the power of the test.
This test is applicable only in
situations where the alternative hypothesis states that the error variance is an
increasing function of the magnitude of the predictor variable. When such an
assumption is untenable, the Breusch-Pagan test may be used. In a normal
linear regression model, suppose the heteroscedasticity is specified by
°i = /(7 + <5Zi )
(8.1.12)
where /(.) is a general function which accommodates linear and logarithmic
forms, while Z could either be a specific explanatory variable X, or a subset
of other predictor variables. The Breusch-Pagan test procedure (Breusch and
Pagan, 1979) consists of the following steps. First, we obtain the OLS residuals
of the simple linear regression of Y on X, and compute a2 = MSE. We next
fit the regression
ef/<j2 = 7 + SZi -}- Ui .
Under the null hypothesis of homoscedasticity, the test statistic SSR/ 2 has a
chi-square distribution with q degrees of freedom, where q denotes the number
of variables represented by Z.
A large value of the test statistic implies a
high correlation between the error variance and Z, which leads us to reject
the null hypothesis.
Alternatively, we can construct a likelihood ratio test.
The log-likelihood function for a normal regression model with multiplicative
heteroscedasticity
log(of ) = 7 + <5\og( Zi )
(8.1.13)
has the form
log(L) = -f log(2*) -11 log(^
2)
PiXi )2 /al
2=1
2=1
Substituting (8.1.13) into the log-likelihood function and maximizing with re-
spect to the regression and heteroscedasticity parameters yields estimates for
A) » /?I > 7
5, based on which the test may be constructed. The assumption
of normal errors is critical for this test. The test proposed by White (1980)

8.1. DEPARTURES FROM MODEL ASSUMPTIONS
291
is closely related to the Bruesch-Pagan test, but does not depend critically on
normality of errors.
To implement White’s test, we use the OLS residuals from the regression of
Y on X to fit the regression
s2 — 'yd- SZi + Ui
and calculate the coefficient of determination R2. The statistic for White’s test
is NR2, which has a chi-square distribution with q degrees of freedom under the
null hypothesis of homoscedasticity, where once again, q denotes the number of
Z variables.
8.1.4
Serial correlation
When data is observed over time, the assumption of independent errors is often
suspect, as indicated by a plot of residuals versus order (time).
The linear
regression model with serially correlated errors has the form
Yt = (3o +
-{- * • * + PkXtk + £* > t — 1, • • • , N,
where the subscript t is used to indicate time, and error terms from different time
periods are correlated. This model can be expressed in the form (4.5.1) with
E(eef ) = cr2V, where V is an N x N p.d. matrix whose form is specified under
an assumption that {e*} is a stationary stochastic process. This assumption
implies that the first two moments of the distribution of e does not depend on
t, and we can write
(8.1.14)
(
1
Pi
" " "
PAT-I
" * *
PAT-2
" " "
PAT-3
P2
1
Pi
Pi
1
P2
Pi
v =
1
\PN-1
PN
2
PN-3
• • •
E{etet- j )/ cr2
E{et£t+j )/ &2, 3 = 1, 2, - - - , is the autocorrelation
where pj
between two random errors j time periods apart.
This specification is still
rather general, and the most commonly assumed specification for the stationary
stochastic process is the first-order autoregressive ( AR(1)) process, which we
introduced in Example 4.5.1.
Consider the multiple regression model with serially cor-
Example 8.1.3.
related errors in (8.1.14), with E(et ) = 0, and Cov(et ) £s ) =
<J 2 p^
~sK s,t =
1, • • • , AT, with |p| < 1. Let (3 denote the OLS estimate of the vector of regres-
sion coefficients, and let e denote the vector of OLS residuals. We estimate the
serial correlation p by
p = ( jrm-1)/(£$).
(8.1.15)
t=2
1

CHAPTER 8.
MULTIPLE REGRESSION MODELS
292
In addition to the residual versus time plot to detect serial correlation, we may
use significance tests and enhanced graphical procedures to diagnose correlation
in errors. The most popular test for serial correlation is the Durbin-Watson test
(Durbin and Watson, 1950, 1951). The null hypothesis is Ho : p = 0 while the
alternative hypothesis is either Hi : p^
0, or H\ : p > 0, or H\ : p < 0.
The exact distribution of the Durbin-Watson test statistic
Result 8.1.4.
DW = { jh (et - et-1 )2 } / { jyt }
(8.1.16)
t=2
t=1
depends on X.
For a given level of significance a, the bounds test has the
following decision rules based on lower and upper critical values (at level a)
d>L,N ,p and du,N ,P
>
1. If H\ is p > 0, reject H0 at level of significance a if DW < dL,7v,P; do not
reject Ho if DW > du,N ,P\ and the test is inconclusive if dL,N,p
DW <
du,N ,P-
2. If Hi is p < 0, the decision rule has the form of the rule for Hi : p > 0,
replacing DW by (4 — DW ).
3. If Hi is p 7^ 0, reject Ho at level of significance 2a if DW < dL ,N ,p or
4 — DW < dL,N ,p\ do not reject Ho if DW > du,N , p or 4 - DW > du,N ,P
',
and the test is inconclusive otherwise.
Proof.
The Durbin-Watson statistic can be written as
DW = e' Ae/ efe
where
0
0\
0
0
(
1
-1
0
• • •
2
-1
• • •
-1
A =
0
0
0
• • •
2 -1
\ 0
0
0
-1
1/
is a symmetric matrix with eigenvalues Ai < A2 <
• • • < XN , £ = (I — P)£,
and e ~ SN (0, cr2(l - P)) (see property 2 of Result 7.1.1).
Under Ho, the
ratio DW can be reduced to a canonical form by simultaneous diagonalization
of the numerator and denominator quadratic forms (see section 2.5). Since A
is symmetric, there exists an orthogonal transformation e = B£ such that
e' Ae/ e'e = X)
N-p
E ff
2— 1
2=1
being iid JV(0, cr2) variables, and where Vi are the N — p nonzero eigenvalues
of (I — P)A(I — P). Correspondingly,

8.1. DEPARTURES FROM MODEL ASSUMPTIONS
293
V— p
/ N — p
E M t
2 / E e
1=1
/
2=1
rw =
The exact distribution of DW depends on the eigenvalues of A and (I — P).
Under the assumption of normality and independence of the errors, Durbin
and Watson obtained upper and lower bounds for the distribution oi DW and
tabulated percentage points of these bounding distributions. Since
A* < Vi
Ai+fc, i = 1, • • • , N - p,
it follows that
di < DW < dy
where
N — p
N — p
dL
=
£ » and
2=1
2=1
N — p
j N — p
Y2 v*+ p&/
2=1
/
2=1
(8.1.17)
dy
These bounds do not depend on the particular predictor matrix X, and lead
to the Durbin-Watson test. Since the values of DW lie between 0 and 4, and
are symmetric about 2, property 2 follows.
The decision for the two-sided
alternative is immediate.
Note that the summation in the numerator of the DW statistic runs from
t = 2 to N since ?o is not available; this is referred to as an “end effect” . The
statistic DW lies in the range of 0 to 4, with a value of 2 indicating the absence
of first-order serial correlation. When successive values of et are close together,
DW is small, indicating the presence of positive serial correlation. For a relation
between DW and p, see Exercise 8.4. It is important to realize that not rejecting
the null hypothesis does not necessarily mean that the errors are uncorrelated;
it simply means that there is no significant first-order autocorrelation. More
complex linear stationary time series models may be employed in order to model
autocorrelation in regression errors. An example is the autoregressive moving-
average process of order (p, q ) defined as
£t =
+ 4>2£t-2 +
* * * •+- <j)pSt- p -b Ut + OiUt-l -b 02Ut-2 + * * ' OqUt-q
where ut s are iid N {0,a2 ) variables, and the (p -b (?) parameters ( <£i, • • • , (j>p),
and (0i, • • • , 9q ) must satisfy certain restrictions (for details, see Brockwell and
Davis, 1998). Plots of the sample autocorrelation function and partial autocor-
relation function enable us to identify the model order in these cases.
If the errors in a regression model are serially correlated, an iterative estima-
tion procedure called the Cochrane-Orcutt procedure is useful. Let /? denote the
OLS estimate of /3, and let e be the OLS residual vector. In this initial step, the

CHAPTER 8. MULTIPLE REGRESSION MODELS
294
first-order serial correlation is estimated by (8.1.15). Denote these quantities by
/?(°
) ,
and
respectively. Using p^°\ we transform the original model and
obtain OLS estimates from the transformed regression
Y* = /3o(l - p<0)) + fax;J +
• + (3kx;k + ut, t = 1, • • • , N ,
where Yt* = Yt - pWYt-U and X *tj = Xtj - fi0 )Xt-ltj , j = ! ,
,k. Let
ft( l ) and e*1* respectively denote the vector of regression estimates, and residual
vector from this first iteration. We repeat this procedure several times until
convergence. This procedure works well in practice, although there is always the
danger that the procedure may tend to a local rather than a global maximum.
Numerical Example 8.1.
Consider data from the first quarter of 1952 to
the fourth quarter of 1956 on consumer expenditure in the U.S. in billions of
dollars (F), and money stock in billions of dollars (X). Let t denote the time
period (Chatterjee and Price, 1991). The regression estimates and the ANOVA
table are shown below.
Least squares estimates for Numerical Example 8.1.
Parameter
d.f.
Estimate
s.e.
lvalue
Pr > t
Intercept
1 -154.954
19.883
-7.79
< .0001
2.302
0.115
20.06
< .0001
X
1
ANOVA table
Source
d.f.
SS
MS
.F-value
Pr > F
Model
1
6395.170
6395.170
402.34
< .0001
Error
18
286.107
15.895
Corrected
Total
19
6681.278
Also, a = 3.987, R2 = 957, R^dj.= 955, and the estimate of the first-order
serial correlation p = 0.752, with s.e. 0.160. The Durbin-Watson statistic is
DW = 0.326; comparing it to the lower and upper critical values di = 1.201
and d\j = 1.411, we reject HQ : p = 0 at the 5% level of significance. One way
to fit a regression model with AR(1) errors to the data is via a two-step least
squares procedure. The results from this fit are shown below.
Simultaneous estimates of /3 and p
Parameter
d.f.
Estimate
s.e.
£-value
Pr > t
Intercept
1 -158.280
32.127
-4.93
.0001
2.327
0.186
12.54
< .0001
X
1
The estimate of the AR(1) parameter is p = 0.752, with standard error
0.160.
A

8.1. DEPARTURES FROM MODEL ASSUMPTIONS
295
8,1.5
Stochastic X matrix
In (4.1.1), suppose the N x p matrix X is stochastic. In practice, this is the
case when some or all of the explanatory variables either cannot be measured
accurately, or can only be measured indirectly.
Suppose the stochastic predictor X has rank p, and is inde-
Result 8.1.5.
pendent of e. The least squares estimator /3 is an unbiased estimator of (3 with
covariance matrix cr2£'{(X/X)_1}. Also, a2 is an unbiased estimator of a2.
Proof.
The independence of X and e implies that
E(e|X) = E(e ) = 0,
Cov(e|X) = Cov(e ) = <T2IA/-.
and
It follows that
E0) = (3 + ^{(X'X)-1^}^
) = (3
and
E{ (X'X)"toXtX'X)"1}
E{ E [{ X' X ) ~lX'ee'X(X'X)_1]|X}
^{(X'X)"1}.
Cov((3)
Also
E(a2 ) = E{ E(a2 )|X} =
E{ E [( N - p)-l?s\X } }
=
E{ ( N - py' EpelX ) }
=
E{ ( N - p)-\N - p)a2 } = a2. U
Although /3 is a stochastic function of y, and is no longer considered the
b.l.u.e. of (3, it is still efficient if we consider its covariance conditional on a
given X. If e has a 3V(0, CT2IN ) distribution, and the distribution of X does not
involve /3 or cr2, then the pdf of e coincides with the conditional pdf of e given
X, and the MLE of (3 coincides with its least squares estimator.
In some situations, the independence of X and e is untenable. For example,
consider the model
yt = /J1xt + /?2yt.1 + e<
where Xt is a deterministic predictor. By repeated substitution, we can verify
that the distribution of Y t-1 depends on
* * * • The following result
summarizes some asymptotic properties of the least squares estimator in the
situation where X is partially independent of e.
If the sequence of estimators 0/v based on N observations converges in prob-
ability to a constant 0, we say that 0 is the probability limit of the sequence 0/v,
and write plim 0^ — 9.
* =!,. •. , iV

CHAPTER 8. MULTIPLE REGRESSION MODELS
296
In the model (4.1.1), suppose X and e are partially indepen-
Result 8.1.6.
dent, and assume that plim e'e/ N — a2, plim X' X /N = Exx is nonsingular,
and plim X'e/ N = 0. Then plim /3 = /3, and Nl / 2{ f3 —
/3) has a A/
’(0, <J2E“x1)
distribution.
The least squares estimator /3 is consistent since
plim /3 =
(3+ p\im( X' X /N )~1( X.,
£/N )
=
/3 + plim(X,X/Ar)-1plim(X,e/iV) = /3,
using Slutsky’s theorem (Casella and Berger, 1990).
The limiting distribu-
tion of /3 follows directly from the limiting distribution of Nl / 2((3 — /3) =
(X'X/N)-1(X'e/iV1/2).
The reader is referred to Judge et al., (1988) for more details on this topic,
particularly on measurement errors in the predictors, and instrumental variable
estimators.
Proof.
Model selection in regression
8.2
We discuss procedures in regression useful for selecting variables from a set of
possible regressors Xi, • • • , Xk . In particular, we describe selection of the best
regression equation via (a) all possible regressions, using criteria such as R2,
adjusted i?2, S2, Mallows Cp statistic and the PRESS statistic; (b) best subset
regression using these criteria; (c) forward selection; (d) backward elimination;
and (e) stepwise regression. We begin with a description of the selection criteria.
Although the coefficient of determination R2 is a traditional criterion for
regression model selection, it is not prediction performance oriented, and hence
should always be used in conjunction with other criteria for choosing the best
prediction model from a set of candidate models. Since the inclusion of a new
explanatory variable into a model can never decrease SSR, and consequently
the value of R2
y there might be a tendency to overparametrize in order to
achieve a large R? value. The adjusted R2 ensures parsimony by imposing a
penalty for including marginally important explanatory variables at the cost
of error degrees of freedom. The use of a2 as a model comparison criterion
would entail choosing the model which has the smallest d2 value. Let d2( k\ )
and d2( k2 ) denote the residual mean squares from two fitted regression models
with k\ and &2 regressors respectively, where k\ <
It could happen that
d2( k\ ) > <?2(A;2), which would perhaps imply that the reduction in residual mean
squares by fitting the model with k\ parameters did not counterbalance the loss
in residual degrees of freedom. We define another criterion for model selection
which compares the standardized total mean squared error of prediction for the
observed data (Mallows, 1973, 1995).
Definition 8.2.1.
gression model, where /3 is a ^-dimensional vector. Let y = X 1(3i ~\-e denote the
Mallows Cn.
Let y = X/3 4- e denote the “true” re-
v

8.2. MODEL SELECTION IN REGRESSION
297
fitted model, where the dimension of (5\ is p. Let yi = X\(3\ denote the fitted
vector. Then,
E{yi) = X1(X'1X1)“ 1Xi£J(X/? + e ) = X^
X' Xx^
X' X/? = r/p, say.
The Cp statistic which is useful for comparing various fitted models with p
parameters to the full q parameter model is
y'(i - Pi )y
SSEP
~ ( N ~ 2p)
— ( N — 2p) =
(8.2.1)
Cp —
az
a2
where SSEP is the residual sum of squares from a model with p regression
coefficients (including the intercept), and a2 is the residual mean square from
the “full-model” containing all the regressors and is presumed to be a reliable
estimator of a2.
Note that Cq is a fixed quantity given by
SSEq/a2 — ( N — 2q)
( N - q)a2/a2 — ( N - 2q) = q.
Cq
Otherwise, Cp is a random variable, which is an approximately unbiased esti-
mator of the expected standardized total mean square of the predicted values
as the following result shows.
Result 8.2.1.
Let
j; = E(yi - X/3)'(yi - X(3)/ a2.
Then, E(CP ) « j;.
Proof.
Since I — Pi is symmetric and idempotent,
E{ y' ( I - P 1 ) y } =
tr {(l - P 1 )a2IN } + f3' X' (l - P 1 )X(3
=
<J 2{ N - p ) + p' X\I-P1)(I- P1)X/3
-
a2( N - p) + (X/3- P1X/3)'(X/3- PxX/J)
=
a2( N - p)+ ( XP - T]py( XP - r]p ).
(8.2.2)
Also,
E(yi - X/?)'(y, - X/3)
= £(yi - tfr)'(yi - r?P) + (X/? - i,p)'(X/J -% )
=
tr{Cov{ yi ) } + ( Xf3- rjpy( XP- r] p )
=
pa2 + (X/? — rip)'{X.P — rjp ).
(8.2.3)
Subtracting (8.2.3) from (8.2.2), we get
E(yi - X^
)'(y!- X/?)-J5{y'(I-Pi)y} = (2p- N )a2.
(8.2.4)

CHAPTER 8.
MULTIPLE REGRESSION MODELS
298
The proof follows by dividing both sides of (8.2.4) by a2.
Computed values of Cp for fitted models with different subsets of variables
are useful for model selection. If the fitted model is unbiased, we have r)p = X/?,
so that E(CP ) = p. Adequate models with Cp values close to p will be indicated
in a plot of Cp versus p. Biased models with considerable lack of fit will be
indicated by points that are substantially above the line Cp = p, although
because of randomness, adequate models may sometimes fall below the line.
Since the actual value of Cp is an estimate of the expected standardized total
mean square of the predicted values J*, the height of each plotted point is
also important. As regressors are included in the model, SSEP decreases, and
Cp usually increases. The “ best” regression model is one that gives the lowest
possible value of Cp for which Cp « p. Note that there are multiple Cp values
corresponding to a given p, i.e., corresponding to different subsets of p (out of q)
regressor variables. In many cases, when the choice of model is not obvious from
the Cp plot, personal judgment may be employed, or alternative approaches may
be used.
We next define the PRESS or Prediction Sum of Squares criterion. Let
Yiy(i ) = x'/?( j) denote the prediction for the ith response from fitting the regres-
sion model (4.1.2) with the zth case excluded. That is, we fit a regression model
to observations (YJ, xj), l j- i, l = 1, • • • , N. Then, the estimated (3 vector with
the ith case excluded is
(X'X)-1 +
P(i )
(X'X)_1Xjx'(X'X)_1 x(i)y« -
(8.2.5)
1 ~ Pa
The last equation on the right follows from observing that (X'^
X^
) 1
(X'X — x^x')-1 and property 5 of Result 1.2.10. Let
£i( i)
Y
"
i
,
i
1, *
•> N
denote the corresponding prediction residuals or PRESS residuals.
Corre-
sponding to each candidate model that we fit, there will be N PRESS residuals
based on which we define the PRESS statistic.
(8.2.6)
Definition 8.2.2.
The PRESS statistic is defined as
N
N
PRESS =T (Yt - yi(i) )2 = J2?2
(8.2.7)
»(*) *
i=1
i=1
This statistic condenses information in the form of N validations, each with
a fitting sample of size ( N — 1). The model which has the smallest computed
PRESS statistic value is preferred. Although it seems that in order to construct
the PRESS statistic, we must run N separate regressions, this cumbersome

8.2. MODEL SELECTION IN REGRESSION
299
computation is not necessary since the PRESS residuals are related to the
ordinary residuals as the next result shows.
Result 8.2.2.
The predicted residual is related to the ordinary residual by
Si
(8.2.8)
1- Pti
It is left as an exercise for the reader to verify that (3^
may be
Proof.
written as
(8.2.9)
Pd ) = P ~
1- Pit
Substituting (8.2.9) into (8.2.6), the result follows. If the regression errors are
normally distributed,
have the same correlation structure as the £i, have 0
means and variances equal to a2/(l - pa ). Use of
will tend to emphasize
cases with large pa, while use of Si will emphasize cases with smaller pa.
Using this result, the PRESS statistic is easily computed as
N
2
PRESS =£(r?k)
(8.2.10)
A related statistic based on PRESS residuals is
PRESS
D2
_ 1
;
pred
1
N
(8.2.11)
i=i
We next give a brief description of some widely used selection procedures
that use these criteria in addition to t-tests and F-tests. All Possible Regressions
is a cumbersome procedure that requires the fitting of every possible regression
equation that always includes an intercept and may include any of the variables
Since each of the k explanatory variables can either be included
or not in the regression model, we must fit 2k possible regressions; even when
k = 10, this requires 1024 regression fits. Each fitted regression is generally
assessed on the basis of three criteria, viz., i?2, 52, and Mallows Cv statistic. In
general, when there are several regressors, an analysis of all possible regressions
is quite unwarranted in terms of effort and computer time. With some initial
thought, a majority of the models that are considered in such an analysis could
be avoided and a suitable selection procedure that compares only a subset of all
possible regressions, such as best subset regression is usually preferred.
Stepwise regression is the most widely used approach for variable selection
and is available in most standard regression packages. The procedure uses t-
statistics (or corresponding p-values) to determine the significance of the predic-
tor variables. At the outset, we choose values of aenter 7 and astay 5 each of which

CHAPTER 8. MULTIPLE REGRESSION MODELS
300
may be equal to 0.05. Here, aenter is the probability of a Type I error related
to including a predictor variable into the existing regression model, while astay
is the probability of a Type I error related to retaining in the model a predictor
variable that was previously entered. In the first step, we consider k regression
models of the form
Y — Po + PiXj + e,
which includes only the jth predictor, j = 1, • • • , k. For each model, we compute
the ^-statistic (and p-value) for testing Ho : Pi = 0 versus H\ : /3i ^
0. Let Xjq
denote the variable corresponding to the largest absolute value of the t-statistic
(i.e., the smallest p-value) and suppose the corresponding regression model is
Y = Po + PiX [i ] + £.
If Ho is not rejected (because the absolute value of the ^-statistic is smaller than
the t
Center), then the stepwise procedure terminates and the chosen model is
Y = p0 + e .
If, however the absolute value of the t-statistic is greater than £ jv-2,c*/2 > then
X[ij is retained, since we see it as being significant at the center level. In the
second step, we then consider k — 1 possible regressions, each with two predictor
variables
critical value, with a = aenter, he., if the p-value is greater than
N-2,a/2
Y — Po H- P\X {i\ + PzXj + £,
which includes the predictor X^
chosen in Step 1, and one of the other k - 1
predictors. For each model, the t-statistic (and p-value) associated with testing
#o : P2 = 0 versus #i : P2 ¥= 0 is computed. Let X^\ denote the variable
corresponding to the largest absolute value of the t— statistic (i.e., the smallest
p-value) and suppose the corresponding regression model is
Y = Po +• PlX
^
i ] + /?2-^[2] + s.
The variable X^
] is retained if the t-statistic indicates that it is significant at
the aenter level, and the stepwise procedure also checks to see whether or not
X[i] should continue to remain in the model. If the p-value corresponding to
#o : Pi = 0 versus H\ : Pi ^
0 is smaller than astay > then X^
is significant and
is retained; otherwise, it is dropped from the model. If
is retained in the
model, we have a two-variable model and we proceed to the next step. If, on
the other hand, X[Xj is dropped, the current one-variable model is
Y = Po + P2X (2] +e.
We now are back to the position at the start of the second step, and must
search for another predictor variable that is significant and will be included in
the model.
We continue with this procedure of adding predictor variables into the model,
one at a time. At each step, a variable is included in the model only if it has

8.2. MODEL SELECTION IN REGRESSION
301
the largest t-statistic among all variables not in the model, and further, it is
significant at the aenter level. After including a variable, the procedure checks
all the variables in the model and excludes any variable that is not significant at
the astay level. All necessary exclusions are made before the procedure attempts
to include a “ new” variable. The procedure terminates when all the predictor
variables that are excluded are insignificant at the aenter level, or when the
variable to be included in the model is the one that was just removed. The
choice of values for center and astay is arbitrary and has been discussed in the
literature. In general, it is recommended that astay is greater than aenterj since
this will preclude the subsequent “easy” inclusion of the same variable which
was previously excluded. Draper and Smith (1998) suggest that aenter and astay
are equal, and each is either .05 or .10. If aenter and astay are set higher than
0.10, more independent variables are likely to be included in the model. In
general, it is likely that some important variables (such as higher order terms
or interaction terms) may be omitted, while some unimportant variables may
be included in the model.
The forward selection procedure is a sequential variable selection procedure
that systematiclly adds explanatory variables to the existing regression model
on the basis of partial F-tests (see section 8.1.2). The idea behind the procedure
is similar to that of stepwise regression, the difference being that once a pre-
dictor variable is included in the model, it is never removed. The initial model
contains only the intercept term. In the next step, the explanatory variable
which corresponds to the largest partial F-value enters the model, provided it
is significant at the center level. Let us denote this variable by X^. In the
next step, Xp], the regressor which has the largest partial F-value among the
predictors outside the model is included, provided it is significant at the aenter
level. This process is continued until we have included all significant predictors
into the model. Forward selection is usually considered to be less effective than
stepwise regression.
The backward elimination procedure is a sequential procedure that system-
atically removes a predictor from an existing model based on partial F-test
statistics. We first consider a model which includes all k potential predictor
variables and an intercept. We pick the independent variable having the small-
est partial F-statistic. If this statistic is significant at the astay level, then the
final model contains all k variables, and the procedure is terminated. If, how-
ever, this varaible is not significant at the c*stay level, it is excluded from the
model. At the next step, we run a regression with the remaining ( k — 1) predic-
tors, and repeat the first step. The backward elimination procedure continues
by excluding, if possible, one variable at a time from the regression, and ter-
minates when no predictor variable in the model can be removed. If a modeler
prefers to start the model fitting by including all possible variables, then the
backward elimination procedure is a reasonable approach. Quite often, it results
in the same final model that is given by the stepwise regression procedure.
Another procedure known as the MAXR procedure selects variables for in-
clusion into a regression model based on R2 values (see 4.2.20). Unlike the other
methods discussed until now, the MAXR method looks for the “ best” jf-variable

CHAPTER 8. MULTIPLE REGRESSION MODELS
302
model, for j = 1, • • • , k. We first select the one-variable model which gives the
highest R2 value. We then consider two-variable models, by including, one at a
time, the remaining ( k — 1) predictor variables, and choose the best two-variable
model yielding the largest R2 value. Each variable in this selected two-variable
model is now compared to each of the ( k — 2) variables not in the model. In
each comparison, the MAXR procedure determines whether substituting one of
these ( k — 2) variables for a variable currently in the model will increase the
R2 value. This process continues until we decide that no further substitutions
which increases R2 can be made, and we have chosen the ’’ best” two-variable
model. Another variable is included in the model, and the process is continued
to find the “ best” three-variable model, and so on. This procedure therefore
gives us k “ best” models.
Numerical Example 8.2. Variable selection.
to an engineering application that was concerned with the effect of the composi-
tion of cement on heat evolved during hardening (Woods, Steinour and Starke,
1932). The data consists of 4 predictor variables, Xi ) the amount of tricalcium
aluminate, X 2, the amount of tricalcium silicate, X3, the amount of tetracal-
cium alumino ferrite, and X 4, the amount of dicalcium silicate. The response
variable is Y , the heat evolved per gram of cement (in calories). Results from
the different selection procedures are summarized below. All variables left in
the model are significant at the 0.10 level.
The following data relates
R2 selection method
R2
No. in
model
Cp
Vars. in
model
*4
0.674
138.731
0.666
142.486
X 2
1
1
XUX 2
XUX 4
2
0.979
2.678
0.973
5.496
2
xux2 ,x4
X U X2 , X3
0.982
3.018
0.982
3.041
3
3
XUX 2, X3, X 4
4
0.982
5.000
Summary of forward selection
Step
Var.
No. of
Partial
Model
Cv
vars. in
R2
F-value
Pr > F
R2
in
A4
1
0.675
0.675
138.731
22.80
0.299
0.973
5.496
108.22
0.010
0.982
3.018
5.03
0.0006
< .0001
0.052
1
X i
2
X2
3
2
3

8.2. MODEL SELECTION IN REGRESSION
303
Final parameter estimates under forward selection
Var.
Estimate
s.e.
F-value
Pr > F
Intercept
71.648
14.142
1.452
0.117
0.416
0.186
-0.237
0.173
25.67
0.0007
154.01
< .0001
5.03
0.052
1.86
0.205
X i
X 2
X4
Summary of backward elimination
Step
Var.
No.
removed
vars. in
Partial
Model
Cv
F-value
Pr > F
R2
R2
X 3
3
0.000
0.982
3.018
0.02
0.004
0.979
2.678
1.86
0.896
0.205
1
2
V4
2
Final parameter estimates under backward elimination
Estimate
s.e.
F-value
Pr > F
Var.
Intercept
52.577
1.468
0.662
2.286
528.91
< .0001
0.121
146.52
< .0001
0.046
208.58
< .0001
Xx
X 2
Summary of stepwise selection
Var.
No.
Partial
Model
Cv
removed
vars.
R2
Pr > F
Step
Var.
entered
F-
R2
value
V4
0.675
0.675
138.731
22.80
0.298
0.973
5.496
108.22
0.010
0.982
3.018
5.03
0.004
0.979
2.678
1.86
.0006
< .0001
.0517
.2054
1
1
2
2
V2
3
3
V4
2
4
Final parameter estimates under stepwise selection
F-value
Pr > F
Var.
Estimate
s.e.
Intercept
52.577
2.286
528.91
< .0001
0.121
146.52
< .0001
0.046
208.58
< .0001
Xi
1.468
V2
0.662
For more details, see the first author’s website.
A

CHAPTER 8. MULTIPLE REGRESSION MODELS
304
8.3
Orthogonal and collinear predictors
Suppose that in the model y = X /3 + e we can partition the N x p matrix X
into (r + 1) sets of columns denoted in matrix form by
X = (Xo,Xi, • • • , Xr),
and that the p-dimensional vector ft is partitioned conformably so that
0' = (Po, Pi,'
,0r ).
The linear regression model can be written as
y = XQ/?O + Xi/?i -f
• • • + xrpr.
Orthogonality in regression
Orthogonality among the predictors comes up occasionally in regression prob-
lems.
We take a brief look in this section. In Figure 8.3.1, ljv and x are
orthogonal vectors. It follows that OC2 = OA2 -f OB2.
8.3.1
y
/
A
,-r
1
A
y \
a
B
/
Orthogonal predictors, p ==
• 2.
Figure 8.3.1.
Result 8.3.1.
of Xj for all h^
j, i.e., X'^Xj = 0. The least squares estimate of ftj in the full
model y = X /3 + e is unchanged if any of the other /3^’s are set equal to zero,
i.e., if the corresponding sets of regressors X^’s are omitted from the model.
Suppose that the columns of X^ are orthogonal to the columns
The matrix X'X in its partitioned form has a block-diagonal struc-
Proof.
ture, i.e.,

8.3. ORTHOGONAL AND COLLINEAR PREDICTORS
305
( X'0 X0
0
0 \
XiXi
•
0
0
XX =
x;xry
V
o
and X'y = (Xoy
X [ y
• • •
X^
y)
7. The least squares estimator of (3 is then
/3' = (/?o, /?i, • • - fir ), where (3j = (X'Xj)_1X'y, j = 0, • • • ,r has the form of
the least squares estimate of (3j in the model y =
'Kjffj+e. Due to the block-
diagonal structure of (X'X)-1, the least squares estimate of (3j in the full model
y = X/? + e is unchanged if any of the other /Vs are set equal to zero, i.e., if
the corresponding sets of regressors X^’s are omitted from the regression. This
is a special property unique to orthogonal regressors. In this case the regression
sum of squares is
0
/3'X'y = E^
X'y,
j=o
so that if we omit the regressor X/* from the model, the residual sum of squares
is increased by /3'hX'hy.
Use of the Gram-Schmidt orthogonalization permits the full-rank model in
(4.1.2) to be reparametrized and expressed as Yi = 70 +7iZ»i H
1-7k%ik + £ii
i = 1, •
• , N , where the matrix Z has orthogonal columns, and for r = 0, • * • , k,
7r = 7r+1 = • • • = 7fc = 0 if and only if (3r = (3r+\ = • • • = 0k = 0 (Seber, 1977,
p. 60). Orthogonal polynomials have been used for overcoming problems, such
as ill-conditioning, that are encountered in fitting polynomial regression models
(Hayes, 1974), as the following examples describe.
Orthogonal polynomials in curvilinear regression.
Example 8.3.1.
In the simple linear regression model (4.1.6), E(Y \X ) is a linear function of
a predictor X . In some examples, however, there might be a curvilinear rela-
tion between the response and predictor variables, which may be adequately
modeled by a polynomial model of degree m:
Yi =0o +(hXi + /32X?+ * - -+0mX?
l + ei, i = l, - - - , N.
Written in the form y = X/3-he, with {X}*i = 1, and {X}^ = Xf , j =1,
•
i = 1,
• • , iV, the properties of Corollary 7.1.1 hold for this mth order polyno-
mial model. In practice, the degree m is unknown, and is determined using
suitable tests of hypotheses (see section 7.4), which could be computationally
intensive. Prior to the extensive availability of powerful computing, orthogonal
polynomials in X were used.
Let 4>r( Xi ) be an rth degree polynomial in X*, r = 0,1, • • • , m, and suppose
the polynomials are orthogonal over a set, i.e.,
(8.3.1)
• , m
Ny^
MXi )MXi ) = 0
(8.3.2)
2— 1

CHAPTER 8. MULTIPLE REGRESSION MODELS
306
for r 7^ s = 1, •
• , m. The orthogonal polynomial regression model
m
Yi = y^j3r (pr ( Xj ) + Si
(8.3.3)
r=0
can be written in the form (4.1.1) with
( M X l )
M X i )
• • •
0m(*l )\
M*2 )
M X 2 )
0m(x2)
x* =
\0o(^Ov)
01(^0v )
0m(-
3Ov)/
having mutually orthogonal columns, so that X'X is a diagonal matrix with rth
diagonal element YALI 0r(^i) > r = 0» * * *
From (4.2.4), the least squares
estimate of (3 is (3= (/?o,
• • , Pm )\ where, for all m,
Pr = {£f=1 MXi )Yi )/{£f=1 <j>2r( Xi ) } , r = 0,1, • • • , 771.
Due to the orthogonal structure of X, /?r, r < m is independent of the polyno-
mial degree m. If we set 0o(Xi) — 1, we get /?o = Y > and
=
y'y - /J'x'x/3
AT
m
N
= XX -
2=1
r=0
i=l
AT
m
N
= ^
Y i -Y )2 ~'£ pr<r<f>2r( Xi ).
SSE
i=i
r— 1
2=1
Suppose we wish to test if : /2m = 0. By orthogonality,
N
m-1
N
SS-Ej*
i=1
r=l
2=1
AT
55£ +^E^
(X*)
2=1
and it follows from (7.2.12) that the test statistic
N
F( H ) = {£#,(*,)&} / { S S E/ ( N - m-1)} ~ F,,N — m— 1
2=1
under H.
The advantage of this approach is that the model of polynomial degree m
may be easily enlarged to a model of polynomial degree m+1 by simply adding
one more term (3m+i <fim+i { Xi ) to (8.3.3), with 0m+i (Xj) satisfying (8.3.2). It

8.3. ORTHOGONAL AND COLLINEAR PREDICTORS
307
may be verified that the resulting computations are simplified by the assumption
of orthogonality.
Example 8.3.2. Response surfaces.
Consider the model
Yi —
<j>( Xn , • • • , Xik ) 4- Si, i — 1, * * *
(8.3.4)
N ,
where <j){ Xn,Xn, • • • , Xik ) is a polynomial of degree m i n X i, - * - , X & (usually,
m = 2, or m = 3). When m = 2, we can write for i = 1, • • • , N,
k
k
k
k
Yi = Po + y^fijXjj + y
+Y^
pjiXijX* + ei -
(8.3.5)
j=l 1=1
j=1
j=l
The function 0(Xi, • • • , Xk ) is the response surface, and the model (8.3.5) can
be written in the form (4.1.1) with X = (lyv,Xi, X2), where for i = 1, • • • , iV,
{Xi}itj = X i j , and {X2}iJ >7- corresponds to values of X?j , and X i j X u, j,l =
1, • • • ,
j ^
l. Note that X2 is automatically determined once the variables
in Xi are chosen. For details on optimum choice of Xi to achieve an optimum
response Y , see Myers (1971). The results of Chapter 7 are useful in order to
obtain the b.l.u.e.’s of the parameters and to carry out inference.
Multicollinearity
A multiple regression model fit is useful if the response variable is highly cor-
related with the set of explanatory variables. However, it is necessary that the
explanatory variables are not highly correlated among themselves. A situation
where this occurs is referred to as multicollinearity. In this section, we assume
that the regressors are scaled to unit length (by dividing Xij by y^
LiXfj ) ^
but are not centered (Belsley, 1984). The resulting column-equilabrated pre-
dictor matrix XE (see Example 4.1.2) is used for detecting multicollinearity.
Collinearity (or multicollinearity) exists when there is “ near-dependency” be-
tween the columns of XE, i-e., when X^
XE is nearly singular. In such cases,
the data/model pair is said to be ill-conditioned, and the resulting least squares
estimates tend to be unstable with large variances and covariances. The pres-
ence of a high degree of multicollinearity tends to cause the following problems.
First, the standard errors of the regression coefficients will be very large, re-
sulting in small associated ^-statistics, thereby leading to the conclusion that
truly useful explanatory variables are insignificant in explaining the regression.
Second, the sign of regression coefficients may be the opposite of what a mech-
anistic understanding of the problem would suggest. Third, deleting a column
of the predictor matrix will cause large changes in the coefficient estimates
corresponding to the other variables in a model based on the remaining data.
Multicollinearity does not however, greatly affect the predicted values.
Several approaches have been suggested in the literature for the detection
of multicollinearity. Detection consists of two aspects: (a) is multicollinearity
8.3.2

CHAPTER 8.
MULTIPLE REGRESSION MODELS
308
present? and (b) if it is, how severe is it? The following measures usually in-
dicate severity of multicollinearity. The simple correlation between a pair of
predictors exceeds 0.9, or exceeds R2. The multiple correlation coefficient be-
tween the explanatory variables is large, and some of the partial correlations
are high too, which helps in identifying problem variables. The value of the
overall F-statistic is large, but values of (some) ^-statistics for individual re-
gression coefficients are small. For j — 1, • • • , /c, let i?2 denote the coefficient of
determination of a regression of the explanatory variable Xj on the remaining
(A;— 1) explanatory variables. A large value of Rj also indicates multicollinearity.
Variance inflation factors formalize this notion.
Definition 8.3.1. Variance inflation factor.
For j = 1, • • • , /c, the quantity
VIF3 = 1/(1- R] )
denotes the variance inflation factor for Xj (the name is due to Marquardt).
In the ideal case when Xj is orthogonal to the other predictors, R* = 0, so
that VIFj = 1. As R2
j increases from zero, VIFj increases as well. For example,
if R2. = 0.8, VIFj = 5.0, while if i?2 = 0.99, VIFj = 100. It has been suggested
in the literature that any VIFj greater than 10 indicates multicollinearity. Al-
ternatively, we may compute the average of the variance inflation factors, i.e.,
VIF = Ylj-i VIFj / k. If VIF substantially exceeds 1, multicollinearity is in-
dicated.
The column-equilibrated matrix XE is useful in detecting multicollinearity
(Belsley, 1991). We find the singular value decomposition of X# (see Result
2.2.6), i.e., X# —
UDV', where U is N x p, D is p x p, V is p x p, and
U'U = V'V ~ VV'= Ip. Let D = diag(di, - - - , dp), where the nonnegative
dj’s are the singular values of X#. Now
X'EXE = VDU'UDV' = VD2V/
gives the spectral decomposition of the symmetric matrix X'EXE, so that D2 =
diag(ci, • • • , Cp), where Cj = d2 are the eigenvalues of XEXE, while V — {%}
is the corresponding orthogonal eigenvector matrix. We obtain the p condition
indices in terms of dj’s and dmax — maxi< j<Pdj.
The jth condition index is defined
Definition 8.3.2. Condition index.
by
Vj — dmax/dj ,
j — 1, “
,p*
A value of dj which is relatively close to zero will be associated with a
large condition index. If dj is exactly equal to zero, there is an exact linear
relationship among the columns of X. The quantity
C — dmax / dmin 5

8.3. ORTHOGONAL AND COLLINEAR PREDICTORS
309
where dmin = mini< j<pdj, is called the condition number. The condition num-
ber C always exceeds 1.
A large condition number (say, C > 15) indicates
evidence of strong multicollinearity, and empirical evidence suggests the need
for corrective action when C > 30. Since
Cov{P )/a2 = (X'£XE)-1 = VD-2V'
= diag(l/d2, l/d2, • • • ,1/dp), we see that
Var{fij )/ cT2 =Y1
where D
2
».1
= (E!.i«*)ES-, »,V<
-—
v
is a decomposition of the variance structure of /3j, the q*s being proportions
/
—
(adding to 1) of Var( /3j )/a2. Large values of
again indicate serious depen-
dencies.
Once multicollinearity is detected, an obvious remedy is to drop from the
model variables that are highly correlated with others. The disadvantage of this
is that if the dropped variable is potentially valuable in an understanding of the
response, we get absolutely no information about it, and moreover, it may not
always be clear as to how the omission of a variable will affect the estimates of the
remaining model parameters. Other statistical procedures for dealing with the
problem of multicollinearity include ridge regression and principal components
regression, which we discuss next.
Ridge regression
Hoerl (1962) first introduced the ridge trace procedure as a solution to certain
multicollinearity problems in regression (see also Hoerl and Kennard, 1970a, b).
8.3.3
Consider the centered and scaled multiple regression model
(see Example 4.1.2). Let X, = £f=1 Xy/JV, Sjh = £t=i ( Xl3- X j )( Xih- X h ),
j, h = 1, • * • , fc, and Syy =
Let Ai,i = 1, • • • , p be the eigenvalues
of X*'X*.
1. The ridge regression estimates of the coefficients /?*, j = 1, •
• , k are given
Result 8.3.2.
by
P*(d ) = (X*'X* + eik )-lX*' y
(8.3.6)
where 6 is a positive number, usually assuming values between 0 and 1.
Further, the corresponding least squares estimates of the coefficients in
the model (4.1.2) are
dm = PjWVs^
/VsTi,
k
dm =
Y -Yfimxy
j = 1, • • • , A;, and
j=i

CHAPTER 8. MULTIPLE REGRESSION MODELS
310
2. The expectation and covariance of P* {9 ) are
E{0* {6 ) } =
(X'X + 9I )~1 X' XP, and
(X'X + 0I)_
1X'X(X'X + 0I) ~V.
Cov{0* (9 ) }
3. The total mean square error of the ridge regression estimator is
E{(0*(0)- PY (P* (e)- P )} = a2£>(Ai + 0) ~2
3=1
+
02/3'(X'X + 0I)-2/3,
where\j > j = 1, • • • ,p are the eigenvalues of X'X.
The form of /3* = (X'X 4- 01) 1X'y follows from the method of
Proof.
Lagrangian multipliers (see section 4.6.1). The transformation to the estimates
in terms of the original coefficient vector /3 follows directly from the reasoning in
Example 4.1.2. Since /3*(0) is a linear function of y, property 2 follows directly.
To prove property 3, we see that by Result 2.3.4, X'X = QAQ' for orthogonal
Q, so that the total variance of /3*(0) is
a2£r[(X'X + 0I)~1X,X(X'X -1- 0I)"1]
a2tr[(QAQ' + 0I)-1(QAQ')(QAQ' + 0I)
"1]
cr2tr[(A + ^I)""1A(A 4- ^I)” 1]
O*YMX* + 0) ~2
>
tr [V ar(P* (0 ) )\
3=1
where A = diag(Ai, • • • , Ap). Since the total mean square error of (3*{9 ) is the
sum of the total variance and the square of the bias, property 3 follows. Note
that the total variance of the OLS estimator of /3 is a2Y^
j=i AJ
1. When X'X
is nearly singular, some of the A/s may be very small, and the total variance of
the OLS estimator of /3 is highly inflated in comparison to the total variance of
the ridge estimator.
When 0 = 0, the ridge regression estimates reduce to the usual least squares
estimates. A plot of /3|(0) versus 0 or of (3j(9 ) versus 0 for j = 1, • • • , fc, is
known as a ridge trace and is used to select a suitable value 8* of 0 using the
following suggestions by Hoerl and Kennard (1970a, p. 65):
1. The estimates stabilize at a value of 0, with the general characteristics of
an orthogonal system.
2. Estimated coefficients do not have unreasonable values.
3. Estimated values that had apparently incorrect signs at 0 = 0 have changed
to the proper signs.

8.3. ORTHOGONAL AND COLLINEAR PREDICTORS
311
4. The SSE does not have an unreasonable value.
Alternatively, there is suggestion in the literature for an automatic choice of
0* as the value
0* = kd2 //?*(0)7>(0)
where a2 is the MSE from the usual least squares fit and
£*(o) = (&(<)), •
• Jmy =
/-
V
The corresponding values (3* (6* ) are taken to be the final estimated coefficients
and can be used for prediction.
M
/
I
0\
)\ \
Figure 8.3.2.
Ridge regression.
We can characterize ridge regression as a restricted least squares problem.
Consider least squares in the centered and scaled multiple regression model
y* = X*/?* + e subject to the spherical restriction
/r'/r < d 2
for a given value d2. Using the method of Lagrangian multipliers (see section
4.6.1) to minimize
(8.3.7)
5* = (y* - X*/?*)'(y* - X*/?*) + 0(0*P ~ d2 ),
yields the equations
(X*'X* + 01)0*
=
X*V, and
/r'/r
d2

CHAPTER 8. MULTIPLE REGRESSION MODELS
312
whose solution leads to the ridge estimate of 0* in (8.3.6), and a solution for 9 in
terms of d. See Figure 8.3.2 for a geometric illustration for the case k = 2. While
0 lies at the center of the innermost elliptical contour, the restricted solution
lies on the innermost elliptical contour just touching the spherical restriction.
As we reduce the radius from where the circle would pass through 0 to d = 0,
we obtain the entire sequence of ridge solutions; the ridge is the path traced by
the solution point as the radius of the circle is increased from zero.
Note that use of an ellipsoidal restriction 0*' A0* < d2 leads to a solution of
the form
0*A{9 ) = (X*'X* + 0A)~lX*' y.
(8.3.8)
The choice of the restriction can be viewed as a model selection problem.
Numerical Example 8.3.
Multicollinearity.
scribes the manpower needs for operating a U.S. Navy bachelor officers’ quar-
ters, consisting of 25 establishments (Freund and Littell, 1991). The response
variable Y is the monthly man hours needed to operate an establishment. The
predictor variables are X\, the average daily occupancy, X 2, the monthly aver-
age number of check-ins, X3, the weekly hours of service desk operation, X4,
common use area (in sq. ft.), X5, number of building wings,
operational
berthing capacity, and X7, the number of rooms. Before we show diagnostics
for multicollinearity, we give the basic results from OLS estimation. The last
column in the table of least squares estimates shows the variance inflation factor
for each variable.
The following data de-
ANOVA table for Numerical Example 8.3.
Source
d.f.
SS
MS
F-value
Pr > F
Model
Error
Corrected
Total
7
87382503
12483215
60.17
< .0001
17
3526698
207453
24
90909201
Least squares estimates
t-value
Pr > \t
Parameter
d.f.
Estimate
VIF
s.e.
Intercept
1
148.221
221.627
0.67
0.513
XI
1
-1.287
0.806
-1.60
1.810
0.515
0.590
1.800
0.33
X4
1
-21.482
10.223
-2.10
5.619
14.756
0.38
X6
1
-14.515
4.226
-3.43
X7
1
29.360
6.370
0
0.129
2.166
3.51
0.003
4.500
0.747
1.406
0.051
2.353
0.708
3.653
0.003
37.185
4.61
0.0003
63.713
X2
1
X3
1
X5
1

8.3. ORTHOGONAL AND COLLINEAR PREDICTORS
313
Condition Index
No.
Eigenvalue
Condition Index
1
6.476
2
0.594
3
0.356
4
0.268
5
0.142
6
0.083
7
0.076
8
0.005
1.000
3.302
4.263
4.914
6.748
8.840
9.216
37.448
Proportion of Variation by predictors
X2
*3
*4
*5 *6
*7
No.
Int.
.003
.005
.002
.002
.004
.003
.0003
.106
.111
.021
.041
.011
.001
.001
.052
.231
.006
.024
.113
.123
.0001
.0002
.464
.132
.001
.047
.048
.006
.029
.010
.006
.002
.688
.434
< .0001
< .0001
.480
.0002
.187
.584
.001
.042
.025
.328
.0001
.556
.344
.028
.045
.034
.002
.179
.089
.001
.107
.305
.934
.0002
.0004
.0002
1
2
3
.001
4
5
.005
6
.006
7
8
.987
One can also compute intercept adjusted collinearity diagnostics; see the first
author’s website for more details.
Principal components regression
This is a more unified way to handle multicollinearity, but requires computa-
tions beyond standard regression computations. The procedure is based on the
observation that every linear regression model can be restated in terms of a set
of orthogonal predictor variables, which are constructed as linear combinations
of the original variables. The new orthogonal variables are called the principal
components (Johnson and Wichern, 1988) of the original variables. Principal
components regression is an approach that inspects the sample data (y, X) for
directions of variability and uses this information to reduce the dimensionality
of the estimation problem. Let X'X = QAQ' denote the spectral decomposi-
tion of X'X, where A = diag(Ai,
• • , Ap) is a diagonal matrix consisting of the
(real) eigenvalues of X'X, with Ai > • • • > Ap and Q = (qx , • • • , qp) denotes the
matrix whose columns are the orthogonal eigenvectors of X'X corresponding to
the ordered eigenvalues.
Consider the transformation
8.3.4
y — XQQ'/3-j- £ — Z6 + £,
where Z = XQ, and 0 = Q'(3. The elements of 9 are known as the regression
parameters of the principal components.
Using this spectral decomposition,

CHAPTER 8.
MULTIPLE REGRESSION MODELS
314
every regression model can be expressed in terms of orthogonal predictors, which
are linear combinations of X\, * • • , X/c. The matrix Z = (z1 ? • • • , zp) is called
the matrix of principal components of X'X, while zj = Xqj is the jth principal
component of X'X. Note that z'z j = Aj, the jth largest eigenvalue of X'X.
The principal components enable us to assess the presence of multicollinearity
in a regression problem, and provides an alternate estimation approach. Notice
however that the principal components lack a simple interpretation, each being
some linear combination of the original predictors.
Principal components regression consists of deleting one or more of the vari-
ables z j (which correspond to small values of Aj), and using OLS estimation on
the resulting reduced regression model. Suppose we partition Z = (Z1 ? Z2) and
the other matrices and vectors conformably, so that we can write the regression
model as
y = Zi0i -f-Z202+£.
Note that (3 = Q0 = Q^
i + QI#2- Assume that Zi corresponds to the trans-
formed predictor variables that will be retained in the model (they correspond
to larger values of Aj), while Z202 will be discarded from the model.
Let
0i = (Z'1Zi )
’"1Z'1y be the OLS estimator of 01 in the reduced model. Clearly,
E(6\) = 0i , and Var(01) =^(Z
^
Zi )
. The principal components estimator
of (3 is
/3P = Qi0i = Q(0' , O')'.
The principal components regression estimator (3p is biased, but in general has
smaller variance than (3.
8.4
Prediction intervals and calibration
We first discuss prediction intervals. Example 8.4.1 illustrates this for a simple
linear regression model.
Suppose Xo,i is a specified value of X in a simple regres-
Example 8.4.1.
sion model. The predicted mean response corresponding to this value of the
explanatory variable is
(8.4.1)
X i ).
,1 -
Let X'0 = (1, X0, i ).
Then we can write YQ as a linear combination of /3
(fio ,ft )':
A)
*0 = (l,*0,l )
~
= X'0(3.
PI

8.4. PREDICTION INTERVALS AND CALIBRATION
315
Therefore, To has a (singular) normal distribution with mean E(YQ ) = /?0 +
Pi X 0,1 and variance
Var(Y0 ) =
Var(Y ) + ( X0A - X^
Vari^
)
a2
(Xo.t - XQV2
N +
N
—
'
2=1
Alternately, we can compute this variance using
Var(Y0 )= Var( p0 ) + X$tlVar((h ) + 2X0 >1Cov(P0,Pi ).
The estimated standard deviation of To is then
1/2
(%) =* { I / N + ( x0 A - x x f /
-*i )2}
est.s.e.
which attains a minimum value when Xo,i coincides with Xj, and increases
as the distance between the two values increases. Intuitively, this tells us that
we expect inaccurate predictions for Xo,i values that are outside the observed
range of X values. We may now construct a 100(1— a)% prediction interval for
the mean value of the distribution of Y corresponding to a given Xo,i value:
Y0 ± tN _2,a/i{est.s.e.(Yo )}
where tN _2 >a/2 corresponds to the upper a/2th critical point from a ^-distribution
with N — 2 degrees of freedom. The actual (unobserved) value of Y varies about
this true mean value with variance a2j, the predicted value of an individual
response corresponding to XQJ is still YQ, with estimated variance
Var(Y0) = 52 (l + l /N + (X0,i - X 1 )2/ ££I(*U - Xx)2) .
est.
Then
1/2
Y0 ± tN.2,a/2?(l + l / N + ( X0,i -*i)2/
is the 100(1 — a)% prediction interval for an individual unknown response.
An extension of the marginal prediction intervals approach enables us to
predict the average of L unknown future observations at Xo,i, which we denote
by To (when L = 1, we get the case we discussed above). We can verify that
To has a normal distribution with mean /?o + /3iXo,i, and variance cr2/L, which
is independent of the distribution of To. Hence,
T0 - To- iV(0, (72/L + Var(To));
replacing a1 by its estimate a2, we derive the 100(1 — a)% confidence interval
for TQ as
u
X 1/2
-*o2)
.
-r.)2/ E£,(*
TQ ± t ]\j— 2,a/ 2 $
0,1 “
i,1

CHAPTER 8.
MULTIPLE REGRESSION MODELS
316
These limits are wider than those for the mean response of Y corresponding to
XQ i, which is to be expected.
This procedure is useful for constructing point predictions and correspond-
ing confidence intervals for unknown true mean responses or unknown individual
responses corresponding to a set of different X values, say, Xo,i, • • • , XO,L- The
confidence intervals for the responses Vo,!? *
* * , YO,L are called marginal inter-
vals; the jth interval contains YQJ with probability 1 — a. However, the joint
probability that all the intervals simultaneously contain Yoj , j = 1, • • • ,L is
usually less than 1 — a. We can alternatively construct simultaneous predic-
tion intervals, as well as simultaneous confidence curves for the whole regression
function over its entire range. The latter are confidence bands which involve a
critical value from the F-distribution.
We now show some results for the multiple regression mod-
Example 8.4.2.
els. Given a specified value of the predictors, viz., XQ = (l, Xi 5o, • • • ,X/c,o), the
predicted response is
YQ
XQ/?
fio + ftXi.0 4- •
• + fikXk'Q
(8.4.2)
with mean E(YQ ) —
fio 4- /3i*i,o 4-
b /3fc*fc,o and variance
k
k
k
Var{Y0 )
= ]T X 2
] fiVar{d3 ) +
A)
(8.4.3)
j=l 1=1
3=1
3^
1
=
cr2Xo(X,X)-1x0 .
Apart from
<r2, the prediction variance depends on the term XQ(X
/X)-1
XO,
/N
/N
which we may denote by poo- Clearly, if we let Y* denote x'/3, then Var(Yi ) = pa,
the ith diagonal element of P.
The property l /N < pa < 1 implies that
l /N < Var(Yi )/a2 < 1. Apart from
<r , the sum of the prediction variances
over the N data locations is equal to the number of regression parameters, i.e.,
YliLi Var{Yi )/a2 = tr{P) — k+l. This suggests the advantage of parsimonious
models from the point of view of reducing the overall prediction variance. The
100(1- a)% confidence interval for the true mean value of Y at x.'Q is given by
CT^
X'X)-1
(8.4.4)
Yo ± t N-k-l,a/2
On the other hand, if we wish to construct a prediction estimate of an individ-
ual Y response given XQ, the point estimate is still Yo, while the 100(1 — a )%
confidence interval is
x0.
d\J1 + XQ(X,X)
_ 1X0 .
(8.4.5)
Yo ± t p f - k- l ,a / 2

8A. PREDICTION INTERVALS AND CALIBRATION
317
Calibration is the problem of constructing confidence intervals for an un-
known Xo given YQ. Consider fitting a straight line regression model to the pairs
of observations ( Xi,Y*),i = 1, • • • , N , for which the fitted line is Y = /?o + P\X .
Our interest is in obtaining point and interval estimates of the predictor variable
Xo corresponding to an observed Yo. From the fitted least squares model, we
may write Yo = (3o + /?iXo, so that Xo = (Yo - A))//?i * This estimator Xo,
which is also the MLE of Xo, is in general, biased. We consider two situations
under which the confidence interval for Xo is determined. First, let Yo denote
the true mean value of the underlying response distribution. We solve for XL
and Xu as points of intersection of the line
Y = Yo = Po + AX0
and the curves (see Figure 8.4.1)
1/2
j?2
52(XL -Xf
YL - tN-2 ,a/ 2
~Tf +
}
V
Sxx
<72( XU
X)2
1/2
52
Y
Yu + tN _ 2 >a/ 2
iV +
5XX
where sxx — YliLi { Xi ~ X) , YL
(3o + /3IXL and Yb = /3o + /?iXf/. That is,
we draw a horizontal line parallel to the x-axis at a height Yo- From the point of
intersection of the fitted line and this horizontal line, we drop a perpendicular
to the rr-axis, which gives the inverse point estimate XQ.
Y
Fitted line
Lower
\ ^
\
Upper
\
\\
\
Y0
\\\
\
Xo ^
Figure 8.4.1.
Calibration.
From the points where this line cuts the confidence interval curves, we drop
perpendiculars onto the rc-axis to give the lower and upper 100(1 — a)% inverse

CHAPTER 8. MULTIPLE REGRESSION MODELS
318
confidence limits XL and Xu - Williams (1959) referred to these as “fiducial
limits” . Algebraically, we set
A> + 01Xo = 00 +0\X * ± t {02/ N + a2( XL - X )2/ sxx }
where X * represents XL or Xu and we denote
2,a/ 2 simply by t. This
simplifies to a quadratic expression in X *:
{0l - t2a2sxx) X*2 + 2 ( Xt2a2/ sxx - XQ0\) X * +
(0?X§ - t2a2/ N - X 2t2d2 / sxx ).
Setting this equal to zero and solving, we obtain
1/2
1/2
0i (Yo — Y ) —
fd{(y0 -Y )2/sxx + 02/ N - t2a2/Nsxx}
X +
XL
PI - t2a2/ s X x
1/2
0i (Yo-Y )+ td{(Fo-Y )2/ sxx + 02/ N
/Nsxx|
- t2d2
Xv
x +
02 - t2a*/ sxx
In general, inverse estimation is not very informative unless the regression
of Y on X is significant, i.e., P\ is significantly different from zero. In such
cases, it might happen that (a) the two roots of the quadratic equation in X *
are complex, or (b) the roots XL and Xu are real-valued, but the interval
( XL, Xu ) does not contain XQ because the endpoints lie on the same side of the
regression line.
We next assume that YQ denotes an individual value from the response dis-
tribution. In this case,
^we solve for XL and Xu as points of intersection of the
line Y = YQ ~ P0 + PIXQ and the curves
1/2
y ,
,
2
a2
a2( XL - X )2
—
tJV-2,a/2
& + — +
Y
S X X
a2(xv - x )2
1/2
a2
Yu + tN — 2>oc/ 2 ' O’2 + -jy +
Y
S X X
Setting
02( XO - X * )2/t2a2 = (1 + l / N + ( X * - X )2/sxx )
we solve the resulting quadratic equation in X * for XL and Xu'-
t2d2
Xt2u2 - XoPlj X* +
x
2t 2d2
X*2 + 2
sXX
s X X
t2a2
nn - 1202
= 0.
N
sXX

8.5. REGRESSION DIAGNOSTICS
319
Numerical Example 8.4. Inverse simple linear regression.
consists of observations taken at intervals from a steam plant which is part of
a large industry. The data is a portion of a larger data set used to fit a mul-
tiple regression model (Draper and Smith, 1998). The response and predictor
variables are respectively Y , the monthly use of steam (pounds), and X , the
average atmospheric pressure (degrees F ). Given a true mean response value of
Yo — 10, we construct a point estimate and an interval estimate of the predictor
Xo corresponding to YQ. The least squares estimates and ANOVA table for the
regression of Y on X are shown below.
The data
Least squares estimates for Numerical Example 8.4.
t-value
Pr > \t
d.f.
Variable
Estimate
s.e.
Intercept
1
13.623
0.581
23.43
< .0001
1
-0.080
0.011 -7.59
< .0001
X
ANOVA table
Source
d.f.
SS
MS
F-value
Pr > F
Model
1
45.592
45.592
57.54
< .0001
Error
23
18.223
0.792
Corrected
Total
24
63.816
Also, a = 0.89, R2 = 0.714, X- 52.6,
= 7063.4, and t22, .025 = 2.069. When
Yo — 10 is the true mean response, we compute Xo — 45.4, and also solve for
( X L, X U ) = (40.576, 48.881). If on the other hand, lo is an individual value,
the value of XQ remains the same, while ( X i, X y ) == (20.356,69.102).
Regression diagnostics
8.5
In this section, we continue to explore aspects of regression model fitting. In
particular, we look more closely at properties of the residuals and the pro-
jection matrix that enable us to diagnose abnormal features in the regression
model. Diagnostic measures are statistical measures used for detecting prob-
lems with regression models and/or observations or variables in the data set,
including problems of model misspecification, outliers, influential observations,
and collinearity. Belsley, Kuh and Welsch (1980) give an extensive description
of these topics, while a more theoretical discussion is found in Chatterjee and
Hadi (1988).

CHAPTER 8.
MULTIPLE REGRESSION MODELS
320
Further properties of the projection matrix
As seen earlier, the symmetric and idempotent hat matrix or projection ma-
trix P = { p i j } = X ( X f X ) ~l X' plays an important role in regression analy-
sis.
We write pa = x'(X/X)"1Xj, i = 1, • • • , iV, and p^ = x'(X'X)-1
i , j = 1, • • • , N . The matrix (I — P) is also symmetric and idempotent and is
often called the residuals matrix. We have seen in section 4.2 that PX = X,
(I - P)X = 0, Y = PY, and e = (I - P)Y = (I - P)e. That is, e, = e< -
N
J2 Pij£ j > and the relation between the zth error and the ith residual only de-
pends on P. If pij's are sufficiently small, then £ is a reasonable substitute for
e . Further, Var(Yi ) = a2 pu , Var(£i ) = a2( 1 - pa ), and Cov{£i , £ j ) = -cr2Pij ,
i, j = 1, • • • , N . The ith predicted response can be written as
8.5.1
X,J >
3=1
N
Yi — ^ ^
PijYj — PiiYi +
'y ] pijYj , i
(8.5.1)
=!, - • • , N,
i=l
from which we see that
dYi / dYi = pa , i = l, -
- , N
and
dYi / dYj = P i j , i j =!, • • • , N.
(8.5.2)
Suppose X = (Xi
X2), where Xi is an N x q matrix of rank q, and X2
is an N x ( p - <7) matrix of rank (p - q). Let Pj = Xj(X'Xj)-1X' denote
the projection matrix for Xj, j = 1, 2. Let X£ denote the projection of X2
onto C±(X1), i.e., X| = (I - Pi )X2 .
Let P£ = X5(X5
/X5)"1X5' = (I -
Pi )X2(X^
(I - P 1 )X 2 )- 1 X'2 { I - PI). The projection of y onto C(X) can be
partitioned into the sum of two projections, i.e., P = Pi -F P£ (see Example
2.1.2).
Result 8.5.1.
Let P = { p i j } denote the N x N projection matrix. Then,
N
N
1. EE4
i= lj= l
2. 0 < pa < 1, i = 1, • • • , N .
3. -0.5 < p^ < 0.5, i , j = l , - - - , 7V, i^
j .
4. In the regression model with intercept, pa > I / N , i = 1, • • • , N .
Proof.
Since P is symmetric and idempotent, YliLi J2^
=\ Pij = tr( P 2 ) =
r(P) = p, proving property 1 .
Also, since pu = $2 j=iPij = Pu +
P%,
property 2 follows directly. To prove property 3, we write pu = p\4- pfj +
T,ijH , jPu > so that Pij
Pa( l
i e-, -0.5 <
< 0.5, using property 2.
We next prove property 4. Let X = (l /v , X2). Here Pi = l^v (l'Arl^)
” 1l'N =
1N1'N/Y, Xg = (I - Pi )X2 = (I - 1NI'N / N )X 2 = (I - JN)X2, and P
^ =
X3(X5,X5)-1X5
/. Then, P = Pj + P* (see Example 2.1.2); (P^ = 1/7V,
= p.

8.5. REGRESSION DIAGNOSTICS
321
i — 1, • • • , 7V, and (?$)« > 0, i = 1, • • • , TV (since P£ is a projection matrix),
from which it follows that pa > 1/TV, i = 1, •
• , TV.
The relationship between the elements of P and the residuals e* is stated
below and will be useful for a later discussion on the relative usefulness of
different transformed residuals for diagnostic purposes.
Notice that if pa is
large (i.e., near 1), or small (i.e., near 0), then pij , j ^
i will be small. Also,
pa + e
^
/e'e < 1, where e*e = J2iLi £? > so
observations with larger pa will
tend to have smaller residuals ?*, and the usual residual plots will not identify
these observations as anomalies.
8.5.2
Types of residuals
In Chapter 4, we defined ordinary least squares residuals £*, i = 1, * • * , TV.
We have seen that the vector e = (?!, • • • , £ jv) has mean 0, while E(ee* ) =
a2( I — P). We now introduce four transformations of the ordinary residuals, viz.,
normalized residuals, standardized residuals, internally Studentized residuals
and externally Studentized residuals. In general, the 2th transformed residual
is defined by
£*i = ei/ai, i = 1,- • • ,N,
where
<7* is the standard deviation of the 2th residual £i (the square-root of the
Tth diagonal element of <r2(I — P)). By using different estimates for the unknown
parameter <T* in (8.5.3), we obtain the four definitions shown below.
The Tth normalized residual is defined by
(8.5.3)
Definition 8.5.1.
(8.5.4)
The 2th standardized residual is defined by
0>i =
Definition 8.5.2.
(8.5.5)
i =!, - • • , 7V,
a
where a — {e,e/ {N — p)}1//2.
The 2th internally Studentized residual is
Definition 8.5.3.
(8.5.6)
i = I, *
* ,N.
n = 5(1 - Pii)1/2 ’
*
Definition 8.5.4.
The 2th externally Studentized residual is
£i
(8.5.7)
* =
i =!, - • • ,N,
5(*)(1 - Pii )1/2 ’
2
where

CHAPTER 8. MULTIPLE REGRESSION MODELS
322
SI ) = SSE{i )/ ( N - p-1) = y'(i)(I- P(i))y {i )/ ( N - p-1),
* = 1, • • • , N
is the residual mean square estimate when the rth observation is omitted from
the fit, and
P(i) = x(t)[X( j)X(i)] 1XJi), i =
is the projection matrix for X^
, the matrix of explanatory variables when the
ith observation is omitted.
Result 8.5.2.
The following relationships between the residuals hold:
1. k = d i ( N - p)1/2.
2. n = bi/ {1- Pu )1/ 2 = di( N - p)1/ 2/ { 1- Pu )1/ 2.
3. r* = cii ( N - p- l)1/2/(l- pa- a2 )1/ 2 = n( N - p- l)1/2/(7V -p- r2)1/2.
Proof.
initions. To prove property 3, observe that omission of the zth observation is
equivalent to fitting the mean-shift outlier model
The proof of the first two relations follows directly from their def-
E(Y ) = X/3 + Ui0 = (X, u*)(/?',0 )
f
where u* is an iV-dimensional vector with 1 in the ith position, and zero else-
where, and 6 is the corresponding regression coefficient.
Using results from
section 2.1, we can show that
SSE^
) =
y(i) (i - P(i) )y«
=
y'[(i- P) - (I - P)UiU'(I- P) ]y
u'(i- P)u*
g?
SSE - 1- pa
so that, dividing throughout by ( N — p — 1), and using rf
( N - p)
2
( N - p- 1 )
( N - p- r f )
2
( N - p- 1 )
'
Substituting this result into the definition of r*, we get
e?/ S2( l - pu ),
e2
s?.(i )
( N - p- 1)(1- pa )
X-
s.
£i
r*
%) (1- Pit )1/2
ei( N - p- l )1'
2
S( l - PiiyG( N - p- r2 )' N
from which property 3 follows.

8.5. REGRESSION DIAGNOSTICS
323
Thus, r* is a monotonic transformation of r*, which itself is a monotonic
transformation of a*.
As r? — > ( N - p), r*2 —
oo, and therefore the latter
reflects large deviations more dramatically. Both being constant multiples of
Si, the normalized residuals a* and the standardized residuals 6* are equivalent
as diagnostic measures. A disadvantage may be that neither residual takes into
account Var{ei), i.e., the diagonal elements of the projection matrix. Although
this might not be critically important in some situations, it would be prefer-
able to use the r* when the pu vary substantially. We state two distributional
properties (for proof, see Chatterjee and Hadi, 1988, section 4.2). Result 8.5.3,
which is due to Ellenberg (1973), implies that \ri\ cannot exceed ( N — p)1/2,
while Result 8.5.4, which was proved by Beckman and Trussel (1974), shows
that the square of the externally Studentized residual follows an F-distribution.
If r(X( j) ) = p, then r2 / (TV — p), i = 1, • • • , N are identically
distributed as Beta(1/2, ( N — p — l)/2) variables, and the covariance between
n and rj is Cov(ri,rj ) = -(1- p«)“ 1/2(l- Pjj )~l / 2Pij-
Result 8.5.3.
Provided r(X(*) ) = p, r*
Result 8.5.4.
tributed as Student t-variables with ( N — p
i = 1, • • * , N are identically dis-
— 1) degrees of freedom.
The externally Studentized residuals r* are usually preferred since
is
robust to gross errors in the Tth observation and the r* itself has a ^-distribution
for which critical values are easily available. Note that the Studentized residuals
stabilize the variance in the ordinary residuals £*, but leave the correlation
pattern unchanged. We now define other residuals that also play a useful role
in regression analysis. Let X = (Xi, x*).
Definition 8.5.5.
predictor Xj is defined by
The vector of partial residuals corresponding to the
£ j = y- Xi/?i = e 4- XjPj.
These partial residuals are residuals that have not been adjusted for the predic-
tor variable Xj. In section 8.1.1, we saw that the partial residual plot or residual
plus component plot of 6 j versus Xj has estimated slope /3j.
The ith predicted residual is defined by
^i(i ) =
, i = 1, • • • , N
where /?(*) denotes the least squares estimate of (3 with the zth case excluded.
Unlike the ordinary and Studentized residuals, the Tth predicted residual
vector £(i) is based on a fit to the data with the ith case excluded. We may
think of Sj(i) as a prediction error, since we exclude the zth case while obtaining
Definition 8.5.6.

CHAPTER 8. MULTIPLE REGRESSION MODELS
324
the fit. From Result 8.2.2, it is clear that if the errors are normally distributed,
will have the same correlation structure as the
with zero means and
variances equal to <J2/(1 — pa ). It is preferable to use Studentized versions of
£*(i), which gives back the internally and externally Studentized residuals r* and
r*, as the reader may verify.
We define a vector of BLUS (best linear unbiased and
Definition 8.5.7.
possessing a scalar covariance matrix) residuals to be a vector of uncorrelated
residuals u = (Sj, • • • ,UN ) which is a linear function of y, say By, such that
E(u) — 0, and E(uu') = a21.
In order to choose a matrix B that defines the BLUS residuals u, we look at
properties of the projection matrix P. Recall that I — P has N — p eigenvalues
equal to unity while the remaining p eigenvalues are zero. Let B be an ( N — p) xp
matrix containing the eigenvectors of I — P corresponding to the nonzero eigen-
values. Using properties of the linear regression model and the spectral decom-
position of symmetric matrices it can be shown that BX = 0, BB' = I, B'B =
I — P, u = BY = 3e = Be, and E(uu')
the order of u is at most N — p, and further, u is not unique. This is a serious
disadvantage in terms of using Ui as case statistics for regression diagnostics,
since identification of these N — p residuals based on the N cases is messy and
lacks interpretation. Also, the independence of the BLUS residuals holds only
if the regression errors are normally distributed and are homoscedastic. Hence,
u is not very useful for testing the normality assumption, although it has been
used to construct a test for serial correlation (see Theil, 1965) and a test for
heteroscedasticity (Heyadat et al., 1977).
Recursive residuals (Brown, Durbin, and Evans, 1975) are another example
of BLUS residuals, but lacking the minimum variance property of the BLUS
residuals. Let
be a j xp matrix consisting of the first j rows of the matrix
X, where j > p, and let x'+1 be the ( j 4- l)th row of X. Let Xj+i = (X', x'+1)',
yj = (Yi, •
• ,YjY , andWj = (X'Xj)-1, while /3j — WjX'yj denotes the least
squares estimate of (3 based on the first j observations.
2B(I — P)B
/ = (T2I/v_p. Note that
= a
The ( N — p)-dimensional vector of recursive residuals is
Definition 8.5.8.
defined by
Y j -t y i-1
£ R
,
j = p+
, N
(1+ x'Wj-iXj )' /*
eR
0,
j =
, p.
The following relationships are useful for the computation of
Result 8.5.5.

8.5. REGRESSION DIAGNOSTICS
325
recursive residuals:
Wj-i X j (Yj - x'/jj-Q
0j-i +
Pi
, and
1 + X j W j-i X j
WJ-XXJ
- X
^
WJ-X
l + x'-Wj-iXj
W*
w,J-l "
For the normal linear regression model with homoscedastic errors, ef , j > p are
independent iV(0,a2 ) variables. Heyadat and Robson (1970) and Harvey and
Phillips (1974) constructed tests for heteroscedasticity using recursive residuals,
while Phillips and Harvey (1974) used them in order to develop a test for serial
correlation.
Outliers and high leverage observations
Violations in the regression model assumptions can occur in a variety of ways.
Frequently, the data may contain outliers, i.e., anomalous observations that
do not reasonably fit the assumed model. In general, an observation may be
influential in the regression fit either because it is an outlying response, or it
is a high-leverage point, or both. In the framework of linear regression, we
classify the ith observation as an outlier if the magnitude of r* or r* is large by
comparison with the rest of the observations in the data set. The presence of
outliers may seriously bias parameter estimation and inference. The traditional
tool for detecting outliers is an analysis of the residuals ?*, i = 1, * • • , N. There
are two approaches for outlier detection, viz., use of formal test procedures, and
informal graphical displays. We describe both approaches.
From (8.5.1) and (8.5.2), it follows that we may interpret pij as the amount of
leverage each Y3 has on determining Y$, irrespective of its actual value (Hoaglin
and Welsch, 1978). The leverage of the zth case is pa, which is the zth diagonal
element of the projection matrix P, while the reciprocal l / pu is the effective
/N
or equivalent number of observations that determine Yt (Huber, 1981). When
pa = 1/2, an equivalent of two observations determine the fitted value Y*; when
pa = 1, Yi solely determines Y*; while if pa — 0, Y{ has no influence on Y*.
Huber suggested that if pa > 0.2, then the ith case is a high-leverage point.
Since £T=i Pa —
Hoaglin and Welsch suggested that cases with pa > 2p/N
may be classified as high-leverage points. A wide variation in the values of pa
indicate nonhomogeneous spacing of the rows of P.
Geometrically, suppose X contains a constant column 1#, or suppose that
the columns of X are centered at their respective averages. For a p-dimensional
vector u, and a constant c, the quadratic form u'(X/X)“ 1u = c determines
p-dimensional elliptical contours centered at x. The smallest convex set which
contains the scatter of N sample values of X lies within ellipsoids of radius c,
where c < max( pu ). The implication is that a large value of pa indicates that
Xi is far removed from the center x, i.e., it is an outlier in the X space. We
define the following quantities that enable us to measure the distance of x* from
8.5.3

CHAPTER 8. MULTIPLE REGRESSION MODELS
326
the rest of the cases. Assume that we have a regression model with intercept
and recall that X = (l /v, X).
The Mahalanobis distance is defined by (see section 6.3)
X(i))'{X'(i)(I- ( N - l)-1ll,)X(i)}-1(xj- X(4)),
(8.5.8)
Definition 8.5.9.
M i = ( N — 2)(xi
for i = 1,
• • , N , where, X(^
is the average of X^
). It is easily shown that
M . = N ( N - 2 ){pu - l / N ) / ( N -1)(1- Pii ) , i = 1, •
• , N .
The weighted squared standardized distance (WSSD) is
defined as the weighted sum of squared distances of X x j from the mean of X j,
the weights being 0j (Daniel and Wood, 1971):
Definition 8.5.10.
*-
ci? / SY
(8.5.9)
Y ,
i —
• i N ,
Wi
j=l
where cXj = 0j{ Xxj — X j ), i = 1, • • * , TV, j = 1, • • •
denotes the effect of X j
on Vi, and J2 j=i°ij = Yi — Y . If Xij is far removed from X, or if 0j is large,
or both, then W* will be large, and the zth case is influential on the distance
/N
between Yx and Y .
An L-R plot combines information about leverages and residuals into a sin-
gle graphical display, and enables us to distinguish between high-leverage points
and outliers. It is a scatterplot of leverages pa versus the squared normalized
residuals of . The scatter of points must lie within the triangle defined by these
conditions: (i) 0 < pa < 1, (ii) 0 < af < 1, and (iii) pu 4- of < 1. Points that
lie in the lower right corner of the L — R plot are outliers, while points that lie
in the upper left corner have high leverage. Neither outliers nor high-leverage
points are necessarily influential. We discuss measures based on the influence
function in the next section.
Diagnostic measures based on influence functions
The study of influence is the study of the dependence of conclusions and in-
ferences on various aspects of a statistical problem formulation. This is im-
plemented via a perturbation scheme in which data are modified by deletion of
cases, either singly, or in groups (Hampel, 1974). Suppose ( z i , • • • , z j q ) denotes a
large random sample from a population with cdf F. Let FN = FN ( ZU • • • , ZN )
denote the empirical cdf and let T/v = T ( z\, - - ' , Z N ) be a (scalar or vector-
valued) statistic of interest. The study of influence consists of assessing the
change in T/v when some specific aspect of the problem is slightly changed. The
first step is to find a statistical functional T which maps (a subset of ) the set
of all cdf’s onto 7£p, so that T( F^ ) = T/v. We assume that F/v converges to F
8.5.4

8.5. REGRESSION DIAGNOSTICS
327
and that TJv converges to T. For example, when
= Z , the corresponding
functional is T ( F ) = J zdF( z), and T ( Fj^ ) = f zdFN = Z. The influence of an
estimator T/v is said to be unbounded if it is sensitive to extreme observations,
in which case, T/v is said to be nonrobust. To assess this, one more observation
z is added to the large sample, and we monitor the change in T/v, and the con-
clusions based on T/v. In this section, we present results describing the influence
of the ith case on the least squares regression estimates, /3,
<?2, as well as on
Cov((3), y, and Cov(y). We use measures based on the theoretical influence
function (Hampel, 1974 and Huber, 1981).
Definition 8.5.11.
sample (zi, • • • , z/v) drawn from a population with cdf F, and let T denote a
functional of interest. The influence function is defined by
Let z denote one observation that is added to the large
1>( Z, F,T ) = limV{(l - e )F + eSz } - T { F}],
0£
provided the limit exists for every z
7£, and where 6Z = 1 at z and zero
otherwise. The influence curve is the ordinary right-hand derivative, evaluated
at e = 0, of the function T[(l — e )F + e6z] with respect to e.
(8.5.10)
The influence curve is useful for studying asymptotic properties of an esti-
mator as well as for comparing estimators. For example, if T = \i = f zdF,
then
tp[z, F,T] = lim{[(1- e )ii + ez) - n}/e = z - //,
t—
»0
which is “ unbounded” , so that
= Z is nonrobust. To use the influence func-
tion in the regression context, we must first construct appropriate functionals
corresponding to /3 and a2.
The functionals for 0 and a2 are
E-x
1(F)Exy (F),
&Y Y ( F ) — £xr(F)£
“
x (F)Exy (F)
CTYY ( F )- E'xy(F)/J(F),
Result 8.5.7.
0( F )
<72(F)
(8.5.11)
where (x',Y ) has cdf F ) and
£XX(F)
£xy (F) \
KY ( F )
V Y Y ( F ) y
F[(x', y)'(x', K)] =
Recall that the least squares regression estimates /3 and a2 are
Proof.
obtained by simultaneously solving the equations
1
N
-]Txt(y - x'/j) = o
2=1
~<0?= 52.
P i=1
1
(8.5.12)
N -

CHAPTER 8. MULTIPLE REGRESSION MODELS
328
Corresponding to (8.5.12), we can write
/ x(y - x'(3)dFN ( x' ,Y )= 0
(x',K)
IiY - x'(3)2dFN ( x' ,Y ) = <x2,
(8.5.13)
(x',y)
the solution of which gives the required functionals for (3 and a2.
Result 8.5.8. The influence curves for (3 and a2 are respectively
E-x
1(F)x[j/-x'/J(F)],
[Y - x'/3(F)]2- OYY { F) + KY ( F )0( F ). (8.5.15)
ICp,F {xf ,Y )
ICa2tF { x!,Y )
(8.5.14)
Substitute (x',F) for z, and 13( F ) from (8.5.11) for the functional T
Proof.
into (8.5.10) to get
1
</;{(x', F),F, /3(F)} = lim-[/?{(!- e)F+ e<5(x,,y)}- (3{ F }\.
(8.5.16)
Using property 8 of Result 1.2.10, we can write
^
[/?{(!— e )FYed^^
y ) } —
f3{ F }]
as
1:K«{(1- e )F + e<5(x,,y)}ExK{(l - e )F + eS{ x,,Y ) }- E^
(F)Sxy(F)]
|
[{(1- ^)SXX(F) + £XX'}-1{(1-s)ExV (F) + exV)- Elxx(F)Exy (F)]
1[{EXX(F) + e(xx'- Exx(F))}-1{Exy (F) + £(xE- Exy(F))}
-Exx
1(F)Exy (F)]
±[{I + eZxi(F)(xx'
-Exy(F))}- Exi(F)Exy(F)]
“ [{!-^xi(F)(xx'- EXX(F)) + o(e2)}{/3(F) + £E^
(F)(xF
-Exy(F))}- Exx1(F)Exy (F)].
Exx(F))}-1Exx
1(F){Exy(F) + e(xy
0, this expression yields ICpfF(x', Y). Since (F - x'/3(F))
is unbounded, ICptF(x', Y) is unbounded, i.e., the least squares estimate /3 is a
nonrobust estimator.
Similarly, substituting the form of the functional cr2(F) from (8.5.11) into
(8.5.10), we see that cr2{ ( l - e )F + eS^
y)} — a2{F} =
<ryy (F) + eY 2 —
eaYy ( F ) - EfxY ( F )(3( F ) - e(3' ( F )x(Y - x'/3(F)) - e(Yx' - Exy (F))/3(F) -
e2(Yx' - (3
f ( F )x(Y - x'(3( F ) ) - aYY ( F ) + E^
(F)/?(F) — F2 - aYY ( F ) -
2Yxf (3( F ) + (x'/?(F))2 + E^
y (F)/3(F), which after some simplification yields
In the limit as E

8.5. REGRESSION DIAGNOSTICS
329
ICa2 yF( x.' ,Y ). Once again, /C'cr 2
>F(x/, Y) is unbounded, so that a2 is not a
robust estimate of a2.
The theoretical functions are intended to measure the influence on /3 and
d2 due to adding one observation (x', Y) to a very large sample. We do not,
however, always have very large samples in practice, and therefore need finite
sample approximations of these influence functions. Four approximations to
JCkF(x', Y) are shown below. For more details, see Cook and Weisberg (1982),
or Chatterjee and Hadi (1988).
Result 8.5.9.
/C/3iF(x',Y ) are given below.
1. The empirical influence function (EIC) based on N observations is
ElCi = NiX' Xy' xiSi
Four approximations to the theoretical influence function
(8.5.17)
i =!, - • • , N.
2. The empirical influence function based on ( N -1) observations, ( EIC^
) )
has the form
£i
EIC{1) = ( N - l)(X,X)_
1Xf
(8.5.18)
i =!, - • • , N.
‘(1 ~ P r i )2
3. The sample influence function {SIC ) based on N observations is
£i
5/Ci = (Ar- l)(X'X)-1xi-
(8.5.19)
* =!, • • • , N.
- Pii
4. The sensitivity curve (SCi) based on N observations is
SCt =
Z
(8.5.20)
{ =!, • • • , JV.
The function ElCi is obtained by setting (x', Y) = (x', Y*), F =
= 0 and E ~± { F ) = N(X'X)-1 in (8.5.14). The function EIC{i ) is
Proof.
FN , P( F )
obtained by setting (x', Y) = (x
^
, Y*), F = FN^
, /3( F ) = /3(*) and E ~* { F ) =
( N - l)(X^
X(i))_1 in (8.5.14), which gives, for i = 1, • • • , iV,
EIC{ 1) = { N - lflX^
X^
-^iY - x'%>).
Using (2.1.22) and (8.2.8), property 2 follows. The sample influence curve SICi
is obtained by setting (x', Y) = (x', Y*), F = F/v, and e = -1/ { N - 1) in
(8.5.16), omitting the limit and simplifying. The proof is left as an exercise
(Exercise 8.10). To obtain 5(7*, set (x',Y ) = (x -, Y*), F = F/v(i), and £ = 1/N
in (8.5.16), omit the limit and simplify.
The main difference between these approximate influence functions is in the
power of (1 — pa ). We see that ElCi is least sensitive to high leverage points,

CHAPTER 8. MULTIPLE REGRESSION MODELS
330
while EIC^
) is the most sensitive. Note that SICl and SCl are equivalent and
are proportional to the distance (0 — /?(*)). Note that each of these approximate
influence curves for /3 is a p-dimensional vector, which is unwieldy. In practice,
it would be useful to obtain an ordering of the N observations based on a scalar
summary measure of influence. The quantity
Di{M,c) = [V/{X', F2, F, /3(F)}MV;{X',^
, F, /3(F)}]/C
for appropriate choices of M and c is useful. If
c) is large, then the zth
observation has strong influence on 0 relative to M and c. Four different choices
of M and c leads to the following measures that are popular regression diagnos-
tics measures - (a) Cook’s distance (Ci ), (b) Modified Cook’s distance (MC*),
(c) DFFITS or Welsch-Kuh’s distance (WKi ), and (d) Welsch’s distance (Wi ).
The essential difference between these is in the choice of scale. Further, Ci only
measures the influence of the zth observation on 0, whereas the other three
statistics measure the influence on both 0 and a2.
Cook’s distance is defined by (Cook and Weisberg, 1982
Definition 8.5.12.
and Atkinson, 1985) as
•"v~9
pa
Di { X' X
?)
Ci
' ( N - 1)
0 - 0wnX' X )@ - 0(ii )
..
»
2
(8.5.21)
=!, • • • , N.
paz
The 100(1 — a )% joint ellipsoidal confidence region for 0 given in (7.3.2)
is centered at 0. The quantity Ci measures the change in the center of this
ellipsoid when the zth observation is omitted, and thereby assesses its influence.
Ci may be interpreted as the scaled distance between 0 and 0^
, or alternately,
as the scaled distance between y and y^
). The following result summarizes
alternate forms for Cook’s distance.
Result 8.5.10.
For i = 1, • • • , 7V,
{9 - 9(i )Y ( y - 9d ) )
Pa
xi(X,X)~ 1Xt
p{l - pa )
52(1- pa )
p (l - pii)
(8.5.22)
Ci
1
Pit
„2
r i -
(8.5.23)
Ci
By partitioning X = (X^
x*) and y = (y^
),!*), we can write
/3 = (X,X)_1X(i)y(i) 4- (X'X)
Proof.
_1x'Y).
(8.5.24)

8.5. REGRESSION DIAGNOSTICS
331
Using the form for S( i ) in (8.2.5) and simplifying, it follows that
0 ~ P{i ) = (X'X)-1Xj
(8.5.25)
l - P a
Substituting (8.5.25) into (8.5.21), we prove (8.5.23).
It is clear from the previous result that we certainly need not run ( N +1) re-
gressions (one with all N observations, and N regressions successively omitting
observations one at a time). The quantity Ci will be large if pa is large, or if r2
is large, or both. Although it has been suggested that each Ci be compared to
percentiles of the Fp>^-p distribution to see if it is large, we can also use a Box-
plot or stem-and-leaf plot or index plot of Ci to answer the question “ how large
is large?” An index plot is a plot of Cook’s distance against observation number.
Points that are above some threshold value such as the 50th percentile of an
Fp,N - p distribution are regarded as influential observations. Several modifica-
tions of Cook’s distance are available for normal regression models (Chatterjee
and Hadi, 1988, section 4.2, and section 5.4), while Pregibon (1981) used a one-
step approximation to extend this statistic to binary response models as well
(see section 11.4). Atkinson (1981) suggested a modification of Ci in order to
(a) give more emphasis to extreme points, and (b) be more suitable for graphi-
cal displays such as the normal probability plots. The modification consists of
replacing a2 by 5^
, taking the square root of Ci, and adjusting Ci for sample
size.
Definition 8.5.13.
We define the modified Cook’s distance by
1/2
Pa
{ N - p)
.(1- Pii)
V
.
In the case where pa = p/ N , i = 1, •
• , JV, the plot of MCi versus i is identical
to the plot of |r*|.
MCi =|r*
(8.5.26)
Figures 8.5.1 (a) and (b) give graphical interpretations of Cook’s distance and
the modified Cook’s distance for a 2-dimensional vector (3. In (a), the ellipsoid
is centered at /?. Cases i and l are equally influential on /3 in (a), while case
m is more influential since /3(m) lies on an outer contour corresponding to a
larger value of Cook’s distance. The modified Cook’s distance in (b) measures
the distance from (3^
to (3 relative to the ellipsoids constructed using all the
observations, but with a scale d2
^
specific to the ith. case. Since MCi does not
compare cases relative to a fixed metric, the ellipsoids in (b) can have different
shapes. Two other measures are related to the ellipsoidal confidence region for
/3 and are defined next.
Definition 8.5.14.
and Pregibon, 1978)
The Andrews-Pregibon statistic is defined as (Andrews
SSE(i )|X'WX(0|
.
SSE\X' X\
’
1
(8.5.27)
APi =
=!, • • • , N,

CHAPTER 8. MULTIPLE REGRESSION MODELS
332
which measures the influence of the ith observation on the volume of the confi-
dence ellipsoids for /?, with and without the ith observation.
X
(a)
(C)
(d)
/3.
fl.
Graphical interpretation of distance measures.
Figure 8.5.1.
We leave it to the reader to verify the relationships between AP{ and the
values of pu and r* (Exercise 8.15). In particular, since 1 — AP{ = (l — p^){l —
rf / ( N - p) }, APi combines information about high-leverage and outliers, and is
therefore potentially less informative than pa and r* (Draper and John, 1981).
The Cook-Weisberg statistic (Cook and Weisberg, 1980)
Definition 8.5.15.
is defined by
1/2 _
p/ 2
|x'(i)x(i)|
Fp,N- p,a
(8.5.28)
CWt = log
|X'X|
Ep,N — p— 1,0:
(0
The Welsch-Kuh distance or DFFITSi measures the
Definition 8.5.16.
influence of the ith observation on
by the change in the prediction at x* by
omitting the ith observation relative to s.e.(Yi) as
- /%))!
DFFITSi =WKi
CT(i)\/Vii
I[ei/(l - Pii )]x'(X'X)-1xi|
°{i )\fPii
Pii
(8.5.29)
\r*
r i
1- Pit

8.5. REGRESSION DIAGNOSTICS
333
Weslch’s distance is defined by
Definition 8.5.17.
I 1/2
Pa
( N - l)r*2
Wt
(8.5.30)
(1- Pu )2.
N -1
(8.5.31)
WKi
1- Pu
and clearly gives more emphasis to high leverage points than does WKi.
Note that the relation between MCi and WKi is
MCi = WKiy/ ( N - p)/ p.
WKi is also called DFFITSi (Belsley et al., 1980) because it is the scaled
difference between Y* and Y^*). It provides a measure of the influence of the ith
observation on the prediction at x*. Large values of DFFITSi indicate that
the 2th observation is influential on the fitted regression. Again, “ how large
is large? ”
Velleman and Welsch (1981) recommended that “values greater
than 1 or 2 seem reasonable to nominate for special attention” . Based on the
tjy-p-1 distribution for r*, a cut-off point for WKi of tN ^ p_ i
^a/ 2\p/ {N -p)]1/2
could be used. Alternately, we could replace
1>Q/2 by 2 to get a cut-
off point. Similarly, the influence of the 2th observation on the prediction at
Xfc, h 7^ 2 is given by \x!h(0-0{ i ) )\/ ( (7 y/ Phh, ) > which can be shown to be at most
equal to WKi. The implication is that if the ith observation is not seen to be
influential on the prediction at x*, as indicated by a small WKi value, then the
ith observation cannot be influential on the prediction at any other x^, h / i.
Figures 8.5.1 (c) and (d) give graphical representations of the Welsch distance
/•V
^
and the Welsch-Kuh distance. While Wi measures the distance from 0^
to (3
relative to the ellipsoid centered at 0^ , Wj measures the distance from (3^
to (3 relative to the ellipsoid centered at 0^ . Similar to the modified Cook’s
distance, the Welsch-Kuh distance measures the distance from 0^
to 0 using
the entire data, but with a different scale for each case.
Until now, we looked at measures that study the influence of the ith ob-
servation on the entire regression coefficient vector. In some cases, we may
be especially interested in certain components of 0. It might happen that an
observation is influential only on one dimension (predictor variable). Further,
an observation which has moderate influence on all the 0j s may be judged to
be more influential than an observation which has large influence on just one
regression coefficient, and negligible influence on the other coefficients. The fol-
lowing result describes the influence of an observation on a single 0j. Suppose
that without loss of generality, we partition X = (X(j), Xj), and
y = XU )0U ) + XjPj + £.

CHAPTER 8.
MULTIPLE REGRESSION MODELS
334
Let P = P( j) + WjWj/ wjWj , where wj = (I — P(j))xj is the residual vector
when regressing Xj on X(j).
Result 8.5.11.
Then
Si
Wij
(8.5.32)
Pj
Pj(i ) ~ (1- pa ) w'Wj '
Proof.
Using (2.1.22), we can write
0( i ) = {(X'X)-1 + [(X'X)-1x4xi(X
#X)-1]/(l-l>4i)}X'(s)y(0.
We can also write
(3 = (X'X)-1X^
)y(i) + (X'X)-1^^
,
which implies that
/3 = /3(i) - (X'X)-1xiyi.
These equations yield the required result after some simplification.
In addition to the four measures which quantify the influence of the ith
observation on the entire vector 0, it is possible to measure its influence on a
single regression coefficient 0j using statistics called DFBETASij which are
defined as follows.
Definition 8.5.18.
1
W i j
using estimate 5, or
DFBETASij
n
y/w'Wj V1'Pii
1
r*•yw
^
w
using estimate
j \/i - Pii ’
(8.5.33)
where X = (X^
Xj), the linear model is y = X^
/3^
) + Xj(3j + £, P( j) =
X { j ) [X'U ) X { j ) }-' X'{ jy Wj = [I- P(j,]xj, and
“
^ W'Wj
*
Note that
denotes the projection matrix for X( j), while Wj denotes
the residual vector when
is regressed on X( j); Wij denotes the zth element
of Wj. Belsley et al. (1980) suggested that values of \DFBETASij\ exceeding
2/N are influential on 0j.
d-p(i) )xjx;(i-p(i) )
p = p w) +
xj(i-p
<i) )xj

8.5. REGRESSION DIAGNOSTICS
335
A diagnostic measure (Belsley et al., 1980) which assesses
Definition 8.5.19.
the influence of the ith observation by comparing the estimated variance of /3
and (3(i ) is called the Covratio, and has the form
^
(X'X)-1 !
’
Now, CRt will be approximately equal to one when all the N observations
have equal influence on Cw(/i), so that deviation of CR, from unity suggests
that the ith observation is influential. Exercise 8.14 shows a relationship be-
tween CRi and the values of pa and r*, and the resulting calibration points for
CRi. The Cook-Weisberg statistic is equivalent to CRi since
CWi = - log(Ci?i)/2 + plog(FpiN_p,a/Fp, jv_p_i (Ct)/2.
Numerical Example 8.5. Influence diagnostics in regression.
following analysis pertains to a study of production waste and land use. There
are N = 40 observations on a response variable Y and five regressor variables.
The response Y is solid waste (in millions of tons), while X\ is industrial land
(acres), X 2 is fabricated metals (acres), X 3 denotes trucking and wholesale trade
(acres), X 4 denotes retail trade (acres), and X 5 is number of restaurants and
hotels. The data is taken from Golueke and McGauhey (1970), and is discussed
in Chatterjee and Hadi (1988). Graphical summaries of the Studentized residu-
als and some influence diagnostics, as well as a summary of least squares fit are
shown below. R2 = 0.85, the F-statistic is 38.39 and a2 = 0.023.
CRr =
(8.5.34)
The
Residuals and influence diagnostics for Numerical Example 8.5.
Obs.
Ci
Dffits
Obs.
Pa
Ci
Dffits
n
Vi
Pa
0.02
-0.31
21
-0.99
0.04
-0.34
0.04
-1.21
0.13
-0.57
0.04
-0.32
0.05
0.06
0.03
-0.47
0.04
-1.37
0.20
0.40
0.03
0.28
0.08
3.15
0.92
0.14
0.03
-1.14
0.05
-1.14
0.08
0.01
-0.20
-0.07
-0.47
-0.12
-0.07
0.011
-0.01
-0.68
-1.17
0.07
4.41
0.73
0.03
0.04
0.74
0.36
0.16
0.04
0.10
0.04
-0.63
0.13
-2.55
0.23
1.50
0.04
1
2
5.62
7.21
22
< 0.01
0.01
23
3
< 0.01
0.04
0.05
0.56
24
< 0.01
< 0.01
< 0.01
< 0.01
4
< 0.01
< 0.01
0.04
25
5
0.02
6
26
0.01 -0.24
-1.40
27
7
0.28
28
0.08
8
0.02
< 0.01
< 0.01
14.68
< 0.01
0.07
9
0.32
29
0.08
0.04
0.02
0.31
30
10
1.45
10.54
0.51
0.03
< 0.01
< 0.01
< 0.01
< 0.01
0.09
31
11
0.03
0.36
0.03
-0.19
0.03
0.56
0.04
2.68
0.61
1.85
0.09
-0.52
0.04
-0.41
0.04
0.12
0.04
-0.29
0.06
0.06
32
12
-0.25
-0.34
33
0.01
13
-0.04
0.02
0.11
34
14
0.22
1.15
1.59
3.36
35
1.24
0.47
15
-0.04
0.05
-0.19
0.05
1.89
0.04
-0.39
0.03
-0.60
0.03
-4.51
0.89
< 0.01
16
0.58
36
0.38
0.02
< 0.01
< 0.01
< 0.01
< 0.01
-0.10
-0.08
37
17
< 0.01
< 0.01
17.24
-0.07
-0.11
-12.74
38
18
0.03
39
19
-0.07
40
20

CHAPTER 8. MULTIPLE REGRESSION MODELS
336
Least squares results for Numerical Example 8.5.
Parameter
d.f.
Estimate
t-value
Pr > |i|
s.e.
Intercept
0.122
0.032
1 -0.00005
0.00002
1
0.00004
0.0002
0.0002
0.0001
1
-0.0009
0.0004
0.0134
0.0022
1
3.85
0.0005
X x
-2.96
0.0056
0.30
0.7691
2.83
0.0079
-2.30
0.0275
5.87
< .0001
*2
X3
1
*4*5
1
10
0.9
0.8 -
0.7 -
& 06 -
2
0.5 -
I5
0.4 -
<D
0.3
0.2 -
0.1
k - •
0.0
5
10
0
re$**2
L — R plot for Numerical Example 8.5
These results indicate that cases 2, 15, 31 and 40 are influential. The L — R
plot enables us to distinguish outliers from high leverage points.
Dummy variables in regression
8.6
The explanatory variables in a regression model may be quantitative variables
or qualitative variables. Quantitative variables assume continuous values over
some interval on the real line.
The explanatory variables are qualitative or
categorical, when they assume one of a few discrete values. A dummy variable,
or indicator variable is any variable in a regression equation that assumes one of
a finite number of discrete values and identifies different categories of a nominal
variable. The values assumed by such variables do not represent any meaningful
measurement; they are values such as 0, 1, or -1, and indicate the category into
which the subject falls. If the nominal regressor has L categories, we can create
L dummy variables to index these categories. We must include exactly L — l

8.6. DUMMY VARIABLES IN REGRESSION
337
of these indicator variables in a regression model which includes an intercept
term, whereas if the regression model does not include an intercept, then we
may include all L indicator variables.
An example of a dummy variable is
gender, i.e., W\ = 1 if the subject is a female, and W\ = 0 if the subject is
a male. Suppose a qualitative variable represents the location of a factory in
one of three geographic regions of the U.S., South, West, or Midwest. Two
dummy variables suffice: let W\ = 1 if the factory is in the South, and zero
otherwise; let W2 = 1 if the factory is in the West, and zero otherwise. I11
practice, a response variable Y may be explained by L dummy variables and k
quantitative predictors. This situation leads to a set of L parallel planes, each
representing a regression equation between Y and the k quantitative predictors
for a particular level of the dummy variable predictor. When k = 1, the model
corresponds to sets of L regression lines.
In Example 7.2.4, we discussed sets of L regression lines, which can be
represented by
SH0 — {Yi,i — 0i,0 +
4- £i,i,
< =!, - • • , n/ ,l = 1, • • • , L}.
We derived the least squares estimates both under the assumption of different
slopes, and the assumption of the same slope. We also derived a test for paral-
lelism. We continue with inference for sets of regression lines, and derive tests
that enable us to compare these lines. Although the notation gets cumbersome,
this approach can be easily extended to compare L regression planes as well.
In each case, the number of model parameters when there is no restriction is
p= 2L, while the total number of observations is N =
ni -
Result 8.6.1. Test for concurrence.
The null hypothesis of concurrence
of L regression lines at a point on the t/-axis, (i.e., when X = 0) is Hi : 0\,o =
02,o = • * • = 0L,0 = 0o , say. The test statistic has an FL- I ,N -2L distribution
under the null hypothesis.
Proof. We can write
00
0l ,\Xi ,i
~\~ £l ,i ,
i — I ?
* *
’lull ’ll — 1?
} Ly.
The least squares estimates of the common intercept and L slopes under H\
may be derived using (4.6.6) as
Y..- {Eh x,EZiYiM ESx xl }
(8.6.1)
0o
N -iEl.xl/Y^Uxl )
ni
~
E (Yi,i -0o )Xi,i
2=1
01A
(8.6.2)
,
/ =
,L.
n1£*i.<
2=1

CHAPTER 8.
MULTIPLE REGRESSION MODELS
338
It follows that
L
nt
SSEHl
=
"A> - Poxo )\ and
R{H0\HX )
1=1 2=1
SSEHI
SSEH0’
so that
F ( HQ\HI ) = { R( HQ\HI )/ ( L-1)} /{ SSE/ ( N - 21)}- FL_1,N_ 2L
distribution under Hi.
Result 8.6.2. Test for coincidence.
regression lines is given by Hi :0hO = 02,o =
*= PL,o = Po and /?M = /52,I =
• • • , /?L, I = Pi - The test statistic has an F2( L-\ ),N-2L distribution under the
null hypothesis.
Proof. The least squares estimates of the common intercept and common slope
under Hi may be derived using (4.6.6) as
The hypothesis of coincidence of L
0o
Y .., and
L ni
EEQ'M -i'- - )(*M - x.. )
l=li=l
(8.6.3)
0i,l
L
Til
EE (*M -*-)2
l— 12=1
We can show that
L
ni
L
ni
SSEH, = ££a^- K -)2 -^ED^-*- )2 and
J=1 2=1
( SSEHl - SSEH0 )/ ( 2L - 2)
SSE/ ( N - 2L )
/=1 2=1
F{ H0\Hi )
~ F2( L- l ) ,N - 2L
under Hi.
Numerical Example 8.6. Dummy variables in regression.
consists of weights (in lbs.) and the ages (in weeks) of 13 Thanksgiving turkeys.
Of these 13 birds, four were reared in Georgia (G), four in Virginia (V) and five
in Wisconsin (W). Let Y denote the weight, X denote the age, and O denote the
origin of these turkey (Draper and Smith, 1998). The least squares estimates
and ANOVA for a regression between Y and X , ignoring the grouping by origin
are shown below.
The data
Least squares estimates of Y versus X , ignoring O
t-value
Pr > \t\
SS
Variable
d.f.
Estimate
s.e.
Intercept
1.983
2.332
0.85
0.413
2124.803
0.417
0.089
4.67
0.0007
26.202
1
X
1

8.7. ROBUST REGRESSION
339
Analysis of variance for Y versus A, ignoring 0
Source
d.f.
SS
MS
F-value
Pr > F
Model
1
26.202
26.202
21.81
0.0007
Error
11
13.215
1.201
Corrected
Total
12
39.417
We have a = y/ MSE = 1.096, R2 = 0.665, and R^dj. — 0.634. We next seek to
verify a linear relationship between weights and ages, incorporating the effect of
different origins into the model, and compare whether the effect of age on weight
is the same for each origin in the context of sets of regression lines (testing for
parallelism).
Least squares estimates of Y versus X , G, and V
Variable
d.f.
Estimate
s.e.
lvalue
Pr > \t\
SS
Intercept
2.18
0.0575
2124.803
18.91
< .0001
-1.918
0.202
-9.51
< .0001
-2.192
0.211
-10.37
< .0001
1.431
0.657
0.487
0.026
1
X
1
G
1
V
1
26.202
2.717
9.687
Analysis of variance for Y versus X , G, and V
Source
d.f.
SS
MS
F-value
Pr > F
Model
Error
Corrected
Total
12
39.417
3
38.606
12.869
142.78
< .0001
9
0.811
0.090
In this model, a = 0.3, R2 = 0.979 and R2
covariance matrix of the estimates is
= 0.973. The estimated variance-
adj.
/ 0.432
-0.017
-0.010
0.023
-0.017
0.001
-0.0003 -0.002
-0.010 -0.0003
0.041
0.019
0.023
-0.002
0.019
0.045 )
8.7
Robust regression
Least squares estimates have several optimality properties within the class of
normal linear models, which in addition to their computational simplicity, have
made this procedure popular. However, the method of least squares can be
extremely sensitive to (a) a departure of the error distribution from normality,

CHAPTER 8. MULTIPLE REGRESSION MODELS
340
and (b) the presence of outliers, even under normality. In general, the class of
Ld-estimators are obtained by minimizing the Ld-norm
d > l.
When d = 2, we have the Euclidean metric, or Z/2-norm, and leads to the least
squares estimator. The case d = 1 corresponds to the absolute metric or L\-
norm, and yields the Li-estimator or LAD estimator, which we discuss below.
Note that d = oo leads to the minimax method (Berger, 1980).
Many robust and resistant methods have been developed since the 1960’s
with the purpose of obtaining statistical procedures that are less sensitive to
outliers or anomalous data values, and are reasonably efficient with data from
the ideal Gaussian distribution or a range of alternative distributions.
Ro-
bust methods are designed to have high efficiency in a neighborhood of an
assumed model (Huber, 1964). In the following sections, we describe LAD or
Li-regression and M-regression (see Birkes and Dodge, 1993).
Least absolute deviations ( LAD ) regression
Least Absolute Deviation(LAD) regression or L\-regression is a natural gen-
eralization^
the median to a regression problem and consists of finding the
estimator /3LAD as any solution to the minimization problem
8.7.1
N
N
- x'/?| = mingle
(8.7.1)
2
J
2=1
2=1
where Si are iid having a continuous distribution with location 0, scale a2 and
cdf F(.).
Example 8.7.1.
Suppose the errors Si are random samples from a double
exponential distribution,
f ( S i )= ( 2cr ) 1exp(— |£*|/a ), -OO < Si < 00
(see (A16)).
Assuming that a is fixed, it is clear that maximum likelihood
estimation of (3 involves minimization of the expression in (8.7.1).
In the literature, LAD regression has alternately been referred to as MAD
(Minimum Absolute Deviations) regression, or MAE (Minimum Absolute Er-
rors) regression, or LAR (Least Absolute Residuals) regression, or LAV (Least
Absolute Values) regression, or simply L\-regression. The Li-regression is a
special case of Lp-regression where we minimize X
^ili
” x'/?\
p- Although
there is a solution that fits (8.7.1) exactly at p observations, this solution is
not necessarily unique. The computation of the solution may be carried out
by reducing the problem to one of linear programming (Charnes, Cooper and
Ferguson, 1955). The argument is that the minimum of the criterion YliLi M
occurs at one of the finite set of points on a piecewise convex surface. Therefore,
any minimum of the Li-norm must lie at one of the vertices of a finite set of

8.7. ROBUST REGRESSION
341
N
points in 7Zp. This enables the linear programming algorithm to search
among the finite number of vertices for a solution.
The problem is to minimize J2iLi kil > where s and /3 are unrestricted in
sign. Define nonnegative variables ef and e~ by
P
if Si > 0
if £i < 0, and
4 =
o,
if £i > 0
if £i
0;
0,
either ef or £J will be zero, \£i \ = e* 4
* e” , and Si = ef — e~. Geometrically,
and
can be interpreted respectively as the vertical deviations above and
below the fitted hyperplane corresponding to the estimated parameters. We
then minimize the function (YliLi4 + JliLi ei ) subject to
> 0, e~ > 0,
and the equation ef — e~ = ei — Yi — x'/3, while /3 is unrestricted. Since we
are minimizing a continuous function which is bounded below over a convex
set, the problem always has an optimal solution, although this solution need
not be unique, even when r(X) = p. The solution is (3LAD • In some cases
(Dodge and Jureckova, 2000), LAD regression is faster than OLS for sufficiently
large N and moderate p. Provided the cdf of E{ has a continuous and positive
derivative in a neighborhood of the population median, it is true that as N — > oo,
PLAD ~ N(/3, c72(X'X)“ 1). In practice, a2 is unknown, and is estimated as
1, • • • ,*
follows based on the LAD residuals SI ^ LAD = Yi — x'/?^AD, i =
Arrange the M —
( N - p -1) nonzero residuals in ascending order, and denote
these by e{ l ),LAD < • •
[(M -f l)/2 + y/
~M\. Then
Let Ki = [(M + l)/2 — VM\ and K2
*
E ( M ),LAD-
4M{ E{ K2 ),LAD - £( KI ),LAD }
-2
_
°LAD ~
(8.7.2)
2^a/2
where za/ 2 is the upper a/2th critical value from a standard normal distribution.
Consider the simple linear regression model Yi = /3Q +
Example 8.7.2.
(3\Xi A £{. The LAD estimates of Po and Pi, as well as cr2 are obtained as
described above. It is clear that the p-value of the test Ho : Pi = 0 is
P( \T\ > |£|), where
1* 1- { \PULAD\IZL( X* - X )2Y / 2 }/ 2LAD ,
and T ~ tu-2- It has been proved that
y/N(PLAD ~ P ) converges in distribution
(as N — * oo) to a N (0,(2/(0)}-2Q-1) distribution, where /(0) is the pdf at
the median, and Q = limX'X/iV.

CHAPTER 8. MULTIPLE REGRESSION MODELS
342
A more general formulation given by Bassett and Koenker (1978) is useful.
Suppose as before that the errors Si in the linear model are iid with cdf F(.),
which is symmetric about 0. The 0th regression quantile (0 < 0 < 1) is defined
as any solution to the minimization problem
E
o\Yi-<0\ + E
(8.7.3)
min
(3
{ i’Y
When 0 = 1/2, this is equivalent to minimizing YliLi\^i “ x'/31, and the result-
ing estimator coincides with 0LAD- Let /?*(0) denote the solution to (8.7.3). For
a general discussion of asymptotic properties of this estimator, and for a dis-
cussion of the trimmed least squares estimator, see Bassett and Koenker (1978)
and Ruppert and Carroll (1980).
Example 8.7.3. Censored Regression.
We describe a limited dependent
variable model in which the response variable, which is observed over time, is
censored. The tobit model is, for t = 1, • * • , AT,
Yt = x'tp + eu
if Tt > 0
if Yt < 0,
(8.7.4)
Y * =
t
0,
where we assume that et, t = l, -- , 7V are iid, whose cdf F(.) has positive
derivative /(0) at zero, and has median zero. We also assume that the true
regression parameter belongs to a bounded open set of 7Zp. Let I denote the
indicator function of a set of values; i.e., I (u) = 1 if u belongs to that set,
otherwise I (u ) = 0. The tobit model can also be written as
Yt+ = (x't/3 + et )
where the function qf denotes qtl{qt > 0). Powell (1984) studied the properties
of the LAD estimator of (3 which is obtained by minimizing
+
(8.7.5)
N
Em+ -«PLAD )+ I.
(8.7.6)
i-1
Under some regularity conditions, Powell (1984), and Rao and Zhao (1993)
proved the asymptotic normality of this estimator.
Suppose we wish to test Ho : C'0 = d versus the alternative H\ : C'/J ^
d,
where as before, C is a p x s matrix of rank s, and d is a constant vector. Let
0LAD,HO denote the solution to (8.7.6) subject to Ho. The likelihood ratio test
statistic is
N
N
LRT = ElYt+ ~ ( X'AADMO) -Y,\Yt+ ~ {<hAD )+\.
+
(8.7.7)
z=i
i=i

8.7. ROBUST REGRESSION
343
In practice, /(0) is unknown and is estimated by
N
IN {0)
=
h\5^J (x't(3L,AD > 0)
l ]
2=1
N
x
[£/(x'f/?L /,D > 0 )I (x'tpLAD < yt+ < x't0LAD + h)}. (8.7.8)
2=1
Under certain conditions (see Rao and Zhao, 1993), 4/(0 )LRT has a limiting
Xs distribution under Ho-
8.7.2
M-regression
The M-estimator, which was introduced by Huber (1964), is a robust alternative
to the least squares estimator in the linear model. It is attractive because it is
relatively simple to compute, and offers good performance and flexibility. In the
model (4.1.2), we assume that £{ are iid random variables with cdf F(.), which
is symmetric about 0. The distribution of e* can be fairly general; in fact we
need not assume existence of the mean and variance of the distribution. Recall
from section 7.5.1 that the MLE of /3 is obtained as a solution to the likelihood
equations
N£x,{/'(>, -*iP )/ f (Yi ~ x'/3)} = 0,
(8.7.9)
2=1
where /(.) denotes the pdf of £*, and /'(.) denotes its first derivative. Suppose
(8.7.9) cannot be solved explicitly; by replacing /'(.)//(.) by a suitable function
we obtain a “ pseudo maximum likelihood” estimator of /3, which is called
its M-estimator. The function i/>(.) is generally chosen such that it leads to an
estimator /3M which is robust to alternative specifications of /(.). Suppose p( .)
is a convex, continuously differentiable function on 7£, and suppose ?/>(.) denotes
the first derivative of p(.), i.e., ip(t ) = (d/ dt ) p(t). The M-estimator /3M of /3 in
the model (4.1.2) is obtained by minimizing, over /3, the objective function
N£PW - X'/3)
(8.7.10)
2=1
or, equivalently, by solving the set of p equations
N£ Xiip{Yi - x'/3) = 0.
(8.7.11)
2=1
When (a) the function p( .) is not convex, or (b) the first derivative of p(.) is
not continuous, or (c) the first derivative exists everywhere except at a finite or

CHAPTER 8. MULTIPLE REGRESSION MODELS
344
/S
countably infinite number of points, we consider (3M to be a solution of (8.7.10),
since (8.7.11) might not have a solution, or might lead to an incorrect solution.
By a suitable choice of p(.) and t/>(-) > we can obtain an estimator which is robust
to possible heavy-tailed behavior in /(.).
To ensure that (5M is scale-invariant, we generalize (8.7.10) and (8.7.11)
respectively to
N
^
P{(Y;. - x'/j)M and
(8.7.12)
i=1
N
- x'/?)/<r} = 0.
(8.7.13)
i— l
In practice,
<r is unknown, and is replaced by a robust estimate. One such
estimate is the median of the absolute residuals from an LAD regression.
As we have seen, M-regression generalizes least-squares estimation by al-
lowing a choice of objective functions. Least squares regression corresponds to
p(t ) = t2. In general, we have considerable flexibility in choosing the func-
tion p(t), and this choice in turn determines the properties of (3M - The choice
involves a balance between efficiency and robustness.
Example 8.7.3. Median regression.
Suppose the true regression function
is the conditional median of y given X.
The choice of p(t ) is \t\, and the
corresponding problem is called median regression.
Example 8.7.4.
M^-regression is an alternative form of A/-regression; (3w is
a solution to the p simultaneous equations
t ( v‘
i— i \
- x'/?
w*x - — 0,
(8.7.14)
a
whereWi = w{ (Yi— x.'i(3)/a}. The equations in (8.7.14) are obtained by replacing
z/>(t) by tw(t ) in (8.7.10); w(t ) is called a weight function.
8.8
Nonparametric regression methods
The parametric approach to linear model inference assumes that the functional
form of the relationship between the response and predictors is linear and known,
so that the problem reduces to that of estimation and inference pertaining to the
parameters (under some distributional assumption on the errors). On the other
hand, nonparametric methods only make a few general assumptions about the
regression surface. For instance, in smoothing techniques with kernel, nearest-
neighbor, or spline methods (Hastie and Tibsirani, 1990; Thisted, 1988), the
estimate of the regression surface at a point xo is the average of the responses
of those observations with predictors in the neighborhood of XQ . In this section,

8.8. NONPARAMETRIC REGRESSION METHODS
345
we give a brief introduction to computer-intensive regression methods that are
useful for nonparametric fitting, when the set of explanatory vectors is high-
dimensional, or when a parametric linear model does not satisfactorily explain
the relationship between Y and X. Although these methods do not involve non-
linear parameterizations, the procedures do allow for the possibility of choosing
a nonlinear function of the explanatory variables.
Additive models and projection pursuit regression are described in the next
two subsections. The additive model is a generalization of the linear regression
model, where the usual linear function of observed covariates is replaced by
a sum of unspecified smooth functions of these covariates. Projection pursuit
(Friedman and Tukey, 1974) is an exploratory procedure which seeks to unravel
structure within a high-dimensional predictor set by finding interesting orthog-
onal linear projections of the data onto a low-dimensional linear subspace, such
as a line or a plane.
8.8.1
Additive models
An additive model is defined by
p
r 4- y
^
g j ( X j j ) + e»,
E(ei ) = 0,Var( £i ) = <r2,
(8.8.1)
Yi =
3=1
where the
s are arbitrarily defined functions, one for each predictor. A simple
assumption may be that each of these component functions is univariate and
smooth; however, the smoothness assumption may be relaxed, and it is possible
to have a function that is continuous and multi-dimensional, or even categorical.
One attractive feature of the additive model is that the variation of the response
surface obtained by holding all other predictors but one fixed does not depend
on the values of these predictors. Hence, after fitting the additive model to data,
it is possible to plot the p coordinate functions separately in order to assess their
usefulness in modeling Y .
In an additive model, no parametric form is imposed on the functions and
they are estimated nonparametrically in an iterative way using smoothers. A
smoother is a nonparametric tool for estimating the trend of a response Y as
a function of predictors X\, • • • ,
The estimated trend is less variable than
Y itself, which leads to the name “smoother” . The scatterplot smoother is a
special case when there is a single predictor. Given data y = (Yi, • • • ,Yn )' and
x = (Xi, - “ , Xn)', a scatterplot smoother is a function of x and y. When
there are several predictors, we have multiple predictor smoothers. The pri-
mary functions of smoothers are (a) as graphical descriptions of the data by
enhancing the scatterplot of Y versus X, and (b) as vehicles for estimation of
the dependence of Y on X in the context of additive models. If there is a sin-
gle categorical predictor, the smoother is very simple and consists of averaging
the Y values in each category corresponding to the predictor. For continuous
predictors, we may use the notion of local averaging, i.e., averaging Y values in
the neighborhood of a target predictor value. Two questions to be resolved are

CHAPTER 8.
MULTIPLE REGRESSION MODELS
346
(a) the size of these neighborhoods, and (b) the nature of the averaging within
the chosen neighborhoods, i.e., the type of smoother to use. A large neighbor-
hood produces an estimate with low variance but high bias, with the converse
for small neighborhoods. There is thus a tradeoff between bias and variance of
estimation. The size of the neighborhood is expressed in terms of an adjustable
smoothing parameter.
The estimated model consists of a function for each covariate. Apart from
their use in predicting the response variable, additive models enable us to explore
the appropriate shape of each covariate effect on the response. The idea behind
the scatterplot smoother is to explore and delineate the functional dependence in
multivariate data without a rigid parametric assumption about that dependence,
such as linearity, or an exponential form, etc.
Hastie and Tibsirani (1990)
present a detailed description of this procedure for data analysis.
The most general approach for fitting additive models to data consists of
estimating each component function by an arbitrary smoother, such as cubic
smoothing splines, locally-weighted running lines, or kernel smoothers. The fit-
ting then proceeds iteratively, possibly by minimizing the appropriate penalized
least squares criterion; the back fitting algorithm is a general algorithm that en-
ables us to fit an additive model to data. The algorithm consists of the following
steps.
(i) Initialization: Set r = ave(Yi ), where ave could be the mean, median,
etc., and set gj = <$, j = 1, • • • , p.
(ii) Update: For j = 1, • • • ,p, set gj = Sj (Y- r -
9j( X j ) lxfc) > where
Sj denotes the jth scatterplot smoother.
(iii) Continue (ii) until the individual functions do not change.
The backfitting algorithm is motivated by the fact that, if the additive model
is correct, then for any fc,
E{Y - T - '£l 9j ( XJ )\Xk } = gk { Xk ).
(8.8.2)
The usual linear regression estimates may be used for the initialization.
By
design, the algorithm is only given as a modular skeleton; details for a particular
user would depend on the choice of smoother and the data analytic context in
which the model is employed. It has been pointed out that when the smoothers
are linear operators, the back fitting algorithm is a Gauss-Seidel algorithm for
solving a certain set of estimating equations, and convergence in many practical
situations has been proven. When all the smoothers Sj are projection operators,
the whole back fitting algorithm may be replaced by a global projection that
involves no iteration and whose convergence is guaranteed. Although smoothers
such as smoothing splines are not projections, they do possess properties of
projections that guarantee that the solution converges.
Generalized additive models (GAMs) are an extension to the class of gener-
alized linear models (GLIMs), where the linear form T +
Xijpj is replaced

8.8. NONPARAMETRIC REGRESSION METHODS
347
by the sum of component functions, r -I-
1 9j( Xij )- A suitable link func-
tion between the predictors and the response which has a general probability
distribution is chosen similar to the GLIM setup (see section 11.4).
8.8.2
Projection pursuit regression
In the linear regression model (4.1.2), we assumed that the response surface has
a fixed linear form. In projection pursuit regression, the regression surface is
approximated by a sum of smooth functions of linear combinations c'X of the
predictors, viz.,
J
PPfl(X) = Co +^
Sc . (c'X)
(8.8.3)
3-1
and y = PPR(K )+e. In other words, projection pursuit regression employs an
additive model on predictor variables which are obtained as projections of X in J
chosen directions. The terms in (8.8.3) are constant in all directions except one,
and are called ridge functions. As shown by Diaconis and Shahshahani (1984),
these models can approximate arbitrary continuous functions of X when J is
sufficiently large.
Consider a set of observations (Yi, Zi ), i =!, * • • , AT.
Definition 8.8.1.
Based on the notion of local averaging, we define a smooth representation of Y
as ordered in ascending value of Z by
s<z*> =
where AVE can denote any average, such as the mean, median, etc.
We define a criterion of fit /(a, v, U) by
7(a, v, U) = 1- EhiVi ~ 5a(a'Ui )}2/Ef=1 V?,
where v' — (Vi, • • • , Vw ), U = (ui, • • • , up), uj is an iV-dimensional vector, and
Sa(a'ui) follows from Definition 8.8.1.
Given data (y
*,x'), i — l, * * * , AT, the iterative algorithm (Friedman and
Stuetzle, 1981) applies an additive model to projected variables and consists of
the following steps. Let S denote a user-supplied threshold value (lower bound)
for smoothness.
Definition 8.8.2.
(i) Set the iteration counter J = 0, and initialize residuals to the centered
= y< -F, i = i, .. - , iv.
^
0)
responses, £\
(ii) Search for the next term in the model. For a chosen vector c, obtain the
one-dimensional projection Zi = c'x*, i = 1, • • • , N. Construct a smooth
representation of the current residuals sequenced in ascending order of
Z. Find the vector cj+i that maximizes 7(c,e, X). Let S,
corresponding smooth.
denote the
C j+ i

CHAPTER 8.
MULTIPLE REGRESSION MODELS
348
(iii) If the criterion of fit is smaller than
<5, we terminate the procedure. If not,
we increment the iteration counter to J + 1, and update the residuals to
£i
— ei
and repeat from Step (ii).
Notice that the models at the jth step are sums of j smooth functions of arbi-
trary linear combinations of Xi, • • • , Xp.
Unlike additive models, projection pursuit regression models can include
interactions between the X j's in the model function, and are therefore useful
for representing general regression surfaces.
(c'j+jXi),
i = 1, •- * , N ,
C j+1
8.8.3
Neural networks regression
Neural networks are mathematical objects representing a system of intercon-
nected computational units (Cheng and Titterington, 1994; Hertz, Krogh and
Palmer, 1991). That is, they constitute a collection of a possibly large num-
ber of simple computational units which are interlinked by a system of possibly
intricate connections. In the neural networks framework, specification of the
general linear model and associated parametric inference are handled by con-
structing a network of nodes and links from which the linear model can be
written down. The most common neural-networks approach to linear models is
multilayer perceptrons and generalizations of single-layer perceptrons. This ap-
proach is extremely valuable when the problem involves very high-dimensional
vectors and matrices which impose a limitation on the usual matrix methods
that we discussed in earlier chapters. In most applications of neural networks,
there is no explicit assumption of randomness or an ensuing probabilistic struc-
ture in the underlying variables. Instead, the aim is function approximation,
and optimality criteria that enable the choice of the approximant, such as the
least squares criterion, can no longer be interpreted as a log-likelihood function
(under a normality assumption, say). A flexible approach to generalize linear
regression functions uses feed-forward neural networks.
The simplest feed-forward network is the single-unit perceptron, which con-
sists of K input units (covariates) ATi, - - - , Xfc, and one output (response) Y.
The mean response is related to the covariates via the function
N
E(Y ) = g( x,w) = w0 4-^
W j X
(8.8.4)
J >
3=1
where W j is the weight corresponding to X j, j — 1, • • • , K , and g ( .) is a known
activation function. Given N sets of observations (xj,!*), i = 1, * * * , JV in a
training sample, we must determine the vector of weights such that the energy
function or learning function
N
£(w) =
-s(xi, w)}2
(8.8.5)
t=l

8.8. NONPARAMETRIC REGRESSION METHODS
349
is minimized with respect to w.
Using the least squares approach, we must
solve the system of estimation equations
N
dE{w)
dgfc, w) = 0,
k = 0,l , - - ,K
(8.8.6)
dwk
using numerical techniques such as the generalized delta rule or error back-
propagation (Rumelhart et al., 1986), or gradient methods (Thisted, 1988).
dwk
1=1
®i , k
\
I
\
{AI
3
\
\
o
Hidden layer
\ /
Figure 8.8,1.
A feed-forward neural network with one hidden layer.
A neural network with one hidden layer is shown in Figure 8.8.1. The input
units distribute the inputs to the hidden units in the second layer. The hidden
units cumulate their inputs, together with a constant term (the bias). An acti-
vation function 4k is applied to this sum. The output units have the same form,
except with output function (j>Q. This gives
Yi — 4*0
^ ^^k ,i4k(77c 4
*^ ^
UJj kXj )}.
k
j
(8.8.7)
The usual choice for the activation function of the hidden layer is the logistic
function
4>k (u) = exp(u)/{1+ exp(u) }.
In practice, the weights must be chosen to minimize some fitting criterion,
such as the least squares criterion which minimizes the Z/2-norm between the
output and the target, or in other words, minimizes S =
II ^
“ Yl ||2, where
Tl is the target, and Yl is the output for the Zth pattern. Alternatively, we can
minimize the function S 4- AD(/), where, for L patterns, the penalty function
on the second derivatives of g(.) has the form

CHAPTER 8. MULTIPLE REGRESSION MODELS
350
Dig ) = SXiji&Yi/dXpdx «
d2YJdX 2 }/ L.
The reader is referred to Warner and Misra (1996) for a accesible summary of
this technique.
8.8.4
Curve estimation based on wavelet methods
Wavelet methods, which incorporate wavelet bases, are becoming increasingly
useful in several branches of statistical modeling, including regression analysis.
Consider the simple nonparametric regression model
(8.8.8)
Yi = g( Xi ) + <7£i, i =!, - • • , iv,
where the X*’s are equally spaced points, the ei s are iid N (0,1) variables, and
the regression function g(.) is unknown. We can write (8.8.8) in vector notation
as
(8.8.9)
y = g + oe
where g = ( <7i , * • • , #JV). In the literature, two versions of this model are dis-
cussed (Antoniadis, Gregoire and McKeague, 1994). In the fixed design model,
the Xi s are nonrandom design points, usually denoted by U , and ordered, i.e.,
0 < *i <
• • • < £yv < 1; the e* are iid with zero mean and variance a2. The
random design model assumes that the (X*, Y*)’s are independently distributed,
g( x ) = E(Y \ X = x ), and £i = Yi — g( Xi ). Under both versions, the objective
is to estimate g based on (Yi,X*), i = 1, • • • , N using functions of wavelets. To
be more specific, our goal is to estimate the regression function g(u ), 0 < u < 1
based on wavelet kernels (which are defined later).
In order to estimate <7(.) based on available discrete data, we must process
the discrete wavelet transform, which maps the TV-dimensional response vector
y to an N-dimensional vector d = (d\, * • • , d^ )' in the wavelet domain.
Definition 8.8.3.
mation defined by
The discrete wavelet transformation is a linear transfor-
(8.8.10)
d = Wy
where W is an orthogonal N x N matrix.
Let 9 = (0i, • • • ,6NY = Wg, and let e* = (sj, • • • , £^
)' = We. By Exercise
5.9, e* ~ TV(0, IAT). The wavelet image of (8.8.8) under this orthogonal wavelet
transformation is
(8.8.11)
di — 0i + ere*, i =!, • • • , JV,
or, in vector notation,
(8.8.12)
d = 6 4- <re*.

8.8. NONPARAMETRIC REGRESSION METHODS
351
Since e* and e have the same distribution, we can write the model in the wavelet
domain as d = 9 4- ere.
The discrete wavelet estimator g which is based on thresholding in the
wavelet domain was discussed by Donoho and Johnstone (1994). The proce-
dure of constructing this estimator consists of three steps.
(i) Transform the observations Y*, i = 1, • • • , iV to the wavelet domain by
applying a discrete wavelet transformation, yielding a sequence of wavelet
coefficients d*, i — 1, • • • , N.
(ii) Using an estimate of <r, threshold (or shrink) the wavelet coefficients.
(iii) Invert the shrunk coefficients, obtaining y*, i — 1, *
• , N.
In the rest of this section, we describe linear wavelet regression estimators
(see Vidakovic, 1999). We first give a few definitions on the set of complex
numbers C. Note that if a is a complex number, we denote its complex conjugate
by a.
A complex vector space TL is
Definition 8.8.4. Inner-product space.
called an inner-product space if for each pair of elements x and y in W, there
exists a complex number < x, y >, called the inner product of x and y, such
that
1. < x, y >= < y, x >
2. < x + y, z >— < x, z > 4- < y, 2 > for all x, y, 2:
3. < ax, y >= a < x, y > for all x, yGH and a
C
4. < x, x >
> 0 for all x
H
5. < x, x >
— 0 if and only if x = 0.
A sequence {xn, n = 1, 2, - • •} of
Definition 8.8.5.
Cauchy sequence.
elements of an inner-product space is called a Cauchy sequence if
II xn - X m ||-> Oasm. n ^ oo,
i.e., if for every e > 0, there exists a positive integer N ( e ) such that
< e for all m,n > N (e ).
II
3?m
Definition 8.8.6. Hilbert space.
A Hilbert space H is an inner-product
space which is complete, i.e., an inner-product space in which every Cauchy
sequence {xn} converges in norm to some element x G H. The norm of an
element x 6 H is defined as || x ||= ^/< x, x >.
A linear subspace V of a Hilbert space H is said to be a closed subspace of H
if V contains all its limiting points, i.e., if xn G V and || xn — x ||—
> 0 as n —
00,

CHAPTER 8. MULTIPLE REGRESSION MODELS
352
then x
V. The space £2 is the space of all square-integrable functions, i.e., if
f ,g e £2, < f , g >= f fg and II / ||= yjf //. Let V be a subspace of £2 and
let {ei,e2, • • •} be a complete orthonormal basis of V.
The scaling function
( f ( x) is the solution of a two-scale
Definition 8.8.7.
difference equation:
tp( x ) =
2x — k ), with
k e z
/
(p( x )dx
Jn
i;
the coefficients Ck are called filter coefficients, and their choice enables the con-
struction of wavelet functions with desirable properties.
The primary wavelet function 'ip( x ) is defined by
Ip(x ) = £ (-l)fccfe+iv>(2x+ k ),
kez
Definition 8.8.8.
where Z is the set of integers.
As we said earlier, our goal is to estimate the regression function using
wavelet kernels, viz., integral operators /Cj ( x, y ) that project onto closed sub-
spaces Vj of £2(7?.), which is the space of all square-integrable functions. The
increasing sequence of subspaces Vj form a multiresolution analysis of £2(7£).
A multiresolution analysis (MRA) is a sequence of closed subspaces Vn, n
Z
in £2(7£) such that
• • • C V_
2 C V-i C V0 C Vi C V2 C • • •,
njVj = {0}, and UjVj = £2(7Z ) (A denotes the closure of the set A). Further,
g( 2 jx )
Vj if and only if g( x ) 6 Vo,
and there exists a scaling function </>
. Vb such that
Vo = { g e C2{U )\g{x ) = J2k ck$( x - k ) }.
For the fixed-design model, Antoniadis et al. (1994) suggest the waveletized
Gasser-Mueller kernel estimator
N
f
<=1
(8.8.13)
d( x ) =
ICj( x, y)dy,
A,
where
2J^
(2}x,2Jy), with
(8.8.14)
(8.8.15)
£ j ( x, y )
K,(u,v ) =
4>(u — k )(j>{v - k )
k

Exercises
353
and JC(u, v ) and JCj( x, y ) are respectively reproducing kernels of Vo and V j . The
Ai = [5i_ i, Si) are intervals that partition [0,1] so that Xi
Ai. For example,
we can set so = 0,
= 1, and Si = (x* + Xi+i)/2, i = 1, • • • , N — 1. Since
the sum of the weights f /C(x, y )dy is equal to one, no normalizing constant is
needed.
For the random design model, the proposed estimator is a wavelet version
of the Nadaraya-Watson estimator (Nadaraya, 1964; Watson, 1964):
N
/
N
g( x) = C^
YilCj&Xi ) } /{£ AC,(*,*<)}.
(8.8.16)
i=1
2=1
The resolution j is an integer-valued tuning parameter, and an optimal value
must be selected; see Vidakovic (1999) for details on this as well as on more
efficient non-linear estimators of #(.). Wang(1995) discussed analysis of change-
points via wavelets.
Exercises
8.1. Suppose Yi — 0o+0i ( Xn - Xi )+(32( Xi2- X 2 )+ £i,i = 1, * • • , iV, where
Xj = ^2^
=1 Xij /N , j = 1, 2, and Si are iid normal random variables
with mean 0 and variance a2. Quantify the width of the marginal 95%
confidence interval for (3\ in terms of r\2 y the sample correlation between
Xi and X 2.
8.2. In the normal linear regression model Yi =
+0iXiti H
h /3kX^
k +
i = 1, • • • , N , obtain a test statistic for H : fij = d where 0 < j < k and d
is a constant.
8.3. In the full-rank regression model y = X/3 + e, with r(X) = p and e ~
iV(0, cr2I), show that as an estimator of
<r2, y'Aiy, with
Ai = (IJV
P)/(JV
p + 2), has smaller MSE than the least squares
estimator of cr2.
(a) Ignoring end effects, show that DW ^2 2(1 — p).
(b) Show that
8.4.
E( DW )
Var( DW )
Q/(iV — p), and
2{R- QE( DW ) }/ { ( N - p)( N - p+ 2)},
where
Q
=
tr( A)- tr [X' AX(X'X)-1]), and
tr{A2) - 2tr[X,A2X(X'X)-1] + fr[{X'AX(X,X)-1}2].
R
8.5. Consider the regression lines Yi }i =
+ ej,,, l = 1, 2, £iti
being iid A^(0, <r2) variables. Given observations
i = l, - - - , n;,
l = 1, 2, N = ni + ri2, carry out the following tests:

CHAPTER 8. MULTIPLE REGRESSION MODELS
354
(a) H :
= /?2,i; he., a test for parallelism.
(b) H : Pi' O = /?2,o; i.e., a test for concurrence.
(c) H : /J^o = /32,0, and /3i,i = /?2,i; i.e., a test for coincidence.
8.6. Consider the independent regressions
—
&k 4"0{ Xk ,i
Xk }
£k,ii
% — 1, ’
* ‘ >
& — 1, 2,
where £k,i are iid iV(0, cr2) variables, and Xk =
(a) Estimate oq, OL<I and /3, and thus the vertical distance between the
two lines, measured parallel to the y-axis.
(b) Construct a 95% C.I. for this distance.
8.7. Show that || /3*(0) ||<|| /3||.
8.8. In the regression model (4.1.2), show that the MSE after deleting the zth
case is given by
MSE(i ) —
( N — p)MSE/ ( N - p-1- r*2).
8.9. [Chatterjee and Hadi, 1988.] Let pzu to be the diagonal elements of the
matrix Pz = Z(Z/Z) _1Z/ where Z = (X, y). Show that
Pzii = Pii + £%/?£.
8.10. Derive the form of the sample influence function (SICi) in (8.5.19).
8.11. For the simple regression model YJ = /3o + PiXi +
i = 1, • • • , 7V,
show that the weighted squared standardized distance in (8.5.9) is Wi =
{ Npa — l )px y > where px ,Y is the simple correlation between X and
Y .
8.12. Show that
(a) a\ < (1- p^).
(b) As o?
N — py and r?2
8.13. Show that Cook’s distance can be expressed as
1 — Pa y show that r2
oo.
Ci = {g?/(l - Pil)2}Wp52}-
8.14. Show that
CRi = { {N - p- rf )/ ( N - p- l)}» /(1- Pit ) .

Exercises
355
Show that when|rj| > 2, and pa — 1/ N , then CRi < 1 — {3p/ ( N — p)}
approximately. Also show that when r* = 0, and pa > 2p/ Ny CRi >
1 + {3p/ ( N — p)} approximately. Use these to arrive at the approximate
calibration bounds |CRi — 1| > 3p/ N (Belsley et al., 1980).
8.15. Show that
?2
APi
Pa "h e’e
ri
Pa + (1- Pa) N - p
8.16. Show that for inference on the location parameter, the influence curve for
the median functional is
ICTA* ) = sgn[x - T ( F )\/ 2 f [T (F )\
where /(.) is the pdf corresponding to the cdf F(.).


Chapter 9
Fixed Effects Linear Models
Consider an experiment involving a single factor, Factor A, with a levels. In
fixed-effects models, estimation and inference is required for these particular
a treatment levels. By contrast, in random-effects models, the a treatments
are regarded as a random sample from a large population of treatments (see
Example 4.1.4). Inference for these models will be described in Chapter 10. In
Chapter 4, we introduced the least squares approach for balanced fixed-effects
models, and discussed the ANOVA decomposition and F-test under normality
in Chapter 7. We begin this chapter with a discussion of procedures for assessing
assumptions in fixed-effects ANOVA models. In the next subsection, we describe
inference for unbalanced higher-order models, including cross-classified models
and nested models.
The final sections deal with analysis of covariance and
nonparametric procedures.
Checking model assumptions
9.1
In section 8.1, we described graphical and numerical tests for normality of er-
rors. Another assumption is that of equality of error variances. Fortunately, the
assumption of equality of variances is less critical with nearly balanced data, i.e.,
when the sample sizes are nearly equal. When approximately equal samples are
available from each population, the p-values in the ANOVA test for equality of
means is only slightly distorted when the population variances are unequal. In
situations where the heterogeneity of variances is a problem, a data transforma-
tion may help to stabilize the variance, after which the usual ANOVA procedure
may be implemented. In this section, we present three tests for the homogeneity
of variances.
Recall the two sample F-test of HQ : of = o\ versus H\ : G\ > erf , based on
normal random samples of sizes n\ and n2. If the observed test statistic FQ =
S1 / S2 is greater than the critical Fni-i,n2-i,a > we reject the null hypothesis at
level a. We extend this approach for testing the assumption of homogeneity of
variances in a populations.
357

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
358
Result 9.1.1. Bartlett’s test of homogeneity of variances.
a test of the equality of a population variances, i.e.,
Ho : of = o-2,
Consider
(9.1.1)
i
1>
*
* *
? u,
against the alternative that not all the population variances are the same. Let
• , a
where Vi — rii — 1, and
q
Q
CL
s
2 = E^i S f / Z^
i=1
i=1
Define
= y^
log52 -
Vi logSf , and
M
i=l
2=1
1 + {]TlM - l/5» /{3(a - l)} •
c
2=1
2=1
Under HQ, the quantity M/C is approximately distributed as a Xa-i variable.
Clearly, each Sf has a chi-square distribution with Vi degrees of free-
Proof.
dom. The approximate null distribution of the statistic M /C is a consequence
of the Satterthwaite approximation. In the balanced model, we have rti = n, for
• ,a. Define S
2 = Yli=I
/ai an(l M = (n — l){aln52 -Yli=i
Sf }.
i = 1* “
Let C = 1+ (a H- l)/{3a(n- 1)}. Under /fo, the quantity M/C ~ X«-i *
We looked at this test in the context of heterogeneity in section 8.1.3. The
chi-square approximation is less satisfactory if most of the associated degrees of
freedom Ui are less than 5. For such cases, special tables of critical values due
to Pearson and Hartley (1970) are useful. In many cases, an approximate quick
check of the homogeneous variances assumption is possible using Hartley’s test
(for balanced data) which is described below. Bartlett’s test is very sensitive to
the normality assumption. If observations come from a heavy-tailed distribu-
tion, the test would reject Ho too often. We state the next two results without
proof.
Result 9.1.2. Hartley’s test.
Consider a balanced one-factor model. Let
Sf denote the variance of the zth sample, and let 5^
ax and S^
in denote the
maximum and the minimum of these sample variances. The statistic for testing
(9.1.1) is
HT = SlJSlin,

9.2. INFERENCE FOR UNBALANCED ANOVA MODELS
359
which has an Fa >n_i distribution under the null hypothesis.
For a specified level of significance a, Hartley’s test rejects Ho if HT >
If the model is unbalanced, we can run Hartley’s test with the largest
n* as the denominator degree of freedom for the F-distribution; a consequence
is that the probability of Type I error will slightly exceed the nominal level a.
Hartley’s test is simpler than Bartlett’s test. Like the latter, it too is extremely
sensitive to departures from normality, and for this reason it is not very widely
used to test for homogeneity of variances. Levene’s test (Levene, 1960) is an
approximate test for the homogeneity of variances from a populations, and is
somewhat less sensitive to departures from normality than the other two tests.
This is because Levene’s test uses the average of the absolute deviations instead
of mean square deviations as a measure of variation within a group.
As a
consequence of the absence of “squaring” terms, this test is less sensitive to
heavy-tailed distributions.
Ra,n— l,a-
Define the quantity
Result 9.1.3. Levene’s test.
w? = IYtJ
Yi.l /m-,
compute
W
2 = £i=i W f /a
in the balanced case, and
W
2
in the unbalanced case. The statistic for Levene’s test is then derived based on
these expressions in a manner similar to Bartlett’s test.
9.2
Inference for unbalanced ANOVA models
The fixed-effects one-way analysis of variance model that we discussed in Chap-
ter 4 and Chapter 7 has the form
Yij = /i + T% + £ij ,
j = 1, *
* * ,'ft'ii i — 1?
*
* ’
> Q",
where Y\j is the jth observation from the ith level of treatment, /i is the overall
mean, T* is the ith. treatment effect, and the random error components £ij are iid
N (0,a2 ) variables. If m = n, i = 1, • • • , a, we have a balanced model; otherwise,
the model is said to be unbalanced. For a detailed study of unbalanced ANOVA
models, see Searle (1971, 1987). Searle (1971) describes “overparametrized”
models, of which Example 4.1.3 was a special case. The cell means model is the
primary subject of Searle (1987). In this section, a concise discussion of both
these approaches is given, with slightly more emphasis on the overparametrized
model, which seems to be the approach taken in most statistical software.

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
360
Unbalanced data can result either due to a planned design, or due to un-
planned missingness. We focus on the former. All-cells-filled data refers to the
situation where there is at least one observation in each cell. By contrast, some-
cells-empty data correspond to some possibly empty cells. This distinction is
of importance in determining whether inference on certain interaction effects is
possible. Further, with some-cells-empty data, the extent and nature of sparsity
determines which effects are estimable.
Definition 9.2.1. Connectedness.
model, if all differences between levels of a factor are estimable, the data is said
to be connected. Otherwise, the data is disconnected.
For every main effects factor in the
Consider the two-factor additive model (with no interac-
Example 9.2.1.
tion)
Yijk — /i- H" Ti + f3j H“ Sijk i
k — 1, • * * ,Tlij,i — 1» *
* * i
j — 1>
* * * i 6>
where the effects /x, Ti and f3j have been defined earlier, and e^
k are iid iV(0,a2)
variables. Table 9.2.1 shows two sets of data configurations for a — 3, and 6 = 4,
so that there are ab = 12 cells. A symbol “x” in a cell indicates the presence of
data while an empty cell indicates no data.
Table 9.2.1. Connected data (left) and disconnected data (right)
x
x
x
x
X
X
X
X
X
X
X
X
X
If the data are disconnected, some differences of the form T*
TV, z ^
z', or
j3j — (3y, j ^
j' are not estimable. If every such difference can be estimated,
then the cell means pij = \x 4- T% 4- (3j are estimable for all z, j, irrespective of
whether the cell is filled or empty. We leave it to the reader to verify that in
Table 9.2.1, the data on the left is connected, while the data on the right is not
connected.
In a two-way crossed classification,
Definition 9.2.2. ^-connectedness.
data are said to be geometrically connected, or ^-connected when the filled cells
in the a x b grid can be joined by a continuous line consisting only of horizontal
and vertical segments, and which only changes direction in filled cells. All-cells-
filled data are trivially ^
-connected. The data on the left in Table 9.2.1 are
^-connected, while the data on the right are not.
Sometimes, reordering rows or columns of the a x b grid enables us to show
^-connectedness. Weeks and Williams (1964) gave an algebraic characterization
to verify connectedness in an m-factor model; when m — 2, the method reduces
to (/-connectedness. This gives a sufficient (though not necessary) condition for

9.2. INFERENCE FOR UNBALANCED ANOVA MODELS
361
estimability of differences in the levels of main effects of the factors. Suppose
the model has L main effects. Let the array of L subscripts on the sub-most
cell mean be
1, i2, * • *
> V] (for details, see Weeks and Williams, 1964). Two
such z-arrays are called “ nearly identical” if they differ in only a single element.
Connected data are defined as data for which the i-array of every filled sub-most
cell is nearly identical to that of at least one other filled sub-most cell. If data is
disconnected, separate analyses are recommended for each of the separate sets
of data which are connected within themselves.
I11 the following subsections, we describe inference for one-factor and two-
factor unbalanced models. As stressed by Searle (1987), the cell means models
are a natural way for carrying out inference on fixed-effects unbalanced ANOVA
models. A discussion of this approach for the one-way model is given in section
9.2.1. In the rest of this section, we describe higher-order overparametrized
models, and defer the reader to Searle (1987) for a study of the cell means
models for these situations.
9.2.1
One-way cell means model
In Chapter 4 and Chapter 7, we have described modeling and inference for a one-
factor experiment with unequal replication corresponding to the factor levels.
The model that we used there is often referred to as the overparametrized model
because there are (a + 1) parameters, but only a cell means YSearle (1987)
described an alternative model called the cell means model for the one-way
fixed-effects classification as
(9.2.1)
Yij —
p>i T £ij ,
j — 1» * * *
»
i — T *
* *
? &•>
where pi is the population mean of the ith class, i.e., E(Yij ) = pt , and eZj are
assumed to be iid N (0, a2 ) variables. The least squares estimates of the unknown
cell means are obtained by minimizing with respect to /Vs the function
a
Hi
s = EE«r f t)2
1=1J-l
Solving the resulting set of a equations
m
nr
£fc = T,Yij
3=1
3=1
we obtain pi — 1%., with Var( pi ) —
<J 2/rn, i = 1, • * • ,a. It follows that the fitted
values and the residuals are respectively Ytj = pi = YV, and Sij = Y\j - Y*.,
j = l, * * *
and (4.2.29). The ANOVA table coincides with Table 7.2.3 and is useful for
testing the hypothesis HQ : fi\ =
rii, i = 1, • • • ,a, while the forms of SSE and d2 coincide with (4.2.30)
= Ma-
Let UJ = Yli=i
denote a linear function of the cell means,
Result 9.2.1.
where Vs are constants, and let d be a constant.

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
362
1. The b.l.u.e. of to is u) = Yli=i ci^i- with Var(Q ) = a2Yli=i cl/ni-
2. A symmetric 100(1 — a)% confidence interval for UJ is
^
[est.Varfi )] 1/ 2.
3. The statistic for testing H :Yli=i czMz — d is
a;i tj\f
(9.2.2)
— a,a
(taYi. - d )*
i=l
(9.2.3)
F =
52EC<M
i=l
which has an F^
/v-a distribution under i/.
4. The statistic for testing H : X^
=i
= ^i > X^
=i C2,W = d2 is
(52/1 + 51/1- 25/1/2)
(9.2.4)
F =
252(5ip2 - 52)
and has an F2 N-a distribution under H. where
a
a
/1
= 2ci.iyj.- d!, /2-£ C2,iYi.- da,
2=1
2=1
a
a
=
= HCl/ni’ and
91
2=1
2=1
a
5 — ^ ^
Ci^
i/Uj.
2=1
Proof.
The proof of property 1 is a direct consequence of the Gauss-Markov
theorem. Property 2 follows immediately (see section 7.3.1). To prove property
3, we see that the restricted least squares estimates /I*,// of (it , i = 1, • • • ,a are
obtained by minimizing
a
rii
a
E E (Xu -Vi )2 + 2-M Ecm - «0
2=lj= ]
with respect to //*, i — 1, • • • ,a and the Lagrange multiplier A as
Yi.- ACi/ rii,
J2(ciYi.- d ) /2(ci /ni) •
2=1
X
2=1
2=1
It is easy to verify that
SSEH = SSE 4-
{ ZaYi. - d }2
. 2=1
Eci /n«
1
2=1

9.2. INFERENCE FOR UNBALANCED ANOVA MODELS
363
and (9.2.3) follows. Property 4 corresponds to a two- part hypothesis, and its
proof is left as an exercise (see Exercise 9.1).
Two contrasts Yli=i °i,«/•** and E;=i c2,iMi are said to be
orthogonal contrasts when Yli=i ci ,ic2,i / ni = 0- Their b.l.u.e.’s are respectively
Hi=ichiyi' and SiLi c2,jTi., while the covariance between them is given by
:= 0*
Definition 9.2.3.
C72
Higher-order overparametrized models
In this section, we present inference for additive cross-classified models as well
as models with interaction, and for nested or hierarchical models, all in the
unbalanced case. Instead of the cell means models (Searle, 1987), we describe
the overparametrized models.
9.2.2
Two-factor additive models
Suppose an experiment involves Factor A with a levels and Factor B with b
levels. A suitable model for unbalanced data is
Yijk = (A H" T~i H~0j 4“ £ijk i
fc= 1J
*
> ll'ij •> ^ =
’ * * ,
J — 1, * *
* ,5,
(9.2.5)
where p is the overall mean effect, T* is the effect due to the ith level of Factor A,
Pj is the effect due to the jth level of Factor B, and Sijk are iid N (0,a2 ) variables.
Let p = (l+a+6) denote the number of model parameters, n*. = £T=1 niL n j —
Eh mj , N = Eh EU^Y - = EU
Yijk , Y. j . = Eh E
and Y. .. = Yli=i Ej=i Efc= i Y^
k - Let N = {n^} denote the a x b incidence
matrix of cell replications, let Da = diag(ni., • * • ,na.), D& = diag(ra.x, •
• , n.*>),
U = Da - ND^
N', V = Db - N'D-XN, ya = (Yv . y • • • , Fa..)', and y6 =
(Kx.,... ,^
)'.
The normal equations obtained by minimizing Y2i=i E*
ri
~ 0j )2 with respect to p°, rf , i = 1, • • • , a and /?°, j = 1, • • • , b are
N f j° +YniTi +Y1
ntj Yijk
fc=1
E^
X (^-M°-
.7=1
(9.2.6)
Y...
3
=
m.(n° + r?) +^2
i
3
=
j(M° +0j ) +Y,nijri ' i =
(9.2.7)
Yi..
= 1, • • • ,a
(9.2.8)
Yj .
Since the set of equations in (9.2.7) add up to (9.2.6), as do the set of equations
in (9.2.8), the number of LIN normal equations is at most r = (a+ b — 1), which
is the rank of the matrix X'X with the form
( N
ri
r2
X'X = I ri
D„
N
r2
N'
Db

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
364
• • , 71.5), and the other terms were defined
where ri = (n1., • • • , na.), r2 = (n
earlier.
•i. •
Result 9.2.2.
The following rank conditions hold.
Da
N \
N'
D6;-
2. r(X'X) = r- (^
a
= r(V) 4- a.
3. r(X'X) = r^
= r(U) + b.
1. r(X'X) = r
As we mentioned earlier, the first column of X'X is equal to the sum
Proof.
of the next a columns, as well as to the sum of the last b columns. This proves
property 1, since deleting the first row and column of the matrix does not alter
its rank. Properties 2 and 3 follow because D0 and D*> are diagonal,
Da
N
N'
D6
la
0
r(X'X) = r
, and
-N'D-1
Ift
la
r(X'X) = r (£“
N
0
-VN' h) -
D6
We derive reduced normal equations in r’s alone by eliminating the /3’s, a
process which is known in the literature as absorption (see Result 3.2.10). When
a < b, it is easier to eliminate /3’s from the normal equations, rather than the
r’s, in order to obtain least squares solutions of the model parameters.
Provided the data is connected, i.e., r(U) = a — 1, the least
Result 9.2.3.
squares solutions are
(9.2.9)
0,
r,?
(9.2.10)
= 1, • • • ,a, and
i'= 1
$
(9.2.11)
i =!, • • • A
i — 1
where Un
> is the (z, z')th element of U , a g-inverse of U, and
Wi = Yi..- Zbj=dnvYr /n^
i
a.
Proof.
We substitute
M° +0$ = Y.j./n.j -£“=1 rnjT° ,
j =!, • • • ,b

9.2. INFERENCE FOR UNBALANCED ANOVA MODELS
365
from (9.2.8) into (9.2.7) and simplify to get
b
b
n-
r° - J212 nvni'3T° /ni’
i
i'=1 j=1
a
^
Uq' Tj,, say,
i'=l
where Ua> are elements of the matrix U defined by U = Da — ND^
N'. Since
i Wi = 0, and £“ =1
9.2), and the a reduced normal equations in (9.2.12) are not LIN. The case
when r(U) — a - l corresponds to the presence of connected data, and exactly
one additional constraint on the reduced normal equations is required, such as
r£ —
0, or XXi rf = 0. Subject to this constraint, a solution of (9.2.12) is
obtained as (9.2.10). Since r(X'X) = r(U) + b — a — 1 -f 5, we need another
constraint to solve the normal equations. Suppose we set fjP = 0, which is
(9.2.9). Substituting this into the reduced normal equations yields the result in
(9.2.11).
^ ^
/n.j
=!, • • • ,a, i.e.,
3=1
(9.2.12)
Wi
Uu' = 0, for i — 1, • • • ,a, r(U) < a — 1 (see Exercise
If 6 < a, it is simpler to eliminate the r’s from the normal equations, yielding
a set of reduced normal equations
Zj = Vixft +
•+ Vjb0°b ,
j — 1, -
, b,
where Zj = Y.r - E“=i
> 3 = 1,
,q, z =
, Z6)' = (yb -
N'D-Va)', Vy = n.j - E,a
=i
1)
• • , 6.
(9.2.13)
—
St=l
3 ¥= f =
TlijTlijf / fli.,
Result 9.2.4.
1. A linear function
CiT*
estimable if and only if the vector c =
(ci, • • * ,ca)/ is a linear combination of the row vectors of the matrix U.
2. If the data is connected, all contrasts in r’s and all contrasts in /3’s are
estimable.
3. The b.l.u.e. of a contrast c'r is c'U
“ w, with variance c'C“ co-2, where
W =(Wj, * * • ,Wa)'.
4. The b.l.u.e.
of a contrast d' /3 is d'D^
1(yf, — N'U“ w), with variance
d^
D"1 + D^
1N'U-ND^
1)d<r2.
Proof.
we can infer that a linear function c'r is estimable if and only if there exists a
vector u ^
O such that c = Uu, irrespective of whether the data are connected
or not. If the data set is connected, we must have c'l = lUu — 0, since 1 is
orthogonal to the rows of U, so that c'r is a contrast, and is estimable. The
The proof of property 1 is left as an exercise. From property 1,

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
366
proof that contrasts in /3’s are estimable is similar. The form of the b.l.u.e. of
c'r follows from (9.2.10) and the Gauss-Markov theorem. Now,
Var(u'\JT° ) = Var(u'w)
u'Uinr2 = c'ucr2
c'LPccr2,
proving property 3. The proof of property 4 is similar.
Far(c'r0)
We next discuss sums of squares that enable tests of hypotheses. Based on
the least squares solutions via reduction by eliminating the /3’s, the model sum
of squares is
a
b
a
J2 yi-T?+YlYj {Yr /ni ~
(9.2.14)
1=1
j=l
SS( H,T, /?)
i'=l
(9.2.15)
j=i
i= i
with (a + b — 1) d.f. The quantity Yli=i Ti^i ls called the SS due to Factor A,
adjusted for Factor B. Also, SSE is
SSTC - SSU(adj.) - SSB( unadj.)
(9.2.16)
iEEE
-1'2/JV}-it,n/ni- y*m-EM -
2=1 j—
1 k=1
j=1
2=1
(9.2.17)
SSE
b
riijk
The corresponding sums of squares are shown in Table 9.2.2.
Table 9.2.2. ANOVA table after eliminating /3 effects
Source
d.f.
SS
E?=l WiTi
EU Y.j./n.j - Y.../N
Factor A (adj.)
Factor B (unadj.)
Error
Corrected Total
a — 1
6 - 1
N — a — 6 + 1
N -1
SSE
SSTC
Note that the sum of squares due to Factor B is unadjusted. In order to
test r effects, we use Table 9.2.2. However, the same table cannot be used to
test hypotheses on /3 effects. This situation is peculiar to the unbalanced case.
Unlike the balanced case, the unbalanced model is said to be non-orthogonal,
i.e., the b.l.u.e.’s of contrasts of r’s and (5's are not orthogonal. Table 9.2.3
shows the form of 55^(adj.), and two tests of hypotheses are shown in Example
9.2.2 for connected data.

9.2. INFERENCE FOR UNBALANCED ANOVA MODELS
367
Table 9.2.3. ANOVA table after eliminating r effects
Source
d.f.
SS
E“=i Y*.-Y
'*./ N
{T.U Yi /n j -Y.../N }
+ EUWir?- (EUYl -Y* /N )
Factor A (unadj.)
Factor B (adj.)
a — 1
6- 1
SSE
Error
Corrected Total
N — a — 6 + 1
N — 1
SSTC
Example 9.2.2.
testable hypothesis HA : r\ = • • • = ra is
F = {£?=1If wy(a-1)} /{SSE/ ( N - a- b+ 1)}
which has an Fa-i^-a-b+i distribution under HA• Consider a test of HB :
Pi =
* • * = 0b- The numerator sum of squares in the F-statistic is
When the data is connected, the test statistic for the
SSB{adj.) =
SSTC - SSA(unadj.) - SSE
{EY r /ni - Y-/N}+E
- (E
- Y-
2/N}
j=i
Z=1
z=1
with (6 — 1) d.f. The test statistic is
F = {55B(adj.)/(6-1)}/ { SSE/ {N - a- b + 1)}
which follows an F*>_ i,N-a-6+i distribution under HB •
Numerical Example 9.1.
Consider the following example (Montgomery, 1991) where a chemical engi-
neer believes that the time of reaction for a chemical process is a function of
the type of catalyst employed. For four catalysts under investigation, the ex-
periment consists of selecting a batch of raw material, loading the pilot plant,
applying each catalyst in a separate run of the pilot plant and observing the
reaction time. Believing that variations in the batches of raw material may
affect the performance of the catalysts, the engineer decides to use the batches
of raw material as blocks, although each block is large enough to permit only
three catalysts to be run. We describe a suitable procedure to compare the
treatments in this unbalanced design, i.e., a balanced incomplete block design
(BIBD). Information is not available for observation 3 in catalyst 1, observation
1 in catalyst 2, observation 4 in catalyst 3 and observation 2 in catalyst 4. There
are only N = 12 observations used for this analysis. The ANOVA table and
F-test are shown below.
Fixed-effects two-factor additive model.

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
368
ANOVA table for Numerical Example 9.1.
d.f.
55
M S
Source
F-value
Pr > F
Model
6
Block
Catalyst
3
Error
Corrected
Total
77.750
12.958
19.94
3
55.000
18.333
28.21
32.750
7.583
11.67
5
3.250
0.650
0.0024
0.0015
0.0107
11
81.000
Both effects due to the catalyst and due to blocks are significant, as evidenced
by the F-statistics.
A
Two-factor models with interaction
Consider the model
(9.2.18)
Y% jk — /i T 'T’i H" ftj T'Jij T £% jk >
^ — 1? ' * * i
j —
* ’ ' i
with riij > 1 observations in each of / filled cells (/ < ab); p is the overall mean
effect, Ti is the effect due to the zth level of Factor A, 0j is the effect due to the
jth level of Factor B, 7ij is the effect due to interaction between the 2th level
of Factor A and the jth level of Factor B, and £ijk are iid N(0, a2 ) variables.
The number of model parameters is p = l + a + 6-b/ (7^
’s correspond to the /
filled cells), which we attempt to estimate based on / observed cell means Y ij..
We use the matrix notation introduced in Chapter 4 and Chapter 7. It may
be verified that r(X'X) for this model is /. In order to solve the p normal
equations in the least squares approach, we set p— f ~ 1T a+ b elements of the
overall parameter vector equal to zero. It is simplest to set /2°, rf , i = 1, • *
and $ , j = 1
,6 equal to zero to obtain the reduced normal equations
n%3lij = Yi:i . ,
corresponding to this solution is a matrix G consisting of an / x / lower block-
diagonal matrix D{l/n^}, and zeroes elsewhere. The proof of the next result
is left as an exercise.
* ,a
with solution 7^
= Yij
for the / filled cells. The g-inverse
Consider a model with some-cells-empty data. The following
Result 9.2.5.
functions of (3 are estimable.
1. A cell mean pij = p + T* + (3j + 7^
corresponding to a filled cell, i.e., for
riij ^
0. Its b.l.u.e. is pij — 7^
= Yij., with Var( p,ij ) = a2/n^. Further,
Cov( pij , Jli' j' ) = 0.
2. Any linear function of estimable pij’s is estimable. For example, if the
(l, 2)th cell and (2, 2)th cell are filled, pn and P22 are estimable, and so
is P12 - P22 = Ti — r2 + (712- 722)-

9.2.
INFERENCE FOR UNBALANCED ANOVA MODELS
369
3. For i
i' , the function r* — T*
/ -f
is estimable provided
and Ci' j = 0 when nyj — 0. Its b.l.u.e. is
with varianceSj=i(cij/nu + cffj / ni' j )&2
-
cij (Pj + lij )
YZj— 1 ci' j(0j
*+
* 7i'j)
Cjj =^
j=i
= 1? and Cij ~ 0 when mj = 0
j=l cijYij < “ !C?
i=i
j=i
Ci' jY i' j .
3=1
dij (ri “ I"7ij)
5Zi=l dij' {Ti A *Yij' )
is estimable provided Y2i=i dij — Y2i=i diy = 1, and dZj — 0 when mj = 0
and dij > — 0 when my — 0. Its b.l.u.e. is Y2i=I dijYij - ~ Y2i=i diyYiy .
with variance Yli=i (d>ij / nij A d%jt / my )#2
•
It is clear that with some-cells-empty data, the pattern of nonzero nzj’s
determines which functions are estimable (see Exercise 9.6). Also, differences
such as Ti - Tf , i ^
i', or ftj - /3y, j ^
j' are not estimable. For example,
f-lij
fJ'i 1 j — (M A TZ A (3j A 7ij
') “ (M “h 7
*
2' 4" (3j ~h 7i’ j ) “
7
*
2
Tj' “ j~ 7zj
7i' ji
and we cannot eliminate the 7’s from the final expression. In other words, no
estimable function of mj's is possible which involves only differences such as
Ti - Tv or (3j - fiy , without the 7^’s. Differences between levels of Factor A can
be estimated only in the presence of average effects due to Factor B and the
interaction. Any testable hypothesis will involve linear functions of mj's-
4. For j^
jr , the function (3j — fty AY2i=i
Consider a model with all-cells-filled data.
Result 9.2.6.
1. For all i ± if , the test statistic for
b
b
H \ T i - T i’ A [^
7ij -
= 0
(9.2.19)
3= 1
3=1
is
F =
-Y«r ) }2 /
+ V^'i)?2}
(9.2.20)
3-1
3=1
which has an F\,/_a_6+i-distribution under H .
2. The test statistic for
b
H : Ti A
7ij / b equal , i
(9.2.21)
= 1, -
• , a
7=1
is
1
(9.2.22)
F = (a -1)52 Lt=l
i=l
j=l
where Ui = (£j=i Y^
. )2 , ht =
l/n^, and V) =^
j=i

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
370
Property 1 is a special case of property 3 in Result 9.2.5 with
Proof.
°ij — °i' j ~ 1/fe* Its b.l.u.e. is
EU^r - Y i' j )}/ b
with variance
cr2 [l/nij - l /nifj }/ b,
from which (9.2.20) follows directly. The proof of property 2 is left as an exercise.
Observe that (9.2.20) is also the test statistic for H : r*- Ty = 0, to which
the hypothesis (9.2.19) reduces under the restriction
i 7ij = 0 for all i =
the hypothesis (9.2.21) reduces to #
1, • • • ,a in the model. Likewise,
• • • = ra under this restriction, and is tested by (9.2.22).
: T\ =
Nested or hierarchical models
We introduced this model in Example 4.2.7, while in Example 7.4.5, we de-
scribed inference in the balanced case. With unbalanced data, the nested model
is written as
(9.2.23)
Y%jk — /i H~ T-t -j- &j( i } + £ijki
k = 1, • • • , riij, j — 1, • • • ,
i = 1, • • • ,a. The normal equations are derived
by minimizing £“=1£*=1T!k=i(Yijk - H° - if - /3°(t))2 with respect to the
parameters as
+EE
i= l j — 1
f +EMw)- i
= Nn°+E"*-7^
K..
i=l
+ rii.r?
Yi..
3=1
Yij.
=
riij(/i° 4- T?/??(i)), j = 1, • • • , 6», i =
where n*. = ^/=1. nij • By adding the last set of equations over j, we get the
second set, and by adding them over i and j, we get the first equation. Let
b. =
bi- Only b. of the 1 4
* a 4- b. normal equations are LIN, on which we
impose the constraints fi° = 0, and rf = 0, i = 1, • • • , a. The solutions to the
normal equations subject to these constraints are
Pj(i ) ~ Yij./ Hi j = Yij.,
The corresponding g-inverse is
, a,
(9.2.24)
j — I >
* * * ,bi,i — 1?
* * *
G = °
0
0
D{1/ nij } )'

9.3. ANALYSIS OF COVARIANCE
371
It is routine to verify that // 4 r* 4
is estimable with b.l.u.e.
Functions of the form
- fij' ( i ) , j / f are also estimable, with b.l.u.e.
Yij.-Yif ..
In the framework of a nested sequence of hypotheses, we write
SH0 = {Yijk = /x 4- r» +
4
r° = 0,i =1, • • • ,a, /x° = 0}
3
= {Yijk = M + Ti + /??(z) +
rj° = 0, fjP = 0}
=
{
"^ijk = /X 4
4 Sijlc},
3 <S//2
M 4 7
"
^ 4
4 Sijkt 7~i
0,i
1, * • • , U, /i — 0,
0j (i ) = 0, Ti equal ,i = 1, - - - ,a}
=
M 4 £ij/c}
3 «SH3{Fijfe = £ijk}•
Here, dim(SH0 ) = b., dim(«S//1 ) = a, dim(SH 2 ) = 1, and dim(«S//3) = 0. The
fitted values are Yijk = *V, and SSR{ HQ ) = y'Hoy//0 =
SST = E,EjEfc(y«t -i7-)2- Let SSM = NY
2
the sums of squares.
y«
ZJ..
... Table 9.2.4 summarizes
Table 9.2.4. ANOVA table for nested effects model
Source
SS
d.f.
b. - 1
SSR( Ho ) - SSM
N - b.
SSE
SSTC
Model
Error
Corrected Total
N — 1
Analysis of covariance
9.3
Consider an example where we study the effect of three different diets on the
weights of rats. The experiment consists of assigning rats randomly to the diets
and maintaining them on the same diet for a period of 2 months. The response
variable is the weight of a rat at the end of the two month period. A one-way
ANOVA model enables us to study the effect of the treatment (diet) on the
weight. Note that it is also reasonable to suppose that the weight of a rat after
being on a diet for two months is correlated with its initial weight before the
diet application. An analysis of covariance (ANACOVA) model is a combina-
tion of ANOVA and regression, which enables us to study the effect of diet on
the final weight of the rats, adjusting for their initial weight. Supposing that
the initial weight is linearly related to the final weight; the elimination of this
linear effect should result in a smaller MSE of fit. We call the initial weight a
covariate or a concomitant variable. Note that as an alternative, we could have
blocked the rats into different groups based on their initial weight and used a
two-way ANOVA model, thereby converting a continuous variable (weight) into

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
372
a class variable. However, use of the ANACOVA model improves the preci-
sion of treatment comparisons. In general, we may use k continuous covariates
Xi, -
* * , X*. Analysis of covariance tests for differences in treatment effects,
assuming a constant regression relation among groups. Of course, we must first
test whether or not the regression coefficients are similar in the different groups.
Regression lines will be parallel when there is a single covariate X, and there is
no interaction between X and factor levels. Dissimilar coefficients could reflect
the presence of an interaction between treatment groups and covariates.
A general formulation of the ANACOVA model is
y = Xr + Z(i+ e,
where y is an AT-dimensional vector, X is an N xpdesign matrix with rank(X) =
r < p, r is a p-dimensional vector of fixed-effects parameters, Z is an N x q
regression matrix with rank(Z) = q, /3 is a ^-dimensional vector of regression
parameters, the columns of X are linearly independent of the columns of Z,
and e has an 7V-variate normal distribution with mean vector 0 and covariance
matrix <J 2IN - We can rewrite the model in (9.3.1) as
(9.3.1)
(X
Z) and 7 =Q
(9.3.2)
y = W7 + £,
where W =
Result 9.3.1.
The least squares solutions for /3 and r are
=
[Z'(I- X(X,X)-X')Z]
_1Z'(I- X(X'X)-X,)y and
(9.3.3)
=
(X'XJ-X'y- (X'XJ-X'ZjS.
Proof.
The normal equations have the form
XX
X'Z
Z'X zz
Using results on g-inverses of partitioned matrices, we see that
XX
X'Z
Z'X
Z'Z
which upon simplification, yields the required result, with /3° = /3 (unique).
For an alternate approach, write the normal equations as
X'XT0 + X'Z/?0 = X'y
Z'XT0 + Z'Z/?0 = Z'y.
From (9.3.5), r° = (X'X)- [X'y - X'Z/?0]. In a model without the covariate,
i.e., y = Xr + e, let r* denote a least squares solution of r. Then,
T° — T* — (X'X)-X'Z/?0.
Substituting (9.3.7) into (9.3.6) and solving for /?°, we get
/?° = {Z'(I- X(X'X)-X')Z}-Z'(I- X(X'X)-X')y.
Substituting (9.3.8) back into (9.3.7), we get
0
T°
(9.3.4)
X'y
Z'y
X'y
Z'y
(9.3.5)
(9.3.6)
(9.3.7)
(9.3.8)

9.3. ANALYSIS OF COVARIANCE
373
T° = (X/X)-X'y- (X'X)-X/Z{Z'(I-X(X,X)-X')Z}- Z'(I-X(X,X)-X')y.
Let P = X(X'X) X' as before, and Q = I- P, and recall from Result 3.1.8
that X(X'X)“ X' is invariant to (X'X)-. Then,
0° = (Z'QZ)-Z'Qy.
Now, Z'QZ = Z'Q'QZ = (QZ)
/(QZ), so that
r(Z'QZ)- r(QZ) = <?,
(9.3.9)
(see Exercise 9.12). Hence, Z'QZ is nonsingular, and the regression parameter
estimate is /3 = (Z'QZ)_1Z'Qy. It follows that r° = (X'X) “ {X'y - X' Z0}.
Note that whereas 0 is the unique estimator of 0, we can think of r° as one
of infinitely many solutions to the normal equations resulting from minimizing
the sum of squares in the least squares setup. We next describe inference using
the framework of a nested sequence of hypotheses (see section 7.4.1). First, note
that At(QZ) = A4x(X) H.M(X, Z) (see Exercise 9.17). This is illustrated in
Figure 9.3.1.
y
t
///
//
/
/
*M(Q2 )
/
/
/
/
M(Z)
4
/
/
/
/ /
>Ho
/
/
ASH,
/
/
M(X)
\
MX,Z)
Figure 9.3.1. Geometry of ANACOVA.
We can write the decomposition of the total sum of squares as
SST = SS(p) + SS(r\ fj.) + SS(0\T ) + SSE,
(9.3.10)

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
374
where
y'y > with N = an d.f.
SS( fi )
=
NY
2 with 1 d.f.
SS(r\ p) =
[(X'X)_
X,y],(X'X)[(X,X)_
X'y] — NY
2, with r-1 d.f.
SS(/3\T ) =
p' Z' ( I - X ( X' X )- X' )Z p,
with q d.f.
y' y-SS( fi ) - SS(r\ fj,) + SS(/3|r),
with N - r- q d.f.
SST
SSE
To test H0,i : K'r = m, (provided this is a testable hypothesis), we use the test
statistic
F = {Q/r( K' ) } /{ SSE/ ( N - r - q)}
under i/o,n where
which has an iV(K') >w-r-q
(K;7 - m)
/(K'GK)
_ 1(K
/7- m), and
XX
X'Z
ZX
Z'Z
Q
T°s L
G =
7
0
To test H0 ,2 :0 — 0, we use the test statistic
F = {SS(/?|r)M / { SSE/ ( N - r - q)} ,
which has an Fo-
under Ho ,2’
r-q
Example 9.3.1.
the statistical analysis for ANACOVA involving one factor, Factor A, and one
continuous covariate X . Given data (Yij, Xij), the experimenter first does some
preliminary analysis to verify the equality of the slope of the regression of Y on
X in the a groups. If this is indeed so, the model is
One-factor model with one covariate.
We discuss
Y^ — /i + Ti +0( X { j
(9.3.11)
X ..)+ Sij, i =
,a, j =!, • • • , n,
where 0 is the common slope parameter and £ij are iid N (0, <r2) variables. If
we assume that T* correspond to fixed effects, with
ri — 0, and that Xij
are unaffected by the treatments, our main interest is in testing for equality of
the effects due to the levels of the factor, eliminating the effect of the covariate
X on the response. Before we do this, we would like to test whether 0 = 0.
When /3 = 0, the reduction in MSE is likely to be very small, and in this case,
the data is better analyzed as a simpler ANOVA model. If we decide that 0 is
significantly different from zero, we would use each of the a regression lines to
estimate the mean value of Y for a given treatment level and for a given value of
X and also to estimate differences among the T* for any value of the covariate.

9.3. ANALYSIS OF COVARIANCE
375
The following notation is useful.
a
n
syy = ££(*«- y--)2
>
2=1 j=l
a
n
££(*« -*-)2
Sxx
2=l j=l
a
n
s*y
=
-* )(>« - y-),
i=ij=i
a
TYY
= E(y‘- y -)2
2=1
a
?xx
2= 1
a
TXY
2=1
a
n
SVy “ Tyy =
- Yj.)2
2=1 j=l
a
n
Exx
=
Sxx ~ Txx =
- X*.)2, and
2=1 j=l
a
n
£*y
=
S'xy - TXy =
- X,)(^
'-Yv ).
2=1J=1
In the absence of the covariate, we would have Sxy = Sxx — £xy = Exx = 0.
In this case, the total sum of squares, treatment sum of squares and error sum
of squares would respectively be Syy, Tyy, and Eyy. In the presence of a
covariate however, we must adjust these quantities for the regression of Y on
Eyy
X.
The least squares solutions of the parameters in the
full model are
M°
T? = (Yv - Y .. ) - /3° ( Xi. - X .. ),
< =!, - -
and a2 = MSE =
Y..
• ,a
E\Y
SSE
/3°
J
a(n — 1) — 1
Exx
where
SSE = Eyy - (Exy )2/ Exx

CHAPTER 9.
FIXED EFFECTS LINEAR MODELS
376
is the ” adjusted” error sum of squares with a(n — 1) — 1 d.f.
If there were no effects due to the levels of Factor A, i.e., we reject the null
hypothesis HQ : T\ = • • • = ra, then the reduced model is
Yij — n+ ft( Xij — X ..) + £ij
i
1 y
iCL, j
1 •>
, 71,
and the least squares estimates of the parameters in this reduced model are
£ = Y.., and (3 = SXY / SXX •
Under this reduced model, the a parallel regression lines coincide. The error
sum of squares in this reduced model is
SSE' = SYY - (SXY )2/ SXX with (an- 2) d.f.
Note that ( SXY )2/ SXX is the reduction in SYY through the regression of Y on
X, and is the regression sum of squares with 1 d.f. The difference SSE' — SSE
is a reduction in sum of squares due to r effects, and has (a — 1) d.f. Yet another
reduced model is
Yij = H + Ti + £ij ,
i =!, * • • , a, j =!, - • • , n.
The least squares estimates of the “adjusted treatment means”
fi+Ti under this
model are
Adj.Yi. = Yi.- j)( Xi. - X ..),
« =!, •
* , a
where f3 = EXY / EXX - The standard error of this adjusted treatment mean is
s.e.(Adj.YY ) = [MSE{l /n+ (X,.-X..)2/^x}]1/2.
The test statistic for HQ : r^’s equal, i = 1, • • • , a is
F0 = {(SS£'-SS£»/(a -1)} / { SSE/ [a(n-1)- 1]} ~ F(a_1),a(n_
1)_1
under i/o- We can represent the ANACOVA as an adjusted ANOVA, as shown
in the following table.
Table 9.3.1. Adjusted ANOVA table for an ANACOVA model
Source
d.f.
SS
MS
( SXYV / SXX
(SXYY / SXX
SSE' - SSE
[SSE' - SSE ] / {a - 1)
SSE/ [a(n-1) — 1]
Regression
1
Treatment
a — 1
Error
Total
a{n- 1) - 1
SSE
SYY
an - 1

9.3. ANALYSIS OF COVARIANCE
377
The F-statistic is F = {(SSE' — SSE )/ (a — 1)} / MSE.
We use data from Littell, Freund and Spector
Numerical Example 9.2.
(1991). Four bags, each containing 10 oysters were randomly placed in each of
five locations (TRT) in the cooling water canal of a power-generating plant. Two
locations are in the intake canal, and two locations are in the discharge canal.
In each case, one is at the top and the other at the bottom. One mid-depth
location is in the shallow portion. Suppose A and B denote the intake locations,
bottom and top respectively; C and D denote the discharge locations, top and
bottom; let E denote the mid-depth location. Water in the discharge canal has
higher temperature than water in the intake canal, while E is an overall control.
The initial weight (A) of the oysters in each bag are recorded, and the final
weight (F) after one month. The results from an ANACOVA analysis with
TRT as the fixed-effects factor and X as the continuous covariate are shown
below. R2 was 0.988, and a was 0.549.
Least squares estimates for Numerical Example 9.2.
t-value
Pr > |£|
Parameter
Estimate
s.e.
2.43
0.029
0.0001
0.678
0.579
3.85
0.002
2.35
0.034
Intercept
2.495
1.028
1.083
0.048
X
22.75
-0.244
0.577
-0.42
-0.280
0.493
-0.57
1.655
0.429
1.107
0.472
0.000
A
B
C
D
E
ANOVA Table I
d.f.
SS
MS
F-value
Pr > F
Source
Model
Error
Corrected
Total
5
354.447
70.889
235.05
0.0001
14
4.222
0.302
19
358.670
ANOVA Table II
Source
d.f.
Unadj. SS
MS
F-value
Pr > F
49.602
164.47
0.0001
156.040
517.38
0.0001
TRT
4
198.407
1
156.040
X
Source
d.f.
Adj. SS
MS
F-value
Pr > F
3.022
10.02
0.0005
156.040
517.38
0.0001
TRT
4
12.089
1
156.040
X
From ANOVA Table II, we see that that the adjusted treatment SS is much
smaller than the unadjusted SS. We can argue that the power of the test for

CHAPTER 9.
FIXED EFFECTS LINEAR MODELS
378
treatment differences is increased by the inclusion of X in the model, because
most of the error in the analysis is due to variation in X . The next table shows
the adjusted treatment means Yi - @ { Xi . - X .. ).
Adjusted treatment means
TRT
Unadj. means
Adj. means
s.e.
34.475
30.153
30.117
32.052
1
0.334
0.283
0.280
0.276
0.362
31.650
30.850
32.225
25.025
2
3
4
31.504
30.398
5
The inclusion of initial weight as a covariate adjusts for the biased allocation of
oysters of differential weights to the locations.
Nonparametric procedures
9.4
In this section, we describe nonparametric procedures for carrying out infer-
ence in designed experiments, mostly based on the use of the rank transform in
classical tests. The use of the rank transform consists of replacing all the obser-
vations in a classical test statistic by their ranks in the entire data set (Conover
and Iman, 1981). Akritas (1990, 1991, 1993) and Thompson (1991) have argued
however that the rank transform procedure is inappropriate for many common
hypotheses. Akritas and Arnold (1994) showed in the context of multivariate
repeated measures designs that the rank transform procedure is always appro-
priate for testing fully nonparametric hypotheses, which are defined in terms of
distribution functions.
The one-factor robust parametric model has the form
Example 9.4.1.
Yij = fi + n 4- eij, » = o
(9.4.1)
2=1
where Sij’s have common cdf F(£), which could be chosen from a family of
distributions in section 5.5.3, say. In general, it may be assumed that Yij are
from a location family of distributions. The hypothesis of interest is H : T\ ~
The well known Kruskal-Wallis procedure, which is discussed in section
9.4.1 is a rank test based on the robust parametric model.
Given n* observations Yij corresponding to the ith level of Factor A, i =
1, • • • , a, the nonparametric one-factor model only assumes that YZj has cdf F*,
where Fi = M (u) + Ai (u), with
= 0. For instance, M (u) = F .(u),
and Ai (u) = Fi (u) - F. (u), where we compute F .(u) = J2i^i(u)/a-
the
nonparametric model, the distributional assumption is very general; there is no
• • = Ta -

9.4. NONPARAMETRIC PROCEDURES
379
assumption that Yij are from a location family. The hypothesis of interest is
Ai = • • • = Aa, i.e., all Fj’s are equal.
Example 9.4.2.
levels and Factor B at b levels. The robust parametric model is
Yijk — n
Ti T (3j 4* 'yij H- £ijk >
where r*, (3j and jij satisfy the usual constraints (see Example 4.2.6), and we
assume that e% jk have a common cdf F(.). Friedman’s test is a rank test which
is widely used in this framework.
The nonparametric model assumes that there are
observations Yij cor-
responding to the zth level of Factor A and the jth level of Factor B, and that
Y^
are distributed with cdf Fij. Let
Consider an experiment with two factors, Factor A at a
(9.4.2)
F^ = M (u ) 4- Al (u) 4- Bj (u) 4- Cij (u),
where Y,iAi {u ) =
Ej Bj (u) = 0, Y,iCij(u) = 0, and J2jCij(u) = 0; let
M (u ) = F..(u), Aj(u) = Fj.(u) — F..(tz), F^
(u) = F.j (u) — F..(u), and Cij(u) =
Fij (u)— Fi.(u)— F.j (u)+F..(u ), The hypothesisCij (u) = 0 is the nonparametric
hypothesis of no interaction, and is equivalent to hypothesizing that Fij (u ) is
a mixture of two distributions, one depending on i and the other on j, and
having the same mixing parameter for all (z, j ). The nonparametric version
of the hypothesis for testing whether all the effects due to Factor A are equal
(averaged over b levels of interactions) is the test H : Ai(u) 4- Cij (u ) = 0, i.e.,
F^
(u) does not depend on i.
(9.4.3)
9.4.1
Kruskal-Wallis procedure
In situations where the normality assumption is not justified, the Kruskal-Wallis
test (Kruskal and Wallis, 1952) is used as an alternative to the F-test in the one-
way model. This is an extension of the well-known rank sum test to one-factor
ANOVA problems. Suppose independent random samples of sizes n i, - - -
are drawn from a univariate populations with unknown cdf’s Ft, i = 1, • • • ,a.
We describe a test of the null hypothesis H : F\ —
• •= Fa versus alternatives
of the form F*(ar) = F( x — 0i ), for all x, i = 1, • • • ,a, with all Oi s not equal.
The observationsY^
, j = 1, • • • , n^, i = 1, •
• ,a are first ranked in ascending
order, and Yij is replaced by its rank Rij in the overall sample. If there are ties,
i.e., two or more observations are equal, each of the tied observations is given
the average of the ranks for which it is tied. Suppose there are G groups of
ties. Let tg be the number of tied observations in group #, g = 1, • • • ,G. Let
Ri = Ejii Rij denote the sum of the ranks in the zth sample, and i?*. = Ri./ rii.
Clearly,
R{. = N ( N 4- 1)/2.
Result 9.5.1.
The Kruskal-Wallis test statistic is
na
KW = {\2N ~\N + l)"1
Rl /m ~ 3(JV + 1) }/ A
(9.4.4)
1=1

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
380
where N = Yli=i ni > an(^
G
(9.4.5)
A= l
- N ).
9=1
If the a populations are identical, and rii s are not too small, KW has a Xa-i
distribution. With no ties, we set A = 1 in (9.4.4).
Proof.
as values the iV! permutations of (1, • • • , N ) with equal probability under if0-
The test statistic corresponding to the standard analysis of variance (see Ex-
ample 7.2.7) based on the ranks R^
’s has the form
The iV-dimensional vector (Rn, * * *
> -Rim , * • • , Rai > • * * , Rana ) takes
ndRi- ~ ( N + l)/2}2/{E -=1 E%i (Rv ~ Ri )2}
E“=i
and is a monotonically increasing function of
rti
E
~ ( N +vw
2 / (EE^- ( N + 1)/2)2>
•
(9.4.6)
i—\ j— 1
The denominator in (9.4.6) is a constant because of the use of ranks, and let B
denote its numerator. First, we divide B by (N 2 — 1)/12, which is the variance
of the uniform distribution on the integers {1, 2, • • • , N }, and we multiply the
resultant expression by the factor ( N - 1)/ N (to yield a - 1 in expectation
under H ). An algebraic simplification yields the expression in the numerator of
(9.4.4). The term A in the denominator is an adjustment for ties.
We show that the statistic has an approximate x2 distribution. Suppose
a = 3. Provided n*’s are not too small, the joint distribution of Rand Rj. is
approximately bivariate normal with
i= i
Hj — ( N A l)/2,
( N + l)(iV — rii ) j\2ni,G2
j = ( N -f- 1)( N — nj )/ l2rij ) and
-{rn/ ( N -ndy^ dnj /iN - nj )}1/ 2.
Note that
= ( N +1)( N - ni )/\2rii is the variance of the mean of n* numbers
drawn at random without replacement from N consecutive integers, while p is
the correlation between means of samples of sizes n* and rij when all rii + nj
are drawn at random without replacement from a population of N elements. It
is well known that -2 times the exponent of this bivariate normal distribution
has a x\ distribution (Mood, 1950, sec. 10.2). Since rii + rij -f n/c = TV, and
riiRi + rij Rj + rikRk = N ( N -f- l)/2, it can be shown that the value of the
bivariate normal exponent will be the same whichever pair of samples is used
in its computation, and this value will be KW in (9.4.4). In general, with a
samples, the mean ranks for any (a- 1) of them have an approximate (a - 1)-
variate normal distribution, provided that rii s are not too small. The exponent
Pi
(9.4.7)
P

381
Exercises
of this distribution involves means, variances and correlations of the form shown
in (9.4.7). It will have the same value irrespective of the set of (a — 1) samples
used, and will yield KW when multiplied by — 2, and have an approximate
Xa-i distribution. Kruskal (1952) provides a formal proof of the asymptotic
distribution of KW.
If KW > xl
we reject the null hypothesis at level of significance a. The
Kruskal-Wallis test is equivalent to the usual F-test applied to the ranks. It has
been suggested that when the normality assumption is suspect, we implement
the usual analysis of variance procedure both on Yij and on Rij. If the results
are similar, we would conclude that the assumptions of the usual F-test are
satisfied.
However, if the results are different, we would suspect that some
assumption has been violated and we prefer to use the Kruskal-Wallis test.
— l,a >
9.4.2
Friedman’s procedure
This procedure relates to the two-way classification. Consider the two-factor
fixed-effects additive model Y
~ fi + Ti + /3j + £ij, i = 1, • • • , a, j = 1, • • • , 6.
That is, suppose we have balanced data with one observation in each of the ab
cells, and N = ab. Assume that the errors eij are independently distributed
random variables from some continuous population. Suppose we wish to test
H : T\ =
• • • = ra. Within each block, rank the a observations in ascending
order. Let
= r(YlJ ). Set Ri =Y^
}=\
= Ri/n, and R.. = (a + l)/2.
The test statistic is
S =
l2n^2( Ri. - R..)2 /a(a+ 1)
(9.4.8)
2=1
=
{12^
Rf /na(a T 1)}- 3n(a + 1).
(9.4.9)
2=1
oo, we can show that the statistic S has a Xa-i distribution, so that
As n
critical values from this distribution determine the rejection region. This test
arises naturally if we apply the usual F-statistic in the two-factor additive model
to ranks instead of the actual responses. For details, see Conover (1980) and
Hollander and Wolfe (1973).
We do not discuss nonparametric tests for other designs here. The reader is
referred to Akritas and Arnold (1994) and references therein for a discussion of
fully nonparametric hypotheses for general factorial designs.
Exercises
9.1. Prove property 4 of Result 9.2.1.
9.2. In the model (9.2.5), show that Ula = 0, where U = Da — ND^
N'.
Hence argue that r(U) < a — 1.

CHAPTER 9. FIXED EFFECTS LINEAR MODELS
382
9.3.
(a) Prove property 1 of Result 9.2.4.
(b) In the model (9.2.5), show that a linear function
dipj is es-
timable if and only if the vector d = (d\, • • • , da )' is a linear combi-
nation of the row vectors of the matrix V. Hence show that if the
data is connected, a necessary and sufficient condition for a linear
function of /?’s to be estimable is that it is a contrast in /3’s.
j=i
9.4. In model (9.2.5), show the following.
(a) Var( ya ) = a2Da, Var( yb ) = <T2D6, and Cov( ya, yb ) = <r2N.
(b) Var(w) = cr2U, Var{z) =
<J2V, and Cov(w, z) = — a2UD“ 1N.
(c) Cov(w, yb) = 0 = Cov(z,ya).
9.5. Prove Result 9.2.5.
9.6. In (9.2.18), verify whether the following functions are estimable.
TlijTli' j ITl . j
^
Ti'
nijni' j/n- j ]l'i' j j i = 1> ’ *
(a) [n,-£$=i n? /n.j]rj-E*,'Ej=i
+ E?=iK
' - n^
/n,!^-E“#i,E5=I
(b) [n.,- - E“=i
-EWE =I
+ Ei=l[nij ~ nij / ni- ]lij ~ Ej^j'Ej=l n i j n i j’
> 3
9.7. Suppose a = 3 and 6 = 4 in (9.2.18), and suppose cells (1, 2), (2,3), (2,4)
and (3,1) are empty. Verify whether the following functions are estimable:
(a) r2 - r3 + 722- 732,
(b) T\ - T3 + 712- 732.
9.8. Prove property 2 of Result 9.2.6.
• ,a.
nijTiij' / rii ](3y
9.9. In the model Yijk = [i 4- n + /3j + 7^ + eijk > fc = 1, • • • ,
i = 1, • •
, u,
j = 1, • • • , 6, suppose riij = rii.n.j/ N. Given that
c are iid iV(0, (72)
variables, obtain a statistic for testing H : 7^ = 0 V i, j.
9.10. In the two-way unbalanced nested model, suppose we impose the restric-
tions
= 0 and rf = 0, i = 1, • • • ,a. Obtain the least squares estimates
of
and write down the corresponding g-inverse of X'X.
9.11. In the two-way unbalanced nested model, show that the function
M + Tj +^2 j— 1
U )ij = 1
is estimable, with b.l.u.e. given by
WijYij..
9.12. Verify equation (9.3.9).
9.13. Let Yij =
4- 71Zy + 72WJy +
i = 1, • • • , a, j = 1, • • • , 6, and let Sij
be iid N (0,a2 ) variables.

Exercises
383
(a) Derive the least squares estimate of 71, and show that it is unbiased.
(b) Find the variance-covariance matrix of the least squares estimate of
7 = (71.72)'.
(c) Under what conditions are 71 and 72 statistically independent?
9.14. Consider the model Yij = Hi+'yiXij+ £ij, j = 1, • • • , 6, i = 1, 2, where eij
are iid iV(0,a2 ) variables. Set this up as an ANACOVA model, and derive
the F-statistic for testing the hypothesis 71 = 72. Relate this statistic to
the usual ^-statistic for testing whether two straight lines are parallel.
9.15. Consider the model Yij = [i + r*
-F £ij, j = 1, • • • , 71*, i = 1, • • • ,a, where
£ij are iid N (0,a2 ) variables. Suppose we observe a continuous variate Z*,
i = 1, • • • , a (for each j = 1,
• , ni ).
(a) Find the sum of squares due to regression, viz., SS( fio, /3\) in the
model Y^ = /3Q + (3i ( Zi - Z ) -1- u%j , where we define
Z = (Z)i=l niZi )/ (Ei=l ni).
(b) Compare 55(/?o, /3i) with 55(/x, ri, * * * ,ra), which is the “ usual”
model sum of squares in the one-way fixed-effects ANOVA model.
9.16. Consider the model
= fii3 + 7ijZijk + £ijk, i = 1, • • • ,a, j =1, • • • , &,
and k = 1, • • • ,c, where
are iid JV(0, cr2) variables. Obtain a test
statistic for H : 7^ = 7, i = 1, • • • ,a, j = 1, • • • ,6.
9.17. In the ANACOVA setup, show that A4(QZ) = M ± { X ) n A4(X, Z).
9.18. Write down the steps involved in hypothesis testing for the ANACOVA
model in the framework of a nested sequence of hypotheses (see section
7.4.1).


Chapter 10
Random-Effects and
Mixed-Effects Models
In Chapter 9, we discussed fixed-effects linear models. In many experiments,
the levels of the factor of interest are assumed to be randomly drawn from a
large population of levels. We then assume that the effects due to this factor
are random. The simplest model corresponds to an experiment with a single
random factor, Factor A, with a levels. Inference for this model is described
in detail in section 10.1.
In many practical situations, we find the need to
incorporate both fixed effects as well as random effects in order to explain the
response variable Y. Such situations are modeled by mixed-effects models, i.e.,
an additive model involving both fixed effects and random effects. Section 10.2
gives a general description of inference for mixed-effects models, of which the
random-effects model is a special case (i.e., there are no fixed effects). Different
estimation procedures are described and illustrated on one-factor and two-factor
models.
10.1
One-factor random-effects model
In many situations, an experimenter is interested in a factor which has a large
number of possible levels. In this case, a of these population levels may be
chosen at random for investigation; we then refer to the factor as a random
factor. Inference is made not about these randomly selected a levels, but rather
about the entire infinite population of factor levels. For example, consider an
experiment designed to study the maternal ability of mice using litter weights
of ten-day old litters. There are four mothers, each of which has six litters. A
single laboratory technician manages the entire experiment. Let Yij denote the
weight of the jth litter corresponding to the ith mouse; a possible model is the
one given in (10.1.1), where fi is an overall mean effect, and T* denotes the effect
due to the ith mouse, i = 1, • • • , a. Since maternal ability is certainly variable
across parents, it is unlikely that the experimenter is interested in these four
385

386CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
specific female mice. Rather, these mice may be considered to be a random
sample from a very large population of mice, and T* is a random effect. As
another example, suppose that a textile company weaves a fabric with a large
number of looms. The process engineer suspects the existence of significant
variation between looms in addition to the usual variation in strength of fabric
from the same loom. Four looms are selected at random, and four fabric sample
strengths are obtained from each. Once again, the loom effect r* is treated as a
random-effect.
The random-effects model corresponding to a single random Factor A with
a levels, and with n replications in each level, is given by
(10.1.1)
Yij = fj,+ Ti + 6i j, j =!, - • • , n,
% =!, * • • ,a
where both rx and tij are random variables, while ^
is a constant. We assume
that Ti ~ N (0, cr
^
), and are independent of 6ijy which are assumed to be N (0, <rf )
variables. The quantities
and
are called variance components and are
unknown parameters that must be estimated along with the overall mean //,
which is a fixed effect. Since Cov(ri,Tk ) = 0, i ± &, and CovfcyEi' y ) = 0, we
see that
(7?
if i = i' , j = f
if i = i' , j
if i = i'.
Cov( YlJ , Yxi j > ) —
0
The quantity p =
<7^/(<T^
+ <jf ) is called the intra-class correlation, and is the
proportion of variance of an observation which is due to the differences between
treatments.
Let N — an. Equation (10.1.1) can be written in matrix notation, using
Kronecker product notation (see section 2.8) as
(10.1.2)
y = (la 0 ln)M + (la ® 1n )r + £,
where r = (TI, • • • ,ra)', Cov(e ) =
and Cov{r ) = ofIa , so that
—
(Ia ® In)^rIa(Ia ® In) A 0"elN
=
(T* (la 0 jn) + a* (Ia 0 In)
=
la 0
A CreIn),
Cov(y)
(10.1.3)
and Jn denotes an n x n matrix with all elements equal to unity.
If e ~
N(0,ofl/v), it follows that y ~
V), where V = Ia 0 (of Jn 4- ofIn).
Notice that if there are nx observations in the ith level, i = 1, • • • ,a, then the
model is unbalanced, and the product formulation in (10.1.2) no longer holds.
With N = Yli=i nii we can write the unbalanced random-effects model as
y = lNp+ {dlnj^
r + e,
(10.1.4)

10.1. ONE-FACTOR RANDOM-EFFECTS MODEL
387
where {din,}“=i denotes a block diagonal matrix with diagonal components
Inn • • •.!«„• Also, {dJnJLi = 0“=iJni - In this case,
&r { d^ ni}i=i "1“
{ d^r^ rii + ^clni}i=i *
There are many approaches for estimation of the variance components in a
random-effects model. Before we discuss these, we look at estimation of the
overall mean /i, which is a fixed effect, and may be estimated using GLS in the
model (10.1.4).
Cov( y )
(10.1.5)
The GLS estimate of [i in the model (10.1.4) is
Result 10.1.1.
nlrl.
nx
V'GLS =
£
(10.1.6)
of + n,crf
erf 4- TijCrf
i=l
i=l
Proof.
The GLS estimate of the vector INF is
lyvCljvidO'rJni + erfIni }_1lw)
_1l'yv
J„( + <rfln > }
_1y,
so that the GLS estimate of /i is
(10.1.7)
l'N { d{a2
T3ni + allni )
1}y
l'N { d(a'*Jni +afIni )-1}lN '
(10.1.8)
l*GLS =
Since
<*T
O
(erfJnj + <jfl„j)
1
9
,
9
),
erf + 77-iCr^
(10.1.9)
it follows that
£{Y). — rua^
Yi./ { <J1 + n*erf )}/erf
1=1
H-GLS
E { m - nferf/(<rf + n^erf )}/erf
i=l
T,niYi./ ( <jf + njcrf )
7=1
E n i/(<
+ ««<*?)
i=l
tYi./VariYi.)
1=1
(10.1.10)
El/Varfy*.)
i=i
where yar(Yi.) =
+ a^
/ rti. The GLS estimate of /i is the weighted average
of the cell means, the weights being the reciprocals of their variances. When

388CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
rii = n, i = 1, • • • ,a,
}UGLS = Y ... Note that JIGLS depends on the unknown
variance components which define the variance covariance matrix of y. The
final form of the estimate will be obtained by substituting estimates for
and
into (10.1.10).
We discuss three methods for the estimation of the variance components- the
ANOVA method, method of maximum likelihood and the restricted maximum
likelihood (REML) method.
The ANOVA method starts from the ANOVA
table that we have described in the fixed-effects case, and using the expected
mean squares, derives an F-statistic. The likelihood based methods use the
assumption that the errors Sij are normally distributed.
10.1.1
ANOVA method
In the balanced random-effects model, we estimate the parameters
and
as
follows. First, we write down the ANOVA table as if all effects were fixed. In
particular, recall the ANOVA decomposition
EE(y«- y-)2 = En(y- - y-)2 +EE^ - y-
)2
’ Le- -
SSTC
The ANOVA estimates of the variance components are
i=l j=l
2=1
i=lj=l
SSA + SSE.
Result 10.1.2.
1
= MSE, and a\ = ~ { MSA- MSE ).
(10.1.11)
Let MSA and MSE be the mean squares corresponding to Factor
Proof.
A and the error respectively. Using results in section 5.4, it can be shown that
MSA and MSE are distributed independently with E( MSA) —
+
and
E( MSE ) = G\. It follows that E( SSA) = (a — 1)(ncr^
+ <j
^
), and E(SSE ) =
a(n— l) <jg. The ANOVA method of estimation consists of equating the expected
sums of squares to the corresponding observed values, i.e.,
(a — 1)(na* + Sf ), and
SSA
SSE
=
a(n-1)?*,
and solving the resulting equations (which are linear in the variance components)
for the expressions in (10.1.11).
This procedure is called the ANOVA method for estimating variance com-
ponents because it utilizes the components in the usual ANOVA table that we
write down in the fixed-effects model. Recall that
— Y .. in the balanced case.
These estimators are unbiased for the true parameters, and are computationally
simple to obtain. One disadvantage of the ANOVA estimator of
is that it
may be negative, which occurs when MSA < MSE. The method itself offers
no protection against a negative estimate for cr^
, which may occur with some

10.1. ONE-FACTOR RANDOM-EFFECTS MODEL
389
data. A negative variance component might be interpreted as evidence that o2
is in fact equal to zero, and the negative value is a result of sampling variability.
This interpretation seems especially sensible when the unbiased estimator o2
has a large negative value. Then, the model is reduced to
=\x + Sij, and the
ANOVA estimator of o2 is o2 — SSTc/ (an — 1). Alternatively, a negative o2
could indicate model misspecification, and we might try to fit a more suitable
model to the data. Yet again, it might be an indication of “ undersampling” , so
that collecting more data might yield a positive estimate. In general, we would
wish to avoid negative estimates of the variance components.
Under the normality assumption of the errors,
Result 10.1.3.
{a- l )MSA
o2 + no2
a(n-1)MSE
r* j \2
Afl-D
~
X%-a
MSA/ {na2
T + a2)
(10.1.12)
~
Fa— l,o(n — 1) *
MSE jo2
That the distributions of SSA and SSE are chi-square follows
Proof.
directly from results in section 5.4. Also, SSA and SSE are independently
distributed. Note that
= MSE ~ {<r2/a(n- l)}Xa(n-i)
i.e., o2 is distributed as a scaled xi
variable, the scale factor being
a( n— 1)
ol /a(n - 1). Although MSA and MSE are each distributed as a multiple of
a x2 random variable, and are independently distributed, it does not follow
that ( MSA — MSE ) has a\
2 distribution, and therefore, neither does 3^
; in
fact, this estimator does not have a simple closed form distribution. It follows
directly from normal distribution theory that
Var( MSE ) = 2o\ja(n- 1)
Var(oj )
Var(d* )
Var [( MSA- MSE )/n\
2
( (no2
T + ol )2
and
n2
a(n-1)
_2/T4
-Var( MSE ) = — —
.
n
an(n- 1)
a - 1
1
Cov(d2,ol ) =
(10.1.13)
Unbiased estimates of these quantities are obtained by replacing o2 and o2 by o2
and o2 respectively, and by replacing a(n— 1) by a(n — 1) 4-2 in the denominator
(i.e., adding 2 to the denominator degrees of freedom) in each formula. Further,
MS A/ (no2 + o2 )
(10.1.14)
^ FQ l,a(n— 1) >
MSE/o2

390CHAPTER 10, RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
so that MSA/MSE has a distribution which is a multiple of a central F-
distribution, the multiple reducing to 1 when a2 = 0.
Based on the distribution theory under normality, we can construct confi-
dence interval estimates for the variance components. Since
a/ 2 < ( N - a)MSE/al < X%-a,a/ 2 ) = 1
the exact 100(1 — a)% confidence interval for a2 is
P(xlN — a,l—
( ( N - a)MSE/ xj
)
( .N - a)MSE/ Xj
(10.1.15)
N — a,ck/2’
N — a,l — a/ 2
There is no exact confidence interval for a2. The distribution of a2 is a linear
combination of two x2 random variables, i.e., ciXa-i ~~ C2 X%-a> where,
c\ — o\ + na2/ ( N — a), and C2 = a2/ {n( N — a)}.
Since there is no known closed form expression for this distribution, only an
approximate confidence interval for a2 can be obtained. It is however, possible
to construct exact confidence intervals for functions such as p = a2/ (a2 -1- <J2 ),
a|/(cr|+ 0% ) > and VT /V 2- For instance, the confidence interval for p is derived
using the fact that
{ MSA/ (a* + na2 )} / { MSE/a2 } ~ Fa-lyN-a.
Hence
a/ 2
{ MSA/ {a1+nal ) } / { MSE/cD < F0_ liW_0,a/2) = 1-a,
and after some rearrangement of terms, we obtain the 100(1 — a )% confidence
interval for o2/a2 as (L, H), where
E( Ea-i ,N — a,1—
MSA
1
L
-1
n \ (Ea— 1,7V— a,cx/ 2 )MSE
MSA
1
U
a/ 2 )MSE
V
'
n\{ Fa- xyN
Then, the 100(1 — a)% confidence interval for p is
T+u )
discuss hypothesis tests. In the random-effects model, it is no longer meaningful
to test hypotheses comparing treatment effects T*.
-O,I-
. Let us now
Consider a test of
Result 10.1.4.
HQ : a2 — 0 versus H\ : cr2 > 0.
(10.1.16)
The ratio
E0 = MSA/MSE ~ Fa-i.AT-a
(10.1.17)

10.1. ONE-FACTOR RANDOM-EFFECTS MODEL
391
distribution under HQ.
Proof.
Under the null hypothesis, all treatments are identical, while treat-
ments are variable under HUnder the null hypothesis, we have SSA/ cr\ ^
SSE/a* ~ XN-O an(i^SA and SSE are distributed independently. Note
that under Ho, both MSA and MSE are unbiased estimators of of , while under
Hi, we would expect that E( MSA) > E( MSE ). The ratio Fo = MSA/MSE
has an Fa_i,N_a distribution under Ho, and we would reject Ho for large values
of the test statistic, i.e., if Fo > Fa-\ yN-ay0i, where a is the chosen level of
significance.
xl-i >
Under normality, it is possible to compute the probability of a negative
estimate for a? as
p = p(d2r < 0) =
P( MSA < MSE )
=
P( Fa-itN-a <
(10.1.18)
a2 + no2
for various choices of a and n. Studies indicate that p decreases as either a or n
increases. In any experiment, it is more important to have many classes (large
a) than it is to have many observations per class (large n). Also, if of >
p
is zero, except for small values of a (say, a < 4). If o\ < erf /10, then p can be
appreciably large (see Searle, Casella and McCulloch, 1992 for more details).
Numerical Example 10.1. Random-effects one-factor model.
process engineer in a textile company suspects that there might be significant
variations in strength of a fabric between looms in addition to the usual varia-
tion within samples of fabric from the same loom. Four looms are selected at
random and four strength determinations are made from fabric on each loom
(Montgomery, 1991). We show results from an F-test to see whether the looms
are significantly different.
The
ANOVA table for Numerical Example 10.1.
Source
d.f.
SS
MS
F-value
Pr > F
Model
Error
Corrected
Total
3
89.188
29.729
15.68
0.0002
12
22.750
1.896
15
111.938
From the ANOVA table, we see that the F-statistic is significant at the 5% level
of significance, and we reject H : a\ ~ 0.
In the unbalanced case, the ANOVA decomposition is
EXX -^-)2 - E^(y-- y -)2 + EDy« - y^
2
’ Le-
SSTC
i=1
i=lj=l
i= l j= l
SSA+ SSE.

392CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
In this case
E(SSA) =
(AT- J2n!/N )(?r + («- IK
2, and
2=1
E(SSE ) =
( N - a)o*.
By equating the sums of squares to their expected values, as in the balanced
case, we obtain estimates of the variance components as
a2 = MSE, and a2 = ( MSA- MSE )/n*
where
n*
i=l
2=1
i=l
When Hi = n for i = 1, • • • , a, these formulas reduce to those we saw in the
balanced case. Once again, there exists the possibility of a negative estimate for
a2. In this case, we can show that SSE/a2 ~ XN-O,• However, although SSA
and SSE are independently distributed in the unbalanced case, neither SSA
nor a multiple of SSA has a x2 distribution. This is in contrast to the balanced
case with the random-effects model, as well as the balanced and unbalanced
cases with the fixed-effects model. After considerable algebra, we can show that
2a*
Var(dl )
Var(a2 )
Var( MSE ) = N — a
2N
(iV2 - f>
2)
2=1
N ( N — l)(a — 1)
_4
,
r,
2
2
<y£
2(7£ CTT
[
x
( N - a )(W - En?)
2=1
N 2± n?+ (± nf )2 - 2N±nl
2=1
2=1
2= 1
+
N ( N 2 - E^
2)
— 2(a — 1) <J|
2=1
Cov(a£ ,dr )
(10.1.19)
( N - a)( N - E n
2/N )
2=1
In the unbalanced random-effects model, MSA/ MSE does not even have a
distribution which is a multiple of an F-distribution when a2 > 0.
When
a2 — 0, then SSA ~ &2 X 2-n and F ~ MSA/ MSE ~ Fa_i , N_
a. We can
therefore use the F-test for testing Ho : a2 = 0 versus H\ : a2 > 0 in the
unbalanced case as well.

10.1. ONE-FACTOR RANDOM-EFFECTS MODEL
393
10.1.2
Maximum likelihood estimation
We describe maximum likelihood estimation of the parameters in the unbalanced
model.
In the unbalanced one-factor random-effects model, the
Result 10.1.5.
MLE of /x is
-i
«
= A =
)]\ E
(10.1.20)
+ ni°
2r,ML
I*ML
i=1
2=1
. 2
. 2
and 5?
and the MLE’s 5?
equations
are obtained as the solutions a£ and oT to the
e,ML
T,ML
^
f2,ni/ Pi +^
{n2
i {Yl. - IX )2 }/2
'p,
\E Vft + SSE/ 2al +E{n.(F, - yu)2}/2p- = 0
(10.1.21)
: = 0, and
2=1
2=1
.2
-(AT- a)/2c£ -
2=1
2=1
. 2
provided the solution aT in (10.1.21) is positive. However, if this estimate is
negative, a\
Proof.
function is defined by
= 0, while ]UML = Y ...
T,ML
Since y ~ N ( [I1N ,V), where V ={d<x2Jn, + cr^
Ini}, the likelihood
L( p,V ; y) = (27r)-JV/2|V|-1/2 exp{-I(y-/ilN)'V-1(y-/il;v)}.
where,
|V| =
+ ni<7r )i
and V
<Tj+nicr:}.
}*
(in,
<7?
-1 _
2=1
We can write the likelihood function as
(27T)
N/2
<T£
2[( N
a)/2]
+ nidi )
2=1
a n,
a
E£w* -
-E
-1/2
L(/x, <J2, o-2; y)
1
(Fi. - rij/i)2]},
x
exp{
+ riiCT^
2a|
2=1 j=l
2=1

394CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
and its logarithm is
(Af __o) iog^
2 _ l^
log^
2 + n .a2 )
1
a n*
-
o
^EEaw)2 +^
E
i= l j=l
6 2=1
( N — a)
2
A
i
2
log (j£ ~ 2E og
N log 27r-
2=1
2 (
-
iP)2
2<7|
<J|+ njcr^
1^
TV
SS£
j log 2TT-
T' HiiYi. - v )2
2aj
2=1
-E
2p;
2=1
1, and d p i/ d a — n*, setting the partial
where pi =
+ 71*0^
. Since dpi/ da\
derivatives of the log-likelihood with respect to each parameter equal to zero
yields the set of maximum likelihood equations
^
ru(Yi. ^ = 0.
+En?(Fv ” M)2
^ i=iPi
i=i
di
E
9//
Pi
2=1
dl
= 0, and
. 2
dal
2p,
(iv —
A)
i
I
o2-^
•
i=iPi
2aE \m{Yi.- fi )2
dl
SSE ^
- +E
= 0.
. 2
. 2
2Pi
2=1
Solving dl/dp — 0, we obtain the estimate p in (10.1.20), which has the form of
_
. 2
.
2
PGLS • There are no closed form expressions for
<re, and aT\ they are obtained
as numerical solutions to the nonlinear equations dl/dal = 0 and dl/da\ = 0.
.
. 2
. 2
These solutions p ) ae, and aT are the maximum likelihood estimates of the
corresponding parameters only if they lie within the support of these parameters.
Since the likelihood function tends to zero as a2e —
0, or cr|—
oo, it must attain
its maximum at a positive value of of , thereby precluding the possibility of a
negative o\ ML. However, it is possible that aT is negative. If 0 < ar < oo,
= a
. 2
. 2
. 2
. 2
. 2
5?
al
— ae, and PML
P\ if, on the other hand, aT < 0, then,
e,ML
= 0, s?
asymptotic normal distributions.
r,ML
we set al
T »
= SSTm/ N , and PML = Y \ these estimators have
r,ML
£,ML
In the balanced case, the log-likelihood function has the simple form
N
1
^(P. cr2
> c2) =
•
-flogA-
-y l°g(27T) - -a(n- 1) logof
SSE
SSA
an(Y .. - nY
(10.1.22)
2crj
2A
2A

10.1. ONE-FACTOR RANDOM-EFFECTS MODEL
395
where\= a£ + na%. Setting the partial derivatives of the log likelihood with
respect to /x,
and
equal to zero, and solving the resulting equations yields
the solutions
. 2
Ya£ = MSE, and
(1-1/a)MSA- MSE
. 2
®T
n
Once again, these solutions will be the maximum likelihood estimators if they
lie within the support of the parameter space.
10.1.3
Restricted maximum likelihood (REML) estima-
tion
REML is a variation of the method of maximum likelihood in which we maximize
just that part of the likelihood function that is location invariant. In the one-
way random-effects model, this refers to maximizing the portion of the likelihood
function that does not involve /x. The restricted likelihood is also referred to as
the marginal likelihood of a\ and
and its logarithm has the form
~ log an- ia(n- 1) logcr^
(10.1.23)
1
lR(a2
£ ,a2
T \SSA,SSE )
-(an- 1) log 27r
-5(0- 1) log A-
The REML estimators are obtained by maximizing (10.1.23) within the para-
meter space of > 0, and
> 0. Equating the partial derivatives
— a(n — 1)
SSE
SSE
SSA
2A
'
2°e
dlR
Serf
2*2
2ai
-{a - 1)
SSA
91R
2A2
9X
2A
to zero, and solving simultaneously, we get the solutions
SSE
. 2
= MSE
ae,R
a(n- 1)
SSA
(10.1.24)
A R
- MSA
(a - l )
so that
1
. 2
aT <R = -{ M S A- M S E ).
f l
These are the REML estimates provided they are nonnegative. Note that these
coincide with the ANOVA estimators in the balanced case. The REML solu-
tions are very complicated for the unbalanced case. Westfall (1987) compared
the different estimation procedures for the one-way unbalanced random effects
model. The conclusion seems to be that REML is favored for estimating cr^
,
the ANOVA method for cr£ , while for simultaneous estimation of both o\ and
<7g, the ML method is favored.
(10.1.25)

396CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
10.2
Mixed-effects linear models
The linear mixed-effects model underlies analysis of a large number of statistical
problems, and has the general form
(10.2.1)
y = Xr + Z7 + e,
where y is an JV-dimensional response vector, X and Z are known N x p and
Nxq matrices of covariates, r(X) = r, r is a p-dimensional vector of fixed effects,
7 is a g-dimensional vector of random effects, and e is an AT-dimensional vector
of random errors which has a N (0,R) distribution. Assume that 7 ~ N(0, D7),
and Cov(i,s ) = 0. Then, y ~ iV(Xr, V = R -j- ZD7Z'). In general, V need not
be p.d. Also, R and D7 may be assumed to be diagonal. Let 6 — (#i, • • • , 9S )'
denote the unknown parameter vector which represents the variance components
in R and D^.
7
Suppose an experiment involves a fixed factor, Factor A
Example 10.2.1.
with a levels, and a random factor, Factor B with b levels. Suppose there are n
replications of each combination of the levels of Factor A and Factor B. Let y be
an N = abn-dimensional response vector, X be an N x a known design matrix
corresponding to the fixed-effects r = (77, • • • , TQ)', Z be an N x (a 4-1)6 known
design matrix corresponding to the random main effects and interactions, viz.,
7 = (/3i, • • • , /?&, (T(3)n , • • • , (rj3)a&)', and e be an N-dimensional error vector.
The design matrices X and Z can be written as
X — IG & lfr &1ri ) and Z — (Zi, Z2) — (la & Ife^ lru la ® 16 ® In)•
For instance, when a = 4, 6 = 3, and n= 2,
f16
0
0
o \
0
16
0
0
0
0
16
0
•
0
0 u j
Suppose Var( Pj ) — aj|, Var [(r0) jk ] —
and Var( £i ) = a*, then
°llb
For Kronecker product algorithms that are useful in constructing sums of squares
and covariance matrices in balanced designs of this type, see Moser and Sawyer
(1998).
I2
X = I4 (g> I3
<g> I2 = I4 ® I I2
I2
\°
0
and Var(e ) = G2
E Ia ® I& ® In.
Var(7) =
In the next section, we discuss an important optimality result for mixed
linear models, the extended Gauss-Markov theorem. It is assumed that V is
known. In practice, when V is unknown, these results may be applied with a
suitable estimator of V.

10.2. MIXED-EFFECTS LINEAR MODELS
397
10.2.1
Extended Gauss-Markov theorem
Let X* denote a n i V x r matrix consisting of (any) r LIN columns of X, and
suppose 7* is a realized, though unobserved value of the vector 7 of random
effects corresponding to a realized response vector y. Let 8+ and r+ denote the
true values of 6 and r. Suppose c
^
r is estimable. Recall that an estimator t(y)
of C[ T +c^
7 is linear in y and unbiased if t(y) = do 4- d'y for some constant do,
and some TV-dimensional vector of constants d, and if I£{£(y)} = c[r.
The estimator £(y) is called an essentially-unique b.l.u.e.
Definition 10.2.1.
of c[ r + C27* if for any other linear unbiased estimator do -1- d'y of c[r +
M5£
,(c'1r° -1- C27*0) < MSE(do + d'y), with equality holding if and only if
do + d'y = c'jT° + C27*0 with probability 1.
The following result is called the extended Gauss-Markov theorem for ran-
dom effects (Harville, 1976, 1977), and states an important optimality result
for mixed-effects linear models. When C2 = 0, Result 10.2.1 reduces to the
generalized Gauss-Markov theorem stated in Result 4.5.3. Let r° denote any
solution to the general normal equations
( X'V* X )T° = X'V^y,
where V11 G ftv, a class of g-inverses of V with certain properties. The notations
used here are similar to those used with Result 4.5.3.
(10.2.2)
Let c'jT0 be an essentially-unique b.l.u.e. estimate of the
Result 10.2.1.
estimable function c\r and suppose 7*0 — D7Z'V
“ (y - Xr°). Then,
1. the estimator c[ r0 + C27*0 is invariant to the choices of V1*, V
and the
solution to (10.2.2),
2. c[r0 -1- C27*0 is an essentially-unique b.l.u.e. of c[ T + C27*, and
3. when y has a normal distribution, C[ T° + C27*0 is an essentially-unique
best unbiased estimator (b.u.e.) of c\r + C27*.
From Exercise 10.4, and Rao and Mitra (1971, Lemma 2.2.6(c)), it
Proof.
follows that for y G C(X, V), the vector (y — Xr°) G C(VN). Further, for any
choice of V-,
D^Z'V-V = D^Z'.
(10.2.3)
7
7
For y G C(X, V), property 1 follows directly. To prove property 2, let u —
ciT
*+
* C27, and^ t{ y ) be any estimator with finite second moment. Then,
£{My) - £My)][£(«|y) - w)} = E [E{ [t( y )-^(«|y)][^(u|y) - «]|y}] = o,
so that

398CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
E{t( y ) - u}2 = E{t( y ) - £(w|y)}2 + E{ E{u\y ) - u }2.
Since £(y) is an unbiased estimator of u if and only if E{t(y) - E(u\y ) } = 0, it
follows that t(y) is the b.l.u.e. of u if and only if t( y ) — c
^
D-yZ'V'y is b.l.u.e.
for (c[ — C2D7Z/V X)T. Prom Result 4.5.3, it follows that t( y ) is the b.l.u.e.
of u if and only if t(y) —
C2D7Z'V“ y = (c* — C2D7Z'V~X)r with probability
1. When y is normal, replace b.l.u.e. by b.u.e. in this conclusion to obtain
property 3 (see Zyskind and Martin, 1969, p. 1200).
The dimension of V is iV, which may result in computational burden. In
many applications however, we can assume some structure for V, which reduces
the computational effort (see Harville, 1976, section 3). The next result gives
the covariance of the essentially-unique b.l.u.e. estimators. In general, suppose
C[ r are s estimable functions of r, and let C2 be an arbitrary q x s matrix.
Result 10.2.2.
defined in Result 10.2.1,
Kar(C;r°)
=
C,
1(X,V,X)-X'V*V(VII ),X{(X/V*X)~ }'CX
=
C'1(X/V“ X)-Ci , if C(X) cC(V),
D7Z'V ~ ZD7 + D7Z'V ~ X(X'VltX)-X'VltZD
For the estimators that are essentially-unique b.l.u.e. as we
(10.2.4)
Kar(7’°
7) — D
7 1
(10.2.5)
Ccw(CiT° ( 7*° - 7) = -COV (C [ T0^
) = -C'1(X'V#X)-X'VIIZD7 .
(10.2.6)
Proof.
Consider
Kar{C,
1(r0 - r) + C,
2(7*0 - 7)}
=
V^KC'JT0) + C'2{Var(Y° - 7)}C2
+
{Co?;(C'1r0, 7*t, - 7)}C2
+
C'2{ C O V(C[ T°
- j ) }' .
The results follow from (10.2.3), Zyskind and Martin (1969, Theorems 1 and 2)
and Rao and Mitra (1971, Lemma 2.2.6 (c)).
Estimation procedures
The ANOVA method is a method of moments type approach, which we illus-
trated for the one-factor random-effects model in section 10.1. The advantage
of the ANOVA method is that estimation does not require any distributional
assumption on y, although assumption of normality leads to the usual F-tests
of hypotheses. We first give examples of the ANOVA method for two-factor
mixed-effects models. In addition to the ANOVA method, there are alternative
estimation approaches for the mixed-effects models, which we discuss here. We
first describe maximum likelihood estimation under normality. This is followed
by the method of restricted maximum likelihood (REML) in the next subsection.
We end with a discussion of the minimum norm quadratic unbiased estimation
(MINQUE) procedure.
10.2.2

10.2. MIXED-EFFECTS LINEAR MODELS
399
ANOVA method
We have discussed the ANOVA approach in detail in earlier chapters. We give
some examples.
Example 10.2.2. Random-effects two-factor nested model.
the two-factor model where Factor A has a levels, the b levels of Factor B are
nested within each of the levels of Factor A, and both factors are random. The
two-way nested model is
Consider
Vijfc — /x + Ti 4-
+ £ijk,
for i = 1, • • • ,a, j = 1, • • • , by k = 1, • • • , n. Suppose T* are iid iV(0, a2 ) vari-
ables, fai ) are iid N (0,ap ) variables, eijk are iid N (Oya2 ) variables which are
independent of T* and
Similar to the fixed-effects model, the ANOVA
decomposition is
SSTm — SSA 4- SSg ( A ) 4- SSE,
where the sums of squares were defined in Example 4.2.7. When the effects are
random, we can verify that the expected mean squares are
a2 4-
-f- nba2,
a2 + nap > and
E( MSa )
E( M S B( A ) )
E( MSE )
a2
e .
(10.2.7)
This list of expected mean squares serves as a guide for the construction of
test statistics. Under the HQ : ap = 0, E( MSB( A ) ) =
and we use the test
statistic F
MSB( A )/MSE to test HQ . This statistic has an -Fa(6-i),af>(n-i) dis-
tribution under HQ . Under the hypothesis HQ : a2 = 0, we see that E( MSA ) =
a2 A nap, and we reject the null hypothesis at level of significance a if F —
MSA/ MSB( A ) > Ea-l,a(6— l),a -
Example 10.2.3. Random-effects two-factor model.
Recall the two-
way cross classified model with interaction between the two factors (balanced
case)
(10.2.8)
Yijk —
M 4-
4- (3j 4- (Tp)ij 4- £ijk
for A; = 1, • • • , n, i = 1, • • • , a, and j = 1, • • • , b. Suppose that the r, /3, and (r/3)
effects are all random. Suppose Ti are iid N (0,a2 ) variables, (3j are iid N (0,ap )
variables, (r/3) are iid N (0,a2p ) variables, Cov(ri, /3j ) — 0 = Cou(rf , (r/3)^
) =
Cov( ri,£ijk ), with similar assumptions about /?’s and (r/3)’s.
Based on the ANOVA table we saw under the fixed-effects case, we can
compute the expected mean squares when both Factor A and Factor B are

400CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
random as
bnal + na\p +
,
an<j
^
-f na^
p + <rf ,
ncr^
+ cr^
, and
P(MSA )
E(MSb)
H(M5ab)
E( MSE )
<7*E •>
which are linear combinations of the unknown variance components.
The ANOVA method of estimating variance components from balanced data
consists of equating the observed mean squares to these expected mean squares,
and solving the resulting equations for
??§, 5^
, and 3^
. We obtain
~2°E
MSE,
^
( MSA - MS„b),
— (MSB
MSAB ) > and
an
-(MSAB ~ MSE ).
-2ar
-2
__
op -
~2ar/3
(10.2.9)
n
Note that once again, there exists a positive probability of negative estimates. In
a balanced model, the ANOVA estimators of the variance components are min-
imum variance, quadratic unbiased estimators (Graybill and Hultquist, 1961),
even if normality is not assumed. Under normality, Graybill (1954) showed that
the ANOVA estimators are unbiased and have minimum variance.
Under normality, we obtain the following distributions of the sums of squares:
Xa-1»
v2
X(a— 1)(6— 1)’
and these are independently distributed. It follows that the distribution of a*
is a linear combination of scaled x2 variables, i.e.,
bncrj-fncr^
+cr
^
bn( a— 1)
(a- 1 )MSA /{ bna* + no\p + al ) ~
(a - 1)( b- 1)MSAB / {nalp + cr2} ~
2 _
i_ _2
c
2
/toT/3-f-CTe
2
Xa— 1
6n(a— 1)(6— 1) X(a— 1)(6— 1)’
However, 5^ ~ {cr^/a6(n- l)}x
2
i.e., MSE always has a scaled x2 distri-
a6(n — 1)
bution. The expected mean squares often enable us to determine which mean
squares are appropriate in the denominators of test statistics. For instance, in
this model, the statistic MSAB/ MSE has an F(a-i)(&_i),afc(n-i) distribution
under Ho
•
~ 0. To test Ho : G2
T = 0, we use the statistic MSA/ MS AB,
which has an Fa_i^
a_i )(t>~i ) distribution under this null hypothesis.
To construct confidence intervals, note that the mean squares are distributed
independently as multiples of x2 variables. Suppose we denote the kth mean
square by Mk with corresponding degrees of freedom fk. Then, an exact 100(1—
a)% confidence interval for E(M^) has the form

10.2. MIXED-EFFECTS LINEAR MODELS
401
{ fkMk }/ x2
htU < E( Mk ) < { fkMk }/ X }k
where X 2
fk ,u and X 2
f„,L are defined by P( x2
fk ,L
x )k < Xfk ,u ) = 1 ~ <*•
Example 10.2.4.
Mixed-effects two-factor model.
cross-classified model, suppose Factor A is fixed, Factor B is random, and we
consider interaction between the factors. The expected mean squares are
In the two-factor
E( MSA )
=
4- na^
0 +
/ (a -1)
a\ + ancrp
<J\ -f
and
%— l
F(MSS)
E( MSAB )
E{ MSE )
a?
(10.2.10)
e •
The ANOVA estimators (which are also REML estimators) are
M°
Y ...
0
n.- y
( MSB ~ MSAB )/an,
( MSAB — MSE )/n, and
MSE.
i = 1, • • • ,a
dlT (3-2
a£
Under ifo : 0^
= 0 > we can show that the test statistic Fo = MSAB/MSE has
an F(a_1)(5_1) >ab(n_1) distribution. Under Ho : cr|= 0, the test statistic Fo =
MSB/ MSAB has an Ft_1)(a_1) (5_1) distribution. Under HQ : T\ —
• • • = ra, the
test statistic FQ = MSA/MSAB has an Fa_ i >(a_1)(5_1) distribution.
Method of maximum likelihood
We write the mixed-effects model (10.2.1) as
y — XT 4* Z171 4- • • • + Zm7m + e
where 7j is a ^-dimensional vector of random effects, and Zj is the correspond-
ing N x qj design matrix, j = 1, • • • , m, £ is the iV-dimensional error vector,
X is an AT x p matrix of known regressors, and r is a p-dimensional vector
of fixed effects. We assume that E{^
j ) = 0, Var^
j ) =
j = l, - * - , m,
Cot)(7j,7i) = 0, j / l, Var(e ) =
and Cov{^
j,e ) = 0, j = 1, - -
Then, F(y) = Xr and
(10.2.11)
• , m.
V = Var(y) =
For ease of notation, we set e = 70, N = qo 1 and ZQ = IAT.
The likelihood function has the form
L{D V|y) = (2TT)
JV/2|V| 1/2 exp{-
^
(y- Xr)'V
J (y- Xr)}.
(10.2.12)

402CHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
Differentiate log L(r, V|y) with respect to r and a?, to yield the likelihood
equations
d log L/dr
d log L/dcrj
X'V-iy _ X'V^XT = 0, and
-tr(V-%Z')/2 + (y - XT)'V-%Z'V^
y- Xr) = 0,
(10.2.14)
(10.2.13)
. 2
The solutions r and
are obtained by solving (10.2.13) and (10.2.14), the latter
being nonlinear functions of the variance components.
Usually, closed form
solutions do not exist, and moreover, the solutions of the variance components
. 2
. 2
may be negative. Provided a0 > 0, and
<jj > 0, j = l, * * * ,ra, they are the
MLE’s of the variance components, which are otherwise set to zero. The MLE
of r is then obtained by substituting V for V in (10.2.13). Let us denote the
MLE’s by TML, and
, j = 0, • • • , m. For more details, the reader is referred
to chapter 6 of Searle et al. (1992). In Exercise 10.5, we ask the reader to derive
the asymptotic variances of these estimators.
Example 10.2.3. (continued).
the maximum likelihood estimators do not have a closed form even with balanced
data. These must be obtained numerically for each data set. The ML equations
are defined by
For the two-factor model with interaction
SSA
SSB
SSAB
, SSE
. 2
.2
. 2
^11
^12
6 - 1
1
a- 1
_—
1_
#4
011
012
(a — 1)(6 — 1)
a6(n — 1)
. 2
0o
0i
0i
0o
55A
1
a-1
— +
. 2
04
011
hi
SSB
1
6-
~—
I— :—
04
012
and
. 2
012
a — 1
6 — 1
(a — 1)(6 — 1)
55A
. 55B
,
. 2
. 2
. 2
SSAB
1_— |— —
04
011
012
01
On
^12
01
0n = 0e+nalp+bn<rT > 012 = cr^
+na^
pAana^
where 0o = cr|, 0i = cr\+ncr*
and 04 = 0n + 0i2 - 0i. The MLE’s are obtained by solving these equations
T/3
simultaneously.
REML estimation
The REML procedure was described in section 7.5.4. For a balanced model,
the REML estimator coincides with the ANOVA estimator.
We show some
examples.

10.2. MIXED-EFFECTS LINEAR MODELS
403
Consider the two-factor nested model
Example 10.2.5.
Yijk —
fl F
~f - Pj( i )
“1" £% jki
for i = 1, • • * ,a, j = 1, • • • , 6 and k = 1, • • • , n. Suppose Factor A is fixed, while
Factor B is a random factor whose levels are nested within the levels of Factor
A. In this mixed-effects model, we assume that
Ti = 0, and 0j(i )'s are iid
7V(0,of ) variables , j = 1, • * • , 6, i = 1, • • • ,a. In this case,
and rf have
the same form as in the fixed-effects case, while the estimates of of and of are
given under the random-effects model (see Exercise 10.6). Then,
o\ + ncrl + bn^
rf / ia - 1)
<72 + nof , and
E( MSa )
2=1
E( MSB( A ) )
E( MSE ) =
a2.
The hypothesis HA ‘
=
* • * = ra is tested by FQ = MSA/ MSB( A ) which has
an
i) distribution under HA • The test statistic for HB : cr^ = 0 is
FQ = MSB( A )/MSE, which has an Fa^
_i^
a^
n_ i
^
distribution under HB.
MINQUE estimation
Consider the model formulation in (10.2.11), with the same moment assump-
tions on 7/s and £. It follows from Rao (1971a, b) that
Var( y ) =£ =1*jZJZi +
=^
To + Z?=1° f T j t
where Tj = ZjZ', j = 1, • • • , ra, To = ZOZQ, Z0 = IN- For ease of notation,
set of = aft. The problem is to estimate a function of the variance components,
F = ]Cj=o fjah ky some quadratic form y'Ay. We require y'Ay to be
(i) invariant with respect to translation in r, i.e., if r
require y'Ay = ydAyd ; a condition for this is Ax = 0, and
(ii) we require F(y'Ay) = F, for which we must have
E( y' Ay ) = ZT=otr( ATj }a] = F,
which in turn requires that tr(ATj ) = /? > j = 0, * •
Now, Var{^
j ) —
&2Iqj\ this implies that if 7j’s are known, a natural esti-
mator of of would be 7j7j/^j- If £ were also known, then e'e/ N would be a
natural estimate of of - In this case, F would be estimated by
F = jyjU fj'Yj'Yj/ Qj + foe'e/ N = 77'A77,
Trf = T ~ To, we
• , m.

mCHAPTER 10. RANDOM-EFFECTS AND MIXED-EFFECTS MODELS
say, where,
(7i ) • ‘ • , 7
a n d
diag{(/i/gi )l9l , • • • , ( fm/ qm )l9m , (h/ N )lN}.
Since AX = 0, the proposed estimator is
y'Ay = {£”11 z
+ Zoe}'A{£7=1 Zi7j + Z0e} = i/Z'AZr,,
where, Z = {Zi, • * • , Zm, Zo}. Now, Z'Ar? will be closest to y'Ay if the term
Z'AZ — A | is small. The matrix A is chosen to minimize this norm. Since
Z'AZ — A | is the sum of squares of elements of Z'AZ — A, we see after some
simplification that
V
A
tr{(Z'AZ - A)'(Z'AZ - A)}
Z'AZ - A ||
771
=
tr(AZZ')2 -
+ /o/A}.
3=1
Let T = £”L0 Tj
A is such that tr(AZZ')2, which is equal to £r(AT)2, is minimum subject to
AX = 0, and tr( ATj ) = /j, j = 0, • • • , m.
Rao (1971) showed that this
minimum occurs when A = YljLo AjSTjS, where,
S = T_ 1 - T_ 1X(X'T_1X)
” X'T_ 1,
and Xj are solutions of Y^
jLo Xjtr{ STjSTi } = //, / = 0, - -
the MINQUE of F is y'Ay =
Xj^
STjSy = Yl’jLo^jQji where Qj =
y'STjSy, j = 0,
, m. To find A = (A0, - “ , Am)', let Wji = tr( STjSTi),
j,l = 0, • • • , m, and suppose W = {^
/} is an (m + 1) x (ra 4- 1) matrix with
these elements.
Then, ^
j=0 AjfrjSTjST*} = /z , / = 0, • • • , m if and only
if WA = f , where f' = (/o, *
*
*
i /m); the solution is A = W“ f . Hence, the
MINQUE of F is
y'Ay =
AjQi = A'Q = (W-f)'Q = f'W"Q = f'5 = £”10
where 5 = (5£,
• • , 3^
)' = W~ Q.
£jl0 ZjZ' = ZZ'. The MINQUE of F is y'Ay, where
Hence,
• , m.
Exercises
10.1. In the one-way random-effects model, show that
VarCpGLs ) = [£n«/(ni<r2 + <r|)] U
i=1
10.2 Consider the balanced one-way random-effects model where Sij are iid
jV(0,a|) variables, and T* are iid iV(0, a2) variables. Show that the sum
of squares due to the Factor A can be written as SSA = yf (Yn=i Jn/n ~
Jiv/iV)y.

Exercises
405
10.3. In the formulation of Exercise 10.2, show that E( MSA) = no2 4- o2.
Derive the distributions of SSA/ E( MSA) and SSE/ E( MSE ).
10.4. Show that
(a) X(X'V«X)-X,V«y is invariant to V# and (X'V#X)“ , and
(b) C(X, V) = C(X,VN),
where N appeared in (4.5.7), and the other quantities appear in Result
10.2.1.
10.5. In the mixed-effects linear model (10.2.11), let o2 = ( <To > ^7l > • • •
Show that, as N —
oo,
1. Far(fML) « (X,V-1X)-1,
2. Var(o]^ L ) = 2S-1, where S = {5^
-} is an (ra + 1) x (ra +1) matrix
whose (i, j)th element is Sij « 2{£r(V_1ZiZ'V_1ZjZ')}, and
3. COV (JMLI &ML )-> 0.
10.6. Obtain the ANOVA estimators and maximum likelihood estimators of
the variance components in the two-factor (balanced) nested model with
random-effects which is described in Example 10.2.2.
10.7. Consider the two-factor additive random-effects model
Yijk —
Ti
(3j + £% jk
1, • • • ,6, and k = 1, • • • , n. Suppose r* are iid
for i = 1, • • • , a, j
N (0,o2 ) variables, (3j are iid N (0,Op ) variables, eijk are iid
jV(0,of )
variables, Cov(ri, /3j ) = 0, Cov(ri,£ij ) = 0, and Cov( fij ,£ij ) = 0. Let
N = nab. Derive the ANOVA, ML and REML estimators of the variance
components.


Chapter 11
Special Topics
Bayesian linear models
11.1
We start with a statement of Bayes’ theorem, which is a time-honored result
dating back to the late eighteenth century. Let B — (Si,£2, * * * , Bk ) denote a
partition of a sample space
<S, and let A denote an event with P( A) > 0. By
the definition of conditional probability (see Casella and Berger, 1990), we have
P( Bj\A) = P(BJnA)/ P( A) = P{ A\Bi )P( Bj )IP{ A).
By substituting for P( A) from the law of total probability, i.e.,
k
P( A)= J2 P( A\Bi )P( Bi )
i— 1
it follows that for any event Bj in S,
k
P{ Bj\A) = P{ A\ B3 )P{B3 ) / Y,P{ A\Bi )P( Bi ) .
2=1
When the partition B represents all possible mutually exclusive states of nature
or hypotheses, we refer to P{ Bj ) as the prior probability of an event Bj. An
event A is then observed, and this modifies the probabilities of the events in B.
We call P( Bj \ A) the posterior probability of Bj.
Now consider the setup in terms of continuous random vectors. Let x be a
fc-dimensional random vector with joint pdf /(x; 0), where 0 is a g-dimensional
parameter vector. We assume that 0 is also a continuous random vector with pdf
7r(0), which we refer to as the prior density of 6. Given the likelihood function
is L(x;6 ) =/(x; 0), an application of Bayes’ theorem gives the posterior density
of 6 as
7r(0|x) =L(X; 0)7T(0) / f L( x; 6 )7r(6 )d6 .
The following matrix results are useful in combining quadratic forms, and find
application in a Bayesian treatment of linear models (Box and Tiao, 1973).
407

CHAPTER 11. SPECIAL TOPICS
408
Result 11.1.1.
B be k x k symmetric matrices such that (A + B)*1 exists. Then,
(x - a)'A(x - a)
+
(x - b)'B(x — b) = (x — c)'(A 4- B)(x - c)
4-(a - b)'A(A + B)
“ 1B(a - b)
Let x, a, and b denote fc-dimensional vectors, and let A and
where c = (A 4- B) *(Aa 4- Bb).
Proof. Clearly,
(x — a)'A(x — a) + (x — b)'B(x - b)
x'(A + B)x - 2x'(Aa + Bb) + a'Aa + b'Bb
x'(A 4- B)x - 2x'(A + B)c 4- c'(A 4- B)c 4- d
(x - c)'(A 4- B)(x — c ) + d
where d = a'Aa + b'Bb - c'(A + B)c. The right side is
c'(A + B)c
=
(Aa 4- Bb)'(A + B)
^1(Aa + Bb)
=
[A(a - b) + (A + B)b]'(A + B)_ 1
x
[(A + B)a — B(a — b)]
= -(a - b)'A(A + B)~ 1B(a - b) + a'Aa 4- b'Bb,
and the result follows immediately.
Result 11.1.2.
Let x, a, and b denote fc-dimensional vectors, and let A and
B be k x k p.s.d. symmetric matrices such that r(A 4- B) = q < k . Then,
subject to the constraints Gx = 0,
(x — a)'A(x — a)
4-
(x — b)'B(x — b) = (x — c*)'(A + B 4- M)(x — c*)
+
(a - b)'A(A + B + M)
"1B(a - b),
where G is any (k — q) x q matrix of rank ( k — q) such that the rows of G are
LIN of the rows of the matrix A + B, M = G'G, and
c* = (A + B 4- M)_ 1(Aa + Bb).
Proof. Since r(G) = ( k — q) , there exists a ( k — q) x q matrix U of rank (A: — q)
such that UG' is nonsingular and U(A + B) = 0. Now,
U(A + B 4- M)(A + B + M)'1 = U.
Since U(A 4- B) = 0, UM(A 4- B 4- M)- 1 = U. Post multiplying both sides
by A 4- B, we get UM(A 4 B 4- M)- 1(A 4- B) = 0. Since M = G'G and UG'
is nonsingular, this implies that
G(A 4 B 4 M)‘1(A 4 B)
M(A 4 B 4 M)_ 1(A 4- B)
Premultiplying both sides of the last equation by (A -1- B 4- M)- 1M, we obtain
(11.1.1)
0, so that
0.

ILL
BAYESIAN LINEAR MODELS
409
M(A + B + M)-1A(A + B + M ) ~lM
+M(A + B + M)"1B(A 4 B 4 M )~ XM
Both terms on the left are p.s.d. matrices, which follows from the fact that A
and B are p.s.d. The above equality therefore implies that each term must be
equal to 0. For k x k matrices C and D, write A = C'C and B = D'D. We
must then have
0.
M(A 4 B 4 M)_1C' = M(A 4 B 4 M)_1D' = 0
which implies that
M(A + B + M)-1A = M(A 4 B 4 M)_1B- 0.
Since Mx = 0,
x'(A + B + M)x-2x'(A + B + M)c*
4(c*)'(A 4 B 4 M)c* -f d\
(x- c*)'(A 4 B 4 M)(x- c*) 4 d\
where d\ = a'Aa -1- b'Bb— (c*)'(A + B 4- M)c*, and c* was defined in (11.1.1).
We can now show that
(c*)'(A 4 B + M)c* =-(a ~ b)'A(A 4 B 4 M)
“ 1B(a- b) 4 a'Aa 4 a'Ab
so that
(x — a)'A(x — a) 4 (x — b)'B(x — b)
di = (a- b)'A(A 4 B 4 M)"xB(a- b)
which proves the result.
Result 11.1.3.
vector, and suppose that
Let y be an A-dimensional vector, let 6\ be a /ci-dimensional
y|0i-iV(M,C!).
(11.1.2)
Also, assume that given a A^-dimensional vector of hyperparameters 62,
(11.1.3)
It is assumed that AI, A2, CI, and C2 are known p.d. matrices of appropriate
dimensions (Lindley and Smith, 1972).
1. The marginal distribution of y is given by
y ~N ( A\A262,Ci 4 A1C2A1).
0 X\02^N ( A202 ,C 2 ).
(11.1.4)
2. The conditional distribution of 6\ given y is JV(Bb, B) where
B-1 = A'jC^
Ai + C21
(11.1.5)
and
b = A
^
C^V + CJ
1A202.
(11.1.6)

CHAPTER 11. SPECIAL TOPICS
410
From (11.1.2), we can write y = Ax0i + u, where u ~A/’(0, Cj).
Proof.
We can also write (11.1.3) as 0i = A2O2 + v, where v ~ 7V(0, C2) and v is
independent of u. From these observations, it follows that
y = AxA2O2 + Aiv -I- u.
By the independence of u and v, and Result 5.2.6, Aiv 4- u ~ N (0, Cx +
A1C2A1), which leads directly to the proof of property 1. To prove property
2, we see from Bayes’ theorem that
7r(<?i|y) a L{y^M^i).
By substituting expressions for the respective normal densities in the product
on the right side, and simplifying (by completing the square in) the quadratic
form in the exponent, we see that the product is exp(-Q/2), where
Q = {0i - Bb)'B-1(0i - Bb) + [y'CfV + O
^
A'^
1 A262 - b'Bb]
from which property 2 follows.
We give a more general result (Lindley and Smith, 1972) that involves a
two-stage hierarchy. The proof is left as an exercise.
Result 11.1.4. Suppose
y|0! ~
N ( A191 ,C 1 ), e1\e2^N ( A292 ,C 2 ),
and
O2|#3 ~
A^(A3^3, C3)
(11.1.7)
where #3 is a known ^3-dimensional vector, and Ai, A2, A3, Ci, C2 and C3 are
known p.d. matrices of appropriate dimensions. The posterior distribution of
0\ given y is AT(Dd, D) where
D-1 = A
^Cf
^1A1 + [C2 + A2C3A'2 }- 1
(11.1.8)
and
d = A'iCfV + [C2 + A2C3A,
2]-1A2A3^
.
(11.1.9)
The mean of the posterior distribution is seen to be a weighted average of the
least squares estimate (A
^
C^
1Ai)-1 A'XCX Jy of 6\ and its prior mean A2A303,
and is a point estimate of 6\. The marginal density (11.1.4) is also called the
predictive density of y. The three-stage hierarchy can be extended to several
stages.
In the simple linear regression model (4.1.6), suppose a
Example 11.1.1.
prior on (3 =
is the noninformative prior 7r(/3) = 1. The OLS estimator
f3 is a sufficient statistic for /3. The posterior distribution is 7r(/?|y) = 7r(/?|/?),
which is a N2{ /3,a2T> ) distribution, where

11.2.
DYNAMIC LINEAR MODELS
411
-X
S = SSTC
1
and S5TC was defined in Table 7.2.2. Also, the joint density of (y, /3) given x
is normal. The predictive distribution p( y\X * ) of y given a specific preditor
value X * is also normal with mean /3o+ PiX * and variance cr2[l + (1/ N ) + (A
" —
A*)2/S5TC],
Example 11.1.2.
with e ~ 3V(0, <T2IJV)- Assume that the components of 0 =
, /3jk)/ are
exchangeable (which may not always be pertinent). Suppose we assume the prior
distribution 0j ~ iV(^
, <Tg) , where we may assume that ip = 0. We consider two
cases, one where cr2 and
are assumed to be known, and the second, and
more realistic one in which both the variance parameters are unknown nuisance
parameters. In the first case, the posterior mode is
/3* = {Ip + c(X,X)-1}-1^
,
where c = cr2 / crp is the ratio of the variances. Note the similarity of this estimate
to the ridge regression estimate (see section 8.3.3). In the second case, let us
assume that the prior distributions for a2 and cr|are given by
u\/a2 ~ xl , and v0 Xp / o} ~ %l0 -
Consider the multiple regression model y = X /3 + e,
Then
7T(/3, a2 , a|)
oc
( <T2)-(^-2)/2 exp{-[„A + (y - X/3)'(y - X/3)]/2a2}
k
exp{ - [u0\0 +^
(/3j - /3)2]/2a} } ,
(cr
^
)-(p-^+ 1)/2
X
3=0
where 0 —
Pj / P - The marginal posterior distributions of /3, a2 and
may be obtained in closed form, and the posterior modes are obtained from
[Ip + c*(X'X)_ 1(Ip - p-1Jp)]-1/?,
M + (y - X/3*)'(y - X/T)]/(iV + 1/ + 2) ,
k
[vpX0 +^
(/3* - (3* )2 ] / ( p + vp + 1),
/3’
a2*
_2*
a0
3=0
where c* = o2* /af .
Dynamic linear models
The general linear model in (4.1.1) describes the relationship between inde-
pendent responses on N subjects and a set of covariates. In contrast to such
cross-sectional data, we frequently encounter situations where the responses and
11.2

CHAPTER 11. SPECIAL TOPICS
412
covariates are observed sequentially over time. It is of interest to develop infer-
ence for such problems in the context of a dynamic linear model.
Let yi, •
• , yr denote p-dimensional random variables which are available at
times 1, • • • ,T. Suppose y£ depends on an unknown g-dimensional state vector
Ot (which may again be scalar or vector-valued) via the observation equation
(11.2.1)
yt = Ft9t + vt
where Ft is a known p x q matrix, and we assume that the observation error
vt ~ 7V(0, V£), with known V£. The dynamic change in 6t is represented by the
state equation
6t = Gt0t_ i + w£
(11.2.2)
where G£ is a known q x q state transition matrix, and the state error w£ ~
iV(0, W£), with known W£. In addition, we suppose that v£ and w£ are inde-
pendently distributed. Note that 0t is a random vector; let
Ot\ yt ~ N (Ot ,Et )
(11.2.3)
represent the posterior distribution of 0£. The next two subsections describe
the well-known Kalman filter recursions and Kalman smoother recursions that
enable estimation of the state vector.
Kalman filter equations
The Kalman filter is a recursive procedure for determining the posterior distri-
bution of 0£, and thereby predicting y£. Let Y[t] = (y£, • • • ,yi)' denote all the
observations up to time t and let 0o and So denote the initial guess about the
mean and variance of the distribution of 0. We assume that (0£_ I|Y[£_
IJ ) ~
N {0t
knowledge at time t about 0t :
11.2.1
S£_ i). The filter employs Bayes’s theorem which describes the state of
-l >
P(yi|^
, Y[£_1])F(0£|Y[£_1])
(11.2.4)
miY[t]) =
/ P(y£,0£|Y[£_1j)d0£
all Qt
(see Meinhold and Singpurwalia, 1983). Equation (11.2.4) represents the pos-
terior distribution for 6 at time t as the product of the likelihood (first term
on the right side) and the prior distribution for 9 (second term on the right).
We show how this posterior distribution is derived using the notions of Bayes’
Theorem and the results on multivariate normality from section 5.2.
Result 11.2.1. At time t, the prior distribution of 6 is
9t\Y [t-1] ~ N {Gt0i
(11.2.5)
R*)

11.2. DYNAMIC LINEAR MODELS
413
where
Rt = GjSt-iGj + Wt
Proof. Prior to observing yt ) the best guess for 9t is based on (11.2.2) and is
9t —
Since (0t-i|Y[t_i]) ~ N (9t-i,St-i), we see using Result 5.2.6
that (0^|Y[t— x] ) is normal with mean GtOt-i and covariance R< = GtE(_iG't +
Wt.
(11.2.6)
Result 11.2.2. The posterior distribution of 0 at time t after observing yt is
normal with mean
Ot = GtOt-i + RtKiVt + FtRtF't) -et
-l
(11.2.7)
and covariance matrix
St = Rt - RtF't (Vt + FtRtF,
t )-1FtfLt
(11.2.8)
where
(11.2.9)
et = y* - yt = y« - FtGt6t-i .
Proof. Let yt denote the prediction of y* based on Y[t— i], i.e.,
yt
=
EiytlYit-!] )
=
E [( Ftet ^ vt )\Y [t_l } }
=
FtGtOt-L
We denote the prediction error by et — y* — yt, which is equal to yt — FtGt0t_i.
We can write (11.2.4) as
(11.2.10)
P(0t|et,Y^
j)
cx
P(e,|0t,Y[t_i])P(0t|Y[t_1])
where P(e*|0t , Y[t_ i_ j) is equivalent to the likelihood function L(9t\ yt ) by virtue
of the fact that observing yt is the same as observing e*, when Ft, Gt, and 6t-i
are known. Using (11.2.1) we can write (11.2.9) as et =
and it follows that
E(et \9t ,Y [t_1] ) = Ft ( ot
Again, it follows from Result 5.2.6 that
miyt.Y,*.!,)
Ft ( 9t — Gt^t-i) + vt
, and Var(et\9t ,Y[t_ i|) = Vt.
- Gt0t-i
(11.2.11)
(et|^t,Y[t_ i,) ~ N (Ft ( 0t - GtOt-x ) ,Vt).
An application of (11.2.4) and the equivalence of probabilistic information in
Yt and et implies

CHAPTER 11 . SPECIAL TOPICS
414
f P {euQt \Y [t _x ] )det .
all Ot
It is possible to evaluate (0f|Y[*]) easily by using properties of the multivariate
normal distribution that we discussed in section 5.2. Use Result 5.2.11, and let
xi correspond to e^
Y^
x], and X2 correspond to 0t|Y[t_ x]. From (11.2.5), /X2
corresponds to
while E22 corresponds to R F r o m (11.2.11), it follows
that fj-i.2 = /ii +Ei2Ri
"1(^t-GtOt-i ) corresponds to Ft (0
fii corresponds to 0, and E12 to FtRt. Likewise, En.2 = Eu — Ei2E^
21£2i =
En — FtRtFj corresponds to Vt, from which we see that En corresponds to
Vt + FtR(F't. Using the converse relationship discussed following Result 5.2.11,
we see that
miYW ) = P(et \OuY[t.i1)P(ft|Y[t_i])
- GtOt- x
so that
t
RtF't
FtRt
Vt + FtRtFi
Rt
e
GtOt-1
t
~ N
Y \t ~ D
0
et
Now, let the conditioning variable X2 correspond to et|Y[t_ i], with correspond-
ing mean and covariance equal to 0 and Vt + FtRtFJ, we see from Result
5.2.11 that (Ot\ety Yjt— 1]) has a normal distribution with mean 6t = Gt6t- i +
RtF{(Vt +FtRtFJ)"1et and covariance Et = Rt — RtF£(Vt + FtKtF,
t )~ 1FtRt >
which are the expressions for the posterior mean and covariance of 0t\Y [t ] given
in (11.2.7) and (11.2.8).
Suppose an observed univariate quarterly time series{Yt}
Example 11.2.1.
is expressed as
Yt — Tt H- St 4- Vt
where Tt denotes trend, St denotes the seasonal component, and ut, the error
in this structural model (Shumway and Stoffer, 2000, p. 334). Suppose we set
—
4>Tt ~1
and
+
St ~1 4- St-2 A St-3 = Wt2 y
Tt
St
where <j> > 1, to characterize exponentially increasing trend, and a seasonal com-
ponent that is expected to sum to zero over 4 quarters. To write this model in the
form (11.2.1) and (11.2.2), set the state vector to be 0t = (Tt , St > St ~i , St-2 )' •
The observation and state equations are
Tt\
(‘
1 °
0) ssit
\5«-2/
t
Yt
+ vt
( Tt\
( <t>
0
0
0 \ (Tt- 1\
0
-1 -1 -1
0
1
0
0
U
0
Wtl
St
St-1
St-2
1
0 )\Si-3y
Wt2
+
St-1
\St-2 j
0
0 )

11.2. DYNAMIC LINEAR MODELS
415
where the observation error variance is Vn, and the state error variance is
diag(Wn ,W22,0,0).
The filter equations follow directly from Result 11.2.1.
For convenience, we adopt the following notation (see Shumway and Stoffer,
2000). Let 6st = E(0t\Y [s ] ), Efi t2 = E [{0tl -0sti )(0t2 -O f J ], and condense E£t
to E£. Kalman filter estimates are obtained when s < t. The derivations in the
next section employ this notation.
Kalman smoothing equations
Estimators of the state vector 6t based on the entire data yi, -
* - , yr > where
t < T are called smoothers and are denoted by 6 j, t = 1, • • • ,T. The proof of
the next result is along the lines of Rauch, Tung and Streibel (1965).
11.2.2
With initial conditions 0£ and
obtained from the filter
Result 11.2.3.
recursions
+ Jt-i(0 j - 0\ *), and
Ci
t-1
(11.2.12)
(11.2.13)
0t-1
YT
^ t-1
t-1 )
where Jt-i = E^
G'^
"1)-1.
Proof.
Now
P^-L^
Y,*,)
a
P(6»t_ i ,^
, Y[T]) = P(6't_ i ) 6>/ , Y[t_1], yt, - - - , yT)
=
P(Y[f_1|)P(0t_1 , 0t|Y[t_11)P(yt , - - - , yT’|^-i,^t, Y(t_ 1]),
which can be simplified to
P(0t_1, 0t|Y[,_1]) = <51(0,)P(0t-i|Ylt_ 1,)P(0t|0e_ 1),
where <5i (0t) does not depend on 0*_1. Clearly,
E*_}), and
Wt).
The smoothers Oj and 6 j_ x are obtained by minimizing
— 2 log P{0t-\,6t\Y [t~i ] )
oc
(0t_
1-Ci1){^:l}-1(^-i -d )'
+
(dt - Gt0t-i)W ;r
1^
- Gt0t_i)' + <52(0t) >
(11.2.14)
t-1
0e_ i|Y[t_ 1]- iV(0
0e_
1|0e_ i ~ iV(Ge0e
t-i »
-1?
Substitute the available 0^
for 0t in
where 62(0e) is independent of 0t_i.
(11.2.14), and minimize the resulting expression with respect to 0e_ i to obtain
Ci =
+ G'twt-1Gt]-1[{^:i}-1CJ + G'fwt-1C

CHAPTER 11. SPECIAL TOPICS
416
t-i
which yields (11.2.12), on using Exercise 2.28 with A = E
C = G*. To derive (11.2.13), we see that from (11.2.12),
(0t-i -Ci) + h- iOj =
-Cl) + J
B = W£, and
t-i >
Gtd-
t-1
and
=
+ Jt-iGtE^llCl'JGpUi -
Ci + Ji-i£(CC)J'tt -1
Since
E{ejC) =
-sf = G.^.-JCOG; + wt - Ef , and
^(ClCl')
t-1
t-l >
(11.2.13) follows.
In practice, the parameters in the model specification, such as elements
of Gt, V(, and Wt may be unknown, and must be estimated. We refer the
reader to Shumway and Stoffer (2000) for details on parameter estimation via
maximizing the innovations form of the likelihood or via a variant of the EM
algorithm which we described in section 7.5.4.
Diagnostics for the dynamic
linear model is described in Harrison and West (1991), while West and Harrison
(1997) is a good reference for the study of nonnormal dynamic linear models.
11.3
Longitudinal models
In longitudinal studies, measurements on subjects are obtained repeatedly over
time. Measurements obtained by following the subjects forward in time are said
to be collected prospectively, while extraction of data over time from available
records constitutes a retrospective study.
Longitudinal studies enable us to
estimate changes over time within subjects. In this section, we describe general
linear models for longitudinal data on m subjects over several time periods. For
example (Diggle, Liang and Zeger, 1994, p. 5), in a study of the effect of ozone
pollution on growth of Sitka spruce trees, data for 79 trees over two growing
seasons were obtained in normal and ozone-enriched chambers. A total of 54
trees were grown with ozone exposure at 70 ppb, while 25 trees were grown in
normal (control) atmospheres. The response variable was Y = log(/id2), where
h is the height and d, the diameter of a tree.
In general, in longitudinal studies, the natural experimental unit is not the
individual measurement Y*j, but the sequence y* of repeated measurements over
time on a particular subject. Here, replication refers to the number of subjects,
and not to the number of individual measurements. A longitudinal study is
effective for measuring change. There are two modeling approaches, viz., use of
a multivariate model, or use of a two-stage random effects model. We describe
both strategies in the next subsections.

11.3. LONGITUDINAL MODELS
417
11.3.1
Multivariate models
Let
Yij — Po + f3\Xiji + @2Xij 2 H
+ PkXijk 4- £ij — XUp + Eij
where Yij denotes the response variable, and X*j is a fc-dimensional vector of ex-
planatory variables, all observed at times Uj, for j = 1, • • • , rii, and for subjects
i — 1, * * * , 771. In (11.3.1), P —
{Po, Pu -"
» PkY is a p-dimensional vector of un-
known regression coefficients and e# denotes the random error component with
Eeij — 0, and Cov(eij,Sij> ) ^
0. In matrix notation, let y* = {Yu ,- - - ,Yini)'
denote the rii-dimensional vector of repeated outcomes on subject i with mean
vector E( yi ) = pi = (pii,
• • ,PimY and nixni-dimensional variance-covariance
matrix a2Vi = cr2{vijk }- Let y = (y*, • • • , y*mY denote the iV-dimensional re-
sponse vector, where N =
x
and let
be the block-diagonal matrix
with m non-zero blocks representing the covariance structure for each subject.
We can write the model (11.3.1) as
(11.3.1)
(11.3.2)
yi = XiP 4- £i,
where Xi is an rii x p matrix with Xij in the ith row and Si = (e*i, • • • , £in.).
Clearly, y ~ TV(X/?, e72V), where X is an N x p matrix. Although such mul-
tivariate models for longitudinal data with a general covariance structure are
straightforward to implement with balanced data, they may be cumbersome
when subjects are measured at arbitrary times, or when the dimension of V
is large. We first show examples with two possible assumptions on the block-
diagonal structure of cr2V, each using two parameters. We then describe some
estimation approaches for longitudinal models.
Uniform correlation model.
Example 11.3.1.
1, • • • ,m, we assume that Vi = Vo = (1 — p)I 4- pJ, viz., we assume a positive
correlation p between any two measurements on the same subject. One inter-
pretation of this model is that we introduce a random subject effect Ui, which
are mutually independent variables with variance v2 between subjects, so that
For each subject i —
Yj,j
p>ij 4- Ui “l- Zij,
j
1, • • • , Tli, i
1, * ’
where Ui are 7V(0, v2 ) variables, and Z\j are mutually independent JV(0, r2)
variables, which are independent of Ui. In this case, p = v2 j(y2 4- r2), and
a2 = v2 4- r2.
Example 11.3.2. Exponential correlation model.
correlation between any two measurements on the same subject decays towards
zero as the time separation between the measurements increases, i.e., the (j, A;)th
element of VQ is
We assume that the
Cov{Yij,Yik ) = cr2 e x p{—
- £ifc|}.

CHAPTER 11. SPECIAL TOPICS
418
A special case corresponds to equal spacing between observations times; i.e., if
Uj - Uk = d, for k = j — 1, and letting p = exp(-(j)d ) denote the correlation
between successive responses, we can write Cov(Yij ,Yxk ) = a2 p\ j — k\. One
justification for this model is to write
Yij = pij 4“ Wij,
j ~ 1, * * * , Tli, i — 1, • • • , 77Z,
whereWij ~ pWij-i+Zij, Zij are independently distributed as N (0, cr2{l— p2})
variables.
We describe several estimation approaches. We start with the simplest ap-
proach, which is weighted least squares (WLS) estimation. We next describe
and contrast the method of maximum likelihood and the method of restricted
maximum likelihood (REML).
Generalized least squares estimation
We discussed least squares estimation in section 4.5. It follows from (4.5.5) that
the weighted least squares estimator of (3 is
Pw = (X'WX) ~1X/Wy,
where W is a symmetric weight matrix. Note that W = I yields the OLS
estimate of /?, while setting W = V_1 leads to the most efficient estimator.
Implementation of this procedure requires a knowledge of the correlation struc-
ture in y. In general, when the parameters in Cov(y) are unknown, a two stage
procedure may be used, in which we estimate ft and the parameters in Cov(y)
in alternate stages, which we iterate to convergence.
Maximum likelihood estimation
The normal log likelihood function for the model (11.3.1) is
l(P,a2,V0; y) = {-^ log(c72) + mlog(|V0|) + o~2(y- X/3)'V-l(y- X/?)}/2.
(11.3.3)
For fixed Vo, the MLE of 0 corresponds to its generalized least squares estimator
/?(Vo), which has the form (4.5.5). The corresponding error sum of squares is
SSE(V 0) = {y — X/?(V0)}W“ Hy — X£(Vo)}. Substituting PGLS into (11.3.3)
yields
10(V 0), <r2, V0; y) = {-Vlog(<r2) + mlog(|V0|) + a-2SSE(V 0 ) }/ 2. (11.3.4)
Differentiating (11.3.4) with respect to cr2 gives
52(V0) = SSE{V 0 )/ N.
The reduced log-likelihood function for Vo is proportional to
ir(V0; y) = 1{P(Vo), ff2(VO), V0} = -JVlogSS£(V0)/2- mlog(|V0|)/2.
(11.3.6)
The estimate Vo is obtained by numerical maximization of Zr(Vo) with respect
to its distinct elements. The final estimates of 0 and a2 are 0(VQ ) and a2(Vo).
(11.3.5)

11.3. LONGITUDINAL MODELS
419
REML estimation
A general description of the REML procedure was given in section 7.5.4. It
follows from (10.1.23) that the REML estimator for a2 is
9%M (Vo ) = SSE(V0 )/ ( N - p).
The REML estimator of VQ maximizes the reduced log likelihood function
(11.3.7)
WV0; y) = — N \ogSSE(Vo )/ 2 — mlog(|V0|)/2- log(|X,V“ 1X|)/2.
(11.3.8)
Substituting the resulting estimator Vo, KM into the form of the GLS estimator
of (3 and (11.3.7) gives the REML estimates of /3 and a2. A comparison of the
forms of Zr(Vo;y) and /KM(VO; y) shows that we expect considerable difference
between the MLE and REML estimates when p is large (see also Verbyla and
Cullis, 1990).
Another approach that is widely used for longitudinal models is the estima-
tion equations approach, which is a multivariate analogue of the quasi-likelihood
method that we described in section 7.5.4 (see Diggle, Liang and Zeger, 1994,
and references therein).
Example 11.3.3. Repeated measures.
used in experiments or surveys where more than one measurement of the same
response variable is obtained on each subject, usually taken over time. Examples
include systolic blood pressure measurements made once a week for five weeks
on subjects who are on a given medication, scores on a spelling test taken four
times by each of 20 students with remedial instruction between tests, etc. In such
cases, all the subjects may either belong to a single homogeneous population,
or to a populations, which we then compare. The simplest repeated measures
design is when data is collected as a sequence of equally spaced points in time,
i.e., the experimenter assigns treatments to experimental units, and collects data
sequentially. The goal of the analysis is to measure (a) how treatment means
change over time, and (b) how treatment differences change over time.
The repeated measures model for a single factor experiment is
p “f T% + f3j + £ij
where p is the overall mean effect, n is the effect of the ith treatment level
and (3j is the jth subject effect. The subject effect is treated as random, and
we assume that f3j are iid Af (0, crj|). The treatment effect can be either fixed
or random; we treat it as a fixed effect here, and assume Yli-i T* = 0 *
A.
consequence of having the term (3j be common to all a observations on the same
subject is that Cov(Yij ,Y^ j ) is in general nonzero. A simple assumption is one
of equal correlation among responses pertaining to the same subject. It is easy
to see that
Repeated measures designs are
(11.3.9)
YiJ
Var(Yij ) = cr|+ a2 = cr2, say

CHAPTER 11. SPECIAL TOPICS
420
which is the same for each subject and for each period, and that
Cav(Yij,Ylj ) =al
The quantity
P
a0 + CTs
denotes the correlation between Y\j and Yij. Writing in the form y ~ N(X./3,f2),
where f2 has the intra-class correlation structure, REML estimation is straight-
forward. In Exercise 11.7, we ask the reader to derive ANOVA type inference for
repeated measures in the framework of a split-plot experimental design.
11.3.2
Two-stage random-effects models
Laird and Ware (1982) described two-stage random-effects models, which are
based on explicit identification of population and individual characteristics. The
probability distributions for the response vectors of different subjects are as-
sumed to belong to a single family, but some random-effects parameters vary
across subjects, with a distribution specified at the second stage. The two-stage
random-effects model for longitudinal data is given below (see (10.2.1)).
1. For each individual i = 1, • • • , m,
(11.3.10)
yi = X*r + Z*7i -f Ei , i = 1, • • • , m,
where r is a p-dimensional vector of unknown population parameters, X*
is an rii x p known design matrix, 7* is a ^-dimensional vector of unknown
individual effects, and Z* is a known n* x q design matrix. The errors
£i are independently distributed as Af (0,R*) vectors, while r and 7* are
fixed parameter vectors.
2. The 7i are independently distributed as TV(0,D7), and Cov{^i,£i ) = 0.
The population parameters r are fixed effects.
Marginally, the y* are independent iV(X*r, V* = R* + ZjD7ZQ. Let W* =
V” 1. Inference can be based on least squares, maximum likelihood and Bayesian
approaches. Let 6 = (0i, • • • , 0S)' denote an s-dimensional vector of “ variance
components” from R*, i = 1, •• • .m, and D7.
Estimation of mean effects
The classical approach to inference derives estimates of r and 6 based on the
marginal distribution of y7 = (y
^
, • • • , y w h i l e an estimate for 7 is obtained
by use of Harville’s (1976) extended Gauss-Markov theorem for mixed-effects
models (see Result 10.2.1). Let T = (7^
, • • • ,7^
)'.

11.3. LONGITUDINAL MODELS
421
Suppose 0 is known. Assuming that the necessary matrix inversions exist
— * m
m
9 = EX'WtXi
Ex*w<y
i— 1
7i = D7Z'W.l(yJ
(11.3.11)
and
i )
i=1
X;?),
(11.3.12)
with variances
-l
m
Var(r ) = £ X'W.X*
and
(11.3.13)
2=1
m
Var(7i) = D1Z'{Wi-W,Xt (^
X'WjXi)-1X;Wi}Z1Dr
(11.3.14)
2=1
A better assessment of the error of estimation is provided by
D7 - D7Z'WzZzD
Var{Sji -7^)
7
m
D7Z'WzXi[^
X'W,Xi]-1X'WiZzD7 ? (11.3.15)
+
2=1
which incorporates the variation in 7z.
Under normality, the estimator r is
the MVUE, and 7 is the essentially-unique MVUE of 7 (see Result 10.2.1).
The estimate f also maximizes the likelihood based on the marginal normal
distribution of y. We can write
r = £(r|y,?, <?),
so that 7i is the empirical Bayes estimator of 7
Suppose, as is usually the case in practice, 9 is unknown. We estimate r and
6 simultaneously by maximizing their joint likelihood based on the marginal
distribution of y. Let 6 and r (6 ) denote the MLE’s of r and 0 respectively. It
can be shown that
-1 m
m
2— 1
where Wz = Wz(0). The variance of 9(6 ) follows by replacing Wz by Wz in
(11.3.13). The corresponding empirical Bayes estimate of T in this case is
f (0) = E(r\y,r(6 ),6 ) = D7Z(W,{yi - Xtf(0)}.
m = Ex'iw^x^
(11.3.16)
2=1
Estimation of covariance structure
Popular approaches for estimating 6 are the method of maximum likelihood
and the method of restricted maximum likelihood (REML). In balanced ANOVA
models, MLE’s of variance components fail to account for the degrees of freedom
in estimating the fixed effects r, and are therefore negatively biased. The REML
estimates on the other hand are unbiased. Recall from section 7.5.4 that the
REML estimate of 9 is obtained by maximizing the likelihood of 9 based on any
full-rank set of error contrasts, z = B'y.

CHAPTER 11. SPECIAL TOPICS
422
11.4
Generalized linear models
Although linear models are very versatile in many applications, there are some
problems: restriction to normality, and the assumption of a linear model func-
tion relating the response to the predictors. A motivation for using GLIM is
that it permits more general distributions than the normal for the response
Y (McCullagh and Nelder, 1991). A link function is used to relate the linear
model to the mean of the response variable through a suitable scale. We may
think of GLIM’s as extensions of the usual normal linear models, which we recall
from Chapter 4 and Chapter 7. Let y be an iV-dimensional response vector, with
E( y ) = p, where p
7ZN . The systematic component of the model specifies p in
terms of a linear function of an unknown p-dimensional parameter vector /?, and
a given predictor matrix X, i.e., p = X/3 (see (4.1.1)). Since X/3 also assumes
values in a subset of llN , this is perfectly sensible! The random component of
the model specification assumes independence, homoscedasticity, and normality
of the response random variables on each subject, i.e., y ~ N(X/3, <72Lv). In
section 11.4.1, we present a generalization of this specification.
11.4.1
Components of GLIM
There are many situations where E'(y) does not assume values in subsets of
7ZN. For example, suppose we are interested in modeling binary responses, the
expectation of Y is the unknown proportion of success 7r, 0 < 7r < 1. In order
to sensibly ’’relate”
IT to the linear predictor, we find the need to introduce a
link function of the form, say, log[7r/(l —
7r)], which assumes values on the real
line. As a second example, suppose Y follows a Poisson distribution with mean
A > 0. The function log(A) enables us to relate the expected response to the
linear predictor in a sensible way. These notions are formalized below.
Note that we may rewrite the systematic component of the linear model as
follows: set a linear predictor rj = X/3, and set 77 = g( p) = p (i.e., g(.) denotes
the identity function). Further, the random specification in the linear model is
generalized to permit a wider class of distributions than the normal, including
discrete families of distributions (such as the binomial, Poisson, etc.).
The GLIM specification has the following three components:
1. The random component specifies the probability distribution of the
response variables. Specifically, it states that the components of y have
pmf or pdf from an exponential family of distributions (see (A22)). Let
E( y ) = P-
2. The systematic component specifies a linear predictor 77 = X/3 =
as a function of explanatory variables Xi, - - - , X* and un-
known parameters.
3. The link function
<?(.) provides a functional relationship between the
systematic component and the expectation of the response in the random
component, viz., 77 = g( p ).

11.4. GENERALIZED LINEAR MODELS
423
If g( fii ) = /ii, i.e., 7}i = Hi, i — 1, • • • , AT, we call g(.) the identity link
function. As we saw earlier, the usual normal linear model has the identity link
function.
Definition 11.4.1. Canonical link.
variable Y belongs to the unit dispersion exponential family with /(y*; #*) =
exp{w(Oi )yi — b(Qi )+c( yi ) }, we define the canonical (or natural) link function by
g{^
i ) —
Oi) = Y^
j=\ xi,jPj • For example, the canonical links for the normal,
binomial, and Poisson distributions are respectively r? = /i, rj = log[7r/(l —
7r)],
and T) = log(A).
If the pmf or pdf of the response
For convenience, we write the log likelihood function in terms of the mean-
value parameter \x rather than the canonical parameter 0, and denote it by
y)-
Suppose that for the zth
Example 11.4.1. Models for binary responses.
subject, the response Yi is binary, i.e., it can assume either the value 0, or the
value 1. Suppose P(Y* = 1) = 7T* = 1 — P(Yi = 0). Let rji = x'/3 as before, and
suppose we choose the logit link function gL(^i ) = log[7Ti/(1— TTi)] = Jfc. We seek
to carry out inference on the unknown vector of proportions 7r = (7ri, • • • , n^
)'
as a function of the linear predictor rj via this link function. The logit link
function, which is the canonical link function, maps the unit interval [0,1] onto
R, as do each of the following alternative link functions that are commonly used
with binary responses:
1. the probit or inverse Gaussian link function
grain) =
4>(.) denoting the cdf of a standard normal variable;
2. the complementary log-log link function
gc{n ) = log{-log(l - 7r)}; and
3. the log-log link function
9c(n) = - log{-log(7r)}.
The logit function is widely used due to its interpretability as the logarithm of
the odds ratio 7r/(l — 7r), and it induces a logistic regression model. In particular,
notice that the link function satisfies the condition that g( 7r) = H ~l ( 7r), where
H is the cdf of a given distribution. For instance, the logit link GL(7T) corresponds
to the cdf H ( TT ) = exp(7r)/{l +exp(7r)}, whereas the complementary log-log link
is obtained by specifying H (n ) = exp{— exp(— 7r)}. By considering H ( 7r) to be
the cdf of an asymmetric distribution, we can develop a skewed link (Stukel,
1988; Chen, Dey and Shao, 1999). If we assume that Yi ~ Binomial( rrii, TT*),
i = 1, • • • , AT, the variance function is V (n) = 7r(l —
7r). The log-likelihood
function, apart from a constant term, has the form

CHAPTER 11. SPECIAL TOPICS
424
r; y) = J^iLi Vi log[7r»/(l - 7r»)] + milog(l - 7rf)
which reduces under the logit link to
N
p
N
p
KAPY y ) = K0\ y ) = XIS yixijPj -
+ exp^
2 xij0j\. (11.4.1)
i=1 j=1
i=l
3=1
We return to this example in the next subsection.
Example 11.4.2. Log-linear model for counts.
on the ith subject, i = 1, • •
, JV, and suppose first that Y^
s are independently-
distributed as Poisson random variables with mean A*. The log-linear GLIM
postulates a logarithmic link function log(Ai) = rji = x'/?, i = l, - - * , JV. The
variance function is V(A) = A. The logarithm of the likelihood function, apart
from a constant is
Let Yi denote counts
y) =
log
- AI ).
Under the log link function, this reduces to
N
P
N
p
*(A(/?); y) = 1(0; y ) =YY,YiXi^
i ~
(11.4.2)
i=1 j=l
Next, consider the situation where the dispersion of the data exceeds that pre-
dicted by the Poisson model, i.e., Var(Y ) > E(Y ). Without knowledge of the
precise mechanism that causes the over (or under) dispersion, it is convenient
to assume that Var(Y ) = <fi\, and carry out inference of 0, as well as (3.
i=i
i
Estimation approaches
Several approaches are useful for estimating parameters in GLIM. In this section,
we describe the method of iteratively reweighted least squares (IRLS), and the
quasi-likelihood approach.
11.4.2
IRLS estimation
Result 11.4.1.
y have pdf (or pmf ) from the exponential family, let E(y) = /i be linked to
the linear predictor 77 = X/3 by g( /i ) = 77. The MLE of (3ML is obtained as
the iteratively reweighted least squares estimate in the linear regression where
the response variable is a linearized form of the link function applied to y, and
the weights are functions of the fitted vector )u. For large JV, (3ML ~ JVP(/?,fi),
where ft =Y.iLiKdPi/dPYVf 1(dPi/dP )}' 1
Let the components of the JV-dimensional random vector
Consider the exponential family log likelihood (in canonical) form
Proof.
for a single observation J(0, </>; Y) = (Y 6 — b(6 ) )/a( <j)) + c(Y, </>). Recall that

11.4. GENERALIZED LINEAR MODELS
425
b'(6 ) = fi, and bn( Q ) = V ( p). To derive the maximum likelihood equations for
(3j , j = 1, • • • ,p, we first use the chain rule of differentiation to write
01 _ 01 d9 dfi dr]
d(3j
09 dfi dr] 00j
Since d]x/ d9 = V (n), and 0r]/dl3j = Xj , it follows by letting W = (dfi/ dr] )2/V
(11.4.3)
that
dl
(Y —
fi )
1
d]i
a( <P ) Vi^
dr)
dPj
W
dr]
a{<t>)
say, leading to the ML estimating equations
EZi WiiVi - iHXdTH / dttJXij = 0
In a GLIM, u depends only on the mean and variance of Yi. If the dispersion
is constant, i.e., a{ (p) —
</>, then Wi will reduce to Wi. Let
3 =!» • • • , P-
=
~E{O2l/O0rO0s ) = — E{Our/O0s )
<=1
°^
5
^
d(ii
0/3s
A
* *rs
Xi^
+Wip- Xi
rr(F- m )]
1dm
t,r
dm
dps
N
^
YWiXi ,rXi ,s-
dm
i=l
i=1
To use Fisher’s scoring method set u = 01/00, and A = — E(O2l/O0jO0&).
Given a current estimate
we obtain a solution to A60
= u, which gives
an adjustment
to the current estimate. It is easy to verify that the new
estimate 0^ =
+ 60^ satisfies the equations
N
,
(A/3(1))r = V WiXi.rlift + (Fi - vn ) ~p.].
7EI
dm
(11.4.4)
We now show that these equations correspond to the weighted least squares
equations.
Let
denote an initial estimate of the linear predictor, based on
obtain
from g( p^ ) = rf 0
^. Let
be the N x N matrix with (i, j)th
element Or]i/Ofij , evaluated at
and let v<°
) denote the variance function
vector evaluated at
Define the adjusted dependent variate by
z(°) _ fj{o) _|_ D^°^(y — £<°>),
and the quadratic weights by the AT-dimensional vector w^
0
^, with the reciprocal
of its ith element given by the ith element of (D^
0))2v^°^a(^
>). Let 0^ denote the
0
(°\and

CHAPTER 11. SPECIAL TOPICS
426
weighted least squares estimate from the regression of
on X, with weights
(compare with (11.4.4)). The process of creating the adjusted dependent
variate and the weights in order to do weighted least squares is iterated to
convergence. The final converged estimate 0 is called the iteratively reweighted
least squares (IRLS) estimate of 0.
Example 11.4.3.
was shown in (11.4.1). From (11.4.3), the likelihood equations become
The log likelihood function for the binary logit link model
^
dl d^i drji
diu drji d0r
N
> ul
\
>
= ^
2=1
2=1
^
(Yi - rriiiTi )
E
7Ti(l- TTi )Xi2,r -
7Tj(l - 7Vi )
2=1
Given an initial estimate 0^
define the adjusted dependent responses
Zi = rj(-
0) + { (Yi - miTff
))/mi}{l/7ff
}(1- wt
(0) )},
with a diagonal weight matrix W = diag{mi7Tj(l —
7r.;)}, evaluated at 7?(°b In
fact, we can write the maximum likelihood equations in the form X'WX/3 =
X'Wz, from which we can see that 0^ = (X'WX) ~ 1X/Wz, all quantities
evaluated at 0^.
compute 7r(°
) and ff°\ based on which we
we
Example 11.4.2. (continued).
For the log-linear model that we introduced
in Example 11.4.2, the likelihood equations follow from (11.4.2) and (11.4.3):
^
01
v dl d\i drji
^ dX i drji d0r
E d(5r
2=1
2=1
N
Ai)Xiir.
2=1
The IRLS estimation proceeds using the steps in Result 11.4.1.
Quasi-likelihood estimation
We defined the notion of a quasi-likelihood function in section 7.5.4. Its useful-
ness for situations where there is insufficient information to construct a likeli-
hood function is well documented in the literature. We first describe the con-
struction of a quasi-likelihood function for problems where the observations are
independent and we model E(y). Suppose E(y) =
and Cov( y ) = a2V ( ji ),
where V { fjt ) = diag{Vi (//), • • • , V/v(^
)} is an N x N matrix of known func-
tions, and cr2 is unknown. Further, we assume functional independence, i.e.,

11A. GENERALIZED LINEAR MODELS
427
V (^
) = diag{Vi(/ii), • • • , Vjv (M;v)}- The log quasi-likelihood function for y is
defined by
N
with
1=1
j\y -t )/ {a2V (t )}dt
Q( p, y )
? Yi )
(11.4.5)
V(.) being the variance function. For many examples, the quasi-likelihood func-
tion corresponds to the actual likelihood.
Result 11.4.2.
1. The quasi-likelihood estimator /3QML is obtained by iterating to conver-
gence the sequence of estimate with the form
= fi0 ) +
- M
(0)),
(11-4.6)
where (3^ is an initial value.
2. COV ((3QML ) « <J2(D/V
1D)-1.
= (y - £)'v_ 1(y - v)KN - p)-
3. d2rQ M L
Proof.
Consider the function
Ui = (Yi - IM )/ {a2V (m ) }
for which
E(Ui ) = 0,
and Var(Ui ) = 1/ {a2V (& ) }.
We refer to U = (t/i, • • • , U^Y = U(/3) as the quasi-score function. Clearly,
0, and
-E{ dU (0)/d(3} = D'V^D/CJ2.
E{U(/3)}
Cot;{U(/?)}
(11.4.7)
Note that Cov{ XJ ((3) } is similar to the expected Fisher information matrix. The
quasi-likelihood estimating equations are obtained by setting equal to zero the
first derivatives of the log quasi-likelihood function Q( fi > y ):
U(0 ) = DV- 1(y — /Z)/ CT2 = 0
(11.4.8)
where D is an N x p matrix whose (z, j)th element Dij = d(ii/d/3j. Fisher-
scoring yields (11.4.6), and iterating to convergence yields
TO show
property 2, note that
Cov( pQML ) « (CowtU^
)]}-1.

CHAPTER 11. SPECIAL TOPICS
428
The estimator a*
vector.
is the method of moments estimator based on the residual
Q M L
If (i) U(0) is asymptotically normal as N —
oo, and (ii) the eigenvalues of
Cov{U ( /3) } tend to infinity for all 0 in an open neighborhood of the true value
/3°, then 0QML is consistent and has an asymptotic normal distribution.
Example 11.4.5.
is V ( Xi ) =\i, i = 1, • • • , N. Then,
For the Poisson model, recall that the variance function
Qi {^i\Yi ) — Yilogfli
M i *
For the binomial model, V ( iTi ) = 7^(1- TTI ) and
Qii^uYi ) = Yi log{7T</(l - 7Ti ) } + log(l - 7T<).
To model dependent observations, we replace the assumption of a diagonal
V matrix by the assumption that V is an N x N p.d. matrix of known functions
Vij ( fT). In this case too, (11.4.7) holds, the quasi-likelihood estimating equations
have the same form as in the independent case, and the form of COV (0QML ) is
the same. However, there are some issues because the matrix of first derivatives
of U(/3) with respect to 0 is not necessarily symmetric; the reader is referred to
McCullagh and Nelder (1991) for details.
Residuals and model checking
The residuals as well as goodness of fit criteria enable us to assess the adequacy
of fit of the GLIM. Let Z(/x, 0; y) denote the maximized log likelihood function
for a fixed value of 0, and let l ( y,4>; y ) denote the maximum log likelihood in a
full model with N parameters. As before, suppose 6 denotes the canonical pa-
rameter, and suppose that a*( <?!>) =
The discrepancy of fit is proportional
to 2[Z(y, 0; y) —
Z(ju, <f\y)]. Let 0 = 6(ji), and 9 = 0(y) denote the estimates of
the canonical parameter 0 under the fitted and full models respectively. Let
11.4.3
di = 2wi [Yi(6i - Oi ) - b(0i ) + b(0i )l
< =
, iV.
(11.4.9)
Definition 11.4.2.
We define three types of residuals.
1. The Pearson residual is defined by
ritPmXi - M ,/Vnfii), i =
N.
(11.4.10)
2. The Anscombe residual has the general form
= { A(Yt ) - AfcmA' faWVW) } ,
(11.4.11)
A
where the function A(.) is given by
A( z ) = fdz/V^ iz ).

11A. GENERALIZED LINEAR MODELS
429
For the Poisson model,
2/3
ri , A = 3(K
3. The zth deviance residual is defined by
sign(Yi - fii )\fdi,
(11.4.12)
n,D =
where d* was defined in (11.4.9). For the Poisson model,
U ,D = sign(y, -iliMYi\ogiYifci )- K* + Mi)]1/2
-
For the Poisson model, YliLi r i P = ^
2
> Pearson’s goodness of fit statistic,
which led to the construction of r{ yp. The distribution of r^
p is usually skewed
for non-normal response distributions, and the Anscombe residuals may be pre-
ferred for this reason. The values of Anscombe residuals and deviance are very
similar in most cases (see Pierce and Schafer, 1986 for details). We define two
goodness of fit measures, the deviance (and scaled deviance) function, and the
generalized Pearson X 2 statistic.
Definition 11.4.3. Deviance function. We define the deviance for the fitted
model to be the following function of the data (only):
N
D(y;
=
-$i ) ~ HO* )+ KM -
(11.4.13)
2=1
The scaled deviance function is
Definition 11.4.4. Scaled deviance.
defined by
D*( y;£) = D(y; /5)/0.
(11.4.14)
It is straightforward to compute D(y; /2) for the normal linear model, the
binomial logit model, and the Poisson log-linear model (with intercept) for
counts as
“ Mi)2 (which is equal to SSE), 2YliLiK log(li/jSi)+ (mi —
Yi ) log[(mj-Yi )/ (mi - j5*)], and 2^
i=1Yi log(Yi / fii ). In each case, further in-
clusion of covariates into the model reduces the deviance. For the normal linear
model, we have seen that D(y; j£) (or SSE ) has a XN-P distribution. In the
binomial case, the deviance statistic has a limiting XN -P distribution, provided
for fixed N , rrii —
> oo for each i, or in fact,
- 7r^) — > oo.
The deviance function is most useful for the comparison of nested models,
and not so much as an absolute measure of goodness of fit Consider a “ full
model” and a model reduced by a null hypothesis H0, by setting some of the
/3 coefficients to zero. Let jup and jur denote the fitted vectors under the full
and reduced models. It is easy to see that the deviance difference corresponds
to the LRT statistic for testing Ho versus the alternative of the “full model” :

CHAPTER 11. SPECIAL TOPICS
430
D{ y; fir ) - D( y; jlF ) = 2/(/IF; y) - 2l( jlr; y ).
Another criterion that enables us to assess discrepancy in fit is the generalized
Pearson X 2 statistic.
The generalized Pearson X 2 statistic defined by
Definition 11.4.5.
N
x2 =
- £i)2m&)-
(11.4.15)
i-1
For the normal model, the statistic is SSE, while for the binomial and Poisson
models, it is the usual Pearson X 2 statistic.
Model diagnostics similar to those defined in section 8.5 have been studied
in the literature (Pregibon, 1981). Diggle, Liang and Zeger (1994) describe gen-
eralized linear models for longitudinal data (binary responses, count responses
etc.) using (a) marginal models, (b) random effects models, and (c) transition
models.
11.4.4
Generalized additive models
Generalized additive models (GAMs) are an extension to the class of GLIMs,
where the linear form r -f
^ij^
j 1S replaced by the sum of component
functions, r + Y^
j=i
A suitable link function between the predictors
and the response which has a general probability distribution is chosen similar
to the GLIM setup. Specifically, we assume that the response variable Y has an
exponential family density, with mean /2 = E(Y \Xx, •
• , Xp ) which is linked to
the predictors via the functional form
(11.4.16)
S(M)=
J
'=I
We estimate r, f \, • • • , fp by an algorithm called the local scoring procedure,
which is a generalization (using local averaging) of Fisher’s scoring procedure
that we discussed under the GLIM setup, and has the following steps (for details,
see Hastie and Tibsirani, 1990).
(i) Initialization: Set r = g(Y^
=1 L^/iV), and
= 0, j = 1, • • • , p.
(ii) Update: For j = 1, • • • , p, construct an adjusted dependent variable
drji
Vi + (2H - Mi )
Zi
d^
i J 0
a0 +
and
# -
3=1
(11.4.17)

Exercises
431
Construct weights
drjA
2
dpi / ow?)-1
(11.4.18)
=
where V
^° is the variance of Y at /i®. Fit a weighted additive model to Zi,
to obtain estimated component functions fj , additive predictor 771, and
fitted values pj. Compute the convergence criterion
(11.4.19)
i-i
i=i
where,||/|| may be computed as the length of the vector of evaluations of
/ at the n sample points.
(iii) Continue (ii), replacing rf by ry1, until the convergence criterion A(ry1, rj° )
is smaller than some small value.
Exercises
11.1. Prove Result 11.1.4.
11.2. Suppose yj ~
j — 1, 2.
Also, suppose 7r(/ij,a2) oc
1/°
"^
j = 1, 2 and 7r(/ii , a2) and 7r(/42, 02) are independent. What is the
posterior distribution of {Si/Sf}/{^lA7!}?
11.3. Let y ~ iV(/xl^, cr2I/v).
(a) Suppose (/i,a2) has a normal-inverse x2 prior. Show that the poste-
rior distribution 7r(//, a2|y) also has the normal-inverse x2 form and
derive its parameters.
(b) Suppose we use a noninformative prior, i.e., 7r(^,a2 ) oc 1/a2. Derive
the form of the posterior distribution 7r(/z,a2|y). What is the form
of the marginal posterior distribution of VN ( p — Y )/ S‘?
11.4. Let F(.) denote the cdf of Y \6, and suppose nj (0 ), j = 1, • • * , L are con-
jugate prior pdfs for 6. Consider the class TT(6 ) =
wj'Kj{Q ) of finite
mixture prior densities, where the weights satisfy Wj > 0, j = 1, • • • , L
and Y^
j=I wj ~ 1- Is this also a conjugate class?
11.5. Write a univariate AR(1) model Yt = pYt-\+Vt as a dynamic linear model
and derive the Kalman filter equations.

CHAPTER 11. SPECIAL TOPICS
432
11.6. Let Zt be an observed time series at time t, t — 1, • * *
> F, which measures
a signal with error.
Suppose the signal and error terms are additive,
and the signal satisfies an AR(1) model. Assuming independent Gaussian
errors for the observation and signal processes, derive the Kalman filter
recursions.
11.7. For the single factor repeated measures model in (11.3.9), write down the
ANOVA table corresponding to the split-plot design framework. What are
the forms of the F-test statistics for the treatment effect, the time effect,
and the treatment/time interaction ?
11.8. Consider the “conditional-independence model” , i.e., the two-stage random-
effects model for longitudinal data in (11.3.10), with R* =
<j2I. Obtain
the maximum likelihood estimates of the parameters.
11.9. Let (Y \P = p) ~ Rm(ra,p), where P ~ Beta(a,(3), 0 < p < 1. For a > 0
and /? > 0, find the marginal distribution of V, and show that it does not
belong to the exponential family. Find E(Y ) and Var(Y ).
11.10. Assume that Ytl the number of fatal accidents in a year t is modeled by
a Poisson distribution with mean Xt = fio + Pit. Given data for 20 years,
derive the IRLS estimates for /?o and Pi.
11.11. Let Yi denote the number of cars that cross a check-point in one hour,
and let Y^ denote the number of other vehicles (except cars) that cross
the point. Consider two models. Under Model 1, assume that Y\ and Y^
have independent Poisson distributions with means Ai and A2 respectively,
both unknown. Under Model 2, assume a Binomial specification for Y\
with unknown probability p and sample size Y\ 4
*Y2.
(a) If we define p= Ai/(Ai -k A2), show that Model 1 and Model 2 have
the same likelihood. (Hint: Use Definition 11.4.1).
(b) What is the relationship between the link canonical functions?
(c) How can we incorporate a covariate AT, which is a highway charac-
teristic, into the modeling?

A Review of Probability
Distributions
The pdf of the normal distribution with mean /z
1. Normal distribution.
and variance a2 is given by
f -(z- M)2}
(Al)
exp <
2a2
y/ 2ixa2
where, — oo < x < oo, — oo < /z < oo, and a2 > 0. Note that E( X ) = /z, and
Var( X ) = cr2. The random variable Z — (X — /z) / <r is called the standard
normal variable with mean i£(Z) = 0, variance Var( Z ) = 1, and pdf
r -22
i
/(*) =
(A2)
, -oo < z < oo.
exp
y/2n
2
It is easy to verify that
oo
J
f { z )dz = V^
7T.
(A3)
— oo
The moment generating functions of X and Z are respectively
1
Mx(t ) = E [etx] = exp{ fit + -a2t2 }
(A4)
and
Mz (t ) = E [etz ] = exp{t2/2}.
(AS)
2. Chi-square\\ distribution.
distribution with k degrees of freedom (d.f.) if its pdf is
A random variable U has a chi-square
1
u(fc/2) le
u/2,
0 < u < oo.
(A6)
/(*) = 2fc/2r(fc/2)
The cdf of the distribution is
0,
u < 0
1 — exp{— tz/2},
u > 0.
F {u ) = P(U < u) =
433

434
Appendix
The mean and variance of a xt random variable are respectively E(U ) = k, and
Var(U ) = 2k, while its mgf is
— fc/2,
t < 1/2.
The
may be derived as a sampling distribution related to a normal ran-
dom sample. Let
, Xk+i be a random sample of size ( k -f 1) from a
N ( p,a2 ) distribution. Then the random variable U = kS2/a2 has a chi-square
distribution with k degrees of freedom, where S2 =
— X )2/ k, and
* = £ • +,1 Xi / (k + I).
3. Student’s ^-distribution.
^-distribution with k degrees of freedom has pdf
m= {r((* + l)/2) / {T ( k/ 2 ) } ( kir )-1'
2^
+t2/kYk+V / 2
Mv{t ) = (1- 21)
(A7)
A random variable T which has a Student’s
— oo < t < oo.
(A8)
Let Xu * * *
> Xe+i be a random sample from a N ( p
^ a2 ) distribution, and the
sample mean and variance respectively be computed as X =
Xi/ ( k + l )
and S2 = Ylj-i { Xi — X )2/ k. Then, the random variable \/ ( k 4- 1)(X — p)/ S
has a ^-distribution with k degrees of freedom with pdf given by the expression
in (A8). The moment generating function of T does not exist.
Recall that
E(T ) = 0, if k > 1, and Var(T ) = k/ ( k - 2) if k > 2.
4. Snedecor’s F-distribution.
distribution with numerator degrees of freedom p and denominator degrees of
freedom q if its pdf is
A random variable F has Snedecor’s F-
uv/ 2-i
r ((p + «)/2)
p/2
/(«) =
(P/2)
(A9)
0 < -u < oo.
[1+ ( p/ q ) IA](P+9)/2
r (p/2)r (9/2)
The mean and variance of the distribution are
E( F )
q/ {q- 2),
q > 2
q /
9 V p + g - 2
9 - 2 7
p(? - 4)
Var(F)
9 > 4
and the rth raw moment is
r( (p+2r)/2)r( (q— 2r)/2) (Q/ PY
r < qj2.
The mgf of the distribution does not exist. It can be shown that if
E( Fr ) =
V ( p/ 2 )r(q/ 2 )
(i) U ~ F,
(ii) U ~ tq, then U 2 ~ Fi > <?,
(hi) t/ ~ F
then 1/£/ ~ F(
p,<7 >
<7.P’
( P/ QW ~ Beta( p/ 2,q/ 2 ),
then
p> <? >
[l+(p/ <?) C/)

Appendix
435
(iv) U\ ~ Xp » U2 ~ Xg >
and f^2 are independent, then U —
j
~
Fp,q.
The F-distribution is a sampling distribution which may also be derived as
follows. Let Xi, • • * , Xni be a random sample from a N ( px,cr2 ) population, and
let Yi, • • • ,Yn2 be a random sample from a N ( py,a2 ) population and suppose
the two populations are independent. The random variable
Sl / (m -1)crl
(A10)
F = 52/ (n2 - 1)0-2
has Snedecor’s F-distribution with numerator and denominator degrees of free-
dom (ni — 1) and (n2 -1) respectively.
5. Gamma distribution.
A random variable X is said to have a Gamma(a,0 )
distribution if its pdf is
1
xa 1exp{ — x/0}, x > 0
0 )
(All)
r{a )0«
where a > 0 is called the shape parameter, 0 > 0 is the scale parameter, and
the Gamma function is defined by
00
Iu
0
a— 1 exp{— u]du.
r(a) =
(A12)
The mean and variance of the Gamma distribution are respectively
E{ X ) = a0,
and Var( X ) = a02.
The mgf of X is
Mx{t ) = [1/(1 — (3t )]a , t < l/(3.
A random variable X is said to have a Beta(a,0 )
6. Beta distribution.
distribution of the first kind if its pdf is
x“-1(l- x )13-1
f ( x ) =
(A13)
0 < x < 1
B(a j3)
where a > 0, 0 > 0, and B(a,0 ) = F(a)r(0 ) /T (a+0) is called the Beta
function. The mean and variance of X are
E( X ) = a/ (a + /?), and V a r{ X ) = a0/ { (a +0)2{a +0 + 1)}.
The mgf of X is
k-1
00
Mx(t ) = 1 + E n ( <x + j )/ (<x+0+ j )
{tk / k - }
k=l V J— 0
/

436
Appendix
which cannot be simplified further. If V ~
then
{ p/ q )V
~ Beta( p/ 2, <j/2).
(A14)
X = [1+ (p/«)K]
A random variable y is said to have a Beta(a,(3) distribution of the second
kind if its pdf is
/fo) = B^
t'“
“ 1(1 + y)
"(m+n)
Notice that the relation between X and y i s X = y/(l + y).
0 < y < oo.
7. Cauchy distribution.
distribution if its pdf is
A random variable X is said to have a Cauchy
f (x; p.,a ) = — { 1+ {x - p )/ cr }
2
(A15)
— OO < X < oo,
7TCT
where ^
is the location parameter, — oo < p < oo, and a > 0 is the scale
parameter. The mean and variance of this distribution do not exist and neither
does the mgf. Note that if U and V are independent standard normal variables,
then X = U/V has a Cauchy distribution with location 0 and unit scale. The
Cauchy distribution is a special case of Student’s ^-distribution with 1 degree
of freedom.
8.
Double exponential distribution (or Laplace distribution).
random variable X is said to have a double exponential (or Laplace) distribution
if its pdf is
A
f ( x; p,cr ) = ± exp{-— — — }
ZG
G
(A16)
— OO < X < oo,
where p is the location parameter, — oo < p < oo, and G > 0 is the scale
parameter. The pdf attains a maximum value of (2cr)“ 1 at x = 0, and tails off
to zero as x
±oo. The mean and variance of this distribution are
E( X ) = p, Var( X ) = 2a2.
The mgf of X is
Mx (t ) = exp{ pt }/ ( l - G2t2 )
The standard form of the distribution is obtained by setting p = 0, and a = 1
and has pdf
t < 1/a
f { z ) =\exp{-|z|}, -OO < 2 < OO .
9. Finite mixture distribution,
pdf
Let X denote a random variable with
(A17)
f (x ) = P i f i ( x ) H
+ PL/L(X)

Appendix
437
where pj > 0, j = 1, • • • , L, Ylj=i Pj — 1» and /j(x) are themselves valid pdfs.
Then X is said to have a finite mixture distribution with L mixands, and with
mixing proportions (or mixing weights) pi, * • • , p£. The pdfs /j(a;) are known
as the components of the mixture.
10. Mixture of normals distribution.
in the literature, these are the most widely studied finite mixture distributions.
It is assumed that the jth component of a finite mixture is a univariate normal
with mean pj and variance cr?. The pdf of a finite normal mixture with mixing
proportions pi, • • • ,PL is given by
Also referred to as normal mixtures
L
V^ft
3=1
J
2 ^
1
1
X - P j
fix ) =
(A18)
-00 < X < 00.
exp
<
>
V27T
2
This pdf is determined by (3L — 1) parameters, consisting of /ij, j = 1, • • • , L,
a?, j = 1, • • • , L, and Pj ) j — 1, • • • , L subject to the constraint
ft = 1-
A normal mixture affords a great deal of flexibility for modeling by allowing for
multimodality. The expectation and variance of a mixture random variable can
be obtained from the means and variances of its components:
L
L
L
E( X ) = p=Y^
PJPJ
and Var( X ) =
T) +
- p)2.
(A19)
3=i
3=i
j=i
11. Scale mixture of normals (SMN) distribution.
The distribution of a
univariate standardized random variable X = Z* A is said to be a scale mixture
of normals (SMN) distribution if Z has a standard normal distribution, and is
independent of A, which itself has either a discrete or a continuous distribution
over the positive half of the real line, lV
~. The pdf of X has the form
J N ( x; 6, K,(\)a2 )n(\)d\
f ( x; 0,a ) =
(A20)
n+
where K(.) is a positive function on 1Z+, the mixing density
7r(.) is a valid
probability function (either discrete or continuous), and A is called the mixing
parameter. In (A20), 6 is the location parameter and a is the scale parameter
(Andrews and Mallows, 1974).
12.
Stable distribution
A random variable X is said to have a four-
parameter stable distribution, Sa(a,(3,6 ) if its characteristic function has the
form
f exp{— |cr0|a(l — i/?sign(0) tan(7ra/2) + idO }
if
a
1
\
exp{— |<J0|(1 + ~i(3\n\0\sign(8 ) 4
* iSO }
E [exp(iOX )} =
if
a = 1
(A21)
where 9 is a real number, and

438
Appendix
1,
if 0 > 0
0,
if 0 = 0
if 0 < 0
sign(0) =
-1,
(Samorodnitsky and Taqqu, 1994). The stability parameter a lies in the range
(0, 2], and measures the degree of peakedness of the pdf and the heaviness of
its tails. When a = 2, the stable distribution reduces to a normal distribution
with mean 6 and variance 2a2. The skewness parameter 0, which lies in the
range [-1,1], measures the departure of the distribution from symmetry. The
distribution is symmetric when 0 = 0, is skewed to the right when 0 > 0,
and is skewed to the left when 0 < 0. The location parameter
<5 lies in the
range (— 00, 00), and shifts the distribution to the right or the left. The scale
parameter a lies in the range (0, 00), and is the parameter in proportion to
which the distribution of X around S is compressed or extended. When 0 — 1,
0 < a < 1, and 5 = 0, the distribution is totally skewed to the right, and is
referred to as the positive stable distribution. The pdf of this distribution is not
available in closed form.
13. Studentized range distribution.
N ( p,a2 ) random variables. Let R — max^ X* — min^ X* = X(n) — X^
) denote
the range of the n variables. Let S2 be an unbiased estimate of a2 based on v
degrees of freedom, and let vS 2 jo2 ~
independent of (Xi, • • * , Xn). Then
q — R/ S has a Studentized range distribution with (n, v ) degrees of freedom.
Let Xi, • • • , Xn be independent
14. Exponential family of distributions.
called an exponential family if it has the form
A family of pmf ’s or pdf’s is
k
- b(0)\/a( (p) + c(x,</>) }.
f {x-,6,4>) =
(A22)
1=1
Here,
(0), - - - ,^(0) are real-valued functions of the possibly vector-valued
parameter 0 (they may not depend on x), while ti ( x ),
• • ,tk ( x ) are real-valued
functions of x (they may not depend on 0), and 6(0) is called the cumulant func-
tion. If the dispersion parameter <\> is known, this is a /^-parameter exponential
family with canonical parameter 0.
For example, the N ( p,a2 ) distribution is a two-parameter exponential fam-
ily, with 0 = p,
(f) = a2, a(4>) = a2, 6(0) = p2/2, c( x,0) = -[x2/a2 +
l0g(27T<T2)]/2.
Consider a simple case of (A22)
f {x\ <9, <t>) = exp{[x6>- b(6 )\/a( (j>) + c( x, </>) },
(A23)
and let 1(6, (j)\ x ) = log f ( x; 0, 4>) denote the logarithm of the likelihood function.
Since,
E(dl/d6 ) = 0,
E(d2l/ d02 ) + E(dl/ d0 )2 = 0

Appendix
439
and
{ x - b' (0 ) }/ a( <t>),
-b" (0 )/a( <fi),
dl/ dO
dH /de2
it follows that E{ X ) = ft, = b' (6 ), and Var( X ) = b"(6 )a{4>). The function b" {6 )
depends on the canonical parameter, and hence on p, and is called the variance
function, and denoted by V ( p).
A random variable X has an inverse
15. Inverse Gamma distribution.
Gamma distribution if
f ( x\a,0 ) — {T(a)} l0 ax
exp(-l/x/3),
x > 0,a > 0, /? > 0.
(A24)
We can verify that E( X ) = 1/ { /3(a — 1)} if a > 1, and Var( X ) — 1/ { /32(a -
l)2(a — 2) if a > 2. Also, the random variable l/ X has a Gamma(a,0 ) distri-
bution.
A p-dimensional random vector x has the
16. p-variate ^-distribution.
p-variate ^-distribution with location vector 5, a p.d. scale matrix fl and degrees
of freedom v if its pdf is
f ( x\8M
= {r[(i/ + p)/ 2]/T { iy/ 2 ) }\n\-1/ 2( iy7r )-p/ 2
x
{l -f (x- <5)'f2_1(x- d ) }-(i/+p)/2
(A25)
Then, (x —
*(x — 5)/ p ^ F]p,w


Solutions to Selected
Exercises
Chapter 1
1.1. Verify that |a•b| <|| a
1.3. a = ±l/\/2 and b = ±l /V2 .
1.7. Yes.
b II .
1.9. Obtain conditions for AB = BA.
1.10. A k = (
1 3
1.11. Show that C = A*-1 + Afc~2B + • • • + B*-1
where, (3 = 6(1 + a 4
4- a k
x ).
1
1.14. Use property 4 of Result 1.2.8.
1.16. -8.
1.17. An- (1 + a2 + a4 + • • • + a2n) = [1- a2(n+1>]/[1- a4].
1.19. 1+ J2U*.
1.23. For (a), use the definition of orthogonality and Result 1.2.9. For (b), use
Definition 1.2.8 and Definition 1.2.21.
1.24. Use properties 1 and 3 of Result 1.2.16.
1.25. The dimension of the column space is 2.
1.27. r(A) = 2.
1.29. The eigenvector corresponding to A = 3 is v = t(l,1,1)', for arbitrary t.
1.32. Use the Cauchy-Schwarz inequality.
441

Solutions
442
Chapter 2
2.3. Use Result 2.1.2 to write the determinant of the matrix as |1||P — xx'|.
2.4. For (a), use Result 1.2.29. For (b), use (a) and property 5 of Result 1.2.12.
2.6. For (a), use Result 2.1.2. To prove (b), let A be a nonzero eigenvalue of
AB, so that we have P( A) = |AB— AIn| = 0, where P( A) is the charac-
teristic polynomial of AB; use (a).
2.7. Show that Aa = aa'a = (a'a)a, and use Definition 1.2.32.
2.8. Write (A + BC) = A(I^ + A XBC), and use Result 2.1.2 on the matrix
-A-1B
h
C
In
2.10. Write (A + aa') = A( Ik + A_1aa') and use Result 2.1.2.
2.14. (a) yes; (b) symmetric but not idempotent in general.
2.15. Construct an n x (n — k ) matrix V such that the columns of (U, V) form
an orthogonal basis for Rn.
2.19. Given that QAQ-1 = D, invert both sides.
2.20. For (a), use the fact that the trace of a matrix is the sum of its eigenvalues.
For (b), use Result 2.3.10.
2.21. Suppose on the contrary, that r(C) < <7, and use Exercise 2.10 to contra-
dict this assumption.
/
y/2
0
0 \
2.23. B = ( -l / y/2
l / y/2
0
.
V-1/V2
l / y/2
y/Zj
2.24. — l/(n - 1) < a < 1.
2.25. Use the simultaneous diagonalization of A and B and choose a A such
that j > d > j > d i V i = l, -
*
* , n, o r l — Ad* > 0 V i =
2.29. For an n x n nonsingular matrix P such that P-1A*P = D*, i = 1, • • • , fc,
show that for j ^
i = 1, * • • , fc, P-1AjA^P = P_1A^
AjP.
2.31. Using the properties of Kronecker product and trace, the result follows
directly.
2.32. If possible, let Pi and Pi be two such matrices. Since u is unique, (Pi —
P2)y = 0 for all y £ Pn, so that Pi must equal P2.
1, * * *

Solutions
443
Chapter 3
-1
2
0
1
-1
0
3.1. For (a), r(A) —
2, and A
=
/1/7
2/7
0\
3/7 -1/7
0
0
0
0
'
0
0
0/
. For (b), r(A) = 2, and
A" =
3.2. Verify (3.1.1).
3.4. For (a), use property 1 and 2 of Result 3.1.8. For (b), transpose both sides
of (a). Use (3.1.7) to obtain (c). For (d), use property 3 of Result 3.1.8.
3.6. Use Definition 3.1.1.
3.10. AG2X = AG2AG1X = AGix = x.
3.12. This follows from property 2 of Result 3.1.8.
3.13. (a) Let G = HA
” 1. Then, ABGAB = ABHA
_1AB = ABHB = AB.
Conversely, let ABGAB = AB. That is, ABGABHB = ABHB, i.e.,
GABHB = HB, i.e., GAB = HB. The solution for (b) is similar.
3.15. Using Result 3.2.2, the system is consistent.
3.16. The unique solution is (— 1, — 4, 2, — 1)'.
3.17. This follows directly from Result 3.2.9 (see proof of this result).
3.18. C(c) C C(A) and 7£(b) C 7£(A); use Result 3.2.9.
Chapter4
4.1. Use property 8 of Result 1.2.12 to show that r(X'X, X'y) > r(X'X).
Use properties 4 and 6 of Result 1.2.12 to show that r(X'X, X'y) =
r[X'(X, y)] < X' = r(X'X).
4.3. 6 = 2ZLmt ZLti, and Var(e ) = 4a2/ ZL tf .
4-4- v = ZiLi SiUl ZiLi
and Var(v ) = a2/ ZiLi A-
4.5. (3 = (0, 3,1)', and a2 — 4.
4.6. ni = 2ri2.
4.9. (i) (3\ = {V2 + Y\ -h Ye — Y\ — I3 — Ys)/6, with variance cr2/6. (ii) Pi =
{5(Y2 - Yb ) + 8(Y4 - Ys ) + ll(y6 - Yi)}/48, with variance 420(j2/(48)2.
The ratio of variances is then 32/35.
4.10. Argue that there are no d.f. for the error.

444
Solutions
4.11. Use (3.1.7).
4.17. For (a), equate (4.5.5) with (4.2.4) and set y = Xz.
(b) follows since
V-1 = (1- p)_1I — p(1- p)"1!1 + ( N - l)p]-1J, and l'y = 0.
y, and pr = ( IN - VC(C'VC)_1C'y.
4.24. p
Chapter 5
5.1. (b) 7T/ \/3.
5.2. a = 19.65.
5 -1
2\
-1
3
1
.
2
1
6/
4
-2
and E —
5.4. p =
1
5.7. 1/3.
5.9. From Result 5.2.6, y ~ Nk (0,cr2PP'); use the orthogonality of P.
5.13. For(a),
~ 7V(p2, a2) and X$ ~ V(p3,a2). For (b) ( X\\ X 2, Xz ) is
P
2) - p2( x3 - M3>/(1- p2 ) and
normal with mean pi + p(x2 — p2)/(l
variance
<J2(1 — 2p2)/(l — p2); it reduces to the marginal distribution of
Xi when p = 0. For (c), p = — 1/2.
5.21. For (a), by Result 5.4.5, and Result 5.3.4, E(U ) = k + 2A and Var(U ) =
2( k + 4A). For (b), using Result 5.4.2, U ~ x2(& > A) with A — p'E_1p.
For (c), x'Ax ~ x
2(^
_ l, p'ap/2).
5.29. By Result 5.2.5, X ~ (p, [1 + (/c - l)p]a2/fc). Suppose Q = x'Ax, with
A = [I — ll']/cr2(l — p). By Result 5.4.5, Q ~ X2_i - By Result 5.4.8, X
and Q are independent since ^
l'[I — 11' / k ] = 0.
5.31. For (a), use Result 5.4.7. For (b), use Result 5.4.5.
5.32. Verify that Q\ = x'AiX, and Q2 = x'A2X, with
Ai =
AXEA2 = 0.
5.34. The proof follows from Driscoll and Krasnicka (1995).
(-1
'0'“ dA’ = (i 1). Use idempotency of A\E and that
Chapter 6
6.1. Since EML = { ( N - 1 )/ N }Sn, \%ml\n/ 2 = [^
]
tuting this into the expression (6.1.10),
k N/ 2 |Siv|N/2; substi-
exp(-^
)
(27r) Nfc/ 2[
iV-J.]
fe"/2\sN \w

Solutions
445
6.2. The first identity is immediate from the definition of the sample mean,
and some algebra. To show the second identity, write
m+1
^ ^
(x^
Xm+i)(Xi
Xm_
j_i)
Sm+1
i-1
m
^
T
^
(Xj
X m + X m
Xm+l )(xz
Xm + Xm
Xm+i)
i=1
"b
(xm+l
xm+l )(xm+l
xm+l )
—
+ m(xm
Xm^
_ x)(xm
Xm_|_i)
H”
(xm+ l — xm+l)(xm+l — xm+l) ,
use the first identity for xm+i and simplify.
6-7- ^
(1,... tfc) = l-|S|/{Soo|S<1)|}.
6.8. Use (6.2.11) and (A14).
Chapter 7
7.1. Property 1 is a direct consequence of Result 5.2.6, while property 2 follows
from Result 5.2.7. Property 3 and property 5 are direct consequences of
Result 5.4.5, while property 4 follows from the orthogonality of X and
I - P.
7.2. (c) The test statistic Fo = ( N — 3)Q/ SSE ~ F^
g under HQ where Q =
E AXi(200 + PiXi + 2p2xf ), and SSE =
- p0 -
- P2Xf ).
7.5. Let N = m + n2 + n3 . SSEH = E?=i E”ii Yij ” Ei=i
)2/N, and
SSE = E -=1E;=i 1$ ~ Ei=i yi2/n«-
so that F( H ) = ( N - 3)( SSEH -
SSE )/ {2SSE ) ~ F2 <N-3 under if.
7.7. The MLE is 0 = [£f=1 UfTt-i]/[Et=i*?]•
7.8. The 95% C.I. for Pi is px ± 4.73^/1174. For p2, it is P2 ± 4.73\/3. For p3,
it is /?3 ± 4.73. For (5\ — /32, it is (3\ - 02 ± 4.73>/59/4. For /?i + /?3, it is
ft + iS3 ± 4.73^/2775.
7.9. Scheffe’s simultaneous confidence set for c'/3 has the form {C'PQLS ±
°GLs [sFs,N -r,ac
/(X'V-1X)-1c]1/2} for all c
£, with dim(£) =
r(X) = r.
7.14. (a) Let /3' = ( Xu A2). Then, Ax = [(l+c2)n-c3yfc+i-c2n+2]/(l+c2+c4)
and A2 = [cYk + F/c+i - c(l -F c2)yfc+2]/(1+ c2 + c4), with
1+ c2
c
<7*
Var(/3) =
1+ c2 /
*
(1+C2+C4)
C

Solutions
446
(b) Under the restriction Ai
CY/C+2)/2(1 + c 4- c2), with Var(A) = <T2/2(1 4- c 4- c2).
(c) Q/<T2 ~ Xi > where Q =
c(1-he 4- c2)lfc+2]}2-
7.15. Fo ={2(r1- y2)2}/(Eti^
)2-
7.21. Write cj = l/{^(i -f 1)}1/2(0J1,
• • ,1, — Z,0, • • • , 0). The 95% marginal C.I.
for c[ p is c|/3° ± ta(n-1),.025 x
s.e. cj/?0).
7.22. Due to orthogonality, the least squares estimates of (3Q and /?i are un-
changed, and equal to /3o = (Fi + I2 + l3)/3, and (3\ = (F3 — Y\ )/ 2.
7.28. F(tf ) = (3n-3)Q/SSF- F1)3n-3 under H , where Q = % [Y 2.- ± {YV +
FL )]2, and SSE =£f=1E”=i(^
-F,)2.
7.30. The mgf of By is MBy(t) = My (B't) = ^
(t'BVB'tJexpjt'B/i}. The
result follows.
-A2 = A, A = \Yk - (1 + c)Yk+1 +
(l-c+c2 )
[(l4-c+c2)n-h (l-c3)n+i-
(l+C^+C4)
7.31. Differentiating the expression in (7.5.22) twice with respect to 0, and take
expectations over the distribution of ymis given ycbs and
Chapter 8
8.1. Var(0i ) = o2 jYlZ=i ( Xn ~ Ai)2(l ~ r22). ^he width of the 95% confi-
dence interval for /?i is 2^-3,.025s.e.(/?i).
~ Fi }N-k-u where Q = ( fy - d )2/ajj, ajj being
Q
8.2. F(Jf ) = S S E/ ( N - k- l )
the jth diagonal element of (X'X) ” 1. Also, s = 1, and SSE = y'(I —
X(X'X)_1X')y which has a chi-square distribution with N — k — 1 degrees
of freedom.
8.3. MSE(a2 ) = 2<J 4/ { N - p), while MSE(y'Aiy) = 2cr4/(iV- p + 2).
8.6. (a)3i = Y i, i = 1, 2, /3 = [5$, + S’xyl/I^xx + S&
The vertical
distance between the lines is D = (ai - a2 ) + (3{ X\ - X 2 ), with D =
(Yi -F2) -
- X 2 ). Then, E( D ) = D.
(b) A 95% symmetric C.I. for D is D ± 5[i?ii„l+ri2_3i.05]1/,2s.e.(Z)).
8.8. SSE{i ) = SSE - ef /(l - pa ) = SSE - rfa2
{i ) (see Result 8.5.3), from
which the result follows after simplification.
8.9. By Example 2.1.2, Pz = Px + {(I - Px)yy'(I - Px)}/y'(I - Px)y =
Px + ee' /i's.
8.10. From (8.5.16), SIC, = ( N - 1){T ( FN ) - T(FW )} - ( N - 1 )0- p{i ) ).
Simplify using (8.5.24).

Solutions
447
8.15. Use Exercise 8.9.
Chapter 9
Uij = Uu +
^ = Ni. - E/=1E“=1 Tiijnij / N . j
Ni. — Ni.= 0, i = 1, * • • , a. This implies r(U) < a — 1.
9.3. (b) In (9.2.5), in order that £(E“=i Ej=i cijYij ) = A*E =I Ej
E“=i(Ej=i cij )Ti + Ej=i(E“=i cij )Pj is t0 be function only of ff's,
must have E“=i Ej=1 cv = 0, Ej=i cij = 0, i = 1, • • • , a. Then, RHS =
j=1 diPh where E$=i d3 = Ej=i E“=i
= o.
9.2. Ula = E?J= I
Cjj +
j=l
we
E5=I(E =I Cij)/3j = Ej
9.7. (a) Yes, (b) No.
9.10. /3°
= Yij. /Uij = Fjj. , j = I , -
* - , &i
*, z = l, - - - , a. The corresponding
g-inverse is G =
itt)
0
0
0
D
where D = diag{1/ rtij}.
9.15. (a) SS(A>, /?i) = [r2/iV] +
~ ^)2-
(b) SS( fiyru • • • ,r«) = E -=i (^
2M) - Y-
2/ N.
Hence, SS(/?0, /3i)— S£(/x, TI, " " " , ra ) = A/B, where A = [Ej=i
1 __
(Ei=i^di)2] and 5 = Ei=in<(^t “ 2)2j, Q =
- ^- j and
di = >/ni(Zi — Z). The difference in SS is always nonnegative (by Cauchy-
Schwarz inequality), with equality only when a oc di, i = 1, • • ,a, i.e.,
only when T*. — T.. = w( Z{ — Z), where u; is a constant.
9.16. F( H ) = (a&c- 2a&)(SS-E/j -5S£)/(a6- 1 )SSE
Fab-i abc-2ab under
H , where SSEH = E,EjEfc (^
-7(^
-^i-)]a ~ xL-(a6+i).
and SS# = EiEjE/cKjfc ~ ^tj- “
— Zij- )]2-
Chapter 10
10.2. Let the N x (a 4-1) matrix X = (1^, xi, • • • ,xa), where the N-dimensional
vector Xi = (0, ln, 0)', with the l’s in the zth place.
Then, SSA —
E -=i^
2/n-ivF
2 = y'{iE:
10.3. Suppose SSA = y'My, then, E( SSA) = tr(MV), where V denotes
Cov(y) = a\IN + a2 E“=i Jn! simplify.
10.6. Let SSA= E, nb(Yt..-Y ... )2, SSB( A) = E,Ejn(Fw.-F;..)2,
=
EjEj Efc(^ijfc “ Fjj. )2 with respective d.f. (a-1), (a(b-1) and ab(n-
1).
The ANOVA estimators are d2 —
{ MSA — MSB( A) }/ nb,
=
{ MSB( A) - MSE }/ n, and d2 —
MSE. The ML solutions are oT =
. 2
.
2
{(1 — 1/a)MSA — MSB( A) }/ nb, while
and a£ coincide with their
ANOVA estimators.
a+ Xjx' — ^
ljvl'N}y, which gives the result.

448
Solutions
10.7. The ML equations are
SSA
, SSB
, SSAB + SSE
. 2
. 2
+
1
^
a — 1
6 — 1
abn — a — 6 + 1
. 2
04
0u
012
0o
012
0Q
55,4
1
a-1
— + —
. 2
04
011
011
SSB
1
6 - 1
_— |— —
04
012
. 2
012
where 0o = 0i = of , 0n =
+ 6n<72, 0\2 =
+ ancrj|, and 04 —
0n + 0i2 - 0o- The explicit MLE of 02 is MSE, while estimates of the
remaining variance components must be obtained by numerical solution
of these nonlinear equations.
Chapter 11
11.2. F/v!
11.4. Yes.
11.5. The state equation is xt+i = pxt + wt, and the observation equation is
Yt = xt.
11.6. The autoregressive signal-plus-noise model is defined by
Yt + vt
pYt-1 + Wt,
vt and wt are independent error processes, vt ~ iV(0, <72) and wt ~
7V(0, <72,).
Express this in the form of (11.2.1) and (11.2.2), and the
Kalman filter recursions are obtained from (11.2.7)-(ll.2.9).
11.8. In the formulas in section 11.3.2, set V* = cr2l + Z*D7Z'.
11.9. Y has a Beta-Binomial distribution with E(Y ) = ma/ (a+/3) andVar(Y ) =
maf3/ (a + /3)2.
Zt
Yt

References
Akaike, H. (1974). A new look at the statistical model identification. IEEE
Trans. Aut. Cont., AC-19, 716-723.
Akaike, H. (1978). A Bayesian analysis of the minimum AIC procedure. Ann.
Inst. Statist. Math., 30, 9-14.
Akritas, M.G. (1990). The rank transform method in some two-factor designs.
J. Am. Statist. Assoc., 85, 73-78.
Akritas, M.G. (1991). Limitations of the rank transform procedure: a study of
repeated measures designs, Part I. J. Am. Statist. Assoc., 86, 457-460.
Akritas, M.G. (1993). Limitations of the rank transform procedure: a study of
repeated measures designs, Part II. Stat. Prob. Letters, 17, 149-156.
Akritas, M.G. and Arnold, S.F. (1994). Fully nonparametric hypotheses for fac-
torial designs I: multivariate repeated measures designs. J. Am. Statist. Assoc.,
89, 336-343.
Anderson, T.W. (1984). An Introduction to Multivariate Statistical Analysis
2nd edition, J. Wiley & Sons, New York.
Anderson, T.W. and Fang, K.T. (1987). Cochran’s theorem for elliptically con-
toured distributions. Sankhya, A49, 305-315.
Andrews, D.F., Gnanadesikan, R. and Warner, J.L. (1971). Transformations of
multivariate data. Biometrics, 27, 825-840.
Andrews, D.F. and Mallows, C.L. (1974). Scale mixtures of normal distribu-
tions. J. R. Statist. Soc., B36, 99-102.
Andrews, D.F. and Pregibon, D. (1978). Finding outliers that matter. J. R.
Stat. Soc., B40, 85-93.
449

450
References
Ansley, C.F. (1985). Quick proofs of some regression theorems via the QR al-
gorithm. Am. Statistician, 39, 55-59.
Antoniadis, A., Gregoire, G. and McKeague, I.W. (1994). Wavelet methods for
curve estimation. J. Am. Statist. Assoc., 89, 1340-1353.
Atiqullah, M. (1962). The estimation of residual variance in quadratically bal-
anced least squares problems and the robustness of the F-test. Biometrika, 49,
83-91.
Atkinson, A.C. (1981). Two graphical displays for outlying and influential ob-
servations in regression. Biometrika, 68, 13-20.
Atkinson, A.C. (1985). Plots, Transformations and Regression. Oxford Univer-
sity Press, UK.
Azzalini, A. and Dalla Valle, A. (1996). The multivariate skew-normal distrib-
ution. Biometrika, 83, 715-726.
Bartlett, M.S. (1937). Properties of sufficiency and statistical tests. Proc. R.
Soc., A160, 268-282.
Bassett, G, and Koenker, R. (1978). Asymptotic theory of least absolute error
regression. J. Am. Statist. Assoc., 73, 618-622.
Basu, D. (1964). Recovery of ancillary information. Sankhya, 26, 3-16.
Beckman, R.J. and Trussel, H.J. (1974). The distribution of an arbitrary stu-
dentized residual and the effects of updating in multiple regression.
J. Am.
Statist. Assoc., 69, 199-201.
Belsley, D. A., Kuh, E., and Welsch, R.E. (1980). Regression diagnostics. John
Wiley & Sons, New York.
Belsley, D.A. (1984). Demeaning conditioning diagnostics through centering.
Am. Statistician, 38, 73-93.
Belsley, D.A. (1991). Conditioning Diagnostics, Collinearity and Weak Data in
Regression. John Wiley & Sons, New York.
Berger, J.O. (1980). Statistical Decision Theory and Bayesian Analysis. Springer-
Verlag, New York.
Bickel, P.J. and Doksum, K.A. (1981). An analysis of transformations revisited.
J. Am. Statist. Assoc., 76, 296-311.

References
451
Birkes, D. and Dodge, Y. (1993). Alternative Methods of Regression. John Wi-
ley & Sons, New York.
Bishop, C.M. (1995). Neural Networks for Pattern Recognition. Oxford Univer-
sity Press, Clarendon.
Bloomfield, P. and Steiger, W.L. (1983). Least Absolute Deviations. Birkhauser,
Boston.
Box, G.E.P. and Cox, D.R. (1964). An analysis of transformations. J. R. Stat.
Soc., B26, 211-252.
Box, G.E.P. and Tiao, G.C. (1973). Bayesian Inference in Statistical Analysis.
Addison-Wesley, Reading, Massachusetts.
Branco, M. and Dey, D.K. (2001). A general class of multivariate skew-elliptical
distributions. J. Mult. Anal., 79, 99-113.
Breusch, T.S. and Pagan, A.R. (1979). A simple test for heteroscedasticity and
random coefficient variation. Econometrica,, 47, 1287-1294.
Brockwell, P. J. and Davis, R. A. (1987). Time Series: Theory and Methods.
Springer-Verlag, New York.
Brown, R.L., Durbin, J. and Evans, J.M. (1975). Techniques for testing the
constancy of regression relationships (with discussion).
J. Roy. Stat. Soc.,
B37, 149-63.
Carroll, R.J. and Ruppert, D. (1988). Transformations and Weighting in Re-
gression. Chapman & Hall, London.
Casella, G. and Berger, R.L. (1990). Statistical Inference. Wadsworth & Brooks
Cole, California.
Charnes, A., Cooper, W.W. and Ferguson, R.O. (1955). Optimal estimation of
executive compensation by linear programming. Mgt. Sci., 1, 138-151.
Chatterjee, S. and Hadi, A.S. (1988). Sensitivity Analysis in Linear Regression.
John Wiley & Sons, New York.
Chatterjee, S. and Price, B. (1991).
Regression Analysis by Example. John
Wiley & Sons, New York.
Chen, M.-H., Dey, D.K. and Shao, Q.-M. (1999). A new skewed link model for
dichotomous quantal response data. J. Am. Statist. Assoc., 94, 1172-1186.

References
452
Cheng, B. and Titterington, D.M. (1994). Neural networks: a review from a
statistical prespective. Stat. Sci., 9, 2-54.
Chmielewski, M.A. (1981). Elliptically symmetric distributions: a review and
bibliography. Inter. Stat. Rev., 49, 67-74.
Cochran, W.G. (1934). The distribution of quadratic forms in a normal system,
with applications to the analysis of covariance. Proc. Camb. Phil. Soc., 30,
178-191.
Cochrane, D. and Orcutt, G.H. (1949). Application of least-squares regressions
to relationships containing autocorrelated error terms. J. Am. Statist. Assoc.,
44, 32-61.
Conover, W.J. (1980). Practical Nonparametric Statistics. John Wiley & Sons,
New York.
Conover, W.J. and Iman, R.L. (1981). Rank transformations as a bridge be-
tween parametric and nonparametric statistics (with discussion). Am. Statisti-
cian, 35, 124-129.
Cook, R.D. and Weisberg, S. (1980). Characterization of an empirical influence
function for detecting influential cases in regression. Technometrics, 22, 495-
508.
Cook, R.D. and Weisberg, S. (1982). Residuals and Influence in Regression.
Chapman & Hall, New York.
Cook, R.D. and Weisberg, S. (1994). An Introduction to Regression Graphics.
J. Wiley h Sons, New York.
Cox, D.R. and Small, N.J.H. (1978).
Testing multivariate normality.
Bio-
metrika, 65, 263-272.
Craig, A.T. (1943). Note on the independence of certain quadratic forms. Ann.
Math. Stat., 14, 195-197.
Daniel, C. and Wood, F.S. (1971). Fitting Equations to Data: Computer Analy-
sis of Multifactor Data. John Wiley & Sons, New York.
David, F.N. (1938). Tables of the Correlation Coefficient. Cambridge Univer-
sity Press, Cambridge, UK.
Dempster, A.P., Laird, N.M. and Rubin, D.B. (1977). Maximum likelihood es-
timation from incomplete data via the EM algorithm (with discussion). J. R.

References
453
Stat. Soc., B39, 1-38.
Devlin, S.J., Gnanadesikan, R. and Kettenring, J.R. (1976). Some multivariate
applications of elliptical distributions. In Essays in Probability and Statistics
(S. Ikeda, ed.), 365-395, Shinko Tsusho, Tokyo.
Diaconis, P. and Shahshahani, M. (1984). On non-linear functions of linear
combinations. SIAM J. of Scientific and Statistical Computing, 5, 175-191.
Diggle, P.J., Liang, K.-Y., and Zeger, S.L. (1994).
Analysis of Longitudinal
Data. Oxford University Press, UK.
Dixon, L.C.W. (1972).
Nonlinear Optimization.
English Universities Press,
London.
Dodge, Y. and Jureckova, J. (2000). Adaptive regression. Springer-Verlag, New
York.
Donoho, D.L. and Johnstone, I.M. (1994). Ideal spatial adaptation by wavelet
shrinkage. Biometrika, 81, 425-455.
Draper, N.R. and Smith, H. (1998). Applied Regression Analysis. 3rd edition.
John Wiley & Sons, New York.
Draper, N.R. and John, J.A. (1981). Influential observations and outliers in
regression. Technometrics, 23, 21-26.
Driscoll, M.F. and Krasnicka, B. (1995). An accessible proof of Craig’s theorem
in the general case. Am. Statistician, 49, 59-62.
Driscoll, M.F. (1999). An improved result relating quadratic forms and chi-
square distributions. Am. Statistician, 53, 273-275.
Duncan, D.B. (1955). Multiple range and multiple F-tests. Biometrics, 11,
1-42.
Dunnett, C.W. (1964). New tables for multiple comparisons with a control.
Biometrics, 20, 482-491.
Durbin, J. and Watson, G.S. (1950).
Testing for serial correlation in least
squares regression I. Biometrika, 37, 409-428.
Durbin, J. and Watson, G.S. (1951).
Testing for serial correlation in least
squares regression II. Biometrika, 38, 159-177.

References
454
Dyer, D.D. and Keating, J.P. (1980). On the determination of critical values
for Bartlett’s test. J. Am. Statist. Assoc., 75, 313-319.
Ellenberg, J.H. (1973). The joint distribution of the standardized least squares
residuals from a general linear regression. J. Am. Statist. Assoc., 68, 941-943.
Englefield, N. J. (1966). The commuting inverses of a square matrix.
Proc.
Camb. Phil. Soc., 62, 667-671.
Fang, K.T. and Anderson, T.W. (1990).
Statistical Inference in Elliptically
Contoured and Related Distributions. Allerton Press Inc., New York.
Fang, K., Kotz, S. and Ng, K. (1990). Symmetric Multivariate and Related
Distributions. Chapman & Hall, London.
Fisher, R.A. (1921). On the probable error of a coefficient of correlation de-
duced from a small sample. Metron, 1, 1.
Fraser, D.A.S. and Ng, K.W. (1980).
Multivariate regression analysis with
spherical error, in Multivariate Analysis V (ed. P.R. Krisnaiah). North-Holland,
New York, p. 369-386.
Freund, R.J., and Littell, R.C. (1991). SAS System for Regression. SAS Insti-
tute Inc.
Friedman, J.H. and Tukey, J.W. (1974).
A projection pursuit algorithm for
exploratory data analysis. IEEE Transactions on Computers, C23, 881-890.
Friedman, J.H. and Stuetzle (1981). Projection pursuit regression. J. Am. Sta-
tist. Assoc., 76, 817-823.
Ghosh, M. (1996). Wishart distribution by induction. Am. Statistician, 50,
243-246.
Gnanadesikan, R. (1977). Methods for Statistical Data Analysis of Multivariate
Observations. John Wiley & Sons, New York.
Goldfeld, S.M. and Quandt, R.E. (1965). Some tests for homoscedasticity. J.
Am. Statist. Assoc., 60, 539-547.
Golub, G.H. and Styan, G.P. (1973). Numerical computations for univariate
linear models. J. Stat. Comput. Simul, 2, 253-274.
Golub, G.H. and Van Loan, C.F. (1989). Matrix Computations. 2nd edition.
Johns Hopkins University Press, Baltimore.

References
455
Golueke, C.G. and McGauhey, P.H. (1970).
Comprehensive studies of solid
waste management. U.S. Department of Health, Education, and Welfare, Pub-
lic Health Services Publication No. 2039.
Graybill, F.A. (1954). On quadratic estimates of variance components. Ann.
Math. Stat., 25, 367-372.
Graybill, F.A. (1961). An Introduction to Linear Statistical Models. McGraw-
Hill, New York.
Graybill, F.A. (1983). Matrices with Applications in Statistics. 2nd edition.
Wadsworth, Belmont, CA.
Graybill, F.A. and Hultquist, R.A. (1961). Theorems concerning Eisenhart’s
Model II. Ann. Math. Stat., 32, 261-269.
Guenther, W.C. (1964). Another derivation of the noncentral chi-square distri-
bution. J. Am. Statist. Assoc., 59, 957-960.
Gupta, S.S. (1963). Probability integrals of multivariate normal and multivari-
ate t. Ann. Math. Stat., 34, 792-828.
Gupta, A.K. and Varga, T. (1993). Elliptically Contoured Models in Statistics.
Kluwer Academic Publishers, Netherlands.
Halmos, P.R. and Savage, L.J. (1949). Applications of the Radon-Nikodym the-
orem to the theory of sufficient statistics. Ann. Math. Stat., 20, 225-241.
Hampel, F.R. (1974). The influence curve and its role in robust estimation. J.
Am. Statist. Assoc., 62, 1179-1186.
Harrison, J. and West, M. (1991). Dynamic linear model diagnostics.
Bio-
metrika, 78, 797-808.
Harvey, A.C. and Phillips, G.D.A. (1974). A comparison of the power of some
tests for heteroscedasticity in the general linear model. J. Econ., 2, 307-316.
Harville, D.A. (1974). Bayesian inference for variance components using only
error contrasts. Biometrika, 61, 383-385.
Harville, D.A. (1976). Extension of the Gauss-Markov theorem to include the
estimation of random effects. Ann. Statist., 4, 384-395.
Harville, D.A. (1977). Maximum likelihood approaches to variance component
estimation and to related problems (with discussion). J. Am. Statist. Assoc.,

References
456
72, 320-340.
Harville, D.A. (1997). Matrix Algebra From a Statistician’s Perspective. Springer-
Verlag, New York.
Hastie, T. J. and Tibsirani, R. J. (1990). Generalized Additive Models. Chap-
man & Hall, London.
Hayes, J.G. (1974). Numerical methods for curve and surface fitting. J. Inst.
Math. Appl., 10, 144-152.
Hayes, K. and Haslett, J. (1999). Simplifying general least squares. Am. Sta-
tistician, 53, 376-381.
Hertz, J., Krogh, A. and Palmer, R.G. (1991). Introduction to the Theory of
Neural Computation. Addison-Wesley, CA.
Heyadat, A. and Robson, D.S. (1970). Independent stepwise residuals for test-
ing homoscedasticity. J. Am. Statist. Assoc., 65, 1573-1581.
Heyadat, A., Raktoe, B., and Telwar, O. (1977). Examination and analysis of
residuals: a test for detecting a monotonic relation between mean and variance
in regression through the origin. Comm. Statist., A6, 497-506.
Hinkley, D.V. (1975). On power transformation to symmetry. Biometrika, 63,
101-111.
Hoaglin, D.C. and Welsch, R.E. (1978).
The hat matrix in regression and
ANOVA. Am. Statistician, 32, 17-22.
Hochberg, Y. and Tamhane, A.C. (1987).
Multiple Comparison Procedures.
John Wiley & Sons, New York.
Hoerl, A.E. (1962). Application of ridge analysis to regression problems. Chem-
ical Engineering Progress, 58, 54-59.
Hoerl, A.E. and Kennard, R.W. (1970a). Ridge regression: biased estimation
for nonorthogonal problems. Technometrics, 12, 55-67.
Hoerl, A.E. and Kennard, R.W. (1970b).
Ridge regression: applications to
nonorthogonal problems. Technometrics, 12, 69-82.
Hollander, M. and Wolfe, D.A. (1973). Nonparametric Statistical Methods. John
Wiley & Sons, New York.

References
457
Huber, P. (1964). Robust estimation of a location parameter.
Ann. Math.
Statist., 35, 73-101.
Huber, P. (1981). Robust Statistics. John Wiley & Sons, New York.
James, G.S. (1952). Notes on a theorem of Cochran. Proc. Camb. Phil. Soc.,
48, 443.
John, J.A. and Draper, N.R. (1980). An alternate family of transformation.
Appl. Statist., 29, 190-197.
Johnson, N.L. and Kotz, S. (1972). Distributions in Statistics: Continuous Mul-
tivariate Distributions. John Wiley & Sons, New York.
Johnson, R.A. and Wichern, D.W. (1988).
Applied Multivariate Statistical
Analysis. 2nd edition. Prentice-Hall, Englewood Cliffs, NJ.
Judge, G.G., Griffiths, W.E., Hill, R. C., Lutkepohl, H. and Lee, T.-C. (1985).
The Theory and Practice of Econometrics. John Wiley & Sons, New York.
Judge, G.G., Griffiths, W.E., Hill, R. C., Lutkepohl, H. and Lee, T.-C. (1988).
Introduction to the Theory and Practice of Econometrics. John Wiley & Sons,
New York.
Kelker, D. (1970). Distribution theory of spherical distributions and a location-
scale parameter generalization. Sankhya, A32, 419-430.
Kendall, M. G. and Stuart, A. (1958). The Advanced Theory of Statistics, Vol.
1. Hafner Publishing Co., New York.
Kendall, M.G. and Stuart, A. (1963). The Advanced Theory of Statistics, Vol.
2. Hafner Publishing Co., New York.
Keuls, M. (1954). Testing differences between means in an analysis of variance.
Biometrics, 10, 167-168.
Khuri, A. I. (1999). A necessary condition for a quadratic form to have a chi-
squared distribution: an accessible proof. Int. J. Math. EDuc. Sci. Technol.,
30, 335-339.
Kruskal, W.H. (1952). A nonparametric test fo the several sample problem.
Ann. Math. Stat., 23, 525-540.
Kruskal, W. (1968). When are Gauss-Markov and least squares estimators iden-
tical? A coordinate-free approach. Ann. Math. Stat., 39, 70-75.

References
458
Kruskal, W.H. and Wallis, W.A. (1952). Use of ranks in one-criterion variance
analysis. J. Am. Statist. Assoc., 47, 583-621.
Laha, R.G. (1956). On the stochastic independence of two second-degree poly-
nomial statistics in normally distributed variables.
Ann.
Math.
Stat., 27,
790-796.
Laird, N.M. and Ware, J.H. (1982). Random-effects models for longitudinal
data. Biometrics, 38, 963-974.
Larsen, W.A. and McCleary, S.A. (1972). The use of partial residual plots in
regression analysis. Technometrics, 14, 781-790.
Levene, H. (1960). In Contributions to Probability and Statistics. Stanford Uni-
versity Press, Stanford, CA., p. 278.
Liang, K.-Y. and Zeger, S.L. (1986). Longitudinal data analysis using general-
ized linear models. Biometrika, 73, 13-22.
Lindley, D. V. and Smith, A.F.M. (1972). Bayes estimates for the linear model.
J. R. Stat. Soc., B34, 1-41.
Littell, R.C., Freund, R.J. and Spector, P.C. (1991). SAS System for Linear
Models. SAS Institute Inc., Cary, NC, USA.
Little, R.J.A. and Rubin, D.B. (1987). Statistical Analysis with Missing Data.
John Wiley &; Sons, New York.
Lucas, H.L. (1962). Unpublished lecture notes on the linear model and its analy-
sis. Univ. North Carolina, Raleigh.
McCullagh, P. and Nelder, J.A. (1991). Generalized Linear Models. Chapman
& Hall, London.
McElroy, F.W. (1967). A necessary and sufficient condition that ordinary least-
squares estimators be best linear unbiased. J. Am. Statist. Assoc., 62, 1302-
1304.
Magnus, J.R. and Neudecker, H. (1988). Matrix Differential Calculus with Ap-
plications in Statistics and Econometrics. John Wiley & Sons, New York.
Mallows, C.L. (1973). Some comments on Cp. Technometrics, 15, 661-675.
Mallows, C.L. (1995). More comments on Cp. Technometrics, 37, 362-372.

References
459
Manly, B.F. (1976). Exponential data transformation. The Statistician, 25,
37-42.
Mardia, K.V. (1970). Measures of multivariate skewness and kurtosis with ap-
plications. Biometrika, 57, 519-530.
Mardia, K.V. (1975).
Assessment of multinormality and the robustness of
Hotelling’s T2 test. Appl. Statist., 24, 163-171.
Mardia, K.V., Kent, J.T. and Bibby, J.M. (1979). Multivariate Analysis. Aca-
demic Press, London.
Mathew, T. and Nordstrom, K. (1997). An inequality for a measure of deviation
in linear models. Am. Statistician, 51, 344-349.
Meinhold, R.J. and Singpurwalla, N. D. (1983). Understanding the Kalman
filter. Am. Statistician, 37, 123-127.
Miller, R.G. (1981). Simultaneous Statistical Inference. Springer-Verlag, New
York.
Montgomery, D.C. (1991). The Design and Analysis of Experiments. John Wi-
ley & Sons, New York.
Mood, A. M. (1950). Introduction to the Theory of Statistics. McGraw-Hill,
New York.
Moser, B.K. and Sawyer, J.K. (1998). Algorithms for sums of squares and co-
variance matrices using Kronecker products. Am. Statistician, 52, 54-57.
Mosteller, F. and Tukey, J.W. (1977). Data analysis and Regression. Addison
Wesley, Reading, MA.
Muirhead, R.J. (1982). Aspects of Multivariate Statistical Theory. John Wiley
& Sons, New York.
Mukhopadhyay, N. (2000). Probability and Statistical Inference. Marcel Dekker,
Inc., New York.
Myers, R.H. (1971).
Response Surface Methodology.
Allyn & Bacon, Inc.,
Boston.
Nadaraya, E.A. (1964). On estimating regression. Theory Probab. Appl., 9,
141-142.

460
References
Newman, D. (1939). The distribution of the range in samples from a normal
population, expressed in terms of an independent estimate of standard devia-
tion. Biometrika, 31, 20-30.
Ogasawara, T. and Takahashi, M. (1951). Independence of quadratic forms in
normal system. J. Sci. Hiroshima University, 15, 1-9.
Ogawa, J. (1950). On the independence of quadratic forms in a noncentral nor-
mal system. Osaka Mathematical Journal, 2, 151-159.
Ogawa, J. (1993). A history of the development of Craig-Sakamoto’s theorem
viewed from Japanese standpoint (in Japanese and English). Proc. Inst. Sta-
tist. Math., 41, 47-59.
Patterson, H.D. and Thompson, R. (1971). Recovery of intra-block information
when block sizes are unequal. Biometrika, 58, 545-554.
Pearson, E.A.S. and Hartley, H.O. (1970). Biometrika Tables for Statisticians,
Vol. 1 and Vol. 2. Cambridge University Press, UK.
Phillips, G.D.A. and Harvey, A.C. (1974). A simple test for serial correlation
in regression analysis. J. Am. Statist. Assoc., 69, 935-939.
Pierce, D.A. and Schafer, D.W. (1986). Residuals in generalized linear models.
J. Am. Statist. Assoc., 81, 977-986.
Powell, J.L. (1984). Least absolute deviations estimation for the censored re-
gression model. J. Econ., 25, 303-325.
Pregibon, D. (1981). Logistic regression diagnostics. Ann. Statist., 9, 705-724.
Pringle, R.M. and Rayner, A.A. (1971). Generalized Inverse Matrices with Ap-
plications to Statistics. Griffin, London.
Rao, C.R. (1952). Some theorems on minimum variance estimation. Sankhya,
12, 27-42.
Rao, C.R. (1971a). Estimation of variance and covariance components-MINQUE
theory. J. Mult. Anal.,1, 257-275.
Rao, C.R. (1971b). Minimum variance quadratic unbiased estimation of vari-
ance components. J. Mult. Anal., 1, 445-456.
Rao, C.R. (1973a). Linear Statistical Inference and its Applications. 2nd edi-
tion. John Wiley & Sons, New York.

References
461
Rao, C.R. (1973b). Representations of best linear unbiased estimators in the
Gauss-Markoff model with a singular dispersion matrix. J. Mult. Anal., 3, 276-
292.
Rao, C.R. (1988). Methodology based on the Li-norm in statistical inference.
Sankhya, A50, 289-313.
Rao, C.R. and Mitra, S.K. (1971). Generalized Inverse of Matrices and its Ap-
plications. John Wiley & Sons, New York.
Rao, C.R. and Toutenberg, H. (1995). Linear Models: Least Squares and Alter-
natives. Springer-Verlag, New York.
Rao, C.R. and Zhao, L.C. (1993). Asymptotic normality of LAD estimator in
censored regression models. Mathematical Methods of Statistics, 2, 228-239.
Rauch, H.E., Tung, F. and Streibel, C.T. (1965). Maximum likelihood estima-
tion of linear dynamic systems. J. AIAA, 3, 1445-1450.
Reid, J.G. and Driscoll, M.F. (1988). An accessible proof of Craig’s theorem in
the noncentral case. Am. Statistician, 42, 139-142.
Rodgers, J.L. and Nicewander, W.A. (1988). Thirteen ways to look at the cor-
relation coefficient. Am. Statistician, 42, 59-66.
Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986). Learning internal
representation by back-propagation errors. Nature 323, 533-536.
Ruppert, D. and Carroll, R.J. (1980). Trimmed least squares estimation in the
linear model. J. Am. Statist. Assoc., 75, 828-838.
Samorodnitsky, G. and Taqqu, M.S. (1994).
Stable Non-Gaussian Random
Processes: Stochastic Models with Infinite Variance. Chapman & Hall, New
York.
Saxena, K.M. and Alam, K. (1982). Estimation of the noncentrality parameter
of a chi-squared distribution. Ann. Stat., 10, 1012-1016.
Scheffe, H. (1953). A method for judging all contrasts in the Analysis of Vari-
ance. Biometrika, 40, 87-104.
Scheffe, H. (1959). The Analysis of Variance. John Wiley & Sons, New York.
Schwarz, G. (1978). Estimating the dimension of a model. Ann. Statist., 6,
461-464.

462
References
Searle, S. R. (1971). Linear Models. John Wiley & Sons, New York.
Searle, S.R. (1982). Matrix Algebra Useful for Statistics. John Wiley & Sons,
New York.
Searle, S.R. (1987). Linear Models for Unbalanced Data. John Wiley & Sons,
New York.
Searle, S.R., Casella, G. and McCulloch, C.E. (1992). Variance Components.
John Wiley & Sons, New York.
Seber, G.A.F. (1977). Linear Regression Analysis. John Wiley & Sons, New
York.
Seber, G.A.F. (1984).
Multivariate Observations. John Wiley & Sons, New
York.
Seber, G. A. F. and Wild, C.J. (1989). Nonlinear Regression. John Wiley &
Sons, New York.
Seely, J.F., Birkes, D., and Lee, Y. (1997). Characterizing sums of squares by
their distributions. Am. Statistician, 51, 55-58.
Shanbhag, D. N. (1966). On the independence of quadratic forms. J. R. Stat.
Soc., B28, 582-583.
Shapiro, S.S. and Wilk, M.B. (1965). An analysis of variance test for normality
(complete samples). Biometrika,, 52, 591-611.
Shumway, R.H. and Stoffer, D.S. (2000). Time Series Analysis and its Applica-
tions. Springer Verlag, New York.
Solomon, RJ. (1985). Transformations for components of variance and covari-
ance. Biometrika, 72, 233-239.
Sprent, P. (1961). Some hypotheses concerning two phase regression lines. Bio-
metrics, 17, 634-645.
Stukel, T. (1988). Generalized logistic models.
J. Am. Statist. Assoc., 83,
426-431.
Stewart, G.W. (1973). Introduction to Matrix Computations. Academic Press,
New York.
Tang, P.C. (1938). Power function of the analysis of variance tests with tables
and illustrations of their use. Statistical Research Memoirs, 2, p.126.

References
463
Theil, H. (1965). The analysis of disturbances in regression analysis. J. Am.
Statist. Assoc., 60, 1067-1079.
Thisted, R.A. (1988). Elements of Statistical Computing: Numerical Computa-
tion. Chapman & Hall, New York.
Thompson, G.L. (1991). A unified approach to rank tests for multivariate and
repeated measures designs. J. Am. Statist. Assoc., 86, 410-419.
Tukey, J.W. (1957). The comparative anatomy of transformations. Ann. Math.
Stat., 28, 602-632.
Velleman, P.F. and Welsch, R.E. (1981). Efficient computing of regression di-
agnostics. Am. Statistician, 35, 234-242.
Verbyla, A.P. and Cullis, B.R. (1990). Modeling in repeated measures experi-
ments. Appl. Statist., 39, 341-356.
Vidakovic, B. (1999). Statistical Modeling by Wavelets. John Wiley & Sons,
New York.
Wang, Y. (1995). Jump and sharp cusp detection by wavelets. Biometrika, 82,
385-397.
Warner, B. and Misra, M. (1996). Understanding neural networks as statistical
tools. Am. Statistician, 50, 284-293.
Watson, G.S. (1964). Smooth regression analysis. Sankhya, A26, 359-372.
Wedderburn, R.W.M. (1974).
Quasi-likelihood functions, generalized linear
models, and the Gauss-Newton method. Biometrika, 61, 439-447.
Weeks, D.L. and Williams, D.R. (1964). A note on the determination of con-
nectedness in an N-way cross classification. Technometrics, 6, 319-324.
West, M. and Harrison, J. (1997). Bayesian Forecasting and Dynamic Models.
Springer-Verlag, New York.
Westfall, P.H. (1987). A comparison of variance components estimates for ar-
bitrary underlying distributions. J. Am. Statist. Assoc., 82, 866-874.
White, H. (1980). A heteroscedasticity-consistent covariance matrix estimator
with a direct test for heteroscedasticity. Econometrica, 48, 817-838.

464
References
Williams, E.J. (1959). Regression Analysis. John Wiley & Sons, New York.
Wood, F.S. (1973). The use of individual effects and residuals in fitting equa-
tions to data. Technometrics 15, 677-695.
Woods, H., Steinour, H.H., and Starke, H.R. (1932). Effect of composition of
Portland cement on heat evolved during hardening. Industrial and Engineering
Chemistry, 24, 1207-1214.
Wu, L.S.-Y., Hosking, J.R.M. and Ravishanker, N. (1993). Reallocation outliers
in time series. Appl. Statist
42, 301-313.
Zyskind, G. and Martin, F. B. (1969). On best linear estimation and a general
Gauss-Markov theorem in linear models with arbitrary nonnegative covariance
structure. SIAM J. Appl Math., 17, 1190-1202.

Author Index
Chmielewski, M.A., 185
Cochran, W.G., 172, 180
Conover, W.J., 378, 381
Cook, R.D., 283, 285, 329, 330, 332
Cooper, W.W., 340
Cox, D.R., 210, 211, 283
Craig, A.T., 172, 178, 180
Cramer, H., 153
Cullis, B.R., 419
Aitken, M., 142, 146
Akaike, H., 271
Akritas, M.G., 378, 381
Alam, K., 232
Anderson, T.W., 185, 190, 197
Andrews, D.F., 206, 211, 331, 437
Ansley, C.F., 217
Antoniadis,A., 350
Arnold, S.F., 381
Atiqullah, M., 121
Atkinson, A.C., 330, 331
Azzalini, A., 190
Dalla Valle, A., 190
Daniel, C., 326
Davis, R.A., 293
Dempster, A.R, 275
Devlin, S.J., 185
Dey, D.K., 190, 423
Diaconis, R, 347
Diggle, RJ., 416, 430
Dixon, L.C.W., 69
Dodge, Y., 340, 341
Doksum, K.A., 210
Donoho, D.L., 351
Draper, N.R., 210, 265, 281, 301, 319,
Bartlett, M.S., 288, 358
Bassett, G., 342
Basu, D., 198
Beckman, R.J., 323
Belsley, D.A., 307, 308, 319, 333, 335
Berger, J.O., 340
Berger, R.L., 141, 166, 196, 296, 407
Bibby, J.M., 197
Bickel, RJ., 210
Birkes, D., 172, 340
Box, G.E.R, 210, 211, 283, 407
Branco, M., 190
Breusch, T.S., 290
Brockwell, RJ., 293
Brown, R.J., 324
332
Driscoll, M.F., 172, 175, 179, 194
Duncan, D.B., 241
Dunnett, C.W., 244
Durbin, J., 292, 324
Dyer, D.D., 289
Carroll, R.J., 211, 288, 342
Casella, G., 141, 166, 196, 296, 391,
Ellenberg, J.H., 323
Englefield, N.J., 77, 89
Evans, J., 324
407
Charnes, A., 340
Chatterjee, S., 107, 108, 126, 294, 319,
323, 329
Chen, M.-H., 423
Cheng, B., 348
Fang, K.T., 184, 185, 190
Ferguson, R.O., 340
Fisher, R.A., 180, 202
465

466
Author Index
Freund, R.J., 229, 312, 377
Friedman, J.H., 345, 347
Keating, J.P., 289
Kelker, D., 185, 189
Kendall, M.G., 164, 202, 231
Kennard, R.W., 309
Kent, J.T., 197
Ketternring, J.R., 185
Keuls, M., 243
Khuri, A., 172
Khuri, A.I., 175
Koenker, R., 342
Kotz, S., 181, 184, 185
Krasnicka, B, 179
Krasnicka, B., 194
Krogh, A., 348
Kruskal, W., 284
Kruskal, W.H., 379
Kuh, E., 319
Ghosh, M., 197
Gnanadesikan, R., 185, 205, 206
Goldfeld, S.M., 289
Golub, G.H., 40, 223
Golueke, C.G., 335
Graybill, F.A., 1, 172, 190, 400
Gregoire, G., 350
Guenther, W.C., 164
Gupta, A.K., 190
Gupta, S.S., 150
Hadi, A.S., 108, 319, 323, 329
Halmos, P.R., 198
Hampel, F.R., 326
Harrison, J., 416
Hartley, H.O., 169, 232, 358
Harvey, A.C., 325
Harville, D.A., 1, 45, 47, 87, 272, 397,
Laha, R.G., 179, 181
Laird, N.M., 275, 420
Larsen, W.A., 284
Lee, Y., 172
Levene, H., 359
Liang, K.-Y., 274, 416, 430
Lindley, D.V., 409
Littell, R.C., 229, 312, 377
Little, R.J.A., 274
420
Haslett, J., 172
Hastie, T.J., 344, 346, 430
Hayes, J.G., 305
Hayes, K., 172
Hertz, J., 348
Heyadat, A., 324, 325
Hinkley, D.V., 211
Hoaglin, D.C., 325
Hochberg, Y., 239
Hoerl, A.E., 309
Hollander, M., 381
Hosking, J.R.M., 278
Huber, P.J., 325, 327, 340, 343
Hultquist, R.A., 400
Magnus, J.R., 63, 174
Mallows, C.L., 296, 437
Manly, B.F., 210
Mardia, K.V., 197, 206, 207
Marquardt, D.W., 308
Martin, F.B., 129, 398
Mathew, T., 132
McCleary, S.J., 284
McCullagh, P., 422, 428
McCulloch, C.E., 391
McElroy, F.W., 125
McGauhey, P.H., 335
McKeague, I.W., 350
Meinhold, R. J., 412
Miller, R.G., 239
Misra, M., 350
Mitra, S.K., 73, 82, 397
Montgomery, D.C., 231, 240, 367, 391
Iman, R.L., 378
James, G.S., 180
John, J.A., 210, 332
Johnson, N.L., 181
Johnson, R.A., 149, 313
Johnstone, I.M., 351
Judge, G.G., 288, 296
Jureckova, J., 341

Author Index
467
Schafer, D.W., 429
Scheffe, H., 170, 237
Schwarz, G., 271
Searle, S.R., 1, 359, 361, 391, 402
Seber, G.A.F., 121, 199, 305
Seely, J.F., 172
Shahshahani, M., 347
Shanbhag, D.N., 172, 180
Shao, Q.-M., 423
Shapiro, S.S., 205
Shumway, R.H., 414, 415
Singpurwalla, N. D., 412
Smith, A.F.M., 409
Smith, H., 265, 281, 301, 319
Solomon, P.J., 211
Spector, P.C., 229, 377
Sprent, P., 261
Starke, H.R., 302
Steinour, H.H., 302
Stewart, G.W., 40
Stoffer, D.S., 414, 415
Streibel, C.T., 415
Stuart, A., 164, 202, 231
Stuetzle, W., 347
Stukel, T., 423
Styan, G.P., 223
Mood, A.M., 380
Moser, B.K., 68, 396
Muirhead, R.J., 184
Mukhopadhyay, N., 141, 196
Myers, R.H., 307
Nadaraya, E.A., 353
Nelder, J.A., 422, 428
Neudecker, H., 63, 174
Newman, D., 243
Ng, K., 184, 185
Nicewander, W.A., 158
Nordstrom, K., 132
Ogasawara, T., 180
Ogawa, J-, 179
Pagan, A.R., 290
Palmer, R.G., 348
Patterson, H.D., 272
Pearson, E.A.S., 150, 169, 232, 358
Phillips, G.D.A., 325
Pierce, D.A., 429
Powell, J.L., 342
Pregibon, D., 331, 430
Price, B., 107, 127, 294
Pringle, R.M., 82
Takahashi, M., 180
Tamhane, A.C., 239
Rao, C.R., 1, 70, 73, 82, 121, 129, 170,
Tang, P.C., 169, 231
172, 180, 201, 268, 342, 397,
Taqqu, M.S., 183, 438
Theil, H., 324
Thisted, R.A., 344, 349
Thompson, G.L., 378
Thompson, R., 272
Tiao, G.C„407
Tibsirani, R.J., 344, 346, 430
Titterington, D.M., 348
Toutenberg, H., 70
Trussel, H.J., 323
Tukey, J.W., 209, 242, 345
Tung, F., 415
Quandt, R.E., 289
403
Rauch, H.E., 415
Ravishanker, N., 278
Rayner, A.A., 82
Reid, J.G., 179
Robson, D.S., 325
Rodgers, J.L., 158
Rubin, D.B., 274, 275
Rumelhart, D.E., 349
Ruppert, D., 211, 288, 342
Samorodnitsky, G., 183, 438
Savage, L.J., 198
Sawyer, J.K., 68, 396
Saxena, K.M., 232
Van Loan, C.F., 40
Varga, T., 190
Velleman, P.F., 333

Author Index
468
Verbyla, A.P., 419
Vidakovic, B., 351
Wallis, W.A., 379
Wang, Y., 353
Ware, J.H., 420
Warner, B., 350
Watson, G.S., 292, 353
Wedderburn, R.W.M., 273
Weeks, D.L., 360
Weisberg, S., 283, 285, 329, 330, 332
Welsch, R.E., 319, 325, 333
West, M., 416
Westfall, P.H., 395
White, H., 290
Wichern, D.W., 149, 313
Wilk, M.B., 205
Williams, D.R., 360
Williams, E.J., 318
Wold, H.O., 153
Wolfe, D.A., 381
Wood, F.S., 284, 326
Woods, H., 302
Wu, L.S-Y., 278
Zeger, S.L., 274, 416, 430
Zhao, L.C., 342
Zyskind, G., 129, 398

Subject Index
Connectedness, 360
Contrast(s), 117
orthogonal, 117, 232
Correlation
z-transformation of, 160
matrix, 160
MLE of multiple, 204
MLE of partial, 203
MLE of simple, 200
multiple, 161
partial, 160
simple, 157
Covratio, 335
QR decomposition, 42
i?2, see . Coefficient of determination,
104
^-connectedness, 360
Absorption, 364
Adjusted sum of squares, 366, 376
AIC, 271
All-cells-filled data, 360
Analysis of covariance, 371
ANOVA decomposition, 101
Assess normality
bivariate, 205
multivariate, 205
univariate, 204, 205
Autoregression, 291
Deviance function, 429
scaled, 429
Diagonability, 43
of p.d. matrices, 55
of symmetric matrices, 47
orthogonal, 43, 47
Discrete wavelet transform, 350
Distance
B.l.u.e., 118
projection approach for, 119
Bayes’ theorem, 407
BIC, 271
Bilinear form, 51
Cook’s, 330
Mahalanobis, 207, 326
modified Cook’s, 331
Welsch’s, 333
Welsch-Kuh, 332
Distribution(s)
bivariate normal, 148
chi-square, 165
conditional, 156, 409
doubly noncentral F, 170
marginal, 154, 409
multivariate
e-contaminated normal, 182
t, 183
Cauchy sequence, 351
Cell means model, 361
Coefficient of determination, 104
Coincidence of regression lines, 338
Complete data analysis, 274
Concentration ellipse, 164
Concurrence of regression lines, 337
Confidence intervals
simultaneous, 236
Bonferroni t, 239
marginal, 233
Scheffe’s F, 237
Confidence regions, 233
Conjugate prior, 431
469

470
Subject Index
symmetric stable, 183
Cauchy, 187
elliptical, 185
Laplace, 184
logistic, 183
mixture of normals, 181
normal, 145, 147
scale mixture of normals, 182
singular normal, 150
spherical, 184
standard normal, 145
noncentral F, 168, 231
noncentral t, 170
noncentral Beta, 169, 204
noncentral chi-square, 177
Wishart, 196
Durbin-Watson test, 292
Dynamic linear model
observation equation, 412
state equation, 412
generalized, 127
General linear model
see linear model, 91
Generalized eigenvalue problem, 58
Generalized estimating equations, 274
Generalized inverse, 18, 73, 84, 98
commuting, 77
of a diagonal matrix, 77
of a partitioned matrix, 80
of a symmetric matrix, 76, 77
Generalized least squares, 123
Geometry of
generalized least squares, 123
goodness of fit, 104
influence diagnostics, 331
least squares, 96
leverage, 325
simple regression, 102
GLIM, 422
link function, 422
random component of, 422
systematic component of, 422
Gram-Schmidt orthogonalization, 9
EDA, 93
Eigenspace, 25
Eigenvalues, 24, 45
Eigenvectors, 24, 45
Elasticity, 94
EM algorithm, 275
Empirical Bayes estimator, 421
Essentially unique b.l.u.e., 128
Estimability, 114, 268
Estimable functions, 114
Estimating equations, 419
Hat matrix, see projection matrix, 18
Heteroscedasticity, 287
Hierarchical model, 410
Hilbert space, 351
Homogeneity of variances
Bartlett’s test, 358
Hartley’s test, 358
Levene’s test, 359
Hyperparameter, 409
Fitted values
vector of, 99
Fixed-effects
m-stage nested model, 256
one-factor model, 94, 227, 249
two-factor additive model, 110, 121
229, 250
two-factor model, 111, 230, 253
two-factor nested model, 112, 255
Full-rank factorization, 40
Imputation, 274
Index plot, 331
Inequalities
Cauchy-Schwarz, 6
extended Cauchy-Schwarz, 56
triangle, 6
Influence function, 326
empirical, 329
sample, 329
Inner-product space, 351
Integral
Gauss-Markov theorem, 118
extended, 397, 420
Aitken’s, 142

Subject Index
471
general evaluation theorem, 142,
subspace, 7
subspace of, 21
165
Link
Interaction, 368
Intra-class correlation, 15, 18, 96, 417
IRLS estimate, 424
canonical, 423
complementary log-log, 423
log, 424
log-log, 423
logit, 423
Longitudinal model, 416
exponential correlation model, 417
multivariate model, 417
uniform correlation model, 417
Kalman filter, 412
Kalman smoother, 415
Kruskal-Wallis test, 379
Kurtosis, 206
Lagrangian multipliers, 129, 310, 311
Least squares, 96, 349
estimate of /?, 98
estimate of a2, 98
solution, 97, 98
weighted, 126, 288, 418
Leverage, 325
Likelihood, 198, 267, 407
complete data, 275
ratio test, 269, 270
Linear equations, 82
compatible, 83
consistent, 83
homogeneous, 21
nonhomogeneous, 21, 84
solution set of, 84
Linear form, 51
Linear hypothesis, 219
non-testable, 219, 266
testable, 219
Linear model, 97
definition of, 91
reduced, 246
restricted reduced, 246
Linear restrictions, 129
Linear space, 4
basis of, 8, 21
dimension of, 8
direct sum of, 8
geometry of, 58
of matrices, 20
orthogonal basis of, 9
orthogonal complement of, 59
orthogonal subspace of, 21
orthonormal basis of, 9
Mallow’s Cp, 297
Matrix (or matrices), 1-31
augmented, 82
basis, 8
column space of, 22
column-equilibrated, 94, 307
design, 95
determinant of, 13
direct product of, see Kronecker
product of, 66
direct sum of, 68
idempotent, 50, 99, 172
incidence, 95
inverse of, 16, 18
Kronecker product of, 66, 95
Moore-Penrose inverse of, 81
nonsingular, 16, 23
norm of, 28
null space of, 21
orthogonal, 20
p.d., 53
p.s.d., 53
partitioned, 33, 35, 37
projection, 39, 60, 99, 320
rank of, 23
singular, 16
submatrix, 2
symmetric, 12, 45
trace of, 12
Maximum likelihood estimate (MLE),
266, 267
Maximum likelihood estimation, 199
Mean-shift outlier model, 322

Subject Index
472
Principal components regression, 313
Prior density, 407
Prior probability, 407
Projection matrix, 18
Projection pursuit, 345
Missing at random, 274
Missing data analysis, 274
Mixed-effects
two-factor model, 401
two-factor nested, 403
Mixed-effects model, 396
Model selection, 296
Multicollinearity, 307
condition index, 308
condition number, 309
variance inflation factors, 308
Multiple comparisons, 239
Duncan’s multiple range procedure,
Quadratic form(s), 52
distributions of, 172
independence of, 178
moments of, 174
nonnegative definite, 53
positive definite, 53
positive semidefinite, 53
Qualitative predictors, 336
241
Dunnett’s procedure, 244
LSD method, 240
Newman-Keuls procedure, 243
Tukey’s procedure, 242
Random-effects
ANOVA estimates, 388
MLE’s, 393
one-factor model, 95, 386
REML estimates, 395
two-factor additive model, 405
two-factor model, 399
two-factor nested model, 399
Rank transform, 378
Regression
M, 343
backward elimination, 301
censored, 342
diagnostics, 319
forward selection, 301
goodness of fit, 105
MAXR procedure, 301
multiple, 93, 281
centered and scaled form of, 93
nonparametric, 350
quantile, 342
ridge, 309
sets of, 223, 337
simple, 92
stepwise, 299
variable selection, 299
REML, 419
Residual analysis, 281
Residuals
Nested hypotheses, 246, 250, 253, 280
Nested model, 370
Neural networks, 348
feed-forward, 348
perceptron, 348
Non-orthogonality, 366
Noninformative prior, 431
Normal equations, 98
Orthogonal contrasts, 363
Orthogonal predictors, 304
Orthogonal projection, 61, 99, 131
Orthonormal basis, 352
Overparametrized models, 363
Partial F-test, 287
Partial regression coefficients, 94
Plots
L- R, 326
added-variable , 283
chi-square, 206
partial residual, 284
residuals, 282
Posterior density, 407
Posterior probability, 407
Predictive density, 410
PRESS statistic, 298
BLUS, 324
externally Studentized, 321

Subject Index
473
internally Studentized, 321
normalized, 321
ordinary, 99
partial, 323
predicted, 323
recursive, 324
standardized, 321
Response surfaces, 307
Restricted least squares, 311
Restriction
ellipsoidal, 312
spherical, 311
Ridge trace, 310
Variance components, 386, 402
negative estimate, 391
Vector(s), 1-31
Euclidean norm, 4
inner product of, 4
linear independence of, 7
norm of, 27
normal, 9
orthogonal, 8
projection of, 58
space, see linear space, 6
span of, 6
Wavelet, 350
Wavelet domain, 351
Sample mean, 195
SBC, 271
Sensitivity curve, 329
Sequential F-test, 286
Serial correlation, 291
Singular value decomposition, 44, 308
Skewness, 206
Some-cells-empty data, 368
Spectral decomposition, 47, 57
Standardized regression coefficients, 94
Stochastic regressor, 295
Studentized range, 242
Sufficient statistic(s), 197, 198
Sums of squares
model, 101
residual, 101
total, 101
Tobit model, 342
Transformation(s), 140
Box-Cox, 210
Box-Cox shifted power, 210
linear, 140
polar coordinate, 191
power, 209
to joint normality, 211
to marginal normality, 211
Triangular decomposition, 41
Two-factor additive model, 363
Two-phase regression, 261
Unadjusted sum of squares, 366


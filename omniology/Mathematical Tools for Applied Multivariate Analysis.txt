Preface 
The basic objectives that motivated the first edition of the book serve as the primary 
motivations for the second edition as well. While the original content of this book has been 
left essentially unchanged, this revision has been aimed at making the terminology used in 
the description of some basic mathematical concepts and operations more "up-to-date." 
This also makes this technical terminology more consistent with the usage now current in 
other fields and disciplines of study (e.g., mathematics, computer science, statistics, social 
and behavioral sciences, as well as marketing and business related sciences). A large 
number of these revisions occur in Chapter 5, "Decompositions of Matrix Transformations: 
Eigenstructures and Quadratic Forms," but can be found throughout the book. 
The student willing to learn something about multivariate analysis will find no dearth of 
textbooks and monographs on the subject. From introductory to advanced, theoretical to 
applied, general to specific, the field has been well covered. 
However, most of these books assume certain mathematical prerequisites—typically 
matrix algebra and introductory calculus. Single-chapter reviews of the topics are usually 
provided but, in turn, presuppose a fair amount of advance preparation. What appears to be 
needed for the student who has received less exposure is a somewhat more elementary and 
leisurely approach to developing the necessary mathematical foundations of applied 
multivariate analysis. 
The present book has been prepared to help students with those aspects of 
transformational geometry, matrix algebra, and the calculus that seem most relevant for 
the study of multivariate analysis. Since the author's interest is in applications, both the 
material selected for inclusion and the point of view from which it is presented reflect 
that orientation. 
The book has been prepared for students who have either taken no matrix algebra at 
all or, if they have, need a refresher program that is between a full-fledged matrix 
algebra course and the highly condensed review chapter that is often found in 
multivariate textbooks. The book can serve as a textbook for courses long enough to 
permit coverage of precursory mathematical material or as a supplement to general 
textbooks on multivariate analysis. 
xi 

Xll 
PREFACE 
The title was chosen rather carefully and helps demarcate what the book is not as much 
as what it is. First, those aspects of linear algebra, geometry, and the calculus that are 
covered here are treated from a pragmatic viewpoint-as tools for helping the applications 
researcher in the behavioral and business disciplines. In particular, there are virtually no 
formal proofs. In some cases outlines of proofs have been sketched, but usually small 
numerical examples of the various concepts are presented. This decision has been 
deliberate and it is the author's hope that the instructor will complement the material with 
more formal presentations that reflect his interests and perceptions of the technical 
backgrounds of his students. 
The book consists of six chapters and two appendices. Chapter 1 introduces the topic of 
multivariate analysis and presents three small problems in multiple regression, principal 
components analysis, and multiple discriminant analysis to motivate the mathematics that 
subsequent chapters are designed to supply. Chapter 2 presents a fairly standard treatment of 
the mechanics of matrix algebra including definitions and operations on vectors, matrices, 
and determinants. Chapter 3 goes through much of this same material but from a 
geometrically oriented viewpoint. Each of the main ideas in matrix algebra is illustrated 
geometrically and numerically (as well as algebraically). 
Chapter 4 and 5 deal with the central topics of linear transformations and eigenstructures 
that are essential to the understanding of multivariate techniques. In Chapter 4, the theme of 
Chapter 3 receives additional attention as various matrix transformations are illustrated 
geometrically. This same (geometric) orientation is continued in Chapter 5 as eigenstructures 
and quadratic forms are described conceptually and illustrated numerically. A large number 
of terminological changes made in this edition of the book occur in Chapter 5. 
Chapter 6 completes the cycle by returning to the three applied problems presented in 
Chapter 1. These problems are solved by means of the techniques developed in Chapters 
2-5, and the book concludes with a further discussion of the geometric aspects of linear 
transformations. 
Appendix A presents supporting material from the calculus for deriving various matrix 
equations used in the book. Appendix B provides a basic discussion on solving sets of linear 
equations and includes an introduction to generalized inverses. Numerical exercises appear 
at the end of each chapter and represent an integral part of the text. With the student's 
interest in mind, solutions to all numerical problems are provided. (After all, it was those 
even-numhcTQd exercises that used to give us all the trouble!) The student is urged to work 
through these exercises for purposes of conceptual as well as numerical reinforcement. 
Completion of the book should provide both a technical base for tackling most 
applications-oriented multivariate texts and, more importantly, a geometric perspective for 
aiding one's intuitive grasp of multivariate methods. In short, this book has been written 
for the student in the behavioral and administrative sciences—not the statistician or 
mathematician. If it can help illuminate some of the material in current multivariate 
textbooks that are designed for this type of reader, the author's objective will have been 
well satisfied. 

Acknowledgments 
Many people helped bring this book to fruition. Literally dozens of masters and 
doctoral students provided critical reviews of one or more chapters from the most 
relevant perspective of all—their's. Those deserving special thanks are Ishmael Akaah, 
Alain Blanch rude, Frank Deleo, J. A. English, Pascal Lang, and Gunter Seidel. Professor 
David K. Hildebrand, University of Pennsylvania, prepared a thorough and useful critique 
of the full manuscript. Helpful comments were also received from Professor Joel Huber, 
Purdue University. 
Production of the book was aided immeasurably by the competent and cheerful help 
of Mrs. Joan Leary, who not only typed drafts and redrafts of a difficult manuscript, but 
managed to do the extensive art work as well. The editorial staff of Academic Press also 
deserves thanks for their production efforts and general cooperative spirit. 
J. D. C. 
P. E. G. 
A. D. C. 
xui 

References 
Aaker, D. A. (ed.). Multivariate Analysis in Marketing: Theory & Application. Belmont, California: 
Wadsworth, 1971. 
Afifi, A. A., and Azen, S. P., Statistical Analysis: A Computer-Oriented 
Approach. New York: 
Academic Press, 1972. 
Albert, A., Regression and the Moore-Penrose Pseudoinverse. New York: Academic Press, 1972. 
Anderson, T. W., Introduction to Multivariate Statistical Analysis. New York: Wiley, 1958. 
Bartlett, M. S., "Multivariate Analysis." J. Roy. Stat. Soc. Ser. B 9(1947), 176-197. 
Beveridge, G. S., and Schechter, R. S., Optimization: 
Theory and Practice. New York: McGraw-Hill, 
1970. 
Bickley, W. G., and Thompson, R. S. H. G., Matrices: Their Meaning and Manipulation. London: 
English University Press, 1964. 
Bock, R. D., Multivariate Statistical Methods in Behavioral Research. New York: McGraw-Hill, 1975. 
Bock, R. D., and Haggard, E. A., **The Use of Multivariate Analysis of Variance in Behavioral 
Research," in D. K. Whitla (ed.). Handbook of Measurement and Assessment in Behavioral 
Sciences. Reading, Massachusetts: Addison-Wesley, 1968, 100-142. 
Bolch, B. W., and Huang, C. J., Multivariate Statistical Methods for Business and Economics. 
Englewood Cliffs, New Jersey: Prentice-Hall, 1974. 
Carroll, J. D., "Categorical Conjoint Measurement," in P. E. Green and Y. Wind. Multiattribute 
Decisions in Marketing: A Measurement Approach. Hinsdale, Illinois: Dryden Press, 1973. 
Carroll, J. D., "Algorithms for Rotation and Interpretation of Dimensions and for Configuration 
Matching." Paper presented at the Bell-Penn Multidimensional Scaling Workshop, June 1972. 
Carroll, J. D., "A Generalization of Canonical Correlation to Three or More Sets of Variables." Proc. 
76th Annu. Conven. Amer. Psycholog Assoc. (1968), 227-228. 
Carroll, J. D., "Notes on Factor Analysis." Unpublished paper. Bell Laboratories, Murray Hill, New 
Jersey, 1965. 
Cattell, R. B., "Multivariate Behavioral Research and the Integrative Challenge." Multivariate 
Behavioral Res. 1(1966), 4-23. 
Cliff, N., "Orthogonal Rotation to Congruence." Psychometrika 31(1966), 33-42. 
Comrey, A. L., A First Course in Factor Analysis. New York: Academic Press, 1973. 
Cooley, W. W., and Lohnes, P. R., Multivariate Data Analysis. New York: Wiley, 1971. 
Corbato, F. J., "On the Coding of Jacobi*s Method for Computing the Eigenvalues and Eigenvectors of 
Real Symmetric Matrices." /. Assoc. Comput. Mach. 10(1963), 123-125. 
Coxeter, H. S. U.., Introduction to Geometry. New York: Wiley, 1961. 
Daniel, C, and Wood, F. S., with the assistance of Gorman, J. W., Fitting Equations to Data. New 
York: Wiley, 1971. 
Darlington, R. B., "Multiple Regression in Psychological Research and Practice." Psycholog. Bull. 
69(1968), 161-182. 
Dempster, A. P., Elements of Continuous Multivariate Analysis. Reading, Massachusetts: Addison-
Wesley, 1969. 
364 

REFERENCES 
365 
Dickman, K., and Kaiser, H. F., "Program for Inverting a Gramian Matrix." Ed, Psyeholog. Meas. 
21(1961), 
lll'-ni. 
Draper, N., and Smith, }\., Applied Regression Analysis. New York: Wiley, 1966. 
Dunn, O. J., and Clark, V. A., Applied Statistics: Analysis of Variance and Regression. New York: 
Wiley, 1974. 
Dwyer, P. S., Linear Computations. New York: Wiley, 1951. 
Dwyer, P. S., and MacPhail, M. S., "SymboUc Matrix Derivatives." Ann. Math. Statist. 19(1948), 
517-534. 
Eckart, C, and Young, G., '*The Approximation of One Matrix by Another of Lower Rank." 
Psychometrika 1(1936), 211-218. 
Edwards, A. L., Experimental Designs in Psychological Research, 3rd ed. New York: Holt, 1968. 
Eisenbeis, R. A., and Avery, R. B., Discriminant Analysis and Classification Procedures. Lexington, 
Massachusetts: Heath, 1972. 
Faddeeva, V. N., Computational Methods of Linear Algebra. New York: Dover, 1959. 
Finn, J. D.,^ General Model for Multivariate Analysis. New York: Holt, 1974. 
Francis, J. G. F., "The QR Transformation, Parts I and 11." Comput. J. 4(1961), 265-271; (1962), 
332-345. 
Fuller, L. E., Basic Matrix Theory. Englewood Cliffs, New Jersey: Prentice-Hall, 1962. 
Goldberger, A. S., Topics in Regression Analysis. New York: MacMiQan, 1968. 
Good, I. J., "Some Apphcations of the Singular Decomposition of a Matrix." Technometrics 
11(1969), 823-831. 
Graybill, F. A., Introduction to Matrices with Applications in Statistics. Belmont, CaUfornia: 
Wadsworth, 1969. 
Graybill, F. A., An Introduction to Linear Statistical Models, VoLI. New York: McGraw-Hill, 1961. 
Green, B. F., "Best Linear Composites with a Specified Structure." Psychometrika 34(1969), 
301-318. 
Green, P. E., and Tull, D. S., Research for Marketing Decisions, 3rd ed. Englewood Cliffs, New Jersey: 
Prentice-Hall, 1975. 
Green, P. E., and Wind, Y.,Multiattribute Decisions in Marketing: a Measurement Approach. Hinsdale, 
Illinois: Dryden Press, 1973. 
Guertin, W. H., and Bailey, J. P., Jr., Introduction to Modem Factor Analysis. Ann Arbor, Michigan: 
Edwards, 1970. 
liSLdiQy,G.^ Linear Algebra. Reading, Massachusetts: Addison-Wesley, 1961. 
Hancodc, H., Theory of Maxima and Minima. New York: Dover, 1960. 
Harman, H. U., Modem Factor Analysis, 2nd ed. Chicago: University of Chicago Press, 1967. 
Harris, R. J., A Primer of Multivariate Statistics. New York: Academic Press, 1975. 
Haynes, R. D., Komar, C. A., and Byrd, J., Jr., "The Effectiveness of Three Heuristic Rules for Job 
Sequencing in a Single Production Facility." Management Set 19(1973), 575-580. 
Hoerl, A. E., "Fitting Curves to Data," in J. H. Perry (ed.). Chemical Business Handbook. New York: 
McGraw-Hill, 1954, 55-77. 
Hohn, F. E., Elementary Matrix Algebra. New York: MacMillan, 1964. 
Hotelling, H., "Analysis of a Complex of Statistical Variables into Principal Components." /. Ed. 
Psychology 24(1933), 417-441,498-520. 
Horst, P., "An Overview of the Essentials of Multivariate Analysis Methods," (in R. B. Cattell (ed.). 
Handbook of Multivariate Experimental Psychology. Chicago: Rand McNally, 1966,129-152. 
Horst, P., Matrix Algebra for Social Scientists. New York: Holt, 1963. 
Horst, P., "Relations Among m Sets of Measures,"" Psychometrika 26(1961), 129-150. 
Horst, P., "Least Squares Multiple Classification for Unequal Subgroups." /. Clinical Psychology 
12(1956), 309-315. 
Huang, D. S., Regression and Econometric Methods. New York: Wiley, 1970. 
Jeger, M., Transformation Geometry. London: Allen &Unwin, 1966. 
Johnston, J., Econometric Methods, 2nd ed. New York: McGraw-Hill, 1972. 
Kendall, M. G., A Course in Multivariate Analysis. London: Charles Griff en, 1957. 
Kerlinger, F. N., and Pedhazur, E. J., Multiple Regression in Behavioral Research. New York, Holt, 
1973. 

366 
REFERENCES 
Kettenring, J. R., "Canonical Analysis of Several Sets of Variables." Biometrika 58(1972), 433-451. 
Klein, F., Geometry. New York: Dover, 1948. 
Kramer, C. Y., A First Course in Methods of Multivariate Analysis. Blacksburg, Virginia: Clyde 
Kramer, 1972. 
Lang, S.,y4 First Course in Calculus, Vol. I. Reading, Massachusetts: Addison-Wesley, 1964. 
Lawley, D. N., and Maxwell, A. E., Factor Analysis as a Statistical Method. London: Butterworths, 
1963. 
Lingoes, J. C, The Guttman-Lingoes Nonmetric Program Series. Ann Arbor, Michigan: Mathesis Press, 
1973. 
McDonald, R. P., "A Unified Treatment of the Weighting Problem." Psychometrika 
33(1968), 
351-381. 
McNemar, Q., Psychological Statistics, 4th ed. New York: Wiley, 1969. 
Malinvaud, E., Statistical Methods of Econometrics. Chicago: Rand McNally, 1966. 
Manning, H. P., Geometry of Four Dimensions. New York: Dover, 1956. 
Marriott, F. H. C, The Interpretation of Multiple Observations. New York: Academic Press, 1974. 
Mendenhall, W., Introduction to Linear Models and the Design and Analysis of Experiments. Behnont, 
California: Wadsworth, 1968. 
Moore, E. H., "On the Reciprocal of the General Algebraic Matrix." Bull. Amer. Soc. 26(1920), 
394-395. 
Morgan, J., Sirageldin, I., and Baerwaldt, N., Productive Americans. Ann Arbor, Michigan: Survey 
Research Center, 1965. 
Morrison, D. V., Multivariate Statistical Methods, 2nd ed. New York: McGraw-Hill, 1976. 
Mulaik, S. A., The Foundations of Factor Analysis. New York: McGraw-Hill, 1972. 
Murdoch, D. C, Linear Algebra for Undergraduates. New York: Wiley, 1957. 
Neter, J., and Wasserman, W., Applied Linear Statistical Models. Homewood, Illinois: Irwin, 1974. 
Nunnally, J. C, Psychometric Theory. New York: McGraw-Hill, 1967. 
Ortega, J. M., and Kaiser, H. F., '*The LL 
and QR Methods for Symmetric Tridiagonal Matrices." 
Comput. J. 6(1963), 99-101. 
Overall, J. E., and Klett, C. J., Applied Multivariate Analysis. New York: McGraw-Hill, 1972. 
Paige, L. J., Swift, J. D., and Slobko, T. A., Elements of Linear Algebra, 2nd ed. Lexington, 
Massachusetts: Xerox, 1974. 
Pedoe, D. A., Geometric Introduction to Linear Algebra. New York: Wiley, 1963. 
Penrose, R. A., **0n Best Approximate Solutions by Linear Matrix Equations." Proc. Cambridge 
Philos. Soc. 52(1956), 17-19. 
Penrose, R. A., "A Generalized Inverse for Matrices." Proc. Cambridge Philos. Soc. 51(1955), 
406-413. 
Perry, M., and Hamm, B. C, "Canonical Analysis of Relations between Socioeconomic Risk and 
Personal Influence in Purchase Decisions."/. Marketing Res. 6(1969), 351-354. 
Pettofrezzo, A. J., Matrices and Transformations. Englewood Cliffs, New Jersey: Prentice-Hall, 1966. 
Press, S. } . , Applied Multivariate Analysis. New York: Holt, 1972. 
Pringle, R. M., and Rayner, A. A., Generalized Inverse Matrices with Applications to Statistics. New 
York: Hafner, 1971. 
Quandt, R. E., and Baumol, W. J., **The Demand for Abstract Transport Modes: Theory and 
Measurement."/ RegionalSci. 6(1966), 13-26. 
Ralston, A., and Wilf, H. S., Mathematical Methods for Digital Computers. New York: Wiley, 1960 
(Vol. I); 1967 (Vol. II). 
Rao, C. R., Linear Statistical Inference and Its Applications. New York: Wiley, 1965. 
Rao, C. R., "A Note on a Generalized Inverse of a Matrix with Applications to Problems in Statistics." 
/ Roy. Statist. Soc. Ser. B 24(1962), 152-158. 
Rao, C. R., Advanced Statistical Methods in Biometric Research. New York: Wiley, 1952. 
Rao, C. R., and Mitra, S. K., Generalized Inverse of Matrices and Its Applications. New York: Wiley, 
1971. 
Rorer, L. G. et al, "Configural Judgments Revealed". P/-ocee<ii>z^s o/f/ie 75th Annual Convention of 
AMA, (1967), pp. 195-196. 
Rummel, R. i. Applied Factor Analysis. Evanston, Illinois: Northwestern University Press, 1970. 

REFERENCES 
367 
Schatzoff, M., "Exact Distributions of Wilks' Likelihood Ratio Criteria." Biometrika, 
53 (1966), 
347-358. 
Schonemann, P. H.,and Carroll, R. M., "Fitting One Matrix to Another under Choice of a Central 
Dilation and a Rigid Motion:' Psychometrika, 
35 (1970), 245-256. 
Stewart, G. W. Introduction 
to Matrix Computations. New York: Academic Press, 1973. 
Tatsuoka, M. M., Multivariate Analysis. New York: Wiley, 1971. 
Tatsuoka, M. M.,and Tiedeman, D. V., "Statistics as an Aspect of Scientific Method in Research on 
Teaching," in N. L. Gage (ed.), Handbook 
of Research on Teaching. Chicago, Illinois: 
Rand McNally, 1963, 142-170. 
Thurstone, L. L., Multiple Factor Analysis. Chicago, Illinois: Univ. of Chicago Press, 1947. 
Timm, N. H., Multivariate Analysis 
with Applications 
in Education 
and Psychology. 
Behnont, 
California: Wadsworth, 1975. 
Van de Geer, J. P., Introduction 
to Multivariate Analysis for the Social Sciences. San Francisco: 
Freeman, 1971. 
Ward, J. H., Jr., and Jennings, E., Introduction 
to Linear Models. Englewood Cliffs, New Jersey: 
Prentice-Hall, 1973. 
Widder, D. Y., Advanced Calculus. Englewood Cliffs, New Jersey: Prentice-Hall, 1947. 
Wilde, D. J., and Beightler, C. S., Foundations 
of Optimization. 
Englewood Cliffs, New Jersey: 
Prentice-HaU, 1967. 
Wilkinson, J. H., The Algebraic Eigenvalue Problem. London and New York: Oxford Univ. Press 
(Clarendon), 1965. 
Winer, B. J., Statistical Principles in Experimental Design. New York: McGraw-Hill, 1971. 
Wish, M., and Carroll, J. D., "Apphcations of INDSCAL to Studies of Human Perception and 
Judgment," in E. C. Carterette and M. P. Friedman (eds.), Handbook of Perception. New York: 
Academic Press, 1973. 
Wonnacott, R. J., and Wonnacott, T. H., Econometrics. New York: Wiley, 1970. 
Woods, F. S., Higher Geometry. New York: Dover, 1961. 
Yefimov, N. V., Quadratic Forms and Matrices. New York: Academic Press, 1964. 

CHAPTER 1 
The Nature of Multivariate Data Analysis 
1.1 
INTRODUCTION 
Stripped to their mathematical essentials, multivariate methods represent a blending of 
concepts from matrix algebra, geometry, the calculus, and statistics. In function, as well 
as in structure, multivariate techniques form a unified set of procedures that can be 
organized around a relatively few prototypical problems. However, in scope and variety 
of application, multivariate tools span all of the sciences. 
This book is concemed with the mathematical foundations of the subject, particularly 
those aspects of matrix algebra and geometry that can help illuminate the structure of 
multivariate methods. While behavioral and administrative applications are stressed, this 
emphasis reflects the background of the author more than any belief about special 
advantages that might accrue from applications in these particular fields. 
Multivariate techniques are useful for: 
1. discovering regularities in the behavior of two or more variables; 
2. 
testing altemative models of association between two or more variables, including 
the determination of whether and how two or more groups (or other entities) differ in 
their "multivariate profiles." 
The former pursuit can be regarded as exploratory research and the latter as confirmatory 
research. While this view may seem a bit too pat, multivariate analysis is concerned with 
both the discovery and testing of patterns in associative data. 
The principal aim of this chapter is to present motivational material for subsequent 
development of the requisite mathematical tools. We start the chapter off on a somewhat 
philosophical note about the value of multivariate analysis in scientific research generally. 
Some of the major characteristics of multivariate methods are introduced at this point, 
and specific techniques are briefly described in terms of these characteristics. 
Application of multivariate techniques is by no means confined to a single discipline. 
In order to show the diversity of fields in which the methods have been appHed, a number 
of examples drawn from the behavioral and administrative sciences are briefly described. 
Comments are also made on the trends that are taking place in multivariate analysis itself 
and the implications of these developments for future application of the methodology. 
We next tum to a description of three small, interrelated problems that call for 
multivariate analysis. Each problem is described in terms of a common, miniature data 
1 

2 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
bank with integer-valued numbers. As simple as the problems are, it turns out that 
developing the apparatus necessary to solve them covers most of the mathematical 
concepts in multivariate analysis that constitute the rest of the book. 
1.2 
MULTIVARIATE METHODS IN RESEARCH 
It is difficult to imagine any type of scientific inquiry that does not involve the 
recording of observations on one or more types of objects. The objects may be things, 
people, natural or man-made events. The selected objects-white rats, model airplanes, 
biopsy slides, x-ray pictures, patterns of response to complex stimulus situations, ability 
tests, brand selection behavior, corporate financial activities—vary with the investigator's 
discipline. The process by which he codifies the observations does not. 
Whatever their nature, the objects themselves are never measured in total. Rather, 
what is recorded are observations dealing with characteristics of the objects, such as 
weight, wind velocity, cell diameter, location of a shadow on the lung, speed or latency of 
response, number of correctly answered questions, specific brand chosen, previous year's 
sales, and so on. It is often the case that two or more characteristics (e.g., weight, length, 
and heartbeat) will be measured at the same time on each object being studied. 
Furthermore, it would not be unusual to find that the measured characteristics were 
associated in some way; that is, values taken on by one variable are frequently related to 
values taken on by another variable. 
As a set of statistical techniques, multivariate data analysis is strategically neutral. 
Techniques can be used for many purposes in the behavioral and administrative 
sciences—ranging from the analysis of data obtained from rigidly controlled experiments 
to teasing out relationships assumed to be present in a large mass of survey-type data. 
What can be said is that multivariate analysis is concerned with association among 
multiple variates (i.e., many variables).^ 
Raymond Cattell (1966) has put the matter well. Historically, empirical work in the 
behavioral sciences—more specifically, experimental psychology—has reflected two 
principal traditions: (a) the manipulative, typically bivariate approach of the researcher 
viewed as controller and (b) the nonmanipulative, typically multivariate approach of the 
researcher viewed as observer. 
Cattell points out three characteristics that serve to distinguish these forms of strategic 
inquiry: 
1. bivariate versus multivariate in the type of data collected, 
2. 
manipulative versus noninterfering in the degree of control exercised by the 
researcher, 
3. 
simultaneous versus temporally successive in the time sequence in which 
observations are recorded. 
* Analysis of bivariate data can, of course, be viewed as a special case of multivariate analysis. 
However, in this book our discussion will emphasize association among more than two variables. One 
additional point-some multivariate statisticians restrict the term multivariate to cases involving more 
than a single criterion variable. Here, we take a broader view that includes muhiple regression and its 
various extensions as part of the subject matter of multivariate analysis. 

1.2. 
MULTIVARIATE METHODS IN RESEARCH 
3 
In recent years, bivariate analysis and more rigid forms of controlled inquiry have 
given way to experiments and observational studies dealing with a comparatively large 
number of variables, not all of which may be under the researcher's control. However, if 
one takes a broad enough view of multivariate data analysis, one that includes bivariate 
analysis as a special case, then the concepts and techniques of this methodology can be 
useful for either stereotype. Indeed, Cattell's definition of an experiment as: 
.. .A recording of observations, quantitative or qualitative, made by defined 
operations under defined conditions, and designed to permit non-subjective 
evaluation of the existence or magnitude of relations in the data. It aims to fit these 
relations to parsimonious models, in a process of hypothesis creation or hypothesis 
checking, at least two alternatives being logically possible in checking this 
fit.... 
(p. 9) 
says quite a bit about the purview of multivariate analysis. That is, the process of 
scientific inquiry should embrace the search for naturalistic regularities in phenomena as 
well as their incorporation into models for subsequent testing under changed conditions. 
And in this book we shall be as much, if not more so, interested in using multivariate 
analysis to aid the process of discovery (hypothesis creation) as to aid the process of 
confirmation (hypothesis testing). 
The heart of any multivariate analysis consists of the data matrix, or in some cases, 
matrices.^ The data matrix is a rectangular array of numerical entries whose informa-
tional content is to be summarized and portrayed in some way. For example, in 
univariate statistics the computation of the mean and standard deviation of a single 
column of numbers is often done simply because we are unable to comprehend the 
meaning of the entire column of values. In so doing we often (willingly) forego the full 
information provided by the data in order to understand some of its basic characteristics, 
such as central tendency and dispersion. Similarly, in multivariate analysis we often use 
various summary measures—means, variances, covariances—of the raw data. Much of 
multivariate analysis is concerned with placing in relief certain aspects of the association 
among variables at the expense of suppressing less important details. 
In virtually aU applied studies we are concerned with variation in some characteristic, 
be it travel time of a white rat in a maze or the daily sales fluctuations of a retail store. 
Obviously, if there is no variation in the characteristic(s) under study, there is little need 
for statistical methods. 
In multivariate analysis we are often interested in accounting for the variation in one 
variable or group of variables in terms of covariation with other variables. When we 
analyze associative data, we hope to "explain" variation according to one or more of the 
following points of view: 
1. determination of the nature and degree of association between a set of criterion 
variables and a set of predictor variables, often called "dependent" and "independent" 
variables, respectively; 
2. finding a function or formula by which we can estimate values of the criterion 
variable(s) from values of the predictor variable(s)—this is usually called the regression 
problem; 
^ Much of this section is drawn from Green and TuU (1975). 

4 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
3. 
assaying the statistical "confidence" in the results of either or both of the above 
activities, via tests of statistical significance, placing confidence intervals on parameter 
estimates, or other ways. 
In some cases of interest, however, we have no prior basis for distinguishing between 
criterion and predictor variables. We may still be interested in their interdependence as a 
whole and the possibility of summarizing information provided by this interdependence 
in terms of other variables, often taken to be linear composites of the original ones. 
1.3 
A CLASSIFICATION OF TECHNIQUES FOR 
ANALYZING ASSOCLiTIVE DATA 
The field of associative data analysis is vast; hence it seems useful to enumerate various 
descriptors by which the field can be classified. The key notion underlying the 
classification of multivariate methods is the data matrix. A conceptual illustration is 
shown in Table 1.1. We note that the table consists of a set of objects (the m rows) and a 
set of measurements on those objects (the n columns). Cell entries represent the value Xij 
of object / on variable /. The objects are any kind of entity with characteristics capable of 
being measured. The variables are characteristics of the objects and serve to define the 
objects in any specific study. The cell values represent the state of object / with respect to 
variable /. Cell values may consist of nominal, ordinal, interval, or ratio-scaled 
measurements, or various combinations of these, as we go across columns. 
By a nominal scale we mean categorical data where the only thing we know about the 
object is that it falls into one of a set of mutually exclusive and collectively exhaustive 
categories that have no necessary order vis a vis one another. Ordinal data are ranked data 
\\iiere all we know is that one object / has more, less, or the same amount of some 
variable / than some other object /'. Interval scale data enable us to say how much more 
one object has than another of some variable / (i.e., intervals between scale values are 
meaningful). Ratio scale data enable us to define a natural origin (e.g., a case in which 
Objects 
1 
2 
3 
/ 
m 
TABLE 1.1 
Illustrative Data Matrix 
1 
^11 
^31 
Xil 
^ml 
2 
X32 
xn 
Xm2 
Variables 
3 
^13 •• 
^23 • • 
^33 .. 
Xi3 
Xm3 ' • 
J 
• ^2/ • 
^ii 
• Xjfij 
. 
n 
• X2n 
• X^n 
Xin 
' • Xffin 

1.3. 
TECHNIQUES FOR ANALYZING ASSOCIATIVE DATA 
5 
object i has zero amount of variable /), and ratios of scale values are meaningful. Each 
higher scale type subsumes the properties of those below it. For example, ratio scales 
possess all the properties of nominal, ordinal, and interval scales, in addition to a natural 
origin. 
There are many descriptors by which we can characterize methods for analyzing 
associative data.^ The following represent the more common bases by which the activity 
can be classified: 
1. purpose of the study and the types of assertions desired by the researcher—what 
kinds of statements does he wish to make about the data or about the universe from 
which the data were drawn? 
2. 
focus of research emphasis—statements regarding the objects (i.e., the whole 
profile or "bundle" of variables), specific variables, or both; 
3. 
nature of his prior judgments as to how the data matrix should be partitioned in 
terms of the type and number of subsets of variables; 
4. 
number of variables in each of the partitioned subsets; 
5. type of association under study—linear in the parameters, transformable to linear, 
or "inherently" nonlinear in the parameters; 
6. 
scales by which variables are measured—nominal, ordinal, interval, ratio, mixed. 
All of these descriptors relate to certain decisions required of the researcher. Suppose 
he is interested in studying certain descriptive relationships among variables. If so, he 
must make decisions about how he wants to partition the set of columns (see Table 1.1) 
into subsets. Often he will call one subset "criterion" variables and the other subset 
"predictor" variables.^ He must also decide, however, on the number of variables to 
include in each subset and on what type of functional relationship is to hold among the 
parameters in his statistical model. 
Most decisions about associative data analysis are based on the researcher's "private 
model" of how the variables are related and what features are useful for study.^ His 
choice of various "public models" for analysis—multiple regression, discriminant analysis, 
etc.—is predicated on his prior knowledge of the characteristics of the statistical universe 
from which the data were obtained and his knowledge of the assumption structure of 
each candidate technique. 
1.3.1 
Researcher's Objectives and Predictive Statements 
We have already commented that the researcher may be interested in (a) measuring the 
nature and degree of association between two or more variables; (b) predicting the values 
of one or more criterion variables from values of one or more predictor variables; or (c) 
^ An excellent classification, based on a subset of the descriptors shown here, has been provided by 
M. M. Tatsuoka and D. V. Tiedeman (1963). 
* As Horst (1961) has shown, relationships need not be restricted to two sets. 
^ To some extent this is true even of the scales along which the data are measured. The researcher 
may wish to "downgrade" data originally expressed on interval scales to ordered categories, if he feels 
that the quality of the data does not warrant the "strength" of scale in which it is originally expressed. 
In other cases he may "upgrade*' data in order to use some statistical technique that assumes a type of 
measurement that is absent originally. 

6 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
assessing the statistical reliability of an association between two or more variables. In a 
specific study all three objectives may be pursued. In using other techniques (i.e., those 
dealing mainly with factor and cluster analysis), the researcher may merely wish to 
portray association in a more parsimonious way without attempting to make specific 
predictions or inferential statements. 
1.3.2 
Focus of Research Interest 
Some multivariate techniques (e.g., multiple regression) focus on association among 
variables; objects are treated only as replications. Other techniques (e.g., cluster analysis) 
focus on association among objects; information about specific variables is usually, 
although not necessarily, suppressed. In still other instances one may wish to examine 
interrelationships among variables, objects, and object-variable combinations, as well. 
1.3.3 
Nature of Assumed Prior Judgments or Presuppositions 
In many cases the investigator is able to partition the data matrix into subsets of 
columns (or rows) on the basis of prior judgment. For example, suppose the first column 
of Table 1.1 is average weekly consumption of coffee by households, and the other 
columns consist of various demographic measurements of the m households. The analyst 
may wish to predict average weekly consumption of coffee from some Hnear composite 
of the n — 1 remaining variables. If so, he has used his presuppositions regarding how the 
dependence is to be described and, in this instance, might employ multiple regression. 
In most cases the number of subsets developed from the data matrix partitioning will 
be two, usually labeled as criterion and predictor variable subsets. However, techniques 
have been designed to summarize association in cases involving more than two subsets of 
data. 
Finally, we may have no reasonable basis for partitioning the data matrix into criterion 
or predictor variables. Our purpose here may be merely to group objects into "similar" 
subsets, based on their correspondence over the whole profile of variables. Alternatively, 
we may wish to portray the columns of the data matrix in terms of a smaller number of 
variables, such as linear combinations of the original set, that retain most of the 
information in the original data matrix. Cluster analysis and factor analysis, respectively, 
are useful techniques for these purposes. 
1.3.4 
Number of Variables in Partitioned Subsets 
Clearly, the term "association" impUes at least two characteristics—for example, a 
single criterion and a single predictor variable, usually referred to as bivariate data. In 
other cases involving two subsets of variables, we may wish to study association between 
a single criterion and more than one predictor. Or we may wish to study association 
between composites of several criterion variables and composites of several predictor 
variables. Finally we may want to study the relationship between several criterion 
variables and a single predictor variable. 
Of course, we may elect not to divide the variables at all into two or more subsets, as 
would be the case in factor analysis. Furthermore, if we do elect to partition the matrix 

1.3. TECHNIQUES FOR ANALYZING ASSOCIATIVE DATA 
7 
and end up with two or more variables in a particular subset, what we are usually 
concerned with are various linear composites of the variables in that subset and each 
composite's association with other variables. 
1.3.5 
Type of Association 
Most of the models of multivariate analysis emphasize Hnear relationships among the 
variables. The assumption of linearity, in the parameters, is not nearly so restrictive as it 
may seem.^ First, various preliminary transformations (e.g., square root, logarithmic) of 
the data are possible in order to achieve linearity in the parameters.^ Second, the use of 
"dummy" variables, coded, for example, as elementary polynomial functions of the 
"real" variables, or indicating category membership by patterns of zeroes and ones, will 
enable us to handle certain types of nonlinear relationships within the framework of a 
Hnear model. Third, a linear model is often a good approximation to a nonlinear one, at 
least over restricted ranges of the variables in question. 
1.3.6 
Types of Scales 
Returning to the data matrix of Table 1.1, we now are concerned with the scales by 
which the characteristics are represented. Since all of the multivariate statistical 
techniques to be discussed in this book require no stronger form of measurement than an 
interval scale, we shall usually be interestea in the following types: (a) nominal, (b) 
ordinal, and (c) interval. In terms of nominal scahng we shall find it useful to distinguish 
between dichotomies and (unordered) polytomies, the latter categorization involving 
more than two classes. 
This distinction is important for three reasons. First, many of the statistical techniques 
for analyzing associative data are amenable to binary-coded (zero-one) variables but not 
to polytomies. Second, any polytomy can be recoded as a set of dichotomous "dummy" 
variables; we shall describe how this re coding is done in the next section. Third, when we 
discuss geometrical representations of variables and/or objects, dichotomous variables can 
be handled within the same general framework as interval-scaled variables. 
Finally, mention should be made of cases in which the analyst must contend with 
mixed scales in the criterion subset, predictor subset, or both. Many multivariate 
techniques—if not modified for this type of appHcation—lead to rather dubious results 
under such circumstances. 
^ By linear in the parameters is meant that the bfs in the expression y = b^x^ + b^x^ + • • * + b^x^ 
are each of the first degree. Similarly, z = b^x^^ -^ • - • + b^x^ 
is still linear in the parameters since 
each bj continues to be of the first degree even though xj is not. 
"^ For example, the complicated expression y = ax^e^^ (with both a, x > 0) can be "linearized" as 
In y = In a + b In x + ex and, as shown by Hoerl (1954), is quite flexible in approximating many 
diverse types of curves. On the other hand, the function>^ = l/(a + b~^^} is inherently nonlinear in the 
parameters and cannot be "linearized" by transformation. 

8 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
1.4 
ORGANIZING THE TECHNIQUES 
In most textbooks on multivariate analysis, three of the preceding characteristics are 
often used as primary bases for technique organization: 
1. whether one's principal focus is on the objects or on the variables of the data 
matrix; 
2. 
whether the data matrix is partitioned into criterion and predictor subsets, and 
the number of variables in each; 
3. 
whether the cell values represent nominal, ordinal, or interval scale measurements. 
This schema results in four major subdivisions of interest: 
1. single criterion, multiple predictor association, including multiple regression, 
analysis of variance and covariance, and two-group discriminant analysis; 
2. multiple criterion, multiple predictor association, including canonical correlation, 
multivariate analysis of variance and covariance, multiple discriminant analysis; 
3. analysis of variable interdependence, including factor analysis, multidimensional 
scaling, and other types of dimension-reducing methods; 
4. analysis of interohject similarity, including cluster analysis and other types of 
object-grouping procedures. 
The first two categories involve dependence structures where the data matrix is 
partitioned into criterion and predictor subsets; in both cases interest is focused on the 
variables. The last two categories are concerned with interdependence—either focusing on 
variables or on objects. Within each of the four categories, various techniques are 
differentiated in terms of the type of scale assumed. 
1.4.1 
Scale Types 
Traditionally, multivariate methods have emphasized two types of variables: 
1. more or less continuous variables, that is, interval-scaled (or ratio-scaled) 
measurements; 
2. 
binary-valued variables, coded zero or one. 
The reader is no doubt already famiHar with variables like length, weight, and height that 
can vary more or less continuously over some range of interest. 
Natural dichotomies such as sex, male or female, or marital status, single or married, 
are also famiHar. What is perhaps not as well known is that any (unordered) polytomy, 
consisting of three or more mutually exclusive and collectively exhaustive categories, can 
be recoded into dummy variables that are typically coded as one or zero. To illustrate, a 
person's occupation, classified into five categories, could be coded as: 
Dummy variable 
Category 
1 2 
3 
4 
Professional 
Clerical 
Skilled laborer 
Unskilled laborer 
Other 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 

1.4. 
ORGANIZING THE TECHNIQUES 
p 
II 
Q 
P 
I I I 
Q 
I 
Fig. l.I 
Illustrative partitionings of data matrix. 
For example, if a person falls into the professional category, he is coded 1 on dummy 
variable 1 and 0 on dummies 2 through 4. In general, a /:-category polytomy can be 
represented \>y k — \ dummy variables, with one category—such as the last category-
receiving a value of zero on all /: — 1 dummies.^ 
Multivariate techniques that are capable of dealing with some or all variables at the 
ordinally scaled level are of more recent vintage. With few exceptions our attention in this 
book will be focused on either continuous or binary-valued variables.^ 
Figure 1.1 shows some of the major ways in which the data matrix can be viewed from 
the standpoint of technique selection. 
1.4.2 
Single Criterion, Multiple Predictor Association 
In Panel I of the figure we note that the first column of the matrix has been singled 
out as a criterion variable and the remaining n— \ variables are considered as predictors. 
For example, the criterion variable could be average weekly consumption of beer by the 
/th individual. The n—\ predictors could represent various demographic variables, such 
as the individual's age, years of education, income, and so on. This is a prototypical 
problem for the application of multiple regression in which one tries to predict values of 
the criterion variable from a linear composite of the predictors. The predictors, 
incidentally, can be either continuous variables or dummies, such as marital status or sex. 
Altematively, the single criterion variable could represent the prior categorization of each 
individual as heavy beer drinker (coded one, arbitrarily) or light beer drinker (coded 
zero), based on some designated amount of average weekly beer consumption. If our 
purpose is to develop a linear composite of the predictors that enables us to classify each 
individual into either heavy or light beer drinker status, then we would employ two-group 
discriminant analysis. The critical distinction here is that the criterion variable is 
expressed as a single dummy variable rather than as a continuous one. 
' Not only is greater parsimony achieved by using only k — \ (rather than ^) categories, but, as will 
be shown in later chapters, this type of coding device permits the matrix to be inverted by regular 
computational methods. 
^ While a classification could be coded as 1, 2, 3 , . . ., A: in terms of a single variable, the resulting 
analysis would assume that all classes are ordered and equally spaced-2i rather dubious assumption in 
most kinds of classificatory data. 

10 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
Another possibility can arise in which the criterion variable continues to be average 
weekly beer consumption, but the predictor set consists of a classification of each 
individual into some occupational group, coded as a set of dummy or design variables. 
This represents an instance in which the technique of analysis of variance could be used. 
On the other hand, if the predictor set consists of a classification of occupations as well as 
annual income in dollars, then the latter variable could be treated as a covariate. In this 
case we would be interested in whether average weekly beer consumption differs across 
occupations once the effect of income is controlled for statistically. 
1.4.3 
Multiple Criterion, Multiple Predictor Association 
In Panel II of Fig. 1.1 the p = 3 criterion variables could denote individual 
consumption of beer, wine, and liquor, and the remaining variables could denote 
demographic characteristics. If we were interested in the linear association between the 
two batteries of variables, we could employ the technique of canonical correlation. 
Suppose, alternatively, that all individuals had been previously classified into one of 
four groups: (a) malt beverage drinker only, (b) drinker of spirits (Hquor or wine) other 
than malt beverages, (c) drinker of both spirits and malt beverages, and (d) drinker of 
neither. We could then develop linear functions of the demographics that would enable us 
to assign each individual to one of the four groups in some "best" way (to be defined 
later). This is an illustration of multiple discriminant analysis; note that four mutually 
exclusive groups are classifiable in terms of p = 3 criterion dummies. 
Alternatively, we could continue to let the criterion variables denote individual 
consumption levels of beer, wine, and liquor, but now assume that the predictors 
represent dummies based on an occupational classification. If so, multivariate analysis of 
variance is the appropriate procedure. If income is again included as a covariate, we have 
an instance of multivariate analysis of covariance. 
Panel III of Fig. 1.1 shows a data structure involving association among three batteries 
of variables. Generalized canonical correlation can be employed in this type of situation. 
In this case we would be interested in what all three batteries exhibit in common and also 
in the strength of association between all distinct pairs of batteries as well. 
1A.4 
Dimension-Reducing Methods 
Panel IV of Fig. 1.1 shows a set of t appended columns, each of which is expressed as a 
Hnear composite of the original n variables. Suppose we want to portray the association 
across the m individuals in terms of fewer variables than the original n variables. If so, we 
might employ factor analysis, multidimensional scaling, or some other dimension-
reduction method to represent the original set of n correlated variables as linear (or 
nonlinear) composites of a set of t {t < n) underlying or "latent" variables in such a way 
as to retain as much of the original information as possible. The composites themselves 
might be chosen to obey still other conditions, such as being mutually uncorrelated. 
Thus, if the original n variables are various demographics characterizing a set of beer 
drinkers, we might be able to find a set of more basic dimensions—social class, stage in life 
cycle, etc.—so that linear composites of these basic dimensions account for the observable 
demographic variables. 

1.5. 
ILLUSTRATIVE APPLICATIONS 
11 
1.4.5 
Interobject Similarity 
So far we have confined our attention to the columns of the matrices in Fig. 1.1. 
Suppose now that the n columns represent consumption of various kinds of alcoholic 
beverages—beers, ales, red wines, white wines, liquors, after-dinner cordials—over some 
stated time period. Each individual's consumption profile could be compared with every 
other individual's, and we could develop a measure of interindividual similarity with 
respect to patterns of alcoholic beverage drinking. 
Having done so, we could then proceed to cluster individuals into similar groups on the 
basis of the overall similarity of their consumption profiles. Note here that information 
on specific variables is lost in the computation of interindividual similarity measures. 
Since our focus of interest is on the objects rather than on the variables, we may be 
willing to discard information on separate variables in order to grasp the notion oi overall 
interobject similarity (and the "clusteriness" of objects) more clearly. 
All of these techniques—and others as well—have been employed in the behavioral and 
administrative sciences. As suggested above, the tools of multivariate analysis form a 
unified set, based on a relatively few descriptors for distinguishing specific techniques. 
1.5 
ILLUSTRATIVE APPLICATIONS 
Any empirically grounded discipline has need on occasion to use various types of 
multivariate techniques. Indeed, in some fields like psychometrics and survey research, 
multivariate analysis represents the methodological comerstone. 
Although multivariate analysis can be, and has been, used in the physical and life 
sciences, increasing appHcations are being made in the behavioral and administrative 
sciences. Even at that, the view is a broad one, as the following list of behavioral and 
administrative examples illustrate. 
Example 1 Two economists, Quandt and Baumol (1966), were interested in 
predicting the demand for alternative modes of travel between various pairs of cities. 
They developed a characterization of each mode of travel (e.g., airplane, train, bus, 
private car) as a service profile varying in levels of cost, departure frequency, convenience 
to the traveler, speed, and so on. 
A travel-demand forecasting model for each mode was then prepared which utiHzed a 
linear function of the logarithms of service profile levels. Traffic volumes, involving 
sixteen different city pairs, were available for each of the abovementioned modes to 
serve as criterion variables. The parameters of their demand forecasting model were 
estimated by multiple regression. 
Example 2 
A group of psychologists, Rorer et ah (1967), were concerned with the 
modeling of clinical judgment and, in particular, how subjects combined various 
information cues into an overall judgment. The subjects of their experiment were small 
groups of physicians, nurses, psychologists, and social workers. The judgment to be made 
concerned the subject's probabiHty of granting a weekend pass to each of 128 (presumed 
real) patients. Each "patient" was described according to a six-component profile, 
involving such characteristics as (a) whether he had a drinking problem; (b) whether he 

12 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
had abused privileges in the past; (c) whether his personal appearance was neat, and so on. 
The patient was described simply as to whether he displayed the characteristic or not. 
The six characteristics used by the researchers were formulated in a 2^ design of all 
possible combinations, and two replications were made up of each combination, leading 
to a total of 128 that were presented (in random order) to each subject. An analysis was 
made of each subject's response data separately, using an analysis of variance model 
applicable to a full factorial design. The researchers found evidence that subjects used 
cues interactively in arriving at an overall judgment. The relative importance of these 
interactions was measured as well as the separate main-effect contributions to the overall 
judgment regarding the subjective probability of granting a pass. 
Example 3 A political scientist, R. J. Rummel (1970), was interested in a 
cross-national comparison of some 82 different countries, measured according to 230 
characteristics. In particular, he wished to see what underlying factors or dimensions 
might account for various observed relationships across such characteristics as the nations' 
trade levels, memberships in international organizations, production of various com-
modities, and so on. 
A variety of factor analyses and cluster analyses were performed on the data, leading 
to a set of underlying dimensions, identified principally as the nation's political 
orientation, economic development, and degree of foreign conflict. 
Two mathematical psychologists. Wish and Carroll (1973), were also interested in 
national similarities and differences but, in this case, as subjectively perceived by U.S. and 
foreign students. Their methodology emphasized multidimensional scaling and, in 
particular, individual differences models of perception. They found that different subjects 
gave different importances to perceptual dimensions, depending upon the subject's 
attitude toward U.S. involvement in the Vietnam conflict. 
Example 4 
Two educational psychologists, Cooley and Lohnes (1971), were engaged 
in a massive sampling survey, called Project TALENT, involving measurement on a large 
number of personality and ability variables of a representative sample of American high 
school students. The purpose of the study was to examine interrelationships among these 
variables and various environmental variables in order to predict the students' motivations 
involving subsequent career and higher education activities. 
A variety of multivariate techniques were employed in the analysis of these data. For 
example, one analysis used canonical correlation to examine the association between a set 
of eleven ability-type factors (e.g., verbal knowledge, mathematics, visual reasoning) and a 
set of eleven factors dealing with career motives (e.g., interest in science, interest in 
business). In this case the canonical correlation followed a preHminary factor analysis of 
each separate battery of variables. 
Example 5 
A group of survey researchers, Morgan, Sirageldin, and Baerwaldt (1965), 
were engaged in a large-scale survey in which the criterion variable of interest was hours 
spent on do-it-yourself activities by heads of families and their spouses. A sample size of 
2214 households provided the data, and the predictor variables included a large 
number of demographic characteristics. 
Not surprisingly, the authors found that marital status was the most important 
predictor variable. Single men and women spent relatively little time on do-it-yourself 

1.5. 
ILLUSTRATIVE APPLICATIONS 
13 
activities. On the other hand, married couples with large families who had higher-than-
average education, lived in single-family structures in rural areas, with youngest child 
between two and eight years, devoted a large amount of time to do-it-yourself activities. 
The researchers used a multivariate technique, known as Automatic Interaction 
Detection, to develop a sequential branching of groups. At each stage the program selects 
a predictor variable that accounts for the most variation in the criterion variable and splits 
the sample into two subgroups, according to their values on that predictor variable. The 
result is a sequential branching "tree" of groups that are most homogeneous with regard 
to the criterion variable of interest. 
Example 6 
A group of management scientists, Haynes, Komar, and Byrd (1973), 
were interested in the comparative performance of three heuristic rules that had been 
proposed for sequencing production jobs that incur setup changes. The objective of each 
of the rules was to minimize machine downtime over the whole production sequence. For 
example, Rule 1 involved selecting as the next job that one which has the least setup time 
relative to the job last completed, of all jobs yet unassigned. The researchers were 
interested in how the competing heuristics would perform under variations in setup time 
distributions and total number of jobs to be sequenced. 
The researchers set up an experimental design in which appHcation of the three rules 
was simulated in a computer under different sets of distribution times and numbers of 
jobs. The factorial design employed by the authors to test the behavior of the rules was 
then analyzed by analysis of variance procedures. The experiment indicated that a 
composite of the three heuristics might perform better than any of the three rules taken 
singly. 
Example 7 Two marketing researchers. Perry and Hamm (1969), beHeved that 
consumers might ascribe higher importance to personal sources (in the selection of 
products involving high socioeconomic risk) than to impersonal sources of product 
information. They set up an experiment in which consumers rated a set of 25 products on 
degree of perceived social risk and degree of economic risk. Each respondent was also 
asked to rate the significance of various sources of influence (e.g., advertisements, 
Consumer's Reports, a friend's recommendations) on one's choice of brand within each 
product class. 
The authors used canonical correlation to relate the two sets of measures. They found 
that the higher the risk, particularly the social risk, the greater the perceived importance 
of personal influence on brand choice. The authors concluded that in the advertising of 
high-risk products (e.g., color TV, automobiles, sports jackets), advertisers should try to 
reach prospective buyers through personal channels, such as opinion leaders, rather than 
through general media. Moreover, advertisers should emphasize the social, rather than the 
economic, benefits of the purchase. 
As the preceding examples suggest, virtually any discipline in the behavioral and 
administrative sciences can find appHcations for multivariate tools. It is not surprising 
why this is so, given that multivariate techniques can be used in both controlled 
experiments and observational studies. And, in the latter case at least, data refuse to come 
in neat and tidy packages. Rather, the predictor variables are usually correlated 
themselves, and one needs statistical tools to assist one in finding out what is going on. 

14 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
Even in controlled experiments it is usually not possible to control for all variables. 
Various experimental devices, such as blocking and covariance adjustment, are often used 
to increase precision as well as to reduce some of the sources of statistical bias. 
Moreover, it seems to be the nature of things in both the behavioral and administrative 
sciences that the possible explanatory variables of some phenomenon of interest are 
myriad and difficult to measure (as well as interrelated). It should come as no surprise 
that methods are needed to reveal whatever patterns exist in the data as well as to help 
the analyst test hypotheses regarding his content area of interest. Thus, multivariate 
techniques are becoming as familiar to the marketing researcher, production engineer, and 
corporate finance officer as they are to the empirically oriented psychologist, sociologist, 
poUtical scientist, and economist. 
In addition to the diffusion among disciplines, multivariate techniques themselves are 
increasing in variety and sophistication. Researchers' past emphasis on multiple regression 
and factor analysis has given way to application of whole new classes of techniques-
canonical correlation, multiple discriminant analysis, cluster analysis, and multidimen-
sional scaUng, to name a few. Methods are being extended to deal with multiway matrices 
and time-de pen dent observations. Computing routines have incorporated still-recent 
developments in nonlinear optimization and other forms of numerical analysis. Methods 
typically used for measured variables have been modified and extended to cope with data 
that are expressed only as ranks or in some cases only in terms of category membership. 
In short, multivariate data analysis has become a vigorous field methodologically and a 
cathohc field substantively. Indeed, it is difficult to think of any behavioral or 
administrative discipline in which multivariate methods have no applicability. 
1.6 
SOME NUMERICAL EXAMPLES 
To a large extent, the study of multivariate techniques is the study of linear 
transformations. In some techniques the whole data matrix—or some matrix derived from 
it—may undergo a Unear transformation. Other methods involve various transformations 
of submatrices obtained from partitioning the original matrix according to certain 
presuppositions about the substantive data of interest. 
In the chapters that follow we shall be discussing those aspects of linear algebra and 
transformational geometry that underlie all methods of multivariate analysis. The 
arithmetic operations associated with vectors and matrices, determinants, eigenstructures, 
quadratic forms, and singular value decomposition are some of the concepts that will be 
presented. 
As motivation for the study of these tools, let us consider three problems that could 
arise in an applied research area. While almost any field could supply appropriate 
examples, suppose we are working in the field of personnel research. In particular, 
imagine that we are interested in certain aspects of employee absenteeism. We shall 
assume that all employees are male clerks working in an insurance company. 
Absenteeism records have been maintained for each employee over the past year. 
Personnel records also indicate how long each employee has worked for the company. In 
addition, each employee recently completed a clinical interview with the company 
psychologist and was scored by the psychologist on a l-to-13 point rating scale, with " 1 " 

1.6. 
SOME NUMERICAL EXAMPLES 
15 
TABLE 1.2 
Personnel Data Used to Illustrate Multivariate Methods 
Employee 
a 
b 
c 
d 
e 
f 
g 
h 
i 
J 
k 
1 
Mean 
Standard 
deviation 
Number of days absent 
Y 
1 
0 
1 
4 
3 
2 
5 
6 
9 
13 
15 
16 
6.25 
5.43 
Yd 
-5.25 
-6.25 
-5.25 
-2.25 
-3.25 
-4.25 
-1.25 
-0.25 
2.75 
6.75 
8.75 
9.75 
>^s 
-0.97 
-1.15 
-0.97 
-0.41 
-0.60 
-0.78 
-0.23 
-0.05 
0.51 
1.24 
1.61 
1.80 
Xi 
1 
2 
2 
3 
5 
5 
6 
7 
10 
11 
11 
12 
6.25 
3.77 
Attitude rating 
^ d l 
-5.25 
-4.25 
-4.25 
-3.25 
-1.25 
-1.25 
-0.25 
0.75 
3.75 
4.75 
4.75 
5.75 
^ s l 
-1.39 
-1.13 
-1.13 
-0.86 
-0.33 
-0.33 
-0.07 
0.20 
0.99 
1.26 
1.26 
1.53 
Years with company 
X2 
1 
1 
2 
2 
4 
6 
5 
4 
8 
7 
9 
10 
4.92 
2.98 
^d2 
-3.92 
-3.92 
-2.92 
-2.92 
-0.92 
1.08 
0.08 
-0.92 
3.08 
2.08 
4.08 
5.08 
^s2 
-1.31 
-1.31 
-0.98 
- 0 . ^ 
-0.31 
0.36 
0.03 
-0.31 
1.03 
0.70 
1.37 
1.71 
indicating an extremely favorable attitude and "13" indicating an extremely unfavorable 
attitude toward the company. (The 13 scale points were chosen arbitrarily.) 
For purposes of illustration, a sample of 12 employees was selected for further 
study.^^ The "raw" data on each of the three variables are shown in Table 1.2. Figure 1.2 
shows each pair of variables in scatter plot form. 
From Fig. 1.2 we note the tendency for all three variables to be positively associated. 
That is, absenteeism increases with unfavorableness of attitude toward the firm and 
number of years with the company. Moreover, unfavorableness of attitude is positively 
associated with number of years of employment with the firm (although one might 
question the reasonableness of this assumed relationship). 
Table 1.2 also shows the means and sample standard deviations of each of the three 
variables. By subtracting out the mean of each variable from that variable's original 
observation we obtain three columns of deviation (or me an-corrected) scores, denoted by 
Y^, Xdi, and X^2 in Table 1.2. To illustrate: 
Y^i=Yi-Y 
where F, denoting the mean of Y, is written as 
^^ Obviously, the small sample size of only 12 employees is for illustrative purposes only; 
moreover, all data are artificial. 

16 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
2 ><-
o 
e 
B 
o 
T3 
O 
• ° 
H 
dp 

1.6. 
SOME NUMERICAL EXAMPLES 
17 
H 
\ 
h 
-8 
8 + 
4-h 
•-4 
-8+ 
il 
l-h 
-^—h 
1 
\ 
h-^di 
-1-h 
Fig. 1.3 
Illustrative scatter plots of (I) mean-corrected and (11) standardized data. 
Standardized scores, denoted by Y^, X^i, and Xs2, are obtained by dividing each 
mean-corrected score by the sample standard deviation of that variable.^* To illustrate: 
where Sy, in turn, is defined as 
^si 
^dil^y 
\sy = 
- m 
1/2 
Figure 1.3 shows, illustratively, the scatter plot of mean-corrected and standardized 
scores involving the criterion variable Y versus the first predictor Xi. We note that the 
computation of deviation scores merely changes the origin of the plot to an average of 
zero on each dimension. Interpoint distances do not change, and the configuration of 
points looks just like the configuration shown in the leftmost panel of Fig. 1.2. 
Standardization of the data, however, does change the shape of the configuration, as 
well as shifting the origin. If the right-hand panel of Fig. 1.3 is compared to the left-hand 
panel, we see that the vertical axis, or ordinate, is compressed, relative to the horizontal 
axis, or abcissa. This is because the Y^ values are being divided by a larger constant (5.43) 
than the X^i values, the latter being divided by 3.77, the sample standard deviation of 
^* The reader will note that Sy denotes the sample standard deviation rather than an estimate of the 
universe standard deviation. In this latter case the divisor would be m - 1, rather than m, as used here. 

18 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
1.6.1 
Research Questions 
After this preliminary examination of the data, suppose the researcher raises the 
following questions: 
1. How does Y relate to changes in Xi and X2 ? 
a. 
Can an equation be developed that will enable us to predict values of F as a 
linear function of J!fi and X2? 
b. 
How strong is the overall relationship of Y with Xi and X2 ? 
c. 
Is the overall relationship statistically significant? 
d. 
What is the relative influence of Xi and X2 on variation in Y and are these 
separate influences statistically significant? 
2. 
Next, considering the relationship between the predictor variables Xi and X2, 
some further questions can be asked: 
a. 
Can the 12 scores on Xi and X2 be replaced by scores on a single variable 
that represents a linear composite of the two separate scores? That is, do Xi and X2 
really reflect just a single underlying factor, or are there two separate factors operating? 
b. 
What is the association of Xi 
and X2, respectively, with this Hnear 
composite? 
c. 
How much of the total variation in Xi and X2 is accounted for by the single 
hnear composite? 
3. 
One additional thing that we might do is to spht the sample of employees into 
three groups: Group 1-employees a, b, c, d; Group 2-employees e, f, g, h; Group 
3—employees i, j , k, 1. We could call these three groups low-, intermediate-, and 
high-absenteeism groups, respectively.^^ 
Having classified the 12 respondents in this way, we would then raise the questions: 
a. 
How do we go about defining a Unear composite of Xi and X2 that 
maximally separates the three groups? 
b. 
Is this linear composite statistically significant? 
c. 
How well does the linear composite assign individuals to their correct groups? 
d. What is the relative influence of Xi and X2 on group assignment and are their 
separate contributions statistically significant? 
e. 
How could we find a second Unear composite, uncorrected with the first, 
that does the next best job of separating the groups (and so on)? 
Each set of questions describes a particular multivariate technique which we now 
consider. 
1.6.2 
Multiple Regression 
The first set of questions pertain to a problem in multiple regression. This, in turn, 
involves the subproblems of developing an estimating equation, computing its strength of 
*^ This assumes, of course, that specific numerical data on number of days absent are being 
supplanted by interest only in the three groups of the classification: low, intermediate, and high 
absenteeism. This disregard for numerical information on number of days absent is strictly for 
motivating the discussion of multiple discriminant analysis, although it could be rationalized on other 
grounds, such as possible nonlinear association. 

1.6. 
SOME NUMERICAL 
EXAMPLES 
19 
relationship and statistical significance, and examining the contribution of each predictor 
to changes in the criterion variable. 
Insofar as the first problem is concerned, we shall want to find parameter values for 
the linear equation: 
Y- bo -^ biXi + Z?2-^2 
where Y denotes predicted values of Y; bo denotes the intercept term when Xi and X2 
are both 0; and bi and Z72 denote the partial regression coefficients of Xi and X2, 
respectively. The partial regression coefficient measures the change in Y per unit change 
in some specific predictor, with other predictors held constant. 
In terms of the data of Table 1.2 we shall want to find the parameter values bo, bi, Z?2 
and the 12 predicted values: 
Yi = bo + b,(l) + b2(l) 
Y2 = bo-^b,(2) + b2(l) 
Ys = bo + b,(2) + b2(2) 
Yn = bo + b,(12) + b2(l0) 
(where the numbers in parentheses are actual numerical values of Xi and X2, from 
Table 1.2). As will be shown in subsequent chapters, we shall find the specific values of 
bo, bi, and Z?2, according to the least-squares principle. This entails minimizing the sum 
of the squared errors e/ : 
12 
12 
12 
The least-squares principle leads to a set of linear equations which, when solved, provide 
the desired parameter values. 
The second question, concerning strength of the (linear) relationship, is answered by 
computing ^^, the squared multiple correlation. R^ measures how much of the variation 
in Y, as measured about its mean ?, is accounted for by variation inXi andX2.R^ can 
be expressed quite simply as 
R'-- = 1-
y l 2 
1-UY,-
2 
•Yf\ 
where the denominator represents the sum of squares in Y as measured about its own 
mean. As can be observed, if the sum of squared errors is zero, then the Y/s predict their 
respective Y/s perfectly and R^ = 1. However, if the inclusion of Xi and X2 in the 
estimating equation does no better than use of the Y alone, then the numerator of the 
fraction equals the denominator midR^ = 0, denoting no variance accounted for, beyond 
using the criterion variable's mean. 

20 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
The third question entails a test of the null hypothesis of no linear association between 
Y and Xi and X2, as considered together. This can be expressed either as 
Rp = 0 
where R^ denotes the population multiple correlation, or as 
i3i=/32 = 0 
where j3i and P2 denote population partial regression coefficients. In Chapter 6 these 
tests will actually be carried out in terms of the sample problem of Table 1.2. 
The fourth question concems what are called partial correlation coefficients. One 
interpretation of a partial correlation coefficient considers it as a measure of the linear 
association between the criterion variable and some predictor when both have been 
adjusted for their Unear association with the remaining predictors. Although the question 
of determining the relative influence of predictors is an ambiguous one, we shall comment 
on partial correlations, and their associated tests of significance, in Chapter 6. 
Multiple regression, aside from being the most popular multivariate technique in 
applied research, provides a vehicle for subsequent discussion of all basic matrix 
operations and, in particular, the topics of determinants, matrix inversion, and matrix 
rank. These aspects of matrix algebra are essential in understanding the procedures for 
solving simultaneous equations, as appearing in multiple regression and other multivariate 
procedures. 
1.6.3 
Factor Analysis 
The second set of questions refers to a topic in multivariate analysis that is generically 
called factor analysis. In factor analysis we are interested in the interdependence among 
sets of variables and the possibility of representing the objects of the investigation in 
terms of fewer dimensions than originally expressed. 
To illustrate, let us plot the mean-corrected scores of the predictors, X2 versus Xi, as 
shown in Fig. 1.4. Also shown in the same figure is a new axis, labeled Zi, which makes 
an angle of 38° with the horizontal axis. Now suppose we drop perpendiculars, 
represented by dotted lines, from each point to the axis Zi. Assuming the same scale for 
Zi as used for X^i and X(j2» ^^ could compute the variance of the 12 projected scores on 
Zi as 
where z is the mean (which, because the X\ are in deviation form, is zero) of the 12 
scores on z 1. 
The idea behind this procedure—called principal components and representing one 
type of factor analysis—is to find the axis Zi so that the variance of the 12 projections 

1.6. 
SOME NUMERICAL EXAMPLES 
21 
Fig. 1.4 
Scatter plot of mean-corrected predictor variables. 
onto it is maximal. As such the 12 mean-corrected scores, X^i and Xci2, can be 
represented by a linear composite: 
^/(i) ~ h^dii 
+ 
h^dii 
and, hence, each pair of scores, X^n and X^i2 for ^^ch observation /, is replaced by a 
single score Z/^j^ 
In the present example, not much parsimony would be gained by merely replacing two 
scores with one score. In larger-scale problems, consisting of a large number of variables, 
considerable data reduction might be obtained. Moreover, principal components analysis 
allows the researcher to find additional axes, each at right angles to previously found axes 
and all with the property of maximum variance (subject to being at right angles to 
previously found axes). 
This idea is also illustrated in Fig. 1.4 via the second axis Za. Note that this axis has 
been drawn, as it should be, at right angles to Zj. One could, of course, project the 12 
points onto this second axis and obtain a second set of scores. In this case, however, no 
parsimony would be gained, although the two axes Zi and Z2 would be at right angles to 
each other and Zi would contribute, by far, the greater variation in the pair of derived 
composites. 
The second question, concerning the association of X^i and X(X2 with Zi, can be 
answered by computing product-moment correlations, X^i with Zi and X^2 withzi. 
These are called component loadings and are measures of the association between each 
contributing (original) variable and the Unear composite variable Zi that is derived from 
them. Component loadings could also be computed for Z2. 

22 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
The third question regarding how much of the total variation in X^i and X^2 is 
accounted for by Zi is also found from the principal components technique. In this 
example it happens to be 98 percent (leaving only 2 percent for Z2). That is, almost all of 
the original variation in Xdi and X^2 is retained in terms of the single composite variable 
Zi. This is evident by noting from Fig. 1.4 that the original 12 points lie close to the new 
axis Zi, and little information would be lost if their projections onto Zi were substituted 
for their original values on X^^i and X^2-
Subsequent chapters will discuss a number of important concepts from transfor-
mational geometry and matrix algebra-rotations, quadratic forms, eigenstructures of 
symmetric matrices-that pertain to solution procedures for principal components. 
Finally, in Chapter 6 the solution for the present problem will be described in detail. 
1.6.4 
Multiple Discriminant Analysis 
Multiple discriminant analysis also entails a maximization objective. To illustrate, 
Fig. 1.5 shows a plot of X^2 versus X^i. 
This time, however, each of the three 
groups—low, intermediate, and high absenteeism—is represented by different symbols. 
The first axis Wi is the one, in this case, that maximizes among-group variation relative to 
average within-group variation. 
That is, we wish to find a linear composite 
^i{l) = ViX^il-^V2X^i2 
Fig. 1.5 
A discriminant transformation of the mean-corrected predictor variables. Key: • Group 
1; o Group 2; x Group 3. 

1.7. 
FORMAT OF SUCCEEDING CHAPTERS 
23 
with the property of maximizing the variation across group means relative to their pooled 
within-group variation. 
Note that the first of these axes, Wj in Fig. 1.5, makes an angle of 25° with the 
horizontal and, hence, is a different axis than the first principal component of Fig. 1.4. 
Note, also, that the second axis, W2 in Fig. 1.5, is not at right angles to the first. In 
multiple discriminant analysis we can often find additional axes by which the groups can 
be discriminated, but these axes need not make right angles with each other. 
In the case of multiple discriminant analysis, the points are projected onto Wi, and 
this axis exhibits the property of maximally separating the three group means, relative to 
their pooled within-group variation. Each point would be assigned to the closest group 
mean on the Wi discriminant axis (i.e., the average of the four W/i's making up that 
group). As it turns out in this particular illustration, if this rule were followed, no point 
would be misclassified.^^ 
The remaining questions dealing with statistical significance of the discriminant 
function(s), classification accuracy, and the relative importance of the predictor variables 
{X^i and XfX2) ^^^ answered in ways analogous to the multiple regression case. We 
consider these, and related, questions in Chapter 6. 
However^ from the standpoint of transformational geometry and matrix algebra, 
multiple discriminant analysis involves such concepts as general linear transformations, 
simultaneous diagonalization of two different quadratic forms, and the eigenstructure of 
nonsymmetric matrices. Moreover, multiple discriminant analysis can also be related to 
principal component analysis in a space in which the points have previously been 
transformed (i.e., "spherized") to a pooled within-groups variance of unity. 
In short, the three preceding problems provide sufficient motivational material for all 
of the remaining chapters, including the appendixes. Moreover, a full understanding of 
how these three problems can be solved will serve the appHed researcher well insofar as 
understanding almost any other multivariate technique he may encounter. 
1.7 FORMAT OF SUCCEEDING CHAPTERS 
Succeeding chapters of the book are designed to develop the necessary concepts from 
transformational geometry and matrix algebra to deal with most problems in multivariate 
analysis, including the sample problems outHned in the preceding section. Obviously, no 
definitive treatment of the subject has been attempted. What we have tried to do is to 
select those aspects of matrix algebra that are most relevant for subsequent discussion of 
multivariate procedures. 
Chapter 2 discusses definitions and operations on vectors and matrices. Here our 
emphasis is on the mechanics of working with vectors and matrices, rather than their 
geometric conceptualization. Elementary material on determinants is also presented as 
well as a demonstration of how various computations in multivariate analysis—for 
example, sums of squares and cross products—can be compactly expressed in matrix 
notation. 
*^ Such would not be the case for the second discriminant w^ • As will be shown in Chapter 6, this 
second function is not statistically significant and would not be used for classification purposes 
anyway. 

24 
1. 
THE NATURE OF MULTIVARIATE DATA ANALYSIS 
Chapter 3 is concerned with the conceptual aspects of vectors and matrices. Geometric 
representations are employed extensively as we discuss various operations on vectors and 
matrices, including scalar products and related topics in EucUdean distance geometry. 
Common statistical measures—standard deviation, covariance and correlation—are also 
portrayed from a geometric viewpoint. 
Much of multivariate analysis is concerned with linear transformations, and this is the 
focus of Chapter 4. Each type of matrix transformation is described geometrically as we 
discuss such topics as rotations, stretches, and other transformations that have simple, 
geometric representations. Matrix inverses and the notion of matrix rank are also 
introduced here. We conclude the chapter with a description of the geometric effect of 
composite transformations that represent the matrix product of simpler ones. 
Chapter 5 takes the opposite (and complementary) point of view. Here we are 
concemed with decomposing general matrix transformations into the product of simpler 
ones. The topic of matrix eigenstructure is introduced and related to the idea of changing 
basis vectors in order to bring out simpler geometric interpretations of the space. Matrix 
rank-first introduced in Chapter 4—is discussed more thoroughly in the context of the 
singular value decomposition of a matrix. Quadratic forms are also introduced and 
related to matrix eigenstructures. Again, geometric analogy is used wherever it can help 
illuminate the algebraic operations. 
Chapter 6 completes the cycle by taking the reader back to multivariate methods per 
se and, in particular, to the three sample problems introduced in the present chapter. 
Each of these problems—centering around multiple regression, principal components 
analysis, and multiple discriminant analysis—is described from the standpoint of concepts 
developed in Chapters 2-5. Numerical solutions are obtained for each case, and several 
geometric aspects of the methods are illustrated. We conclude the chapter by presenting a 
complementary framework for multivariate technique classification in terms of the nature 
of the transformations characterizing the matching of one set of numbers with some 
other set or sets. As such, this classification descriptor serves as both a way to unify 
earlier material and as a prologue for various textbooks on the general topic of 
multivariate analysis. 
The appendices cover more advanced material relevant to the general topic of 
multivariate analysis. Included here are such concepts as symbolic differentation, constrained 
optimization, generalized inverses, and other special topics of relevance to multivariate 
analysis. 
1.8 
SUMMARY 
The purpose of this chapter has been to set the stage for later material dealing with 
aspects of transformational geometry and matrix algebra of interest to multivariate 
analysis. The topic of multivariate analysis was introduced as a set of procedures for 
dealing with association among multiple variables. A classification system based on 
aspects of the data matrix and the researcher's objectives and presuppositions was 
described as a way of matching problem with technique. 
We then described briefly a number of substantive applications of multivariate 
methods so as to give the reader some flavor of their breadth and diversity of use. 

REVIEW QUESTIONS 
25 
Following this, three prototypical problems, calling for various types of multivariate 
analysis, were described in terms of a miniature and common data bank. These problems 
will serve to motivate subsequent discussion of algebraic and geometric tools, leading to 
the solution of the sample problems in the concluding chapter of the book. 
REVIEW QUESTIONS 
1. What other systems for classifying multivariate techniques can you find in the 
Uterature of your field? 
a. 
How would you compare these classifications with the one presented here? 
b. 
Criticize the present classification and indicate how you would modify it for 
purposes of research in your own discipline. 
2. 
Examine the literature of your field and select a number of examples using 
multivariate analysis. 
a. 
In each example, what was the substantive problem of interest? 
b. 
How did the author's use of the technique(s) relate to the content side of the 
problem? 
c. 
Do other techniques suggest themselves for the specific problem examined by 
the author? 
3. 
In terms of your own research try to formulate a problem that appears suitable for 
multivariate analysis. 
a. 
How would you classify the problem in terms of the system described in this 
chapter? 
b. 
What multivariate procedures are suggested by your classification of the 
problem? 

CHAPTER 2 
Vector and Matrix Operations 
for Multivariate Analysis 
2.1 
INTRODUCTION 
Facility in the arithmetic of vectors and matrices, just like skill in applying ordinary 
arithmetic to daily affairs, is essential in multivariate analysis. In this chapter our purpose 
is to review the fundamentals of vector and matrix operations and the concept of the 
determinant of a matrix. The emphasis here is on defining vector and matrix operations 
and illustrating the mechanics of their appHcation. 
We begin the chapter with a description of vectors as ordered Ai-tuples of numbers that 
are subject to certain manipulative rules. Selected arithmetic operations on vectors are 
defined and illustrated numerically. A number of special-purpose vectors, such as the null 
vector, unit vector, and sign vector, are also described. 
Matrices are then introduced and discussed from the same kind of viewpoint. Also, we 
describe various kinds of special matrices, such as symmetric, diagonal, scalar, and 
identity matrices and illustrate their appHcation via small numerical examples. 
The determinant of a matrix plays an important role in more advanced topics, such as 
matrix inversion, rank, and quadratic forms, that are introduced in later chapters. For this 
reason it seems appropriate to discuss determinants and some of their numerical 
properties at an early stage, and we do so in this section of the chapter. 
We conclude the chapter with a discussion of certain matrices of particular interest to 
multivariate analysis, namely mean-corrected sums of squares and cross product (SSCP) 
matrices, covariance matrices, and correlation matrices. Computation of these major 
types of statistical matrices is carried out as a demonstration of how concise the matrix 
formulation of various arithmetic operations can be. 
A word on notation: boldface lowercase letters, a, b, c, etc., will be used to denote 
vectors and boldface capitals. A, B, C, etc., will be used for matrices. The determinant of 
a matrix A will be expressed as |A|. A prime, for example, d! or A', will denote the 
transpose of a vector or matrix, respectively. (The concept of transpose is taken up later 
in the chapter.) 
The material of this chapter is presented rather crisply since our purpose here is to 
provide only the mechanics of vector and matrix operations before introducing the more 
conceptually oriented material of later chapters. However, sufficient numerical examples 
are presented to illustrate the computational aspects in some detail. 
26 

2.3. 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
27 
2.2 
VECTOR REPRESENTATION 
Multivariate analysis makes liberal use of vector concepts from linear algebra. Vectors 
can be defined in four major ways: (a) as strictly abstract entities on which certain 
relations and operations are specified, (b) as directed line segments in a geometric space, 
(c) as coordinate representations of points in a geometric space, or (d) as ordered ^-tuples 
of numbers. We adopt the last viewpoint in this chapter in order to demonstrate the kinds 
of operations that can be performed on vectors. The geometric representations of (b) and 
(c), which can be illustrated graphically if two or three dimensions are involved, are 
discussed in Chapters 3-5. 
2.3 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
A vector a of order n x 1 is an ordered set of n real ^ numbers (called scalars), which 
we can write as 
ai 
The a\ denote real numbers and are called components, elements, or entries of a. The 
form above is called a column vector and consists of n rows and 1 column of elements 
(from which the designation « x 1 derives). Alternatively we can write a vectora' of order 
1 X n as 
a =(ai,fl2,. . . ,««) 
and call this a row vector, consisting of 1 row and n columns of elements. 
We shall use the notation a to denote a column vector and the notation a', which is 
called the transpose of a, to denote a row vector. By vector transpose, generally, is meant 
that a column vector of order n by 1 becomes a row vector, involving the same ordered 
set of entries, but now of order 1 by n. Similarly, the transpose of a row vector of 1 by « 
is a column vector, involving the same ordered set of entries, but now of order ^ by 1. 
Examples of column vectors are 
3 
1 
J 
"4 
2.6 
5 
-0 . 
; 
2.718 
5 
1 
3 
"18" 
21 
0 
_ 0. 
; 
X = 
Xi 
X2 
^ 3 
Examples of row vectors are 
(3,1); 
(18,42,6); (V^, 13,0,5.2); 
(0,0,2,7); \! = {t,,t^) 
* Throughout the book we shall always assume that the scalars are drawn from the set of real (as 
opposed to complex) numbers. 

28 
2. VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
We can transpose the 3 x 1 column vector 
^l" 
2 
to get the 1 X 3 row vector 
a =(1,2,3) 
Similarly, we can then find the transpose of the 1 >^ 3 row vector a' = (1,2, 3) as follows: 
l" 
(ay = a = 
and note that we are back where we started, that is, where a is a 3 x 1 column vector. 
2.3.1 
Null, Unit, Sign, and Zero-One Vectors 
If all components of a vector are zero, we shall call this a null or zero vector, denoted 
as 0. This should not be confused with the scalar 0. If all components of a vector are 1, 
this type of vector is called a unit vector, denoted as 1. If the components consist of 
either I's or — I's (with at least one of each type present), this is called a sign vector. If 
the components consist of either I's or O's (with at least one of each type present), this is 
called a zero-one vector. To illustrate: 
Column vectors Row vectors 
Null vectors 
Unit vectors 
r 
Sign vectors 
L 
Zero-one vectors 
ro'' 
0 
Lo-
rr 
Ll. 
- 1 
1 
- 1 . 
0" 
1 
_1_ 
(0,0,0,0) 
(1,1,1) 
(1,-1) 
(0,0,1,1,0) 
As will be shown in subsequent chapters, the zero vector frequently plays a role that is 
analogous to the scalar 0 in ordinary arithmetic. The unit vector is useful in certain kinds 
of summations, as is illustrated in Section 2.8. Sign and zero-one vectors are also useful 

2.3. 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
29 
in various kinds of operations involving either algebraic sums or the isolation of rows, 
columns, or elements of an array of numbers. 
2.3.2 
Vector Equality 
Two vectors of the same order (either both n x 1 or both 1 xn) are equal if they are 
equal component 
by component. 
Let 
Then 
a = 
and 
b = 
bi 
b2 
For example. 
But 
1 
a = b 
if and only if 
ai = bi 
( / = 1 , 2 , . . •,n)\ 
'3' 
0 
_4_ 
= b = 
• 3 " 
0 
A-
i ^ C = 
i ^ e = 
•3 
0 
2 
3 
4 
LO 
2i^d = 
a ^ a ' = (3,0,4) 
In the last case a and a' are not of the same order, since the first is a column vector and 
the second is a row vector. 
Throughout most of this chapter, we shall present definitions in terms of column 
vectors, although our remarks will also hold true for row vectors.^ Moreover, in discussing 
various operations on vectors, it will be assumed, unless otherwise specified, that the 
vectors are of common order—either all are « x 1 or all are I xn. 
^ When row vectors are employed as numerical examples, emphasis is primarily on conserving 
space. The reader should remember that we could just as appropriately describe the operations in 
terms of column vectors. 

30 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
2.3.3 
Vector Addition and Subtraction 
Two or more vectors of the same order can he added by adding correspondent 
components. That is 
a + b = 
a^ -^b„ 
Examples are 
b = 
a + b = 
c =(1,3); 
d' = (4,13); 
4 | 
2 
_23_ 
c' + d' = (5,16) 
But we cannot add 
4 
2 
13 
to 
f = 
or 
g = 
to 
h' = (6,3,0,2) 
since in each case the order differs. 
The difference between two vectors a and b, of the same order, is defined to be that 
vector, a - b, which, when added to b, yields the vector a. Again, subtraction is 
performed componentwise.^ That is, 
a - b = 
a\-b\ ~ 
a2 — b2 
_an-bn_ 
Examples are 
b = 
c' = (l,3); 
d' = (4,13); 
- 2 
a - b = 
2 
L ^_ 
c ' - d ' = (-3,-10) 
^ A more rigorous presentation would first define multiplication of a vector by a scalar 
(specifically multiplication by -1), followed by vector addition. Here, however, we follow the more 
natural presentation order of traditional arithmetic in which subtraction follows addition. 

2.3. 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
31 
But we cannot subtract 
e = 
4 
2 
13 
from 
f -
or 
g = 
from 
h'= (6,3,0,2) 
since in each case the order differs. 
The operation of vector addition—of either column or row vectors-possesses the 
following properties: 
1. The sum of two vectors a and b is a unique third vector c. 
Let 
a = 
Then 
a + b = c = 
and 
3 
7 
b = 
IS unique. 
2. 
Vector addition is commutative. 
a + b 
b + 
T 
_3_ 
+ 
"21 
4^ 
= 
~2" 
A^ 
+ 
1 
_3_ 
= 
3. 
Vector addition is associative. 
(a + b) + d = a + (b + d) 
Let 
d = 
Then 
a + b 
b + d 
"3" 
L7_ 
+ 
"3" 
_5_ 
= [1] + 
_3J 
'5" 
_9_ 
= 
" 6" 
.12-
4. 
There exists a null or zero vector 0 having the property a + 0 = a for any vector a. 
3 
1 
_ 3 j + 
0" 
_0j 
= 
5. 
Each vector a has a counterpart negative vector —a so that a + —a = 0. 
= 0 
We shall have occasion to refer to one or more of these properties quite frequently in 
subsequent dicussions. 
'V 
_3_ 
r-1 
+ L-3_ 
= or 
_oj 

32 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
2.3.4 
Scalar Multiplication of a Vector 
Assume we have some real number k. As pointed out earlier, this is called a scalar in 
vector algebra. Scalar multiplication of a vector involves multiplying each component of 
the vector by the scalar. 
\k2i = k 
di 
an 
an 
= 
'kax' 
ka2 
_kan_ 
To illustrate the scalar multiplication of vectors, assume 
r 
a = 
and 
k = 3 
Then 
1 
2 
^3_ 
= 
'3x 1 
3 x 2 
3 x 3 
~3^ 
6 
9 
/:a = 3 
Next, let b' = (4, 5, 6). Then 
A:b' = 3(4,5,6) = (3 X 4,3 X 5,3 X 6) = (12,15,18) 
As in the case for vector addition, scalar multipHcation of vectors exhibits a number of 
useful properties."* 
1. If a is a vector and A: is a scalar, the product /:a is a uniquely defined vector. 
Let 
Then 
a = 
2 
rr 
_3 
rii 
L3j 
[21 
= 
L6j 
and 
k = 2 
is unique. 
2. 
Scalar multiplication is associative. For example, for two scalars ki and k2, it is 
the case that ki(k22[) = {kik2)^. 
k2^ 
a 
Let 
ki = 2 
and 
k2 = 3. 
Then 
"3 
L9_ 
= 6 
"1 
L3_ 
__ " 6' 
Lisj 
"* These properties and the properties listed in Section 2.3.3 collectively define a vector space for 
all vectors a, b, c, etc., and all scalars (real numbers) k^, k^y etc. 

2.3. 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
33 
3. 
Scalar multiplication is distributive. For example, k(a + b) = ka-^kb. 
Also, 
(ki + A:2)a = kin + A:2a. 
a + b 
a 
b 
Let 
b = 
Then 
3 
L7J 
= 2 
1 
_3j 
+ 2 
2 
-4] 
— ' 6' 
_14j 
1 
3 
= 2 
1 
3 
+ 3 
1 
3 
_ 
5 
15 
Also 
4. 
For any vector a, we have the products 0-a = 0; la = a and —la = —a. For 
example, 
rr 
L3j 
= roi 
LoJ 
; 
1 
ri 
L3J rr 
= 
L3j 
; 
- 1 
[11 
j \ 
= [-11 
_-3j 
We can now consider an operation that generalizes both vector addition and scalar 
multiplication of vectors. 
2.3.5 
Linear Combinations of Vectors 
Most of our comments about vector addition and scalar multipHcation in Sections 
2.3.3 and 2.3.4 can be succinctly summarized in terms of the concept of a hnear 
combination of a set of vectors. Let ai, aa,. . . , a^ denote a set of m vectors (each of 
order n x 1) and letA:i, ^2» • • •, /^y„ denote a set of m scalars.^l linear combination of a set 
of vectors is defined as 
Vi = ^iai + A:2a2 + 
"*' ^m ^m 
If we take another (arbitrary) linear combination involving another set of m scalars 
^ l * j ^ 2 
'^m*»wehave 
V2 = ^i*ai + A:2*a2 + 
•*" ^ m 
^m 
Next, suppose we add the two linear combinations. If so, it will be the case that the 
following properties hold: 
1. vi + V2 = (/:i + /:i*)ai + (^2 + A:2*)a2 + * * * + (A:^ + A:^*)a^ 
2. 
Moreover, if c denotes still another scalar, then cvi = (c^i)ai +{ck2)2i2 + • • • 
yyi is also a linear combination of ai, a2,. . . , 2irn • 
What this means is that linear combinations of vectors can be added together (such as 
Vi + V2) or can be multiplied by a scalar (such as cvi), resulting in new vectors that bear 
simple relationships to the old. 

34 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
To illustrate, let 
Ki ~ i.j 
K2 ~ -^j 
ai = 
k3 
~l' 
2 
= 1 
; 
a2 = 
it,* = 
' 0 ' 
3 
_2_ 
= 0; 
; 
aj = 
k2* = 
T 
4 
_2_ 
4; 
fcs* 
c = 2 
Tlien, v^e can first write Vi and V2 as 
Vi =ki2Li + ^232 + ^333 
= 2 
T 
2 
3 
+ 3 
"0" 
3 
2 
+ 1 r 
4 
2 
= 
" 3 
17 
14 
V2 = /:i*ai + A:2*a2 + A:3*a3 
= 0 
~r 
2 
3 
+ 4 
rol 
3 
2 
+ 5 
" f 
4 
2 
= 
' 5' 
32 
18 
The first property above can now be illustrated as 
Vi + V2 
3 
17 
14 
+ 
5 
32 
18 
= 2 
1 
2 
3 
+ 7 
0 
3 
2 
+ 6 
1 
4 
2 
2 
4 
6 
+ 
0 
21 
14 
+ 
6 
24 
12 
= 
8 
49 
32 
The second property above can now be illustrated as 
CVi 
= 
e' 
34 
_28 
= 4 
1 
2 
3 
+ 6 
0 
3 
2 
+ 2 
1 
4 
2 
4 
8 
12 
+ 
0 
18 
12 
+ 
2 
8 
4 
= 
6 
34 
28 
The concept of a linear combination of a set of vectors is one of the most important 
aspects of vector algebra. We shall retum to this topic in the next chapter deaUng with the 
geometric aspects of vectors. 

2.3. 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
35 
2.3.6 
The Scalar Product of Two Vectors 
The last operation involving vectors to be discussed in Section 2.3 is that of the scalar 
product (sometimes called inner product, or dot product) of two vectors. As is well 
known, when we multiply two numbers (scalars) together, we obtain another element of 
the same kind, namely, a number that represents their product. However, in vector 
algebra, multipUcation of two vectors need not lead to a vector. For example, one way of 
multiplying two vectors (of the same order of course) yields a number rather than a 
vector. This number is called their scalar product. To illustrate the scalar product of two 
vectors, consider the column vectors 
and 
b = 
b2 
bn 
Their scalar product is defined as 
a'b = aibi + ^2^)2 + * * * + a^bj^ + • • * + a„Z?„ 
n 
= ^ 
(^kbk 
k = l 
Notice that the first vector is treated as a row vector and the second vector is treated as a 
column vector. However, either one can serve as the first (row) vector. To illustrate. 
a = 
T 
4 
0 
3 
; 
b = 
~o~ 
2 
7 
4j 
Hence 
a'b = b'a = (1 x 0) + (4 x 2) + (0 x 7) + (3 x 4) 
= (0 X 1) + (2 X 4) + (7 X 0) + (4 X 3) 
= 20 
We might now check to see if the associative and distributive laws are valid for scalar 
products. As it turns out, the associative law, illustrated by (a'b)c is not valid because 
the scalar product of a scalar, that results from a'b, and a vector c has not been defined. 
That is, the scalar product idea is limited to the product of a row and column vector. Of 
course, we earlier defined the operation of multiplying a vector by a scalar, but this is not 
a scalar product. 

36 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
However, the distributive laws for the scalar product, with respect to addition, are 
valid: 
a (b + c) = (a b) + (a'c) 
Also, 
(a + byc = (a'c) + (b'c) 
To illustrate the distributive laws, let. 
Then 
Also, 
r 
2 
_3_ 
b = 
~A~ 
0 
1 
c = 
"3~ 
2 
_2_ 
a'(b + c) = (a'b) + (a'c) 
(1,2,3) 
7~ 
2 
3_ 
2C 
= (1,2,3) 
) = 7 + 13 
'4" 
0 
1 
+ (1,2,3) 
3 
2 
_2 
(a + b)'c = (a'c) + (b'c) 
(5,2,4) 
3" 
2 
2 
27 
= (1,2,3) 
= 13+14 
^3" 
2 
2 
+ (4,0,1) 
3 
2 
2 
2.3.7 
Some Special Cases of the Scalar Product 
In the definition of scalar product given above, no requirement was made that a had to 
differ from b. That is, one can legitimately compute the scalar product of a vector with 
itself. To illustrate. 
If 
a = 
then 
a'a= 1+4 + 9 = 14 
Notice, then, that one obtains a sum of squares if one takes the scalar product of a vector 
with itself. And a'a > 0 unless, of course, a = 0. 

2.3. 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
Consider now the unit vector 1' = (1, 1, 1) and the vector 
f 
a = 
37 
Their scalar product is 
l'a=l +2 + 3 = 6 
Thus, the scalar product of the unit vector and a given vector results in the sum of the 
entries in the given vector. If the vector is a sign vector, then the algebraic sum is taken. 
For example. 
(-1,1,-1) 
= - 2 
Finally, consider the relationships d!{kh) = (ka)h = k(ah), w^here /: is a scalar. To 
demonstrate that these relations hold, let 
a = 
rr 
2 
3 
; 
b = 
4' 
0 
1^ 
k = 2 
Then 
a'(kh) = (A:a')b 
(1,2,3) 
(2,4,6) 
= 14 
Also 
(1,2,3) 
a'(A:b) = A:(a'b) 
8" 
0 
2(1,2,3) 
= 14 
We can sum up this part of the discussion by recapitulating the following properties of 
scalar products, as illustrated above: 
1. a'(b + c) = a'b + a'c 
2. (a + b)'c = a'c + b'c 
3. 
a'(/tb) = (A:a')b = A:(a'b) 

38 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
We shall have more to say about the utility of scalar product multiplication in the 
concluding section of the chapter, which deals with the computation of various matrices 
derived from statistical data. 
Finally, it should be mentioned that two types of vector-by-vector multipUcation are 
not defined in matrix algebra. That is, 
1. a row vector cannot be multiplied by a row vector; 
2. 
a column vector cannot be multipUed by a column vector. 
The remaining case—that of multiplying a column vector by a row vector— is covered in 
our discussion of matrices since in this instance their product is a matrix, not a scalar. 
2.3.8 
Some More Examples 
To help review the vector operations described in this section, consider the following: 
a =0,2,3); 
b^ 
b' = (0,2,5) 
A:i = 2; 
A:2 = 3 
We can now illustrate the following operations. 
Transpose of Vector We recall that the transpose of the 3 x 1 column vector 
f 
a=| 2 
^3 
is the 1x3 row vector, written as 
a =0,2,3) 
Moreover, were we to take the transpose of a', we would have 
rr 
(a')' = a = 2 
[3 
That is, the transpose of a transpose equals the original vector. Similarly, we can fmd 
((a'yy = a =0,2,3) 
Addition and Subtraction 
The sum and difference of a' and b', respectively, are 
simply 
a + b' = (1 + 0, 2 + 2, 3 + 5) = 0,4,8) 
a - b ' = 0 - 0 , 2 - 2 , 3 - 5 ) = (1,0,-2) 

2.3. 
BASIC DEFINITIONS AND OPERATIONS ON VECTORS 
39 
Scalar Multiplication of a Vector 
Some illustrations of scalar multiplication of a 
vector are 
A:ia'=(2x 1,2x2,2x3); 
= (2,4,6) 
Oa = 
Ox f 
0 x 2 
_0x3 
= 
~0 
0 
_0_ 
= 0 
= 6(1,2,3) 
A:2b = 3| 
= (6,12,18) 
Linear Combinations of Vectors Illustrations of linear combinations of vectors are 
V i = / : i a + /:2b 
; 
V2' = ^ib' —/:2a' 
~o' 
2 
_5_ 
= 
"0^ 
6 
_15_ 
= 2 rr 
2 
3 
+ 3 
"0" 
2 
5 
= 
' 2" 
10 
21 
_ 
-J 
= 2(0,2,5)-3(1,2,3) 
= (0,4,10)-(3,6,9) 
= (-3,-2,1) 
If c = 4, then 
CV2' =cfcib'-cA;2a' 
= 8(0,2,5)-12(1,2,3) 
= (0,16,40)-(12,24,36) 
= (-12,-8,4) 
The Scalar Product 
Some scalar products of interest are 
0 
a'b = (l,2,3) 
= (1 X 0) + (2 X 2) + (3 X 5) 
= 19 
b'b = (0,2,5) 
a'l =(1,2,3) 
= 6 
= 29; 
0'b = (0,0,0) 
a'(l+b) = (l,2,3) 
= 0 
1 
1 
1 
+ (1,2,3) 
0 
2 
5 
= 6+19 
= 25 
Additional examples appear at the end of the chapter. 

40 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
2.4 
MATRIX REPRESENTATION 
As in our introduction to vector arithmetic, our purpose here is to describe various 
operations involving matrices as they relate to subsequent discussion of multivariate 
procedures. Again, we attempt no definitive treatment of the topic but, rather, select 
those aspects of particular relevance to subsequent chapters. 
We first present a discussion of elementary relations and operations on matrices and 
then turn to a description of special types of matrices. More advanced topics in matrix 
algebra are relegated to subsequent chapters and the appendixes. 
2.5 
BASIC DEFINITIONS AND OPERATIONS 
ON MATRICES 
A matrix A of order m by n, and with general entry (atj), consists of a rectangular 
array of real numbers (scalars) arranged in m rows and n columns. 
an 
«12 
fl22 
^1/ 
^2/ 
ain 
an 
ai2 
\flij /mxn 
^m2 
• • • "m/ 
For example, a 4 x 5 matrix would be explicitly written, in brackets, as 
dw 
(1x2 
^13 
^14 
fllS 
^21 
^22 
^23 
^24 
^25 
^31 
^32 
^33 
^34 
^35 
J4I 
042 
^43 
^44 
^45 
where / = 1, 2, 3,4 and / = 1, 2, 3, 4, 5. As is the case for vectors, matrices will appear in 
boldfaced type, such as A, B, C, etc. 
A matrix can exhibit any relation between m, the number of rows, and n, the number 
of columns. For example, if either m > n or n > m, y/e have a rectangular matrix. (The 
former is often called a vertical matrix, while the latter is often called a horizontal 
matrix.) 
If m = n, the matrix is called square. To illustrate, 
bn 
bi2 
bi3 
B 3 x 3 
Z)2l 
^22 
^23 
^31 
^32 
^33 

2.5. 
BASIC DEFINITIONS AND OPERATIONS ON MATRICES 
41 
The set of elements on the diagonal, from upper left to lower right, 
{^11,^22,^33} 
is called the main or principal diagonal of the square matrix B. Square matrices occur 
quite frequently as derived matrices in multivariate analysis. For example, a correlation 
matrix, to be described later in the chapter, is a square matrix. 
In either the rectangular or square matrix case, the order or "dimensionality" specifies 
the number of rows and columns of the matrix. Sometimes this order is made expUcit in 
the form of subscripts: 
^ 2 x 3 
In other cases, the order is inferred from context, such as 
" 1 2 
7 
9 3" 
D = 
0 
4 
3 
1 1. 
While we note that no subscript appears on D, it is clear that this matrix is of order 2 x 5 . 
If m = 1, the matrix is equivalent to a row vector. lfn= 1, the matrix is equivalent to a 
column vector. If m = « = 1, we have a 1 x 1 matrix.^ 
A column vector, written as 
can now be viewed as an m by 1 matrix and a row vector a' = (ATI, ^^2, • - •, ^w) can now 
be viewed as a 1 by w matrix. 
As will be shown later, analogous to earlier discussion of vectors, various kinds of 
special matrices can be defined. For the moment, however, we define the matrix that is a 
generalization of the 0 vector. This matrix, called a null matrix, and denoted^^, consists 
of entries that are all zeros. 
Illustrations of null matrices of various orders are 
0 
0 0 
0 0 0. 
Notice that each of the above null matrices is made up entirely of 0 vectors and all entries 
are O's. 
; 
^ = 
"0 
0" 
_o o_ ; 
4> = 
0 
0 
0 
0 
0 
0 
0 
0 
^ It is often convenient to consider a scalar as a 1 x 1 matrix. 

42 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
2.5.1 
Matrix Transpose 
Consider the 2 by 3 matrix: 
- 1 
4 
3' 
0 
5 2 
Suppose we write the elements of each row of ^ as columns and obtain 
f-l 
O' 
A' = 
4 
5 
3 
2 
Note that this new matrix A' is a 3 by 2 matrix in which the entries of the first row of A' 
denote, in the same order, the first column of A. This is also true of the elements in the 
second and third rows of A' compared, respectively, to the second and third columns of 
A. 
The new matrix A' represents the transpose of the original matrix A. A transpose 
of 
m xn ^^ ^ matrix obtained from A by interchanging rows and columns so 
that 
To illustrate, 
If 
^m xn 
(f^ij)m xn 
\^ji)nxm 
A = 
1 
4 
2 
5 
3 
6 
then 
A = 
1 
2 3 
4 
5 6 
If 
B = 
1 4 
7 9 
3 
1 0 
2 
4 
2 
1 3 
then 
1 
3 
4 
4 
1 2 
7 
0 1 
9 
2 
3 
If 
"0 
0 
0 
0" 
0 
0 
then 
<t>' = 
0 0 0 
0 0 0. 
Next, we can use the operation of matrix transpose to describe a symmetric matrix. A 
square matrix A is called symmetric if 
A = (a,;) = A' = {aji) 

2.5. 
BASIC DEFINITIONS AND OPERATIONS ON MATRICES 
43 
That is, a symmetric matrix equals its transpose. To illustrate, 
A = 
Finally, it should also be evident that the transpose of the transpose of a given matrix is 
the original matrix itself. That is, 
1 
2 
T 
3 
; 
A' = 
"1 
2 
L2 
3 
To illustrate, 
A = 
1 
5 
2 
6 
3 7 
(A')' = A 
1 
2 
3 
.5 6 7. 
(A')' 
1 5 
2 
6 
3 7 
2.5.2 
Matrix Equality, Addition, Scalar Multiplication, 
and Subtraction 
Tvjo matrices A and B are equal if and only if they are of the same order and each 
entry of the first is equal to the corresponding entry of the second. That is, 
for 
A = B 
if and only if 
{an) = (Pij) 
i= 1,2,... 
,m; 
/ = 1, 2 , . . .,n\ 
To illustrate, 
A = 
1 
4 
3" 
0 
5 
2j 
= B = 
-1 
4 
3 
0 
5 2. 
since they are of the same order and aij= bip entry by entry. 
In matrix addition each entry of a sum matrix is the sum of the corresponding entries 
of the two matrices being added, again assuming they are of the same order. 
That is, we define the matrix C, denoting the result of adding A to B as 
for 
C = A + B 
if and only if 
(Q/) = fe/) + (M 
/= 1,2,... ,m; 7= 1,2, 
,« 

44 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
To illustrate, 
A = 
- 1 
0 
4 
5 
B 
0 
--1 
2 
1 
4 3. 
A + B = C = 
- 1 
L-1 
6 
9 
Next, we can consider the case of the transpose of the sum of two matrices. 
// A and B are of common order and z/C = A + B, then 
C' = A' + B' 
To illustrate. 
2 
5 
3 
6 
B = 
C' = 
9 
6 
9 
= 
A' 
1 
4 
2 
5 
3 
6^ 
+ 
B' 
0 
5 
1 1 
1 3 
Matrices can also be multiplied by a number (scalar), and this is called scalar 
multiplication of the matrix. The procedure is simple: One merely multiplies each entry 
of the matrix by the scalar k. That is. 
for 
E = A:A 
if and only if 
{eij) = kiatj) 
/= 1,2,. .. ,m; /= 1,2,. .,n 
For example, if we wish to multiply A by 3, we have 
3A = 3 
1 
2 
3 
4 
5 6 
3 
12 
6 
15 
9 
18 
Subtraction of matrices is now defined as involving the case in which the matrix being 
subtracted is first multiplied by -1 and then the two matrices are added. That is 
for 
C = A-B 
if and only if 
(Cif) = 
(aij)-{bij) 
i= 1,2,. .. ,mi 
/•= 1,2,.. . ,n 
To illustrate. 
A = 
1 4 
3 
0 
5 
2j 
; 
B = 
0 
2 
1 
- 1 4 
3 
A - B = C = 
- 1 2 
2 
1 
1 
- 1 

2.5. 
BASIC DEFINITIONS AND OPERATIONS ON MATRICES 
45 
2.5.3 
Properties of Matrix Addition and 
Scalar Multiplication 
Some of the properties exhibited by matrix addition^ and scalar multiplication are 
hsted below for future use: 
1. Matrix addition is commutative: 
A + B = B + A 
2. 
Matrix addition is associative: 
A + (B + C) = A + B + C = (A + B) + C 
3. 
Scalar multiplication of a matrix is commutative: 
A(A:B) = (A:A)B 
4. 
Scalar multiplication is associative: 
k^{k2X) = {kik2)X 
5. 
Scalar multipHcation is distributive: 
(^i + A:2)A = A:iA + ^2A 
6. 
There exists a null matrix (already defined) <j) with the property that 
A + ^ = A 
7. 
Every matrix A has a counterpart matrix —A such that 
A + (-A)=0 
Not surprisingly, the preceding rules are similar to the ones discussed for vector 
operations in Sections 2.3.3 and 2.3.4 and could be numerically illustrated in similar 
fashion. 
2.5.4 
Matrix Multiplication 
In discussing the multipHcation of two (or more) matrices, conformability should first 
be pointed out. Similar to the previous discussion of matrix equality, addition, and 
subtraction, in which the matrices were assumed to be of common order before the 
relation or operation was meaningful, in multiplication the matrices must be con-
formable. If we wish to multiply A by B, they must exhibit commonality of interior 
dimensions. 
For example, if A is of order m rows by n columns, it can be written as A^ xw • Next, 
suppose we have a second matrix B of order n rows by p columns, written as B„ xp • Using 
this form, we have 
^m xn 
"nxp 
^m xp 
* It should be noted, however, that matrix subtraction is neither commutative nor associative. 

46 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
Note that A, called the prefactor, has n columns. This is the "interior" dimension. If A 
has an interior dimension of n columns, then B, called the postfactor, must have an 
interior dimension of n rows. This is the condition of conformability and is necessary for 
matrix multiplication. Note that their product C, then, is of order m by p. These are the 
"exterior" dimensions of A and B, respectively. 
A simple way to find the order of the matrix product is shown below: 
*^fn xn B/1 xp 
, 
t 
= C m xp 
Note that the interior dimensions are the same (n columns of A and n rows of B) and that 
the exterior dimensions are obtained from the "outer" dimensions of A and B, 
respectively. 
This same idea holds for more than two matrices, again assuming that all are 
conformable. For example, with three matrices, we have 
^m xn 
t 
B n xp 
X 
•'P xr 
^m xr 
z=^ 
LJ 
Note further that the interior dimensions of the matrices conform. 
Matrix multiplication follows a row-by-column rule, equivalent to the scalar product 
(Section 2.3.6) of each row of the first matrix with each column of the second. That is, 
we take the entries of each row of the prefactor, and these are multiplied by the 
corresponding entries of each column of the postfactor and then summed. If we use the 
first row of A and the first column of B, then the first element in C (i.e., Cn) will be the 
result of the preceding operation. The second element of C (i.e., Ci2 ) is found by using 
the first row of A and the second column of B, and so on. 
This can be summarized as follows: 
for 
C = AB 
is defined as 
n 
k = \ 
/= 1,2,. .. ,m; 
7=1,2, 
To illustrate, we let 
^ 2 x 3 
-1 
3 2 
2 
0 
1 
and 
>3x2 
2 
3 
1 
4 
1 
2 

2.5. 
BASIC DEFINITIONS AND OPERATIONS ON MATRICES 
47 
Then 
C = AB 
'(-1 x2 + 3xl+2xl) (-1x3 + 3x4 + 2x2) 
(2x2 + 0x1 + 1x1) (2x3 + 0x4+1x2) J 
"3 13 
' 2 x 2 
Now, let us reverse the order of multiplication. In this case we can do so since B is of 
order 3 by 2 and A is of order 2 by 3. In other instances, however (e.g., if A were of order 
3 by 3), we could not multiply B by A since they would then not be conformable. 
If we now multiply BA = D, we obtain the following: 
D - B 3 x 2 
A 2 x 3 ~ 
2 
3 
1 
4 
1 
2 
- 1 3 
2 
2 
0 1 
^ 3 X 3 = 
4 
6 7 
7 
3 6 
3 
3 
4 
Notice that D 9^ C and, as a matter of fact, they are not even of the same order. Even if 
two matrices are conformable, in general, AB^ BA. That is, matrix multipHcation, in 
general, is noncommutative. Hence, in discussing matrix multipHcation we should refer 
explicitly to the order in which they multiply. For example, the matrix product AB can 
be described as "A is postmultiplied by B," or "B is premultipHed by A." Altematively, 
we could use the terms "prefactor" and "postfactor" as mentioned earlier. 
2.5,4.1 
Multiplication of a Vector and a Matrix In some cases of interest we shall 
want to postmultiply some vector a by a matrix B. To illustrate, suppose we have the 
following: 
a =(1,0,3); 
B = 
2 
3 
1 
2 
0 
1 
where a' is a row vector of order 1 by 3. Note that a' and B are conformable, and we have 
(1,0,3) 
2 
3 
1 
2 
0 
1 
(2,6) 

48 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
where their product displays the order of the exterior dimensions of a' and B, namely, a 
1x2 row vector. Alternatively, suppose we have the column vector c = [2]- Then we can 
find the product B as 
"2 
3] 
1 2 
_0 
1 J 
rr 
L2_ 
= 
~8~ 
5 
2 
where their product is a 3 x 1 column vector. As can be seen, no new rules are involved 
for either of these cases. 
2.5,4,2 
Matrix Product of Two Vectors We now might ask what happens when two 
vectors are multiplied. As shown in Section 2.3.6, the scalar product of two vectors 
results in a single number (scalar) if row vector times column vector multipUcation is 
performed. However, one might have the case of a column vector multiplying a row 
vector. In this case the results are quite different, and we consider it next. 
To illustrate, assume we have the column vector 
'2 
1 
and the row vector (1, 1,2). Their matrix (or outer) product is obtained as 
'2 
1 
.3 J 
(1,1,2) = 
(2x1) 
(2x1) 
(2x2) 
(1x1) 
(1x1) 
(1x2) 
(3x1) 
(3x1) 
(3x2) 
= 
2 
2 4 
1 
1 2 
3 
3 6 
which is a 3 by 3 matrix. Note in this case that each row of the first "matrix" has only a 
single element, as does each column of the second. Thus, the row-by-column rule is not 
violated in this special case. 
2.5.4.3 
Triple Product -Vector, Matrix, Vector Multiplication 
To round out the 
discussion we might wish to consider the triple product of 
a =(1,1,2); 
That is, we desire to find the product a'Bc. If so, we can proceed in stages. We first find 
the vector by matrix product: 
1 
2 3" 
0 
1 2 
3 
0 1 
c = 
'2 
1 
3 
aB = (1,1,2) 
1 
2 
3 
0 
1 2 
3 
0 1 
= (7,3,7) 

2.5. 
BASIC DEFINITIONS AND OPERATIONS ON MATRICES 
49 
Next, we find the scalar product: 
a Be = (7,3,7) 
= 38 
Hence, we see that the result of all this is a scalar. Note, of course, that it is of "order 
1 X 1" and, thus, is in agreement with the order of the exterior dimensions of a' and c. 
A special case of vector, matrix, vector multipHcation takes the form of a'Ba. This can 
be illustrated by 
a =(1,1,2); 
3~ 
2 
1 
; 
a = 
"r 
1 
_2_ 
Then 
a'B = (l,l,2) 
2 
1 
0 
= (7,3,7) 
and 
a'Ba = (7,3,7) 
= 24 
As noted, the result is also a scalar. Both of these cases are relevant to multivariate 
analysis and are discussed in later chapters. 
2.5.5 
Some Properties of Matrix Multiplication 
We have already pointed out that matrix multipHcation, in general, is noncommu-
tative. However, matrix multiplication does obey certain other properties. 
1. Associativity—assuming all matrices to be conformable we can state that 
(AB)C = A(BC) 
2. 
Distributivity—again assuming conformable matrices we can state that 
A(B + C) = AB + AC; 
left distributive law 
(B + C)A = BA + CA; 
right distributive law 
3. 
If/: is a scalar, then we have the associativity property: 
A:(AB) = (A:A)B 

50 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
Finally, it is of interest to point out the rule involving the transpose of the product of 
two (or more) matrices. In the case of two matrices, the rule is 
(AB)' = B'A' 
That is, the transpose of the product of two (or more) matrices is equal to the product of 
their respective transposes, multiplied in reverse order. To illustrate. 
Then 
A = 
1 
3 
L2 4J 
; 
B = 
1 3 
_2 
l\ 
1 
2 
3 
4. 
B' 
AB = 
(AB)' = 
1 
L3 
"7 
10" 
_9 14_ 
Fl 3" 
_2 2_ 
= B'A' 
1 
3 
' 7 
_10 
21 
2j 
9 
14_ 
[1 
13 
2 
4_ 
__ 7 
10 
_9 
14_ 
2.5.6 
Some Differences between Scalar and Matrix Arithmetic 
Probably the most difficult temptation to suppress in working with matrix 
multiplication involves attributing properties to matrices that we associate with ordinary 
scalars. Table 2.1 shows some of the pitfalls that one should be wary of in doing 
TABLE 2.1 
Some Differences between Scalar and Matrix Relations 
Scalars 
Matrices 
1. 
ab - ba 
2. 
If ab = ac and c ¥= 0, then b - c. 
3. 
If ab = 0, then either a = 0, or 6 = 0, or 
both fl, Z? = 0. 
4. 
Ifflft = 0, then6fl = 0. 
1. 
AB =7^ BA, in general 
2. 
If AB = AC and A =?^ 0, then it is not necessary 
that B equals C. 
3. 
If AB = 0, then it is not necessarily the case 
that either A = 0, B = 0, or both A, B = 0. 
4. 
If AB = 0, then BA does not necessarily 
equal 0. 
arithmetic with matrices. As shown in the table, some marked differences exist. Not only 
does commutativity fail to hold in general for matrix multipHcation, but other 
characteristics involving products equal to zero also do not hold generally. For example, 
if some matrix product AB equals the null matrix 0, we note that neither A, the 
prefactor, nor B, the postfactor, need to be equal to ^. 

2.5. 
BASIC DEFINITIONS AND OPERATIONS ON MATRICES 
51 
2.5.7 
The Problem of Matrix Division 
Up to this point we have discussed addition, subtraction, and multiplication of 
matrices, but division has been conspicuous by its absence. And for good reason: division, 
as we know it in scalar arithmetic, is not defined in matrix algebra. 
What is defined is something more analogous to multiplication by a reciprocal. For 
example, in ordinary arithmetic, instead of dividing some number by 5, we could 
multiply the number by the reciprocal of 5: 
1/5 = (5)-' 
assuming that the divisor is not equal to zero. 
The analogous operation in matrix algebra is called matrix inversion. This operation is 
so special (and considerably more complex) that we defer discussion of it until Chapter 4. 
What can be said for now is that the inverse of a matrix A, if said inverse exists, is 
analogous to multipHcation of A by a reciprocal in ordinary algebra. As such, in matrix 
algebra there is an analogy to the scalar relation: 
axa 
1 
Needless to say, we shall spend a considerable amount of time on the topic of matrix 
inversion in subsequent chapters. 
2.5.8 
Some More Examples of Matrix Operations 
To facilitate the review of matrix operations described in Section 2.5, consider the 
following: 
"2 
4' 
"1 2 3~ 
A = 2 
1 4 
B = 2 
5 
L3 
6_ 
5 
2 
k = 3', 
a = ( 2 , l ) ; 
b = 
Matrix Transpose The transpose of A is 
A = 
"1 2 
2 1 
3 
4 
and the transpose of B is 
B' = 
"2 2 3' 
4 
5 6 

52 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
Addition of Matrices Addition of matrices is illustrated by 
C = A + B' = 
1 
2 3" 
2 
1 4_ 
+ 
2 
2 3" 
4 
5 6 
"3 4 
6 
6 
6 
10 
Subtraction of Matrices Matrix subtraction is illustrated by 
C = A - B 
" 1 2 
3 
2 
1 4 
2 
2 3" 
4 
5 6 
-1 
0 
0 
-2 
- 4 
- 2 
Scalar Multiplication of a Matrix 
C = kA = 3 
Varieties of Multiplication 
1 
2 
3 
.2 
1 4_ 
C = AB = 
C' = (AB)' = B'A' = 
D = A:AB = 3 
1 
2 3" 
2 
1 4_ 
"2 2 3' 
4 
5 6 
"15 32' 
18 37_ 
E = a'Ab = (2,l) 
1 2 
3 
2 
1 4 
"3 6 
9 
.6 
3 
12 
2 
4 
2 
5 
3 
6J 
"1 2 
2 1 
3 
4 
45 
96 
54 
l l l j 
^5 
2 
3 
ri5 
Ll8 
"15 
_32 
32 
37 
18 
37 
= 60 
F = b'B(a'y = (5,2,3) 
2 
4' 
2 
5 
3 
6 
= 94 
Additional examples appear at the end of the chapter. 
2.6 
SOME SPECIAL MATRICES 
So far we have been dealing mainly with arbitrary rectangular matrices. In a few cases 
we have used square matrices for illustrative purposes. In matrix algebra there are a 
number of special matrices that are encountered in multivariate analysis. We consider 

2.6. 
SOME SPECIAL MATRICES 
53 
some of these here, particularly those that are frequently utilized in multivariate 
procedures. 
2.6.1 
Symmetric Matrices 
Figure 2.1 shows, in schematic form, various special matrices of interest to 
multivariate analysis. The first property for categorizing types of matrices concerns 
\\iiether they are square (m = n) or rectangular. In turn, rectangular matrices can be either 
vertical (m > n) or horizontal (m < n). 
As we shall show in later chapters, square matrices play an important role in 
multivariate analysis. In particular, the notion of matrix symmetry is important. EarHer, a 
symmetric matrix was defined as a square matrix that satisfies the relation 
A = A' 
or,equivalently, 
(«//) = (^/i) 
TTiat is, a symmetric matrix is a square matrix that is equal to its transpose. For example, 
A = 
3 
2 
4 
2 
0 
- 5 
4 
- 5 
1 
; 
A' = 
3 
2 
4 
2 
0 
- 5 
4 
- 5 
1 
Square 
Rectangular 
(m = n) 
(mi^n) 
Vertical Horizontal 
(m>n) 
(m<n) 
Symmetric 
Nonsymmetric 
Diagonal 
Other 
Scalar 
Sign 
Other 
Identity 
Other 
Fig. 2.1 
Various types of matrices. 

54 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
Symmetric matrices, such as correlation matrices and covariance matrices, are quite 
common in multivariate analysis, and we shall come across them repeatedly in later 
chapters.^ 
A few properties related to symmetry in matrices are of interest to point out: 
1. The product of any (not necessarily symmetric) matrix and its transpose is 
symmetric; that is, both A A' and A'A are symmetric matrices. 
2. 
If A is any square (not necessarily symmetric) matrix, then A + A' is symmetric. 
3. 
If A is symmetric and /: is a scalar, then kA is a symmetric matrix. 
4. 
The sum of any number of symmetric matrices is also symmetric. 
5. 
The product of two symmetric matrices is not necessarily symmetric. 
Later chapters will discuss still other characteristics of symmetric matrices and the special 
role that they play in such topics as matrix eigenstructures and quadratic forms. 
2 
0 
0 
0 
3 
0 
0 
0 
1_ 
; 
B = 
- 1 
0 
0 
0 
0 0 
0 
0 3_ 
; 
c = 
1 
0 
_0 5_ 
2.6.2 
Diagonal, Scalar, Sign, and Identity Matrices 
A special case of a symmetric matrix is a diagonal matrix. A diagonal matrix is defined 
as a square matrix in which all off diagonal entries are zero, (Note that a diagonal matrix 
is necessarily symmetric.) Entries on the main diagonal may or may not be zero. 
Examples of diagonal matrices are 
A = 
If all entries on the main diagonal are equal scalars, then the diagonal matrix is called a 
scalar matrix. 
Examples of scalar matrices are 
3 
0 O' 
A = | 0 
3 
0 
_0 
0 3_ 
If some of the entries on the main diagonal are - 1 and the rest are +1, the diagonal 
matrix is called a sign matrix. Examples of sign matrices are 
-1 
0 
0^ 
B = 
2 
0 
_0 2_ ; 
c = 
'-4 
0 
0 
-4_ 
A = 
0 
1 0 
0 
0 
1 
B = 
'\ 
_0 
0 
- 1 ^ ; 
c = 
- 1 
O" 
0 
1_ 
"^ While we do not go into detail here, a skew symmetric matrix A is a square matrix in which all 
main diagonal elements an are zero and/I = -A'. For example, 
If A = 
0 
- 3 
1' 
3 
0 
2 
1 
- 2 
0. 
then 
— A' = 
0 
- 3 
- 1 
3 
0 
2 
1 
- 2 
0. 
is skew symmetric. 

2.6. 
SOME SPECIAL MATRICES 
55 
If the entries on the diagonal of a scalar matrix are each equal to unity, then this type of 
scalar matrix is called an identity matrix, denoted I. Examples are 
"l 
0 
0' 
0 
1 0 
0 
0 
1 
; 
1 = 
"1 
0 
_o i_ 
A number of useful properties are associated with diagonal matrices and, hence, the 
special cases of scalar, sign, and identity matrices. 
1. The transpose of a diagonal matrix is equal to the original matrix. 
2. 
Sums and differences of diagonal matrices are also diagonal matrices. 
3. 
PremultipUcation of a matrix A by a diagonal matrix D results in a matrix in 
which each entry in a given row is the product of the original entry in A corresponding to 
that row and the diagonal element in the corresponding row of the diagonal matrix. To 
illustrate, 
D 
fs 0 o1 
0 
2 0 
^0 0 ij 
A 
fl 
2 
3 4^ 
2 
1 4 
3 
[3 
2 
1 1_ 
= 
DA 
"3 
6 
9 
4 
2 
8 
_3 2 1 
12" 
6 
1 
4. 
Postmultiplication of a matrix A by a diagonal matrix D results in a matrix in 
which each entry in a given column is the product of the original entry in A 
corresponding to that column and the diagonal element in the corresponding column of 
the diagonal matrix. To illustrate, 
AD 
3 
4 3" 
6 
2 
2 
9 
8 1 
12 
6 1 
A 
" l 
2 
3l 
2 
1 2 
3 4 1 
4 3 
ij 
D 
[3 
0 Ol 
0 
2 0 
0 0 1_ 
= 
5. 
Pre- and postmultiplication of a matrix A by diagonal matrices Di and D2 result 
in a matrix whose z/th entry is the product of the /th entry in the diagonal of the 
premultipHer, the z/th entry of A, and the /th entry of the postmultiplier. For example, 
Di 
'2 
0 0~] 
0 
1 0 
LO 
0 3J 
A 
n 
31 
2 2 
[4 il 
D j 
ri 0' 
Lo 3J 
— 
D1AD2 
" 2 
18 
2 
6 
12 
9 

56 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
6. 
The product of any number of diagonal matrices is a diagonal matrix, each of 
whose entries is the product of the corresponding diagonal entries of the matrices. For 
example, 
Di 
D2 
D3 
D1D2D3 
1 o"i \i 01 r4 on fs 0 
0 3j Lo 2JL0 - i j 
Lo -( 
7. 
Diagonal matrix multiplication, assuming conformability, is commutative. 
8. 
Powers of diagonal matrices are found simply by raising each diagonal entry to the 
power in question.* (Roots are found analogously.) 
9. 
Pre- or postmultipHcation of a matrix A by a scalar matrix multipUes all entries of 
A by the constant entry in the scalar matrix. It is equivalent to scalar multiplication of 
the matrix, by that scalar appearing on the diagonal. 
10. 
As a special case, pre- or postmultipHcation of a matrix A by I, the identity 
matrix, leaves the original matrix unchanged. 
11. 
Powers of an identity matrix equal the original matrix. 
While the above properties are by no means exhaustive of the characteristics of diagonal 
matrices, or the special cases of scalar, sign, and identity matrices, they do represent the 
main properties of interest to applied researchers. 
2.6.3 
Some Additional Examples 
As an aid to integrating some of the discussion of this section, consider the following: 
A = 
' 1 
4 
1 
2 
5 1 
3 
6 2 
; 
Di = 
["3 
0 
|_0 
Premultiplication by a Diagonal 
DiA = 
~3 
0 
0I 
0 
2 
0 
0 0 1J 
[1 
2 
[3 
PostmultipHcation by a Diagonal 
ADi = 
"1 4 
1I 
2 
5 1 
3 
6 
2] 
[3 
0 
[o 
0 
2 
0 
4 
5 
6 
0 
2 
0 
0 
0 
5 
.J 
1" 
1 
2 
= 
[3 
4 
_3 
0~ 
0 
1 
= 
"3 
6 
9 
D2 = 
"2 
0 
0" 
0 
2 
0 
0 
0 
2 
12 3~ 
10 2 
6 
2 
8 
1~ 
10 1 
12 
2_ 
* In general, a square matrix A can be raised to any power n that is a positive whole number by 
multiplying it by itself n times, denoted as A". Roots can also be found for certain square matrices 
(not restricted to being diagonal). If a square matrix A has an «th root, then the matrix A^/", when 
multiplied by itself n times, equals A. The topic of powers and roots of (square) matrices is covered in 
Chapter 5. 

2.6. 
SOME SPECIAL MATRICES 
57 
Pre- and Postmultiplication by Diagonals 
D2AD1 = 
Scalar Matrix Multiplication 
2 0 0I 
0 2 0 
_0 0 2J 
fl 
4 
ll 
2 5 1 
L3 6 2] 
[ 3 0 0 
0 2 0 
[o 0 1 
= 
~i 4 
r 
2 5 1 
3 6 2 
= 
"3 0 0I 
0 3 0 
0 0 3J 
fl 
4 
f 
2 5 1 
[3 6 2 
= 
6 
16 2 
12 20 
2 
18 24 
4 
3 
12 
3 
6 
15 
3 
9 
18 6 
Powers and Roots of a Diagonal with Positive Entries 
w = 
"3 0 0] 
0 2 0 
0 0 
i j 
[3 0 o~ 
0 2 0 
[0 0 1 
= 
"9 0 0" 
0 4 0 
0 0 Ij 
W^ = 
~2 0 0" 
0 2 0 
0 0 2 
1/2 
= 
1.414 
0 
0 
0 
1.414 
0 
0 
0 
1.414 
Identity and Sign Matrices 
lA = AI = 
Let 
Then 
"1 4 
1] 
2 5 1 
_3 6 2 1 
[1 0 0" 
0 
1 0 
[0 0 1 
= 
'\ 4 r 
2 5 1 
3 6 2_ 
AF = 
F = 
-1 
0 
0 
0 
0 
-1 
0 
0 
1 
6 2 
1 
0 
0 
0 0' 
-1 
0 
0 1 
= 
= A 
- 1 
- 4 
1 
- 2 
- 5 
1 
- 3 
- 6 
2 
Additional examples appear at the end of the chapter. 

58 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
2.7 
DETERMINANTS OF MATRICES 
The determinant of a matrix plays an important role in more advanced matrix 
concepts such as matrix inversion and matrix rank, as well as in multivariate analysis 
involving generalized measures of variance. Only square matrices have determinants. The 
determinant of a square matrix is a scalar function of the entries of the matrix. We denote 
the determinant of a matrix A by the symbol |A| and reiterate that the value of the 
determinant is expressed as a single number (scalar). 
The early development of determinants was intimately connected with procedures for 
solving simultaneous equations. As historical background, and motivational interest, 
consider the two linear equations: 
ax -^ by = c 
dx + ey=f 
These equations could be expressed in matrix times vector form as 
~a 
bl 
_d 
ej 1 "^ 
= 
c 
_/_ 
Note that the left-hand side of the equations is simply the product of a 2 x 2 matrix times 
a 2 X 1 vector while the right-hand side is another 2 x 1 vector. 
As may be recalled from elementary algebra, this system of equations can be solved, 
for, say, JC by the formula 
ce- fb 
X = ae-db 
assuming that the denominator of the above ratio is not equal to zero. 
We can consider the right-hand side of the above equation in the context of 
determinants by expressing both numerator and denominator of the ratio as 
X —-
c 
b\ 
f A 
a 
b\ 
d 
e 
In the simple case shown here, the determinants of [/ *] and [§ 
That is 
'] are easy to defme. 
= ce-fb 
and 
b\ -ae-db 
these are called second-order determinants. The unknown quantity x is the ratio of two 
determinants (scalars). 
Historically, determinants were employed widely in the solution of simultaneous 
equations. With the advent of newer solution methods, however, their application in this 
context has diminished. Still, it is important to have some grasp of the rudiments of 
determinants, if only as a precursor to other procedures for solving equations that are 
developed in subsequent chapters. 

2.7. 
DETERMINANTS OF MATRICES 
59 
2.7.1 
Operational Definition of a Determinant 
The theoretical definition of a determinant for matrices of larger order than 2 x 2 is 
rather cumbersome and, therefore, as in the 2 x 2 case, we shall define it operationally as 
a series of computational steps. Assuming a square matrix A of order mxm with general 
entry (aij), the determinant of that matrix is found by carrying out the following 
sequence: 
1. 
Form all possible products of m factors each, such that each factor is an entry of 
A and no two factors are drawn from the same row or column of A. There are ml (m 
factorial) products of this type. For example, if A is of order 3 x 3, we have 3! or six 
products: 
(i) 
an 
«22 
^33 
0 0 
«12 
^23 
^31 
(iii) ai3 a2i ^32 
(iv) ai3 a22 «3i 
(v) an 
^23 ^32 
(vi) an 
a2i ^33 
We note that each of the subscripts (1, 2, or 3) appears just once as a row subscript and 
just once as a column subscript in each of the six triple products. The connections shown 
below illustrate these six products. 
First three products 
Second three products 
2. 
Within each separate triple product arrange the factors so that row subscripts are 
in their natural order; this has been done above. Then count the number of inversions or 
transpositions involving column subscripts. In this case an inversion takes place when a 
larger column subscript precedes a smaller one. For the six triple products above, we have 
the following frequencies of inversions: 
(i) 
0 inversion 
(ii) 
2 inversions 
(iii) 
2 inversions 
(iv) 
3 inversions 
(v) 
1 inversion 
(vi) 
1 inversion 
For example in case (ii), involving the product ^12^23^315 we note that column subscripts 
1 and 2 need to be interchanged, followed by the interchange of column subscripts 3 and 
2, in order to obtain the natural order. 
3. 
Having done this for all ml products, multiply each product that has an odd 
number of inversions by —1. If zero or an even number of inversions is involved, multiply 
by +1; that is, leave the product as is. In the above case the first three products 
(associated with an incidence of even-type inversions of 0, 2, and 2) will not be changed 
in sign, while the last three products will. 
(0 
(iv) 
1(^11^22^33) 
1(^13^22^31) 
(ii) 
(v) 
1(^12^23^31) 
-l(^ll«23«32) 
(iii) 
1(^13^21^32) 
(vi) 
-1(^12^21^33) 

60 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
4. 
Add the products (observing sign) together. This sum is the determinant. 
I AI = +(^11^22^33) + (^12^^23^31) + (^13^21^^32) 
-(^13^22^31) - (^ll«23^32) - (flf 12^21^^33) 
5. 
Notice, then, that three steps are involved in finding a determinant. The first step 
is to form all possible products that can be obtained by taking one element out of one 
row and column, another out of another row and column, and so on. A matrix of order 
mxm 
will yield m\ such products, each composed of m elements. The second step is to 
affix an algebraic sign to each product via the rule proposed above. The third step is to 
sum the ml signed products. 
6. The procedure can now be formalized by defining the determinant of A^ x m ^s 
the sum of all ml products (each with m factors) in A of the form 
( - l ^ l / i f l 2 / 2 
'"^m/^ 
where the sum is understood to be taken over all permutations of the second subscripts. 
The exponent t denotes the number of inversions required to bring the second subscripts 
into their natural sequence ( 1 , 2 , . . . , m). 
Now let us illustrate the computation of determinants for two simple cases. 
The 2x2 Case 
A = 
^11 
an 
_fl21 
^22_ 
= 
~1 
3 
2 
4 
I A\=(~lfana22 
+ (-1)^^12^21 = ^11^22-^12^21 = (1 x 4)-(2 x 3) = - 2 
The3x3 
Case 
^11 
^12 
^13 I 
1 
2 
3 
A = 
fl21 
^22 
^23 p 
2 
—1 
4 
\_a31 
Ciyi 
^33 J 
|_2 
1 
1_ 
I A I = (-1)^^11^22^33 + (-1)^^12^23^31 + (-1)^^13^21^32 
+ (-I)^^13fl22flf31 + (-I)^flfllflf23flf32 
+ ("1)^^12^21^33 
= (1 x - 1 X l) + ( 2 x 4 x 2 ) + ( 3 x 2 x l ) - ( 3 x - l x 2 ) - ( l x 4 x l ) - ( 2 x 2 x 1) 
= - 1 +16 + 6 + 6 - 4 - 4 
JA|= 19 
2.7.2 
Expansion of Determinants by Cofactors 
Even on the basis of the step-by-step demonstration shown above, the evaluation of a 
determinant (i.e., the process of finding the numerical value of the determinant) is rather 
complicated and prone to error. Understandably, we might seek some easier procedure in 
which the arithmetic is simpler and the computations more straightforward. Expansion 
by cofactors is one such method. 

2.7. 
DETERMINANTS OF MATRICES 
61 
As shown above, a particularly simple determinant can be computed for the case of a 
2 x 2 matrix: 
~an 
(i\2^ 
In this case |A| = anUn - ai\a\i. 
We can take advantage of this simple 2 x 2 case in attempting to evaluate high-order 
determinants (i.e., those of matrices of order 3 x 3 and higher). 
To do this, we first define the minor of an entry (uij) of a square matrix A = (aij) as 
the determinant of a submatrix obtained by deleting the ith row and jth column of A. 
For example, the minor of the entry ^23 in the matrix. 
A = 
a\\ 
ai2 
^13 
«21 
^22 
^23 
^31 
^32 
^33 
is 
minor (^23) = an 
an 
^31 
^32 
Notice that this entails omitting those entries in the shaded area: 
\//A 
Similarly, we could find the minors of each of the other eight entries in A. 
The CO factor of an entry a^ of a square matrix A = {aij) is the product of the minor 
of (aij) and (-1)' "^ L The cofactor is also called a signed minor and is denoted by A^y. In 
the above case. 
^23=(-iy 
2+3 dn 
an 
^31 
^32 
[(^11^32)-(^31^12)] 
Notice that the placement of signs follows an alternating pattern: 
+ 
- 
+ 
_ 
+ 
-
+ 
- 
+ 
- 
+ 
-
- 
+ 
+ 
-

62 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
Similarly, we could develop a cofactor (or a signed minor) for each of the eight remaining 
entries in A. However, to evaluate |A|, it tums out that we need only develop cofactors 
for any single ro\^, or any single column, of the original matrix (rather than one each for 
all nine entries in, for example, the 3 x 3 matrix above). 
To see why this is so, we can rearrange the entries in |A| so that those in the second 
row appear first. 
I A | 
=^22(^11^33)+^23(^12^31)+^2l(^13^32)-^22(^13^3l)-^23(flll^32)-^2l(«12^33) 
We then simpUfy as follows: 
I A I = fl2i(«i3«32 - ^12^33) + ^22(^11^33 - ^13^31) + ^23(^12^31 - ^11^32) 
However, since the cofactor ^23 has already been defined as 
^ 1 2 I 
^23=(-iy 
2+3 
^31 
^32 
[(^11^32-^12^31)] =(^12^31 -^11^^32) 
we can substitute the cofactor for the last term on the right in the above expression for 
|A|. Similarly, we can substitute the other two cofactors: A^i and >l22 involving the 
second row of A. 
These are 
^ 2 1 = ( - 1 ) 
^ 2 2 = ( - ! ) • 
2 + 1 
2 + 2 
ai2 
^32 
^11 
«31 
^13 
« 3 3 
^13 
« 3 3 
[(fll2flf33 - "13^32)] = (^13^32 - ^12^33) 
-(^11^33-^13^31) 
Having done all this, we can evaluate |A| via cofactor expansion as 
1^1 
=^21^21+^22^22+^23^23 
Alternatively, we can evaluate |A| by expanding along the first or third rows, or along any 
of the three columns of A. 
In summary, in computing a determinant of a matrix of order m, expansion by 
cofactors transforms the problem into evaluating m determinants of order m-\ 
and 
forming a linear combination of these. This procedure is continued along successive stages 
until second-order determinants are reached. For high-order matrices (e.g., m > 4), 
expansion by cofactors provides a simple stage wise, if still tedious, way to compute 
determinants by hand, ultimately arriving at computations involving second-order 
determinants. 
Let us now illustrate the evaluation of determinants through cofactor expansion for 
the 3 x 3 and 4 x 4 cases: 
A = 
1 
2 
3 
2 - 1 4 
2 
1 1 

2.7. 
DETERMINANTS OF MATRICES 
63 
We can now apply cofactor expansion using, illustratively, the second row of A: 
^2. = ( - l ) ' " ' 
^22= (-1)'"' 
(-ly 
2+3 
2 
1 
1 
2 
1 
2 
3 
1 
3 
1 
2 
1 
= 1 
= - 5 
= 3 
Then, by continued expansion we get 
| ^ | = 2 ( l ) - l ( - 5 ) + 4(3)=19 
The same principle, illustrated above, applies in the case of fourth- or higher-ordered 
determinants. To illustrate, suppose we expand the determinant around the first column 
of the matrix 
^ 1 2 
3 
5' 
0 
1 3 
3 
2 
1 0 
1 
0 
1 2 
2 
As noted, the entries of the first column of A are 1, 0, 2, and 0. (By choosing a column 
with several zeros in it, the computations are simplified.) 
We first find 
A| = i(-iy 
1 
1 
1 
3 
0 
2 
3 
1 
2 
+ 0 + 2 (-1)'^^ 
2 
1 
1 
3 
3 
2 
5 
3 
2 
+ 0 
We now continue to expand around the first column of each of the two 3 x 3 minors of 
A, above. 
| A | = 1 l(-l)(i-i) 
+ 2^2(-l)(i+i) 
0 
1 
2 
2 
3 
3 
2 
2 
+ 1(_1)(2+1) 
+ 1(_1)(2 + I) 
3 
3 
2 2 
3 
5 
2 
2 
+ 1(_1)(3+1) 
+ 1(_1)(3 + 1) 
3 
3 
0 
1 
3 
5 
3 
3 
= l ( - 2 - 0 + 3) + 2(0 + 4 - 6 ) 
| A | = - 3 
While we stop our illustrations with the case of fourth-order determinants, the same 
principles can be applied to fifth and higher-ordered determinants. Fortunately, the 
availability of computer programs takes the labor out of finding determinants in problems 
of realistic size. 

64 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
2.7.3 
Some Properties of Determinants 
A number of useful properties are associated with determinants. The most important 
of these are listed below: 
1. If a matrix B is formed from a matrix A by interchanging a pair of rows (or a pair 
of columns), then |AI = —|B|. 
2. 
If all entries of some row or column of A are zero, then lAl = 0. 
3. 
If two rows (or two columns) of A are equal, then lAl = 0. 
4. 
The determinant of A equals that of its transpose A'; that is, lAl = IA'|. 
5. 
The determinant of the product of two (square) matrices of the same order equals 
the product of the determinants of the two matrices; that is, |AB| = |A| |B|. 
6. 
If every entry of a row (or column) of A is multiplied by a scalar k, then the value 
of the determinant is k\A\. 
7. 
If the entries of a row (or column) of A are multiplied by a scalar and the results 
added or subtracted from the corresponding entries of another row (or column, 
respectively), then the determinant is unchanged. 
Illustrations of these various properties follow: 
Property 1 
A = '3 r 
.2 
4^ 
; 
B = 
"1 
3' 
_4 
2. 
A = - B =10 
Property 2 
A = 
0 1 
0 4 
|A| = 0 
Property 3 
A = 
3 3" 
2 2 
|A| = 0 
Property 4 
A = "3 r 
_2 4_ 
| A | = | A > 1 0 
A' = 
3 
2' 
1 
4 
Property 5 
A = 
3 
1 
.2 
4. 
A| = 10; 
; 
B = 
B|=14; 
"4 2" 
3 5 
AB = 
15 
11 
20 
24 
AB| = | A l | B | = 140 

2.7. 
DETERMINANTS OF MATRICES 
65 
Property 6 
A = 
|Ah 
3 r 
-2 4_ 
10; 
; 
A: = 3; 
B = 
| B | = 3|A| = 30 
9 
1 
6 
4 
Property 7 
"3 
1 
A= 
; fc = 3; 
b = 3 
L2 4J 
C = 
~3 
r 
-2 4_ 
— 
"0 
9" 
-0 
6_ 
= 
'3 
-81 
_2 - 2 J 
1 ^' 
L2. 
= 
"9" 
.6_ 
| A | = | C | = 10 
In addition to the above (selected) properties of a determinant, we state two very 
important aspects of determinants that are relevant for discussion in subsequent chapters. 
1. A (square) matrix A is said to be singular if |A| = 0. / / |A|9^0, it is said to be 
nonsingular. This aspect of determinants will figure quite prominently in our future 
discussion of the regular inverse of a square matrix. 
2. 
The rank of a matrix is the order of the largest square submatrix whose 
determinant does not equal zero. 
To illustrate the characteristics of these definitions, consider the matrix: 
1 
2 
3' 
A = 
1 
2 
4 
6 
Assume that we wish to find its determinant by cofactor expansion. We expand along the 
first column. 
A| = i(-iy^i 
iA| = 0 
1 
2 
4 
6 
+ 0 + 2(-l) 1 + 3 2 
3 
1 
2 
= -2 + 0 + 2 
We see that lAl = 0 and, according to the definition above, A is singular. In this case we 
note that the entries of the third row of A are precisely twice their counterparts in the 
first row of A In general, if a particular row (or column) can be perfectly predicted from 
a linear combination of the other rows (columns), the matrix is said to be singular. 
Proceeding to the next topic (i.e., the rank of A), we check to see if a 2 x 2 submatrix 
exists whose determinant does not equal zero. 
i 
21 
minor (aii) = 
= - 2 

66 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
Note that we need go no further, since we have found a 2 x 2 submatrix whose 
determinant does not equal zero. 
Notice, also, that even though the matrix is of order 3 x 3, the rank of A cannot be 3, 
since |A| = 0. However, it does turn out that at least one submatrix of order 2 x 2, as 
illustrated above, has a nonzero determinant. Hence, the rank of A, in this case, is 2. 
2.7.4 
The Pivotal Method of Evaluating Detenninants 
In relatively large matrices, such as those of fourth and higher order, the evaluation of 
determinants even by cofactor expansion, becomes time consuming. Over the years 
mathematicians have developed a wide variety of numerical methods for evaluating 
determinants. One of these techniques, the pivotal method (Rao, 1952), has been chosen 
to illustrate this class of procedures. While we illustrate the method in the context of 
evaluating determinants, much more is obtained, as will be demonstrated in Chapter 4. 
The easiest way to describe the pivotal method is by a numerical example. For 
illustrative purposes let us evaluate the determinant of a fourth-order matrix: 
2 
3 
1 2 ' 
4 
2 
3 
4 
1 4 
2 
2 
3 
1 0 
1 
Evaluating the determinant of A proceeds in a step-by-step fashion, with the aid of a 
work sheet similar to that appearing in Table 2.2. 
The top, left-hand portion of Table 2.2 shows the original matrix A, whose 
determinant we wish to evaluate. To the right of this matrix is shown an identity matrix 
of the same order (4 x 4) as the matrix A. The last column (column 9) is a check sum 
column, each entry of which represents the algebraic sum of the specific row of interest. 
(Other than for arithmetic checking purposes, column 9 plays no role in the 
computations.) 
The objective behind the pivotal method is to reduce the columnar entries in A 
successively so that for each column of interest we have only one entry and this single 
entry is unity. Specifically, the boxed entry in row 01 (the number 2) serves as the first 
pivot. Row 10 is obtained from row 01 by dividing each entry in row 01 by 2, the pivot 
item. Note that all entries in row 01 are divided by the pivot, including the entries under 
the identity matrix and the check sum column. Dividing 2 by itself, of course, produces 
the desired entry of unity in the first column of row 10. 
Row 11 is obtained from the results of two operations. First, we multiply each entry 
of row 10 by 4, the first entry in row 02. This particular step is not shown in the work 
sheet, but the nine products are 
4; 
6; 2; 4; 2; 0; 0; 0; 
18 
These are then subtracted from their counterpart entries in row 02, so as to obtain row 
11. 

2.7. 
DETERMINANTS OF MATRICES 
67 
TABLE 2.2 
Evaluating a Determinant by the Pivotal Method 
Row 
No. 
0 
01 
02 
03 
04 
10 
11 
12 
13 
20 
21 
22 
30 
31 
40 
30* 
20* 
10* 
Original matrix 
1 
0 
4 
1 
3 
1 
2 
3 
2 
4 
1 
1.5 
0 
2.5 
-3.5 
1 
1 
1 
3 
4 
1 
2 
3 
4 
2 
2 
0 
1 
0.5 
1 
1 
0 
1.5 
1 
-1.5 
- 2 
-0.25 
2.125 
-2.375 
0 
1 
- 2 
1 
0.471 
-0.88l| 
1 
1 
\ 
5 
1 
0 
0 
0 
0.5 
- 2 
-0.5 
-1.5 
0.5 
-1.75 
0.25 
-0.824 
-1.707 
1.938 
-1.737 
0.066 
-0.668 
Identity matrix 
6 
0 
1 
0 
0 
0 
1 
0 
0 
-0.25 
0.625 
-0.875 
0.294 
-0.177 
0.201 
0.199 
-0.200 
-0.001 
7 
0 
0 
1 
0 
0 
0 
1 
0 
0 
1 
0 
0.471 
1.119 
-1.270 
1.069 
0.267 
0.334 
8 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
1 
0 
1 
-1.135 
0.534 
0.134 
0.667 
Check 
sum 
column 
9 
9 
14 
10 
6 
4.5 
-4 
5.5 
-7.5 
1 
3.0 
-4.0 
1.412 
-0.646 
0.733 
1.267 
1.332 
I AI = (2)(-4)(2.125)(-0.881) = 15 
Note, particularly, that this subtraction has the desired effect of producing a zero 
(shown as a blank) in the first entry of row 11. Note further that the entries of row 11 
add up to —4, the row check sum in the last column; the check sum column is provided 
for all rows and serves as an arithmetic check on the computations. 
Row 12 is obtained in an analogous way; here, since the first element in row 03 is 1, 
we multiply row 10 by unity and then subtract the row 10 elements from their 
counterparts in row 03. Row 13 is also obtained in the same way. First, the row 10 
entries are multiplied by 3, the first entry in row 04. Then these entries are subtracted 
from their counterparts in row 04. Finally, we see that in rows 10 through 13, all entries 
in column 1 are zero (and represented by blanks) except the first entry which is unity. 
At the next stage in the computations, the first element in row 11 becomes the pivot. 
All entries in row 11 are divided by —4, the new pivot, and the results are shown in row 

68 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
20. Row 20 now becomes the reference row. For example, row 21 is found in a way 
analogous to row 11. First, we multiply all entries of row 20 by 2.5, the first entry of row 
12. Although not shown in the work sheet, these are 
2.5; -0.625; 0; 
1.25; 
-0.625; 0; 0; 
2.5 
These elements are then subtracted from their counterparts in row 12 and the results 
appear in row 21. 
The procedure is then repeated by multiplying row 20 by -3.5, the leading element in 
row 13 and subtracting these new entries from their counterparts in row 13. Note that in 
rows 20 through 22, entries in columns 1 and 2 are all zero, except for the leading 
element of 1 in row 20. 
The third pivot item is the entry 2.125 in row 21. All entries in row 21 are divided by 
2.125 and the results listed in row 30. Finally, the entries of row 30 are multiplied by 
—2.375, the leading entry in row 22. Although not shown in the work sheet, these entries 
are 
-2.375; 
-1.119; 
1.957; -0.698; -1.119; 0; 
-3.353 
These entries are subtracted from their counterparts in row 22, providing row 31. The last 
pivot item is —0.881 and appears in row 31. 
Finally, the four pivots are multiplied together, leading to the determinant 
I AI = (2)(-4)(2.125)(-0.881) = 15 
At this point the reader may well wonder what is the role played by the various changes 
being made in the identity matrix as the pivot procedure is applied. Moreover, we have 
not discussed the various calculations appearing in rows 40 through 10*. 
As it tums out, the pivotal method is much more versatile and useful than illustrated 
here. While the determinant of the matrix is, indeed, obtained, the pivotal method can be 
employed for three important purposes: 
1. computing determinants (as the product of pivot elements);^ 
2. 
solving a set of simultaneous equations; 
3. 
finding the inverse of a matrix. 
Here we have only described the first objective. Later on (in Chapter 4) we review the 
pivotal method in terms of all three of the above objectives and, in the process, discuss 
the remaining computations in Table 2.2. 
The reader may also have wondered about what happens when a candidate pivot is 
zero (which, fortunately, did not happen in the preceding example). Clearly, we cannot 
divide the other entries of that row by zero. It tums out, however, that there is a 
straightforward way of dealing with this problem. We shall illustrate it in the continued 
discussion of this method in the context of matrix inversion in Chapter 4. 
' In general, the determinant of an upper triangular matrix (i.e., a square matrix, all of whose 
elements below the main diagonal are zero) is given by the product of its main diagonal elements. 
Similar remarks pertain to the determinant of a lower triangular matrix (Le., a square matrix, all of 
whose elements above the main diagonal are zero). The pivot procedure produces a derived triangular 
matrix via transformation. 

2.8. APPLYING MATRIX OPERATIONS TO STATISTICAL DATA 
69 
In summary, our discussion of determinants does not end here. Since determinants 
figure quite prominently in other topics such as matrix inversion and matrix rank, we 
shall retum to further discussion of them in subsequent chapters. 
2.8 
APPLYING MATRIX OPERATIONS TO STATISTICAL 
DATA 
Much of the foregoing discussion has been introduced for a specific purpose, namely, 
to describe matrix and vector operations that are relevant for multivariate procedures. 
One of the main virtues of matrix algebra is its conciseness, that is, the succinct way in 
w^iich many statistical operations can be described. 
To illustrate the compactness of matrix formulation, consider the artificial data of 
Table 2.3. For ease of comparison these are the same data that appeared in the sample 
problem of Table 1.2 in Chapter 1. That is, Y denotes the employee's number of days 
TABLE 2.3 
Computing Various Types of Cross-Product Matrices from Sample Data 
Employee 
a 
b 
c 
d 
e 
f 
g 
h 
i 
J 
k 
1 
Y 
B = X, 
X^ 
Y 
c = x. 
X, 
Raw 
Y 
"823 
702 
_542 
Y 
1 
0 
1 
4 
3 
2 
5 
6 
9 
13 
15 
16 
75 
Y" 
1 
0 
1 
16 
9 
4 
25 
36 
81 
169 
255 
256 
823 
X, 
1 
2 
2 
3 
5 
5 
6 
7 
10 
11 
11 
12 
75 
cross-product matrix 
X, 
702 
639 
497 
X, 
542" 
497 
397 _ 
Covariance matrix 
Y 
X, 
X, 
"29.52 
19.44 
14.44" 
19.44 
14.19 
10.69 
_ 14.44 
10.69 
8.91_ 
X,-
1 
4 
4 
9 
25 
25 
36 
49 
100 
121 
121 
144 
639 
X, 
1 
1 
2 
2 
4 
6 
5 
4 
8 
7 
9 
10 
59 
Y 
S = ^ij 
x,\ 
Y 
K = X^ 
X, 
X,' 
1 
1 
4 
4 
16 
36 
25 
16 
64 
49 
81 
100 
397 
SSCP matrix 
Y 
X, 
YX, 
1 
0 
2 
12 
15 
10 
30 
42 
90 
143 
165 
192 
702 
YX^ 
1 
0 
2 
8 
12 
12 
25 
24 
11 
91 
135 
160 
542 
X, 
"354.25 
233.25 
] 
233.25 
170.25 
] 
_173.25 
128.25 
] 
Correlation matrix 
Y 
X, 
ri.OO 
0.95 
0.95 
1.00 
L0.89 
0.95 
X, 
0.89" 
0.95 
1.00_ 
L73.25~ 
128.25 
L 06.92. 
X,X^ 
1 
2 
4 
6 
20 
30 
30 
28 
80 
11 
99 
120 
497 

70 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
absent during the past year; Xi denotes his attitude rating (the higher the score the less 
favorable his attitude toward the firm); and X2 denotes the number of years he has been 
employed by the firm. 
As recalled from Chapter 1, this miniature data bank will be used later on in the book 
to demonstrate several multivariate techniques, including multiple regression, princip^ 
components analysis, and multiple discriminant analysis. For the moment, however, let us 
consider the role of matrix algebra in the development of data summaries prior to 
employing specific analytical techniques. 
The computation of means, variances, covariances, correlations, etc., is a necessary 
preliminary to subsequent multivariate analyses in addition to being useful in its own 
right as a way to summarize aspects of variation in the data. 
2.8.1 
Sums, Sums of Squares, and Cross Products 
To demonstrate the compactness of matrix notation, suppose we are concerned with 
computing the usual sums, sums of squares, and sums of cross products of the "raw" 
scores involving, for example, Y and Xi in Table 2.3: 
27; 
2X1; 2 7 ^ 
SXi^ 
1:YXI 
In scalar products form, the first two expressions are simply 
i : r = l V = 75; 
SXi = l'xi=75 
where 1' is a 1 x 12 unit vector, with all entries unity, and y and Xi are the Y and X 
observations expressed as vectors. Notice in each case that a scalar product of two vectors 
is involved. 
Similarly, the scalar product notion can be employed to compute three other 
quantities involving Y and Xi: 
2^2 = y'y = 823 
ZXi^ = X 1X1 = 639 
Zr^i = y'xi = 702 
Table 2.3 lists the numerical values for all of these products and, in addition, the products 
involving X2 as well. 
As a matter of fact, if we designate the matrix A to be the 12 x 3 matrix of original 
data involving variables Y, Xi, and X2, the following expression 
B=A'A 
which is often called the minor product moment (of A), will yield a symmetric matrix B 
of order 3 x 3 . The diagonal entries of the matrix B denote the raw sums of squares of 
each variable, and the off-diagonal elements denote the raw sums of cross products as 
shown in Table 2.3. 
2.8.2 
Mean-Corrected (SSCP) Matrix 
We can also express the sums of squares and cross products as deviations about the 
means of Y, Xi, and X2. The mean-corrected sums of squares and cross-products matrix 

2.8. 
APPLYING MATRIX OPERATIONS TO STATISTICAL DATA 
71 
is often more simply called the SSCP (sums of squares and cross products) matrix and is 
expressed in matrix notation as 
S = A ' A - - ( A ' l ) ( r A ) 
m 
v^ere 1 denotes a 12 x 1 unit vector and m denotes the number of observations; m = 12. 
The last term on the right-hand side of the equation represents the correction term and is 
a generalization of the usual scalar formula for computing sums of squares about the 
mean: 
Sx^ = SX^ - {^xf 
m 
where x=X - X\ that is, where x denotes deviation-from-mean form. Alternatively, if the 
columnar means are subtracted out of A to begin with, yielding the mean-corrected 
matrix A^, then 
S=AyA 
d ^d 
For example, the mean-corrected sums of squares and cross products for FandXi are 
V = 27^ -(sry 
m 
= 823-
2xi^ = 2 Z i ^ -(zxo^ = 639-
m 
12 
(75)^ 
12 
= 354.25 
= 170.25 
^y., = ^YX.J^XHLK 
702 - < Z i ^ ) = 233.25 
m 
12 
The SSCP matrix for all three variables appears in Table 2.3. 
2.8.3 
Covariance and Correlation Matrices 
The covariance matrix, shown in Table 2.3, is obtained from the (mean-corrected) 
SSCP matrix by simply dividing each entry of S by the scalar m, the sample size. That is. 
C = - S 
m 
In summational form the off-diagonal elements of C can be illustrated for the variables Y 
and Xi by the notation 
cov(rXi) = i:yxilm = 233.25/12 = 19.44 
Note that a covariance, then, is merely an averaged cross product of mean-corrected 
scores. The diagonals of C are, of course, variances; for example, 
Xy^'/m 

72 
2. 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
(In some applications we may wisli to obtain an unbiased estimate of the population 
covariance matrix; if so, we use the divisor m - 1 instead of m). 
The correlation between two variables, y and Xi, is often obtained as 
where y and Xi are each expressed in deviation-from-mean form (as noted above). 
Not surprisingly, R the correlation matrix is related to S, the SSCP matrix, and C, the 
covariance matrix. For example, let us return to S. The entries on the main diagonal of S 
represent mean-corrected sums of squares of the three variables Y, Xi, and X2. 
If we take the square roots of these three entries and enter the reciprocals of these 
square roots in a diagonal matrix, we have 
D = 
0 
0 
1/V^^ 
Then, by pre- and post multiplying S by D we can obtain the correlation matrix R. 
R=DSD 
s / V W V W V W 
V w V s ^ 
^yx2 
DX1X2 
2X2 
^ V z ^ ^ V^^vS? VsVv^^ 
The above matrix is the derived matrix of correlations between each pair of variables and 
is also shown in Table 2.3 
Ordinarily, we could then go on and use R in further calculation, for example, to find 
the regression of Y on Xi and X2. Since our purpose here is only to show the conciseness 
of matrix notation, we defer these additional steps until later. In future chapters we shall 
have occasion to discuss all four of the preceding matrices: (a) the raw sums and 
cross-products matrix, (b) the (mean-corrected) SSCP matrix, (c) the covariance matrix, 
and (d) the correlation matrix. 
At this point, however, we should note that they are all variations on a common 
theme: All involve computing the minor product moment of some matrix. 
1. 
Raw sums of squares and cross-products matrix: 
B = AA 

2.9. 
SUMMARY 
73 
2. 
The (mean-corrected) SSCP matrix: 
S = Ad'Ad 
where A^ is the matrix of deviation-from-mean scores; that is, each column of A(j sums to 
zero since each columnar mean has been subtracted from each datum. 
3. 
The covariance matrix 
C = 1/mAd'Ad 
4. 
The correlation matrix 
R= 1/mAs'As 
where A^ is the matrix of standardized scores. 
As can be found from Table 1.2, in which the sample problem data first appear, both 
deviation-from-mean and standardized scores are shown along with the original scores. 
Finally, the matrices Ad of mean-corrected scores and As of standardized scores are 
derived from A, the matrix of original scores, in the following way. We first find 
Ad = A — la' 
where 1 is a 12 x 1 unit column vector and a' is a 1 x 3 row vector of variable means. The 
vector of means is, itself, obtained from 
a' = I'A/m 
where 1' is now a 1 x 12 row vector. Next, we find the matrix of standardized scores 
from Ad as follows: 
As=AdD 
\\iiere D is a diagonal matrix whose entries along the main diagonal are the reciprocals of 
the standard deviations of the variables in A. 
The standard deviation of any column of Ad, say Ed/, is simply 
5«d/"^ad/ad//m 
In summary, any of the operations needed to find various cross-product matrices are 
readily expressible in matrix format. In so doing we arrive at a very compact and graceful 
way to portray some otherwise cumbersome operations. 
2.9 
SUMMARY 
The purpose of this chapter has been to introduce the reader to relations and 
operations on vectors and matrices. Our emphasis has been on defining various operations 
and describing the mechanics by which one manipulates vectors and matrices. Such 
elementary operations as addition and subtraction, multiplication of vectors and matrices 
by scalars, the scalar product of two vectors, vector times matrix multiplication, etc., 
were described and illustrated numerically. Various properties of these operations were 
also described. 

74 
VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
Special kinds of vectors (e.g., null, sign, unit) and special kinds of matrices (e.g., 
diagonal, scalar, identity) were also defined and illustrated numerically. We then turned 
to an introductory discussion of determinants of (square) matrices. Evaluation of 
determinants via expansion by cofactors and the pivotal method was described and 
illustrated. 
We concluded the chapter with a demonstration of how matrix algebra can be used to 
provide concise descriptions of various statistical operations that are preparatory to 
specific multivariate analyses. These matrix operations are particularly amenable to 
computer programming and are used extensively in programs that deal with multivariate 
procedures. 
REVIEW QUESTIONS 
1. Write the following equations in matrix form: 
a. 
4x + y - z = 0 
3x-4y-\-2z= 
\ 
5x- 
y -2z = l 
Given the matrices 
"1 2 
A = 4 
0 
find 
- 3 
1 
a. 
A + B 
d. A-(B + C) 
b. 
2x + 3y+ 
z=\\ 
x+ y + lz = 24 
3x+ 5y + 4z = 25 
b. 
(A + C) + B 
e. 
-(A + B) 
C = 
0 
- 2 
c. 
A + (B + C) 
f. 
(A - B) + C 
3. 
Given the vectors 
a = 
[l 
2 
4 
b = 
1 
3 
4 
and the scalars 
ki 
find 
a. 
b b 
b. 
—ki2L 
c. k2h' 
d. 
a'b 
e. 
kik2isLa) 
4. 
Given the matrices, vectors, and scalars of Problems 2 and 3, find 
a. 
a'A' 
b. 
kiB 
c. 
(AB')' 
d. 
kiC 
e. 
k2BA'C 
f. 
^^(ba) 
f. 
ab 

REVIEW QUESTIONS 
75 
5. 
Examine the relationships among (DE)', D'E', and E ' D ' under the following two sets 
of conditions. 
Let: 
a. 
D = 
Let: 
6. 
Given the matrices 
- 1 
F = 
b. 
D = 
a 
b 
c 
d_ 
"1 
3 
0 
2 
and 
E = 
and 
E = 
e 
f 
3 4" 
0 
2 
3 
5 
1 
- 3 
- 5 
-
1
3 
5 
and 
G = 
2 
- 3 
- 5 
- 1 4 
5 
1 
- 3 
- 4 
demonstrate that 
a. 
FG = <^ does not imply that either F = ^ or G = 0. 
b. 
Find GF. Is this product equal to <j>1 
7. 
Given the matrices and vectors in Problems 3 and 6, find the products: 
a. 
a'Fb 
b. 
b'Gb 
c. 
a'FGa 
d. 
b'FGa 
8. 
Consider the diagonal matrices 
4 
0 
0 
Hi 
and 
Ho 
0 
3 
0 
0 
0 
1 
and the vectors and matrices of Problems 3 and 6. Find 
a. 
a'(HiF) 
b. 
b'(HiGH2) 
c. 
a'(HiH2)b 
9. 
In ordinary algebra, we have the relationship 
1 
0 
0 
0 
0 
0 
0 
0 
2 
d. 
a(FGH2) 
x - 2 = (x+ l ) ( x - 2 ) 
In matrix algebra, if 
see if the following holds: 
10. 
If 
a 
b 
c 
d 
and 
1 
0 
0 
1 
X - 2 I = (X + I ) ( X - 2 I ) 
Find 
a. 
y 
1 
i 
2 
3 
b. 
K^ 
and 
c. 
(JK)^ 
K = 
2 
0 
0 
3 
d. 
(KJ)^+(JKy 

76 
2. VECTOR AND MATRIX OPERATIONS FOR MULTIVARIATE ANALYSIS 
11. 
Evaluate the determinants of the following 2 x 2 matrices: 
a. 
L, = 
L3 = 
~l/2 
1/4 
X 
x^\ 
1/3-, 
1/6 
b. 
d. 
L2 = 
L4 = 
- 1 
1 
a 
b 
o] 
oj 
-b' 
a 
12. 
By means of cofactor expansion, evaluate the determinants of 
4 
-12 
- 4 ' 
Mo ~ 
c. 
M3 = 
2 
L-1 
0 
2 
4 
1 
3 
2 
4 
1 
-3 
3 5" 
3 
2J 
13. 
Evaluate the determinant of the fourth-order matrix used in Section 2.7.4 via 
cofactor expansion and check to see that it equals the value of the determinant found 
from the pivotal method. 
14. 
Apply the pivotal method to matrix M3 in Problem 12 and check your answer with 
that found by cofactor expansion. 
15. 
Assume the following data bank: 
a 
b 
c 
d 
e 
f 
Find, via matrix methods, 
a. 
Sr; 
Ji; 
SrXj; 
2A^3^-(SA^a) Vm 
b. the 4 x 4 (mean-corrected) SSCP matrix S 
c. 
the covariance matrix C 
d. 
the correlation matrix R 
e. 
the matrix of mean-corrected scores 
f. 
show that the sum of the deviations about the mean equals zero for the first 
colunm Y-
Y 
2 
4 
3 
7 
8 
9 
X, 
1 
2 
5 
3 
7 
8 
X2 
0 
3 
2 
4 
7 
7 
^ 3 
9 
8 
4 
5 
2 
1 

CHAPTER 3 
Vector and Matrix Concepts 
from a Geometric Viewpoint 
3.1 
INTRODUCTION 
This chapter is, in part, designed to provide conceptual background for many of the 
vector and matrix operations described in Chapter 2. Here we are interested in "what goes 
on" when a scalar product, for example, is computed. Since geometry often provides a 
direct intuitive appeal to one's understanding, Hberal use is made of diagrams and 
geometric reasoning. 
To set the stage for the various geometric descriptions to come, we define a Euclidean 
space—the comerstone of most multivariate procedures. This provides the setting in 
which point representations of vectors and such related concepts as vector length and 
angle are described. The operations of vector addition, subraction, multipHcation by a 
scalar, and scalar product are then portrayed geometrically. 
We next turn to a discussion of the meaning of linear independence and the 
dimensionaUty of a vector space. The concept of a basis of a vector space is described, 
and the process by which a basis can be changed is also illustrated geometrically. Special 
kinds of bases—orthogonal and orthonormal—are illustrated, as well as the Gram-Schmidt 
process of orthonormalizing an arbitrary basis. Some comments are also made regarding 
general (oblique) Cartesian coordinate systems. 
Our discussion then turns to one of the most common types of transformations-
orthogonal transformations (i.e., rotations) of axes. These transformations are portrayed 
in terms of simple geometric figures and also serve as illustrations of matrix multiplication 
in the context of Chapter 2. 
We conclude the chapter with a geometric description of some commonly used 
association measures, such as covariance and correlation. Moreover, the idea of viewing a 
determinant of a matrix of association coefficients (e.g., a covariance or a correlation 
matrix) as a generalized scalar measure of dispersion is also described geometrically and 
tied in with counterpart material that has already been covered in Chapter 2. In brief, 
presentation of the material in this chapter covers some of the same ground discussed in 
Chapter 2. Here, however, our emphasis is on the geometry rather than the algebra of 
vectors.^ 
^ In this chapter (and succeeding chapters as well) we shall typically present the material in terms 
of row vectors a', b', etc., particularly when explicit forms of the vectors are used, such as 
a' = (1, 2, 2). This is strictly to conserve on space. The reader should get used to moving back and 
forth between column vectors (as emphasized in Chapter 2) and row vectors as emphasized here. 
77 

78 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
3.2 
EUCLIDEAN SPACE AND RECTANGULAR CARTESIAN 
COORDINATES 
Before moving right into a discussion of the geometric aspects of vectors, it is useful to 
estabUsh some preliminaries, even though they may be famiHar to many readers. These 
preliminaries involve the construction of a coordinate system and a description of 
standard basis vectors. 
3.2.1 
Coordinate Systems 
For illustrative purposes let us review a system that is familiar to most, namely, a 
three-dimensional coordinate system.^ To do this, we need three things: 
1. A point called the origin of the system, that will be identified by 0', the zero 
vector. 
2. Three lines, called the coordinate axes, that go through the origin. We shall assume 
for the time being that each line is perpendicular to the other two, and we shall call these 
lines rectangular Cartesian axes, denoted by jc, y, and z 
3. 
One point, other than the origin, on each of the three axes. We need these points 
to establish scale units and the notion of direction, positive or negative, relative to the 
origin. Here we assume that the unit of length on each axis is the same. 
Figure 3.1 shows a simple illustration of the type of coordinate system that we can set up. 
(1,-1,0) 
(-1, 1,0) 
Fig. 3.1 
A three-dimensional coordinate system with illustrative points. 
^ Later on we shall refer to two-dimensional as well as to three-dimensional systems. However, this 
particularization should cause no problems in interpretation. 

3.2. 
EUCLIDEAN SPACE AND RECTANGULAR CARTESIAN COORDINATES 
79 
By convention we have marked the positive directions of x, y, and z. Note further that 
three coordinate planes are also established: 
1. The xy plane containing the x and y axes; this is the plane perpendicular to the z 
axis and containing the origin. 
2. The xz plane containing the x and z axes; this is the plane perpendicular to they 
axis and containing the origin. 
3. 
The yz plane containing the y and z axes; this is the plane perpendicular to the x 
axis and containing the origin. 
These planes cut the full space into eight octants. The first octant, for example, is the one 
above thexy plane in which all coordinates are positive. 
Having estabhshed a coordinate system and the idea of signed distances along the axes, 
we can assign to each point in the space an ordered triple of real numbers: 
a =(ai,a2,as) 
where ai is the coordinate associated with the projection of a onto thex axis, ^2 is the 
coordinate associated with the projection of a' onto the y axis, and as is the coordinate 
associated with the projection of a' onto the z axis. 
The (perpendicular) projection of a point onto a Hne is a vector on the line whose 
terminus or arrowhead is at the foot of the perpendicular dropped from the given point 
to the line. With the x, y, and z axes that have been set up in Fig. 3.1, the length of each 
projection is described on each axis by a single number, its coordinate. The coordinate is 
a signed distance from the origin; the sign is plus if the projection points in the positive 
direction and minus if the projection points in the negative direction. Figure 3.1 shows a 
few illustrative cases in different octants of the space. 
In Chapter 2 we talked about a vector as a mathematical object having direction and 
magnitude. We need both characteristics since we can have an infinity of vectors, all 
having the same direction (but varying in length or magnitude), or all having the same 
length (but varying in direction). Furthermore, before we can talk meaningfully about 
direction, we need to fix a set of reference axes so that "direction" is considered relative 
to some standard. 
In one sense vectors can originate and terminate anywhere in the space, as illustrated 
in Fig. 3.2. However, as also illustrated in Fig. 3.2, we can always move some arbitrary 
vector in a parallel direction so that the vector's tail starts at the origin. All vectors that 
start from the origin are called position vectors, and we essentially confine our attention 
to these. Since we have not changed either the direction or the length of the arbitrary 
vector by this parallel displacement, any vector can be portrayed as a position vector. 
By concentrating our interest on position vectors, it turns out that any such vector can 
also be represented by a triple of numbers that we called components of a vector in 
Chapter 2. In the present context these components are also coordinates. By convention, 
the /th component of a vector is associated with the /th coordinate axis. This is illustrated 
in Fig. 3.3, by the projection of the terminus of a' onto x, y andz, the coordinate axes. 
Notice that each projection lies along the particular axis of interest. The (signed) length 
of each of these projections is, of course, described by a single number that is plus or 
minus, depending upon its direction along each axis relative to the origin. 

80 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
Fig. 3.2 
Fig. 3.3 
Fig. 3.2 
Parallel displacement ot arDitrary vectors. 
Fig. 3.3 
Vector components shown as signed lengths of projections. 
Thus, given a fixed origin that is called O', we can always make a one-to-one 
correspondence between position vectors and points. For each point P we can find a 
corresponding position vector from the origin toP; for each position vector with its tail at 
O' we can locate a point P at the vector's terminus. 
By restricting our attention to vectors emanating from the origin, any vector is both a 
geometric object, possessing length and direction, and an ^i-tuple of numbers (three 
numbers in this case). Since the vectors that we shall discuss will have their tails at the 
origin, two vectors are equal if and only if they terminate at the same point. If it were the 
case that two vectors had their tails at two different points, then they would be equal if 
and only if one of the vectors could be moved, without changing its direction or length, 
so that it coincided with the other. 
In summary, then, by making sure that all of the vectors are position vectors (i.e., start 
at the origin of the coordinate system), we can pass freely back and forth between the 
geometric character of a vector (length and direction) and its algebraic character (an 
ordered n-tuple of scalars). The length of a vector's projection is given by the vector's 
coordinate on the x, y, and z axes, respectively, and the sign of its projection on x, jv, and 
z depends on where the projection terminates, relative to the origin. 
3.2.2 
Standard Basis Vectors 
Continuing on with the preHminaries, let us next consider Fig. 3.4. This figure shows a 
three-dimensional space with the vector a' = (1, 2, 2) appearing as a directed line segment. 

3.2. 
EUCLIDEAN SPACE AND RECTANGULAR CARTESLVN COORDINATES 
81 
a ' = ( 1 , 2 , 2) 
Fig. 3.4 
Vector representation in three-dimensional space. 
To set up this coordinate space, we define a special set of zero-one coordinate vectors, 
denoted e/, as 
1. vector Ci' of unit length in the positive (by convention) x direction: 
ei' = (l,0,0) 
2. vector 62' of unit length in the positive y direction: 
62'= (0,1,0) 
3. 
vector 63' of unit length in the positive z direction: 
63'= (0,0,1) 
We shall continue to let 0', the zero vector, denote the origin of the space. As suggested in 
the discussion of vector addition and scalar multiplication of a vector in Chapter 2, we 
can now write the vector a' = (1, 2, 2) as a linear combination of the coordinate vectors: 
le/ + 262' + 263'= 1(1,0,0) + 2(0,1,0) + 2(0,0,1) 
= (1,0,0)+ (0,2,0)+ (0,0,2) 
a'= (1,2,2) 
Note that what we have done is to perform scalar multiplication followed by vector 
addition, relative to the coordinate vectors e/. We shall call the e/ vectors a standard basis 
and comment later on the meaning of basis vectors, generally. 
Note, further, that if we had the oppositely directed vector —a', this could also be 
represented in terms of the standard basis vectors as the linear combination: 
- l e / - 2 e 2 ' - 2 6 3 ' = (-1,0,0)+ (0,-2,0)+ (0,0,-2) = (-1,-2,-2) 
In this case —a would extend in the negative directions of x, y, and z. 

82 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
What is shown above in particularized form can be generalized in accordance with the 
discussion of linear combinations of vectors in Chapter 2. As recalled: 
Given p n-component vectors bj', b 2 ' , . . . , bp' the n-component vector 
p 
a' = Z kihl = A:ibx' + k^h^' + • • • + kj,h' 
i = \ 
is a linear combination of p vectors, b / , \)2\ . • ., bp' for any set of scalars kj 
(i = 
l2,...,p). 
In the illustration above we have p = 3 basis vectors, each containing n = 3 
components. The components of the vector a' = (1, 2, 2) involve p = 3 scalars. The b/' 
vectors in the more general expression above correspond to the specific e/ vectors in the 
preceding numerical illustration. 
The introduction of a set of standard basis vectors allows us to write any ^-component 
vector a', relative to a standard basis of w-component e/' vectors, as 
a'= Z fl/e/ 
1 = 1 
where a,- denotes the fth component of a', and each of the n basis vectors has a 1 
appearing in the zth position and zeros elsewhere. In this special case of a Hnear 
combination, the number of vectors p equals the number of components in a', namely, n. 
Figure 3.5, incidentally, shows a' in terms of the triple of numbers (1, 2, 2). This point 
representation, as we now know, is equally acceptable for representing a' since the vector 
is already positioned with its tail at the origin. 
The important point to remember is that a', itself, can be represented as a linear 
combination of other vectors—in this case, the standard basis vectors e,'. In a sense the e/' 
Fig. 3.5 
Point representation in three-dimensional space. 

3.2. 
EUCLIDEAN SPACE AND RECTANGULAR CARTESLVN COORDINATES 
83 
represent a standard scale unit across the three axes. Any projection of a', then, can be 
considered as involving (signed) multiples of the appropriate e/ vector. 
With these preliminaries out of the way, we can now introduce the central concept of 
the chapter, namely, the EucUdean space and the associated idea of the distance between 
two points, or vector termini, in EucHdean space. This idea, in turn, leads to the concepts 
of angle and length. 
3.2.3 
Definition of Euclidean Space 
A Euclidean space of n dimensions is the collection of all n-component vectors for 
which the operations of vector addition and multiplication by a scalar are permissible. 
Moreover, for any two vectors in the space, there is a nonnegative number, called the 
Euclidean distance between the two vectors. ^ 
The function^ that produces this nonnegative number is called a Euclidean distance 
function and is defined as 
la'-b'll = [(a, -b,y 
+ {a2 - b^f + • • • + 
{an-b^fV 
Altematively, we can define ||a' — b'|| in terms of a function of the now-familiar scalar 
product of (a — b) with itself: 
|a'-b'||=[(a-by(a-b)] 1/2 
where the vector (a — b) is a difference vector. 
To get some geometric view of the EucHdean distance between two position vector 
termini (i.e., between two points), let us first examine Panel I of Fig. 3.6. Here in two 
dimensions are the two points 
a'= (1,1); 
b' = (1.5,2) 
Note that their straight-line distance can be represented by the square root of the 
hypotenuse of the right triangle, as sketched in the chart. In terms of the distance 
formula, we have 
||a'-b'|| = [(l-1.5)^+(l-2)^]i/^=Vr25 = 1.12 
Panel II of Fig. 3.6 merely extends the same idea to three dimensions for two new points: 
a'= (1,1,-2); 
b' = (2,l,2) 
lla' -b'll = [(1 -If 
+ (1 - \f + {-2-2fY^^ 
= vT7 = 4.12 
^ A more formal definition considers a Euclidean space as a finite-dimensional vector space on 
which a real-valued scalar or inner product is defined. 
"* The Euclidean metric is, itself, a special case of the Minkowski metric. The Minkowski metric 
also obeys the three distance axioms (positivity, symmetry, and triangle inequality). Since we have 
used the single bars |A| to denote the determinant of a matrix in Chapter 2, we use the double bars 
lla' - b'll to denote the distance between two vectors, taken here to mean Euclidean distance. 

84 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
y 
2t 
1 + 
b ' - (1.5,2) 
M l , 1) 
- h x 
*a' = (1, 1,-2) 
Fig. 3.6 
Illustrations of Euclidean distances between pairs of points. Key: I, two dimensions; 
II, three dimensions. 
Euclidean distance, then, entails adding up the squared differences in projections of each 
point on each axis in turn, and then taking the square root of the sum. 
As might be surmised from either panel in Fig. 3.6, the EucHdean distance function 
possesses the following properties: 
lla'-b'||>0 
l|a'-b'|| = ||b' 
lla'-b'll + llb'-
unless 
a ' - b ' = 0 ' ; 
positivity 
-a'll; 
symmetry 
- c'll > ||a' — c'll; 
triangle inequality 
The first of the above properties, positivity, precludes the possibility of negative 
distances. Symmetry, the second property, means that the distance from a' to b' is the 
same as the distance from b' to a'. The third property, triangle inequaHty, states that the 
sum of the distances between a' and b' and between b' and some third point c' is no less 
than the direct distance between a' and c'. If b' Hes on the Hne connecting a' and c', then 
the sum of the distances of a' to b' and b' to c' equals the direct distance from a' to c'. 
We next define the concept of vector length or magnitude. The length of a vector 
a' = («i, flf2 5 • • • , ^Ai) is defined as 
l|a'|| = 
n 
i = \ 
1/2 

3.3. GEOMETRIC REPRESENTATION OF VECTORS 
85 
Note that this is a special case of the Euclidean distance function in which the second 
vector is the origin of the space, or the O' vector. That is, 
2 i l / 2 
lla'll = lla'-O'll = [(a,-Oy + (a^-O)^ + • • • + (fl„-0)^] 
Furthermore, we can also observe that the squared vector length equals the scalar product 
of a with itself: 
lla'll^ = a'a 
Thus, in the case of a' = (1, 2, 2), we see that 
3 
lla'll^ = Z ^?=(l)^ + (2)^ + (2)^ 
= [{a,-Or + ia,-Of 
+ (a.-Of] 
= (if + (if + (2f 
= aa = (1,2,2)'(1,2,2) = (1)^ + {2f + (2f = 9 
are all equivalent ways of finding the squared length of a'. The square root of ||a'|p , that 
is, \/9 = 3, is, of course, the Euclidean distance or vector length of the vector terminus as 
measured from the origin 0'. 
We now discuss some of these notions in more detail. Since it will be intuitively easier 
to present the concepts in terms of the standard basis vectors e/—that is, where 
rectangular (mutually perpendicular) Cartesian coordinates are used—we discuss this case 
first and later briefly discuss more general coordinate systems in which the axes are not 
necessarily mutually perpendicular, although the space is still assumed to be Euclidean. 
3.3 
GEOMETRIC REPRESENTATION OF VECTORS 
We have already commented on the fact that a vector can be equally well represented 
by the directed line segment, starting from the origin (Fig. 3.4), or the triple of point 
coordinates (Fig. 3.5). In both representations, the coordinate on each axis is the foot of 
the perpendicular dropped from a' to each axis. 
Our interest now is in expanding some of these geometric notions so as to come up 
with graphical counterparts to the various algebraic operations on vectors that were 
described in Chapter 2. 
3.3.1 
Length and Direction Angles of a Single Vector 
Let us again examine vector a' = (1, 2, 2), represented as the directed line segment in 
Fig. 3.4. As shown earlier, the length or Euclidean distance, denoted ||a'||, of a' from the 
origin O' is 
lla'll = [(^1-0)^ + (^2-0)' + (^3-0)^] '^^ 
= [(l)' + (2)2 + (2)^]^/2 = 3 

86 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
a ' = ( 1 , 2 , 2) 
Fig. 3.7 
Vector length and direction cosines. 
Our interest now focuses on various aspects of length and direction angles. First, observe 
from Fig. 3.7 that a' makes angles of a, |3, and 7 with the three reference axes x, y, and z. 
The cosines of these angles are called direction cosines relative to the reference axes and 
are computed as ratios of each vector component to the vector's length. 
Since the length of a' is 3 and the components of a' are 1, 2, and 2, the cosines of a, ^, 
and 7, respectively, are 
cos a = 3; 
cos j3 = § 
Notice that these can be written out in full as 
cos a. = 
cos j3 = 
cos 7 = 
W^a.'^ai'Y'' 
02 
^3 
cos 7 = I 
The angles corresponding to these cosines are 
2 
3 
2 
3 
a ^ 7 r 
^^48° 
7 = 48° 
Notice that our use of the cosine is in accordance with basic trigonometry. For example, 
the cosine of the angle a, which the vector a' makes with the x axis, is equal to the length 
of the adjacent side of the right triangle, formed by projection of a' onto the x axis, 
divided by the hypotenuse of that right triangle. The adjacent side has length 1, or unit 
distance from the origin, and the hypotenuse is of length 3. Hence, the cosine of a is 5. 
By similar reasoning the cosine of j3 is | with respect to the 7 axis, and that of 7 is § with 
respect to the z axis. 

3.3. 
GEOMETRIC REPRESENTATION OF VECTORS 
87 
We can also discuss some of these notions in somewhat more general terms. Once a 
coordinate system is chosen, any position vector that emanates from the origin can be 
represented by 
1. the angles a, jS, 7, made by the line with respect to the x, y, and z axes, where 
0 < a, 13, 7 < 180°, and 
2. 
the vector's length or magnitude. 
We have already discussed the case of vectors that emanate from locations other than 
the origin of the space. Therefore, by appropriate parallel displacement to a position 
vector, any vector in the space can be represented by its direction angles and length. 
If we had a vector —a' = ( - 1 , —2, —2) that was oppositely directed from a', this would 
cause no problems since the direction cosines and angles would then be 
cos a = - ^ ; 
a = 109° = 180°-71° 
cosi3 = - | ; 
13= 132° =180°-48° 
cos 7 = - ^ ; 
7= 132° = 180°-48° 
It is also useful to examine the sum of the squared cosines of a, ]3, and 7. Since 
^l^+fl?2^+^3^ = l|a'||^wehave 
cos^ a + cos^ i3 + cos^ 7 = 1 
We can state the above result in words: The sum of the squares of the direction cosines of 
some vector a!, originating at the origin, is equal to L This fact holds true in any 
dimensionality, not just three dimensions. 
Furthermore, it is a simple matter to work backward to find the components of a 
vector if we know its direction angles and length. Continuing with the illustrative vector, 
a' = (l,2,2) 
with direction angles and cosines, 
a = 71°, cosa = ^; 
/3 = 48°, cos/3 = | ; 
y 
3-1-
= (-3, 1) 
_ 2 
7 = 48 , 
cos 7 = 5 
\ 
H-x 
b' = (-2,-3) 
Fig. 3.8 
Direction angles and lengths of illustrative vectors. 

88 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
and length ||a'|i = 3, we have, by simple algebra, the vector components: 
^ i = K 3 ) = l ; 
a2=i(3) = 2; 
^3 = 1(3) = 2 
In the case of negative coordinates, the cosines, of course, would be negative for those 
axes involving negative projections. 
Working with negative cosines is most easily shown in two dimensions, as illustrated in 
Fig. 3.8. Here are portrayed three different vectors, terminating in three different 
quadrants. We first note the smaller angle (<180°) made by each vector with each axis. 
Then, by means of the formulas shown earlier, the direction cosines of each vector are 
computed as follows: 
0 
0 
0 
c o s OCQ 
c o s OLfy — 
[(-3? 
[i-2? 
- 3 
+ (1)2] 1/2 
- 2 
+ (_3)2]l/2 
1 
= -0.95; 
^"^fe=[(_3)a.^(i)^].n=Q-32 
= -0.55; 
cos fe = - j ^ - ^ ^ , - ^ | 5 ^ ^ =-0.83 
[(1)2 + (_2)2]i/2 -0-45; 
oosPc = [(i)2 + (_2)2]i/2 =-0.89 
with correspondent direction angles: 
a^-161°; 
Pa= 71° 
a5-123°; 
^^ = 146° 
a^ ^ 
63°; 
13^ ^153° 
as shown in Fig. 3.8. 
Notice, in particular, that as any angle becomes obtuse, the formulas for finding 
direction cosines still hold since changes in the sign of the cosine are taken care of by 
corresponding changes in the appropriate vector components. 
In summary, then, any position vector is uniquely determined by knowledge of its 
magnitude and direction. In tum, its direction is given by the angles it makes with the 
reference axes. These angles are obtained from the cosines that are computed by the 
expression 
cos "^i = all 
where ^/ 
denotes the angle between the vector and the /th reference axis 
(0° < "^i < 180°), Ui denotes the /th component of a', and ||a'|| denotes its length. 
Note, in particular, that if cos ^/ 
= 0, then the angle is 90° and the vector is said to be 
orthogonal or perpendicular to the ith reference axis, 
3.3.2 
Geometric Aspects of Vector Addition and Multiplication 
by a Scalar 
While we have earlier discussed in Chapter 2 the rules of vector addition and 
subtraction and multiplication of a vector by a scalar, it is useful now to show these 

3.3. 
GEOMETRIC REPRESENTATION OF VECTORS 
89 
^•H-
hi 

90 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
operations geometrically. First consider the two vectors a' = (1, 2, 2) and b' = (0, 1, 2) 
shown in Panel I of Fig. 3.9. As already known from Chapter 2, their vector sum is 
( l + 0 , 2 + l , 2 + 2) = (l,3,4) 
We can formalize this by saying that if a' and b' are 1 xn vectors, their vector sum is 
defined by 
a' + b' = (flri + Z?i,j2 + ^ 2 , . . - ,ai + bi,. .. 
,an+bn) 
As noted from Panel I of the figure, vector addition proceeds on a component-by-
component basis. Geometrically, c' = a' + b' is represented by the diagonal of a 
parallelogram determined by a' and b'. 
Panel II of Fig. 3.9 shows a case for three vectors in two dimensions, a', b' and c'. 
When a and b' are added, their sum is represented by d', the diagonal of a parallelogram. 
The parallelogram rule also applies as d' is added to c', resulting in their vector sum, 
shown by e'. 
Vector subtraction presents no major additional compHcations. Suppose, for example, 
that we wish to show the difference 
d' = a'-b' = (1-0, 2 - 1 , 2-2) = (1,1,0) 
geometrically. Figure 3.10 shows the difference vector, denoted by d', as a vector 
emanating from the origin with the same length and direction as that indicated by the line 
connecting the arrowheads of a and b'. Notice, then, that we maintain the concept of 
position vector by making a parallel displacement of the difference between a and b so 
that d' starts from the origin. 
If we had the vector a' and another vector —a', it would, of course, be the case that 
a +(-a') = 0' 
b'-a' = e'= (-1,-1,0) 
b' = (0, 1,2) 
Difference: a'-b' 
a'=(1,2, 2) 
Fig. 3.10 Illustrations of vector subtraction. 

3.3. 
GEOMETRIC REPRESENTATION OF VECTORS 
91 
Note also that the vector subtraction 
e' = b'-a' = (0-1, 1-2, 2-2) = (-1,-1,0) 
is handled analogously and, furthermore, that - l e ' = d', as should be the case. We find 
that d' and e' are merely oppositely directed vectors of equal length.^ 
Multiplication of a vector a by a scalar k is formally defined as 
k{a) = (kai,ka2,.. 
. ,kai,. . . , kan) 
and is also illustrated in Fig. 3.10 for the special case in which k = —\. That is, 
e' = -i(d') =-1(1,1,0) = (-1,-1,0) 
As a more general example. Fig. 3.11 shows the case of multiplying the vector 
a'= (2,3,2) 
by ki = -I, k2 = \y ks = 2. We note that the sign of k determines the direction of ka 
while the magnitude of k determines how far kd! extends in the appropriate direction 
from the origin, relative to ||a'||, the length of a' when k= I. 
As a concluding example we combine the operations of addition and scalar 
multipHcation of a vector by considering the case of a linear combination: 
^a'+ 2b'= Kl,2,2) +2(0,1,2) = ( i 1,1) +(0,2,4) = (i3,5) 
-y —\-
Fig. 3.11 
Illustrations of vector multiplication by a scalar. 
^ The vector e' is used here as an arbitrary vector and is not to be confused with the standard basis 
vectors e/, introduced earlier in the chapter. 

92 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
Ysa' + 2b' = {V2, 3, 5) 
Fig. 3.12 An illustration of the combined operations of scalar multiplication and addition. 
The result of these operations appears in Fig. 3.12. This same idea can, of course, be 
extended to more than two vectors. For example, in three dimensions the sum of three 
three-component vectors would be represented by the diagonal of a parallelepiped formed 
from the three contributing vectors. As long as we confine the number of components of 
each vector to at most three, it becomes quite straightforward to picture the operations 
of addition, subtraction, scalar multiplication of a vector, and their generalization, a 
linear combination of vectors. 
The properties of addition, subtraction, and multiplication of a vector by a scalar were 
listed in Chapter 2. These properties, of course, apply here since our current purpose is 
simply to portray the same vector relations geometrically rather than algebraically. 
3.3.3 
Distance and Angle between Two Vectors 
In Section 3.3.1 we considered the special case of the angle between two vectors when 
one of those vectors was a coordinate axis. We can now discuss the general situation of 
the angle between any pair of position vectors in Euclidean space. Suppose we continue 
to consider the case of the two vectors a' = (1, 2, 2) and b' = (0, 1, 2). As shown earlier, 
vector a' has length 3. Vector b' has length 
||b'||=[(0)^ + (l)^ + (2)^]i/^=V5" 

3.3. 
GEOMETRIC REPRESENTATION OF VECTORS 
93 
b'=(0, 1,2) 
a'=(1,2, 2) 
c^ = df^ ^-e^ - 2de cos 6 
Fig. 3.13 Finding the angle between the two vectors. Key: I, position vectors; II, arbitrary 
triangle. 
with direction cosines and angles, with respect to x, j^, and z, of 
cos a* = 0/\/5"= 0; 
a* = 90° 
cos /3* = l/\/5"= 0.45; 
j3* ^ 63° 
cos 7* = 2/>/5'= 0.89; 
7* = 27° 
So far, nothing new. Now we ask: What is the EucHdean distance between d! and b'? 
Again, as we know, the distance between a' and b' can be computed as 
||a'_b'||= [(1-0)^ +(2-1)^ + ( 2 - 2 ) ' ] ' / ' = V r = 1.41 
That is, we find the difference between the two vectors on a component-by-component 
basis, square each of these differences, sum the squared differences, and then take the 
square root of the result. Notice that this is similar to finding a vector's length in which 
the origin, or zero vector, plays the role of the second vector. 
Again, nothing new. However, at this point we can note from Panel I of Fig. 3.13 that 
2! and b' make some angle 6 with each other. The problem, now, is to determine what 
this angle is. That is, analogous to the case of finding the angle that a single vector makes 
with each of the reference axes, we now wish to find the angle between two different 
vectors referred to the same set of coordinate axes. To do so, we make use of the cosine 
law of trigonometry. 
As the reader may recall from basic trigonometry, the law of cosines states: 
For any triangle with sides c, d, and e, the square of any side is equal to the sum of the 
squares of the other two sides minus twice the product of the other two sides and the 

94 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
cosine of their included angle d. Or, to illustrate (see arbitrary triangle in Panel II of 
Fig. 3.13), 
c^ = d^ -^ e^-2de cos ( 
de 
Similarly, we could find J^ or e'^, as the case may be. 
Returning to our specific example in Panel I of Fig. 3.13, by simple algebra we can 
first express the law of cosines in terms of the cosine of 
cos ^a'b' = 
2||a'||-||b'|| 
where the above formula represents the particularized version of 
cos dde 
d^^e^-c^ 
2de 
as applying to any triangle of interest. 
In terms of our specific problem, the cosine of the angle d between the vectors a' and 
b' is expressed as a ratio in which the numerator is the squared length of a' plus the 
squared length of b' minus the squared length of the difference vector a'—b'; the 
denominator of the ratio is simply 2 times the product of the lengths of a' and b'. 
If we then substitute the appropriate numerical quantities, we have 
_ 
. 
lla'iP + llb'iP-lla'-b'lP 
cos ^a'b' = — 
9 + 5-2 
12 
2||a'| 
with the correspondent angle 
llb'll 
2(3)(V5) 
13.416 = 0.894 
e, 
a'b' 127° 
Notice further that we can turn this procedure around. If we know the angle that two 
vectors make with each other and their lengths, another way of finding the squared 
distance between them makes use of a rearrangement of the above formula to 
l|a'-b'||2 = lla'll^ + ||b'||^-2 cos ^a'b'Ha'll • ||b'|| 
The concepts illustrated here for two dimensions also hold true in higher dimensions 
since two noncollinear vectors will entail a (plane) triangle embedded in higher 
dimensionality.^ The vector lengths, of course, will be based on projections on all axes of 
the higher-dimensional space. 
A few other observations are of interest. First, if the angle B between two vectors is 
90°, then cos 0=0, and one has the familiar Pythagorean theorem for a right triangle in 
which the square of the hypotenuse is equal to the sum of the squares of the sides. In the 
case where cos 0=0, the two vectors are said to be orthogonal {2iS mentioned earlier). If 
cos ^a'b' ~ 1? then a' and b' are coUinear in the same direction, and the sum of the 
^ By noncollinear is meant that the vectors are not superimposed so that all points of one vector 
fall on the other vector. 

3.3. 
GEOMETRIC REPRESENTATION OF VECTORS 
95 
' 
1 
CN 
P 
^ 
£ 
II 
1 
N 
1 
.-
CN 
JD 
+ 
CN 
JO 
II 
ID 
JO 
\ 
CO 
+ 
ir> 
+ 
CM 
II 
^^ 
CN 
II 
1 
1 
^ 
1 
1 
cs 
In 
1 
1 
II 
'a 
:3 
W * . -.^^)> 
-+•^1-
XI 
CO 

96 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
squared lengths of a' and b' is appropriately reduced by 2||a'|| • || b'||. If cos ^a'b' = -1» 
then a' and b' are oppositely directed, and the sum of the squared lengths of a' and b' is 
increased by 2i|a'11 • ||b'||. 
These latter two relationships are easily seen by recalling that for scalars we have the 
identities 
(x-yy=x^ 
-^y^-2xy 
and 
[x-{-y)]^ 
= (x +yy =x^ +7^ + 2xy 
Figure 3.14 shows geometrical examples of all three of the preceding cases. 
Later on, when we discuss some of the more common measures of statistical 
association, we shall fmd that the above relationships are useful in portraying various 
statistical measures from a geometric standpoint. At this point, however, we proceed to a 
geometric description of still another concept of vector algebra, namely, the scalar 
product of two vectors and its relationship to EucUdean distance. 
3.3.4 
The Scalar Product of Two Vectors 
In Chapter 2 we defined the scalar (or inner or dot) product of two vectors a and b (of 
conformable order) as 
a'b 
in which, if a'= (j 1,^2, • . . ,flfc. • • • ,an)mdh' 
= (bu b2,. . . ,bj„ . . . ,Z7„), then 
a'b= X akbj, 
k = l 
and the result was a single number, or scalar. 
A geometrically motivated (and more general) definition of scalar product, which 
takes into consideration the angle d made between the two vectors and their respective 
lengths, can now be presented. This definition of scalar product is given by the expression 
a'b= Hall-llbll cos ^ab 
In the above example in which a' = (1, 2, 2) and b' = (0, 1, 2), we have 
ab = 3(v^)(0.894) = 6 
We also recall that a'b = b'a. The geometric counterpart of this is 
b'a=||b||-Hall cos ^ab 

3.3. 
GEOMETRIC REPRESENTATION OF VECTORS 
97 
Moreover, the counterpart to a'a, the scalar product of a vector with itself, is simply 
lla||-||a||cos^aa=lla|P-l 
This is the vector's squared length, inasmuch as the angle 9 that a vector makes with itself 
is, of course, zero; hence cos ^aa ~ 1 • 
3.3.5 
Vector Projections and Scalar Products 
Still another way of looking at the scalar product of two vectors is in terms of the 
signed length of a projection of one vector along another. At the beginning of this 
chapter, we talked informally about the projection of a vector onto the coordinate axes 
X, y, and z. Its projection on some axis, say x, was referred to as the signed distance from 
the origin, along x, to the foot of a perpendicular dropped from the vector onto x. 
Similar interpretations pertained to the vector's projections on Sixesy and z. 
However, suppose we have two arbitrary position vectors in the space. Clearly, we 
could consider the projection of one vector onto the other, in a fashion analogous to 
coordinate projections. This concept is most simply described in two dimensions. 
Accordingly, let us select two arbitrary vectors 
a'= (1,2) 
and 
b' = (0,2) 
These vectors are shown in Panel I of Fig. 3.15. We now project b' onto a' by dropping a 
perpendicular from b''s terminus to a'. The number 
\\hp'\\ = llb'llcos^a'b' 
a'b 1 
lla'll 
1 
. 1 , 
1 
-
—
, 
i 
is defined as the length'^ of the projection of the vector h' along the vector aV The length 
of the projection is also frequently called the component of b' along a'. 
This concept is most easily understood by first recalling that the cosine can be viewed 
in terms of the length of the projection of a unit length vector, in this case one in the 
direction of b', onto the adjacent side (vector a') of a right triangle. Here, the unit length 
is multiplied by ||b1|. In this example l|b'|| = [(0)^ + {2fY'^ = 2. 
The cosine ^a'b' is next found from the cosine law: 
cos 0a'b' = lla'lP + llb'lP-lla'-b'll^ 
2||a'||-||b'|| 
5 + 4-1 
2(V5)(V4) = 0.89 
t^a'b' 
•2T 
"^ Note that in defining ||bp'|| we use the absolute value of the expression ||b'll cos 0a'b' since 
lengths are taken to be nonnegative. However, the signed distance is in the direction of a' if cosa'b' is 
positive (i.e., the angle d^'\y' is acute) and in the direction of - a ' if cos e^'b' is negative (i.e., the angle 
^a'b' is obtuse). 

98 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
b' = (0,2) 
a' = (1,2) 
H — X 
4 — X 
III 
IV 
d'= (-1,0.5) 
c'= (1, 1) 
Projection 
onto plane 
Fig. 3.15 
Geometric interpretation of vector projection. 
Panel I of Fig. 3.15 shows the projection vector bp along the direction of a'. We note 
that b' makes an angle of 27"^ with a'. First, let us consider the projection vector hp, and 
then let us consider its length. The projection vector is found by the formula 
bp' = "llb'llcoseav 
lla'll 
a 
In terms of the problem, we have 
v= 
2(0.89)" 
V5 
(1,2) = (0.8,1.6) 
Panel II of Fig. 3.15 shows the coordinates of bp' = (0.8,1.6). Since the angle flj'b' = 27° 
is acute, bp' is in the same direction as a'. 

3.3. 
GEOMETRIC REPRESENTATION OF VECTORS 
99 
The length of hJ is given by 
Ibp'|| = llb'llcos^aV = 2(0.89)= 1.78 
and this also appears in Panel II of Fig. 3.15. 
Should we desire the length of the projection of a along b, this is obtained analogously 
as 
l|a/|| = ||a'||cos(9a'b' 
V5(0.89) = 2 
Notice that if a' and b' are each of unit length, the length of the projection of b' along a' 
(or a' along b') is simply |cos B a'^'l-
If the two vectors should make an obtuse angle with each other, the procedure remains 
the same, but the direction of the projection vector is opposite to that of the reference 
vector. Panel III of Fig. 3.15 shows a case in which c' = (1, 1) and d' = (-1, 0.5) make an 
angle of 108° with each other. The cosine of this angle is -0.316, and we have 
id;ii= Id'llcos^c'd' 
1.12(-0.316) = 0.35 
as shown in Panel III. However, since cos ^c'd' is negative, the direction of dp is opposite 
to that of c'. 
The idea of (orthogonal) projection can, of course, be extended to the projection of a 
vector in three dimensions into a subspace, such as the xy plane in Panel IV of Fig. 3.15. 
For example, the vector g can be projected into the xy plane by dropping a perpendicular 
from the terminus of g to the xy plane. The distance between the foot of the projection 
(represented by the terminus of g*) and g must be the minimum distance between g and 
the xy plane. Any other vector in the xy plane, such as h, must have a terminus that is 
farther away from g since the hypotenuse of a right triangle must be longer than either 
side. Subspace projections are discussed later (in Section 4.6.4). 
All of this discussion can be straightforwardly related to the geometric aspects of a 
scalar product. The scalar product a'b was earlier defined in general terms as 
a'b = cos0ablla||-||b|| 
which can now be expressed in absolute-value terms as the product of two scalars: 
|a'b|=||bp'||-||a'||=1.78(V5) = 4 
Furthermore, the preceding definition of projection length is fully consistent with an 
informal description presented at the beginning of the chapter. For example, if we have 
the vector a' = (1, 2), its projection lengths onto the standard basis vectors ei' and 62' are 
found as follows: 
api 
a c i 
ei = 
llallcOS^a'e; 
lle'ill 
(l,2y(l,0) 
1 
(1,0) = (1,0) 

100 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
It foDows that 
a;ill=l 
acj 
lle^ll-lle^l 
62 
(1,2)'(0,1) (0,1) = (0,2) 
and 
13^2 II = 2 
Incidentally, we shall always take ^^'b' to be the smaller angle between a' and b'. If 
the vectors are oppositely directed, the direction of the projection will be the negative of 
the reference vector's direction since cos ^a'b' will be negative, as illustrated in Panel III 
of Fig. 3.15. 
3.3.6 
Recapitulation 
At this point we have provided geometric interpretations of all the various algebraic 
operations on vectors that were illustrated in Chapter 2. In particular, the addition of two 
vectors foUowed a parallelogram rule, as illustrated in Fig. 3.9. Subtraction of two vectors 
also involved a parallelogram rule, in which the difference vector was displaced so as to 
start at the origin; this is shown in Fig. 3.10. 
Multiphcation of a vector by a scalar k involves stretching the vector if /: > 1 and 
compressing it if 0 < A: < 1. These cases are illustrated in Fig. 3.11. If /:= 1, the vector 
remains unchanged. \i k = 0 the vector becomes 0, the zero vector. If k is negative, the 
vector is stretched and oppositely directed if \k\ > 1 and compressed and oppositely 
directed if 0 < l/tl < 1, as shown in Fig. 3.11. 
The operations of sum and difference between two (or more) vectors and 
multiphcation of a vector by a scalar are summarized in terms of the concept of linear 
combination, as illustrated in Fig. 3.12. 
The definition of a EucUdean space enabled us to consider the distance and angle 
between two vectors. By means of the cosine law, illustrated in Fig. 3.13, the cosine of 
the angle formed by two vectors and their lengths were related to the (squared) Euclidean 
distance between them. This concept, in tum, led to the geometric portrayal of the 
projection of one vector onto another, as illustrated in Fig. 3.15. From here it was a short 
step toward portraying the scalar product of two vectors as a signed distance involving the 
product of the component (projection length) of one vector along some reference vector 
and the reference vector's length. In short, all of the algebraic operations of Chapter 2 
involving vectors were given geometric interpretations here. 
We can summarize the various formulas involving aspects of the scalar product as 
follows: 
1. 
||a-b||^ = llall^ + l|b|p-2||a|| • ||b||cos ^ab = l|a||H ||b||^-2[a'b] 
lla||^ + ||b||2-||a-b||^ 
a'b 
2. 
cos ^ab = 
2||a||-||b|| 
llall-llbll 

3.4. 
LINEAR DEPENDENCE OF VECTORS 
101 
3. 
Ilbpll 
lUplI 
la'b 
lllall 
a'b 
; lib II 
_ 
Ibllcos^ab 
|a||cose/ab 
It is worth noting that the scalar product plays a central role in all of these formulas. 
3.4 
LINEAR DEPENDENCE OF VECTORS 
In the beginning of the chapter we chose a set of reference vectors e/, called standard 
basis vectors, that in three dimensions were defined as follows: 
e/ = (1,0,0); 
62'= (0,1,0); 
63'= (0,0,1) 
As we shall see in a moment this set of vectors is linearly independent. The concept of 
linear independence plays a major role in vector algebra and multivariate analysis. As we 
know from elementary geometry, a line is one-dimensional, an area is two-dimensional, 
and a volume is three-dimensional. By analogy, a space of n dimensions entails 
"hypervolume." 
Lx)osely speaking, linear independence of vectors has to do with the minimum number 
of vectors in terms of which any given vector in the space can be expressed and, in effect, 
is related to the "volume" of the space spanned by the vectors. Linearly dependent 
vectors display a kind of redundancy or superfluity in the sense that at least one vector of 
a linearly dependent set can be written as a linear combination of the other vectors. 
Somewhat more formally, if a/, a2', . . . , ap' denote a set of p vectors and ki, k2, 
. . . , kp denote a set of p scalars, it may be the case that the following linear equation is 
satisfied: 
ki2ii 
+ k2SL2 + ' ' ' + kpSip 
=0' 
where O' is the zero vector. 
For example, if ki = k2 = - . . = kp =0, any set of p vectors trivially satisfies the above 
equation. If, however, the equation can be satisfied without all kf being equal to zero, the 
solution is called "nontrivial." 
// a nontrivial solution can he found, then we say that the set of vectors is linearly 
dependent. If only the trivial solution is satisfied, the set of vectors is said to be linearly 
independent. 

102 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
To illustrate the case of nontrivial satisfaction of the above equation, let us assume 
three four-component vectors: 
and let 
Since 
a/ = (1,2,0,4); 
a^' = (-1,0,5,1); 
33' = (1,6,10,14) 
3(l,2,0,4) + 2(-l,0,5,l)-l(l,6,10,14) 
= (3,6,0,12)+ (-2,0,10,2)+ (-1,-6,-10,-14) = (0,0,0,0) 
it is seen that aj', ai', and 33' are linearly dependent and at least one of the vectors is a 
linear combination of the remaining p — 1 vectors. To see that this is so, we note that 
ai 
= - ! ( -
= (io. 
(az')-
1,0,5, 
10 
, ! ) • » 
i) + 
(as') 
-Kl,6,10,14) 
(i2,f,^) 
ai' = (l,2,0,4) 
and ai' is, indeed, a linear combination of 32' and aa'. It is also pertinent to note that any 
set of p vectors is always linearly dependent if p> n, where n is the number of vector 
components in an « by 1 column vector or a 1 by w row vector, as the case may be. 
While no proof of this assertion is given, the statement relates to the fact that if one 
v^shed to solve n equations for n unknowns, one could take the first n vectors, assuming 
they are linearly independent, and solve for any of the other vectors as linear 
combinations of these n linearly independent vectors. 
The concept of linear independence is of particular importance to multivariate 
analysis. A set of Hnearly independent vectors is said to span some Euclidean space of 
interest. Ultimately the idea of linear independence relates to the dimensionality of the 
space in which the researcher is working. And, as we shall see, once a set of such vectors is 
found, all other vectors can be expressed as linear combinations of these. 
In brief, then, two ideas are involved in the study of linear independence. First, we 
wish to find a set of nonredundant vectors. Second, we wish to make sure that we have 
enough linearly independent vectors to span some space of interest or, as indicated 
earlier, to contain some hypervolume of interest. 
3.4.1 
Dimensionality of a Vector Space and the Concept of Basis 
In line with our earlier discussions involving geometric analogy, we can now examine 
the dimensionality of a vector space. The dimensionality of a vector space is equal to the 
maximum number of linearly independent vectors in that space. To illustrate for the case 
of three dimensions, we return to the e/ standard coordinate vectors: 
e / = (1,0,0); 
62' = (0,1,0); 
63'= (0,0,1) 

3.4. 
LINEAR DEPENDENCE OF VECTORS 
103 
If we set up the equation 
ki^i 
+ k2^2' + ^3^3' = 0 
we find that the above equation is satisfied only if ki=k2=k^= 
0. Hence, Ci', 62', and 
63' are linearly independent, and the dimensionality of the space is three. In general, if 
a/, 82', . .. , a„' denote a set of n linearly independent n-component vectors, then any 
other vector of that n-space can be written as 
^' ~ ^ 1 ^ / + ^^2^2 + * * • + ^na„' 
and the2Li\ 2i2,.. . y 2Ln vectors are said to constitute a basis of the n-space. In a space of 
n dimensions, any set of n linearly independent vectors can constitute a basis of the 
space. Thus the basis vectors e/ above represent only one type of basis, one that we have 
called the standard basis. 
The e/ standard basis vectors, however, are particularly convenient. Indeed, unless 
stated otherwise we shall assume that the particular basis being chosen is the standard 
basis. Still, we should indicate that any other set of n linearly independent vectors could 
qualify as the reference set. Accordingly, we spend some time on the process by which 
one can change one set of basis vectors to some other set, for example, to a set of 
standard basis vectors. 
3.4.2 
Change of Basis Vectors 
Up to this point we have emphasized rectangular Cartesian coordinates, where it is 
natural to view the coordinate vectors e/ as both (a) mutually orthogonal (i.e., exhibiting 
pairwise scalar products of zero) and (b) of unit length. This type of basis is called 
orthonormal. In this intuitively simple case, the vector a' = (^i, ^ 2 , . . . , fl^n) can be easily 
written as 
a' =fifiei' + fl2e2' + • • "^^n^n 
where 
Ci'= (1, 0 , . . . , 0), 62'= (0, 1, 0,.. ., 0), and 
e„'=(0, 0,. . . , 1). Hence 
{ai, a2,. .. ,an) 
are the coordinates of a' relative to the orthonormal basis 
61 ,62 , . . . , e„ . 
An equally satisfactory way of showing this concept is to represent the standard 
coordinate vectors in columnar form. A given vector a can then be written as 
3 = ^1 
[l^ 
0 
0 
0 
+ C2 
"0" 
1 
0 
0_ 
+ ••• + «« 
ro~ 
0 
0 
1 
— 
« i 
^2 
as 
^^nJ 
and we have an illustration of a linear combination of standard basis vectors in which the 
components of a (i.e., ai, a2, etc.) are the scalars of interest. 

104 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
Orthonormal bases are easy to work with, and we shall usually assume that this type of 
basis, more specifically, the standard basis vectors e,-, underlies the coordinate 
representation of interest. However, as indicated above, any set of linearly independent 
vectors, unit length or not, orthogonal or not, can be used to define a basis. Hence, it is 
pertinent to point out how one can move from one basis of a space to some other basis of 
that space. 
Accordingly, let us now illustrate the idea of general coordinate systems whose basis 
vectors need not be mutually orthogonal or of unit length. Suppose we start with two sets 
of basis vectors—first, the more familiar e/ standard basis vectors, e / = (1, 0) and 
€2' = (0, 1), and second, another set of basis vectors f/ =Ciei' + ^262' and f2' = ^ 1 ^ / + 
^262'. 
To be specific, we let Ci = 0.707, C2 = 0.707, di = 0.940, and d2 = 0.342. Then 
f/ = 0.707ei' + 0.707e2' = (0.707,0.707) 
f2' = 0.940ei' + 0.34262' = (0.940,0.342) 
Note that f/ and f2' are each of unit length but are not orthogonal; that is, fi'f2 ^ 0. 
Note further that we can write the preceding equations in columnar form as 
fi 
f, = 
/n" 
/22_ 
= 0.707 
ei 
_0_ 
+ 0.707 
62 
"0" 
_1_ 
= 
"0.707 
_0.707 
ei 
62 
= 0.940 "r 
_0_ 
+ 0.342 
"0" 
_1_ 
= 
"0.940 
_0.342 
Figure 3.16 shows a plot of fi and £2 relative to the standard basis ei and 62- By 
finding their projections on ei and 62, we can note that their coordinates are given by the 
preceding equations. 
Now let us select a new vector a = aifi +^2^2- That is, we shall assume that a is 
referred to the new (and nonorthogonal) basis, f 1 and £2. To be specific, we assume that 
the coordinates of a relative to f 1 and £2 are 
^1=0.5; 
^2=0.5 
We can find these coordinates by extending Unes parallel to f 1 and f2 and noting the 
coordinates of OQ and OR, respectively, on fi and £2- The basis vectors fi and £2 are 
often called oblique Cartesian axes since the angle that they make with each other is not 
equal to 90"^. Notice, however, that a is still given by the (parallelogram) law for vector 
addition: 
2L = aifi 
+^2^2 
and that ^ifi and ^2^2 are scalar multiples of fi and £2, respectively. Since we have 
chosen f 1 and f2 to be of unit length, the coordinates ai and ^2 are merely the lengths 
OQ and OR. Had fi and £2 not been of unit length, ai and ^2 would still be regarded as 

3.4. 
LINEAR DEPENDENCE OF VECTORS 
105 
1 + 
^-e1 
O 
1 
Fig. 3.16 
An illustration of generalized coordinates and change of basis. 
coordinates, but they would correspond to lengths of OQ/||fill and 0/?/||f2ll, 
respectively. 
We now seek a set of coordinates for the vector a, referred to the oblique basis f i and 
£2, in terms of the original (and standard) basis ei and 62. This can be done by the 
following substitution: 
a = flrifi +^2^2 
But, since f 1 and £2 have been defined in terms of Ci and 62, we can write 
a = ^iCciCi + 0262) + ^^2(^1^1 + d2^2) 
= (ci^i + (^1^2)^1 + {c2(i\ + ^2^2)^2 
However, since ei and 62 denote a basis, we can also represent a in terms of ei and 62 as 
a = ^i*ei +^2*^2 
Hence, through substitution of Ci^i + c?i^2 for flfi *, and C2flf2 + <^2^2 for ^2 *, we find 
^1* = c^ai + d^a2 = 0.707(0.5) + 0.940(0.5) = 0.82 
^2* = ^2^1 + <^2^2 = 0.707(0.5) + 0.342(0.5) = 0.52 
As can be observed from Fig. 3.16, the length of the projection of a on Ci is, indeed, 
0.82, and its projection length on 62 is 0.52. 
Thus, one can work "backward" to relate a vector described in terms of one set of 
basis vectors to a description of that same vector in terms of another set of basis vectors, 
assuming we know how the basis vectors themselves are connected. And, as a matter of 
fact, one can always fmd an orthonormal set of axes (mutually orthogonal and of unit 
length) by which a set of arbitrary basis vectors can be represented, even though the 
original axes might be oblique and not of unit length. The next section illustrates one 

106 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
procedure for finding an orthonormal basis from an initial set of nonorthonormal basis 
vectors. 
3.4.3 
Finding an Orthonormal Basis 
As indicated earlier, a special kind of basis in a vector space-one of particular value in 
multivariate analysis—is an orthonormal basis. This basis is characterized by the facts that 
(a) the scalar product of any pair of basis vectors is zero and (b) each basis vector is of 
unit length. As we know, the standard basis vectors e^ represent one such orthonormal 
basis. 
In multivariate data analysis, it is usually the case that multiple measurements on a set 
of objects will be associated; for example, weight will be correlated with height. 
Sometimes we may want to transform the original (and correlated) variables to a set of 
uncorrected variables. As will be shown later, this process can be viewed as transforming 
a set of n nonorthogonal vectors into a set ofn orthogonal vectors. In the process we may 
also want to make all of these vectors unit length; this is the "norming" aspect of the 
process. 
We have already observed that the scalar product is a central concept in vector algebra 
and is a function that assigns a real number to each pair of vectors in the Euclidean space 
of interest. In particular, the concepts of vector length, distance, and cosine can all be 
expressed in terms of the single idea of a scalar product: 
llall = [a'a]"^ 
||a-b||=[|la||^ + ||b|P-
^•"' ""^ 
llalMlbll 
-2(a'b)]"^ 
As we shall see in a moment, the scalar product also provides a simple representation 
of vectors that are mutually orthogonal (perpendicular): 
a and b are orthogonal 
if and only if 
a'b = 0 
We can now proceed to construct an orthonormal basis, one whose vectors are mutually 
orthogonal and of unit length. 
Any arbitrary basis can be transformed to an orthonormal basis by a procedure known 
as Gram-Schmidt orthonormalization. To illustrate the process, consider the three 
arbitrary row vectors: 
a,'= (2,1,2); 
a2' = (3,-1,5); 
a3' = (0,1,-1) 

3.4. 
LINEAR DEPENDENCE OF VECTORS 
107 
The Gram-Schmidt process starts out by selecting (arbitrarily) one of the vectors, say ai', 
as the first reference vector.* The idea here is to keep this vector fixed and then find 
other vectors, two other vectors in this case, so that the resultant sets are mutually 
orthogonal. As a final step each of the orthogonal vectors is normalized to unit length. To 
start off the process we first set 
b
t 
I 
1 =ai 
and then find 
b
t 
t 
2 ^ ^2 " 
32'bi 
b j ' = a 2 ' -
(3x2)+ ( - 1 x 1 ) + (5x2) 
2^ + 1^ + 2^ 
b / 
= (3,-l,5)-(15/9)(2,l,2) = (-l/3,-8/3,5/3) 
b2' = (-0.33,-2.67,1.67) 
Let us now examine the expression 
aa'bi 
bi'b, b / = (15/9)(2,1,2) = (10/3,5/3,10/3) 
This expression is the orthogonal projection of ^2 
onto bj' (as discussed in 
Section 3.3.5). 
The "residual" is then equal to the difference 
32'bi 
b2' = 82'-
bi'b, b,' = (3,-l,5)-(10/3,5/3,10/3) 
= (-1/3,-8/3,5/3) 
and should be orthogonal to bi', as is shown illustratively in two dimensions, in Fig. 3.17. 
That is, 
b2'bi =(-1/3,-8/3,5/3)1 1 1 = 0 
2 
bo = 80 
bi = a2p 
Fig. 3.17 
Finding the orthogonal projection of aj'onto b / (illustrated in two dimensions). 
* It should be mentioned that the specific results of the Gram-Schmidt process depend on the 
order on which the vectors are selected; however, in any case the resulting set will be orthogonal and 
of unit length. 

108 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
We encountered orthogonal projections, both in the beginning of the chapter and in 
Section 3.3.5. Accordingly, there is nothing new here, except for the fact that we are now 
interested in the orthogonal complement or that part of the di^ vector that does not lie 
along the reference vector. In this case it is the vector 
b2' = (-0.33,-2.67,1.67) 
As we noted above, the scalar product of b2'bi is indeed zero. Thus, b2 is now 
orthogonal to bi' = ai'. We now have to orthogonaHze 33' with regard to the two, already 
orthogonal, vectors b2' and bi': 
, , 
, r^^'bi], , I'aa'bi 
= (0, 1, -1) + ^ (-1/3, -8/3, 5/3) + ^(2, 1, 2) 
= (0,1,-1) + (-13/90,-104/90,65/90) + (2/9,1/9,2/9) 
= (7/90,-4/90,-5/90) 
b3' = (0.08,-0.04,-0.06) 
And, in general for r vectors, we would have 
a^ b^-i 
bj._ib^_ 
br-r 
a/bi^ 
b/bi W 
After the b''s are obtained, we would find that they are mutually orthogonal. Each set is 
then normalized by its respective divisor ||b/'||. That is, we find the length of each of the 
b''s and divide each vector component by the length of that vector. In the above example, 
the lengths of b / , b2', and ba', respectively, are 3, 3.17, and 0.108. The normalized 
vectors then become 
bf' = (l/3)(2,1,2) = (0.67,0.33,0.67) 
b f = (l/3.17)(-0.33,-2.67,1.67) = (-0.10, -0.84,0.53) 
b j ' = (1/0.108)(0.08, -0.04, -0.06) = (0.74, -0.37, -0.56) 
Within rounding error, we first note that all three vectors have unit length. If we then find 
the scalar product of each pair of vectors, we observe, again within rounding error, that 
all three scalar products equal zero. 
We conclude by saying that the vectors bf', b*', b*' form a three-dimensional 
orthonormal basis—one whose axes are mutually orthogonal and of unit length. 
It is rather difficult to show the Gram-Schmidt procedure for the specific vectors 
utilized in our example. This being the case, Fig. 3.18 shows a more stylized 
conceptualization of the procedure. The pictures first show orthonormalization of the 
first two vectors in two dimensions and then orthonormalization of all three in three 
dimensions. (In the figure the orthonormalized vectors are expressed as column vectors.) 

3.4. 
LINEAR DEPENDENCE OF VECTORS 
Starting vectors 
First two vectors 
109 
a 
ai 
2 
^ 
Before 
After 
Third vector 
Before 
After 
Fig. 3.18 
Conceptualization of Gram-Schmidt orthonormalization procedure. 
3.4.4 
Scalar Products in Oblique Coordinate Systems 
Now that we have discussed both oblique and orthonormal bases, it is of interest to 
point out that vector addition and subtraction as well as multiplication of a vector by a 
scalar are carried out the same way under either oblique or orthonormal basis conditions. 
However, this correspondence does not hold in the case of the scalar product. 
The reason why the usual scalar product formula does not work in the nonorthogonal 
case is most easily seen by observing the two basis vectors a and b in the diagram of 
Fig. 3.19. Note that the angle they make with each other is 45° rather than 90°. If we 
Fig. 3.19 
Scalar product in oblique coordinate system. 

no 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
(incorrectly) assumed that these basis vectors were orthogonal, their scalar product would 
be zero by application of the special case of a scalar product that entails a'b.^ 
However, using the formula described in Section 3.3.4, we find the following: 
a'b = Hall • llbll cos l9ab = 1(1)(0.707) = 0.707 
As noted above, this form of computing the scalar product is not dependent on referring 
vectors to an orthonormal basis. Still, as can be observed from this section, orthonormal 
bases simplify calculations quite a lot. 
In more general terms, if we have two column vectors a and b referred to obUque unit 
length basis vectors f^-, this situation can be represented as 
a = aifi +^2^2 + • • • + ^«f« 
and 
h = bifi 
+Z?2f2 + - • 
'-^bJn 
In this case the scalar product between a and b is given by 
a 'b = 
n 
I 
n 
7 = 1 
^1 bj cos % 
where Sij is the angle between the pair of basis vectors f/ and fj for /, / = 1, 2,. . . , « . 
However, if fi, f2,. .. , f„ are also orthogonal, cos Oij = 0 for all pairs of basis vectors in 
which/¥= / and, hence, we have the special case, discussed earlier, of 
n 
a'b = X ciibi 
What if the oblique f/ and fy are not of unit length? If this is the case, then the scalar 
product becomes 
a'b = 
n 
X 
/ = 1 
n 
ajbj- cos %l|f,ll llf/ll 
Thus, if we need to account for basis vectors whose lengths are not equal to unity, the 
more general expression above is applicable.^^ 
^ It is important to note that the definition of scalar product as a'b = E/Li c/Z?/ in Chapter 2 has 
implicitly assumed that both vectors are referred to standard basis vectors ej. In general, this will 
indeed be the case; however, in oblique coordinate systems the geometrically oriented definition 
a'b = Hall • llbll cos 0abshould be used. 
*° The computation of scalar products in an oblique coordinate system entails the concept of a 
(positive definite) quadratic form, a topic that is discussed in Chapters. Throughout the book, 
however, we shall emphasize the simpler case in which the standard basis vectors e/ are assumed to be 
applicable. 

3.5. 
ORTHOGONAL TRANSFORMATIONS 
111 
3.5 
ORTHOGONAL TRANSFORMATIONS 
Up to this point we have presented geometric interpretations of all of the principal 
algebraic operations that were performed on vectors in Chapter 2, such as addition, 
subtraction, the scalar product of two vectors, and so on. So far, matrices have been 
ignored for the most part, except in our discussion of basis vectors. 
It is now appropriate to discuss some preliminary aspects of a matrix transformation 
of a vector or set of vectors. In this chapter we limit our discussion to a special but quite 
important case, namely, orthogonal transformations or rotations. 
Most readers probably have an intuitive idea about what is meant by a rigid rotation of 
a set of points. Often in multivariate analysis we wish to perform a transformation on a 
set of points that will preserve their angles, lengths, and interpoint distances, while at the 
same time referring them to a new, perhaps simpler, coordinate system. Since rotations, 
as a special type of linear transformation, play such a key role in understanding more 
general kinds of matrix transformations, this concept is introduced here and related to 
earlier discussions of distance and angle. 
The definition of an orthogonal matrix as used in multivariate analysis differs 
somewhat from researcher to researcher. We shall use the term "orthogonal" to refer to a 
square matrix A that exhibits the property 
A'A = AA = I 
That is, any two column vectors or any two row vectors in the matrix A are mutually 
orthogonal and, furthermore, each vector is of unit length. Some authors call this type of 
matrix "square, orthonormal," but we shall use the more common term of orthogonal 
matrix. 
3.5.1 
Axis and Point Rotations 
To motivate the discussion let us consider the column vector a = [ i ] in the diagram of 
Fig. 3.20. We adopt a set of standard basis vectors e^ for the space in order to simpHfy 
Fig. 3.20 
Rotation of reference axes. 

112 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
subsequent discussion. Now suppose we wish to apply an orthogonal transformation to 
the vector a. Geometrically, this can mean one of two things: 
1. we can rigidly rotate the axes, either counterclockwise or clockwise, from their 
original e/ orientation, while leaving the point fixed, or 
2. we can leave the original e^- axes fixed and rigidly rotate the vector a = [i] to a 
new location. 
Let us consider the first case. Suppose we wish to rotate the original axes Ci and 62 
counterclockwise through an angle of 30°, as shown in Fig. 3.20. To do this we shall need 
a set of direction cosines for each angle made by the (new) f 1 and f2 axes with the 
(original) ei and 62 axes. Let us first find the cosines that we shall need. From basic 
trigonometry we have 
cos 30° = V3/2 = 0.867; 
cos 60° = ^ = 0.5; 
cos 120° =-^ = -0.5 
Next, we shall use the symbol ^^y to denote angles between pairs of axes, where i denotes 
the original axis and / denotes the new axis. If we examine the four angles ^u, ^12, O21, 
622, in which the first subscript refers to the old axis and the second to the new axis in 
Fig. 3.20, we see that 
1. ^11 involves a 30° counterclockwise rotation with cos 30° = 0.867. 
2. 
612 involves a 120° counterclockwise rotation with cos 120° = —0.5. 
3. 
^21 involves a 60° clockwise rotation with cos 60° =0.5. 
4. 
622 involves a 30° counterclockwise rotation with cos 30° = 0.867. 
The angle of 30° that fi makes with Ci involves a cosine that is equal to 0.867. And, 
since fi makes an angle of 60° (with a cosine of 0.5) with 62, we have the linear 
combination 
fi = cos ^iiCi + cos ^21^2 = 0.867 r 
_0J 
+ 0.5 
0' 
_i_ 
_ "0.867 
_0.5 _ 
r 
_0_ 
+ 0.867 
0" 
_1_ 
_ "-0.5 
" 
0.867_ 
as the coordinates of f 1. 
Similarly, we can compute the coordinates of £2 as follows: 
f2 = cos ^1261 + cos ^22^2 = ""^-^ 
As can be seen from Fig. 3.20, fi and f2 display the coordinates indicated above. We also 
note that the sum of squares of each set of direction cosines is unity. That is, 
(0.867)^ + (0.5)^ = 1; 
(-0.5)^ + (0.867)^ = 1 
At this point we have expressed f 1 and £2 in terms of Ci and 62. We also know that the 
(assumed fixed) point a is expressed in terms of Ci and 62, the original basis vectors, as 
a = 2 ~r 
_o_ 
+1 
0" 
_i_ 
_ ~2 
_1_ 

3.5. 
ORTHOGONAL TRANSFORMATIONS 
113 
Our problem, now, is to find the coordinates of that same point—which we can call a*-in 
terms of the new basis vectors f^-. 
We can express this transformation in the form of a matrix postmultiplied by a vector. 
That is, we can let a* = [^' * ] denote the new coordinates of the point a by the following 
substitution: 
"^1*1 
_^2*J 
2.23" 
-0.13_ 
_ 
fcos ^11 
[cos ^12 
' 0.867 
_-0.5 
cos 621I 
cos ^22j 
0.5 
] 
O.867J 
r^i 
L^2 
[2" 
Ll_ 
These are the coordinates of the point with respect to the new basis f/ in Fig. 3.20. 
Let us examine the transformation somewhat more closely in Fig. 3.20. First, as noted 
above, we see that the unit length portion of fi has coordinates of fj = [g;|^''] with respect 
to ei and 62, respectively. Similarly, the unit length portion of £2 has coordinates of 
^2 = [~o!867] w^t^ respect to ei and 62, respectively. 
However, we can tum the coin over and look at the coordinates of e^- in terms of the 
new axes f^-. If we project Ci and 62 onto fi and f2, we have, from Fig. 3.20, 
gi = 
0.867 
-0.5 
g2 
0.5 
0.867 
where we use gi and g2 to denote the fact that the reference vectors are now the f/. Since 
a has been defined originally in terms of e^, and the e^- have now been represented in terms 
of f/, we have 
a = 2gi + lg2 
0.867 
-0.5 
+ 1 
"0.5 
_0.867_ 
2.23 
_-0.13_ 
a = 2 
L~^-^ 
J 
LU.50/J 
L~^-^-=^-
But, as already shown, this can also be written as 
r^i*j 
fcos ^11 cos ^21"! r^i 
^1 
n * I 
.^2 J 
cos ^12 
cos ^22 
a2 
2.23 
-0.13 
0.867 
0.5 
-0.5 
0.867 
Thus, while the point remains fixed, its coordinates are determined by the particular basis 
by which they are expressed. 
In summary, we see that the new coordinates of the original point a= [1] are now 
^1*= 2.23 and ^2*= —0.13 in terms of the fi and £2 axes. However, there is a second way 
of looking at this transformation. That is, we can make the original Ci, 62 plane remain 
the same and assume that it is the point [1 ] which moves from its old location to the 
position [-o.'ia] • This second way of interpreting things is shown in Fig. 3.21. Notice in 

114 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
Fig. 3.21 
Rotation of point with basis vectors fixed. 
this case that the point [i] is rotated clockwise 30°.^^ Either interpretation is equally 
suitable. The one that is selected will depend on the researcher's purpose since it is only 
relative motion that is indicated. 
In Chapter 4 we shall explore basis vector and coordinate transformations much more 
thoroughly. At this point, however, we wish merely to show that there are two 
compatible ways of looking at things: 
1. One can rotate the basis vectors and refer the unchanged point to the new 
reference axes. 
2. 
One can rotate the point and refer its new location to the original reference axes. 
In each case, under rigid rotations we should note that angles and distances are preserved. 
Finally, we could simplify the angular representation of the preceding rotation—in the 
special case of two dimensions—by means of a single angle of rotation. 
If we let ^ = 011, we can note the following: 
cos Byi 
cos Qi2 
cos ^21 
cos ^22 
cos^ 
sin ^ 
-sin "^ cos ^ 
It is instructive to see how the rotation of a set of basis vectors through the single angle ^ 
(in the case of two dimensions) leads to the matrix above. 
3.5.2 
The Trigonometry of Rotation 
The trigonometry of rotation can be shown fairly straightforwardly. Panel I of 
Fig. 3.22 shows a point A with original coordinates a\ and ^2 ^^ ^he Ci, 62 basis. If the 
" As will be discussed in more detail in Chapter 4, the clockwise rotation of a = [ j ] can also be 
represented by the product 
2.23 
-0.13 
0.867 
-0.5 
0.5 
0.867 
where [?] and [_J:"] are both expressed in terms of the e/ basis vectors. 

3.S. 
ORTHOGONAL TRANSFORMATIONS 
115 
i 
1 
1 
1 
^r 
32 
\ 1 
\ 1 
\l 
M 
I 
^^ 
^ -
1 
A^, 
\ \ 
\ 
^2* 
\ \ 
\ \ 
A J^ 
, ^ ^ 
< ^ i 
- ^ - e 
O 
ai 
/ 
Fig. 3.22 
A trigonometric demonstration of basis vector rotation. 
original axes are rotated counterclockwise through the angle ^ , we obtain a^* and ^2* as 
the coordinates of ^ in the fi, i^ basis. We now ask: How can ^i* and ^2* be expressed 
in terms of the old coordinates a\ and a-^p. 
The trigonometric argument is simple to describe. Panel II shows the construction of 
the rectangle IJKL. Angle OHl is the complement of the angle ^ and, in tum, equals 
angle AHK. Hence, angle HAK is equal to >I^, the angle of rotation. Given these facts, we 
can now say 
ai"" = 0K-- OH^HK= OL ^rIJ = 01 cos ^+>4/sin >I^ = fliCos ^I^+ ^2 sin ^ 
a2*=AK=AJ-JK=AJ-IL=AIcos^-OIsm'i^=a2 
cos>I^-ai sin^ 
The coordinates of ^ in terms of the new basis vectors fi, f2 are then given by 
ai* = ai cos>I^ +a2 sin>I^; 
^2 * = —^1 sin ^ + flf2 cos ^ 
or, in matrix form. 
ai' 
cos ^ 
sin ^ 
1^2""] 
L~sin ^ 
cos ^ J L«2_ 
as desired. 
It should be remembered, however, that expressing a basis vector rotation in terms of a 
single angle >I^ is restricted to two dimensions. On the other hand, the more cumbersome 
notation involving four angles 
^ 1 1 i 
^12? 
^21? 
^ 2 2 
is more general, since the concept of direction cosines generalizes to three or more 
dimensions. Thus, any time that we work with rotations involving three or more 
dimensions we shall assume that direction cosines are involved throughout. 
3.5.3 
Higher-Dimensional Rotations 
What has been illustrated above for the case of two-dimensional rotations can be 
extended to three or more dimensions by using the appropriate matrix of direction 

116 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
Fig. 3.23 
Rotation in three dimensions. 
cosines. Figure 3.23 portrays, in general form, a counterclockwise rotation from the e/ 
basis to an f^- basis. The coordinates of some point a in the original basis can be expressed 
as a* in the new basis by 
^ 1 * 
^ 2 * 
J^\ 
= 
= 
= 
COS Oil 
cos ^21 cos ^31 
COS 6i2 
COS ^22 COS 6^2 
COS ^13 COS ^23 COS ^33 
Fig. 3.23 shows the angles that are considered in this more complex case. However, no 
new principles are involved. 
Orthogonal matrices play a central role in various multivariate procedures, and their 
special properties should be noted; these are taken up next. 
3.5.4 
Properties of an Orthogonal Matrix 
Now that we have illustrated what goes on when a point, or points, are subjected to a 
rotation, let us examine some of the properties of the transformation matrix used in 
Section 3.5.1. We have 
A'A = 
AA' = 
k. — 
cos Oil 
cos ^21 
cos 612 
COS ^22. 
0.867 
0.5 
_-0.5 
5 following: 
0.867 
-0.5 
0.5 
0.867 
0.867 
0.5 
0.5 
0.867 
0.867 
-0.5 
r 0.867 
[0. 5 
0.867 
0.5 
0.867_ 
-0.5 
0.867_ 
1 
0 
0 
1 
~1 0 
_0 1 

3.5. 
ORTHOGONAL TRANSFORMATIONS 
117 
Fig. 3.24 
Improper rotation. 
As can be seen, within rounding error, we obtain an identity matrix in each case. Hence A 
is observed to be an orthogonal matrix. 
However, before concluding that all matrices that satisfy the above conditions are rigid 
rotations, let us consider the following modification of A. 
B = 
cos ^11 
COS ^21 
-COS di2 
—COS ^22-
Note that B differs from A only in the fact that the second-row entries of A have been 
each multiplied by - 1 . If we examine the properties of B, we see that 
B'B = BB' = I 
That is, the same conditions are met with the B matrix as were met with the A matrix.^^ 
However, what is happening here is something that is a bit different from a rigid 
rotation. Figure 3.24 illustrates what is going on. In this latter case we have a rigid 
rotation that leads to a new axis fi which is in the same orientation as fi in Fig. 3.20 but 
an axis £2 which is the negative of £2 in Fig. 3.20. 
This new situation represents a case of rotation followed hy 2i reflection of the £2 axis. 
Alternatively, we could have affixed minus signs to the first row of A and, in this case, it 
would be the fi axis that was reflected. However, if all entries of A are multiplied by 
- 1 , a rigid rotation of ^n + 180° would result. It is only when an odd number of rows 
receive minus signs that we have what is known as an "improper" rotation, that is, a 
rotation followed by reflection. 
*^ The reader may verify this numerically or, in more general terms, write out the implied 
trigonometric relationships. 

118 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
How do we know before hand whether a proper versus improper rotation is involved? 
It turns out that this distinction is revealed by examining the determinant of A. 
1. If the determinant of A equals 1, then a proper rotation is involved. 
2. 
If the determinant of A equals —1, then an improper rotation is involved. 
And, it tums out that any orthogonal matrix will have a determinant that is either 1 or 
- 1 . 
To sum up, if A'A = AA' = I, we say the matrix is orthogonal. If |A| = — 1, it represents 
a rotation followed by an odd number of reflections, for example, one axis in the 2 x 2 
case, one or three in the 3 x 3 case, one or three in the 4 x 4 case, and so on. If |A| = 1, 
then we are deaUng with a proper rotation.^^ 
3.6 
GEOMETRIC ASPECTS OF CROSS-PRODUCT MATRICES 
AND DETERMINANTS 
In Chapter 2 we defined a determinant as a scalar function of a square matrix. 
Evaluation of a determinant in terms of both cofactor expansion and the pivotal method 
was also described and illustrated numerically. At this point attention focuses on the 
geometric aspects of a determinant and, in particular, its role in portraying generalized 
variance among a set of statistical variables. In the course of describing this relationship, 
we shall also point out geometric analogies to a number of common statistical concepts. 
By way of introduction, we first illustrate some geometric aspects of a determinant at 
a simple two-dimensional level. We then discuss how determinants can be linked with 
various statistical measures of interest to multivariate analysis. 
3.6.1 
The Geometric Interpretation of a Determinant 
Certain aspects of the determinant of a matrix can be expressed in geometric format. 
To illustrate, let us consider the unit square OIJK, as shown in Fig. 3.25. The coordinates 
representing the vertices of the square are 
O = (0,0); 
/ = (1,0); 
/ = (1,1); 
^ = (0,1) 
Next, suppose we were to multiply these coordinates by the matrix 
"2 r 
3 4. 
as follows: 
U = 
[2 
n 
_3 4J 
0 
I J K 
[0 
1 1 0" 
Lo 0 1 i_ 
u = 
O I* 
J* 
K* 
0 
2 
3 
1 
.0 
3 
7 
4J 
*' It should be mentioned that any reflection of two or more axes can itself be represented by a 
proper rotation followed by just one reflection. For example, in the 3 x 3 case, only one axis need be 
reflected after an appropriate rotation is made. 

3.6. 
CROSS-PRODUCT MATRICES AND DETERMINANTS 
119 
Fig. 3.25 
Geometric aspects of a determinant. 
These transformed points also appear in Fig. 3.25 as the quadrilateral O/*/*^*. 
The key aspect of this transformation has to do with the ratio of the area of the 
quadrilateral to the area of the original unit square. The ratio of the two areas equals the 
determinant of the transformation matrix T. That is, 
|T| = ( 2 x 4 ) - ( 3 x l ) = 5 
Thus, if one were to measure the area of OI*J*K* and compare it to the area of OIJK, 
one would find that it is exactly five times the latter area. And this would be true for any 
starting figure that is transformed by T. 
This concept generalizes to determinants of matrices of order 3 x 3 and higher. In the 
3 x 3 case, the determinant measures the ratio of volumes between the original and 
transformed figures. In the 4 x 4 and higher-order cases, the determinant measures the 
ratio of hypervolumes between original and transformed figures. 
Finally, if the sign of the determinant should be negative, this does not affect the ratio 
between hypervolumes of original and transformed figures. Rather, the presence of a 
negative determinant has to do with the orientation of the transformed figure in the space 
of interest.^"^ Hence, it is the absolute value of the determinant that indicates the ratio of 
hypervolumes. Moreover, if that absolute value is less than unity, then the transformed 
figure's hypervolume is a fraction of that of the original figure. 
3.6.2 
The Geometry of Statistical Measures 
In Chapter 1 we introduced a small and illustrative data bank (Table 1.2), involving 
only twelve cases and three variables. In Chapter 2 we used this miniature data bank 
^* To illustrate this, the reader should work out the case of T = ['l ~\] with |TI = - 5 . This entails a 
reflection of the quadrilateral in Fig. 3.25 across the JK axis. However, the ratio of areas is still 5 : 1 . 

120 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
(Table 2.2) to illustrate the application of matrix operations in the computation of 
various cross-product matrices, such as the SSCP, covariance, and correlation matrices. 
We continue to refer to this sample data bank. However, in line v^th the focus of 
Chapter 3, the data of the sample problem are now discussed from a geometric viewpoint. 
In the course of analyzing multivariate data, it is useful to make various scatter plots 
for showing relationships among variables. Figures 1.2 and 1.3 are illustrations of the 
more usual type of plot in which variables are treated as axes, and cases (employees in 
this example) are treated as points. This more conventional way of portraying data is 
often called a response surface or point model, since with one criterion variable and two 
predictors Xi and X2, one could visuaUze the fitting of a response surface to Y, the 
criterion variable. 
Altematively, however, we could imagine that each of the twelve employees, or cases, 
represents a dimension, and each of the three variables in the sample problem represents a 
vector embedded in a twelve-dimensional space. (Actually, if the three variables are 
linearly independent, they will lie in only a three-dimensional subspace of the original 
twelve dimensions, as is discussed in more detail in later chapters.) 
For the moment, let us simplify things even further and consider only two of the 
variables of the sample problem, namely, Y and Xi. If so, a vector representation of these 
two variables could be portrayed in only two dimensions, embedded in the full, 
twelve-dimensional space. 
From Table 2.3 we note that the covariance and correlation matrices for only the Y, 
Xi pair of variables are 
Xi 
y 
X, 
29.52 
19.44" 
.19.44 
14.19. 
R = 
X, 
Y 
X, 
1.0 
0.95" 
.0.95 
1.0 . 
As recalled, both the C and R matrices are based on mean-corrected variables; as such, 
the origin of the space will be taken at the centroid of the variables, represented by the 0 
vector.^^ 
As in Chapter 2, we can define the variance of a variable X\ as 
S4 
m 
whereX/i -Xn—Xi 
(i.e., 
each X is expressed as a 
deviation about the mean, and 
m denotes the number of cases^^) 
^ ^ By centroid is meant a vector whose components are the arithmetic means of Y and X^, 
respectively. Then, if we allow the centroid to represent the origin or 0 vector, the individual vectors 
are position vectors whose termini are expressed as deviations from the mean of Y and X^, 
respectively. 
** One could use m — 1 in the denominator if one wished to have an unbiased estimate of the 
population variance. (Such adjustment does not mean that the sample standard deviation is an 
unbiased estimate of the population standard deviation, however.) Here, for purpose of simplification, 
we omit the adjustment and use m in the denominator. 

3.6. CROSS-PRODUCT MATRICES AND DETERMINANTS 
121 
SimUarly, the correlation of a pair of variables is defined as 
where yi and Xn are 
each expressed as deviations 
about their respective means 
When Y and Xi are each expressed in mean-corrected form, their correlation is related 
to their scalar product as follows: 
which, we see, is just the cosine of 6y^ , their angle of separation 
In terms of the sample problem, the correlation is 
233.28 
ryx, = cos ^' 
\/354.24 • VI70.28 = 0.95 
where the scalar product and squared vector lengths are computed from Y^ and X^i in 
Table 1.2. The covariance of a pair of variables is defined as 
cov, yxi 
m 
where Sy and Sx are standard 
deviations of Y and Xi, respectively 
Our current objective is to tie in these statistical notions with concepts from vector 
algebra and, in particular, to show how the determinant of a covariance matrix can be 
used as a generalized scalar measure of dispersion. 
To do this, we first imagine a geometric space in which the axes are the m cases (e.g., 
the employees). The variable Y can then be thought of as a "test" vector y in a space of 
m persons. Similarly, the variable Xi can be thought of as another vector Xi in the person 
space. The components of y and the components of Xj each sum to zero since each 
variable is expressed in terms of deviations from its own mean. 
Once this has been done, we can see that the length of the vector y tums out to be 
proportional to the standard deviation of the variable Y. That is. 
Similarly, for the vector Xi we have 
llyll -
^ 
- 
y/mSy 
ehave 
llxilh -Jl?'-
= \/mSx^ 

122 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
Fig. 3.26 
A vector representation of the covariance between Y and X^ (sample problem). 
The correlation ry^^ between the two test vectors, measured as deviations from each 
variable's mean but based on vector lengths that are each normalized to unity, is equal to 
the cosine of the angle separating them. Furthermore, the scalar product of these two 
vectors is proportional to their covariance. That is, 
y'xi =cos^yxJ|y||-||xill=m[cov3;;cJ 
=m[ryxfySxJ 
Insofar as the sample problem is concerned. Fig. 3.26 shows a plot of the vectors y and 
Xi. The angle corresponding to a cosine of 0.95 (denoting their correlation) is 18°. Their 
respective lengths are 
lly|| = \ / n - \/2932 = 18.82; 
||xil|= Vl2- xA4l9= 13.05 
If one is dealing with standardized scores, then vector lengths would, of course, each 
be equal to \/m since Sy and s^. would each be equal to unity. 
In brief, with mean-corrected variables, all three cross-product matrices—the SSCP, 
covariance, and correlation matrices—can be portrayed in geometric terms. The key 
concept involves the scalar product between two variables. In all three cases, we have 
cos ^yx, = ryx^ 
The vector lengths in each case are as follows: 
SSCP matrix: 
Covariance matrix: 
Correlation matrix: 
fm 
Frn 
msv y , 
yjms^ 
This complementary view of association between variables will serve us in good stead in 
the interpretation of various aspects of multivariate analysis in later chapters. 
3.6.3 
A Generalized Variance Measure 
Having established the various correspondences shown above, we now wish to illustrate 
how the determinant of the covariance matrix represents a scalar measure of generalized 

3.6. CROSS-PRODUCT MATRICES AND DETERMINANTS 
123 
variance.^'' Given, illustratively, two variables Y and X, the covariance matrix can be 
written as 
2 
^ 
o 
c 
C = 
_^V3C ^V ^X 
where the main diagonal components are variances, and the off-diagonal component is 
their covariance. If we compute the determinant of this matrix, we get 
and, from basic trigonometry, in which we have the identity sin^ 6 + cos^ 0 = 1, we can 
write 
|C| = Sy'^Sx^ sin^ ^yx = (SySx sin ^yx)^ 
where ^yx is the angle between the (deviation-score) vectors y and x. 
As pointed out earlier, the standard deviation of a variable is l/\/m times the length of 
its corresponding vector. Hence, we have 
. . 
Ilyll 
11x11 . . 
Sys^ sm ^yx = - ; ^ - ^ 7 ^ sm ^yx 
and we can conveniently set up the equivalence 
as the height of a parallelogram with base given by 
llx[l 
as shown in Fig. 3.27. So, if the vector lengths are each scaled by l/y/m, we see that the 
area of the resulting (scaled) parallelogram equals SySx sin ^yx . The square of this area 
ilyll 
y/^ 
ylm / 
/ e 
/ 
\ 
/? = Ilyll sine/ym 
/ 
/ 
/ 
/ 
/ 
/ 
11x11 
Tij) 
Fig. 3.27 
Representing the determinant of a 2 x 2 covariance matrix as the area of a 
parallelogram. 
^"^ For example, even in a 2 x 2 covariance matrix we have four dispersionlike entries. Our interest 
here is on developing a single number that represents the four entries in certain multivariate statistical 
appUcations. 

124 
3. 
VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
equals the generalized variance (i.e., determinant of the dispersion matrix). Ifn variables 
are involved, the generalized variance equals the square of the volume formed by n such 
(rescaled) vectors. 
Figure 3.27 shows, in general form, the nature of this parallelogram in the two-variable 
case. We see that each vector appears in scaled (by l/y/m) form, and the parallelogram is 
completed as shown. 
For a numerical illustration of the correspondence, let us again refer to the sample 
problem of Table 2.3. We illustrate the equivalence for only the first two variables 7 and 
First, from the covariance matrix involving variables Y and Xi, 
Y 
X, 
Y 
Xi 
r29.52 
19.44 
19.44 
14.19_ 
we obtain 
||y||=vT2 • \/2932 =18.82; 
||xill = xA2 • >/r4.19 =13.05 
cos ^yx^ =0.95; 
sin ^yx^ = 0.31 
Hence the area of the parallelogram formed by y and Xi is 
llyll 
llx.ll 
. 
18.82 
13.05 
-—ZT ' 
;z=r- • sm 0VX = — ; = " * —7=^ (0.31) = 6.34 
The square of 6.34 is equal to 40.22. This value, within rounding error, equals the 
determinant of C, the covariance matrix. Thus we have shown geometrically and 
numerically how the determinant of C is equal to the square of the area of the 
parallelogram in Fig. 3.27. 
The concept of generalized variance is quite important in multivariate analysis since it 
enables us to portray a matrix of variances and covariances in terms of a single number, 
namely, the determinant of the covariance matrix. Just as importantly, we also see that 
the statistical measures of standard deviation, covariance, and correlation can be 
portrayed in terms of length and/or angle of test vectors in person and, more generally, 
object space. 
3.7 
SUMMARY 
The purpose of this chapter has been to describe a number of the vector and matrix 
operations outlined in Chapter 2 from a geometric standpoint. After setting up a 
rectangular Cartesian coordinate system and defining the concept of a EucHdean space, 
we discussed such topics as vector length and angle, vector addition and subtraction, 
scalar multiplication of a vector, and the scalar product of two vectors from a geometric 
point of view. 

REVIEW QUESTIONS 
125 
We then described the notion of hnear independence. We also illustrated how the 
Gram-Schmidt process could be employed to fmd an orthonormal basis starting from any 
given (arbitrary) basis. Following this we briefly discussed the idea of generalized 
(nonorthogonal) coordinate systems. 
Matrix times vector multipHcation was introduced from a geometric viewpoint for the 
special case of orthogonal (i.e., proper or improper rotation) matrices. The properties of 
this class of matrices were discussed, and their application was illustrated numerically. We 
concluded the chapter with a geometric representation of various statistical measures, 
including the central concept of generalized variance, as applicable to multivariate 
statistical tests to be considered in later chapters. 
REVIEW QUESTIONS 
1. 
Sketch a three-dimensional coordinate system. 
a. 
Plot points with coordinates (2,1,0), ( 1 , - 1 , - 1 ) , (\/3, TT,-2). What is the 
length of each? 
b. 
What is the set of points whose x and y coordinates sum to 1? 
c. 
What is the graph of z = x^ ? 
d. 
What is the graph of the inequality x^ + j ^ ^ + z^ < 1? 
2. 
Let P be the point (4, 3, - 1 ) , 2 be the point (1,0, 2), and R be the midpoint of the 
segment joining P and Q. 
a. 
What are the coordinates of i^? 
b. 
Sketch the vectors PR, OR, and PQ, where O denotes the origin. 
c. 
Verify that PR = PQjl by computing the distance from P to R,R 
to Q, and P to 
Q; then show that the first two distances are each half of the last distance. 
3. 
In the context of linear combinations, 
a. 
find a scalar k such that 
(1,0,2) + ^(2,1,1) = ( - 1 , - 1 , 1 ) 
b. 
find scalars ki, k2, and ks such that 
kiiSei + 62) + k2(e2 + 63) + k^ie^) = Se^ + 3e2 + 63 
c. 
find ^1 and /:2 such that 
ki(5ei + e2) + k2(ei — e2) = 0 
4. 
Let a and b be vectors with given lengths and angle 6. Compute their scalar product 
under the conditions 
a. ||a|| = 0.5; 
||b|| = 4; 
6= 
45° 
b. ||a|| = 4; 
l|b|| = l; 
0= 
90° 
c. I | a | | = l ; 
l|b|| = l; 
0 = 120° 
d. 
What is the possible range of values for the scalar product a b if 
Hall = 2 
and 
||b|| = 3? 

126 
3. VECTOR AND MATRIX CONCEPTS FROM A GEOMETRIC VIEWPOINT 
5. 
Let a, b, and c be vectors. Let ||ap|| be the component of a along c and let ||bp|| be 
the component of b along c. What is the component of a + b along c? Sketch the 
relationship in two-dimensional space. 
6. 
Let a' = (2, - 1 , 7) and b' = ( - 3 , 6, 1). Find direction cosines for 
a. 
a' + b' 
b. 
a ' - b ' 
c. 
5a'+ 10b' 
d. 
^ ( a ' - b ' ) 
Next, find the cosine of the angle between the two vectors obtained in parts c and d. 
7. 
Apply the Gram-Schmidt orthonormalization procedure to the following sets of 
vectors: 
a. a' = (l,2,3); 
b. a'= (2,1); 
b =(3,0,2); 
b' = (l,2); 
c' = ( 3 , l , l ) 
c' = ( l , l ) 
c. 
What do you notice about the vectors obtained in part b? 
8. 
Find coordinates of the vector a' = (2, 3) relative to the basis vectors f/ = (1, — 1) 
andf2' = (3, 5). 
9. 
Show that the vectors a' = (1, 4, —2) and b' = (2, 1, 3) are orthogonal and find a 
third vector that is orthogonal to both. 
10. 
Find the equations for the eUipse 4x^ -^ y^ = 4 and the circle ;c^ -^ y^ = I after the 
xy axes have been rotated counterclockwise through angles of 
a. 
45° 
b. 
60° 
c. 
120° 
11. 
Find X and y so that the vectors ( 4 , - 2 , 1 , 7 ) and ( 2 , - 3 , x , >') are Unearly 
dependent. 
12. 
Express the standard basis vectors e / = (1,0, 0), 62' = (0, 1, 0), and 63' = (0, 0, 1) 
as linear combinations of f/ = (1, 2, 4), f2' = (-2, 1, 5), and fa' = (—1, - 1 , 2). 
13. 
Rotate the vector a'= (1,2) counterclockwise through an angle of 45° while 
keeping the basis vectors fixed. Rotate b' = (3, 2) clockwise through an angle of 60 . 
a. 
What is the scalar product a'b before and after the two rotations? 
b. 
What are the vector lengths of a' and b' after the rotations? 
c. 
Show each of the above steps geometrically. 
Assume that we have the expression 
14. 
a2' 
cos 45 
sin 45 
-sin 45 
cos 45° 
-^2-
and OP is the Une joining the origin 0 to the point P = (2, 3). Show in diagram form the 
position of OP*, the rotated point. 
15. 
Apply the transformation 
_ f l 2 * _ 
= 
'3 
5] 
-2 4_ 
^1 
L^2_ 
to a square with vertices of (1, 1), (3, 1), (3, 3), (1,3) and show geometrically that the 
ratio of the area of the new figure to the area of the original is 2 to 1. 
16. 
In the sample problem of Table 2.3, consider the full 3 x 3 covariance matrix. 
a. 
Plot the mean-corrected y, x j , and X2 in a three-dimensional space. 
b. 
Plot the standardized form of y, Xi, and X2 in a three-dimensional space. 
c. 
Show how the correlation between y and X2 is related to 
n 
y'x2 
^OSUy^^- 
| | y | , . , | ^ ^ | | 

CHAPTER 4 
Linear Transformations 
from a Geometric Viewpoint 
4.1 
INTRODUCTION 
In Chapter 3 the reader was exposed briefly to one type of transformation, namely, 
that involving the rotation of either a point or a set of basis vectors by an orthogonal 
matrix. A large part of multivariate analysis is concerned with linear transformations, of 
which rotation represents only one type—albeit an important special case. 
The purpose of this chapter is to describe the geometric aspects of various kinds of 
matrix transformations. In particular, we shall be interested in how general linear 
transformations can be viewed as composites of simple kinds of matrices that, 
individually, are more easily portrayed from a geometric viewpoint. 
In the course of describing matrix transformations geometrically, two additional 
concepts of major importance to multivariate analysis—matrix inversion and the rank of a 
matrix-are described and illustrated numerically. These concepts are useful in the 
solution of sets of linear equations and play important roles in multivariate analysis. 
The chapter starts out with an overview description of matrix transformations and 
their representation as sets of linear equations. Then the topic of orthogonal 
transformations, first introduced in Chapter 3, is reviewed and expanded upon. The 
distinction between point and basis vector transformations is also emphasized. Discussion 
then proceeds to more extensive cases involving general linear transformations. In the case 
of basis vector changes, these kinds of transformations require the use of matrix inverses; 
hence, the basic ideas of matrix inversion are introduced at this point. 
The next major section of the chapter is devoted to the geometric representation of 
various types of matrix transformations, such as rotations, stretches, central dilations and 
reflections. The geometric character of combinations of various matrix transformations is 
also illustrated so that the reader can see how simple geometric changes, when taken in 
combination, lead to complex representations. 
The remainder of the chapter focuses on the solution of simultaneous equations and 
the central roles that matrix inversion and matrix rank play in this activity. In particular, 
we discuss the solution of linear equations in multivariate analysis and, in the process, tie 
in the present topic with material presented in earlier chapters on determinants and the 
pivotal method for solving sets of linear equations. 
127 

128 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
4.2 
SIMULTANEOUS EQUATIONS AND MATRIX 
TRANSFORMATIONS 
The concept of a function or mapping is fundamental to all mathematics. By a 
mapping we mean an operation by which elements of one set of mathematical entities are 
transformed into elements of another. In scalar algebra we recall that functions like the 
following are often employed: 
y=f(x) = bx] 
y=f(x) = e'' 
y=f(x) = ax'', 
y=f(x) = ab'' 
For example, for a specific value of x, and a value for the parameter by we can find a 
value of y from the linear equation y = bx. The possible values that x can assume are 
called the domain of the function. The possible values that y can assume are called the 
range of the function. 
In scalar algebra our interest centers on the description of rules (i.e., the functions) by 
\^4iich pairs of numbers are related. In vector algebra we are interested in the rules by 
which pairs of vectors or points are related. 
In our discussion of vector mappings we consider only single-valued, linear mappings 
of one vector space onto another, which may, of course, be the same space. Thus, we are 
interested in cases where both j^ and x are vectors. 
By restricting ourselves to linear transformations—illustrated byy = bx above—we can 
state three conditions of interest: 
1. Every vector of a vector space is transformed into a uniquely determined vector of 
the space. 
2. 
If a is transformed to a* by a linear transformation T, then ka is transformed (by 
T) to /:a* for any scalar k. 
3. 
If a is transformed to a* and b to b* by a linear transformation T, then a + b is 
transformed (by T) to a* + b*. 
linear transformations are those that satisfy the above conditions. Moreover, any 
linear transformation can be represented in matrix form (e.g., as the multiplication of a 
vector by a matrix). 
Returning to the topic of mappings, values obtained by a mapping are often callec 
images, while the values being transformed are often called preimages of the transfor-
mation. Here we are mainly concerned with mappings that transform vectors into vectors, 
that is, mappings that represent vector-valued functions. Hence, both the preimages and 
the images of the mapping are vectors. Moreover, we shall mostly be concerned with 
linear transformations that involve square matrices as representations of the transfor-
mation so that the two vector spaces, before and after the transformation, are of the same 
dimensionality.^ 
* Or, essentially the same space, before and after the transformation. 

4.2. 
SIMULTANEOUS EQUATIONS AND MATRIX TRANSFORMATIONS 
129 
4.2.1 
Simultaneous, Linear Equations Expressed in Matrix Form 
The need for matrix transformation arises quite naturally in the solution of linear 
equations. Consider the following set of linear equations: 
Xi* = aiiXi + ^112:^:2 + • • • + ai^Xn 
^2* =^21^1 +^22^2 + • • •-^a2nXn 
which can be written, in matrix form, as 
X* = Ax 
As a simple case of expressing a matrix transformation as a set of simultaneous 
equations, consider the point x = [J^] in Fig. 4.1. Also consider the second point 
X* = {J» *] in the same figure. A linear mapping of x onto x* can always be expressed by a 
set of linear equations: 
Xi* =anXi + ai2X2', 
X2* =^21^1 +^22^2 
which relate the coordinates Xi*, X2* to the coordinates Xi, X2 in the standard basis e/. 
But, as noted above, these equations can be expressed in matrix form as 
Xi"^ 
X 2 * 
^11 
L^21 
ai2 
a22 
Xi 
X2J 
>\^ere the matrix 
an 
«i2 I 
_«21 
^22j 
represents the mapping of x, the preimage, onto x*, the image. 
eo 
• x^ =[:::] 
• 
X = [:;] 
Fig. 4.1 
A point transformation. 

130 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
All of the mapping? that we shall be considering are capable of being expressed in 
terms of a set of linear equations, similar to those illustrated above. 
We review some simple examples of matrix transformations, first considered in 
Chapter 3, and then show how these cases can be extended to more general kinds of 
linear mappings. 
4.2.2 
Orthogonal Matrix Transformations 
As recalled from Chapter 3, an orthogonal matrix A is one in which A'A = AA' = I. 
That is, rows (and columns) of A are mutually orthogonal, and each is of unit length. This 
type of transformation is called a rotation, either proper or improper, depending upon 
the sign of its determinant. 
In the preceding chapter, where we introduced the reader to orthogonal transforma-
tions, we recall that rotations were expressed as sets of direction cosines. Although the 
distinction was not emphasized, we also illustrated two types of rotations: 
1. Point rotations, where the original basis vectors remained fixed and the point(s) 
moved, clockwise or counterclockwise, around the origin. 
2. 
Basis vector rotations, where the original point(s) remained fixed and the basis 
vectors moved, clockwise or counterclockwise. In this latter case the fixed point was then 
expressed as a linear combination of the new basis vectors. 
Since rotations deal with relative motion, either of the above approaches is equally 
appropriate in interpreting the nature of a rotation. 
Figure 4.2 illustrates the four cases that are involved in rotating an arbitrary point x. 
In Panel I x undergoes a counterclockwise rotation, mapping onto the point y. 
Alternatively, we can rotate x clockwise to map it onto the point z. Note that the 
standard basis vectors remain fixed throughout both of these rotations. 
Panel II of Fig. 4.2 shows the set of basis vector rotations in which the e,- are mapped 
onto f,- via a counterclockwise rotation. Panel III shows the case of mapping the e,- onto g,-
via a clockwise rotation. In each of these latter two cases, the coordinates of x are in 
terms of the new basis vectors. As will be shown, all four types of rotations are variations 
on a common theme and are related to each other in a straightforward way. 
Now, however, we shall want to distinguish more carefully between point transfor-
mations and basis vector transformations by adopting a specific notation for each. An 
image obtained by a point transformation, in which the original basis vectors remain 
fixed, is denoted by x*. If obtained by a change in basis vectors—and, hence, the fixed 
point is referred to the new basis—the image is denoted by x°. 
Point transformations are much less complicated than basis vector transformations. 
However, it is important to study both kinds since many multivariate techniques involve 
situations in which the change of basis vectors simplifies the character of the 
transformation quite markedly. 
As a quick review of the two types of rotations, suppose we choose a standard basis 
ei = [o]and62 = [?] and a point in that spacex = [2] = Ici + 2e2.This linear combination 
of vectors can also be written as 
1 
2 
= 1 
1 
0 
+ 2 
0 
1 

4.2. 
SIMULTANEOUS EQUATIONS AND MATRIX TRANSFORMATIONS 
131 
I 
\ 
h 
•"«s. 
Counterclockwise 
H 
1 
Iei 
Clockwise 
Counterclockwise 
Fig. 4.2 
Point and basis vector transformations. 
\Aere the standard basis vectors are shown explicitly as column vectors. Let us first 
consider point rotations and then basis vector rotations. 
4,2,2.1 
Point Rotations 
Suppose we examine the rotation of x=[2] in a 
counterclockwise direction through an angle ^ of 30°. In the case of two dimensions, we 
have, in general, the equations 
Xi* =aiiXi 
+^12^2; 
^ 2 * =^21^1 + ^22^2 
Here we use x* to denote the image of the vector x under the matrix transformation A. 
We assume A to be orthogonal. The system of equations can be written as 
= Ax 

132 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
Since only two dimensions are involved, we can simplify the problem a bit by relating all 
direction cosines to the single angle of >I^ = 30°, as was illustrated in Chapter 3. Hence for 
this specific example we have 
L^2 . 
-0.13 
2.23 
cos ^ 
—sin ^ 
sin ^ 
cos "i^ 
^0.87 
-0.50 
0.50 
0.87 
Notice in this illustration that we made use of the trigonometric facts (involving 
complementary angles) that 
1. cos(90° + ^) = -sin ^ = cos 120° = -sin 30° = -0.5 
2. cos(90°-'gk)= 
sin>I^ = cos 60°= 
sin 30° = 0.5 
This device allows us to avoid the more complex application of direction cosines involving 
pairs of angles dij, as discussed in Chapter 3, although its use, as recalled, is restricted to 
two dimensions. 
Figure 4.3 shows the results of this mapping. We observe specifically that the new 
coordinates x* = [^^al ^^ ^^® point are still expressed in terms of the old basis, namely, 
Ci and 62. It is the point, that is rotated counterclockwise, while the axes maintain their 
original orientation. 
Now let us see what happens when we rotate the point x = [2 ] clockwise through an 
angle of ^ = 30°, as given by 
J C i * 
L^2*_. 
~1.87~ 
_1.23_ 
cos ^ 
sin^ 
_-sin ^ 
cos ^_ 
0.87 
0.50l[ 
_-0.50 O.87J [ 
ir 
Xi 
JL:^2 
r 
_2_ 
In this case the new point coordinates are given, still in terms of the original basis vectors, 
in Fig. 4.4. Notice, in the case of a clockwise rotation of the point, the matrix A' is the 
transpose of that (A) used to rotate the point counterclockwise. Finally, if we first rotate 
the point counterclockwise, given by A, and then rotate it clockwise, given by A', we end 
up where we started, since in the case of orthogonal matrices: 
A'A = I 
Similarly, had we started with the clockwise rotation and "undone" this by means of the 
counterclockwise rotation, we would have 
AA' = I 
which, again in the special case of an orthogonal matrix, gets us back to where we started. 
Point transformations—either orthogonal or more general linear transformations-
present relatively few problems and can all be represented simply by 
x* = Ax 

4.2. 
SIMULTANEOUS EQUATIONS AND MATRIX TRANSFORMATIONS 
133 
62 
'' = [2.23] 2 * 1 " ^ - . . x=[;] 
1 + 
-1 
24 
1 
Fig. 4.3 
Fig. 4.4 
1.87 
1.23 
Fig. 4.3 
Counterclockwise rotation of point—new coordinates given in terms of old basis vectors. 
Fig. 4.4 
Clockwise rotation of point—new coordinates given in terms of old basis vectors. 
where we should remember, of course, that it is the point(s) or vector(s) that moves 
relative to a fixed basis. Moreover, each row of the transformation matrix represents a 
linear combination of the original point coordinates. 
In Chapter 3, however, we also considered the case of rotations which involved basis 
vector changes. In this case the point(s) remains fixed, but is then referred to a set of new 
(rotated) basis vectors. 
4.2.2.2 
Basis Vector Transformations In discussing rotations of basis vectors, we 
shall want to review a few fundamentals and introduce some new features as well. To be 
specific, suppose we wish to rotate a set of standard basis vectors clockwise through an 
angle of 30°. As noted above, the orthogonal matrix that effects this type of rotation is 
A' = 
cos ^ 
sin ^ 
-sin ^ 
cos "^ 
' 0.87 
0.50' 
-0.50 
0.87 
Let us start out with the standard basis vectors Ci and 62- Based on our earlier 
remarks—and as verified by Fig. 4.5—we see that a clockwise rotation of the basis vectors 
results in a new basis in which the fixed point x = [2 ] is now x° = ["2123] i^ terms of the 
new basis.^ 
To distinguish the two types of transformations, we let x° denote the image of x 
under a basis vector transformation, while x* denotes its image under a point 
transformation. Note further that the coordinates of x° are the same as those found when 
the point was rotated counterclockwise. This is as it should be inasmuch as we are 
concerned with only relative motion. 
The new basis vectors f 1 and £2 can be expressed in terms of the original as 
fi 
f2 
E 
A' 
F = 
0.87 
0.50 
-0.50 
0.87 
1 
0 
0 
1 
0.87 
0.50 
-0.50 0.87_ 
^ It is well to remember that the vector x exists independently of its coordinates. However, the 
specific coordinate values that it assumes are dependent upon the basis vectors to which it is referred. 

134 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
Fig. 4.5 
Fig. 4.6 
Fig. 4.5 
Clockwise rotation of axes—old coordinates given in terms of new basis vectors. 
Fig. 4.6 
Counterclockwise rotation of axes—old coordinates given in terms of new basis vectors. 
Here we employ E to denote the matrix of the original (standard) basis and F to denote 
the matrix of the new basis. Also, note that the columns of F represent linear 
combinations of the (column) basis vectors of E. 
As can be seen in Fig. 4.5, f i, the first column of F, passes through the point (0.87, 
—0.5) positioned in the original e,- basis, while f2 passes through the point (0.5, 0.87) in 
the original Cj basis. 
In this case the original vector x = [2] is now x° = ["2.23I when referred to the fi, fa 
basis. Recall that these are the coordinates found by a 30° counterclockwise rotation of 
the point in the original basis. 
Similarly, if the original basis vectors are rotated counterclockwise, this gives us the 
same result as found by rotating the point clockwise. A picture of this change in basis 
vectors is shown in Fig. 4.6. 
At this point it might sound confusing and redundant to have two ways, point and 
basis vector changes, for expressing linear transformations. As we shall try to show later, 
however, there are advantages to considering basis vector as well as point transformations. 
This is particularly true when the transformation matrix is not orthogonal; that is, when 
the transformation does not entail a simple rotation. 
To sum up, if we wish to find the coordinates of transformed x* relative to the 
standard ei, 62 basis (i.e., to move the point relative to the fixed basis), we use the point 
transformation 
x* = Ax 
where x is originally referred to the ei, 62 basis. In this case the coordinates of the point 
X* are also expressed directly in terms of ei, 62. (See Figs. 4.3 and 4.4.) 
Alternatively,we may care to transform the e,- basis itself to a new set of basis vectors 
f|. In this case it is the new basis vectors that are expressed in terms of e,-, while the old 
coordinates of the point x are now expressed as x° in terms of the new basis f,. (See Figs. 
4.5 and 4.6.) Notice, then, that the values of the new coordinates depend on which 
method we use to carry out the transformation. 

4.2. 
SIMULTANEOUS EQUATIONS AND MATRIX TRANSFORMATIONS 
135 
2 + 
1 f 
••'a 
L2.20j 
1 
2 
Fig. 4.7 
Point transformation of x with fixed basis vectors. 
4.2.3 
Generalizing the Results 
It is now of interest to describe what goes on when we do not restrict ourselves to 
rotations. Consider the more general transformation matrix 
r0.90 
0.44" 
T = 
LO.60 
0.80 
and the vector x = [2 ], again relative to the standard basis vectors 
ei = 
and 
e2 = 
Application of a point transformation to x (retaining the original basis) is quite 
straightforward and is shown in Fig. 4.7. Here we see that x moves to the position 
Xi* 
X2* 
= 
1.78 
2.20 
= 
0.90 
0.44 
0.60 
0.80. 
However, notice that this point movement does not involve a simple rotation. Still, the 
process of finding the image vector x* presents no new problems. That is, we still have a 
straightforward task of premultiplying the original point by the matrix of the 
transformation T. Moreover, x* is still referred to the original C/ basis. 
Unfortunately, things are not so simple when we attempt to construct the counterpart 
basis vector transformation. Unlike the special case of rotation, the present transfor-
mation matrix T presents complications in referring x to a new basis f,-. Rather than try to 
solve this problem now, we can address ourselves to a related problem in point 
transformations. Solution of this related problem will pave the way for handling the basis 
vector transformation in the case of the linear transformation matrix T. 
The related problem can be expressed as follows: Suppose we were given the point 
X* = [2:20] in Fig. 4.7 to begin with and wanted to find x, knowing only the 
transformation matrix T and the original basis E. In other words, we now wish to find a 
transformation, call it T"^, that will get us back to x, given the transformed coordinates 

136 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
X*. For this task we shall need to describe the nature of T"/ , the inverse of T. Discussion 
of matrix inversion will require a brief digression, after which we can return to the 
particular problem at hand. 
4.3 
MATRIX INVERSION 
In Chapter 2 we briefly described a special diagonal matrix, referred to as the identity 
matrix I. As discussed there, I consists of a square matrix with I's along the main diagonal 
and zeros elsewhere. As will be shown, the identity matrix— and the concept of matrix 
inverse-play special roles in the matrix algebra "equivalent" of division (or, more 
appropriately, multiplication by a reciprocal). We recall from Chapters 2 and 3 that 
although we have discussed addition, subtraction, and multipHcation as operations in 
matrix algebra, we have not discussed, as yet, the companion operation of division. As we 
shall see, matrix inversion in linear algebra is analogous to the operation of division in 
scalar algebra. 
The identity matrix I plays a special role in matrix algebra. The effect of pre- or 
post multiplying any conformable matrix by I is to leave the original matrix unchanged: 
lA = AI = A 
That is, I in matrix algebra plays the role of the number 1 in ordinary arithmetic. We now 
ask if there is a type of matrix that is analogous to the reciprocal of a number, that is, a 
number, a^ 0, for which the relation ax = \ is true. If so, we should be able to develop 
the concept of "division" in the context of matrix algebra. 
In the case of square matrices there is an analogue, in some instances, to the notion of 
a reciprocal in scalar arithmetic. This is called an inverse. As a matter of fact, matrix 
inverses—in a very generaUzed sense—can be obtained for rectangular matrices too. We 
discuss this more advanced topic in Appendix B while here we confine ourselves to 
regular inverses of square matrices. 
A regular inverse of the (square) matrix A is denoted by A'^and, when it exists, it is 
unique and satisfies the following relations: 
AA"^ =A-^A = I 
For example, let us take the matrix 
^1 
2 0~ 
A = | 0 
3 1 
2 
2 4 
Assume, now, that we have gone on to solve for A"^ and have found it to be 
10/14 
-8/14 
2/14" 
A-i = I 2/14 
4/14 
-1/14 
_-6/14 
2/14 
3/14_ 

4.3. 
MATRIX INVERSION 
137 
Then the relation AA"^ (or A ^ A) = I should hold. The reader can verify that it does: 
A 
1 
2 O' 
0 
3 1 
2 
2 4 
10/14 
-8/14 
2/14 
2/14 
4/14 
-1/14 
-6/14 
2/14 
3/14 
I 
1 
0 O" 
0 
1 0 
0 
0 1 
The problem, of course, is to find A ^ when it exists.^ As mentioned above, if A ^ exists, 
it will be unique for a given A. 
4.3.1 
The Determinant and the Adjoint of a Matrix 
In Chapter 2 we also discussed the concept of the determinant of a (square) matrix. 
The determinant of A, denoted as |A|, is merely a scalar or number that is computed in a 
certain way. We also discussed cofactors of a matrix and defined them to be determinants 
of order n — I by n — I obtained from a matrix A of order « by w by omitting the /th 
row and /the column of A and affixing the sign (-1)^"^^ to the determinant of the n- I 
by n - I submatrix. We further recall that a matrix A will have as many cofactors as there 
are entries in A. 
The cofactors themselves can be, in tum, transformed in a way that possesses some 
special characteristics relative to A and the identity matrix. This matrix, called the adjoint 
of A, possesses the useful property that'* 
A "adj(A)" 
_ 1^' 
= 1 
We now need to define the adjoint of A in terms of the cofactors of A 
The adjoint of a (square) matrix A, denoted as adj(A), is defined as the transpose of 
the matrix of cofactors obtained from A. 
^ If each column of A~* is considered originally as a set of unknowns, then all that is involved is 
solving a set of linear equations. For example, 
Ix^ + 2x^ + OJC, = 1 
Ox^ +3^2 + Ix, = 0 
2xj +2^:2 + 4^:3 = 0 
has the solution x^ = 10/14, x^ = 2/14, x^ = -6/14, which is the first column of A"^ Similar 
procedures lead to the second and third columns of A~^ That is, A"* could be found by solving three 
sets of Unear equations each, in which the right-hand side of the equation is, respectively, the column 
vectors 
~l~ 
0 
_0_ 
, 
"o~ 
1 
_0_ 
and 
Details of the method appear in Section 4.7.5. 
"* As the reader might surmise, this property is also displayed by A"^; as will be shown, the inverse 
A"* does equal the adjoint of A divided by the determinant of A. 

138 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
In the simple case of a 2 x 2 matrix 
A = an 
dn 
^21 
^22 
we recall that the determinant |A| is 
CL\\(l22 — ^12^21 
and the cofactors—consisting of single elements—are 
^11=^22; 
An-—ci2\\ 
^21=—an', 
^22=^11 
Let us now place these cofactors in a square (2 x 2) matrix. Next, let us take the 
transpose of this matrix. 
Then, the adjoint of A, in the 2 x 2 case, is just 
adj(A) = 
^22 
—^12 1 
In words, this says that in the 2 x 2 case, the adjoint of the matrix A involves (a) 
switching the entries along the main diagonal of the original matrix and (b) changing signs 
of the off-diagonal entries. 
If the nxn matrix A is of higher order than 2 x 2 , computation of adj(A) is a bit more 
complicated but proceeds in the same manner as stated above: 
1. 
Find the minors of A via the procedure described in Chapter 2. 
2. 
Find the cofactors, or signed minors of A, again via the procedure described in 
Oiapter 2. 
3. 
Place these cofactorsimnnxn matrix. 
4. 
Find the transpose of this matrix and call this transpose adj(A). 
From here it is but a short step to finding the inverse A"^ 
4.3.2 
The Matrix Inverse 
Both the determinant and adjoint of A figure prominently in the computation of its 
inverse. // the square matrix A has an inverse A"\ this inverse, defined such that 
AA"^ = A"^A = I, can be computed from 
That is, the inverse is found by scalar multiplication of a matrix. In the simple 2 x 2 case 
described above, this is 
A-'=J. 
^22 
—a 12 
-«2i 
an 
lAI^O 

4.3. 
MATRIX INVERSION 
139 
We now have an operational way, based on the transposed matrix of cofactors divided by 
the determinant of the matrix, to find the inverse of a square matrix A. We next consider 
this problem in the eariier context of point transformations. 
4.3.3 
Applying the Concept of an Inverse to Point Transfonnations 
Suppose we now return to the problem of finding the preimage vector x, given the 
image vector x* and the transformation matrix 
"0.90 
0.44 
T = 
0.60 
0.80 
As recalled from Section 4.2.3, x* = [2:20] is shown in Fig. 4.7. Now we wish to find x 
which, of course, we already know to be x = [2]. The starting equation is x* = Tx, and 
we wish to solve for x. 
By taking advantage of the fact that T''^T = I, we can find x from a matrix 
transformation that premultiplies both sides of x* = Tx by T"^ Thus 
T-^x* = T-^Tx = Ix = x 
In terms of the specific problem of interest, we need to perform the following 
calculations. First, we obtain the determinant of T: 
ITI = 0.9(0.8) - 0.6(0.44) = 0.46 
Based on the simple definition of the adjoint in the 2 x 2 case, we find 
adj(T) = 
^22 
-t2l 
-tn 
hi. 
0.80 
-0.44 
-0.60 
0.90 
Having found both |T| and adj(T), we compute T 
as follows: 
T-^=: 0.46 
It now remains to show that 
X = 
0.80 
-0.60 
-0.44 
0.90 
1.74 
-0.96 
-1.30 
1.96 
1.74 
-0.96 
-1.30 
1.96. 
X * 
1.78" 
2.20 
which is, indeed, the case. 
We can also verify that, within rounding error, TT"^ = T"^T = I. Finally, we should 
state that if |T| is zero, then 1/|T| is not defined, and the (regular) inverse of T does not 
exist. In this case the matrix T is said to be singular. Otherwise, as is the case here, it is 
called nonsingular. 
A nonsingular matrix A, then, is one in which 
lAl^O 

140 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
Nonsingularity is very important to the topic of matrix inversion since every nonsingular 
matrix has an inverse; moreover, only nonsingular matrices have (regular) inverses. 
Now that we have found out how to compute a matrix inverse and solve the equation 
x = T-'x* 
we should also state a property involving the inverse of the product of two (or more) 
matrices. 
Given the product of two or more conformable matrices^ Ti T2 * * * T^, the inverse of 
that product equals the product of the separate inverses in reverse order: 
( T I T 2 - - - T , ) - ^ = T ; ^ •••T2-'Tr^ 
Notice that this property is similar to the property involving the transpose of the product 
of two or more matrices. 
Having discussed some introductory aspects of matrix inversion, we return to the topic 
of vector transformation, but now in the context of changing basis vectors for the case of 
general linear transformations. As it tums out, the concept of matrix inverse is also 
needed here. 
4.3.4 
Transformation by Basis Vector Changes 
As recalled in our earlier discussion of basis vector changes in the context of 
orthogonal transformations, a second way to examine transformations is in terms of 
referring some vector x to a new set of basis vectors f,-. Let us return to the discussion 
involving transformations using the matrix T. 
0.9 
0.44 
0.6 
0.8 . 
Our interest now centers on the case of transformation via changed basis vectors where, as 
we know, T is not orthogonal. To find the new basis vectors f,- in the current problem of 
interest, we make note of the fact that linear combinations of the standard basis vectors 
e^ and 62 are found by 
u = 
"0.90" 
[0.44 J 
= 0.9 "r 
_oJ 
+ 0.44 
"0" 
1 
f,= 
0.6" 
Lo.8^ 
= 0.6 r 
[oj 
+ 0.8 
~0~ 
LiJ 
To express this in matrix form, where the f/ also appear as column vectors, we have 
E 
T' 
F = 
0.90 
0.60 
0.44 
0.80. 
0.90 
O.6OI 
0.44 
0.80 
Notice, then, that the new basis vectors are given by the transpose of the matrix T, 
where T itself was used in the point transformation in which x moved relative to the fixed 
6/ basis. 

4.3. 
MATRIX INVERSION 
141 
Fig. 4.8 
Transformation of x via change in basis vectors. 
Fig. 4.8 shows the new basis vectors fi and (2 plotted in terms of the old basis. We 
note from the figure that the new basis vectors f 1, (2 are oblique. Our problem now is to 
refer the original x = [2], as portrayed in E, to the new (obhque) basis in F. As we know, 
the point transformation T yields the point x* = [Ho], shown in the original e^. basis of 
Fig. 4.7. 
But we want to find the coordinates of the points, as referred to F. To do this we note 
that a point can either be expressed as x in E or as x° in F. 
Fx° = Ex 
But, as we know from the basis change shown above, 
F = ET' 
Hence, to solve for x'' in Fx° = Ex, we can premultiply both sides by F"^ to get 
F-^Fx°=F-^Ex 
Noting that F"^F =1 on the left-hand side and substituting ET' for F on the right-hand 
side, we recall the relationship involving the inverse of the product of two or more 
matrices shown in the preceding section to get 
x^ = (ET'r'Ex = (T'r^ E-'Ex 
But,E"^E = I,sothat 
| X ° = ( T V X I 

142 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
What this all says is that to find x° in terms of the new basis vectors f/ we are going to 
have to find ( T ' ) " \ the inverse of T'. 
We have already found T~^ in the case of moving x* = [2!2o]v^^ ^ point transformation 
back to X = [2]. We use the same type of matrix inversion procedure to find (T')"*, and 
this turns out to be 
(T')-» = 
1.74 
-1.30 
-0.96 
1.96 
Having found (T') ^ we then solve for x° as 
-0.86" 
2.96 
1.74 
-0.96 
-1.30 
1.96 
Figure 4.8 shows the transformed coordinates, x° = ["liitl of the original and fixed 
point X, now expressed in terms of the new (oblique) basis vectors f,-. 
How does this result relate to our earlier discussion involving basis vector changes that 
are given by orthogonal matrices? In this case it turns out that for orthogonal matrices 
the following relationship holds: 
\-'=A' 
Hence, in the special case of orthogonal matrices we find the relationship 
x° = (A')-' X = Ax 
in solving the general equation 
Fx° = Ex 
for x°. This, of course, is what we expect to find in the case of rotations. In other cases 
involving basis vector changes that employ general linear transformations such as T, the 
appropriate expression, as we now know, is 
x°=(Trx 
So, in summary, we can recapitulate the following points: 
1. For point transformations, the simpler type of transformation, we have 
a. 
X* = Tx 
b. 
X =T"^x* 
with X* and x in terms of E 
with X and x* in terms of E 
2. 
For transformations involving basis vector changes, the new basis vectors are given 
in terms of the old by F = ET'. We then have the cases 
a. 
x° = (T')"^X 
with x° in terms of F and x in terms of E 
b. 
X = T'X^ 
with X in terms of E and x in terms of F 

4.3. 
MATRIX INVERSION 
143 
Finally, in the special case of orthogonal matrices, denoted by A, we have 
1. For point transformations: 
a. 
X* = Ax 
with x* and x in terms of E 
b. 
X = A'x* 
with X and x* in terms of E 
2. 
For transformations involving basis vector changes, the new basis vectors are still 
given by the matrix F = EA'. We then have the special cases: 
a. 
x° = Ax 
with x° in terms of F and x in terms of E 
b. 
X = A ' X ° 
with X in terms of E and X° in terms of F 
As the reader has probably gathered by now, point transformation, in which the basis 
vectors remain fixed, is considerably easier to follow intuitively. However, instances 
arise in multivariate analyses where the selection of an appropriate basis to which the 
vectors can be referred results in a significant degree of simpUfication in characterizing 
the nature of the transformation that the researcher is employing. 
For this reason, we carry our analysis one more step, albeit the most complex one so 
far. We can pose the problem as one of starting with a point transformation of x, as given 
by the matrix T, relative to the standard basis vectors e,-. 
However, suppose the e,- basis is related, in turn, to a new basis f,- via a matrix L. If so, 
how is the point transformation given by T in the context of e,- represented in the new 
basis f,? We shall call the new transformation matrix T° to take into consideration the 
fact that it transforms points in the new basis f,-. 
The practical import of all of this is that in applied multivariate problems we often 
seek special sets of basis vectors in which the transformation matrix T° displays a 
particularly simple character. This pragmatic aspect of basis vector transformations is 
deferred until Chapter 5. However, here we can at least go through the mechanics of the 
process of relating a transformation represented by T in the standard basis e,- to its 
counterpart T° in the derived basis f,-. 
4.3.5 
Transformations under Arbitraiy Changes of Basis Vectors 
Suppose we continue to consider the matrix 
"0.90 
0.44" 
T = 
but let us also consider a second matrix 
0.60 
0.80_ 
0.83 
0.55 
.0.20 
0.98^ 
that can be used to transform the vectors in the standard basis e,- to a new basis f/. Note 
that the sums of squares of the row elements of L equal unity. Hence, the rows of L can 
be considered as direction cosines. However, since the scalar product of row 1 with row 2 
does not equal zero, the new f,- basis is oblique. 

144 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
As before, our first job is to find the oblique basis f, in terms of e^, the standard basis. 
Again, we define the column vectors f/ as follows: 
fi = 0.83ei + 0.5562; 
f2 = 0.20ei + 0.98e2 
which, in matrix multiplication form, are obtained as column vectors from 
E 
L' 
0.83 
0.20 
0.55 
0.98 
1 
0 
0 
1 
0.83 0.20 
0.55 0.98 
where, of course, we have the Hnear combinations 
fi = 0.83 "r 
LoJ 
+ 0.55 
"0" 
Ld 
fj = 0.20 r 
LoJ 
+ 0.98 ~o' 
LlJ 
Panel II of Fig. 4.9 shows the oblique basis vectors f i and £2 plotted in terms of the 
original basis e/. 
From earher discussion we know how to find the point transformation: 
X* = T x 
From Fig. 4.7 we note thatx* = [2;2o]- F^^ ^^^e of comparison this reappears in Panel I of 
Fig. 4 9 relative to the original basis E. 
Our objective now is to find the counterpart point transformation of x° = [2], 
referred now to the oblique basis f/ in Panel II of Fig. 4.9. 
Let us first present the solution to this problem and then examine it, piece by piece. 
First, we have the relationship 
0.0 
rw^O O 
^* = T x 
62 
3-r 
1.78 
2.20 
Fig. 4.9 
Point transformations relative to different bases. 

4.3. 
MATRIX INVERSION 
145 
but T° is found from the product 
T° = (L')-'TL' 
Since we already have L' and T above, the problem now involves the computation of 
(L')"'. The determinant of L' is 
|L' i = (0.83 X 0.98) - (0.20 x 0.55) = 0.703 
We then need to find adj(L') and, finally, (L')"'- This is carried out as follows: 
adj(L') 
(L')-' = 
1 
0.703 
0.98 
-0.20 
-0.55 
0.83 
1.39 
-0.78 
Hence 
T = 
1.39 
-0.28' 
-0.78 
1.18 
T 
"0.90 0.44" 
0.60 
0.80 
L' 
"0.83 0.20" 
0.55 
0.98 
-0.28 
1.18. 
1.11 
0.60 
0.34 
0.59 
Returning to the vector x = [2 ], we recall under the fixed basis vectors 61,62, that x is 
transformed by T onto x* = [2'Io]. ^s noted in Panel I of Fig. 4.9. However, if we use the 
basis fi, f2, the point transformation of x° = [2] in the basis F utiHzes T° and is 
2.31' 
1.52 
1.11 
0.60 
0.34 
0.59 
Notice, then, that x*° is a point transformation of x° = [2] in the F basis. 
Panel II of Fig. 4.9 shows the nature of the transformation. First, we plot x° = [2] in 
terms of the obUque basis F. Note that fi = [oils] andf2 = [oils! ^re, in turn, plotted in 
terms of the original E basis, while x° = [2] is positioned with respect to the new basis F. 
The process of finding the point transformation x*° = [lil^] in F proceeds by 
decomposing 
T° = (L)-' XL' 
as follows, starting from the far right of the expression to the right of the equals sign: 
1. 
L' maps x° onto x; that is, 
L 
X = x 
"0.83 
0.20 
0.55 
0.98 
2. T maps x = [2 Ji] onto x* 
T 
"0.90 
0.44 
0.60 
0.80 
1.23 
2.51 
1.23 
2.51 
2.21 
2.75 

146 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
3. 
(L') * maps x* onto x*° 
1.39 
-0.28' 
-0.78 
1.18 
[2.21 
L2.75 = 
2.31 
1.52 
We can follow through each of these steps from Panel II of Fig. 4.9. First, in step one 
we note that the coordinates of x° after mapping onto x are, indeed, x= [2;5i] with 
respect to the E basis. In step two x is mapped onto x* = [2*75]» again with respect to the 
E basis. However, to refer the point to the F basis we employ step three, giving us 
x*° = [lisi] with respect to the obUque basis F. 
As will be pointed out in Chapter 5, the practical matter is to find a suitable basis F 
such that the matrix of the transformation with respect to this new basis takes on a 
particularly simple form, such as a diagonal matrix. Finally, it is worth noting that if L is 
orthogonal we have the simplification 
T° = LTL' 
since, as noted earlier, if L is orthogonal, then 
( L ' r ' = L 
4.3.6 
Recapitulation 
The concept of vector transformation is central to matrix algebra and multivariate 
analysis. The simplest type of transformation is represented by a point transformation, 
relative to a fixed basis: 
X* = Tx 
Usually, we choose the fixed basis to be E, the standard basis. Then the axes are mutually 
orthogonal and of unit length. In point transformations x moves according to T while the 
basis stays fixed. Figure 4.7 shows the geometric character of this type of transformation. 
Alternatively, we can allow the point to stay fixed and, instead, transform the basis 
vectors to some new, and possibly oblique, orientation. This type of transformation, 
called a basis vector transformation, is exempUfied by 
x°=(T'rx 
and is illustrated in Fig. 4.8. 
As also pointed out, if T is orthogonal, various simpUfications result that make the 
geometric interpretation easier. Finally, the specific character of some particular 
mapping, denoted generally by r, depends on the reference basis. We showed how one 
matrix of the transformation, represented by T with respect to C/, the original basis, can 
be represented by T° if we know the transformation that connects f,-, the basis for T° 
with 6/, the basis for T. 
While not illustrated in the cases that were covered, it should be mentioned in passing 
that |T| = |T°|. That is, the determinant of a linear transformation is independent of the 
basis to which the transformation is referred. 

4.4. 
GEOMETRIC RELATIONSHIPS INVOLVING MATRIX TRANSFORMATIONS 
147 
Inasmuch as matrix transformations are so central to the subject, we continue our 
discussion of the geometric character of various types of special matrices. While we have 
described transformations represented by orthogonal matrices (i.e., rotations), it turns 
out that many other kinds of matrices have intuitively simple geometric representations 
as well. 
4.4 
GEOMETRIC RELATIONSHIPS INVOLVING MATRIX 
TRANSFORMATIONS 
As we have illustrated, many matrix operations can be usefully represented 
geometrically if two or three dimensions are involved. At this point it seems useful to 
extend this line of geometric reasoning to other aspects of matrix transformations. In so 
doing the reader may get some intuitive understanding of what is involved in higher 
dimensionalities where geometrical representation is no longer feasible. 
In order to motivate the discussion, let us consider the small 9 x 2 matrix of synthetic 
data shown in Table 4.1. This matrix consists of nine points, positioned in two 
dimensions, as diagrammed in Panel I of Fig. 4.10. Note that the points involve a square 
lattice arrangement. 
TABLE 4,1 
Original Matrix X of Nine Points 
in Two Dimensions 
Point 
(code letter) 
a 
b 
c 
d 
e 
f 
g 
h 
i 
^-
Dimension 1 
1 
1 
1 
2 
2 
2 
3 
3 
3 
2 
Dimension 2 
0 
1 
2 
0 
1 
2 
0 
1 
2 
1 
We shall now describe a variety of operations on this synthetic data matrix and show 
their effects geometrically. Whereas our earHer discussions of matrix transformations 
involved transforming a single vector x into another vector x*, here we shall transform a 
set of vectors, which can be represented as a matrix X. The basic principles remain the 
same. 
In all examples of this section we deal with the simpler case of point transformations, 
rather than basis vector transformations. However, so as to show the flexibility of matrix 
transformations, in this section we postmultiply 
the original matrix X by the 
transformation matrix, in order to obtain X*. No new principles are involved in this 

148 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
c • 
b. 
a 
f. 
e • 
d 
/'• 
IT 
9 , 
c* 
f 
b 
e 
III 
2 
/ • 
h 
9* 
f. 
e 
d 
c» 
b 
a» 
VI 
2 
c , 
f i 
i\ 
b 
e\ 
h 
am 
d \ 
gm 
IV 
2 
9 • 
h 
d 
e 
a • 
b 
VII 
2 
f • 
/ ? • 
c • 
b 
a • 
f 
e 
d , 
> 
i • 
h 
9 • 
VIII 
2 
/ • 
s A ^. 
b* 
d\ 
a • 
Fig. 4.10 Geometric effect of various simple transformations. Key: I, original data; II, transla-
tion; III, reflection; IV, permutation; V, central dilation; VI, stretch; VII, counterclockwise rotation 
of 45°; VIII, shear. 

4.4. 
GEOMETRIC RELATIONSHIPS INVOLVING MATRIX TRANSFORMATIONS 
149 
change, as the reader will note in due course. Thus, the basic format to be followed 
consists of the transformation 
X* = XA 
where A assumes various special forms that exhibit simple geometric patterns. These 
special forms are of particular relevance to multivariate analysis. 
Notice, however, that each row vector in X is being postmultipHed by A to obtain each 
row vector in X*. As we shall see later, this viewpoint modifies the specific entries of 
the matrices, although all basic concepts remain unchanged from our earlier discussion in 
which A premultiplied column vectors. 
4.4.1 
Translation 
Matrix translation has to do with the problem of relating a set of points to a particular 
origin in the space. Suppose, for example, that we wished to refer the nine points of Panel 
I of Fig. 4.10 to a centroid-centered origin. By "centroid" we mean, of course, the 
arithmetic mean of each set of coordinates on each dimension. As shown in Table 4.1, the 
mean of the points on the first dimension is 2; their mean on the second dimension is 1. 
By expressing each original value as a deviation from its mean, we arrive at matrix Xj as 
shown in Table 4.2. Note that this simply involves a subtraction of X, whose entries 
represent the mean of the points on each dimension, from the original matrix X. Panel II 
of Fig. 4.10 shows the effect of the translation geometrically. 
TABLE 4.2 
Translation of Matrix X 
to Origin at Centroid 
X 
1 
0 
1 
1 
1 
2 
2 
0 
2 
1 
2 
2 
3 
0 
3 
1 
3 
2 
-
X 
2 
1 
2 
1 
2 
1 
2 
1 
2 
1 
2 
1 
2 
1 
2 
1 
_2 
I j 
= 
Xd 
- 1 
- 1 
- 1 
0 ! 
- 1 
1 
0 
- 1 
0 
0 
0 
1 
1 
- 1 
1 
0 
1 
iJ 
A translation, then, involves a parallel displacement of every point to some new origin 
of interest. In this case the centroid of the points is the origin of interest. The particular 
nature of the matrix X, consisting of the column means of X, is 
X=l/mll'X 
where 1 is the unit column vector and m = 9. We can then find X^ as 
Xd = X - l / m l l ' X 
= ( I - l / m i r ) X 

150 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
In previous sections of the chapter, all of our discussion of matrix transformations, 
involved a multiplicative form, such as 
or, in the present format. 
Ax 
X* = XA 
In translating a set of points in two dimensions, denoted by the matrix X, we see that for 
each transformed point we have the coordinates 
where h and k are constants. Hence, translation departs from the usual matrix 
multiplication format by involving the sum or difference of two matrices. 
However, by using the following device: 
{xThxf2,\) = (xn.,Xi2,l) 
1 
0 
0 
0 
1 0 
h 
k 
1 
we can obtain 
x?i=Xii-^h; 
xf2=Xi2-^k; 
1 = 1 
Note that the last equation (1 = 1) is trivial, but does enable us to express a translation in 
the multiphcative format used earlier.^ 
Translations are frequently used in multivariate analysis. In particular, the SSCP, 
covariance, and correlation matrices ail utilize mean-corrected scores and involve, among 
other things, a translation of raw scores into deviation scores. 
4.4.2 
Reflection 
Reflection of a set of points, as noted in the discussion of improper rotation in 
Chapter 3, entails multipHcation of the coordinates of each point to be reflected by - 1 . 
For example, suppose we wished to reflect the nine points of Panel II in Fig. 4.10 
"across" axis 2. This can be accomplished by multiplying each of the coordinates on axis 
1 by a - 1 , as shown in Table 4.3 and illustrated graphically in Panel III of Fig. 4.10. 
The matrix used for this purpose is represented by 
-1 O" 
- 0 1 
Similarly, if desired, one could reflect the nine points across axis 1. It should be 
reiterated, however, that reflection typically involves an odd number of dimensions. If we 
* This particular computational trick can be useful in the preparation of computer routines for 
translating the origin of a set of points. 

4.4. 
GEOMETRIC RELATIONSHIPS INVOLVING MATRIX TRANSFORMATIONS 
151 
TABLE 4.3 
Reflection of Matrix Xd 
across Axis 2 
Xd 
-1 
-1 
-1 
0 
0 
0 
1 
1 
1 
- 1 
0 
1 
-1 
0 
1 
-1 
0 
1_ 
Reflection 
[-1 
0" 
L 0 ij 
= 
1 
1 
1 
0 
0 
0 
-1 
- 1 
- 1 
-1 
0 
1 
-1 
0 
1 
-1 
0 
1_ 
were to reflect the points across both axis 1 and axis 2, the overall effect—called a 
reflection through the origin—is equivalent to a rotation of the points about an angle of 
180° from their original orientation. 
As discussed earUer, rotation followed by reflection is often referrred to as an 
"improper" rotation in the case of orthogonal matrices whose entries consist of direction 
cosines. It is relevant to point out that improper rotation satisfies the same conditions as 
proper rotation: 
1. All transformation vectors are of unit length. 
2. 
All transformation vectors are mutually orthogonal. 
The differentiating feature of improper rotation is that the determinant of this type of 
orthogonal matrix is —1, while the determinant of an orthogonal matrix constituting a 
proper rotation is +1. 
Perhaps all of this can be summarized by saying: 
1. A reflection can always be described as a proper rotation followed by reflection of 
one dimension. 
2. 
Reflection of an even number of dimensions (e.g., two, four, six, etc.) is 
equivalent to a proper rotation in the 1-2 plane, the 3-4 plane, and so on. 
3. 
Reflection of an odd number of dimensions is equivalent to a proper rotation 
followed by reflection of one dimension. 
4.4.3 
Axis Permutation 
Permutation of a set of points, as the name suggests, involves a matrix transformation 
that carries each coordinate value on axis 1 into a corresponding coordinate on axis 2, 
and vice versa. This is illustrated in Table 4.4, and the effect is shown graphically in Panel 
IV of Fig. 4.10. 
The permutation matrix 
^0 
1 

152 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
TABLE 4.4 
Axis Permutation of Matrix X^ 
Xd 
r-1 
-1 
-1 
0 
0 
0 
1 
1 
L 1 
-11 
0 
1 
-1 
0 
1 
-1 
0 
1 
Permutation 
"0 11 
1 0 
= 
r-1 
0 
1 
-1 
0 
1 
-1 
0 
1 
-r 
-1 
-1 
0 
0 
0 
1 
1 
1 
TABLE 4.5 
Central Dilation of Matrix Xd 
Xd 
Central 
dilation 
2 
0 
0 
2 
1 
1 
1 
0 
0 
0 
1 
1 
1 
-1 
0 
1 
-1 
0 
1 
-1 
0 
1 
2 
-2 
-2 
0 
0 
0 
2 
2 
2 
- 2 
0 
2 
- 2 
0 
2 
- 2 
0 
2 
that postmultiplies Xd produces an interchange of columns. However, if the permutation 
matrix premultiplies a matrix of preimages, then the rows of the preimages are 
interchanged. 
In Table 4.4, however, we see that the first and second columns of Xd are 
interchanged by means of postmultiplication of Xd by the permutation matrix. 
4.4.4 
Central Dilation 
Central dilation of a set of points entails scalar multiplication of the matrix of 
coordinates, which is equivalent to multiplication by a scalar matrix; that is, a diagonal 
matrix in which each diagonal entry involves the same positive constant X. Central 
dilation leads to a uniform expansion, if X> 1, or a uniform contraction, if X< 1, of 
each dimension. If X= 1, then the scalar matrix becomes an identity matrix, and the 
point positions remain as originally expressed. 
Table 4.5 shows appHcation of a central dilation where X = 2. Panel V of Fig. 4.10 
shows the results graphically. Scalar matrix transformations are particularly simple from a 
geometric standpoint since we see that uniform stretching or compressing of the 
dimensions takes place along the original axes of orientation. According to the present 
case, the scalar matrix is 
"2 
0 
0 2_ 
where each of the original axes is dilated to twice its original length. 
4.4.5 
Stretch 
A stretch transformation of a set of points involves application of a diagonal matrix 
where, in general, the diagonal entries are such that Xa i^ Xjf. In contrast to central 
dilation, a stretch involves differential stretching or contraction (rescaUng) of points 
corresponding, again, to directions along the original axes. 

4.4. 
GEOMETRIC RELATIONSHIPS INVOLVING MATRIX TRANSFORMATIONS 
153 
TABLE 4.6 
A Stretch of Matrix X^ 
r- 
^ d 
- 1 - l ~ \ 
-1 
0 1 
-1 
1 
0 
-1 
0 
0 
0 
1 
1 
- 1 
1 
0 
- 1 
1_ 
Stretch 
[2 
0 
[o ij 
= 
-2 
-2 
-2 
0 
0 
0 
2 
2 
2 
-11 
0 
1 
-1 
0 
1 
-1 
0 
1_ 
For example, if Xn = 2 and X22 = 1, the effect of this transformation is to stretch axis 
1 to twice its original length, thus producing a latticelike rectangle out of the original 
latticelike square. Table 4.6 illustrates the computations involved, and Panel VI of Fig. 
4.10 shows the graphical results of the transformation 
2 O] 
Here we shall restrict the term "stretch" to the case in which all X// > 0. If some Xa 
were zero, those dimensions would be annihilated. A X,7 < 0 would correspond to a 
stretching (or contraction) followed by reflection. 
4.4.6 
Rotation 
As described earlier in the chapter, axis rotation involves appHcation of a rather special 
kind of matrix—an orthogonal matrix. An orthogonal matrix is distinguished by the 
properties: (a) the sum of squares of each column (row) equals 1, and (b) the scalar 
product of each pair of columns (rows) equals zero. To illustrate, the matrix 
corresponding to a 45° rotation 
0.707 
0.707' 
-0.707 0.707_ 
meets these conditions, since, by columns, for example, 
(a) 
(0.707)2 + (-0.707)2 = 1; 
(0.707)^ + (0.707)^ = 1 
(b) 
(0.707, -0.707) 
0.707 
0.707 
= 0 
As we know, in two dimensions an orthogonal matrix entails a rigid rotation of the 
original configuration of points about some angle '^. In the preceding example we rotate 
the points counterclockwise about an angle, ^ = 45°, and this involves the following: 
cos ^ 
sin ^ 
-sin ^ 
cos ^ 
That is, cos 45° = sin 45° = 0.707, while -sin 45° = -0.707. 

154 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
TABLE 4.7 
Counterclockwise Rotation of Matrix X^ 
through 45° Angle 
Xd 
f-l 
-
- 1 
- 1 
0 
-
0 
0 
1 
-
1 
L 1 
-1 
0 
1 
-1 
0 
1 
1 
— 1 
0 
I j 
Rotation 
r 0.707 
0.707] 
L-0.707 
0.707_ 
0 
-0.707 
-1.414 
0.707 
0 
-0.707 
1.414 
0.707 
_ 
0 
-1.414 
-0.707 
0 
-0.707 
0 
0.707 
0 
0.707 
I.414J 
The reader should note that the above rotation matrix is postmultiplying each row 
vector of the matrix X^. This is in direct contrast to the rotation depicted in Fig. 4.3, in 
which the matrix of this transformation is premultiplying the column vector of 
coordinate values x. As such, the orthogonal matrix, whose effect is depicted in Fig. 4.3, 
still represents a counterclockwise point rotation but now has the form 
cos ^ 
—sin ^ 
sin ^ 
cos ^ 
This is the transpose of the matrix form shown above. 
Table 4.7 shows the appHcation of the 45° counterclockwise point rotation, and Panel 
VII of Fig. 4.10 summarizes the results graphically. Note that the zero point (intersection 
of the axes) can be viewed as the hub of a wheel and remains fixed during the rotation. 
As first discussed in Chapter 3, entries of the rotation matrix can all be expressed as 
direction cosines of the angles ^ n , ^12, ^21, ^22 made between old and new axes. The 
first subscript refers to the old axis, while the second refers to the new axis. And, as 
stated earlier, this type of generalization is important when more than two dimensions are 
involved. 
A second point of interest is that the same kind of transformation noted above can be 
made by rotating the axes, rather than the points, around an angle of -45°, relative to 
the original orientation. This altemative view, of course, was covered earlier in the 
chapter. 
4.4.7 
Shear 
A shear transformation is characterized by the following form: 
1 
1" 
0 
1 

4.4. 
GEOMETRIC RELATIONSHIPS INVOLVING MATRIX TRANSFORMATIONS 
155 
I 
II 
5 
4-1-
sl 
S 
R 
24-
4 
3 
2 | 
1 + 
Q 
- f -
Fig. 4.11 A shear transformation. Key: I, before; II, after. 
SO that postmultiplication of some matrix by a shear has the effect of adding columns. 
For example, 
a b 
c d 
Shear 
1 f 
0 
1 
a a -\- b 
c 
c + d 
while premultiphcation has the effect of adding rows. For example, 
Shear 
1 
1 
0 
1 
a b 
c d 
a + c 
b + d 
c 
d 
The geometric effect of a shear is shown, illustratively, in Fig. 4.11 for the simple case 
involving the rectangle 
/> = (!,0); 
e = (3,0); 
i^ = (3,2); 
^ = (1,2) 
When the shear transformation postmultipUes the vertices of the rectangle, we obtain: 
p 
Q 
R 
S 
"1 f 
3 
3 
3 
5 
1 3_ 
~i o] 
3 
0 
3 
2 
_1 
2J 
[l l" 
Lo i_ 
as shown in Panel II of Fig. 4.11. 

156 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
TABLE 4.8 
A Shear Transformation 
Matrix Xj 
r 
^ d 
^ 
- 1 
- 1 
- 1 
0 
-1 
1 
0 
- 1 
0 
0 
0 
1 
1 
- 1 
1 
0 
L 1 
i_ 
Shear 
fl 
l1 
[o i] 
- 1 
- 1 
- 1 
0 
0 
0 
1 
1 
1 
of 
-2l 
- 1 
0 
- 1 
0 
1 
0 
1 
2] 
If we apply the shear to the matrix X^, we obtain the coordinates shown in Table 4.: 
and plotted in Panel VIII of Fig. 4.10. 
4.5 
COMPOSITE TRANSFORMATIONS 
The transformations described in the preceding section have each been applied singly. 
It is instructive to see what happens when some of these transformations are appHed on a 
composite basis. For illustrative purposes we consider the following: 
1. a rotation followed by reflection of the first axis 
2. 
a stretch followed by a rotation 
3. 
a rotation followed by a stretch 
4. 
a rotation followed by a stretch followed by another rotation 
5. 
an arbitrary Unear transformation. 
The theoretical rationale for applying successive matrix transformation is based on the 
idea that if T is the matrix of one linear transformation and S is the matrix of a 
transformation that maps images obtained from T, then the matrix to be transformed can 
be mapped by a composite transformation that involes the matrix product TS.^ This 
same idea can be extended in the same manner, to more than two transformation 
matrices. 
The order in which successive matrix transformations are applied is quite important. 
That is, in general the results of the composite mapping involving TS are not the same as 
the images that would be obtained from the composite mapping ST, as will be 
demonstrated shortly. 
Moreover, it should be reiterated that the matrix of a transformation is uniquely 
defined only relative to a set of specific basis vectors.^ All along we have been using the 
standard basis vectors e^-, and we shall continue to do so here.^ 
* We are continuing to assume that T (and S) are postmultiplying 
(say) X^, the initial 
configuration of points. 
"^ Here we continue to distinguish between r, the transformation (e.g., a stretch or a rotation), and 
T, its characterization with respect to a specific set of basis vectors. 
• We shall continue to refer to the transformation of X^j, the configuration of points in Panel II of 
Fig. 4.10. 

4.5. 
COMPOSITE TRANSFORMATIONS 
157 
4.5.1 
Rotation Followed by Reflection 
If we multiply the following matrices: 
0.707 
0.707" 
-0.707 
0.707 
-1 
0 
0 
1 
we obtain 
-0.707 
0.707 
0.707 
0.707 
As can be easily verified, the resultant matrix has a determinant of — 1. And as indicated 
earlier, this type of matrix is called an improper rotation. Although it meets the 
conditions of an orthogonal matrix, its application actually involves a "proper" rotation, 
in which the determinant is +1, followed by a reflection of axis 1. Table 4.9 shows the 
TABLE 4.9 
An Improper Rotation of Matrix Xj 
Xd 
f-l 
-1 
-1 
0 
0 
0 
1 
1 
L 1 
- i 1 
0 
1 
- 1 
] 
0 
1 
-1 
0 
i j 
Improper rotation 
-0.707 
0.707" 
0.707 
0.707 i 
0 
0.707 
1.414 
-0.707 
0 
0.707 
-1.414 
-0.707 
0 
-1.414 
-0.707 
0 
-0.707 
0 
0.707 
0 
0.707 
1.414 
results of this composite transformation, while Panel I of Fig. 4.12 shows the geometric 
results. That is, the points in Panel II of Fig. 4.10 are first rotated counterclockwise about 
an angle of 45°, and then the (imphed) Ci * axis is reflected, as observed in Panel I of Fig. 
4.12. 
4.5.2 
Stretch Followed by Rotation 
If we multiply the following matrices: 
we obtain 
2 
0 
0 ij 
0.707 
0.707 
L-0.707 
0.707 
1.414 
1.414' 
0 .707 
0.707 
This latter composite matrix first involves a stretch of the configuration followed by a 
counterclockwise rotation of 45°. Table 4.10 shows the computations, while Panel II of 

1 5 8 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC 
VIEWPOINT 
/ 7 . 
/ ? . 
IV 
2 
d . 
h c 
Fig. 4.12 Geometric effect of various composite transformations. Key: I, rotation followed by 
reflection; II, stretch followed by rotation; III, rotation followed by stretch; IV, rotation-stretch-
rotation; V, arbitrary linear transformation. 

4.5. 
COMPOSITE TRANSFORMATIONS 
159 
TABLE 4.10 
A Stretch of Matrix X^i Followed 
by Counterclockwise Rotation 
through an Angle of 45'' 
Xd 
- 1 
- 1 
- 1 
0 
0 
0 
1 
1 
L 1 
- i l 
0 
1 
- 1 
0 
1 
- 1 
0 
ij 
Stretch-rotation 
' 1.414 
1.414 
-0.707 
0.707 
^-0.707 
-1.414 
-2.121 
0.707 
0 
-0.707 
-2.121 
1.414 
_ 0.707 
-2.121 
-1.414 
-0.707 
-0.707 
0 
0.707 
0.707 
1.414 
2.121 
Fig. 4.12 shows the geometric results. The first matrix maps the square lattice into a 
rectangular lattice, while the second transformation rotates this rectangular lattice 45°, 
counterclockwise. 
4.5.3 
Rotation Followed by Stretch 
In general, the matrix product AB 9^ BA. That is, usually the multipHcation of 
matrices (even if conformable) is not commutative. This can be illustrated rather 
dramatically by considering the following matrix product: 
0.707 
0.707' 
-0.707 
0.707 
2 
0 
0 
1 
1.414 
0.707 
-1.414 
0.707 
While the same two matrices as those used in the preceding section are employed here, we 
see that their matrix product differs markedly. In the present case we have a 
counterclockwise rotation of X^ through an angle of 45° followed by a stretch. The 
result of this is that even the original "shape" of the points (a square lattice) is deformed 
TABLE 4,11 
A Counterclockwise Rotation of Matrix Xj 
through an Angle of 45° 
Followed by a Stretch 
Xd 
- 1 
-
- 1 
- 1 
0 
-
0 
0 
1 
-
1 
L 1 
-1 
0 
1 
-1 
0 
1 
-1 
0 
IJ 
Rotation 
1.414 
-1.414 
-stretch 
0.707 
0.707 
0 
-1.414 
-2.829 
1.414 
0 
-1.414 
2.829 
1.414 
0 
-1.414 
-0.707 
0 
-0.707 
0 
0.707 
0 
0.707 
1.414 

160 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
TABLE 4.12 
Rotation-Stretch -Rotation 
Composite 
Transformation of Matrix Xj 
Xd 
r-1 
-1 
-1 
0 
0 
0 
1 1 
1 
L 1 
-11 
0 
1 
-1 
0 
1 
-1 
0 
1 
Rotation-stretch-rotation 
0.87 
1.32] 
1.58 
-0.10 
0.71 
-0.87 
-2.45 
1.58 
0 
-1.58 
2.45 
0.87 
-0.71 
-1.22 
-1.32 1 
-1.42 
0.10 I 
0 
-0.10 I 
1.42 
1.32 1 
1.22_ 
TABLE 4,13 
Application of an Arbitrary Linear 
Transformation to Matrix Xd 
r- 
^ d 
-1 -l] 
-1 
0 
-1 
1 
0 
-1 
0 
0 
0 
1 
1 
-1 
1 
0 
L 1 
1 
V 
[l 2 
[3 4 
-4 
-6 
-1 
-2 
2 
2 
-3 
-4 
0 
0 
3 
4 
-2 
-2 
1 
2 
4 
6j 
into a rhomboidlike figure. The computations appear in Table 4.11, and the geometric 
results appear in Panel III of Fig. 4.12. 
4.5.4 
A Rotation-Stretch-Rotation Composite 
As an extended case, let us now consider a 45° counterclockwise rotation followed by 
a stretch followed by a 30° counterclockwise rotation. This combination can be 
illustrated by the following matrix product: 
^ 0.707 
0.707 
-0.707 
0.707 
2 
0 
0 
1 
0.866 
0.500 
-0.500 
0.866 
0.87 
-1.58 
1.32 
-0.10 
Table 4.12 shows the results of applying this composite transformation, while Panel IV 
of Fig. 4.12 portrays the results graphically. 
4.5.5 
An Arbitrary Linear Transformation 
To round out discussion, assume that we had the arbitrarily selected Hnear 
transformation 
1 
2' 
3 4_ 
and wished to find out what would happen if this transformation were appHed to the 
matrix X^. If X^ is postmultipHed by V, we obtain the product shown in Table 4.13. 
Panel V of Fig. 4.12 shows the effect graphically. Compared to the preceding cases the 
pattern of the transformed points may look a bit strange. As we shall show in the next 
chapter, however, even the arbitrary linear transformation V can be represented as a 
composite of more simple transformations, of the types illustrated in Fig. 4.10. 
Anticipating material to be described in Chapter 5, Fig. 4.13 shows three configura-
tions that successively portray the movement of the points of X^ from their original 
positions in Panel II of Fig. 4.10 to their positions shown in Panel V of Fig. 4.12. For the 
moment we shall do a bit of "hand waving" and present the results of decomposing V 
into the product of simpler transformations. 

4.5. 
COMPOSITE TRANSFORMATIONS 
161 
III 
2 
f 
. c 
/ 
'^ 
\ 
9 
° 
b 
9* 
• 
•d 
/• 
• f 
h 
XdP 
Xd[PA] 
Xd[PAQ'] 
Fig. 4.13 
Decomposition of general linear transformation V = PAQ'. 
First, suppose we postmultiply X^j by the orthogonal transformation 
"-0.41 
-0.91" 
P = 
-0.91 
0.41 
In this case P produces a 66° clockwise rotation of points in X^, followed by a reflection 
of the first axis. This preliminary result appears in Panel I of Fig. 4.13. 
Next, let us assume that the configuration in Panel I is stretched in accordance with 
the diagonal matrix 
~5.47 
0 ^ 
A = 
0 
0.37 
This operation is shown in Panel II of Fig. 4.13 
Finally, let us assume that the configuration in Panel II is further rotated by the 
orthogonal matrix 
Q' = 
-0.58 
-0.82 
0.82 
-0.58 
This rotation involves a clockwise movement of 125° from the orientation in Panel II. 
The results appear in Panel III of Fig. 4.13. Table 4.14 shows the accompanying 
numerical results. 
The upshot of all of this is that the arbitrary hnear transformation 
"1 2" 
V = 
3 4 
has been decomposed into the product of an improper rotation, followed by a stretch, 
followed by another rotation. This triple product can be represented as 
V = PAQ' 

162 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
•S 
•-3 
^ 
I 
a 
< 
" 1 — 
o ^ 
< ' 
TJ 1 
X 1 
1 
00 
S o 
O 1 
O 00 
o 
1 
'ON 
t-H 
.—, o 
<I 
0U 
1 
<N 
1 
^ 
1 
^ 
00 
«o 
o 
1 
00 
d 
-^ 
CO 
O 
'^ 
(N 
«N 
11 
1 
^ ^ 
o 
1 
r^ 
en 
o 
o 
<N 
<N 
1 
0^ 
-"^ 
o 
TT 
t--
1 
1 
J 
'^ 
1 
CO 
1 
lO 
?-H 
o 
1 
oo 
O N 
Tt 
o 
o 
o 
o 
Tf 
ro 
«0 
1-H 
o 
00 
ON 
1 
<N 
1 
<N 
1 
ON 
"^ 
O 
1 
-^ 
r^ 
rs 
<s 
^ 
-* 
CO 
CO 
I 
rf 
CN 
1 
1 
<D 
^ 
1 
1 
^—( 
o 
1 
ra 
<N 
1 , 
i o O N c o T f O ' « i - c o a N » o 
d d - 5 d 
d r - ; d d 
Q. 
I 
I
I
I 
C O T t l O O N O O N ' O ' ^ C O 
^ d d d 
S d) d> ^ 
I 
I
I
I 
c 
o 
•
^ 
(Tl 
u 
CX 
c 
o 
•^ 
o 
(U 
j;^ 
o 
1 
1 
ON Tj-
o o 
1 
r—1 r-H 
-^ ON 
O O 
1 1 
1 
^
O
^
r
^
O
^
^
O
^ 
13 
X ^
^
^
O
O
O
—
t
^
r
-
< 

4.6. 
INVERTIBLE TRANSFORMATIONS AND MATRIX RANK 
163 
where both P and Q are orthogonal and A is diagonal. This rather remarkable result is of 
extreme importance to multivariate analysis.^ A major part of Chapter 5 is devoted to 
discussing this class of decompositions in some detail. 
4.5.6 
Observations on Composite Transformations 
Clearly we could continue with various other kinds of composite transformations, 
although sufficient variety has been shown for the reader to get some idea of their 
geometric effect. As noted above, each simple matrix, such as a stretch, rotation, or 
reflection, is associated with a geometric analogue. It is when these operations are 
considered in a composite way that the overall transformation appears complex. 
As it turns out, however, the value of this approach Hes precisely in looking at the 
other side of the coin. That is, by decomposing seemingly complex-appearing matrices 
into the product of simpler ones, we can gain a geometric understanding of the 
transformation in a direct, intuitive way. 
And, so it turns out, any nonsingular matrix transformation with real-valued entries 
can be uniquely decomposed into the product of either (a) a rotation^ followed by a 
stretchy followed by another rotation or (b) a rotation^ followed by a reflection, followed 
by a stretch, followed by another rotation. 
This important and useful result will stand us in good stead in examining the geometric 
aspects of various multivariate techniques in Chapters 5 and 6. Its value Hes in 
contributing to our understanding of what goes on under various matrix transformations. 
Indeed, still further generalizations are possible in cases where the matrix transformation 
is singular, as will be examined in Chapter 5. 
4.6 
INVERTIBLE TRANSFORMATIONS AND MATRIX 
RANK 
As pointed out at the beginning of the chapter, all matrix transformations involve sets 
of linear equations; conversely, sets of linear equations can be compactly displayed in 
matrix form. The purpose of this section is to pull together material briefly presented 
earlier on the topics of matrix inversion and determinants, along with additional concepts 
as related to the general objective of solving sets of simultaneous Hnear equations. 
As the reader may recall from basic algebra, in the general problem of attempting to 
solve m linear equations in n unknowns, three possibilities can arise: 
1. The set of equations may have no solution; that is, they may form an inconsistent 
system. 
2. 
The set of equations, while consistent, may have an infinite number of solutions. 
3. 
The set of equations may be both consistent and have exactly one solution. 
Solutions of simultaneous linear equations based on the application of inversion assume 
that the number of equations equals the number of unknowns. In this instance, the 
matrix of coefficients is square. If other conditions (to be described) are met, the matrix 
' The representation V = PAQ' is variously called decomposition to basic structure or singular 
value decomposition. 

164 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
of coefficients has a unique inverse, and exactly one solution exists for the set of 
equations. Accordingly, we emphasize the last of the three cases above since this is the 
one that involves matrix inversion and, furthermore, is most relevant for multivariate 
analysis. 
Appendix B discusses the topic of simultaneous hnear equations from a much broader 
point of view, one that encompasses all three of the preceding cases and describes their 
characteristics in detail. 
As recalled, a set of n linear equations in n variables can be compactly written in 
matrix form as 
where A is the « x « matrix of coefficients, x is the « x 1 column vector of unknowns, 
and b is the ^ x 1 column vector of constants. For example, suppose we had the 
following equations: 
4JCI - 
10JC2 
3^1 + 
1X2 '• 
These can be written in matrix form as 
13 
A 
r4 
- l o i 
_3 
7J 
x 
fjCi 
L-^2_ 
b 
~-2~ 
13_ 
If the inverse A ^ exists, then we also know that the following relationship holds: 
AA-' =A-'A = I 
where A" Ms unique. We can solve for x, the vector of unknowns, as follows: 
Ax = b 
A-'Ax=A-'b 
Ix=A-'b 
x = A-'b 
Furthermore, from previous discussion we also know that A"^ exists, provided that 
|A| 9^ 0. In the current example, |A| = 58. From Section 4.3.2, we can find A"^ by 
dividing each entry of the adjoint of A, adj(A), by the determinant of A: 
adj(A) 
A-^ = 
10 
4 
7/58 
5/29 
L-3/58 
2/29. 
If b is then premultiplied by A \ we obtain the solution vector: 
'2~ 
x = 1 

4.6. 
INVERTIBLE TRANSFORMATIONS AND MATRIX RANK 
165 
In short, if the determinant of |A|, the coefficient matrix, is not equal to zero, computing 
the inverse A" Ms a useful way to solve sets of linear equations in which the number of 
equations equals the number of unknowns. 
Two major questions crop up in the discussion of general solution methods for sets of 
linear equations: 
1. If the conditions are such that matrix inversion methods can be employed, what 
are some of the properties of matrix inverses? 
2. 
Suppose A"^ does not exist, but we still want to say something about those 
aspects of the space that are preserved under the linear transformation A. What is the 
connection between the number of linearly independent dimensions in the transfor-
mation and the number of dimensions that are preserved under that transformation? 
Discussion of these two questions constitutes the primary focus of this section of the 
chapter. 
4.6.1 
Properties of Matrix Inverses 
In Section 4.3.3 we described one important property of matrix inverses, namely, that 
the inverse of the product of two or more (conformable) matrices equals the product of 
the separate inverses in reverse order: 
(TiT2---T,)-^=T s 
. ^ - 1 ^ - 1 
Moreover, in Section 4.3.4 we made note of the fact that if a matrix B is orthogonal, then 
Some other aspects of matrix inverses are also useful to point out: 
1. The inverse of an inverse is the original matrix: 
(A->)-'=A 
2. 
The inverse of a scalar times a matrix equals the reciprocal of the scalar times the 
matrix inverse: 
(A:A)-^ = 1/A:A-' 
3. 
The inverse of the transpose of a matrix, equals the transpose of the inverse: 
(A')-'=(A-')' 
4. 
The inverse of the diagonal matrix D is obtained by simply finding the reciprocals 
of the entries on the main diagonal: 
(p-') = dmg(dfly 

166 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
5. If A, B, and C are each square and of order nby n, and if A is nonsingular, then 
AB = AC 
implies 
B = C 
Note, then, that the cancellation law of scalar algebra holds over the set of nonsingular 
matrix transformations. We shall use several of the above properties in Chapters 5 and 6 
that employ inverses in various types of multivariate computations. In the present 
geometrically oriented context, however, we note that inverses relate to invertible 
functions in which for every vector in one space we have one and only one uniquely 
paired vector in another space. This "other" space may, of course, be the original space. 
The image vector is then another point in the same space as the preimage vector. 
4.6.2 
Characteristics of Invertible Transformations 
If we consider the following transformation: 
T 
X 
2.23 
2.20 
0.45 
0.89 
0.60 
0.80 
we note that a point x = [2] in two dimensions is mapped by T onto x*, another point in 
that same space. Furthermore, by methods discussed earher, we could find T"^ and 
observe the following: 
-4.60 
5.11 
3.45 
-2.59 
2.23 
2.20 
Here, the inverse T"^ maps x* in the plane onto x in the plane. This is an illustration of a 
one-to-one, or invertible^ transformation in which to each point in the (xi, JC2) plane, we 
have one and only one point in the {x^ *, X2*) plane. Most of our discussion in Sections 
4.4 and 4.5 centered around invertible transformations. 
If T~^ exists, every x has a unique x* and vice versa. Invertible transformations exhibit 
the important property of being nonsingular and, hence, square. Furthermore, as long as 
all (square) transformations Ti, T2, .. . , Tp are each nonsingular, their product is also 
nonsingular, and all information about x is preserved in the mapping in the sense that 
(T1T2 • • -Tp)"^ could undo the original composite transformation by transforming x* 
back to X. 
What happens if T is singular and, hence, T"^ does not exist? As might be surmised, if 
such is the case, we lose the one-to-one correspondence between x and x*. Moreover, it 
may be the case that additional information about x is irretrievably lost in the mapping 
process. What can be said about the dimensionality of the image space under these 
circumstances? A discussion of this question involves a topic of central importance to 
matrix algebra, namely, the rank of a matrix. 

4.6. 
INVERTIBLE TRANSFORMATIONS AND MATRIX RANK 
167 
4.6.3 
The Rank of a Matrix 
The concept of matrix rank is related to two topics that have already been discussed in 
earlier chapters: 
1. linear dependence of a set of row or column vectors, 
2. 
the determinant of a matrix. 
There are two basic, and compatible, ways of defining the rank of a matrix. 
One definition takes a dimensional, or geometric, viewpoint. Assume that we have a 
matrix A that is not necessarily square. The rank of Pi, denoted by r(A), is defined as the 
maximum number of linearly independent rows (columns) of A. While it may seem 
strange to say that the row rank of a matrix is always equal to its column rank, such is the 
case, as we shall illustrate subsequently. Any matrix, square or rectangular, has a unique 
rank, one that equals the maximum number of linearly independent vectors. 
In Chapter 3 we discussed the concept of linear independence of row or column 
vectors. In fact, the Gram-Schmidt process was illustrated as a way of finding an 
orthonormal basis from a set of arbitrary basis vectors. 
At that time we pointed out that n linearly independent vectors, each of n 
components, are sufficient to define a basis. If more that n vectors are present, then the 
set cannot be linearly independent. However, if less than n vectors are present, they are 
insufficient to span a space of A? dimensions. That is, either the number of components or 
the number of vectors is sufficient to constrain the dimensionality. 
What this boils down to is that if a matrix A is rectangular, its rank cannot exceed its 
smaller dimension (rows or columns as the case may be). Moreover, its row rank equals its 
column rank so that we can say without ambiguity that r(A) = k, where k is some 
nonnegative integer. Of course, this in itself does not say that r(A) must equal the lesser 
of m or /t; its rank k may be less than the minimum of m or n. All that is being said is that 
one can talk just as appropriately about m points in n dimensions as n points in m 
dimensions and that the lesser of the two numbers denotes the maximum subspace in 
which the points are contained. 
Notice that the definition of matrix rank refers to the maximum number of rows 
(cohimns) that are linearly independent. As suggested above, if a matrix A is of order 
mxn, 
and if the matrix has rank r(A) = k, then there exist k rows and k columns, where 
k < min(m, n) that are linearly independent. Furthermore, any set of A: + 1 rows 
(columns) is linearly dependent. 
The reader will recall that we also discussed determinants in Chapter 2 and elsewhere. 
It turns out that a fully compatible definition of rank can be developed from the 
foundation of determinants. 
The rank of a matrix A, denoted r{A), is the order of the largest square submatrix of A 
whose determinant is not zero. 
To illustrate the nature of the latter definition, suppose we have the following square 
matrix: 
A = 
1 
2 
4 
2 
1 2 
3 
2 
4 

168 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
By inspection we note that the third column is a multiple of the second; clearly |A| = 0 
and /-(A) is not 3. However, consider one of the 2 x 2 submatrices, for example, 
B = 
1 
2 
2 
1 
We find that |B| = - 3 ; hence, r(A) = 2. 
Next, suppose we have the following rectangular matrix: 
A = 
1 
2 3 
1 
1 2 
Clearly, there are no square submatrices of order 3 x 3 since the matrix is only 2 x 3 ; 
hence, A'(A) is at most 2. If we take one of the 2 x 2 submatrices 
B = 
2 
3 
1 
2 
we find that |B| = 1; hence r(A) = 2. 
It should be stated that we have to find only one square submatrix (of the desired 
order) with a nonzero determinant. Once we have done so, we can stop the search. 
By the systematic examination of determinants of various submatrices of A, we have a 
way to go about finding r(A) in either the square or rectangular matrix cases. Also, we 
should note that the lowest rank of any matrix must be zero, and this would happen only 
if A=0; that is, the matrix consisted of all zeros. Otherwise, there would be some 
nonzero element in the single-element minors, and r(A) would at least be 1. 
How do we relate the concept of matrix rank to the topics of matrix inverse and 
invertible transformations described earher? Perhaps the most direct way is to state that if 
we have a square (n x n) transformation matrix A, whose inverse A"^ exists, the following 
statements are all equivalent: 
1. A is nonsingular; that is, |A| 9^ 0. 
2. 
The rank r(A) =n; that is, A is of full rank in which its rank equals its order. 
3. 
The row vectors of A are linearly independent. 
4. 
The column vectors of A are linearly independent. 
5. 
The image space obtained from A fully preserves the preimage space in a 
one-to-one fashion. 
6. 
One can obtain the unique preimages transformed by A from their counterpart 
images by means of A"\ the inverse. 
7. 
The specific image points and preimage points are in one-to-one correspondence. 
Of course, not all transformations of interest to multivariate analysis will involve cases in 
which the inverse A"^ exists. Accordingly, means are needed to find out what happens 
when the transformation is not fully invertible. Accordingly, we now consider some of 
the difficulties that arise when the transformation matrix is singular. 

4.6. 
INVERTIBLE TRANSFORMATIONS AND MATRIX RANK 
169 
4.6.4 
The Relationship of Rank to Linear Transfonnations 
As might be surmised at this point, the rank of a matrix transformation is quite 
important in matrix algebra. Consider the transformation matrix 
T = 
~1 
0 
0 
o' 
1 
1 
and the vector x = [I]. Via matrix multiplication, we can find the point transformation 
x* = Tx as follows: 
"l 
0~ 
0 
1 
0 
1^ 
1 
2 
— 
~r 
2 
_2j 
and, presumably T has taken x into three dimensions. However, as can be seen in Fig. 
4.14, the transformation rotates the Cj, 62 plane through a 45° angle. All of the points in 
the Ci, 62 plane, including x = [ 2 ] , undergo this rotation. The range of the 
transformation is still a plane. What should be remembered is that the range of any 
transformation that takes a point in n dimensions into a point in m dimensions (where 
m> n) cannot exceed n; that is, the higher space cannot be filled from a space of lower 
dimensionahty. 
2 + 
1 + 
(1,2) 
i{1,1) 
^^ 
Fig. 4.14 
Fig. 4.15 
Fig. 4.14 
Geometric effect of a linear transformation involving a point in a lower dimensionality. 
Fig. 4.15 
Geometric effect of a linear transformation involving a point in a higher dimensionality. 

170 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
Notice that T has only two columns, and r(T) is at most equal to 2. In this caser(T)/5 
equal to 2, and we observe that the third row of T equals the second. Thus, the rank of T 
has placed restrictions on the dimensionaUty of the transformation. 
Now consider the matrix 
1 
0' 
0 
0 
In this case if we desire to find x* = Sx, we have 
1 
O l 
_0 OJ 
1 
L2_ 
= 1 
_0_ 
and, indeed, all points in two dimensions will be mapped onto the (single) Ci axis. That 
is, r(S) = 1 and, hence, the transformation maps all vectors, Xi, X2, X3, etc., onto a line, 
regardless of the original dimensionality of Xi, X2, and X3. Fig. 4.15 shows this effect 
geometrically. Note in particular that points (1, 2) and (1,1) are not distinguished after 
this transformation. Thus, the transformation preserves the first component of the vector 
but not the second; these all become zero, and this information is irretrievably lost. 
The fact that the rank of a matrix and the number of linearly independent vectors of a 
matrix are equal is important in the understanding of matrix transformations generally. 
As noted above, knowledge of the rank of a transformation matrix provides information 
about the characteristics of the original dimensionality that are "preserved" under the 
mapping. 
To round out the preceding comments, we can extend the discussion of Section 3.3.5 
on vector projection to the more general case of projecting a vector in n dimensions onto 
some hyperplane of dimension k (k < n) passing through the origin of the space (see 
Panel IV of Fig. 3.15 for an illustrative case). 
For example, suppose we have a three-dimensional space and a plane passing through 
the origin of that space and through the two points 
bi 
b2 = 
Let us also assume that the vector of interest is represented by 
"2" 
a = 
1 
in the full space of three dimensions. If so, the projection a* of a onto the plane defined 
by 0, bi, and b2 is given by 
a* = B(B'B)-^ B'a 

4.6. 
INVERTIBLE TRANSFORMATIONS AND MATRIX RANK 
171 
where B is the matrix whose columns are the vectors bi and bj in the plane of interest. In 
terms of this numerical example, we have 
B 
(B'B)-' 
B' 
a 
'l 
f 
1 0 
0 1 
1 
3 
2 
- 1 
-l1 
2J 
1 1 0 
1 0 ij 
[2^ 
3 
[l 
= 
' 8 / 3 ' 
7/3 
_2/3_ 
a^^ = 
and a* is the vector representing the projection of a onto the plane defined by 0, the 
origin, and bi and b2. 
Since the line through 0 and bi is in the plane, an orthogonal projection implies that 
(a* — a) be perpendicular to bi; similarly so for the line through 0 and b2, giving the 
equations 
(a'^-aVbi =0; 
(a*-ayb2 =0 
A brief sketch of the derivation of a* may be in order. In more general terms, if the 
dimensionality of the full space is n and k denotes the dimensionality of the hyperplane 
defined by 0 and the b^-, then the /i x A: matrix B has rank k, and we have 
(a* - a)'bi = 0; 
(a* - a)'b2 = 0;.. . ; (a* - a)'b;t = 0 
or 
a*'b| = a'b/ 
for 
/ = 1,2,. . . , /: 
In matrix notation, this can be written as 
a*'B = a'B 
However, since a* is itself in the hyperplane, it represents a linear combination of the 
vectors b,-: 
or, equivalently, 
a* = Z Pihi 
i=l 
a* = Bp 
v^ere p is a vector of arbitrary scalars defining the linear combination. Substituting for 
a*' above, we have 
p'B B = a'B 
B is n xk and of rank k. B'B is k xk and nonsingular, and we can postmultiply both 
sides by (B'B)"^ to get 
p' = a'B(B'B)-' 
Next, we can find the transpose of p': 
p = (B'B)-^B'a 

172 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
and recall that (B'B)"^ is symmetric. Finally, we substitute B"^a* for p and then 
premultiply both sides by B to get 
a* = B(B'B)-^ B'a 
as desired. 
The reason for introducing this generalization of vector projection at all is that the 
idea of (orthogonal) projection plays a central role in least squares, one of the 
comerstone methods in multivariate analysis. 
4.6.5 
Transformations Involving Singular Matrices 
To round out our discussion of matrix rank and its relationship to determinants and 
linear independence, let us consider the geometric effect of a singular matrix from 
another viewpoint. In Section 3.6.1 we showed how the determinant of a 2 x 2 
transformation matrix T measures the ratio of areas of the transformed to original figure. 
As illustrated, in the case of vertices of the unit square 
O = (0,0); 
/ = (1,0); 
/ = (1,1); 
K = iO,l) 
Transformed by 
T = 
2 
1 
3 
4 
we obtained the quadrilateral, reproduced in Panel I of Fig. 4.16. The ratio of areas 
between transformed and original figure is given by |T| = 5. 
Now suppose we transformed the vertices of the unit square by 
U^ 
2 
4 
3 
6 
10-
8-
6 
4-
2 
Jt^* 
fj 
fjr 
P^", 
. 
, , 
10 
8 
6-
4-
2 
/ J * 
• A-
H— X 
—4 
2 
4 
6 
8 10 
6 
8 
10 
Fig. 4.16 
Geometric relationships involving nonsingular and singular transformations. 

4.6. 
INVERTIBLE TRANSFORMATIONS AND MATRIX RANK 
173 
leading to 
V = 
2 
4 
3 
6 
O I 
J K 
0 
1 1 0 " 
0 
0 
1 1 
O* I* 
J* 
K* 
0 
2 
6 
4 
0 
3 
9 
6 
When we plot the new points of V in Panel II of Fig. 4.16, the disconcerting outcome is 
that V becomes a straight line of zero area. 
This result, of course, is not hard to understand when we note that |U| = 0 and that 
the entries in the second column of U are twice those in the first. Under T"\ the inverse 
of T, we could reverse the mapping and get back to the unit square in Panel I of the 
figure. This is not possible in the case of the second matrix U since U"^ does not exist. 
Hence, whenever a transformation matrix U is singular, that mapping is not invertible. 
Panel III shows another case, this one involving a 3 x 3 transformation matrix: 
W = 
2 
1 1 
1 
2 
2 
2 
4 
4 
as applied to the vertices of the unit cube. Here, the entries in the third row are twice 
those of the second, and all points of the unit cube lie on a plane through the origin that 
has the equation z = 2y. 
Hence, the parallelepiped that would have been obtained had W been nonsingular has 
collapsed into a parallelogram of only two dimensions. Again we have the case in which 
the original mapping obtained from W is not invertible, and the rows (columns) of W are 
not linearly independent.^^ 
4.6.6 
Finding the Rank of a Matrix via Determinants 
A number of methods are available for finding the rank of an arbitrary matrix. Perhaps 
the most straightforward, if tedious, precedure is by means of determinants. If A is square 
and of order n xn,we first see if |A| ^ 0. If so, then r(A) = n. 
If |A| is zero, we then examine square submatrices of order (n — 1) x (« — 1). If one of 
these has a nonzero determinant, then we stop and state that r(A) = /t - 1. If all 
determinants are zero, we continue with square submatrices of order (w - 2)x(n - 2), 
and so on. 
If A is rectangular of order m xn, we know at the outset that r(A) < min(m, n). 
Having established which order is smaller—suppose it is m—we examine square 
^^ Note, however, that the x, y dimensions are retained; it is only the third dimension that 
collapses. The rank of the transformation indicates how many dimensions will be retained (two out of 
three in this case). 

174 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
submatrices of order m x m to see if one can be found whose determinant is nonzero. If 
so, then r(A) =m. If not, we examine square submatrices of order (m — 1) x (m — 1), and 
so on, as indicated above. 
To illustrate how one might go about finding the rank of a matrix via these 
procedures, we can examine a few examples. First, consider the 2 x 2 matrix 
~1 2 
4 3 
We compute the determinant of A and find that |A| 
that A has rank 2. 
On the other hand, consider the following case: 
-5. Since |A| 9^ 0, we conclude 
B = 
3 
1.5 
where we first find that |B| = 0. Since the determinant of the original matrix has vanished, 
we must examine single-element minors. Since there are first-order determinants, such as 
|2| or |3|, that are not equal to zero, B has rank 1. 
Now let us examine the 3 x 4 matrix: 
1 
1 2 
2 
2 
4 
1 
0 
3 
5 
10 
1 
At the outset we know that the rank of C cannot exceed 3, the smaller order. However, 
we see immediately that the second row is twice the first row; hence C cannot be of rank 
3 since the row vectors are not linearly independent. But, since the first and third rows 
are linearly independent (i.e., neither is a multiple of the other), C has rank 2. This could 
be checked by observing that any of the six square 2 x 2 submatrices: 
'I. 
3!' 
'l 
2 
0 
3 ' 
~1 5 
1 1_ ' 
"l 
5 
0 
1 ) 
2 
5" 
3 
1_ 
made up of elements from rows 1 and 3 each has a nonzero determinant. [Of course, only 
one such submatrix would be needed to establish that r (C) = 2.] 
If the order of the matrix is large, the procedure outlined above becomes rather 
tedious. Fortunately, other ways of finding the rank of a matrix exist. Some of these are 
described in Chapter 5, and one of these-based on the echelon matrix procedure-is 
discussed in the last main section of this chapter. 
4.6.7 
The Uses of Rank in Matrix Algebra 
The rank of a matrix plays several important roles in matrix algebra. For example, in 
solving a set of simultaneous linear equations, it is the case that when (and only when) 
the rank of the matrix of coefficients equals the rank of the augmented matrix, the set of 
equations has at least one solution. By "augmented matrix" is meant a matrix consisting 

4.6. 
INVERTIBLE TRANSFORMATIONS AND MATRIX RANK 
175 
4xi -
3A:I + 
[4 
3 
10X2 
1X2 
-10 
7 
= -2 
= 13 
-2~ 
13 
of the coefficients to which has been appended an additional column made up of the 
constants. For example, in the equations described earUer: 
the augmented matrix is 
If the rank of this matrix equals the rank of the matrix of coefficients: 
"4 
-10" 
the system is consistent and has at least one solution. 
Second, if the set of equations is consistent, their solution is unique if (and only if) the 
rank of the coefficients matrix and, hence, that of the augmented matrix equals the 
number of unknowns. In this case the sytem can be solved by inversion of the coefficients 
matrix. If the rank is less than the number of unknowns, then an infinite number of 
solutions exist. While we do not delve into detailed discussion here of these two major 
uses of matrix rank (see Appendix B for this), enough has been said to show the 
importance of the concept in the solution of simultaneous equations. 
A second important role that is played by matrix rank concerns those aspects of a 
configuration of points that are preserved under matrix transformations. As we know, 
two linearly independent vectors are needed to span a plane, three to span a 
three-dimensional space, and so on. And, as illustrated in Figs. 4.14 and 4.15, the rank of a 
matrix determines what aspects of the configuration will be retained after transformation. 
Furthermore, as illustrated in Fig. 4.16, if the transformation is not of full rank, the 
image of a two-dimensional unit square could collapse to a line or to the origin; the image 
of a three-dimensional unit cube could collapse to an area or to a line, or to the origin./^ 
general then, the rank of a transformation matrix determines the dimensionality of the 
image space. Perhaps more than anything else, this is the essential aspect of matrix rank in 
multivariate analysis. 
At this point we have discussed matrix inversion as a way to solve simultaneous 
equations in which the number of equations equals the number of unknowns and, hence, 
where it is possible that an inverse of the matrix of coefficients exists. We have also 
examined what happens when a square matrix is singular and the effect that this has on 
the transformation. 
However, what has not been discussed in detail as yet are three related topics: 
1. What other procedures are available for finding the rank of a matrix? 
2. 
What procedures, other than computation of the adjoint matrix, are available for 
finding the inverse of a matrix? 
3. 
How can these numerical methods for finding inverses be applied to problems in 
multivariate analysis? 
The last main section of the chapter takes up these questions. 

176 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
4.7 
METHODS FOR RANK DETERMINATION AND MATRIX 
INVERSION 
In multivariate analysis numerous occasions arise in which we wish to solve a set of 
simultaneous equations. Often the system will be consistent, and the matrix of 
coefficients will be square and of rank equal to its order; if so, a solution based on matrix 
inversion can be found. 
Sometimes, however, the matrix of coefficients will either be rectangular or, even if 
square, singular. In the latter case its rank will not equal its order, and we may wish to 
find out—by means other than the tedious examination of determinants of square 
submatrices-what the rank of the transformation matrix is. A highly general approach to 
determining the rank of a matrix makes use of what are called elementary operations and 
the associated construction of echelon matrices. We first discuss this alternative approach 
to the determination of rank and its relationship to the solution of simultaneous linear 
equations. 
We then return to the pivotal method, first used in Chapter 2, to compute 
determinants. As mentioned there, the pivotal method is also applicable to solving 
simultaneous equations and computing inverses. We discuss these extensions and illustrate 
their application to the 4 x 4 matrix that was described in Table 2.2 and to statistical data 
drawn from the sample problem of Table 1.2. 
4.7.1 
Elementary Operations 
Elementary operations play an essential role in the solution of sets of simultaneous 
equations. Illustratively taking the case of the rows of a transformation matrix, there are 
three basic ope rations-called elementary row operations—that can be used to transform 
one matrix into another. We may 
1. interchange any two rows; 
2. multiply any row by a nonzero scalar; 
3. 
add to any given row a scalar multiple of some other row. 
/ / we change some matrix A into another matrix B by the use of elementary row 
operations, we say that B is row equivalent to A. 
Elementary row operations involve the multiplication of A by special kinds of matrices 
that effect the above transformations. However, we could just as easily talk about 
elementary column operations-the same kinds as these shown above-that are applied to 
the columns of A. A matrix so transformed would be called column equivalent to the 
original matrix. To simplify our discussion, we illustrate the ideas via elementary row 
operations. The reader should bear in mind, however, that the same approach is 
apphcable to the columns of A. 
To illustrate how elementary row operations can be applied to the general problem of 
solving a set of simultaneous equations, let us again consider the two equations described 
earher: 
r4xi-10jC2 = - 2 
ISxi + 7:^2 = 13 

4.7. 
METHODS FOR RANK DETERMINATION AND MATRIX INVERSION 
177 
III 
Suppose we first multiply all members of the first equation by —| and then add the result 
to those of the second equation: 
C4xi - 10^2 = - 2 
Next, let us multiply the second equation by ^ . If so, we obtain 
f4xi - 10;c2 = - 2 
JC2 = 1 
All these sets of equations are row equivalent in the sense that all three sets have the same 
solution: 
Xi = 2 
X2 = 1 
and each can be transformed to either of the others via elementary row operations. 
However, it is clear that the solution to the third set of equations is most apparent and 
easily found. 
While elementary row operations are useful in the general task of solving sets of 
simultaneous equations, this is not their only desirable feature. A second, and major, 
attraction is the fact that elementary operations (row or column), as applied to a matrix 
A, do not change its rank. Moreover, as will be shown, elementary operations transform 
the given matrix in such a way as to make its rank easy to determine by inspection. As it 
tums out, all three sets of equations above have the same rank (rank 2) since they are all 
equivalent in terms of elementary row operations. 
Elementary row operations are performed by a special set of square, nonsingular 
matrices called elementary matrices. An elementary matrix is a nonsingular matrix that 
can be obtained from the identity matrix by an elementary row operation. For example, 
if we wanted to interchange two rows of a matrix, we could do so by means of the 
permutation matrix 
^0 
\ 
.1 0_ 
For example, if we have the point x = [2] in two dimensions, the premultipHcation of x 
by the permutation matrix above would yield:^^ 
0 
1 
1 
0 
We note that the coordinates of x have, indeed, been permuted. 
** In this section of the chapter we use the general format x* = Ax, in which the transformation 
matrix premultiplies the vector (or matrix) of interest. The reader should become comfortable in using 
either (pre- or postmultiplication) mode. 

178 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
As mentioned above, elementary matrices are nonsingular. In the 2 x 2 matrix case, 
the set of elementary matrices consists of the following: 
I. Permutation 
III. 
k 
0' 
0 
1 
A stretch or compression 
of the plane that is 
parallel to the x axis; 
1 0 
0 
k 
A stretch or compression 
of the plane that is 
parallel to the y axis 
Shears 
1 c~ 
0 
1 
A shear parallel to 
the X axis; 
"1 
0 
c 
1 
A shear parallel to 
the y axis 
~k 0l 
_0 
i j 
[\ 
L2_ 
~k 
_2_ 
5 
1 Ol 
0 
k\ 
Ti" 
_2_ 
1 
_2'fe_ 
Continuing with the numerical example involving x = [2 ], we have 
Stretches 
Shears 
But, there is nothing stopping us from applying, in some prescribed order, a series of 
premultiplications by elementary matrices. Furthermore, the product of a set of 
nonsingular matrices will itself be nonsingular. 
The geometric character of permutations, stretches, and shears has already been 
illustrated in Section 4.5. Here we are interested in two major apphcations of elementary 
row operations and the matrices that represent them: 
1. determining the rank of a matrix, and 
2. 
finding the inverse of a matrix, when such inverse exists. Each application is 
described in turn. 
'1 c~\ 
0 IJ 
fl 
[2. 
= 
~1 +2c 
2 
5 
"1 
0 
_c ij 
[1 
_2_ 
= 
1 
_c + 2_ 
4.7.2 Elementary Operations and Matrix Rank 
Three properties of matrix rank are of general interest to matrix algebra: 
1. The rank of an AI x « identity matrix I„x«» is equal to n. 
2. 
The rank of a matrix is not changed by its premultipHcation (or postmultipH-
cation) by a nonsingular matrix. In particular, elementary row operations involve 
nonsingular matrices and, hence, do not change the rank of the matrix being transformed. 

4.7. 
METHODS FOR RANK DETERMINATION AND MATRIX INVERSION 
179 
3. 
If two matrices A and B have ranks that are denoted by r(A) andr(B), and their 
product AB is possible, then the rank of their product, denoted r(AB), is less than or 
equal to the smaller rank of the two matrices: 
r(AB)<min[r(A),r(B)] 
Suppose we first look at the impHcations of the second and third of the above properties. 
The second property indicates that if some matrix A has rank /:, then multiplication by 
an elementary matrix will not change its rank. This is a special case of the third property, 
in which both matrices in the matrix product are square and of rank k. 
The third property is important to our earHer discussion of invertible transformations. 
If the transformation matrix has a rank that is less than the matrix being transformed, all 
information in the preimages will not be preserved in the image space (as was illustrated 
in Fig. 4.15). 
Suppose, now that we wish to find the rank of some arbitrary matrix A. Assume that 
we operate on A via a series of elementary row operations. As noted, application of a 
sequence of elementary matrices, each of which is nonsingular, will not change the rank 
of A but could transform A to a structure in which its rank can be determined by 
inspection. This particular type of matrix—one that is obtained by a series of elementary 
row operations—is called an echelon matrix. To illustrate, consider the following 
rectangular matrix: 
H = 
0 
1 3 
0 
4 
0 
0 
1 - 2 6 
0 
0 
0 
1 3 
0 
0 
0 
0 
0 
This matrix is an example of an echelon matrix. An echelon matrix is any matrix, square 
or rectangular, that exhibits the following structure: 
1. Each of the first k rows (k > 0) of H has one or more nonzero elements. 
2. 
For each such row, the first nonzero element, as one reads from left to right, is 
unity. 
3. 
The arrangement of the first k rows is such that the first nonzero element in a 
given row is always to the right of the first nonzero element of any row that precedes (or 
lies above) the given row. 
4. 
After the first k rows, the elements of all remaining rows, if any, are all zero. 
The importance of echelon matrices relates to the facts that 
1. any matrix A can be transformed by a sequence of elementary row operations 
into echelon form; 
2. 
the rank of the matrix is not altered in the process of changing it to echelon form; 
3. 
the number of nonzero rows in H, the echelon form of A, equals the rank of A. 
A couple of caveats are in order, however. First, it should be pointed out that H, the 
echelon form of A, does not "equal" A but, rather, can be derived from A by elementary 
operations. Second, H is, in general, not a unique representation of A; that is, there is no 

180 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
unique echelon form for a given matrix, A. However, neither of these caveats weakens the 
general usefulness of echelon matrices in rank determination and solving sets of 
simultaneous equations. 
Transforming a given matrix to echelon form represents a relatively straightforward 
procedure. To illustrate, let us take the matrix 
1 2 
3 
4" 
1 3 
2 2 
2 
4 
1 0 
and apply elementary row operations to it. 
1. Subtract row 1 from row 2; subtract twice row 1 from row 3. 
' 1 2 
3 
4" 
2. 
Multiply row 3 by —^. 
0 
1 
0 
0 
1 
2 
-2 
0 
1 - 1 - 2 
0 
0 
1 
8/5 
We note that there are three nonzero rows remaining and, hence, the rank of the echelon 
form of A—and the rank of A as well—is 3. 
Finding the echelon matrix, as indicated by the preceding operations, involves 
concentrating on one row of the matrix at a time in order to obtain (a) a leading entry of 
unity in that row and (b) zeros in all lower rows of the column containing the leading 
entry of unity. 
While elementary row operations are useful in finding the echelon form of a matrix 
and, hence, determining its rank, they are also of value in matrix inversion and the 
solution of simultaneous linear equations. 
4.7.3 
Elementary Operations and Simultaneous Equations 
Let us now examine how the preceding approach to obtaining echelon matrices can be 
adapted to solving simultaneous equations. Since we know that the rank of the matrix 
illustrated above is 3, let us make up a new problem by setting down only the first three 
columns of the matrix used in the preceding example. Next, assume that the following 
simultaneous equations represent a system that we would like to solve: 
Xi +2x2 +3^3 = 14 
Xi +3x2 +2x3 = 13 
2xi +4x2 + -^3 = 13 

4.7. 
METHODS FOR RANK DETERMINATION AND MATRIX INVERSION 
181 
As illustrated earlier, this set of equations can be written as 
1 
2 
3 
1 
3 2 
2 
4 
1 
Xi 
X2 
14 
13 
13 
We see that the first three columns are the same as those appearing in the preceding 
example. Now let us go through the echelon procedure once more, but this time apply 
each elementary row operation to the full, augmented matrix consisting of coefficients 
and the vector of constants: 
1 
2 
3 
1 
3 2 
2 
4 
1 
14 
13 
13 
1. Subtract row 1 from row 2; subtract twice row 1 from row 3. 
2. 
Multiply row 3 by 
1 
2 
0 
1 
0 
0 
1 
2 
0 
1 
0 
0 
3 
-1 
-5 
3 
-1 
1 
14 
- 1 
-15 
14 
- 1 
3 
If we examine the 3 x 3 matrix to the left of the dotted line, we see that the leading entry 
of each row is unity. Moreover, in echelon form the rank of the matrix is seen, by 
inspection, to be 3. 
If we refer to the original set of equations, it is apparent that we can now obtain this 
solution quite readily. From the third row of the echelon form, immediately above, we 
have 
^3 = 3 
If X3 = 3 is then substituted in the second row, we have 
X2 — 3 = —1 
JC2=2 
If X2 =2 and X3 = 3 are substituted in the first row, we have 
Xi +-4 + 9 = 14 
Xi = 1 
The process of finding the value of X3 in the third equation first, and then substituting 
it in the second equation to find X2, and so on, is called back substitution. (The whole 

182 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
process is just an application of the pivotal method described, in the context of 
determinants, in Chapter 2, as will be shown later.) 
4.7.4 
Finding the Inverse 
Now that we have seen how a set of simultaneous equations can be solved by means of 
elementary row operations, let us carry the same general procedure one step further to 
find the inverse of the matrix of coefficients. For ease of illustration we continue with 
the same example. 
First of all, it should be clear that the set of simultaneous equations shown above can 
be written in the following form: 
1 
2 
3 
1 
3 
2 
2 
4 
1 
which, in tum, can be expressed as 
Xi 
X3 
1 
0 
0l 
0 
1 0 
0 0 ij 
ri4~ 
13 
13 
Ax = Ib 
We could then perform a set of elementary row operations on both A and I. In particular, 
we shall try to reduce A to an identity matrix. 
1. Subtract row 1 from row 2; subtract twice row 1 from row 3. 
1 
2 
3 
0 
1 
- 1 
0 
0 - 5 
1 
0 
0 
- 1 
1 0 
-2 
0 1 
2. 
Subtract twice row 2 from row 1. We include this elementary row operation in 
order to make sure that column 2 has only a single nonzero entry (in row 2). 
3. 
Multiply row 3 by -^. 
1 0 
5 
0 
1 - 1 
[o 0 -5 
1 0 
5 : 
0 
1 - 1 
: 
0 
0 
1 : 
3 - 2 
0 
-1 
1 0 
-2 
0 1 
3 - 2 
0 
-1 
1 
0 
2/5 
0 
- l i 
The 3 x 3 submatrix on the left is still not an identity matrix. Accordingly, we can apply 
the following additional row operations: 

4.7. 
METHODS FOR RANK DETERMINATION AND MATRIX INVERSION 
183' 
4. 
Subtract 5 times row 3 from row 1; add row 3 to row 2. 
1 
0 
0 
0 
1 0 
0 
0 
1 
1 
-3/5 
2/5 
- 2 
1 
0 
1 
-1/5 
-1/5 
When we do this, we observe that the left side of the matrix is, indeed, an identity matrix. 
However, while we have been transforming A to I via elementary row operations, we 
have, at the same time, been transforming I on the right side of the dotted line. Recalling 
that 
Ax = b 
A-^Ax = A-^b 
Ix = A-^b 
it seems reasonable to suppose that we have been obtaining the inverse of A. That is 
1 
-3/5 
2/5 
-2 
1 
0 
1 
-1/5 
-1/5 
First, we observe that 
1 
-3/5 
2/5 
A-i 
- 2 
1 
0 
11 
1/5 
1/5 
b 
1 ^"^ 
13 
13 
= 
X 
1 
2 
3 i 
as we know it should. Second, we check to see that 
1 
0 
1 
0 
0 
I 
0 0 
= 
1 2 3] 
1 3 2 
.2 4 iji 
1 
-3/5 
2/5 
- 2 
1 
0 
1 
-1/5 
-1/5 
and the solution is complete. 
In summary, application of elementary row operations has served three main purposes: 
1. By reducing the matrix to echelon form we were able to discern its rank by simply 
counting the number of rows containing a nonzero entry. 
2. 
If an explicit set of simultaneous equations is involved—here we assume that a 
unique solution exists—we can apply elementary row operations to the augmented matrix 
and obtain, via back substitution, the desired values of the unknowns. 
3. 
If the inverse of the given transformation matrix is also desired, we can apply 
elementary row operations to both A and I, transforming the former into I and the latter 
into A~^ 

184 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
In actuality, elementary row operations (or elementary column operations) have 
applicability to solving sets of simultaneous equations in more general settings where an 
inverse of the coefficients matrix may not exist. These matters are taken up in Appendix 
B. 
In summary, from a somewhat more theoretical viewpoint, we can use the echelon 
approach to reduce any matrix to the following form : 
kxk 
^ 
0 
where the first k rows and k columns represent SL k xk identity matrix (of rank k, of 
course), and the remaining entries (if any) all consist of zeros. 
In the preceding example involving the nonsingular matrix A, no zeros appeared since 
A itself was of full rank. In other instances zeros will be found. In general, the above form 
is found in two steps. Given an arbitrary matrix B, we first apply a set of elementary row 
operations to get the echelon form: 
H = FB 
where F denotes the (nonsingular) matrix product of all of the separate elementary 
matrices used to carry out the reduction of B to echelon form. This step has been 
illustrated above. 
Next, we could apply elementary column operations in order to get the matrix 
product: 
HG = 
kxk 
0 
= FBG 
where r{B) = L In the preceding numerical example it was not necessary to apply 
elementary column operations after the elementary row operations were performed. In 
other cases, it may be more efficient to reduce the matrix to echelon form via row 
operations and then to obtain the form above via elementary column operations that 
entail postmultiplication of the echelon matrix H. At any rate we include the general 
approach that involves both row and column operations. 
The complete transformation that entails row and column operations is also called an 
equivalence transformation. More formally, if F and G are nonsingular, an equivalence 
transformation of an arbitrary matrix B is defined as 
C = FBG 
and C is defined to be equivalent, via elementary row and column operations, to the given 
matrix B. 
4.7.5 
The Pivotal Method 
Having seen how elementary operations are used to (a) determine rank, (b) solve a set 
of simultaneous equations, and (c) find the inverse of the coefficients matrix, our 
discussion of the pivotal method, first described in Chapter 2, can now be completed. For 
ease of reference, Table 2.2 is reproduced here as Table 4.15. 

4.7. 
METHODS FOR RANK DETERMINATION AND MATRIX INVERSION 
185 
TABLE 4.15 
Evaluating a Determinant by the Pivotal Method 
Row 
no. 
0 
01 
02 
03 
04 
10 
11 
12 
13 
20 
21 
22 
30 
31 
40 
30* 
20* 
10* 
1 
s" 
4 
1 
3 
1 
1 
Original matrix 
2 
3 
3 
1 
2 
3 
4 
2 
1 
0 
1.5 
0.5 
RH 
1 
2.5 
1.5 
-3.5 
-1.5 
1 
-0.25 
1^.125 1 
-2.375 
1 
1 
1 
4 
2 
4 
2 
1 
1 
0 
1 
-2 
0 
1 
-2 
0.471 
-0.881 
1 
5 
1 
0 
0 
0 
0.5 
-2 
-0.5 
-1.5 
0.5 
-1.75 
0.25 
-0.824 
-1.707 
1.938 
-1.737 
0.066 
-0.668 
Identity matrix 
6 
7 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
-0.25 
0 
0.625 
1 
-0.875 
0 
0.294 
0.471 
-0.177 
1.119 
0.201 
-1.270 
0.199 
1.069 
-0.200 
0.267 
-0.001 
0.334 
8 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
1 
0 
1 
-1.135 
0.534 
0.134 
0.667 
Check 
sum 
column 
9 
9 
14 
10 
6 
4.5 
-4 
5.5 
7.5 
1 
3.0 
-4.0 
1.412 
-0.646 
0.733 
1.067 
1.267 
1.332 
lAl = (2)(-4)(2.125)(-0.881) = 15 
In Chapter 2 we illustrated how the pivotal (boxed) entries in Table 4.15 were 
obtained and how |A| was found as the product of the four pivotal items. From what we 
have now leamed, we see that the pivotal method is just a systematic way of applying 
elementary row operations, leading in the example of Table 4.15 to the echelon matrix: 
1 
0 
0 
0 
1.5 
1 
0 
0 
0.5 
-0.25 
1 
0 
1 
0 
0.471 
1 
H 
which, we note, is of full rank where r(H) equals n, its order. Our task now is to explain 
the back substitution procedure that leads to the inverse of the original 4 x 4 matrix (call 
it A) in the top left-hand corner of Table 4.15. 

186 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
First, if we treat column 5 as a vector of constants, it should be apparent from earlier 
discussion how we can obtain the following values of X4, X3, X2, Xi via back substitution: 
X4 = 1.938 
(from row 40 and column 5) 
X3 + 0.471(1.938) = -0.824 
(from row 30 and column 5) 
JC3 = - 1 . 7 3 7 
X2 - 0.25(-l .737) + 0(1.938) = 0.5 
(from row 20 and column 5) 
X2= 0.066 
Xi + 1.5(0.066) + 0.5(-l .737) + 1(1.938) = 0.5 
(from row 10 and column 5) 
X, =-0.668 
These entries are shown as a column vector in rows 40, 30*, 20*, and 10*, and column 5. 
What we are doing here is building up the inverse of A one column at a time. Then we 
repeat the whole procedure, using the entries of column 6, followed by column 7, and 
finally by column 8. The result of all of this is the inverse A"\ shown in rows 40, 30*, 
20*, and 10*, and columns 5, 6, 7, and 8. 
However, it is important to note that the inverse A~^ is shown in Table 4.15 in reverse 
row order. That is, the first row of A"^ is given by 
(-0.668 
-0.001 
0.334 
0.667) 
as designated by the row 10*. Hence, if one uses the row order 10*, 20*, 30*, and 40, 
the inverse A~^ will be in correct row order. 
The pivotal procedure proceeds somewhat differently from that used in Section 4.7.4, 
but the results are the same. Rather than applying additional elementary operations to 
transform the echelon matrix H into identity form, in the pivotal method we simply treat 
each of the columns—columns 5 through 8—as a separate set of constants and solve for 
A"^ in this manner. Either way we have the relation 
AA-'=I 
as we should. 
Furthermore, had an explicit set of constants been present, in the usual form of a set 
of simultaneous equations, the same procedure could have been applied to this column as 
well. In brief, the pivotal method can be used for the same purposes noted earUer: 
1. finding the determinant of A, 
2. 
finding the rank of A, 
3. 
solving an explicit set of simultaneous equations, 
4. 
finding the inverse of A (if it exists). 
In applying the pivotal method one should be on the lookout for cases in which a zero 
appears in the leading diagonal position. If such does occur, we cannot, of course, divide 
that row by the pivot. If the matrix is nonsingular—which will usually be the case in 
data-based applications—the presence of a leading zero suggests that we want to move to 
the next row and select it as the pivot (i.e., permute rows) before proceeding. This has no 
effect on the inverse, but will reverse the sign of the determinant (if an odd number of 
such transpositions occur). 

. 
METHODS FOR RANK DETERMINATION AND MATRIX INVERSION 
187 
If the matrix is singular, the situation will be revealed by the presence of all zeros in 
some row to be pivoted. This is the type of situation that we encountered earlier in 
transforming a matrix to echelon form. 
In summary, the pivotal method is now seen as just a specific step-by-step way to go 
about applying elementary row operations. While other approaches are available, the 
pivotal method does exhibit the virtues of simplicity and directness. To round out 
discussion, we consider its use in finding the inverse of a correlation matrix in the context 
of multiple regression. 
4.7.6 
Matrix Inversion in Multiple Regression 
The sample data introduced in Table 1.2 involved three variables: 
Y 
number of days absent 
Xi 
attitude toward the company 
X2 
number of years employed by the company 
As discussed in Section 1.6.2, the least-squares principle entails finding a linear equation 
that minimizes the sum of the squared deviations 
12 
12 
A 
between the original criterion Yi and the predicted criterion 7/. 
The Hnear equation is represented by the form 
Yi = bo^biX^ 
+Z72X2 
where b^, bi, and Z?2 are parameters to be estimated (by least squares) from the data. 
The derivation of the normal equations underlying the least-squares procedure is 
described in Appendix A. 
If we work with the original data from the sample problem, the normal equations can 
be represented in matrix form as 
(X'X)b = X'y 
where X is the matrix of predictor variable scores (to which has been appended a column 
vector of unities); b is the vector of parameters: 
bo 
bi 
b2 
to be solved for, and y is the criterion vector. 
The intercept bo denotes the value of Y when Xi and X2 are each zero; bi measures 
the change in Y per unit change in Xi and Z?2 measures the change in Y per unit change in 
X2. 

188 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
TABLE 4.16 
Computing the Matrix Product Required for 
Solving the Normal Equations in the 
Sample Regression Problem 
XX = 
1,1,1, 
1,2,2, 
1,1,2, 
X'y = 
, 1 
.12 
.10 
1 
1 
1 
1 
2 
1 
1 
2 
2 
1 
12 
10 
12 
75 
59 
75 
639 
497 
59 
497 
397 
1 , 1 , 1 , . . 
1,2,2,.. 
1,1,2,.. 
., l] 
.,12 
.,ioJ 
r 1 
0 
1 
L16_ 
= 
75 
702 
542 
Table 4.16 illustrates the computations required to obtain X'X and X'y. To solve for 
b, the vector of regression parameters, we express the normal equations in the form 
b = (X'X)-^ X'y 
and proceed to solve for b. 
Table 4.17 shows the pivotal method apphed to the sample problem. In this case the 
original matrix is only 3 x 3 so the procedure involves fewer steps; otherwise, the 
approach is the same as that followed in Table 4.15. After (X'X)"^ is found by the pivotal 
procedure, this is postmultiplied by X'y- The solution vector b is shown in the lower 
portion of Table 4.17. The vector of parameters b was actually obtained via computer 
and so differs somewhat from that found by carrying only three decimal places in Table 
4.17. 
The desired linear equation is 
Yi = -2.263 + l.SSOXi -0.239^2 
In Chapter 6 we discuss this sample problem in considerably more detail. At this point, 
however, we simply wish to show how the pivotal procedure can be used to find a matrix 
inverse in the context of multiple regression analysis. 
Matrix inversion represents a central concept in multivariate analysis. While illustrated 
here in the context of multiple regression, matrix inversion goes well beyond this type of 
application. It is used in a variety of multivariate techniques, including analysis of 
variance and covariance, discriminant analysis and canonical correlation, to name some of 
the procedures. Along with matrix eigenstructures, to be discussed in Chapter 5, matrix 
inversion represents one of the most important and commonly appHed operations in all of 
multivariate analysis. 

4.8 
SUMMARY 
189 
TABLE 4.17 
Applying the Pivotal Method to the 
Sample Regression Problem 
Row 
no. 
0 
01 
02 
03 
10 
11 
12 
20 
21 
30 
20* 
10* 
Original matrix 
1 
QE 
75 
59 
1 
1 
2 
3 
75 
59 
639 
497 
497 
397 
6.250 
170.250 
128.250 
4.917 
128.225 
106.897 
1 
0.753 
10.325 
1 
1 
4 
1 
0 
0 
0.083 
-6.225 
-4.897 
-0.037 
-0.152 
-0.015 
-0.026 
0.320 
Identity matrix 
5 
0 
1 
0 
0 
1 
0 
0.006 
-0.770 
-0.075 
0.062 
-0.019 
6 
0 
0 
1 
0 
0 
1 
0 
1 
0.097 
-0.073 
-0.021 
Check 
sum 
column 
7 
147 
1212 
954 
12.250 
293.250 
231.250 
1.722 
10.403 
1.007 
0.963 
1.280 
b = 
(X'X)-' 
0.320 
-0.019 
-0.021' 
-0.026 
0.062 
-0.073 
-0.015 
-0.075 
0.097 
X'y 
r '^^ 
702 
L542 
; 
b = 
r-2.263l 
1.550 
-0.239 
4.8 
SUMMARY 
This chapter has focused on linear transformations, a key concept in multivariate 
analysis. As indicated at the outset of the chapter, all matrix transformations are Hnear 
transformations. Furthermore, in two or three dimensions various kinds of matrix 
transformations can be portrayed geometrically. 
We first reviewed transformations involving orthogonal matrices, that is, rotations. We 
described how one can move a point (or points) relative to a fixed basis or, alternatively, 
leave the point fixed and rotate the basis vectors. This idea was then extended to linear 
transformations generally. Numerical examples were used to illustrate the concepts. 
We next discussed matrix inversion and its role in solving for the original vector, given 
some transformed vector and the matrix of the transformation. Allied concepts involving 
the adjoint of a matrix and the use of the matrix inverse in finding transformations 
relative to a new set of basis vectors were also discussed. The concept of basis vector 
transformation was then extended to the case of showing the effect of a given 
transformation on points referred to two different sets of basis vectors if one knows (a) 
the matrix of the transformation with respect to one basis and (b) the Hnear 
transformation that relates the two sets of basis vectors. Some discussion was also 
presented on general (oblique) coordinate systems. 

190 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
The next principal section of the chapter attempted to integrate many of our earlier 
comments, in this and preceding chapters, by showing what happens geometrically as 
various kinds of matrix transformations are appUed. An important extension of this 
concept involves the idea of composite mappings and, in particular, the observation that 
any nonsingular matrix transformation with real-valued entries can be decomposed into 
the unique product of either a rotation followed by a stretch followed by another 
rotation, or a rotation followed first by a reflection and then by a stretch that is followed 
by another rotation. Explanation of this assertion represents one of the main topics of 
the next chapter. 
The next major area of interest concemed the rank of a matrix and its relationship to 
matrix inversion and linear independence. Various properties of matrix rank were Hsted 
and illustrated in the context of linear transformations. As was pointed out, the rank of a 
transformation matrix is important in determining what characteristics of the original 
space are preserved under a matrix transformation. 
A topic that is related to the foregoing involves the applicarion of elementary 
operations to determine matrix rank, solving sets of simultaneous equations, and matrix 
inversion. This subject was taken up next, and various examples of applying elementary 
operations were worked through and the results integrated with earlier material. 
The concluding section dealt with numerical procedures for matrix inversion and 
emphasized the pivotal method, first introduced in Chapter 2. This example was 
continued here and, in addition, the pivotal method was applied to inverting a 
cross-products matrix in the context of multiple regression. 
REVIEW QUESTIONS 
1. 
Let gi denote a mapping of E^ (EucHdean 3-space) that represents the projec-
tion of a point onto the xy plane; let g2 denote a mapping ofE^ that represents the pro-
jection of a point onto the z axis. Describe the following mappings geometrically: 
a. 
^1+^2 
b. 
-g2 
c. 
gi-g2 
d. 
3^1+^2 
2. 
Use the transformation 
1 oir^i' 
1/2 
I J L ^ 2 . 
to carry out the following transformations: 
x'^ = 
a. 
a triangle with vertices (1, 1), (4, 2), (5, 4) 
b. 
a parallelogram with vertices (1, 1), (4, 2), (5, 4), (2, 3) 
c. 
a rectangle with vertices (1, 2), (2,2), (2, 5), (1, 5) 
By means of diagrams show the original and transformed figures. What aspects—lengths, 
angles, areas—are invariant over the transformation? 
3. 
Using the same figures as those of the preceding question, 
a. 
show the effect of a stretch with ku = 2 and ^22 - ^» 
b. 
show the effect of a shear parallel to the x axis with c = 2. 

REVIEW QUESTIONS 
191 
4. 
Use 2 x 2 matrices to represent the following transformations: 
a. 
A counterclockwise rotation of points through 45° followed by the translation 
^ i * = ^ i + 2 ; j C 2 * = X 2 — 1. 
b. 
A counterclockwise rotation of points through 30 
followed by the stretch 
Xi* = 3xi;x2* = :'£:2/2. 
c. 
A stretchxi* = 3xi; JC2* = A:2/2, followed by a counterclockwise rotation of points 
through a 30° angle. 
5. 
Consider the following transformations: 
1 
2 
0 
Ij 
; 
A2 = 
1/2 
0 
0 
l/2_ 
; 
A3 = 
0.707 
-0.707 
0.707 
0.707 
Express 
in 
matrix 
form, 
and 
show 
geometrically, 
the 
following 
composite 
transformations: 
a. 
A2A1 
b. 
A3A1A2 
c. 
A2A3 
d. 
A3A2A1 
6. 
Find A " \ if it exists, and verify that AA"^ = A"^ A = I for the following: 
a. 
c. 
5 
2 
.7 
5_ 
1 
0 
~1 
1 
0 
- 1 
d. 
A = 
a 
-c 
1. 
What is the rank of the following matrices: 
c. 
A = 
b. 
18 
10 
29 
8. 
Use elementary row operations to find the echelon form of 
6 
2 
3 
9 
2 
1 
4 
0 
8~ 
1 
3 
0 
2 
b. 
A = 
4 
3 

192 
4. 
LINEAR TRANSFORMATIONS FROM A GEOMETRIC VIEWPOINT 
9. 
For each of the following sets of equations, determine the rank of the coefficients 
matrix and the rank of the augmented matrix. Calculate the inverse (if it exists) by 
elementary row operations. 
Xi + 2x2 + 6x3 = 16 
Xi + 3x2 - 
JC3 = 12 
xi + 2x2 + 0x3= 10 
a. 
2xi + X2 + 4x3 = 13 
Xi - 
X2 + X3= 2 
xi + 2x^ + 3x3= 10 
c. 
Xi —2x2 + X3 = 7 
3xi - 2 x 2 + 5x3 = 43 
xi + 2x2 + 3x3 = 29 
10. 
Use the pivotal method to soWe the following set of equations; invert the matrix 
of coefficients and find its determinant: 
2xi + 5x2 + 3x3 = 1 
3xi + X2 + 2x3= 1 
Xi + 2X2+ X3 = 0 
11. 
Use the adjoint method to invert the 2 x 2 correlation matrix of predictors in the 
sample problem of Table 1.2 and then perform the indicated multiplication: 
Xi 
X2 
^ 1 
1.00 
0.95 
0.95 
1.00 
b* = R"^r(>^) 
where r(y) is the vector of simple correlations between the criterion Y and the predictors 
Xi and, Z2, respectively: 
r{>') = 
0.95 
0.89 
The vector b* denotes the standardized regression coefficients bi * and ^2* (often called 
beta weights) that measure the change in Y per unit change in Xi and X2, respectively, 
when all variables are expressed in terms of mean zero and unit standard deviation. 
12. 
Let 
T = 
denote the matrix of a point transformation in the basis E. Let the basis vector 
transformation relating F to E be 
fi = 1 l e i + 762 
fa = 3ei + 2e2 
Find the point transformation T° corresponding to T relative to the basis F. 
a. 
If x = [3] in E, what is it after transformation in F? 
b. 
If x* = [ 1 ] in E, what is its pre image in E? 
What is the determinant of T? of T°? 

REVIEW QUESTIONS 
193 
13. 
A linear transformation relative to the E basis has been defined as 
Xi* = Xi +:x:2 
The transformation linking F, some new basis, with the original E basis is 
1 
2 
1 
L = 
Find the point transformation defined above in terms of the new basis F. 
14. 
Relative to the E basis we have the Unear transformation: 
Xi* = 
Ixi — 2;c2 
X2* = —2:JCI + 6^2 — 2x3 
X3* = 
— 2^2 + 5^3 
A transformation of E to F has been found and is represented by 
fi = 
ei/3 + 262/3 + 263/3 
f2= 
26i/3+ 
62/3-263/3 
f3 = - 2 6 i / 3 + 262/3 - 
63/3 
What (particularly simple) form does the mapping of x onto x* take with respect to the 
new basis F? 

CHAPTER 5 
Decomposition of Matrix Transformations: 
Eigenstructuresand Quadratic Forms 
5.1 
INTRODUCTION 
In the preceding chapter we discussed various special cases of matrix transformations, 
such as rotations, reflections, and stretches, and portrayed their effects geometrically. We 
also pointed out the geometric effect of various composite transformations, such as a 
rotation followed by a stretch. 
The motivation for this chapter is, however, just the opposite of that in Chapter 4. 
Here we start out with a more or less arbitrary matrix transformation and consider ways 
of decomposing it into the product of matrices that are simpler from a geometric 
standpoint. As such, our objective is to provide, in part, a set of complementary 
approaches to those illustrated in Chapter 4. 
Adopting this reverse viewpoint enables us to introduce a number of important 
concepts in multivariate analysis-matrix eigenvalues and eigenvectors, the eigenstructure 
properties of symmetric and nonsymmetric matrices, the singular value decomposition of 
a matrix and quadratic forms. This new material, along with that of the preceding three 
chapters, should provide most of the background for understanding vector and matrix 
operations in multivariate analysis. Moreover, we shall examine concepts covered earlier, 
such as matrix rank, matrix inverse, and matrix singularity, from another perspective-one 
drawn from the context of eigenstructures. 
Finding the eigenstructure of a square matrix, like finding its inverse, is almost a 
routine matter in the current age of computers. Nevertheless, it seems useful to discuss the 
kinds of computations involved even though we limit ourselves to small matrices of order 
2 X 2 or 3 X 3. In this way we can illustrate many of these concepts geometrically as well 
as numerically. 
Since the topic of eigenstructures can get rather complex, we start off the chapter with 
an overview discussion of eigenstructures in which the eigenvalues and eigenvectors can be 
found simply and quickly. Emphasis here is on describing the geometric aspects of 
eigenstructures as related to special kinds of basis vector changes that render the nature of 
the mapping as simple as possible, for example, as a stretch relative to the appropriate set 
of basis vectors. 
This simple and descriptive treatment also enables us to tie in the present material on 
eigenstructures with the discussion in Chapter 4 that centered on point and basis vector 
194 

5.2. 
AN OVERVIEW OF MATRIX EIGENSTRUCTURES 
195 
transformations. In so doing, we return to the numerical example shown in Section 4.3 
and obtain the eigenstructure of the transformation matrix described there. 
The next main section of the chapter contmues the discussion of eigenstructures, but 
now in the context of multivariate analysis. To introduce this complementary 
approach—one based on fmding a linear composite such that the variance of point 
projections onto it is maximal—we return to the small numerical problem drawn from the 
sample data of Table 1.2. We assume that we have a set of mean-corrected scores of 
twelve employees on Xi (attitude toward the company) and X2 (number of years 
employed by the company). The problem is to find a linear composite of the two 
separate scores that exhibits maximum variance across individuals. This motivation leads 
to a discussion of matrix eigenstructures involving symmetric matrices and the 
multivariate technique of principal components analysis. 
The next main section of the chapter deals with various properties of matrix 
eigenstructures. The more common case of symmetric matrices (with real-valued entries) 
is discussed in some detail, while the more complex case involving eigenstructures of 
nonsymmetric matrices is described more briefly. The relationship of eigenstructure to 
matrix rank is also described here. 
The singular value decomposition of a matrix either square or rectangular and its 
relationship to matrix decompositon is another central concept in multivariate procedures. 
Accordingly, attention is centered on this topic, and the discussion is also related to material 
covered in Chapter 4. Here, however, we focus on the decomposition of matrices into the 
product of other matrices that individually exhibit rather simple geometric interpretations. 
Quadratic forms are next taken up and related to the preceding material. Moreover, 
additional discussion about the eigenstructure of square nonsymmetric matrices, as related 
to such multivariate techniques as multiple discriminant analysis and canonical correlation, 
is presented in the context of the third sample problem in Chapter 1. 
Thus, if matrix inversion and matrix rank are important in linear regression and related 
procedures for studying single criterion, multiple predictor association, matrix 
eigenstructures and quadratic forms are the essential concepts in deaUng with multiple 
criterion, multiple predictor relationships. 
5.2 
AN OVERVIEW OF MATRIX EIGENSTRUCTURES 
In Chapter 4 we spent a fair amount of time discussing point and basis vector 
transformations. In particular, in Section 4.3.5 we discussed the problem of finding the 
transformation matrix T°, relative to some basis F, if we know the transformation matrix 
T, denoting the same mapping relative to the E basis. As shown, to find T° requires that 
we know L the transformation that connects F with E. We can then find T° from the 
equation 
T° = (L')-^TL' 
While the discussion at that point may have seemed rather complex, it was pointed out 
that this procedure for changing basis vectors has practical utility in cases where we are 
able to find some special basis F in which the matrix (analogous to T° above) of the 

196 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
linear transformation takes on some particularly simple form, such as a stretch or a 
stretch followed by a reflection. 
The development of a special basis, in which a linear transformation assumes a simple 
(i.e., diagonal) form, is the motivation for this section of the chapter. As it turns out, if 
such a basis exists, it will be found from the eigenstructure of a matrix that is analogous 
to T above.^ Moreover, the (diagonal) matrix that represents the same transformation 
relative to the new basis will also be found at the same time. In all cases we assume that 
the original matrix of the transformation is square (with real-valued entries, of course). 
By way of introduction to matrix eigenstructures, let us first take up an even simpler 
situation than that covered in Section 4.3.5. Assume that we have a 2 x 2 transformation 
matrix: 
A = 
- 3 
4 
Next, suppose we wished to find an image vector 
that has the same (or, possibly, precisely the opposite) direction as the preimage vector 
Xx 
x = 
X2 
If we are concerned only with maintaining direction, then x* the image vector can be 
represented by 
x^ = 
\Xi 
XX2 
= 1 
Xl 
X2 
where X denotes a scalar. That is, we can stretch or compress x, the preimage, in any way 
we wish as long as x* is in the same (or precisely the opposite) direction as x. 
If x is transformed by A into x* = Xx, we state the following: 
Vectors, which under a given transformation map into themselves or multiples of 
themselves, are called invariant vectors under that transformation. 
It follows, then, that such vectors obey the relation 
Ax = Xx 
wdiere, as noted, X is a scalar. 
^ What we shall call eigenvalues (and eigenvectors) some authors call characteristic roots (vectors) 
or latent roots (vectors). 

5.2. 
AN OVERVIEW OF MATRIX EIGENSTRUCTURES 
197 
To illustrate, suppose we try out the vector Xi = [3] to see whether this is invariant 
under A: 
A 
- 3 
5 
4 
- 2 
Xl 
2 
3 
Xl 
9 
2 
Such is not the case. We see that the relationship for an invariant vector does not hold, 
since the components of the vector Xi*= [2] are not constant multiples of the vector 
Xl = [3]. However, let us next try the vector X2 = [3]. 
A 
- 3 
5 
4 
-2 
X2 
3 
3 ~" 
X2 
6 
6 
* 
= 2 
3 
3 
In the case of X2 = [3], we do have an invariant vector. Moreover, if we try any vector in 
which the components are in the ratio 1 : 1, we would find that the relation is also 
satisfied. For example, 
- 3 
4 
5 1 
- 2 
4 
4 
8 
8 
= 2 
4 
4 
where X = 2 is the constant of proportionality. 
Is it the case that only preimage vectors of the form X/ = [^] satisfy the relation? Let 
us try another vector, namely, X3 = [ i ] : 
- 3 
4 
5] 
- 2 
r 5" 
[-4 
-35~ 
28 
= - 7 
5 
-4_ 
We see that this form xy = [_4^], works also. But are there others? As we shall see, there 
are no others that are not of the form of either 
or 
Sk 
-4k 
To delve somewhat more deeply into the problem, let us return to the matrix equation 
Ax = Xx 
which can be rearranged (by subtracting Xx from both sides) as follows: 
Ax-Xx = 0 
Or equivalently, 
Ax-XIx = 0 

198 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
where I is an identity matrix. Next, we can factor out x to get 
(A-XI)x = 0 
As can now be seen, the problem of finding an invariant vector x is reduced to the 
problem of solving the equation 
(A-XI)x = 0 
~a 
b\ 
c 
d\ \X2 
= 0 
One trivial solution is, of course, to let x= [o]. Generally, however, we would be 
interested in nontrivial solutions; that is, solutions in which x 9^ [o]. 
For the moment, let us set A — XI equal to B and examine what is implied about B if x 
is to be nontrivial (i.e., contain nonzero elements). The above expression can then be 
written as 
Bx 
\^^ich, in turn, can be written as the set of simultaneous linear equations: 
axi +bx2 = 0 
ex 1 + dx2 - 0 
After multiplying the first equation by d, the second by —b, and adding the two, we have 
{ad-bc)x^ =0 
We then repeat the process by multiplying the first equation by —c, the second by a, and 
adding the two, to get 
(ad-bc)x2 
=0 
So, if either XI ^ 0 or X2 ^ 0 , we must have the situation in which 
\a b\ 
c d 
= |B| = |(A-XI)| = 0 
What this all says is that the determinant of A - XI must be zero if we wish to allow Xi 
and X2 to be nonzero. 
5.2.1 
The Characteristic Equation 
Returning to the original expression of A — XI, the above reasoning says that we want 
the determinant of this matrix to be zero. We can write out the above matrix expHcitly as 
A-XI = 
I ^ 2 1 
^ 2 2 I 
I 0 
1 I 
aii-\ 
an 
CI21 
fl?22~"^ 
« 1 1 
«21 
an 
« 2 1 
^12 
^22 _ 
a\2 
^22 
-X 
1 
0 
"x 
0 
0~ 
1 
0" 
X 

5.2. 
AN OVERVIEW OF MATRIX EIGENSTRUCTURES 
199 
Then, we can find the determinant and set it equal to zero:^ 
^21 
^ 2 2 " " ^ 
^ ^2 
X^-\(an 
+^^22) +^11^22-^12 ^21 = 0 
This last expression is called the characteristic equation of the transformation matrix A. 
The roots of this equation, which shall be denoted by X/, are called eigenvalues, and their 
associated vectors X/ are obtained by substituting the roots in 
(A-X,I)x, = 0 
and solving for x^. These vectors X/ are called eigenvectors. They are the vectors that are 
invariant under transformation by the matrix A. That is, by setting up the format of the 
characteristic equation and then solving for its roots and associated vectors, we have an 
operational procedure for finding the invariant vectors of interest. We obtain two central 
results from the process: 
1. the eigenvalues X^- that indicate the magnitude of the stretch (or stretch followed 
by reflection), and 
2. 
the eigenvectors X/ that indicate the new directions (basis vectors) along which the 
stretching or compressing takes place. 
In the case of a 2 x 2 matrix, not more than two values of X^- are possible. We can see this 
from the fact that the characteristic equation is quadratic, and a quadratic equation has 
two solutions, or roots. In general, if Ais n xn, n roots are possible, since a polynomial 
of degree n is involved. 
As indicated above, the characteristic equation of A is defined as 
|A-X,-I| = 0 
The determinant itself is defined as 
-X,-I| 
and is called the characteristic function of A. 
It should be clear, then, that only square matrices have eigenstructures, since we know 
already that only square matrices have determinants. Moreover, since Ax = Xx, A must be 
square. 
5.2.2 
A Numerical and Geometric Illustration 
Now that we have concerned ourselves with the rationale for finding the eigenstructure 
(i.e., the eigenvalues and eigenvectors) of a square matrix, let us apply the procedure to 
the illustrative matrix shown earlier: 
^-3 
5' 
A = 
4 
-2 
^ The reader should note that the characteristic equation is a polynomial of degree n (given that A 
is n xn). It will have, in general, n roots, not all of which may be either real or, even if real, distinct. 
We consider these possibilities in due course. 

200 
First we write 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
( A - X , I ) x - 0 
-3 
5 
4 
- 2 
Next, we set up the characteristic equation 
|~3'X^ 
5 
-X,-
'l 
0 
o' 
1 
1 
J 
^1/ 
X2l_ 
= 
0 
0 
-2-X,-
= 0 
and expand the determinant to get 
X,-^ + 5X,—14 = 0 
We then find the roots of this quadratic equation by simple factoring: 
(X, + 7)(X,-2)=0 
X i = - 7 
X2=2 
Next, let us substitute Xi= —7 in the equation (A—X/I)x/ = 0: 
f 1 
' - 3 
4 
5' 
- 2 
~-7 
0 
-
o' 
-7 i[ 
J 1 
~4 
5l 
4 5j 
Xn 
^21 1 
[xn 
[^21 
'o" 
0 
"o" 
0 
The obvious solution to the two equations, each of which is 
4xii + 5JC2I = 0 
is the vector 
5' 
or, as illustrated earlier, more generally, 
Xi = 
5k 
-4k 
Next, let substitute X2 = 2 in the same way, so as to fmd 
- 3 
_ 4 
5 
- 2 
[2 
[0 
- 5 
4 
0^ 
2 
\ 
J 
5 
- 4 
X12 
_-^22_ 
^ 1 2 
•^22_ 
0I 
OJ 
~o' 
0 
L J 

5.2. 
AN OVERVIEW OF MATRIX EIGENSTRUCTURES 
201 
^{30, 30) 
Fig. 5.1 
Vectors that are invariant under the transformation matrix A. 
A solution to these two equations: 
— ^^ 12 
^^ 22 ~ ^ 
4Xi2—4X22 = 0 
is evidently the vector 
or, again more generally, 
X2 = 
X2 
1 
1 
Ik 
Ik 
Hence, insofar as Xi and X2 are concerned, any vector whose components are in the ratio 
of either 
5 : - 4 
or 
1:1 
represents an eigenvector of the transformation given by the matrix A. 
Figure 5.1 shows the geometric aspects of the preceding computations.^ If we consider 
^ For ease of presentation, in Fig. 5.1 we let 
kxl 
kxl 
1 5 x 1 
1 5 x 1 

202 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
the eigenvector Xi = [_|], we see that this is mapped onto 
h 
Xii 
^21 
= -1 
5~ 
-4 = 
"-35' 
28 
L J 
while the second eigenvector X2 = [j^ ^ 1 ] ~ [}5 x 11 ^^ mapped onto 
^1? 
X22 
= 2 
~15 
15 
fso 
— 30 
Furthermore, the eigenvalues Xi= —7 and X2= 2 represent stretch (or stretch followed by 
reflection) constants. 
5.2.3 
Diagonalizing the Transformation Matrix 
Let us retum to the two eigenvectors found above and next place them in a matrix, 
denoted by U: 
5 f 
U^ 
- 4 
1 
As noted, the two column vectors above are the invariant vectors of A. We now ask the 
question: How would A behave if one chose as a basis for the space the two eigenvectors, 
now denoted by Ui and U2, the columns ofW. 
As we shall show numerically, if U is chosen as a new basis of the transformation, 
originally represented by the matrix A relative to the standard E basis, then the new 
transformation matrix is a stretch. This is represented by the diagonal matrix D, given by 
the expression 
D = U-'AU 
If U and D can be found, we say that A is diagonalizable via U. The matrix U consists of 
the eigenvectors of A, and the matrix D is a diagonal matrix whose entries are the 
eigenvalues of A. Note, then, that U must be nonsingular, and we must find its inverse 
Recalling material from Chapter 4, we know that we can find the inverse of U in the 
2 x 2 case simply from the determinant and the adjoint of U: 
1/9 
-1/9" 
U-' = 
^ 
adj(U)=^ 
lU 
4/9 
5/9 
Next, we form the triple product 
'\I9 
D = U-'AU = 
4/9 
-1/9 
5/9 
[-3 5' 
[ 4 -2_ 
" 5 r 
-4 1 = 
-7 0" 
0 2 
We see that the transformation matrix A, when premultiplied by U ^ and postmultiplied 
by U, the matrix whose columns represent its eigenvectors, has been transformed into a 

5.2. 
AN OVERVIEW OF MATRIX EIGENSTRUCTURES 
203 
diagonal matrix D with entries given by the eigenvalues of A. That is, if a set of basis 
vectors given by U is employed, the transformation, represented by A, behaves as a 
stretch, or possibly as a stretch followed by a reflection, relative to this special basis of 
eigenvectors. 
We shall be coming back to this central result several times in the course of elaborating 
upon matrix eigenstructures. The point to remember here is that we have found an 
instance where, by appropriate choice of basis vectors, a given linear transformation takes 
on a particularly simple form. This search for a basis, in which the nature of the 
transformation is particularly simple, represents the primary motivation for presenting 
the material of this section. 
Next, let us look at the expression 
D = U-^AU 
somewhat more closely. First of all, we are struck by the resemblance of this triple 
product to the triple product 
T° = (L')"'TL' 
described in Section 4.3.5. There we found that T° denoted the point transformation of a 
vector x°, referred to a basis f/, onto a vector x*°, also referred to F. T° can be found if 
we know T, the matrix of the same linear mapping with respect to the original basis e/, 
and L, the matrix of the transformation linking the f,- basis to the e^ basis. 
Note in the present case that D plays the role of T°, A plays the role of T, and U plays 
the role of L'. As such, the analogy is complete. Since U is the transpose of the matrix 
used to find the two linear combinations with respect to the standard basis ^f. 
fi = 5ei-4e2; 
f i = 5 [l 
0 
- 4 
0 
1 
1 — 
— 1 
( _ _ l 
1 
0 
+ 1 
~0| 
ij 
f2 = lei + le2; 
f 2 = 1 
we see that the analogy does, indeed, hold. The current material thus provides some 
motivation for recapitulating, and extending, the discussion of point and basis vector 
transformations in Chapter 4. 
5.2.4 
Point and Basis Vector Transformations Revisited 
Suppose we now tie in directly the current material on the special basis vectors 
(eigenvectors) obtained by finding the eigenstructure of a matrix to the material covered 
in Section 4.3.5. There we set up the transformation matrix 
0.9 
0.44 
0.6 
0.8 
and the vector x = [2 ] with respect to the original e/ basis. 

204 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
We also considered the transformation matrix 
0.83 
0.55 
L = 
0.20 
0.98 
which denoted point transformations with respect to the e,- basis according to the 
transformation L. As recalled, when we wish to fmd some new basis f/ with respect to e,, 
we use the transpose of L: 
L' = 
0.83 
0.20 
0.55 
0.98 
Note, then, that we must keep in mind the distinction between a linear transformation T 
and its matrix representation with respect to a particular basis. By way of review, Fig. 5.2 
shows the geometric aspects of the mapping: 
where 
x * ° = T V = ( L ' r ^ T L V 
L' = 
0.83 
0.20 
0.55 
0.98 
1.11 
0.60 
0.34 
0.59 
2.31 
1.52 
(L')-^ = 
1.39 
-0.28 
-0.78 
1.18 
Fig. 5.2 
Geometric aspects of the transformation x*° = T° x°, 

5.2. 
AN OVERVIEW OF MATRIX EIGENSTRUCTURES 
205 
in which the vector x° is mapped onto x*° by the point transformation T°. As recalled 
T° is the matrix of the transformation with respect to the f/ basis (given knowledge of T, 
the transformation matrix with respect to the e/ basis), and L' is the matrix connecting 
basis vectors in F with those in E. 
In the present context, U plays the role of L'. Hence, to bring in the new material, we 
shall want to fmd the eigenstructure of T. Without delving into computational details, we 
simply state that the eigenstructure of T is found in just the same way as already 
illustrated for the matrix A. In the case of T, the decomposition, as derived from its 
eigenstructure, is 
D U^'TU 
D 
U"^ 
~1.37 0 
0 
0.66 
-0.80 
-0.62] 
0.74 -0.7oJ 
T 
U 
ro.9 0.44~| 
[0.6 0.8 J 
[-0.69 
0.61 
[-0.73 -0.79 
Next, in line with the recapitulation in Fig. 5.2, we find the transformation 
x*°=u-iTUx° = Dx° = 
1.37 
0 
0 
0.33 
1.37 
0.66 
Figure 5.3 shows the pertinent results from a geometric standpoint. First, we note that 
the columns of U appear as the new basis vectors denoted fi and £2, respectively, so as to 
maintain the analogy with the column vectors of L' in Fig. 5.2. 
First, U takes x° onto x with respect to E. Then, the transformation T takes x onto x* 
with respect to E. Finally, U"^ takes x* onto x*° with respect to F, the matrix of the 
new basis. The interesting aspect of the exercise, however, is that the f/ basis (given by U 
in the present context) is not just any old basis; rather, it is one in which the mapping of 
x° onto x*° involves a stretch as given by the transformation 
x*° = Dx" = 
D 
1.37 
0 
0 
0.33 
X" 
"1.37" 
0.66 
Figure 5.3 shows the point transformation from x° to x*° with respect to the f/ basis.^ 
In one sense, then, the eigenstructure problem is precisely analogous to finding the nature 
of a transformation relative to two different sets of basis vectors. And this is one reason 
why the latter topic was discussed in Section 4.3.5. However, the distinguishing feature of 
an eigenvector basis is that the nature of the transformation assumes a particularly simple 
geometric form, such as the stretch noted above. 
"* As shown in Fig. 5.3, x° = [ 2] is stretched along the f^ axis in the ratio 1.37 : 1 but compressed 
along the f^ axis in the ratio 0.67 : 2 (or 0.33 : 1). 

206 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
Fig. 5.3 
Basis vector transformation involving eigenvectors of T. 
5.2.5 
Recapitulation 
In summary, obtaining the eigenstructure of a (square) matrix entails solving the 
characteristic equation 
|A-X,-I| = 0 
If A is of order nxn, then we shall obtain n roots of the equation; these roots are called 
eigenvalues. Each eigenvalue can then be substituted in 
(A-X,I)x, = 0 
to obtain its associated eigenvector. 
But what about the eigenvalues (and eigenvectors) of some arbitrary matrix A? All we 
have said up to this point is that if A is « x «, then n eigenvalues and eigenvectors are 
obtained. However, we shall find it is possible that 
1. some, or all, of the eigenvalues are complex, rather than real valued (even though 
A is real valued); 
2. 
some, or all, of the eigenvectors have complex elements; 
3. 
even if all eigenvalues (and their eigenvectors) are real, some eigenvalues may be 
zero; 
4. 
even if all eigenvalues are real and nonzero, some may be repeated. 

5.3. TRANSFORMATIONS OF COVARIANCE MATRICES 
207 
Moreover, so far we have not said very much about the new basis of column eigenvectors 
in U, other than to indicate that it must be nonsingular in order for the relationship 
D = U-^AU 
to hold. Furthermore, by the following algebraic operations: 
UD = UU-^AU = AU 
and 
UDU-^ = AUU-^ 
we can express A as 
A = UDU" 
Other than the conditions that U is nonsingular and D is diagonal, however, no further 
requirements have been imposed. We might well wonder if situations exist in which U 
turns out to be orthogonal as well as nonsingular. Also, we recall that the illustrative 
matrices, whose eigenstructures were found above, are not symmetric. Do special 
properties exist in the case of symmetric transformation matrices? 
We shall want to discuss these questions, and related ones as well, as we proceed to a 
consideration of eigenstructures in the context of multivariate analysis. 
Here a complementary approach to the study of eigenstructures is adopted. Emphasis 
is now placed on symmetric matrices and the role that eigenstructures play in reorienting 
an original data space with correlated dimensions to uncorrected axes, along which the 
objects are maximally separated, that is, display the highest variance. While it may seem 
that we are starting on a brand-new topic, it turns out that we are still interested in basis 
vector changes in order to achieve certain simpHfications in the transformation. Hence we 
shall return to the present topic in due course, but now in the context of symmetric 
transformation matrices. As it turns out, the eigenstructure of a symmetric matrix 
displays properties that can be described more simply than those associated with the 
nonsymmetric case. Accordingly, we cover this simpler case first and then proceed to the 
situation involving nonsymmetric matrices. 
5.3 
TRANSFORMATIONS OF COVARIANCE MATRICES 
At the end of Chapter 1, a small sample problem with hypothetical data was 
introduced in order to illustrate some of the more commonly used multivariate tools. The 
basic data, shown in Table 1.2, have already been employed as a running example to show 
1. how matrix notation can be used to summarize various statistical operations in a 
concise manner (Chapter 2); 
2. the geometric aspects of such statistical measures as standard deviations, 
correlations, and the generalized dispersion of a covariance matrix (Chapter 3); 
3. 
how the pivotal method can be used to find matrix inverses and solutions to sets 
of simultaneous equations (Chapter 4). 

208 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
Xd2 
H 
h 
H 
1 
1 
1 
1 
iXdi 
-5+ 
Fig. 5.4 
Scatter plot of mean-corrected predictors from sample problem of Chapter 1. 
In this chapter we continue to use this small data bank for expository purposes. 
Suppose we were to start with a plot of the mean-corrected scores of the two 
predictors Xfi2 (years employed by the company) versus X^xi (attitude toward the 
company). Figure 5.4 shows the scatter plot obtained from the mean-corrected data of 
Table 1.2. 
We note that X^2 ^^^ ^di ^r^ positively associated (their correlation is 0.95). In line 
with the discussion of Chapter 1, suppose we wished to replace Zdi and X^2 ^Y ^ single 
linear composite 
^i~ h^dil 
•*" h^di: 
for i= 1,2, ... ,12, the total number of employees, so that the variance of this linear 
composite 
12 
Var(z,)= E (z/-z)^/12 
/ = i 
subject to t't = 1, is maximized. By this is meant that the twelve employees are maximally 
separated along the linear composite. 

5.3. 
TRANSFORMATIONS OF CO VARIANCE MATRICES 
209 
The motivation for doing this sort of thing is often based on parsimony, that is, the 
desire to replace two or more correlated variables with a single linear composite that, in a 
sense, attempts to account for as much as possible of the variation shared by the 
contributory variables. The vector of weights t is constrained to be of unit length so that 
Var(Z|) cannot be made indefinitely large by making the entries of t arbitrarily large. 
If our desire is to maximize the variance of the linear composite, how should the 
weights ti and ^2 be chosen so as to bring this about? 
To answer this question we need, of course, some kind of criterion. For example, one 
approach might be to bring the external variable 7 into the problem and choose ti and ^2 
so as to result in a linear composite whose values maximally correlate with Y. As was 
shown in our discussion of multiple regression in Chapter 4, this involves finding a set of 
predicted values f'l whose sum of squared deviations from 7/ is minimized. 
However, in the present case, let us assume that we choose some "internal" criterion 
that ignores the external variable Y. The approach suggested earlier is to find a vector that 
maximizes the variance (or a quantity proportional to this, such as the sum of squares) of 
the twelve points if they are projected onto this vector. This is also equivalent to 
minimizing the sum of the squared distances between all pairs of points in which one 
member of each pair is the to-be-found projection and the other member is the original 
point. 
It is relevant to point out that we are really concerned with two types of vectors. The 
vector t = [Ji ] is a vector of direction cosines or direction numbers in terms of the 
original basis. The vector z comprises the particular point projections whose variance we 
are trying to maximize through the particular choice of t. 
We have, of course, several possible candidates for measuring the original association 
between X^i ^^^ ^di ? such as S, the SSCP matrix, C, the covariance matrix (which is 
proportional to S), and R, the correlation matrix. 
Illustratively, let us develop the argument in terms of the covariance matrix which, in 
the sample problem, is 
C = 
X2 
Xi 
X2 
14.19 10.69 
10.69 8.91 
Since, as it tums out, we shall be finding two linear composites, we shall refer to these 
new variables as zi and Z2, respectively. 
Finding the first of these linear composites Zi represents the motivation for 
introducing a new use for computing the eigenstructure of a matrix. That is, we shall wish 
to find a vector in the space shown in Fig. 5.4 with the property of maximizing the 
variance of the twelve points if they are projected onto this vector. We might then wish to 
find a second vector in the same space that obeys certain other properties. If so, what we 
shall be doing is changing the original basis vectors to a new set of basis vectors. (These 
new basis vectors are often called principal axes.) And, in the course of doing this, it will 
turn out that we are also decomposing the covariance matrix into the product of simpler 
matrices from a geometric standpoint in just the same spirit as described in Section 5.2. 

210 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
5.4 
EIGENSTRUCTURE OF A SYMMETRIC MATRIX 
Let us now focus on the covariance matrix of the two predictors 
Xi 
X2 
14.19 
10.69" 
10.69 
8.91 
C = 
X2 
The first thing to be noticed about C is that it is not only square but also symmetric. 
Many derived matrices in multivariate analysis exhibit these characteristics. We now wish 
to consider various linear composites of X^i and X^2 that have some chance of 
maximally separating individuals. 
Suppose, arbitrarily, we consider the following, overly simple, linear combinations of 
Xdiandxd2: 
zi = 0.707xdi +0.707xd2 = 0.707(xdi +Xd2) 
Z2 = 0.707xd2 -0.707xdi = 0.707(xd2 -Xdi) 
In the case of Zi we are giving equal weight to Xdi and Xd2 » while in the case of Z2 we are 
concerned with their difference, that is, the "increment" (component by component) of 
Xd2 over Xdi- Notice, further, that (a) the transformation matrix representing these linear 
combinations, which we shall call T, is orthogonal and (b) we shall be postmultiplying 
each point represented as a row vector in Xd by the matrix 
T = 
0.707 
-0.707 
0.707 
0.707 
As surmised, 0.707 is chosen so that (0.707)^ +(0.707)^ = 1, and we have a set of 
direction cosines. Since T is orthogonal, the following relationships hold: 
T'T=TT' = I 
Finally, we also note that the determinant |T| = 1. Hence, a proper rotation, as a matter 
of fact, a 45° rotation, is entailed since cos 45° = 0.707. We first ask: Suppose one were 
to consider the mean-corrected matrix Xd of the two predictors. What is the relationship 
of the derived matrix Z, found by the rotation XdT, to the matrix Xd? 
Panel I of Table 5.1 shows the linear composites for Zi and Z2, respectively, as 
obtained from the 45° rotation matrix. Since we shall be considering a second 
transformation subsequently, we use the notation Z^ and T^ to denote the particular 
rotation (involving a 45° angle) that is now being appUed. 
The solid-line vectors, Zi^ and Z2a , of Fig. 5.5 show the results of rotating the original 
basis vectors 45° counterclockwise. If we project the twelve points onto the new basis 
vectors Zj^ and Z2a , we find the projections shown in the matrix Z^ in Panel I of 
Table 5.1. Note further that, within rounding error, the mean of each column of Z^ is 
zero. (In general, if a set of vectors is transformed by a linear function, we will find that 
the means of the transformed vectors are given by that linear function applied to the 
means of the original variables.) A more interesting aspect of the transformation is: What 
is the relationship of the new covariance matrix, derived from the transformed matrix Z^, 
to that derived from the original matrix Xd? 

5.4. 
EIGENSTRUCTURE OF A SYMMETRIC MATRIX 
211 
TABLE 5.1 
Linear Composites Based on 45" and 38" Rotations of Original Basis Vectors 
I 
45" rotation 
^a 
Xj 
~ 6.48 
0.94" 
5.78 
0.23 
5.07 
0.94 
-4.36 
0.23 
-1.53 
0.23 
0.12 
1.65 
-0.12 
0.23 
-0.12 -1.19 
4.83 
0.47 
4.83 
1.89 
6.24 
0.47 
= 
7.66 
-0.47 j 
-5.25 
-3.92" 
-4.25 -3.92 
-4.25 -2.92 
-3.25 -2.92 
-1.25 -0.92 
-1.25 
1.08 
-0.25 
0.08 
0.75 -0.92 
3.75 
3.08 
4.75 
2.08 
4.75 
4.08 
5.75 
5.08 
Variances 
22.23 
0.87 
1 0.707 
0.707 
T« 
-0.707 1 
O.707J 
II 
38° rotation 
^b 
^d 
~-6.55 
0.15" 
5.76 -0.46 
-5.15 
0.32 
-4.36 -0.29 
-1.55 
0.05 
-0.32 
1.62 
-0.15 
0.09 
0.02 -1.19 
4.85 
0.11 
5.02 -1.29 
6.26 
0.28 
7.66 
0.45 
r-5.25 
-3.92" 
i -4.25 -3.92 
-4.25 -2.92 
-3.25 
v2.92 
-1.25 -0.92 
-1.25 
1.08 
-0.25 
0.08 
0.75 -0.92 
3.75 
3.08 
4.75 
2.08 
4.75 
4.08 
5.75 
5.08 
Variances 
22.56 
0.54 
[0.787 
0.617 
T& 
-0.617] 
0.787] 
Fig. 5.5 
Applying 45° and 38° counterclockwise rotations to axes of Fig. 5.4. 
We recall from Chapter 2 that the covariance matrix can be computed from the 
raw-data matrix X by the expression 
C= l/m[X'X-(l/m)(X'l)(l'X)] 
where X'X is called the minor product moment of the raw-data matrix X, and the second 
term in the brackets is the correction for means. 
In the present case by using X^ the mean of each column is already zero and similarly 
so for the columns of the transformed matrix Z^ . Hence, the second term on the right of 

212 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
the above equation consists of a null matrix. The original covariance matrix can then be 
restated as 
C(X)-
14.19 
10.69 
10.69 
8.91 
5.4.1 
The Behavior of the Covariance Matrix under Linear 
Transformation 
If C(X) is computed as shown above, we could find C{Za) by the same procedure, 
namely, 
C(Zj=l/mZ/Z« 
since, as before, the mean of each column of Z^ (within rounding error) is also zero and, 
hence llm{Za'l)(l'Za)= <t>. 
Actually, however, a much more direct way to find C{Za) is available. Since 
Za = XdT^ and C(ZJ = 1/m Z^'Z^, we have 
Q{Za) = [(XdT,y(XdT,)]/m = [T,%'XdTJ/A72 = T ; [C{X)]Ta 
That is, we can find C(Za ) through knowledge of the transformation matrix T^ and C(X), 
the covariance matrix prior to transformation. 
To find C(Za ) we compute 
C(Z„) = 
T ' 
0.707 0.707 
-0.707 0.707 
22.23 -2.64] 
-2.64 
0.87] 
C{X) 
[14.19 10.691 
[10.69 
8.9 ij 
T 
r0.707 
[o.707 
-0.707 
0.707 
The first thing to be noticed about C{Za) is that the sum of the main diagonal entries 
(22.23 + 0.87) is, within rounding, equal to the sum of the main diagonal entries of C(X). 
The second thing of interest is that C(Za) continues to remain symmetric, but now the 
off-diagonal entries are much smaller in absolute value (2.64) than their counterpart 
entries (10.69) in C(X), 
What has happened here is that the rotation of axes, effected by T, has resulted in a 
new set of basis vectors in which projections on the first of the new axes display a 
considerably larger variance than either contributing dimension x^i or Xd2 • 
We might next inquire if we can do still better in variance maximization. Does some 
other rotation result in a still higher sum of squares for the first transformed dimension? 
What we could do, of course, is to proceed by brute force. Based on what we have seen so 
far, we could try other orthogonal transformations in the vicinity of a 45° angle and duly 
note how the parceling out of variance between Zi and Z2 behaves under each 
transformation. Fortunately, however, an analytical approach is available—one that again 
utilizes the concept of the eigenstructure of a matrix. 

5.4. 
EIGENSTRUCTURE OF A SYMMETRIC MATRIX 
213 
5.4.2 
The Characteristic Equation 
In preceding chapters we have talked about changing the basis of a vector space. We 
have also discussed transformations that involve a rotation (proper and improper) and a 
stretch. Finally, we know that a linear combination such as Zi = 0.707X^1 + 0.707Xd2 
can be so expressed that the sum of the squared weights equals unity. That is, we 
can—and have done so in the preceding illustration—normalize the weights of the linear 
combination so that they appear as direction cosines. 
Suppose we take just one colunm vector of some new rotation matrix T (for the 
moment we drop the subscript for ease of exposition) and wish to maximize the 
expression 
U'[C(X)]t, 
subject to the normalization constraint that ti 'ti = 1. This restriction on the length of ti 
will ensure that our transformation meets the unit length condition for a rotation. And, 
incidentally, this restriction will also ensure that the resultant scalar ti' [C(X)] ti cannot 
be made arbitrarily large by finding entries of ti with arbitrarily large values. 
The above problem is a more or less standard one in the calculus, namely, optimizing 
F=u'[C(X)]t,-Hu'U-l) 
vAiere X is an additional unknown in the problem, called a Lagrange multiplier. 
While we shall not go into details (see Appendix A for these), we can briefly sketch 
out their nature by differentiating F with respect to the elements of ti and setting this 
partial derivative equal to the zero vector. The appropriate (symbolic) derivative is 
bF 
g^=2[C(X)ti-Xti] 
Setting this expression equal to the zero vector, dividing both sides by 2, and factoring 
out ti leads to 
[C(X)-AI]ti = 0 
where I is the identity matrix. In terms of our specific problem, we have 
C(A^)-XI 
t, 
0 
14.19-X 
10.69 
10.69 
8.91-
tn 
hi 
We may also recall from the calculus that satisfaction of the above equation is a necessary 
condition for a maximum (or minimum).' 
Immediately we are struck by the resemblance of the above expression to the matrix 
equation of Section 5.2: 
(A-AI)x = 0 
Although not shown here, sufficiency conditions are also met. 

214 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
that was used in finding the eigenstructure of A. Indeed, the only basic differences here 
are that C{X) is symmetric, and the eigenvectors are to be normaUzed to unit length. 
If the matrix C(X) — XI, for a fixed value of X, were nonsingular (i.e., possessed an 
inverse), it would always be the case that the only possible solution to the equation 
involves setting ti equal to the zero vector. Hence, in line with the discussion of 
Section 5.2, we want to accompHsh just the opposite, namely, to find a X that will make 
C(Z) — XI singular. But we recall that singular matrices have determinants of zero. Hence 
we want to find a value for X that satisfies the characteristic equation: 
|C(X)-XI| = 0 
Another way of putting things is that we wish to find ti such that 
C(Z)ti = Xti 
where ti is a vector, which if premultiplied by C{X), results in a vector Xti whose 
components are proportional to those of ti. 
This, of course, is the same line of reasoning that we discussed earlier in the chapter in 
the context of finding a new basis in which the matrix of the transformation in terms of 
that new basis could be denoted by a stretch or, possibly, by a stretch followed by a 
reflection. 
As recalled, however, for an nxn 
matrix one obtains n roots in solving the 
characteristic equation. Since we wish to maximize F, we shall be on the lookout for the 
largest X^- obtained from solving the characteristic equation. That is, we shall order the 
roots from large to small and choose that eigenvector t/ corresponding to the largest X/. 
Either the approach described in Section 5.2 or the current approach leads to the same 
type of result so long as we remember to order the roots of \Q{X) ~ Xi= 0 from large to 
small.^ As observed, |C(^) - X| = 0 is simply the characteristic equation of the covariance 
matrix C(X). 
Now, while we initially framed the problem in terms of a single eigenvector ti and a 
single eigenvalue Xi, the characteristic equation, as formulated above, will enable us to 
solve for two eigenvalues Xi and X2, and two eigenvectors ti and t2. As already noted in 
Section 5.2, if C{X) is of order n xn, V/Q shall obtain n eigenvalues and n associated 
eigenvectors. 
5.4.3 
Finding the Eigenvalues and Eigenvectors of Q{X) 
It is rather interesting that following either (a) the (present) variance-maximizing path 
or (b) the basis vector transformation path that seeks a new basis in which vectors are 
mapped onto scalar multiples of themselves leads to the same result—the characteristic 
equation. However, let us now concentrate our attention on the variance-maximizing path 
as the one that appears more appropriate from an intuitive standpoint in the context of 
the current problem. 
* It should be mentioned, however, that C(X), the covariance matrix, exhibits special properties in 
that it is symmetric and represents the minor product moment of another matrix (in this case 
X^l\/m). 
As such, all of its eigenvalues will be real and nonnegative and T will be orthogonal. We 
discuss these special properties later on. 

5.4. 
EIGENSTRUCTURE OF A SYMMETRIC MATRIX 
215 
The problem now is to solve for the eigenstructure of C(X). First, we shall want to 
find the eigenvalues of the characteristic equation 
|C(X)-X,I| = 
0 
14.19--X,- 
10.69 
10.69 
8.91-X/l 
Expansion of the second-order determinant is quite easy. In terms of the above 
problem we can express the characteristic equation as 
(14.19-X,)(8.91-X,)-(10.69)' =0 
X,-^-23.1X,- + 126.433-114.276 = 0 
X,-^-23.1X, + 12.157 =0 
The simplest way of solving the above equation is to use the quadratic formula of the 
general form y = ax^ +Z7X + c. First, we note that the coefficients in the preceding 
expression are 
a=l; 
Z7 = -23.1; 
c=12.157 
Next, let us substitute these in the general quadratic formula: 
-b ± y/b^~^^^Aac _ 23.1 ± >/(=^23:1)2-4(12.157) 
X,- = 
23.1 ± V484.982 
2a 
Xi= 22.56; 
X2 = 0.54 
As could be inferred from the sign of the discriminant (b^ — 4ac) of the general 
quadratic, we have a case in which the roots Xi and X2 are both real and are unequal. If 
we go back to our original problem of trying to optimize F[C(X)] subject tot't = 1, we 
see that Xi = 22.56 denotes the maximum variance achievable along one dimension by a 
linear composite of the original basis vectors. 
In the 2 x 2 case, solving for the eigenvectors ti and t2 is rather simple. Let us first 
substitute the value of Xi = 22.56 in the expression Q{X) — XI: 
14.19-22.56 
10.69 
10.69 
8.91-22.56 
C(X)-XiI 
-8.37 
10.69' 
10.69 
-13.65 
The next step is to set up the simultaneous equations needed to solve for ti, the first 
eigenvector: 
C(X)-XiI 
-8.37 
10.69 
10.69 
-13.65 
^11 
hi 
0 
0 
If we set ^21 = 1 in the first of the two equations: 
-8.37^11 + 10.69^21 = 0 

216 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
we obtain tu = 1.277. (Note that these values also satisfy the second equation.) This gives 
us the first (nonnormaUzed) eigenvector: 
t, = 
1.277^ 
lA: 
As we know, we can then divide the components of ti by ||ti || = 1.622, its length, to get 
the normalized version: 
(norm)ti 
0.787 
0.617 
In exactly the same way, we substitute X2 = 0.54 and perform the following calculations: 
C(X)-X2l 
14.19-0.54 
10.69 
10.69 
8.91-0.54 
^13.65 
10.69 
c w - X j i 
13.65 10.69] 
10.69 
8.37 
\t12~ 
tz2 
0' 
0 
10.69 
8.37 
Next, we set ^22 = 1 in the first of the two equations: 
13.65ri2 +10.69^22 = 0 
to obtain ti2 = -0.783 and note, further, that these values also satisfy the second 
equation. We then obtain 
t2 = 
-0.783A: 
\k 
(norm)t2 = 
-0.617 
0.787 
Let us now assemble the two normaUzed eigenvectors into the matrix, which we shall 
denote by T^: 
T , = 
0.787 
-0.617 
0.617 
0.787 
Then, as before, we can collect the various terms of the decomposition into the following 
triple product: 
Z. = D. 
22.56 
0 
0 
0.54 
:T^^C(X)T^ 

5.4. 
EIGENSTRUCTURE OF A SYMMETRIC MATRIX 
217 
While we have just found Z^ and T^, and we know C(X) to begin with, we must still solve 
for T^^ We obtain T^^ from 
T,,= 
0.787 
0.617 
-0.617 
0.787 
by the now-familiar adjoint matrix method, first described in Chapter 4. 
T;'=i^adKT,)=l 
0.787 
0.617 
-0.617 
0.787 
0.787 
0.617 
-0.617 
0.787 
Here we see the somewhat surprising result that T^^ = T^'. 
However, as recalled from Chapter 4, one of the properties of an orthogonal matrix is 
that its inverse equals its transpose: 
T' 
and we see that such is the case here. Moreover, it is easily seen that T^ exhibits the 
orthogonality conditions of tj^ t2^, = 0 and tj^ti^ = t2ftt2ft = 1. 
Thus, in the case of a symmetric matrix, illustrated by C{X), the general diagonal 
representation 
D = U-^AU 
of Section 5.2 now can be written as 
D = T'C(X)T 
where T is orthogonal and D continues to be diagonal. 
By the same token we can write 
A = UDU"^ 
in the case of a symmetric matrix as 
C(Z) = TDT' 
Thus, the rather interesting outcome of all of this is that starting with a symmetric 
matrix C(X), we obtain an eigenstructure in which the matrix of eigenvectors is orthogonal. 
That is, not only is the transformation represented by a stretch (as also found in 
Section 5.2), but the basis vectors themselves, in the symmetric matrix case, are 
orthonormal. We shall return to this finding in due course. For the moment, however, let 
us pull together the results so far, particularly as they relate to the problem of computing 
some "best" linear composite for the sample problem. 

218 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
5.4.4 
Comparing the Results 
When we rather arbitrarily tried a 45° rotation of the original axes in order to obtain 
the linear composites Z^ shown in Panel I of Table 5.1, we noted that the covariance 
matrix derived from this transformation was 
C(ZJ = T;C(X)T. 
22.23 
-2.64 
-2.64 
0.87 
Let us now compare this result with the maximum variance attainable from T^. From the 
immediately preceding discussion, we know that the comparable results are 
T^' 
CiX) 
Tt 
C(Z,) = 
0.787 0.617] 
-0.617 0.787] 
22.56 0 
0 
0.54 
14.19 
10.69 
10.69 
8.91 
0.787 
-0.617 
0.617 
0.787 
The matrix T^ in the present case involves a counterclockwise rotation of 38°. 
Panel II of Table 5.1 shows the computed Z^ values. Also, Fig. 5.5 shows this optimal 
rotation as a dashed line for comparison with the 45° rotation tried earlier. Clearly, the 
two rotations are quite close to each other. Moreover, when we compare the first 
(selected to be the largest) eigenvalues above: 
Xi = 22.23 
(for 45° rotation) 
Xi = 22.56 
(for 38° optimal rotation) 
we again see how close the 45° rotation, which was just guessed at for expository 
purposes, is to the optimal. 
Another point to note is that the off-diagonal elements of C(Zft), after the optimal 
rotation, are zero, and hence, Zi^ and Z2ft the two linear composites obtained from the 
optimal rotation, are uncorrelated. This is a bonus provided by the fact that C(X) is 
symmetric and, hence, T^ is orthogonal. 
To recapitulate, we have demonstrated how a set of vectors represented by the matrix 
Xa and a transformation of those vectors C(X) = (l/m)XJX^ can be rotated such that 
the projections of the points X^T^ (with T^ orthogonal) onto the first axis Zi^have the 
property of maximum variance. Moreover, at the same time it turns out that the new 
covariance matrix 
C(Z^)=l/m[Tft'Xd'XdT^] = 
22.56 
0 
0 
0.54 
is diagonal That is, all vectors referenced according to this new basis will be mapped onto 
scalar multiples of themselves by a stretch. This means that all cross products in the 
C(Zft) matrix vanish, as noted above. 

5.5. 
PROPERTIES OF MATRIX EIGENSTRUCTURES 
219 
Hence, we have diagonalized the original transformation C{X) by finding a rotation T^, 
of the Xd space that has the effect of making C(Zfy) a diagonal matrix. The second axis 
Z2b will be orthogonal to the first or variance-maximizing axis Zi^ 
Reflecting a bit on the above example and observing the configuration of points in 
Fig. 5.5, it would appear that the point pattern roughly resembles an ellipse. 
Furthermore, the new axes, Zi^ and Z2^, correspond, respectively, to the major and 
minor axes of that eUipse, called principal axes in multivariate analysis. 
If the distribution of points is multivariate normal, it turns out that the loci of equal 
probabiHty are represented by a family of concentric ellipses (in two dimensions) or 
ellipsoids or hypereUipsoids (in higher dimensions). The "ellipse" in Fig. 5.5 could be 
construed as an estimate of one of these concentration ellipses. 
It also turns out that by solving for the eigenstructure of C(X), we also obtain the axes 
of the "eUipse." This reorientation of the plane along the axes of the implied ellipse in 
Fig. 5.5 (via the 38° rotation of basis vectors) will also be relevant to quadratic forms, a 
topic that is discussed later in the chapter. 
5.5 
PROPERTIES OF MATRIX EIGENSTRUCTURES 
At this point we have discussed eigenstructures from two different, and comple-
mentary, points of view: 
1. finding a new basis of some linear transformation so that the transformation 
relative to that new basis assumes a particularly simple form, such as a stretch or a stretch 
followed by reflection; 
2. 
finding a new basis—by means of a rotation—so that the variance of a set of points 
is maximized if they are projected onto the first axis of the new basis; the second axis 
maximizes residual variance for that dimensionality, and so on. 
In the first approach no mention was made of any need for the basis transformation to be 
symmetric. In the second case the presence of a symmetric matrix possessed the 
advantage of producing an orthonormal basis of eigenvectors (a rotation). 
In the recapitulation of Section 5.2.5, we alerted the reader to a number of problems 
concerning the eigenstructure of nonsymmetric matrices. In general, even though we 
assume throughout that A has all real-valued entries, if A is nonsymmetric, 
1. we may not be able to diagonalize it via U"^AU;^ 
2. 
even if it can be diagonalized, the eigenvalues and eigenvectors of A need not all 
be real; 
3. 
even if the eigenvalues (and eigenvectors) of A are all real valued, they need not be 
all nonzero;* 
4. 
even if they are all nonzero, they need not be all distinct.^ 
"^ However, any matrix can be made similar to an upper triangular matrix (a square matrix with all 
zeros below the main diagonal). We do not pursue this more general topic here. 
^ If A has at least one zero eigenvalue, it is singular, a point that will be discussed in more detail in 
Section 5.6. 
' Points 3 and 4 pertain to symmetric matrices as well. 

220 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
Delving into the properties of eigenstructures involving complex eigenvalues and 
eigenvectors would take us too far afield in this book. 
Fortunately for the reader all nonsymmetric matrices of interest to us in multivariate 
analysis will have real eigenvalues and real eigenvectors. However, if A is nonsymmetric, 
then U, the new basis of eigenvectors, is not orthogonal. Moreover, the problem of 
dealing with zero (or nonzero but nondistinct) eigenvalues must be contended with in any 
case, and will be discussed in the context of symmetric matrices. 
5.5.1 
Properties of Symmetric Matrices 
As could be inferred from earlier discussion, in order to satisfy the expression 
D = U-'AU 
U"^ must, of course, exist. However, it turns out that in order for some (not necessarily 
symmetric) matrix Anxn to be made diagonal, it is necessary and sufficient that U consist 
of linearly independent vectors and, hence, forms a basis (and possesses an inverse). If this 
condition is not met, then A is not diagonalizable. However, even if a matrix is 
diagonalizable, it may not necessarily be orthogonally so. And, even if a matrix can be 
made diagonal, it need not consist of eigenvalues and eigenvectors that are all real valued. 
Symmetric matrices take care of these problems. If A is symmetric, it is not only 
always diagonalizable but, in addition, it is orthogonally diagonalizable where we have the 
relation 
U"^ =U' 
This is a very important condition since it states that for any pair of distinct eigenvectors 
U/, uy their scalar product u/uy = 0. 
Notice that this was the situation in Section 5.4.3 in which we had the result 
C(Z,) = 
0.787 
-0.617 
0.617 
0.787 
T^' 
C(X) 
0.787 
O.6I7] [14.19 
10.69 
-0.617 
0.787 I I 10.69 
8.91 
22.56 
0 
0 
0.54 
Not only is 0(2^) diagonal, but T^, is orthogonal. Since orthonormal basis vectors are 
highly convenient to work with in multivariate analysis, orthogonally diagonalizable 
matrices are useful to have. 
A further differentiating property for symmetric matrices versus their nonsymmetric 
counterparts is also useful in multivariate applications. If a symmetric matrix A consists 
of all real-valued entries, then all of its eigenvalues and associated eigenvectors will be real 
valued. 

5.5. 
PROPERTIES OF MATRIX EIGENSTRUCTURES 
221 
In practice, however, even the nonsymmetric matrices that we encounter in 
multivariate analysis—such as those that arise in mukiple discriminant analysis and 
canonical correlation—will have real eigenvalues. Hence, in the kinds of applications of 
relevance to multivariate analysis, the researcher does not need to worry very much about 
cases involving complex eigenvalues and eigenvectors. Still it is nice to know that the 
problem of complex eigenvalues and eigenvectors does not arise if A is symmetric. 
Now, let us next examine the case of equal eigenvalues in symmetric matrices. Suppose 
we have tied Xfs of multiplicities Ij^ for blocks k= 1,2, . . . ,s where 
s 
k = l 
First, it is comforting to know that the orthogonality property is maintained across 
subsets of eigenvectors associated with tied eigenvalues. That is, if t/ and ty are drawn 
from different sets, then t/ty = 0. The problem, then, is to obtain a set of orthogonal 
eigenvectors within each tied set of eigenvalues. Since this can usually be done in an 
infinity of ways, the solution will not be unique. 
To illustrate, suppose we have eigenvalues Xi = 1, X2 "^ 2, X3 = 2 with associated 
eigenvectors ti, t2, and ta. We note that X2 and X3 are tied. In the present case there is, 
in a sense, too much freedom with regard to the eigenvectors associated with X2 and X3. 
What can be done, however, is 
1. 
Find the eigenvector ti associated with the distinct eigenvalue Xi; this is done 
routinely in the course of substituting Xi in the equation (A—Xil)ti = 0. 
2. 
Choose eigenvectors t2 and ts (e.g., via Gram-Schmidt orthonormalization) so 
that they form an orthonormal set within themselves. Each, of course, will already be 
orthogonal to ti. 
While the above orientation is arbitrary, in view of the equality of X2 and X3, it does 
represent a way to deal with the problem of subsets of eigenvalues that are equal to each 
other. 
If it turned out that all eigenvalues were equal, that is, Xi = X2 = X3, then we have a 
case in which the transformation of A is a scalar transformation with coefficient X (i.e., 
just the scalar X times an identity transformation). As such, all (nonzero) vectors in the 
original space can serve as eigenvectors. Thus, any set of mutually orthonormal 
vectors—including the original orthonormal basis that could lead to this condition—can 
serve as a new basis. 
In general, if we have k eigenvalues, all with the same value X, then we must first find k 
linearly independent eigenvectors, all having eigenvalue X. Then we orthonormalize them 
via some process like Gram-Schmidt. If we have two or more subsets of tied eigenvalues, 
the orthonormalizing process is done separately within set. As noted earlier, all 
eigenvectors in different sets, where the eigenvalues differ, will already be orthogonal. 
However, tied eigenvalues arise only rarely in data-based product-moment matrices, 
such as C{X) and R(X). However, if they do, the analyst should be aware that the 
representation of A in terms of its eigenstructure is not unique, even though A may be 
nonsingular. 

222 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
by 
In summary, if A^xn is symmetric, we can say the following: 
1. 
An orthogonal transformation T can be found such that A can be made diagonal 
D = T'AT 
\\^ere D is diagonal, and the columns of T are eigenvectors of A. 
2. 
All eigenvalues and eigenvectors are real. 
3. 
IfX,-9^Xy,thent/ty = 0. 
4. 
If tied eigenvalues occur, of multiplicity /^ for some block k, then A has Ij^ but 
not more than Ij^ mutually orthogonal eigenvectors corresponding to the A;th block of tied 
eigenvalues. In such cases, the eigenstructure of A will not be unique, but T, the n xn 
matrix of eigenvectors, will still be nonsingular. 
The topic of zero eigenvalues—2ind their relationship to matrix rank—is so important that 
a special section in the chapter is reserved for it. For the moment, however, we turn to 
some additional properties of eigenstructures, appropriate (unless stated otherwise) for 
both the symmetric and nonsymmetric cases. 
5.5.2 
Additional Properties of Eigenstructures 
Two quite general properties of eigenstructures that apply to either the nonsymmetric 
or symmetric cases are: 
1. 
The sum of the eigenvalues of the eigenstructure of a matrix equals the sum of the 
main diagonal elements (called the trace) of the matrix. That is, for some matrix W^xw, 
n 
^ 
i = l h 
n 
= 1 
/= 1 
^ii 
2. 
The product of the eigenvalues of W equals the determinant of W: 
Notice here that if W is singular, at least one of its eigenvalues must be zero in order that 
|W| = 0, the condition that must be met in order for W to be singular. However, even 
though W may be singular, T in the expression 
D = T'WT 
is still orthogonal (and, hence, nonsingular). 
In addition to the above, a number of other properties related to sums, products, 
powers, and roots should also be mentioned. (The last two properties that are Hsted 
pertain only to symmetric matrices with nonnegative eigenvalues.) 

5.5. 
PROPERTIES OF MATRIX EIGENSTRUCTURES 
223 
3. 
If we have the matrix B = A + M, where /: is a scalar, then the eigenvectors of B 
are the same as those of A, and the ith eigenvalue of B is 
X/ + A: 
where X/is the ith eigenvalue of A. 
4. 
If we have the matrix C = /cA, where kisa scalar, then C has the same eigenvectors 
as A and 
^X,-
is the ith eigenvalue of C, where X/ is the zth eigenvalue of A. 
5. 
If we have the matrix A^ (where p is a positive integer), then A^ has the same 
eigenvectors as A and 
h" 
is the rth eigenvalue of A^, where X/ is the ith eigenvalue of A. 
6. 
If A~^ exists, then A"^ has the same eigenvectors as A and 
xr" 
is the eigenvalue of A""^ corresponding to the ith eigenvalue of A. In particular, 1/X/ is the 
eigenvalue of A~^ corresponding to X/, the ith eigenvalue of A. 
7. 
If a symmetric matrix A can be written as the product 
A = TDT' 
where D is diagonal with all entries nonnegative and T is an orthogonal matrix of 
eigenvectors, then 
and it is the case that A^^^A^^^ = A.^^ 
8. 
If a symmetric matrix A~^ can be written as the product 
where D~^ is diagonal with all entries nonnegative and T is an orthogonal matrix of 
eigenvectors, then 
A-l/2 = JD-l/2 J ' 
and it is the case that A -1/2^-1/2 ^ A -
*^ In Chapter 2 the square root of a diagonal matrix was defined as D*'^D*^^ = D where the 
diagonal elements of D*^^ are yjdji and it was assumed that all dff in D are nonnegative to begin with. 
In the present case A^^^A*^^ = A, where A need not be diagonal, but the conditions stated in point 7 
must be met. In the present context we see that (TD^^^T')(TD^^^T') = TD^^^D^^^T' = TDT' = A. 

224 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
We can illustrate these properties of eigenstructures by means of a simple example: 
~1 1 
A = 
1 2 
The eigenvalues of A are Xi = 3 and \i^\. 
The associated (and normalized) eigenvectors 
are 
^0.70?] 
r 0.707 
0.707J 
[-0.707 
Since A is symmetric, we can write the decomposition as 
D = T'AT 
0.707 
0.707 
0.707 
-0.707 
2 ^^ 0.707 
0.707 
1 2J ^0.707 
-0.707 
3 0 
0 1 
The various properties, discussed above, are now illustrated: 
Trace of A 
tr(A) = a n + ^22 = ^1 + ^2 = 3 + 1 = 4 
Eigenvalues of A + 21 
r4 r 
A + 21 = 
1 4 
X^-8X+ 15 - 0 
Xi = 5; X2 - 3 
Eigenvalues of A^ 
[4 5j 
X'-10X + 9 = 0 
X, = 9 
X2 = 1 
Determinant of A 
|A| = XiX2 = 3(l) = 3 
Eigenvalues of 2A 
[4 2~ 
2A = 
I 2 4 
X^-8X+ 12 =0 
Xi = 6; X2=2 
Eigenvalues of A 
; 2/3 -1/3 
A-^ =1 
L-1/3 
2/3 
X^-4/3X+1/3 = 0 
Xi = 1/3 
X2 = 1 
5/9 -4/9 
-4/9 
5/9 
X^-10/9X+1/9=0 
Xi = 1/9 
X2 = 1 
The Square Root of A 
The Square Root of A ^ 
1/2 _ 
= 
TD'^T 
0.707 
0.707 
1.366 
0.366 
0.707] 
-O.707J 
0.366] 
1.366 
TN/S 
OJ 
0 N/IJ 
A"-A' 
1 0.707 
[0.707 
^-
2 
1 
0.707 
-0.707 
2j 
A-''^ = 
= 
y j ) - l / 2 j 
"0.707 
0.707 
0.788 
-0.211 
0.707] 
-0.707J 
-0.211 
0.788 
ri/N/3 
L 0 
!• 
A 
J' 
0 ] 
i/Vi^ 
- 1 / 2 ^ -
[0.707 
0.707 
[o.707 
-0.707^ 
n _ 
2/3 -1/3 
-1/3 
2/3 J 

A"^ 
Pi.366 0.366 
0.366 1.366 
^-1/2 
0.788 -0.211 
-0.211 
0.788 
5.6. 
EIGENSTRUCTURES AND MATRIX RANK 
225 
I 
1 
0 
0 
1 
The preceding properties are quite useful in various aspects of multivariate analysis, 
and we shall return to a discussion of some of them later in the chapter. 
5.6 
EIGENSTRUCTURES AND MATRIX RANK 
In Chapter 4 we described two procedures for finding the rank of a matrix, square or 
rectangular, as the case may be: 
1. 
The examination of various square submatrices in order to find that one with the 
largest order for which the determinant is nonzero. 
2. 
The echelon matrix approach followed by a count of the number of rows with at 
least one nonzero entry. 
Eigenstructures are computed only for square matrices. However, by some procedures to 
be described in this section, we shall see how eigenstructures also provide a way to 
determine the rank of any matrix, even if the matrix is rectangular. 
In addition, it is now time to discuss the topic of zero eigenvalues in solving for the 
eigenstructure of a matrix.^^ As noted in Section 5.5, the presence of one or more zero 
eigenvalues is sufficient evidence that the matrix A is singular. 
5.6.1 
Square Matrices 
First, we recall that if Xnxn is symmetric, then all of its eigenvalues are real. It is 
possible, of course, that some may be positive, others negative, and some even zero. Also, 
from the previous section we know that 
n 
iAi=n \i 
i=\ 
Hence, if any \- is zero, A is singular. But what about the rank of A? Or, if A is 
rectangular, how can its rank be found by means of eigenstructures? 
In the case where A^x« 
is symmetric, finding the rank of A is simple. We first find the 
eigenstructure of A and then count the number—including multiple values, if any are 
present—of nonzero (either positive or negative) eigenvalues. The number of nonzero 
eigenvalues of A is equal to its rank. Since we assume here that any square matrix 
(symmetric or nonsymmetric) of interest to us in multivariate analysis will have real 
eigenvalues—this, of course, must be the case if A is symmetric—we can use this same 
procedure for finding the rank of any A as long as A is square. 
^ ^ Our remarks will pertain to nonsymmetric as well as symmetric matrices. 

226 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
5.6.2 
Rectangular Matrices 
Finding the rank of A^xn » where mi^n, 
by means of eigenstructures rests on an 
important fact about the minor and major product moments of a matrix: 
A'A; 
minor product moment of A 
AA'; 
major product moment of A 
and that fact is 
r(A'A) = r(AA') = r(A) 
The rank of either a minor or major product moment is the same as the rank of the 
matrix itself 
Since r(A'A) =r(AA'), we should, of course, find the eigenstructure of the smaller of 
these two orders, so as to reduce computational time and effort. And, if the researcher 
finds it easier to work with the eigenstructures of symmetric matrices, if A„x« is square 
but nonsymmetric, one can also compute its product moment, either minor or major, and 
find the eigenstructure of the symmetrized matrix. 
Another virtue attaches to finding the eigenstructure of a product moment matrix, 
A'A or AA', and that is that all eigenvalues will be either positive or zero. In the process 
of finding the product moment, any negative eigenvalues of A will become positive in the 
case of either A'A or AA', as we shall note later on. 
For the moment, however, let us set down the procedure for rank determination in a 
step-by-step way. First, if A is originally nonsymmetric or rectangular, we can always find 
the minor product moment (A'A) or the major product moment (AA') of A, whichever is 
of smaller order. The product-moment matrix will be square and symmetric. Further-
more, the eigenstructure of the product-moment matrix will exhibit either positive or 
zero eigenvalues, and a general procedure for rank determination can be followed. This 
general procedure, appUcable to finding the rank of square but nonsymmetric and 
rectangular matrices alike, is as follows: 
1. 
Find A'A or AA', whichever is of smaller order in the case of rectangular matrices. 
Their product will be symmetric, and all eigenvalues will be nonnegative. The number of 
positive eigenvalues of A'A (or AA') equals the rank of A. 
2. 
If r(A) -n and A is « x AI, then the vectors, either row or column vectors in A, are 
linearly independent. 
3. 
If /*(A) =n and n<m 
(where A is of order m x n), then the row vectors are 
linearly dependent. If/*(A) =m < n, then the column vectors are linearly dependent. 
4. 
If r ( A ) < n < m , then the set of either row or column vectors are linearly 
dependent and r(A) =/: is the largest number of linearly independent vectors in A. (The 
number k is the number of positive eigenvalues in A A or AA .) 
Next, suppose that the symmetric matrix being examined is still of the form A'A or 
AA', where we have adopted this form because A is either rectangular or nonsymmetric. 
However, even if A'A (or AA') is nonsingular, some of the (positive) \- may be equal to 
each other. If so, their multiplicities are still counted up in finding the rank of A. If A (or 
A'A or AA') is singular with a subset of l^ nondistinct eigenvalues, we can still find a 

5.6. 
EIGENSTRUCTURES AND MATRIX RANK 
227 
mutually orthonormal set of eigenvectors of rank l^ by some process, such as the 
Gram-Schmidt orthonormalization process, for the tied block kP 
In summary, finding the eigenstructure of a matrix—either symmetric to begin with or 
else a derived product-moment matrix—is a highly useful procedure for determining 
matrix rank. If thtrixn 
original symmetric matrix has rank r(A) = k, then k<n nonzero 
eigenvalues will be found. If the derived matrix 
A'A 
or 
AA' 
(whichever is of smaller order) 
has r(A'A) or r(AA') equal to k, then k < min(m, n) positive eigenvalues will be found. In 
short, one can always fmd the rank of a matrix via the eigenstructure approach. 
5.6.3 
Special Characteristics of Product-Moment Matrices 
Product-moment matrices, like the SSCP, covariance, and correlation matrices, play a 
unique role in multivariate analysis. For example, let us return to the covariance matrix 
used in the eigenstructure problem of Section 5.4: 
C(Z): 
Xl 14.19 
10.69 
X2 
10.69 
8.91 
We recall that this represents the minor product moment found from 
C(X) = (l/m)Xd'Xd 
where X^ is the matrix of deviations about column means. 
By way of summarizing some aspects of matrix rank and their relationship to 
eigenstructures, let us set down a few properties of product-moment matrices, such as 
C{X). We can illustrate the properties in terms of the covariance matrix: 
1. If C(X), the covariance matrix, has all distinct eigenvalues, it can be written 
uniquely as the triple product 
C(X) = TDT' 
wdiere D is diagonal, and T is an orthogonal matrix of associated eigenvectors. 
2. 
Since C{X) is of product-moment form, all of its eigenvalues are nonnegative, and 
we can always order the eigenvalues of C{X) from large to small. 
3. 
Whether nonsingular or not, the rank of C{X) equals the number of positive 
eigenvalues in its eigenstructure (since all eigenvalues in this case are nonnegative). 
4. 
It is generally the case that if any square matrix A is symmetric with nonnegative 
eigenvalues, then A = B'B. One way of writing the relationship involves defining B' as 
B' = TD 1/2 
*^ In a sense, then, the problem of deaUng with tied (but nonzero) eigenvalues is independent of 
the problem of determining the rank of a matrix. 

228 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
where D^''^ is a diagonal matrix of the square roots of the eigenvalues of A, and T is the 
orthogonal matrix whose columns are the associated eigenvectors of A. 
5. 
Since all of the eigenvalues of C(X) are either positive or zero, there exists a 
matrix B of order k xn such that 
C(X) = B'B 
6. 
The preceding definition of B' = TD^^^ is, however, not unique. If Bi = VB is the 
product of an orthogonal matrix V and B, then A can also be written, equally 
appropriately, as 
A = B/Bi = (VBy(VB) = B V V B 
The last three points can be illustrated by returning to the eigenstructure of C(Z) in the 
sample problem. First, we can write C{X) as 
Tz, 
D^ 
Tft' 
C(X)-
0.787 
0.617 
-0.617 
0.787 
22.56 
0 
0 
0.54 
0.787 
0.617 
-0.617 
0.787 
Next, we define B' = T^ D^'^ and B = Dj,^^ T^ and, furthermore, restate C(X): 
B' = 
r3.74 
L 2.93 
-0.45 
0.58 
B = 
3.74 
2.93 
-0.45 
0.58 
B'B 
14.19 
10.69 
10.69 
8.81 
We can then check to see that B'B = C{X). 
Next, however, let us take some orthogonal matrix V and write 
V 
B 
r 
Bi 
Then, we can write 
C(X) = 
0.707 
0.707 
-0.707 
0.707 
Bi' 
2.33 
-2.96 
2.48 
-1.66 
3.74 
2.93 
-0.45 
0.58 
Bi 
2.33 
2.48 
-2.96 
-1.66 
2.33 
2.48 
2.96 
-1.66 
14.19 
10.69 
10.69 
8.91 
and see that C(X) can be reproduced in this way just as well as the original way. This 
latter property : 
C(Z) = B'B = BVVB 
where V is orthogonal (V'V = VV' = I) is of particular relevance to factor analysis and has 
to do with the so-called rotation problem. That is, suppose we were first to find the 
eigenstructure of C(X) and then write C(X) in the context of the sample problem as 
C(X) = B ' B = [ T , D r D i ' % ' ] 

5.6. 
EIGENSTRUCTURES AND MATRIX RANK 
229 
By means of the preceding argument, B' = T^D^^^ is not unique, and we are free to rotate 
B as we please. This, of course, introduces a certain ambiguity into the question of what 
factor dimensions are "correct." As recalled, the principal components axes found from 
the unique representation: 
C(X) = T^,D^T^,' 
are unambiguous in the sense of maximizing variance in the derived covariance matrix 
C(Z,). 
In summary, data-based product-moment matrices exhibit a number of virtues, such as 
real-valued eigenstructures and orthogonal transformations for rotating the matrix to 
diagonal form. As we saw in the sample problem, one can order the eigenvalues from 
largest to smallest in the process of transforming a data matrix into a set of linear 
composites that are mutually orthogonal. Finally, the rank of product-moment matrices 
is easily discerned by simply counting up the number of positive eigenvalues. In most 
data-based problems the rank of C(X), and other types of derived product-moment 
matrices, will equal the order of the (minor) product-moment matrix. 
5.6.4 
Recapitulation 
At this point we have covered quite a bit of ground regarding eigenstructures and 
matrix rank. In the case of nonsymmetric matrices in general, we noted that even if a 
(square) matrix A could be diagonalized via 
D = U-^AU 
the eigenvalues and eigenvectors need not be all real valued. (Fortunately, in the types of 
matrices encountered in multivariate analysis, we shall always be dealing with real-valued 
eigenvalues and eigenvectors.) 
In the case of symmetric matrices, any such matrix A is diagonalizable, and 
orthogonally so, via 
D = T'AT 
where T' = T~^ since T is orthogonal.^^ Moreover, all eigenvalues and eigenvectors are 
necessarily real. If the eigenvalues are not all distinct, an orthogonal basis—albeit not a 
unique one—can still be constructed. Furthermore, eigenvectors associated with distinct 
eigenvalues are already orthogonal to begin with. 
The rank of any matrix A, square or rectangular, can be found from its eigenstructure 
or that of its product moment matrices. If Anxn is symmetric, we merely count up the 
number of nonzero eigenvalues k and note that r(A) = /: ^ w. If A is nonsymmetric or 
rectangular, we can fmd its minor (or major) product moment and then compute the 
eigenstructure. In this case, all eigenvalues are real and nonnegative. To find the rank of 
A, we simply count up the number of positive eigenvalues k and observe that r(A) = /: ^ 
min(m, n) if A is rectangular or r(A) = /: ^ ^z if A is square. 
'^ Note also that A = TD^'^D^'^j' = jj^j' 
^^ desired. 

230 
S. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
Finally, if A is of product-moment form to begin with, or if A is symmetric with 
nonnegative eigenvalues, then it can be written-although not uniquely so-as A = B'B. 
The lack of uniqueness of B was illustrated in the context of axis rotation in factor 
analysis. 
5.7 
THE SINGULAR VALUE DECOMPOSITION (SVD) OF A MATRIX 
With some oversimpUfication, we can summarize the intent of the chapter so far by 
saying that, given a transformation matrix A, we would like to find a representation of it 
in some way that simplifies its geometric nature. 
In the case of a square, but nonsymmetric, matrix A, we found that under fairly 
general circumstances (in which the vectors of U are linearly independent), A could be 
written as 
A = UDU" 
where U is nonsingular and D is diagonal. The eigenvalues and eigenvectors of A need not 
all be real valued, however. 
In the case of a symmetric matrix A, orthogonal diagonaHzation can be achieved. In 
this case the above equation is satisfied and, in addition, we have 
A = TDT' 
since, given an orthogonal matrix T, we know that T' = T"^ Moreover, all real-valued 
symmetric matrices are orthogonally diagonahzable with real-valued eigenvalues and 
eigenvectors. 
Not all matrix transformations are symmetric, however, and not all matrices are 
square. Thus, with the exception of the preceding discussion of matrix rank, we have said 
relatively httle on the topics of (a) the eigenstructure of square, nonsymmetric matrices 
and (b) the role of eigenstructure analysis in dealing with rectangular matrices which, by 
definition, do not have eigenstructures. It is now time to bring up these aspects, 
particularly the latter one. 
This section of the chapter deals with singular value decomposition, the most general 
decomposition of a transformation matrix that is discussed in the book. As we shall see, 
any matrix can be decomposed into its basic structure (although not necessarily uniquely 
so). As such, basic structure represents an extremely powerful concept in multivariate 
analysis and unifies much of our earlier discussion of matrix decomposition.^"^ Again, we 
shall tie in the current material with some comments made on related matters in Chapter 4. 
In Section 4.5.5 we demonstrated that an arbitrary nonsingular matrix 
V = 
1 
2 
3 
4 
^"^ While eigenstructure analysis plays a central role in finding the basic structure of a matrix, it 
should be stressed that the concepts are distinct. 

5.7. 
THE BASIC STRUCTURE OF A MATRIX 
231 
• f 
•I 
/ ? • 
• b 
_L_iL 
•9 
- 5 
^r, 
^ 
e 
• 
^d 
b 
9* 
• 
• d 
• f 
h 
X H P 
Xd[PA] 
Xd[PAQ'] 
Fig. 5.6 
Decomposition of general linear transformation V = PAQ' (reproduced from Fig. 4.13). 
could be uniquely decomposed into the triple product 
P 
A 
V = PAQ' 
-0.41 
-0.91 
-0.91 
0.41 
5.47 0 
0 
0.37 
Q' 
-0.58 
-0.82 
0.82 
-0.58 
wdiere P is orthogonal (specifically, an improper rotation), A is diagonal (a stretch), and 
Q' is orthogonal (a proper rotation). 
For ease of reference, Fig. 5.6 reproduces Fig. 4.13 in which a square lattice of points 
(shown in Panel II of Fig. 4.10) was transformed, in three stages, by the matrices making 
up the specific decomposition of V. Each stage is shown in Fig. 5.6. Moreover, at that 
point we indicated that any nonsingular matrix could be uniquely decomposed into the 
product of (a) a rotation-stretch-rotation or (b) a rotation-reflection-stretch-rotation. 
It turns out, however, that this type of decomposition is a very general type of 
decomposition. It is so general, in fact, that any matrix, square or rectangular, 
nonsingular or singular, symmetric or nonsymmetric, can be so decomposed, albeit not 
necessarily uniquely so. 
In the case of symmetric matrices, we know, of course, that the geometric relationship 
does hold: 
A = TDT' 
since, in this case, T and T' are orthogonal (rotations), and D is diagonal (a stretch).^^ 
Thus, if A is symmetric, the above decomposition holds as a special case. 
However, as already observed in describing the eigenstructure of nonsymmetric 
matrices, there is no requirement that T be orthogonal. Furthermore, no such 
decomposition—orthogonal or otherwise—has been discussed for rectangular matrices. 
We now consider the cases in which A is either square but nonsymmetric or A is 
rectangular. As it turns out, both cases can be handled by the same procedure, and we 
shall illustrate the approach by assuming that A is rectangular, of order mxn. 
For 
convenience, assume that A is "vertical" with m> n, although this is not essential. We 
shall also assume for the moment that rank of A is A: (^ ^ « < m). 
' Or, possibly, a stretch preceded by a reflection. 

232 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
We first examine some of the algebra of this type of decomposition. Then we apply 
the decomposition to an illustrative problem. Let us set down our objective at the outset. 
And that is: Given an arbitrary rectangular transformation matrix A, we wish to fmd a 
way to express A in terms of the product of three, relatively simple, matrices: 
1. ?in mxk 
matrix P which is orthonormal by columns and, hence, satisfies the 
relation P'P = I; 
2. 
2L k xk matrix A which is diagonal and consists of k positive diagonal entries 
ordered from large to small (with ties allowed); 
3. 
an « X A: matrix Q which is orthonormal by columns and, hence, satisfies the 
relation Q'Q = L 
The representation of the matrix A as the triple product FAQ' is called its singular value 
decomposition (abbreviated SVD for reasons of brevity from now on). It is occasionally 
referred to as basic structure (Horst, 1963). 
5.7.1 
The Algebra of Singular Value Decomposition (SVD) 
The mathematical aspects of SVD become rather complex, and so we shall settle for a 
less technical discussion. Given an arbitrary rectangular matrix A,„ x «. where m> n and 
r(A) = /: ^ « < m, its SVD involves the triple product 
^m xn ~ * m x k^k x k^k xn 
where P and Q are each orthonormal by columns (P'P = Ikxk'^ QQ = Ifcxfc)? ^^^ ^kxk is 
diagonal with ordered positive entries. 
Note, in particular, that Q cannot be orthogonal (where QQ' = lnxn) unless k = n. 
Moreover, P cannot be orthogonal (where PP' = I^xm) unless k = n = m. As such, P^XA: 
^ d Qnxk might be called orthonormal sections in which all columns are of unit length 
and mutually orthogonal. 
Next, let us comment on the diagonal matrix Akxh which we will call the singular 
value {or SV) matrix. First, as we shall see, all elements of A^x/: can be 
1. taken to be positive; 
2. 
ordered from large to small (with ties allowed). 
Moreover, it will turn out that there is one and only one SV matrix for any given matrix; 
that is, the singular values, which are the diagonal entries of the SV matrix of the 
decomposition, are always unique, and this will be true regardless of whether A is of full 
rank, square, or rectangular. It may happen, however, that some entries in A/:x/: are tied. 
If such is the case, only those portions of P and Q that relate to distinct entries in Akxk 
will be unique, a point to which we return later. Finally, the rank of A is given quite 
simply by the number of positive entries in A, the SV matrix. 
For the moment, let us examine the relationship of A = PAQ' to its major and minor 
product moments, AA' and A'A, respectively: 
AA' = (PAQ')(PAQ')' = PAQ'QAP' 
but since Q'Q = I and letting A^ = D, we see that D is still diagonal. Thus, we get 
AA' = POP' 

5.7. 
THE BASIC STRUCTURE OF A MATRIX 
233 
Furthermore, 
A'A = (PAQ'y(PAQ') = QAP'PAQ' 
but since P'P = I and A^ = D, we get 
A'A = QDQ' 
Note that in both cases we have the eigenstructure formulation shown earUer for the case 
of symmetric matrices, namely, the triple product of an orthogonal, diagonal, and 
transposed orthogonal matrix. This is not surprising since both AA and A A are 
symmetric. However, the diagonal matrix D of each triple product is the same for both 
product moments A A' and A'A. Furthermore, 
1. all entries of D are real; 
2. 
all entries of D are nonnegative; 
3. 
all positive entries of D can be ordered from large to small (with possible ties, of 
course). 
We take advantage of these facts in describing A, the ^ x /: portion of D that has positive 
(as opposed to zero) entries, in terms of the following definition: 
At this point, then, we are starting to get some hints about how to fmd P^ x/c? ^kxk ? ^"^^ 
Qkxn As observed above, P^xfc represents the first k columns of the orthogonal matrix 
^mxm obtained from the eigenstructure of AA', while Q^XA: represents the first k 
columns of the orthogonal matrix Qnxn obtained from the eigenstructure of A'A. Their 
common diagonal matrix D has all nonnegative entries. Furthermore, we can order (with 
ties allowed) the positive entries of D from large to small, until we get k of them. The 
remaining entries on the main diagonal will all be zero. The columns of P and Q can, of 
course, be made to correspond to the ordered diagonal elements in D and, hence, to their 
square roots in A. 
Next, let us take the argument one step further. If we let A be the first k rows and k 
columns embedded in a (larger) mxn 
rectangular matrix, with m - k rows and n - k 
columns of zeros elsewhere, both P and Q' could be made fully orthogonal and, in this 
sense, properly constitute rotation matrices of order mx m and n x n, respectively. This 
generalization can be called the full SVD of a matrix. 
The preceding generaUzation is a significant one. It tells us that any matrix—not just 
square, nonsingular ones—with real-valued entries can be represented as the product of 
1. a rotation (possibly followed by a reflection), followed by 
2. 
a stretch, followed by 
3. 
a rotation. 
Note further that if, indeed, A is symmetric to begin with, we have the special case 
A = TDT' 
since AA'= A'A, and therefore P' = Q' = T'. Hence, this same approach to matrix 
decomposition can be applied even to the more famiUar case of a symmetric matrix. 

234 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
However, the diagonal D is to be interpreted as A in the current context since we refer to 
A rather than to its product moments AA' or A'A. 
In summary, any matrix of real entries has a S VD and can be written as 
A = PAQ' 
where A is diagonal, and P and Q are orthonormal by columns. The concept of full SVD 
embeds iht kxk diagonal matrix of positive entries in an m x « matrix in which m - k 
rows and n - k columns are zeros. Alternatively, we can require A to be kxk 
with all 
positive entries and, hence, P and Q will not, in general, be rotation matrices, although 
they will still be orthonormal by columns and constitute orthonormal sections. 
Finally, a special case of the above involves the case in which A is symmetric to begin 
with. If so, it can be written as 
A = TDT' 
where T, of course, is an orthogonal matrix and D = A in the present discussion. 
Figure 5.7 shows schematically the two cases that we have been considering. Panel I 
shows the case in which P is orthonormal by columns (P'P = I) but is not orthogonal. 
Similarly, Q is orthonormal by columns (Q'Q = I) but is not orthogonal. The diagonal 
matrix A has k positive eigenvalues, where k <n< 
m. 
Panel II shows the full SVD in the sense that P and Q can be made orthogonal 
by embedding A in a larger matrix, consisting of m - k rows and n - k columns of zeros 
(in addition, of course, to the zeros in the off-diagonal entries of the /: x /: portion). Not 
surprisingly, the m -/: columns of P and the n -k rows of Q—while they could be made 
orthogonal—are mainly of theoretical interest since those dimensions would be anni-
hilated by the m - /: rows of zeros and the n - k columns of zeros in the mx n matrix in 
which A is embedded. 
5.7.2 
Conditions for Full Rank Matrices 
The foregoing discussion applies to any matrix of interest since any matrix possesses 
a SVD, written as the triple product PAQ'. While any matrix can be decomposed into its 
SVD, not all matrices are full rank. This distinction needs to be made in discussing the 
rank of a matrix. 
A 
Ar 
0 
A = 
^^— 
k xk 
0 
0 
0 
Fig. 5.7 
Alternative formulations of the basic structure of A„ 

5.7. 
THE BASIC STRUCTURE OF A MATRIX 
235 
A full rank matrix A is one whose rank equals its smaller order. If A is m x n 
(and m> n), and if A is full rank then r(A) = k = n. 
If A is square and full rank, then r(A) equals its (common) order, and we have called 
this kind of matrix nonsingular. If A is rectangular and full rank, then the rank of the 
major product moment AA' or the minor product moment A'A is equal to the smaller 
order of A. If A is not full rank, then its rank is k{k 
<n^m). 
This can all be summarized by saying that any matrix A can be decomposed into 
the SVD 
^mxn 
'^m xk^k xk^k y 
where k ^ min(m, n). Then A is full rank if and only ifk = min{m,n). 
5.7.3 
Finding the Singular Value Decomposition (SVD) 
It is one thing to define the SVD of a matrix A and quite another to solve for its 
representation as A = PAQ'. Finding the SVD of a matrix A makes use of concepts 
already discussed under the topic of eigenstructure. First, as previously discussed, if 
A = PAQ', then 
A'A = QAP' FAQ' 
and, since P is orthonormal by columns, we have 
A'A = QA^Q' 
Then, after we solve for A^ and Q by finding the eigenstructure of the symmetric matrix 
A'A, we can find A and then find P from 
P=AQA^ 
Since A is diagonal, A ^ consists simply of the reciprocals of the diagonal entries of A. At 
this point, then, we have a procedure for solving for the triple product 
A = PAQ' 
Note that the above implies the singular value matrix for the SVD of A is the square 
root of the diagonal matrix of eigenvalues of the minor product moment matrix of 
A, A'A. The diagonal entries of A are called "singular values" (thus the term "singular 
value matrix" for A), so that the singular values of A are the square roots of the eigenvalues 
of A'A. The matrix Q is the matrix of principal eigenvectors (those associated with positive 
eigenvalues) of A'A, but it is also called the matrix of principal right singular vectors of 
A. It has already been seen that the k nonzero eigenvalues of the minor product moment 
of A (A'A) are identical to the {k) nonzero eigenvalues of its major product moment matrix 
(AA'). It tums out that P is the matrix whose columns are the principal eigenvectors of A A' 
(those associated with its k nonzero eigenvalues and ordered in the same descending order in 
which those eigenvalues themselves are conventionally ordered in the diagonal eigenvalue 
matrix D = A^). The columns of P are also called principal left singular vectors of A. 

236 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
Thus the singular value decomposition (SVD) decomposes A into a triple product of 
three matrices, the first that of its left singular vectors (P), the second the diagonal singu-
lar value matrix(A), and the third being the transpose of the matrix of right singular vec-
tors (Q')- These matrices, as we have seen, all are very closely related to, and can be defined 
via, the eigenvectors of the minor and major product moment matrices of A. (We could, for 
example, estimate P directly by computing the eigenvectors of AA', instead of using the 
equation P = AQA"^ above, but this approach would be computationally more expensive, in 
general, and could also lead to certain indeterminacy problems in case of tied eigenvalues— 
so the approach discussed above is definitely preferable to this alternative approach.) 
The procedure involves the following steps, assuming first that A is of order m x « with 
« < m: 
1. 
Compute the minor product moment A'A which results in a square symmetric 
matrix of order A? x «, where n ^ m. 
2. 
Find the eigenstructure of A'A, thus yielding the matrix of eigenvalues A^ of rank 
k{k<n) 
and the matrix Q where Q is the matrix of associated eigenvectors that are 
orthonormal by columns. 
3. 
Find the square roots of the diagonal entries of A^. 
These are called "singular values of P". 
4. 
Find A'^ the reciprocals of the diagonal entries of A. 
5. 
FindP = AQA-^ 
It turns out that 'ifk = n, then A is full rank. lfk<n, then A is not full rank but of rank k. 
On the other hand, if « > m we apply the same type of procedure to A' and transpose 
the result. That is, let 
A'=PiAiQi' 
Then its transpose is 
Then, if we defme 
A = (PiA,Q/y = QiAiP/ 
P=Qi; 
A^Ar, 
Q = Pi 
it turns out that we have the desired decomposition of A into the SVD 
A = PAQ' 
5.7.4 
Illustrating the Singular Value Decomposition (SVD) Procedure 
To illustrate the procedure described above, let us take a particularly simple case 
involving a 3 x 2 matrix: 
Since m > «, we first find the (smaller) minor product-moment matrix: 
" 1 
0 
- 1 
2~ 
2 
1 
A'A = 
2 
1 
1 9 
0 
2 
- 1 
1 
r 1 
0 
|_-i 
2~ 
2 
1_ 

5.7. 
THE BASIC STRUCTURE OF A MATRIX 
237 
Table 5.2 shows a summary of the computations involved in fmding the eigenstructure of 
A'A. Note that this part of the problem is a standard one in finding the eigenstructure of 
a symmetric matrix. 
After fmding A^ and Q, by means of solving the characteristic equation, we find A 
and then A "^. The last step is to solve for P in the equation 
P = AQA-' 
These results also appear in Table 5.2. Finally, we assemble the triple product 
F
A
Q
' 
1 2 
0 
2 
1 
1_ 
= 
-0.70 
-0.65 
_-0.28 
0.52, 
-0.20 
-0.83 J 
[3.02 
Lo 
0 ] 
1.36J 
r-0.14 
L 0.99 
-0.99 
-0.14 
As can be noted, after taking the transpose of Q, the matrix A has been decomposed into 
the product of an orthonormal (by columns) matrix times a diagonal times a (square) or-
thogonal matrix. In general, however, Q' will be orthonormal by rows if A is not full rank. 
TABLE 
5,2 
Finding the Eigenstructure 
o/A'A 
Minor product-moment matrix 
"2 
f 
A'A = 
1 
9 
Quadratic formula 
y - ax^ + bx + c 
Characteristic equation 
|(A'A)-M| = 
2~X 
1 
1 
9-A 
= 0 
X ' - 1 U + 17 = 0 
Substitution in general quadratic 
-b ± yjb^~4ac 
11 ± V ( - l l ) 2 - 4 ( 1 7 ) 
2a 
Eigenvalues of A' A 
\ i = 9 . 1 4 
\^ = 1.86 
Diagonal singular value (SV) matrix 
(9.14)'^' 
0 
Matrix of eigenvectors of A'A 
-0.14 
0.99 
-0.99 
-0.14 
0 
(1.86)'^' 
3.02 
0 
0 
1.36 
Solving for the matrix P 
A 
Q 
1 
2 
0 
2 
-1 
1 
-0.14 
0.99 
-0.99 
-0.14 
1/3.02 
0 
0 
1/1.36_ 
-0.70 
0.52 
-0.65 
-0.20 
-0.28 
-0.83 

238 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
A = 
1 2 
0 2 
-1 1 
c=(-^,^)^ 
i + 
H 
h 
b = (0, 2) 
• a=(1,2) 
H 
h 
I 
h-
1 
-0.5 
II 
I T 
0.5 + 
III 
2T 
+ 
PA 
I 
h 
-2m by 
H 
1 
IV 
^ = (0,2) 
y t ^ ^ ^ a = (1,2) 
I / 
1 1 — 
^2 
I 
^2 
Fig. 5.8 Decomposition of A to SVD A = PAQ'. Key: I, "target" configuration as defined by A = PAQ'; 
II, orthonormal-by-columns matrix P; III, application of stretch (defined by A) to P; IV, rotation of PA 
via matrix Q'. 
Fi gure 5.8 shows the separate aspects of the decomposition. We first start with the 
matrix P. P is then differentially stretched in accordance with A. Finally, the points of 
PA are rotated in accordance with Q , leading to a reproduction of what we started out 
with, namely, the matrix A shown at the top of the figure. 
5.7.5 
The SVD of the Sample Problem Matrix of Predictors 
In Section 5.4 we found the eigenstructure of the covariance matrix of the two 
predictor variables in the sample problem. As recalled, 
C{X) = X^'Xjm = TDT' 
T 
D 
T' 
~0.787 
0.787 
0.617 
0.617 
-0.787 
22.56 
0 
0 
0.54 
0.617 
0.617 
0.787 
14.19 
10.69 
10.69 
8.91 
where T and T' are orthogonal and D is diagonal. 
Suppose, now, that we wished to find the SVD of Xd/Vw^, the mean-corrected matrix 
of predictors, scaled by the square root of the sample size. 
Based on what we have just covered, we know how to proceed. First, we find 

5.7. 
THE BASIC STRUCTURE OF A MATRIX 
239 
A-' = D-
1/V22.56 
0 
0.211 
0 
0 
l/\/a54 I [o 
1.361 
Next, analogous to solving for P in the expression P = AQA"^, we now solve for U in the 
expression 
which leads to the SVD of Xd/Vm^ in the present notation as 
Xd/\/w=UAT' 
Since no new principles are involved, we do not go through the extensive computations to 
find U, which is of order 12x2. What can be said, however, is that any data matrix, X, 
Xd, Xs, X/\/m, Xs/\/m can be expressed in terms of SVD injust the way described above. 
5.7.6 
Recapitulation 
The concept of SVD represents the most general of decompositions that are considered 
in this chapter. We have only provided introductory material on the topic, but, having 
done so, it seems useful to recapitulate the main results and add a few more comments as 
well: 
1. 
Any matrix A can be decomposed into the triple product: 
A = PAQ' 
where P and Q are each orthonormal by columns, and A is diagonal with ordered 
positive elements. 
2. 
The number of positive elements in A, the singular value (SV) matrix, is equal 
to the rank of A. Moreover, the SV matrix is unique. 
3. 
If all entries of A are distinct, then P and Q' are also unique (up to a possible 
reflection). 
4. 
If some entries of A are tied, then those portions of P and Q' corresponding to 
tied blocks of entries are not unique. The portions of P and Q corresponding to the subset 
of distinct entries of A are unique (up to a reflection), however. 
5. 
Mutually orthogonal vectors for tied blocks of entries in A can also be found by 
the Gram-Schmidt process, after first finding a set of r linearly independent vectors in 
the tied block. (These are unique up to orthogonal rotation within the r-dimensional 
subspace corresponding to the r tied eigenvalues.) 
6. 
A full rank matrix is one whose rank equals its smaller dimension. 
7. 
A square full rank matrix is one whose rank equals its smaller order. More 
commonly, a square full rank matrix is called nonsingular. 
8. 
If A is nonsingular, then P'P = PP' = I; Q'Q = QQ' = I, and we have the case of 
rotation-stretch-rotation or rotation-reflection-stretch-rotation. 
9. 
If A is rectangular or square but singular, the concept of full SVD, in which 
Ay^xy^ is embedded in a larger m x n matrix (see Fig. 5.7), still involves the sequence of 
transformations shown immediately above. Some dimensions of P and/or Q are 
annihilated, however. 

240 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
10. 
The orthogonal diagonalization of a symmetric matrix 
A = TDT' 
was shown to be a special case of SVD. 
11. 
The concepts of matrix nonsingularity and decomposition uniqueness should be 
kept separate. A square matrix Anxn can be nonsingular but still nonunique in terms of 
its SVD if it contains tied (positive) entries in Anxn-
12. A square matrix A^ x n can be singular but still unique in terms of its SVD 
if it contains all distinct entries in A^ x «» thus implying that only one entry is 
zero. 
13. 
A square matrix A^xn can, of course, be both nonsingular and unique in terms 
of SVD if all entries in A„ x A? are positive and distinct. 
As we know at this point, if a matrix is of rank k, then the SVD procedure will repro-
duce it in terms of the triple product PAQ', where the diagonal SV matrix A is ^ x ^. 
What has not been covered is the case in which we would like to approximate A with a 
triple product whose diagonal is of order less than k xk. This type of problem crops up in 
principal components analysis, among other things, when we wish to reduce the original 
space to fewer dimensions with the least loss of information. 
Fortunately, SVD provides a reduced-rank approximation to A whose sum of squared 
deviations between A and PAQ' is minimal for the order of the diagonal being retained. 
While it would take us too far afield to explore the topic of matrix approximation via 
SVD, this turns out to be another valuable aspect of the technique. Not surprisingly, the 
fact that the entries of A are ordered from large to small figures prominently in this type 
of approximation. 
5.8 
QUADRATIC FORMS 
In multivariate analysis one often encounters situations in which the mapping of some 
vector entails a quadratic, rather than linear, function.^^ At first blush it may seem 
surprising that matrix algebra is relevant for this situation. After all, thus far we have 
emphasized the appUcabihty of matrices to linear transformations. It is now time to 
expand the topic and consider quadratic functions and, in particular, quadratic forms. 
5.8.1 
Linear Forms 
We have already encountered linear forms in our discussion of simultaneous equations 
in Chapter 4. If we have a set of variables Xf and a set of coefficients flp a linear form can 
be written in scalar notation as 
n 
g(x) = aiXi + ^2^2 + • • • + a^Xn = X ^iXi 
in which all X/, as noted, are of the first degree. In vector notation we have 
^^ Clearly, the idea of the variance of some variable entails a quadratic function, and variances 
represent a central concept in statistical analysis. 

5.8. 
QUADRATIC FORMS 
241 
^(x) = a'x = (dri,^2,- • • ,^n)j 
X2 
which, of course, equals some scalar, once we assign numerical values to a and x. 
Next, suppose we consider a set of several linear forms, with the matrix of coefficients 
given by A and the vector of constants given by c. Then we have 
A 
X 
c 
Ax = c = 
an 
ai2 
" ' ^i« I I ^i | 
| Ci 
^2 1 ^22 
• • • ^2« I I -^2 I I C2 
This, of course, represents a set of simultaneous linear equations. Hence, a linear form is 
simply a linear function in a set of variables X/. 
5.8.2 
Bilinear Forms 
Bilinear forms involve only a slight extension of the above. Here we have two sets of 
variables Xi and^'y, each of the first degree, as illustrated specifically by 
/(X, j ) = X i J i +6X2;^i-4X3ji +2Xi72 +3X2^2 +2X3^2 
in which exactly one X/ and one yj (each of the first degree) appears in each term. More 
generally, expressions of this type can be written in scalar notation as 
m 
n 
f(x, y)= T T aifX^yf 
/ = 1 7 = 1 
and are called bilinear forms in X/ and yj. If we write the vectors x' = (xi, X2, • . - , x^) 
^ d y' = (yi, y2^' • • * yn), a bilinear form involves terms in which every possible 
combination of vector components is formed. In matrix notation we can write a bilinear 
form as 
/(x,y) = x'Ay 
In the numerical example above, we have 
^11 = 1; 
^12 = 2; 
^^21= 6; 
^22 = 3; 
^31 = ~4; 
^32 = 2 
and the function can be expressed as 

242 
5. EIGENSTRUCTURES AND QUADRATIC FORMS 
x' 
A
y 
/(x,y) = (xi,X2,X3) 
^ 1 ^1 
6 3 
_-4 
2] 
hi 
iy2_ 
(xi + 6X2-4JC3,2XI + 3^2 +2x3) 
yi 
= Xiyi +6x2>'i-4x3j^i +2xij;2 + 3x2j^2 +2x3>^2 
The matrix A is called the matrix of the bilinear form, and it determines the form 
completely. Note that, in general, A need not be square. 
By assigning different values to x and y one obtains different values of the bilinear 
form, each of which is a scalar. The set of all such scalars, for a given domain of x and y, 
is the range of the bilinear form. 
5.8.3 
Quadratic Forms 
Next, let us speciaUze the bilinear form to the specific case in which x = y. In this case 
we assume that y can be replaced by x and, given their same dimensionality, the matrix of 
coefficients A will be square rather than rectangular. For example, 
/(Xi,X2) = 2Xi^ + 5X1X2 + 3X1X2 + 6X2^ 
can now be written in matrix form as 
/ ( x ) = (Xi,X2) 
"2 3] 
5 6j 
r^i 
[^2 
= (2xi + 5x2,3xi + 6x2) 
Xi 
X2 
= 2xi^ + 5x1X2 + 3x1X2 + 6x2^ 
and the result, again, is a scalar, once numerical values are assigned to Xi and X2. Also, by 
assigning different values to x over its domain, we can obtain the range of/(x), the 
quadratic form. 
By way of formal definition, a quadratic form is a polynomial function 
o / x i , 
X2,. . . , x„ that is homogeneous and of second degree. For example, in the case of two 
variables, we have 
/(Xi,X2)=Xi^ +6X1X2 +9X2^ 
However, we can also write this as 
/(x)=Xi^ + 6X1X2 +9X2^ 
in which the vector x' = (xi,X2) is mapped from a two-dimensional space into a 
one-dimensional space. Similarly, an example of a quadratic form in three variables is 
/(x)=Xi^+X2^+X3^ 
where the vector x' = (xi, X2, X3) is mapped from three dimensions to one dimension. 

5.8. 
QUADRATIC FORMS 
243 
In general, a quadratic form in n dimensions can be written in scalar notation as 
n 
Qiu)= 
Z aijUiUj 
where u' = (wi, W2> • • • , w«), the Utj are real-valued coefficients and the Ut Uj are the 
preimages of the mapping. If / = /, we obtain the squared term fif/W/^, and if i i=^ /, we 
obtain the cross-product term UijUiUj. 
By "homogeneous" we mean that all terms are of the above form and, in particular, 
there are no linear terms in the w/s nor is there a constant term. While the function 
V=Xi^ + IX-i" + JC1JC2 + Xi + 3^2 
is a second-degree polynomial, it is not a quadratic form since the last two terms are not 
of the general form UijUiUj, 
Quadratic forms are of particular interest to multivariate data analysis inasmuch as we 
are often concerned with what happens to variances and covariances under various linear 
functions of a set of multivariate data. 
While we did not bring up the topic of quadratic forms at that time, our 
diagonalization of the sample problem covariance matrix in Sections 5.3 and 5.4 involved 
a quadratic form, with matrix Q{X). Indeed, all of the cross-product matrices employed in 
multivariate analysis, such as the raw cross product, SSCP, covariance, and correlation 
matrices, are illustrations of quadratic forms. In these cases the diagonal entries are some 
measure of single-variable dispersion, and the off-diagonal entries are some measure of 
covariation between a pair of variables. 
In working with quadratic forms, our motivation is similar to diagonalizing 
transformation matrices. That is, we shall wish to fmd a linear function of the original 
data that has the effect of leading to a cross-products matrix in which two things are 
desired: (a) an arrangement of the linear composites so that the main diagonal entries in 
the cross-product matrix decrease in size and (b) off-diagonal entries of the cross-products 
matrix being zero, indicating uncorrelatedness of all pairs of composites. This, of course, 
is the same motivation underlying principal components analysis, as illustrated in 
Section 5.4. 
5.8.4 
An Illustrative Problem 
Suppose we have the quadratic form ^(x) = 66x1^ + 24x1^2 + 59x2^. This can be 
expressed in matrix product form as 
I 66 12 
q'Aq = (Xi,X2) 
^1 
X2 
12 59 
= (66x1 + 12x2, 12xi + 59x2) 
Xi 
X2 
= 66x1^ + 12x1X2 + 12x1X2 + 59x2^ 
q'Aq = 66x1^ + 24xiX2 + 59x2^ 
Notice that we set the off-diagonal entries of A to half the coefficient of x 1X2 which is 
24/2=12. 

244 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
Fig. 5.9 
Change of basis vectors of matrix representing quadratic form. 
Notice further that a quadratic form involves a transformation into one dimension of 
an /7-component vector in which the transformation is characterized by an n xn 
symmetric matrix.^^ On the other hand, a Unear mapping of a vector in n dimensions into 
one dimension entails a single vector (either a 1 x « or an « x 1 matrix), whose entries are 
usually expressed as direction cosines. 
Now let us see what happens when we take various values of Xi and X2 and substitute 
them in q'Aq. The way this can be done graphically, as shown in Fig. 5.9, is to take 
various pairs of Xi, X2 values on the unit circle in which we have the condition 
(^l,->^2) 
^1 
X2 
= 1 
For example let 
Xi = \; 
X2 = 0 
Xi = 0; 
^2 = 1 
Xi = -0.707; 
X2=-0.707 
Xi =0.707; 
X2 = 0.707 
Xi =-0.707; X2 = 0.707 
Xi= 0.707; 
X2 =-0.707 
We can select still other vectors of points on the unit circle and multiply the length or 
distance from the origin of each by (q'Aq)*^^. This "stretching" of q on the unit circle 
q'Aq = 66 
= 59 
= 74.5 
= 74.5 
= 50.5 
= 50.5 
(q'Aq)^/^ = 8.1 
= 7.7 
= 8.7 
= 8.7 
= 7.1 
= 7.1 
^'^ The matrix of a quadratic form does not have to be represented by a symmetric matrix. 
However, the original matrix can always be symmetrized by setting each off-diagonal entry equal to 
half the sum of the original off-diagonal entries; that is, (ajj + aji)/2. 

5.8. 
QUADRATIC FORMS 
245 
into the point [(q'Aq)^^^]q results in the eUipse shown in Fig. 5.9. This eUipse may be 
viewed as a geometric representation of the quadratic form. 
Now, however, suppose we consider another quadratic form: 
u Bu = ISxr 
+ 50x1" 
that can, in tum, be represented as 
U'BU = ( X I * , X 2 * ) 
75 
0 
0 
50 
X2' 
We see from Fig. 5.9 that if we rotate the coordinate system 37° to the axes :vi * and ^2* 
then this function also lies on the previously obtained ellipse. Vector lengths of the major 
and minor semiaxes of the ellipse are \/75 = 8.7 and \/50 = 7.1, respectively. That is, by a 
change in orientation of the axes, we obtain a new representation of the quadratic form 
in which the X1JC2 cross product vanishes. Moreover, one axis of this form coincides with 
the major axis of the ellipse, while the other corresponds to the minor axis of the eUipse. 
These axes are usually referred to as principal axes. By eliminating the cross-product term 
the second matrix is seen to be a simpler representation of the quadratic form than the 
first. Moreover, the entries of the diagonal matrix B are in decreasing order. 
5.8.5 
Finding the New Basis Vectors 
As the reader has probably surmised already, the new representation of the quadratic 
form U'BU is obtained by solving for the eigenstructure of A. Primarily in the nature of 
review we set up the characteristic equation 
lA-XIl 
66-X 
12 
12 
59-X 
0 
and solve for its eigenvalues by finding the second-order determinant and setting it equal 
to zero: 
X'-125X +3750 = 0 
(X-75)(X-50) = 0 
Xi=75; 
X2 = 50 
After substitution of Xi and X2, we find the normaHzed eigenvectors [^Q*^ ] and {J^% ], 
which can be arranged in the matrix Q. 
-0.8 
-0.6 
0.6 
-0.8 
Notice that |Q| = 1 and QQ' = Q'Q = I. That is, Q is orthogonal and represents a proper 
rotation. 

246 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
We then have the relationship 
-0.8 
-0.6 
0.6 
-0.8 
Q'AQ = D 
66 
12 
12 
59 
0.8 
0.6 
0.6 
-0.8 J 
75 
0 
0 
50 
Finally, we see that the last matrix on the right is equal to B, the diagonal matrix of the 
new quadratic form that reorients the axes. Furthermore, if we reflect the first column of 
Q, we note that cos 37° =0.8, indicating that Xi* makes an angle of 37° with the 
horizontal axis, while X2* makes an angle of —53° with the horizontal axis. 
In brief, no new principles are involved in the present diagonalization process. As 
noted, A is symmetric to begin with, so all of our previous discussion about diagonalizing 
symmetric matrices is relevant here. We note that the present formula 
D = Q'AQ 
is the same as that found in Section 5.2: 
D = T'AT 
where D is diagonal. The matrix Q, an orthogonal matrix, is the same as T in the context 
of Section 5.2. 
5.8.6 
Types of Quadratic Forms 
Quadratic forms can be classified according to the nature of the eigenvalues of the 
matrix of the quadratic form: 
1. If all X/ are positive, the form is said to be positive definite. 
2. 
If all X/ are negative, the form is said to be negative definite. 
3. 
If all X| are nonnegative (positive or zero), the form is said to be positive 
semidefinite. 
4. 
If all X/ are nonpositive (zero or negative), the form is said to be negative 
semidefinite. 
5. 
If the \i represent a mixture of positive, zero, and negative values, the form is said 
to be indefinite 
In multivariate analysis we are generally interested in forms that are either positive 
definite or positive semidefinite. For example, if a symmetric matrix is of product-
moment form (either A'A or AA'), then it is either positive definite or positive 
semidefinite. Since various types of cross-products matrices are of this form, the cases of 
positive definite or positive semidefinite are of most interest to us in multivariate analysis. 
5.8.7 
Relating Quadratic Forms to Matrix Transformations 
As might be surmised from our earlier discussion of matrix eigenstructure and basic 
structure, quadratic forms are intimately connected with much of the preceding material. 
For example, suppose we have the point transformation 
u = Xv 

5.9. EIGENSTRUCTURES OF NONSYMMETRICAL MATRICES 
247 
\s^ere X, whose rows are sets of direction cosines, maps v, considered as a column vector, 
onto u in some space of interest. 
To illustrate, we let 
Hence, if v = [o] ^ we have 
X = 
0.8 
0.6 
0.71 
0.71 
u = Xv = 
0.8 
0.6 
0.71 
0.71 
0.8 
0.71 
Now suppose we want to find the squared length of u. 
The squared length of u is defined to be u'u. Given v and the linear transformation X, 
we set up the expression 
u'u = (Xv)'(Xv) = V X'Xv 
But now we see that X'X is just the minor product moment of X which we have already 
discussed. We can denote this as A. Thus, we have 
u'u = V'AV = (1,0) 
u'u=1.14 
1.14 
0.98 
0.98 
0.86 
Hence, product-moment matrices, which were discussed earlier in the context of 
eigenstructure and SVD, also appear in the present context as matrices defining 
quadratic forms. That is, A = X'X = S(X) is the matrix of the quadratic form that finds 
the squared length of\ under the linear transformation X. 
Up to this point we have said relatively little about the process of finding 
eigenstructures of nonsymmetrie matrices. We did indicate, however, that for the matrices 
of interest to us in multivariate analysis their eigenstructures will involve real-valued 
eigenvalues and eigenvectors. Be that as it may, it is now time to discuss their 
eigenstructure computation. 
5.9 
EIGENSTRUCTURES OF NONSYMMETRIC MATRICES 
IN MULTIVARIATE ANALYSIS 
In multivariate analysis it is not infrequently the case that we encounter various types 
of nonsymme trie matrices for which we desire to find an eigenstructure. Canonical 
correlation, multiple discriminant analysis, and multivariate analysis of variance are 
illustrative of techniques where this may occur. 
As a case in point, let us examine the third sample problem presented in Section 1.6.4. 
As recalled, the twelve employees were divided into three groups on the basis of level of 
absenteeism. While an underlying variable, degree of absenteeism, is present in this 
example, let us assume that the three groups represent only an unordered polytomy. 

248 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
Fig. 5.10 
Mean-corrected predictor variables (from Fig. 1.5). Key: 
Group 3. 
Group 1; o Group 2; x 
The two predictor variables were Xi (attitude toward the firm) and X2 (number of 
years employed by the firm). Figure 5.10 reproduces the scatter plot of the mean-
corrected values of A'd2 versus X^i , as first shown in Fig. 1.5. The three groups have been 
appropriately coded by dots, circles, and small JC'S. We note from the figure that the 
individuals in the three groups show some tendency to cluster. 
However, we wonder if a linear composite of Xdi and X^2 could be found that would 
have the property of maximally separating the three groups in the sense of maximizing 
their among-group variation relative to their within-group variation on this composite. 
Somewhat more formally, we seek a linear composite with values 
W/(l) = l^lXd/i +y2^d/2 
with the intent of maximizing the ratio 
\\diere SS/^ and 5'5'w denote the among-group and within-group sums of squares of the 
linear composite Wi. 
We can rewrite the preceding expression in terms of quadratic forms by means of 

5.9. 
EIGENSTRUCTURES OF NONSYMMETRIC MATRICES 
249 
where A and W denote among-group and (pooled) within-group SSCP matrices, 
respectively. Thus, we wish to find a new axis in Fig. 5.10, that can be denoted Wi, with 
the property of maximizing the among- to within-group variation of the twelve points, 
when they are projected onto it. 
The reader will note the similarity of this problem to the motivation underlying 
principal components analysis. Again we wish to maximize a quantity Xi, with respect to 
vi. However, Xi is now considered as a ratio of two different quadratic forms. As such, 
this problem differs from principal components analysis in several significant ways. 
As shown in Appendix A, the following matrix equation 
(A-XiW)v = 0 
is involved in the present maximization task. However, if W is nonsingular and hence W ^ 
exists, we can multiply both sides by W~^: 
(W"'A-XiI)v-0 
with the resulting characteristic equation 
|W-'A-XiI| = 0 
and the problem now is to solve for the eigenstructure of W~^A. 
So far, nothing new except for the important fact that W~^A is nonsymmetric, even 
though both W~^ and A are symmetric. Up to this point relatively Httle has been said 
about finding the eigenstructure of a nonsymmetric matrix. We can, however, proceed in 
two different, but related, ways.^^ First, we solve directly for the eigenstructure of the 
matrix involved in the current sample problem. This approach is a straightforward 
extension of earlier discussion involving the eigenstructure of symmetric matrices (as well 
as material covered in Section 5.3). 
Second, we can show geometrically and algebraically an equivalent approach that 
involves the simultaneous diagonalization of two different quadratic forms. This 
presentation ties in some of the material here with previous comments on principal 
components analysis. 
5.9.1 
The Eigenstructure of W~^A 
Probably the most popular approach to solving for the eigenstructure of W~^ A is to 
find the eigenvalues and eigenvectors directly, in the same general way as discussed earlier 
for symmetric matrices. In this case, however, W"^A is nonsymmetric; hence V, the 
matrix of eigenvectors, will not be orthogonal. 
Table 5.3 shows the prehminary calculations needed for finding the (pooled) 
within-group SSCP matrix W and the among-group SSCP matrix A. 
Table 5.4 shows the various quantities needed to solve for the eigenstructure of W"^A 
in terms of the characteristic equation 
^* As a matter of fact still other ways are available to solve this problem. The interested reader can 
see McDonald (1968). 

250 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
TABLE 5.3 
Preliminary Calculations for Multiple Discriminant Analysis 
Employee 
^dl 
^A2 
Within-group 
deviations 
Xfc-X 
Among-group 
deviations 
5.25 
4.25 
4.25 
3.25 
4.25 
1.25 
1.25 
0.25 
0.75 
0.50 
3.75 
4.75 
4.75 
5.75 
4.75 
-3.92 
-3.92 
-2.92 
-2.92 
-3.42 
-0.92 
1.08 
0.08 
-0.92 
-0.17 
3.08 
2.08 
4.08 
5.08 
3.58 
-1 
0 
0 
1 
-0.75 
-0.75 
0.25 
1.25 
-1 
0 
0 
1 
-0.5 
-0.5 
0.5 
0.5 
-0.75 
1.25 
0.25 
-0.75 
-0.50 
-1.50 
0.50 
1.50 
-4.25 
-4.25 
-4.25 
-4.25 
-0.5 
-0.5 
-0.5 
-0.5 
4.75 
4.75 
4.75 
4.75 
-3.42 
-3.42 
-3.42 
-3.42 
-0.17 
-0.17 
-0.17 
-0.17 
3.58 
3.58 
3.58 
3.58 
Within-group SSCP matrix 
Among-gioup SSCP matrix 
A = (X;,-^)'(X;,-^) 
|W-^A-X,I| = 0 
As noted in Table 5.4, we first compute the (pooled) within-group SSCP matrix W and 
the among-group SSCP matrix A. 
One then finds W"^ and the matrix product W~*A. From here on, the same general 
procedure appHes for finding the eigenvalues. These turn out to be 
Xj = 29.444; 
•• 0.0295 
which appear in Table 5.4 along with the matrix V whose columns are eigenvectors of 
W"^ A. And, as indicated earlier, V is, in general, not orthogonal. 
Returning to Fig. 5.10, we note that the first column of V entails direction cosines 
related to a 25° angle with the horizontal axis. The resulting linear composite Wi has 
scores that maximize among- to within-group variation. The second discriminant axis W2 
(with an associated eigenvalue of only 0.0295) produces very little separation and, in 
cases of practical interest, would no doubt be discarded. 
Other parallels with the principal components analysis of Sections 5.3 and 5.4 are 
found here. For example, discriminant scores—analogous to component scores—are found 
by projecting the points onto the discriminant axes. The discriminant score of the first 
observation on Wi is 
wi(i) = 0.905(-5.25) + 0.425(-3.92) = -6.42 
as shown in Fig. 5.10. 

5.9. 
EIGENSTRUCTURES OF NONSYMMETRIC MATRICES 
251 
TABLE 5.4 
Finding the Eigenstructure o/W"^A 
SSCP matrices of sample problem 
Within-group SSCP matrix 
Among-group SSCP matrix 
W = 
6.75 
1.75 
1.75 
8.75 
Solving for the eigenstructure of W ^A 
0.156 
-0.031' 
-0.031 
0.121 
w-» = 
W-'A = 
163.50 
126.50 
126.50 
98.17 
21.594 
16.698" 
10.138 
7.880 
Eigenvalues of W"*A 
29.444 
0 
0 
0.0295 
Eigenvectors of W *A 
'0.905 
-0.612 
0.425 
0.791 
However, unlike principal components analysis, we can observe from the figure that Vi 
and V2 are not orthogonal, even though the scores on Wi versus W2 are uncorrelated. 
From the V matrix in Table 5.4 we can compute the cosine between Vi and V2 as follows: 
cos "^ = (0.905 
0.425) 
-0.612 
0.791 
-0.21 
The angle ^ separating Vi and V2 is 90° + 12° = 102°, as shown in Fig. 5.10. 
In summary, finding the eigenstructure of the nonsymmetric matrix W~ ^A proceeds in 
an analogous fashion to the procedure followed in the case of symmetric matrices. Note, 
however, that the matrix of eigenvectors V is not orthogonal even though the 
discriminant scores on Wi and W2 are uncorrelated. 
5.9.2 
Diagonalizing Two Different Quadratic Forms 
The preceding solution, while straightforward and efficient, does not provide much in 
the way of an intuitive guide to what goes on in the simultaneous diagonalization of two 
different quadratic forms: 
Vi'Wvi; 
Vi'Avi 
However, we can sketch out briefly a complementary geometric and algebraic approach 
that relates this diagonalization problem to the earlier discussion of principal components 
analysis. 
As recalled from Chapter 2, variances and covariances can be represented as vector 
lengths and angles in person space. Moreover, as shown in Fig. 5.9, quadratic forms can be 
pictured geometrically as ellipses in two variables, or ellipsoids in three variables, or 
hypereUipsoids in more than three variables.^^ The thinner the ellipse, the greater the 
correlation between the two variables. The tilt of the ellipse and the relative lengths of its 
*' Of course, if more than three dimensions are involved, a literal "picture" is not possible. 

252 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
2 
A (among-groups) 
Principal 
axes of W 
W (within-groups) 
III 
IV 
A (transformed) 
W (spherized) 
Principal axes 
of A (transformed) 
W (spherized) 
Fig. 5.11 
Simultaneous diagonalization of two different quadratic forms. 
axes are functions of the covariances and variances of the two variables. As we know, 
larger variances lead to longer (squared) lengths and also tilt the ellipse in the direction of 
the variable with the larger variance. 
For what follows we shall use the matrix Qi to refer to the matrix of eigenvectors of 
W~ ^^ ^and the matrix Q2 to refer to the matrix of eigenvectors of the transformed matrix 
W" '^ ^AW ^^ 2(as will be explained). 
As motivation for this discussion, suppose we wished to find a single change of basis 
vectors in Panel I of Fig. 5.11 that diagonaHzes both quadratic forms.^^ One quadratic 
form could involve a pooled within-group SSCP matrix W. Similarly, the second form 
could be represented by an among-group SSCP matrix A. We assume that the quadratic 
form denoting the within-group variation is positive definite. (One of the two forms must 
be positive definite for what follows.) 
Geometrically, what is involved is first to rotate the within-group ellipse in Panel I to 
principal axes, as shown in Panel II. We then change scale, deforming the reoriented 
ellipse W to a circle in Panel III. This, in general, is called "spherizing." After this is done, 
any direction can serve as a principal axis. Hence, we can rotate the new axes to line them 
^^ By diagonalization we mean a transformation in which off-diagonal elements vanish in the 
matrix of the quadratic form. 

5.9. 
EIGENSTRUCTURES OF NONSYMMETRIC MATRICES 
253 
up with the principal axes of the second eUipse A, representing the among-group SSCP 
matrix in Panel IV. And that, basically, is what simultaneous diagonalization is all about. 
Let us see what these geometric operations mean algebraically. First, any observation 
X^ij in Table 5.3 can be represented as the sum of 
^dij - O^k-^k) "•" O^k-^k) 
Xd = J + G 
where J denotes the matrix of within-group deviations and G the matrix of among-group 
deviations. For example, the first observation on variable Xi in Table 5.3 is 
-5.25 = -1+(-4.25) 
where - 1 indicates that it is one unit less than its group mean, and —4.25 indicates that 
its group mean is 4.25 units less than the grand mean. If we can fmd a transformation of 
Xd that spherizes the J portion (the within-group variation), we could then fmd the 
eigenstructure of the adjusted cross-products matrix. 
The J portion can be readily spherized by the transformation: 
X H W -
where W"^^^, in tum, can be written as Qi A'^^^Q/. In this case A"^^^ is a diagonal matrix 
of the reciprocals of the square roots of the eigenvalues of W, and Qi is an orthogonal 
matrix of associated eigenvectors (since W is symmetric).^^ 
Note, then, that what is being done here is to fmd the "square root" of W~\ the 
inverse of the (pooled) within-group SSCP matrix. To do so we recall that if W is 
symmetric and possesses an inverse W~^ we can write 
^ - 1 / 2 ^ - 1 / 2 ^ ^ - 1 
where 
Geometrically, the multiplication of X^ by W"^^^ has the effect of normalizing the 
within-group portion of the vectors in Xd to unit length, after rotation to the principal 
axes of W by means of the direction cosines represented by Qi. Subsequent rotation by 
Qi' has no effect on what happens next, since the spherizing has already occurred. 
Next we set up the equation 
[W-^/^AW-^^']Q2 = Q2A 
where Q2 is the matrix of eigenvectors, and A the matrix of eigenvalues of 
[ W - ' ^ ^ A W ^ ^ ] . This, in tum, follows from 
(XdW-^/^y(XdW-^/^) = w-^/^Xd'XdW-^/^ = w"^ym-"^ 
+ W-^^'G'GW-^'^ 
= I + W-^^'^G'GW'^/^ = I + W^'^kW"'' 
^' The square root of a symmetric matrix was discussed in Section 5.5.2. 

254 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
where the within-group portion has been transformed to an identity matrix I, as 
desired.^^ 
We then find the eigenstructure of [W^^^AW"^^^] which, given the preUminary 
spherization, is tantamount to a rotation to principal axes orientation. A nice feature of 
this procedure is that [W^^^AW"^^^] is also symmetric. The final transformation to be 
applied to the original matrix of mean-corrected scores Xj involves 
Y = X H W -
\\iiich effects the desired simultaneous diagonahzation of W and A. Note, however, that 
W"^^^Q2 is not a rotation since the data are rescaled so that the J portion is spherized. In 
summary, then, a principal components analysis of data that are first spherized in terms 
of pooled within-group variation provides a counterpart approach to the direct attack on 
finding the eigenstructure of W"^A. 
5.9.3 
Recap itulation 
While two methods have been discussed for solving 
(A-XW)v=0 
the first method, utilizing a direct approach to computing the eigenstructure of a 
nonsymmetric matrix, is probably the better known. The second procedure appears 
useful in its own right, however, as well as serving as an alternative method to the more 
usual decomposition. 
If we return to the ratio of quadratic forms, stated earlier: 
_ v/Avi 
Ai 
; 
vi Wvi 
the problem of multiple discriminant analysis can be stated as one of finding extreme 
values of the above function where V, the matrix of discriminant weights, exhibits the 
properties: 
V'AV = A 
V'WV=I 
Thus, V diagonalizes A (since A is diagonal) and converts W to an identity matrix. Notice, 
then, that the correlation of group means on any of the linear composite(s) is zero, since 
A is diagonal. Similarly, the average within-group correlation of individuals on each 
discriminant function (linear composite) is also zero, since I is an identity. 
However, it should be remembered, as shown in Fig. 5.10, that V is not orthogonal 
since W"^A, the matrix to be diagonalized, is not symmetric. Moreover, a preliminary 
transformation such as that applied in the second method described above, still ends up 
with a V that is not orthogonal. 
" Since W-'^' is symmetric, W"^^' = (}N~"^)'. Since J'J = W, W^^' rm"^ 
= I. 

5.10. 
SUMMARY 
255 
5.10 
SUMMARY 
This chapter has primarily been concerned with various types of matrix decom-
positions—eigenstructures, singular value decompositions, and quadratic forms. The 
common motivation has been to search for special kinds of basis vector transformations 
that can be expressed in simple ways, for example, as the product of a set of matrices 
that individually permit straightforward geometric interpretations. In addition, such 
decomposition provides new perspectives on the concepts of matrix singularity and rank. 
The topic was introduced by first reviewing the nature of point and basis vector 
changes. This introduction led to a discussion of the role of eigenstructures in rendering a 
given matrix (not necessarily symmetric) diagonal via nonsingular transformations. The 
geometric aspects of eigenstructures were stressed at this point. 
We next discussed eigenstructures from a complementary view, one involving the 
development of linear composites with the property of maximizing the variance of point 
projections, subject to all composites being orthogonal with previously found composites. 
This time we discussed the eigenstructure of symmetric matrices with real-valued entries. 
In such cases all eigenvalues and eigenvectors of the matrix are real. 
Since eigenstructures are not defined for rectangular matrices and their computation 
can present problems in the case of square, nonsymmetric matrices, we discussed these 
cases next in the context of the S VD. Matrix decomposition in this case involves finding 
a triple matrix product by which any matrix can be expressed as 
A = PAQ' 
where P and Q are orthonormal by columns and A is diagonal. This form of matrix 
decomposition represents a powerful organizing concept in matrix algebra, since it can be 
applied to any matrix of full, or less than full, rank. Furthermore, it shows that any 
matrix transformation can be considered as the product of a rotation-stretch-rotation or 
rotation-reflection-stretch-rotation under a suitable change in basis vectors. 
By using product-moment matrices—A'A or AA', whichever has the smaller order—we 
were able to relate the SVD of a matrix to earlier ideas involving symmetric matrices. 
One can solve for the eigenstructure of A'A (or AA') in order to find Q' and A and then 
solve finally for P. The net result is the determination of matrix rank as well as 
the specific geometric character of the transformation. And, if A is symmetric to begin 
with, the general procedure leads to the special case of Q'AQ = D = A^ . 
Related ideas were presented in our discussion of quadratic forms, a function that 
maps n-dimensional vectors into one dimension. Again, the motivation is to find a new set 
of basis vectors, via rotation, in which the function assumes a particularly simple form, 
namely, one in which cross-product terms vanish. 
The last main section of the chapter dealt with ways of finding the eigenstructure of 
nonsymmetric matrices as they may arise in the simultaneous diagonalization of two 
different quadratic forms. The geometric character of this type of transformation was 
described and illustrated graphically. 
The material of this chapter represents a major part of the more basic mathematical 
aspects of multivariate procedures. Typically, in multivariate analysis we are trying to 
find linear combinations of the original variables that optimize some quantity of interest 

256 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
to the researcher. As Appendix A shows, function optimization subject to certain 
constraints, such as Lagrange muUipUers, is used time and time again in many of the 
statistical techniques that appear in mukivariate data analysis. 
REVIEW QUESTIONS 
1. 
Form the characteristic equations of the following matrices and determine their 
eigenvalues and eigenvectors: 
d. 
A = 
5 
7 
2 
6 
0 
8J 
- 1 
7 
b. 
e. 
A = 
-1 
- 2 
2 
2 - 4 
-2 
4 
2 
0 
2 - 2 
A = 
A = 
5 
- 2 
3 
0 
0 
3 
-9 
9 
2. 
Calculate the trace and determinant of each of the first four of the matrices above 
and verify that 
n 
n 
tr(A)= Z X,-; 
|A|= 0 X,-
/ = i 
/=! 
3. 
Find the invariant vectors (i.e., eigenvectors) under the following transformations: 
a. 
c. 
A shear 
' ^ 1 * " 
_ ^ 2 * _ 
-1 
0-1 
-3 .J 
[xi 
[X2^ 
A central dilation 
J C i * 
JC2*_ 
'3 o1 
_o 3J 
[xi 
L-^2j 
b. 
d. 
A stre 
X i * 
_ ^ 2 ^ 
tch 
^3 OI 
_0 2J 
1^1 
L^2_ 
A rotation 
Xi* 
X2* 
0.707 
0.707] 
-0.70' 7 
0 .707J 
fxi 
L^2_ 
4. 
Starting with the matrix 
"5 
f 
1 
3 
find the eigenstructures of the following matrices: 
a. 
3A 
b. 
A+ 21 
c. 
A - 3 I 
d. 
A^ 
e. 
f. 
A' 

REVIEW QUESTIONS 
257 
5. 
Given the set of linearly independent vectors 
ai = 
32 
and the matrix 
2 
-1 
0 
2 
3 
2 
a3 = 
show that B can be made diagonal via the 3 x 3 matrix A (made up from the linearly 
independent vectors) and its inverse. 
6. 
Find an orthogonal matrix U such that U~^AU is diagonal, where 
A = 
- 2 
10 
- 2 
7. 
Using the minor product-moment 
procedure, and subsequent 
calculation of 
eigenstructure, what is the rank of the following matrices: 
1 
2 
3 
2 
1 
4 
2 
1 
1 
A = 
0 
1 
0 
0 
2 
0 
0 
1 
3 
1 
0 
- 1 
d. 
' l 
2 
3 
4 
5 
^1 
2 
0 
0 
3_ 
A = 
"l 
2 
3 
5 
2 " 
4 
6 
10 
Find the SVD and rank of each of the following matrices: 
a. 
^ 
-, 
b. 
0 
1 
1 
0 
0.707 
0.707 
-0.707 
0.707 
A = 
d. 

258 
5. 
EIGENSTRUCTURES AND QUADRATIC FORMS 
9. 
In the first two examples of Question 8, what is the SVD of 
a. 
[ A ' A ] ^ 
b. 
[A'A]^^' 
10. 
Compute A^^'^ and K'^'^ , by finding eigenstructures for the matrices: 
a. 
,_ 
_, 
b. 
^ 
^ 
c. 
4 
3 
3 
4 
A = 
7 
1 
1 
7 
A = 
8 
2 
2 
5 
d. 
A = 
11. 
Represent each of the following quadratic forms by a real symmetric matrix and 
determine its rank 
a. 
Xi + 2xiJC2 +^2^ 
c. 
9JCI^ —6JC1X2 +^2^ 
b. 
jci^ + 2JC2^ —4JCIX2 
d. 
2^1^ —3xiJC2 + 3^2^ 
12. 
Diagonalize the matrix of each quadratic form in Question 11 and describe its 
geometric character. 
13. 
In the sample problem, whose mean-corrected data appear in Table 1.2: 
a. 
Find the covariance matrix of the full set of three variables. 
b. 
Compute the principal components of the three-variable covariance matrix 
and the matrix of component scores. 
c. 
Compare these results with those found in the present chapter. 
14. 
Simplify the following quadratic forms and indicate the type of definiteness of each 
form: 
J 9 -3 
L-3 
l_ 
X 
,[ 2 
-1.5" 
L-1.5 
: 3 
- 4 
2 
- 1 
4 
2 
- 2 
2 
- 4 
2 
2 
4 
2 
15. 
Returning to the multiple discriminant function problem considered in Section 5.9: 
a. 
Spherize the (pooled) within-group SSCP matrix and compute the eigenstruc-
ture in accordance with the procedure outlined in Section 5.9.2. 
b. 
Compare 
these results with those found 
from 
the procedure used in 
Section 5.9.1. 

CHAPTER 6 
Applying the Tools to Multivariate Data 
6.1 
INTRODUCTION 
In this chapter we come around full circle to the three substantive problems first 
introduced in Chapter 1. As recalled, each problem was based on a "toy" data bank and 
formulated in terms of three commonly used techniques in multivariate analysis: (a) 
multiple regression, (b) principal components analysis, and (c) multiple discriminant 
analysis. 
We discuss the multiple regression problem first. The problem is structured so as to 
require the solution of a set of linear equations, called "normal" equations from 
least-squares theory. These equations are first set up in terms of the original data, and the 
parameters are found by matrix inversion. We then show how the same problem can be 
formulated in terms of either a covariance or a correlation matrix. 
R^, a measure of overall goodness of fit, and other regression statistics such as partial 
correlation coefficients, are also described. The results are interpreted in terms of the 
substantive problem of interest, and comments are made on the geometric aspects of 
multiple regression. 
We then discuss variations on the general linear model of multiple regression: analysis 
of variance and covariance, two-group discriminant analysis, and binary-valued regression 
(in which all variables, criterion and predictors, are expressed as zero-one dummies). This 
discussion is presented as another way of showing the essential unity among single-
criterion, multiple-predictor models. 
Discussion then turns to the second substantive problem, formulated as a principal 
components model. Here the solution is seen to entail finding the eigenstructure of a 
covariance matrix. Component loadings and component scores are also defined and 
computed in terms of the sample problem. 
After solving this sample problem, some general comments are made about other 
aspects of factor analysis, such as the factoring of other kinds of cross-product matrices, 
rotation of component solutions, and dimension reduction methods other than the 
principal components procedure. 
The three-group multiple discriminant problem of Chapter 1 is taken up next. This 
problem is formulated in terms of finding the eigenstructure of a nonsymmetric matrix 
which, in turn, represents the product of two symmetric matrices. The discriminant 
259 

260 
6- 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
functions are computed, and significance tests are conducted. The results are interpreted 
in the context of the third sample problem. 
We then turn to other aspects of multiple discriminant analysis (MDA), including 
classification matrices, alternative ways to scale the discriminant functions, and the 
relationship of MDA to principal components analysis. Finally, some summary-type 
comments are made about other techniques for dealing with multiple-criterion, 
multiple-predictor association. 
The last major section of the chapter is, in some respects, a prologue to textbooks that 
deal with multivariate analysis per se. In particular, the concepts of transformational 
geometry, as introduced in earlier chapters, are now brought together as another type of 
descriptor by which multivariate techniques can be classified. Under this view 
multivariate methods are treated as procedures for matching one set of numbers with 
some other set or sets of numbers. Techniques can be distinguished by the nature of the 
transformation(s) used to effect the matching and the characteristics of the transformed 
numbers. 
This organizing principle is described in some detail and suggests a framework that 
can be useful for later study of multivariate procedures as well as suggestive of new 
models in this field. 
6.2 
THE MULTIPLE REGRESSION PROBLEM 
We are now ready to work through the details of the sample problem in Chapter 1 
dealing with the relationship of employee absenteeism Y, to attitude toward the firm Xi 
and number of years employed by the firm X2. To simplify our discussion, the basic data, 
first shown in Table 1.2, are reproduced in Table 6.1. 
As recalled from the discussion in Chapter 1, here we are interested in 
1. finding a regression equation for estimating values of the criterion variable Y from 
a linear function of the predictor variables Xi and X2 ; 
2. 
determining the strength of the overall relationship; 
3. 
testing the significance of the overall relationship; 
4. 
determining the relative importance of the two predictors Xi and X2 in 
accounting for variation in Y. 
6.2.1 
The Estimating Equation 
As again recalled from Chapter 1, the multiple regression equation 
Yi = bo + baii+b.Xt2 
is a linear equation for predicting values of Y that minimize the sum of the squared errors 
12 
12 
le^=l(Y,-Y,r 
i=l 
i=l 

6.2. 
THE MULTIPLE REGRESSION PROBLEM 
261 
TABLE 6.1 
Basic Data of Sample Problem (from Table 1.2) 
Employee 
a 
b 
c 
d 
e 
f 
g 
h 
i 
J 
k 
1 
Mean 
Standard 
deviation 
Number of days absent 
Y 
1 
0 
1 
4 
3 
2 
5 
6 
9 
13 
15 
16 
6.25 
5.43 
Yd 
-5.25 
6.25 
-5.25 
-2.25 
-3.25 
-4.25 
-1.25 
-0.25 
2.75 
6.75 
8.75 
9.75 
ys 
-0.97 
-1.15 
-0.97 
-0.41 
-0.60 
-0.78 
-0.23 
-0.05 
0.51 
1.24 
1.61 
1.80 
^ 1 
1 
2 
2 
3 
5 
5 
6 
7 
10 
11 
11 
12 
6.25 
3.77 
Attitude rating 
^ d l 
-5.25 
-4.25 
-4.25 
-3.25 
-1.25 
-1.25 
-0.25 
0.75 
3.75 
4.75 
4.75 
5.75 
-^sl 
-1.39 
-1.13 
-1.13 
-0.86 
-0-33 
-0.33 
-0.07 
0.20 
0.99 
1.26 
1.26 
L53 
Years with company 
X2 
1 
1 
2 
2 
4 
6 
5 
4 
8 
7 
9 
10 
4.92 
2.98 
^ d 2 
-3.92 
-3.92 
-2.92 
-2.92 
-0.92 
1.08 
0.08 
-0.92 
3.08 
2.08 
4.08 
5.08 
^s2 
-1.31 
-1.31 
-0.98 
-0.98 
-0.31 
0.36 
0.03 
-0.31 
1.03 
0.70 
1.37 
1.71 
Appendix A shows how the set of normal equations, used to find bo, bi, and Z?2, are 
derived. In terms of the specific problem here, we have in matrix notation:^ 
C 
J(.i JC2 
1 
1 
12 
10 
bo 
bi 
b2 
e2 
en 
The model being fitted by least squares is 
y = Xb + e 
Notice that the model, in matrix form, starts off with the observed vector y and the 
observed matrix X. As will be shown later, the device of including a column of ones as the 
first column of X (called C) is employed for estimating the intercept bo. 
We wish to solve for b, the vector of parameters, so that X!L 1^1^= e'e is minimized. As 
can be checked in Appendix A, the problem is a standard one in the calculus and leads to 
the so-called normal equations which, expressed in matrix form, are 
b = (X'X)-iXy 
^ The sample entries in y and X are taken from Table 6.1. 

262 
APPLYING THE TOOLS TO MULTIVARIATE 
DATA 
That is, we first need to find the minor product moment of X, which is X'X. Next, we 
find the inverse of X'X and postmultiply this inverse by X'y. 
In terms of the specific problem of Table 6.1, we have 
X' 
X 
X' 
^0 
b2 
1 
12 
10 
l] 
12 
loj 
[l 
0 
1 
[l6j 
b = 
1 
12 
10 
-2.263 I 
1.550 
-0.239 
Hence, in terms of the original data of Table 6.1, we have the estimating equation 
Yi = -2.263 + 1.550^n - 0.239X/2 
The 12 values of ?^ appear in the lower portion of Table 6.2, along with the residuals e^. 
If one adds up the squared residuals, one obtains (within rounding error) the residual 
term shown in the analysis of variance table of Table 6.2: 
residual = 34.099 
The total sum of squares is obtained from 
12 
Z (Y-Yf 
= 354.25 
/ = i 
and the difference 
due to regression = 320.15 
6.2.2 
Strength of Overall Relationship and Statistical Significance 
The squared multiple correlation coefficient is R^, and this measures the portion of 
variance in Y (as measured about its mean) that is accounted for by variation in Xi and 
X2. As mentioned in Chapter 1, the formula is 
R^ 
m . ei^ 
V;t,iYi-rf 
/?2 = 1 _ 34.099 
354.25 = 0.904 

6.2. 
THE MULTIPLE REGRESSION PROBLEM 
263 
TABLE 
6.2 
Selected Output from Multiple Regression 
R^ = 0.904; 
R = 0.951; 
variance of estimate 
3.789 
Analysis of Variance for Multiple Regression 
Source 
Due to regression 
Residual 
Variable 
Total 
Regression 
coefficients 
df 
2 
9 
11 
Sums of 
squares 
320.151 
34.099 
354.250 
Standard 
errors 
r values 
Mean 
squares 
160.075 
3.789 
Partial 
correlations 
F ratio 
42.25 
Proportion 
of cumulative 
variance 
X, 
X, 
Employee 
a 
b 
c 
d 
e 
f 
Y 
1 
0 
1 
4 
3 
2 
1.550 
0.239 
Y 
-0.95 
0.60 
0.36 
1.91 
4.53 
4.05 
0.481 
0.606 
Y intercept 
3.225 
-0.393 
-2.263 
Table of Residuals 
e 
1.95 
-0.60 
0.64 
2.09 
-1.53 
-2.05 
Employee 
g 
h 
i 
J 
k 
1 
0.732 
-0.130 
Y 
5 
6 
9 
13 
15 
16 
Y 
5.85 
7.63 
11.33 
13.11 
12.64 
13.95 
0.902 
0.002 
e 
-0.84 
-1.63 
-2.33 
-0.11 
2.36 
2.05 
The statistical significance of i^, the positive square root of R^, is tested via the analysis 
of variance subtable of Table 6.2 by means of the F ratio: 
F= 42.25 
which, with 2 and 9 degrees of freedom, is highly significant at the o: = 0.01 level. Thus, 
as described in Chapter 1, the equivalent null hypotheses 
are rejected at the 0.01 level, and we conclude that the multiple correlation is significant. 
Up to this point, then, we have established the estimating equation and measured, via 
R^, the strength of the overall relationship between Y versus Xi and X2. 
If we look at the equation again 
Yi = -2.263 + 1 .SSOXa - 0.239X/2 

264 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
we see that the intercept is negative. In terms of the current problem, a negative 2.263 
days of absenteeism is impossible, illustrating, of course, the possible meaninglessness of 
extrapolation beyond the range of the predictor variables used in developing the 
parameter values. 
The partial regression coefficient for Xi seems reasonable; it says that predicted 
absenteeism increases 1.55 days per unit increase in attitude rating. This is in accord with 
the scatter plot (Fig. 1.2) that shows the association of Y with Xi alone. 
The partial regression coefficient for X2, while small in absolute value, is negative, 
even though the scatter plot of Y on X2 alone (Fig. 1.2) shows a positive relationship. 
The key to this seeming contradiction lies in the strong positive relationship between the 
predictors Xi and X2 (also noted in the scatter plot of Fig. 1.2). Indeed, the correlation 
between Xi and X2 is 0.95. The upshot of all of this is that once Xi is in the equation, 
X2 is so redundant with Xi that its inclusion leads to a negative partial regression 
coefficient that effectively is zero (given its large standard error). 
6.2.3 
Other Statistics 
The redundancy of X2, once Xi is in the equation, is brought out in Table 6.2 under 
the column 
Proportion of 
cumulative variance 
X^ 
0.902 
X^ 
0.002 
That is, of the total R^ =0.904, the contribution of Xi alone represents 0.902. The 
increment due to X2 (0.002) is virtually zero, again reflecting its high redundancy with 
This same type of finding is reinforced by examining the t values and the partial 
correlations in Table 6.2. These are 
X, 
x^ 
t Values 
3.225 
-0.393 
Partial 
correlations 
0.732 
-0.130 
The Student t value is the ratio of a predictor variable's partial regression coefficient to its 
standard error. The standard error, in tum, is a measure of how well the predictor variable 
itself can be predicted from a linear combination of the other predictors. The higher the 
standard error, the more redundant (better predicted) that predictor variable is with the 
others. Hence, the less contribution it makes to Y on its own and the lower its t value. 
We see that the ratio of ^1 to its own standard error is 
, . 
1.550 
< ^ ) = 0 : ^ = 3.225 
which is significant at the 0.01 level. The t value for X2 of -0.393 is not significant, 
however. Without delving into formulas, the t test is a test of the separate significance of 
each predictor variable Xy, when included in a regression model, versus the same 

6.2. 
THE MULTIPLE REGRESSION PROBLEM 
265 
regression model with all predictors included except it. We note here that only Xi is 
needed in the equation. 
The partial correlations also suggest the importance of Xi rather than X2 in 
accounting for variation in Y. The partial correlation of Y with some predictor Xj is their 
simple correlation when both variables are expressed on a residual basis, that is, net of the 
linear association of each with all of the other predictors. In the present problem, the 
partial correlation of Y with Xi is considerably higher than Y with X2, supporting the 
earlier conclusions. 
But what if X2 is entered first in the regression? What happens in this case to the 
various statistics reported in Table 6.2? As it turns out, the only statistic that changes if 
X2 is credited with as much variance as it can account for before Xi is allowed to 
contribute to criterion variance is the last column, proportion of cumulative variance. If 
X2 is entered first, it is credited with 0.79, while Xi is credited with only 0.11 of the 
0.90 total. The rest of the output does not change, and X2 is still eliminated from the 
regression on the basis of the t test results. 
What this example points out is that in the usual case of correlated predictors, the 
question of "relative importance" of predictors is ambiguous. Many researchers interpret 
relative importance in terms of the change inR^ occurring when the predictor in question 
is the last to enter. Other importance measures are also available, as pointed out by 
Darlington (1968). However, in the case of correlated predictors, no measure is entirely 
satisfactory. 
6.2.4 
Other Fonnulations of the Problem 
In the sample problem of Table 6.1, the normal equations were formulated in terms of 
the original data. Alternatively, suppose we decided to work with the mean-corrected 
scores Y^, X^y X^2' ^^ this case we would compute the covariance matrix 
and the vector of partial regression parameters would be found from 
b = C-'a(j;) 
where a(y) is the vector of covariances between the criterion and each predictor in turn, 
with elements 
ai =yd'xdi/m 
«2 = yd'xd2/m 
in the sample problem. 
The preceding formula for computing b would find only bi and Z?2 since all data 
would be previously mean centered. To work back to original data, we can find the 
intercept of the equation by the simple formula: 
bo - Y — biXi — Z?2^2 

266 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
If we decided to work with the standardized data Y^, X^i, and X^2^ the appropriate minor 
product moment is the correlation matrix 
R = X,X/m 
and the vector of parameters b* (often called beta weights) would be found from 
b* = R-y>^) 
where r(y) is the vector of product-moment correlations between the criterion and each 
predictor in tum, with elements 
r2 = Ys^si/m 
in the sample problem. 
The vector b* measures the change in F per unit change in each of the predictors, 
when all variables are expressed in standardized units. To find the elements of b, we use 
the conversion equations 
^2 
= ^: * 'y 
^X2 
These simple transformations, involving ratios of standard deviations, enable us to express 
changes in Y per unit change in Xi and X2 in terms of the original Y units. Having done 
this, we can then solve for the intercept term in exactly the same way: 
bo = Y- 
biXi 
- Z72^2 
as shown in the covariance matrix case. Many computer routines for performing multiple 
regression operate on the correlation matrix. As seen here, any of the cross-product 
matrices—raw cross products, covariances, or correlations—can be used and, in the latter 
two cases, modified for expressing regression results in terms of original data. 
6.2.5 
Geometric Aspects—the Response Surface Model 
Figure 1.2 showed two-dimensional scatter plots of Y versus Xi, Y versus X2, and X2 
versus Xi It is also a relatively simple matter to plot a three-dimensional diagram of Y 
versus Xi and X2. This is shown in Fig. 6.1. 
We also show the fitted regression plane, as computed by least squares. This type of 
model, in which observations are represented by points and variables by dimensions, is 
often called the response surface or point model. 

6.2. 
THE MULTIPLE REGRESSION 
PROBLEM 
267 
= -2.263+ 1.55X1-0.239X2 
Fig. 6.1 
Three-dimensional plot and fitted regression plane. 
The intersection of the regression plane in Fig. 6.1 with the Y axis provides the 
estimate bo, the intercept term. If we next imagine constructing a plane perpendicular to 
the Xi axis, we, in effect, hold Xi constant; hence Z?2 represents the estimated 
contribution of a unit change in X2 to a change in Y. Similar remarks pertain to the 
interpretation of Z?i. 
The regression plane itself is oriented so as to minimize the sum of squared deviations 
between each Yf and its counterpart value on the fitted plane, where these deviations are 
taken along directions parallel to the Y axis. Similarly, we can find the sum of squared 
deviations about the mean of the Y/s by imagining a plane perpendicular to the Y axis 
passing through the value Y. Total variation in 7 is thus partitioned into two parts. As 
indicated earlier, these separate parts are found by 
1. subtracting unaccounted-for variation, involving squared deviations (Y^ - 
f^f 
about the fitted regression plane, from 
2. 
total variation involving squared deviations {Yj — Y)^ from the plane imagined to 
be passing through F. 
The quantity X/3j(yi- YfY represents the unaccounted-for sum of squares, and the 
quantity [ Z / = i ( ^ ' - ^)^ - Z / = i ( ^ - ^)^] represents the accounted-for sum of 

268 
6. 
APPLYING THE TOOLS TO MULTIVARLVTE DATA 
squares. If no variation is accounted for, then we note that using Y is just as good at 
predicting Y as introducing the variation in Xi and X2« 
6.2.6 
An Alternative Representation 
The foregoing representation of the 12 responses in variable space considers the 12 
observations as points in three dimensions, where each variable, 7, Xi, or X2, denotes a 
dimension. Alternatively, we can imagine that each of the 12 employees represents a 
dimension, and each of the variables constitutes a vector in this 12-dimensional person 
space. As we know from the discussion of matrix rank in Chapter 5, the three vectors will 
not span the whole 12-dimensional space but, rather, will lie in (at most) a 
three-dimensional subspace that is embedded in the 12-dimensional person space. 
We also remember that if the vectors are translated to a mean-centered origin and are 
assumed to be of unit length, the (product-moment) correlation between each pair of 
vectors is given by the cosine of their angle. In this case we have three two-variable 
correlations: ry^^, ryx^,and f'x^x^-
This concept is pictured, in general terms, in Fig. 6.2. In the left panel of the figure 
are two unit length vectors Xi and X2 emanating from the origin. Each is a 12-component 
vector of unit length, embedded in the "person" space. The cosine of the angle ^ 
separating Xi and X2 is the simple correlation rx x • 
Since the criterion vector y is not perfectly correlated with Xi and X2 , it must extend 
into a third dimension. The cosines of its angular separation between Xi and X2 are each 
measured, respectively, by its simple correlations ryx and ryx . However, one can 
project y onto the plane formed by Xi and X2. The projection of y onto this plane is 
denoted by y. 
In terms of this viewpoint, the idea behind multiple regression is to find the particular 
vector in the Xi, X2 plane that minimizes the angle 9 with y. This vector will be the 
projection y onto the plane formed by Xi, X2. Since any vector in the Xi, X2 plane is a 
linear combination of Xi and X2, it follows that we want the vector y = bi*Xi + Z?2*X2, 
where the bj*^s are beta weights, that minimizes the angle, or maximizes the cosine of the 
angle with y. The cosine of this angle 6 (see Fig. 6.2) is R, the multiple correlation. The 
problem then is to fmd a set of Z?y*'s that define a linear combination of the vectors Xi 
Fig. 6.2 
Geometric relationship of y to y in vector space. The graph on the right shows geometric 
interpretation of partial regression weights in vector space. 

6.2. THE MULTIPLE REGRESSION PROBLEM 
269 
and X2 maximizing the cosine R of d, the angle separating y and y. However, this is 
equivalent to minimizing the square of the distance from the terminus of y to its 
projection y. This criterion 
minimize 
12 
J=i 
(ysi-
n 
-1 
7 = 1 
-] 
^/ 
^sij) 
again leads to the least-squares equations. (Since all variables are assumed to be measured 
in standardized form, the intercept Z?o is zero.) 
In general, the Xi, X2 axes will be obHque, as noted in Fig. 6.2. The right panel shows 
that a linear combination of Xi and X2, which results in the predicted vector y, involves 
combining oblique axes via Z?i*and Z?2* In this case, Z?i*and Z?2*are direction cosines. 
Figure 6.3 shows some conditions of interest. In Panel I we see that y is uncorrected 
with Xi and X2. This lack of correlation is indicated by the 90° angle between y and the 
Xi, X2 plane. Panel II shows the opposite situation where y is perfectly correlated with Xi 
and X2 and, hence, can be predicted without error by a Unear combination of Xi and X2. 
Panel III shows the case where Xi and X2 are uncorrected and y evinces some 
correlation with Xi and none with X2. Panel IV shows the case in which Xi and X2 are 
uncorrected, but the projection of y Hes entirely along X2. 
• - X 2 
Fig. 6.3 Some illustrative cases involving multiple correlation. Key: I, no correlation; II, perfect 
correlation; III, y correlated with x, only; IV, y correlated with Xj only. 

270 
6. APPLYING THE TOOLS TO MULTIVARL\TE DATA 
In summary, the multiple correlation coefficient R is the cosine of the angle 6 made 
by y and y. The bj*'s are normalized beta weights and represent coordinates of y in the 
oblique space of the predictor variables. If more than two predictors are involved, the 
same geometric reasoning applies, although in this case the predictors involve higher-
dimensional hyperplanes. 
Partial correlations between y and Xi and X2, respectively, can also be interpreted. For 
example, if we consider a plane perpendicular to X2 and project y and Xi onto this plane, 
^yxi-x^, the partial correlation of y with Xi (with X2 partialed out) is represented by the 
cosine of the angle separating them on this plane. Similar remarks pertain to the partial 
correlation of y with X2 and would involve a projection onto a space that is orthogonal to 
Xi. The same general idea holds for larger numbers of predictors. 
6.3 
OTHER FORMS OF THE GENERAL LINEAR MODEL 
The typical multiple regression model considers each variable as intervally scaled. This 
representation is overly restrictive. Indeed, by employing the dummy-variable device, as 
introduced in Chapter 1, we can extend the linear regression model to a more general 
model that subsumes the techniques of 
1. analysis of variance 
2. 
analysis of covariance 
3. 
two-group discriminant analysis 
4. 
binary-valued regression 
All of these cases are developed from two basic concepts: (a) the least-squares criterion 
for matching one set of data with some transformation of another set and (b) the dummy 
variable. 
Figure 6.4 shows, in a somewhat abstract sense, various special cases in terms of the 
response surface or point model involving m observations in three dimensions. 
Panel I of Fig. 6.4 shows each of the three columns of a data matrix as a dimension 
and each row of the matrix as a point. If we were then to append to the m x 2 matrix of 
predictors a unit vector, we have the famiUar matrix expression for fitting a plane or, 
more generally, a response surface, in the three-dimensional space shown in Panel I. 
Predicted values y of the criterion variable y are given by 
y = Xb 
where b is a 3 x 1 column vector with entries bo, Z?i, Z?2 denoting, respectively, the 
intercept, partial regression coefficient for Xi, and partial regression coefficient for X2. 
However—and this is the key point—nothing in the least-squares procedure precludes y 
(or Xi or X2 for that matter) from taking on values that are just zero or one. Panel II 
shows the case where y assumes only binary values, but Xi and X2 are allowed to be 
continuous. Panel III shows the opposite situation. Panel IV shows a "mixed" case where 
y and Xi are continuous and X2 is binary valued. Panel V shows a case where all three 
variables are binary valued. 

6.3. 
OTHER FORMS OF THE GENERAL LINEAR MODEL 
271 
III 
IV 
1,0,0 
Fig. 6.4 
Variations of the response surface model. 
Not surprisingly, from Chapter 1 we recognize Panel I as a traditional multiple 
regression formulation. Panel II appears as a two-group discriminant function. Panel III 
appears as a one-way analysis of variance design with one treatment variable at three 
levels. Panel IV represents a simple analysis of covariance design with a single two-level 
treatment variable (X2) and one continuous covariate (xi). Panel V seems less familiar, 
but could be viewed as a type of binary-valued regression where the criterion and the 
predictors are each dichotomies (e.g., predicting high versus low attitude toward the firm 
as a function of sex and marital status). 
As observed from Fig. 6.4, we can now conclude that all of these models are variations 
on a common theme—namely, one in which we are attempting to find some type of linear 
transformation that results in a set of scores that best match, in the sense of minimum 
sum of squared deviations, a set of criterion scores. In each case we are fitting a plane in 
the three-dimensional space of Xi, X2, and y and then finding estimates y of y that result 
in a minimum sum of squared deviations. 
All of the cases depicted in Fig. 6.4 are characterized by the fact that a single-criterion 
variable, either 0-1 coded or intervally scaled, is involved. Our interest is in finding some 
linear combination of predictors, where b denotes the set of combining weights, that 
leads to a set of predicted scores y that are most congruent with the original scores y. 
Extension of the multiple regression model to handle binary-valued predictors is 
described in various texts (e.g., Neter and Wasserman, 1974) in terms of a general linear 
model. 
If a further extension is made in order to allow for a binary-valued criterion, least 
squares can still be used to estimate parameter values, although the usual statistical tests 

272 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
are not strictly appropriate since the normality and constant variance assumptions are 
missing. Still, as a descriptive device least squares can be used to find estimating equations 
for all of the cases depicted in Fig. 6.4. 
Discussion of the multiple regression problem has thus resulted in a much wider scope 
of application than might first have been imagined. Through the dummy-variable coding 
device, one can subsume all cases of interest—analysis of variance and covariance, 
two-group discrimination, binary-valued regression—that involve a single-criterion variable 
and multiple predictors. Moreover, although detailed discussion of the geometric aspects 
of the models was more or less confined to multiple regression, all of these methods can 
be represented by either 
1. the response surface or point model in variable space, or 
2. 
the vector model in person or object space. 
From the standpoint of matrix algebra, all of the preceding models entail solutions 
based on a set of linear (the normal) equations from least-squares theory. As such, the 
operation of matrix inversion becomes germane, as does the concept of matrix rank and 
related ideas such as determinants. In brief, the algebraic underpinnings of single-
criterion, multiple-predictor association are concepts of matrix rank and inversion. Thus, 
it is no accident that much of the discussion in earlier chapters was devoted to these 
topics. 
6.4 THE FACTOR ANALYSIS PROBLEM 
If matrix inversion and rank are the hallmarks of single-criterion, multiple-predictor 
association, then eigenstructures are the key concepts in dimension-reducing methods like 
factor analysis. Eigenstructures are also essential in multiple-criterion, multiple-predictor 
association, as we shall see later in the chapter. 
In Chapter 1 we introduced a small-scale problem in principal components analysis in 
the context of developing a reduced space for the two predictors: (a) Xi, attitude score 
and (b) X2, number of years with company. Using the X^i and X^2 ^^ta of Table 6.1 we 
wish to know if a change of basis vectors can be made that will produce an axis whose 
variance of point projections is maximal. 
This is a standard problem in finding the eigenstructure of a symmetric matrix. Here 
we employ the covariance matrix, although in some cases one might want to use some 
other type of cross-products matrix. Table 6.3 details the steps involved in finding the 
eigenstructure of C, the simple 2 x 2 covariance matrix of the sample problem of 
predictor variables in Table 6.1. (Supporting calculations appear in Chapter 5.) 
As observed from Table 6.3, the first eigenvalue Xi= 22.56 accounts for nearly all, 
actually 98 percent, of the variance of C, the covariance matrix. The linear composite Zi, 
developed from ti, makes an angle of approximately 38° with the horizontal axis, as 
noted in Fig. 6.5. Thus, if we wished to combine the vectors of scores X^n and X^i2 into 
a single linear composite, we would have, in scalar notation. 
zi(i) = 0.787Xd/i +0.617Xd,-2 
Note also that the second linear composite Z2 is at a right angle to Zi. 

6.4. 
THE FACTOR ANALYSIS PROBLEM 
273 
TABLE 
6.3 
Finding the Eigenstructure of the Covariance Matrix 
(Predictor Variables in Table 6.1) 
Covariance matrix 
Xi 
X^ 
c = x. 14.19 
10.69 
XiLlO.69 
8.91 
Matrix equation 
( C - \ , I ) t / = 0 
Characteristic equation 
14.19-^/ 
10.69 
10.69 
8.91-X,-
ic-\/ih 
= 0 
Expansion of determinant 
\/^-23.1\/+126.433-114.276 = 0 
Eigenvalues 
\, = 22.56; 
Eigenvectors 
^0.54 
t, = 
0.787 
Lo.617 ; 
u = 
0.617 
L-0.787J 
6.4.1 
Component Scores 
Component scores are the projections of the twelve points on each new axis, Zi and 
Z2, in turn. For example, the component score of the first point on Zi is 
zi(i) = 0.787(-5.25) + 0.617(-3.92) = -6.55 
as shown in Fig. 6.5. The full set of component scores appears in Table 5.1. 
The variance of each column of Z will equal its respective eigenvalue. If one wishes to 
find a matrix of component scores with unit variance, this is done quite simply by a 
transformation involving the matrix of eigenvectors T and the reciprocals of the square 
roots of the eigenvalues: ^ 
Z, = XdTA -1/2 
In the sample problem, the product of T and A ^^'^ is given by 
T 
A-'^^ 
0.787 
0.617 
0.617 
-0.787 
0.211 
0 
0 
1.361 
S = 
0.166 
0.130 
0.840 
-1.071 
In the sample problem, Z^ denotes the 12 x 2 matrix of unit-variance component scores; 
Xd is the 12x2 matrix of mean-centered predictor variables; T is the matrix of 
^ In this illustration we use A to denote the diagonal matrix of eigenvalues of C, the covariance 
matrix. Accordingly, A"*^^ is a diagonal matrix whose main diagonal elements are the reciprocals of 
the square roots of the main diagonal elements of A. 

274 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
{-6.55)5, 
Fig. 6.5 
Principal components rotation of mean-corrected predictor variables. 
eigenvectors from Table 6.3; and K'^'^ is a diagonal matrix of the reciprocals of the 
square roots of the eigenvalues. By appHcation of the transformation matrix S (instead of 
T), we would obtain unit-variance component scores. That is, in this case, 
Z'Zjm 
= l 
Geometrically, then, postmultiplication of Xj by S has the effect of transforming the 
ellipsoidal-like swarm of points in Fig. 6.5 into a circle, along the axes of the ellipse. 
6.4.2 
Component Loadings 
Component loadings are simply product-moment correlations of each original variable 
X^i and X^2 with each set of component scores. 
To illustrate, the (unit-variance) component scores Zs/(i)On the first principal 
component are 
a 
-1.38 
b 
-1.21 
c -1.08 
d -0.92 
e -0.33 
f 
-0.07 
g 
-0.03 
0.01 
1.02 
J 
1.06 
1.32 
1 
1.62 
These represent the first column of Z^. For example, 
Zs,(i) = 0A66X^n + 0.130;i:d/2 = 0.166(-5.25) + 0.130(-3.92) = -1.38 
The product-moment correlation of Zs(i) with x^i is 0.99, and the product-moment 
correlation of Zs(i) with x^2 is 0.98. Not surprisingly, given the high variance accounted 
for by the first component, both loadings are high. 
A more general definition of a component loading considers a loading as a weight, 
obtained for each variable, whose square measures the contribution that the variable 
makes to variation in the component. However, usually in appHed work it is the 

6.4. THE FACTOR ANALYSIS PROBLEM 
275 
correlation matrix that is factored rather than the covariance, or some other type of cross 
products, matrix. Hence, the simpler definition of loading, namely, as the correlation of a 
variable with a component, is most prevalent. 
In the present problem, the principal components analysis of a 2 x 2 correlation 
matrix would necessarily effect a 45° rotation, rather than the 38° rotation shown in 
Fig. 6.5. Hence the loadings of Xi and X2 on each component would necessarily be 
equal. However, this will not, in general, be the case with correlations based on three or 
more variables being analyzed by principal components. 
The matrix of component "loadings" for the covariance matrix in the present problem 
is found quite simply from the relationship 
F = TA' 
0.787 
0.617 
0.617 
-0.787 
X2 
^1/2 
4.75 
0 
0 
0.73 
Xx 
X2 
3.74 
0.45 14.19 
2.93 O.57J 8.91 
\i 22.56 0.54 23.10 
An interesting property of F is that the sum of the squared "loadings" of each 
component (column) equals its respective eigenvalue. For example, 
Xi = (3.74)^ + (2.93)2 = 22.56 
the variance of the first component, within rounding error. 
Similarly, the sum of the squared entries of each variable (row) equals its respective 
variance. For example, 
(3.74)2 + (0.45)2 = j4 19 
the variance of Xi. 
Finally, we see that both components together exhaust the total variance in the 
covariance 
matrix 
C. 
Furthermore, the 
first 
component 
itself accounts 
for 
22.56/23.10 = 0.98 of the total variance.^ Clearly, little is gained by inclusion of the 
second component insofar as the sample problem is concerned. 
6.4.3 
The Basic Structure of Xj 
Another way of looking at the principal components problem is in terms of the basic 
structure of a matrix, as described in Chapter 5. In line with our earlier discussion, 
suppose we wished to find the basic structure of 
Xd/>/m = UAT' 
Of additional interest is the fact that X^ accounts for 14.19/23.10 or 0.61 of the total variance. 

276 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
where, as we know, the minor product of the left-hand side of the equation represents the 
covariance matrix 
C=Xd'Xd/m 
As shown in Chapter 5, C is symmetric and, hence, orthogonally decomposable into the 
triple product 
C = TAT' 
0.787 
0.617 
0.617 
-0.787 
A 
22.56 
0 
0 
0.54 
T' 
0.787 
0.617 
0.617 
-0.787 
14.19 
10.69 
10.69 
8.91 
where T is orthogonal, and A is diagonal. Note that A is the matrix of eigenvalues of 
Xd'Xd/m, and T is the matrix of eigenvectors, as shown in Table 6.3. As shown in Chapter 
5, we can next solve for the orthonormal-by-columns matrix U by the equation 
where A 
U = Xd/\/mTA-^ 
A~^^^. This, in turn, leads to the basic structure of 
X^/\/m: 
X^/yJm =UAT' 
As recalled, U is orthonormal by columns; A is diagonal (a stretch transformation); and 
T' is orthogonal (a rotation). Moreover, as also pointed out in Chapter 5, if the 
eigenstructure of the major product moment X^X^'/m is found instead, the matrix of its 
eigenvalues A will still be the same, and the representation is now 
XdXd7^ = UAU' 
where U is the same matrix found above. One then goes on to solve for T' in a manner 
analogous to that shown above .^ 
Finally, by similar procedures we could find the basic structure of any of the following 
matrices of interest in Table 6.1: 
X; 
Xc; 
X/V^; 
or 
XJ^ 
by procedures identical to those shown above. As we know, division of X, X^, or Xgby 
the scalar \/m has no effect on the eigenvectors of either the minor or major product 
moments of X, X^, or X^. Corresponding eigenvalues of the product-moment matrix are 
changed by multipUcation by 1/m, which, in this case, represents the sample size. 
6.4.4 
Other Aspects of Principal Components Analysis 
The example of Table 6.3 represents only one type of principal components analysis, 
namely, a components analysis of the covariance matrix C. As indicated above, one can 
component-analyze the averaged raw cross-products matrix X'Xfm or the correlation 
"* Alternatively, we could find U and T' simply by finding the eigenvectors of X^X(^'/m 
and 
X^'Xf^/m 
separately. 

6.4. THE FACTOR ANALYSIS PROBLEM 
277 
matrix X^'XJm. In general, the eigenstructures of these three matrices will differ. That is, 
unlike some factoring methods, such as canonical factor analysis (Van de Geer, 1971), 
principal components analysis is not invariant over changes in scale or origin. 
Principal components analysis does exhibit the orthogonality of axes property in 
which the axes display sequentially maximal variance. That is, the first axis displays the 
largest variance, the second (orthogonal) axis, the next largest variance, and so on. In 
problems of practical interest a principal components analysis might involve a set of 30 or 
more variables, rather than the two variables Xi and X2, used here for illustrative 
purposes. Accordingly, the opportunity to replace a large number of highly correlated 
variables with a relatively small number of uncorrected variables, with little loss of 
information, represents an attractive prospect. It is Httle wonder that principal 
components analysis has received much attention by researchers working in the 
behavioral and administrative sciences. 
It is also not surprising that a large variety of other kinds of factoring methods have 
been developed to aid the researcher in reducing the dimensionahty of his data space. 
Still, principal components represents one of the most common procedures for factoring 
matrices and, if anything, its popularity is on the rise. 
However, from a substantive viewpoint, the orientation obtained from principal 
components may not be the most interpretable. Accordingly, applications researchers 
often rotate the component axes that they desire to retain to a more meaningful 
orientation from a content point of view. A number of procedures (Harman, 1967) are 
available to accomplish this task. Generally, the applied researcher likes to rotate 
component axes with a view to having each variable project highly on only one rotated 
dimension and nearly zero on others. 
Another problem in any type of factoring procedure concerns the number of 
components (or factors, generally) to retain. Most data matrices will be full rank; hence, 
assuming that the number of objects exceeds the number of variables, one will obtain as 
many components as there are variables. Often the "lesser" components (those with lesser 
variance) are discarded; one often keeps only the first r (<n) components that account 
for some appreciable proportion (e.g., 80 to 90 percent) of the total variance in the data. 
Other rules for deciding how many factors to retain are also in use, including various 
statistical and graphical criteria. Still, the decision is largely a judgmental one, and factor 
analysis remains something of an ad hoc set of procedures. 
Factor analysis—either principal components or other type of factoring procedure-
represents only one class of methods for effecting dimensional reduction of one's data. 
More recently, new classes of techniques, such as multidimensional scaling (Green and 
Wind, 1973), have been used to develop reduced spaces. Many of these newer methods 
require only rank order input data. For example, the elements of a covariancelike matrix 
need only be ranked in order for these "nonmetric" procedures to be used. 
However, insofar as the metric procedure of principal components analysis is 
concerned, we see that the major mathematical tool involves the eigenstructure of 
symmetric matrices. Related concepts such as the singular value decomposition of a 
matrix into its basic structure, quadratic forms, and matrix rank are also of interest. 
From a geometric viewpoint we seek a rotation of the original basis of the space that 
coincides with the axes of the hyperellipsold of points, assumed to represent the objects, 
in the original n-dimensional space. The eigenvalues correspond to the variances of these 

278 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
new axes, and the normalized eigenvectors of the particular cross-product matrix 
employed are the direction cosines that define the rotation. 
6.5 
THE MULTIPLE DISCRIMINANT ANALYSIS PROBLEM 
The third problem described in Chapter 1 concerns the development of linear 
composites of X^i and Xd2 with the property of maximally separating the three groups 
(shown in Fig. 1.5). That is, in this multivariate application, the 12 employees, based on 
the data of Table 6.1, were split into three groups with regard to degree of absenteeism: 
Group 1—low 
(employees a, b, c, d) 
Group 2—intermediate 
(employees e, f, g, h) 
Group 3—high 
(employees i, j , k, 1) 
While it happens to be the case here that the three groups are ordered with respect to 
extent of absenteeism, this is not a requirement of multiple discriminant analysis (MDA). 
Any polytomy consisting of a set of mutually exclusive and collectively exhaustive groups 
is sufficient for application of MDA. 
In the sample problem application of MDA, we wish to find a linear composite of X^i 
and Xf^2 with the property of maximizing among-group variation relative to (pooled) 
within-group variation. Like principal components analysis, this involves finding the 
eigenstructure of a matrix. However, in this case the matrix is nonsymmetric, although it, 
in turn, represents the product of two symmetric matrices. 
6.5.1 
Finding the Eigenstructure 
The quantity to be maximized in MDA consists of the ratio 
where, as it tums out, Xi is an eigenvalue, and A and W denote among-group and pooled 
within-group SSCP matrices, respectively. The vector Vi denotes the set of weights used 
to develop the linear composite (denoted as Wi) of the original mean-corrected score 
matrix Xj, while SS/^ and SSyj^ denote the among-group and within-group sums of 
squares of the linear composite. In scalar notation. 
^i{i)=ViX^n+V2Xai2 
Let us develop these concepts, step by step. 
Figure 6.6 shows the first linear composite Wi that we seek. We see that Wi makes an 
angle of 25° with the horizontal axis. We can project the 12 points onto Wi and find 
their discriminant scores W/(i).(The grand mean of these scores will be zero.) Also, we can 
find the three group means on Wi and the associated among-group and pooled 

6.5. 
THE MULTIPLE DISCRIMINANT ANALYSIS PROBLEM 
279 
Fig. 6.6 
Discriminant transformation of the mean-corrected predictor variables from Table 6.1. 
Key: • Group 1; o Group 2; x Group 3. 
within-group sums of squares. According to the preceding criterion, we have found 
of scores W/(i) with the property that 
a set 
Xi = ^ % ( W i ) 
is maximal. That is, if we find (a) the sum of squares of the three group means from the 
grand mean, which is zero in the case of mean-corrected data and (b) the pooled 
within-group sum of squares of each of the scores about their respective group means on 
Wi, then (c) the ratio Xi of these two sums of squares is greater than that found by any 
other suitably normalized^ axis in the space of Fig. 6.6. 
Table 6.4 shows the preliminary calculations of interest. First, we compute each group 
mean on X^i and X^2 ? respectively; with equal-size groups these means, of course, sum to 
zero, within rounding error. Then we find the matrix of within-group deviations and the 
matrix of among-group deviations from group and grand mean, respectively. 
From here, we compute the minor product moment of 
X^—Xf^, 
the matrix of within-group deviations and, similarly, the minor product 
moment of 
Xj^ — Xj^, 
the matrix of among-group deviations (for k= \,2, .. . ,K = 3 groups) 
^ That is, we seek a linear composite in which the coefficients u i andV'j ^ ^ direction cosines. 
Also, it should be remembered in the computation of .S^^CWj ) that each group mean is based on four 
observations. 

280 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
TABLE 6.4 
Preliminary Calculations for Multiple Discriminant Analysis 
Employee 
^ d l 
5.25 
5.25 
-4.25 
•3.25 
-4.25 
1.25 
1.25 
0.25 
0.75 
0.50 
3.75 
4.75 
4.75 
5.75 
^ d 2 
-3.92 
-3.92 
-2.92 
-2.92 
-3.42 
-0.92 
1.08 
0.08 
-0.92 
-0.17 
3.08 
2.08 
4.08 
5.08 
^k- -Xfc 
Within-group 
deviations 
- 1 
0 
0 
1 
-0.75 
-0.75 
0.25 
1.25 
- 1 
0 
0 
1 
-0.5 
-0.5 
0.5 
0.5 
-0.75 
L25 
0.25 
-0.75 
-0.50 
-1.50 
0.50 
2.50 
Xfc-
Between 
X 
-group 
deviations 
-4.25 
-4.25 
-4.25 
-4.25 
-0.5 
-0.5 
-0.5 
-0.5 
4.75 
4.75 
4.75 
4.75 
-3.42 
-3.42 
-3.42 
-3.42 
-0.17 
-0.17 
-0.17 
-0.17 
3.58 
3.58 
3.58 
3.58 
Mean 
Mean 
Mean 
4.75 
3.58 
yf = (Xk-XkYiXk-Xf,); 
A = 
{Xk-Xy(Xk-X) 
w = 
6.75 
1.75 
1.75 
8.75 
T = W + A = 
A = 
163.5C 
126.50 
170.25 
128.25~ 
128.25 
106.92 
) 
126.50 
98. n j 
to find W, the pooled within-group SSCP matrix, and A, the among-group SSCP matrix, 
as shown in Table 6.4. Their sum equals the total sample SSCP matrix T, which is also 
shown in Table 6.4. 
The problem, as shown in Appendix A, is to maximize Xi with respect to Vi. The 
resulting matrix equation is 
(A-XiW)vi = 0 
Assuming that W is nonsingular and, hence, that W ^ exists, we can premultiply both sides 
of the above equation to get 
(W-*A-Xil)vi=0 
with characteristic equation 
|W-^A-XiI|=0 
Note, then, that we have another eigenstructure problem, one now involving the 
nonsymmetric matrix W"^A. 

6.5. 
THE MULTIPLE DISCRIMINANT ANALYSIS PROBLEM 
281 
TABLE 6.5 
Finding the Eigenstructure of the W"^A Matrix 
w = 
Eigenva 
A = 
0.156 
-0.031 
-0.031 
0.121_ 
lues of W-^ A 
'29.444 
0 
~| 
_ 0 
0.0295J 
; 
W-'A = 
21.594 
16.698" 
10.138 
7.880 
Eigenvectors of W'^A 
; 
v = 
0.905 
-0.612~| 
0.425 
O.791J 
As is the case with principal components analysis, generally the characteristic equation 
will have more than a single root. In fact, in this problem we shall be able to fmd two 
eigenvalues, Xi and X2 ? and their associated eigenvectors. 
Table 6.5 shows the eigenvalues and eigenvectors obtained for this sample appHcation. 
Note the parallel between this problem and the principal components problem. In each 
case we are finding the eigenstructure of a matrix, but here the matrix is nonsymmetric. 
From Table 6.5 we see that the first discriminant function displays a relatively large 
eigenvalue of 
Xj = 29.444 
with associated, and normalized, eigenvector 
Vl 
0.905 
0.425 
representing an angle of 25° from the horizontal axis. 
Also, similar to principal components analysis, we can obtain a second discriminant 
function W2 with scores that are uncorrected with those of the first function. The 
eigenvalue associated with W2 is 
X. = 0.0295 
with normalized eigenvector 
V2^ 
-0.612 
0.791 
Note that X2 is much smaller than Xi; for all practical purposes it appears that a single 
discriminant function might account for these data. 
In general, with K groups and n predictors one obtains 
mm{K-\,n) 
different discriminant functions; here, of course K — \=n = 2, and we note that two 
functions are obtained. Usually, however, the number of predictors will greatly exceed 
the number of groups, and a great deal of parsimony can often be achieved by the use of 
discriminant scores. 

282 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
As before, discriminant scores are found by projecting the points onto the 
discriminant axes. For example, the discriminant score of the first observation on Wi is 
wi(i) = 0.905(-5.25) + 0.425(-3.92) = -6.42 
as shown in Fig. 6.6. 
We also observe in Fig. 6.6 that Vi and V2 are not orthogonal, even though the scores 
on Wi versus those on W2 are uncorrelated. From the matrix of eigenvectors in Table 6.5 
we can compute the cosine between Vj and \2 as follows: 
cos ^ = (0.905 
0.424) 
= -0.21, 
so that 
xl^=102 
-0.612 
0.791 
Thus, the angle ^ separating vi and V2 is 90° + 12° = 102°, as shown in Fig. 6.6. 
6.5.2 
Statistical Significance and Classification 
It is one thing, of course, to find linear composites with the properties described 
above; it is quite another to test their statistical significance and to use them for 
classifying observations. Accordingly, each of these problems is taken up, in turn. At this 
point we have found two eigenvalues: 
Xi = 29.444; 
0.0295 
Bartlett (1947) has proposed a statistic that can be used to test the significance of the 
discriminant functions (actually, their eigenvalues). 
Bartlett's statistic starts out by testing the null hypothesis that group centroids are all 
equal in the full discriminant space, in this case involving both the wi and W2 axes. 
Bartlett's statistic is expressed as follows: 
F=2.3026[m-l-(«+A:)/2] Z 
log(l+X,) 
where m denotes sample size, n denotes number of predictor variables, K denotes number 
of groups, and X/ denotes the /th eigenvalue (/ = 1, 2,. .., r). In terms of the sample 
problem, 
V= 2.3026[12-l-(2 + 3)/2](log 30.444 + log 1.0295) 
= 2.3026(8.5)(1.48350 + 0.01263) 
= 29.035 + 0.247 
= 29.282 
Bartlett's V statistic is approximately distributed as chi square with n{K — 1) = 4 degrees 
of freedom. In the sample problem, Fis significant beyond the 0.01 alpha level. 
However, one wonders whether the second discriminant function, whose eigenvalue is 
almost zero, adds anything beyond the first. Fortunately, Fcan be decomposed into the 
separate parts 
Fi = 29.035; 
F2 = 0.247 

6.5. 
THE MULTIPLE DISCRIMINANT ANALYSIS PROBLEM 
283 
The first portion Vi has already been tested in the context of V. However, if Vi is 
"partialed out," is V2 statistically significant? As it turns out, V2 can be tested in the 
same way that V was tested: V2 = 0.247 is also approximately distributed as chi square 
with 
n(K-l)-(n+K-2) 
= 
(n-l)(K-2)=l 
degree of freedom. This approximate chi square (V2 =0.247) is clearly nonsignificant. 
Not surprisingly, we conclude that only the first discriminant function need be retained. 
Bartlett's procedure can be used for more than two discriminant functions in a similar 
manner. Had a third discriminant function been involved, its associated degrees of 
freedom would be (n — 2)(K — 3); those associated with a fourth discriminant function 
would be (n — 3)(K — 4), and so on. However, the reader interested in applying this test 
in substantive research should be aware of its assumptions (Harris, 1975; pp. 109-113). 
In the sample problem it is not hard to see why only the first discriminant function is 
significant. The following ratio: 
^1 
29.444 
- 0.999 
X1+X2 
29.444 + 0.0295 
shows that Wi exhausts virtually all of the variation in the discriminant space. 
Classifying the twelve observations by means of Wi, the retained and significant 
discriminant function, is quite straightforward. All that is entailed is to compute a 
discriminant score for each observation, according to 
One also computes the discriminant scores for the three group means 
Wi(Group 1) = 0.905(-4.25) + 0.425(-3.42) = -5.30 
Wi(Group 2) = 0.905(-0.50) + 0.425(-0.17) = -0.52 
iviCGroup 3) = 0.905(4.75) + 0.425(3.58) = 5.82 
One then assigns each observation to that group whose mean score on Wi is closest to the 
individual score, w,(i). 
When this procedure is implemented for the sample problem, it turns out that all 
twelve cases are correctly assigned to their respective groups. Had W2 also been 
statistically significant and retained for classification purposes, the classification 
procedure would have been modified to involve the computation of Euclidean distances 
between each individual observation and each group centroid in discriminant function 
space.^ Each observation would then be assigned to the group whose centroid, in 
discriminant function space, was nearest. 
It should be mentioned, however, that the use of Bartlett's statistic and the 
classification rules enumerated above only scratch the surface of the topics of statistical 
^ It should be noted that in discriminant function space the pooled within-group SSCP matrix 
would first be spherized by means of the procedure described in Section 5.9.2; it is this space in which 
ordinary Euclidean distance is appropriate (given equal prior probabilities and equal costs of 
misclassification) for assigning objects to groups. 

284 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
significance and assignment. For example, Bartlett's statistic can be modified by 
Schatzoff s tables (Schatzoff, 1966) to produce an exact test. Also, other tests (Rao, 
1952) are available for testing the null hypothesis of group centroid equality in the full 
discriminant space. 
The classification rules described above also need modification in cases where the prior 
probabilities of inclusion differ across the groups or where the costs of misclassification 
differ. Modern approaches to the problem (e.g., Eisenbeis and Avery, 1972) formulate the 
classification task in terms of statistical decision theory. As such, both prior probabilities 
of an observation belonging to each of the groups and costs of misclassification can be 
expHcitly introduced into the assignment procedure. 
6.5.3 
Other Aspects of Multiple Discriminant Analysis 
One of the questions posed in Chapter 1 concemed the relative importance of the two 
predictors X^i and ^^2 ^^ effecting group discrimination. In the case of correlated 
predictors, this represents an ambiguous question and shares, along with multiple 
regression and other multivariate techniques, the difficulties of parceling out variance 
among nonorthogonal predictors. While we do not go into this question in detail, a few 
procedures that have been suggested for ascribing relative importance to Xdi versus Xd2 
can be mentioned. 
First, the entries in the normalized eigenvector Vi are 0.905 and 0.425 for Xdi and 
X(^2 > respectively. These are analogous to partial regression coefficients in multiple 
regression. To convert them into standardized (beta-type) numbers, each is multiplied by 
that predictor variable's pooled within-group standard deviation:^ 
Standardized weight (X^i); 
0.905 x V0L75 = 0.783 
Standardized weight (Xdi); 
0.425 x V0L972 = 0.419 
As can be noted, on either a standardized or nonstandardized basis, Xji receives the larger 
weight. 
Cooley and Lohnes (1971) recommend what they call structure correlations to 
ascertain predictor importance. These are merely the product-moment correlations 
between scores on each original variable and the discriminant scores. In this example they 
tum out to be 
Structure correlation (Xdi) = 0.998 
Structure correlation (Xd2) = 0.976 
In this case both predictors correlate highly with the retained discriminant function Wi, 
although the correlation for Xdi is slightly greater. 
Still other procedures, such as Bock and Haggard's (1968) step-down F ratios, can be 
employed to measure the relative importance of various predictors. However, we do not 
delve into these more esoteric methods, other than to say that the question of ascribing 
"relative importance" remains ambiguous in the case of correlated predictor variables no 
matter what procedure is used. 
"^ Other standardization procedures, based on multiplication of each discriminant coefficient by 
the total-sample (as opposed to pooled within-group) standard deviation of the variables of interest, 
are also in use. 

6.5. 
THE MULTIPLE DISCRIMINANT ANALYSIS PROBLEM 
285 
Another topic of interest concerns the relationship of MDA to other multivariate 
techniques. For example, an intimate connection exists between MDA and principal 
components analysis. Without delving deeply into the technical details, it turns out that a 
preliminary "spherizing" of the data matrix via 
XaW-
results in a new set of coordinates with spherical (pooled) within-group variation. 
One can then find the eigenstructure of the matrix 
W-^/^AW-
so as to satisfy the equation 
[W-i/^AW-^/^]Q = QA 
where Q is orthogonal and A is diagonal.^ The final transformation is then 
XHW-^/^O 
which, of course, is a spherizing transformation followed by a rotation of the spherized 
within-group variation to principal axes, on the basis of among-group variation. This idea 
was described in the discussion in Chapter 5 of the simultaneous diagonalization of two 
different quadratic forms.^ 
Probably the most important point to mention, however, is that MDA is one member 
of the same general family that includes 
1. 
canonical correlation, 
2. 
multivariate analysis of variance and covariance, 
3. 
categorical canonical correlation. 
The linkage among these multiple-criterion techniques is provided by a generaHzed 
canonical correlation model that allows for dummy variables on one or both sides of the 
equation. For example, one could have developed a multiple discriminant function for 
the sample problem by means of a canonical correlation in which the criterion variables 
were represented by the dummies 
1 0 
0 1 
0 0 
(Group 1) 
(Group 2) 
(Group 3) 
By a similar judicious choice of dummy and continuous variables, one can find linear 
composites of both the criterion and predictor batteries that relate to any of the specific 
multivariate techniques described above, and in Chapter 1 as well. 
'AW-
while Q 
' In this illustration A denotes the diagonal matrix of eigenvalues of W' 
denotes the associated matrix of eigenvectors. 
^ Still other procedures are available for finding the eigenstructure of W"*A (see Overall and Klett, 
1972). 

286 
6. 
APPLYING THE TOOLS TO MULTIVARL\TE DATA 
Full discussion of the interrelationships among techniques would take us far beyond 
the scope of the book. As we have illustrated in Chapter 5, however, the eigenstructure of 
nonsymmetric matrices and the simultaneous diagonalization of two different quadratic 
forms figure prominently in the computation of discriminant functions for three or more 
groups. These concepts are also central in canonical correlation, multivariate analysis of 
variance and covariance, and categorical canonical correlation; in the last case, all 
variables are expressed as dummies. 
6.6 
A PARTING LOOK AT MULTIVARIATE TECHNIQUE 
CLASSIFICATION 
In Chapter 1 a number of characteristics were enumerated that provided guidance for 
classifying the large, and still growing, variety of multivariate techniques. In particular, 
the following descriptors represented the main bases of classification: 
1. whether the data matrix is kept intact versus partitioned into criterion and 
predictor subsets; 
2. the number of variables in each subset (if partitioning is undertaken); 
3. 
the types of scales by which the variables are measured. 
At this point, however, the various types of linear transformations described in 
Chapters 4 and 5 are behind us. And, even in the introductory material of Chapter 1, it 
was suggested that multivariate analysis is largely concemed with transformations for 
matching one set of numbers, such as a data vector, a data matrix, or a linear composite, 
with some other set of numbers. 
The degree of matching is usually assessed by a residual sum of squared deviations or 
some other measure that can be related to this. This idea was illustrated at the beginning 
of the present chapter in the context of multiple regression. Here we desired to minimize 
the quantity 
m 
^' 
i = l ei' = (Yr •^ /» 
- > - , • ) ' 
where Y^ denotes a datum, and ff denotes a predicted value of Y^. As a further aid to 
technique classification, we now take the view that multivariate techniques may differ 
according to the nature of the allowable transformations and the properties that the 
transformed matrices exhibit in the matching process. 
Partly by way of review of Chapters 4 and 5 and partly by way of prologue, let us list 
the major classes of transformations that vectors or matrices can undergo in the course of 
achieving various types of matching. For illustration, let us assume a general data matrix, 
denoted by X, of m rows and n columns (m > n). 
Our objective here will be to recapitulate various types of transformations described in 
earlier chapters as a way to make explicit the present descriptor, the nature of the linear 
transformation, for characterizing multivariate techniques. 

6.6. 
A PARTING LOOK AT MULTIVARIATE TECHNIQUE CLASSIFICATION 
287 
6.6.1 
Types of Transformations 
By way of an overview, Fig. 6.7 shows a directed graph of the transformations that are 
considered. This list of transformations is not meant to be exhaustive. However, those 
shown in Fig. 6.7 appear to be the most frequently encountered ones in multivariate 
analysis. We consider the more general classes first, followed by the more restricted 
transformations. 
We shall let T denote an arbitrary matrix. The matrices U and V denote either 
orthogonal matrices or orthonormal sections, while A denotes a diagonal matrix. From 
Section 5.7 we know that T can always be decomposed into the triple product 
T = UAV' 
We shall take advantage of this type of singular value (Eckart-Young, 1936) 
decomposition in describing various special cases of a general linear (or affme) 
transformation. 
General linear (affine) 
X* = XT+ 1c' 
= XUAV'+ 1c' 
Extended similarity 
X* = XUA+ 1c' 
Homogeneous linear 
X* = XT 
= XUAV' 
JTI 
Stretch 
X* = XA 
Central dilation 
X* = XA 
Identity 
X* = XI 
Extended 
permutation 
X* = XWA 
Simple 
permutation 
x* = xw 
Restricted 
similarity 
X* = XUA 
Rotation 
x* = xu 
Rotation-
annihilation 
X* = XU^ 
where 
U 'U = I 
Fig. 6.7 
A directed graph of various types of linear transformations. 

288 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
6,6,1.1 General Linear (Affine) Transformation The most general transformation 
of X to be considered is an affine transformation, defined as 
XT+lc 
where T(=UAV') denotes an arbitrary linear transformation. The matrix product of the 
m X 1 unit column vector 1 and c' a 1 x « row vector of constants defines the permissible 
shift of origin (as illustrated in Chapter 4 in the case of a centroid-centered orientation).*^ 
6.6,1,2 Homogeneous Linear Transformation A homogenous linear transformation 
can be defined as 
X* = XT 
with no shift in origin, but T, defined as before, is otherwise not restricted. 
6.6.1.3 Similarity Transformations An extended similarity transformation involves 
a rotation, achieved by the orthogonal matrix U, a central dilation, effected by the scalar 
matrix A, and a shift in origin: 
X* = XUA + Ic' 
where 1 and c' are defined as before. 
A restricted similarity transformation is a special case of this in which no shift in origin 
is permitted: 
X* = XUA 
where U and A are defined as before. 
6.6.1.4 Rotation 
As illustrated in earlier chapters, a rotation is a transformation 
that is carried out by an orthogonal matrix. This type of matrix is denoted by U, where 
U'U = UU' = I. The transformation is written as 
XU 
If the determinant |U| = 1, then a proper rotation of X is entailed. If |U| = - 1 , then a 
rotation of X followed by a reflection is entailed (i.e., an improper rotation). 
6.6.1.5 
condition Up'Up 
Rotation-Annihilation 
One type of transformation stipulates that only the 
I be met; that is. Up can be an orthonormal section (rectangular rather 
than square) whose columns are mutually orthogonal and of unit length. This amounts to 
a rotation followed by annihilation of some dimensions. 
In Fig. 6.7 we show this transformation with a dotted rather than solid line. This is 
because Up is not a special case of U for the reason that Up Up' ^ I. As such, Up is rather 
tangentially related to the overall schema of Fig. 6.7. 
^° Note that an affine transformation is nonhomogeneous in the sense that there are no fixed 
points (e.g., the 0 or origin vector) under this type of mapping. However, Section 4.4.1 shows how it 
can be carried out via matrix multiplication. 

6.6. 
A PARTING LOOK AT MULTIVARIATE TECHNIQUE CLASSIFICATION 
289 
6,6.1.6 
Permutation 
An extended permutation permits both a reordering of 
dimensions and a stretch or rescaling of the configuration. This is written as 
= XWA 
where W is a permutation matrix and A is diagonal. As recalled, a permutation matrix is 
an orthogonal matrix, all of whose entries are either 0 or 1, that changes the order of 
dimensions. 
A simple permutation is written as 
X* = XW 
with W defined as before. 
6.6.1.7 
Stretch 
A stretch transformation 
involves a simple rescaling of the 
configuration by a diagonal matrix A. That is. 
X* = XA 
6.6.1.8 
Central Dilation 
A special case of a stretch transformation involves a 
central dilation, given by the scalar matrix A. That is. 
X* = XA(=AX) 
6.6.1.9 
Identity 
Transformation 
A special case of the central dilation transfor-
mation is the identity transformation 
X* = IX = XI = X 
where I, of course, is the identity matrice. 
While still other combinations are possible, the preceding ones cover most cases of 
practical interest. 
6.6.2 
Constructing the Classification 
With the various geometric illustrations presented in the preceding section, it is now 
appropriate to discuss the nature of multivariate techniques from the standpoint of 
configuration matching. We consider the following classes: 
1. 
vector-matrix matching, 
2. 
matching of two matrices, 
3. 
matching of three or more matrices, 
4. 
matching of a data-based and an internally derived matrix. 
Within each of these classes, two additional aspects are discussed: 
1. 
types of scores—continuous or binary (dummy variable), 
2. 
type of permissible transformation applicable to each matrix or vector. 

290 
6. 
APPLYING THE TOOLS TO MULTIVARIATE DATA 
l1 
1 
n- 1 
P 
II 
q 
p 
III 
q 
IV 
Fig. 6.8 
Illustrative partitionings of data matrix. 
Each of the above major classes is examined in turn. To assist us in this regard, we 
reproduce in Fig. 6.8 the schema that appeared as in Fig. 1.1. However, now we 
emphasize the nature of the linear transformation. 
6.6,2.1 
Vector-Matrix Matching Panel I of Fig. 6.8 is the prototype of the family 
of multivariate techniques illustrated by the matrix equation 
y = Xb 
where b is a vector of combining weights, and y is a set of predicted values for y, the 
criterion variable. As illustrated earlier, by allowing y or X to be mixtures of continuous 
or binary-coded variables, this family is broad enough to include 
1. multiple (and simple) regression; 
2. 
two-group discriminant analysis, where y is binary valued; 
3. 
analysis of variance and covariance, where some columns of X are binary valued; 
4. 
binary-valued regression, where both y and X are binary valued. 
In least-squares theory the scalars R^ or 97^ (eta squared) are usually the quantities being 
maximized.^^ Both R^, in the context of regression, and i?^, in the context of analysis of 
variance, are invariant over linear transformations of y or X. Moreover, both 7^^ and T?^ 
can be simply related to the criterion of minimizing the sum of the squares of y - y, as 
described earlier. 
6.6.2.2 Matching Two Matrices In Panel II of Fig. 6.8 we have the case of two 
matrices, Y^nxp ^^^ ^mxq^ and are interested in the association between these two 
batteries of variables. If we assume that both sets of variables represent continuous values, 
the canonical correlation problem can be represented by separate affine transformations 
of Y and X such that each pair of linear composites is most congruent with each other, 
subject to being uncorrected with previously "extracted" composites. This uncorrelated-
ness condition is an illustration of the kinds of restrictions that may be placed on the 
transformed values. 
" The scalar 17^ (eta squared) is computed in just the same way as 7?^ except for the fact that all 
predictor values are dummy variables. This is equivalent to 
, - 
SSA 
while SS/i^ denotes the among-group sum of squares and SSj denotes the total-group sum of squares. 

6.6. 
A PARTING LOOK AT MULTIVARIATE TECHNIQUE CLASSIFICATION 
291 
Other possibilities come to mind, however. For example, one could allow only a 
separate homogenous linear transformation of each matrix with no shift in origin 
permitted. Or, one could permit a shift in origin but require each transformation to be an 
extend similarity transformation which, as shown earlier, is less general than an affine 
transformation. 
In other kinds of applications we may desire Y to remain fixed (i.e., transformed by an 
identity matrix) but permit X to be transformed by an affine transformation, extended 
similarity, or a similarity transformation. Some "procrustes" solutions, as used in 
matching factor score solutions from different studies, are of this general type (Rummel, 
1970). 
Still other restrictions are possible. Schonemann and Carroll (1970) describe a 
matching 
procedure in which one matrix undergoes an extended 
similarity 
transformation, while the other undergoes either (a) an extended similarity, (b) a 
similarity, (c) a rotation, or (d) an identity transformation. CHff s procedure (Cliff, 1966) 
allows a similarity transformation on one side and a similarity, rotation, or identity 
transformation on the other. 
If one matrix consists of two or more binary-valued variables, we have an instance of 
either multiple discriminant analysis or multivariate analysis of variance, depending upon 
how one frames the problem. From the standpoint of permissible transformations, 
however, the techniques are similar. That is, one can formulate either a multiple 
discriminant problem or a multivariate analysis of variance problem in terms of the 
canonical correlation model with one of the two matrices represented by binary-valued 
dummies. Generally, however, we are interested in special kinds of output that are related 
to the particular procedure employed. Therefore, while one could use a canonical 
correlation program to find discriminant weights, ordinarily we would not do so since we 
would be interested in various ancillary outputs as well. 
If both data sets consist of dummy variables, we may have a case of categorical 
canonical correlation or categorical conjoint measurement (Carroll, 1973). Insofar as the 
solution to the problem is concerned, these techniques are special cases of canonical 
correlation in which both matrices consist of dummy variables. 
Variations can be developed, however. For example, Horst (1956) describes a type of 
multiple discriminant analysis in which the dummy-variable criterion matrix, defining 
group membership, remains fixed. The predictor matrix is transformed linearly to best 
match it, subject to the predicted values maintaining the same column means and 
variances as the columns of the criterion-variable matrix. 
6.6.2.3 
Three or More Matrices Heretofore, we have described multivariate analysis 
of covariance in terms of a matrix of criterion variables and a matrix of predictor 
variables. The latter matrix consists of a mixture of dummy variables, the design variables, 
and covariates, whose effect on the criterion variables we desire to remove. Alternatively, 
we can partition the data matrix into three matrices: criterion, design dummies, and 
covariates, as illustrated in Panel III of Fig. 6.8. 
Problems involving three, or more, matrices fall into two major types: 
1. a multivariate analysis of covariance situation, or multiple, partial correlation 
(Cooley and Lohnes, 1971), in which one of the data matrices consists of a set of 

292 
6. 
APPLYING THE TOOLS TO MULTIVARL\TE DATA 
covariates, moderators, or contingency variables whose effect is to be removed before 
considering the association between the remaining matrices; 
2. 
a generahzed canonical correlation situation where the status of all three, or more, 
matrices is considered to be the same (in this case, we extend two-group canonical 
correlation to cover three or more matrices). 
Multivariate analysis of covariance problems occurs frequently in the behavioral and 
administrative sciences. For example, one may set up various experiments in which 
several response measures are sought from the subjects and, furthermore, certain 
covariates like task familiarity, education, and IQ level are also included in the analysis. 
In multivariate analysis of covariance one matrix, the response matrix, is typically 
made up of continuous scores, while the design matrix is typically made up of dummy 
variables. The matrix of covariates is usually made up of continuous scores. However, this 
is not necessary. In principle, any (or all three) of the matrices could consist of 
continuous or binary-valued scores, or, indeed, as mixtures. In this class of problems one 
generally allows afflne transformations to be applied to any of the three matrices, in the 
spirit of two-set canonical correlation. 
Generahzed canonical correlation, employing three or more data-based matrices of 
equal status, is concerned primarily with configuration matching. Horst (1961), Carroll 
(1968), and Kettenring (1972) have all proposed models for this type of problem. For 
example, in the Carroll and Chang approach, an r + 1st space is defined such that the r 
original spaces, each consisting of the same m observations on r sets of variables, are 
transformed to match it as well as possible. This procedure allows an affme 
transformation of each "contributing" configuration. 
While generahzed canonical correlation has usually been considered in the context of 
all scores being continuous, this, again, is not necessary provided that the researcher's 
interest is centered on data description and summarization, rather than on statistical 
inference. Binary-valued scores, or mixtures of continuous and binary valued, can be dealt 
with just as readily. Again, afflne transformations would generally be permitted. 
6.6.2.4 Matching Based on an Internal Criterion Multivariate techniques can also 
cover the possibility of deriving a matrix (e.g., a "latent" matrix) that best reproduces the 
scores of a data-based matrix, or some matrix derived from it, subject to meeting certain 
internal criteria. For example, in our earlier discussion of principal components analysis 
employing the covariance matrix as input, we found a rotation of the space whose 
successive dimensions accounted for the greatest amount of residual variance. This can 
also be viewed as defining successively higher-dimensional subspaces that maximize 
variance for that dimensionality. 
As pointed out earUer, most factor analytic techniques (e.g., principal components 
analysis) are not independent of scale. That is, different results are obtained depending 
upon whether the averaged raw sums of squares and cross products, covariance, or 
correlation matrix is the one being factored. A major exception to this is canonical factor 
analysis (McDonald, 1968). This technique produces results that are comparable across 
various types of data scaling. That is, the solution obtained from one type of scaling can 
be readily transformed to a solution obtained from a different scaling of the original data 
matrix. Maximum likelihood factor analysis (Van de Geer, 1971) also yields results that 
are independent of scale. 

6.7. 
SUMMARY 
293 
As pointed out earlier, some types of factor rotation (e.g., Varimax) are based on 
achieving internal criteria of "simple" structure (Horst, 1966). Simple structure entails 
the idea of a hypothetical zero-one matrix in which each variable is, ideally, supposed to 
load with unity on one factor only (i.e., with zeros appearing elsewhere). In this sense a 
type of "matching" of one matrix to another is also involved. 
In brief, a useful descriptor in characterizing multivariate methods is the type of linear 
transformation involved in the matching process and the restrictions placed on the nature 
of the transformed data. As we have illustrated, if only briefly, the various possibilities 
are extensive. Combined with the descriptors of Chapter 1, the type of linear 
transformation descriptor provides a rather comprehensive system for characterizing all 
current multivariate techniques. Moreover, it can be suggestive of still other combinations 
to be invented. 
6.7 
SUMMARY 
In this chapter we have tried to show how the mathematical tools developed in the 
foregoing chapters and the appendixes underlie the formulation and solution of various 
multivariate techniques. In particular, multiple regression, principal components analysis, 
and multiple discriminant analysis were presented as prototypical techniques. 
In multiple regression, the concepts of matrix inversion, determinants, and matrix rank 
figured prominently in the solution. We also showed how the multiple regression problem 
could be described geometrically, both from the standpoint of a response surface or point 
model and from the standpoint of a vector model. Finally, the notion of generalized 
regression, as a least-squares model that encompasses analysis of variance and covariance, 
two-group discrimination, and binary-valued regression, was illustrated graphically. 
Principal components, the technique described next, entailed the rotation of a set of 
basis vectors to a new orthogonal basis with projections whose variance was sequentially 
maximal. The concepts of matrix eigenstructure of a symmetric matrix, matrix rank, and 
quadratic forms were most important here. 
Multiple discriminant analysis then provided us with a procedure for extending our 
discussion to cover the eigenstructure of a nonsymmetric matrix. The simultaneous 
diagonaUzation of two different quadratic forms represented the central concept from 
matrix algebra. Geometrically, this entailed a rotation to align the configuration with the 
principal axes of the within-group SSCP matrix, a spherizing along these axes and then a 
further rotation to principal axes of the transformed among-group SSCP matrix. 
The various matrix transformations of Chapter 4 were then recapitulated and 
organized into a framework within which various multivariate techniques could be 
described. In conjunction with the descriptors of Chapter 1, the specific nature of the 
linear transformation provided a useful way to characterize various multivariate 
procedures. 
This chapter (and the entire book) has served as something of a prologue for textbooks 
dealing with multivariate methods per se. A large number of such texts are listed in the 
references, although no attempt has been made to be exhaustive. We do hope, however, 
that this book will make the going a bit easier as the reader delves more deeply into the 
subject matter of multivariate analysis. 

294 
6. 
APPLYING THE TOOLS TO MULTIVARL\TE DATA 
REVIEW QUESTIONS 
1. 
Using the data of Table 6.1, 
a. 
compute the parameter values of Y regressed on Xi and X2 by means of the 
covariance matrix; 
b. 
repeat the process, now employing the correlation matrix; 
c. 
regress Y on Xi and Xi^ (in place of X2) by means of a raw cross-products 
matrix. How does the/?^ of this compare with the simple squared correlation found from 
the regression of Y on Xi alone? 
2. 
Again using the data of Table 6.1, 
a. 
find the principal components of the correlation matrix R, obtained from Xi 
and X2. How do the eigenvectors compare with those obtained from C, the covariance 
matrix? 
b. 
find the principal components of the averaged raw cross-products matrix X X/m, 
obtained from Xi and X2 ; 
c. 
returning to Section 6.4.1, find the multiple regression of Y on the two columns 
of component scores computed from the covariance matrix C. How does the value of this 
R^ compare to that obtained by regressing Y on Xi and X2 originally? What happens to 
the squared correlation if only the scores on the first component are used? 
d. 
find the eigenstructure of C, the covariance matrix based on all three variables, 
y, Xi, and X2. Compare the eigenvectors obtained here with those appearing in Fig. 6.5. 
3. 
Again using the data of Table 6.1, 
a. 
perform a three-group discriminant analysis on the standardized columns x^j and 
Xs2 using the same group designation as before. How do these discriminant weights 
compare with those found earher? 
b. 
using the procedure of Chapter 5, perform a simultaneous diagonalization of the 
W and A matrices in Table 6.4 and compare your results with those of Table 6.5. 
c. 
split the mean-corrected columns, X^i and X^2 i^ Table 6.1, into two groups 
(viz., the first six versus the second six employees) and compute a two-group discriminant 
function. What simpUfications in the computations are noted in this case? 
4. 
Regress Y on Xi 
and X2, where the latter predictor is now dichotomized with 
^2 < 5 receiving the code value 0 and Z2 > 5 receiving the code value 1. 
a. 
How does the regression equation compare to the original shown in Table 6.2? 
b. 
What is the effect on R^ and the proportion of cumulative variance column, as 
illustrated in Table 6.2? 

APPENDIX A 
Symbolic Differentiation and Optimization 
of Multivariable Functions 
A.l 
INTRODUCTION 
In our earlier discussions of multiple regression, principal components analysis, and 
multiple discriminant analysis, matrix equations were employed to solve for the various 
parameter, values of interest. However, relatively little has been said so far about the 
characteristics of the functions being optimized and the process by which the matrix 
equations are derived. 
In each of the three preceding cases, it is the calculus that provides the rationale and 
specific techniques for optimization. Accordingly, this appendix provides a selective 
review of those topics from the calculus that bear on problems of optimizing functions of 
multivariable arguments. No exhaustive treatment is attempted; rather, we confine our 
discussion to specific aspects of optimization involving only the case of differentiable 
variables where all appropriate partial derivatives can be assumed to exist. 
We first provide a rapid review of formulas from the calculus that involve functions of 
one argument. This is followed by a similar discussion that covers functions of two 
variables. At this point, optimization subject to side conditions is introduced, and the 
topic of Lagrange multipliers is described and illustrated numerically. 
The next main section of the appendix deals with symbolic differentiation of 
multivariable functions, with respect to vectors and matrices. Constrained optimization in 
this most general of cases is also discussed. 
We then turn to each of the three major techniques described in the book: 
1. multiple regression, 
2. 
principal components analysis, 
3. 
multiple discriminant analysis, 
and show how their respective matrix equations are obtained from application of 
optimization procedures drawn from the calculus. 
295 

296 
APPENDIX A 
A.2 
DIFFERENTIATION OF FUNCTIONS OF ONE ARGUMENT 
In Section 6.2.1, a matrix equation (involving the so-called normal equations of 
multiple regression) was described: 
b = (X'X)-^XV 
where b is the to-be-solved-for vector of regression coefficients, X is the data matrix of 
predictor variables (augmented by a column of unities), and y is the data vector 
representing the criterion variable. The parameter vector b is chosen so as to minimize the 
sum of squared deviations between the original criterion vector and the fitted values 
obtained from the regression equation. 
In Section 6.4, the matrix equation 
(C-Xil)ti = 0 
was set up to find the eigenvalue Xi and its associated eigenvector ti that maximized the 
variance of point projections of the deviation-from-mean data. The axis itself was 
obtained by considering the entries of ti as direction cosines defining the first principal 
component. C denotes the covariance matrix. 
In Section 6.5 we sought a vector Vj that maximized the ratio 
where Xi is a scalar (actually an eigenvalue), A denotes the among-group SSCP matrix, 
and W denotes the pooled within-group SSCP matrix. One solves for Xi via the matrix 
equation 
(A-XiW)vi=0 
The discriminant analysis problem involves finding the eigenstructure of W~^A, a 
matrix that is nonsymmetric.^ 
Note that in all three cases we are trying to optimize some function that involves 
multiple arguments. Also recall that in the case of principal components and multiple 
discriminant analysis, certain side conditions, such as ti 'ti or vi 'vi = 1, are imposed. 
Appendix A is motivated by the desire to provide a rationale for the preceding matrix 
equations. As such, we shall need to draw upon various tools from the calculus, starting 
with the simplest case of functions involving one argument and then working up to more 
complex problems involving several variables. 
* In the cases of principal components and (multiple) discriminant analysis, we shall generally find 
successive X^s, subject to meeting stated side conditions with regard to their associated eigenvectors. 

A.2. 
DIFFERENTIATION OF FUNCTIONS OF ONE ARGUMENT 
297 
A. 2.1 
Derivatives of Functions of One Argument 
By way of introduction, assume that we have some function of one argument, such as 
the quadratic 
We can find the value of >' =f(x) for each value x of interest. For a given value of x, let 
us next imagine taking a somewhat larger value, such as Xi =XQ + AX. If so, the function 
y will change, as well, fromj; tojv + Ay. That is, 
y + Ay=f(x,)=f{xo 
+ Ax) 
If we plot y versus x, the ratio Ay/Ax can be viewed as the tangent of the angle between 
the X axis and the chord joining the point {XQ, y) to the point (xi, >» + Ay). Furthermore, 
if Ax is made smaller and smaller, the angle that the chord makes with the x axis will 
approximate the angle between the x axis and the tangent line of the point (XQ, y). This 
appears as a dotted line in Fig. A. 1. 
If we let dy/dx denote the Umit of the ratio Ay/Ax as Xi approaches XQ , then we can 
call this limit the (first) derivative of/(x), denoted variously as dy/dx, y\ or /'(x): 
Note that as Xi =^Xo, Ax =^ 0. 
To illustrate this notion numerically, let us go through the computations for the 
preceding example: 
7 = / ( x ) = 2x^ 
y^•Ay 
= l{x^r Ax)^ 
= 2x^ + 4x Ax + 2(Ax)^ 
Fig. A.1 Graph of function j = 2JC^ (-3 < X < 3). Dashed line is tangent line. 

298 
APPENDIX A 
TABLE A.l 
Derivatives of Elementary Functions 
Constant 
The derivative of a constant is 0. For example, if 
/ W = 1 2 
then 
/'(x)= 0 
Algebraic 
The derivative of ax" is nax" ~^ (for n i= 0). For example, if 
fix) = 3x* 
then 
/ ' ( J C ) = 1 2 J C ^ 
Exponential 
The derivative of the exponential e^ is e^. The derivative of g/(^) isf'{x)ef^^\ 
The derivative of 
fl^ is (In a){a^). As one example, if 
fix) = e-2^ 
then 
fix) 
= - 2 e - 2 ^ 
The derivative of the natural logarithm hix is l/x. The derivative of hi vix) is l/v 
dv/dx. For 
example, if 
then 
d . i\nv) = 
dx 
[ l l 
uj 
[di^ 
Idx] = 
r 1 
— 
^JC^J 
2 
i2x) = -
JC 
Since y = 2x^, we can simplify the above expression to 
Ay = 4x AJC + 2(Ax)^ 
Dividing both sides by A^ gives us 
Ay I Ax = 4JC + 2 Ax 
However, as Ax becomes smaller and smaller, the second term on the right can be 
neglected, and we get the (first) derivative 
! = / • ( . ) = -

A.2. 
DIFFERENTIATION OF FUNCTIONS OF ONE ARGUMENT 
299 
TABLE A.2 
Basic Rules of Differentiation Involving Simple Functions 
Sum or Difference of Two Functions 
The derivative of a sum (difference) of two functions w(jc) and vix) equals the sum (difference) of 
their derivatives. If u and v are functions oix, then 
l("*''^= 
w = 2JC + 2; 
du 
du 
dv 
dx ~ dx 
v= 3x-4 
dx 
- ^ ( w + i;)= -5-(5x-2) = 5 = 2 + 3 
dx 
dx 
Product of Two Functions 
The derivative of the product of two functions u(x) and v(x) is given by 
d 
^ 
du 
dv 
-J- (uv) = v-j- +u -r-
dx 
dx 
dx 
Let 
i; = 3x - 4 
u = 2x; 
du 
^ 
dv 
^ 
-T-=2; 
T T ^ ^ 
dx 
dx 
S 
^"^^ " ^ 
(6JC' -8JC) = 12x- 8 = 2(3x-4) + 3(2;c) 
Quotient of Two Functions 
The derivative of the quotient of two functions w(x) and vix) is given by 
d_ 
dx 
v(du/dx) 
-uidv/dx) 
du 
dx 
•^x' 
v= 3x 
dv 
dx 
Ix 
u 
V 
d 
dx 
~2x'~ 
3x^ = 2/3 = 
3;C"(8JC^)-2X^(9X^) 
(3P)^ 
We could, in turn, take the derivative of the function f (x) = 4x and obtain the second 
derivative f\x) 
= 4. (Higher-order derivatives are all zero since 4 is a constant.) That is, 
second-order derivatives are simply derivatives of first-order derivatives; third-order 
derivatives are derivatives of second-order derivatives, and so on. 

300 
APPENDIX A 
The above procedure can be generalized to the case of finding the derivative of any 
algebraic expression of the form y = ax". In general, ify= flx", we have^ 
ax 
By way of review, Table A.l lists this formula and others that involve various elementary 
functions of interest to the applied researcher.^ 
Not only are derivatives of the more common elementary functions of use, but we 
may also be interested in the derivatives of simple functions of these. Accordingly, 
Table A.2 lists the basic rules that are applicable to simple functions of two elementary 
functions, such as their sum or product. 
A.2.2 
The Chain Rule 
Another concept of the elementary calculus that should be reviewed is the chain rule. 
The chain rule applies to functions of functions. Suppose we have a function/(g-) where 
g, in turn, is g{x), a function of x. If such is the case, the chain rule states that 
dx 
'df] 
dg\ [-1 
To illustrate application of the chain rule, consider the function/(x) = 2(x^ + 3x)^ —1, 
which in turn, can be written as 
g{x) = x^ + 3x 
The chain rule states that 
dl, 
dx 
dg 
dg 
dx 
• 4g{2x + 3) 
Next, we substitute the expression foi g(x) to get 
df 
dx 
= 4(x^ + 3X)(2JC + 3) = 8x^ + 36x^ + 36x 
We can verify this result directly by making the substitution ofg(x) in f{g) to get 
f(x) = 2{x^ + 3x)^-l = 2x^ + 12x^ + 18x^-1 
and 
df 
~=8x^ 
+ 36x^ + 36x 
dx 
as desired. 
^ We assume that« is a real number not equal to 0 and that / (x) is defined and differentiable. 
^ Although not shown in Table A.l, the derivative of sin x is cosx, the derivative of cosx is 
-sin X, and the derivative of tan x is 1/cos^ x. 

A.2. 
DIFFERENTIATION OF FUNCTIONS OF ONE ARGUMENT 
301 
As a second example, consider the function /(;c) = ln(x^ + 3). This, in turn, can be 
expressed as 
Then, by appHcation of the chain rule, we have 
dx' 
dg 
dg 
dx 
{2x) 
and, substituting g{x) = ^^ + 3 for g, we have 
df 
2x 
dx 
jc^ + 3 
The chain rule can be easily extended to three or more functions in terms of the 
following: 
dx 
df 
dg 
dg 
dh 
dh 
dx 
and so on, for additional functions. 
As an example of the case involving three functions, consider the expression 
fix)=Mx+i)r 
This, in turn, can be expressed as 
fig) = g^; 
g(h) = \nh; 
h(x) = x + l 
Applying the chain rule leads to 
dx 
dg 
dg 
dh 
dh 
dx 
The chain rule, augmented by the formulas shown in Tables A.l and A.2, provides a 
flexible procedure for differentiating the more common functions encountered in applied 
research. 
A.2.3 
Optimization of Functions of One Argument 
As recalled from the elementary calculus, a function of one argument has a local 
maximum at some point XQ if the values of the function on either side of JCQ are less than 
f(xo). 
On the other hand, if the values of the function on either side of XQ are greater 
than /(XQ), 
the function has a local minimum. Maxima and minima are called extreme 
values, and the values of JC for which/(x) takes on an extreme value are cdHedextreme 
points. 
Suppose that f(x) 
has a continuously varying first derivative in an interval that 
includes XQ. l(f{xo) is a maximum, the first derivative must then change from positive to 
negative. Conversely, if/(:x:o) is a minimum, the first derivative must change from 
negative to positive. These facts relate, of course, to the basic definition off'(x) 
as the 
slope of the curve y =fix) at the point XQ. Under the preceding conditions then,/'(x) is 
zero at the point XQ where the curvature off(x) 
changes direction. 

302 
APPENDIX A 
More generally, f'{x) = 0 is the necessary condition for a stationary point (that 
includes the instances of maxima and minima as special cases). At a stationary point, the 
function may have either a maximum, a minimum, or neither. For example, each of the 
following functions displays a stationary point at x = 0, but we note that for 
f{x) = x^, 
the stationary point is a minimum 
f{x) = —(x^), 
the stationary point is a maximum 
f(x) = x^, 
the stationary point is neither a maximum nor minimum"* 
Finding local extreme points for differentiable functions of one argument involves the 
following steps: 
1. Since local extreme points can only occur at stationary points, where /'(x) = 0, 
first find all solutions to the equation 
2. 
For each of the stationary points obtained from the above solutions, compute 
higher-order derivatives / " (x), f" (x), etc., as needed, so as to find the value of the 
/owesr-order derivative that is not zero at the stationary point in question. 
3. 
Examine the lowest-order derivative that is nonzero to determine if 
a. its order is even. 
(i) 
If the value of the derivative of this order is positive, the function 
exhibits a local minimum at the stationary point under evaluation. 
(ii) 
If the value of the derivative of this order is negative, the function 
exhibits a local maximum at the stationary point under evaluation. 
b. its order is odd; if so, the stationary point is an inflection point.^ 
As a simple numerical illustration of the above procedure, consider the function 
/ ( X ) = X^ + 2JC2+X 
Its first derivative is 
/'(x) = 3x2+4x + l 
Next, we solve for the stationary points by setting/'(x) equal to zero: 
/ ' ( X ) = 3 X 2 + 4 J C + 1 = 0 
and find, as solutions, 
Xj = — 1; 
^2 ~ ""3 
Next, let us find the second derivative of/(x). This is, of course, the derivative of/'(x). 
That is, 
/"(x) = 6x + 4 
* In this case the stationary point is an inflection point where the first derivative f {XQ) is zero 
and, furthermore, the second derivative f"{x) changes sign as the function goes through x^. 
^ As noted earUer, the necessary condition for XQ to be a stationary point is that/'(Xo) = 0. The 
sufficient conditions appearing above can be obtained by examining successive terms of the Taylor 
series expansion (Lang, 1964). 

A.2. 
DIFFERENTIATION OF FUNCTIONS OF ONE ARGUMENT 
303 
minimum 
-40-f 
X2=-1/3 
Fig. A.2 
Graph of function / (jc) =^ x^ + 2x^ + x (-4 < x < 4). 
We then evaluate /"(x) at the first stationary point Xi to see if the value of f\x) 
is 
nonzero: 
/ " ( - I ) = - 6 + 4 = - 2 
Since /"(—I) = - 2 is nonzero, the order of the first nonzero derivative is even; moreover, 
we have a local maximum at this point since/"(—I) is negative. 
Next, the same thing is done for the second stationary point X2 '• 
/ V ^ ) = - 2 + 4 = 2 
Since /"(-l/3) = 2 is also nonzero, the stationary point is an extreme point; since 
/"(—1/3) is positive, we have a local minimum at this point. 
Figure A.2 shows a plot of the function 
/(x) = x^ + 2x2+jc 
over the (illustrative) domain —4 < jc < 4. At the point Xi = —1 ,/(x) = 0, which is a local 
maximum. At the point X2 = - L / W = -4/27, which is a local minimum. 
It should be kept in mind that what is being found are local stationary points in which 
interest centers on the behavior of the function in a relatively small interval. As indicated 
in Fig. A.2, neither Xi nor X2 represents a global extremum over the domain (—4 < x < 4) 
of interest. 
Figure A.3 further illustrates the distinction between global and local extrema. If we 
consider local extrema within the interval of XQ < X < ^6, a local maximum is found at 
X2 and a local minimum at X4. Also, other stationary points (viz., inflection points) are 
found at Xi, X3, and Xs. However, when the end points XQ and x^ are also considered, 
the global maximum turns out to be at x^, while the global minimum is still at X4. The 

304 
APPENDIX A 
Fig. A.3 
Global versus local extrema. 
search for global extrema can be quite tedious, particularly if various discontinuities 
appear in the function.^ However, further discussion of this specialized process exceeds 
our intended coverage. 
A.3 
DIFFERENTIATION OF FUNCTIONS OF TWO 
ARGUMENTS 
The next step in this review discussion of the calculus involves functions of two 
arguments: 
z=f{x,y) 
In the case of functions of one argument we were able to represent / (x) versus x by a 
curve in two dimensions. Analogously, in the present case f{x,y) 
versus x and y is 
represented by a surface embedded in three dimensions. Graphical devices, such as 
contour lines and projective drawings, are useful in portraying certain three-dimensional 
relationships in two-dimensional space. 
In this part of the appendix we discuss the concepts of level curve, partial 
differentiation, unconstrained optimization, and optimization subject to equality 
constraints. 
A.3.1 
Level Curves and Partial Differentiation 
The notion of level curves is employed in a variety of appUcations. For example, in 
map making one may use a series of contour lines to represent altitudes, as schematized in 
Panel I of Fig. A.4. We note that the level of 300 feet consists of all of those points on 
the hill that involve 
/(x,7) = 300 
^ If discontinuities appear, the function must be examined at each of these points (as well as at end 
points and stationary points). 

A.3. 
DIFFERENTIATION OF FUNCTIONS OF TWO ARGUMENTS 
305 
North 
^—/--h 
/ 
East 
A: = 16 
South 
Fig. A.4 Illustrations of level curves. Key: I, altitude of land; II, z = x ' + J^^ 
If one were to hike around the hill on that level curve, one would remain at a constant 
height of 300 feet. Similarly, other contour lines show other altitudes, such as 200 feet, 
100 feet, and sea level. 
Level curves appear in many fields, such as pictorial displays of air pressure (as 
isobars), temperature (as isotherms), and consumer utility (as indifference curves). As 
inferred from Panel I, we could "build up" the hill if we imagined that each level curve 
were made of cardboard and we stacked one piece on top of another in building up the 
surface. 
Panel II shows level curves for the function 
In this case we can find any level curve of interest by choosing a fixed number k and then 
finding the set of points Xi and y^ for which 
For example, if we set /: = 9, we have 
If we let j^ = 0, then x = ±3; conversely if x = 0, then y - ±3. As can be inferred from 
examining the various level curves of Panel II in Fig. A.4, the surface of the function 
looks like that of a bowl with its center point at the origin of the x, j plane. The level 
curves are, of course, circles of varying radius. 
As a third example of level curves, consider the surface, depicted in Panel I of Fig. A.5, 
representing the function 
fix, j ) = Zxy 
where we assume that x,y>^. 
Panel I shows the surface itself as a conelike figure that is 
cut in half.^ Panel II shows selected level curves for 
/:i=0; 
/:2 = 1; 
A:3 = 3; 
A:4 = 6; 
ks=% 
ke = l2 
We retum to this function after a brief review of partial differentiation. 
^ The plane ABC, depicted as a slice through the surface in Panel I, and dotted line>lC in Panel II 
are discussed later (in Section A.3.3) in the context of Lagrange multipliers. 

306 
APPENDIX A 
/r, = 12 
Fig. A.5 Surface and level curve plots of f(x, y) = 3xy {x, y > 0). Key: I, response surface,/(x, 
y) = '}>xy (x, y > 0); II, level curves, k - Zxy. 
Partial differentiation is a straightforward generalization of simple differentiation. 
Partial derivatives are found by differentiating the function fix, y^ with respect to each 
variable separately. The variable not involved in the differentiation is treated as a 
constant. For example, for the function portrayed in Fig. A.5, we have 
a/ 
f{x,y)^2>xy\ 
3JC 
^y 
Partial derivatives are usually denoted by 3//3x or/^. 
Second-order partial derivatives involve differentiation of first-order partial derivatives, 
since partial derivatives are, themselves, functions. That is. 
ax 
and 
9 j 
are second-order derivatives, denoted by the symbols 
aV. 
aV. 
^(^,>^) 
or 
f^A^.yY 
^ ( x , ; ; ) 
or 
fyyix^y) 
Moreover, the function (3//3x)(x,3^) may also be differentiated with respect to;;, and 
(V/Bj^Xx, y^ may be differentiated with respect to x. These are called mixed partial 
derivatives and are usually denoted by 
aV 
(or/-^ ) 
and 
aV 
(or/vx) 
respectively. If the mixed partial derivatives of/(x, y) are continuous, then 
ay ^ ay 
3x 3;; 
dy bx 

A.3. 
DIFFERENTIATION OF FUNCTIONS OF TWO ARGUMENTS 
307 
and the order in which differentiation proceeds is irrelevant. Continuing with the example 
we have 
— = 3jv; 
—T ^ 0 ; 
=3 
bx 
dx^ 
bx by 
¥ 
, 
aV ^ 
aV 
-
— =3x; 
—-r=0; 
^ ^ =3 
oy 
dy^ 
by bx 
As an additional example, consider the more elaborate polynomial 
f(x,y) = 2x'^ •^y'^ + 3xy+x-y 
+ 3 
The first- and second-order partial derivatives are 
bf 
b^^f 
b^f 
— = 4x + 3j; + l; 
^ = 4 ; 
-r-4- 
=3 
bx 
bx^ 
bxby 
bf 
b'^f 
b'^f 
by 
by 
by bx 
Since all second-order partial derivatives are constants, all third- (and higher) order partial 
derivatives are zero. 
A.3.2 
Unconstrained Optimization of Functions of Two Arguments 
Analogous to the case involving functions of a single argument, conditions for local 
extrema can be listed for the two-argument case. To be specific, let us continue to 
consider the case of the function 
/(x,>;) = 2x^ + j ^ + 3xy ^rx-y + 3 
A necessary condition that (XQ, j'o) be a local stationary point is that the following 
equations are satisfied: 
^ixo,yo) = 0; 
-^(.xo,yo) = 0 
In the above example we have 
^(x, J) = 4JC + 3;; + 1 = 0; 
^(x,y) 
= 3x + 
2y-l=0 
On solving these equations simultaneously we get 
X = 5; 
y = -l 
Therefore, (5, —7) is a stationary point. 

308 
APPENDIX A 
Sufficiency conditions for local extrema involving functions of two arguments are 
somewhat more complex than the counterpart case for one variable. To summarize what 
these are, one first sets up the determinant of second-order partial derivatives as follows: 
62 = 
which, in the illustrative problem, is 
4 
: 
aV 
aV 
aj^ax 
a^a^ 
= ( 4 x 2 ) - ( 3 x 3 ) = - l 
Next, we examine b^ = d^f/bx'^ = 4 and note that its sign is positive. Sufficiency rules, for 
the general case, can now be stated in terms of 61 and 62 • 
1. A local extreme point exists if 62 ^ 0-
a. 
The local extreme point is a minimum if 51 > 0. 
b. 
The local extreme point is a maximum if 51 < 0. 
2. 
A local extreme point does not exist if 62 < 0. 
3. 
A local extreme point may or may not exist if 62 =0. In general, additional 
examination of the function at the stationary point values is needed to see if an extreme 
point exists and, if so, whether it is a minimum or a maximum.^ 
In the illustrative problem we note that 62 = —1 (^O)^ a^^d, hence, no local extreme point 
exists for this function. 
Since quadratic functions of the general form 
f(Xy y) = ^^^ + bxy -^^ cy'^ -^ dx + ey+f 
appear so frequently in multivariate statistical work, it is useful to examine their 
properties more generally. First, we shall find, as noted earlier, that all second-order 
partial derivatives are constants. Stationary points are found by solving the equations 
a//ajc = 0, bf/by = 0, simultaneously: 
3/ 
-- = 2ax+by^d = 0; 
ox 
by = 2cy +Z?x + e =0; 
av 
bx^ 
av 
^y' 
= 2a 
••2c 
which can be expressed as 
2ax -^by = —d\ 
bx + 2cy = —e 
* In particular, if both b^f/dx^ and d^f/by^ are equal to zero, the stationary pomt is not a local 
extreme point. If b^f/dx by = d^f/dy ax = 0 and if d^f/bx^ and d^f/dy^ have the same sign, the 
stationary point is a local extreme point. Still, specific examination of the function is generally needed 
to see whether the point is a minimum or a maximum. 

A.3. 
DIFFERENTIATION OF FUNCTIONS OF TWO ARGUMENTS 
309 
to obtain the solutions 
2cd-eb 
2ae-bd 
b -4ac 
b^-4ac 
Moreover, since the second-order partial derivatives are 
ay 
dx 2 ~ 2a; 
= 2c; 
9V 
9'/ 
dx by 
by bx = b 
we have, as the expression for 62, 
5,= 
3V 
bx-" 
b'f 
aY 
dx by 
••4ac-b^ 
by dx by'^ 
If 4ac-b^ > 0, an extreme point is indicated. That is, 
1. if §2 = 4ac-b^ > 0, the stationary point is an extreme point. 
a. 
If 5i = b^f/bx^ = 2a>0, 
then the extreme point is a local minimum. 
b. 
If 51 = 2fl < 0, then the extreme point is a local maximum. 
2. if52^0jno extreme point exists. 
3. if 62 = 0, additional examination of the function is needed to see if an extreme 
point exists and, if so, whether it is a minimum or a maximum. 
In the preceding numerical case, XQ ~ 5 and>'o = —7. Substituting these values leads to 
4ac-62 = 4(2)(l)-(3)2=-l 
and the stationary point (5, —7) is neither a minimum nor a maximum, as was indicated 
earlier. 
A.3.3 
Constrained Optimization and Lagrange Multipliers 
In the previous section our concern was with finding local extrema without 
constraining the domain over which jc and y, the two arguments of/(jc, y), might vary. 
However, cases frequently arise where we are interested in setting up certain side 
conditions that must be satisfied in the course of optimizing some function/(x, y). 
Optimization of functions subject to constraints is a vast topic which goes well beyond 
our coverage. Here we concern ourselves with only one technique, the method of 
Lagrange multipliers for optimizing functions subject to equality constraints. 
The basic idea behind Lagrange multipliers involves setting up a more general function 
that includes the constraint and optimizing this more general function. Suppose we have a 
function f(x, y) and a side condition, expressed aiSg(x, y) = 0. If so, we can define a new 
function, composed of three variables: 
w(^, y. X) 
=f(x,y)-Xg(x,y) 
where X is the Lagrange multiplier. (The variable X is an artificial variable that is 
employed to provide as many unknowns as there are equations.) Having set up the general 

310 
APPENDIX A 
function u{x, j , X), we can find its partial derivatives with respect to x, y, and X and set 
each of them equal to zero: 
bu 
du 
du 
— =0; 
^ = 0 ; 
^ = 0 
bx 
by 
b\ 
Any point that satisfies the above necessary conditions is a stationary point. We continue 
to assume, of course, that both f(x,y) 
and g(x, y), the latter being the constraint 
equation, are differentiable in the neighborhood of the stationary point. 
Let us illustrate the Lagrange multiplier technique by returning to the function 
f(x,y) = 3xy 
as first depicted in Fig. A.5. However, now let us impose the constraint equation that 
g(x,y) = 2x+y = 4 
or, equivalently, 
g(x,y) = 2x-\-y-4 
= 0 
As shown in Fig. A.5, application of this constraint results in a plane (labelled as ABC) 
intersecting the surface in Panel L Furthermore, y4C in Panel II represents the same linear 
constraint as a dotted straight line; this line is the locus of all points on the xy plane that 
satisfy the constraint equation 2x •\-y =4. 
We next set up the function w(jc, y, X) that incorporates the Lagrange multiplier 
u(x,7, X) = 
3xy-X(2x-^y-4) 
Then w(jc, y, X) is differentiated with respect to each argument, in turn, and each 
derivative is set equal to zero: 
Notice that the partial derivative with respect to X is just the constraint equation itself. 
The next step is to find the stationary point by solving the three preceding equations for 
X, y, and X. This is easily done by first expressing both y and x in terms of X: 
y = 2X/3; 
x = X/3 
and solving for X in the third equation: 
2(X/3) + 2X/3-4 = 0; 
X = 3 
We then find x and y to be 
x = l ; 
y = 2 
Also we note that in terms of the original function f(x,y) 
= 3xy, the value of the 
function at the stationary point (1, 2) is 
/(1,2) = 3(1)(2) = 6 

A.3. DIFFERENTIATION OF FUNCTIONS OF TWO ARGUMENTS 
311 
Panel II of Fig. A.5 provides a graphical representation of what is going on. First, we 
examine the constraint equation, represented by the dotted line AC = g{x,y) = 
2x +y = 4. The one level curve for which AC represents the tangent line is 
k4=f(x,y) = 6 
Thus, when the stationary point (JCQ, yo) = (1.2) is found with/(l, 2) = 6, we see that its 
tangent line is represented by the constraint equation g{x, y) = 2x -^y = 4. The value of 
the function/(I, 2) = 6 coincides with the highest level curve that can be reached via^C. 
Furthermore, in looking at the plane ABC, slicing through the surface in Panel I, we also 
see that the local extreme value is a maximum point B on the arch traced out by ABC. 
One additional point of interest concerns the value of X itself; in this case X = 3. If we 
examine the original function 
f{x,y) = 3xy 
and assume that the (negative) constraint equation 2JC+J^ = 4 were "relaxed" to 
2x + jv = 5, we would have an extra unit of (say) j to work with. If so, we could compute 
the value of f(x,y) 
before and after allowing for one unit increase in y. Letting XQ = 1, 
the X coordinate at the original stationary point, we have 
f{x^y) = 3(l)y = 3y 
f(x,y + l) = 3{l){y^l) 
= 3y^3 
Hence, an increase in f(x,y) 
of 3 units could be effected if one more unit of y were 
available. This is equal to X, the value of the Lagrange multiplier, as found in the earlier 
computations. 
In summary, Lagrange multipliers are useful in handling one (or possibly more) 
equality constraints in cases where it would be difficult to solve the problem via direct 
substitution of the constraint equation(s) into/(x, y).^ Since a necessary condition for a 
stationary point is that each first-order partial derivative equals zero, introduction of the 
Lagrange multiplier X adds a needed artificial variable to balance out the number of 
equations with the number of unknowns. In terms of the numerical example 
u(x,y, X) = 3xy-\(2x 
+y-4) 
we see that partial differentiation with respect to x, y, and X separately leads to three 
equations in three unknowns. 
Furthermore, if the constraint equation is always met, then the term (2x +y —4) in 
the general function w(x, y, X) will always equal zero so that u{x, y, X) will behave in the 
same way as/(x, y), the original function.^^ 
^ With only two original variables, one would not generally deal with more than a single constraint 
equation since two constraint equations in two variables would normally have only a single point in 
common. With a large number of original variables, however, two (or more) equality constraints might 
be employed. 
^^ Sufficiency conditions for local extrema in the context of Lagrange multiphers are found in 
Hancock (1960). 

312 
APPENDIX A 
A.4 
SYMBOLIC DIFFERENTIATION 
The most complex cases in partial differentiation and function optimization involve 
functions of several arguments. Often, this situation is portrayed by vector and matrix 
notation. The term "symbolic differentiation" has been coined to refer to partial 
differentiation of vector or matrix functions whose results are also described in the same 
format. For example, symbolic differentiation of a function with respect to a vector 
involves finding the partial derivative of the function with respect to each entry of the 
vector; the partial derivatives themselves are then arranged in vector form. 
To illustrate, if y =/(x), where x is a column vector with elements Xi, X2,. . . ,x„, 
then one can express its symbolic derivative by means of the column vector 
ax 
df/dx, 
a//ax2 
Notice that each entry of bf/dx is a partial derivative of/(x) with respect to a specific 
variable. By the same token, one can find a row vector of partial derivatives:^^ 
ax 
In applied multivariate analysis, we frequently have occasion to find the symbolic 
derivatives of functions that are bilinear or quadratic forms, such as 
u = x'Ay 
(where A may be rectangular) 
V = X'AX 
(where A is square, nonsymmetric) 
w = X'AX 
(where A is symmetric) 
t = x'lx 
(where I is the identity matrix) 
We first consider symbolic differentiation with respect to vectors. We can then turn to 
problems of optimization of functions involving multivariable arguments. 
A.4.1 
Symbolic Differentiation with Respect to Vectors 
If we take one of the simplest cases first, namely, the linear combination j ; = a'x, 
where a' is a row vector of coefficients, the symbolic derivative of y with respect to x is 
simply the row vector 
'* Note that 8x' appears as a row vector in the denominator since bf/bx 
is being 
expressed explicitly in this form. That is, we shall adopt the notation of ax or dx' on the basis of how 
the final vector of derivatives is displayed—in column or row form, respectively. 

A.4. 
SYMBOLIC DIFFERENTIATION 
313 
This follows from the fact that we can write y expHcitly as 
y =aiXi +^2^2 + * • • -^a^Xn 
Then, 
each 
partial 
derivative 
of 
y 
is 
found, 
in 
turn: 
dy/bxi=ai, 
dyldx2 =a2,. . ., byl^x^ =a„. These elements can then be arranged in the row vector 
a. 
Next, suppose we have the bilinear form 
u = x'Ay 
which can be written out explicitly (for A of order m x /?) as 
u=x^anyi 
+^1^12^2 + • • • -^^xainyn 
+ X2^2lJ^l +^2^22;^2 + • • 
'"^^idlnyn 
+ • • • 
The partial derivative bu/dxi ofu = x'Ay with respect to the first element Xi is 
bu 
3 ^ =^iiJ^i +«i2;^2 + • • • +^i«7« 
which can be written as the scalar product 
du 
, 
By the same procedure the other partial derivatives are obtained: 
du 
, 
9M 
, 
which can all be arranged in the m x 1 column vector 
du 
By a similar rationale we can obtain bu/dy as the I xn row vector 
bu 
,. 
—, =xA 
9y 
By taking appropriate transposes of the two preceding equations, we could also write 
bu 
, 
bu 
. 
as a row and column vector, respectively. 

314 
APPENDIX A 
By similar reasoning we can find that the symboUc derivative of 
V = X'AX 
(for square, nonsymmetric A) 
with respect to x is the column vector 
dv 
dx ^(A + A')x 
Furthermore, the symbolic derivative of 
w = X'AX 
(for symmetric A, so that A = A') 
is the column vector 
bw 
T - =2Ax 
ox 
In particular, if A = I, we have the special case of the sum of squares 
t = x'lx = x'x; 
As a numerical illustration of the case involving a bilinear form, consider the function 
yi 
T— = 2x 
3x 
" = (^1,^2) 
1 
3 1 
2 4 
3 
yi 
ys 
= xiyi +2x2yi +3xij^2 +4^23^2 +^i>^3 + 3x2^3 
If we differentiate u with respect to x, we have the column vector 
yi +372+J^3 
2yi + 4^2 +33^3 i 
Ox :Ay = 
In a similar way, we could find symbolic derivatives of other functions with respect to x 
or y. As would be surmised, however, no new principles are involved.^^ 
A.4.2 
Some Aspects of Optimization in Matrix Notation 
Extreme values can be found for functions of vectors in much the same way as 
described earlier for functions of scalars. In particular, suppose we had the function 
7 = 2JCI^+3X2^ 
subject to the constraint equation 
g{x) = Xi-X2-l 
= 0 
^^ Bilinear and quadratic forms can also be differentiated with respect to A, the matrix of the 
form. For example, for nonsymmetric A, (a/aA)(x'Ax) is xx'. However, this more advanced topic 
exceeds our scope. The interested reader is referred to Tatsuoka (1971). 

A.4. 
SYMBOLIC DIFFERENTIATION 
315 
We could, of course, apply the same Lagrange multiplier procedure described in Section 
A.3.3 to find the extreme values (if any) of this function. However, let us express the 
equations in vector or matrix form and work through their solution in this format: 
y = X'AX = (:vi,X2) 
2 0 
0 3 
subject to 
g = c x - l =0=^(1,^1) 
X2 
Xi 
X2 
- 1 = 0 
In matrix equation form, the general function is, analogously, 
u - X'AX—X(c'x—1) 
We set its partial derivatives equal to the zero column vector: 
3w 
ax = 2Ax-Xc = 0 
and find the vector solution 
x= XA *c/2 
Moreover, since c'x —1=0, then c'x = 1, so that after multiplying both sides of the 
preceding equation by c', we have 
1 = c'x = XC'A ^C/2 
and X can then be found from 
X=2(C'A-^C)-' 
Substituting this expression for X in x = XA"^c/2 gives us 
x = (c'A-^c)-^A-^c 
In the simple numerical illustration shown above, we have 
c' 
A-^ 
c 
X = 2^(l,-1) 
12 
1 
^ = 1 - 2 
1/2 
0 
0 
1/3 
1/2 
0 
0 
1/3 
12/5 
3/5 
-2/5 
At the stationary point x' = (3/5, -2/5), the value of the function is 
;; = 2(3/5)2 + \-2\Sf 
= 0 

316 
APPENDIX A 
and it is noted that the constraint equation 
^(x) = 3 / 5 - ( - 2 / 5 ) - l = 0 
is alsosatisfied.^^ 
In summary, use of matrix notation provides a compact way to set down procedures 
for function optimization, in this case optimization under a constraint equation. 
A.4.3 
Conditions for the Optimization of Functions Involving 
Multivariable Arguments 
In preceding sections of the appendix, necessary and sufficient conditions for 
identifying local extreme points have been listed for the single-argument and (uncon-
strained) two-argument cases. Things become considerably more compHcated when we 
consider functions of multivariable arguments. Accordingly, we do not delve into the 
topic in much detail; in particular, all proofs are omitted. The reader interested in a more 
detailed discussion is referred to books by Beveridge and Schechter (1970) and Wilde and 
Beightler(1967). 
In the case of a multivariable function, the necessary condition for a stationary point 
continues to be the vanishing of all first-order derivatives. That is, at a staionary point, we 
have the condition 
- ^ =0; - ^ = 0 ; . . . ; - ^ = 0 
This condition holds for either unconstrained or constrained functions (in the context of 
Lagrange multipliers). 
However, as was observed in the cases of one or two arguments, a multivariable 
function does not necessarily have a local extremum at the stationary point of interest. 
To examine sufficiency conditions for a local extreme point, use is again made of the 
determinant of second-order partial derivatives: 
ay 
ay 
3V 
3V 
3x2 ajci 
3V 
3V 
3Xi dXn 
ay 
3x2 3x„ 
3V 
3x„2 
3x„ 3xi 
bXyi 3x2 
' Although we do not delve into details, the stationary point (3/5, —2/5) is a minimum. 

A.4. 
SYMBOLIC DIFFERENTIATION 
317 
As was the case with two arguments, we set up the principal minors of 5„ as follows:'" 
§1 = 9^/ 
5,= 
9V 
ay 
dX2 dXi 
ay 
dxi bx2 
9V 
6.= 
9V 
aV 
9V 
9x,' 
9V 
3JC2 
3^1 
JIL. 
dxi bx2 
9V 
9X2^ 
9V 
3JCI 
8x3 
9V 
8x3 3^1 
3x3 3x2 
. .. , up to, and including, 6„. 
Having done this, we evaluate each of the n determinants. In order for a stationary 
point to be an extreme point, all of the n determinants must be nonzero. A local 
minimum is distinguished from a local maximum in terms of the pattern of signs of the 
(evaluated) determinants: 
1. If 6y > 0 for «/// = 1, 2 , . . . , «, then the stationary point is a local minimum. 
2. 
If 61 < 0, 62 > 0, §3 < 0, 64 > 0, . . . , then the stationary point is a local 
maximum. 
3. 
If neither situation occurs, one must examine the specific nature of the stationary 
point by computing values of the function in the neighborhood of the point. 
Notice, then, that these conditions generaUze what was discussed earlier for functions of 
one and two arguments.^^ 
In case the function is subject to an equality constraint of the type illustrated in the 
context of Lagrange multipliers, the necessary condition for an extreme point involves 
finding a vector XQ that satisfies the n + 1 equations 
3/(x) 
\ dgix) 
= 0; 
^(x) = 0 
3x 
3x 
that are obtained by setting the derivatives of 
w(x,X)=/(x)-Xg(x) 
with respect to x and X each equal to zero. 
Sufficiency conditions for a local extremum in the case of Lagrange constraints 
become rather complex, particularly if more than one constraint equation is involved. 
Accordingly, the interested reader is referred to more specialized books on the subject, 
such as the book by Hancock (1960). 
^* By the term principal minors is meant successive determinants computed for submatrices of 
order 1, order 2, etc., formed along the main (principal) diagonal of the original n xn matrix. 
^^ It should also be mentioned that a rather elegant approach to examining sufficiency conditions 
utilizes the matrix of partial derivatives as the matrix of a quadratic form. One then checks on whether 
the form is positive definite, negative definite, etc., and the type of definiteness is related to the type 
of extremum represented by the stationary point. This approach is fully compatible with the principal 
minor procedure, described above. 

318 
APPENDIX A 
A.5 
APPLICATION OF THE CALCULUS TO 
MULTIVARIATE ANALYSIS 
At this point we have discussed (albeit selectively and rapidly) a number of concepts 
from the calculus that relate to the development of various matrix equations that arise in 
solving problems in multiple regression, principal components, and multiple discriminant 
analysis. It is now time to examine the specific nature of these central equations in 
multivariate analysis. 
A.5.1 
Multiple Regression Equations 
The so-called normal equations of multiple regression theory represent a straight-
forward application of function minimization that utilizes the least-squares criterion. In 
multiple regression we have the case in which the matrix equation 
y = Xb 
has more equations (one for each case) than unknowns. As recalled from Chapters 1 and 
6, X is the data matrix of predictors (augmented by a column vector of unities); y is the 
data vector representing the criterion variable; b is the to-be-solved-for vector of 
regression coefficients (including the intercept term); and = denotes least-squares 
approximation. 
The vector of prediction errors can be written as 
e = y - y 
where y denotes the set of predicted values for y. As we know, the least-squares criterion 
seeks a vector b that minimizes 
Since y = Xb, we have 
/=(y-y)'(y-y) 
/=(y-Xby(y-Xb) 
= y'y-b'X'y-y'Xb + b'X'Xb 
= b'X'Xb-2y'Xb + y'y 
where y'Xb = b'X'y since each term denotes the same scalar. Our objective is to find a 
vector of parameters b that minimizes/. This suggests finding the symboHc derivative and 
setting it equal to the 0 column vector: 
~ =2X'Xb-2Xy = 0 
We note that X'X in b'X'Xb is symmetric, with derivative 2X'Xb. Furthermore, we 
observe that the partial derivative with respect to the row vector b' is being found; hence, 

A.5. 
APPLICATION OF THE CALCULUS TO MULTIVARIATE ANALYSIS 
319 
we take the transpose of 2y'X to obtain 2XV, the second term in the preceding equation. 
Dividing both sides by 2 and transposing leads to 
X'Xb = X'y 
and solving for b, we get 
b = (X'X)-^XV 
We now recognize the matrix equation as that appearing in the discussion of multiple 
regression in Chapter 6. Although no check of sufficiency conditions has been made here, 
it tums out that b is the vector of parameters that does, indeed, minimize the function/. 
A.5.2 
Principal Components Analysis 
In principal components analysis, we recall that interest centers on rotation of a 
deviation-from-mean data matrix Xj so as to maximize the quadratic form 
/=t'(Xd'Xd)t 
where we denote the SSCP matrix by X^'X^, the minor product moment of Xj. 
(Alternatively, we could use the raw cross products, covariance, or correlation matrix.) 
Furthermore, we want to restrict the vector t to be a set of direction cosines that define 
the vector of linear composites: 
y = Xjt, 
where 
t't = 1 
If we let A = Xj'X^, the principal components problem is to maximize 
/ = t'At 
subject to the constraint that t't = 1. 
Based on our discussion of Lagrange multipliers, we can formaUze the task by writing 
w = t'At-X(t't-l) 
where t't - 1 = 0 represents the constraint equation. As we know, the problem is to find 
the symbolic partial derivative of u with respect to t and set this equal to the 0 vector. 
Remembering that A is symmetric, we obtain 
bu 
T- =2At-2Xt = 0 
Next, dividing through by 2 and factoring out t, we get 
(A-XI)t = 0 
This represents the necessary condition to be satisfied by a stationary point t in which the 
constraint equation t't = 1 is also satisfied. Again, we do not delve into the more complex 
topic of checking on sufficiency conditions, other than to say that the eigenvector ti 
associated with the largest eigenvalue Xi of A is the vector of direction cosines that 
maximizes the function u. 

320 
APPENDIX A 
A.5.3 
Multiple Discriminant Analysis 
As recalled from Chapter 6, in multiple discriminant analysis we seek a vector vwith 
the property of maximizing the ratio 
_ V'AV 
A — —-, 
vWv 
where A is the among-group SSCP matrix and W is the pooled within-group SSCP matrix. 
(Again, we could place some restriction on the vector v, such as vV = 1.) Note, however, 
that X, the discriminant ratio in the present context, is simply the quotient of two 
functions (as illustrated in Table A.2). We can then find the symbolic derivative of X with 
respect to v, by means of the quotient rule, and set it equal to the 0 vector: 
3X ^ 2[(Av)(v^Wv)-(v^Av)(Wv)] 
Bv 
(v'Wv)' 
This can be simplified by dividing numerator and denominator by (v'Wv) and making the 
substitution 
V'AV 
X — —;; 
vWv 
to obtain 
2[Av-XWv] 
v'Wv 
Next, we divide both sides by the scalar 2 and further simplify to 
(A-XW)v = 0 
Next, assuming that W is nonsingular, we have the familiar expression of Chapter 6: 
(W-'A-XI)v = 0 
where, as we know, W~^A is nonsymmetric. Again, we omit discussion of the sufficiency 
conditions, indicating that a maximum has been found. Suffice it to say that all three 
procedures: 
1. 
multiple regression, 
2. 
principal components analysis, and 
3. 
multiple discriminant analysis 
involve aspects of the calculus that deal with the optimization of functions of 
multivariable arguments. The concept of symboUc differentiation is central to the topic as 
well as the techniques of function optimization, either unconstrained or constrained 
optimization, as the case may be. 
A.6 
SUMMARY 
This appendix has dealt with those aspects of the calculus—particularly symbolic 
differentiation and optimization theory—related to the matrix equations that appear in 

REVIEW QUESTIONS 
321 
various multivariate methods, such as multiple regression, principal components, and 
multiple discriminant analysis. 
The review was brief and selective. We first discussed the differentiation of functions 
of one argument, including the statement of necessary and sufficient conditions for local 
extrema. This was followed by a similar discussion of the case involving functions of two 
arguments. Also, the technique of Lagrange multipliers was introduced at this point. 
We next described the most general case of functions of multivariable arguments and 
the concept of symbolic differentiation. SymboUc derivatives of common matrix 
functions were illustrated, and necessary and sufficient conditions for local extrema of 
multivariable functions were also listed. We concluded the appendix with applications of 
the calculus to the derivation of matrix equations in multiple regression, principal 
components, and multiple discriminant analysis. 
REVIEW QUESTIONS 
1. 
By means of the chain rule, find the derivative of the following functions: 
a. 
ln(2x-x^) 
b. 
l - 2 x 
c. 
e 
x^ + 4 
2. 
Find (and identify) extreme points for the function 
6x 
2 x ^ 1 
f(x)-
JC^+ 1 
over the domain—2 < jc < 2. 
3. 
Find the partial derivative of/(x, y) a t / ( I , 3) where 
/(jc, 7) = jc^ + 2JC + 4j; + ln(x^ + y'^) 
4. 
Find the minimum of f(x, j ) = 2x^ + 4x + 87 + j ^ . 
5. 
Find (and identify) a stationary point of the function 
subject to the constraint 
x^ly^l 
6. 
Find the derivative with respect to x of the quadratic form 
2 
3 
3 
1 
^4 
2 
Evaluate the derivative at x' = (3, 1, 2). 
7. 
If ^(x) = X'AX + b'x + c where A is symmetric, then it can be shown that the 
derivative of this general quadratic function with respect to x is 
^(x) = (xi, X2, X3) 
4] 
2 
3J 
pi 
X2 
L^3_ 
ax 
2Ax + b 

322 
APPENDIX A 
Furthermore, a stationary point is given by 
2AxH-b = 0; 
x = - ^ A ~ ^ b 
If 
x' 
A 
X 
b 
X 
c 
(xi, X2) 
1 
2 
ij 
r^i 
L^2 
+ (2,2) 
JCi 
X2 
is the function ^(x), find (and identify) a stationary point of ^(x). 

A P P E N D I X B 
Linear Equations and Generalized Inverses 
B.l 
INTRODUCTION 
In various sections of the book, and particularly in Section 4.6, we have discussed how 
one solves a set of simultaneous linear equations where the matrix of coefficients has a 
regular inverse. Moreover, the pivotal method has been described as an illustrative 
computational procedure for obtaining the desired inverse. 
In this appendix interest centers on a matrix of coefficients A for which no regular 
inverse A"^ exists. That is, A may be rectangular or, even if square, it may be singular.^ 
The generalized inverse is a concept that provides a way to solve a set of consistent linear 
equations in which a regular inverse does not exist. As we shall note later, several 
different types of generalized inverse have been defined, although we concentrate here on 
only two variations, the Moore-Penrose inverse (Penrose, 1955) and the g inverse (Rao, 
1962). 
Before discussing generalized inverses, we provide a review of the types of solutions: 
(a) none, (b) one, or (c) infinitely many, that one can obtain in attempting to solve a set 
of simultaneous linear equations. Aspects of homogeneous equations and nonhomo-
geneous equations are described and illustrated numerically. Finally, a general method is 
evolved for solving sets of equations. 
We then introduce the topic of generalized inverse in terms of a set of properties that 
such inverses are designed to satisfy. Following this, the Moore-Penrose type of inverse is 
defined and related to the concept of basic structure (or singular value decomposition) 
discussed in Chapter 5. 
The second type of inverse, called the g inverse, is then introduced. This inverse is 
required to satisfy only one of the Penrose properties and, in practice, is easier to 
compute. Illustrations of its computation are presented, and this type of generalized 
inverse is related to procedures for solving linear equations. In so doing, a general 
procedure for computing inverses—regular or generaHzed—is described and related to 
earlier material involving the solution of simultaneous equations. 
* That is, its determinant is zero. 
323 

324 
APPENDIX B 
B.2 
SIMULTANEOUS LINEAR EQUATIONS 
In the examples considered in the book we often had occasion to solve the system of 
equations 
Ax = b 
by means of matrix inversion. Recall that A is the matrix of coefficients, x is the vector 
of unknowns, and b is the vector of constants. In this case A was n xn and r(A)=n. That 
is, A was square and nonsingular. The pivotal method was employed as a general solution 
technique. 
However, suppose A is either rectangular or square singular so that a regular inverse 
does not exist. What happens then? Before launching into this topic, let us review some of 
the basic results related to solving a system of simultaneous linear equations. First, let us 
consider the set of equations 
3xi +X2 = 5 
5JCI +2X2 = 9 
This set of equations, as could be easily verified, has the solution Xi = 1, X2 =2. 
Furthermore, the solution is unique—only that specific set of values satisfies the set of 
equations. 
Let us next consider the simultaneous equations 
3JCI +JC2 = 
5 
6X1 + 2X2 = 11 
If we try to eliminate Xi by taking twice the first equation and subtracting it from the 
second, we get the result 
0= 1 
and, of course, something is wrong. This is most easily observed by noting that insofar as 
the left-hand side of the equations is concemed, the second equation is twice that of the 
first, but this relationship is not true for the right-hand side. The equations in this case are 
said to be inconsistent, and no solution exists. 
Next, let us take the three equations 
Xi +X2 + 3x3 = 8 
Xi + 2x2 +6x3 = 14 
X2 + 3x3 = 6 
We first eliminate Xi from the second equation by subtracting the first from the second 
to get the pair of equations 
X2 + 3x3 = 6 
X2 + 3x3 = 6 

B.2. SIMULTANEOUS LINEAR EQUATIONS 
325 
Ax = b 
Consistent 
Inconsistent set-
set 
no solution 
Unique 
Infinite number 
solution 
of solutions 
Fig. B.l Tree diagram of types of solutions to a set of linear equations. 
Note that these are identical. Thus, we have 
X2 = 6 — 3x3 
Xi = S-(6-3x3)-3x3 
= 2 
In this case, then, more than a single solution exists. For example, we have 
Xi = 2; 
X2 = 3; 
x^ - 1 
Xi = 2; 
^2 = 0; 
X3 = 2 
Xi = 2; 
X2 = - 3 ; 
X3 = 3 
and so on. 
The tree diagram in Fig. B.l shows the three cases of interest. We first want to 
examine whether the set of equations is consistent or not. If inconsistent, no solutions 
exist. If consistent, either a single (and unique) solution exists or an infinite number of 
solutions exist. 
The three theorems^ of interest in determining which condition prevails are: 
1. A set of linear equations is consistent if and only if the rank of the augmented 
matrix (found by appending the b vector to the matrix of coefficients A) is equal to the 
rank of the original coefficients matrix. 
2. 
A set of consistent linear equations has a unique solution if and only if the rank of 
the coefficients matrix A equals its order; that is, if and only if r(A) -n, where A is of 
order ny.n and n unknowns are present. 
3. 
A consistent set of linear equations, where A is of rank k, can be solved for k 
unknowns in terms of the remaining n -k 
unknowns if and only if the submatrix of 
coefficients (obtained from A) is of rank k. 
With these theorems to guide us, let us retum to the pair of equations 
DX \ T Xi ~ 
J 
6X1 + 2X2 = 11 
^ Proofs can be bound in Graybill (1969). 

326 
APPENDIX B 
and now form the matrix of coefficients A and the augmented matrix M: 
A = 
By inspection we see that the second row of A is twice the first; hence r(A) = 1. However, 
if we apply the reduction to echelon form procedure (from Section 4.7) to M we get the 
echelon matrix H^: 
3 
1 
6 
2 
; 
M = 
3 
1 
6 
2 
5" 
11 
HM -
1 
1/3 
0 
0 
5/3 
1 
We note that both rows of Hj^ have at least one nonzero entry. Hence, ^(HM) = ^(M) = 2 
while r(A) = 1; the set of equations is not consistent, and no solution exists. 
Next, talcing the equations 
3A: 1 + JC2 = 5 
5xi + 2JC2 = 9 
we have the matrices 
^3 f 
5 2_ 
After reduction to echelon form we obtain 
A = 
M = 
HA = 
1 
1/3 
0 
1 
HM -
1/3 
1 
5/3 
2 
and note, then, that r(A) = r(M) = 2, which is also equal to the order of A. In this case the 
equations are consistent, and a unique solution exists. 
Finally, if we take the set of the three equations 
Xi +X2 
+ 3JC3 = 
8 
jCj + 2x2 + 6 x 3 = 14 
X2 + 3x3 = 
6 
we have 
M 
8 
14 
6 
with associated echelon forms 
1 
1 
H A = 1 0 
1 
0 
0 0 
3 
3 
0 
; 
HM -
1 
1 3 
0 
1 3 
0 
0 
0 
8 
6 
0 

B.2. 
SIMULTANEOUS LINEAR EQUATIONS 
327 
and, we note that the rank in each case is 2, while the order of A is 3. Thus, we can solve 
for k = 2 unknowns in terms oin — k = 3 — 2 = 1 remaining unknown. The results 
suggest a general approach to solving sets of simultaneous linear equations. 
B.2.1 
A General Procedure for Solving Linear Equations 
As might be surmised at this point, in solving sets of linear equations we must 
determine whether a solution exists and, if so, whether the solution is unique or whether 
an infinity of solutions exists. The reduction of the matrix to echelon form via 
elementary row (or column) operations provides a practical way to find the rank of the 
coefficients matrix A and the rank of the augmented matrix M. As it turns out, however, 
reduction of a matrix to echelon form, followed by a few additional operations, provides 
us with a very general method for solving sets of simultaneous equations. As recalled, 
elementary row (column) operations permit 
1. the interchange of two rows (columns); 
2. 
the multipUcation of each entry in a row (column) by any scalar X #=0; 
3. 
the addition, to each entry of some row (column), of X times the corresponding 
element of some other row (column). 
Each of these operations can be carried out on the rows of A by means of premultiplying 
A by a matrix that, in tum, can be obtained by performing the given elementary row 
operation on the identity matrix.^ For example, let 
A = 
1 
4 
2 5 
3 
6 
and assume that we wish to 
1. interchange rows 1 and 2; 
2. multiply row 2 by the scalar 4; 
3. 
add twice row 3 to row 1. 
If these three operations are separately performed on the identity matrix I, of order 
3 X 3, we have, respectively, 
"l 
0 
2 
[o 
1 o~ 
1 0 
0 
0 
0 1 
; 
1 
0 
0 
0 
4 
0 
0 
0 1 
5 
0 
1 0 
0 
0 1 
The reader can convince himself that premultiplication of A by each of the three matrices 
above will effect the desired row operation. Similarly, elementary column operations can 
be carried out by performing the indicated operation on the columns of a 2 x 2 identity 
matrix and postmultipfying A by the appropriate matrix. Successive operations are 
represented, of course, by a set of matrices whose sequence is determined by the desired 
sequence in which the elementary operations are to be performed. 
^ In the case of elementary column operations, the matrix A is postmultiplied by the specified 
elementary column operation on the identity matrix. 

328 
APPENDIX B 
To illustrate the notion of a sequence of elementary row operations, let us 
simultaneously transform A and I by the three operations noted above, in the order 
given: 
Interchange rows 1 and 2: 
Ai = 
Multiply row 2 by the scalar 4: 
A.= 
Add twice row 3 to row 1: 
2 
1 
3 
2 
4 
3 
8 
4 
3 
5 
4 
6j 
5 
16 
6 
17 
16 
6 
Bi 
B,= 
0 
1 
0 
0 
4 
0 
0 
4 
0 
1 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
1 
0 
0 
1 
1 
0 
1 
Finally, we note that A3 can be obtained from the combined row operations—in the 
indicated order—hy 
B3 
0 
1 2 
A3=|4 
0 0 
0 
0 1 
A 
Ti 4 
2 5 
3 
6 
= 
8 
4 
3 
17 
16 
6 
Moreover, we also recaU from Chapter 4 that Bi, B2,.. ., is each nonsingular and that 
the rank of A is unaffected by elementary row (column) operations. 
With this review information out of the way, the formal method for solving sets of 
simultaneous equations can be stated. First, we start with the augmented matrix M. Then 
we carry out elementary operations to reduce M to echelon form H^. As a final step we 
carry out additional elementary row operations on H^ so as to obtain an identity matrix 
in the subset of columns corresponding to A, the matrix of coefficients. To illustrate, let 
us take the matrix M, as used earlier for the set of two simultaneous equations for which 
the unique solution was Xi = 1, JC2 =2. We then apply the echelon reduction procedure 
to get HM. That is. 
5^1 +2x2 = 9 
M = 
3 
1 
5 
2 
5 
9 
> 
Hjvi -
1 
1/3 
0 
1 
: 
5/3 
2 

B.2. 
SIMULTANEOUS LINEAR EQUATIONS 
329 
Next, subtract 1/3 of the second row of H^ from the first row to get 
N = 
1 
0 
0 
1 
Now, let us consider N in its original context of two linear equations 
Ixi + 0JC2 = 1 
Oxi + 1^:2 = 2 
with the desired solution Xi = 1, X2 =2. 
Next, let us take the case of the two inconsistent equations: 
3:JCI +X2 = 
5 
6X1 + 2^2 =11 
M = 
~3 1 
6 
2 
5~ 
11 
j 
HM -
'1 
1/3 
0 
0 
5/3 
1 
In this case we need go no further since in the second row of H^ we note the 
inconsistency: 
Oxi + 0^2 = 1 
which, of course, shows that this set of equations has no solution. Further evidence for 
this is found by examining the left-hand submatrix of H]vi;it is of rank 1 while HM itself 
is of rank 2. 
Finally, let us take the third example: 
Xi +X2 + 3^3 = 
8 
Xi + 2 x 2 + 6x3 = 14; 
X2 + 3x3 = 
6 
M = 
1 
1 3 
1 
2 6 
0 
1 3 
14 
6 
HM -
1 
1 3 
0 
1 3 
0 
0 
0 
8 
6 
0 
If we subtract the second row of H^ from the first, we get 
1 
0 
0 
: 
2I 
Ijci + 0x2 + 0x3 = 2 
N = p 
1 3 
; 
6 ; 
0x1 + 1x2 + 3x3 = 6 
[ 0 0 0 : 0 ] 
0x1+0x2 + 0x3 = 0 
In this case the best we can do is obtain a 2 x 2 identity matrix for the first two rows and 
columns of N. As illustrated earlier, we can then transfer X3 to the right-hand side, giving 
us 
Xi = 2; 
X2 = 6—3x3 
If we then treat X3 as a parameter, by setting it equal to (say) 73, we have 
Xi = 2; 
X2 = 6-373; 
^3 = 73 
or, in vector form, 
' 2 
x = | 6-373 
73 
and an infinity of solutions exists depending upon what value we choose for 73. 

330 
APPENDIX B 
B.2.2 
Other Cases 
In the cases examined so far we deah with square matrices A of order n xn and 
vectors x and b, each of order n x \. 
In the more general case, A can be of order mxn. 
First, let us assume that 
r(A)=r(M) =/:. If so, then at least one solution must exist. Next, let us assume that 
k = n, the number of unknowns. Since k cannot exceed m, the number of rows of A (or 
M), and^ = ^2, then either m = n or m> n.Um = n = k, then we know that A is square and 
nonsingular, and the solution is unique. However, if m > «, the echelon matrix HM will 
have m - k rows of zeros, and we can say that m - k equations are redundant. If so, we 
proceed as before and solve k equations in n = k variables. The submatrix N, of order 
k xk,is still nonsingular, and the solution is still unique. 
Next, suppose that k< n. If this case exists, then either k = m or k< m. (We know 
that k cannot exceed m.) If we assume that k = m< n,wQ shall have an infinite number 
of solutions, as illustrated earlier. That is, n - k of the unknowns can be treated as 
parameters. 
Finally, assume that k< m (and k< n). In this case, we have not only unknowns to 
spare but redundant equations as well. Not surprisingly, by following the formal method 
outhned earlier, we shall end up with an infinity of solutions and redundant equations in 
the bargain. To illustrate. 
Xi + ^ 2 + 3X3 = 
8 
JCi + 2 x 2 + 6 x 3 = 14 
X2 + 3JC3 = 
6 
2xi +3x2 + 9^3 = 22 
M = 
1 
1 
0 
2 
1 
2 
1 
3 
3 
6 
3 
9 
8 
14 
6 
22 
If we then reduce M to echelon form, we get 
HM -
1 1 
0 
1 
0 
0 
0 
0 
3 
3 
0 
0 
8 
6 
0 
0 
As can be noted from H^, r{k) =r(M) = 2, and at least one solution exists. The next step 
is to find an identity submatrix by further elementary row operations on H^, giving us 
N: 
I 
0 
0 
0 
0 
1 
0 
0 
0 
3 
0 
0 
2 
6 
0 
: 
0 

B.2. 
SIMULTANEOUS LINEAR EQUATIONS 
331 
with the solution 
x = l 6-373 
73 
as found earlier. We see in this case that the fourth equation is redundant with the others. 
As a matter of fact, it is simply the sum of the first two equations (whereas the third 
equation represents their difference). 
Thus, if we have m equations in n unknowns (of the form Ax = b) in which 
r(A) = r(M) = k, while k<n and k< m,'we have an infinite number of solutions in which 
n- k variables can be treated as parameters and m - k equations are redundant. In 
effect, then, the relationship between k and n deals with the question of a single versus 
infinite number of solutions, while the relationship between k and m concerns whether 
some of the equations (viz., m - k) are redundant. 
B.2.3 
Homogeneous Equations 
Up to this point we have been discussing the case of Ax = b, involving nonhomo-
geneous equations. Sometimes the multivariate analyst will encounter sets of linear 
equations of the form 
Ax = 0 
These are called homogeneous equations. First of all, we note that one possible solution is 
to let X = 0. That is, if we assign 0 to each unknown, the equation is satisfied, since 
AO = 0. This is called the trivial solution. 
Viewed another way, if we append the zero vector to A to get the augmented matrix 
M, then r(A) will always equal r(M), and the equations will always be consistent. Hence, 
we shall always have at least one solution, namely, the trivial solution. 
The basic question, then, becomes one of determining the conditions under which 
solutions other than the trivial one exist. As with the case for nonhomogeneous 
equations, the answer depends on the relationship between r(A)=r(M) = ^ and n, the 
number of unknowns. Since k cannot exceed n, we are left with the two cases: (a) A: = « 
and {b)k<n. 
The results in each case are contained in the following assertions: 
1. 
Given a set of homogeneous Unear equations Ax = 0, involving m equations in n 
unknowns, a unique (and trivial) solution x = 0 exists if r(A) = k = n. 
2. 
Given a set of homogeneous linear equations Ax = 0, involving m equations in n 
unknowns, an infinite number of solutions exist if r(A) = k<n. 
We can illustrate these cases by the following set of m = 3 equations inn = 2 unknowns: 
2JCI + X 2 = 0 
3JCI + 2 X 2 = 0; 
7xi + 4x2 = 0 
A = 
2 
1 
3 
2 
7 
4 

332 
APPENDIX B 
With homogeneous equations, there is no point in obtaining M, the augmented matrix, 
since the appended column would be the zero vector. Rather, we can reduce A itself to 
echelon form, so as to get 
^1 
1/2 
HA =10 
1 
0 
0 
From H^ we see that r(A) ~k = n =2. Next, if we subtract j times row 2 from row 1, we 
get 
N = 
with the trivial solution JCi = 0, ^2 =0. Moreover, the third original equation is redundant 
and, as a matter of fact, equals twice the first equation plus the second. 
An illustration of the second case, r(A) = ^ < AI, is the following set of m = 2 equations 
inn = 3 unknowns: 
2Xi +^2+3X3 = 0 
Xi •^X2-X2 
= 0 
As before we find the echelon form of A as 
'l 
1/2 
3/2' 
1 
0 
0 
1 
0 
0 
IXi + 0 X 2 = 0 
; 
0^1 + 1x2 = 0 
OJCI + 0 ^ 2 = 0 
HA = 
0 
1 
and note that A'(HA) =r(A) = 2 and, hence, k<n. Next, we find the identity submatrix 
for the first two rows and two columns by subtracting 5 of row 2 from row 1: 
N = 
1 
0 
0 
1 
The equations can now be written"* as 
Ixi +0^2 = -4x3 
Oxi + 1x2 = 5x3 
and if we let X3 = 73, we have the general solution, in vector form, as 
x = 
* Note here that the implied vector I _5jf^ J in the third column of the preceding matrix has simply 
been transposed to the right-hand side of the equation. 
- 4 7 3 
573 
73 
= 73' 
~-4~ 
5 
1 

B.2. 
SIMULTANEOUS LINEAR EQUATIONS 
333 
Note, here, that m<n; hence, k<n.lfthe 
number of equations is less than the number 
of unknowns, we must have an infinite number of solutions.^ 
One point of major difference between the present case and the counterpart case 
involving nonhomogeneous equations concerns the solution vector in situations involving 
k<n. In the case of homogeneous equations, we find that 
-473 
X = I 573 
73 
and observe that each entry of x involves the arbitrary parameter 73. Thus, if we set 
73 = 0, then X = 0, and the trivial solution is included. Also, in the present illustration, 
where we have only one parameter 73, each solution is a scalar multiple of each other 
solution. For example, if we let 73 = 1 and then let 73 = 2, we have 
Xi = 
- 4 
5 
1 
X2 = 
10 
2 
= 2xi 
In contrast, if we reproduce the solution 
x = l 6-373 
73 
in Section B.2.2 dealing with nonhomogeneous equations, we see that the first entry (2) 
does not involve 73. 
To sum up, if some x° 9^ 0 is a solution, then Xx° (where X is an arbitrary scalar) is 
also a solution in the case of homogeneous equations (a fact that was noted in Chapter 5 
in the context of matrix eigenstructures), provided that n-k= 
I. If more than one free 
parameter is found (i.e., n-k> 
\), then it no longer follows that all solutions are scalar 
multiples of each other. However, if x° 9^ 0 is a solution, it still follows that Xx° is also 
a solution. This can be easily seen by noting that 
XAx''=A(Xx°) = 0 
Again, as recalled from the discussion of eigenstructures in Chapter 5, if A is nonsingular 
and r(A) =k = n, then nontrivial solutions cannot exist. 
In summary, we can recapitulate the general method for solving either case: 
1. nonhomogeneous equations of the form Ax = b; 
2. homogeneous equations of the form Ax = 0. 
^ As observed, in the case of homogeneous equations, either one solution exists (i.e., the trivial 
solution) or an infinity of solutions exists, depending upon the relationship between matrix rank and 
number of unknowns. 

334 
APPENDIX B 
In the nonhomogeneous equations case the augmented matrix M is reduced to echelon 
form and then, via additional elementary row operations, to identity matrix form for the 
appropriate submatrix.^ In the homogeneous equations case the analogous operations are 
carried out on the coefficients matrix A. 
Figure B.2 recapitulates the various outcomes in tree diagram form. If we examine the 
case of nonhomogeneous equations first, we see that the primary outcomes are (a) none, 
(b) one, and (c) infinitely many solutions. We check r(A) versus r(M) to ascertain which 
condition prevails. 
Assuming that r(A) =r(M) = /:-and, hence, at least one solution exists—we check to 
see whether k = n, the number of unknowns. If so, then a unique solution exists. Next, if 
/: = m, all equations are independent, while if A: < m, some equations {m - k o{ them) are 
redundant. Part (a) designates the first case in the tree diagram, while Part (b) designates 
the second. 
\i k<n, 
an infinite number of solutions exist. Again, we check to see \{k = m or 
A: < m so as to see if the equations are either all independent or not. Similar remarks 
pertain to the case of homogeneous equations with the exception, of course, that just one 
(the trivial solution) or infinitely many solutions exist in this instance; that is, if the 
system is homogeneous, it is consistent. 
B.3 
INTRODUCTORY ASPECTS OF GENERALIZED 
INVERSES 
In Section B.2 a general method, utilizing reduction to echelon form followed by 
additional elementary row operations for finding an appropriate identity submatrix, was 
illustrated for solving sets of simultaneous equations. As was noted, provided that the 
equations are consistent, a solution—indeed an infinite number of solutions—can be found 
if A~^ the regular inverse of the coefficients matrix, does not exist. 
At this point our interest centers on cases in which A"\ in the usual sense, does not 
exist, and yet we would still like to solve the set of equations of interest. This is the type 
of problem that the concept oigeneralized inverse has been developed to solve. 
The literature on generalized inverses is of relatively recent origin, and its 
nomenclature and mathematical notation are not standard across authors. Two basic 
types of generalized inverse are discussed here: 
1. the Moore-Penrose inverse (sometimes referred to as the pseudoinverse), written 
asA^ 
2. 
the g inverse (sometimes referred to as the conditional inverse), written as A~. 
However, the reader should be made aware of the fact that many types of generalized 
inverses exist—each obeying a particular set of properties.^ Also, different ways have been 
developed to define these inverses. Our discussion in this appendix merely scratches the 
surface of an already broad and still expanding topic. 
^ The general procedure for converting the echelon reduced form to an identity submatrix is 
formalized in Section B.4. As noted there, the complete procedure involves reduction of M to what is 
known as Hermite form (in the case of a square coefficients matrix). 
^ The names and symbols for the Moore-Penrose and the g inverse follow those of Good (1969); 
other authors use different names, symbols, or both. 

B.3. 
INTRODUCTORY ASPECTS OF GENERALIZED INVERSES 
335 
•S 
--§ 
^ s 
O 
CL 
03 "D 
CO 
n 
* 
ii^ 

336 
APPENDIX B 
B.3.1 
The Penrose Conditions 
Research on generalized inverses goes back at least to 1920 with the work of Moore. 
Working independently, Penrose (1955) later defined the concept of 2i unique generalized 
inverse (now often called the Moore-Penrose inverse, denoted by A"^) as a matrix that, in 
conjunction with the matrix A from which it is derived, satisfies four conditions: 
(i) 
AA^A = A 
(ii) 
A^AA^ = A^ 
(iii) (AA'^)' = AA'^ (symmetry) 
(iv) (A'^A)'=A"^A 
(symmetry) 
There are alternative contexts in which to discuss A"^. One context concems the familiar 
case of solving a set of nonhomogeneous linear equations: 
Ax = b 
As we know, if A is nonsingular, a unique solution exists and is given by 
x = A-^b 
Moreover, the reader can easily observe that A"S the regular inverse, satisfies the four 
Penrose conditions. Still, cases might exist where A is either rectangular or else square and 
singular so that A"^ does not exist. 
Suppose, then, that we defme a matrix A of order m xn where r(A) = A: < min(m, n). 
If a Moore-Penrose inverse A"^ exists, it will be of order nxm; this must be so because 
AA^ is symmetric and, hence, square. It can be proved that for any matrix A, there exists 
a unique matrix A"*" that satisfies the four Penrose conditions.^ 
However, in solving a set of simultaneous equations, it is not always necessary that the 
solution be unique, as pointed out in Section B.2 in the context of echelon matrices. 
Moreover, it might be of interest to consider generalized inverses that obey only one (or 
more) of the four Penrose conditions. For example, if 
x = A-b 
is a solution to a set of consistent equations, A" (also of order n xmif Aism xn) need 
not be unique. 
Indeed, in order for A~, called the g inverse, to exist, only the //A'^/'of Penrose's four 
conditions, namely. 
AA-A = A 
need be satisfied. Thus, if our interest centers on solving simultaneous equations, and we 
do not require the generalized inverse to be unique, it may be easier to compute A"—and 
usually it is—than to compute A"^, the Moore-Penrose generalized inverse. 
Accordingly, we shall wish to examine both the "stronger" (Moore-Penrose inverse) 
case and the "weaker" (g inverse) case. We say "stronger" since all Moore-Penrose 
generalized inverses are g inverses but not the converse. 
A proof of this assertion can be found in Graybill (1969, p. 97). 

B.3. 
INTRODUCTORY ASPECTS OF GENERALIZED INVERSES 
337 
We start with the Moore-Penrose generalized inverse by showing its relevance to basic 
structure (or singular value decomposition), a concept already discussed in Chapter 5. Not 
only does the concept of basic structure provide one way to define A"^, but the present 
discussion should also help illuminate earlier remarks on matrix decomposition into its 
basic structure. 
Then we tum to a discussion of A", the g inverse. Our interest in this type of 
(nonunique) generalized inverse stems from the relative ease with which it can be 
computed and its close connection with solution methods for simultaneous equations 
that have already been discussed in Section B.2. 
B.3.2 
Left and Right General Inverses 
In the discussion (Section 5.7) of the basic structure of an arbitrary matrix A, we 
recall that A, of order m xn, can be decomposed into the triple product 
A = PAQ' 
where P'P = Q'Q = I and A is diagonal of order k xk, with k < min(m, n) positive entries 
that can be arranged in decreasing order of magnitude. 
For the moment, let us place no restrictions on A—it need be neither square nor basic 
and, hence, the rank of A may be less than its smaller order. Next, let us consider the 
following matrix A"^, defined as being of rank k and of order n xm:^ 
A^ = QA-^P' 
We obtain A"^ from A = PAQ' by taking the reciprocals of the diagonal entries of A and 
then transposing the triple product PA"^Q' into QA"^P'. 
Let us see what happens if we then premultiply A by A"^: 
A^A = (QA-ip')(PAQ') = QA-HP'P)AQ' = QA-^(I^xfc)AQ' 
= Q(A-^A)Q' = Q(I;,,^)Q' = QQ' 
What is found here is the major product moment of the right orthonormal section of A. 
As we know QQ' is symmetric, since it is a product moment matrix. 
Suppose next that A is postmultiplied by A"". Without going through the algebra, the 
result is 
AA^ = PP' 
where P is the left orthonormal section of A. Again, PP' is symmetric since it is a (major) 
product-moment matrix. 
The reader will observe that A^ A and AA"^ are each symmetric. Moreover, we also 
have 
(i) AA^A = A 
(ii) A^AA^ = A^ 
PP'PAQ' = PAQ' 
QQ'QA-^P' = QA'^P' 
= A 
=A" 
^ As recalled from Chapter 5, decomposition to basic structure is unique; also A^ is unique, given 
that QA~* P' is also of rank k. 

338 
APPENDIX B 
and, hence, conclude that all four of the Penrose conditions are met. Notice that 
r(A"^) =r(A) =/: where k < min(m, n). 
Next, let us suppose that Ay„x« (with m> n) is basic (as described in Chapter 5). If 
so,k = n and Q' in the triple product PAQ' will be square, of order nxn, resulting in 
A^A = QQ' = Q'Q = I„,„ 
If this relation is met, we let A"^ = L; the matrix L is sometimes referred to as the left 
pseudoinverse of A. 
By the same token, if A is "horizontal," of order m xn{m< 
n), and basic, then k=m 
and P will be square, of order m xm, and we shall have 
AA^ = PP' = P'P = K 
If this relation is met, we let A "^ = R; the matrix R is sometimes referred to as the right 
pseudoinverse of A. Finally, it should be clear that if and only if A is both square and 
basic (i.e., nonsingular) will it possess both a left and right pseudoinverse and these 
inverses will be the same. ^^ 
However, suppose we retum to the first case in which A is nonbasic: 
r(A) = k<min(m, n) 
It is possible, of course, to find a generalized inverse of A that meets only the first 
Penrose condition 
AAA = A 
by defining A~ in terms of the square roots of all of the eigenvalues of either AA', if 
m < rt, or A'A, if m > «, including those eigenvalues that turn out to be zero. 
If so, we refer to this case as a ^ inverse of A and continue to denote it as A". In this 
version of the generalized inverse the basic diagonal A of A(=PAQ') has mm(m, 
n)-k 
zeros, and P and Q' are no longer unique. It tums out, however, that 
x = A-b 
is still a solution to the set of consistent equations 
Ax = b 
and, in this sense, A is still a generalized inverse, specifically a^ inverse. 
^® Still, it should be pointed out that although A"^ is unique, the matrix A-if basic but 
singular-will, in general, have an infinity of other matrices that satisfy LA = I or AR = I, as the case 
may be. However, only one of this infinity of matrices will be the Moore-Penrose inverse. 
Furthermore, if A is nonsingular, only one matrix A"^ (=A^) exists. Thus, A, if nonsingular, has 
exactly one inverse, A"'. If A is singular, it has an infinity of generalized inverses, one of which is the 
(uniquely specified) Moore-Penrose inverse, A"^. 

B.3. 
INTRODUCTORY ASPECTS OF GENERALIZED INVERSES 
339 
Why should we ever want to find a version of QA"^P' whose diagonal is of larger order 
than k xk, where r(A) =kl Again, the motivation may be pragmatic in that it may be 
easier to compute A" (as defined above) even though it is no longer unique. 
We now turn to computational methods for finding the Moore-Penrose inverse, after 
which the g inverse A" is discussed. 
B.3.3 
Some Numerical Procedures for Computing A^ 
To illustrate the computation of A"*", let us consider the 3 x 2 matrix 
1 
3 
A = | 1 2 
2 1 
If r(A) = 71 = 2, we should be able to find a left general inverse such that LA = I2 x2 • 
Using the procedure described in Section 5.7.3, we first find the product-moment 
matrix with the smaller order, in this case the minor product moment 
A'A = 
7 
14 
and solve for its eigenstructure 
[18.062 
0 
D = 
L 0 
1.938 
We then compute the basic diagonal and its inverse 
4.250 
0 
-0.502 
-0.865 
0.865 
-0.502 
and then solve for P: 
P = AQA"^ = 
0 
1.392 
-0.502 
-0.865 
A-^ = 
0.865 
-0.502 
0.235 
0 
0 
0.718 
0.235 
0 
0 
0.718 
-0.728 
-0.460 
-0.525 
-0.099 
-0.439 
-0.882 
The matrix A is now expressed in terms of basic structure as A = PAQ'. The left general 
inverse is then 
A'^ = L = QA-^P' = 
-0.2 
0 
0.6 
0.313 
0.142 
-0.228 

340 
APPENDIX B 
-0.2 
0 
0.313 
0.142 
0.6 ] 
•0.228 J 
fl 3 
1 2 
[2 1 
= 
1 0 
0 1 
L. 
_J 
and we have the desired result: 
[-0.2 
LA = 
L 0.31 
However, now let us take the case where A is nonbasic. To illustrate, consider 
1 i 
A = | 1 2 
2 
4 
In this case we know that A is not basic since the second column is twice that of the first 
column and r(A) = 1. We first find 
A'A: 
By procedures identical to those just illustrated, the basic structure of A is then found to 
be 
P
A 
Q' 
6 12 
12 24 
= 6 
1 2 
2 4 
A = 
-0.408 
-0.408 
-0.816 
[5.477] [-0.477 -0.894] = 
1 2 
1 2 
2 4 
and A"*", of rank /^(A) = 1, is 
A^ =Qi 
^-ip' = 0.033 0.033 0.067 
0.067 0.067 0.133 
The reader can verify that the four Penrose conditions are met, although now it is no 
longer true that A'^A = I. 
Computing A"^ after first solving for the basic structure of a matrix is only one of 
many solution methods. By way of contrast, let us consider another method, due to 
Penrose himself (1956), that can also be used to find A"^. This method involves 
implementation of a fairly simple algorithm that entails the following steps: 
1. 
Compute B = A'A. 
2. 
Let Ci = I, the identity matrix. 
3. 
Compute C/ + i = I(l//)tr(C/B)-C/B for / = 1,2,. . ., 
k-\}^ 
4. 
Compute /:CfcA7tr(C;,B), to get A"". 
5. 
Also, it will be found that C^ + iB = 0; trCC^^B) ^ 0, so that r(B) = r(A) = k. 
* * The reader should recall that the trace (tr) of a square matrix A„x« is equal to the sum of its 
main diagonal elements: 
n 
tr(A)= Z Hi 
/ = 1 

B.3. 
INTRODUCTORY ASPECTS OF GENERALIZED INVERSES 
341 
Applying the procedure to the last problem (where A is nonbasic) gives us 
1. B = A'A 
6 
12 
12 
24 
2. 
Ci = 
1 
0 
0 
1 
CiB = 
3. 
C2=Itr(CiB)-CiB 
1 
0 
0 
1 
30 
O' 
0 
30 
6 
12 
12 24 
6 
12 
12 24 
6 
12 
12 24 
24 
"12 
12 
6 
trCiB = 30 
CoB = 
0 0 
0 0 
Since C2 B = 0 and tr(Ci B) i= 0, we know that r(B) =r(A) = 1; we can then go on to find 
4. 
A" = ICiA' 
tr(C,B) 
1 
30 
1 
1 2 
2 
2 
4 
0.033 
0.033 
0.067 
0.067 
0.067 
0.133 
We find, of course, the same solution for A"^ as found earlier. The Penrose procedure, like 
the basic structure approach, can be used to find A"^, whether or not A is basic. To 
complete the discussion, let us apply the Penrose computational procedure to the first 
case, where A is basic: 
1. A^ 
A" = 
1 
3 
1 
2 
2 
1 
I ( 
3 
I ( 
3 
2C2A^ 
tr(C2B) 
14 
-7 
B = A'A = 
6 
7 
7 
14 
2. 
3. 
Ci = 
C2 = 
1 
0 
0 
1 
~1 0~ 
0 
1 2 0 -
CiB = 
6 
7 
1 
6 
7 
7~ 
4 = 
7 
14_ 
' lA 
-1 
; 
t 
6j 
tr(CiB) = 20 
C2B = 
where 
tr(C2B) = 70 
_2_ 
70 
1 
1 2 
3 
2 
1 
-0.2 
0 
0.313 
0.142 
35 
0 
0 
35 
0.6 
-0.228 
Note that the Penrose computational procedure involves less computation— 
particularly, no need to find eigenstructures—than the method based on matrix 
decomposition via basic structure.^^ 
Note also that 
€3 = 
1 
0 
0 ij 
(70/2)-
"35 
0' 
_ 0 35J 
and, Xi{Q^B) i= 0; hence, /-(A) = 2. 

342 
APPENDIX B 
B.3.4 
Some Properties of the Moore-Penrose Inverse 
In many respects the Moore-Penrose inverse A"*" acts Uke a regular inverse A"^ 
(Indeed, A"^ equals A"^ if A is nonsingular.) However, even in other cases. A"*" possesses a 
number of properties, many of which are similar to those displayed by the regular inverse. 
Some of the more important of these properties are Usted below: 
1. The Moore-Penrose inverse of the transpose of A is the transpose of the 
Moore-Penrose inverse of A: (A')"^ = (A"^)'. 
2. 
The Moore-Penrose inverse of A"*" is equal to A: (A"^)"^ = A. 
3. 
The rank of the Moore-Penrose inverse of A is equal to the rank of A: 
r(A^) = r(A). 
4. 
For any matrix A, (A'A)* = A^CA')"" . 
5. 
For any matrix A, (AA^)^ = AA^; (A^A)^ = A^A. 
6. 
If A = A', then A^=(A^y. 
7. 
If A = A',thenAA^ = A'"A. 
8. 
If A is nonsingular, then A~^ = A"*^. 
9. 
If A is an m X « matrix of rank m, then A"*" = A'(AA')"^ and AA"*" = I (as related to 
Section B.3.2). 
10. 
If A is an m X « matrix of rank n, then A"^ = (A'A)"^A' and A'^A = I (as related 
to Section B.3.2). 
In addition to the properties Usted above, the Moore-Penrose inverse figures 
prominently in the solution of sets of linear equations. More specifically, given the set of 
nonhomogeneous equations 
Ax = b 
where A is of order m xn and b is an m x 1 vector of constants, the system of equations 
is consistent if and only if 
AA^b = b 
Second, given that the system is consistent (and, hence, has at least one solution), then 
for each nx\ 
vector 7, the « x 1 vector x is a solution where 
x = A''b + (I-A^A)y 
and every solution to the system can be so written for some nx\ 
vector y. 
As just indicated, the Moore-Penrose generalized inverse, assuming it can be found 
easily, provides a way to solve sets of linear equations. While in principle, we could always 
compute A"^ in solving sets of simultaneous equations, it is usually the case that we do 
not need the stronger properties of the Moore-Penrose inverse to get the job done. 
However, as recalled, a g inverse A" also provides a solution to a set of consistent 
equations, albeit one that is not unique but, on the other hand, one that is relatively easy 
to compute. Accordingly, we now tum to a discussion of the g inverse A" and its role in 
solving sets of simultaneous equations. 

B.4. 
THE g INVERSE 
343 
B.4 
THE g INVERSE 
If we let A be an m x/? matrix, a matrix A", of order n xm, is defined to be a ^ 
inverse of A if and only if it satisfies the first of the Penrose conditions 
AAA = A 
As pointed out earlier, the Moore-Penrose inverse of A is also a g inverse of A, but the 
converse does not hold in general. Moreover, in general A" is not unique for a given A. 
A g inverse is particularly useful in the practical setting of solving sets of simultaneous 
equations. Fully analogous to the Moore-Penrose inverse, the system of equations Ax = b 
has a solution if and only if 
AA-b = b 
Second, given that the system is consistent, then for each nxl 
vector 7, the « x 1 vector 
X is a solution where x is 
x = A-b + (I-A-A)y 
Finally, every solution to the system can be so written for some nxl 
vector y. 
The value of a ^ inverse relates to its relative ease of calculation, particularly by means 
of the echelon form of a matrix, as considered in Section B.2. However, before discussing 
a general approach to computing A" (in the context of solving sets of equations), we 
consider the concept of Hermite form. The Hermite form of a matrix provides the key 
concept for obtaining A". 
B.4.1 
The Hermite Form of a Square Matrix 
A square {n x n) matrix J is defined to be in (upper) Hermite form if and only if it 
satisfies the following conditions: 
1. J is upper triangular. 
2. 
Only zeros and ones are on its main diagonal. 
3. 
If a row has a zero on the diagonal, then every entry in the row is zero. 
4. 
If a row has a one on the diagonal, then every other entry is zero in the column in 
which the one appears. 
If J is of Hermite form, it is also the case that 
- 
T2 
J = J 
and J is said to be idempotent. Moreover, for any nxn 
matrix A, there exists a 
nonsingular matrix G such that 
GA = JA 

344 
APPENDIX B 
and so A can always be reduced to Hermite form via G. (However, G is nonunique, in 
general, although J^ will be.) 
Just as was the case in reducing A to echelon form H^, we can use elementary row 
operations to reduce A„XM to Hermite form J^ ^^ The matrix G is the nonsingular matrix 
that brings about the reduction of A„x« to Hermite form. 
Moreover, once this is done it turns out that the job is finished since A" can be defined 
as 
A- = G 
That is, G is the g inverse of A, and all we have to do to find G is to reduce A to Hermite 
form via elementary row operations while performing companion operations on I. 
However, since the Hermite form does not exist for rectangular matrices, a slight 
modification is required to find A" = G when A is rectangular. If A is vertical Qn xn, 
with m >«), we can append a set of 0 column vectors to make A square. That is, 
Ao = 
where AQ is m x m. Then, if G is a nonsingular matrix, such that GQAQ = JAO ' where J^^ 
is the Hermite form of AQ , we have 
Go = G 
Gi 
where G is the upper n xm submatrix of Go. This is the g inverse of A. Similarly, if A is 
horizontal (m xn, with n > m), we can append a set of 0' row vectors to make A square 
and proceed to find Qyixm ? the left-hand submatrix of GQ.^"* 
The strategy should now be clear. In the rectangular case, we make A square by adding 
columns or rows of zeros, as the case may be. We then find a nonsingular matrix that 
reduces AQ to Hermite form. The matrix G is the g inverse of A. 
However, one more facet of the problem has to be introduced before proceeding to 
find A~. In Section B.2 a general procedure was introduced for solving sets of 
simultaneous equations via reduction of either the coefficients matrix A or the 
augmented matrix M to echelon form. As might be surmised, if A is already square, or 
made square by appending columns (or rows) of zeros, the Hermite form JA of A can be 
obtained from its echelon form H^. This is done by transforming rows of H, via 
additional elementary row operations, until JA is found. The matrix G that summarizes 
the full set of elementary row operations used in reducing A to HA and then HA to JA is 
^^ As will be shown, if A is square to begin with (or can be made square by procedures to be 
described later), we can compute J A via additional elementary operations on H A -
^* The matrix would appear as 
Go = [G : G J . 
The next section shows some numerical examples of the general procedure, including a case in which 
A is rectangular. 

B.4. 
THE^ INVERSE 
345 
A", the desired g inverse. In general, A" will not be unique.^^ However, the matrix JA in 
Hermite form is unique for a given matrix A. 
Before proceeding with the computation of A", three additional properties of matrices 
in Hermite form are of interest to note: 
1. The Hermite form J^ of A has the same rank as A. 
2. 
If A, of order n xn,is nonsingular, then J^ is the n xn identity matrix I. 
3. 
The rank of A is equal to the number of diagonal elements of JA that are equal to 
unity. 
With the foregoing comments as background, a general procedure can now be stated for 
finding A": 
1. If A is rectangular, make it square by appending columns (or rows) of zeros. 
2. 
Via elementary row operations reduce A to echelon form and then to Hermite 
form. At the same time, perform the same operations on I, the associated n xn identity 
matrix. 
3. 
If A is nonsingular, then its Hermite form is I and A" = A~^ 
4. 
If A is singular, then I will be transformed to G = A" as A is being reduced to JA , 
its Hermite form. And A" will be the g inverse of interest. 
1 
2 
3 
4 
5 
6 
2~ 
3 
5 
; 
1 = 
B.4.2 
Some Numerical Examples 
Let us now consider some illustrations of finding A" by means of the method 
presented above. First, let us take a nonsingular matrix A and its companion identity 
matrix: 
"l 
0 0 
0 
1 0 
0 
0 1 
As outlined earher, the task is to reduce A to echelon form H^ and then into Hermite 
form JA via a series of elementary row operations. Each elementary row operation that is 
performed on A is also performed concurrently on the associated starting identity matrix 
I. As A is reduced to Hermite form JA , I is transformed to A~, the desired g inverse. 
The reader should note the similarity of this procedure to that followed in 
Section B.2. In the present case J^, the Hermite form of A, takes on the role of the 
identity submatrix computed from the echelon matrix in Section 3.2. 
We can now start the row operations, bearing in mind that these, in general, are not 
unique. We first subtract twice row 1 from row 2 and subtract 3 times row 1 from row 3: 
" 1 0 
0 
-2 
1 0 
^-3 
0 
1 
^^ The reason why A" (=G) is not unique is simply because, in general, there are different sets of 
elementary row operations (summarized in G) that can lead to JA-as a matter of fact, an infinity of 
such sets. 
1 
0 
0 
4 
- 3 
- 6 
2 
-1 
-1 
; 

346 
APPENDIX B 
Subtract twice row 2 from row 3: 
1 
0 
0 
4 
- 3 
0 
2 
-1 
1 
; 
1 
•2 
1 
0 
0 
1 
0 
- 2 
1 
Multiply row 2 by —1/3: 
HA = 
1 
4 
0 
1 
0 
0 
2 
1/3 
1 
1 
:/3 
1 
0 
-1/3 
- 2 
0 
0 
1 
At this point we note that A is in echelon form H^ and that r(A) = 3. The next task is to 
reduce H^ to Hermite form J^ • To do this, subtract 4 times row 2 from row 1: 
1 
0 2/3" 
0 
1 
1/3 
0 
0 
1 
; 
-5/3 
2/3 
1 
4/3 
0 
-1/3 
0 
-2 
1 
Subtract 2/3 of row 3 from row 1 and subtract 1/3 of row 3 from row 2: 
JA =1 = 
The reader can then check to see that 
1 
0 
0 
0 
1 0 
0 
0 
1 
; 
-7/3 
8/3 
-2/3 
1/3 
1/3 
-1/3 
1 
-2 
1 
= G = A" = A"' 
AA- = AA 
- I 
_ 
Note also that the Hermite form J^ of a nonsingular matrix A is an identity matrix of the 
same order. 
Next, we consider the case where A is square but not of full rank. In Section B.2.1 we 
encountered a matrix of this type: 
1 
0 
0 
0 
1 0 
0 
0 1 
1 
1 3 
1 
2 
6 
0 
1 3 
; 
1 = 
If we subtract the first row from the second, we get 
\ 
1 3~ 
0 
1 3 
0 
1 3 
; 
1 
0 
0 
- 1 
1 0 
0 
0 
1 

B.4. 
THE g INVERSE 
347 
Next we subtract the second row from the third: 
HA 
and note that we have the echelon form of A, H^, in which r(A) = 2. Next, we subtract 
the second row from the first: 
1 
1 3 
0 
1 3 
0 
0 
0 
5 
1 
1 
1 
0 
0 
1 
0 
- 1 
1 
= A-
1 0 
0 
2 - 1 0 
J A = I O 
1 
3 ; 
- 1 
1 0 
0 
0 
oj 
[ 1 - 1 
1 
to obtain J^, the Hermite form of A. We find A" as well and note that 
JA 
as it should, since J^ is idempotent. 
We then solve for x as 
JA - JA 
x = A-b 
where, from Section B.2.1, b' = (8, 14, 6), so that 
2 
1 
1 
A" 
- 1 
0 
1 
0 
-1 
1 
b 
r s' 
14 
L ^ 
= 
2 
6 
0 
X = 
However, as noted earUer, when the rank of A is less than the number of unknowns, there 
is an infinite number of solutions. Hence, we express x in the general form: 
x = A-b+(I-A-A)y 
where y is an arbitrary vector. Specifically, we have 
X = 
A-b 
2 
6 
0 
I 
1 
0 0~ 
0 
1 0 
0 
0 1 
-
A A 
1 
0 
0 
0 
1 3 
0 
0 
0 
7 
7i 
72 
73 
~2"" 
6 
0 
+ 
o ' 
-373 
73 
= 
~ 
2 
~ 
6-373 
73 
where 73 is considered as an arbitrary parameter. Note that this is the same result as 
found in Section B.2.1. 

348 
APPENDIX B 
Finally, let us consider the case where A is rectangular. As an illustration, we again 
return to Section B.2.2 and consider the 4 x 3 vertical matrix 
A = 
1 
1 
0 
2 
1 
2 
1 
3 
3 
6 
3 
9 
After appending a column vector of zeros to make A square, we have 
Ao^ 
1 
1 3 
1 
2 
6 
0 
1 3 
2 
3 9 
0 
0 
0 
0 
; 
io = 
1 0 
0 
0 
0 
1 0 
0 
0 
0 
1 0 
0 
0 
0 
1 
Note that the associated identity matrix \Q has an extra row added. Reduction of AQ to 
echelon form (with concurrent elementary row operations on IQ) gives us first the 
echelon form: 
H. 
1 
1 3 
0 
1 3 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
-1 
1 
- 1 
0 
0 
0 
1 
0 
0 
-1 
1 0 
- 1 
0 
1 
and a further elementary row operation that subtracts row 2 from row 1 leads to 
JA = 
1 
0 
0 
0 
1 3 
0 
0 
0 
0 
0 
0 
2 
-1 
1 
- 1 0 
0 
1 0 
0 
- 1 
1 0 
- 1 - 1 0 
1 
= Go 
We then write A by dropping the fourth row of the transformed identity matrix to 
get the 3 x 4 matrix 
G = A- = 
2 
1 
1 
- 1 0 
0 
1 
0 
0 
- 1 
1 0 

B.4. 
THE^ INVERSE 
349 
Having found A , the g inverse, we can go on to solve for x in terms of the vector b' = (8, 
14,6, 22), as from Section B.2.2: 
x = 
2 
1 
1 
A" 
-1 
0 O] 
1 0 
0 
- 1 1 0 
b 
8 
14 
6 
22 
= 
2 
6 
0 
Since the rank of A is only two, we have an infinite number of solutions, written in 
general form as 
x = 
A'b 
2 
6 
0 
+ s 
I 
1 
0 
0 
0 
1 0 
0 
0 
1 
A-A 
1 
0 O" 
0 
1 3 
0 
0 
0 
T 
7i 
72 
73 
r2~ 
6 
0 
+ 
~ 
0 ~ 
-373 
73 
= 
~ 2 ~ 
6-73 
73 J 
as was also obtained in Section B.2.2. 
We observe, in passing, that the fourth row in A is redundant with the others, since it 
is the sum of the first two rows. 
The procedure just outlined is quite general and can be applied to square or 
rectangular coefficients matrices, using the modification (to make A square) that was just 
illustrated. Of course, if one or more rows of zeros are appended to make A square, then 
the resulting additional columns of the transformed identity matrix are dropped in 
finding A"; Otherwise, the method is the same. In brief, the present method of 
computing g inverses is fully consistent with the echelon procedure of Section B.2. Thus, 
the echelon procedure (and consequent reduction of A to an identity submatrix) of 
Section B.2 is a general method for solving a specific set of linear equations. 
The present method of solving for A" is also fully general for finding a g inverse and 
then solving the system 
x = A-b + (I-A-A)T 
for any desired b of constants. If A is nonsingular, then A" = A~^ and the second term on 
the right drops out. If A is singular, then A" will still exist (as long as the equations are 
consistent, a test that can be made via the procedure of Section B.2). 
Generalized inverses, of various types, are playing increasingly important roles in 
multivariate analysis. For more extensive discussion of the topic, the reader is referred to 
books by Pringle and Rayner (1971) and Rao and Mitra (1971). 

350 
APPENDIX B 
B.5 SUMMARY 
The role of generalized inverses in solving sets of linear equations has been the main 
subject of this appendix. GeneraUzed inverses provide a counterpart role to regular 
inverses in cases where the coefficients matrix is singular. We started the discussion by 
reviewing a general procedure, involving reduction of either a coefficients or an 
augmented matrix to echelon form, for determining whether a set of simultaneous 
equations had (a) none, (b) exactly one, or (c) infinitely many solutions. 
A general solution procedure, employing elementary row operations, was described 
and illustrated numerically. After reducing the augmented matrix M or the coefficients 
matrix A to echelon form, Hj^ or H^, additional elementary row operations were 
employed to reduce H^ (or H^ ) to an identity submatrix. Illustrations were provided for 
both nonhomogeneous and homogeneous sets of equations. 
We then turned to a discussion of generalized inverses. The Penrose conditions were 
introduced, and the Moore-Penrose inverse A"^ was described in the context of basic 
structure. The related concepts of left and right pseudoinverses were also illustrated, and 
properties of the (unique) Moore-Penrose were listed. 
The appendix was concluded with a companion discussion of the g inverse A". This 
(nonunique) generalized inverse need satisfy only the first Penrose condition. In general, 
A" is easier to compute than A"*" and, furthermore, plays a central role in solving sets of 
simultaneous equations. Several numerical illustrations of one procedure, involving 
reduction of a square coefficients matrix to Hermite form, were presented and tied in 
with the general procedure (of Section B.2) that was based on matrix reduction to 
echelon form. 
REVIEW QUESTIONS 
1. 
By means of reduction to echelon form, find the rank of 
c. 
1 
3 
1 
3 
1 
3 
4 
2 
2 
6 
r 
7 
4 
12 
b. 
d. 
B = 
D = 
2. 
Reduce the following matrices to identity submatrices: 
1 
1 
2 
2 
2 
4 
3 
5 
8 
1 2 
0 - 1 
3 
4 
1 
2 
-2 
3 2 
5 
b. 
0 
2 
3 
4 
2 
3 
5 
4 
4 
8 
13 
12 

REVIEW QUESTIONS 
351 
3. 
Using elementary row operations on A and I, simultaneously, find the inverse of 
a. 
A = 
b. 
A = 
4. 
Find a left pseudoinverse of 
A = 
5. 
Consider the system of equations 
Xi + 3^2—2^3—X4 + 2^5 = 
1 
2x 1 + 6x2 — 4:x:3 — 2x4 + 4^5 = 
2 
Xi + 3x2 —2:JC3 + X4 
2xi + 6x2 +X3—X4 
= -1 
= 
4 
Reduce the augmented matrix M to echelon form and find x. 
6. 
Consider the matrix 
^1 
3 
A = 2 
1 
3 
2 
(a) 
Find the Moore-Penrose inverse A"*". 
(b) 
Find a g inverse A" via reduction of A to Hermite form. 

Answers to Numerical Problems 
CHAPTER 2 
2.1a 
I 4 
1 - 1 
3 - 4 
2 
5 
- 1 
- 2 
2.2a 
\~3 
5 
r 
L3 
2 
1. 
X 
y 
z 
2.2b 
2.1b 
2.2d 
- 1 
- 2 
- 7 
1 
- 1 
3 
0 
1 
7 
"3 
6 
1 
7 
1 - 1 _ 
2.2e 
r - 3 
- 5 
3 
- 2 
~2 
1 
3 
3 
1 
5 
l1 
7 
4J 
X 
y 
\_z 
= 
'if 
24 
_25_ 
2.2c 
'3 
6 
L7 
1 
2.2f 
r 
- i _ 
"-1 
9 
2.3a 
26 
2.3f 
23/2 
2.3b 
- 2 
- 4 
2.3c 
(5,15,20) 
2.3d 
23 
0 
- 7 ] 
-3 
- i j 
2.3e 
210 
2.4d 
-7,8) 
0 
2 
8 
- 2 
2.4b 
0~ 
- 4 j 
4 
6 
8 
_-2 4 oj 
2.4e 
240 
_-80 
2.4c 
- 8 0 
35 
[-4 
L 3 
120" 
40_ 
1 2 ' 
- 4 _ 
2.4f 
1 
3 
4 
2 
6 
8 
4 
12 
16 
2.5a 
(i) 
( D E ) ' = E ' D ' 
(ii) 
D'E' = 
352 
ea + gb 
ec + gd 
fa + hb 
fc-\- hd_ 
ae + cf 
ag + chl 
be + df 
bg+dhj 
(iii) 
E'D' = See (i) above. 

CHAPTER 2 
353 
2.5b 
r 3 0 
(i) 
(DEy = E'D'= 
(ii) 
LlO 
4_ 
(iii) 
E'D' = See (i) above. 
2.6a 
F¥=0; 
G ^ 0 ; 
FG = 0 
2.6b 
D'E' 
3 
0 
17 
4 
GF = 
-10 
30 
50 
0 
0 
0 
0 
0 
0 
^<t> 
2.7a 
84 
2.7b 
- 3 0 
2Jc 
0 
2.7d 
D 
2.8a 
( - 2 , 6 , 1 0 ) 
2.8b 
(-3,0,18) 
2.8c 
28 
2.8d 
(0,0,0) 
2.9 
. 
X ^ - X - 2 I = (X + I)(X-2I) = 
2.10a 
2.11a 
5 
8 
_8 
13_ 
0 
2.1 
a^ + bc-a-2 
ab + 
bd-b 
_ac + cd-c 
bc + 
d^-d-2_ 
2.10b 
["4 
0 
LO 9_ 
lib 
0 
2.11c 
2.10c 
0 
2.11d 
28 
66 
44 
105_ 
fl2 + Z>^ 
2.10d 
30 
_72 
48 
114_ 
-128 
2.12a 
148 
2.12b 
- 3 2 
2.12c 
2.13 
|A| = 3(10) + 2 - 1 7 = 15 
2.14 
|M| = (l)(-8)(-2.75)(-5.8182) = -128 
2.15a 
S F = 3 3 
Xi = 4.33 
2 7 X 2 = 1 6 5 
2X3^-(SZa)^/??! = 50.83 
2.15b 
S = 
2.15c 
C = 
2.15d 
R = 
2.15e 
Ad = 
41.5 
31 
38.5 
-37.5 
6.9 
5.2 
6.3 
-6.3 
1 
0.76 
0.96 
-0.82 
[-3.5 
-1.5 
-2.5 
1.5 
2.5 
L 3.5 
31 
39.3 
33.3 
-43.7 
5.2 
6.6 
5.6 
-13 
-
0.76 
1 
0.85 
-0.98 
-3.3 
-2.3 
0.7 
-1.3 
2.7 
3.7 
38 
33.3 
38.3 
-38.2 
6.3 
-
5.6 
-
6.4 
-
-6.4 
0.96 
0.85 
1 
-0.86 
-3.8 
-0.8 
-1.8 
0.2 
3.2 
3.2 
- 37.5] 
-43.7 1 
-38.2 
50.8J 
6.3" 
7.3 
6.4 
8.5_ 
- 0 . 8 2 ' 
-0.98 
-0.86 
1 
_ 
4.2~ 
3.2 
-0.8 
0.2 
-2.8 
- 3 . J l_ 
2.15f 
- 3 . 5 - 1 . 5 - 2 . 5 + 1.5 + 2.5 + 3.5 = 0 

354 
ANSWERS TO NUMERICAL PROBLEMS 
CHAPTER 3 
3.1a 
| | a | | = V 5 ; 
INI = V B ; 
llc|| = V7r' + 7 
3.1b 
the plane y= 1 - x for all z 
3.1c 
a parabola z = x^ for all y 
3.Id 
a sphere with center (0, 0, 0) and radius 1 
3.2a 
r5/2l 
3.2c 
\\PR\\ = 3 V^l 2 •A\RQ\\ = 3y/3/2-A\PQ\\ = 3
^ 
3/2 
1/2^ 
3.3a 
k = -'\ 
3.3b 
ki = l; 
k2 = 2; 
k2 = -\ 
3.3c 
linearly independent vectors; 
ki = k2 = 0 
3.4a 
1.414 
3.4b 
0 
3.4c 
- ^ 
3.4d 
- 6 < a'b < 6 
3.5 
la'cl 
Ib'cl 
3.6 
3.6a a + b 
3.6b a ' - b ' 
3\A0 
xATo 
(if a'c and b'c have same sign) 
a 
P 
7 
- l / 3 > / l O 
5/3VT0 
8/3\/T0 
s/y/uo 
-ijyjno ejyJVib 
3.6c 5a'+10b' 
\/5450 
-20/\/5450 
55/V5450 
45/\/5450 
3.6d ^ ( a ' - b ' ) 
V n O / 2 
5/VnO 
-7/\/TTo 
6/\/nO 
-43 
3.7a 
3.7b 
UUS (7-^y -
"I/VH" 
2/V14 
_3/Vl4_ 
"2/V5"] 
J/\/5"J 
2^5995 ' 
b* = 
b* = 
C I C JV 
r 33/\/l414l 
-18/\/l414 
L 1/V1414 J 
-I/V5"" 
- 2/V5"j 
C* = 
c* = 
ro" 
' " 
, 
J' 
2 V 
4/vroi" 
7/VlOl 
_-6/Vl0l_ 
3.7c 
The third vector vanishes; in a two-dimensional space no more than two vectors can 
constitute a basis. 
3.8 
1/8" 
.5/8. 
3.9 
a'b = 0. The orthogonal vector c is of the form 
-2k 
k 
k 

CHAPTER 4 
355 
3.10 
The equation for the circle is independent of parameter ^ , denoting the angle of 
rotation. New equations for the ellipse with coordinates (w, v) instead of (jc, y) are 
n .^^ 
7 2 ^ 13 2 
3V3 
3.10b - w 
+ —rlr 
r — u v = 4 
4 
4 
2 
3.10a 
^u^ + ^v^-3uv 
= 4 
1 -y 
13 o 
3\/3^ 
3.10c 
-1/2 + -f-v^ + ^ u v 
= 4 
4 
4 
2 
3.11 
The vectors ( 4 , - 2 , l,7)and(2,-3,x,>') are linearly independent for any x,;' since 
(4, —2) and (2, —3) are linearly independent 4n a two-dimensional space. 
3.12 
7 
8 
6 
-1 
6 
-13 
3 
-1 
5 
(ei,e2,e3)= 
~r(fi,f2,f3) 
^ • 1 ^ ^ 
'K 
-7 
*'u* 
3 V 2 - I I V 6 
a b = 7; 
a* b* = —^^^—;; ^^— 
4 
3.13b ||a||=V5'; ||b||=VT3; ||a*|| = Vs; ||b*|| = VTI 
Vector lengths are preserved under rotation. 
3.15 
3 
5l 
2 
4j 
1 1 
Li 
3 
3 
1 
1 
3 
3_ 
8 
_6 
14 
10 
24 
18 
18 
14. 
Area of {ABCD) is 4 and area of U ^ C Z ? ) is 8. 
3.16c 
y'x2 =m cov(r,X2); 
Ily||=\/w^y; 
Ilx2ll=\/^^x 
CHAPTER 4 
4.1a 
an identity mapping 
4.1b 
a projection on z, followed by a reversal of direction along z 
4.1c 
a reversal of direction along z 
4. Id 
leaves terminus of vector at same height but moves vector three times as far from z. 
4.2a 
4.2b 
4.2c 
4.3a 
X i * = 
X i * = 
X5* = 
r 1 " 
L3/2_ 
r 11 
L3/2J 
r 1" 
L5/2_ 
J 
X2* = 
; 
X2* = 
; 
X6* = 
"4~[ 
_4j 
"41 
.4J 
"21 
_3j 
; 
X3* = 
; 
X3* = 
"5" 
A 
' 5" 
"21 
_6j 
; 
X4* = 
X8* = 
'21 
.4] 
Areas are invariant while angles and lengths change. 
(i) 
2 
8 
10 
3~ 
6 
12 
(1 i) 
[ 2 
3" 
8 
6 
10 
12 
[ 4 
9_ 
(iii) 
~2 
4 
_2 
1 
1 
6" 
6 
5 
5_ 

356 
ANSWERS TO NUMERICAL PROBLEMS 
4.3b 
(i) 
3 
1 
8 
2 
13 
4 
(ii) 
4.4a 
4.4b 
4.4c 
4.5a 
4.5c 
4.6a 
(Xi*,X2*)=(Xi,X2) 
(Xi*,X2*)=(Xi,X2) 
~l/2 
1 
. 0 
l/2j 
0.35 
0.35 
-0.3^5 
0.35. 
5/11 
3 
1 
8 
2 
13 
4 
. 8 
3 
0.707 
0.707 
-0.707 
0.707 
0.866 
0.5 
n 
L-0.5 
0.866_ 
3 
0 
(iii) 
" 5 
6 
12 
_11 
2" 
2 
5 
5_ 
+ (2,-1) 
"3 
0 " 
0 
1/2 
0.866 
0.5 
LO l/2j L-0.5 
0.866 
4.5b 
r 0.35 
1.06 
0.35 
-0.35 
4.5d 
A-^ = 
L-7/11 
-2/1 r 
5/11. 
0.35 
1.06 
L-0.35 
-0.35J 
4.6b 
A"^ is not defined 
4.6c 
A ^ is not defined. 
4.6d 
- 1 
ad + be 
d 
b 
L c 
aj 
4.7a 
4.8a 
4.7b 
3 
4.7c 
3 
1 2 
3 
9 
2 
0 
1 2 
1 
4 
0 
0 
1 - 1 7 / 2 - 7 / 2 
Lo 0 0 
1 
-8/37J 
4.8b 
1 
0 
0 
4 
1 
0 
3~ 
- 1 
1 
4.8c 
1 
0 
0 
0 
0 
1 
1 
0 
0 
0 
1 
6 
1 
0 
0 
4.9a 
4.9b 
4.9c 
4.10 
4.11 
r(A) = 2; 
r(B) = 3. Regular inverse does not exist. 
r(A) = 3 = r ( B ) = 3 ; 
A-^ = 
- 1 / 3 
1/6 
1/6 
-2 
20/6 
1 
-7/6 
0 
-1/6 
r(A) = r(B) = 2. Regular (3 x 3) inverse does not exist. 
-3 
1 
7 
1/4 I - 1 
- 1 
5 
5 
1 
- 1 3 
-1 
| A | = 4 ; x i * = - l / 2 ; x 2 * = -l/2;jC3* = 3/2 
R"^ = 
10.26 
-9.74 
-9.74 
10.26 
|R| = 0.0975; 
b* = 
1.072 
-0.128 

CHAPTER 5 
357 
4.12a 
4.13 
4.14 
X '^ = 
•^1 ^ 
I 
O-fe 
I 
L^3 *J 
-377 
1406 
4.12b 
" 7 
1 
0 
1 
_-4 
0 
r 
3 
0 
0 
0 
6 
0 
0 
0 
9 
X2 
o 
. ^ 3 
T° is a stretch transformation. 
10 
0 
-6. 
X 
c 
o 
X2 
o 
•^3 
X = 
o 
X 
< 
X3 J 
- 5 
4 
4.12c 
|T| = - 2 
CHAPTER 5 
5.1a 
Xi = 5; 
X2 = 8; 
U = 
5.1b 
Xi = - 4 ; 
X2 = - 6 ; 
U = 
5.1c 
X i = 2 ; 
X2 = - 7 ; 
3/V58 
0 
i/Vss 
1 
"2/V5" 4/V17 
Li/Vs" 1/VT7J 
V2/2 
-5/V4i 
_>/272 
4/V4I. 
~ 1/VIO 
l/\/5 
_-3/\/lO 
-2/V5 
5.1e 
X i = 4 ; 
X2 = 2; 
X3 = - 2 ; 
U 
U = 
5.1d 
Xi = 5; 
X2 = 4; 
U = 
5.1f 
Xi=3; 
X2 = 3; 
X3 = 3; 
U = 
All eigenvectors are of the form 
i/Vn 
3/Vi4 V2/2 
3/Vri 
2/\/T4 
0 
1/vn 
I/VH 
V2/2_ 
l/\/3 1/V3 I/V3" 
l/V^" 1/V3 1/V3" 
Ll/V3" I/VS" lly/3J 
k 
k 
k 
5.2a 
tf(A)=13; 
|A| = 40 
5.2b 
tr(A) = - 1 0 ; 
|A| = 24 
5.2c 
tr(A) = - 5 ; 
|A| = - 1 4 
5.2d 
tr(A) = 9; 
|A| = 20 
5.3a 
0 
k 
5.3b 
k 
_0_ 
or 
0 
5.3c 
Any vector remains invariant under 
central dilation. 

358 
ANSWERS TO NUMERICAL 
PROBLEMS 
5.3d 
There is no vector remaining invariant under this transformation since eigenvalues 
are complex. 
rS 
1-
5.4 
The matrix A = [ j 
3] 
has eigenvalues given by the diagonal elements of 
D = [ 
^ 
4 _ /2 ] ^^^ associated eigenvectors that are 
V2-I. 
U2 
\/4-2\/2 
1-V2-
1 
as columns of U. Then, let A = UDU'. 
5.4a 
5.4c 
5.4e 
5.4g 
5.5 
u 
u 
u 
u 
12 + 3V2 
0 
0 
12-3 
"1+ \/2 
0 
1 
_ 0 
1-V2J 
1 
0 
1 
4 + V2 
_i 
_ 
0 
4 - \ / 2 j 
>/2j 
U' 
5.4b 
5.4d 
r 
J 
U 
5.4f 
U 
U 
"(4 + y/iy^^^ 
0 
0 
(4-V2)-'''_ 
A-^= -
^ 
4 
'1 
2 
1 
1 
-2 
1 
1 
2 
- 3 
u 
'88 + 
rv4 
6 + \/2 
0 " 
_ 
0 
6-V2_ 
50\/2 
0 
"W^ 
r 
0 
88-50\/2_ 
0 
1 
1 
0 
V 4 - \ / 2 
rr ' 
u 
U' 
; 
ABA'^ = 
"5 
0 
0" 
0 
1 0 
0 
0 1 
u' 
5.6 
Xi = 12; 
X2 = 6; 
X3 = 6. The last two eigenvalues are not unique. One possible 
solution is 
U = 
Then 
1/V6 
I/V2 
-1/V3 
-2/V6 
0 
-l/\/3" 
1/V6 
-I/V2 
-\l\/3 
D = U'AU = 
5.7a 
3 
5.8a 
A 
5.8b 
A = 
12 
0 
0 
0 
6 
0 
0 
0 
6 
5.7b 
3 
5.7c 
2 
i/\/2 
i/\/2nri 
0" 
L1/V2 -1/V2JL0 i_ 
p 
-0.650 
0.646 
-0.616 
-0.173 
0.430 
0.744 
5.7d 
1 
I/V2 
l/\/2' 
-I/V2" l/\/2. 
'2.022 
0 
.0 
3.86IJ 
Q' 
"0.290 
-0.957 
.0.957 
0.290J 

CHAPTER 5 
359 
5.8c 
5.8d 
A = 
A = 
0.707 0.707" 
.-0.707 0.707_ 
"0.114 
0.634' 
0.282 
0.396 
0.553 
0.667 
-0.108 
0.526 
-0.551 
0.083_ 
"1 0" 
0 1 
10.592 
. 0 
1 0' 
0 1_ 
0 
2.969 
0.994 
_0.107 
-0.107 
0.994 
In all four cases, r(A) = 2. 
5.9a 
(5.8a): 
(5.8b): 
5.9b 
(5.8a): 
(5.8b): 
5.10a 
5.10b 
5.10c 
5.10d 
(A'A)^ = 
2 _ 
(A'A)^ = 
(A'A)"^ = 
(A'A)"^ = 
f^V2 
^ 
1/V2" 
l/\/2" 
0.290 
-0.957 
1/V^ 
-1/V2'J 
0.9571 
0.290J 
fl 
Ol 
Lo 
i j 
r i 6.744 
L 0 
ri/\/2 
L1/V2 
0 
227.248 
I/V2 
-I/V2. 
"0.290 
0.957 
-0.957" 
0.290 
'I/V2 
-I/V2 
0.290 
-0.957 
I/V2 
-I/V2J 
0.957] 
O.290J 
Ai/2 = 
-1/2 _ 
,1/2 . 
I/V2 
l/\/2 
-I/V2 
I/V2J 
I/V2 
I/V2I 
L-I/V2 
1/V2"J 
I/V2 
I/V2 
-I/V2 
I/V2 
I/V2 
I/V2 
-I/V2 
I/V2 
I/V5" 2/V5' 
-llsjs 
\l\Js] 
I/V5" 2/V5'1 
\_-2l\j5 
\l\JsA 
"1 01 ri/\/2 
i/\/2 
_0 Ij L1/V2 
-l/\/2J 
2.022 
0 
~| r0.290 
-0.957 
0 
3.86IJ L0.957 
0.290 
"1 
0 ] r i / \ / 2 
- l / \ / 2 
Lo V7jLl/\/2 
l/\/2 
"1 
0 1 ri/\/2 
-l/\/2" 
-0 I/V7J Ll/\/2 
I/V24 
V6 
0 ] ri/\/2 
-l/\/2 
_ 0 
2V2JL1/V2 
l/\/2 
"1/V6 
0 iri/\/2 
-1/V2" 
_ 0 
l/2\/2jLl/\/2 
l/\/2 
"2 OI \\I\JS 
-l/y/J 
_0 3J [21^/5 
\/\/5 
'1/2 
Oiri/Vs" 
-2/\/5" 
- 0 
I/3J L2/\/5" 
l/\/5 
A = U 
A^/2=U| 
2 
0 
0 
2 
V2 
0 
L 0 \/2 
U', where U is any set of two orthonormal vectors. 
U' 
u 
l/\/2 
0 
0 
l/\/2"J 
U' 

360 
ANSWERS TO NUMERICAL PROBLEMS 
5.11a 
5.11b 
5.12a 
5.12b 
5.12c 
5.13a 
5.14a 
5.14b 
5.14c 
5.14d 
1 
1 
1 
1 
" 1 
- 2 
- 2 
2 
x; 
rank 1 
A = "1/V2 
|x; 
rank 2 
i/VTl 
5.11c 
X 
s 
5.lid 
2 
X 
Vi 01 
Lo oj 
ri/V2 
L1/V2 
> 
-3' 
\ 
1_ x; 
rar 
2 
-3/2~ 
..-3/2 
3 _ 
l/\/2" 
-1/V^J 
rank 2 
Rotation—projection on first axis and stretch-rotation 
3 + VTy 
0 
A = U 
2 
0 
3-yJvf 
U' 
Rotation-«tretch-rotation 
A = -3/\/ro i/vr^ 
i/vro" 3/\/To" 
10 0" 
0 0 
-3/Vio i/VTo" 
L i/VTo" 3/\/ro 
5.12d 
Rotation—projection on first axis and stretch-rotation 
A = U 
5 + VToT 
0 
s-VTo 
2 
0 
u' 
Rotation-stretch-rotation 
C = 
29.52 
19.44 
14.44 
19.44 
14.19 
10.69 
14.44" 
10.69 
8.91 
5.13b 
Zi 
Z2 
Z3 
0.755y^j + 0.521X^1 "^ 0.396Xd2; 
0.618yd - 0.366Zdi - 0.696X^2; 
0.2\%Y^ - 0.771Xjjl + 0.598JiQ}2; ^3 = O-^^ 
\ 
= 50.50 
Xo = 
1.72 
A = x' -3/Vro 
\I\/YQ' 
. i/Vio 3/vro. 
10 
_ 0 
"-3/vro i/Vio" 
. i/Vio" 3/\/io"-
Positive semidefinite 
0.542 
-0.643 
0.542 
0.585 
-0.811 
-0.707 
0 
0.707 
0.811" 
0.585 
0.455 
0.765 
0.455 
-7.372 
0 
0 
0 
- 3 
0 
Negative definite 
4.081 
0 
0 
0.919 
0.585 
0.811 
0 
0 
-1.628 
-0.811' 
0.585_ 
0.542 
-0.707 
0.455 
-0.643 
0 
0.765 
0.542 
0.707 
0.455 
Positive definite 
0.707 
-0.408 
0.707 
0.408 
0 
0.816 
-0.577 
0.577 
-0.577 
0 
0 
6 
0 
0 
0 
0.707 
-0.408 
-0.577 
0.707 
0 
0.408 
0.816 
0.577 
-0.577 
Positive semidefinite 

CHAPTER 6 
361 
.15a 
^-1/2^^-1/2 ^ 
0.393 
-0.0424 
_-0.0424 
0.345 _ 
"21.216 
13.203" 
_13.203 
8.257_ 
Q2 
A 
^ 
Q2' 
0.849 
-0.529] 
_0.529 
O.849J 
Y = XdW-^^^Qa = Xd 
•. 15b 
Both transform ations yield same 
[29.444 
0 
L 0 
0.029. 
0.311 
-0.2441 
0.146 
0.315j 
diagonal matrix. 
i r 0.849 
0.529" 
J L-0.529 0.849_ 
CHAPTER 6 
6.1a 
Y = -2.263 + 1.550X1-0.239^2 
6.1b 
f = - 2 . 2 6 3 + 
1.550X1-0.239X2 
6.2a 
6.2b 
C = 
R = 
14.19 
.10.69 
"1 
0.951 
10.69" 
8.91_ 
0.951 
1 
bi*= 1.076; b2* =-1.309 
6.1c 
Y= 1.125-0.179X1+0.117Xi^ 
1 
0.9781 
_0.978 
1 
R^ = 0.954 exceeds R'^ = 0.902 for Y regressed on Xi alone. 
R = 
/?^ = 0.954 
R = 
"1 
0.951 
_0.951 
1 
Zi = 0.707Xsi + 0.707Xs2; 
Z2 = 0.707Xsi-0.707Xs2; 
T = 
0.707 
0.707" 
0.707 
-0.707_ 
Xi = 1.951 
X2 = 0.049 
^1/2 
F = 
0.707 
0.707 
0.707 
-0.707 
"1.398 
0 
0 
0.208 
0.988 
0.988 
0.147" 
-0.147 
A 45° rotation is involved. Eigenvectors (and loadings) differ from those obtained 
from C, the covariance matrix. 
A = 
T = 
"0.787 
0.617" 
.0.617 
-0.787_ 
Rotation is 38 . 
53.25 41.42 
41.42 
33.08_ 
Xi = 85.79; 
X2 = 0.54. 
6.2c 
R^ is the same if both columns of component scores are used, i^^ is lower 
(R^ = 0.88) if only the first column of component scores is used. 

362 
ANSWERS TO NUMERICAL PROBLEMS 
6.2d 
6.3a 
6.3b 
6.3c 
6.4a 
29.52 
19.44 
14.44 
C= 
19.44 
14.19 
10.69 
|_14.44 
10.69 
8.91 
Xi = 50.51; 
X2 = 1.72; 
X3 = 0.398 
V.= 
0.937 
-0.700 
.0.348 
0.715 
A = 
T = 
0.756 
0.521 
0.396 
29.155 
0 
0 
-0.618 
0.366 
0.696 
-1 
0.028 
-0.218 
0.771 
0.599 
Discriminant weights change under standardization of data matrix. 
r0.905 
-0.612" 
W - ^ ' 2 Q 2 = 
LO.425 
0.791 
As can be seen from Table 6.5, W"^^^Q2 is equal to V, the matrix of eigenvectors 
obtained from W A. 
Xi = 3.768; 
ti = 
0.868 
-0.497 
In the two-group case only one discriminant is computed. 
C = 
R = 
14.19 
1.46 
1.46 
0.25 
r = -2.488 + 1 A96Xi 
-1.229^^2 
6.4b 
i?^= 0.907 
1 
0.774 
0.774 
1 
APPENDIX A 
A.la 
2 
A.lb 
, 2 . .x2 
A.lc 
r-
2x-x 
(;c + 4) 
2\/x 
A.2 
X = —I 
(minimum); 
x = 1 
(maximum) 
A.3 
| ^ ( 1 , 3 ) = 4.2; 
| ^ ( 1 , 3 ) = 4.6 
A.4 
min/(jc, >^) = - 1 8 
ox 
oy 
A.5 
/ ( - 2 / 3 , 4/3) = 16/3 
(maximum) 
1 
^w. 
. , , 4(x'-\?(9x' 
-I2x') 
— 
e^ 
A . l a 
. 3 
.5 
(2JC^ + 1)' 
A.6 
| ^ ( X ) = 2AJC. 
Ifx' = (3,l,2), - | ^ = 
ox 
dx 
34 
28 
40 
A.7 
(minimum) 

APPENDIX B 
363 
APPENDIX 
B.la 
) 
B.2a 
B.3a 
B.4 
B.5 
Ji 
B.6a 
B 
^(A)=l 
"l 
0 
0 
0 
1 
0 
0 
0 
1 
k-' = 
4/5 
L-1/5 
A^ = 
A" 
2.56 
-1.37 
-0.78 
B.lb 
r(B) = 2 
B.lc 
r(C) = 2 
: o" 
0 
0_ 
B.2b r 
-3/5" 
2/5J 
1 
0 
0 
1 
0 
0 
: 
0 o~ 
* 
0 
0 
: 
0 
0 
B.3b 
A-^ = 
-2.11 
-1.11 
2.34~ 
1.40 
0.41 
-1.22 
0.55 
0.55 
-0.67 
~-372 +(1/5)75+ 6/5" 
72 
3/5(7< 
T s - l 
_ 
Ts 
= 
r-0.25 
L 0.41 
> + l) 
3 
2 
-
0.226 
0.107 
( 
- ( 
r^r 
X2 
X3 
X4 
Us. 
).266 
).067 
" 7 
- 3 
-
- 1 
1 
- 1 
0 
B.6b 
A" = 
B.ld 
3 ' 
0 
1 
— 0.2 
0.4 
r(D) = 2 
0.6 
-0.2 

Index 
Adjoint, of square matrix, 137-138 
Arbitrary matrix, equivalence transformation of, 
184 
Associative data analysis, techniques in, 4-5 
Automatic Interaction Detection, 13 
Axis permutation, 151-152 
Axis rotations, 111-114, 134 
B 
Baerwaldt, N., 12 
Bartlett, M. S., 282-283 
Basic structure 
algebra of, 232-234 
conditions for, 234-235 
finding of, 235-236 
of matrix, 230-240 
of sample problem matrix of predictors, 
237-238 
Basic structure decomposition procedure, 
236-237 
Basis 
concept of, 102-103 
fixed, 133 
orthonormal, 103 
Basis vector(s), 80-83 
original and new, 112-113 
in quadratic forms, 245-246 
Basis vector changes, 103-106, 133, 213 
arbitrary, 143-146 
Basis vector transformations, 133-134, 140-146 
eigenstructures in, 203-205 
Baumol, W. J., 11 
Bilinear forms, 241-242 
Bock, R. D., 284 
Byrd, J., Jr., 13 
Calculus 
chain rule in, 300-301 
function of one argument in, 297-304 
function of two arguments in, 304-311 
in multivariate analysis, 318-321 
Canonical correlation, 12 
Carroll, J. D., 12,291-292 
Carte sian axes, oblique, 104 
Cartesian coordinates, 78-85 
Cattell, Raymond, 2-3 
Central dilation, of set of points, 152 
Centroid of variables, 120 
Chain rule, 300-301 
Characteristic equation, eigenstructures and, 
198-199,213-214 
Characteristics, variation in, 3 
see also Eigenstructures 
Cliff, N., 291 
Cluster analysis, 6, 12 
Cofactors, expansion of determinants by, 60-63 
Column vector, 27, 41 
Component loadings, in factor analysis problem, 
274-275 
Component scores, in factor analysis problem, 
273-274 
Components analysis, other aspects of, 276-278 
Composite transformations, geometric effect of, 
194 
Confidence, statistical, 4 
ConformabiUty, in matrix multiplication, 45 
Constrained optimization 
of functions of two arguments, 307-309 
Lagrange multipliers in, 309-311 
Cooley,W.W., 12, 291 
Coordinate systems, 78 
Correlation matrices, 71-73 
Cosines, law of, 93-94 
369 

370 
INDEX 
Co variance matrix, 71-72 
determinant of, 121-123 
under linear transformation, 212 
transformations of, 207-209 
Cross-national comparisons, 12 
Cross products, in matrix operations, 70, 
118-124 
D 
Data matrix 
in associative data analysis, 3-4 
partitioning of, 6, 9 
types of scales in, 7 
Decomposing, of matrix transformation, 163 
194-256 
Decompositions, basic structure of, 230-239 
Determinant(s), 58-69 
computation of, 60 
of covariance matrix, 121 
evaluation of by pivotal method, 66-69, 
184-187 
expansion of by cofactors, 60-63 
geometric interpretation of, 118-119 
matrix rank and, 173-174 
operational definition of, 59-60 
properties of, 64-66 
Diagonal matrix, 54 
Dichotomies, vs. polytomies, 7 
Differentiation 
basic rules of, 299 
partial, 304-307 
symboUc, 312-316 
Dimension-reducing methods, 10 
Distributive laws 
for matrix multiplication, 49 
for scalar product, 35-36, 45 
Do-it-yourself activities, survey of, 12-13 
Dummy variable, 7-8 
of nonsymmetric matrices, 247-254 
of symmetric matrices, 210-219 
of W-» A, 249-251 
Eigenvalues 
of C(JO, 214-217 
characteristic equation in, 199 
defined, 196 
zero, 222 
Eigenvector basis, distinguishing feature of, 205 
Eigenvectors 
of C(X), 214-217 
defined, 196 
Eisenbeis, R. A., 284 
Elementary matrix, defined, 179 
Elementary operations 
matrix inverse in, 182-184 
matrix rank in, 178-180 
in simultaneous equations, 176-178, 
180-181 
Equivalence transformation, of arbitrary matrix, 
184 
Euclidean distances, 84 
Euclidean space, 78-85 
defined, 83-85 
Euclidean metric, 83 n. 
Factor analysis, 6, 10, 12 
numerical example of, 20-22 
Factor analysis problem, 272-278 
basic structure of X^ in, 275-276 
component loadings in, 274-275 
component scores in, 273-274 
other aspects of principal components analy-
sis in, 276-278 
Functions of one argument 
derivatives of, 297-300 
differentiation of, 296-304 
optimization of, 301-304 
Functions of two arguments 
differentiation of, 304-311 
unconstrained optimization of, 307-309 
Echelon matrix, 174, 179 
Eckart, C, 287 
Eigenstructure(s), 195-207 
additional properties of, 222-225 
characteristic equation in, 198-199, 213-214 
defined, 196 
finding of, 237 
matrix rank and, 225-230 
in multiple discrimination analysis 
problem, 278-282 
Generalized inverse 
computation of A^ in, 339-341 
concept of, 334 
introductory aspects of, 334-342 
left and right, 337-339 
linear equations in, 323-350 
Penrose conditions for, 336-337 

INDEX 
371 
^inverse, 343-349 
numerical examples of, 345-349 
Good, I. J., 334 n. 
Gram-Schmidt orthonormalization process, 
106-107,221,239 
H 
Linear transformation 
see also Matrix transformation; Transforma-
tion(s) 
arbitrary, 160-163 
covariance matrix in, 212 
geometric viewpoint for, 127-190 
matrix rank and, 169-172 
Lohnes, P. R., 12, 291 
Haggard, E. A., 284 
Hamm, B. C, 13 
Hancock, H., 317 
Harman, H. H., 277 
Harris, R. J., 283 
Haynes, R. D., 13 
Horst, P., 5 n., 291-293 
Hypervolume, 101 
I 
Identity matrix, 55, 177 
linear equations in, 327 
Identity transformation, 289 
Image, in mapping, 128 
Image space, dimensionality of, 175 
Interobject similarity, 8,11 
Invariant vectors, under transformation, 196, 201 
see also Vector(s) 
Inverse 
generalized, see Generalized inverse 
of inverse, 165 
matrix, see Matrix inverse; Matrix inversion 
Invertible transformation, characteristics of, 166 
Kettenring, J. R., 292 
Komar, C A., 13 
Lagrange multiplier, 213 
Law of cosines, 93-94 
Level curves, partial differentiation and, 304-307 
Linear dependence, of vectors, 101-110 
Linear equations 
generalized inverses and, 323-350 
general procedure for solving, 327-329 
matrix transformation in, 129-130 
Linear forms, 240-241 
Linearity in parameters, defined, 7 n. 
Linear model, forms of, 270-272 
M 
McDonald, R. P., 249 n., 292 
Mapping 
concept of, 128 
images and preimages in, 128 
matrix rank and, 170 
Matrix (matrices) 
addition of, 43-45 
adjoint of, 137-138 
basic definitions and operations for, 40-52 
basic structure of, 230-240 
bilinear forms of, 241-242 
cofactorsof, 137-138 
decomposition of, 230-239 
defined, 40 
determinants of, 58-69, 137-138, 167 
diagonal, 54 
echelon, 179 
elementary, 177 
examples of, 56-57 
"exterior" dimension of, 46 
horizontal and vertical, 40 
identity, 55 
"interior" dimension of, 46 
linear forms of, 240-241 
multiplication of with vector, 47-48 
null, 41 
pivotal method for, 66-69, 184-187 
postmultiplying of, 147-148, 327 
pre- and postmultiplication of by diagonal, 
55-57 
product-moment, 227-229 
quadratic form of, 240-247 
rank of, see Matrix rank 
rotation of, 153-154 
scalar, 54 
sign, 54 
special, 52-57 
square, see Square matrix 
subtraction of, 43-44 
symmetric, see Symmetric matrix 
synthetic data, 147 
trace of, 222 

372 
INDEX 
transformation, see Matrix transformation; 
Transformation matrix 
transpose of, 42-43, 50 
vector multiplication with, 47-48 
Matrix addition, 43-45 
Matrix algebra, 22-23, 51 
Matrix concepts, from geometric viewpoint, 
77-124 
Matrix division, 51 
Matrix eigenstructures 
see also Eigenstructure(s) 
overview of, 195-207 
properties of, 219-225 
Matrix equality, 43-44 
Matrix inverse 
elementary operations and, 182-184 
point transformations and, 139-140 
properties of, 165-166 
Matrix inversion, 51,127, 136-147 
methods for, 176-189 
in multiple regression, 187-189 
Matrix matching 
internal criterion in, 292-293 
in multivariate technique classification, 
290-292 
Matrix multiplication 
conformability in, 45 
order in, 47 
properties of, 49-50 
triple product in, 48-49 
Matrix operations 
covariance and correlation matrices 
in, 71-73 
examples of, 51-52 
in statistical data, 59-73 
Matrix product, of two vectors, 48 
Matrix rank, 65, 127, 167-168 
determinants and, 173-174 
determination of, 176-189 
eigenstructures and, 225-230 
elementary operations and, 178-180 
invertible transformations and, 163-175 
use of in matrix algebra, 174-175 
Matrix relations, vs. scalar, 50 
Matrix representation, 40 
Matrix transformation 
see also Linear transformation; Point trans-
formation; Transformation matrix; Trans-
formations 
arbitrary linear, 160-163 
axis permutation and, 151-152 
central dilation and, 152 
composite, 156-163 
decomposing of, 163, 194-256 
elementary row operations in, 176-178 
geometric relationships involving, 147-156 
invertible, 163-175 
matrix rank and, 163-175 
orthogonal, 130-134 
quadratic forms and, 246-247 
reflection, 150-151 
rotation, 153-154 
rotation followed by reflection, 157 
rotation followed by stretch, 159-160 
rotation-stretch-rotation, 160 
shear, 154-156 
simultaneous equations and, 128-136 
singular matrices and, 172-173 
stretch, 152-153, 159-160 
stretch followed by rotation, 157-159 
translation, 149-150 
Matrix transpose, 42-43 
in matrix multiplication, 50 
MDA, see Multiple discriminant analysis 
Mean-corrected (SSCP) matrix, 70-71 
see also Sums of squares and cross products 
matrix 
Minkowski metric, 83 
Mitra, S. K., 349 
Model, public and private, 5 
Moore, E. H., 336 
Moore-Penrose inverse, 323, 334, 336-337, 343 
properties of, 342 
Morgan, J., 12 
Multidimensional scaling, 10 
Multiple correlation coefficient, 262 
Multiple criterion, in multiple predictor associa-
tion, 10 
Multiple discriminant analysis, 278-286 
calculus in, 320 
eigenstructure in, 278-282 
example of, 22-23 
other aspects of, 284-286 
preUminary calculations for, 250 
statistical significance and classification in, 
282-284 
Multiple predictor association 
multiple criterion in, 10 
single criterion in, 8-10 
Multiple regression, 18-20, 260-270 
alternative representation of problem 
in, 268-270 
defined, 20 
geometric aspects of problem in, 266-268 
least squares principle in, 18-20 
matrix inversion in, 187-189 
other formulations of problem in, 265-266 
overall relationship and statistical 
significance in, 262-264 
pivotal method in, 189 

INDEX 
373 
purpose of, 268-269 
response surface model in, 266-268 
selected output from, 263 
Student t value in, 264 
Multiple regression equation, 18-20, 260-262, 
318-319 
Multiple variates, association among, 2 
Multivariate analysis 
characteristics in, 2, 5-8 
data matrix in, 3 
defined, 2 
dimension-reducing methods in, 10 
eigenstructures of nonsymmetric matrices in, 
247-254 
illustrative applications of, 11-14 
interobject similarity in, 8, 11 
linear transformations in, 127 
major subdivisions of, 8 
mixed scales in, 7 
prior judgments or presuppositions in, 6 
quadratic forms in, 240-247 
translations in, 149-150 
types of associations in, 7 
types of scales in, 7 
vector and matrix operations in, 26-74 
Multivariate data, applying tools to, 259-293 
Multivariate data analysis 
see also Multivariate analysis 
nature of, 1-25 
numerical examples of, 14-23 
Multivariate functions, symbolic differentiation 
and optimization in, 295-322 
Multivariate technique classification, 286-293 
construction of, 289-293 
matrix matching in, 290-292 
transformation types in, 287-289 
vector-matrix matching in, 290 
Multivariate techniques 
see also Multivariate analysis 
characteristics of, 2 
linear transformations in, 14 
personnel data as illustration of, 15 
in scientific research, 1-4 
two types of variables in, 8-9 
uses of, 1 
N 
Neter, J., 271 
Nonsingular matrix, 177 
Nonsymmetric matrix, eigenstructures of, 
247-254 
«-space, basis of, 103 
Null matrix, 41, 45 
Null vector, 28 
O 
Oblique Cartesian axes, 104 
Oblique coordinate systems, scalar products in, 
109-110 
Optimization 
of functions involving multivariate 
arguments, 316-317 
in matrix notation, 314-316 
Orthogonal complement, 108 
Orthogonal matrix, properties of, 116-118 
Orthogonal transformations, 111-118 
Orthonormal basis, 103-104 
finding of, 106-108 
three-dimensional, 108 
Pair of variables, correlation and covariance of, 
121 
Parallelogram law, 104 
Parameters, linearity in, 7 
Partial differentiation, level curves and, 304-307 
Partitioned subsets, number of variables in, 6-7 
Partitionings, of data matrix, 9 
Penrose, R. A., 323, 336 
Penrose conditions, for generalized inverse, 
336-341 
Permutation, of set of points, 151-152 
Permutation transformation, 289 
Perry, M., 13 
Personnel data, multivariate methods in, 14-17 
Pivotal methods 
evaluation of determinant by, 66-69, 
184-187 
in sample regression problem, 189 
Point rotations, 111-114, 130-133 
Points 
permutation of , 151 -15 2 
reflection of, 150-151 
rigid rotation of set by, 111 
stationary vs. extreme, 317 
Point transformations, 132 
eigenstructures ai^, 203-205 
fixed basis vectors in, 135 
matrix inverse ar^, 139-140, 145 
Polytomies, vs. dichotomies, 7 
Predictor variables, basic structure and, 237-238 
Prefactor, in matrix multipUcation, 46 
Preimages, in mapping, 128 
Premultiplying, of vectors, 154 
Principal components analysis, calculus in, 319 
Pringle, R. M., 349 

374 
INDEX 
Product information, personal vs. impersonal 
sources of, 13 
Production jobs, sequencing of, 13 
Product-moment matrices, 255 
special characteristics of, 227-229 
Project TALENT, 12 
Pseudoinverse, 334 
Pythagorean theorem, 94 
Quadratic forms, 240-247 
basis vectors and, 245-246 
defined, 242 
diagonalizing of, 251-254 
illustrative problem in, 243-244 
matrix transformations and, 246-247 
ratio of, 249 
Quandt, R. E., 11 
R 
Rank, of matrix, 167-168, 174-176 
see also Matrix rank 
Rao, C R., 66, 284, 323, 349 
Ratio scales, 5 
Rayner, A. A., 349 
Rectangular matrix 
eigenstructure of, 226-227 
rank of, 226-227 
Reflection, of set of points, 150-151 
Regression problem, defined, 3 
see also Multiple regression 
Research 
focus of interest in, 6 
multivariate methods in, 2-4 
objectives and predictive statements in, 5-6 
Response surface model, variations in, 273 
Rorer, 11 
Rotation(s) 
followed by reflection, 157 
followed by stretch, 159-160 
higher-dimensional, 115-116 
improper, 117 
of matrix, 153-154 
point, 111-114, 130-133 
three-dimensional, 116 
trigonometry of, 114-115 
two-dimensional, 111-115 
Rotation 
problem, 
matrix 
eigenstructure in, 
228-229 
Rotation transformation, 288 
Row vector, 27 
Rummel, R. J., 12 
Scalar arithmetic, vs. matrix arithmetic, 50 
Scalar matrix, 54 
Scalar multipUcation, 43-44 
properties of, 45 
Scalar product, 35-38 
defined, 35 
distributive laws for, 35-36, 45 
examples of, 39 
in oblique coordinate systems, 109-110 
special cases of, 36-38 
of two vectors, 96-97 
vector projections and, 97-100 
Scatter plot 
in factor analysis, 21 
in multivariate analysis, 16-17 
Schatzoff, 284 
Schonemann, 291 
Scientific research, multivariate techniques in, 
2-6 
Set of points 
central dilation and, 152 
permutation of, 151-152 
rigid rotation of, 111 
stretch transformation of, 152-153 
Shear transformation, 155-156 
Sign matrix, 54 
Sign vector, 28 
Simultaneous equations 
elementary operations and, 180-181 
in matrix form, 129-130 
matrix of coefficients and, 182-184 
matrix transformations and, 128-136 
Simultaneous linear equations, 324-334 
homogeneous equations and, 331-334 
Single criterion, in multiple predictor 
association, 9-10 
Sirageldin, I., 12 
Skew symmetric matrix, 54 
Spherizing, 252 
Square matrix, 40-41 
adjoint of, 137 
cofactor of entry of, 61 
eigenstructure of, 195-206, 225 
Hermite form of, 343-345 
inverse of, 136 
minor of an entry of, 61 
singular and nonsingular, 65 
symmetric, 42-43, 53 
vs. triangular matrix, 219 n. 
SSCP, see Sums of squares and cross products 
matrix 
Stationary point, as extreme point, 317 
Statistical data, matrix operations in, 69-73 

INDEX 
375 
Statistical measures, geometry of, 119-122 
Statistical variables, generalized variance among, 
118 
Stretch, 157-160 
Stretch transformation, 153-154, 289 
Student t value, defined, 264 
Sums of squares, in matrix operations, 70 
Sums of squares and cross product matrix, 
26, 71-73, 120, 150, 252-253, 283, 296, 
319-320 
Symbolic differentiation, 312-316 
Symmetric matrix, 42-43, 53-54 
eigenstructure of, 210-219 
orthogonal diagonalization in, 230 
properties of, 220-222 
Tatsuoka, M. M., 5 n. 
Tiedeman, D. V., 5 n. 
Transformational geometry, 22-23 
Transformation matrix, 119 
see also Matrix transformation; Transforma-
tions 
diagonalizing of, 202-203 
postmultiplying of, 202 
rank of, 175 
Transformations 
see also Matrix transformation 
arbitrary linear, 160-163 
by basis vector change, 140-146 
central dilation, 289 
composite, 156-163 
of covariance matrices, 207-209 
general linear (affine), 288 
homogeneous linear, 288 
identity, 289 
invertible, 166 
linear, see Linear transformation 
permutation, 289 
rotation, 288 
rotation-annihilation, 288 
similarity, 288 
of singular matrices, 172-173 
stretch, 289 
types of in multivariate technique 
classification, 287-289 
Translation, of matrices, 149-150 
Travel-demand forecasting model, 11 
Triple matrix product, 255 
Trivial solution of homogeneous equations, 331 
Unit vector, 28 
Van de Geer, J. P., 277, 292 
Variable(s) 
batteries of, 10 
correlation of pairs of, 121 
covariance of pairs of, 121 
criterion vs. predictor, 3, 5 
dependent and independent, 3 
dummy, 7-8 
latent, 10 
linear composites and, 7 
scale types and, 8-9 
variance of, 120 
Variable interdependence, analysis of, 8 
Variance measure, generalized, 122-124 
Vector(s) 
addition and subtraction of, 30-31, 38-39 
basic definitions and operations in, 27-39 
basis, see Basis vector(s) 
column and row, 27 
defined, 27 
difference, 90 
direction cosines of, 86 
distance and angle between, 92-96 
equality of, 29, 80 
geometric representation of, 77, 85- 100 
length and direction angles of, 85-88 
linear combination of, 33-34, 39 
linear dependence of, 101-110 
matrix product of, 48 
multiplication with matrix, 47-48 
multiplication with scalar, 91 
/i-component, 82 
nonredundant, 102 
null or zero, 28 
orthogonal, 94 
parallel displacement of, 80 
premultiplying of, 154 
representation of in three-dimensional space, 
81 
row, 27 
scalar multiplication of, 32-33, 39 
scalar product of, 35-38, 96-97 
sign, 28 
squared distance between, 95 
subtraction of, 90 
transformation of, 128, 140-146 
transpose of, 38 
unit, 28 
zero-one, 28 
Vector addition, 88-92 
Vector and matrix operations, 26-74 
Vector concepts, from geometric viewpoint, 
77-124 

376 
INDEX 
Vector equality, 29 
Vector length 
direction angles and, 86 
scalar product and, 106 
Vector-matrix matching, in multivariate 
technique classification, 290 
Vector multiplication, 47-49 
geometric aspects of, 88-92 
Vector notation, 26 
Vector projection 
geometric interpretation of, 98 
scalar products and, 97-100 
Vector representation, 27 
Vector space 
basis change in, 213 
dimensionality of, 102-103 
Vector transformation, 128 
basis vector change and, 140-146 
W 
Wasserman,W., 271 
Wind, Y., 277 
Wish, M., 12 
Young, G., 287 
Zero eigenvalues, 222 
Zero-one vectors, 28 


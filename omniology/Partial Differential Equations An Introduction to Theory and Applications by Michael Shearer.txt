
Partial Differential Equations

Partial Differential Equations
An Introduction to Theory and Applications
Michael Shearer
Rachel Levy
PRINCETON UNIVERSITY PRESS
Princeton and Oxford

Copyright © 2015 by Princeton University Press
Published by Princeton University Press, 41 William Street, Princeton, New Jersey 08540
In the United Kingdom: Princeton University Press, 6 Oxford Street, Woodstock, Oxfordshire OX20 1TW
press.princeton.edu
Cover photograph courtesy of Michael Shearer and Rachel Levy.
Cover design by Lorraine Betz Doneker.
All Rights Reserved
Library of Congress Cataloging-in-Publication Data
Shearer, Michael.
Partial differential equations : an introduction to theory and applications / Michael Shearer, Rachel Levy.
Pages    cm
Includes bibliographical references and index.
ISBN 978-0-691-16129-7 (cloth : alk. paper)—ISBN 0-691-16129-1 (cloth : alk. paper)
1. Differential equations, Partial.    I. Levy, Rachel, 1968–    II. Title.
QA374.S45    2015
515′.353—dc23
2014034777
British Library Cataloging-in-Publication Data is available
This book has been composed in Minion Pro with Myriad Pro and DIN display using ZzTEX by Princeton
Editorial Associates Inc., Scottsdale, Arizona
Printed on acid-free paper. 
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

Contents
Preface   ix
   1.    Introduction   1
  1.1.   Linear PDE   2
  1.2.   Solutions; Initial and Boundary Conditions   3
  1.3.   Nonlinear PDE   4
  1.4.   Beginning Examples with Explicit Wave-like Solutions   6
Problems   8
   2.    Beginnings   11
  2.1.   Four Fundamental Issues in PDE Theory   11
  2.2.   Classification of Second-Order PDE   12
  2.3.   Initial Value Problems and the Cauchy-Kovalevskaya Theorem   17
  2.4.   PDE from Balance Laws   21
Problems   26
   3.    First-Order PDE   29
  3.1.   The Method of Characteristics for Initial Value Problems   29
  3.2.   The Method of Characteristics for Cauchy Problems in Two Variables   32
  3.3.   The Method of Characteristics in Rn   35
  3.4.   Scalar Conservation Laws and the Formation of Shocks   38
Problems   40
   4.    The Wave Equation   43
  4.1.   The Wave Equation in Elasticity   43
  4.2.   D’Alembert’s Solution   48
  4.3.   The Energy E(t) and Uniqueness of Solutions   56
  4.4.   Duhamel’s Principle for the Inhomogeneous Wave Equation   57
  4.5.   The Wave Equation on R2 and R3   59
Problems   61
   5.    The Heat Equation   65
  5.1.   The Fundamental Solution   66
  5.2.   The Cauchy Problem for the Heat Equation   68
  5.3.   The Energy Method   73

  5.4.   The Maximum Principle   75
  5.5.   Duhamel’s Principle for the Inhomogeneous Heat Equation   77
Problems   78
   6.    Separation of Variables and Fourier Series   81
  6.1.   Fourier Series   81
  6.2.   Separation of Variables for the Heat Equation   82
  6.3.   Separation of Variables for the Wave Equation   91
  6.4.   Separation of Variables for a Nonlinear Heat Equation   93
  6.5.   The Beam Equation   94
Problems   96
   7.    Eigenfunctions and Convergence of Fourier Series   99
  7.1.   Eigenfunctions for ODE   99
  7.2.   Convergence and Completeness   102
  7.3.   Pointwise Convergence of Fourier Series   105
  7.4.   Uniform Convergence of Fourier Series   108
  7.5.   Convergence in L2   110
  7.6.   Fourier Transform   114
Problems   117
   8.    Laplace’s Equation and Poisson’s Equation   119
  8.1.   The Fundamental Solution   119
  8.2.   Solving Poisson’s Equation in Rn   120
  8.3.   Properties of Harmonic Functions   122
  8.4.   Separation of Variables for Laplace’s Equation   125
Problems   130
   9.    Green’s Functions and Distributions   133
  9.1.   Boundary Value Problems   133
  9.2.   Test Functions and Distributions   136
  9.3.   Green’s Functions   144
Problems   149
 10.    Function Spaces   153
10.1.   Basic Inequalities and Definitions   153

10.2.   Multi-Index Notation   157
10.3.   Sobolev Spaces Wk,p(U)   158
Problems   159
 11.    Elliptic Theory with Sobolev Spaces   161
11.1.   Poisson’s Equation   161
11.2.   Linear Second-Order Elliptic Equations   167
Problems   173
 12.    Traveling Wave Solutions of PDE   175
12.1.   Burgers’ Equation   175
12.2.   The Korteweg-deVries Equation   176
12.3.   Fisher’s Equation   179
12.4.   The Bistable Equation   181
Problems   186
 13.    Scalar Conservation Laws   189
13.1.   The Inviscid Burgers Equation   189
13.2.   Scalar Conservation Laws   196
13.3.   The Lax Entropy Condition Revisited   201
13.4.   Undercompressive Shocks   204
13.5.   The (Viscous) Burgers Equation   206
13.6.   Multidimensional Conservation Laws   208
Problems   211
 14.    Systems of First-Order Hyperbolic PDE   215
14.1.   Linear Systems of First-Order PDE   215
14.2.   Systems of Hyperbolic Conservation Laws   219
14.3.   The Dam-Break Problem Using Shallow Water Equations   239
14.4.   Discussion   241
Problems   242
 15.    The Equations of Fluid Mechanics   245
15.1.   The Navier-Stokes and Stokes Equations   245
15.2.   The Euler Equations   247
Problems   250

Appendix A. Multivariable Calculus   253
Appendix B. Analysis   259
Appendix C. Systems of Ordinary Differential Equations   263
References   265
Index   269

Preface
The field of partial differential equations (PDE for short) has a long history going
back several hundred years, beginning with the development of calculus. In this
regard, the field is a traditional area of mathematics, although more recent than
such classical fields as number theory, algebra, and geometry. As in many areas
of mathematics, the theory of PDE has undergone a radical transformation in the
past hundred years, fueled by the development of powerful analytical tools,
notably, the theory of functional analysis and more specifically of function
spaces. The discipline has also been driven by rapid developments in science and
engineering, which present new challenges of modeling and simulation and
promote broader investigations of properties of PDE models and their solutions.
As the theory and application of PDE have developed, profound unanswered
questions and unresolved problems have been identified. Arguably the most
visible is one of the Clay Mathematics Institute Millennium Prize problems1
concerning the Euler and Navier-Stokes systems of PDE that model fluid flow.
The Millennium problem has generated a vast amount of activity around the
world in an attempt to establish well-posedness, regularity and global existence
results, not only for the Navier-Stokes and Euler systems but also for related
systems of PDE modeling complex fluids (such as fluids with memory, polymeric
fluids, and plasmas). This activity generates a substantial literature, much of it
highly specialized and technical. Meanwhile, mathematicians use analysis to
probe new applications and to develop numerical simulation algorithms that are
provably accurate and efficient. Such capability is of considerable importance,
given the explosion of experimental and observational data and the spectacular
acceleration of computing power.
Our text provides a gateway to the field of PDE. We introduce the reader to a
variety of PDE and related techniques to give a sense of the breadth and depth of
the field. We assume that students have been exposed to elementary ideas from
ordinary differential equations (ODE) and analysis; thus, the book is appropriate
for advanced undergraduate or beginning graduate mathematics students. For the
student preparing for research, we provide a gentle introduction to some current
theoretical approaches to PDE. For the applied mathematics student more
interested in specific applications and models, we present tools of applied
mathematics in the setting of PDE. Science and engineering students will find a
range of topics in the mathematics of PDE, with examples that provide physical
intuition.
Our aim is to familiarize the reader with modern techniques of PDE,

introducing abstract ideas straightforwardly in special cases. For example,
struggling with the details and significance of Sobolev embedding theorems and
estimates is more easily appreciated after a first introduction to the utility of
specific spaces. Many students who will encounter PDE only in applications to
science and engineering or who want to study PDE for just a year will appreciate
this focused, direct treatment of the subject. Finally, many students who are
interested in PDE have limited experience with analysis and ODE. For these
students, this text provides a means to delve into the analysis of PDE before or
while taking first courses in functional analysis, measure theory, or advanced
ODE. Basic background on functions and ODE is provided in Appendices A–C.
To keep the text focused on the analysis of PDE, we have not attempted to
include an account of numerical methods. The formulation and analysis of
numerical algorithms is now a separate and mature field that includes major
developments in treating nonlinear PDE. However, the theoretical understanding
gained from this text will provide a solid basis for confronting the issues and
challenges in numerical simulation of PDE.
A student who has completed a course organized around this text will be
prepared to study such advanced topics as the theory of elliptic PDE, including
regularity, spectral properties, the rigorous treatment of boundary conditions; the
theory of parabolic PDE, building on the setting of elliptic theory and motivating
the abstract ideas in linear and nonlinear semigroup theory; existence theory for
hyperbolic equations and systems; and the analysis of fully nonlinear PDE.
We hope that you, the reader, find that our text opens up this fascinating,
important, and challenging area of mathematics. It will inform you to a level
where you can appreciate general lectures on PDE research, and it will be a
foundation for further study of PDE in whatever direction you wish.
We are grateful to our students and colleagues who have helped make this book
possible, notably David G. Schaeffer, David Uminsky, and Mark Hoefer for their
candid and insightful suggestions. We are grateful for the support we have
received from the fantastic staff at Princeton University Press, especially Vickie
Kern, who has believed in this project from the start.
Rachel Levy thanks her parents Jack and Dodi, husband Sam, and children
Tula and Mimi, who have lovingly encouraged her work.
Michael Shearer thanks the many students who provided feedback on the
course notes from which this book is derived.
1. www.claymath.org/millennium/.

Partial Differential Equations

CHAPTER ONE
Introduction
Partial differential equations (PDE) describe physical systems, such as solid and
fluid mechanics, the evolution of populations and disease, and mathematical
physics. The many different kinds of PDE each can exhibit different properties.
For example, the heat equation describes the spreading of heat in a conducting
medium, smoothing the spatial distribution of temperature as it evolves in time;
it also models the molecular diffusion of a solute in its solvent as the
concentration varies in both space and time. The wave equation is at the heart of
the description of time-dependent displacements in an elastic material, with wave
solutions that propagate disturbances. It describes the propagation of p-waves
and s-waves from the epicenter of an earthquake, the ripples on the surface of a
pond from the drop of a stone, the vibrations of a guitar string, and the resulting
sound waves. Laplace’s equation lies at the heart of potential theory, with
applications to electrostatics and fluid flow as well as other areas of mathematics,
such as geometry and the theory of harmonic functions. The mathematics of PDE
includes the formulation of techniques to find solutions, together with the
development of theoretical tools and results that address the properties of
solutions, such as existence and uniqueness.
This text provides an introduction to a fascinating, intricate, and useful
branch of mathematics. In addition to covering specific solution techniques that
provide an insight into how PDE work, the text is a gateway to theoretical studies
of PDE, involving the full power of real, complex and functional analysis. Often
we will refer to applications to provide further intuition into specific equations
and their solutions, as well as to show the modeling of real problems by PDE.
The study of PDE takes many forms. Very broadly, we take two approaches in
this book. One approach is to describe methods of constructing solutions, leading
to formulas. The second approach is more theoretical, involving aspects of
analysis, such as the theory of distributions and the theory of function spaces.
1.1. Linear PDE
To introduce PDE, we begin with four linear equations. These equations are basic
to the study of PDE, and are prototypes of classes of equations, each with
different properties. The primary elementary methods of solution are related to
the techniques we develop for these four equations.
For each of the four equations, we consider an unknown (real-valued)

function u on an open set U ⊂ Rn. We refer to u as the dependent variable, and x
= (x1, x2, …, xn) ∈ U as the vector of independent variables. A partial differential
equation is an equation that involves x, u, and partial derivatives of u. Quite
often, x represents only spatial variables. However, many equations are
evolutionary, meaning that u = u(x, t) depends also on time t and the PDE has
time derivatives. The order of a PDE is defined as the order of the highest
derivative that appears in the equation.
The Linear Transport Equation:
This simple first-order linear PDE describes the motion at constant speed c of a
quantity u depending on a single spatial variable x and time t. Each solution is a
traveling wave that moves with the speed c. If c > 0, the wave moves to the right;
if c < 0, the wave moves left. The solutions are all given by a formula u(x, t) =
f(x − ct). The function f = f(ξ), depending on a single variable ξ = x − ct, is
determined from side conditions, such as boundary or initial conditions.
The next three equations are prototypical second-order linear PDE.
The Heat Equation:
In this equation, u(x, t) is the temperature in a homogeneous heat-conducting
material, the parameter k > 0 is constant, and the Laplacian Δ is defined by
in Cartesian coordinates. The heat equation, also known as the diffusion
equation, models diffusion in other contexts, such as the diffusion of a dye in a
clear liquid. In such cases, u represents the concentration of the diffusing
quantity.
The Wave Equation:
As the name suggests, the wave equation models wave propagation. The
parameter c is the wave speed. The dependent variable u = u(x, t) is a
displacement, such as the displacement at each point of a guitar string as the
string vibrates, if x ∈ R, or of a drum membrane, in which case x ∈ R2. The
acceleration utt, being a second time derivative, gives the wave equation quite
different properties from those of the heat equation.

Laplace’s Equation:
Laplace’s equation models equilibria or steady states in diffusion processes, in
which u(x, t) is independent of time t,1 and appears in many other contexts, such
as the motion of fluids, and the equilibrium distribution of heat.
These three second-order equations arise often in applications, so it is very
useful to understand their properties. Moreover, their study turns out to be useful
theoretically as well, since the three equations are prototypes of second-order
linear equations, namely, elliptic, parabolic, and hyperbolic PDE.
1.2. Solutions; Initial and Boundary Conditions
A solution of a PDE such as any of (1.1)–(1.4) is a real-valued function u satisfying
the equation. Often this means that u is as differentiable as the PDE requires, and
the PDE is satisfied at each point of the domain of u. However, it can be
appropriate or even necessary to consider a more general notion of solution, in
which u is not required to have all the derivatives appearing in the equation, at
least not in the usual sense of calculus. We will consider this kind of weak solution
later (see Chapter 11).
As with ordinary differential equations (ODE), solutions of PDE are not
unique; identifying a unique solution relies on side conditions, such as initial and
boundary conditions. For example, the heat equation typically comes with an
initial condition of the form
in which u0 : U → R is a given function.
Example 1. (Simple initial condition) The functions u(x, t) = ae−t sin x + be−4t
sin(2x) are solutions of the heat equation ut = uxx for any real numbers a, b.
However, a = 3, b = −7 would be uniquely determined by the initial condition
u(x, 0) = 3 sin x − 7 sin(2x). Then u(x, t) = 3e−t sin x − 7e−4t sin(2x).
Boundary conditions are specified on the boundary ∂U of the (spatial)
domain. Dirichlet boundary conditions take the following form, for a given function
f : ∂U → R:
Neumann boundary conditions specify the normal derivative of u on the boundary:

where ν(x) is the unit outward normal to the boundary at x. These boundary
conditions are called homogeneous if f ≡ 0. Similarly, a linear PDE is called
homogeneous if u = 0 is a solution. If it is not homogeneous, then the equation or
boundary condition is called inhomogeneous.
Equations and boundary conditions that are linear and homogeneous have the
property that any linear combination u = av + bw of solutions v, w, with a, b ∈
R, is also a solution. This special property, sometimes called the principle of
superposition, is crucial to constructive methods of solution for linear equations.
1.3. Nonlinear PDE
We introduce a selection of nonlinear PDE that are significant by virtue of
specific properties, special solutions, or their importance in applications.
The Inviscid Burgers Equation:
is an example of a nonlinear first-order equation. Notice that this equation is
nonlinear due to the uux term. It is related to the linear transport equation (1.1),
but the wave speed c is now u and depends on the solution. We shall see in
Chapter 3 that this equation and other first-order equations can be solved
systematically using a procedure called the method of characteristics. However, the
method of characteristics only gets you so far; solutions typically develop a
singularity, in which the graph of u as a function of x steepens in places until at
some finite time the slope becomes infinite at some x. The solution then
continues with a shock wave. The solution is not even continuous at the shock,
but the solution still makes sense, because the PDE expresses a conservation law
and the shock preserves conservation.
For higher-order nonlinear equations, there are no methods of solution that
work in as much generality as the method of characteristics for first-order
equations. Here is a sample of higher-order nonlinear equations with interesting
and accessible solutions.
Fisher’s Equation:
with f(u) = u(1 − u). This equation is a model for population dynamics when
the spatial distribution of the population is taken into account. Notice the
resemblance to the heat equation; also note that the ODE u′(t) = f(u(t)) is the
logistic equation, describing population growth limited by a maximum population
normalized to u = 1. In Chapter 12, we shall construct traveling waves, special

solutions in which the population distribution moves with a constant speed in
one direction. Recall that all solutions of the linear transport equation (1.1) are
traveling waves, but they all have the same speed c. For Fisher’s equation, we
have to determine the speeds of traveling waves as part of the problem, and the
traveling waves are special solutions, not the general solution.
The Porous Medium Equation:
In this equation, m > 0 is constant. The porous medium equation models flow in
porous rock or compacted soil. The variable u(x, t) ≥ 0 measures the density of a
compressible gas in a given location x at time t. The value of m depends on the
equation of state relating pressure in the gas to its density. For m = 1, we
recover the heat equation, but for m ≠ 1, the equation is nonlinear. In fact, m ≥
2 for gas flow.
The Korteweg-deVries (KdV) Equation:
This third-order equation is a model for water waves in which the height of the
wave is u(x, t). The KdV equation has particularly interesting traveling wave
solutions called solitary waves, in which the height is symmetric about a single
crest. The equation is a model in the sense that it relies on an approximation of
the equations of fluid mechanics in which the length of the wave is large
compared to the depth of the water.
Burgers’ Equation:
The parameter ν > 0 represents viscosity, hence the name inviscid Burgers
equation for the first-order equation (1.6) having ν = 0. Burgers’ equation is a
combination of the heat equation with a nonlinear term that convects the
solution in a way typical of fluid flow. (See the Navier-Stokes system later in this
list.) This important equation can be reduced to the heat equation with a clever
change of dependent variable, called the Cole-Hopf transformation (see Chapter
13, Section 13.5).
Finally, we mention two systems of nonlinear PDE.
The Shallow Water Equations:
in which g > 0 is the gravitational acceleration. The dependent variables h, v

represent the height and velocity, respectively, of a shallow layer of water. The
variable x is the horizontal spatial variable, along a flat bottom, and it is assumed
that there is no dependence or motion in the orthogonal horizontal direction.
Moreover, the velocity v is taken to be independent of depth.
The Navier-Stokes Equations:
describe the velocity u ∈ R3 and pressure p in the flow of an incompressible
viscous fluid. In this system of four equations, the parameter ν > 0 is the
viscosity, the first three equations (for u) represent conservation of momentum,
and the final equation is a constraint that expresses the incompressibility of the
fluid. In an incompressible fluid, local volumes are unchanged in time as they
follow the flow. Apart from special types of flow (such as in a stratified fluid),
incompressibility also means that the density is constant (and is incorporated into
ν, the kinematic viscosity).
Interestingly, the momentum equation, regarded as an evolution equation for
u, resembles Burgers’ equation in structure. The pressure p does not have its own
evolution equation; it serves merely to maintain incompressibility. In the limit ν
→ 0, we recover the incompressible Euler equations for an inviscid fluid. This is a
singular limit in the sense that the order of the momentum equation is reduced. It
is also a singular limit for Burgers’ equation.
1.4. Beginning Examples with Explicit Wave-like Solutions
The linear and nonlinear first order equations described in Sections 1.1 and 1.3
nicely illustrate mathematical properties and representation of wave-like
solutions. We discuss these equations and their solutions as a starting point for
more general considerations.
1.4.1. The Linear Transport Equation
Solutions of the linear transport equation,
where c ∈ R is a constant (the wave speed), are traveling waves u(x, t) = f(x −
ct). We can determine a unique solution by specifying the function f : R → R
from an initial condition

Figure 1.1. Linear transport equation: traveling wave solution. (a) t = 0; (b) t >
0.
in which u0 : R → R is a given function. Then the unique solution of the initial
value problem (1.8), (1.9) is the traveling wave u(x, t) = u0(x − ct). A typical
traveling wave is shown in Figure 1.1.
Instead of initial conditions, we can also specify a boundary condition for this
PDE. Here is an example of how this would look, for functions ϕ, ψ given on the
interval [0, ∞):
The solution u of (1.8), (1.10) will be a function defined on the first quadrant Q1
= {(x, t) : x ≥ 0, t ≥ 0} in the x-t plane. The general solution of the PDE is u(x,
t) = f(x − ct); the initial condition specifies f(y) for y > 0, and the boundary
condition gives f(y) for y < 0. Both are needed to determine the solution u(x, t)
on Q1.
1.4.2. The Inviscid Burgers Equation
This equation,
has wave speed u that depends on the solution, in contrast to the linear transport
equation (1.8) in which the wave speed c is constant. If we use the wave speed to
track the solution, we can sketch its evolution. In Figure 1.2 we show how an
initial condition (1.9) evolves for small t > 0. Points nearer the crest travel
faster, since u is larger there, so the front of the wave tends to steepen, while the
back spreads out. Notice how Figure 1.2 differs from Figure 1.1. The solution u =
u(x, t) can be specified implicitly in an equation without derivatives:

Figure 1.2. Inviscid Burgers equation: nonlinear wave propagation. (a) t = 0; (b)
t > 0.
Eventually, the graph becomes infinitely steep, and the implicit solution in (1.12)
is no longer valid. The solution is continued to larger time by including a shock
wave, defined in Chapter 13.
PROBLEMS
1. Show that the traveling wave u(x, t) = f(x − 3t) satisfies the linear transport
equation ut + 3ux = 0 for any differentiable function f : R → R.
2. Find an equation relating the parameters k, m, n so that the function u(x, t) =
emt sin(nx) satisfies the heat equation ut = kuxx.
3. Find an equation relating the parameters c, m, n so that the function u(x, t) =
sin(mt) sin(nx) satisfies the wave equation utt = c2uxx.
4. Find all functions a, b, c : R → R such that u(x, t) = a(t)e2x + b(t)ex + c(t)
satisfies the heat equation ut = uxx for all x, t.
5. For m > 1, define the conductivity k = k(u) so that the porous medium
equation (1.7) can be written as the (quasilinear) heat equation
6. Solve the initial value problem
7. Solve the initial boundary value problem
Explain why there is no solution if the PDE is changed to ut − 4ux = 0.

8. Consider the linear transport equation (1.8) with initial and boundary
conditions (1.10).
(a) Suppose the data ϕ, ψ are differentiable functions. Show that the function
u : Q1 → R given by
satisfies the PDE away from the line x = ct, the boundary condition, and
initial condition. To see where (1.13) comes from, start from the general
solution u(x, t) = f(x − ct) of the PDE and substitute into the side conditions
(1.10).
(b) In solution (1.13), the line x = ct, which emerges from the origin x = t =
0, separates the quadrant Q1 into two regions. On the line, the solution has
one-sided limits given by ϕ, ψ. Consequently, the solution will in general have
singularities on the line.
(i) Find conditions on the data ϕ, ψ so that the solution is continuous
across the line x = ct.
(ii) Find conditions on the data ϕ, ψ so that the solution is differentiable
across the line x = ct.
9. Let f : R → R be differentiable. Verify that if u(x, t) is differentiable and
satisfies (1.12), that is, u = f(x − ut), then u(x, t) is a solution of the initial value
problem
10. Let u0(x) = 1 − x2 if −1 ≤ x ≤ 1, and u0(x) = 0 otherwise.
(a) Use (1.12) to find a formula for the solution u = u(x, t) of the inviscid
Burgers equation (1.11), (1.9) with −1 < x < 1, 
.
(b) Verify that u(1, t) = 0, 
.
(c) Differentiate your formula to find ux(1−, t), and deduce that ux(1−, t) →
−∞ as 
.
Note: ux(x, t) is discontinuous at x = ±1; the notation u (1−, t) means the
one-sided limit: 
. Similarly, 
 means, 
, with 
.
1. However, there are time-dependent solutions, for example u(x, t) linear in x or independent of x.

CHAPTER TWO
Beginnings
In the previous chapter we constructed solutions for example equations.
However, much of the study of PDE is theoretical, revolving around issues of
existence and uniqueness of solutions, and properties of solutions derived without
writing formulas for the solutions. Of course, existence and uniqueness issues are
resolved if it is possible to construct all solutions of a given PDE, but commonly
this constructive approach is not available, and more abstract methods of analysis
are required. In this chapter we outline theoretical considerations that will come
up from time to time, give a somewhat general classification of single equations,
and then give a flavor of theoretical approaches by presenting the Cauchy-
Kovalevskaya theorem and discussing some of its ramifications. Finally, we show
how PDE can be derived from balance laws (otherwise known as conservation
laws) that come from fundamental considerations underlying the modeling of
most applications.
2.1. Four Fundamental Issues in PDE Theory
Generally, the theoretical study of PDE focuses on four basic issues, three of
which are lumped together as well-posedness in the sense of Hadamard.1
1. Existence: Is there a solution of the PDE satisfying a specific set of boundary
and initial conditions?
2. Uniqueness: Is there only one solution for a specific set of boundary and initial
conditions?
3. Continuous dependence on data: Do small changes in initial conditions,
boundary conditions, and parameters create only small changes in the
solution? We might say the solution is robust to changes in the data.
Sometimes, this property is called structural stability, or more loosely, stability.
The fourth property is generally separated from considerations of well-posedness:
4. Regularity: How many derivatives does the solution have? We sometimes refer
to this property as the smoothness of the solution.
Well-posedness is a desirable property if the goal is to model a repeatable
experiment, for example. Of the four properties, one could argue that the most
important property is existence. After all, what use is a PDE model if it does not
have a solution? In the theory of ODE, showing the existence of solutions is
generally straightforward, at least locally, based on the classical existence and

uniqueness theorem for initial value problems. In the previous chapter we
established existence by constructing solutions. However, in general the theory of
existence of solutions for PDE is a complex and highly technical subject.
Existence. The approach of this book is to study existence issues only for classes
of equations (and classes of solutions) for which the theory is elementary, such as
classical (i.e., continuously differentiable) solutions of first-order equations. For
second-order equations, we begin by choosing problems for which we can
construct explicit solutions, thus avoiding the technicalities of proving general
existence theorems. Toward the end of the book (see Chaps. 9–11), we introduce
some of the theoretical underpinnings of more general theories of PDE, such as
the theory of distributions, the use of Sobolev spaces, and maximum principles.
Uniqueness. Uniqueness is often the easiest property to establish. Moreover, it
does not require the existence of solutions, as we can state: “There exists at most
one solution.”
Continuous dependence. Continuous dependence can be established using
techniques from analysis that estimate the closeness of distinct solutions with
different data, in terms of the closeness of the data. Closeness of course involves
defining a suitable notion of distance—a metric—on both the space in which
solutions reside and on the space of data. These notions will be formally
introduced as needed.
Regularity. Regularity is generally the hardest property to characterize,
requiring the most delicate analysis. In this text we make observations about
regularity from explicit solutions; regularity more generally and theoretically
involves more technical machinery.
2.2. Classification of Second-Order PDE
When studying ODE, it is convenient to be able to distinguish among different
kinds of equations based on such criteria as linear vs. nonlinear and separable vs.
nonseparable. For PDE, there are also multiple ways to distinguish among
equations, some similar to the criteria for ODE. In the next chapter we discuss
first-order PDE in detail, showing that the theory is linked closely to systems of
first-order ODE.
For second-order equations, there are distinct families of equations,
distinguished by typical properties of their solutions. We identify the class of
hyperbolic equations, with wave-like solutions, and elliptic equations,
representing steady-state or equilibrium solutions. Between these two general
classes are the parabolic equations, which, like hyperbolic equations, have a

time-like independent variable but also have properties akin to those of elliptic
equations. The heat equation, the wave equation, and Laplace’s equation are
second-order linear constant-coefficient prototypes of parabolic, hyperbolic, and
elliptic PDE, respectively. Although this chapter is primarily about linear
equations in two variables, we include some remarks about equations with more
independent variables and nonlinear equations.
2.2.1. Constant Coefficients
To explain how the terms hyperbolic, elliptic, and parabolic come to be associated
with PDE, it is simplest to consider a second-order equation of the form
where the coefficients a, b, c are real numbers, and the right-hand side f = f(x, y,
u, ux, uy) is a given function containing any lower-order derivatives of u. The type
of the equation is determined by the nature of the quadratic form obtained from
the left-hand side of (2.1) by replacing each partial derivative by a real variable.
More formally, we define the principal part of the PDE as the left-hand side of
(2.1). Then the corresponding differential operator with principal indicated by the
superscript (p) is
Associated with this differential operator is the quadratic form, known as the
principal symbol,
in which ξ = (ξ1, ξ2) ∈R2. The connection between principal part and principal
symbol is the observation
This conversion from differential operators ∂x, ∂y to multiplication by iξ1, iξ2 is
typical of integral transforms; in this case, the connection is to Fourier
transforms. The vector (ξ1, ξ2) is the Fourier transform variable, or wave number.
Fourier transforms and their importance for the analysis of PDE are discussed in
Chapter 7.
The quadratic form (2.2) is associated with either a hyperbola (if b2 > ac), an
ellipse (if b2 < ac), or is degenerate (if b2 = ac). Correspondingly, we say the
PDE (2.1) is hyperbolic if b2 > ac, elliptic if b2 < ac, and parabolic if b2 = ac,
provided the equation is second order (i.e., not all of a, b, c are zero).
Example 1. (Classification) The partial differential operator L = ∂2x + α∂2y, is

elliptic for α > 0, hyperbolic for α < 0, and parabolic for α = 0.
2.2.2. More General Second-Order Equations
A similar classification applies to second-order equations in any number of
variables. As usual, write x = (x1, x2, …, xn) ∈ Rn. Consider the equation
where f = f(x, u, ux1, …, uxn). We assume the real coefficients aij in the principal
part L(p)u (given by the left-hand side) are constant and symmetric in i, j: aij = aji.
(If they were not symmetric, we could rearrange the PDE using the equality of
mixed partial derivatives to achieve symmetry.) The principal symbol is then
The type of the PDE depends on the nature of this quadratic expression, which
we can write in matrix form:
where A = (aij) is a real symmetric n × n matrix. If we change independent
variables with an invertible linear transformation B,
then the chain rule changes the PDE (2.3). It is instructive (see Problem 2) to
work out that the principal symbol now has coefficient matrix BABT. If B is an
orthogonal matrix, then B−1 = BT, so that the linear change of independent
variables corresponds to a similarity transformation of A. Now let’s choose B to
diagonalize A, so that BABT has the n eigenvalues of A on the diagonal and zeroes
elsewhere. This is achieved by letting the columns of B be the orthonormal
eigenvectors of A. The effect on the PDE is to convert the principal part into a
linear combination of pure second-order derivatives, in which the coefficients are
the eigenvalues of A.
We say the PDE is elliptic if the eigenvalues of A are all nonzero, and all have
the same sign. The PDE is called hyperbolic if all eigenvalues are nonzero, and all
but one of them have the same sign. (There is the third possibility that, for n ≥
4, all but k eigenvalues, with 2 ≤ k ≤ n/2, have the same sign. This case is
called ultrahyperbolic, but it does not occur much, so we ignore it.) Finally, if
there is at least one zero eigenvalue, then we could consider the PDE to be
parabolic. In practice, parabolic equations occur most commonly as time-

dependent PDE like the heat equation, with a single zero eigenvalue. Such
parabolic equations typically have the form
where u = u(x, t), L is a linear elliptic operator with respect to the spatial
variables, and f = f(x, t, u, ux1, …, uxn). In this equation, only one eigenvalue of
the coefficient matrix A is zero.
For each type of linear second-order PDE, we can find a change of
independent variables to transform the equation into a canonical form, in which
the corresponding matrix A is diagonal, so that only pure second-order
derivatives occur (i.e., no cross derivatives). In fact, the change of variables can
be done in general by observing how a linear change of independent variables
corresponds to a similarity transformation of A. Then we can reverse the process
to find the appropriate change of variables from a diagonalization of A.
Let x ∈ Rn be the independent variable, and suppose we introduce a linear
change of variables to y, through the orthogonal matrix B defined above, so that
BABT is diagonal:
In coordinates, this reads 
. If u = u(x), we define w(y) = u(Cy),
where C = B−1. Then a careful calculation gives
where λ1, …, λn are the eigenvalues of A.
Example 2. (Sample PDE operators) Let’s adopt the notation ∂j interchangeably
with ∂/∂xj. Here we display a PDE operator, the corresponding matrix A, and the
type of the operator:
1. 
; elliptic.
2. 
; x1 = t, x2 = x, x3 = y; 
; hyperbolic.
Notice that for a hyperbolic equation, the one eigenvalue with a different sign
suggests a time-like direction (associated with the corresponding eigenvalue).
After diagonalizing A, we can scale each independent variable so that in the
new variables, we have

Variable coefficients and nonlinear equations. When the coefficients aij in
(2.3) are functions of x, u, ux1, …, uxn, then the classification can vary with x and
can also depend on the solution. Here are some examples:
1. The Tricomi Equation (related to steady transonic flow): uyy = yuxx. This linear
equation is hyperbolic for y > 0, elliptic for y < 0, and the x-axis y = 0 is
called the parabolic line. We say the equation changes type.
2. The Nonlinear Small Disturbance Equation: 
. This equation
changes type on the line φx = 1.
3. The Quasilinear Wave Equation: utt = F(ux)x is hyperbolic when F′(ux) > 0. To
see that it is hyperbolic, we write the equation as
However, when F′(ux) < 0, the equation is elliptic. For a given solution, the
change from hyperbolic to elliptic occurs on a curve F′(ux(x, t)) = 0, where the
equation is parabolic.
4. The Semilinear Wave Equation: utt = Δu + f(u, ∇u, ut). For u = u(x, t), x ∈ Rn,
the principal part is linear and hyperbolic, but the equation is nonlinear if f :
Rn+2 → R is nonlinear, for example f = f(u) = u2.
2.2.3. Dispersion Relations
For time-dependent linear PDE with constant coefficients, we can sometimes get
more information about solutions from a dispersion relation, which is connected to
the Fourier transform in the same way as the principal symbol (2.4). It is easiest
to see how this works in one space dimension and time, where u = u(x, t). The
basic idea is to consider a Fourier mode u0(x) = eiξx as an initial condition. The
parameter ξ ≥ 0 is the wave number; it is the spatial frequency of u0. It is
convenient to use the complex form, because then derivatives are also
exponentials. Solutions will be of the form u(x, t) = eiξx+σt for some complex
number σ. But σ = σ(ξ) depends on the wave number. This dependence is called
the dispersion relation. In general, σ(ξ) is not a homogeneous function, unlike L(p)
[ξ], because σ(ξ) involves the entire PDE, not just the principal part.
For the linear transport equation ut + cux = 0, we find σ = −icξ.
Corresponding solutions u(x, t) = eiξ(x−ct) of the PDE are traveling waves (which is
no surprise, since all solutions of this equation are traveling waves). The linear

wave equation utt = c2uxx has σ(ξ) = ±icξ, corresponding to the traveling waves
u(x, t) = eiξ(x±ct).
For the heat equation ut = kuxx, we have σ = −kξ2. Therefore, every Fourier
mode decays exponentially, provided k > 0, and the rate of decay increases
quadratically with frequency. However, if k < 0, then each Fourier mode has
exponential growth in time, and the growth σ(ξ) is unbounded as a function of
wave number ξ. This corresponds to ill-posedness, as it implies that a general
initial condition (which involves arbitrarily high wave numbers) will blow up
immediately. The same issue arises for initial value problems for elliptic
equations, such as Laplace’s equation. (See Section 2.3.3.)
The linearized KdV equation ut + cux + βuxxx = 0 is an example of a
dispersive equation. We find that σ = −iω is imaginary for all wave numbers,
and ω = cξ − βξ3. The corresponding solutions u(x, t) = eiξ(x−(c−βξ2)t) are
traveling waves, but the speed c − βξ2 depends quadratically on the wave
number. From another point of view, ω is the temporal frequency, so that
different Fourier modes oscillate in time at different frequencies. This is
dispersion in the mathematical sense of different spatial wave numbers giving
rise to traveling waves with different speeds and to oscillations at different
frequencies.
The linear Benjamin-Bona-Mahoney (BBM) equation ut + cux + βuxxt = 0 is
also dispersive, but the dispersion relation involves a bounded function ω.
Another example of a dispersive equation is the beam equation utt + k2uxxxx = 0.
For dispersive equations the traveling wave speed ω = ω(ξ) is called the
phase speed or phase velocity. Another speed of interest is the group velocity,
defined as 
. The group velocity of dispersive equations is different
from the phase velocity. For nondispersive equations, such as the linear transport
and wave equations, both velocities are the same as the single traveling wave
speed or transport velocity. The roles of group velocity and phase velocity in
linear and nonlinear wave equations are discussed in detail by Whitham in his
classic text [46].
2.3. Initial Value Problems and the Cauchy-Kovalevskaya
Theorem
Up to this point we have only constructed solutions with explicit formulas. In this
section we outline an approach that constructs solutions as power series, leading
to a version of the celebrated Cauchy-Kovalevskaya2 Theorem. We consider initial
value problems in a fairly general context, that of the second-order equation (2.1):

An initial value problem consists of the PDE, together with initial conditions:
We assume that all functions a, b, c, f, g, h are all C∞.
In this problem, y is time-like, in the sense that y = 0 is an initial time, and
we want to solve the initial value problem at least for a short time interval. The
analysis of this section applies to both positive and negative y. In this section we
discuss the existence of solutions that can be represented as a formal power series
about y = 0. Such a series would take the form
Remark. If (2.7) is a convergent series, then u has y derivatives of all orders, and
Here the superscript indicates repeated derivatives: 
. Let’s make the key
assumption in (2.5) that c(x, y) is nonzero for all x in some interval I (and all y
near zero).3 Then (2.7) can be written (by dividing by c):
where 
.
Claim 2.1. For any g, h ∈ C∞(I), (2.9) plus initial conditions (2.6) uniquely
determine the C∞ functions uk(x), k = 0, 1, ….
Remarks. While the claim seems to be a uniqueness result, it is also an existence
result, because it asserts that the functions uk(x) exist.
We are not going to prove the claim, but it is instructive to consider why it is
true. The terms in (2.8) with k = 0 and k = 1 are given by the initial conditions
(2.6). Differentiating these m ≥ 1 times with respect to x gives 
 g(m)(x),
and 
. In particular, this gives us G on the right-hand side of (2.9)
when y = 0. Hence we have found ∂yyu(x, 0), which is u2(x).
To get uk(x) for k ≥ 3, we differentiate the PDE (2.9) with respect to x and y,
successively calculating derivatives of higher and higher order in terms of
derivatives of the functions a, b, c, f, g, h, and G. For example, to calculate 
, we differentiate the PDE with respect to y and set y = 0. Then

(from the chain rule) the right-hand side has a term with ∂yuxx(x, 0). But we
already know this from (2.6): 
.
2.3.1. Limitations of the Power Series Representation of Functions
To examine the issue of convergence of the series (2.7) to a solution, we focus on
some properties of power series. Taylor’s Theorem with remainder (in one
variable) is the formula
A stringent condition (see (2.11)) is needed to be able to pass to the limit N → ∞
and ensure that the infinite series converges.
Example 3. (A function ζ(x) that is C∞, but the Taylor series for ζ fails to
converge to ζ(x) except at x = 0) Let
Note that
so the power series
converges to zero for all x, but not to the function ζ(x), which is nonzero for x >
0.
The term real analytic is reserved for C∞ functions with convergent Taylor
series: A function f ∈ C∞(I) is called real analytic on the interval I if, for every x0
∈ I, the power series
converges to f(x) for all x in some neighborhood of x0.
Proposition 2.2. Let f ∈ C∞(I). If there exist positive constants C and ϵ such that (for
all x ∈ I)

then f is real analytic on I.
This result makes sense if you are familiar with the root test for convergence
of series of numbers. The converse of the proposition is true with a restriction: if f
is real analytic on I, then the estimate (2.11) is uniform for x (C independent of
x) in compact subintervals of I.
We can extend the concept of real analytic to functions of two variables in an
open set Ω ⊂ R2, which will be relevant for the theorem below.
Definition. If u ∈ C∞(Ω), u is real analytic if for every (x0, y0) ∈ Ω, there is a
neighborhood N(x0,y0) of (x0, y0) such that for all (x, y) ∈ N(x0,y0), the double series
converges to u(x, y).
2.3.2. The Cauchy-Kovalevskaya and Holmgren Theorems
These two theorems are part of the classical culture of the study of PDE. The
Cauchy-Kovalevskaya Theorem establishes the existence of real analytic solutions
of initial value problems for PDE (or systems of PDE) with analytic coefficients.
The Holmgren Theorem states that, under the conditions of the Cauchy-
Kovalevskaya Theorem, the real analytic solution is the unique C2 solution
locally.
Theorem 2.3. (Cauchy-Kovalevskaya) Suppose that the functions a, b, c in (2.5) are
real analytic in I × (−δ, δ), f is real analytic, and g, h are real analytic in I. Assume
(as before) that c(x, 0) ≠ 0 for x ∈ I. Then the series (2.7) converges to a real
analytic solution of the initial value problem, for (x, y) in some neighborhood Ω of I ×
{0}.
Remark. The real analytic solution of the theorem is the sum of the series (2.7),
but it also has a double series expansion (2.12), since it is real analytic in a two-
dimensional open set.
There is only one real analytic solution, since a real analytic function is
determined by its derivatives at one point, and by Claim 2.1, these derivatives
are uniquely determined. The next result shows that even if the analyticity
assumption on solutions is relaxed, the solution is still unique.
Theorem 2.4. (Holmgren) Under the above hypotheses, there is a neighborhood Ω of
I × {0} in R2 with the property that if v ∈ C2(Ω) satisfies (2.5) and (2.6), then v(x,
y) is the solution Theorem 2.3.

These theorems are proved elegantly in the classic text of Garabedian [16].
2.3.3. An Important Cautionary Example
Despite its generality, the Cauchy-Kovalevskaya Theorem is of limited utility in
the theory of PDE, because the assumption of real analyticity of the data is too
restrictive. For example, we cannot find a power series solution to solve initial
value problems with the function (2.10) as initial data, because the function is
not real analytic.
However, initial value problems raise other significant issues, connected with
Hadamard’s notion of a well-posed problem, as discussed in Section 2.1. The
following classic example illustrates Hadamard ill-posedness for the initial value
problem for Laplace’s equation:
Let k > 0 be a parameter that is fixed for now. The parameter k is a spatial
frequency, and in this context is referred to as the wave number. The
corresponding wavelength (of the periodic function cos kx) is 2π/k. The Cauchy-
Kovalevskaya Theorem implies there is a unique solution of this initial value
problem, and indeed we can find it using the important technique of separation
of variables. For each k > 0, we have the explicit solution
Now consider the solutions as k → ∞. We observe that:
1. The initial condition: 
 (along with all x-derivatives).
2. For any y ≠ 0,
These two observations mean that there is not continuous dependence of the
solution on the initial data. Moreover, these are the analytic solutions of the
Cauchy-Kovalevskaya Theorem, which guarantees a solution even when the
initial value problem is not well-posed. It is also interesting to note that the
solutions grow exponentially in y for each k > 0, and the rate of growth
increases exponentially with k. In this sense, the general solution is not just
unstable 
(growing 
exponentially), 
but 
is 
catastrophically 
unstable, 
a
manifestation of ill-posedness.

2.4. PDE from Balance Laws
The theory describing the mechanics of continuous materials, such as solids,
fluids, and gases, is called continuum mechanics. It is based on conservation laws
of mass, momentum, and energy. The independent variables are x, representing a
point in the material, and time t. Typical dependent variables are density,
velocity, stress, and internal energy. They are defined at each point and at each
time in a specified region of space-time.
A balance law is an equation expressing a conservation principle; it equates
the rate of change of a quantity in a region with the sum of two effects: the rate
at which the quantity is entering or leaving through the boundary (the flux
through the boundary), and the rate at which the quantity is being created or
destroyed in the region. The derivation of a PDE from a balance law typically
involves the following steps:
1. Write the balance law in an arbitrary bounded region V with smooth boundary
∂V.
2. Use the Divergence Theorem to relate the flux through the boundary ∂V to an
integral over V, and deduce that the sum of the integrands in the integrals over
V must balance. This gives an equation or a system of equations. However,
both the quantity and the flux are unknowns; consequently, there are more
variables than equations.
3. Close the system by relating the variables through additional equations (not
necessarily PDE) called constitutive laws, resulting in the same number of
equations as variables.
Let’s consider a region U ⊂ Rn (n = 1, 2, or 3, generally), and a quantity
(such as mass, momentum, or energy) that is to be conserved, represented by a
density function u. That is, the quantity is represented by u(x, t) measured at
each point x in U at each time t. For example, the material density u = ρ is the
density function for mass (since it is the mass per unit volume), u = ρv is the
density function for momentum (where v is a velocity), and the temperature u =
θ is the density function for heat energy.
Let V be an open subset of U, with smooth boundary ∂V having unit outward
normal ν = ν(x). The amount of u in V is a quantity that depends on time:
The time rate of change of A is then

Suppose the quantity u can flow in or out of V, and can be created or destroyed
within V. Then the rate of change of u in V is balanced by the flux of u across the
boundary ∂V plus the creation (due to a source) or the destruction (a sink) of u in
V.
The net flux through the boundary is represented by an integral ∫∂V Q(x, t) · ν
dS, where the vector-valued function Q(x, t), x ∈ U, is called the flux function.
Note that ∫∂V Q(x, t). ν dS > 0 if Q points out of V; this has the effect of
decreasing the amount A(t). The creation or destruction of u in V is likewise
specified by a function, this time a scalar function f(x, t); the net rate of
creation/destruction is given by ∫V f (x, t) dx.
Step 1. Now we can write a balance law:
Step 2. Next we apply the Divergence Theorem to convert the surface integral
into a volume integral and combine terms (since they are all integrals over V):
If we assume the integrand is continuous, then (2.13) implies that it is zero
everywhere in U. Thus, we have the PDE
In this equation, we regard u = u(x, t) as the unknown, but there are
additional functions Q(x, t) and f(x, t). These must be determined from
additional equations that could specify Q and f as functions of x and t.
However, Q in particular is more often related to u and derivatives of u, or to
additional dependent variables. This leads to the final step.
Step 3. Specify constitutive laws to close the system, as in the following
examples.
Example 4. (Balance laws and the heat and wave equations) The heat and
wave equations are examples of PDE derived from balance laws. Conservation of
heat energy e = ρcu relates the temperature u(x, t) to the heat flux Q and source
terms F(x, t). Here, the density ρ of the material, and its specific heat c are taken
to be constant. The balance law leads to the equation

Fourier’s Law of heat conduction expresses the thermodynamic property that heat
energy flows from higher temperatures to lower. Specifically, this constitutive
law links the heat flux linearly to the temperature gradient:
When the thermal conductivity κ > 0 is constant, we obtain from (2.14) the heat
equation with source term f :
The same equation also models the diffusion of a solute in solution, with solute
concentration u. In this context, the heat equation is called the diffusion equation.
The proportionality between flux and concentration gradient is then termed Fick’s
Law.
We will derive the one-dimensional wave equation carefully in Section 4.1,
but here is the idea of how the wave equation arises from conservation of
momentum.
Figure 2.1. Traffic flow: cars traveling on a section of highway.
The balance law equates the rate of change of momentum with the divergence of
the momentum flux:
Here ρ > 0 is the density, which we take to be constant, and 
 is a velocity,
the time derivative of displacement u. In some applications, such as elasticity, a
reasonable constitutive law specifies that the momentum flux is proportional to
the gradient of the displacement: Q = −k ∇u, where k > 0 is the constant of
proportionality. This leads directly to the wave equation:
with c2 = k/ρ.
Example5. (Traffic flow) Traffic flow models help to illustrate how the
conservation law and constitutive equation are formulated separately. Since these
models are one dimensional, the Fundamental Theorem of Calculus replaces the
Divergence Theorem in step 2. Consider a single lane highway and let u(x, t) be

the density of cars at location x ∈ R on the highway at time t:
That is, each time t and every point x on the highway is associated with a traffic
density u(x, t). Suppose the cars are moving to the right, as shown in Figure 2.1.
To formulate the balance law, we consider the number of cars in a section of
highway between fixed locations x = a and x = b at time t:
The time rate of change of N should be equal to the net rate at which cars enter
at x = a and leave from x = b. (We assume no cars are manufactured or
scrapped in the middle of the highway, so there are no source or sink terms: f ≡
0.) Let Q(x, t) denote the flux of cars past a particular point x at time t:
Then, since cars enter the section [a, b] at a rate Q(a, t), and leave at the rate
Q(b, t), we have
That is, the rate of change of the number of cars in the region (section of the
highway) is balanced by the flux of cars through the boundary.
Now use the Fundamental Theorem of Calculus to obtain
If we assume continuity of the integrand, then
Equation (2.16) has two unknowns, Q and u, and we need an additional equation.
In traffic flow models, typically we add a constitutive law that relates the flux Q
to the density u. First, it makes sense that Q should be the product of speed and
density, the number of cars per unit time across a fixed location:
To complete the description of Q as a function of density alone, we need to
specify the speed v as a function of density. We can attempt to fit data from real
observations of traffic, or, as is often done when first formulating a mathematical

model, we introduce a functional form that is consistent with natural qualitative
or physical properties. In the current context a simple model takes the traffic
speed v to be a linear and decreasing function of traffic density:
(see Fig. 2.2). Here the parameters α, β have the interpretation of maximum
density and maximum speed. The equation ut + Qx = 0 is now a single equation
for the unknown u(x, t):
Equation (2.17) is known as the Lighthill-Whitham-Richards model. Whitham
[46] identifies various scenarios, such as the timing of traffic lights, in which
solutions of the equation describe the behavior of traffic. Modeling traffic flow
with PDE is of considerable interest, as the equations are easy to work with
numerically. However, there are significant challenges in devising realistic
models that incorporate important behavior, such as multilane traffic and how
traffic divides at intersections or entrances and exits to freeways.
Equation (2.17) is related to the inviscid Burgers equation
Figure 2.2. Traffic flow: speed v vs. density u.
in that both have a quadratic flux, but the convexity is different. This difference
in convexity is relevant when considering blow-up of ux (i.e., the steepening
observed in Fig. 1.2). In traffic flow, in which the flux is convex down, there will
be a jam (indicated by blow-up of ux to infinity—the cars get really scrunched!) if
the traffic density u increases ahead (i.e., is an increasing function of x).
However, in the inviscid Burgers equation, for which the flux u2/2 is convex up,

we saw in Section 1.4.2 that there is blow-up of ux (to −∞) when u is a
decreasing function of x (see Fig. 1.2).
PROBLEMS
1. (a) Determine the type of the equation uxx + uxy + ux = 0.
(b) Determine the type of the equation uxx + uxy + αuyy + ux + u = 0 for
each real value of the parameter α.
(c) Determine the type of the equation utt + 2uxt + uxx = 0. Verify that there
are solutions u(x, t) = f(x − t) + tg(x − t) for any twice differentiable
functions f, g.
(d) The equation (1 + y)uxx − x2uxy + xuyy = 0 is hyperbolic or elliptic or
parabolic, depending on the location of (x, y) in the plane. Find a formula to
describe where in the x-y plane the equation is hyperbolic. Sketch the x-y
plane and label where the equation is hyperbolic, where it is elliptic, and
where it is parabolic.
2. Show that with the change of variables y = Bx, the principal symbol of (2.4)
corresponding to (2.3) has coefficients cij given by C = BABT, where C = (cij).
One approach to this is to write everything in coordinate form, such as 
, BA = (bakj), 
, and use the chain rule to
convert xj derivatives to sums of yk derivatives.
3. For the series (2.7), write formulas for u3(x) and u4(x) in terms of derivatives
of the functions a, b, c, f, g, h, and G.
4. Show that ζ ∈ C∞(R), where ζ is given by (2.10).
5. Find the dispersion relation σ = σ(ξ) for the following dispersive equations.
(a) The beam equation utt = −uxxxx. Why is the equation dispersive and not
dissipative? What makes this equation dispersive, whereas the wave equation
is not dispersive?
(b) The linear Benjamin-Bona-Mahoney (BBM) equation ut + cux + βuxxt = 0.
Deduce that the equation is dispersive, and show that the corresponding
solutions u = eiξx+σ(ξ)t are traveling waves. Write a formula for their speed as a
function of wave number ξ. Identify a significant difference between this
formula and the wave speeds of KdV traveling waves.
6. Suppose in the traffic flow model discussed in Section 2.4 that the speed v of
cars is taken to be a positive monotonic differentiable function of density:
v = v(u).

(a) Should v be increasing or decreasing?
(b) How would you characterize the maximum velocity vmax and the maximum
density umax?
(c) Let Q(u) = uv(u). Prove that Q has a maximum at some density u∗ in the
interval (0, umax).
(d) Can there be two local maxima of the flux? (Hint: Make Q(u) quartic.)
1. Jacques S. Hadamard (1865–1963) is well known for contributions to number theory, matrices,
differential equations, geometry, elasticity, geometrical optics, and hydrodynamics. His papers of 1901 and
1902 discuss ill-posed and well-posed problems, respectively.
2. Augustin-Louis Cauchy (1789–1857) made many contributions to mathematics, including fundamental
developments in real and complex analysis, modern algebra, and the theory of elasticity. Sofia Vasilyevna
Kovalevskaya (1850–1891) made important contributions to analysis, differential equations, and mechanics.
3. Since we shall be discussing solutions only locally in (x, y) we could simply assume c(0, 0) ≠ 0.

CHAPTER THREE
First-Order PDE
First-order equations enjoy a special place in theory of PDE, as they can generally
be solved explicitly using the method of characteristics. Although this method
applies more generally to fully nonlinear equations, such as Hamilton-Jacobi
equations, we will restrict attention to linear and quasilinear equations, in which
the first-order derivatives of the dependent variable u occur linearly, with
coefficients that may depend on u. The method of characteristics reduces the
determination of explicit solutions to solving ODE. We develop the theory in
several stages, with increasing sophistication, but really the idea is the same all
along: first-order PDE become ODE when the PDE are regarded as specifying
directional derivatives in several dimensions.
3.1. The Method of Characteristics for Initial Value Problems
Initial value problems in one space variable x and time t take the form
Let’s assume that c and r are given C1(continuously differentiable) functions, and
the initial condition f : R → R is a given C1 function. The coefficient c will be
established as a wave speed, and the notation r simply stands for the right-hand
side of the PDE.
We can solve (3.1) at least for a short time interval (and perhaps only locally
in space) using the method of characteristics, which reduces the initial value
problem (3.1) to an initial value problem for a system of ODE. In this method, we
depend on the observation that if {(x(t), t) : t ≥ 0} is a smooth curve, then along
the curve, u(x(t), t) has rate of change
given by the chain rule. Comparing (3.2) with the PDE in (3.1), it looks as though
we can make progress by setting 
 and interpreting c as a speed. The left-
hand side of the PDE can also be interpreted as the derivative of u(x, t), in the
direction (c, 1) in x-t space.1
Now the PDE (3.1) can be replaced by the ODE system
These ODE are called the characteristic equations. Note that the characteristic

equations are autonomous only if c and r are independent of t.
Initial conditions for the ODE system are derived from the initial condition
u(x, 0) = f(x) for the PDE problem (3.1). To see what the ODE initial conditions
should be, let’s write x(0) = x0, and u(t) in place of u(x(t), t). Then the initial
conditions for (3.3) are
From the theory of ODE, we know that the initial value problem (3.3), (3.4) has a
unique solution (x(t), u(t)), at least locally in time for each x0. To emphasize that
we have a solution for each x0, let’s write the solution as 
. The
semicolon indicates that x0 is regarded as a parameter in the ODE initial value
problem, but now we are going to treat x0 as a second variable, so that x and u
are functions of the two variables t, x0.
The parameter x0 specifies the curve in the x-t plane 
, which
we refer to as the characteristic through x = x0, t = 0. As long as curves with
different values of x0 do not cross, the family of characteristics fills a region of
the upper half-plane {(x, t) : t ≥ 0}, thereby parameterizing points in the region
with x0, t. At each point P : (x, t) of this region, we know the solution u, since 
 on each characteristic. Figure 3.1 illustrates the characteristic
originating at x0 that passes through point P. Once we have identified the value
of x0, then 
.
There is a nice physical interpretation of this construction. The parameter x0,
called the Lagrangian variable, labels a material point. Then 
 is the
Eulerian variable describing the location at time t of that material point. The value
u of the variable can be thought of either in Lagrangian variables, for which 
, or in Eulerian variables, for which u is observed at a fixed location: u
= u(x, t), the solution we seek.
Mathematically, to get the solution u explicitly at each point (x, t), we need to
invert the change of variables 
. To do so, we eliminate x0 and
write 
 as the solution of the equation 
. Then 
is the solution of (3.1).

Figure 3.1. Characteristic 
: t ≥ 0}, along which 
, for the
initial value problem (3.1).
For this kind of initial value problem, the method of characteristics is
summarized as:
1. Rewrite the initial value problem (3.1) as a system of ODE consisting of the
characteristic equations (3.3) with initial conditions (3.4).
2. Solve the ODE and initial conditions for x(t), u(t), with parameter x0 = x(0) to
get the solution along each characteristic.
3. Solve for x0 as a function of x, t. This effectively changes variables from t, x0 to
x, t.
4. Write the solution u = u(x, t).
Example 1. (Initial value problem) Solve
In this example, y is time-like in the sense that the initial condition is posed at y
= 0. The PDE written as ∇u. (1, 1) = u shows that the left-hand side is the
directional derivative of u in the direction (1, 1). Consider the lines x = y + k
parallel to (1, 1), where the parameter k plays the same role as x0 above. The rate
of change of u along each line is
Therefore,
where A(k) is an arbitrary function. Thus, since k = x − y,
is the general solution of the PDE, depending on the arbitrary function A(k) of a

single variable.
To complete the solution, we use the initial condition to determine A(k).
Setting y = 0 in (3.6) gives
Thus, the solution of the problem is
Since the left-hand side of (3.5) is a directional derivative, it is an ordinary
derivative in that direction. Thus, u′ = u in this direction, explaining the
exponential growth of the solution along each characteristic x = y + k. Likewise,
the solution would be u(x, y) = cos(x − y) if the right-hand side of the PDE were
zero.
In this example, we found the characteristics before determining the behavior
of u along them. Generally, the characteristics for (3.1) will also depend on the
solution, if c depends on u.
3.2. The Method of Characteristics for Cauchy Problems in Two
Variables
In this section we present a more general version of the method of characteristics
for first-order quasilinear PDE in two independent variables. First-order
quasilinear PDE in two independent variables take the form
where a, b, c are given C1 functions from R2 × R to R. In this equation, neither
of the variables necessarily has a special role, such as time. Consequently, the
notation is somewhat different from the previous section.
Rather than posing an initial condition, we pose a more general side condition
for (3.7) in the form
where x0, y0, z0 are given C1 functions on an interval I. This is sometimes referred
to as the initial curve Γ. Problem (3.7), (3.8) is referred to as the Cauchy problem.
We shall show that for C1 solutions, the PDE is really an ODE in disguise (as
we saw in Example 1). Suppose u(x, y) is a solution of the Cauchy problem. Then
the graph z = u(x, y) is a two-dimensional surface in x-y-z space that includes the
curve Γ. Equation (3.7) states that the vector field (a(x, y, z), b(x, y, z), c(x, y, z))
is tangent to the solution surface z = u(x, y), since the solution surface has
normal

Figure 3.2. Initial curve Γ, the characteristic curve tangent to (a, b, c), and the
solution surface.
The solution surface can therefore be generated by integrating along the vector
field, starting at each point of the curve Γ = {(x, y, z) : x = x0(s) …, s ∈ I}. (See
Fig. 3.2.) If τ is the variable of integration along these integral curves, then the
surface generated is parameterized by (s, τ) : x = x(s, τ), y = y(s, τ), z = z(s, τ).
To recover u(x, y), we transform from (s, τ) back to (x, y) in z and set u(x, y) =
z(s, τ), establishing the existence of the inverse using the Inverse Function
Theorem (see Appendix A).
This procedure to solve the Cauchy problem (3.7), (3.8) is divided into three
steps:
1. Generate the solution surface from integral curves. In this step we solve the one-
parameter family of initial value problems
for each s ∈ I. Denote the solution (x, y, z)(s, τ). From ODE theory, the
solution exists, is C1, and is unique, at least in a neighborhood of Γ. The
solution curves in R3 are known as characteristic curves. We reserve the term
characteristics to mean the projection of the characteristic curves onto the x-y

plane.
2. Apply the Inverse Function Theorem. In this step we solve the equations
for (s, τ) as a function of (x, y) : (s, τ) = (s, τ)(x, y). The solution is
guaranteed by the Inverse Function Theorem.
3. Write the solution surface as a graph z = u(x, y).
Now we are able to write the solution as a function of x, y:
This procedure will work as long as the transformation (3.10) is invertible. We
can guarantee this locally by appealing to the Inverse Function Theorem.
Specifically, let P = (x0(s0), y0(s0), z0(s0)) be a point on Γ. For (3.10) to be
invertible near (x, y) = (x0(s0), y0(s0)), we require the Jacobian matrix ∂(x, y)/∂(s,
τ) to be invertible at this point. That is, at P we require
where we have used (3.8), (3.9). This condition means that the tangent (a, b) to
the characteristic at (x0(s0), y0(s0)) is not parallel to the tangent 
 of the
projection γ of Γ at P onto the x-y plane. Consequently, when (3.11) holds, we
say that the curve Γ is noncharacteristic at P. Thus, provided the initial data are
noncharacteristic in the sense of (3.11), we have a unique C1 solution u(x, y) of
(3.7), (3.8) for (x, y) near (x0(s0), y0(s0)).
Example 2. (A Cauchy problem) Solve the Cauchy problem
Here a = z, b = 1, c = 1, and the initial condition is u = 0 on the line y = x.
We parameterize the initial condition as follows:
Characteristic equations are
with corresponding initial conditions x(0) = s, y(0) = s, z(0) = 0. Thus, z = τ,
so that x′ = z = τ. Now we can solve for x and y:

Eliminating s, we get a quadratic equation for 
. Thus,
But τ = z = u(x, y), and to satisfy the initial condition, we have to take the
negative square root:
The solution is valid only for 
. In fact, the solution surface z
= u(x, y) is the lower half of the smooth parabolic surface (z − 1)2 1 + 2x − 2y,
which has a fold along the line 
. Since the surface becomes vertical
at the fold, the solution u(x, y) has a singularity on the line 
, where the
derivative ux − uy blows up.
3.3. The Method of Characteristics in Rn
In this section we repeat the method of characteristics for a single quasilinear
first-order equation to show how the method works for any number of
independent variables. Characteristic curves are of course one dimensional, and
thus contribute one dimension to the solution surface, which is n − 1
dimensional. The remaining dimensions in the surface are provided by the initial
conditions.
Consider x ∈ Rn; u = u(x) ∈ R. The first-order equation we consider has the
general form
where a : Rn × R → Rn, a vector of coefficients, and c : Rn × R → R, a scalar,
are given C1 functions.
The Cauchy problem involves an (n − 1)-dimensional hypersurface γ ⊂ Rn
that provides initial conditions for characteristic curves:
Characteristic curves in (x, z) space (Rn+1) are solution curves of the system
Note that for each s, we have existence and uniqueness of solutions of (3.14) for |

τ| small, since a and c are C1. Moreover, since the data are C1, the solutions are
C1 in s also.
As before, we write the solutions in the form
The solution u(x) = z(s, τ) is expressed in physical variables x if we can invert
(3.15a) to get s = s(x), τ = τ(x). This is guaranteed by the Inverse Function
Theorem, at least locally, if we assume the hypersurface is noncharacteristic, i.e.,
∂x/∂(s, τ) is invertible on γ (where τ = 0), with z = u0(s), and recall that ∂x/∂τ
= a:
In components:
Then we have the solution
More precisely, the method of characteristics and the Inverse Function Theorem
have been used to prove the following result.
Theorem 3.1. Suppose the data x0, u0 are C1 in a neighborhood of s = 0 and are
noncharacteristic in the sense of (3.16) at s = 0. Then there exists a neighborhood N
of x0(0) and a C1 function u : N → R that solves the Cauchy problem (3.12), (3.13)
in N.
Example 3. (Particle size segregation in an avalanche) Avalanches and rock
slides are examples of granular flow, typically involving particles of different
sizes. In this example, we write a PDE for the transport of two sizes of particles (a

bidisperse mixture) that have the same density. We assume that as the avalanche
flows down the hillside, it establishes a constant depth, and that the velocity
varies linearly with depth. We ignore all but the component of velocity that is
parallel to the hillside. In these circumstances, Gray and Thornton [20]
formulated a model that describes the distribution of particles in the avalanche.
Let x, y denote the spatial variables, and let v(y) = y denote the parallel
velocity. These are shown in Figure 3.3. The dependent variable u = u(x, y, t) is
the volume fraction of small particles. In the flow, large particles tend to rise, and
small particles tend to fall. Gray and Thornton argued that small particles fall at a
speed proportional to the volume fraction 1 − u of large particles, essentially
because they depend on space opened up by the motion of large particles. Then
large particles have to move upward to balance the motion of the small particles.
With these assumptions, the PDE is
Figure 3.3. Coordinates and velocity profile v(y) for avalanche flow model.
where S > 0 is a constant of proportionality. Let’s suppose there is an initial
distribution of small particles given by
Characteristic equations for this equation can be written

Thus, u = u0(x0, y0) is constant on the characteristic curve through (x0, y0) at t =
0. Consequently,
At t = 0, we have x = x0, y = y0, so that
Thus, characteristics are parabolas in the x-t plane. Now we solve for x0, y0 in
terms of u, x, y, t:
Finally, we have a formula for the solution u = u(x, y, t), defined implicitly by
the equation
This solution technique can be used to study the dynamics of avalanche flow with
various initial and boundary conditions.
3.4. Scalar Conservation Laws and the Formation of Shocks
In this section we consider the initial value problem for the inviscid Burgers
equation. We show that solutions generated by the method of characteristics
typically break down in finite time. This nonlinear wave behavior occurs in such
applications as gas dynamics, combustion and detonation, and nonlinear
elasticity. The breakdown of solutions signals the formation of a shock wave,
across which the solution is discontinuous.
Consider the initial value problem
with initial condition
The method of characteristics of Section 3.1 is applicable here:
Thus, u is constant on each characteristic, and characteristics are therefore
straight lines with speed u:

The solution u = u(x, t) is then given implicitly by the equation
Let F(u, x, t) = u − u0(x − ut). Generally, we cannot solve for u explicitly, but
we can use the equation to prove local existence near any initial point x = x0, by
applying the Implicit Function Theorem to the equation F(u, x, t) = 0. (See
problem 11.)
3.4.1. Breakdown of Smooth Solutions
As we saw in Section 1.4.2, the graph of the solution u(x, t) steepens where it has
negative slope, because larger positive values of u travel faster than smaller
values. For negative values of u, the characteristics travel to the left, but the same
is true: the graph steepens where the slope is negative. Mathematically, we find
ux → −∞ at some x as t increases to a time t∗. The notion that some values of u
travel faster than others, leading to steepening, may be expressed in the following
statement:
Characteristics x = ut + x0 that originate at points x0 in an interval
where 
 cross one another in finite time.
Figure 3.4. Inviscid Burgers’ equation: crossing characteristics associated with
the breakdown of a smooth solution.
In Figure 3.4 we show the characteristics for the solution shown in Figure 1.2
and see that characteristics ahead of the crest of the wave eventually cross one
another. If this first occurs at a time t = t∗, then the method of characteristics
gives a multivalued function of (x, t), for t > t∗ in the region where the
characteristics cross. We say the solution breaks down at t = t∗.
Our goal is to make this argument rigorous and to find a formula for the
breakdown time t∗. To do so, we derive an equation for ux by taking 
 of the
PDE, thus deriving an ODE for the evolution of ux along characteristics. First we

differentiate (3.21):
Let v = ux. Then we have
Along characteristics x = ut + x0 we get the ODE
This equation (known as a Riccati equation due to the quadratic nonlinearity) is
solved easily. Notice that it states that v decreases in t, and the more it decreases
through negative values, the more rapidly it continues to decrease.
Now we differentiate the initial condition u(x, 0) = u0(x) to obtain a
corresponding initial condition for v:
We solve (3.24), (3.25) to find v along the characteristic x = ut + x0:
We distinguish two cases:
1. If 
, then v stays finite for all t > 0. Consequently, if u0 is monotonically
increasing, then u(x, t) is defined for all x, t. Note from (3.23) that u(x, t) only
takes on values of u0(x0), x0 ∈ R. Therefore, if u0 is bounded by m, M, m ≤
u0(x) ≤ M, x ∈ R, then m ≤ u(x, t) ≤ M for all x ∈ R, t > 0.
2. If u0 is not monotonically increasing, so that 
 for some values of x0,
then
in (3.26). Thus, the solution breaks down (ux → −∞) at different times t on
each characteristic (depending on x0). Consequently, the solution u(x, t) of the
initial value problem breaks down at the earliest such time t = t∗:
Note that the minimum is achieved where u0 has minimum slope, which will

be at an inflection point if u0 is C2.
To continue the solution beyond t = t∗, we define weak solutions, in which the
function u(x, t) is allowed to be discontinuous. We pursue this topic in Chapter
13, after first considering solution and analysis techniques for second-order
equations.
PROBLEMS
1. Use the substitution v = uy to solve for u = u(x, y):
2. Solve for u = u(x, t):
3. Solve for u = u(x, t):
4. Solve (3.5) using the general method of characteristics. You will need to set up
the initial condition with a parameter s. Show that the initial curve Γ is
noncharacteristic.
5. Verify that u(x, t) constructed in general in Section 3.1 is indeed a solution of
(3.1). Start by working out what calculation you have to do to carry out this
check. You will have to use the chain rule repeatedly to check carefully.
6. Take an alternative direct approach to Example 1, reversing the roles of x and
y, by setting y = x + k. The PDE then becomes the ODE 
 u along
characteristics. Solve and incorporate the initial condition, finally obtaining the
solution u(x, y).
7. For the avalanche flow equation (3.17), suppose an initial distribution of
particles is given by
and an inlet boundary condition is specified by
Find the solution u(x, y, t), 0 < x, 0 < y < 1, t > 0 by the method of
characteristics. (The side conditions and hence the solution do not obey the
physical constraint 0 ≤ u ≤ 1, and hence are not intended to be physically
significant. This problem is an exercise in using the method of characteristics.)
8. (a) Use the method of characteristics to solve the initial value problem

(b) Show that the solution blows up as t → 1:
9. Sketch the graph of the traffic flow flux Q (see (2.16), (2.17) in Chapter 2) as a
function of density u. Explain each zero of Q in terms of the physical model.
10. Formulate constitutive laws for the traffic flux Q (see Example 2 in Chapter
2) as a function of density assuming that traffic speed v(ρ) is a quadratic
decreasing function of density ρ. How many parameters are there in the model?
Is it possible to make the flux nonconcave as a function of density?
11. Write the details of how to use the Implicit Function Theorem on (3.23) to
prove: If u0 is smooth and bounded on (−∞, ∞) then for each x0 ∈ R, there is an
interval I ⊂ an interval I ⊂ R containing x0 such that the solution u(x, t) exists, is
C1, and is unique for all x ∈ I and all small enough t.
12. Let u0(x) = H(x)x2, where H(x) = 0 for x < 0 and H(x) = 1 for x ≥ 0 is the
Heaviside function. Write the solution u(x, t) of (3.21), (3.22) as an explicit
formula for t > 0.
13. Get the answer (3.26) by differentiating the implicit solution (3.23):
(This simpler approach depends on having the implicit equation for u available,
which is not the case for systems of equations.)
14. Use the method of characteristics to prove global (for all t > 0) existence of a
smooth solution of (3.21), (3.22) when the initial data are given by a strictly
increasing but bounded C1 function u0.
15. Carry through the analysis presented in Section 3.4 for a general scalar
conservation law
where f : R → R is a given C2 function. Derive an implicit equation for the
solution u(x, t) of the Cauchy problem, and formulate a condition for the solution
to remain smooth for all time. Likewise, if the condition is violated, find an
expression for the time at which the solution first breaks down.

1. Strictly speaking, the direction is 
; the magnitude 
 sets the parameterization to be
by t rather than by arclength.

CHAPTER FOUR
The Wave Equation
The wave equation
is the prototype for second-order hyperbolic PDE, modeling the propagation of
sound waves; electromagnetic waves, such as light; and waves in elastic solids.
We show in detail how the wave equation describes the deformation of one-
dimensional elastic solids, specifically, thin rods and elastic strings.
Central to the study of the one-dimensional equation is d’Alembert’s solution,
an explicit formula for solutions of initial value problems. The method of
spherical means provides a corresponding explicit formula in two and three
dimensions. In three dimensions, this formula embodies Huygens’ principle of
light propagation. From the wave equation we derive an energy principle in
which the total energy (the sum of kinetic and potential energy) is conserved.
4.1. The Wave Equation in Elasticity
We introduce the wave equation with a simple derivation from one-dimensional
elasticity theory. The derivation illustrates the basic notions of conservation laws
and constitutive equations introduced in Chapter 2. Then we discuss a second
application, to an elastic string vibrating in a plane. Conservation of momentum
leads to a system of nonlinear PDE. Considering small-amplitude vibrations near
a stationary string, we linearize the equations, thereby deriving two wave
equations with different wave speeds. One equation represents longitudinal
motion along the string, and the other represents transverse motion—the
vibrations seen in a guitar or violin string.
4.1.1. Longitudinal Motion of a Thin Elastic Rod
Consider a thin elastic rod undergoing only longitudinal deformation (extension
or compression), with no bending.
We label locations of cross sections in the rod by using points in a reference
configuration, an interval, say 0 ≤ x ≤ 1. (See Fig. 4.1.) The physical configuration
is also an interval 0 ≤ u ≤ L, depending on the deformation. The cross section
labeled x in the reference configuration has coordinate u(x, t) in the physical
configuration at time t. It is convenient, but not essential, to think of the
reference configuration as being the rod in equilibrium, with no forces acting on
it. The function u is called the displacement; it is the unknown, or dependent

variable. We assume the density ρ (mass per unit volume in the reference
configuration) is constant, and that the cross-sectional area A of the rod is
constant along its length.
Figure 4.1. Deformation u in a one-dimensional rod. (a) Reference configuration
(Lagrangian variables); (b) physical configuration (Eulerian variables).
Figure 4.2. Forces on a small section of the rod. (a) Reference configuration
(Lagrangian variables); (b) physical configuration (Eulerian variables).
Consider forces on a cross section labeled x0 in the rod, at a specific time t.
The part of the rod with x > x0 exerts a force F(x0, t) on the part with x < x0,
and the part with x < x0 exerts an equal and opposite force −F(x0, t) on the part
with x > x0, so that across each cross section, forces are balanced (see Fig. 4.2).
However, the variation of these forces along the rod means that the net force
acting on a segment of rod may be nonzero and induces a change in momentum.
In our formulation of the equation of motion, it is convenient to express the
force distribution as a function of Lagrangian variable x rather than Eulerian
variable u, even though we think of the force acting in the physical domain
rather than the reference configuration. In fact, if we were to label forces in the
physical domain as f(u, t), then F(x, t) = f(u(x, t), t). Moreover, it is convenient to
consider the stress σ, which is force per unit area, rather than force F. In the
present context F = Aσ.
Since ut is the velocity of a point (i.e., cross section) in the rod, the
momentum density (meaning momentum per unit volume) is the quantity ρ ut.
Now consider a short segment of the rod a < x < b. The momentum of this
section is 
. The balance law states that the rate of change of
momentum is equal to the net force:

Notice that if ut(x, t) is constant in x, then this is precisely Newton’s law:
where mass means the mass of the little section of rod.
As in Chapter 2, we can now derive a PDE from the balance law by writing
both sides of the equation as integrals over a < x < b:
Thus, provided utt, σx are continuous, we have the PDE
which expresses conservation of momentum.
To this equation we add a constitutive law, an equation that relates σ to u in a
different way. In elasticity, this constitutive law states that the stress σ is a
function of the strain. The strain is the deformation gradient; in the one-
dimensional context of the rod, we have
In engineering, it is common to define strain to be ux − 1, so that zero strain
corresponds to no deformation: u(x, t) = x. In both cases, elasticity is expressed
by a functional relationship between σ and ux:
Substituting into the PDE (4.1), we obtain
As we observed earlier, this equation is hyperbolic if σ′(ux) > 0, in which case,
stress increases with strain, but it is elliptic if σ′(ux) < 0. The hyperbolic case is
more significant, especially for small deformations (more precisely, for small
strains), but the elliptic case is also important for large deformations; it is
associated with an effect called strain softening.
Perhaps the most important form of the constitutive law is Hooke’s law, which
states that increases in stress are proportional to increases in strain. This is
expressed in the formula
Note that this can also be stated as stress is proportional to strain if we define the
strain to be ux − 1.
Substituting (4.2) into (4.1), we obtain the one-dimensional wave equation

in which 
.
Remarks on Hooke’s law. The constant k > 0 is a constitutive parameter called
the elastic modulus that depends on the elastic properties of the material; it can be
measured in experiments. The same experiments assess the range of strains in
which Hooke’s law is reasonable.
The parameter c has dimensions of a speed, that is, L/T, where L and T are a
typical reference length (perhaps the length of the rod) and a typical time scale,
respectively. Correspondingly, density (mass per unit volume) has dimensions
M/L3, where M is the mass of the rod. It follows that k has dimensions LM/T2,
that is, the dimensions of mass × acceleration, the same dimensions as force.
Note that this is consistent with (4.2), since both u and x have dimensions of
length, so that ux is dimensionless.
Hooke’s law is familiar from elementary mechanics or the study of ODE. It
arises in relating the extension of a spring to the tension in the spring. To see the
connection with the rod, consider a uniform deformation given by u(x) = Lx.
Then σ(ux) = k(L − 1). But L − 1 is the extension (if L > 1); the stress σ is
constant and corresponds to the tension in the spring. Thus, the tension is
proportional to the extension. Indeed, just as for springs, the constant k in
Hooke’s law can be found by performing simple extension experiments.
4.1.2. The Vibrating String
Consider a thin elastic string, such as a guitar string or bungee cord, which we
treat as a one-dimensional curve. For simplicity, we assume that the string moves
only in two dimensions, and that the tension in the string is high enough that we
can ignore gravity. Another scenario with no effect of gravity would be an
experiment with a string constrained to a horizontal frictionless table. The
effectively one-dimensional elastic body is called a string when we assume that it
can be bent with no resulting force. Then we say there is no resistance to bending.
Let’s consider the motion of a point on the string labeled by x ∈ [0, 1]. At each
time t, this point will be located in the plane at (r1, r2) = r(x, t) ∈ R2 (see Fig.
4.3). Then the tangent to the string is rx(x, t). Since there is no resistance to
bending and no gravity, the only force on the string is due to the tension 
,
which acts tangentially and is the only nonzero component of the stress. If the
string has a uniform cross-sectional area A and constant density ρ (gm/cm3), then
the equations of motion (Newton’s second law, or conservation of momentum)
are

Figure 4.3. The elastic string; one-dimensional string deforming in two
dimensions.
Now we make the constitutive assumption that the tension 
 depends only on
the strain |rx| and write 
. Thus, the string equations are
Suitable boundary conditions, in which the string is fixed at two locations, are
With these boundary conditions, there is an equilibrium solution r0 = (xL, 0) in
which the string is stretched between the two fixed ends, as in a guitar string
before it is plucked or strummed. We assume that the tension at equilibrium is
positive: T (L) > 0, and also that it is increasing with strain: T′(L) > 0.
Now consider small deviations (u, v) from the equilibrium solution, and write
r = (xL + u, v). We aim to find equations for the new variables u, v as functions
of x, t. Of course, we can simply substitute this expressi on into the string
equations and get exact equations for u, v. However, we want to take advantage
of the smallness of u, v. To do this, we substitute into the PDE system (4.4) and
then use a Taylor expansion about the equilibrium solution, which is now u = v
= 0.
When we substitute into (4.4) and expand each term as a Taylor series in u, v
retaining only constant and first-order terms (linear in u, v), we get a lot of terms.
For example, we need
where h.o.t. represents the remaining higher-order terms in the Taylor series;
specifically, 
. Similarly,

Thus,
Finally, the equation (4.4) becomes a pair of wave equations if we retain only
terms linear in u and v; that is, drop the h.o.t. terms in (4.6):
In these linear wave equations, 
 is the longitudinal wave speed, and 
 is the transverse wave speed. It can be argued that the longitudinal
motion represented by u is smaller than the transverse motion if the string is
displaced laterally. Thus, a good approximation is to take the equation for v alone
and represent the string simply by the transverse displacement v(x, t), 0 < x <
1.
This is a useful way to think of solutions of the wave equation; for each fixed
time t the graph of v(x, t) represents the string. As time varies, the graph evolves
as a string in motion.
4.2. D’Alembert’s Solution
In 1747, d’Alembert1 published a paper on vibrating strings that included his
famous solution of the wave equation in one space variable x and time t:
The first, and fundamental, step in deriving d’Alembert’s solution is to show that
the general solution of (4.8) is
where F and G are arbitrary C2 functions. The lines x − ct = const., x + ct =
const., where F(x − ct), G(x + ct) (respectively) are constant, are called
characteristics.
Since c > 0 is constant, we can factor the partial differential operator 
and write the PDE as

Then since (∂t + c∂x) F(x − ct) = 0, and (∂t − c∂x)G(x + ct) = 0, we see that
(4.9) is a solution.
It will be useful when discussing solutions of the wave equation to interpret
(4.9) as the superposition of two waves: the graph of F(x − ct) as a function of x
for various times t is a wave traveling with speed c to the right, and G(x + ct)
represents a wave moving to the left with speed c. Thus, the wave equation (4.8)
models waves of speed c moving in both directions, to the left and right, just as
the linear transport equation models waves of speed c > 0 moving to the right
only. We can add the two waves in (4.9), because the PDE (4.8) is linear and
homogeneous.
To see that every solution can be represented in the form (4.9) for some
choice of functions F, G, we introduce characteristic variables suggested by the
factorized equation (4.10):
and write z(ξ, η) = u(x, t). Then we have
Adding and subtracting as in (4.10), the equation becomes
Integrating over η, we find
for some function g (which is constant with respect to η).
Thus, z(ξ, η) = G(ξ) + F(η), where G(ξ) = ∫ g(ξ) dξ, and F is another
arbitrary function. Back in the original variables, we arrive at
4.2.1. Initial Value Problem (Cauchy Problem)
We use the general solution (4.9) of the wave equation to solve the Cauchy
problem

Just as for ODE, since the PDE is second order in t, to have a well-posed problem
(cf. Section 2.1) we have to specify both the initial displacement u and the initial
velocity ut.
Theorem 4.1. If ϕ is C2 and ψ is C1, then the unique C2 solution of (4.11) is given by
Proof. The general solution of the PDE is
Then the initial conditions give
Integrating the second equation yields 
.
Now we can solve for F and G, leading to (4.12). We leave it as an exercise to
verify that the initial conditions are satisfied.
It is clear from (4.12) that u(x, t) is a C2 function. Uniqueness is a
consequence of the fact that F and G in the general solution are determined by
the initial condition.
Formula (4.12) is known as d’Alembert’s solution. Note that since ψ specifies ut
at t = 0, it is consistent that it should be integrated in a formula for u. In
integrating ψ, we gain a derivative. Thus, we have the following regularity of the
solution:
In other words, the solution inherits regularity from the initial data. There is no
gain or loss of regularity, a property typical of hyperbolic PDE. In fact, (4.12)
makes sense even if ϕ or ψ are less regular than in the theorem; the
corresponding function u(x, t) is then known as a weak solution, even though the
derivatives of the solution seemingly required by the PDE may not exist.
D’Alembert’s solution allows us to establish another part of well-posedness,
namely, continuous dependence on the data.
Proof of continuous dependence. Let u = u1, u = u2 be solutions of problem
(4.11) with initial data ϕk, ψk, k = 1, 2, that are bounded and uniformly close in
the sense of continuous functions:

where ϵ > 0 is small. From (4.12), we have
In this calculation, we have used the triangle inequality and the integral estimate
|∫ f(x)dx| ≤ ∫ |f(x)|dx.
It follows that if |ϕ1 − ϕ2| and |ψ1 − ψ2| are uniformly small, then |u1(x, t) −
u2(x, t)| is small at each x, t < ∞. Note that the estimate gets worse with
increasing time, so that to make u1 − u2 uniformly small in x and t, we have to
take a finite time interval, unless we include 
 in the smallness
condition.
Figure 4.4. Initial displacement ϕ(x).
Example 1. (Representing d’Alembert’s solution graphically) For simplicity,
let’s take the initial velocity to be zero, ψ(x) ≡ 0, choose c = 2, and take the
support of ϕ to be the interval [1, 3]. The support of a function ϕ is defined to be
the closure (including boundary points) of the set where ϕ is nonzero. Thus, supp
.
The solution of (4.11) in this example is
The initial data are piecewise linear, with changes in slope at x = 1, 2, 3 (Fig.
4.4). Correspondingly, the solution (at a fixed t) will be piecewise linear, with
changes in slope expected at values of x for which

The graph of ϕ(x + 2t) as a function of x has the shape of a triangle moving to
the left with speed 2, and ϕ(x − 2t) has the same shape and speed, but moving
to the right. The solution (4.14) is the average of these two graphs.
We can represent the solution in the x-t plane, as shown in Figure 4.5. The
leading edge of the left-moving triangular wave lies on the characteristic x + 2t
= 1; it gets to x = 0 when 
. Starting at x = 1, and moving left with speed 2,
the wave takes until 
 to reach 0.
Domain of dependence and region of influence. The structure of the
characteristics in the x-t plane in Figure 4.5 suggests how the initial data
propagate and influence the solution. Likewise, we can consider the dependence
on the initial data of the solution at a point (x, t).
The backward characteristics through a point a point (x0, t0) with t0 > 0 are the
lines
The backward characteristics intersect the x-axis at x = x0 ± ct0; the solution of
the Cauchy problem at (x0, t0) depends only on the initial data in the interval
between these points. The interval is referred to as the interval of dependence of
the point (x0, t0). More common terminology is to refer to the triangle with base
given by the interval of dependence and sides given by the backward
characteristics as the domain of dependence of the point (x0, t0).
Figure 4.5. The x-t plane for Example 1. Note that the slope of the characteristics
is the reciprocal of the wave speed.

Figure 4.6. (a) Domain of dependence (x0, t0) and (b) region of influence of [a,
b]for the wave equation.
The region of influence of a point (x0, t0) is the set bounded by the forward
characteristics:
We can also speak of the region of influence of a subset of the x-t plane, but more
commonly we refer to the region of influence of an initial interval a ≤ x0 ≤ b,
with t = 0. The region of influence of the interval [a, b] on the x-axis is the set
bounded by characteristics x + ct = a, x − ct = b.
These notions give us a graphical means of understanding how initial data
propagate forward in time, as shown in Figure 4.6. In particular, if the data have
compact (i.e., bounded) support in an interval [a, b], then the solution is
necessarily zero outside the region of influence of the initial interval [a, b], as
can be seen by drawing backward characteristics from any point outside this
region of influence. We say that initial disturbances (meaning where ϕ or ψ are
nonzero) propagate with finite speed c.
4.2.2. The Wave Equation on a Semi-Infinite Domain
The Cauchy problem shows how initial disturbances propagate as waves in free
space. To describe how these waves are reflected at a boundary, we formulate
and solve an initial boundary value problem on the quarter-plane {(x, t) : x > 0,
t > 0} with a single boundary.
Consider the initial boundary value problem
The boundary condition specifying u(0, t) means that the end of the string is held

in place. We could instead specify the slope ux(0, t), which would mean the stress
is specified. In particular, the boundary condition ux(0, t) = 0 is referred to as a
stress-free boundary condition.
For x > ct, we have d’Alembert’s solution (4.12)
with ϕ, ψ evaluated only for positive values of their arguments.
To obtain an expression for the solution in the region 0 < x < ct, we have to
use the boundary condition, since (4.16) does not apply for x − ct < 0. Notice
that characteristics with positive speed c propagate into the domain from the
boundary x = 0 as t increases (see Fig. 4.7). These characteristics carry
information from the boundary condition. Moreover, each point in the quarter-
plane also has a characteristic moving left with speed c that originated on the
initial line x > 0, t = 0. Therefore, the solution for x < ct will involve both the
initial condition and the boundary condition.
The solution can be found from the general solution (4.13), obtaining
expressions for the functions F, G from the initial and boundary conditions, much
as was done in the proof of Theorem 4.1. The result of this calculation is
Observe that this formula satisfies the boundary condition u(0, t) = 0. Formula
(4.17) can be interpreted as the solution of the Cauchy problem with initial data
on the entire real line obtained by extending both ϕ and ψ to be odd functions, so
that ϕ(−x) = −ϕ(x), ψ(−x) = −ψ(x), x > 0. Then (4.17) uses the oddness of
the extension to express the solution entirely in terms of the given data on the
positive x-axis.
Moreover, there is another interpretation of this construction. The solution of
the Cauchy problem with odd initial data involves waves moving left and right in
the upper half-plane. Those with x < 0 and moving right have the property that
along the line x = 0 (the t-axis), they exactly cancel waves moving left. This
cancellation explains how the boundary condition u(0, t) = 0 is satisfied.

Figure 4.7. Characteristics for the quarter-plane problem.
Example 2. (Representing a quarter-plane solution graphically) Let’s suppose
supp ϕ ⊂ [a, b]and supp ψ ⊂ [a, b]. The solution is represented in the x-t plane in
Figure 4.7, where we have drawn forward characteristics from the boundary of
the support of the initial data, including their reflections from the boundary x =
0. The reflected characteristics record the switch from x − ct to ct − x in the
solution; equally, we can think of the reflected characteristics as originating from
the x-axis and carrying information from the extended initial data. Finally, the
reflected characteristics carry information from the boundary (specifically, the
boundary condition) into the interior of the domain. This last point of view is
helpful when considering nonzero boundary data. In Figure 4.7 the domain is
divided into sectors in which we can write the solution in more detail. For
example, the solution is zero in three of the regions. In each of the other regions,
we use backward characteristics to see which part of the support of the initial
data is used in calculating the solution.
For (x, t) in the triangular region labeled Δ in Figure 4.7, the backward
characteristics hit the x-axis at x − ct and x + ct, both of which lie in the
interval [a, b] Thus, there is no simplification, and u is given by d’Alembert’s
formula (4.16).
In region I, x − ct < a, and ct − x < a, while a < x + ct < b, so that both
(4.16) and (4.17) reduce to

which is a wave traveling to left, a function of x + ct.
In region II, u(x, t) is likewise a function of x − ct; the graph is a wave
traveling to the right.
In region III, part of the wave is reflected by the boundary and interacts with
the wave traveling toward the boundary. In fact there is just enough cancellation
so that the boundary condition is satisfied at x = 0. The reflected wave emerges
as a function of (x − t) in region IV. Thus in region III, where both waves are
present, and a < ct − x < x + ct < b, u(x, t) is given by the full formula (4.17).
In region IV, after the left-moving wave has been fully reflected and is now
moving to the right, we have x + ct > b, and thus
In region V, x − ct and ct − x are less than a, while x + ct > b. (This case is
also shown in Fig. 4.7.) Therefore, there is no contribution from the initial
displacement ϕ, and the contribution from the initial velocity is constant:
Similarly, in the region between IV and the t-axis, we find u ≡ 0. We see this by
noting that both x + ct and ct − x are larger than b. Equivalently, since x − ct
< −b < b < x + ct, we find that u is the integral of the odd extension of ψ from
−b to b and hence is zero.
It is instructive to graph the solution carefully for various values of t, say, for
Example 2, with ϕ triangular (see Fig. 4.4) and ψ zero.
Example 3. (Nonzero boundary condition) Consider the initial boundary value
problem with nonzero boundary condition
The solution is similar to (4.16), except there is an additional term that
propagates the boundary data into the domain x > 0. To derive the additional
term, first observe that if ϕ and ψ are zero, then the string is initially horizontal
and at rest. The displacement h(t), specified at the boundary, propagates into the
interior, and induces motion of the string. Consequently, this disturbance

propagates as a wave u(x, t) = F(x − ct) with speed c. Setting x = 0 in this
solution, we match the boundary condition: F(−ct) = h(t). Consequently, F(ξ) =
h(−ξ/c), for ξ < 0, and the solution
follows immediately. The full solution is obtained by simply superimposing the
solution with h ≡ 0. Note that right-moving characteristics carry only half the
information of the solution for the Cauchy problem, but they carry all the
boundary information, since the boundary data travel only to the right in the
physical domain x > 0.
4.3. The Energy E(t) and Uniqueness of Solutions
In this section we define an energy function for the wave equation, show that
energy is conserved for the Cauchy problem (4.11), and use this property to
establish uniqueness of solutions of the Cauchy problem.
Let’s assume that u = u(x, t) is a smooth solution of the Cauchy problem and
the derivatives ut(x, t), ux(x, t) are square integrable (i.e., in L2(R)) for each t ≥ 0.
Then the total energy defined by
is finite.2
Note that E(t) is the sum of the kinetic and potential energies. The potential
energy 
 is the energy stored in the string due to tension, and the
kinetic energy 
 is akin to the quantity 
 in classical
mechanics of a rigid body with mass m and velocity v.
To see how E(t) is connected to the one-dimensional wave equation (4.8), we
multiply the PDE by ut and integrate by parts:
Thus, we have

That is,
Therefore, we have conservation of total energy: E(t) = constant, from which we
deduce
This identity is an important tool for existence and regularity of solutions, but
also for uniqueness of solutions, as we now discuss.
Uniqueness of solutions. Consider the Cauchy problem
in which the inhomogeneity f(x, t) is a specified function representing a time-
dependent distribution of force along the one-dimensional elastic body. For
example, if the PDE represents small transverse vertical vibrations of an elastic
string, then f(x, t) = −g could be the force distribution due to gravity. (Note that
the density ρ has been absorbed into c2.)
We can use the energy calculation to prove uniqueness of C2 solutions of
(4.19). Consider two C2 solutions u1, u2 with the same data ϕ, ψ, f. To prove
uniqueness, we show u1 = u2. Define u(x, t) = u1(x, t) − u2(x, t). Then u satisfies
the homogeneous version of (4.19), with zero initial data:
Since E(t) = E(0) = 0 for this problem, we have
Therefore,
Thus, u is constant in x and t. But u(x, 0) = 0, so the constant is zero. Hence u =
u1 − u2 ≡ 0.
4.4. Duhamel’s Principle for the Inhomogeneous Wave Equation
Duhamel’s principle is used to solve inhomogeneous initial boundary value
problems when we have the solution of the homogeneous problem in hand.

Consider the initial value problem
To solve (4.20) using Duhamel’s principle, let 
 be the solution (for each s
> 0) of
To understand why  might be helpful, consider the special case in which f(x, t)
= F(t) is independent of x, and we seek a solution v(t) independent of x. Then we
have
Integrating twice and reversing the order of integration, we see that
But for each s > 0, w(s, t) = (t − s) F(s) solves the x-independent version of
(4.21), namely,
and 
.
This calculation suggests that 
 solves (4.20). To complete
the solution, it remains to find 
. But  satisfies a Cauchy problem for the
homogenous wave equation with initial condition at time t = s. We can adapt
d’Alembert’s solution by translating t by s in d’Alembert’s formula, with ϕ(x) =
0; ψ(x) = f(x, s). This gives a formula for :
Now let 
. That is,
The double integral is an integration of f over the triangular domain of
dependence of (x, t) shown in Figure 4.6.

Claim 4.2. The function u(x, t) satisfies (4.20).
Proof. Let (x0, t0) be fixed with t0 > 0. The proof involves integrating the PDE
(4.20) over the domain of dependence Δ = {(x, t) : x0 − c(t − t0) < x < x0 +
c(t − t0), 0 < t < t0} and using Green’s theorem in the plane (see Appendix A).
With the exact differential du = uxdx + utdt, the integral on the boundary is
reduced to u(x0, t0).
It is straightforward to use this procedure to solve the more general initial
value problem in which the initial data can be nonzero:
We solve this problem by considering each of f, ϕ, ψ separately, setting the others
to zero; the solution is then the sum of the corresponding solutions:
4.5. The Wave Equation on R2 and R3
The method of spherical means uses the rotational and translational invariance of
the wave equation to find solutions that are the analog in Rn, n ≥ 2, of
d’Alembert’s solution in one dimension. The resulting formulas allow us to
understand the Huygens principle of wave propagation. Huygens originally
expressed his principle geometrically, using spheres centered at points of a
wavefront with radius given by an incremental time and arguing that the
intensities would cancel except along the expanding surface formed as the
envelope of the overlapping spheres.
Here we show briefly how to derive an explicit formula for the solution of
initial value problems in R2 and R3. It is in fact easier to start with R3. Suppose
u(x, t) is a solution of the wave equation
with Cauchy data u(x, 0) = ϕ(x), ut(x, 0) = ψ(x). For r > 0, define the spherical
means 
, where S(x, r) denotes the sphere with center
x and radius r. (See Appendix A for the integral average notation .) If we can
find v(r, t), then we recover u(0, t) = limr → 0 v(r, t) But this will give a formula

for u(x, t) for any x, t, by centering the spheres at a general point x ∈ R3 instead
of at x = 0. Here are the main steps in constructing the formula.
1. Observe that v(r, t) is a rotationally invariant solution of the wave equation
with initial data given by the averages of the data for 
, 
.
Notice that v(r, t) is symmetric about the origin, but we could equally well have
centered the spheres at any point x in space, taking integral averages over the
resulting spheres S(x, r), leading to the same statement of the Cauchy problem for
v, but with different 
.
2. Let w(r, t) = rv(r, t). Then w satisfies the one-dimensional wave equation
with initial data 
, and boundary condition w(0, t) = 0.
Consequently, d’Alembert’s solution can be used to solve this quarter-plane
problem, effectively using the even extensions of 
 or equivalently, the
odd extensions of 
.
3. Since we are interested only in u(0, t) = limr → 0 v(r, t) = limr → 0 w(r, t)/r, we
write the solution with r < ct:
Notice that we write the final ϕ term as the derivative of an integral, which
turns out to be more convenient than the equivalent form used in Section
4.2.1.
4. Now we compute the limit as r → 0:
since w(0, t) = 0. Computing this derivative using (4.22) and including the
formulas for the integral averages (see Appendix A), we find
5. Finally, we observe that by translation invariance, the corresponding formula
applies by centering the spheres at any point x ∈ R3:

Solution in the plane (n = 2). The formula (4.23) can be adapted to find the
solution of the wave equation in two space dimensions. The idea is to consider
solutions v(x1, x2, t) = u(x1, x2, x3, t) that are independent of x3 Then v satisfies
the wave equation in two dimensions and in three dimensions. Since initial data
ϕ, ψ are in two dimensions, we consider them as functions of x1, x2, x3 but
independent of x3. Then we calculate the integrals in (4.23) by representing the
spheres over the projections onto the x1 − x2 plane, which are disks. This process
gives the formula
Everything here is to be interpreted in two dimensions. Thus, x = (x1, x2), y =
(y1, y2), B(x, ct) is the two-dimensional disk centered at x, and with radius ct, and
dy = dy1dy2.
Because of the way (4.24) in two dimensions is related to (4.23) in three
dimensions, it is not surprising that the integrals are over the disks B(x, ct).
However, this has a profound consequence, because now the solution depends on
values of ϕ and ψ inside the disk B(x, ct), in contrast to the three-dimensional
case, in which the dependence is only on values of the data on the expanding
sphere S(x, ct). Thus, the Huygens principle does not hold in two dimensions. For
example, waves generated by dropping a stone into the flat surface of a body of
water generates not just a circular expanding wave, but also lots of concentric
ripples behind the leading wave.
PROBLEMS
1. Consider the initial value problem
Let ϕ(x) be the function with graph shown in Figure 4.4, and ψ(x) ≡ 0. In the x-t
plane representation of the solution in Figure 4.5, we find u ≡ 0 in the middle
section, with 
. Show that if we keep the same ϕ but make ψ nonzero, with

supp ψ = [1, 3], then u will still be constant in this middle section. Find a
condition on ψ that is necessary and sufficient to make this constant zero.
2. Consider C3 solutions of the wave equation
For c = 1, define the energy density 
, and let p = utux (the
momentum density).
(a) Show that et = px, pt = ex.
(b) Conclude that both e and p satisfy the wave equation.
3. Suppose u(x, t) satisfies the wave equation (4.25). Show that:
(a) For each y ∈ R, the function u(x − y, t) also satisfies (4.25).
(b) Both ux and ut satisfy (4.25).
(c) For any a > 0, the function u(ax, at) satisfies (4.25). Note that the
restriction a > 0 is not necessary.
4. (a) Let u(x, t) be a solution of the wave equation (4.25) with c = 1, valid for
all x, t. Prove that for all x, t, h, k,
(b) Write a corresponding identity if u satisfies (4.25) with c = 2.
5. Consider the quarter-plane problem
Let ϕ(x) be the function with graph shown in Figure 4.4, and let ψ(x) ≡ 0.
Sketch the solution u(x, t) as a function of x for 
.
6. Consider the quarter-plane problem with a homogeneous Neumann boundary
condition
Suppose supp ϕ = [1, 2]= supp ψ.

(a) Solve for u(x, t), x ≥ 0, t > 0.
(b) Where can you guarantee u = 0 in the first quadrant of the x-t plane?
(c) Consider ϕ ≡ 0; write a formula for u.
(d) If 0 is in the support of ϕ or ψ (e.g., if limx → 0+ ϕ(x) ≠ 0), write conditions
that guarantee u is (a) continuous and (b) C1. Explain your answers in terms of
the behavior of the data around the boundary of the domain. (Any
compatibility condition will be effectively at the origin, but you will need to
match u, ux, and ut across x = t.)
7. Consider problem 6, but in the more general case in which ux(0, t) = h(t) is
not identically zero. Here, the general solution can be employed with the
boundary condition to find F(ξ) for ξ < 0 in terms of G(−ξ) and h. Using this
approach, derive the solution
for x < t. Derive a suitable compatibility condition at the origin that ensures the
solution is continuous when the data are continuous. What about the first
derivatives across x = t?
8. Consider the wave equation that includes frictional damping:
in which μ > 0 is a damping constant. Show that if u(x, t) is a C2 solution with ux
→ 0 as x → ∞, then the total energy 
 is a decreasing
function.
Incidentally, can you devise a C2 function f(x) with the property f(x)
approaches a constant as x → ±∞, but f′(x) does not approach zero?
9. Consider the quarter-plane problem (4.15).
(a) Formulate the mechanical energy E(t) for solutions, and show that it is
conserved. Specify any assumptions you need on the initial data.
(b) For the nonzero boundary conditions (4.18) of Example 3 evaluate E′(t) in
terms of the data ϕ, ψ, h.
10. Let f(x, t) be a continuous function, and let Δ(x, t) denote the domain of
dependence of the point (x, t) for (4.25). Use the Fundamental Theorem of
Calculus to show directly that 
 satisfies

11. Consider the wave equation in three dimensions, with initial conditions in
which ϕ(x) = f(|x|) is rotationally symmetric, the function f satisfies f(r) = 0, r
≥ ϵ, and ψ ≡ 0. Show that the solution u(x, t) is (a) rotationally symmetric, and
(b) zero outside a circular strip centered at the origin and having width ϵ.
1. Jean le Rond d’Alembert (1717–1783).
2. Since the constant density ρ has been absorbed into c2, the physical energy is actually ρE(t).

CHAPTER FIVE
The Heat Equation
The heat equation
is the prototype of parabolic PDE and models diffusion processes, including heat
flow and the spread of a solute in a fluid. The heat equation also plays a
significant role in models of combustion, fluid flow with temperature dependence
(for example, when density depends on temperature), and population dynamics.
In chemical and biological systems, diffusive processes are commonly modeled by
random walks and Brownian motion, which are closely related to the heat
equation.
The heat equation has the remarkable property that even for rough initial
data, solutions are immediately smoothed. This property is in contrast to
hyperbolic equations, such as the wave equation, for which rough initial data
remains just as rough through its evolution. Solutions of the heat equation also
exhibit infinite propagation speed, meaning that a change in temperature in one
location is immediately detected everywhere. (But the effect decays exponentially
with distance from the source of the change.) Since characteristics are defined
only for hyperbolic equations, the method of characteristics does not apply to the
heat equation. Instead, we introduce several new PDE techniques that are
applicable in general to linear PDE.
The fundamental solution lies at the heart of the theory of infinite domain
problems. On bounded domains, the fundamental solution is adapted to take
account of boundary conditions. The adapted functions are called Green’s
functions.
The maximum principle applies to the heat equation on domains bounded in
space and time. This important property of parabolic equations is used to prove a
variety of results, such as uniqueness of solutions and comparison principles.
The energy method for the heat equation has a few key differences from the
method for the wave equation. For example, the physical heat energy is not very
useful, and instead we introduce a mathematical energy function. Typically, this
energy is not conserved and decays in time. The decay of the energy leads to
straightforward uniqueness results, just as for the wave equation. The energy
decay is also useful for obtaining estimates that are part of the existence and
regularity theory for solutions of parabolic equations.

Separation of variables is a procedure for solving certain initial boundary value
problems. The procedure is straightforward for the heat equation, and with small
modifications it also applies to the wave equation and Laplace’s equation. The
method yields solutions represented as infinite series of eigenfunctions associated
with the PDE and boundary conditions. We first demonstrate the technique on
specific examples. More generally, in the next chapter we analyze eigenvalue
problems for ODE and PDE, and the convergence of Fourier series.
5.1. The Fundamental Solution
The fundamental solution is important for problems on infinite domains. In this
section we define the fundamental solution Φ(x, t) and show how it is used to
solve the Cauchy problem:
To derive the fundamental solution, we use the scale invariance property of the
heat equation. Let a > 0 be a constant and introduce the change of variables 
. Then the heat equation is unchanged but is expressed in the new
variables:
This scale invariance suggests that we seek solutions of the self-similar form
for some α ∈ R. Substituting into the heat equation, we see that whatever the
value of α, the function v satisfies an ODE with nonconstant coefficients:
To select α, we introduce the property of conservation of heat energy, which
we wish to have satisfied by our solution. Suppose u is a solution of the heat
equation with the property that 
, and ux(x, t) → 0 as x → ±∞.
Then, integrating the PDE, we find
so that the total heat energy is conserved:

However,
which suggests we should scale the function v by 
:
that is, choose 
. With this scaling, heat is conserved in the sense of (5.2).
Now we solve (5.1). Since it is a second-order equation, there will be two
independent solutions. First rewrite the ODE as
Thus,
Since we are really only seeking one solution, it is convenient to set the constant
to zero, resulting in a homogeneous first-order equation that has general solution
Converting back to (x, t), with 
, we obtain the similarity solution
Usually, we choose a particular value of A so that the constant in (5.2) is one;
for this choice of constant, we have the fundamental solution of the heat equation:
(see Fig. 5.1). In higher dimensions, x ∈ Rn, the fundamental solution takes a
similar form:
Then u(x, t) = Φ(x, t) satisfies ut = kΔu.
Properties of the fundamental solution Φ(x, t).
1. Φ(x, t) > 0 for all x ∈ Rn, t > 0.

2. Φ is C∞ in (x, t), t > 0.
3. ∫Rn Φ(x, t) dx = 1 for all t > 0.
Figure 5.1. Graph of the fundamental solution Φ(x, t) for the heat equation, with
t > 0.
Properties 1 and 2 follow directly from the formula for the fundamental solution.
For n = 1, property 3 is verified by direct calculation:
For n > 1, property 3 follows from 
.
From properties 1 and 3, we see that Φ(x, t) is a probability distribution. In
fact Φ is a normal distribution for each t > 0 with interesting dependence on t in
the limits t → ∞ and t → 0. The area under the graph is 1 for all t > 0, yet as t
→ ∞, maxx Φ(x, t) → 0; the tail spreads out to maintain ∫ Φ = 1. As t → 0 the
maximum (at x = 0) blows up like 
, but the integral remains constant. We also
observe Φ(x, t) → 0 for x ≠ 0, as t → 0+.
5.2. The Cauchy Problem for the Heat Equation
We are now ready to solve the Cauchy problem
using the fundamental solution

which satisfies (5.5a) for t > 0.
By translation invariance, Φ(x − y, t) is a solution of (5.5a) for all y. Thus,
is also a solution of (5.5a).
By linearity and homogeneity of the PDE, we can take linear combinations of
solutions, which suggests that
should be a solution. Moreover, properties of Φ suggest that as t → 0+, u(x, t) →
g(x), since Φ(x − y, t) collapses to zero away from y = x and blows up at y = x
while preserving ∫ Φ = 1.
Because of the exponential in Φ(x, t), the integrals for u, ut, uxx all converge
for t > 0 provided g ∈ C(R) is bounded. Then
so that u satisfies the PDE for t > 0.
It is more complicated to prove rigorously that the initial condition (5.5b) is
satisfied, since t = 0 is a singular point for Φ (in that Φ(x, t) is not defined at t =
0). To get a rough idea of how (5.5b) holds as a limit as t → 0+, let’s fix x. Then,
for δ > 0,
The second line has two integrals. In the first integral, g(y) ≈ g(x) for small δ
since g is continuous. The second integral approaches zero as t → 0+, because Φ
→ 0 uniformly and exponentially away from y = x as t → 0+. In the following
theorem we state this limit carefully and prove it by estimating the integrals. The
final integral gives g(x), since it is independent of y, and the integral of Φ over a
small interval around y = x becomes unity as t → 0+, since Φ approaches zero

sufficiently fast elsewhere.
Theorem 5.1. Let g ∈ C(R) be bounded, and let u(x, t) be given by (5.6). Then
1. u is C∞ in (x, t) for t > 0; and
2. u satisfies the heat equation ut = kuxx, x ∈R, t > 0; and
3 
 for all x0 ∈ R.
Proof. Property 1 follows because Φ is C∞ for t > 0, and since derivatives of Φ
all decay exponentially as |x| → ∞, the integrals converge. Property 2 follows
from Φt = kΦxx, t > 0.
To prove property 3, we consider the difference |u(x, t) − g(x0)|, and estimate
the integrals, guided by the discussion above. This is the first time we have
encountered these kinds of estimates, so we provide the details.
Let ϵ > 0. We wish to show |u(x, t) − g(x0)| < ϵ for (x, t) close to (x0, 0).
Since ∫ Φ dx = 1, we can express the number g(x0) as an integral, so that
Let δ > 0 (we choose δ below), break up the integrals in (5.7), and use the
triangle inequality:
Now we use δ in two ways to show the two integrals are small.
In the first integral, by continuity of g we can choose δ > 0 small enough that
|g(y) − g(x0)| < ϵ for |x0 − y| < δ, which is the domain of the integral. We are
left with an integral of Φ(x − y, t), which is bounded uniformly by 1, even
though Φ(x − y, t) blows up as t → 0.
In the second integral we observe that 
, provided 
, while g(y) is bounded. To make this observation more precise, we
write the right-hand side of (5.8) as Iδ + Jδ. Choose δ > 0 so that |g(y) − g(x0)|
< ϵ for |y − x0| < δ. Then

The integral Jδ is somewhat trickier. Since g is bounded, there is K > 0 such that
|g(y)| ≤ K, for all y. Thus,
Consider x satisfying 
. (Recall that we are considering the limit as x →
x0.) Then 
 in the range of integration |x0 − y| > δ. However, this is not
a good enough estimate of the exponential, because we would still be left with an
integral over an infinite interval of a small but positive quantity. To get a more
useful estimate, we observe (see Problem 1) that in the region of integration, 
. Then
for t > 0 sufficiently small. Here we have used the change of variables z = 
 and the constant 
. Now it follows that |u(x, t) − g(x0)| <
2ϵ for 
 sufficiently small. This proves property 3 and completes the
proof of the theorem.
The solution (5.6) is a convolution (Φ(·, t) ∗ g)(x) for each t > 0. For
integrable functions ϕ, ψ on R the convolution product of ϕ and ψ is a function ϕ
∗ ψ defined by
The convolution product provides a useful construction of the product of two
integrable functions. It is used widely in signal processing and Fourier analysis.
Property 3 of the theorem shows that Φ(·, t) ∗ g → g as t → 0. In this sense, the
functions Φ(·, t) converge as t → 0 to a generalized function δ defined informally

by
We make this precise in Chapter 9, where δ is defined as the Dirac delta function, a
distribution.
Recall that for first-order equations and for the wave equation, solutions
propagate with finite speed, meaning that the region of influence of any point is
bounded over finite time. However, the heat equation has the property, typical of
parabolic equations, that solutions propagate with infinite speed: the region of
influence of a point is immediately all of space.
To make this concept a bit more precise, consider an initial temperature
distribution u(x, 0) = g(x) that is nonnegative and has compact support (see
Appendix A). An example is the function g(x) = ϕ(x) in Figure 4.4 that we
considered for the wave equation. For the wave equation, such an initial
disturbance propagates with finite speed, the wave speed. But for the heat
equation, the solution u(x, t) is immediately positive everywhere, meaning that
for all t > 0, no matter how small, u(x, t) > 0 for all x ∈ R. You can see this
directly from the solution (5.6). Thus, the initial data, confined to a bounded set,
has induced a positive temperature everywhere immediately. Of course, the
temperature drops off exponentially with distance from the support of g and is
tiny outside the support for small t, but nonetheless, the initial temperature
distribution has propagated with infinite speed.
This is pretty clearly not physical (we cannot have signals propagating faster
than the speed of light!) and is a limitation of the heat equation, and in particular
of Fourier’s law of heat transfer. From a different point of view, it is a
consequence of taking a finite speed effect, namely, the exchange of thermal
energy between molecules, and describing the process in a continuum in a
seemingly natural way that introduces this anomaly. Nonetheless, the heat
equation is an extremely useful PDE that describes heat transfer accurately in
many situations.
5.2.1. Using the Fundamental Solution to Solve Quarter-Plane Problems
To introduce a boundary condition, consider the quarter-plane problem

As in the strategy for the wave equation, we reflect the initial data, so the
solution satisfies the boundary condition. Let 
 be the odd extension of g(x):
and define
Then
since Φ(y, t) is an even function of y, and  is an odd function. Note that u(x, t) is
an odd function of x ∈ R. That is, the symmetry in the initial data is carried
through to the same symmetry in the solution.
Now replace 
 with g(y) using 
 for y < 0. Then
For a quarter-plane problem with a homogeneous Neumann boundary
condition, corresponding to an insulated end at x = 0, the calculation involves
the even extension of the initial data:
The solution is obtained by extending g using the even extension , so that the
first derivative is zero at x = 0. Then
satisfies the heat equation. To check the boundary condition, we calculate 
. Now Φ(y, t) is even in y, so Φx(y, t) is odd.
Thus, ux(0, t) = 0, as required.

5.3. The Energy Method
In this section we show the sense in which heat energy is conserved on both
bounded and unbounded domains. The function H(t) = ∫ u(x, t) dx is
proportional to the physical heat energy when u(x, t) is the temperature
distribution. We also consider a mathematical energy integral E(t) = ∫ u2(x, t) dx,
which is useful for the heat equation, just as the mechanical energy was for the
wave equation. The treatment of energy integrals is very significant for the study
of PDE, especially nonlinear PDE, where estimates of mathematical energies help
establish existence and uniqueness.
Consider a C2 solution u(x, t) of the heat equation on a bounded interval:
The heat energy 
 satisfies
Thus, the evolution of heat energy is controlled by the heat flux −kux through
the ends x = a, b. If the ends are insulated, then ux = 0 there, and the heat
energy is constant.
On an unbounded domain, the calculation works similarly, except it is natural
to assume there is no heat loss at infinity and that the temperature u(x, t) is
integrable as well as smooth. Suppose ut = kuxx, −∞ < x < ∞, t > 0. Let 
. Then
To analyze the mathematical energy 
, we multiply (5.11) by
u and integrate over [a, b]:
Therefore,
Thus, the energy integral 
 is decreasing in time t if 
. For
example, if either u or ux is zero at each end point x = a, x = b, then the energy
integral E(t) decreases in t. In this case, we have the important comparison to the

initial data:
That is,
On an infinite domain, we assume the solution u(x, t) is square integrable, so
that 
 is defined. Then, since it is reasonable to assume u(x, t) →
0 as x → ±∞, and ux is bounded in x, we obtain E′(t) ≤ 0. Once again the
energy is decreasing, and we have the comparison (5.12) to the initial energy.
5.3.1. Using the Energy Method to Prove Uniqueness
Decay of the mathematical energy allows us to prove uniqueness of solutions of
the initial boundary value problem
where f, g, h, ϕ are given functions. Because the following result and proof are
very similar to the uniqueness argument for the wave equation, we demonstrate
the method for the initial boundary value problem rather than for the Cauchy
problem.
Theorem 5.2. If u1, u2 solve (5.14) and are C2 functions, then u1 = u2 everywhere.
Proof. Let u = u1 − u2. Then u satisfies (5.14) with zero data: f ≡ g ≡ h ≡ ϕ ≡
0. Thus, from (5.13),
Therefore, u ≡ 0, so that u1 ≡ u2.
5.3.2. The Energy Principle in Higher Dimensions
The same argument works in the more realistic context of higher dimensions but
uses multivariable calculus. Consider a bounded open subset U of Rn and the
initial boundary value problem

We begin by defining the energy integral as in one dimension:
Then
In conclusion, the mathematical energy E(t) decreases in time unless the
boundary conditions inject energy into U. In particular, with either homogeneous
Dirichlet or Neumann boundary conditions, E(t) is decreasing in time.
5.4. The Maximum Principle
Maximum principles provide a powerful alternative to the energy method for
analyzing parabolic equations. We prove the maximum principle for the heat
equation, noting that similar ideas apply to more general linear and nonlinear
second-order parabolic equations, but not in general to systems of equations, nor
to higher-order equations.
The maximum principle states that the maximum of any (smooth) solution of
the heat equation occurs either initially (at t = 0) or on the boundary of the
domain. For a time interval [0, T] with T > 0 and a spatial domain U ⊂ Rn, we
use the notation UT = U × (0, T], shown schematically in Figure 5.2. Note that
UT includes both the interior and the top portion of the boundary U × {t = T}.
The part of the boundary of UT defined by

Figure 5.2. Domain UT for the maximum principle.
is known as the parabolic boundary. Here, ∂U denotes the boundary of U.
Solutions of the heat equation should have two spatial derivatives and one time
derivative, so we define the appropriate space of functions on UT:
To compare values of u in UT with values on the boundary, in the following
theorem we require that u should be continuous on 
.
Theorem 5.3. (The Maximum Principle) Let 
 satisfy
Then 
.
Remarks. Suppose u has a local maximum at (x, t) ∈ U × (0, T). Then (by
calculus) ut = 0, ∇xu = 0 and Δu ≤ 0. If we knew Δu < 0 at (x, t), then the PDE
would immediately give us a contradiction:
Although this is not a proof, since we have to handle the degenerate case in
which Δu = 0, it has the main idea. The actual proof merely perturbs u to
remove a possibly degenerate maximum.
 is closed and bounded, so the continuous function u achieves its maximum
somewhere in 
. That is, there is an (x0, t0) such that u(x0, t0) = 
.
Proof of Theorem 5.3. Let 
. Our goal is to prove that u(x, t) ≤ M
for all (x, t) ∈ UT. To deal with the possibility Δu = 0 at a maximum, we perturb
u a bit. Let v(x, t) = u(x, t) + ϵ|x|2, ϵ > 0. Then (recalling that x ∈ Rn),

since ut − kΔu = 0.
Suppose v has a local maximum in UT, at P0 = (x0, t0) with t0 < T. Then vt =
0, ∇v = 0, and Δv ≤ 0 at P0. But this contradicts (5.15), so v cannot have a
maximum in the interior of UT.
Now suppose v has a maximum on the line t = T, at P1 = (x1, t = T). Then
∇v = 0, Δv ≤ 0, and vt ≥ 0 at P1.1 Now we have vt − kΔv ≥ 0, again
contradicting (5.15).
Therefore the maximum of v on 
 occurs on 
 for all 
. We have proved
where 
. Thus,
Since ϵ > 0 is arbitrary, we have u(x, t) ≤ M for all 
.
Remarks. The weak maximum principle is easy to prove. The related strong
maximum principle is somewhat harder to prove. The strong maximum principle
states that, provided U is connected, the maximum of u is achieved only on the
parabolic boundary, unless u is constant throughout 
 [12].
By applying the maximum principle to −u, which also satisfies the conditions
of Theorem 5.3, we see that there is a corresponding minimum principle:
5.5. Duhamel’s Principle for the Inhomogeneous Heat Equation
Duhamel’s principle gives a formula for the solution of an inhomogeneous PDE
using solutions of the homogeneous equation. For the heat equation on the whole
real line, Duhamel’s principle utilizes the fundamental solution.
Let f(x, t) be a given function representing a heat source or sink, and consider
the initial value problem
The fundamental solution Φ(x, t) of the homogeneous heat equation satisfies ut =

kuxx for t > 0. But then for any y ∈ R and s > 0, the shifted function Φ(x − y, t
− s) satisfies the equation for t > s. We can multiply by an amplitude f (y, s), so
that we have a collection of solutions Φ(x − y, t − s) f(y, s) of the heat equation,
with y ∈R and t > s. Summing (i.e., integrating) these solutions over y ∈ R with
s fixed, we have that
is a solution for all t > s, satisfying 
.
Now the idea is to integrate 
 with respect to s from 0 to t. Define 
. Then
In this calculation we differentiated under the integral sign, ignoring the
singularity of Φ(x − y, t − s) at x = y, t = s on the boundary of the domain of
integration. However, this singularity can be handled with the appropriate limit,
and the result is the same. (See Evans [12], p. 50 for the details.)
Finally, we observe
In summary, the formula
solves the initial value problem (5.16). General initial conditions can be
incorporated easily, just as for the wave equation.
In this chapter, we have constructed solutions of initial value problems for the
heat equation; derived some basic principles, such as the maximum principle; and
examined certain properties, such as uniqueness of solutions. The solution of
initial value problems is described using the fundamental solution of the heat
equation, whereas for the wave equation, the solutions are given by propagating
disturbances with waves, expressed through d’Alembert’s solution. Using the two
formulas, we can make some comparisons, which are a good guide to the
differences between solutions of hyperbolic equations and those of parabolic
equations.
Solutions of hyperbolic equations preserve the regularity of the initial data,

propagating disturbances and singularities along characteristics, which have
finite speed. By contrast, solutions of the heat equation and other parabolic
equations immediately smooth initial data, and information is propagated with
infinite speed.
Nonlinear equations may have different properties. For example, the porous
medium equation ut = Δ(um) is degenerate at u = 0 for m > 1, a consequence
being that initial data with compact support spread but with finite speed; the
solution u(x, t) continues to have compact support for each time t > 0.
PROBLEMS
1. Fill in the following details used in the proof of Theorem 5.1, property 3.
(a) Use the triangle inequality to prove that if δ > 0, and x, x0, y ∈ Rn satisfy 
, then 
. (Hint: Draw a picture in the one-
dimensional case, n = 1, to visualize how the argument works.)
(b) Prove that 
.
2. Show that 
. Consequently, the heat equation for rotationally
symmetric functions u(x, t) = ϕ(r, t), r = |x|, is
3. (a) Let g : [0, ∞) → R be a bounded integrable function. Prove directly that
is an odd function of x ∈ R for each t > 0.
(b) Let h : R → R be an odd bounded integrable function. Prove that
is an odd function of x ∈R for each t > 0. That is, the symmetry in the initial
data is carried through to the same symmetry in the solution.
4. Write the solution of the Cauchy problem for the heat equation
with initial condition 
 in terms of the error function

5. Solve the heat equation (5.17) with initial condition u(x, 0) = epx, where p >
0 is a constant. You can use the identity 
.
6. Solve the heat equation (5.17) with initial condition u(x, 0) = H(x)e−x.
7. Prove the energy inequality for ut = ∇ · (k(x, u)∇u), where k = k(x, u) ∈ R is
a given positive function.
8. Consider the Cauchy problem for (5.17), with initial condition u(x, 0) = x2.
(a) Show that if u(x, t) is the solution, then v(x, t) = uxxx(x, t) satisfies the heat
equation with v(x, 0) = 0.
(b) Find u(x, t) as an explicit formula.
9. Consider the initial value problem
with constant d, and given integrable function g.
(a) Use the change of variable u(x, t) = e−dtv(x, t) to find u using the
fundamental solution.
(b) What is the effect of the constant d?
(c) Suppose d = d(t) is a given continuous function. What would be a suitable
change of variable to solve the problem?
10. Devise a change of variable corresponding to a moving frame of reference to
solve the initial value problem for the convection-diffusion equation with
constant speed c
11. Formulate and prove a statement regarding conservation of energy for the
wave equation on a bounded domain in Rn:
under homogeneous Dirichlet or Neumann boundary conditions.
1. The final inequality is easily proved by contradiction: if vt < 0 at P1, then v(x1, t) > v(x1, T) for t < T
close to t = T, contradicting the assumption that v has a maximum at P1.

CHAPTER SIX
Separation of Variables and Fourier
Series
In this chapter we find explicit solutions of the heat equation and wave equation
on bounded domains using the powerful method of separation of variables. The
method allows us to express solutions of constant-coefficient equations as infinite
series of functions. Recall that infinite series of functions were used for the
Cauchy-Kovalevskaya Theorem of Section 2.3, where solutions were expressed as
power series. In this chapter the solutions are series of trigonometric functions of
the spatial variable x with coefficients that depend on time t. Such series are
called Fourier series. We introduce them here, show how they relate to the
method of separation of variables, and study them more extensively in the next
chapter.
Fourier series solutions have several advantages over power series. Fourier
series can be used to represent functions that are not analytic, for example,
continuous functions. Moreover, solutions expressed as Fourier series are
typically designed to have each term in the series satisfy both the PDE and the
boundary conditions, provided the boundary conditions are homogeneous.
Consequently, the only issues with such an approach are: (1) how to construct
the series and (2) convergence. We deal with the construction in this chapter, and
give rigorous convergence results in the next chapter.
6.1. Fourier Series
In this section we introduce the notation and basic formulas for Fourier series.
Consider a continuous function f : R → R that is periodic with period 2L. Then f
can be represented by a Fourier series, which takes the form:
An important property of the trigonometric functions in this series is that they
are orthogonal on the interval x ∈ [−L, L]. Thus, for all j, k = 0, 1, 2, …,
and

where 
is the Kronecker delta. By multiplying (6.1) by one of the sine or cosine functions
and integrating, the coefficients an, bn are readily obtained from f, in the Euler
formulas:
with n = 0, 1, 2, …. The coefficients an are called Fourier cosine coefficients, and
the bn are Fourier sine coefficients.
For an odd function f, all the cosine coefficients are zero, and the sine
coefficients are
The resulting series of sine functions is called the Fourier sine series of f. Note that
the coefficients (6.5) are defined even if f is specified only on the interval [0, L].
Then the Fourier sine series is the Fourier series of the odd periodic extension of f,
wherein f is extended to be an odd function  that is 2L periodic and such that 
(x) = f(x), 0 < x < L.
Similarly, if f is an even function, then it has a Fourier cosine series, in which
all the sine coefficients are zero, and the cosine coefficients are given by integrals
over the interval [0, L]:
If f is an integrable function on [−L, L], then the Fourier coefficients are well
defined, and we say that f has a Fourier series. The series may converge to f only
in a weak sense that we make precise in Chapter 7, where the convergence of
Fourier series is discussed in detail.
6.2. Separation of Variables for the Heat Equation
The method of separation of variables works most simply for the heat equation in
one space dimension with homogeneous Dirichlet boundary conditions. If you
follow the method in this case, then using separation of variables in other

circumstances becomes a matter of changing some details.
Consider the initial boundary value problem with homogeneous Dirichlet
boundary conditions and initial data given by a continuous function f : [0, L] →
R,
We summarize the method in two steps. In the first step, we seek solutions in
the special separated form
a function of x times a function of t. Substituting into the PDE and the boundary
conditions, we obtain ODE for the functions v, w with corresponding boundary
conditions. This step leads to a family of solutions {un(x, t), n = 1, 2, …}.
In the second step, we write a linear combination of the un (actually, an
infinite series):
This series is a representation of the general solution of the PDE and boundary
conditions. It remains to choose the coefficients bn ∈ R, n = 1, 2, … to satisfy the
initial condition.
Claim 6.1. This process leads to the series solution of (6.7):
Proof. As indicated above, the proof is organized around constructing the
solution in two steps.
Step 1. Substitute u(x, t) = v(x)w(t) into the PDE:
Dividing by kv(x)w(t) and simplifying, we get

a constant. Note that physical constants such as k are always included with the
time portion, so that the spatial piece becomes a standard equation. Also, the
minus sign on the right-hand side of (6.9) is included so that values of λ all
turn out to be real and positive.
From (6.9) we have two ODE for v(x), w(t), with an additional unknown,
the parameter λ:
To summarize, u(x, t) = v(x)w(t) satisfies the PDE if and only if v(x), w(t)
satisfy (6.10) for some λ ∈ R.
Now we substitute into the boundary conditions for u, and note that to get
solutions u that are not identically zero, we must have corresponding boundary
conditions on v. For example, u(0, t) = 0 = v(0)w(t), so that either w(t) = 0
for all t (implying that u(x, t) ≡ 0) or v(0) = 0. Incorporating boundary
conditions at both x = 0 and x = L completes the eigenvalue problem to be
solved for v(x):
The eigenvalue/eigenfunction pairs for (6.11) are
With these eigenvalues, we turn to (6.10a) for w(t), setting λ = λn. The general
solution of this simple first-order ODE is an arbitrary multiple of
Note the important property that all the wn decay in time, because the
eigenvalues are real and positive.
Now we complete step 1 by setting u = un(x, t) = vn(x)wn(t), n = 1, 2, ….
Step 2. To satisfy the initial condition, we form the series
Substituting t = 0 into the series and using the initial condition in (6.7), we
get

Now we can extract the formula for bn from the Euler formulas (6.4).
Having completed a procedure to generate a formula, it is natural to ask
whether the formula in the claim really solves the initial boundary value
problem. In particular, we have not proved that the series converges, or indeed
whether all initial functions f can be represented by a Fourier sine series. Even if
the series for u(x, t) converges, we need to know whether the limit is sufficiently
differentiable to be a solution of the heat equation. We postpone these important
considerations for now, with the remark that the rapid decay of the coefficients
bne−kλnt of 
 for t > 0 implies that the series converges uniformly to
a C∞ function, and differentiation term-by-term can be used to verify that u(x, t)
does indeed solve the problem.
It is worth noting that the boundary conditions are satisfied for t > 0, even
though they may not be satisfied by the initial data. For the wave equation such a
discontinuity is propagated along characteristics, but for the heat equation the
discontinuity is immediately smoothed and does not propagate into the interior
of the domain.
We next illustrate the method of separation of variables with two worked
problems. In the first problem we set the constants k, L and choose the initial f to
show that the infinite series may have only a finite number of nonzero terms.
Example 1. (Homogeneous Dirichlet boundary conditions) Solve the initial
boundary value problem
Separation of variables with these homogeneous Dirichlet boundary conditions
and L = 1 gives eigenfunctions sin nπx. Therefore, since k = 7,
Now we choose the coefficients bn so the series agrees with the initial condition
when t = 0. Since the initial condition is a Fourier sine series with just two
terms, the series for u(x, t) also has only two terms. Let

Then
is the solution.
Example2. (Homogeneous Neumann boundary conditions) In this example
we change the boundary condition and consequently get different eigenfunctions.
Consider the initial boundary value problem, in which we have L = π, k = 4:
Recall that in the first step of separation of variables, we consider the PDE
and boundary conditions only; then in step 2 we form a series of separated
solutions to satisfy the initial condition. Neumann boundary conditions support a
constant solution, which differs from the Dirichlet case: a nonzero constant does
not satisfy the homogeneous Dirichlet boundary conditions.
Let u(x, t) = v(x)w(t). The ODE for v, w with boundary conditions are now
We solve the eigenvalue problem for λ = λn, v = vn. First note that λ0 = 0 gives
v″(x) = 0, so that v is linear. But v′(0) = 0 = v′(π) implies that v is flat at the
ends, so it must be flat everywhere. That is, v is constant:
For λ > 0, the general solution of the ODE is a combination of sines and cosines.
Using the boundary conditions, we arrive at
The time-dependent part of the solution is the same as for Dirichlet boundary
conditions, since the eigenvalues are the same, except that the constant solution
is included here as well:
To complete the separation of variables step, we form the solutions
Now we can form the series solution of the PDE and boundary conditions:

Finally, we satisfy the initial condition by determining the coefficients an using
orthogonality of the eigenfunctions. Setting t = 0 in the series, the initial
condition becomes
Multiplying by cos mx and integrating both sides from 0 to π, we see that all
terms become zero, except where m = n. Consequently, integrating by parts and
using cos nπ = (−1)n, we have
Thus, the solution is
in which we have written n = 2k − 1, k ≥ 1. We discuss convergence of Fourier
series comprehensively in Chapter 7, but it is easy to show that the specific series
(6.14) converges.
Claim 6.2. The series (6.14) converges uniformly for t ≥ 0.
Proof. To prove the claim, we estimate the kth term of the series:
Since 
 is a convergent series of constants, the Weierstrass M-test (see
Appendix B) implies the series (6.14) converges uniformly in x for each t ≥ 0.
To examine whether the series satisfies the PDE, we need to be able to
differentiate term by term. For t > 0, the coefficient of the kth term in the series
decays exponentially in k, since the exponential e−4(2k−1)2t is multiplied only by a
rational function (a ratio of polynomials). The same is true of the coefficients
after differentiating any number of times in x and t. Consequently, from the
standard result on uniform convergence and differentiating series of
differentiable functions term by term, the series converges uniformly to a C∞
function.

The situation at t = 0 is rather different. While the Fourier series converges
uniformly for each x ∈ R to the continuous even periodic extension of f(x) = x,
this function has jump discontinuities in the derivative. Consequently, the series
differentiated with respect to x does not converge uniformly at t = 0, and the
twice differentiated series converges only in a much weaker sense, specifically, in
the sense of distributions (see Section 9.2).
Because Neumann boundary conditions correspond to insulated ends, heat
energy is conserved, which we can see explicitly in the series solution. The heat
energy at time t is proportional to
Then we have
from the boundary conditions. Therefore,
Now, as t → ∞, the series converges to a constant: 
. From conservation of
energy, we calculate
Therefore,
This conclusion agrees with the calculation of a0 from orthogonality.
6.2.1. Robin Boundary Conditions
Robin-type boundary conditions relate the normal derivative of the function on
the boundary to the function itself. We relate the analysis of these boundary
conditions to physical considerations, such as the direction in which heat is being
transported across the boundary. Consider the following problem:

The physical interpretation of the boundary conditions depends on the signs of
the coefficients:
Since heat energy flows from warm to cool, at x = L if u > 0, a radiating
boundary condition will give ux > 0, so that heat will flow out of the boundary
(the heat flux is negative), whereas for an absorbing boundary condition, ux < 0,
and heat will flow into the domain through the boundary at x = L. Consequently,
for radiating boundary conditions, all eigenvalues are positive, because heat is
transported out of the domain. For absorbing boundary conditions, there may be
one or more negative eigenvalues.
Separation of variables leads to the eigenvalue problem
We consider the implications of radiating and absorbing boundary conditions
separately.
Radiating boundary conditions a0 > 0, aL > 0. For positive eigenvalues λ =
β2 > 0, the general solution of the ODE is
Now we use the boundary conditions to find equations for A, B, and β:

Figure 6.1. Plots of tan βL (solid curves) and β(a0 + aL)/(β2 − a0aL) (dashed
curves) with L = π, a0 = 1, aL = 2.
Dividing by cos βL and substituting in for A, we find either B = 0 or
which is an equation for β, since all the other constants are given. We reject B =
0, because it leads only to the trivial solution u ≡ 0.
From Figure 6.1, we observe that there are solutions β1 < β2 < … :
As n → ∞ the βns are approaching the lower limit to leading order:
Thus, as n → ∞ the eigenvalues 
 approach the eigenvalues 
 that
we found for either Neumann or Dirichlet boundary conditions.
The corresponding eigenfunctions are given by
Absorbing boundary conditions. In the case of absorbing boundary conditions,

a0 < 0, aL < 0, heat energy is absorbed through the boundaries. Consequently,
we might expect the temperature to rise when both boundaries are absorbing.
Correspondingly, at least one eigenvalue should be negative. Positive eigenvalues
λ = β2 satisfy (6.16) and can be represented as intersections of the graphs of tan
βL and the right-hand side of the equation, similar to Figure 6.1.
The issue is less clear when one boundary is absorbing and one is radiating, a
situation explored in the exercises.
6.2.2. Inhomogeneous Boundary Conditions
Up to now the method of separation of variables has relied on homogeneity in
boundary conditions and in the PDE. Here we consider a sample initial boundary
value problem showing how to handle inhomogeneous problems. Consider the
initial boundary value problem
The strategy is to first ignore the inhomogeneities f, u0, uπ so that we can identify
suitable eigenfunctions vn(x). Then we form a series 
 with time-
dependent coefficients wn(t) that are determined in a new way to accommodate
the inhomogeneities.
From the homogeneous version of problem (6.17), we know that the
eigenfunctions are sines, due to the boundary conditions. Thus, we consider the
Fourier sine series for u:
To satisfy the PDE, we also expand f(x, t) as a Fourier sine series:
where the coefficients are defined by
The series (6.18) apparently does not satisfy the inhomogeneous boundary
conditions, because when evaluated at x = 0 or x = π the series gives the value
zero. However, we shall see in the next chapter that Fourier series can converge
to a function that is discontinuous. Where the boundary condition is nonzero, it is

satisfied by the series (6.18) in the sense that u(x, t) approaches the specified
value (u0(t) or uπ(t)) as x approaches the boundary:
To find the coefficients wn(t), we could substitute the series (6.18) into the PDE
and equate terms. However, this procedure would omit the effect of the boundary
conditions, so instead, we use the Euler formulas for the coefficients wn(t):
Guided by the expectation of an ODE for wn(t), we differentiate and then use the
PDE:
integrating by parts twice. We can rewrite this as an inhomogeneous ODE,
where 
. Setting t = 0 in (6.19), we obtain the
initial condition
The solution of the initial boundary value problem (6.17) is given by the series
(6.18), where the coefficients wn(t) are obtained by solving the ODE initial value
problem (6.20), (6.21).
6.3. Separation of Variables for the Wave Equation
Separation of variables works for the wave equation on bounded domains in
much the same way it works for the heat equation. Differences arise because the
wave equation has two time derivatives, rather than the one time derivative that
appears in the heat equation. Previously we emphasized the traveling wave
structure of solutions of the wave equation through d’Alembert’s solution,
whereas separation of variables highlights the role of vibrations, which are time-
periodic solutions. The typical physical context for these solutions is a vibrating
string of finite length, where the eigenfunctions for the wave equation are modes
of vibration for different frequencies.

Consider the initial value problem for the wave equation
We solve this problem by following the separation-of-variables procedure used
for the corresponding initial boundary value problem (6.7) for the heat equation.
Because there are two t derivatives in the PDE, we need two initial conditions
instead of one. Also, the constant k > 0 in the heat equation is replaced by the
constant c2 > 0 in the wave equation.
Claim 6.3. The solution of (6.22) is the series
with
Proof. We substitute u(x, t) = v(x)w(t) into the PDE and boundary conditions,
and divide by c2w(t). This leads to
which is a constant. Now we have two ODE for v(x), w(t), with an additional
unknown, the parameter λ:
Just as for the heat equation, the PDE boundary conditions become boundary
conditions for v(x):
We already know the eigenvalues λ and eigenfunctions v for this problem:
The difference between the heat equation and the wave equation solutions is in
the time-dependent part w(t). The ODE (6.25a), with λ = λn, has the general
solution

Now we let un(x, t) = vn(x)wn(t), n = 1, 2, …, which is a family of solutions of
the PDE and boundary conditions. Next we form the infinite series
and it remains to show how the coefficients an, bn are determined from the initial
conditions.
The initial condition u(x, 0) = ϕ(x) is
leading to the expression for an in the claim.
Similarly, the initial condition ut(x, 0) = ψ(x) leads to the formula for bn,
since (differentiating the series with respect to t),
Thus,
from which the formula in the claim follows.
The form of these solutions of the wave equation leads us to note several
differences between solutions of the wave equation and those of the heat
equation:
1. The individual terms in the series for the wave equation oscillate in time.
Indeed, this is the behavior of the solution as a whole; it is periodic in time,
with period 2πL/c.
2. Using trigonometric identities, you can write (6.23) in the form u(x, t) = F(x
+ ct) + G(x − ct). After all, we know that every solution of the wave
equation in one dimension is of this form. It is a good exercise to find F and G
in terms of ϕ and ψ, and their periodic extensions.
6.4. Separation of Variables for a Nonlinear Heat Equation

Separation of variables is very useful for linear equations. However, for nonlinear
equations, it is not nearly as useful, partly because the technique depends on
linear combinations of solutions also being solutions, which is not the case for
nonlinear equations. Although there generally will not be solutions in which the
independent variables are separated, the porous medium equation has a structure
that admits separated solutions (See Evans [12], p. 170.). This equation is a
simplified model of fluid flow in a porous medium, such as compacted sand or
soil.
Let m > 1 be a given constant, and consider the porous medium equation for
the unknown function u = u(x, t) > 0, x ∈ Rn, t > 0:
Let u(x, t) = v(x)w(t). Then
and therefore,
Note that the function F(u) = um is homogeneous of degree m, meaning F(αu) =
αmF (u), for α > 0. The homogeneity allows us to separate variables. We thus
obtain the pair of equations
To solve (6.27), we integrate once to get 
. Hence,
in which λ, μ are constants; we need λ > 0 to ensure that w(0) is defined.
Next, we find spherically symmetric solutions of the v equation (6.28).
Consider v(x) = rα, r = |x|, with α a constant to be determined. Then
(where ϕ(r) = rmα). Equating powers of r yields

Equating coefficients results in
Finally, combining v and w gives
where α, μ are given by (6.29), (6.30), but λ > 0 is a free parameter. The
solution u(x, t) is well defined as t → 0+. However, as t increases, the negative
fractional power means that u blows up as t increases to 
, since m > 1.
6.5. The Beam Equation
The beam equation
is a fourth-order PDE for the deflection of a homogeneous beam under an applied
external load distribution q(x, t). The beam equation is written here in
nondimensional form, in which the dimensionless parameter G = EI/ρ > 0 is
related to the elastic modulus E, the bending modulus I, and the density (mass
per unit length) ρ. This equation models small deflections from a straight beam
and is based on assuming that cross sections bend around the center line and
exert forces on one another due to bending, leading to compression on one side
of the center line and stretching on the opposite side.
The dispersion relation is obtained by seeking solutions related to the Fourier
transform of (6.31). Let q ≡ 0 and u = exp(iωt + iξx). Then
gives the temporal frequency ω as a function of the spatial frequency ξ.
However, it is somewhat more informative to consider the full separated
equations, as we did in Sections 6.2 and 6.3 for second-order equations.
Let u(x, t) = v(x)w(t). Substituting into the beam equation (6.31) and dividing
by vw, we get two ODE:
where ω2 > 0 is constant, and 
. Thus w(t) = A cos ωt + B sin ωt, and
the general solution for v(x) is

with arbitrary constants a, b, c, d.
Since the equation is fourth order in space, we need four boundary conditions,
and two initial conditions are also needed, since the equation is second order in
time. For example, for a beam fixed at both ends x = 0, x = L, the boundary
conditions are
These clamped-beam boundary conditions translate to the four boundary conditions
on v(x) that require v and v′ to be zero at each end. Then by processing the
conditions on v given by (6.32), we find the spatial wave numbers μ = μn, n =
1, 2, …, are determined from the equation
It is easy to see that μn ∈ ((n − 1)π/L, nπ/L), and (since sech ξ → 0 as ξ → ∞),
μn ∼ nπ as n → ∞.
A cantilevered beam is taken to be clamped at one end x = 0, and free at the
other. The boundary conditions are
In this case, we also find (see problem 8) μn ∈ ((n − 1)π/L, nπ/L).
In this chapter we have seen how to implement the method of separation of
variables in several example problems. In the next chapter we place the method
in a rigorous theoretical framework. In particular, we explore eigenvalue
problems and prove results on the convergence of infinite series of
eigenfunctions, including Fourier series.
PROBLEMS
1. Solve the initial boundary value problem
2. Solve the initial boundary value problem

3. Consider the eigenvalue problem (6.15) with Robin boundary conditions.
Suppose λ = 0 is an eigenvalue.
(a) Find the eigenfunction.
(b) Find a necessary condition on the coefficients a0, aL.
(c) Prove that this condition is also sufficient to guarantee that λ = 0 is an
eigenvalue.
4. Consider the eigenvalue problem (6.15) with Robin boundary conditions.
(a) Prove that there is at least one negative eigenvalue in the absorbing case a0
< 0, aL < 0.
(b) Find a condition on a0, aL that is necessary and sufficient to have two
negative eigenvalues.
5. Consider the eigenvalue problem (6.15) with Robin boundary conditions. In
the radiating case a0 > 0, aL > 0, prove the properties shown graphically in
Figure 6.1. (Hint: As in the text, it is easier to let λ = β2. You can also use tan(nπ
+ θ) = tan θ.)
(a) There are an infinite number of eigenvalues λn, n ≥ 1.
(b) 
.
(c) 
.
(d) If βn = (n − 1)π/L + θn, with θn → 0 as n → ∞, find the leading-order
behavior of θn : θn = A/n + O(1/n2).
6. This problem is a graphical summary of problems 3–5. Sketch the hyperbola
a0aLL + a0 + aL = 0 in the a0, aL plane, showing the asymptotes. In your sketch,
label the (three) regions corresponding to values of (a0, aL) for which there are
two, one, or zero negative eigenvalues. Label the point corresponding to
Neumann boundary conditions. Where in the plane are Dirichlet boundary
conditions represented?
7. For constants 0 < m < L, let
where pj, rj, j = 1, 2, are positive constants. Assuming that eigenfunctions are
continuously differentiable, find an equation for the eigenvalues λ for the
eigenvalue problem

You may assume that the eigenvalues are real and positive.
8. The beam equation is associated with eigenvalue problems for the fourth-order
ODE
Using the general solution (6.32), find an equation for the positive eigenvalues λ
= μ4 for the cantilevered beam, with boundary conditions
From this equation, or using a graph, show that there are an infinite number of
positive eigenvalues λn, corresponding to
State the leading-order dependence of λn on n as n → ∞.
9. (a) Calculate the Fourier series of the function f(x) that is odd and 2π periodic,
with f(x) = 1, 0 < x < π.
(b) Assuming the series converges to f(x) = 1 at x = π/4, calculate the sum of
the series

CHAPTER SEVEN
Eigenfunctions and Convergence of
Fourier Series
In the previous chapter we used the technique of separation of variables to solve
a variety of initial boundary value problems for the heat and wave equations. We
showed how the technique leads naturally to eigenvalue problems for the spatial
dependence of the solutions. The resulting eigenfunctions (sines and cosines in
the previous chapter) are then combined into infinite series with time-dependent
coefficients to represent the solution of the PDE and its initial conditions.
In this chapter we show that the technique can also be used for nonconstant-
coefficient equations, such as
in which the coefficients p, q are specified functions. However, the eigenvalues
and eigenfunctions are generally not available explicitly for such equations.
Nonetheless, with the aid of the Sturm-Liouville Theory of eigenvalue problems
in Section 7.1, we place on a firm footing the method of separation of variables
to construct infinite series of functions representing solutions of the PDE and its
boundary conditions. There remains the issue of convergence of the infinite
series, even for the explicit series of the previous chapter. For example, so far we
have examined convergence of such a series in just one example. In Section 7.2,
we introduce notions of pointwise, uniform, and L2 convergence, proving typical
convergence results in subsequent sections. Finally in Section 7.6, we introduce
the Fourier transform, and show how it is related to Fourier series before
demonstrating the use of the transform in PDE, and discussing its utility in a
range of areas.
7.1. Eigenfunctions for ODE
Consider the following ODE eigenvalue problem, known as a Sturm-Liouville
problem. Let p, q, r be given real C2 functions on the bounded interval [a, b]. We
assume p(x) > 0, r(x) > 0, a ≤ x ≤ b. The eigenvalue problem consists of the
ODE
in which λ ∈ C is a parameter, together with linear and homogeneous boundary
conditions. The treatment of eigenvalue problems in the previous chapter

corresponds to the special case p ≡ 1, q ≡ 0, r ≡ 1. In this section we derive
properties of the eigenvalues λ and eigenfunctions v that help make the method
of separation of variables work for linear PDE with variable coefficients. In
particular, we show that the orthogonality of the sine and cosine eigenfunctions
in the previous chapter was not an accident and can be established easily in the
more general context of (7.1).
We define a differential operator L by 
. Then for any
v ∈ C2[a, b], Lv is a new function (in C[a, b]). To write (7.1) as an eigenvalue
problem
we restrict the domain of L to functions that satisfy the boundary conditions.
We consider complex-valued functions, so as to include the possibility of
complex eigenvalues λ. In this context, we need the weighted L2 space of square-
integrable complex-valued functions, with weight r(x) and inner product defined
by
The corresponding norm (see Appendix B) is
For simplicity, we assume r ≡ 1and leave to problem 2 the consideration of more
general functions r(x) > 0.
Notice that L has a special form, in which the first- and second-order
derivatives are combined into a single term that can be integrated. This allows us
to use integration by parts as follows. For u, v ∈ C2[a, b],
If the boundary conditions are such that the boundary term vanishes:
then we have the identity

In this case, we say L is symmetric. Notice that this depends on two properties,
namely, the special form of the second-order differential operator and condition
(7.4) on the boundary conditions.
Theorem 7.1. Let L be symmetric. Then
1. all eigenvalues of L are real, and
2. eigenfunctions of different eigenvalues are orthogonal.
Proof. The proof is the same as the proof of the corresponding properties of a
symmetric matrix. It depends on the calculation (7.3) formalized in (7.5) and
elementary properties of the complex inner product.
1. Suppose Lu = λu, u ≠ 0, and suppose u satisfies the boundary conditions.
Then
But ‖u‖ ≠ 0, since u ≠ 0, so we must have 
. Therefore. λ is real.
2. Let u, v be eigenvectors for different eigenvalues λ ≠ μ: Lu = λu, Lv = μv.
Then
Therefore, (λ − μ)(u, v) = 0, so that (u, v) = 0.
Remarks. For the eigenvalue problem to make sense, the boundary conditions
have to be linear and homogeneous. Clearly, both homogeneous Dirichlet or
Neumann boundary conditions are symmetric. We leave as an exercise the
property that Robin boundary conditions of the form
are also symmetric.
As we noted earlier, an important property of eigenvalues for the heat
equation is that they are positive, except possibly for a finite number of them. For
symmetric Sturm-Liouville problems we can find a sufficient condition for all the
eigenvalues to be nonnegative.

Suppose Lu = λu. Then
if
For Dirichlet or Neumann boundary conditions, the first of these conditions is
satisfied, but it need not be for Robin boundary conditions (7.6) without
restrictions on the coefficients a0, a1. However, if both coefficients are positive,
then the boundary term in (7.7) is nonnegative, so the eigenvalues are
nonnegative (providing the second condition q(x) ≥ 0 is satisfied.)
In Chapter 6, Claim 6.2, we proved uniform convergence of the Fourier series
solving an initial boundary value problem for the heat equation. In the next three
sections we prove more general convergence results for series: pointwise
convergence, uniform convergence, and mean-square convergence.
7.2. Convergence and Completeness
Before proving results on the convergence of Fourier series and other series of
functions, we introduce some general concepts of convergence and completeness,
and discuss their significance for PDE. The convergence of a sequence 
 in Rn
to a limit v means that ||vk − v|| → 0 as k → ∞. Here, ||x|| denotes the usual
Euclidean norm of the distance of x ∈ Rn to the origin. The idea of convergence
of a sequence of numbers carries over naturally to functions. If 
 is a
sequence of functions defined on a set S, then the sequence converges pointwise on
S to a function f if the sequence of numbers 
 converges to f(x) for each x ∈
S. In this definition, the functions can be vector valued, so that fk : S → Rn, and
they can be defined on a multidimensional set S ⊂ Rm.
We also need to introduce convergence in norm in the sense that ||fk − f || → 0
as k → ∞, where ||f || denotes an appropriate norm on a space of functions. For
example, if C[a, b] denotes the space of continuous functions f : [a, b] → R on a
bounded interval [a, b] ⊂ R, then one norm on C[a, b] is the sup norm ||f ||max =
maxa≤x≤b |f(x)|. This is also called the uniform norm, because ||fk − f ||max → 0
means fk → f uniformly, in which case f ∈ C[a, b], a fundamental result in
analysis. More generally, a sequence 
 in a normed space X is a Cauchy
sequence if ||fj − fk|| → 0 as j > k → ∞ (uniformly in j), so that for every ϵ > 0,
there is N > 0 such that if ||fj − fk|| < ϵ for all j, k > N. If X has the property

that Cauchy sequences converge to a limit in X, then X is complete. Complete
normed vector spaces (such as Rn, and C[a, b]with the sup norm) are called
Banach spaces. In a Banach space, a sequence converges if and only if it is a
Cauchy sequence.
Another norm on C[a, b] is the L2 norm 
. This norm is more
closely analogous to the Euclidean norm, if you think of f(x), a ≤ x ≤ b as a
vector in an infinite-dimensional space, and the norm as the square root of a sum
of values f(x)2, thus measuring the distance of f from the origin f ≡ 0.
Incidentally, the analog of the sup norm in Rn is the norm ||x||max = max{|xj|, j
= 1, 2, …, n}. Interestingly, all norms in Rn are equivalent, so that convergence
in the Euclidean norm and in || · ||max are equivalent. In contrast, the sup norm
and the L2 norm on C[a, b]are not equivalent. The following example shows how
convergence in these two norms is different and moreover that the space C[a, b]
with the L2 norm is not complete.
Example1. (Pointwise and L2 convergence but not uniform) Let fk : [0, 1] →
R be the sequence of continuous functions defined by
Then, as k → ∞,
pointwise, and ||fk − f ||2 → 0. However, fk does not converge to f in the uniform
(sup) norm, since sup0≤x≤1 |fk(x) − f(x)| = 1 for all k.
Example 2. (Pointwise but not uniform convergence) The sequence of
continuous functions gn(x) = xn on [0, 1] converges pointwise to the
discontinuous function
Consequently, gn cannot converge uniformly on [0, 1]. In fact, 
 S(x)|
= 1 does not go to zero as n → ∞. For the same reason, the convergence also
fails to be uniform on the open interval 0 < x < 1, even though the limit is
continuous there. However, the convergence is uniform on the interval [0, a], for
any a ∈ (0, 1), since |xn| ≤ an → 0 as n → ∞.
When dealing with PDE, we often work in spaces of functions that are not

necessarily continuous, such as L2 spaces. Since spaces of continuous or
continuously differentiable functions are surely better suited to finding solutions
of differential equations, it is natural to ask “Why bother?” Moreover, for elliptic
and parabolic equations, the solutions tend to be very smooth, having even more
derivatives than the equation would seem to require.
The short answer to this question is that existence of solutions is much easier
to prove in the weaker L2 spaces. A longer explanation is that, at least for the
beginning theory, existence results and proofs rely on having a space that is
complete in a norm with a corresponding inner product. Such a space is called a
Hilbert space. Spaces of continuous functions on bounded closed sets are complete
in the sup norm, but this norm does not have a corresponding inner product. In
contrast, L2(U) is a Hilbert space with inner product (7.2). Having established the
existence and uniqueness of weak solutions in such a space, then regularity
results for the weak solution are established separately.
The precise definition of L2(U) as a Hilbert space requires some measure
theory. In particular, the Riemann integral is not sufficiently general, and a more
sophisticated theory of integration, based on Lebesgue measure, is most suitable.
In this theory, the Riemann integral is generalized to enable some irregular
functions to be integrable in the Lebesgue sense. Functions that are Lebesgue
integrable are called measurable. More precisely, all measurable functions are
Lebesgue integrable (see Appendix B).
For the L2(a, b) norm to make sense, we need to clarify how the property ||f ||
= 0 ⇒ f = 0 is to be understood. This relies on the notation of sets of measure
zero. A set S ⊂ R has measure zero if for every ϵ > 0, S can be covered by a
countable family {Ij : j = 1, 2, …} of open intervals with lengths (i.e., measures)
mj, j = 1, 2, …, such that 
 and 
. For a measurable function f :
[a, b] → C, we say f(x) = 0 almost everywhere if there is a set S ⊂ [a, b] of
measure zero such that f(x) = 0 for all x ∈ [a, b] with x ∉S.
A key property of the Lebesgue integral applies to L2 functions: If f ∈ L2(a, b),
then ||f || = 0 implies that f(x) = 0 almost everywhere in [a, b]. The space of L2
functions is then defined by identifying functions that are equal almost
everywhere (meaning that their difference is zero almost everywhere). This can
be formalized by making this identification into an equivalence relation and
defining L2 to be the set of equivalence classes. Then the norm is defined on an
equivalence class by defining it on any function in the equivalence class. Similar
considerations apply to the Lp spaces and Sobolev spaces introduced in Chapter
10.

The properties of L2(a, b) that we need can be summarized in the following
theorem, which we state without proof. The theorem is proved in Rudin’s text
[38], where a more extensive treatment of Lebesgue integration can also be
found.
Theorem 7.2. The following properties hold for the L2(a, b) norm:
1. L2(a, b) is complete in the L2 norm, and
2. every L2(a, b) function f can be approximated in norm by continuous functions.
That is, there is a sequence {fk} of continuous functions on [a, b] such that ||f −
fk|| → 0 as k → ∞.
A series 
 of functions converges pointwise in an interval I if the partial
sums 
 converge as a sequence of numbers,
for each x ∈ I. The convergence is uniform on I if
that is, ||SN − S||max → 0 as N → ∞. Note that if SN → S uniformly, then SN → S
pointwise.
The series converges in L2 if ||SN − S||2 → 0 as N → ∞. This is also called
mean-square convergence.
7.3. Pointwise Convergence of Fourier Series
A classic result in Fourier analysis is the proof of pointwise convergence for the
full Fourier series (Chapter 6) of a 2π-periodic function f. To start, let’s assume f
is C1 on R. Then we want to show
in which the coefficients are given by the Euler formulas (Chapter 6):
To prove pointwise convergence of the series, we fix x ∈ [−π, π], and
consider the partial sums:

We insert the Euler formulas for the coefficients and manipulate the finite sum:
To make progress with the integral, consider the sum
It is convenient to use complex exponentials (recall that e±iξ = cos ξ ± i sin ξ):
This is a partial sum of a geometric series with ratio eiθ, so that
The Dirichlet kernel is defined to be 
. Considering the form (7.8)
for PN(θ), we deduce the following properties:
1. DN(θ) is even and 2π periodic,
2. 
, and
3. 
.
From these properties, as N → ∞, DN(0) → ∞ while the area 
 remains
constant. We found similar behavior in Section 5.1 for the heat kernel Φ(x, t) as t
→ 0+. Consequently, as N → ∞, we might expect that DN(θ) approaches the Dirac
delta function. If so, then for a continuous function f,

However, in contrast to the heat kernel, DN(θ) does not approach zero for θ ≠ 0,
as N → ∞. Instead, to take the limit, we appeal to the Riemann-Lebesgue Lemma.
Lemma 7.3. (The Riemann-Lebesgue Lemma) If f ∈ L2(a, b), then
When f is C1 on [a, b], the result follows easily by integration by parts, since
both |f| and |f′| are bounded, and integrating sin βx introduces a factor of 1/β.
The proof is completed by approximating f by smooth functions, as in Theorem
7.2. Details can be found in many analysis texts [38].
Proof of convergence of SN(x) to f(x). Returning to the partial sums SN(x), we
use property (2) of DN(θ) to write
where
from L’Hôpital’s rule at θ = 0. Since g is continuous, we can apply the Riemann-
Lebesgue Lemma 7.3, which establishes that |SN(x) − f(x)| → 0 as N → ∞.
With a bit more care, we can prove pointwise convergence for piecewise
smooth periodic functions. These are periodic functions f that, in every bounded
interval, are continuously differentiable apart from a finite number of points
where either the function or its derivative has a jump discontinuity (meaning the
left and right limits exist but are not equal).
Theorem 7.4. Let f be piecewise smooth and 2π periodic. Then the Fourier series for f
converges at each x to the average of the left and right limits of f:

where
Note that at points x of continuity, the theorem guarantees that the series
converges to f(x).
Proof. Using the notation defined in the theorem, we estimate the error made by
the Nth partial sum. Let x be a specific point, −π < x ≤ π. Then
In this calculation, we have split the integral, and used the fact that PN(θ) is even
in θ, followed by the triangle inequality. Note that the integrand in each integral
is continuous at θ = 0.
To show that each integral approaches zero, we use the Riemann-Lebesgue
Lemma 7.3. Mimicking the development of (7.9) and (7.10), we have to check
that the corresponding g(θ) here satisfies the conditions of the lemma. Consider
the first integral. Paralleling the f ∈ C1 case, we let
Then g is piecewise continuous, and hence integrable, on −π ≤ θ ≤ 0. A similar
construction applies to the second integral, I+.
7.4. Uniform Convergence of Fourier Series
Theorem 7.4 shows that a Fourier series can converge pointwise even if the
function it represents is not smooth everywhere. This can be important for the
study of PDE. For example, solutions of the wave equation describing the
vibrations of a violin string that has been plucked at one point are continuous but

not 
differentiable. 
Discontinuities 
in 
the 
derivative 
propagate 
along
characteristics and are reflected at the ends of the string. Such a solution can also
be represented by a Fourier series that converges pointwise.
In this section we prove in Theorem 7.5 that the pointwise convergence of the
previous section is in fact uniform if f is sufficiently smooth. This result uses
Bessel’s inequality, which will also arise in the context of L2 convergence. Let 
 be a set of orthonormal functions, meaning that (vj, vk) = δjk, j, k
= 1, 2, ….
For 
 a sequence of real numbers, the mean-square error EN
obtained from approximating f by the linear combination 
 is defined to
be
Then
Completing the square, we find
Consequently, EN is minimized by choosing the coefficients cn to be the Fourier
coefficients, defined in this general context by
With this choice, and since EN ≥ 0, we also conclude:
which is known as Bessel’s inequality. In terms of the coefficients of (7.11),

Thus, the infinite series 
 is convergent and the limit is no larger than ||f ||2.
In particular, we have the important result that
a result closely related to the Riemann-Lebesgue Lemma 7.3.
We are ready to prove the following theorem on uniform convergence of
Fourier series for smooth functions f. The essence of the proof is to apply Bessel’s
inequality to get a bound on the Fourier coefficients for the derivative f′. This is
enough to give a strong uniform estimate on the Fourier series for f.
Theorem 7.5. Let f : R → R be a C12π-periodic function. Then the Fourier series of f
converges uniformly.
Proof. Since f′ is continuous, it is integrable on [−π, π]. Therefore, f′ has a
Fourier series. Moreover, since f is periodic, the constant term in the series for f′
is zero. Thus,
and we have Bessel’s inequality (7.12):
Therefore, 
.
Not surprisingly, the Fourier coefficients an, bn of f are related to the
coefficients An, Bn of f′ by integration:
so that
and similarly,

To show uniform convergence, we verify the Cauchy criterion, for which we
estimate how close the partial sums become:
Using the Schwarz inequality, we can make this sum small as p, q → ∞:
Remark. The integration by parts in (7.14) to relate Fourier coefficients of the
function to those of derivatives generalizes to higher derivatives when they are
available. Since each integration pulls down another factor , we see that the
regularity of the function is closely linked to the rate of convergence of the
Fourier coefficients.
7.5. Convergence in L2
For PDE, it is often natural to work in L2 spaces, using both the inner product and
completeness.
Lemma 7.6. Let f ∈ L2(a, b), and suppose 
 is an orthonormal set in L2(a, b).
Then the series 
 is convergent in L2 and
Proof. From Bessel’s inequality, we see that the series of coefficients
is convergent. Thus, the series satisfies the Cauchy condition for convergence, so
that using orthogonality, we have
That is, 
 is a Cauchy sequence in L2(a, b). Completeness of the

space means that the series converges to an L2(a, b) function. Moreover, applying
orthogonality and Bessel’s inequality again, we have
Now that we know the infinite series 
 is convergent, we would like
to know whether it converges to f in L2(a, b). A necessary condition is that if f is
orthogonal to all the vn, then f = 0. That is, (f, vn) = 0 for all n implies f = 0. In
fact, we can now provide a simple proof that not only is this condition necessary
and sufficient, but it is equivalent to having equality in Bessel’s inequality.
Theorem7.7. If 
 is an orthonormal set in L2(a, b), then the following three
conditions are equivalent:
1. If (f, vn) = 0 for all n, then f = 0.
2. For each f ∈ L2(a, b), 
.
3. Parseval’s identity holds for every f ∈ L2(a, b):
Proof. We write the proof in three parts, showing condition 1 ⇒ 2 ⇒ 3 ⇒ 1.
Condition 1 ⇒ 2: Let f ∈ L2(a, b). Then Lemma 7.6 shows that
converges. To see that it converges to f, consider the function
Then g is orthogonal to every vk:
Thus, by condition 1, we have g = 0.
Condition 2 ⇒ 3: Suppose 
. Then orthogonality of {vn} implies

Condition 3 ⇒ 1: Suppose Parseval’s identity holds for f ∈ L2(a, b). If (f, vN) =
0 for all n, then ||f || = 0, which implies f = 0.
A given orthonormal set of functions 
 is complete if any of the three
conditions of the theorem hold. In this case, condition 2 means that the set is an
orthonormal basis for L2(a, b).
Example 3. (An orthonormal basis) The set 
 is an
orthonormal basis for L2(−π, π). This can be proved by approximating f ∈
L2(−π, π) by a continuous function h on [−π, π]. Then h is approximated
uniformly by the partial sums of its Fourier series, hence also in L2. Using
orthogonality and the triangle inequality, we can show that f is approximated by
the partial sums of its Fourier series.
7.5.1. More General Result for Sturm-Liouville Problems
It is convenient to state (without proof) two further properties of the eigenvalue
problem, in addition to those of Theorem 7.1. Recall that L is the differential
operator associated with the Sturm-Liouville problem of Section 7.1.
Theorem 7.8. Let L be symmetric. Then
1. The eigenvalues form a countable set that is bounded below but not above:
Each eigenvalue has finite multiplicity, λ1 is a simple eigenvalue, and λn → ∞ as n
→ ∞.
2. The eigenfunctions vn can be chosen to form a complete orthonormal set on L2(a, b).
For a proof of this result, see Strauss [45]. Note that part 2 of the theorem
tells us that Fourier series of L2(a, b) functions converge in L2.
Parseval’s identity establishes that the correspondence between the L2
function f and its sequence of Fourier coefficients {cn}, is an isometry, in the sense
that the mapping f → {cn} preserves the norm.
7.5.2. Gibbs Phenomenon
The Fourier series of a discontinuous function does not converge uniformly, even
though it converges pointwise. The Gibbs phenomenon1 is the observation that at
each discontinuity, the Fourier series overshoots the function by roughly 9%. The
overshoot appears as a persistently excessive maximum in the oscillating partial
sums of the Fourier series.

Without loss of generality, we can look at this phenomenon by considering a
simple discontinuous function, namely, a step function. Although this seems like
a special example, it in fact embodies the essence of how the Gibbs phenomenon
is manifested. This is because a piecewise continuous function can be written as
the sum of a smooth function and a weighted sum of a finite number of Heaviside
functions.
Let f be the 2π-periodic extension of the function
Since f is odd, it has a Fourier sine series, which we easily calculate, and by the
pointwise convergence theorem, we have
where bn = (2/nπ)(1 − (−1)n), n = 1, 2, …. Thus,
We get a good idea of the Gibbs phenomenon by graphing the partial sums SN(x)
for moderately large N. (See Figure 7.1.) Note that the graph y = SN(x) (N odd)
crosses the line y = 1 exactly N + 1 times in the interval (0, π), and the
maximum appears to be at approximately 
 (and at π − xN.) In fact

Figure 7.1. The Gibbs phenomenon. The partial sum S21(x) of Fourier series
(7.16) is shown.
(see Strauss [45]). Thus, SN(x) overshoots f(x) = 1 by 0.18, or approximately 9%
of the jump in f(x) across x = 0. Incidentally, this example shows the danger in
reversing the order of two limits:
7.6. Fourier Transform
Fourier transforms and other transforms, such as the Laplace transform, are
useful tools to transform equations or data into forms that are easier to analyze
or solve. In this section we explore some of the properties of the Fourier
transform, drawing comparisons to Fourier series and showing how the Fourier
transform is used to solve certain PDE.
The Fourier series of a periodic function, say of period 2π, decomposes the
function into pieces each with a different integer frequency k, with the Fourier
coefficients being the amplitude at that frequency. This point of view is clearest
in the complex form
with the (complex) amplitudes ck given by

Of course, for a real-valued function f, 
. We can regard ck as a function
from the set Z of integers to C, the complex numbers. The periodic function f has
been transformed into the sequence 
. Then the series (7.17) is the inverse
transform; it gets us from the sequence 
 back to f. We have seen how this
transform to Fourier coefficients and back again is useful when solving PDE on
bounded domains. The Fourier transform is a similar technique for functions on
unbounded domains.
The Fourier transform of a function f : R → R that is absolutely integrable,
is a function 
 that looks a bit like (7.18):
Thus, 
 is the complex amplitude contribution to the function f at frequency ω.
We reconstruct the function f from these amplitudes with the inverse transform:
Notice that the infinite sum in (7.17) has been replaced by an integral. In the
Fourier transform, the frequency is a continuous variable ω, and there is a sign
change in the exponential between the transform and the inverse transform.
We have also placed the factor 1/(2π) in the inverse transform rather than in
the transform. A more symmetric placement commonly used is to have a factor of
 in both the transform and its inverse. It is also consistent to have the
1/(2π) in only the transform. Of course, just as for the convergence of Fourier
series to the periodic function f(x), the identity (7.20) requires proof. You can
find the proof in standard texts on Fourier analysis (for example, see Stade [44]);
our purpose in this section is to illustrate how the transform is used. Along the
way, we find some interesting properties.
One more bit of intuition about the Fourier transform. Consider an
experiment that is performed over a long time, and the result is a measurement
f(t), t0 < t < t1 (known as a time series) that is very noisy. The measurement will
be at discrete values of time t, but for the purpose of discussion (and you can
imagine how to make sense of this), we regard f as a function of t, and extend f to

the whole real line. Now take the Fourier transform to get a new function 
,
and plot the graph of . Recall that the transform records the amplitude of f at
each frequency, so if the graph of  shows clear maxima at a finite number of
frequencies ωn, n = 1, …, N, then the function is dominated a combination of
periodic functions at those frequencies, with the amplitudes given by 
. This is a central idea in signal processing, where some sense can
be made out of a noisy signal—the time series—by investigating the Fourier
transform.
Another transform you may be familiar with is the Laplace transform, which
is most useful in the context of initial value problems for ODE and PDE, because
it is defined for integrable functions on the half-line [0, ∞). Other transforms are
useful for different contexts. For example, the Radon transform of functions in
two or three dimensions is used in tomography to express the function (typically
a density) in terms of scattering data that might come from a scan, such as an x-
ray, ultrasound, or magnetic resonance imaging.
Example 4. (Computing a transform) To show how the transform is computed
in a simple example, let f : R → R be defined by
Then
To apply the transform to differential equations, we transform derivatives
using integration by parts and observing that f(±∞) = 0 (since |f | is integrable
on R), to eliminate the boundary terms:
Example 5. (The Cauchy problem for the heat equation) Consider the Cauchy
problem for the heat equation
The solution u = u(x, t) was obtained from the fundamental solution in Section
5.2. Here we show how the same formula can be obtained using Fourier
transform in x for each t.
The solution of (7.21) by Fourier transform depends on some further

properties of the transform. If f, g are in L2(R), then
The compactness of this formula is the reason for putting the 1/(2π) normalizing
factor in the inverse rather than in the transform. The proof of (7.22) involves a
simple change of variables z = x − y:
We also have
We transform everything in (7.21) with respect to x, using 
 to
transform ut:
Thus, 
. Let a = 1/(4kt) in (7.23). Then (7.22) and (7.23) imply
that
This is exactly the formula that we found in Section 5.2.
PROBLEMS
1. Let Lu(x) = a(x)u″(x) + b(x)u′(x) + c(x)u(x), where a, b, c are given C2
functions on the interval 0 ≤ x ≤ 1. Consider C2 functions u, v on [0, 1]
satisfying Dirichlet conditions u = v = 0 at x = 0, 1. Find an ordinary
differential operator L∗ such that
2. Let r : [a, b] → (0, ∞) be C2, and consider the eigenvalue problem (7.1). Using
the weighted L2 space with inner product (7.2), prove that eigenvalues are real
and the corresponding eigenfunctions are orthogonal.
3. Let the piecewise continuous function f : R → R be defined by

Write f(x) as a sum of a continuous function and a linear combination of
Heaviside functions.
4. Fill in the details of Example 3 in Section 7.5: prove L2 convergence of the
Fourier series of an L2 function, using approximation and uniform convergence.
5. Let f, g : R → R be differentiable and have the property that they and their
first derivatives are absolutely integrable over R. Prove that
6. Consider the Fourier transform of a band-limited function f(x), in which 
.
(a) Show that 
 for |ω| ≤ π.
(b) Hence deduce that f(x) only depends on the sequence {f(n)} of sampled
values of f :
7. The pointwise convergence of Fourier series can be used to deduce the sums of
certain kinds of infinite sequences.
(a) Start with calculating the Fourier series of the even 2π-periodic function
f(x) = | sin x|. Use the Fourier series to calculate the sums
(b) Choose a suitable even 2-periodic function f(x) whose Fourier series allows
you to determine the sums
1. Josiah Willard Gibbs (1839–1903) made many contributions to thermodynamics, statistical mechanics,
and other areas of science and mathematics, including vector analysis. The Gibbs phenomenon was
discovered and explained by the English mathematician Henry Wilbraham in 1848; nearly 50 years later it
was found independently by Gibbs.

CHAPTER EIGHT
Laplace’s Equation and Poisson’s
Equation
In this chapter we consider Laplace’s equation and its inhomogeneous
counterpart, Poisson’s equation, which are prototypical elliptic equations.1 They
may be thought of as time-independent versions of the heat equation, with and
without source terms:
We consider these equations in a domain U ⊂ Rn, n ≥ 2, but also on all of Rn.
Applications of these equations include the classical field of potential theory, of
importance in electrostatics and steady incompressible fluid flow. In
electrostatics, f(x) in Poisson’s equation represents a charge density distribution,
inducing the electric potential u(x). In two-dimensional steady fluid flow, u is the
velocity potential or stream function, both of which satisfy Laplace’s equation.
Several properties of solutions of Laplace’s equation parallel those of the heat
equation: maximum principles, solutions obtained from separation of variables,
and the fundamental solution to solve Poisson’s equation in Rn.
8.1. The Fundamental Solution
To solve Poisson’s equation, we begin by deriving the fundamental solution Φ(x)
for the Laplacian. This fundamental solution is rather different from the one for
the heat equation, which is designed to solve initial value problems, and
consequently has a singularity at the initial time t = 0. The fundamental solution
for the Laplacian, being time independent, is used to represent solutions in space
alone. To do this, Φ(x) is chosen to have a singularity at a point x0 in the domain;
since the Laplacian is translation invariant, we can take x0 = 0. Moreover, the
Laplacian is invariant under rotations, so we can seek a rotationally invariant
fundamental solution.
Motivated by this discussion, we seek rotationally symmetric solutions u(x) =
v(r), r = |x|, of Laplace’s equation. Then we have
Therefore,

Integrating, we obtain log v′ = −(n − 1) log r + C. That is, 
, where A =
log C is the constant of integration. Integrating again, we get a two-parameter
family of solutions
The fundamental solution Φ(x) is defined by setting b = 0 and choosing the
constant a to normalize Φ(x) depending on the volume α(n) of the unit ball:
The purpose of the normalization is to make the formula for the solution of
Poisson’s equation on Rn as simple as possible. (See (8.1).) Note that although Φ
has an integrable singularity at the origin (Φ is integrable on bounded sets, even
though it is not defined at x = 0), we will see that the singularity of −ΔΦ is not
integrable, and is in fact the singular distribution δ, which we define carefully in
Section 9.2.
We say a function u is harmonic in an open set U ⊂ Rn if u ∈ C2(U) and Δu(x)
= 0 for each x ∈ U. By construction, Φ(x) is harmonic in every open set not
containing the origin.
8.2. Solving Poisson’s Equation in Rn
In this section we establish a formula for the solution of Poisson’s equation in all
space Rn using the fundamental solution, much as we used the heat kernel to
solve the Cauchy problem for the heat equation.
Let f ∈ C2(Rn) have compact support and define
the convolution product of Φ with f.
Remark. Note that ΔΦ(x) has a nonintegrable singularity at the origin.
Therefore, we cannot differentiate under the integral sign in this formula. If we
could, we would have Δu(x) = 0, since Φ is harmonic away from the origin.

However, as we now show, the nonintegrable singularity makes the convolution
product work to solve Poisson’s equation.
Theorem 8.1. If f ∈ C2(Rn) has compact support and u(x) is given by (8.1), then
Proof. Changing variables in (8.1), we have
Therefore,
In this integral, and in subsequent calculations, we use subscripts to indicate the
variable of differentiation or integration. Thus, Δy indicates that the Laplacian is
taken with respect to the y variables. We would like to integrate by parts to put Δ
back on Φ(y). However, Φ(y) has a singularity at y = 0, so we have to treat the
integral as an improper integral. For ϵ > 0, let 
, where B(x, r)
denotes the open ball of radius r centered at x ∈ Rn. Then
The first integral approaches zero as ϵ → 0, since Φ is integrable at the origin. We
use Green’s second identity on the second integral, observing that (since f has
compact support) the integrand is identically zero outside a large enough ball
centered at x. Thus,
Incidentally, there is no contribution from the boundary of the support of f, since
the integrand is zero there. The final integral is zero since Φ is harmonic in Uϵ.
The integral on the boundary has two terms. The first term converges to zero as ϵ
→ 0, since Φ is integrable at the origin. We need to prove that the second
integral converges to f(x).

Since the unit normal ν is outward with respect to Uϵ, on the sphere ∂B(0, ϵ)
we have ν = −y/ϵ. Therefore, since Φ(y) is a function of r = |y|,
Figure 8.1. A bounded region U.
Note that this formula holds for n ≥ 2 even though the formula for Φ is different
for n = 2. Thus,
where |∂B(0, ϵ)| = nα(n)ϵn−1 denotes the measure of the sphere in Rn. This
integral converges to f(x), because f is continuous.
8.3. Properties of Harmonic Functions
In this section we state and prove the mean-value property of harmonic
functions, and use it to prove the maximum principle, leading to a uniqueness
result for boundary value problems for Poisson’s equation. We state the mean-
value property in terms of integral averages.
Theorem 8.2. (Mean-Value Property) Suppose u ∈ C2(U). Then u is harmonic in U if
and only if it has the mean-value property:
for every ball B(x, r) ⊂ U.
Proof. Suppose u is harmonic. Then, for B(x, r) ⊂ U (see Fig. 8.1),

Now let
Since limr → 0 ϕ(r) = u(x), we complete the proof by using (8.2) to show that ϕ(r)
is constant. To do so, we calculate directly that ϕ′(r) = 0. Let y = x + rz, z ∈
B(0, 1), to facilitate differentiating the integral. Then
by (8.2). Thus, ϕ(r) is constant, so that ϕ(r) = lims → 0 ϕ(s) = u(x).
We use this result to obtain the integral average over the ball B(x, r):
Conversely, suppose u has the mean-value property. Then, as above, we get
since ϕ(r) = u(x) is constant. Thus, 
, and letting r → 0, we obtain
Δu(x) = 0, since Δu is continuous in U.
The mean-value property of harmonic functions is peculiar to solutions of
Laplace’s equation and has no counterpart for more general elliptic equations.
However, it simplifies the proofs of key results that do generalize, in particular
the maximum principle, which we now state in both weak and strong forms.
Theorem 8.3. (The maximum principle) Let U ⊂ Rn be open and bounded, and
suppose 
 is harmonic in U. Then
1. Weak form: 
.

2. Strong form: If U is connected, then either u = constant in 
, or
Proof. We prove the strong form first. The weak form then follows easily.
Suppose U is connected and there is a point x0 ∈ U such that
Choose r so that B(x, r) ⊂ U. Then by the mean-value property,
But u(y) ≤ M everywhere, so equality implies that u(y) = M throughout B(x0, r).
Thus, the set S = {x ∈ U : u(x) = M} is nonempty and open. However, S is also
relatively closed in U: Let xn ∈ S converge to x ∈ U as n → ∞. Then, by
continuity of u, we have u(x) = limn → ∞ u(xn) = M, so x ∈ S. But the only
nonempty open and closed set in U is U itself, so we have S = U, implying that u
is constant in . The weak form 1 follows from 2.
Remarks. The corresponding minimum principle follows by applying the
maximum principle to −u.
If U is not connected (i.e., U = U1 ∪ U2 with U1, U2 disjoint open sets in Rn),
then the weak maximum principle holds, but the strong maximum principle
breaks down. To see this, define u(x) = k, for x ∈ Uk, k = 1, 2. Then u(x) is
harmonic but fails to satisfy either conclusion of part 2 of Theorem 8.3.
We can apply the maximum principle to the Dirichlet problem, which is the
following boundary value problem on a bounded open set U ⊂ Rn:
Theorem 8.4. Suppose g is continuous and 
 is a solution of the
Dirichlet problem. If U is connected and g satisfies g(x) ≥ 0 for all x ∈ ∂U, and g(x)
> 0, for some x ∈ ∂U, then
Proof. From the weak minimum principle, we have 
. But the strong
version gives either u(x) > min∂U g, for all x ∈ U, or u(x) = constant. In either
case, the conclusion follows.

8.3.1. Uniqueness of Solutions of Boundary Value Problems
As with the heat equation, we can prove uniqueness of solutions of boundary
value problems from the maximum principle or from energy considerations. Let U
⊂ Rn be open and bounded. Consider the boundary value problem with Dirichlet
boundary conditions:
where f ∈ C(U), g ∈ C(∂U).
Theorem 8.5. There is at most one solution 
 of the boundary value
problem (8.3).
Proof. Let u1, u2 both be solutions. Applying the weak maximum principle to u1
− u2 and to u2 − u1, both of which satisfy (8.3) with f = 0, g = 0, we deduce
that u1 = u2.
Alternatively, the energy approach sets u = u1 − u2 and applies a version of
Green’s identity:
Thus, ∇u = 0 in U, so that u is constant. Since u = 0 on ∂U, we conclude that u
= 0 in , so that u1 = u2.
8.4. Separation of Variables for Laplace’s Equation
If the domain U ⊂ Rn has special geometry, then separation of variables can work
on Laplace’s equation. Examples include rectangular, spherical, and cylindrical
domains. Here we treat two examples to illustrate the differences from the heat
and wave equations. We then make some remarks about other domains.
Of course, if the boundary conditions are homogeneous, then u = 0 is a
solution, generally the only solution. So it is more natural to consider linear
boundary conditions 
 on ∂U that are inhomogeneous over at least part
of the boundary. If α ≠ 0, then the energy method discussed in Section 8.3 above
can be used to prove that this problem has at most one solution.
8.4.1. Laplace’s Equation in a Rectangle
We consider Laplace’s equation Δu = 0 in a rectangular domain U = (0, a) × (0,
b) ⊂ R2 with a mixture of Dirichlet and Neumann boundary conditions,
representing parts of the boundary where we specify either the temperature u or

the heat flux, which is proportional to the normal derivative.
In this problem we can formulate an eigenvalue problem if boundary
conditions on opposite sides of the rectangular boundary are homogeneous. We
use this observation to implement a solution strategy. We split the boundary
value problem into four problems, setting the boundary condition to zero on
three sides in each problem. To illustrate the process, consider the example in
Figure 8.2. For j = 1, 2, 3, 4, let (Pj) be the problem with fk = 0, k ≠ j, and let uj
be the solution of (Pj) Then by linearity, the solution of the full problem is
We solve (P4) in detail to illustrate this approach.
Figure 8.2. Example of a boundary value problem for Laplace’s equation.
Let u = u4 = v(x)w(y). From the boundary conditions, we guess v(x) = sin
nπx/a, so that u4(x, y) = sin(nπx/a) w(y). Then Δu(x, y) = 0 leads to an ODE for
w(y):
with general solution
We need to satisfy a homogeneous boundary condition at y = b,
Thus,

Now we can form a series to satisfy the nonzero boundary condition at y = 0:
On y = 0,
from which we get formulas for the coefficients An:
The solution u4 is then given by the series (8.4). Similarly, we can obtain u1, u2,
u3, and finally put these series together to get the solution u of the original
problem. Note that the series for u2 is a sine series like (8.4), but the series for u1
and u3 have the form
because of the combination of homogeneous boundary conditions at y = 0, b.
8.4.2. Laplace’s Equation on Spherical and Cylindrical Domains
In spherical and cylindrical domains, it is natural to use curvilinear coordinates
(i.e., polar coordinates and cylindrical coordinates, respectively). Since the
Laplacian in these coordinates has nonconstant coefficients, the ODE that result
will also have nonconstant coefficients. Moreover, the dimension of the space
makes a difference to the type of equation that results. This leads to the study of
special functions, specifically, Legendre functions (solutions of Legendre’s
equation) and Bessel functions (solutions of Bessel’s equation). These special
functions are typically expressed as series solutions of ODE, using the method of
Frobenius. Some details may be found in the PDE book of Strauss [45], in
engineering mathematics books, such as Jeffery [26] and Kreyszig [30], and in
texts that typically have “PDE” and “Boundary Value Problems” in their titles.
Here we give the detailed solution of Laplace’s equation in a disk, leading to
Poison’s formula, a representation of the solution as an integral, which we
eventually interpret in terms of Green’s functions. The disk has the advantage
that we do not need special functions to solve the eigenvalue problem.
Consider the Dirichlet problem in a disk of radius a > 0:

In plane polar coordinates,
we have the following problem for u = u(r, θ):
The boundary ∂U is the circle r = a, whereas the boundaries for the variables r, θ
also include r = 0, θ = 0, 2π. At the disk center, r = 0, we exclude nonphysical
solutions by insisting that solutions remain bounded as r → 0+. The boundaries θ
= 0, 2π represent the same line in the disk, across which the solution should be
as smooth as elsewhere in the domain. These boundaries are accommodated by
making the solution 2π periodic in θ. Similarly, the boundary function f is treated
as a 2π-periodic function of θ.
Let
Substituting into the PDE (8.5), we obtain
whence, separating r from θ,
We then have an eigenvalue problem for H, in which the boundary condition is
that H(θ) is 2π periodic:
The corresponding equation for R(r) is
We can solve the eigenvalue problem (8.6), with the result H = H0 = A0/2 =
const., for λ = 0, and
Note that each eigenvalue λn, n ≥ 1, has two independent eigenfunctions. Setting
λ = n2 in (8.7), we get

For n = 0, the general solution of (8.8) is
However, we seek solutions that are bounded as r → 0 so we set D0 = 0 and
consider only the solution
The arbitrary multiple C0 will be incorporated into A0.
For n ≥ 1, we seek solutions R(r) = rα. Substituting into (8.8), we find α =
±n. However, r−n is unbounded at the origin, so we retain only
Again, the arbitrary coefficient multiplying this solution will be incorporated into
Hn(θ).
So far, we have solutions
These functions are harmonic in the ball B(0, a), and they reduce to functions of
θ alone for r = a.
We form a series 
:
and set r = a to satisfy the boundary condition u(a, θ) = f(θ). Thus,
Consequently, anAn, anBn are Fourier coefficients of the 2π-periodic function f :
If we substitute the coefficients given by (8.10) back into the series (8.9), we get
the solution u in terms of the data, and we can sum the series, just as we summed
the series to get the Dirichlet kernel in proving pointwise convergence. Thus,

After some manipulation of the sum of the geometric series
we obtain Poisson’s formula for the solution of the Dirichlet problem in a disk:
This integral has the form of a convolution product of the Poisson kernel
with the boundary data f(ψ) = u(a, ψ). The formula reduces to the mean-value
property of harmonic functions when r = 0. In the special case f ≡ 1, we have
the solution u = 1, from which we conclude that
Note that you could also have guessed this by integrating the series term by term.
Just as for fundamental solutions, which are singular integral kernels, the
Poisson kernel, P (r, θ − ϕ) is singular at the very place the function u(r, θ) is to
be evaluated on the boundary: r = a, θ = ϕ. The singularity is needed for the
convolution to converge to the boundary data: for f continuous,

Figure 8.3. Geometric interpretation of Poisson’s formula.
A more geometric interpretation of Poisson’s formula generalizes to higher
dimensions. Consider polar coordinates for
Then we have a2 − r2 = |x′|2 − |x|2, and |x′ − x|2 = r2 + a2 − 2ar cos(θ −
ϕ) (see Fig. 8.3). Thus,
The Poisson kernel is an example of a Green’s function, which we study in
detail in the next chapter.
PROBLEMS
1. Prove the weak maximum principle (Theorem 8.3, part 1) using an argument
similar to the proof used for this principle for the heat equation (Theorem 5.2).
2. Prove the weak maximum principle (Theorem 8.3, part 2) from the strong
form.
3. Consider Poisson’s equation on a bounded open set U ∈ Rn with Robin
boundary conditions

(a) Prove that if α > 0, then the energy method can be used to show
uniqueness of solutions 
.
(b) For α = 0, show that solutions are unique up to a constant.
(c) Design an example to show that uniqueness can fail if α < 0. (Hint:
Choose n = 1.)
4. Derive Poisson’s formula (8.11) by summing the series for u(r, θ). Provide the
details.
5. In Rn let Vr = |B(0, r)| = α(n)rn, Sr = |∂B(0, r)|. Explain why
6. Suppose u ∈ C2(U) has the mean-value property:
For all x ∈ U, 
 for all r > 0 such that B(x, r) ⊂ U. Write a
careful proof by contradiction that Δu = 0 in U.
7. Suppose U ⊂ Rn is open, bounded, and connected, and 
 satisfies
Prove that if g ∈ C(∂U), g(x) ≥ 0 for all x, and g(x) > 0 for some x ∈ ∂U, then
1. Pierre-Simon Laplace, 1749–1827, made many contributions to mathematics, physics, and astronomy.
Simeon Denis Poisson, 1781–1840, was a mathematician and physicist known for his contributions to the
theory of electricity and magnetism.

CHAPTER NINE
Green’s Functions and Distributions
Green’s functions, integral kernels that allow linear boundary value problems to
be expressed as integral equations, appear in many contexts.1 The theory and
construction of solutions for the basic linear PDE of mathematical physics use
Green’s functions or fundamental solutions. For example, in linear elasticity, a
Green’s function is the displacement of the elastic material (such as rubber) due
to a point force, whereas in electrostatics, the Green’s function relates an electric
field to a point charge. Numerical analysts use Green’s functions as a convenient
way to convert PDE into integral equations that can be solved with numerical
integration.
In this chapter we show how the Green’s function is a solution of a PDE only
in a generalized sense. This sense is elegantly expressed in terms of the theory of
distributions, which we introduce in this chapter as a separate topic, before
returning to the construction and properties of Green’s functions for the
Laplacian and other linear differential operators.
9.1. Boundary Value Problems
We would like to study boundary value problems, such as the following:
where U ⊂ Rn is open and bounded with smooth boundary ∂U.
From Section 8.2 we see how to solve Poisson’s equation on all space by
writing the solution as a convolution with the fundamental solution Φ (see
Theorem 8.1):
The purpose of Green’s functions is to find a similar formula that takes the
boundary conditions into account. An immediate difficulty is that we have no
reason to suppose that (9.2) will satisfy the boundary condition u = g on ∂U. The
idea is to first modify the fundamental solution so that a formula similar to (9.2)
yields a solution of the inhomogeneous PDE that satisfies homogeneous boundary
conditions (i.e., with g ≡ 0), a construction we explain in Section 9.3. As a
second step, we find a smooth solution of the homogeneous PDE (i.e., with f ≡ 0)
that satisfies the inhomogeneous boundary condition u = g on ∂U. Finally, we put

the two pieces together, relying on linearity, to give a solution of the boundary
value problem (9.1).
Figure 9.1. Area of integration. The diagonal line is z = y.
Let’s start with problem (9.1) in one dimension with homogeneous Dirichlet
boundary conditions:
We solve this problem by integrating twice:
where C, D are constants of integration.
The boundary condition u(0) = 0 leads immediately to D = 0. The boundary
condition u(1) = 0 then gives an equation for C. It is convenient to simplify the
double integral by reversing the order of integration in the shaded region of
Figure 9.1.
Then

Figure 9.2. The integral kernel G(x, y).
The boundary condition at x = 1 becomes
Substituting back into (9.4) and rearranging terms, we arrive at
We can write this formula in the compact form
using the integral kernel
The graph of G(x, y) for fixed x ∈ (0, 1) is shown in Figure 9.2.
Equation (9.5) defines an integral operator G:
The differential operator 
 operates on functions u in the space
X of C2 functions that satisfy the homogeneous boundary conditions. The integral
operator G : C[0, 1] → X goes the other way; it acts on continuous functions and
gives twice-differentiable functions that satisfy (9.3). In this sense, the integral
operator is the inverse of the differential operator. In fact, G is the Green’s

function for the boundary value problem, because (9.4) satisfies (9.3). Observe
that (9.5) has some similarity to (9.2).
Here are some properties of G : [0, 1]× [0, 1] → R:
1. G is nonnegative: G(x, y) ≥ 0;
2. G is symmetric: G(x, y) = G(y, x);
3. G is continuous; and
4. G is differentiable, except on the diagonal x = y. On the diagonal,
 has a
jump discontinuity:
where the bracket notation [F] means the jump in a function F, or difference
between the right limit and left limit. While these properties are specific to the
Green’s function (9.6) for problem (9.3), they have their counterparts for
different differential operators L.
Now 
 except on the diagonal, where 
 may be thought to have infinite
negative slope. We write
where δ(x) is the Dirac delta function. The function δ(x) is a measure that assigns
mass one at x = 0 and zero mass elsewhere. We shall treat δ as a distribution, or
generalized function, which leads us to the theory of distributions, a useful
framework for considering PDE and Green’s functions, such as G(x, y).
It is also useful to relate the Green’s function to the fundamental solution 
. In fact, if we write
then 
 for each y ∈ [0, 1]. We write the superscript y in the function ϕ
(x) to indicate that y is a parameter. Moreover, since G satisfies homogeneous
boundary conditions, ϕy satisfies the boundary conditions (for each y ∈ [0, 1]):
Thus, to modify the fundamental solution to obtain the Green’s function, we
construct for each y a solution u = ϕy of the homogeneous equation u″(x) = 0
that satisfies the boundary conditions (9.7).

9.2. Test Functions and Distributions
In the previous section we constructed a Green’s function that is not twice
differentiable in the classical sense but has a generalized second derivative that is
a delta function, an example of a distribution. The space of distributions, defined
in this section, is used to broaden the notion of solutions of PDE, especially linear
PDE. We discuss distributions in a variety of contexts in the next couple of
chapters, but this narrative begins with a space of smooth functions known as test
functions. Test functions are instrumental in defining distributional solutions of
PDE by using integration by parts to transfer derivatives from distributions onto
the test functions.
9.2.1. Test Functions
To study test functions, we introduce some new notation and terminology. For
simplicity, we start by considering smooth functions on R. Let 
 denote
the space of C∞ functions ϕ with compact support, supp ϕ. Then D is the space of
test functions, with a specific notion of convergence defined as follows.
We denote the jth derivative of ϕ by ϕ(j). A sequence {ϕn} converges to ϕ in D
as n → ∞ if
1. there is a compact (closed and bounded) subset K of R such that supp ϕn ⊂ K
for all n, and supp ϕ ⊂ K; and
2. 
 as n → ∞, uniformly on K, for each j ≥ 0:
Similarly, the space D(Rn) of test functions on Rn is defined by replacing ordinary
derivatives by partial derivatives in the definition of convergence.
Example 1. (A test function) Let’s consider an example of a test function in Rn
that we will refer to repeatedly.
Let
where 
 dx is chosen so that

To see that η is a test function, we note that it has compact support {x : |x| ≤ 1},
and it has continuous derivatives of all orders, even where the definition is split
at |x| = 1. For example, with n = 1, the derivatives approach zero at x = 1. To
see this, observe that every derivative is the product of a rational function and
the exponential 
; the exponential dominates the rational function as x →
1, sending the derivatives to zero.
It is sometimes useful to rescale η using a parameter ϵ > 0:
Then the support is scaled by ϵ, while leaving the integral unchanged: supp ηϵ =
{x : |x| ≤ ϵ}, and 
. The function ηϵ is called a mollifier, because it can
be used to smooth rough functions. The next lemma shows us how this works
using the convolution product.
Lemma 9.1. Let f ∈ C(Rn), and define, for ϵ > 0, fϵ(x) = ηϵ fϵ ∗ f(x). Then fϵ ∈ C∞,
and fϵ(x) → f(x) for all x, as ϵ → 0.
Let 
 denote the space of locally integrable functions on Rn:
Then fϵ is defined and is C∞ for 
. In that case, it takes a little measure
theory (the Lebesgue-dominated convergence theorem) to prove that fϵ → f
almost everywhere (see Evans [12], Appendix). Thus, the convolution of the
mollifier ηϵ with the only-once-differentiable function f(x) provides an infinitely
smooth function fϵ(x).
9.2.2. Distributions
Now that we have established properties of the space D of test functions, we are
ready to define distributions. Each distribution f is a function on D, meaning that
f(ϕ) is a number for each test function ϕ.
Specifically, the space of distributions D′ is defined to be the space of
continuous linear functionals on the space D of test functions. That is, f ∈ D′
means f : D → R, and f has the following properties:
1. f is linear: f (aϕ1 + bϕ2) = af (ϕ1) + bf (ϕ2) for each a, b ∈ R, ϕ1, ϕ2 ∈ D; and
2. f is continuous: ϕn → ϕ in D implies f(ϕn) → f(ϕ) (as a sequence of numbers).

The space of continuous linear functionals on a given topological space X is
called the dual space of X and is typically denoted X′. Thus, D′ is the dual space
of D. Following the usual custom, we denote f(ϕ) by (f, ϕ). The notation should
not be confused with the L2 inner product of integrable functions.
Not every functional is a distribution. For example, if we define f : D → by
f(ϕ) = ϕ(0)2, then f is a continuous functional, but since it is nonlinear, it is not a
distribution.
Example 2. (Four distributions) Here we provide four examples of
distributions. For each j in D′, j = 1, …, 4, test functions ϕ ∈ D are assigned to
numbers (j, ϕ):
1. (f1, ϕ) = fR ϕ(x) dx.
2. 
3. (f3, ϕ) = (δ, ϕ) ≡ ϕ(0).
4. (f4, ϕ) = ϕ′(0).
We leave to the problems verification that these examples are well defined,
linear, and continuous.
Many distributions are associated with locally integrable functions. The first
distribution f1 is associated with 
, which is not integrable on R, but it is in 
. Then 
.
The second distribution f2 is associated with the Heaviside function:
which is in 
 and 
.
More generally, we have the following lemma.
Lemma 9.2. If 
, then  defines a distribution g ∈ D′ by
Proof. Linearity of g follows from the formula in the lemma; continuity follows
from the formula and an estimate:

as n → ∞, where supp ϕ, supp ϕn ⊂ K.
We say a distribution f is regular if it has an 
 representative :
Thus, regular distributions can be thought of as functions. For example, f2 is a
regular distribution; it can be identified with the Heaviside function.
However, not every distribution defines an 
 function. A distribution f is
called singular if it is not regular. There are many singular distributions; even the
delta function f3 = δ is singular, as we now show. It will then follow that f4 in
Example 2 is also singular.
Lemma 9.3. The delta function is a singular distribution.
Proof. Suppose δ is regular, so that there is 
 such that
Now define a one-parameter family of test functions with parameter ϵ,
specifically, ϕϵ(x) = η(x/ϵ):
We calculate the effect of δ on ϕϵ in two ways. First, from the definition of δ:
Second, using the assumption (9.9):
However, then
Since the latter integral approaches zero as ϵ → 0, we can choose ϵ > 0 small
enough that the integral on the right is less than e−1, contradicting the inequality.
We next discuss several general properties of distributions that are useful in
the analysis of PDE, both specific ones (such as Laplace’s equation) and in the

general theory of PDE, including both linear and nonlinear equations.
Convergence of Distributions
We observed in Chapter 5 that the heat kernel Φ(x, t) converges to δ(x) as t →
0+. Here we make this convergence precise by defining what it means for a
sequence of distributions to converge to a limiting distribution.
Let 
 be a sequence of distributions, and let f ∈ D′(Rn). We say fk
→ f in D′ in the sense of distributions if (fk, ϕ) → (f, ϕ) as k → ∞, for all ϕ ∈
D(Rn).
Example 3. (Sequence of distributions) Let n = 1, and define, for k = 1, 2, …:
Then, for ϕ ∈ D(R),
Thus, fk → δ in the sense of distributions as k → ∞.
Example 4. (Convergence of heat kernel Φ(x, t) as t → 0) Let Φ(x, t) = 
. Then Φ(x, t) → δ(x) as t → 0+ means (Φ(x, t), ϕ(x)) → ϕ(0)
as t → 0+.
Distributional Derivatives
Let f ∈ C1(Rn). Then f is differentiable, and each partial derivative is locally
integrable and therefore defines a distribution. We have
This calculation suggests that for any distribution f we define its distributional
derivative ∂f/∂xi to be a distribution given by

Then every distribution is differentiable in the sense of distributions, and hence has
derivatives of all orders. It is not hard to show that ∂f/∂xi is indeed a distribution
by checking directly that it is continuous and linear.
Example 5. (Distributional derivative with n = 1) Consider the Heaviside
function H(x). As we saw earlier, H is in 
, and it acts on test functions by 
. Thus, for any test function ϕ,
Therefore,
We can also differentiate the δ function directly from the definition of
distributional derivative:
In fact, this is related to example f4 in Example 2.
As another example, let f(x) = |x|. Then because
is an 
 function, it defines a distribution. In fact, f′ = 2H − 1; consequently, f″
= 2δ. Here we have used the fact that differentiation is a linear operation on
distributions, just as it is on differentiable functions.
Translation by y ∈ Rn The delta function δ(x) is a distribution that places unit
mass at x = 0. To place the mass at a point y, we define a new distribution with
the notation δ(x − y). In general, we can translate any distribution f by a
constant y as follows. We define the translation of f ∈ D′, by y ∈ Rn, as a new
distribution g ∈ D′, for which
where ϕ(−y) ∈ D is the test function defined by ϕ(−y)(x) = ϕ(x − y). To see that
this makes sense, we simply check that it is consistent for any 
:

Then we can associate the distribution g with the translation f(x + y) of the 
function f(x) by y.
An example where this translation is useful is the δ function. We often write
δ(x − y) to mean the distribution δ, translated by −y, and it is common to leave
x in the arguments of both the distribution and the test function:
To multiply a distribution f ∈ D′ by a function c ∈ C∞, we define cf ∈ D′ by
noting that cϕ is a test function. Again, this definition is motivated by the case of
a regular distribution f, in which case the formula makes sense interpreted as
integrals.
The next two examples illustrate the connection between properties of
distributions and differential equations.
Example 6. (Multiplication by a C∞ function) Consider c(x) = x, f = δ in R.
Then
so that xδ = 0. Since δ = H′, we can interpret xδ = 0 as saying that y(x) = H(x)
is a distribution solution of the differential equation
In fact, the general distribution solution of this equation (which is singular at x =
0) is
for arbitrary constants a, b. Thus, we have a two-parameter family of
distributional solutions for the first-order equation. This highlights a danger in
enlarging the space of functions in which to define solutions, namely, that
uniqueness of solutions may be lost in the larger space. Uniqueness can be
restored by adding suitable boundary or initial conditions.
Example 7. (Application to shock wave solutions of conservation laws)
Shock waves, introduced in Chapter 2, are in fact distributional solutions of PDE,
which we illustrate in this example for the scalar conservation law
in which f : R → R is a given C1 function. When we interpret (9.10) in the sense

of distributions, it allows us to give meaning to shock wave solutions, which are
discontinuous. The equation simply states that if u ∈ D′(R2) and f(u) can be
interpreted as a distribution, then the combination of distributional derivatives
on the left-hand side of the equation should be zero on every test function. Thus,
for a test function ϕ(x, t):
In this way, we can define distribution solutions of differential equations, even
nonlinear equations. To see that this interpretation has some substance, consider
a jump discontinuity
where u± and s are constants. Then
We rewrite these functions using the Heaviside function:
Thus,
and H′ = δ. Equation (9.10) becomes
in the sense of distributions. But then the constant −s(u+ − u−) + f(u+) − f(u−)
must be zero:
This equation is the Rankine-Hugoniot condition for shock wave solutions of
(9.10). It is useful because it relates the shock speed s to the one-sided limits u±
of the solution on either side of the discontinuity. We will derive it for more
general discontinuities in Chapter 13 on scalar conservation laws.
Distributions on Open Subsets of Rn
It is straightforward to formulate the above ideas for distributions on subsets of
Rn. The key step is to define test functions appropriately and to use integration by

parts. Let U ⊂ Rn be open. Then we can define the space 
 of test
functions on U (i.e., the C∞ functions on U that have compact support in U). Then
D′(U) is the space of continuous linear functionals on D(U). Now let u : U → R
be in 
. Then u defines a distribution in D′(U). In particular, u is
differentiable in the sense of distributions:
We say u has a weak derivative 
 if the distributional derivative is a regular
distribution. Recall that this means it is represented by a locally integrable
function: 
. In this way, we distinguish between the weak derivative
and a distributional derivative.
9.3. Green’s Functions
In this section we return to Green’s functions, first giving a general framework
within the theory of distributions and then showing how this applies to the
Laplacian. The primary use of distributions here is related to the delta function
and the notion of fundamental solution for a differential operator. Green’s
functions provide a means to invert the differential operator for boundary value
problems.
9.3.1. General Framework
Consider a linear partial differential operator L (L = −Δ for example), that acts
on functions u : Rn → R. As we introduced for one dimension in Section 9.1,
Green’s functions enable us to provide an integral representation for solutions of
boundary value problems
Here f is a given function on U, and g is a given function on ∂U, but note that U ⊂
Rn may be unbounded. The term Bu represents a linear combination of u and
derivatives of u of lower order than the order of the partial differential operator
L. Associated with L we have the fundamental solution Φ(x, y), which is
required to satisfy
in the sense of distributions. To see why this is helpful, consider Φ to be locally
integrable in y for each x ∈ Rn, and let f ∈ D(Rn). Then (using the L2 inner

product notation, since Φ and f are integrable)
satisfies Lv = f in Rn because
That is, the fundamental solution is the key to solving the inhomogeneous PDE.
However, (9.12) does not generally satisfy the boundary condition Bv = g. To
satisfy the boundary condition, we add a solution w of the homogeneous equation
Lw = 0 so that u(x) = v(x) + w(x) satisfies the boundary condition Bu = g. But
then w must satisfy the boundary condition Bw = g − Bv. Thus, provided we
have the fundamental solution Φ(x, y), the boundary value problem (9.11) is
reduced to solving the problem
The boundary condition for w is a bit clumsy; it relies on applying the boundary
operator to the integral (9.12) and restricting it to the boundary. The way to
express this more smoothly is to introduce the Green’s function for the
differential operator L with boundary operator B. For clarity, we consider L and
B to have independent variable 
, and we let 
 be a second independent
variable, which is treated as a parameter for now.
The Green’s function G = G(x, y) is defined as the solution of the problem
for each 
.
We construct G using the fundamental solution, by defining a function ϕy(x):
Then ϕy(x) satisfies
Now we can express the solution of (9.11) as a sum u = v + w, where v(x) = ∫U
G(x, y) f(y) dy, and w(x) satisfies
This formulation is useful even when we do not have the Green’s function

explicitly, since estimates on the Green’s function can be used to obtain estimates
on the solution u. In special cases, we can complete the solution as an explicit
formula by finding the Green’s function and solving (9.15) for w(x). We next
demonstrate this for Poisson’s equation, where L = −Δ.
9.3.2. Green’s Functions for the Laplacian
We apply the ideas just developed to the Dirichlet boundary value problem for
Poisson’s equation:
Here U is open and bounded, with piecewise smooth boundary ∂U; and f, g are
continuous on U, ∂U, respectively. In terms of the previous section, L = −Δ, and
the boundary operator B is the identity: Bu = u.
The fundamental solution for −Δ is a function Φ(x − y) of x − y, since the
Laplacian Δ is translation invariant. (More generally, the fundamental solution
for any constant-coefficient PDE operator L will be a function of x − y.) The
Green’s function G(x, y) = Φ(x − y) − ϕy(x) is then expressed in terms of the
solution of the boundary value problem
In particular, ϕy is a harmonic function in U. In the proof of the following
theorem, we use the symmetry property ϕy(x) = ϕx(y), which follows since Φ(z)
is an even function of z. With this construction, we can write a formula for the
solution of the boundary value problem (9.16).
Theorem 9.4. If 
 solves (9.16), then
Proof. As explained above, the first integral in (9.17) solves the inhomogeneous
PDE, with homogeneous boundary condition, so it remains to prove that the
second integral is harmonic and satisfies the given boundary condition. The
second integral is harmonic in x, since G(x, y) is harmonic in x ∈ U for each y ∈
∂U, and the second term involves differentiating and integrating only with
respect to the parameter y ∈ ∂U. To verify the boundary condition, we effectively
show that the normal derivative acts like a delta function as x approaches the
boundary. To do this, the proof uses the Divergence Theorem to recover the

boundary data.
Recall that G(x, y) has the same singularity at y = x as does the fundamental
solution. In the verification of the solution of Poisson’s equation on all of space
(Section 8.2), using the fundamental solution, we excluded a small ball around x
∈ U and integrated on the domain
We do the same thing here to accommodate the singularity in G(x, y). To start,
we work with Φ and a function 
 (not necessarily a solution). Then, using
Green’s identity, we have
Since Φ(x − y) is harmonic away from y = x, the first term on the left-hand side
is zero. On the right-hand side there are two terms, but there are also two parts
of the boundary, namely, ∂U and ∂B(x, ϵ). We now show that the contributions
from ∂B(x, ϵ) approach zero as ϵ → 0. Let
As with the solution of Poisson’s equation, Iϵ → u(x) as ϵ → 0 (see Section 8.2).
Similarly, let
We assume in the theorem that u is continuously differentiable. Since Φ(x − y) is
constant on ∂B(x, ϵ), proportional to ln ϵ for n = 2, and to ϵ−(n−2) for n > 2, we
conclude that |Jϵ| → 0 as ϵ → 0.
Letting ϵ → 0, we now obtain
The next step is to use a similar argument, in which we replace Φ(x − y) by

the harmonic function ϕx(y) in the calculation. Since ϕx has no singularity at x =
y, and ϕx(y) = Φ(x − y) for y ∈ ∂U, we have
Rearranging, and using G(x, y) = Φ(x − y) − ϕx(y), we obtain
Finally, when u is a solution of (9.16) we obtain (9.17), and the proof is
complete.
9.3.3. The Method of Images
Theorem 9.4 gives a formula for the solution of (9.11) that relies on Green’s
functions. Here we show how the Green’s function for the Laplacian can be
calculated from the fundamental solution in special cases when the domain U has
symmetry. The general idea is to construct image points  outside U for each x ∈
U such that the function 
 exactly cancels Φ(x − y) on the boundary ∂U,
for some scale factor C, possibly depending on x. Then the new function is
harmonic with respect to y ∈ U (since the Laplacian is invariant under scaling
and translation by a constant), so we can set 
.
Figure 9.3. Method of images to derive Green’s function for a half-plane.
We demonstrate this construction, which is known as the method of images,
in two examples. In the first example U is a half-space, and in the second
example U is a ball.

Example 8. (Method of images (half-space)) Let U = {x ∈ Rn : xn > 0}. For x
= (x1, …, xn) ∈ U, define the image 
 (see Fig. 9.3), and let
Since 
, it follows that 
 is harmonic with respect to y ∈ U (for x ∈ U).
Then G(x, y) will be established as the Green’s function if we can show G(x, y) =
0 for yn = 0, xn > 0. For yn = 0, we have
Therefore, 
, as needed.
Example 9. (Method of images (unit ball)) Consider U = B(0, 1), the unit ball
in Rn. Here, the image  includes a scaling, in addition to reflection through the
boundary. (See Fig. 9.4.) We define 
, and let
(Note that 
.) To prove that G(x, y) = 0 for x ∈ U, y ∈ ∂U, we need to
show that if 0 < |x| < 1, 
, and |y| = 1, then
Figure 9.4. Method of images to derive Green’s function for the unit ball.
But this follows because
Consequently, G(x, y) given by (9.18) is the Green’s function for the unit ball
with Dirichlet boundary conditions.

In Chapters 4–9 we have considered all three canonical second-order linear
constant-coefficient PDE, emphasizing explicit solutions. In the next two
chapters, we introduce more theoretical approaches to general linear elliptic
equations.
PROBLEMS
1. Consider the boundary value problem
in which f : [0, 1] → R is a given continuous function.
(a) Prove that there is no solution unless
(b) Assuming (9.19), prove that solutions are unique up to a constant. In other
words, if u, v are two solutions, then
for some constant C.
(c) Write the solution u(x) in the form
write a formula for the Neumann function N(x, y).
2. Write an explicit formula for ϕy(x) for 
, where G is
given by (9.6), to verify that ϕy(x) is linear for each y.
3. Calculate the fundamental solution u ∈ D′(R) for the differential operator 
, where c ∈ R and c ≠ 0, satisfying lim|x| → ∞ u(x) = 0. That is, solve 
. (Note: The sign of c will affect your solution.)
4. Compute g ∗ f for the functions
and graph the convolution product g ∗ f.
5. Verify that the examples of distributions f1, f2, f3, f4 in Example 2 satisfy the
conditions that define a distribution.

6. Prove that ηϵ → δ in the sense of distributions, as ϵ → 0. (See (9.8).)
7. (a) Let 
. Define the distributional derivative 
.
(b) Let H : R → R be the Heaviside function. Prove that u(x, y) = H(y)
satisfies 
 in the sense of distributions.
8. Let k > 0.
(a) Prove that Gk(x, y) = Cke−k|x−y| is a fundamental solution for the equation
for some Ck. Find a formula for Ck.
(b) Find the Green’s function for the boundary value problem
9. Find a fundamental solution Φ(x) depending only on r = |x|, x ∈ R3, for the
equation
satisfying limx → ∞ Φ(x) = 0. Instead of having a delta function (on the right-hand
side of the equation defining Φ), use the source condition
(Hint: Use a change of variable u = rv.)
10. Let 
, x = (x1, x2) ∈ R2. For what values of α is u in L2(B),
where B = {x : |x| < 1}? Explain your answer.
11. Consider f ∈ D′(R), g ∈ C∞(R).
(a) Derive the formula (gf)′ = gf′ + g′f in the sense of distributions.
(b) Hence prove that
12. Let u ∈ C(U) satisfy the mean-value property in a domain U ⊂ Rn:
provided B(x, r) ⊂ U. Let uϵ = ηϵ ∗ u in Uϵ = {x ∈ U : dist(x, ∂U) > ϵ}, where
dist(x, S) denotes the shortest distance between a point x and a set S. Then uϵ ∈

C∞(Uϵ). Prove the surprising result that uϵ = u on Uϵ. Consequently, u ∈ C∞(U)
and is harmonic in U. In particular, harmonic functions are C∞! (Hint: You will
need a change of variables in the formula for uϵ to write the integral over B(x, ϵ)
in polar coordinates (dx = rn−1drdS) to take advantage of this version of the
mean-value property.)
1. George Green, 1793–1841, was a self-taught British mathematician and physicist who made
fundamental contributions to the theory of electricity and magnetism. Several theorems and functions
related to these topics now carry his name.

CHAPTER TEN
Function Spaces
In this short chapter, we introduce function spaces that are used extensively in
the analysis of partial differential equations.
10.1. Basic Inequalities and Definitions
Much of the theory of PDE relies on a variety of estimates, like the energy
estimates we encountered in Chapter 5 for the heat equation. Estimates are used
to establish all aspects of well-posedness and regularity. PDE estimates routinely
use the inequalities that we introduce and prove in this section, including some
basic inequalities for function spaces, in particular, Lp spaces and Sobolev spaces.
10.1.1. Inequalities on R
We begin with several inequalities between numbers, which form a basis for
inequalities and estimates of functions.
Cauchy inequality. By rearranging (a − b)2 ≥ 0, we arrive at the inequality
Sometimes it is useful to weight the terms differently, as in the ϵ > 0 version of
the Cauchy inequality:
Proof of (10.1). Apply Cauchy’s inequality to 
.
Young’s inequality. This relation is a different generalization of the Cauchy
inequality. For p > 1, define q by 
; we say q is dual to p. Then
This inequality is the Cauchy inequality when p = q = 2.
Proof. Minimize 
. We leave the details to Problem 2.
10.1.2. Function Spaces and Inequalities on Functions
Now we are prepared to define the function spaces we shall use. For each space,

we define a norm, and an inner product where possible.
For 1≤ p < ∞, define Lp(U) to be the space of (measurable) functions whose
pth power is integrable over U ⊂ ∫U Rn : |u|p dx < ∞. We define a norm on Lp by
As discussed for L2 spaces in Section 7.2, this defines a norm only if functions that
are equal almost everywhere are considered equivalent. Then Lp is defined to be
the space of equivalence classes, with the norm of an equivalence class defined as
here, in which u is any element in the equivalence class. Sometimes we
abbreviate the subscript and write ||u||p for the Lp norm.
The space L∞(U) is defined as the space of (measurable) functions that are
essentially bounded over U, with norm given by the essential supremum (see
Appendix B):
Of the following three defining properties of a norm, only the triangle
inequality requires proof, as the first two follow directly from the definition of
norm in Lp:
1. ||u|| ≥ 0, with equality only for u = 0;
2. ||αu|| = |α| ||u|| for all α ∈ R; and
3. ||u + v|| ≤ ||u|| + ||v|| (the triangle inequality).
The proof of the triangle inequality employs a further inequality involving
integrals of functions.
Lemma 10.1. (Hölder’s inequality) Let u ∈ Lp(U), v ∈ Lq(U), where 1 < p, q < ∞
are dual: 
. Then
Proof. Since Hölder’s inequality is homogeneous, it is enough (and simpler to
write) if we take ||u||p = 1= ||v||q. Then we apply Young’s inequality to u(x)v(x)
and integrate over U:

The following case p = q = 2 is important enough to have its own name.
Theorem 10.2. (The Cauchy-Schwartz inequality) Let u, v ∈ L2(U). Then
Now we can prove the triangle inequality for the Lp norm.
Proof of the triangle inequality. Let u, v ∈ Lp(U). Then
The triangle inequality now follows by dividing by ||u + v||p−1.
Recall that L2 is special because it has an inner product
consistent with the L2 norm ||f ||L2(U) = (f, f)1/2. For the inner product (as for
Hölder’s inequality), we allow for the possibility of complex-valued functions.
However, from now on unless otherwise stated, functions will be real valued.
It is sometimes helpful to think of Lp functions in terms of Fourier series. In
the specific case of L2[0, π], we can explicitly and easily make the connection to
the space ℓ2 of sequences that are square-summable:
with inner product and norm defined by:

respectively, where x = {xj}, y = {yj}. In fact, Cauchy sequences in ℓ2 converge,
and their limits are in ℓ2. Thus, the linear inner product space ℓ2 is complete,
making it a Hilbert space.
To establish the connection between L2[0, π] and ℓ2, let u ∈ L2[0, π]. Then
where (.,.) is the L2 inner product, and the sine functions 
 sin jx, j = 1, 2,
… form a complete orthonormal set. Now let bj = (u, uj). Then Bessel’s inequality
(7.12) implies that the sequence {bj} is in ℓ2, and moreover, Parseval’s identity is 
. More concisely,
A fancy way to say this is that the mapping J : L2[0, π] → ℓ2 given by J (u) = {bj}
is an isometric isomorphism.
Integrable functions are not necessarily continuous or even bounded. In the
following examples, we examine the constraints between the nature of a
singularity and the dimension of the space and the exponent p in order for the
function to be in Lp. The first example examines the kind of singularity allowed in
Lp functions. The second example is more sophisticated and shows that integrable
functions can be very singular.
Example 1. (A singular function) Let U = B(0, 1) ⊂ Rn. Consider the function
in which β > 0 is some constant, and u(0) = 0 (In fact, u(0) does not have to be
specified, since Lp functions need only be defined almost everywhere.)
To work out the values of n, β, p for which we have u ∈ Lp(U), we need to
understand when up is integrable. Note that u has a singularity at x = 0. To
resolve this issue, we calculate 
, and find precisely when it is finite. For
example, for n = 1, 
 if and only if βp < 1. Now we have to
work out the effect of changing n on this condition. But in Rn we have

and this is finite precisely when the inner integral is finite, namely, when (as in
the n = 1 case) the power of 1/r is less than unity: βp − n + 1 < 1 :
Example2. (A very singular integrable function) Let 
 be an enumeration
of the (countable) set of rationals in the interval [0, 1]. Define
where 0 < β < 1. Then the Monotone Convergence Theorem (Appendix B)
implies that the series converges in L1[0, 1], but u is unbounded in every
neighborhood of every point. Hence, although u is Lebesgue integrable, not only
is it not Riemann integrable, it is chronically unbounded!
10.2. Multi-Index Notation
To further discuss function spaces, we use multi-index notation. This notation is
also a convenient way to represent PDE of arbitrary order and with unspecified
coefficients. A multi-index, α = α1 … αn, is a sequence of nonnegative integers.
We write the multi-index length as |α| = α1 + … + αn. If x = (x1, …, xn) ∈ Rn,
we write 
, and
a differential operator of order |α|. Be aware that for x ∈ Rn, the notation |x| = 
 is still the Euclidean norm. The multi-index length |α| is reserved for
multi-indices.
Multi-index notation is useful for writing a polynomial of degree k in n
variables x as ∑|α|≤k aαxα = 0, with constant coefficients aα ∈ R. Correspondingly,
the notation allows us to write a general quasilinear PDE of order k as
for theoretical purposes, where the coefficients aα and right-hand side f are
functions of u, derivatives of u, and of x ∈ Rn. In this form it is convenient to
place assumptions on the coefficients (such as ellipticity of the PDE) to capture

whole classes of equations.
If u has a weak derivative Dαu, then
for all ϕ ∈ D(U), the space of test functions.
Although multi-index notation is useful in some general contexts, such as in
the next section, it is often simpler to use more conventional notation, such as 
in place of D010 in R3. Even a simple PDE like the wave equation uxx − uyy − uzz
= 0 looks ugly in multi-index notation:
10.3. Sobolev Spaces Wk,p(U)
When studying solutions u of kth-order PDE, we need derivatives of u of order up
to k. Sobolev spaces Wk,p(U) consist of functions with weak derivatives up to order
k, but with the additional requirement that the derivatives are in Lp; that is, their
pth powers are integrable. Thus, Wk,p(U) is defined to be the space of functions u
such that Dαu ∈ Lp(U) for all α such that |α| ≤ k. Sobolev spaces are used in the
theory of PDE, for example, elliptic PDE in Chapter 11.
The norm in Wk,p(U) is defined for 1 ≤ p < ∞ by
For p = ∞, we define
Just as we did for Lp spaces, we can ask when a function with an algebraic
singularity is in Wk,p.
Example 3. (Another singular function) Consider the example u(x) = |x|−β, x
∈ U = B(0, 1), x ≠ 0 (see Example 1). Then
which has a singularity like 
. Referring back to Example 1, in Lp, we deduce
that 
 if and only if (β + 1)p < n. Thus, u ∈ W1,p(U) if and only if

When we study elliptic equations with homogeneous Dirichlet boundary
conditions in the next chapter, the boundary conditions are built into the Sobolev
spaces as follows. The space 
 is the completion of 
 in the Wk,p(U)
norm. That is, every element of 
 is the limit of a Cauchy sequence of
smooth functions with compact support in U. In this sense we can think of 
as the space of Wk,p functions that are zero on the boundary ∂U. This notion is
made precise with trace theorems, which are developed in Evans [12].
In the important case of p = 2, we write Hk(U) = Wk,2(U). This space is a
Hilbert space, in that it has an inner product and is complete (with respect to the
norm defined by the inner product; see Appendix B). The inner product (·,·) on
Hk(U) is given by
and the norm is 
. We generally work with H1(U), for which the
inner product is
Moreover, we use the space 
 with the same H1 norm.
Sobolev spaces are the natural environment in which to study general elliptic
and parabolic PDE. In the next chapter we give a flavor of the theory of elliptic
equations. A more extensive introduction to the subject is given in Evans [12].
PROBLEMS
1. Let X be a vector space with norm || · ||.
(a) Prove that for all u, v ∈ X,
(b) Show that the function f : X → R given by f(u) = ||u|| is continuous but
not linear.
2. Prove Young’s inequality.
3. Use Hölder’s inequality to prove the following.
(a) The generalization of Hölder’s inequality to three functions u ∈ Lp(U), v ∈
Lq(U), w ∈ Lr(U), with p−1 + q−1 + r−1 = 1:

(b) If p ≤ q ≤ r and 1/q = λ/p + (1 − λ)/r, then for u ∈ Lr(U),
4. The partial derivative in (10.4) is not a function of r alone, so the calculation
we did in Example 1 does not apply directly. Using coordinates x = rω, |ω| = 1,
do the integral that completes the argument to characterize when u ∈ W1,p(U).
5. Let u(α)(x) = |x|(sin |x|)α, x ∈ R3. Find the precise range of α ∈ R in which u(α)
∈ H1(B), where B = {x ∈ R3, |x| < 1}.

CHAPTER ELEVEN
Elliptic Theory with Sobolev Spaces
We use Poisson’s equation as a starting point to prove the existence of solutions
of a boundary value problem in an appropriate Sobolev space. Then we show
how a similar approach can be used for general linear second-order elliptic PDE.
The structure of the more general results and their proofs provides insight into
the techniques at the heart of the modern theory of elliptic PDE [12].
11.1. Poisson’s Equation
Our previous approach to Poisson’s equation involved finding the Green’s
function. Again we let U ⊂ Rn be open and bounded, let f ∈ L2(U), and consider
the boundary value problem
The approach of finding the Green’s function explicitly works only for special
choices of U. Instead of relying on the shape of U, we can use functional analysis
to establish the existence of a solution indirectly. The remainder of this section is
devoted to using this approach to prove the following existence and uniqueness
theorem.
Theorem 11.1. For each f ∈ L2(U) there is a unique weak solution of (11.1).
In this statement, note that f ∈ L2 is no longer required to be smooth.
Moreover, the theorem refers to weak solutions rather than classical solutions. A
weak solution of (11.1) is a function 
 such that
for all 
 satisfies (11.1), we say that u is a classical solution
of (11.1). In this case, u satisfies (11.2) for every 
, as is easily checked by
integration by parts. Since functions in 
 are approximated in the H1 norm by
smooth functions (see Section 10.3), so (11.2) makes sense as a definition of
weak solution.
Weak solutions have the advantage that they require less regularity (one weak
derivative rather than two classical derivatives), and moreover, we can look at
weak solutions in a space of functions that has an inner product and is complete,
which is not possible with smooth functions.

Since the proof of Theorem 11.1 involves several subsidiary results, we
outline the steps that make up the proof. The theorem establishes both existence
and uniqueness of weak solutions of (11.1) satisfying (11.2). In this section we
prove this result using the Riesz Representation Theorem. We first identify the
left-hand side of (11.2) as an inner product in 
, thereby defining an
equivalent norm for 
. Second we verify that the right-hand side defines a
bounded linear functional on 
. We accomplish both using the Poincaré
inequality. Then the conclusion of the Riesz Representation theorem is the
existence and uniqueness of 
 satisfying (11.2). This completes the proof
of the theorem.
11.1.1. The Poincaré Inequality
The Poincaré inequality is an example of an estimate in a function space that
allows a norm of a function to be estimated by the norm of a derivative of the
function. The specific estimate in the Poincaré inequality bounds the L2 norm of
an 
 function by the L2 norm of its derivative, a crucial step in establishing the
left-hand side of (11.2) as an inner product.
Lemma 11.2. (The Poincaré inequality) Let U ⊂ Rn be open and bounded. There
exists a constant C, depending only on U, such that
for all 
.
Proof. We use the approximation idea here for the first time. We prove the
lemma for 
 and then argue by taking limits of sequences that it holds for 
. The proof is simple in one dimension and is very similar in Rn. Let U =
(a, b) ⊂ R, and suppose 
. Then u(a) = 0 = u(b). Now we calculate
Dividing by ||u||L2(a,b) completes the proof in one dimension.

Figure 11.1. The domain U bounded by two hyperplanes.
In higher dimensions, we simply integrate by parts in one of the variables,
say, x1. So, suppose U lies between the hyperplanes {x1 = ±M}, for some M > 0,
as illustrated in Figure 11.1. (This suggests that U could be unbounded, and
indeed the Poincaré inequality is often stated for domains bounded in some
direction, but not necessarily bounded in Rn.) Mimicking the calculation above,
let 
. Then
The second inequality relies on the Cauchy-Schwartz inequality with C = 2M.
This proves the inequality for 
.
Now let 
. (Such a sequence exists from the definition
of 
 as the completion of 
 in the H1 norm.) Then ||um||L2(U) → ||u||L2(U),
and ||Dum||L2(U) → ||Du||L2(U), from which the lemma follows.
11.1.2. An Equivalent Norm on 
We define the new norm

where the subscript 1, 2 refers to one derivative of u in Lp with p = 2. The new
norm is useful because the weak formulation (11.2) only involves the gradient on
the left-hand side, which will now be the inner product corresponding to this
norm. For bounded U and 
, the Poincaré inequality implies the new norm
is equivalent to the 
 norm:
The latter inequality involves a small amount of manipulation:
Because the norms are equivalent, we conclude that 
 is a Hilbert space with
the new inner product given by the left-hand side of (11.2).
To apply the Riesz Representation Theorem (Section 11.1.3), we need to show
that f ∈ L2(U) defines a bounded linear functional on 
. This allows us to
equate the right-hand side of the weak formulation (11.2) with a bounded linear
functional.
Lemma 11.3. Let U ⊂ Rn be open and bounded, and let f ∈ L2(U). Then the linear
functional 
, defined by
is bounded, meaning there is a constant K > 0 such that
Proof. Let 
. Then
Hence F is bounded.
11.1.3. The Riesz Representation Theorem

To complete the proof of Theorem 11.1, we show that for each f ∈ L2(U) there is
a unique 
 such that (11.2) holds; that is, (u, v)1,2 = F(v) for every 
. This follows from the following Riesz Representation Theorem for a
general Hilbert space X with inner product (·, ·). We only consider Hilbert spaces
over the reals, but the result can also be stated for complex Hilbert spaces.
Theorem 11.4. (Riesz Representation Theorem) Let X be a Hilbert space, and let F :
X → R be a bounded linear functional on X. Then there exists a unique u ∈ X such
that
for all v ∈ X.
Proof. The proof involves the null space N of F :
First, let’s dispense with the trivial case, in which N = X. Then only u = 0
satisfies (11.3).
Now we consider N ≠ X. In this case, (11.3) implies u ∉ N. To see this,
suppose u ∈ N, and let v = u in (11.3). Then 0 = F(u) = (u, u), which implies u
= 0. But then (11.3) fails for any v ∉N. Thus, we seek u ∉ N.
We complete the proof in essentially three steps. First, we show there is a
nonzero z ∈ X that is orthogonal to N: (z, v) = 0 for all v ∈ N. Put another way,
F(v) = (z, v) for all v ∈ N, so z is the correct choice for u, except that if u = z,
then (11.3) will not be satisfied in general for v in the complement of N. In the
second step, we adjust z to get the correct u by noting that the complement of N
is one dimensional (so N has codimension one). This follows since the range of F
is one dimensional. Thus, the orthogonal complement N⊥ of N is one dimensional
and hence is spanned by z. Therefore, F(αz) must range through all of R as α
ranges through R, since F(z) ≠ 0. So, we should be able to find u to satisfy (11.3)
by scaling z: u = αz, for some α ∈ R. In fact, setting v = u = αz in (11.3), we
can calculate what α must be for this special case, leading to
Then F(u) = ||u||2. In the third step of the proof, we show that (11.3) holds for
this choice of u.
To find z ∈ N⊥, we use standard functional analysis arguments. First we show
that N is closed. Suppose {un} ⊂ N and un → u as n → ∞. N is closed if u ∈ N. But

as n → ∞, using linearity and then boundedness of F. Therefore, F(u) = 0,
meaning u ∈ N.
Since N is not the entire space, there is x ∈ X such that x ∉ N. But N is closed,
so the distance from x to N (defined in problem 12, Chapter 9) is positive:
We now prove that there is w ∈ N such that dist(x, N) = ||x − w||. Indeed, there
is a sequence {wn} ⊂ N such that ||x − wn|| → dist(x, N). It takes a tricky
calculation to show that {wn} is a Cauchy sequence. First, we appeal to the
parallelogram law
which is verified by expressing the norms in terms of the inner product. Now we
set 
 in the parallelogram law:
As m, n → ∞, the left-hand side of this identity approaches 2d2. However, the
first term on the right is no smaller than 2d2, and so the second term must
approach zero: ||wn − wm|| → 0 as m, n → ∞. Hence {wn} is a Cauchy sequence,
and is therefore convergent to an element w ∈ N, since X is complete and N is
closed. Moreover, ||x − w|| ≤ ||x − wn|| + ||wn − w|| → d as n → ∞. Thus, ||x
− w|| ≤ d, which implies ||x − w|| = d, since w ∈ N, and d = infy∈N ||x − y||.
Now let z = x − w. We show that z ∈ N⊥. Let y ∈ N. Then for any λ ∈ R, w
+ λy ∈ N, so
If we choose λ = (z, y)/||y||2, then the inequality becomes
which implies (z, y) = 0. Since y ∈ N is arbitrary, we have z ∈ N⊥.
Next we prove that u given by (11.4) satisfies (11.3). Since u ∉ N, we have
F(u) ≠ 0. Moreover, using linearity of F, we observe that
Thus, since u ∈ N⊥,

But this leads immediately to (11.3), since u was constructed to satisfy F(u) =
||u||2.
It remains to prove uniqueness of u ∈ X satisfying (11.3). Suppose there are
two values of u, say u = uk, k = 1, 2, both of which satisfy (11.3). Then (uk, v) =
F(v), k = 1, 2, from which we get (u1 − u2, v) = 0 for all v ∈ X. Hence u1 = u2.
This completes the proof of the Riesz Representation Theorem.
The proof of Theorem 11.1 is now complete.
11.2. Linear Second-Order Elliptic Equations
In this section we prove Theorem 11.1 for more general second-order linear
elliptic partial differential equations. First we frame the boundary value problems
that we will consider. Let U ⊂ Rn be open and bounded, and let 
. Define
Note that we suppress the independent variable in u, but retain it in the
coefficients to emphasize that they are functions of x. We will see that L is
associated with a symmetric operator in which the leading-order (i.e., second-
order) terms are written in divergence form. A more general form of a linear
second-order linear differential operator (with leading-order terms in divergence
form), is given by
which is nonsymmetric when the coefficients bi are not all zero.
The coefficients aij, c are given L∞ functions on U. Without loss of generality,
we can assume that the aijs are symmetric in ij: aij = aji. We place additional
conditions on the coefficients as we go along. Generalizing problem (11.1), we let
f ∈ L2(U) and consider the boundary value problem
To define weak solutions, we multiply Lu = f by a test function and integrate by
parts over U. Thus, a weak solution of (11.7) is a function 
 such that

where
As in Poisson’s equation, boundary terms from integration by parts are all zero,
due to the choice of u = 0 as the boundary condition. Note that in the symmetric
case bi(x) ≡ 0, i = 1, …, n, the middle term is absent, and indeed then B is
symmetric:
In the symmetric case, treated in Section 11.2.1, we relate B[u, v]to the inner
product on 
, and to do so, we require L to be elliptic. In fact, we shall
require L to be uniformly elliptic, meaning that there is θ > 0 such that
for all x ∈ U, ξ ∈ Rn.
11.2.1. Existence and Uniqueness of Solutions in the Symmetric Case
Here we give the existence and uniqueness theorem for solutions of problem
(11.7) when L is symmetric. An immediate difficulty is that the zeroth-order term
c(x)u is not controlled, in the sense that unless we make assumptions about c(x),
the corresponding term in the bilinear functional (11.9) is not bounded. An
analogous issue arises in the finite-dimensional case, where the operators are
square matrices. Consider a symmetric matrix A with positive eigenvalues, say, λ
≥ θ > 0, and a given c ∈ R. Then we consider the equation
for a given vector F, where L = A + cI. (I is the indentity matrix.) To obtain
existence and uniqueness, we require that c not be an eigenvalue of −A. This
would be guaranteed if we know c > 0. For the PDE case, we state this slightly
differently, to accommodate the dependence of coefficients on x ∈ U. In terms of
the matrix problem, we require that there exists a γ > 0 such that for all μ > γ
and any F, there is a unique solution of the equation
Of course, this is merely the condition that c + γ be larger than the largest
eigenvalue of −A.

In the PDE version, to make the bilinear functional nonnegative when v = u,
we add a large enough number μ to c(x) so that the final term in (11.9)
resembles a weighted L2 norm when v = u. Effectively this guarantees that μ is
away from eigenvalues of the PDE operator L.
Theorem 11.5. Let L be the symmetric operator given by (11.5). There exists γ ∈ R
such that for all μ > γ and any f ∈ L2(U), there is a unique weak solution 
 of
the problem
To prove this theorem, we will use the Riesz Representation Theorem, but to
establish a suitable inner product in terms of B[u, v], we need two key estimates
provided by the following lemma. One estimate establishes that B is bounded,
and the second estimate allows us to modify B[u, v] in order to define a norm.
Lemma 11.6. There are constants α > 0, β > 0, γ ≥ 0 such that for all u, v in 
,
1. 
, and
2. 
.
Proof of Lemma 11.6. To prove estimate 1, we work directly with B:
For estimate 2, we use ellipticity, replacing ξi by uxi in the definition (11.10).
First note that the Poincaré inequality (Lemma 11.2) implies
with K = 1 + C. Then we have

This proves the lemma, with γ = ||c||L∞(U), β = θ/K.
Proof of Theorem 11.5. We modify B[u, v] in Lemma 11.6 to be
Then Bμ is symmetric and satisfies
Thus, for μ > γ, Bμ[u, u]≥ 0, and Bμ[u, u]= 0 only for u = 0. Consequently,
Bμ[u, v]defines an inner product on 
. The Riesz Representation Theorem
(Theorem 11.4) completes the proof.
11.2.2. The Nonsymmetric Case
When L is nonsymmetric, B[u, v] cannot define an inner product, because it is not
symmetric in u, v. Nonetheless, a natural generalization of the Riesz
Representation Theorem can be formulated to cover the situation. That is, the
functional F is represented by an element 
 as before, and there is a
unique u satisfying (11.8). The proof of this generalization, the Lax-Milgram
Theorem, uses the Riesz Representation Theorem in a different, more subtle way.
To overcome the lack of symmetry in B[u, v], we start by fixing u so that B[u, v]
defines a bounded linear functional, as a function of v. Then the Riesz
Representation Theorem gives an element w depending linearly on u so that B[u,
v] = (w, v) for all v. The subtle part of the proof involves showing that u can be
varied so that w = f, the representative of the functional F(v) used in the
symmetric case. That is, we need to show that the linear mapping u → w is onto;
uniqueness follows by showing it is one-to-one.
Theorem 11.7. (Lax-Milgram Theorem) Let H be a Hilbert space with inner product
(·, ·) and norm || · ||. Let B : H × H → R be a bilinear functional such that the
following properties hold
1. B is bounded, meaning |B[u, v]| ≤ α||u||||v|| for all u, v ∈ H (for some constant α
> 0); and

2. there exists β > 0 such that B[u, u]≥ β||u||2 for all u ∈ H.
Then for any bounded linear functional F : H → R, there exists a unique u ∈ H such
that
Proof. Let u ∈ H. Then v ↦ B[u, v]defines a bounded linear functional on H, by
property 1 of the theorem. Therefore, by the Riesz Representation Theorem, there
exists a unique w ∈ H such that
Let’s define the mapping A : H → H by Au = w. Then
Here is how the rest of the argument goes. Since F is a bounded linear functional,
the Riesz Representation Theorem implies there is a representative w ∈ H for F :
Suppose we can show that w is in the range of A. Then there is a u ∈ H such that
Au = w. But this implies F(v) = (w, v) = (Au, v) = B[u, v]for all v ∈ H, which
completes the proof of existence. Uniqueness follows if we show that A is one-to-
one. Of course, we have no control over w, so to show it is in the range of A, we
must show that the range of A is all of H. We do this in several steps.
First, note that A is linear. It is also easy to see that property 1 implies that A
is bounded:
Dividing by ||Aw||, we obtain ||Aw|| ≤ α||w|| for all w ∈ H.
Next we prove that A is one-to-one. From property 2, we have
Now divide by ||u|| to obtain ||Au|| ≥ β||u||. Suppose Au1 = Au2. Then A(u1 −
u2) = 0. Thus,
Hence, u1 − u2 = 0. This proves that A is one-to-one.
Next we show that A is onto H. The key is to show that the range R(A) of A is
closed: it contains the limits of all Cauchy sequences in R(A).
Let {un} be a Cauchy sequence in R(A). Since H is complete, the sequence
converges to some u ∈ H. Now un = Awn for some wn ∈ H, for each n. We need to

show that {wn} is a Cauchy sequence, since then it converges to an element w,
and we use boundedness of A to show that Aw = u. But we have ||un − um|| =
||A(wn − wm)|| ≥ ||wn − wm||. Since {un} is a Cauchy sequence, so is {wn}. Let w
= limn → ∞ wn. Then
Thus, u ∈ R(A), proving that R(A) is closed.
Now we know from the proof of the Riesz Representation Theorem that if
R(A) ≠ H, then since R(A) is closed, there is u ∈ R(A)⊥ with u ≠ 0. But then
which implies u = 0. Thus, R(A) = H.
We use the construction and properties of A to prove the existence of u ∈ H
satisfying (11.11). Let F : H → R be a bounded linear functional. By the Riesz
Representation Theorem, there exists a unique w ∈ H such that F(v) = (w, v) for
all v ∈ H. But we have just gone to a lot of trouble to prove that every element of
H is also in R(A). In particular, there is a unique u ∈ H such that Au = w. Putting
this all together, for any v ∈ H,
as required.
Uniqueness of u satisfying (11.11) follows naturally. Let u1, u2 be two values
of u satisfying (11.11). Then
for all v ∈ H. In particular, letting v = u1 − u2, we obtain 0 = B[v, v]≥ β||v||2.
Thus, v = u1 − u2 = 0. This completes the proof of the Lax-Milgram Theorem.
Now, to turn the Lax-Milgram Theorem into an existence and uniqueness
theorem for elliptic equations, we have to verify the hypotheses of the theorem
when B is the bilinear functional associated with the elliptic partial differential
operator. This is only interesting in the nonsymmetric case, since the Riesz
Representation Theorem covers the symmetric case. But in the nonsymmetric
case, we have to work a bit harder to prove property 2 of the Lax-Milgram
Theorem. Specifically, we wish to prove the two estimates 1 and 2 of Lemma
11.6, but this time for B given by (11.9) with bi not identically zero. Estimate 1 is
straightforward; we leave it as an exercise. Estimate 2, however, requires a bit
more ingenuity. Let’s proceed much as we did in Lemma 11.6, using ellipticity to
establish

The difference is that now we need to estimate the middle term. This is achieved
with Young’s inequality:
Now the first term is incorporated into the left-hand side of (11.12); this will
change θ > 0, but keep θ positive provided we choose ϵ > 0 small enough.
Similarly, the second term in (11.13) is incorporated into the final term in
(11.12). After some manipulation of the constants, estimate 2 of Lemma 11.6 is
proved. These properties are the key to proving Theorem 11.5 for the general
case, which we now state.
Theorem 11.8. Let L be the PDE operator given by (11.6). There exists γ ∈ R such
that for all μ > γ and any f ∈ L2(U), there is a unique weak solution 
 of the
problem
PROBLEMS
1. Let U ⊂ Rn be a bounded open set, and let u(x) = 1, x ∈ U. Prove that u ∈
H1(U), but 
. (Hint: Use proof by contradiction and Poincaré’s inequality.)
2. Let A(x) = (aij(x)) be the matrix of coefficients of the principal part of a linear
second-order elliptic partial differential operator L. Prove that if L is uniformly
elliptic on U, with parameter θ given in (11.10), then for each x ∈ U, the
eigenvalues λ of A(x) are bounded below by θ: λ ≥ θ.
3. Consider the ordinary differential operator Lu(x) = u″(x) + cu(x). Then, with
U = (0, 1), we should be able to solve
for large enough μ. Find γ > 0 for which Theorem 11.8 holds true by finding the

eigenvalues of 
.
4. Let u = u(x, y), Lu = −xuxx + (2 + y)uxy − 2uyy. Characterize the region in
R2 in which L is uniformly elliptic.
5. Find the smallest c ∈ R for which L given by Lu = −(xuxx + uxy + uyy) is
uniformly elliptic on the set {(x, y) ∈ R2 : x > c + ϵ} for every ϵ > 0.
6. Expand the identity ||u + v||2 = (u + v, u + v). Then use the triangle
inequality to prove the Cauchy-Schwarz inequality (u, v) ≤ ||u||||v||.

CHAPTER TWELVE
Traveling Wave Solutions of PDE
We have seen in earlier chapters how the method of separation of variables can
reduce PDE to ODE. This technique works most effectively on linear PDE. In this
chapter we describe the analysis of traveling wave solutions of nonlinear PDE,
which involves ODE.
There is an overall pattern to the technique of this chapter. For each PDE with
an unknown function u(x, t), −∞ < x < ∞, we consider traveling wave
solutions 
, in which the parameter s is the wave speed, to be
determined in the course of the analysis. Substituting 
 into the PDE yields an
autonomous ODE for 
, ξ = x − st. The ODE will have equilibria, and
traveling waves correspond to solutions that connect those equilibria, either to
one another or to themselves, because the solutions 
 that we seek approach an
equilibrium as ξ → ±∞.
Each section of the chapter is devoted to the analysis of traveling waves for a
different equation. We begin in Section 12.1 with Burgers’ equation, for which
the nonlinear analysis is simplest. Burgers’ equation is central to the study of
nonlinear convection-diffusion equations. In Section 12.2 we consider the KdV
equation, a third-order equation famous for having solitary wave solutions, which
are traveling waves that we calculate explicitly. Section 12.3 is devoted to
Fisher’s equation, a model for population growth and dispersal, and in Section
12.4 we consider special traveling waves for the bistable equation, which has
connections to binary mixtures in material science and complex fluids.
12.1. Burgers’ Equation
Burgers’ equation
where ϵ > 0 is a constant, is a prototypical equation that includes a nonlinear
transport term, and small dissipation. Later we solve the initial value problem for
Burgers’ equation using a special change of variable known as the Cole-Hopf
transformation, but here we consider only traveling wave solutions u(x, t) = 
 with speed s (to be determined) that connect constants u+ and u−.
Dropping the tilde, the function u(ξ), ξ = (x − st)/ϵ should satisfy boundary
conditions at infinity:

Substituting u = u(ξ), ξ = (x − st)/ϵ, into (12.1), we inevitably reduce the
PDE to a second-order ODE, but also the parameter ϵ cancels, which is why we
scaled x and t by ϵ. Integrating the ODE from ξ = −∞ (note that 
), we
obtain the first-order autonomous ODE
From (12.2), (12.3), we see that u± are equilibria of (12.3). Thus, 
 s(u+
− u−) = 0, from which we deduce that either u+ = u− or the parameters s, u±
are related by
Of course, u ≡ u− solves (12.3) for all s but satisfies the boundary conditions
only if u+ = u−. This would be a very uninteresting traveling wave! However,
the constant solution u ≡ u− is the only solution satisfying the ODE and
boundary conditions if u− = u+. (See problem 1.) For the KdV equation of the
next section, the corresponding statement would be incorrect.
Equation (12.3) and its solutions can be represented on the u-axis (a one-
dimensional phase portrait) or using direction fields in the (ξ, u) plane. As ϵ →
0+, the traveling wave solutions converge to a discontinuous function that is in
fact a shock wave solution of the inviscid Burgers equation. Moreover, as is
immediately evident from the one-dimensional phase portrait, solutions exist if
and only if
which relates the wave speed s to the characteristic speed u+ and u− ahead of
and behind the shock wave, respectively. We leave the derivation of this
condition as a part of Problem 1.
12.2. The Korteweg-deVries Equation
The KdV equation
is famous because of the existence of solitary waves, which are special traveling
wave solutions with a remarkable property. As a faster wave catches up to a
slower one, the waves merge, and then the faster wave emerges ahead of the
slower wave, unchanged except that it is shifted forward in space from where it
might have been if the interaction had been through linear superposition. The

slower wave is shifted backward. This property is remarkable because the
equation is nonlinear, so you might expect an even more complicated interaction
between colliding waves.
The KdV equation also has the special property of possessing an infinite
number of invariants. These are spatial integrals that depend on the solution and
are constant in time. The first two invariants for the KdV equation are associated
with momentum and kinetic energy, in which u is interpreted as a velocity:
We leave it to problem 2 to verify that these quantities are invariants in the sense
that they are constant in time if u(x, t) is a solution that decays sufficiently
rapidly as x → ±∞. This property of the KdV equation is related to the inverse
scattering transform in which the solution u(x, t) serves to scatter waves through
a linear equation with coefficients depending on u(x, t). An accessible
introduction to this topic can be found in the classic text by Whitham [46].
In this section we find the solitary waves and other periodic traveling waves,
but first let’s briefly discuss the dispersive nature of the equation. If we linearize
(12.5) about a constant, say, u = 1, we get the linearized KdV equation
This is achieved by setting u(x, t) = 1 + ϵv(x, t) and retaining only terms that
are linear in ϵ. Solutions of (12.7) can be found by separation of variables. We
seek solutions with a single spatial Fourier mode eiζx and time-dependent
coefficient φ(t): v(x, t) = φ(t)eiζx. The parameter ζ is the wave number and is
related to the period L by ζ = 2π/L, since eiθ is 2π periodic in θ. Substituting into
(12.7), we get the ODE φ′ + iζφ − γ iζ3φ = 0. Consequently, 
.
Thus, we can write v(x, t) in the form
where λ = λ(ζ) measures the time-dependent response at the wave number ζ;
λ(ζ) is given by the dispersion relation
The first term (−iζ) corresponds to a traveling wave solution of the linear
transport equation (with speed c = 1) of Chapter 1, in which γ = 0. The second
term is dispersive in the sense that the solution 
 is a
traveling wave, but it has speed 1 − γ ζ2 that depends on the wave number.
Thus, waves with different spatial frequency travel with different speeds.

To characterize solitary waves, we consider traveling wave solutions of (12.5)
of the form u(x, t) = v(x − st), where s is the wave speed, and we suppose that
v(ξ), ξ = x − st approaches zero at ξ = ±∞:
Figure 12.1. The phase plane for (12.8).
Substituting into the PDE and integrating once (assuming that derivatives of v
also approach zero as z → ±∞), we obtain the second-order ODE
If we multiply this equation by v′ and integrate, we find that the quantity
is constant on solutions v(ξ). Moreover, if we define this quantity to be the
Hamiltonian:
then, writing H = H(p, q), p = v′, q = v, we have p′ = −Hq, q′ = Hp, which is
the Hamiltonian structure of (12.8). As observed above, the Hamiltonian is
conserved along trajectories:
The reader should verify these statements using (12.8) and (12.9). It follows that
trajectories in the phase plane are level curves of H(v′, v), shown in Figure 12.1
for s > 0. In this figure, the closed curves in the right half-plane correspond to

periodic solutions, called cnoidal waves. These are periodic traveling waves that
are important in the study of dispersive equations like the KdV equation. They
are named after the cnoidal function, a special elliptic function that gives the
shape of the solutions as functions of ξ = x − st.
The trajectory in Figure 12.1 joining the origin to itself and enclosing the
periodic trajectories is called a homoclinic orbit. The corresponding traveling wave
is the solitary wave. We can derive the formula for this solution by integrating the
equation, but since the solution is known, let’s simply write it down:
Because the speed s is proportional to the amplitude a, solitary waves with larger
amplitude move faster. Thus, if a large solitary wave is behind a smaller one, the
large one will catch up. This leads to the nonlinear interaction of two solitary
waves mentioned earlier in this section. Solitary waves and the connection to
integral invariants and inverse scattering are discussed in papers by Gardner et
al. [17] and by Miura [36].
12.3. Fisher’s Equation
Fisher’s equation [14] is a version of the logistic equation that includes diffusion
(see Section 1.3). While population growth can be modeled by the logistic
equation (either the ODE or a difference equation), populations in many contexts
tend to spread out or even invade nearby territory. This spatial dependence of
population is modeled simply in Fisher’s equation by diffusion. In this context,
diffusion arises because of the tendency of a population u = u(x, y, t), to migrate.
According to Fick’s law, the population flux is proportional to −∇u. (Note that
Fick’s law is similar to Fourier’s law of heat flow.) Combining diffusion with the
logistic population model, we obtain the PDE
in which the constants d, α, umax are all positive. Here, the Laplacian Δ = 
,
and the population u = u(x, y, t) is nonnegative.
We seek traveling wave solutions u = u(x − st). These are plane waves in the
sense that u depends spatially only on x and is independent of the transverse
variable y. Thus, level curves of u at each time t are lines parallel to the y-axis,
propagating (in the direction of the x-axis) with speed s. Since u is typically a
population or concentration, we want u ≥ 0, but we pursue the analysis initially
without this restriction.

Let d = α = umax = 1 in Fisher’s equation. (This covers the general case as
shown in problem 3 as an exercise in rescaling the variables.) Then taking u =
u(x, t) to be independent of y, we have the semilinear parabolic equation in one
space variable x and time t:
For traveling wave solutions, we substitute u = u(x − st), resulting in the ODE
This time, we cannot integrate the equation to reduce the order. However, just as
for traveling wave solutions of the KdV equation, this second-order ODE can be
studied as a first-order system
If we set s = 0, the traveling wave does not travel at all, and ξ = x. These
stationary solutions are oscillations in a potential well 
 (the integral
of u(1 − u)) with a minimum at u = 0. The corresponding spatially periodic
solutions of (12.11) are cnoidal waves, just as for the KdV equation. Notice how
similar the Hamiltonian 
 is to the Hamiltonian for the KdV
traveling waves. The phase portrait for s = 0 is likewise a mirror image of Figure
12.1. The saddle point is at (u, u′) = (1, 0), and the limit of the periodic solutions
around the center at the origin is a wave that approaches u = 1 as x → ±∞,
with a single negative minimum. Of course, none of these solutions is physical if
u represents a population, since they all have u < 0 in an interval.
To find traveling waves with u > 0, we are led to investigate nonzero values
of s. Equilibria (for which u′ = v′ = 0) are (u, v) = (0, 0) and (u, v) = (1, 0),
and are thus independent of s. Their nature is determined from the linearization,
in which we take the Jacobian of the vector field G(u, v) = (v, −sv − u(1 − u))
and calculate eigenvalues. For s > 0, this calculation shows that (u, v) = (1, 0) is
a saddle point (i.e., having real eigenvalues of opposite sign). The origin (u, v) =
(0, 0) is a stable spiral for 0 < s < 2 (complex conjugate eigenvalues 
 with negative real part), and a stable node for s > 2 (both
eigenvalues negative).
The speed s, when positive, acts as a damping parameter. From (12.12) we see
that d/dξ H(u′, u) ≤ 0, so that the energy H decreases along trajectories. By
analogy with damped simple harmonic motion, for small s > 0, we expect
oscillations to be underdamped. Correspondingly, traveling waves oscillate
around u = 0 and are therefore unphysical, but they decay to u = 0. Values of s

> 0 sufficiently large give overdamping. In fact, for s ≥ 2 the eigenvalues of the
equilibrium at u = u′ = 0 are real and negative. Traveling wave solutions that
are monotonic from u = 0 to u = 1 exist for all s ≥ 2. These are the physically
relevant traveling wave solutions of Fisher’s equation. For s > 2, this was proved
by Aronson and Weinberger [5]. The proof is quite technical in order to show
that the solution curve leaving the saddle point at (1, 0) does not cross the
vertical axis u = 0 when s > 2. Stability of the traveling wave as a solution of
the PDE (12.11) was analyzed by Fife and McLeod [13].
12.4. The Bistable Equation
The bistable equation
in which f has the graph shown in Figure 12.2, is a simple model for transitions
between two stable states. This kind of model has become common in studies of
phase transitions in solids, with applications in materials science.
The space-independent equation u′(t) = f(u) has stable equilibria at u = 0
and u = 1; the equilibrium u = a is unstable. Traveling waves between u = 0
and u = 1 are somewhat easier to analyze than for Fisher’s equation, since the
equilibria correspond to saddle points and the issue of keeping the solution
positive does not arise. However, the traveling waves occur only at isolated
speeds s. Traveling wave solutions u(x, t) = v(x − st) of (12.13) satisfy the
second-order equation
As in the previous sections, we analyze this second-order equation by writing it
as a first-order system:
The three zeroes 0 < a < 1 of f are equilibria of the ODE; the outside equilibria
at u = 0, u = 1 are saddle points, and the middle equilibrium at u = a is a node
for s ≠ 0, as is seen from the eigenvalues of the linearization, calculated below.
For s = 0, the middle zero is a center, as shown in Figure 12.3a.
Let G(v, w) = (w, −sw − f(v)). The vector field G has Jacobian (see Appendix
C for the definition)

Figure 12.2. The bistable function (12.13).
Figure 12.3. The phase plane for traveling wave solutions of the bistable
equation, assuming (12.16). (a) s = 0; (b) s < 0 near 0; (c) smaller s < 0.
At the outside equilibria, f′(v) < 0, and the eigenvalues of dG are real, given by

The corresponding eigenvectors are
The unstable manifold MU is the solution curve leaving the saddle point, and
the stable manifold MS is the solution curve entering the saddle point. The
portions of these curves that we consider are labeled in Figure 12.3. Note that in
Figure 12.3a, just as for the KdV equation traveling waves, the saddle point at the
origin has coincident stable and unstable manifolds, giving the homoclinic orbit
shown. In Figures 12.3b,c there are orbits from the middle equilibrium at v = a
to the origin and to the equilibrium at v = 1. These orbits correspond to
traveling waves approximating Lax shocks, for which the Lax entropy condition is
satisfied; see Chapter 13, Section 13.1.4.
The tangents to the invariant manifolds at the equilibria are the eigenvectors r
± of the linearization of (12.15). (See Appendix C.) The solutions represented by
the invariant manifolds are asymptotic to eλ±ξ r± as ξ → ∓∞, so that the
eigenvectors are tangent to the invariant manifolds at the equilibria. Therefore,
each invariant manifold has the same slope as its associated eigenvector,
specifically, λ±.
Our objective is to find a value of the speed s for which a trajectory joins the
two saddle points. Such a trajectory will correspond to a traveling wave from u
= 0 to u = 1 and is called a heteroclinic orbit (as opposed to the homoclinic
orbits we found for solitary wave solutions of the KdV equation). Since we are
joining saddle points in the phase portrait, we can expect heteroclinic orbits to
occur only for isolated values of s. We prove an existence result for heteroclinic
orbits, showing that there is a value of s for which there is a heteroclinic orbit.
When f(u) is given by a cubic function f(u) = u(1 − u)(u − a), the value of s and
the heteroclinic orbit can be calculated explicitly.
Multiplying (12.14) by v′, we obtain the identity
where 
, and 
. Thus, E(v, v′) increases along
trajectories if s < 0, decreases if s > 0, and is constant for stationary waves (s =
0). The phase portrait for s = 0 is the starting point for our analysis; trajectories
are level curves of E(v, v′), shown in Figure 12.3. In this figure we assume

to ensure that the curve E(v, v′) = 0 crosses the v-axis at a point (v0, 0) with a <
v0 < 1, as shown in the figure. Consequently, there is a homoclinic orbit
connecting the origin to itself, as shown in Figure 12.3a.
The heteroclinic orbits we seek join the saddle point at the origin to the
saddle point at (1, 0). We prove the existence of such an orbit for some s < 0
using a shooting argument on the stable and unstable manifolds shown in Figure
12.3.
Theorem 12.1. Suppose f satisfies (12.16). Then there is a speed s < 0 for which the
bistable equation (12.13) has a traveling wave solution u = u(x − st) satisfying
u(−∞) = 0, u(∞) = 1.
Proof. As we vary s ≤ 0 in the ODE system (12.15), the stable and unstable
manifolds move, and we wish to show that for some s < 0, the unstable manifold
from the origin coincides with the stable manifold entering (1, 0). To do so, we
employ a kind of shooting argument, in which the stable and unstable manifolds
shoot inward toward the line L : v = a, and we measure the height w of the
intersections. As shown in Figure 12.3bc, let point P : (v, w) = (a, p(s)) denote
the intersection of MU with L, and let point Q : (v, w) = (a, q(s)) denote the
intersection of MS with L. It is straightforward to argue that the intersections with
L are defined for all s < 0, for example, by considering the curve w = −f(v)/s,
on which w′ = 0 (see (12.15)), so that the trajectories have a maximum or
minimum where they intersect this curve.
For s = 0 (see Fig. 12.3a), P is below Q : p(0) < q(0), due to (12.16).
Therefore, by continuity, for s < 0 near s = 0, P is still below Q: p(s) < q(s) (see
Fig. 12.3b), and we seek a smaller value of s < 0 for which P lies above Q:
as shown in Figure 12.3c. Then by continuity, an intermediate value of s < 0
exists for which the two points coincide, and there is a heteroclinic orbit along
the coinciding manifolds. This will complete the proof.
We show (12.17) for small enough s < 0 in three steps: (1) We establish that
q(s) < q(0) for all s < 0. (2) We choose β > 0 so that βa > q(0). (3) Finally we
show that for this value of β, we can choose s < 0 small enough so that p(s) >
βa. These inequalities together prove (12.17) for some small enough s < 0.
Let’s show that MS for s < 0 lies below the curve MS for s = 0. This will imply

To prove (12.18), we consider the portion of the curve MS with its graph denoted
w = ws(v), a ≤ v ≤ 1, for each s ≤ 0. Then q(s) = ws(a), and from the earlier
discussion,
Consequently, since 
, the slopes of the tangent to MS at the equilibrium
satisfy
Thus, ws(v) < w0(v), for v < 1 near v = 1. Next we show that this inequality
holds all the way to the vertical line L : v = a.
From the chain rule and (12.15), we have
Suppose as v decreases from v = 1, there is a first value of v = v∗ < 1at which
the stable manifold MS with s < 0 crosses the stable manifold MS with s = 0.
Then
the latter inequality following from ws(v) < w0(v), v∗ < v < 1.
Subtracting (12.19) with s = 0 from (12.19) with s < 0, we then have at v =
v∗,
However, the two sides of this equality have opposite signs, providing a
contradiction. Thus, MS with s < 0 lies below MS with s = 0, at least down to v
= a.
Now we consider the unstable manifold MU emanating from the origin. We
use the notation 
, 0 ≤ v ≤ a for the values of w along MU. Then 
.
Let β > 0 be chosen so that
Then, from (12.18), we have

for all s < 0.
The vector field defined by (12.15) has slope w′/v′ at each point in v-w plane.
Specifically, at each point on the line w = βv, the slope of the vector field is
Thus, since f(v)/v is bounded for 0 ≤ v ≤ a, we can choose s < 0 small enough
that this slope is greater than β. Then the trajectories cross the line w = βv from
below to above. In particular, the unstable manifold MU must lie above this line,
at least until it crosses the vertical line L, where v = a, w = p(s). Consequently,
p(s) > βa > q(s). This verifies (12.17) for sufficiently negative s and establishes
the existence of a heteroclinic orbit, as claimed. A similar argument can be used
to show there is a trajectory for some s > 0 that joins u = 1 to u = 0.
Example 1. (Traveling waves) Consider traveling waves for (12.13) with a
cubic function f(u) = u(1 − u)(u − a), with 0 < a < 1. For this special case we
can seek s ∈ R for which there is an invariant parabola w = kv(1 − v) for the
ODE system (12.15) that passes through the two saddle points at v = 0, 1. Then
the traveling wave solution will have speed s and corresponds to a trajectory
lying on the parabola.
To obtain the invariant parabola, let w = kv(1 − v). Then from (12.15),
implies
Therefore,
Thus, −2k2 = −1, which leads to 
, since we want the parabola to lie in the
upper half-plane when 0 < v < 1. Equating the constant terms leads to
Consequently, s < 0 for 
, and s > 0 for 
. The traveling wave is
stationary when 
 and s = 0. In this case, the ODE system is Hamiltonian (see
(12.14) with s = 0, which kills the damping term), with heteroclinic orbits from
(0, 0) to (1, 0) in the upper half-plane and from (1, 0) to (0, 0) in the lower half-
plane, corresponding to the parabola 
.

In this chapter we have seen how to use phase plane techniques of systems of
ODE to study smooth traveling wave solutions of several important PDE. In the
next chapter we study shock wave solutions of scalar conservation laws. Shock
waves are discontinuous traveling waves, requiring new tools for their analysis.
PROBLEMS
1. (a) Sketch the one-dimensional phase portrait for (12.2)–(12.4). This should be
the u-axis, with equilibria u± marked, and arrows indicating the sign of u′.
(b) Show that there is a nonconstant solution of (12.3) satisfying (12.2) if and
only if u+ < u−.
(c) In fact, the solution in part (b) can be found explicitly, since the right-hand
side of (12.3) is quadratic, so that separating variables u, ξ and using partial
fractions yields the solution. Find the solution in the form
by finding constants a, b, c as functions of u±.
(d) Plot or sketch the solution as a graph u = u(ξ) when u− = 2 and u+ =
−1. What is the wave speed?
2. Prove that the two integrals of (12.6) are invariants of the KdV equation
(12.5), and find η (depending on γ) so that the integral
is also an invariant.
3. Scale x, t, u in (12.10) so that the three constants d, umax, α can be set to unity.
4. Use a computer program to sketch phase portraits for the ODE for traveling
wave solutions of Fisher’s equation, for speeds s = 0, 1, 3. The phase portraits
can also be drawn by hand with the aid of the nullcline curves, where u′ = 0 (i.e.,
the u-axis, where the vector field is vertical, and hence trajectories crossing it are
too), or v′ = 0 (where the vector field is horizontal).
5. In the example of the bistable equation (12.13) with cubic f(u), determine a
formula for the traveling wave u = v(x − st), using the invariant parabolic
manifold w = kv(v − 1) found in Example 1. (Note that w = v′.)
6. Consider the KdV-Burgers equation

where α, β are positive constants.
(a) Formulate an ODE for traveling wave solutions u(x, t) = v(η), η = x − st,
with boundary conditions
(b) Prove that there is such a solution if and only if
(c) Find all values of the parameters α, β for which the traveling wave is
monotonic (i.e., find a necessary and sufficient condition for v(η) to be a
decreasing function of η).
7. Let f : R → R be a given C1 function. Show that traveling wave solutions u =
u(x − st) of the PDE
are given implicitly by the equation
where a, b are real constants.

CHAPTER THIRTEEN
Scalar Conservation Laws
The theory of scalar conservation laws was developed in the 1950s by many
people, including Kruzkov [31], Lax [32], and Oleinik [37]. The starting point is
the method of characteristics, treated in detail in Chapter 3. Recall that for a
scalar equation
the characteristic speed is f′(u), and characteristics are curves in the x-t plane
defined by the ODE
if u is a solution of the PDE. However, as we saw in Chapter 3, smooth solutions
of nonlinear equations typically develop singularities after a finite time. This is
the motivation for considering weak solutions.
We begin this chapter with a detailed analysis of the inviscid Burgers
equation, which has a quadratic flux function—the simplest convex flux. We then
sketch how the theory is generalized to scalar equations in one space dimension
with general convex flux functions. Finally, we analyze examples of equations
with nonconvex fluxes and equations with two space variables.
13.1. The Inviscid Burgers Equation
In this section we discuss the inviscid Burgers equation in some detail, motivating
the theory of scalar conservation laws, including the definition of weak solutions
of initial value problems and the analysis of shocks and rarefactions.
13.1.1. Scale Invariance and Rarefaction Waves
In the following initial value problem for the inviscid Burgers equation, the initial
condition has a jump discontinuity separating two constant values:
This kind of initial value problem, with a step function initial condition, is called
a Riemann problem. Riemann problems play a central role in the theory of weak

solutions, partly because they have the property of scale invariance in the
following sense. If we scale x and t by a constant a > 0 and define new variables
then
so the PDE is unchanged by the change of variables. The initial condition
depends only on the sign of x, so it does not change either. Thus, problem (13.1)
is said to be scale invariant.
An immediate consequence of scale invariance is the risk of multiple solutions
of problem (13.1). Let u(x, t) be a solution of (13.1). Then v(x, t; a) = u(ax, at) is
also a solution of (13.1) for any a > 0. Since we want uniqueness of solutions of
(13.1) (part of the well-posedness condition for initial value problems), we need
v(x, t; a) = u(x, t). It follows (see Problem 1) that 
 for some function
. Then u(x, t) is constant on rays x = ct through the origin in the x-t
plane. When  is continuous, this type of solution is called a centered rarefaction
wave. Later we also consider shock wave solutions of (13.1), which correspond to
discontinuous functions .
13.1.2. Centered Rarefaction Waves
Continuous, piecewise smooth (nonconstant) scale-invariant solutions u(x, t) = 
 of the PDE in (13.1) are centered rarefaction waves. Then
implies 
. One possibility for this equation would be 
, but this
leads only to the constant solution. The other alternative gives the solution we
seek:
Since the PDE is translation invariant (i.e., it does not change if x or t is translated
by a constant), rarefaction waves can be translated so that they are centered on
points (x0, t0) other than the origin:
In Figure 13.1 we show the solution of the Riemann problem (13.1) with uL =
−1, uR = 1, in which the solution is constant to the left and right of the

rarefaction wave. Continuity of the solution implies that the derivative ux is
discontinuous at the leading and trailing edges of the rarefaction, but this is
consistent with the PDE, since these points travel with characteristic speed u. A
general result concerning the propagation of discontinuities in derivatives of
solutions is provided in the next chapter (see Section 14.2.5).
Figure 13.1. A rarefaction wave solution of the inviscid Burgers equation.
13.1.3. Shock Waves
In Section 9.2.2, Example 7, we discussed discontinuous solutions of a scalar
conservation law as an application of distributions, including a derivation of the
Rankine-Hugoniot condition for constant-speed shock waves. Here we give a
more general derivation of the Rankine-Hugoniot condition. However, if all shock
waves were allowed in weak solutions, then some Riemann problems would have
multiple weak solutions for the same constants uL, uR. To rectify this lack of well-
posedness, we introduce the notion of the admissibility of shocks.
Consider a solution u(x, t) of Burgers’ equation (13.1) with a jump
discontinuity on a curve x = γ (t) with γ a C1 function. We assume that u is C1
and satisfies the PDE (13.1), except on x = γ (t). Since u has a jump
discontinuity, it has one-sided limits u±(t) = limx → γ (t)± u(x, t) = u(γ (t)±, t).
In Chapter 9, Example 7, we showed how to treat a discontinuous function
with a jump discontinuity as a distributional solution of a conservation law. We
take this up again in the next chapter on systems of conservation laws. Here we
give a more direct treatment going back to the balance law formulation to see
what it takes for u to satisfy the integral form of the equation.
Consider points x = a, x = b with a < γ (t) < b, for t in an interval, as
shown in Figure 13.2. The balance law expresses the PDE in the form

This equation can be derived much as was done for the traffic flow equation
(2.16) of Chapter 2. To evaluate the left-hand side, we break up the integral:
Figure 13.2. A shock wave x = γ (t).
Now we can carry out the differentiation to get
Next we use the PDE in the integrals:
The calculation now reduces to
Since x = γ (t) is a discontinuity for u, we have u+(t) ≠ u−(t), so
which is the Rankine-Hugoniot jump condition. The left-hand side is the speed of

the discontinuity, or shock wave; the condition states that the shock speed γ′(t) is
the average of u across the shock (i.e., γ′(t) is the average of the characteristic
speeds on each side of the shock).
Since the class of possible solutions now includes piecewise smooth functions,
we lose uniqueness of solutions for some initial value problems. This issue is
familiar from simpler contexts. For example, if we consider only real solutions of
x3 = 1, then there is a unique solution x = 1. However, when we widen the
admissible class of numbers to include complex numbers, then there are three
solutions.
In the next example we show a one-parameter family of weak solutions for
the same initial value problem. The example is suggestive of how uniqueness can
be recovered.
Example 1. (Nonunique weak solutions) Consider the Riemann problem (13.1)
with uL = −1, uR = 1:
The rarefaction wave solution is shown in Figure 13.1. However, there is also a
one-parameter family of weak solutions. In Figure 13.3 we represent one member
of this family, with a particular value of the parameter α ∈ (0, 1]. The formula
for the solution is
In these solutions notice that for α > 0, there is a stationary shock on the t-axis,
and characteristics leave the shock on both sides. Since u is constant on
characteristics, the solution adjacent to the shock is not determined from the
initial data. We say that causality fails to hold. To avoid this lack of causality, we
select the solution with α = 0, for which the solution is continuous, consisting of
the centered rarefaction wave whose graph appears in Figure 13.1, but no shock
forms.

Figure 13.3. One-parameter family of solutions of problem (13.3).
13.1.4. The Lax Entropy Condition
Example 1 shows that to have unique solutions of initial value problems, we
cannot allow all solutions with shocks that only satisfy the Rankine-Hugoniot
condition (13.2). In many circumstances a unique solution is selected if we
impose the additional condition that characteristics must impinge on a
discontinuity from both sides. This is consistent with causality, as the
characteristics carry information about the solution forward in time from initial
and boundary conditions. The condition on characteristics is known as the Lax
entropy condition. Formulated in this way, it applies to discontinuous solutions of
any scalar conservation law ut + f(u)x = 0, and it also generalizes to
discontinuities in higher dimensions.
We can write the Lax condition as a pair of inequalities. Consider a shock x =
γ (t), represented in Figure 13.3, with left and right limits u−(t) and u+(t),
respectively. Then the Lax entropy condition states that
In particular, the solution must jump down from u− to u+.
By contrast, in Example 1 the solutions u with α > 0 jump up at the
discontinuity. Thus, the only acceptable solution is the continuous one, for which
α = 0; the others have shocks that satisfy the Rankine-Hugoniot condition but
not the Lax entropy condition.
Example 2. (Solution of the Riemann problem (13.1)) We can now solve the
Riemann problem (13.1) for Burgers’ equation for all choices of uL, uR. The
solution consists of a single wave with the constants u = uL to the left of the
wave and u = uR to the right. If uL < uR, the single wave is a centered rarefaction
wave, whereas if uL > uR, then the single wave is a shock x = st, s = (uL + uR).

In the next example, we compute a shock wave as a free boundary, showing
how information from the left and right determines the location of the shock.
This property of locating the shock based on information from both sides is
another reason for the Lax condition—information flows into the shock from the
left and the right. Since the information (carried by characteristics) varies
continuously with the initial data, the Lax condition ensures that solutions vary
continuously with the data. In this sense, shocks satisfying the Lax entropy
condition are said to be stable.
Example 3. (Piecewise-constant initital data) We can use rarefactions and
shocks to solve initial value problems with piecewise-constant initial data by
solving the Riemann problem and then solving interaction problems when
individual waves collide. As an example, we consider the initial value problem
The solution structure is shown in Figure 13.4. To construct the solution step by
step, first consider t > 0 small. There will be a rarefaction centered at x = t = 0
joining regions x < 0 where u = 0 and x > t where u = 1. The constants u =
1to the left of u = 0 are joined by a shock wave originating at x = 1, t = 0 and
having constant speed . The shock wave therefore lies on the curve 
.
Now the leading edge of the rarefaction wave, specifically, the characteristic x
= t, collides with the shock wave 
, for which x = 2. The solution
is continued with a curved shock x = γ (t), since the limit from the left comes
from the rarefaction wave and is not constant, whereas the limit from the right
continues to be u = 0. The speed is related to the left and right limits by the
Rankine-Hugoniot condition, which takes the form
The shock wave is attached to the collision point, so we have the initial condition
The solution of the initial value problem for γ (t) is 
. This is the curved
portion of the shock shown in Figure 13.4. Notice that γ (t) has speed γ′(t) = 
, which approaches zero as t → ∞. Consequently, the shock strength

decreases, but the shock persists for all time.
Figure 13.4. Solution of the Riemann problem (13.4).
13.2. Scalar Conservation Laws
Much of what we have analyzed for the inviscid Burgers equation applies to the
general scalar equation in one space variable and time
in which the flux f : R → R is C2. We consider the Cauchy problem, with initial
data
For some properties, it is convenient to write the equation as a nonlinear
transport equation
where c(u) = f′(u) is the characteristic speed. We show how the construction of
rarefaction waves and shocks generalizes to (13.5).
First observe that u is constant on characteristics 
, since 
 = 0,
and that therefore characteristics x = x(t) are straight lines. Consequently, the
solution of the Cauchy problem is expressed implicitly by the equation
Using the balance-law version of (13.5), the Rankine-Hugoniot jump
condition for a discontinuous solution, such as that shown in Figure 13.2, is

where 
 is the shock speed. Notice that the shock speed is the slope of the
chord in the graph of f joining the points (u±, f(u±)), as in Figure 13.5. The figure
provides a useful interpretation when considering the Lax entropy condition,
because this slope is easily compared to the characteristic speeds c(u±) = f′(u±),
which are slopes of the tangents to the graph of f at these same points. The Lax
entropy condition states that characteristics should enter the shock from each
side. (See γ (t) in Fig. 13.4 for an example.) This means
Shocks that satisfy this condition are said to be admissible.
Centered rarefaction waves 
 are constant on characteristics and thus
are specified by the implicit equation
Since x/t is increasing from left to right in the x-t plane, this formula makes sense
only in intervals of 
 in which 
 is increasing as a function of x/t.
Therefore, 
 in an interval, which is the range of .
Figure 13.5. Shock speed s as the slope of a chord for a convex flux f.
13.2.1. Traffic Flow Model
The model for traffic flow introduced in Chapter 2 has the form
In this equation, ρ(x, t) ≥ 0 represents the traffic density at a location x on a
highway at time t, and the traffic flux is given by a function f(ρ) = ρv(ρ) in
which v = v(ρ) models how velocity v changes with traffic density ρ. The

simplest v(ρ) is linear: v(ρ) = vm(1 − ρ/ρm), where vm, ρm denote the maximum
speed and maximum traffic density, respectively. Then (13.9) has a quadratic flux
and can be converted into the inviscid Burgers equation with a simple change of
variables.
More generally, forms for v(ρ) can be based on real traffic data, and the result
need not be linear. However, v′(ρ) ≤ 0 is a reasonable assumption, and f(ρ) =
ρv(ρ) is typically a concave function: f″(ρ) < 0, implying that the characteristic
speed is a decreasing function of density ρ.
Note that the characteristic speed f′(ρ) = ρv′(ρ) + v(ρ) is positive for ρ near
zero, but it is negative near maximum density, where v(ρ) = 0. Thus, according
to the model (13.9), information about the traffic may flow forward, in the
direction of the traffic motion at low density, or backward at high density. We
can illustrate both cases with a classic example, in which vehicles approach a
traffic light.
Example 4. (Traffic light problem in the traffic flow model) A line of traffic
with uniform density  approaches a traffic light located at x = 0. Suppose you
are a traffic engineer, and your job is to design the traffic light sequence to make
the light work efficiently.
Let’s suppose the light needs to stay red for a time t1 > 0, to release traffic on
the other roads at the intersection. Then you need to find the length of time T >
0 for the green light. As we shall see, the traffic builds up behind the red light,
and then gradually releases when the light turns green. The challenge is to find
an expression for shortest time T at which the traffic density returns to 
. We
find T as a function of the parameters in the model: , ρm, vm, t1.
First note that when the light turns red, the density immediately to the right
of x = 0 is ρ = 0, and behind, 
. Subsequently, since cars are stopped at the
red light, the density immediately to the left of x = 0 is maximal, ρ = ρm.
Correspondingly, there is a stationary shock at the light, that is, one with speed
zero. Note that this makes sense, since the light presumably does not move, and
but also because it is consistent with the Rankine-Hugoniot condition.
Since there is nothing in the model that allows cars to slow as they approach
the line of stationary cars, a shock forms between the moving cars (which have
density 
) and the stationary cars (with density ρ = ρm). Consequently, the
shock forms and propagates backward as more cars join the queue waiting at the
light. The speed is given by the Rankine-Hugoniot condition

since f(ρm) = 0.
Now suppose the light turns green at time t1. This releases cars at the head of
the queue to start moving. Since there are no cars ahead of the lead car, the
model dictates that it immediately travels with maximum speed vm,
corresponding to ρ = 0. In fact, at t = t1, the solution near x = 0 constitutes a
Riemann initial value problem, with ρ = ρm, x < 0, and ρ = 0, x > 0. This
jump down gives rise to a centered rarefaction wave, as shown in Figure 13.6.
The rarefaction interacts with the backwards-propagating shock at some time t2
> t1, increasing its speed. Finally at time t3 > t2, the shock, labeled γ in the
figure, crosses x = 0, after which the traffic density is back to 
 crossing the
traffic light at x = 0. The interval T = t3 − t1 is the minimum time that should
be set for the green light to release all the vehicles that were held up by the red
light. Notice that, as in Example 3, the shock persists for all time, but the strength
decays as t gets larger.
13.2.2. Nonconvex Flux
The classic scalar conservation law with a nonconvex flux is the Buckley-Leverett
equation, formulated in the 1940s as a simple model for the flow of oil and water
in an oil reservoir. It is based on Darcy’s law relating pressure gradients in the
fluids to their velocities. After some simplification, the equation takes the form
In this equation, u = u(x, t) is the saturation of oil at a location x in the
reservoir, meaning the fraction of volume of pore space occupied by oil. Then,
assuming all the pore space is occupied by either water or oil, the saturation of
water is 1 − u. The functions λo, λw are relative permeabilities of oil and water,
respectively, depending on their saturations; they are both typically taken to be
positive, increasing, and convex functions. The positive constant vT is the total
velocity (i.e., the sum) of the two fluids. We leave it as an exercise (see Problem
3) to show that f(u) in (13.10) is positive, increasing, but nonconvex with a single
inflection point, when λo(u) = kou2 and λw(1 − u) = kw(1 − u)2, for positive
constants ko, kw.

Figure 13.6. Solution of the traffic light problem.
Rarefaction wave solutions 
 of (13.5) with a nonconvex flux f(u) are a
little tricky, because the characteristic speed 
 has to increase with x/t.
Moreover, because 
 is obtained from the equation x/t = f′(u), f′ must be
invertible. Consequently, f″ ≠ 0 throughout the rarefaction wave.
To show the role of nonconvex flux functions, we choose the simpler
nonconvex function f(u) = u3, for which we can make explicit calculations. Thus,
we consider shocks and rarefaction waves for the equation
ut + (u3)x = 0.
In the specific case f(u) = u3, the characteristic speed is f′(u) = 3u2, so that
rarefaction waves u(x, t) = 
(x/t) centered at x = t = 0 are given by 
. Thus, rarefaction waves have positive speed and join constant values
u = u±, provided u+ < u− ≤ 0 or 0 ≤ u− < u+ (where as usual u− is to the left
of the wave and u+ is to the right).
For constant u− > 0, the values of u+ < u− for which the piecewise-constant
function
is an admissible shock for some speed s require the chord from u− to u+ to lie
above the graph of f. The chord has slope s = u2
+ + u+u− + u2
− and becomes
tangent to f in two ways, corresponding to two different values of u+. When u+
= −u−/2, the chord is tangent at u+; when u+ = −2u−, the chord is tangent at
u−. By comparing the slopes 
 of the tangents at u± to the shock speed, we

observe that the Lax entropy condition (13.8) becomes −u−/2 < u+ < u− when
u− > 0.
Having described rarefaction waves and admissible shock waves, we can solve
the Riemann problem
Since the PDE is unchanged by changing the sign of u, we can take uL > 0 and
solve (13.11) for different values of uR. From the discussion of shocks and
rarefactions in Example 2, we see that the solution will be a single shock wave if
−uL/2 < uR < uL and a rarefaction wave if uR > uL. However, for uR < −uL/2, a
single wave cannot solve the problem, and we need a new construction, a shock-
rarefaction. This solution includes a shock wave from uL to −uL/2, with speed 
, the characteristic speed at u = −uL/2. But then we can join −uL/2 to uR by
a rarefaction wave, since uR < −uL/2. The solution then is a shock connected to
the rarefaction wave, hence the name shock-rarefaction. We illustrate the
solutions in Figure 13.7.
Figure 13.7. Solution of the Riemann problem (13.11). S: shock; R: rarefaction;
SR: shock-rarefaction.

13.3. The Lax Entropy Condition Revisited
The Lax entropy condition can be motivated by considering changes in entropy
across shock waves in gas dynamics. But we can also treat entropy as an abstract
quantity, a significant notion in the modern theory of conservation laws,
including the existence and properties of solutions of systems of equations. In this
section we relate the Lax entropy condition to this mathematical notion of
entropy.
Consider the conservation law
in which f : R → R is a C2 strictly convex function: f″(u) > 0 for all u ∈ R. Let η :
R → R be a C2 strictly convex function: η″(u) > 0 for all u ∈ R, called a convex
entropy function. We define an entropy flux q : R → R by
These pairs of functions are associated with an additional conservation law that is
satisfied by smooth solutions. We verify that ηt + qx = 0, using (13.12) and
(13.13):
Note that (13.13) also makes sense when η is only piecewise smooth, that is,
continuous and piecewise C1 (for example, piecewise linear). In that case, we say
η is a convex entropy function if it is a convex function in the weaker sense:
for all x < y, 0 ≤ θ ≤ 1.
The objective of this section is to relate the Lax entropy condition (13.8) for
shock waves to the inequality
To simplify the calculation of the inequality (13.14), we consider a piecewise-
constant shock wave solution of (13.12):
Here the speed

and the solution on each side of the shock are constant.
Recall that for a convex flux f(u), the Lax entropy condition
is equivalent to requiring that the shock jump down:
Inequality (13.14) is to be interpreted in the sense of distributions:
for all test functions ϕ ∈ C∞(R × R) with compact support, satisfying ϕ(x, t) ≥
0. For the shock wave (13.15), let η± = η(u±), q± = q(u±). We find
To verify this inequality from (13.17), write η(u) and q(u) in terms of the
Heaviside function H(x), and use the fact that the distributional derivative H′ =
δ, the Dirac delta function, a nonnegative distribution. Then
if and only if (13.18) holds.
We say the shock satisfies the entropy inequality if we get strict inequality in
(13.18):
The point of the next theorem is to show that if the shock (13.15) satisfies the
Lax entropy condition, then not only is the inequality (13.18) satisfied for any
convex entropy η, but in fact the sharper strict inequality (13.19) holds.
Theorem 13.1. Let f be a C2 strictly convex flux, and let η be a C2 strictly convex
entropy with entropy flux q. If the shock wave (13.15) satisfies the Lax entropy
condition, then it satisfies the entropy inequality (13.19).
Proof. Suppose the shock wave (13.15) satisfies the Lax entropy condition. Then
u− > u+. Let η : R → R be C2, with η″ > 0. We define functions
Then E(u−) = 0. The proof will be complete when we show that E(u+) < 0. To
do so, we differentiate the Rankine-Hugoniot condition
as well as E(u). Differentiating (13.20), we have

Hence, for u < u−, and using (13.13),
since 
 and 
 (by convexity of η) for u < u−.
Moreover, E′(u−) = 0. Consequently, E(u+) < 0 for u− > u+.
We state the converse to Theorem 13.1 slightly differently, as we use
piecewise linear convex entropies in the proof, whereas in the previous theorem
the entropy was assumed to be C2.
Theorem 13.2. Let f : R → R be a C2 strictly convex flux. If the shock wave (13.15)
satisfies the entropy inequality for all convex entropies η, then it satisfies the Lax
entropy condition (13.16).
Proof. Since the shock is assumed to satisfy the entropy inequality for all convex
entropies, we can choose an entropy to give us the result. Suppose for a
contradiction that u− < u+. Let k ∈ (u−, u+), and define
Then from (13.13), we have
Therefore, since u− < k < u+,
Substituting into the entropy inequality (13.19) gives
This is an inequality between chords connected to (u−, f(u−)) in the graph of f.
But since u− < k < u+, the inequality contradicts the assumption that f is strictly
convex. Therefore, u− > u+, since in the case u− = u+, the entropy inequality
(13.19) fails also.
It is helpful to sketch the graph of the convex function f and the various

chords referred to in the proofs above, to understand how the proofs work.
Incidentally, a similar calculation (essentially exchanging u+ and u−) shows that
if u− > u+, then
for u+ < k < u−, which is consistent with the convexity of f.
It is worth noting that the specific entropies (13.21) are a crucial construct in
the existence theory of Kruzkov [31] for the Cauchy problem for scalar
conservation laws. Rather than going more deeply into the existence theory, we
return in the next section to some other issues relating to nonconvex flux
functions.
13.4. Undercompressive Shocks
In this section we return to the example
of a scalar conservation law with concave-convex flux f(u) = u3. Shock waves
(13.15) satisfy the Lax entropy condition only for 
, if u− > 0. For a
convex flux function, all other shock waves are expansive in the sense that
characteristics leave the shock on both sides. In contrast, for a nonconvex flux,
there are shock waves for which the characteristics enter the shock on one side
and leave on the other side. For (13.22), we found earlier (by comparing the
shock speed to characteristic speeds) that these shocks have 
when u− > 0.
In the mid-1990s [24] it was realized that such shock waves can be limits of
traveling wave solutions of equations that include higher-order derivatives. This
was after a decade of research on such undercompressive shocks in systems of
equations, occuring as dynamic phase transitions [1, 25, 40, 42] in elastic solids
and gases, as transition waves in oil recovery models [41, 23] and in
magnetohydrodynamics [47].
To justify the shocks that fail the Lax entropy condition, we seek traveling
wave solutions of the modified KdV-Burgers equation,
The term modified is used because the u2 nonlinearity of Burgers’ equation, and of
the KdV equation, is modified to u3. The names Burgers and Korteweg-deVries
(KdV) are associated with the dissipative and dispersive terms on the right-hand

side, respectively. The parameters ϵ and γ are positive constants. We choose a
positive coefficient for the dispersive (third-order) term; the consequences of
reversing the sign of the dispersive term are left as problem 12.
The traveling waves we seek have the same form as in the previous chapter: 
, ξ = (x − st)/ϵ, satisfying far-field boundary conditions 
 u±.
Since the equation is unchanged by the transformation u → −u, we can restrict
attention to traveling waves with u− > 0.
As in Chapter 12, we convert the PDE into an ODE and integrate once, with
the result (in which we have dropped the bars on u)
Recall that we analyzed a similar equation when seeking traveling waves for the
bistable equation (12.13). As in the analysis of that equation, we write the
second-order ODE as a first-order system:
Let F(u, v) = (v, −γ v + (u3 − u3
−) − s(u − u−)). The vector field F has
Jacobian
For u− > 0, and −2u− < u+ < −u−/2, let 
. Then there are
three equilibria (u, v) = (u, 0), with u = u± and u = u0 satisfying u+ < u0 < u−,
and
At the outside equilibria, u = u±, we observe that s < 3u2; the eigenvalues of dF
are real and of opposite signs, given by
That is, the equilibria are saddle points. The middle equilibrium at u = u0 has 
, so that it is a stable spiral (if the eigenvalues are complex) or a stable
node.
Much as we did for the bistable equation, we seek heteroclinic orbits (u(ξ),
v(ξ)) that are saddle-saddle connections from (u−, 0) to (u+, 0) by finding
parameter values for which there is an invariant parabolic manifold

through these two equilibria. Since u+ < u−, we must have k > 0 to get a
traveling wave that follows the parabola from u− to u+ and hence is decreasing: v
= u′ < 0.
Writing 
 and rewriting the system
(13.24) using v = v(u),
we see that (after canceling factors (u − u−)(u − u+))
Equating coefficients gives
Using (13.25), we get
In particular, the middle equilibrium depends only on γ, so that the chord with
slope s pivots on this point as u− is varied. But then for u0 to be the middle
equilibrium, we must have 
. At the threshold, u+ = u0. In
summary, we have the following theorem.
Theorem 13.3. [24] For each γ > 0, ϵ > 0 and for each 
 there is a
traveling 
wave 
 
of 
(13.23) 
satisfying 
 
and 
, with speed 
.
As ϵ → 0+, the traveling wave u(x, t) of the theorem approaches the shock
wave solution (13.15) of (13.22). This solution is undercompressive in the sense
that characteristics pass through the shock from left to right, meaning that the
shock is subsonic on both sides. As 
, we have 
, and the chord
becomes tangent at u+ = u0 = −u−/2.
Undercompressive shocks have considerable influence on the structure of
solutions of the scalar equation (13.22). The Riemann initial value problem
has weak solutions that are monotonic for all choices of uL, uR if only Lax shocks
are used, but it can be nonmonotonic if undercompressive shocks are considered.
For example, consider γ > 0 to be set, let uL = 1, and 
 
.

Then the solution of the Riemann problem (13.26) consists of two shocks, each of
which has a traveling wave solution of (13.23):
Here 
, 
. The slower shock is
undercompressive, and the faster one is a Lax shock. Because of the structure of
characteristics in this solution, we observe that small disturbances ahead of the
faster wave are overtaken and absorbed by that wave, whereas small
disturbances behind the slower wave pass through the wave and are absorbed by
the faster wave. Details of this two-shock solution and others can be found in the
article by Jacobs et al. [24] and in the book by LeFloch [34].
13.5. The (Viscous) Burgers Equation
In this section we consider the viscous Burgers equation
in which the viscosity ϵ > 0 is a constant. The equation couples nonlinear
transport to linear diffusion. To solve the Cauchy problem (13.28) with initial
data u0 ∈ L1(R),
we convert the PDE into the heat equation with a clever change of dependent
variable known as the Cole-Hopf transformation [9, 21]. To motivate the
transformation, let
and integrate the resulting equation
with respect to x, taking the constant of integration to be zero:
Now u = wx, so the terms with spatial derivatives can be combined and written
as
suggesting an integrating factor

Now multiply (13.30) by 
 to get
Thus, since 
, we have the heat equation for z:
and the initial condition (13.29) becomes an initial condition for z:
We can solve this initial value problem for z(x, t), using the fundamental solution
for the heat equation (see Section 5.1). To exploit the exponential in the initial
condition (13.32), let 
. Then
solves (13.31), (13.32).
To recover the corresponding solution of Burgers’ equation, we have
which is the Cole-Hopf transformation.
Of course, we could have started with the transformation (13.33) and
substituted into Burgers’ equation (13.28), leading to the heat equation (13.31)
after some tedious calculation, but it would not have been so much fun.
Finally, we have the solution of the initial value problem (13.28), (13.29):
This explicit formula for the solution of the Cauchy problem is explored in
various ways in Whitham’s classic book on waves [46]. In particular, with some
careful asymptotic analysis it is possible to take the singular limit ϵ → 0+, to
recover Lax’s solution of the Cauchy problem for the inviscid Burgers equation
[12].
13.6. Multidimensional Conservation Laws

In this short section we consider scalar conservation laws in two dimensions:
The method of characteristics gives lines
thus, x = f′(u)t + x0, y = g′(u)t + y0, on which u(x, y, t) is constant. Thus, the
Cauchy problem, with initial condition
has the solution u(x, y, t) given implicitly by
A calculation due to Conway [10] and reported by Majda [35] shows that shock
formation is easily described as a criterion on the initial data. If we differentiate
(13.35) with respect to x and y, we get equations for the evolution of ∇u = (ux,
uy). Let (v, w) ≡ ∇u. Then along a characteristic,
where 
 is differentiation along the characteristic.
Now let q(t) = f″(u)v(t) + g″(u)w(t). Note that q(t) = ∇. (f′(u), g′(u)). Since u
is constant along the characteristic, we have 
. But using
(13.36), we find
The initial condition for this ODE is
The solution is
Consequently, q(t) blows up in finite time t∗ = −1/q0 if q0 < 0. That is, shock
formation takes place in finite time from smooth initial data if
at some point (x, y).
Example 5. (Shock formation in the avalanche model) The avalanche

example (3.17) gives an interesting twist on this calculation. The PDE (having set
S = 1 for simplicity) is of the form (13.35), except that f = yu depends explicitly
on y:
Differentiating with respect to x, y leads to the system
for the gradient (v, w) = ∇u. Notice that this system resembles (13.36), but the
linear term makes the dynamics work rather differently. Trajectories for system
(13.37) are represented by curves in the v-w plane that we find by writing system
(13.37) as
This is an equation for w2 as a function of v, which can be solved with the result
that the trajectories are all conic sections
where c ∈ R is a parameter, the constant of integration. Equation (13.38)
represents ellipses if c > 0, hyperbolas if c < 0, and a parabola if c = 0. The
trajectories are shown in Figure 13.8. A remarkable property of system (13.37) is
that the solutions can be written explicitly:
Consequently, ∇u = (v, w) becomes unbounded in finite time if q(t) has a
positive zero. This is a condition on the initial gradient (v0, w0). The result is that
all the unbounded trajectories shown in the figure correspond to initial
conditions that result in finite-time shock formation.
The solution (13.39) is obtained using ODE tricks. Let v = zw. Then v′ = z′w
+ zw′. Using (13.37) to eliminate v′ and w′, we find z′ − z2 = 0. Thus, z = v/w
= (c − t)−1, where c = w0/v0 from the initial condition t = 0. Therefore, w = (c
− t)v, and the first equation in (13.37) becomes v′ = 2v2(t − c), which can be
solved by separating variables, leading to the expression for v(t) in (13.39). The
expression for w(t) is then given by w(t) = (c − t)v(t).

Figure 13.8. Phase portrait and trajectories for system (13.37).
13.6.1. Shocks in Two Dimensions
The Rankine-Hugoniot jump condition in multiple dimensions is derived using
the Divergence Theorem. In one space dimension a shock curve is a point at each
time, across which the solution jumps, but we used a direct calculation to derive
the jump condition. In two space dimensions a shock is a space-time surface, a
curve at each time, across which the solution jumps, and which evolves in time.
Equation (13.35) is in conservation form, so that we can apply the Divergence
Theorem in x-y-t space. Suppose a weak solution is piecewise C1, with a jump
discontinuity across a surface ϕ(x, y, t) = 0. The normal to the surface is ∇ϕ =
(ϕx, ϕy, ϕt). Since we want the surface to intersect each constant-time plane in a
curve, we assume that the surface can be parameterized in terms of t and a
combination of the spatial variables. Rotating if necessary, we can assume 
, so that 
, and the normal is 
. Now
consider that (13.35) can be written as
Then the jump condition states that the normal component of the vector field (u,

f (u), g(u)) is continuous across the surface, and hence has no jump:
where 
. Specifically, this becomes
Correspondingly, the Lax entropy condition is formulated just as for the one-
dimensional case, where the single space variable is normal to the shock curve at
each time. The details of the following calculation are covered in some generality
in Serre’s monograph [39]. Let u± be the limits from the right and left in x: 
. The characteristic speeds in the normal direction at the
shock are given by
The Lax entropy condition compares the characteristic speeds with the shock
speed in (13.40):
This condition can be used to show stability of Lax shocks, meaning that for
nearby initial conditions, the Cauchy problem is well posed [39]. Stability of
shock waves for scalar equations and systems is a topic of ongoing research. In
the next chapter we discuss shocks, rarefactions, and the Riemann problem for
systems of equations in one space dimension and time.
PROBLEMS
1. Differentiate u(ax, at) = C with respect to a. Solve the resulting linear first-
order PDE by the method of characteristics to prove that u is a function of x/t.
2. Show that if u(x, t) is continuous; is C1 on either side of a curve C; satisfies the
PDE at each point not on C; and ux, ut are continuous up to C but have a jump
discontinuity across C then the curve C is a characteristic.
3. Suppose λo(u) = kou2 and λw(1 − u) = kw(1 − u)2, with positive constants ko,
kw in (13.10). Show that f(u) in (13.10) is positive, increasing but nonconvex,
with a single inflection point.
4. Show that if
for all u, v, then f(u) is a quadratic polynomial. (Hint: Take v to be fixed, and
solve the ODE for f(u).) Explain the significance of this result.

5. For the solution of Example 3, Figure 13.4, do the following.
(a) Sketch graphs of the solution u(x, t) as a function of x for various times,
such as t = 0, t = 1, t = 3.
(b) Write the solution out as a formula, and describe how u(x, t) approaches
zero as t → ∞.
(c) Is the convergence to zero as t → ∞ uniform? Explain with a proof.
6. Find all values of the positive constants a, b, c, d for which the function
is a weak solution of the inviscid Burgers equation for −∞ < x < ∞, t ≥ 0, and
satisfies the Lax entropy condition at the shock. (Hint: The PDE and the Rankine-
Hugoniot condition give three equations for a, b, c, d.)
7. Consider the PDE
(a) Write the Rankine-Hugoniot condition for a shock wave
(b) Find all the shocks (13.41) with u− = 1 that satisfy the Lax entropy
condition.
(c) Let u− = 1. Find all values of u+ for which there is a rarefaction wave
and write a formula for the function .
8. Use the implicit equation (13.7) or differentiate (13.5) to derive an ODE for
the evolution of v = ux along characteristics. By solving the ODE, find conditions
under which the following are true.
(a) The solution remains smooth for all t > 0.
(b) The solution develops a singularity in finite time, due to |v| → ∞.
Characterize the time at which this singularity forms.

9. Complete the solution of the traffic light problem (Example 4), shown in
Figure 13.6, by doing the following.
(a) Find a formula for the shock curve γ.
(b) Determine the time t3 as a function of the other parameters.
10. Traffic of uniform density is proceeding along a straight single-lane highway
at speed v0. At time t = 0, the car at location x = 0 slows suddenly to half speed
v0/2. Determine what happens subsequently, writing formulas and sketching the
x-t plane to show the structure of the solution.
11. Consider the modified KdV-Burgers equation (13.23) with ϵ = 1 but with the
sign of the dispersive term reversed:
(a) Explain why this equation cannot be reduced to (13.23) with a change of
variables, whereas for the quadratic nonlinearity of the KdV-Burgers equation,
the sign of the dispersive term can be changed (without changing γ > 0).
(b) Write the ODE for traveling wave solutions u = u(x − st) as a first-order
system. Consider values of u± for which there are three equilibria u+ < u0 < u
−. Show that the only heteroclinic orbits join equilibria u± to u0, and the
corresponding shock waves (13.15) satisfy the Lax entropy condition.
(c) Summarize what is different about this example from the modified KdV-
Burgers equation in Section 13.4.
12. Let γ > 0. Solve the Riemann problem (13.26) with uL = 1, in the following
two cases.
(a) When 
. Your solution should have a rarefaction wave and an
undercompressive shock.
(b) When 
. In this case, the solution is a single shock. Show that
it is a Lax shock and has a corresponding traveling wave solution of (13.23).
Show that for 
, the solution consists of two shocks
traveling at different speeds, an undercompressive shock and a faster Lax
shock.
13. Find the curve Γ shown as the heavy solid curve in Figure 13.8. Show that,
together with the positive half of the w-axis, Γ separates initial conditions v0, w0,
for which a shock forms, from initial conditions that do not lead to shock
formation. (Hint: You can solve this problem by examining when the quadratic
equation q(t) = 0 has a positive solution.)

CHAPTER FOURTEEN
Systems of First-Order Hyperbolic PDE
Systems of hyperbolic conservation laws are fundamental to the study of
continuum mechanics: the dynamics of fluids and solids. Nonlinear systems
possess a richer structure than scalar equations or linear systems, because they
have more than one characteristic speed, and waves with different speeds interact
nonlinearly.
We begin the chapter with linear equations, for which the method of
characteristics can be generalized from the scalar equations of Chapters 3 and 13,
leading to a short-time existence theorem for classical solutions. Then we show
how weak solutions of nonlinear systems of conservation laws can be constructed
from shocks and rarefaction waves. Along the way we show applications to the p-
system, to the elastic string, and to shallow water waves.
14.1. Linear Systems of First-Order PDE
We consider linear systems of equations in one space variable x ∈ R and time t >
0 of the form
where U = U(x, t) ∈ Rn is the dependent variable, and A = A(x, t) is an n × n
matrix depending smoothly on its arguments. We shall consider the Cauchy
problem, for which we pose initial conditions
System (14.1) is hyperbolic if A has n real eigenvalues λk, with n linearly
independent (right) eigenvectors rk, k = 1, 2, …, n. The system is strictly
hyperbolic if the n eigenvalues λk, k = 1, 2 … n are all distinct. The eigenvalues of
A are called characteristic speeds, as in the scalar case. We will also need the left
eigenvectors lk, which are row vectors. We can assume the eigenvectors have
been chosen to satisfy
14.1.1. Linear Constant-Coefficient Systems
We begin by considering linear constant-coefficient systems, in which A is a
constant matrix. In this case, the system admits solutions in traveling wave form
(much like the linear transport equation):

Substituting U into system (14.1), we have
so that λ = λk is an eigenvalue of A, and v = rk is a right eigenvector. Such a
solution is a traveling wave with speed λk.
To solve the system (14.1) when A is a constant matrix, we simply diagonalize
the matrix, or equivalently, decompose U using the eigenvectors:
where uk : R × R+ → R. Substituting into (14.1) and using Ark = λkrk, we get
decoupled equations for the unknown coefficients uk:
Thus, uk(x, t) = φk(x − λkt) for scalar functions φk, k = 1, …, n, and we have the
general solution of the system
The initial condition (14.2) is satisfied by setting t = 0 and using the left
eigenvectors:
Example 1. (The wave equation as a system) A simple example of a constant-
coefficient first-order system is provided by the wave equation in one space
dimension and time
We write the PDE as a system by letting
Then we have
and the initial conditions w(x, 0) = f(x), wt(x, 0) = g(x) become

Thus, in terms of system (14.1), we have
The characteristic speeds are ±c, with eigenvectors
The solution of the initial value problem is thus
in which φ± are related to f′, g by φ+ + φ− = f′; φ+ − φ− = g/c.
14.1.2. Method of Characteristics for Variable Coefficients
Now let’s consider how the method of characteristics, which we used so
successfully for scalar equations, can be applied to linear systems when A = A(x,
t) is not a constant matrix. For this purpose, we again diagonalize the matrix A
using the eigenvectors. That is, we substitute (14.4) into system (14.1), but now
the eigenvectors ri depend on x, t as well as on the coefficients ui:
Taking the scalar product of this equation with the left eigenvectors lj (see
(14.3)), we obtain a system of the form
in which the coefficients dij are continuous functions of x, t. Similarly,
decomposing the initial conditions (14.2) into eigenvectors of A(x, 0), we obtain
initial conditions for the coefficients ui:
Characteristics are defined, as for scalar equations, by
In particular, through each point (x, t), with t > 0, there are n characteristics x
= xi(τ), i = 1, 2, …, n, passing through (x, t), each with a different speed. Each
intersects the x-axis at a different point 
. We recognize the left-hand side

of the ith equation in (14.5) as dui/dτ (i.e., differentiation along the ith
characteristic). This enables us to convert equations (14.5) into integral
equations, by integrating along the ith characteristic from 
 to (x, t):
Next we write this system in vector form:
where 
, and K(u)(x, t) denotes the vector of integrals on the right-
hand side of (14.6).
Solving the Cauchy problem is equivalent to finding a fixed point of the
mapping B : u → φ + K(u) in a suitable space of functions. The appropriate
setting is the contraction mapping principle (see Appendix A) in the Banach space X
= CB(R × [0, T], Rn) of continuous bounded functions from R × [0, T] to Rn,
with T > 0 to be chosen. This principle is used to prove existence of solutions of
initial value problems for systems of ODE. The context here is not much different,
as we have reduced the Cauchy problem to a system of integral equations. The
norm in X is the supremum norm on bounded continuous functions:
We assume that the entries in the matrix A(x, t) are in X, and so are their
derivatives. Then the coefficients dij are all in X, since they depend on derivatives
of the eigenvectors ri(x, t), which by standard matrix analysis have derivatives in
X [28]. Since for linear equations the characteristic speeds do not depend on the
solution u, the function φ is also independent of u. (For nonlinear equations, the
dependence on u would be through the points .)
For u, v ∈ X, we have that for 0 ≤ t ≤ T,
Therefore, for T > 0 sufficiently small, K is a contraction; hence the mapping B is

also. By the Banach fixed-point theorem (see Appendix A), B has a unique fixed
point in X. Thus, we have proved the following theorem for the initial value
problem
Theorem 14.1. Let U0 ∈ CB(R), and suppose the matrix A = A(x, t) in (14.7) has
entries in CB(R), together with their derivatives. Then there is T > 0 such that the
initial value problem (14.7) has a unique solution u ∈ CB(R × [0, T], Rn).
14.2. Systems of Hyperbolic Conservation Laws
In this section we consider nonlinear systems of conservation laws
in which F : Rn → Rn is a C2 function. For nonlinear systems we can obtain a
short-time existence and uniqueness theorem similar to Theorem 14.1 under
suitable assumptions. However, in this section we focus on issues arising because
of nonlinearity—notably, shock waves and rarefaction waves—and show how
these waves appear in applications.
We refer to (14.8) as an n × n system. We shall assume strict hyperbolicity,
meaning that the Jacobian dF(U), an n × n matrix for each U ∈ Rn, has distinct
real eigenvalues, namely, the characteristic speeds
with corresponding left and right eigenvectors lj(U), rj(U), j = 1, …, n satisfying
(14.3) at each U:
Strict hyperbolicity allows us to discuss unambiguously the kth characteristic
field by referring to the characteristic speed λk.
The notion of genuine nonlinearity for scalar equations has a counterpart for
systems by considering the characteristic fields separately. The kth characteristic
field is genuinely nonlinear in a set Ω ⊂ Rn if
The field is linearly degenerate in Ω if
A function ψ : Rn → R is a k-Riemann invariant if

Then for each k, there are, locally in U, n − 1 Riemann invariants whose
gradients are linearly independent at each U. This follows because (14.9) is a
scalar equation for ψ(U). The equation says that w is constant on characteristics,
which are parallel to rk(U). Consequently, each Riemann invariant is constant on
rarefaction curves, which are integral curves of the vector field rk(U): U′ = rk(U).
As for scalar equations, the construction of rarefaction waves is closely related to
the property of genuine nonlinearity.
A Riemann invariant ψ can be specified locally near a given point U0 ∈ Rn by
defining its values on an n − 1 dimensional manifold M orthogonal to rk(U0),
say, 
 on M. Then 
 is tangent to M at U0, where ∇M is the gradient
operator along the linear space tangent to M at U0. We define ψ near U0 by
solving rk · ∇ψ = 0 by the method of characteristics. A set of n − 1independent
Riemann invariants is given by choosing n − 1 functions 
 on M with the
property that the n − 1gradients 
 are linearly independent at U0. Then the
gradients ∇ψ(U) are linearly independent for all U near U0 and are orthogonal to
rk(U).
For a 2 × 2 system (of n = 2 equations in 2 unknowns u, v), the Riemann
invariants can be used as an alternative pair of variables, with the advantage that
they partially diagonalize the system. To be specific, let λ±(ψ−, ψ+) denote the
two characteristic speeds, written as functions of the Riemann invariants ψ±.
Then the following holds:
We leave verification of this property to Problem 7. This property means that
each Riemann invariant ψ± is constant on characteristics x′ = λ∓ of the opposite
family!
It is worth comparing the notation and constructions for systems of hyperbolic
conservation laws with the corresponding development in Sections 13.1 and 13.2
on scalar conservation laws from the previous chapter. We shall use three
examples of systems of conservation laws to illustrate the ideas in this chapter,
beginning with the issue of hyperbolicity.
14.2.1. The p-System
This 2 × 2 system is derived from the quasilinear wave equation:
in which σ : R → R is a given smooth function. The equation, derived in Section

4.1.1, is a model of nonlinear one-dimensional elasticity, in which w = w(x, t) is
the displacement, and wx is the strain (or displacement gradient), and σ = σ(wx)
is the stress (a given function of the strain).
A typical stress-strain curve is shown in Figure 14.1. The vertical asymptote at
wx = 0 corresponds to the infinite force needed to compress the material to a
point, and σ(1) = 0 corresponds to a uniform unstressed material with
displacement w(x, t) = x.
To write (14.11) as a system, we set (as for the linear wave equation) u = wx
and v = wt, namely the strain and velocity, respectively:
Figure 14.1. A typical nonlinear stress-strain law.
This is an example of system (14.8) in which
To check hyperbolicity, we calculate
which has eigenvalues and eigenvectors
Thus, the system is strictly hyperbolic if and only if σ′(u) > 0; in words, stress is
a strictly increasing function of strain. In most circumstances it is appropriate to
consider such monotonic stress-strain laws, although strain-softening materials,

which have some range of strains over which the stress decreases, are important
in materials science. Examples include certain plastics (try stretching a thin strip
of soft plastic to feel its strain softening) and some metal alloys used to make
smart materials that change their stress-strain properties in controlled ways.
The system is genuinely nonlinear in both characteristic fields provided σ″(u)
≠ 0. It is linearly degenerate in both fields if σ″(u) ≡ 0, in which case σ(u) is an
affine function of u, corresponding to Hooke’s law. The Riemann invariants are 
 satisfying ∇ψ± · r± = 0 and corresponding to 
, respectively. Recall that the Riemann invariants allow us to
diagonalize the system of equations. (See (14.10).)
System (14.12) also describes one-dimensional isentropic gas dynamics in
Lagrangian variables, in which u represents the specific volume (the reciprocal of
density), v is once again velocity, and −σ = p is the pressure. The name p-system
derives from this context of gas dynamics. Further properties of the p-system are
discussed later in this chapter and in the book by Smoller [43].
Figure 14.2. Shallow water flow.
14.2.2. The Shallow Water Equations
For a body of water on a flat horizontal surface, we wish to describe the motion
of the free surface as the water flows. The full equations of flow for a viscous
incompressible fluid (such as water) are the Navier-Stokes equations, discussed in
the next chapter. The shallow water equations can be derived by a scaling and
approximation argument from the full equations, but here we give an abbreviated
and more direct route.
Let’s restrict to fluid motion in a vertical plane (for example, we assume the
fluid is moving parallel to side walls, like a river flowing parallel to its banks), so
that the velocity and height of the fluid depend only on one horizontal variable x
and a vertical variable y, as in Figure 14.2.
As shown in the figure, let y = h(x, t) be the height of the fluid surface at

time t, and let u(x, t) denote the horizontal velocity of the layer of fluid, which
we take to be independent of depth. More precisely, u(x, t) represents the depth-
averaged horizontal velocity. Note that we are using Eulerian coordinates to
describe the flow, since (x, y) is a fixed physical location. In a Lagrangian
formulation, using a reference configuration, individual fluid particles are
labeled, and velocity is determined from the particle trajectories.
Conservation of mass is expressed by the equation:
The pressure p in the fluid layer depends on depth, and we assume it is
hydrostatic, meaning that pressure increases linearly with depth and is
atmospheric pressure pa at the free surface:
Neglecting viscous forces, we can write the conservation of horizontal
momentum (an expression of Newton’s second law F = ma) as
Substituting for p, we obtain
Equations (14.13), (14.14) constitute a 2 × 2 system of the form (14.8), with
To check hyperbolicity, we calculate
which has eigenvalues and eigenvectors
respectively. Thus, the system is strictly hyperbolic for h > 0, and one family of
characteristics travels faster than the fluid velocity, while the other family moves
more slowly. The Riemann invariants 
 satisfy ∇ψ± · r± = 0.
These Riemann invariants for the shallow water equations will be useful when we
solve the classic dam-break problem, which we treat later in Section 14.3.
There is an extensive literature on shallow water waves and the related
equations of shallow flow [2, 15, 22, 46] used to model tsunamis and avalanches.

The hyperbolic structure of the equations is important when finding solutions in
these applications.
14.2.3. The Elastic String Equations
The equations for motion of an elastic string in two dimensions form a system of
two quasilinear wave equations, which we rewrite as a 4 × 4 system of first-
order conservation laws.
As with one-dimensional elasticity, leading to the p-system, the reference
configuration is one dimensional, but now we allow deformations in two
dimensions. The displacement or position vector, which we write here as r = (r1,
r2) ∈ R2, is a function of x, t, with x in an interval I representing material points
in the string (see Fig. 14.3). Forces in the string are represented by the tension T,
a function of x, t that measures the magnitude of the internal force of the string.
We assume the direction of this force is tangential to the string, based on the
simplifying assumption that the string exerts no resistance to bending. Modeling
the force by elasticity means that T is a function of the strain, here given by the
scalar |rx|.
The equations of motion are then
The density and cross-sectional area of the string, both taken to be constant
functions of x and t, are incorporated into the tension T. On the left of this
equation is the acceleration, and on the right, the tension T is the magnitude of a
force whose direction is that of the unit tangent. The equation expresses
conservation of linear momentum, Newton’s second law F = ma.
Figure 14.3. The elastic string. (a) Reference configuration, interval I; (b)
physical string.
The function T is typically increasing and takes the form of the stress-strain
law shown in Figure 14.1. This makes sense, because for purely longitudinal

motion (along the length of a straight string), the string equations (14.15) reduce
to the single wave equation (14.11), with T = σ. However, even if we assume
linear elasticity given by Hooke’s law,
with elastic constant E > 1, the equations are still nonlinear, unless the motion is
purely longitudinal.
To write (14.15) as a system of first-order equations, we define variables p, q,
u, v by
Then (14.15) is equivalent to
which has the form (14.8) when we write
To check hyperbolicity, we compute dF(U):
in which

The block structure of the matrix
implies that the eigenvalues of dF(U) are square roots of the two eigenvalues of
B. Calculating the latter eigenvalues, we find that the characteristic speeds are
Consequently, the elastic string equations are strictly hyperbolic if and only if
In particular, if we assume the stress-free equilibrium state corresponds to the
normalization T (1) = 0, then T (ξ) < 0 for ξ < 1, so that (14.18) does not hold
for ξ < 1. This is related to the fact that since the string has no resistance to
bending, its motion under compression is not well behaved (the equations are
linearly ill posed). Consequently, we consider the elastic string equations only for
strings under tension (i.e., with ξ > 1).
Hooke’s law (14.16) leads to a strictly hyperbolic system for ξ > 1. More
generally, we see that if
then (14.18) is satisfied for ξ ∈ (1, ξmax), where either ξmax = ∞ or ξmax < ∞ is
the first value of ξ > 1 for which
Then we have
As we saw in Section 14.1.2, the characteristic speeds 
 are associated with
transverse 
waves, 
whereas 
 
are 
associated 
with 
longitudinal 
waves

(corresponding to solutions of the p-system, a connection with longitudinal
motion suggested above). Thus, longitudinal waves travel faster than transverse
waves for 1 < ξ < ξmax.
The notions of longitudinal and transverse waves have counterparts in three-
dimensional elasticity, for example, in modeling subterranian earthquake waves.
Longitudinal waves are pressure waves, whereas transverse waves correspond to
shear waves, in which material is sliding over itself transverse to the direction of
propagation of the wave.
We include Problems 2 and 6 at the end of the chapter to explore further
properties of the elastic string equations, and there is a nice paper on the
equations by Antman [4]. Having established three representative examples, we
return to a general strictly hyperbolic system to discuss rarefaction waves and
then shocks.
14.2.4. Rarefaction Waves
We return to the general system
in which F : Rn → Rn is a C2 function. Our standing assumption is that the system
is strictly hyperbolic, meaning that there are n distinct real characteristic speeds,
the eigenvalues λk(U), k = 1, …, n of dF(U), with associated right/left
eigenvectors rk/lk satisfying
For rarefaction waves, we consider characteristic fields that are genuinely
nonlinear, meaning values of k and ranges for U for which
Genuine nonlinearity is essential for the existence of centered rarefaction waves,
continuous scale-invariant solutions 
, where V : R → Rn. Substituting
this form into (14.19), we find
Multiplying by t and letting 
, we then have
Hence, ξ is an eigenvalue of dF(V), and V′ is an eigenvector:

for some k = 1, …, n. Thus, V(ξ) lies on an integral curve of rk(U). (See
Appendix C.) Moreover, ξ and λk increase together, so the integral curve is
oriented so that λk(U) increases as U moves along the curve.
Figure 14.4. Centered rarefaction wave for the kth characteristic field.
The corresponding rays 
 in the x-t plane are k-characteristics: 
 
. If we differentiate the first equation in (14.21) with respect to ξ and use the
second equation, we find a directional derivative:
We can interpret (14.22) as determining the magnitude of the eigenvector rk; it
also includes the choice of sign so that rk points in the direction of increasing
characteristic speed.
We next relate the oriented integral curve to the rarefaction wave that
continuously connects a constant U− to a constant U+, as shown in Figure 14.4.
In the figure, the straight lines are k-characteristics, and the curved line is a
characteristic of a different field. The solution is continuous even at the edges of
the fan, since the trailing edge of the wave is constructed to have speed λk(U−),
and the leading edge has speed λk(U+). The characteristic speed has to increase
through the rarefaction wave. (In the figure, the slopes of the characteristics
decrease). We define the rarefaction curve Rk(U−) through U− to be the half of the
integral curve through U− on which the characteristic speed increases from λk(U
−). Then U− can be connected to any U+ ∈ Rk(U−) with a rarefaction wave. In
particular, we have
Note that characteristics of the other eigenvalues are not straight as they pass
through the rarefaction wave, as illustrated in the figure with a characteristic

moving left, accelerating as it passes through the wave.
Example 2. (Rarefaction waves for the p-system) Let’s work out the details for
the p-system. Here we have
Thus, the p-system is genuinely nonlinear (in both characteristic fields) if and
only if σ″(u) ≠ 0. In this case, we normalize r±(u) by choosing a = 
,
so that (14.22) is satisfied. For a rarefaction wave, we have from (14.21),
which is more conveniently written as
Thus,
In particular, for a rarefaction from (u−, v−) to (u+, v+), we must have
For the characteristic speed to increase through the rarefaction wave (see Fig.
14.4), we must satisfy (14.23), which for the p-system becomes
for u between u−, u+.
We get more information about the slope and curvature of the rarefaction
curves by differentiating (14.24): 
. For example,
if σ″(u) > 0, then for a forward rarefaction wave (i.e., with positive speed), we
must have u+ > u−, whereas for a backward wave (with negative speed), we
have u− > u+. The corresponding values of u+, v+ lie on the rarefaction curves R
±(u−, v−), shown in the u-v plane in Figure 14.5a. If σ″(u) < 0, for example, as
shown in Figure 14.1 near u = 1, then the configuration changes to that in
Figure 14.5b. We shall use the rarefaction curves shown in Figure 14.5 to solve
the Riemann problem after we have discussed shock waves and shock curves in
Section 14.2.7.

14.2.5. Propagation of Singularities
The edges of a rarefaction wave have discontinuities in the derivative of the
solution, even though the function itself is continuous. By construction, these
singularities propagate along characteristics. Here we prove the more general
result for continuous solutions of (14.19) that jumps in the derivative lie on
characteristics.
Figure 14.5. Rarefaction curves for the p-system. (a) σ″(u) > 0; (b) σ″(u) < 0.
Theorem 14.2. Consider a function U : R × (0, ∞) → Rn that is continuous and is
C1 except on a C1 curve Γ : x = γ (t). Suppose that derivatives ∂xU, ∂tU have left and
right limits ∂xU(γ (t)±, t), ∂tU(γ (t)±, t) that vary continuously on Γ, and that U(x, t)
satisfies the PDE (14.19) for (x, t) not on Γ. Then
1. the curve Γ is a characteristic: 
 for some k = 1, …, n, and
2. the jump [∂xU](t) = ∂xU(γ (t)+, t) − ∂xU(γ (t)−, t) is parallel to the corresponding
right eigenvector: [∂xU](t) = η(t)rk(U(γ (t), t)) for some η(t) ∈ R.
Proof. From (14.19) we have
and by continuity, (14.25) holds with Ut = Ut(γ (t)±, t), Ux = Ux(γ (t)±, t). Thus,
Moreover, differentiating the identity
we have
where 
, and 
. It follows that

Substituting for [∂tU](t) from (14.26), we find
Thus,  is an eigenvalue of dF(U), and [∂xU] is a corresponding eigenvector. The
theorem now follows.
We now discuss shock waves for hyperbolic systems, beginning with the
formation of shocks for 2 × 2 systems.
14.2.6. Shock Formation
For scalar conservation laws, it is a simple calculation to identify the mechanism
for formation of shocks, in which a spatial derivative blows up in finite time.
There are many derivatives for systems, and the analysis is not so simple as it was
for scalar equations. Here we show the calculation by Lax [33] for systems of two
conservation laws with smooth initial data that are constant outside an interval.
Lax’s calculation was generalized to systems of n ≥ 3 equations by John [27]
under similar conditions on the initial data. Lax’s calculation relies on uniquely
defined Riemann invariants, but for n ≥ 3, each characteristic field has n − 1
independent Riemann invariants. John’s analysis instead relies on a
decomposition of Ux into characteristic fields, thereby defining generalized
Riemann invariants, whose propagation and interaction can be tracked much as
Lax does with the calculation in this section.
For n = 2, we let U = (u, v), and write system (14.19) as
Let λ(r, s), μ(r, s) be the characteristic speeds, in terms of the Riemann invariants
r, s. Then system (14.27) is partially diagonalized:
Here we have ∇r(u, v). rμ = 0 and ∇s(u, v) · rλ = 0. Mimicking the scalar case,
we differentiate (14.28) with respect to x and define p = rx, q = sx:
The difficulty lies in the pq term in each equation, representing the interaction of
waves. Let’s focus on the equation for p.

Define differentiation along the λ characteristic by 
. Then 
 and 
. This last equation yields
Then we can rewrite the first equation in (14.29) as
Now we use an integrating factor to collapse the left-hand side. Define a function
h(r, s) so that 
. Then 
, since 
. Consequently, if we let
z(t) = ehp, (14.30) becomes
If the coefficient λre−h were constant, we could write the solution of the Riccati
equation (14.31) explicitly. However, in general we have little information about
the dependence of this coefficient on the solution. Nonetheless, we can use a
comparison argument to show that solutions of (14.31) blow up in finite time,
provided the coefficient is bounded and we assume genuine nonlinearity.
Consider bounded smooth C1 initial conditions for (14.27). Then the Riemann
invariants remain bounded, and as long as their derivatives stay bounded, the
solution of the initial value problem is defined and smooth, and h(r, s) is
bounded.
We assume that the λ characteristic field is genuinely nonlinear, so that λr ≠
0. Then the coefficient λre−h is bounded above and below on the same side of
zero. Let’s take λr > 0. Then there are constants A, B such that 0 < A < λre−h <
B. Consider a specific λ-characteristic originating from x = x0 at t = 0. Then z(0)
= z0 is a specific number given by the initial data at x0. We can compare the
solution z(t) of (14.31) with solutions of the initial value problems
Since A, B are constants, we have solutions
Now suppose z0 < 0. Then Y′(t) < z′(t) < X′(t) for t ≥ 0. To see this, first
observe that it is true at t = 0, so by continuity it remains true for a short time.
But then Y (t) < z(t) < X(t) < 0, so the inequality on derivatives persists, and so
does the comparison of the solutions. Since X(t) → −∞ as t → −1/(Az0), we

conclude that z(t) → −∞ also, as t → t∗, for some t∗ ≤ −1/(Az0). Let zm = min
z0 < 0, where the minimum is taken over the initial points x0. Then the solution
of the system (14.27) has derivatives that blow up in finite time tm ≤ −1/(Azm).
In contrast, we also have conditions under which the derivatives of the solution
stay bounded. If λrz0 > 0, then z(t) stays bounded and decays to zero as t → ∞.
Just as for scalar conservation laws, we presume that as a derivative of the
solution becomes infinite, the weak solution continues with a shock wave. Next
we analyze shock waves explicitly.
14.2.7. Weak Solutions and Shock Waves
Here we formalize the definition of weak solution of the system of conservation
laws. The definition resembles that of weak solutions of linear elliptic equations
in that it is based on integration by parts, or equivalently, by distribution
solutions that are integrable.
It is convenient to define weak solutions of the Cauchy problem, consisting of
(14.19) with initial condition
To derive the appropriate weak formulation of the Cauchy problem, consider a
smooth solution U(x, t), with smooth initial data U0. To incorporate the initial
condition into the weak formulation, we define test functions φ ∈ 
 to have compact support in the upper half-plane, including the
x-axis. That is, φ can be nonzero in a bounded subset of the x-axis. Note that here
we only require the test functions to be C1, but we could just as well consider C∞
test functions.
Multiply (14.19) by φ and integrate by parts, observing that Ut + Fx = div(F,
U), and the operation applies to each component of the vector-valued functions.
This calculation leads to the relation
in which the final integral is zero unless the support of φ(x, 0) includes part of
the x-axis.
Motivated by this calculation, we define a function U ∈ L∞(R × [0, ∞) → Rn)
to be a weak solution of the Cauchy problem (14.19), (14.32) if (14.33) holds for
all 

Note that the idea of integrating by parts to obtain an integral identity for
weak solutions, using test functions, was previously used for elliptic equations.
(See (11.8) in Chapter 11.) However, for second-order equations we assumed that
the weak solution had a derivative in L2, whereas here we want to allow jump
discontinuities, whose derivatives are distributions but not in L2. Consequently,
we consider functions in L∞. This definition of weak solution of systems of
conservation laws applies to scalar equations, but we did not use the generality of
the definition in the previous chapter.
Next we use the definition to characterize weak solutions of (14.19) that have
jump discontinuities across a curve Γ.
Theorem 14.3. Suppose that U : R × (0, ∞) → Rn is a weak solution that is C1
except on a C1 curve Γ : x = γ (t). Suppose the left and right limits U±(t) = U(γ (t)
±, t) vary continuously on Γ. Then these limits satisfy the Rankine-Hugoniot
conditions
Figure 14.6. Domain of integration for a shock.
Proof. Let φ be a test function with support Ω in t > 0 that is divided into two
disjoint sets Ω± by the curve Γ, as indicated in Figure 14.6. Since U is a weak
solution, it satisfies (14.33), which we rewrite as
Now integrate by parts in Ω± separately, to put the derivatives back on U and
F(U). In this process, we are left with only boundary terms, since Ut + F(U)x = 0

in each of Ω±. Since φ vanishes on the boundary of Ω± except on Γ, the only
contribution remaining comes from the integrals on Γ ∩ Ω. We need the unit
outward normals on x = γ (t), which are given by
on ∂Ω±, respectively, where 
. As a result, we obtain n equations, one
equation from each component of (14.35):
That is, if Γ ∩ Ω = {x = γ (t) : a < t < b}, then we have
Consequently, since φ is arbitrary and the integrand is continuous, we obtain the
Rankine-Hugoniot conditions (14.34).
14.2.8. Analysis of the Rankine-Hugoniot Conditions Using Bifurcation
Theory
To understand the jump conditions more thoroughly, it is useful to consider
piecewise-constant solutions, or shock waves
in which s is the shock speed. We say U given by (14.36) is a shock from U− to
U+ if s, U± are related by the Rankine-Hugoniot conditions (14.34):
This system of n equations in 2n + 1 unknowns has one easy family of solutions,
the trivial solution: U+ = U−, s arbitrary. But this corresponds to constant
solutions of the PDE. We seek nontrivial solutions, for which there really is a
discontinuity.
Our approach to this problem derives from bifurcation theory, the analysis of
the branching of curves of solutions of nonlinear equations [8, 19]. To set this
approach up, we fix U− and let G(U+, s) denote the left-hand side of (14.37).
That is, G : Rn × R → Rn is defined by

Then we have
Here are two basic results of the theory, stated informally:
1. Bifurcation points are eigenvalues of the linearized problem. Nontrivial solutions
can only branch off the curve {(U−, s) : s ∈ R} of trivial solutions at
eigenvalues s = λk(U−) of the linearization dF(U−).
2. Bifurcation from a simple eigenvalue gives a curve of solutions. If the eigenvalue s
= λk(U−) is simple, then there is a curve of nontrivial solutions passing
through (U, s) = (U−, λk(U−)).
The first result is a simple consequence of the Implicit Function Theorem.
Suppose s0 is not an eigenvalue. Then dUG(U−, s0) = −s0I − dF(U−) is invertible.
By the Implicit Function Theorem, there is a curve of solutions U = U(s) for s
near s0. Moreover, no other solutions exist in a neighborhood of (U−, s0). But U =
U− is a solution for all s, so we must have U(s) ≡ U−.
The second result is also a consequence of the Implicit Function Theorem, but
it requires much more careful analysis. It appeared in various forms in the late
1960s and early 1970s, culminating in the 1971 paper of Crandall and
Rabinowitz [11]. The basic idea of the proof is to solve for n − 1components of
U as a function of s and a small variable ϵ representing the remaining component
ϵrk(U−). This construction uses the Implicit Function Theorem in a
straightforward way on a system of n − 1 equations obtained by projecting the
original system onto the range of dUG(U−, λk(U−)). The problem is thereby
reduced to a single equation f(s, ϵ) = 0, called the bifurcation equation. This
reduction process to arrive at the bifurcation equation is known as the Lyapunov-
Schmidt reduction. Locally, there are two curves of solutions of the bifurcation
equation. Zeroes of f are {(s, ϵ) = (s, 0)}, forming the curve of trivial solutions,
and a function s = ŝ(ϵ), giving the curve of nontrivial solutions corresponding to
shock wave solutions.
Here is the precise result, which we state without proof.
Theorem 14.4. Suppose F is Cm, m ≥ 2, and the PDE (14.19) is strictly hyperbolic in
a neighborhood of U−. Then for each k = 1, …, n, there exist δ > 0 and Cm−1
functions 
 that are Cm away from zero and are such that
1. the property 
 holds;
2. the identity 
 |ξ| < δ holds; and

3. there is a neighborhood N ⊂ Rn × R of (U−, λk(U−)) such that if (U, s) ∈ N
satisfies (14.37), then either U = U−, or 
 and 
 for some ξ ∈ (−δ,
δ).
We use the theorem to establish some properties of weak shock waves, in
which the jump in U is small. Following the calculation of Lax [32], we
differentiate identity 2 twice and set ξ = 0. Differentiating once, we have
so that, setting ξ = 0 and using property 1, we get
We deduce that 
, where we have rescaled the parameter ξ so that the
scalar multiple of rk(U−) is unity. (Recall that rk(U−) is normalized by (14.22).)
Differentiating (14.38) again, and setting ξ = 0, we get
where d2F(U−) : Rn × Rn → Rn is the bilinear operator involving second
derivatives of F that appears in the Taylor series expansion of F(U) about U = U
−. (See Appendix A for the Taylor series of a scalar function of several variables;
the bilinear operator puts these terms into a vector-valued function.)
Now we can eliminate the second derivatives of by projecting onto the 
complement of the range of 
). This is most easily accomplished by
taking the inner product (scalar product) of the equation with the left eigenvector
ℓk(U−):
Thus, we have
To verify the final equality, we differentiate the identity
with respect to ξ and let ξ = 0.
From (14.39), we deduce that

Now suppose the kth characteristic field is genuinely nonlinear at U−, and
normalize rk(U−) as for rarefaction waves (see (14.22)). Then, 
and for ξ
near zero, we have
if and only if ξ < 0. This inequality is highly significant in terms of the Lax
entropy condition that we arrive at in the next section, where we state the
condition for systems of conservation laws. The consequence is that for weak
shocks (in which |U+ − U−| is small) only half of the branch of solutions, the
half with s < λk(U−), corresponds to shocks that are admissible according to the
Lax entropy condition.
14.2.9. Admissibility of Shock Waves and the Riemann Problem
Much of the theory of weak solutions of hyperbolic conservation laws, for both
scalar equations and systems, is centered on understanding solutions of the
Riemann problem. This is the initial value problem
where UL, UR are given constants (in Rn). We will build scale-invariant solutions 
 of the Riemann problem from centered rarefaction waves and shock
waves, separated by regions where U(x, t) is constant. For now, let’s focus on a
single shock wave, joining constants U− on the left and U+ on the right.
Just as for the inviscid Burgers equation, if we allow all shock waves
(satisfying the Rankine-Hugoniot conditions (14.34)), then solutions are not
unique. We correct this lack of uniqueness by imposing an additional condition
on shock waves, just as was done for scalar equations. The most fundamental
such admissibility condition is the Lax entropy condition.

Figure 14.7. A k-shock with representative characteristics satisfying the Lax
entropy condition.
We say the shock wave from U− to U+ with speed s (see (14.36)) is admissible
according to the Lax entropy condition if the following inequalities hold for some k
= 1, …, n:
Here we define λ0 = −∞ and λn+1 = ∞, so that these inequalities make sense
for k = 1and k = n. The Lax entropy inequalities require that the kth
characteristics enter the shock from both sides, while all other characteristics
pass through, either from left to right (the faster characteristics), or from left to
right (the slower characteristics). This behavior is illustrated in Figure 14.7. We
say that a shock satisfying (14.43) is a k-shock. Note that by continuity of the
characteristic speeds, condition (14.43b) is redundant for weak shocks. Moreover,
from (14.41), half of the shock curve of Theorem 14.4 consists of k-shocks, and
the other half violates the Lax entropy condition.
Example 3. (Shock waves for the p-system) The Rankine-Hugoniot conditions
for the p-system (see Section 14.2.1), are

Figure 14.8. Wave curves W±(u−, v−) for the p-system.
Thus, we have
Note that the latter formula states that s2 is the slope of the chord joining the
points (u±, σ(u±)) in the graph of σ. Now let’s interpret the Lax entropy
inequalities (14.43). As for rarefaction waves, the sign of σ″ will make a
difference. We first assume σ″ > 0. Then for forward shock waves, with s > 0,
and k = 2 in (14.43a):
which implies u+ < u− for s > 0. Note that the other condition (14.43b), 
, is automatically satisfied, since s > 0. For s < 0, we similarly
conclude u+ > u−.
Now let’s look at the shock curves, which are curves of values of (u+, v+) for
which there is an admissible shock for some speed. We again take the case σ″ >
0. For s > 0, eliminating s from (14.44), we obtain a curve S+(u−, v−):
For s < 0, we get the same formula, but now we have u+ > u−. This defines the
shock curve S−(u−, v−) of backward (left-moving) shock waves. The shock curves
meet the rarefaction curves of Figure 14.5 at (u−, v−), forming a wave curve W±
(u−, v−) for each characteristic family, defined by
Finally, we can solve the Riemann problem (14.42) for the p-system using the
wave curves shown in Figure 14.8. The solution of the Riemann problem is

represented in Figure 14.9. In this figure (uL, vL) is fixed, and we show the specific
combination of waves for (uR, vR) in each of the quadrants separated by the wave
curves W±(uL, vL). For example, as shown in the figure, the combination S−R+
indicates a backward shock (with negative speed) from (uL, vL) to a point (uM, vM)
∈ S−(uL, vL), and a forward rarefaction from (uM, vM) to (uR, vR) ∈ R+(uM, vM). The
corresponding graphs of u and v are shown in Figure 14.10.
Figure 14.9. Riemann problem solution for the p-system. Four regions
corresponding to different wave combinations are labeled in blue.
Figure 14.10. Graphs of u, v, the solution of the Riemann problem for (uR, vR) in
the region S−R+ of Figure 14.9.
14.3. The Dam-Break Problem Using Shallow Water Equations
In this short section we solve an initial value problem for the shallow water
equations. Consider a body of water of constant depth above a horizontal valley
floor and to one side of a dam. We are concerned with what happens when the
dam collapses instantaneously. We idealize the problem in a couple of ways.

First, the body of water has infinite extent, and we assume that the behavior after
the dam breaks depends primarily on a single spatial variable x, along the
horizontal direction perpendicular to the dam. This scenario is shown in Figure
14.11. This is an example of a Riemann problem (14.42), but the system has
coincident characteristic speeds when h = 0, to the right of the dam. When the
dam breaks, water rushes to the right, drawing water from the reservoir. Hence,
even though the water moves only to the right, the disturbance is felt at a point
labeled x = x1(t) that travels backward into the reservoir (Fig. 14.11c). Of
primary interest in this problem is the location of the leading edge x = x0(t) of
the water flood, where h goes to zero.
Figure 14.11. The dam-break problem. (a) The rarefaction wave; (b) the
reservoir before the dam break (t = 0); (c) the water after the dam breaks (t >
0).
In Figure 14.11 we show the rarefaction wave solution in the x-t plane, the
graph of h(x, t) before the dam breaks (at t = 0), and the graph of h(x, t) after
the dam breaks (for t > 0). To analyze the problem fully, we start with the

equations and initial condition:
It is convenient to introduce the relative speed 
 (see Section 14.2.2), so
that the characteristics λ± = u ± c have speed c relative to the fluid velocity u.
As predicted by (14.10), the Riemann invariants w± = u ∓ 2c diagonalize the
system
The curved characteristic shown in Figure 14.11a is associated with the faster
speed λ+. The Riemann invariant u + 2c is constant on this characteristic.
The rarefaction wave is associated with the slower speed λ−, so that u − 2c is
constant on each characteristic, but w− = u + 2c is constant throughout the
wave. To the left of the wave, h = h0 and u = 0. Thus, the slowest characteristic
is x = x1(t) = −c0t, 
. Since u + 2c is constant, we have u + 2c = 2c0.
Therefore, at the leading edge of the rarefaction wave, where h = 0 (so that c =
0), we have u + 2c = u = 2c0. But the leading edge travels with characteristic
speed λ− = u − c = 2c0. We conclude that the leading edge of the flood waters
is located at 
.
14.4. Discussion
The theory of systems of hyperbolic conservation laws is now quite extensive. In
this chapter we have focused on the construction of shocks and rarefactions, and
we have shown how these can be combined to solve Riemann problems. This
suggests an effective approximation to solutions of the Cauchy problem, in which
an initial condition
is posed, with U0 an integrable and bounded function. If U0 is approximated by a
piecewise-constant function, then wherever there is a jump between constants, a
Riemann problem can be solved to get the exact solution for some short time
interval, until adjacent waves meet. If these waves are shocks, then the
interaction can be resolved by solving another Riemann problem. However, if
one or both waves is a rarefaction, then these can be approximated by a small
jump, or a staircase of small jumps, in which each jump satisfies the Rankine-
Hugoniot condition but not the Lax entropy condition. These jumps travel with

approximately characteristic speed (as in the rarefaction they replace), and on
meeting other jumps, they generate further Riemann problems. Continuing in this
way, an approximate piecewise-constant solution of the Cauchy problem can be
generated. Justifying the continuation and proving that this method—known as
wave front tracking—converges requires detailed estimates of the interaction of
waves and the use of suitable spaces (such as BV, the space of functions of
bounded variation). Details of this approach are given in the book by Bressan [6].
A related method, involving approximations in a lattice of grid points and
using randomness to capture wave speeds approximately, is known as Glimm’s
random choice method. It is outlined in Glimm’s classic 1965 paper [18], in
which he proves the existence of solutions of the Cauchy problem using a series
of profound insights into wave interactions and convergence of approximate
solutions.
In the next chapter, after a discussion of the equations of fluid mechanics, we
examine the structure of the one-dimensional Euler equations. The Euler
equations of inviscid compressible fluid motion form a system of hyperbolic
conservation laws possessing shocks and rarefactions.
PROBLEMS
1. Check whether the shallow water equations are genuinely nonlinear.
2. Prove that system (14.17) is strictly hyperbolic for all ξ > 1 if T (1) = 0, T′(1)
> 0, and T (ξ) is a convex function: T″(ξ) ≥ 0 for ξ ≥ 1. Discuss the possibilities
if T (1) = 0, T′(1) > 0, and T is concave: T″(ξ) ≤ 0 for ξ ≥ 1.
3. Let σ(u) = u2, u > 0 in the p-system. Find explicit formulas for the rarefaction
curves. Hence derive formulas 
 for rarefaction waves joining (u−, v
−) = (1, 0) to (u+, v+). Graph the functions u, v against x for t = 1.
4. Consider the system of PDE
(a) Show that the system (14.47) with ϵ = 0 is strictly hyperbolic for u ≠ 0, v
≠ 0 if α = −1, but it fails to be hyperbolic everywhere in u, v if α = 1.
(b) Assume that α = −1. For ϵ = 0, write the Rankine-Hugoniot jump
conditions for a shock wave solution

(c) Assume that α = −1. For ϵ = 0, let (u−, v−) = (1, 0). Find all possible
values of (u+, v+) for which (14.48) satisfies the jump conditions for some
speed s. (Hint: The answer should be the union of a line and a hyperbola.)
(d) Assume that α = −1. For ϵ > 0, show that there is a traveling wave
solution (u, v) = (U, V)((x − st)/ϵ) of system (14.47) satisfying
for some value of s (which you need to calculate), but that the corresponding
shock (ϵ → 0+) fails to satisfy the Lax entropy condition because only one
characteristic enters the shock from each side in the x-t plane, and one
characteristic leaves on each side.
5. Write the equation
in which z = u + iv is a complex variable, as a system of two conservation laws
for real variables u, v.
(a) Find conditions on the function f : R → R under which the system is
strictly hyperbolic. Calculate the characteristic speeds and the corresponding
right eigenvectors.
(b) Show that one characteristic field is linearly degenerate and the other is
genuinely nonlinear provided f″(r) > 0, r ∈ R.
(c) Observe that the complex form of the equation is rotationally invariant, by
changing variables: ζ = eiθz, where θ is constant, and writing the PDE for ζ(x,
t). How does this property relate to parts a, b?
(d) Let f(r) = r2. For (u−, v−) = (1, 0), find all values of (u+, v+) ∈ R2 for
which there is a shock satisfying the Lax entropy condition, a contact
discontinuity, or centered rarefaction wave connecting (u−, v−) to (u+, v+).
This pair of equations is known as the Keyfitz-Kranzer system [29].
6. For the elastic string equations (14.17), show the following.
(a) Transverse waves are linearly degenerate.
(b) Longitudinal waves are genuinely nonlinear provided T″ ≠ 0.
7. (a) Prove the property (14.10) that for 2 × 2 systems, the Riemann invariants
diagonalize the system (for smooth solutions).
(b) Verify this property for the p-system. (Hint: Begin by deducing from
(14.24) formulas for the Riemann invariants.)

8. Write out the details of the argument that gets you from (14.40) to (14.41)
and thence to the statement about the admissibility of weak shocks following
(14.41).

CHAPTER FIFTEEN
The Equations of Fluid Mechanics
An abundance of interesting and important PDE exists, many of which are
systems of equations, because physical systems often relate different quantities as
dependent variables. Examples of physical systems are given in Serre’s text [39],
including the equations of electromagnetism (Maxwell’s equations), equations of
three-dimensional elasticity, and those of magnetohydrodynamics. In this chapter
we focus on the equations of fluid mechanics, specifically, the Navier-Stokes,
Stokes, and Euler equations. We discuss how these equations are related, the
contexts in which they are used, and some elementary properties.
15.1. The Navier-Stokes and Stokes Equations
The motion of a fluid is described by the evolution of physical quantities, such as
density ρ, velocity u ∈ R3, pressure p, and temperature θ. We shall assume
constant temperature, even though variable temperature can be very significant,
for example, in understanding patterns that are visible when heating water in a
pot. Assuming constant temperature simplifies the equations quite a bit. The
equations of motion can be derived from conservation laws of mass, momentum,
and energy, coupled to constitutive laws. Details can be found in fluid mechanics
texts, such as the one by Acheson [2].
Let’s start with dimensional independent variables 
 and dimensional
dependent variables 
. For an incompressible fluid such as water, the density ρ
is taken to be constant, although for stratified incompressible fluids, variations in
density can be important. The Navier-Stokes equations of incompressible flow are
a balance between inertial terms and forces:
where 
 is the acceleration due to gravity expressed as a vector in the
vertical direction. Thus, the term ρg is mass per unit volume times acceleration.
In fact, the first equation in (15.1) expresses Newton’s law, force = mass ×
acceleration in each small volume of fluid, so that mass is replaced by density
(i.e., mass per unit volume). The second equation, known as the incompressibility
condition, expresses conservation of mass for an incompressible fluid in which
the density does not change. From this equation, the Divergence Theorem implies
that volumes of fluid are preserved by the flow: if a fluid volume expands in one

direction then it contracts in other directions to compensate.
We now introduce a typical length scale L and time scale T, together with
velocity scale U and pressure scale P. These scales allow us to nondimensionalize
the variables and express the equations in nondimensional form. The
nondimensional variables do not have the tildes:
Substituting into (15.1), we arrive at
Next, divide the first vector equation by ρU2/L and choose scales for T and P :
Then T is a typical time for a fluid particle traveling at the typical velocity U to
travel a typical length L, and the pressure scale P is chosen to simplify the
equation as follows. We define the Reynolds number Re by
and arrive at the nondimensional system
where 
. The Reynolds number measures the relative importance of inertial
terms, with dimension ρU2/L, and viscous forces, with dimension μU/L2.
Since the velocity u has three components, the first equation of (15.2) is
really three scalar equations. They express conservation of linear momentum. The
terms on the left-hand side are the inertial or acceleration terms, and the right-
hand side represents the divergence of the forces, due to pressure, internal
friction (i.e., viscous forces) and the body force due to gravity. The
incompressibility condition ∇. u = 0 is unchanged by the scaling.
The nondimensional form of the equations is a powerful tool. If an experiment
is done on a fluid, such as water, in a laboratory, the results can apply in a much
larger context, such as a river or an ocean, by calculating the appropriate
Reynolds number from typical length and velocity scales. The results with water

would also apply to a much more viscous fluid, such as honey, again by using the
Reynolds number to relate the two scenarios. Moreover, the Reynolds number
controls the relative importance of the various terms in the equations, so it is
important to understand two limits, Re → 0, and Re → ∞.
In the limit Re → 0, corresponding to very slow flow (U small) or very viscous
flow (μ large), is somewhat tricky. If we were to simply multiply by Re and set Re
= 0, we would recover Laplace’s equation. However, this is misleading, as the
flow should be driven by pressure gradients. If we rescale the pressure by the
Reynolds number and define a new appropriate time scale (in effect, we are using
a different nondimensionalization), we recover the Stokes equations in the limit Re
→ 0:
which have the virtue of being linear. In this derivation, we have assumed that
the gravitational terms are significantly smaller than the terms retained. In some
contexts, the effect of gravity can be significant, even for slow flow. Stokes
equations are an important tool for studying the motion of small organisms in
viscous fluids, and for very slow flows, such as lava and glacier flows.
15.2. The Euler Equations
At the other extreme, letting Re → ∞, we immediately get the incompressible
version of the Euler equations:
in which viscosity is negligible. These equations model the flow of slightly
viscous fluids, such as air, provided that the pressure gradients are not so large as
to make compressibility significant.
The compressible Euler equations take the form

In these equations, the density ρ is variable (as it is in a gas, for example).
The variable 
 is the total energy (per unit volume) in terms of the
kinetic energy 
 and the potential (or stored or internal) energy e. The
variables are generally considered to be ρ, u, and e, with p = p(ρ, e) being given
by an equation of state, another term for a constitutive law. Both ρ and e must be
nonnegative, and the case ρ = 0 is known as the vacuum state. The tensor
product u ⊗ u gives a matrix with the (i, j) entry being uiuj. The corresponding
quadratic term in the Navier-Stokes equations (where ρ is constant) looks
different, because u is divergence free.
Let’s consider the one-dimensional equations
in which u is now a scalar velocity (the component of u parallel to the x-axis).
Notice that these equations (and in fact system (15.4)) are in the form of a
system of conservation laws in which the conserved quantities are mass ρ,
momentum ρu, and energy E. There is a natural symmetry in this system
between left and right. Specifically, the equations are unchanged by the
transformations x → −x, u → −u. A similar symmetry holds for the wave
equation, both linear and quasilinear.
If we carry out the differentiations (assuming for the moment that the
variables are smooth functions of x, t), we can write the system in the form of
nonlinear transport equations, with the abbreviation dt = ∂t + u∂x for the
convective derivative:
As in the previous chapter, hyperbolicity of this system depends on the
eigenvalues of the coefficient matrix, which in this case is
Note that in calculating this coefficient matrix, we use px = pρρx + peex, and the
spatial part u∂x of the convective derivative.

To calculate the characteristic speeds, we find eigenvalues of A(ρ, u, e) from
the characteristic equation:
Thus, λ = u, or λ = u ± (pρ + ρ−2ppe)1/2, provided pρ + ρ−2ppe ≥ 0. The
vacuum state ρ = 0 is therefore singular since ρ−2 → ∞, and hyperbolicity for ρ
> 0 requires pρ + ρ−2ppe > 0.
If we write the three characteristic speeds as
where c = (pρ + ρ−2ppe)1/2, we observe that c is the sound speed, meaning the
speed of small disturbances, relative to the fluid speed u. The characteristic speed
λ0 is the same as the fluid speed and does not propagate small disturbances. Not
surprisingly, the λ0 characteristic field is linearly degenerate, as we show below.
Since λ− < λ0 < λ+, 1,2,3-characteristics are associated with λ−, λ0, λ+
respectively. The corresponding eigenvectors are
respectively, and we can check genuine nonlinearity by calculating ∇λ · r. For
the λ± waves, we have
Thus, genuine nonlinearity of these characteristic fields depends on the equation
of state p = p(ρ, e). In the isentropic case, p = p(ρ) is independent of e, and
genuine nonlinearity reduces to the condition p″(ρ) ≠ 0. The λ0 characteristic
field is indeed linearly degenerate, since ∇λ0 · r0 ≡ 0.
The characterization of shock waves and rarefaction waves for system (15.5)
is complicated and is explained carefully and in detail in Serre’s text [39], vol. 1,
Section 4.8. Here we sketch the steps involved in processing the Rankine-
Hugoniot jump conditions for shock waves. We consider a shock with speed s and
left and right limits given by subscripts: v±, ρ±, e±. With the bracket notation [v]
= v+ − v− for jumps, the Rankine-Hugoniot conditions for (15.5) are

If we let z = v − s, then [ρz]= 0 from (15.7a), and we can let m = ρ±z± be the
common value on each side of the shock. Then we are left with the two
conditions:
With some algebra, these conditions can be combined into the equation
so that
Thus, either m = 0, or 
.
If m = 0, and assuming that ρ± > 0, we have z± = 0, so that v± = s. Then
(15.8a) implies p+ = p−, and the Rankine-Hugoniot conditions are satisfied.
These shocks, moving with characteristic speed, that is, the fluid particle speed v,
are contact discontinuities. Across such a wave, density jumps, but velocity and
pressure are continuous.
If m ≠ 0, then we have
From (15.9) and 
, it follows that
Thus, if ρ±, e±, p± = p(ρ±, e±) satisfy (15.10), then there are two real solutions
of (15.11) provided [p]/[ρ−1]< 0. If we label these two solutions m1 and m3, they
correspond to 1-shocks (m1 > 0 associated with 1-characteristics) and 3-shocks
(m3 < 0 associated with 3-characteristics). Then the velocities are given by v± =
z± + s, with the speed s being chosen.
It is then possible (but intricate) to give a parameterization of 1-shocks and 3-
shocks satisfying the Lax entropy condition. The entropy condition is related to
the thermodynamic entropy S = S(ρ, e), which satisfies the PDE ρ2Sρ + pSe = 0.
This entropy has to satisfy m[S]> 0 across the shock, with the result that fluid
particles gain entropy (S increases) as they pass through the shock.
In this chapter we have summarized some basic mathematical properties of the
key equations of fluid mechanics. The mathematical theory of these equations is
vast and is a very lively topic of current research. Moreover, the equations are
used to explain all sorts of phenomena involving fluid flow, from the swimming

of small organisms to the prediction of weather patterns and the aerodynamics of
airplanes. The interested reader will find the books of Chorin and Marsden [7]
and of Serre [39] useful introductions to some more of the mathematical
properties of these equations, whereas the text of Acheson [2] is an intuitive
treatment of the equations, including informal discussions of many applications
and the calculation of physically meaningful solutions.
PROBLEMS
1. Show that if ρ is constant in the Euler equations, then the equation for
conservation of momentum collapses to the corresponding equation in the
incompressible Euler equations. For smooth solutions of the incompressible Euler
equations, derive an energy equality and relate it to the energy equation for the
compressible case.
2. Let u = (u, v, w), x = (x, y, z). Write the Navier-Stokes system (15.2) in
components (so that there are four scalar equations, and ∇ is replaced by partial
derivatives with respect to x, y, z).
3. Consider flow between two parallel horizontal plates held a distance h > 0
apart. Suppose the bottom plate is stationary and the top plate is moving
horizontally at speed U. Use the Navier-Stokes system, neglecting gravity, to find
a simple steady flow (independent of time t) in which fluid particles move
parallel to the plates. Assume the plates are at z = 0, h, there is no dependence
on y, and the top plate is moving to the right, parallel to the x-axis. Begin by
sketching the plates and how you think the flow might look. Then consider which
components of the velocity u = (u, v, w) can be set to zero. You can then solve
the reduced set of equations for the velocity and pressure.

APPENDIX A Multivariable Calculus
When the distinction between scalars (real or complex) and vectors (n-tuples) is
needed, we use boldface for vectors. Thus, x = (x1, … xn) ∈ Rn.
We generally use the notation U to denote an open subset of Rn. Then ∂U is
the boundary of U, and 
 is the closure of U. If U is bounded, then 
 is
closed and bounded, hence it is compact. For example, if f : Rn → R is a function,
then f has support defined as supp 
; thus, f has compact support
if the function is zero outside a bounded set.
If U has a C1 boundary ∂U, then the unit normal ν = ν(x) varies continuously
with x ∈ ∂U. For example, the unit ball U = {x ∈ Rn : |x| < 1} has as its
boundary the unit sphere ∂U = {x ∈ Rn : |x| = 1}, and unit outward normal ν(x)
= x.
The open ball with center at x and radius r > 0 is B(x, r) = {y ∈ Rn : |y − x|
< r}. To calculate ωn, the surface area of the unit sphere ∂B(0, 1) in Rn, we begin
by integrating e−π|x|2 over Rn:
Using the fact that the surface area of ∂B(0, r) is rn−1ωn, we have
(The gamma function Γ is defined below.) Thus 
. This expression
leads to the familiar circumference of the unit circle: ω2 = 2π, and area of the
unit sphere in R3: ω3 = 4π.
Let αn denote the volume of the unit ball B(0, 1) in Rn. It follows that

In particular, α2 = π is the area of the unit disk, and α3 = 4π/3 is the volume of
the unit ball in R3.
The integral averages of a function f over B(x, r) and ∂B(x, r) are defined by
The chain rule gives formulas for differentiating the composition of two
functions. It takes various useful forms, depending on the number of variables
involved. Consider x ∈ Rn and two functions f : Rn → R and y : Rn → Rn, so that f
= f(y) and y = y(x). Then we have
Now consider a function with an extra variable t ∈ R : F = F(t, y), and suppose y
= y(t). Then it follows that
If g : R → R and ξ : Rn → R with g = g(ξ) and ξ = ξ(x), then the following
holds:
For example, for a traveling wave u(x, t) = f(x − ct) with wave speed c, we have
Let F : Rn → R be C1. Then (away from points where ∇F(x) = 0) the equation
F(x) = const defines a manifold M of dimension n − 1, sometimes called a
hypersurface or a level surface of F. For example, if n = 3, then the manifold is a
two-dimensional surface. Let x0 ∈ M. If ∇F(x0) ≠ 0 then the unit normal ν(x0) is
given by

Let f : Rn → R be C∞. The Taylor series of f about x0 ∈ Rn is given by
where 
. This expression uses multi-index notation, which is
explained in Section 10.2.
The gamma function is the function 
. Then Γ(1) = 1, 
, and the recurrence Γ(s + 1) = sΓ(s) implies that for integers n ≥ 0, Γ(n + 1)
= n!, which uses 0! = 1, a convention also implicit in the Taylor series.
The Inverse Function Theorem states that if a differentiable function f from Rn
to Rn has an invertible Jacobian matrix df (x0) at a point x0, then the function
itself is invertible in a neighborhood of that point, and the inverse is as
differentiable as f.
Theorem A.1. (Inverse Function Theorem) Let U ⊂ Rn be open, and suppose f : U →
Rn is Ck for some k ≥ 1. Let x0 ∈ U, and y0 = f(x0). Suppose the Jacobian J = det(df
(x0)) is nonzero. Then there are open sets V ⊂ U and W ⊂ Rn, with x0 ∈ V, y0 ∈ W,
such that:
1. f : V → W is one-to-one and onto, and
2. the inverse f−1 : W → V is Ck.
The Implicit Function Theorem relates to solving simultaneous nonlinear
equations
near a known solution (x0, y0), where F : Rn × Rm → Rn is differentiable.
Similarly to the Inverse Function Theorem, the function is assumed to be linearly
nondegenerate at this solution in the sense that the Jacobian with respect to x is
nonzero. The conclusion is that the equations can be solved uniquely locally for x
as a function of y for y close to y0, with x(y0) = x0, and with x(y) as
differentiable as F.
Theorem A.2. (Implicit Function Theorem) Let U ⊂ Rn × Rm be open, and suppose f
: U → Rn is Ck for some k ≥ 1. Suppose that
Then there are open sets V ⊂ U and W ⊂ Rm, with (x0, y0) ∈ V, y0 ∈ W, and a Ck
function 
such that:

1. 
;
2. 
 for all y ∈ W; and
3. if (x, y) ∈ V and F (x, y) = 0, then 
.
The contraction mapping principle can be used to prove the Inverse Function
Theorem. The principle is stated here in the broader setting of a complete metric
space X with metric d. A mapping T : X → X is a contraction if there exists a
constant L ∈ (0, 1) such that
The contraction mapping principle, also known as the Banach Fixed-Point
Theorem, states that if (X, d) is a nonempty complete metric space with metric d,
and T : X → X is a contraction mapping, then T has a unique fixed point x∗ ∈ X.
That is, T (x∗) = x∗. Furthermore, x∗ = limn → ∞ xn, where x0 ∈ X is arbitrary,
and xn+1 = T (xn), n ≥ 0.
Green’s Theorem in the plane relates a double integral over a bounded open set
U ⊂ R2 to a line integral along the curve ∂U.
Theorem A.3. (Green’s Theorem in the plane) Suppose 
 is
continuous and is C1 in U, and the boundary curve is piecewise C1. Then
where τ is the unit tangent in the counterclockwise direction, s denotes arc length, and
dA = dxdy in Cartesian coordinates is the area metric in the plane.
Stokes’ Theorem is similar to Green’s Theorem in the plane, but it relates the
line integral over the closed boundary curve ∂S of a two-dimensional surface S ⊂
R3 to the surface integral over S:
where F : R3 → R3. Here, ν is the normal to S consistent with τ and the right-
hand rule.
Green’s Theorem in the plane is the two-dimensional version of the Divergence
Theorem for a vector field 
, where U ⊂ Rn. The theorem relates the
net flux of F through the boundary ∂U to the total divergence of F over the entire
region U:

Here, ν is the unit outward normal.
A more fundamental integration is the component version of the Divergence
Theorem. Let 
 be a function in 
. Then for each j = 1, …, n,
Note that the Divergence Theorem and this result are equivalent. However, the
latter result is a direct consequence of the Fundamental Theorem of Calculus.
The Leibniz integral rule describes how to bring a partial derivative with
respect to one variable into the integral of a multivariable function when the
integral is taken with respect to the other variable:
To reverse the order of integration in a double integral, we use Fubini’s
Theorem.
Theorem A.4. (Fubini’s Theorem) f = f(x, y) be continuous over the rectangle R =
{(x, y) : a ≤ x ≤ b, c ≤ y ≤ d}. Then
Two elements x and y of a vector space X with an inner product (·, ·) are
orthogonal if (x, y) = 0. A set S ⊂ X is called orthonormal if all x ≠ y in S are
orthogonal and (x, x) = 1.
A function f : Rn → R is convex on Rn if for all x, y in Rn t ∈ (0, 1),
f is strictly convex if the inequality is strict.

APPENDIX B Analysis
The Lebesgue measure | · | : M → [0, ∞]is a function defined on the family M of
Lebesgue measurable subsets of Rn, which includes all open subsets. The family M
is a σ-algebra, meaning that 0 and Rn are in M; complements, countable unions,
and intersections of members of M are also in M. The Lebesgue measure has
properties consistent with being a generalization of the idea of volume. The
measure of any ball is the volume of the ball; the measure of disjoint unions of
countable families of measurable sets is the sum of their measures; measurable
subsets of sets of measure zero also have measure zero. A property is said to hold
almost everywhere, abbreviated as a.e., if the property holds except on a set of
measure zero.
A function f : Rn → R is measurable if for every open S ⊂ R, the inverse image
f−1(S) ∈ M. Thus, continuous functions are measurable. Nonnegative measurable
functions are integrable, the integral being defined using approximation by
simple functions. More generally, if f is measurable, then f is integrable as long as
the positive f+ and negative f− parts of f can comprise the integral:
where one of the integrals on the right-hand side is finite.
The essential supremum of a measurable function f is defined by
Among many properties of measurable functions, the following two theorems
are especially important for PDE.
Theorem B.1. (Monotone Convergence Theorem) If a sequence 
 of integrable
functions is monotonically increasing:
then
Theorem B.2. (Dominated Convergence Theorem) For a sequence 
 of integrable
functions with fk → f a.e., as k → ∞, and |fk| ≤ g a.e., and for a positive measurable
function g with ∫Rn g dx < ∞, the following limit holds:

Further details on Lebesgue measure and integrable functions can be found in
summary in the Appendix in Evans [12], and in many books on measure and
integration, for example, Ambrosia et al. [3]. A major advantage of using
Lebesgue measure to generalize the notion of integral is that spaces Lp, p ≥ 1 of
measurable functions are complete.
Consider a real or complex vector space X over the scalar field R or C
(respectively). A norm || · || : X → [0, ∞) is a function that is required to have
these properties:
1. ||x|| = 0 if and only if x = 0;
2. ||αx|| = |α|||x||, for all x ∈ X, and scalar α; and
3. ||x + y|| ≤ ||x|| + ||y|| for all x, y in X (the triangle inequality).
A norm defines a metric ρ, expressing the distance between elements x, y ∈ X:
ρ(x, y) = ||x − y||.
Let X be a vector space with norm || · ||. A sequence 
 is a Cauchy
sequence if for every ϵ > 0, there is N = Nϵ > 0 such that
That is, ||xj − xk|| → 0 as j, k → ∞.
X is complete if every Cauchy sequence in X converges to a limit in X: ||xk −
x|| → 0 for some x ∈ X. A Banach space is a complete normed vector space. Thus,
Rn and Cn are Banach spaces, and so is C([a, b]) with the norm ||f || = maxa≤x≤b
|f(x)|.
An inner product (·, ·) : X × X → R is a function satisfying
1. 
 for all x, y ∈ X;
2. (x, x) ≥ 0 for all x ∈ X;
3. (x, x) = 0 if and only if x = 0; and
4. (αx, y) = α(x, y), for all x, y ∈ X, and scalar α.
An inner product defines a norm by 
. A Banach space X that has an
inner product defining its norm is a Hilbert space. Thus, a Hilbert space is a
complete inner product space.
Spaces of infinite sequences of numbers are convenient examples of infinite-

dimensional vector spaces. The space ℓp (“little ell p”), with p ≥ 1, consists of all
sequences 
 satisfying 
 with norm ||x||p = 
. The
space ℓp is a Banach space. The space ℓ2 is a Hilbert space with inner product 
, the infinite-dimensional version of the Euclidean inner product.
When p = ∞, ℓ∞ is the Banach space of all bounded sequences with norm
||x||∞ = supk |xk|.
The space c of convergent sequences x of real or complex numbers with the ℓ∞
norm ||x||∞ = supk |xk| is a closed subspace of ℓ∞ and is thus a Banach space.
The subspace c0 consisting of sequences 
 with limk → ∞ xk = 0 is also a
Banach space.
The Weierstrass M-test states that if {fk} is a sequence of real- or complex-
valued functions defined on a set U ⊂ Rn, and there is a sequence of positive
numbers Mk such that for all k ≥ 1 and all x ∈ U,
then the series 
 converges uniformly on U.

APPENDIX C Systems of Ordinary Differential Equations
Let F : Rn → Rn be continuous. We say that a C1 curve C = {x = x(t), t ∈ I}
(where I is an interval and x : I → Rn is C1) is an integral curve of the vector
field F if
Integral curves are sometimes called trajectories. Since the ODE system is
autonomous, integral curves are translation invariant: for any t0 ∈ R, x(t + t0)
traces the same curve C in Rn as t varies.
Consider the nonlinear autonomous system of two ODE:
Equilibria are points (u0, v0) ∈ R2 satisfying F(u0, v0) = 0. Behavior of the system
(C.1) near an equilibrium (u0, v0) is related to the linearized system
where
is the Jacobian. Let’s assume the eigenvalues λ1, λ2 of dF(u0, v0) are distinct (λ1 ≠
λ2) and have eigenvectors v1, v2, respectively. The eigenvalues and eigenvectors
may be complex. The general real solution of the linear system (C.2) is
where C1, C2 are arbitrary constants. If the eigenvalues are complex, then these
constants are complex, and it is understood that solutions are the real part of this
formula. The equilibrium is classified as stable if Re λj < 0, j = 1, 2, unstable if Re
λj > 0, j = 1, 2. Stable and unstable spirals correspond to complex conjugate
eigenvalues; the integral curves of F(u, v) spiral into or out of (u0, v0). If the
eigenvalues are real and of the same sign, then the equilibrium is a stable or
unstable node. If λ1, λ2 are real and have opposite signs, then the equilibrium is a

saddle, and there are integral curves that are tangent to the eigenvectors v1, v2 at
(u0, v0). These curves are the stable and unstable manifolds MS, MU of (u0, v0).

References
[1]    R. Abeyaratne and J. K. Knowles. Implications of viscosity and strain gradient effects for the kinetics of
propagating phase boundaries in solids. SIAM J. Appl. Math. 51:1205–1221, 1991.
[2]    D. J. Acheson. Elementary Fluid Dynamics. Oxford University Press, New York, 1990.
[3]    L. Ambrosio, G. DaPrato, and A. Mennucci. Introduction to Measure Theory and Integration, volume 10 of
Edizioni della Normale. Springer, New York, 2011.
[4]    S. S. Antman. The equations for large vibrations of strings. Am. Math. Monthly 87(5):359–370, 1980.
[5]    D. G. Aronson and H. F. Weinberger. Multidimensional nonlinear diffusion arising in population
genetics. Adv. Math. 30:33–76, 1978.
[6]    A. Bressan. Hyperbolic Systems of Conservation Laws: The One-Dimensional Cauchy Problem, volume 20 of
Mathematics and Its Applications. Oxford University Press, Oxford, 2000.
[7]    A. Chorin and J. L. Marsden. A Mathematical Introduction to Fluid Mechanics, volume 3 of Pure and
Applied Mathematics. Springer, New York, 1990.
[8]    S.-N. Chow and J. K. Hale. Methods of Bifurcation Theory, volume 251 of Comprehensive Studies in
Mathematics. Springer, New York, 1982.
[9]    J. D. Cole. On a quasi-linear parabolic equation occurring in aerodynamics. Quart. Appl. Math.
9(3):225–236, 1951.
[10]  J. D. Conway. Functions of One Complex Variable, 2nd edition, volume 11 of Graduate Texts in
Mathematics. Springer, New York, 1978.
[11]  M. G. Crandall and P. H. Rabinowitz. Bifurcation from simple eigenvalues. J. Funct. Anal. 8(2):321–
340, 1971.
[12]  L. C. Evans. Partial Differential Equations, volume 19 of Graduate Series in Mathematics. American
Mathematical Society, Providence, 2010.
[13]  P. C. Fife and J. B. McLeod. The approach of solutions of nonlinear diffusion equations to travelling
front solutions. Arch. Rational Mech. Anal. 65(4):335–361, 1977.
[14]  R. A. Fisher. The wave of advance of an advantageous gene. Ann. Eugen. 7:355–369, 1936.
[15]  Y. Forterre and O. Pouliquen. Flows of dense granular materials. Annu. Rev. Fluid Mech. 40:1–24, 2008.
[16]  P. R. Garabedian. Partial Differential Equations. American Mathematical Society, Chelsea, 1998.
[17]  C. S. Gardner, J. M. Greene, M. D. Kruskal, and R. M. Miura. Korteweg-deVries equation and
generalizations. VI. Methods for exact solution. Comm. Pure Appl. Math. 27(1):97–133, 1974.
[18]  J. Glimm. Solutions in the large for nonlinear hyperbolic systems of equations. Comm. Pure Appl. Math.
18(4):697–715, 1965.
[19]  M. G. Golubitsky and D. G. Schaeffer. Singularities and groups in bifurcation theory, volume 1. Springer,
New York, 1984.

[20]  J. M. N. T. Gray and A. R. Thornton. A theory for particle size segregation in shallow granular free-
surface flows. Proc. R. Soc. Lond. A, 461(2057):1447–1473, 2005.
[21]  E. Hopf. The partial differential equation ut + uux = μuxx. Comm. Pure Appl. Math. 3(3):201–230,
1950.
[22]  K. Hutter and K. R. Rajagopal. On flows of granular materials. Continuum Mech. Thermodyn. 6:81–139,
1994.
[23]  E. Isaacson, D. Marchesin, and B. Plohr. Transitional waves for conservation laws. SIAM J. Math. Anal.
21:837–866, 1990.
[24]  D. Jacobs, B. McKinney, and M. Shearer. Travelling wave solutions of the modified Korteweg-deVries-
Burgers equation. J. Diff. Equ. 116(2):448–467, 1995.
[25]  R. D. James. The propagation of phase boundaries in elastic bars. Arch. Rational Mech. Anal.
73(2):125–158, 1980.
[26]  A. Jeffrey. Applied Partial Differential Equations, An Introduction. Academic Press, New York, 2003.
[27]  F. John. Formation of singularities in one-dimensional nonlinear wave propagation. Comm. Pure Appl.
Math. 27:377–405, 1974.
[28]  T. Kato. Perturbation Theory for Linear Operators, volume 132 of Die Grundlehren der mathematischen
Wissenschaften. Springer, New York, 1966.
[29]  B. L. Keyfitz and H. C. Kranzer. A system of non-strictly hyperbolic conservation laws arising in
elasticity theory. Arch. Rational Mech. Anal. 72:219–241, 1980.
[30]  E. Kreyszig. Introductory Functional Analysis with Applications. Wiley, New York, 2007.
[31]  S. N. Kruzkov. First order quasilinear equations in several independent variables. Math. USSR SB,
10(2):217–243, 1970.
[32]  P. D. Lax. Hyperbolic systems of conservation laws II. Comm. Pure Appl. Math. 10:537–566, 1957.
[33]  P. D. Lax. Development of singularities of solutions of nonlinear hyperbolic partial differential
equations. J. Math. Phys. 5:611–613, 1964.
[34]  P. LeFloch. Hyperbolic Systems of Conservation Laws: The Theory of Classical and Nonclassical Shock
Waves. Birkhauser, Zurich, 2002.
[35]  A. Majda. Compressible Flow and Systems of Conservation Laws in Several Space Variables, volume 53 of
Applied Mathematical Sciences. Springer, New York, 1983.
[36]  R. M. Miura. The Korteweg–deVries equation: A survey of results. SIAM Rev. 18:412–459, 1976.
[37]  O. A. Oleinik. Uniqueness and stability of the generalized solution of the Cauchy problem for a quasi-
linear equation. Uspekhi Mat. Nauk 14(2):165–170, 1959.
[38]  W. Rudin. Principles of Mathematical Analysis. McGraw-Hill, New York, 1976.
[39]  D. Serre. Systems of Conservation Laws, I, II. Cambridge University Press, Cambridge, 2000.

[40]  M. Shearer. Nonuniqueness of admissible solutions of Riemann initial value problems for a system of
conservation laws of mixed type. Arch. Rational Mech. Anal. 93(1):45–59, 1986.
[41] M. Shearer, D. G. Schaeffer, D. Marchesin, and P. Paes-Leme. Solution of the Riemann problem for a
prototype 2 × 2 system of non-strictly hyperbolic conservation laws. Arch. Rational Mech. Anal.
97(4):299–320, 1987.
[42]  M. Slemrod. Admissibility criteria for propagating phase boundaries in a van der Waals fluid. Arch.
Rational Mech. Anal. 81(4):301–315, 1983.
[43]  J. Smoller. Shock Waves and Reaction-Diffusion Equations. Springer, New York, 1994.
[44]  E. Stade. Fourier Analysis, volume 69 of Pure and Applied Mathematics. Wiley, New York, 2005.
[45]  W. Strauss. Partial Differential Equations: An Introduction, second edition. Wiley, New York, 2008.
[46]  G. B. Whitham. Linear and Nonlinear Waves. Wiley, New York, 1974.
[47]  C. C. Wu. New theory of MHD shock waves. In M. Shearer, ed., Viscous Profiles and Numerical Methods
for Shock Waves. SIAM, Philadelphia, 1991.

Index
absorbing boundary condition, 88–90
amplitude, 43, 77, 114–115, 179
avalanche model, 36–38, 209–210, 223
backward characteristics, 51
balance law (see also conservation law), 11, 22–26, 45, 191, 196
Banach Fixed-Point Theorem, 256
Banach space, 102, 218, 260–261
beam equation, 17, 94–96
bending modulus, 94
Benjamin-Bona-Mahoney (BBM) equation, 17
Bessel functions, 127
Bessel’s inequality, 109–111, 156
bifurcation, 234–235
bilinear functional, 168, 170, 172
bilinear operator, 235
bistable equation, 181–186
boundary conditions, 3–4, 7
absorbing, 88–90
clamped-beam, 95
Dirichlet, 3, 75, 82–85, 89, 101, 124, 125, 127, 129, 134, 146, 149, 158
homogeneous, 4, 86, 100, 126, 134–136
inhomogeneous, 4, 90–91
Neumann, 4, 72, 75, 85–87, 89, 101, 102, 125
radiating, 88–89
Robin, 88, 101, 102
stress-free, 53, 225
Brownian motion, 65
Burgers equation, 5–6, 175–176, 206–208
Cole-Hopf transformation, 5, 175, 207
inviscid, 4–5, 7–8, 25–26, 38–40, 189–195

cantilevered beam, 95
Cauchy, Augustine-Louis, 17
Cauchy inequality, 153
Cauchy-Kovalevskaya Theorem, 11, 17, 20–21, 81
Cauchy problem, 32–40, 204, 207–211, 215, 218, 232
approximate solutions, 241–242
energy method and uniqueness, 56–59, 73–74
Fourier transform, 116
fundamental solution, 66–72
heat equation, 68–74
method of characteristics, 32–38
scalar conservation laws, 196
semi–infinite domain, 53
wave equation, 49–51
Cauchy-Schwartz inequality, 155, 162–164, 169,
Cauchy sequence, 102, 111, 156, 158, 166, 171, 260
centered rarefaction waves, 190–191, 193, 194, 196, 198, 226, 227, 236
chain rule, 14, 18, 30, 184, 254
change of variables, 15, 30, 66, 71, 116, 190, 197
characteristics, 65, 78, 85, 108, 189, 237, 241, 249
initial value problems, 29–40
d’Alembert’s solution, 48–56
Lax entropy condition, 194–195
systems of conservation laws, 219–222
undercompressive shocks, 204–211
variable coefficients, 217–219
clamped-beam boundary conditions, 95
classification, 12–16
closure, 51, 253
codimension one, 165
Cole-Hopf transformation, 5, 175, 207

combustion, 38, 65
compact support, 51, 71, 78, 120–121, 137, 144, 158, 202, 232–233, 253
completeness, 102–106, 110–112, 156, 159, 162, 166, 171, 255–256, 260
conservation law (see also balance law), 4, 11, 21, 24, 38, 43, 143, 245, 248
scalar conservation laws, 189–211
systems of first-order hyperbolic PDE, 215–242
constitutive law, 22–25, 45, 245, 248
contact discontinuity, 250
continuous dependence on data, 11, 12, 21, 50
continuous functions, 50, 81, 102–104, 135, 217–218, 259
continuum mechanics, 21, 215
contraction mapping principle, 218, 256
convection-diffusion, 175
convergence, 66, 69, 71, 81, 82, 166, 171, 176
convergence in the norm, 102
distributions, 140
Fourier sine series, 84–85
Gibbs phenomenon, 113–114
ℓ2, 156–157
L2, 110–112
Laplace’s equation, 127–130
pointwise, 103–108
Poisson’s equation, 120–122
power series, 19–20
test functions, 137–140
theorems, 259–261
uniform, 87, 108–110
convex function, 199, 201, 203, 257
convolution product, 71, 120, 121, 129, 138
d’Alembert solution, 43, 48–56, 58–61, 78, 91
dam-break problem, 239–241

Darcy’s law, 198
density, 5, 6, 21–26, 94, 116, 119, 221, 223
fluid mechanics, 245–247
traffic flow, 197–198
dependent variable, 2
differential operator, 13, 14, 48, 100–101, 112, 135, 157, 167, 172
Green’s functions, 144–145
symmetric operator, 167, 168
diffusion, 1–3, 23, 65, 179, 207
diffusion equation, 23
Dirac delta function, 71, 106, 136, 202
Dirichlet boundary conditions, 3
boundary value problem, 125
energy principle, 75
Green’s functions, 134
heat equation, 82–86
Laplace’s equation, 127
maximum/minimum principle, 124
method of images, 149
Poisson’s equation, 146
Poisson’s formula, 129
symmetry, 101
Dirichlet kernel, 106, 129
dispersion relation, 16–17, 95, 177
displacement, 1–3, 24, 44, 48, 49, 51, 55, 133, 220, 223
distributional derivative, 141
distributions, 71, 87, 136–144, 202, 232
convergence, 140
regular, 139, 142, 144
singular, 120, 139
divergence form, 167

divergence theorem, 22–24, 146, 210, 246, 256
domain of dependence, 51–52, 58
dominated convergence theorem, 138, 260
dual space, 138
Duhamel’s principle, 57–59, 77–78
eigenfunction, 66, 84–86, 99–101, 112, 128
eigenvalue problem, 15, 84, 112, 168, 205, 230, 248, 263
absorbing boundary conditions, 89
bifurcation, 234
characteristic speeds, 219
eigenfunctions for ODE, 99–102
elastic string equations, 223, 225–227
Fisher’s equation, 180–182
heat equation, 84–91
linear systems, 215–216
p-system, 221
wave equation, 92–93
elastic modulus, 46, 94
elastic rod, 43–46
elastic string equations, 43, 46–48, 57, 223–226
elasticity, 11, 17, 24, 38, 43, 133, 204, 220, 245
elliptic PDE, 13–17, 45, 103, 119, 158, 161, 232
energy function, 23, 56, 75, 87, 247
energy method, 43, 56–57, 65–66, 67, 73–75, 125, 177, 180
entropy flux, 201, 202
entropy inequality, 202–203
equation of state, 5, 248, 249
equivalence class, 104, 154
essential supremum, 154, 158, 259
essentially bounded, 154
Euclidean norm, 102, 157

Euler equations, 6, 247–250
Euler formulas, 82, 84, 91, 105
Eulerian variables, 30, 44
evolution equation, 2, 6
existence, 11–12, 18–20, 50, 57, 66, 73, 103, 165, 218, 226
bistable equation, 183–185
elliptic theory, 168–176
method of characteristics, 33–38
Poisson’s equation, 161–162
extension, 55, 60, 72–73, 82, 87, 92, 113
Fick’s law, 23, 179
Fisher’s equation, 4–5, 179–180
flux, 22–26, 73, 88, 125, 179, 196, 256
entropy flux, 201
convex flux function, 189, 196, 197, 201–204
non-convex flux function, 198–199
Fourier coefficients, 82–94, 109–113, 114, 126, 129
Fourier mode, 16–17, 177
Fourier series, 81–114, 129, 155
convergence, 84, 90, 102–114
convergence in L2, 110–112
Gibbs phenomenon, 113–114
pointwise convergence, 105–108
uniform convergence, 108–110
Fourier transform, 13, 95, 114–117
Fourier’s law of heat conduction (transfer, flow), 23, 72, 179
frequency, 16, 17, 21, 91, 95, 114–115, 177
Fubini’s theorem, 257
fundamental solution, 65, 72, 116, 207
Duhamel’s principle, 77–78
Green’s functions, 133–147

heat equation, 66–68
Poisson’s equation, 119–122
fundamental theorem of calculus, 24–25, 256
gamma function, 253, 255
genuine nonlinearity, 219, 226, 231, 249
Gibbs, Josiah, 113
Gibbs Phenomenon, 113–114
Green, George, 133
Green’s function, 65, 127, 130, 133–138, 144–149, 161
fundamental solution, 133–147
method of images, 147–149
Green’s identity, 121, 125
Green’s theorem in the plane, 256
group velocity, 17
Hadamard, Jacques S., 11, 21
Hamiltonian, 178, 180, 186
Hamilton-Jacobi equations, 29
harmonic function, 120–125, 129, 146–148
maximum principle, 123
mean value property, 122
heat equation, 2–3, 5, 13, 15, 17, 23, 65–78, 81–91
Cauchy problem, 68–72
Duhamel’s principle, 77
energy method, 65, 73–74
fundamental solution, 65, 66–68, 72, 77–78, 116, 207
maximum principle, 65, 75–77
separation of variables, 66, 82–90
heteroclinic orbit, 183–186, 205
Hilbert space, 103, 104, 156, 159, 164–170, 260
Hölder’s inequality, 154–155
Holmgren, Erik, 20–21

homoclinic orbit, 178, 182–183
homogeneity, 4, 94
homogeneous boundary condition, 4, 72, 86, 125, 126
eigenfunctions, 100–101
Green’s functions, 134–136
Hooke’s law, 45–46, 221, 224–225
Huygens’ principle, 43, 59, 61
hyperbolic PDE, 13–15, 43, 65, 78, 215, 248
ill-posedness, 11, 17, 21
implicit function theorem, 38, 234–235, 255
incompressibility, 6, 119, 246
independent variable, 2
inhomogeneity, 4
inhomogeneous boundary condition, 90
initial condition, 2–3, 7, 9, 11, 21, 50, 53, 58, 69, 78, 102, 142, 189, 190, 195
Cole-Hopf transformation, 207–211
dam break problem, 240–241
dispersion relation, 16–18
linear systems, 215–217
method of characteristics, 29–39
separation of variables for heat equation, 83–86
separation of variables for wave equation, 91–95
systems of hyperbolic conservation laws, 231–232
integral average, 122–124, 254
integral kernel, 129, 133, 135
inverse function theorem, 33–36, 255
inviscid Burgers equation, 4, 7–8, 25–26, 38–40, 189–195, 197
Jacobian matrix, 34, 180, 181, 205, 219, 255, 263
kinetic energy, 43, 56, 177, 247
Korteweg-deVries (KdV) equation, 5, 17, 176–186, 204
Kovalevskaya, Sofia Vasilyevna, 17, 20, 21, 81

L∞ space, 154
ℓ2 space, 155–156
L2 space, 100, 104, 164
Lp space, 154–155
Lagrangian variables, 30, 44, 221–222
Laplace’s equation, 3, 13, 17, 21, 66, 119–130
cylindrical domains, 127–130
fundamental solution, 119–122
Hadamard ill-posedness, 21
polar coordinates, 127
rectangular domain, 125–127
separation of variables, 125–130
spherical domains, 127–130
Laplace, Pierre-Simon, 119
Laplacian, 2, 119, 121, 127, 146–149, 179
Lax entropy condition, 183, 194–195, 196, 201–206, 211, 236–238, 241, 250
Lax-Milgram theorem, 170–172
Lebesgue-integrable functions, 104, 157
Lebesgue measure, 104, 259, 260
Leibniz integral rule, 257
Lighthill-Whitham-Richards model, 25
linear PDE, 2, 4
linear transport equation, 2, 4–7, 16, 48, 177, 216
linearly degenerate, 219, 221, 249
locally integrable function, 138–139, 141, 144
logistic equation, 5, 179
Lyapunov-Schmidt reduction, 235
maximum principle, 12, 65, 75–77, 122–125
mean-value property, 122–124, 129
measurable function, 104, 154, 259–260
measure zero, 104, 259

method of characteristics, 4, 65, 78, 85, 108, 189, 237, 241, 249
d’Alembert’s solution, 48–56
initial value problems, 29–40
Lax entropy condition, 194–195
systems of conservation laws, 219–222
undercompressive shocks, 204–211
variable coefficients, 217–219
method of images, 147–149
method of spherical means, 59–61
minimum principle, 77, 124
mollifier, 137–138
momentum, 6, 21–24, 44–46, 177, 222–248
monotone convergence theorem, 157, 259
multi–index notation, 157–158
Navier-Stokes equations, ix, 6, 222, 245–247
Neumann boundary condition, 4, 72, 75, 85–87, 88–89, 101, 102, 125
noncharacteristic curve, 34–36
nondimensionalization, 94, 246–247
nonlinear heat equation 93–94
nonlinear PDE, 16
nonlinear small disturbance equation, 16
nonlinear transport equation 196, 248
open ball, 121, 253
order of a PDE, 2
orthogonal, 6, 14, 15, 81, 86, 88, 100, 101, 111–112, 165, 220, 257
orthonormal, 14, 108, 110–112, 156, 257
oscillations, 17, 93, 113, 180
parabolic boundary, 76–77
parabolic manifold, 205
parabolic PDE, 13–16, 65, 71, 75, 78, 103, 159, 179
Parseval’s identity, 111–113, 156

partial differential equation, 1–2
periodic extension, 82, 87, 92, 113
phase speed (phase velocity), 17
physical configuration, 43–44
plane waves, 179
Poincaré inequality, 162–164, 169
pointwise convergence, 102–117, 129
Poisson kernel, 129–130
Poisson’s equation, 119–121, 161–166
maximum principle, 122–125
uniqueness, 124
Poisson’s formula, 129–130
population models, 5, 65, 179–180
porous medium equation, 5, 78, 93
separation of variables, 93–94
potential energy, 43, 56
power series, 17–18, 19, 21, 81
principal part, 13–14, 16
principal symbol, 13–14, 16, 173
p-system, 215, 220–222, 223, 226
propagation of singularities, 229–230
rarefaction waves, 228
Riemann problem, 236–239
shock formation, 230–232
weak solutions and shock waves, 232–234
quarter-plane solution, 53–54, 72
quasilinear first-order equation, 32, 35
quasilinear PDE, 28, 157
quasilinear wave equation, 16, 220, 223, 248
radiating boundary condition, 88–89
random choice method, 242

random walk, 65
Rankine-Hugoniot condition, 143, 202, 210, 241
Euler equations, 249–250
inviscid Burgers equation, 191–198
Riemann problem, 236–237
systems, 233–234
rarefaction wave, 190–200, 219, 226–228, 236–242, 249
real analytic, 19–21
reference configuration, 43, 44, 222–224
region of influence, 51–52, 71
regularity, 12, 50, 57, 66, 78, 103, 110, 153, 162
Reynolds number, 246–247
Riemann invariant, 219–223, 230–231, 241
Riemann-Lebesgue Lemma, 106–109
Riemann problem, 190–195, 200, 206, 236–241
Riesz representation theorem, 164–166, 168–172
Robin boundary conditions, 88, 101, 102
scale invariance, 66, 189–190
semilinear wave equation, 16
separation of variables, 21, 66, 81–96, 100, 125–130, 177, 210
beam equation, 94–95
Fourier series, 81
heat equation, 82–90
nonlinear heat equation, 93
wave equation, 91–92
series, 47, 66, 155, 157
convergence, 99–117
Fourier series, 81–88, 90–93
Laplace’s equation, 126–130
power series, 18–21
Taylor series, 254–255

shallow water equations, 6, 222–223, 239–242
shock wave, 4, 8, 38, 143, 176, 186, 219, 249
admissibility, 236–237
bifurcation theory, 234–236
formation, 230–232
inviscid Burgers equation, 191–195
Lax entropy condition, 201–211
p-system, 238–239
weak solutions, 232–234
shooting argument, 183
singularity, 4, 35, 78, 119–121, 130, 146–147, 156, 158
smoothness, 12, 50, 57, 66, 78, 103, 110, 153, 162
Sobolev space, 158–159, 161–172
solitary wave, 5, 175–179, 183
solution of a PDE, 3
stability, 11, 211
stable manifold, 182–184
Stokes equations, 247
Stokes’ theorem, 256
strain, 45–47, 156, 220–224
strain softening, 45, 221
stress, 21, 44–46, 53, 220–221, 224–225
stress-free boundary condition, 53, 225
strict hyperbolicity, 215, 221, 223, 225–226, 235
strictly convex flux, 202–203
strong maximum principle, 77, 123–124
Sturm-Liouville problem, 99–101, 112–113
sup norm (uniform norm), 102–103, 218
superposition, 176
support of a function, 51–52, 54, 71, 78, 120–121, 137, 144, 158, 202, 232–233, 253
symmetric differential operator, 101, 167, 168

systems of PDE, 204, 215–241
characteristics, 215–219
conservation laws, 219–220
elastic string equations, 223–226
genuine nonlinearity, 219, 226, 231, 249
hyperbolicity, 219–221, 223, 225, 248–249
linear systems, 215
p-system, 215, 220–222, 223, 226, 228–232, 237–239
Rankine-Hugoniot condition, 233–236
rarefaction wave, 219, 226–230, 236–242, 249
Riemann invariant, 219–223, 230–231, 241
shallow water equations, 222–223, 239
shock formation, 230–234, 236–237
strict hyperbolicity, 219
Taylor Series (Taylor’s theorem), 19, 254
test functions, 136–144, 157, 167, 202, 232–233
trace theorems, 159
traffic flow model, 24–26, 191, 197–199
translation invariance, 60, 68
traveling wave, 2, 5–8, 16–17, 91, 175–186, 204–206, 216
triangle inequality, 50, 70, 107, 112, 154–155, 260
Tricomi equation, 16
ultrahyperbolicity, 15
undercompressive shock, 204–206
uniform convergence, 50, 69, 85, 102–117, 137, 261
uniform norm (sup norm), 102–103
uniqueness, 3, 6–7, 11–12, 18, 30, 65, 124, 142, 230
Cauchy problem, 49–50
elliptic theory, symmetric case, 168–172
energy method, 56–57, 73–74
fixed point, 218–219

Holmgren theorem, 20
implicit function theorem, 255–256
ODE theory, 33–35
Poisson’s equation, 161–166
rarefaction waves, 190–193
shock waves, 236–237
unit normal, 121, 253, 254
unstable manifold, 182–185
vibrating string, 46–52, 91
viscosity, 5, 6, 207, 247
wave equation, 2–3, 13, 43–61, 71, 72, 108, 158, 224, 248
balance laws, 23–24
characteristics, 48–49
d’Alembert’s solution, 48–19
domain of dependence, 51–52, 58
Duhamel’s principle, 57–59, 77–78
energy, 56–57
method of spherical means, 59–61
nonlinear, 17
quasilinear, 16
region of influence, 51–52, 71
semilinear, 16
separation of variables, 91–93
system, 216–217
uniqueness, 56–57
vibrating string, 46–52, 91
wave front tracking, 241
wave number, 13, 16–17, 21, 95, 177
wave speed, 2, 4, 6, 7, 17, 29, 43, 48, 52, 71, 242, 254
Burgers equation, 175–177
weak convergence, 87

weak derivative, 144, 157, 158, 162
weak formulation, 164, 232
weak maximum principle, 77, 124–125
weak solution, 3, 40, 50, 103, 172, 206, 210
inviscid Burgers equation, 189–193
Poisson’s equation, 161–162
second-order linear elliptic equations, 167–168
systems, 231–237
Weierstrass M-test, 87, 261
well-posedness, ix, 11–12, 21, 49, 50, 153, 191
Whitham, G. B., 17, 25, 177, 208
Young’s inequality, 153–154, 172


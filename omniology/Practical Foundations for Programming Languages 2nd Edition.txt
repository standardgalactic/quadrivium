Practical Foundations for Programming Languages
SECOND EDITION
Robert Harper
Carnegie Mellon University

Copyright © 2016 by Robert Harper.
All Rights Reserved.
This is a draft version of a book published by Cambridge University Press (http:
//www.cambridge.org). This draft is made available for the personal use of a single
individual. The reader may make one copy for personal use. No unauthorized distri-
bution of any kind is allowed.

Preface to the Second Edition
Writing the second edition to a text book incurs the same risk as building the second version of a
software system. It is difﬁcult to make substantive improvements, while avoiding the temptation
to overburden and undermine the foundation on which one is building. With the hope of avoiding
the second system effect, I have sought to make corrections, revisions, expansions, and deletions
that improve the coherence of the development, remove some topics that distract from the main
themes, add new topics that were omitted from the ﬁrst edition, and include exercises for almost
every chapter.
The revision removes a number of typographical errors, corrects a few material errors (espe-
cially the formulation of the parallel abstract machine and of concurrency in Algol), and improves
the writing throughout. Some chapters have been deleted (general pattern matching and polar-
ization, restricted forms of polymorphism), some have been completely rewritten (the chapter
on higher kinds), some have been substantially revised (general and parametric inductive deﬁ-
nitions, concurrent and distributed Algol), several have been reorganized (to better distinguish
partial from total type theories), and a new chapter has been added (on type reﬁnements). Titular
attributions on several chapters have been removed, not to diminish credit, but to avoid confusion
between the present and the original formulations of several topics. A new system of (pronounce-
able!) language names has been introduced throughout. The exercises generally seek to expand
on the ideas in the main text, and their solutions often involve signiﬁcant technical ideas that merit
study. Routine exercises of the kind one might include in a homework assignment are deliberately
few.
My purpose in writing this book is to establish a comprehensive framework for formulating
and analyzing a broad range of ideas in programming languages. If language design and pro-
gramming methodology are to advance from a trade-craft to a rigorous discipline, it is essential
that we ﬁrst get the deﬁnitions right. Then, and only then, can there be meaningful analysis and
consolidation of ideas. My hope is that I have helped to build such a foundation.
I am grateful to Stephen Brookes, Evan Cavallo, Karl Crary, Jon Sterling, James R. Wilcox,
and Todd Wilson for their help in critiquing drafts of this edition and for their suggestions for
modiﬁcation and revision. I thank my department head, Frank Pfenning, for his support of my
work on the completion of this edition. Thanks also to my editors, Ada Brunstein and Lauren
Cowles, for their guidance and assistance. And thanks to Evan Cavallo and Andrew Shulaev for
corrections to the draft.
Neither the author nor the publisher make any warranty, express or implied, that the deﬁ-

nitions, theorems, and proofs contained in this volume are free of error, or are consistent with
any particular standard of merchantability, or that they will meet requirements for any particular
application. They should not be relied on for solving a problem whose incorrect solution could
result in injury to a person or loss of property. If you do use this material in such a manner, it is at
your own risk. The author and publisher disclaim all liability for direct or consequential damage
resulting from its use.
Pittsburgh
July, 2015

Preface to the First Edition
Types are the central organizing principle of the theory of programming languages. Language fea-
tures are manifestations of type structure. The syntax of a language is governed by the constructs
that deﬁne its types, and its semantics is determined by the interactions among those constructs.
The soundness of a language design—the absence of ill-deﬁned programs—follows naturally.
The purpose of this book is to explain this remark. A variety of programming language features
are analyzed in the unifying framework of type theory. A language feature is deﬁned by its statics,
the rules governing the use of the feature in a program, and its dynamics, the rules deﬁning how
programs using this feature are to be executed. The concept of safety emerges as the coherence of
the statics and the dynamics of a language.
In this way we establish a foundation for the study of programming languages. But why these
particular methods? The main justiﬁcation is provided by the book itself. The methods we use are
both precise and intuitive, providing a uniform framework for explaining programming language
concepts. Importantly, these methods scale to a wide range of programming language concepts,
supporting rigorous analysis of their properties. Although it would require another book in itself
to justify this assertion, these methods are also practical in that they are directly applicable to imple-
mentation and uniquely effective as a basis for mechanized reasoning. No other framework offers
as much.
Being a consolidation and distillation of decades of research, this book does not provide an
exhaustive account of the history of the ideas that inform it. Sufﬁce it to say that much of the de-
velopment is not original, but rather is largely a reformulation of what has gone before. The notes
at the end of each chapter signpost the major developments, but are not intended as a complete
guide to the literature. For further information and alternative perspectives, the reader is referred
to such excellent sources as Constable (1986), Constable (1998), Girard (1989), Martin-L¨of (1984),
Mitchell (1996), Pierce (2002, 2004), and Reynolds (1998).
The book is divided into parts that are, in the main, independent of one another. Parts I and II,
however, provide the foundation for the rest of the book, and must therefore be considered prior
to all other parts. On ﬁrst reading it may be best to skim Part I, and begin in earnest with Part II,
returning to Part I for clariﬁcation of the logical framework in which the rest of the book is cast.
Numerous people have read and commented on earlier editions of this book, and have sug-
gested corrections and improvements to it. I am particularly grateful to Umut Acar, Jesper Louis
Andersen, Carlo Angiuli, Andrew Appel, Stephanie Balzer, Eric Bergstrom, Guy E. Blelloch, Il-
iano Cervesato, Lin Chase, Karl Crary, Rowan Davies, Derek Dreyer, Dan Licata, Zhong Shao,

Rob Simmons, and Todd Wilson for their extensive efforts in reading and criticizing the book. I
also thank the following people for their suggestions: Joseph Abrahamson, Arbob Ahmad, Zena
Ariola, Eric Bergstrome, William Byrd, Alejandro Cabrera, Luis Caires, Luca Cardelli, Manuel
Chakravarty, Richard C. Cobbe, James Cooper, Yi Dai, Daniel Dantas, Anupam Datta, Jake Don-
ham, Bill Duff, Matthias Felleisen, Kathleen Fisher, Dan Friedman, Peter Gammie, Maia Gins-
burg, Byron Hawkins, Kevin Hely, Kuen-Bang Hou (Favonia), Justin Hsu, Wojciech Jedynak, Cao
Jing, Salil Joshi, Gabriele Keller, Scott Kilpatrick, Danielle Kramer, Dan Kreysa, Akiva Leffert,
Ruy Ley-Wild, Karen Liu, Dave MacQueen, Chris Martens, Greg Morrisett, Stefan Muller, Tom
Murphy, Aleksandar Nanevski, Georg Neis, David Neville, Adrian Trejo Nu˜nez, Cyrus Omar,
Doug Perkins, Frank Pfenning, Jean Pichon, Benjamin Pierce, Andrew M. Pitts, Gordon Plotkin,
David Renshaw, John Reynolds, Andreas Rossberg, Carter Schonwald, Dale Schumacher, Dana
Scott, Shayak Sen, Pawel Sobocinski, Kristina Sojakova, Daniel Spoonhower, Paulo Tanimoto, Joe
Tassarotti, Peter Thiemann, Bernardo Toninho, Michael Tschantz, Kami Vaniea, Carsten Varming,
David Walker, Dan Wang, Jack Wileden, Sergei Winitzki, Roger Wolff, Omer Zach, Luke Zarko,
and Yu Zhang. I am very grateful to the students of 15–312 and 15–814 at Carnegie Mellon who
have provided the impetus for the preparation of this book and who have endured the many
revisions to it over the last ten years.
I thank the Max Planck Institute for Software Systems for its hospitality and support. I also
thank Espresso a Mano in Pittsburgh, CB2 Cafe in Cambridge, and Thonet Cafe in Saarbr¨ucken
for providing a steady supply of coffee and a conducive atmosphere for writing.
This material is, in part, based on work supported by the National Science Foundation under
Grant Nos. 0702381 and 0716469. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the
National Science Foundation.
Robert Harper
Pittsburgh
March, 2012

Contents
Preface to the Second Edition
iii
Preface to the First Edition
v
I
Judgments and Rules
1
1
Abstract Syntax
3
1.1
Abstract Syntax Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Abstract Binding Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2
Inductive Deﬁnitions
13
2.1
Judgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2
Inference Rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.3
Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.4
Rule Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.5
Iterated and Simultaneous Inductive Deﬁnitions . . . . . . . . . . . . . . . . . . . . .
18
2.6
Deﬁning Functions by Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.7
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3
Hypothetical and General Judgments
23
3.1
Hypothetical Judgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.1.1
Derivability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.1.2
Admissibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.2
Hypothetical Inductive Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.3
General Judgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.4
Generic Inductive Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30

viii
CONTENTS
II
Statics and Dynamics
33
4
Statics
35
4.1
Syntax
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.2
Type System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
4.3
Structural Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
4.4
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5
Dynamics
41
5.1
Transition Systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
5.2
Structural Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
5.3
Contextual Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
5.4
Equational Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
5.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
6
Type Safety
49
6.1
Preservation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.2
Progress
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
6.3
Run-Time Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
6.4
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
7
Evaluation Dynamics
55
7.1
Evaluation Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
7.2
Relating Structural and Evaluation Dynamics . . . . . . . . . . . . . . . . . . . . . . .
56
7.3
Type Safety, Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
7.4
Cost Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
7.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
III
Total Functions
61
8
Function Deﬁnitions and Values
63
8.1
First-Order Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
8.2
Higher-Order Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
8.3
Evaluation Dynamics and Deﬁnitional Equality
. . . . . . . . . . . . . . . . . . . . .
67
8.4
Dynamic Scope
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
8.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
9
System T of Higher-Order Recursion
71
9.1
Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
9.2
Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
9.3
Deﬁnability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
9.4
Undeﬁnability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
9.5
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77

CONTENTS
ix
IV
Finite Data Types
79
10 Product Types
81
10.1 Nullary and Binary Products
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
10.2 Finite Products
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
10.3 Primitive Mutual Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
10.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
11 Sum Types
87
11.1 Nullary and Binary Sums
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
11.2 Finite Sums
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
11.3 Applications of Sum Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
11.3.1 Void and Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
11.3.2 Booleans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
11.3.3 Enumerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
11.3.4 Options
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
11.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
V
Types and Propositions
95
12 Constructive Logic
97
12.1 Constructive Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
12.2 Constructive Logic
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
12.2.1 Provability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
12.2.2 Proof Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
12.3 Proof Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
12.4 Propositions as Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
12.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
13 Classical Logic
107
13.1 Classical Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
13.1.1 Provability and Refutability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
13.1.2 Proofs and Refutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
13.2 Deriving Elimination Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
13.3 Proof Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
13.4 Law of the Excluded Middle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
13.5 The Double-Negation Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
13.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
VI
Inﬁnite Data Types
119
14 Generic Programming
121
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

x
CONTENTS
14.2 Polynomial Type Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
14.3 Positive Type Operators
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
14.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
15 Inductive and Coinductive Types
127
15.1 Motivating Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
15.2 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
15.2.1 Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
15.2.2 Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
15.3 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
15.4 Solving Type Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
15.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
VII
Variable Types
137
16 System F of Polymorphic Types
139
16.1 Polymorphic Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
16.2 Polymorphic Deﬁnability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
16.2.1 Products and Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
16.2.2 Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
16.3 Parametricity Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
16.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
17 Abstract Types
149
17.1 Existential Types
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
17.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
17.1.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
17.1.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
17.2 Data Abstraction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
17.3 Deﬁnability of Existential Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
17.4 Representation Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
17.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
18 Higher Kinds
157
18.1 Constructors and Kinds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
18.2 Constructor Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
18.3 Expressions and Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
18.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
VIII
Partiality and Recursive Types
163
19 System PCF of Recursive Functions
165
19.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

CONTENTS
xi
19.2 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
19.3 Deﬁnability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
19.4 Finite and Inﬁnite Data Structures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
19.5 Totality and Partiality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
19.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
20 System FPC of Recursive Types
175
20.1 Solving Type Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
20.2 Inductive and Coinductive Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
20.3 Self-Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
20.4 The Origin of State
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
20.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
IX
Dynamic Types
183
21 The Untyped λ-Calculus
185
21.1 The λ-Calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
21.2 Deﬁnability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
21.3 Scott’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
21.4 Untyped Means Uni-Typed
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
21.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
22 Dynamic Typing
193
22.1 Dynamically Typed PCF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
22.2 Variations and Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
22.3 Critique of Dynamic Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
22.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
23 Hybrid Typing
203
23.1 A Hybrid Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
23.2 Dynamic as Static Typing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
23.3 Optimization of Dynamic Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
23.4 Static Versus Dynamic Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
23.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
X
Subtyping
211
24 Structural Subtyping
213
24.1 Subsumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
24.2 Varieties of Subtyping
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
24.3 Variance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
24.4 Dynamics and Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
24.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222

xii
CONTENTS
25 Behavioral Typing
225
25.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
25.2 Boolean Blindness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
25.3 Reﬁnement Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
25.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
XI
Dynamic Dispatch
237
26 Classes and Methods
239
26.1 The Dispatch Matrix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
26.2 Class-Based Organization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
26.3 Method-Based Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
26.4 Self-Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
26.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
27 Inheritance
249
27.1 Class and Method Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
27.2 Class-Based Inheritance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
27.3 Method-Based Inheritance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
27.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
XII
Control Flow
255
28 Control Stacks
257
28.1 Machine Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
28.2 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
28.3 Correctness of the K Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
28.3.1 Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
28.3.2 Soundness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
28.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
29 Exceptions
265
29.1 Failures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
29.2 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
29.3 Exception Values
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
29.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
30 Continuations
271
30.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
30.2 Continuation Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
30.3 Coroutines from Continuations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
30.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277

CONTENTS
xiii
XIII
Symbolic Data
279
31 Symbols
281
31.1 Symbol Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
31.1.1 Scoped Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
31.1.2 Scope-Free Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
31.2 Symbol References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
31.2.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
31.2.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
31.2.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
31.3 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
32 Fluid Binding
289
32.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
32.2 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
32.3 Type Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
32.4 Some Subtleties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
32.5 Fluid References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
32.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
33 Dynamic Classiﬁcation
297
33.1 Dynamic Classes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
33.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
33.1.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
33.1.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
33.2 Class References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
33.3 Deﬁnability of Dynamic Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
33.4 Applications of Dynamic Classiﬁcation
. . . . . . . . . . . . . . . . . . . . . . . . . . 301
33.4.1 Classifying Secrets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
33.4.2 Exception Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
33.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
XIV
Mutable State
305
34 Modernized Algol
307
34.1 Basic Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
34.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
34.1.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
34.1.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
34.2 Some Programming Idioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
34.3 Typed Commands and Typed Assignables . . . . . . . . . . . . . . . . . . . . . . . . . 313
34.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

xiv
CONTENTS
35 Assignable References
319
35.1 Capabilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
35.2 Scoped Assignables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
35.3 Free Assignables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
35.4 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
35.5 Benign Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
35.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
36 Lazy Evaluation
329
36.1 PCF By-Need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
36.2 Safety of PCF By-Need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
36.3 FPC By-Need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
36.4 Suspension Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
36.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
XV
Parallelism
339
37 Nested Parallelism
341
37.1 Binary Fork-Join . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
37.2 Cost Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
37.3 Multiple Fork-Join
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
37.4 Bounded Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
37.5 Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
37.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
38 Futures and Speculations
355
38.1 Futures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
38.1.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
38.1.2 Sequential Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
38.2 Speculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
38.2.1 Statics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
38.2.2 Sequential Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
38.3 Parallel Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
38.4 Pipelining With Futures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
38.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
XVI
Concurrency and Distribution
363
39 Process Calculus
365
39.1 Actions and Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
39.2 Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
39.3 Replication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369

CONTENTS
xv
39.4 Allocating Channels
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
39.5 Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
39.6 Channel Passing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
39.7 Universality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
39.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
40 Concurrent Algol
381
40.1 Concurrent Algol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
40.2 Broadcast Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
40.3 Selective Communication
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
40.4 Free Assignables as Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
40.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
41 Distributed Algol
391
41.1 Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
41.2 Dynamics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
41.3 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
41.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
XVII
Modularity
399
42 Modularity and Linking
401
42.1 Simple Units and Linking
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
42.2 Initialization and Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
42.3 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
43 Singleton Kinds and Subkinding
405
43.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
43.2 Singletons
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
43.3 Dependent Kinds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
43.4 Higher Singletons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
43.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
44 Type Abstractions and Type Classes
415
44.1 Type Abstraction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
44.2 Type Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
44.3 A Module Language
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
44.4 First- and Second-Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
44.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425

xvi
CONTENTS
45 Hierarchy and Parameterization
427
45.1 Hierarchy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
45.2 Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
45.3 Hierarchy and Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
45.4 Applicative Functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
45.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
XVIII
Equational Reasoning
437
46 Equality for System T
439
46.1 Observational Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
46.2 Logical Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
46.3 Logical and Observational Equivalence Coincide . . . . . . . . . . . . . . . . . . . . . 444
46.4 Some Laws of Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
46.4.1 General Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
46.4.2 Equality Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
46.4.3 Induction Law
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
46.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
47 Equality for System PCF
449
47.1 Observational Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
47.2 Logical Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
47.3 Logical and Observational Equivalence Coincide . . . . . . . . . . . . . . . . . . . . . 450
47.4 Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
47.5 Lazy Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
47.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
48 Parametricity
459
48.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
48.2 Observational Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
48.3 Logical Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
48.4 Parametricity Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
48.5 Representation Independence, Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . 469
48.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
49 Process Equivalence
471
49.1 Process Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
49.2 Strong Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
49.3 Weak Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
49.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477

CONTENTS
xvii
XIX
Appendices
479
A Background on Finite Sets
481


Part I
Judgments and Rules


Chapter 1
Abstract Syntax
Programming languages express computations in a form comprehensible to both people and ma-
chines. The syntax of a language speciﬁes how various sorts of phrases (expressions, commands,
declarations, and so forth) may be combined to form programs. But what are these phrases? What
is a program made of?
The informal concept of syntax involves several distinct concepts. The surface, or concrete, syn-
tax is concerned with how phrases are entered and displayed on a computer. The surface syntax
is usually thought of as given by strings of characters from some alphabet (say, ASCII or Uni-
code). The structural, or abstract, syntax is concerned with the structure of phrases, speciﬁcally
how they are composed from other phrases. At this level a phrase is a tree, called an abstract
syntax tree, whose nodes are operators that combine several phrases to form another phrase. The
binding structure of syntax is concerned with the introduction and use of identiﬁers: how they are
declared, and how declared identiﬁers can be used. At this level phrases are abstract binding trees,
which enrich abstract syntax trees with the concepts of binding and scope.
We will not concern ourselves in this book with concrete syntax, but will instead consider
pieces of syntax to be ﬁnite trees augmented with a means of expressing the binding and scope
of identiﬁers within a syntax tree. To prepare the ground for the rest of the book, we deﬁne in
this chapter what is a “piece of syntax” in two stages. First, we deﬁne abstract syntax trees, or
ast’s, which capture the hierarchical structure of a piece of syntax, while avoiding commitment
to their concrete representation as a string. Second, we augment abstract syntax trees with the
means of specifying the binding (declaration) and scope (range of signiﬁcance) of an identiﬁer.
Such enriched forms of abstract syntax are called abstract binding trees, or abt’s for short.
Several functions and relations on abt’s are deﬁned that give precise meaning to the informal
ideas of binding and scope of identiﬁers. The concepts are infamously difﬁcult to deﬁne properly,
and are the mother lode of bugs for language implementors. Consequently, precise deﬁnitions are
essential, but they are also fairly technical and take some getting used to. It is probably best to skim
this chapter on ﬁrst reading to get the main ideas, and return to it for clariﬁcation as necessary.

4
1.1 Abstract Syntax Trees
1.1
Abstract Syntax Trees
An abstract syntax tree, or ast for short, is an ordered tree whose leaves are variables, and whose in-
terior nodes are operators whose arguments are its children. Ast’s are classiﬁed into a variety of sorts
corresponding to different forms of syntax. A variable stands for an unspeciﬁed, or generic, piece
of syntax of a speciﬁed sort. Ast’s can be combined by an operator, which has an arity specifying
the sort of the operator and the number and sorts of its arguments. An operator of sort s and arity
s1, . . . , sn combines n ≥0 ast’s of sort s1, . . . , sn, respectively, into a compound ast of sort s.
The concept of a variable is central, and therefore deserves special emphasis. A variable is
an unknown object drawn from some domain. The unknown can become known by substitution
of a particular object for all occurrences of a variable in a formula, thereby specializing a general
formula to a particular instance. For example, in school algebra variables range over real numbers,
and we may form polynomials, such as x2 + 2 x + 1, that can be specialized by substitution of, say,
7 for x to obtain 72 + (2 × 7) + 1, which can be simpliﬁed according to the laws of arithmetic to
obtain 64, which is (7 + 1)2.
Abstract syntax trees are classiﬁed by sorts that divide ast’s into syntactic categories. For exam-
ple, familiar programming languages often have a syntactic distinction between expressions and
commands; these are two sorts of abstract syntax trees. Variables in abstract syntax trees range
over sorts in the sense that only ast’s of the speciﬁed sort of the variable can be plugged in for
that variable. Thus it would make no sense to replace an expression variable by a command, nor
a command variable by an expression, the two being different sorts of things. But the core idea
carries over from school mathematics, namely that a variable is an unknown, or a place-holder, whose
meaning is given by substitution.
As an example, consider a language of arithmetic expressions built from numbers, addition,
and multiplication. The abstract syntax of such a language consists of a single sort Exp generated
by these operators:
1. An operator num[n] of sort Exp for each n ∈N;
2. Two operators, plus and times, of sort Exp, each with two arguments of sort Exp.
The expression 2 + (3 × x), which involves a variable, x, would be represented by the ast
plus(num[2]; times(num[3]; x))
of sort Exp, under the assumption that x is also of this sort. Because, say, num[4], is an ast of sort
Exp, we may plug it in for x in the above ast to obtain the ast
plus(num[2]; times(num[3]; num[4])),
which is written informally as 2 + (3 × 4). We may, of course, plug in more complex ast’s of sort
Exp for x to obtain other ast’s as result.
The tree structure of ast’s provides a very useful principle of reasoning, called structural induc-
tion. Suppose that we wish to prove that some property P(a) holds of all ast’s a of a given sort.
To show this it is enough to consider all the ways in which a can be generated, and show that the
property holds in each case under the assumption that it holds for its constituent ast’s (if any). So,
in the case of the sort Exp just described, we must show

1.1 Abstract Syntax Trees
5
1. The property holds for any variable x of sort Exp: prove that P(x).
2. The property holds for any number, num[n]: for every n ∈N, prove that P(num[n]).
3. Assuming that the property holds for a1 and a2, prove that it holds for plus(a1; a2) and
times(a1; a2): if P(a1) and P(a2), then P(plus(a1; a2)) and P(times(a1; a2)).
Because these cases exhaust all possibilities for the formation of a, we are assured that P(a) holds
for any ast a of sort Exp.
It is common to apply the principle of structural induction in a form that takes account of the
interpretation of variables as place-holders for ast’s of the appropriate sort. Informally, it is often
useful to prove a property of an ast involving variables in a form that is conditional on the property
holding for the variables. Doing so anticipates that the variables will be replaced with ast’s that
ought to have the property assumed for them, so that the result of the replacement will have the
property as well. This amounts to applying the principle of structural induction to properties P(a)
of the form “if a involves variables x1, . . . , xk, and Q holds of each xi, then Q holds of a”, so that
a proof of P(a) for all ast’s a by structural induction is just a proof that Q(a) holds for all ast’s
a under the assumption that Q holds for its variables. When there are no variables, there are no
assumptions, and the proof of P is a proof that Q holds for all closed ast’s. On the other hand if x
is a variable in a, and we replace it by an ast b for which Q holds, then Q will hold for the result of
replacing x by b in a.
For the sake of precision, we now give precise deﬁnitions of these concepts. Let S be a ﬁnite set
of sorts. For a given set S of sorts, an arity has the form (s1, . . . , sn)s, which speciﬁes the sort s ∈S
of an operator taking n ≥0 arguments, each of sort si ∈S. Let O = { Oα } be an arity-indexed
family of disjoint sets of operators Oα of arity α. If o is an operator of arity (s1, . . . , sn)s, we say that
o has sort s and has n arguments of sorts s1, . . . , sn.
Fix a set S of sorts and an arity-indexed family O of sets of operators of each arity. Let X =
{ Xs }s∈S be a sort-indexed family of disjoint ﬁnite sets Xs of variables x of sort s. When X is clear
from context, we say that a variable x is of sort s if x ∈Xs, and we say that x is fresh for X , or just
fresh when X is understood, if x /∈Xs for any sort s. If x is fresh for X and s is a sort, then X , x is
the family of sets of variables obtained by adding x to Xs. The notation is ambiguous in that the
sort s is not explicitly stated, but determined from context.
The family A[X ] = { A[X ]s }s∈S of abstract syntax trees, or ast’s, of sort s is the smallest family
satisfying the following conditions:
1. A variable of sort s is an ast of sort s: if x ∈Xs, then x ∈A[X ]s.
2. Operators combine ast’s: if o is an operator of arity (s1, . . . , sn)s, and if a1 ∈A[X ]s1, ...,
an ∈A[X ]sn, then o(a1; . . . ;an) ∈A[X ]s.
It follows from this deﬁnition that the principle of structural induction can be used to prove that
some property P holds of every ast. To show P(a) holds for every a ∈A[X ], it is enough to show:
1. If x ∈Xs, then Ps(x).
2. If o has arity (s1, . . . , sn)s and Ps1(a1) and ... and Psn(an), then Ps(o(a1; . . . ;an)).

6
1.2 Abstract Binding Trees
For example, it is easy to prove by structural induction that A[X ] ⊆A[Y] whenever X ⊆Y.
Variables are given meaning by substitution. If a ∈A[X , x]s′, and b ∈A[X ]s, then [b/x]a ∈
A[X ]s′ is the result of substituting b for every occurrence of x in a. The ast a is called the target,
and x is called the subject, of the substitution. Substitution is deﬁned by the following equations:
1. [b/x]x = b and [b/x]y = y if x ̸= y.
2. [b/x]o(a1; . . . ;an) = o([b/x]a1; . . . ;[b/x]an).
For example, we may check that
[num[2]/x]plus(x; num[3]) = plus(num[2]; num[3]).
We may prove by structural induction that substitution on ast’s is well-deﬁned.
Theorem 1.1. If a ∈A[X , x], then for every b ∈A[X ] there exists a unique c ∈A[X ] such that
[b/x]a = c
Proof. By structural induction on a. If a = x, then c = b by deﬁnition, otherwise if a = y ̸= x,
then c = y, also by deﬁnition. Otherwise, a = o(a1; . . . ;an), and we have by induction unique
c1, . . . , cn such that [b/x]a1 = c1 and ... [b/x]an = cn, and so c is c = o(c1; . . . ;cn), by deﬁnition of
substitution.
1.2
Abstract Binding Trees
Abstract binding trees, or abt’s, enrich ast’s with the means to introduce new variables and symbols,
called a binding, with a speciﬁed range of signiﬁcance, called its scope. The scope of a binding is an
abt within which the bound identiﬁer can be used, either as a place-holder (in the case of a variable
declaration) or as the index of some operator (in the case of a symbol declaration). Thus the set
of active identiﬁers can be larger within a subtree of an abt than it is within the surrounding tree.
Moreover, different subtrees may introduce identiﬁers with disjoint scopes. The crucial principle is
that any use of an identiﬁer should be understood as a reference, or abstract pointer, to its binding.
One consequence is that the choice of identiﬁers is immaterial, so long as we can always associate
a unique binding with each use of an identiﬁer.
As a motivating example, consider the expression let x be a1 in a2, which introduces a variable
x for use within the expression a2 to stand for the expression a1. The variable x is bound by the
let expression for use within a2; any use of x within a1 refers to a different variable that happens
to have the same name. For example, in the expression let x be 7 in x + x occurrences of x in
the addition refer to the variable introduced by the let. On the other hand in the expression
let x be x ∗x in x + x, occurrences of x within the multiplication refer to a different variable than
those occurring within the addition. The latter occurrences refer to the binding introduced by the
let, whereas the former refer to some outer binding not displayed here.
The names of bound variables are immaterial insofar as they determine the same binding.
So, for example, let x be x ∗x in x + x could just as well have been written let y be x ∗x in y + y,
without changing its meaning. In the former case the variable x is bound within the addition, and

1.2 Abstract Binding Trees
7
in the latter it is the variable y, but the “pointer structure” remains the same. On the other hand the
expression let x be y ∗y in x + x has a different meaning to these two expressions, because now
the variable y within the multiplication refers to a different surrounding variable. Renaming of
bound variables is constrained to the extent that it must not alter the reference structure of the
expression. For example, the expression
let x be 2 in let y be 3 in x + x
has a different meaning than the expression
let y be 2 in let y be 3 in y + y,
because the y in the expression y + y in the second case refers to the inner declaration, not the
outer one as before.
The concept of an ast can be enriched to account for binding and scope of a variable. These
enriched ast’s are called abstract binding trees, or abt’s for short. Abt’s generalize ast’s by allowing
an operator to bind any ﬁnite number (possibly zero) of variables in each argument. An argu-
ment to an operator is called an abstractor, and has the form x1, . . . , xk.a. The sequence of variables
x1, . . . , xk are bound within the abt a. (When k is zero, we elide the distinction between .a and a
itself.) Written in the form of an abt, the expression let x be a1 in a2 has the form let(a1; x.a2),
which more clearly speciﬁes that the variable x is bound within a2, and not within a1. We of-
ten write ⃗x to stand for a ﬁnite sequence x1, . . . , xn of distinct variables, and write ⃗x.a to mean
x1, . . . , xn.a.
To account for binding, operators are assigned generalized arities of the form (υ1, . . . , υn)s, which
speciﬁes operators of sort s with n arguments of valence υ1, . . . , υn. In general a valence υ has the
form s1, . . . , sk.s, which speciﬁes the sort of an argument as well as the number and sorts of the
variables bound within it. We say that a sequence ⃗x of variables is of sort⃗s to mean that the two
sequences have the same length k and that the variable xi is of sort si for each 1 ≤i ≤k.
Thus, to specify that the operator let has arity (Exp, Exp.Exp)Exp indicates that it is of sort
Exp whose ﬁrst argument is of sort Exp and binds no variables, and whose second argument is
also of sort Exp and within which is bound one variable of sort Exp. The informal expression
let x be 2 + 2 in x × x may then be written as the abt
let(plus(num[2]; num[2]); x.times(x; x))
in which the operator let has two arguments, the ﬁrst of which is an expression, and the second
of which is an abstractor that binds one expression variable.
Fix a set S of sorts, and a family O of disjoint sets of operators indexed by their generalized
arities. For a given family of disjoint sets of variables X , the family of abstract binding trees, or abt’s
B[X ] is deﬁned similarly to A[X ], except that X is not ﬁxed throughout the deﬁnition, but rather
changes as we enter the scopes of abstractors.
This simple idea is surprisingly hard to make precise. A ﬁrst attempt at the deﬁnition is as the
least family of sets closed under the following conditions:
1. If x ∈Xs, then x ∈B[X ]s.

8
1.2 Abstract Binding Trees
2. For each operator o of arity (⃗s1.s1, . . . ,⃗sn.sn)s, if a1 ∈B[X ,⃗x1]s1, ..., and an ∈B[X ,⃗xn]sn,
then o(⃗x1.a1; . . . ;⃗xn.an) ∈B[X ]s.
The bound variables are adjoined to the set of active variables within each argument, with the sort
of each variable determined by the valence of the operator.
This deﬁnition is almost correct, but fails to properly account for renaming of bound variables.
An abt of the form let(a1; x.let(a2; x.a3)) is ill-formed according to this deﬁnition, because the
ﬁrst binding adds x to X , which implies that the second cannot also add x to X , x, because it is not
fresh for X , x. The solution is to ensure that each of the arguments is well-formed regardless of
the choice of bound variable names, which is achieved using fresh renamings, which are bijections
between sequences of variables. Speciﬁcally, a fresh renaming (relative to X ) of a ﬁnite sequence
of variables ⃗x is a bijection ρ : ⃗x ↔⃗x′ between ⃗x and ⃗x′, where ⃗x′ is fresh for X . We write bρ(a) for
the result of replacing each occurrence of xi in a by ρ(xi), its fresh counterpart.
This is achieved by altering the second clause of the deﬁnition of abt’s using fresh renamings
as follows:
For each operator o of arity (⃗s1.s1, . . . ,⃗sn.sn)s, if for each 1 ≤i ≤n and each fresh
renaming ρi : ⃗xi ↔⃗x′
i, we have bρi(ai) ∈B[X ,⃗x′
i], then o(⃗x1.a1; . . . ;⃗xn.an) ∈B[X ]s.
The renaming, bρi(ai), of each ai ensures that collisions cannot occur, and that the abt is valid for
almost all renamings of any bound variables that occur within it.
The principle of structural induction extends to abt’s, and is called structural induction modulo
fresh renaming. It states that to show that P[X ](a) holds for every a ∈B[X ], it is enough to show
the following:
1. if x ∈Xs, then P[X ]s(x).
2. For every o of arity (⃗s1.s1, . . . ,⃗sn.sn)s, if for each 1 ≤i ≤n, P[X ,⃗x′
i]si(bρi(ai)) holds for every
ρi : ⃗xi ↔⃗x′
i with ⃗x′
i /∈X , then P[X ]s(o(⃗x1.a1; . . . ;⃗xn.an)).
The second condition ensures that the inductive hypothesis holds for all fresh choices of bound
variable names, and not just the ones actually given in the abt.
As an example let us deﬁne the judgment x ∈a, where a ∈B[X , x], to mean that x occurs free
in a. Informally, this means that x is bound somewhere outside of a, rather than within a itself.
If x is bound within a, then those occurrences of x are different from those occurring outside the
binding. The following deﬁnition ensures that this is the case:
1. x ∈x.
2. x ∈o(⃗x1.a1; . . . ;⃗xn.an) if there exists 1 ≤i ≤n such that for every fresh renaming ρ : ⃗xi ↔⃗zi
we have x ∈bρ(ai).
The ﬁrst condition states that x is free in x, but not free in y for any variable y other than x. The
second condition states that if x is free in some argument, independently of the choice of bound
variable names in that argument, then it is free in the overall abt.
The relation a =α b of α-equivalence (so-called for historical reasons), means that a and b are
identical up to the choice of bound variable names. The α-equivalence relation is the strongest
congruence containing the following two conditions:

1.2 Abstract Binding Trees
9
1. x =α x.
2. o(⃗x1.a1; . . . ;⃗xn.an) =α o(⃗x′
1.a′
1; . . . ;⃗x′
n.a′
n) if for every 1 ≤i ≤n, bρi(ai) =α bρ′
i(a′
i) for all fresh
renamings ρi : ⃗xi ↔⃗zi and ρ′
i : ⃗x′
i ↔⃗zi.
The idea is that we rename ⃗xi and ⃗x′
i consistently, avoiding confusion, and check that ai and a′
i are
α-equivalent. If a =α b, then a and b are α-variants of each other.
Some care is required in the deﬁnition of substitution of an abt b of sort s for free occurrences of
a variable x of sort s in some abt a of some sort, written [b/x]a. Substitution is partially deﬁned by
the following conditions:
1. [b/x]x = b, and [b/x]y = y if x ̸= y.
2. [b/x]o(⃗x1.a1; . . . ;⃗xn.an) = o(⃗x1.a′
1; . . . ;⃗xn.a′
n), where, for each 1 ≤i ≤n, we require that
⃗xi /∈b, and we set a′
i = [b/x]ai if x /∈⃗xi, and a′
i = ai otherwise.
The deﬁnition of [b/x]a is quite delicate, and merits careful consideration.
One trouble spot for substitution is to notice that if x is bound by an abstractor within a, then
x does not occur free within the abstractor, and hence is unchanged by substitution. For exam-
ple, [b/x]let(a1; x.a2) = let([b/x]a1; x.a2), there being no free occurrences of x in x.a2. Another
trouble spot is the capture of a free variable of b during substitution. For example, if y ∈b, and
x ̸= y, then [b/x]let(a1; y.a2) is undeﬁned, rather than being let([b/x]a1; y.[b/x]a2), as one might
at ﬁrst suspect. For example, provided that x ̸= y, [y/x]let(num[0]; y.plus(x; y)) is undeﬁned, not
let(num[0]; y.plus(y; y)), which confuses two different variables named y.
Although capture avoidance is an essential characteristic of substitution, it is, in a sense, merely
a technical nuisance. If the names of bound variables have no signiﬁcance, then capture can always
be avoided by ﬁrst renaming the bound variables in a to avoid any free variables in b. In the fore-
going example if we rename the bound variable y to y′ to obtain a′ ≜let(num[0]; y′.plus(x; y′)),
then [b/x]a′ is deﬁned, and is equal to let(num[0]; y′.plus(b; y′)). The price for avoiding capture
in this way is that substitution is only determined up to α-equivalence, and so we may no longer
think of substitution as a function, but only as a proper relation.
To restore the functional character of substitution, it is sufﬁcient to adopt the identiﬁcation con-
vention, which is stated as follows:
Abstract binding trees are always identiﬁed up to α-equivalence.
That is, α-equivalent abt’s are regarded as identical. Substitution can be extended to α-equivalence
classes of abt’s to avoid capture by choosing representatives of the equivalence classes of b and a
in such a way that substitution is deﬁned, then forming the equivalence class of the result. Any
two choices of representatives for which substitution is deﬁned gives α-equivalent results, so that
substitution becomes a well-deﬁned total function. We will adopt the identiﬁcation convention for abt’s
throughout this book.
It will often be necessary to consider languages whose abstract syntax cannot be speciﬁed by a
ﬁxed set of operators, but rather requires that the available operators be sensitive to the context in
which they occur. For our purposes it will sufﬁce to consider a set of symbolic parameters, or symbols,
that index families of operators so that as the set of symbols varies, so does the set of operators. An

10
1.3 Notes
indexed operator o is a family of operators indexed by symbols u, so that o[u] is an operator when
u is an available symbol. If U is a ﬁnite set of symbols, then B[U; X ] is the sort-indexed family
of abt’s that are generated by operators and variables as before, admitting all indexed operator
instances by symbols u ∈U. Whereas a variable is a place-holder that stands for an unknown abt
of its sort, a symbol does not stand for anything, and is not, itself, an abt. The only signiﬁcance of
symbol is whether it is the same as or differs from another symbol; the operator instances o[u] and
o[u′] are the same exactly when u is u′, and are the same symbol.
The set of symbols is extended by introducing a new, or fresh, symbol within a scope using
the abstractor u.a, which binds the symbol u within the abt a. An abstracted symbol is “new”
in the same sense as for an abstracted variable: the name of the bound symbol can be varied at
will provided that no conﬂicts arise. This renaming property ensures that an abstracted symbol
is distinct from all others in scope. The only difference between symbols and variables is that the
only operation on symbols is renaming; there is no notion of substitution for a symbol.
Finally, a word about notation: to help improve the readability we often “group” and “stage”
the arguments to an operator, using round brackets and braces to show grouping, and generally
regarding stages to progress from right to left. All arguments in a group are considered to occur
at the same stage, though their order is signiﬁcant, and successive groups are considered to occur
in sequential stages. Staging and grouping is often a helpful mnemonic device, but has no funda-
mental signiﬁcance. For example, the abt o{a1; a2}(a3; x.a4) is the same as the abt o(a1; a2; a3; x.a4),
as would be any other order-preserving grouping or staging of its arguments.
1.3
Notes
The concept of abstract syntax has its origins in the pioneering work of Church, Turing, and G¨odel,
who ﬁrst considered writing programs that act on representations of programs. Originally pro-
grams were represented by natural numbers, using encodings, now called G¨odel-numberings, based
on the prime factorization theorem. Any standard text on mathematical logic, such as Kleene
(1952), has a thorough account of such representations. The Lisp language (McCarthy, 1965; Allen,
1978) introduced a much more practical and direct representation of syntax as symbolic expressions.
These ideas were developed further in the language ML (Gordon et al., 1979), which featured
a type system capable of expressing abstract syntax trees. The AUTOMATH project (Nederpelt
et al., 1994) introduced the idea of using Church’s λ notation (Church, 1941) to account for the
binding and scope of variables. These ideas were developed further in LF (Harper et al., 1993).
The concept of abstract binding trees presented here was inspired by the system of notation
developed in the NuPRL Project, which is described in Constable (1986) and from Martin-L¨of’s
system of arities, which is described in Nordstrom et al. (1990). Their enrichment with symbol
binders is inﬂuenced by Pitts and Stark (1993).
Exercises
1.1. Prove by structural induction on abstract syntax trees that if X ⊆Y, then A[X ] ⊆A[Y].

1.3 Notes
11
1.2. Prove by structural induction modulo renaming on abstract binding trees that if X ⊆Y,
then B[X ] ⊆B[Y].
1.3. Show that if a =α a′ and b =α b′ and both [b/x]a and [b′/x]a′ are deﬁned, then [b/x]a =α
[b′/x]a′.
1.4. Bound variables can be seen as the formal analogs of pronouns in natural languages. The
binding occurrence of a variable at an abstractor ﬁxes a “fresh” pronoun for use within its
body that refers unambiguously to that variable (in contrast to English, in which the referent
of a pronoun can often be ambiguous). This observation suggests an alternative representa-
tion of abt’s, called abstract binding graphs, or abg’s for short, as directed graphs constructed
as follows:
(a) Free variables are atomic nodes with no outgoing edges.
(b) Operators with n arguments are n-ary nodes, with one outgoing edge directed at each
of their children.
(c) Abstractors are nodes with one edge directed to the scope of the abstracted variable.
(d) Bound variables are back edges directed at the abstractor that introduced it.
Notice that ast’s, thought of as abt’s with no abstractors, are acyclic directed graphs (more
precisely, variadic trees), whereas general abt’s can be cyclic. Draw a few examples of abg’s
corresponding to the example abt’s given in this chapter. Give a precise deﬁnition of the
sort-indexed family G[X ] of abstract binding graphs. What representation would you use
for bound variables (back edges)?

12
1.3 Notes

Chapter 2
Inductive Deﬁnitions
Inductive deﬁnitions are an indispensable tool in the study of programming languages. In this
chapter we will develop the basic framework of inductive deﬁnitions, and give some examples of
their use. An inductive deﬁnition consists of a set of rules for deriving judgments, or assertions, of a
variety of forms. Judgments are statements about one or more abstract binding trees of some sort.
The rules specify necessary and sufﬁcient conditions for the validity of a judgment, and hence
fully determine its meaning.
2.1
Judgments
We start with the notion of a judgment, or assertion, about an abstract binding tree. We shall make
use of many forms of judgment, including examples such as these:
n nat
n is a natural number
n1 + n2 = n
n is the sum of n1 and n2
τ type
τ is a type
e : τ
expression e has type τ
e ⇓v
expression e has value v
A judgment states that one or more abstract binding trees have a property or stand in some
relation to one another. The property or relation itself is called a judgment form, and the judgment
that an object or objects have that property or stand in that relation is said to be an instance of
that judgment form. A judgment form is also called a predicate, and the objects constituting an
instance are its subjects. We write a J or J a, for the judgment asserting that J holds of the abt
a. Correspondingly, we sometimes notate the judgment form J by −J, or J −, using a dash to
indicate the absence of an argument to J. When it is not important to stress the subject of the
judgment, we write J to stand for an unspeciﬁed judgment, that is, an instance of some judgment
form. For particular judgment forms, we freely use preﬁx, inﬁx, or mix-ﬁx notation, as illustrated
by the above examples, in order to enhance readability.

14
2.2 Inference Rules
2.2
Inference Rules
An inductive deﬁnition of a judgment form consists of a collection of rules of the form
J1
. . .
Jk
J
(2.1)
in which J and J1, . . . , Jk are all judgments of the form being deﬁned. The judgments above the
horizontal line are called the premises of the rule, and the judgment below the line is called its
conclusion. If a rule has no premises (that is, when k is zero), the rule is called an axiom; otherwise
it is called a proper rule.
An inference rule can be read as stating that the premises are sufﬁcient for the conclusion: to
show J, it is enough to show J1, . . . , Jk. When k is zero, a rule states that its conclusion holds
unconditionally. Bear in mind that there may be, in general, many rules with the same conclusion,
each specifying sufﬁcient conditions for the conclusion. Consequently, if the conclusion of a rule
holds, then it is not necessary that the premises hold, for it might have been derived by another
rule.
For example, the following rules form an inductive deﬁnition of the judgment form −nat:
zero nat
(2.2a)
a nat
succ(a) nat
(2.2b)
These rules specify that a nat holds whenever either a is zero, or a is succ(b) where b nat for some
b. Taking these rules to be exhaustive, it follows that a nat iff a is a natural number.
Similarly, the following rules constitute an inductive deﬁnition of the judgment form −tree:
empty tree
(2.3a)
a1 tree
a2 tree
node(a1;a2) tree
(2.3b)
These rules specify that a tree holds if either a is empty, or a is node(a1;a2), where a1 tree and
a2 tree. Taking these to be exhaustive, these rules state that a is a binary tree, which is to say it is
either empty, or a node consisting of two children, each of which is also a binary tree.
The judgment form a is b expressing the equality of two abt’s a and b such that a nat and b nat
is inductively deﬁned by the following rules:
zero is zero
(2.4a)
a is b
succ(a) is succ(b)
(2.4b)

2.3 Derivations
15
In each of the preceding examples we have made use of a notational convention for specifying
an inﬁnite family of rules by a ﬁnite number of patterns, or rule schemes. For example, rule (2.2b)
is a rule scheme that determines one rule, called an instance of the rule scheme, for each choice of
object a in the rule. We will rely on context to determine whether a rule is stated for a speciﬁc object
a or is instead intended as a rule scheme specifying a rule for each choice of objects in the rule.
A collection of rules is considered to deﬁne the strongest judgment form that is closed under, or
respects, those rules. To be closed under the rules simply means that the rules are sufﬁcient to show
the validity of a judgment: J holds if there is a way to obtain it using the given rules. To be the
strongest judgment form closed under the rules means that the rules are also necessary: J holds only
if there is a way to obtain it by applying the rules. The sufﬁciency of the rules means that we may
show that J holds by deriving it by composing rules. Their necessity means that we may reason
about it using rule induction.
2.3
Derivations
To show that an inductively deﬁned judgment holds, it is enough to exhibit a derivation of it. A
derivation of a judgment is a ﬁnite composition of rules, starting with axioms and ending with
that judgment. It can be thought of as a tree in which each node is a rule whose children are
derivations of its premises. We sometimes say that a derivation of J is evidence for the validity of
an inductively deﬁned judgment J.
We usually depict derivations as trees with the conclusion at the bottom, and with the children
of a node corresponding to a rule appearing above it as evidence for the premises of that rule.
Thus, if
J1
. . .
Jk
J
is an inference rule and `
1, . . . , `
k are derivations of its premises, then
`
1
. . .
`
k
J
is a derivation of its conclusion. In particular, if k = 0, then the node has no children.
For example, this is a derivation of succ(succ(succ(zero))) nat:
zero nat
succ(zero) nat
succ(succ(zero)) nat
succ(succ(succ(zero))) nat
.
(2.5)

16
2.4 Rule Induction
Similarly, here is a derivation of node(node(empty;empty);empty) tree:
empty tree
empty tree
node(empty;empty) tree
empty tree
node(node(empty;empty);empty) tree
.
(2.6)
To show that an inductively deﬁned judgment is derivable we need only ﬁnd a derivation
for it. There are two main methods for ﬁnding derivations, called forward chaining, or bottom-
up construction, and backward chaining, or top-down construction. Forward chaining starts with the
axioms and works forward towards the desired conclusion, whereas backward chaining starts
with the desired conclusion and works backwards towards the axioms.
More precisely, forward chaining search maintains a set of derivable judgments, and continu-
ally extends this set by adding to it the conclusion of any rule all of whose premises are in that
set. Initially, the set is empty; the process terminates when the desired judgment occurs in the
set. Assuming that all rules are considered at every stage, forward chaining will eventually ﬁnd
a derivation of any derivable judgment, but it is impossible (in general) to decide algorithmically
when to stop extending the set and conclude that the desired judgment is not derivable. We may
go on and on adding more judgments to the derivable set without ever achieving the intended
goal. It is a matter of understanding the global properties of the rules to determine that a given
judgment is not derivable.
Forward chaining is undirected in the sense that it does not take account of the end goal when
deciding how to proceed at each step. In contrast, backward chaining is goal-directed. Back-
ward chaining search maintains a queue of current goals, judgments whose derivations are to be
sought. Initially, this set consists solely of the judgment we wish to derive. At each stage, we
remove a judgment from the queue, and consider all rules whose conclusion is that judgment.
For each such rule, we add the premises of that rule to the back of the queue, and continue. If
there is more than one such rule, this process must be repeated, with the same starting queue, for
each candidate rule. The process terminates whenever the queue is empty, all goals having been
achieved; any pending consideration of candidate rules along the way can be discarded. As with
forward chaining, backward chaining will eventually ﬁnd a derivation of any derivable judgment,
but there is, in general, no algorithmic method for determining in general whether the current
goal is derivable. If it is not, we may futilely add more and more judgments to the goal set, never
reaching a point at which all goals have been satisﬁed.
2.4
Rule Induction
Because an inductive deﬁnition speciﬁes the strongest judgment form closed under a collection of
rules, we may reason about them by rule induction. The principle of rule induction states that to
show that a property a P holds whenever a J is derivable, it is enough to show that P is closed
under, or respects, the rules deﬁning the judgment form J. More precisely, the property P respects
the rule
a1 J
. . .
ak J
a J

2.4 Rule Induction
17
if P(a) holds whenever P(a1), . . . , P(ak) do. The assumptions P(a1), . . . , P(ak) are called the
inductive hypotheses, and P(a) is called the inductive conclusion of the inference.
The principle of rule induction is simply the expression of the deﬁnition of an inductively
deﬁned judgment form as the strongest judgment form closed under the rules comprising the def-
inition. Thus, the judgment form deﬁned by a set of rules is both (a) closed under those rules,
and (b) sufﬁcient for any other property also closed under those rules. The former means that a
derivation is evidence for the validity of a judgment; the latter means that we may reason about
an inductively deﬁned judgment form by rule induction.
When specialized to rules (2.2), the principle of rule induction states that to show P(a) when-
ever a nat, it is enough to show:
1. P(zero).
2. for every a, if P(a), then P(succ(a)).
The sufﬁciency of these conditions is the familiar principle of mathematical induction.
Similarly, rule induction for rules (2.3) states that to show P(a) whenever a tree, it is enough to
show
1. P(empty).
2. for every a1 and a2, if P(a1), and if P(a2), then P(node(a1;a2)).
The sufﬁciency of these conditions is called the principle of tree induction.
We may also show by rule induction that the predecessor of a natural number is also a natural
number. Although this may seem self-evident, the point of the example is to show how to derive
this from ﬁrst principles.
Lemma 2.1. If succ(a) nat, then a nat.
Proof. It sufﬁces to show that the property P(a) stating that a nat and that a = succ(b) implies
b nat is closed under rules (2.2).
Rule (2.2a) Clearly zero nat, and the second condition holds vacuously, because zero is not of the
form succ(−).
Rule (2.2b) Inductively we know that a nat and that if a is of the form succ(b), then b nat. We
are to show that succ(a) nat, which is immediate, and that if succ(a) is of the form succ(b),
then b nat, and we have b nat by the inductive hypothesis.
Using rule induction we may show that equality, as deﬁned by rules (2.4) is reﬂexive.
Lemma 2.2. If a nat, then a is a.
Proof. By rule induction on rules (2.2):
Rule (2.2a) Applying rule (2.4a) we obtain zero is zero.

18
2.5 Iterated and Simultaneous Inductive Deﬁnitions
Rule (2.2b) Assume that a is a. It follows that succ(a) is succ(a) by an application of rule (2.4b).
Similarly, we may show that the successor operation is injective.
Lemma 2.3. If succ(a1) is succ(a2), then a1 is a2.
Proof. Similar to the proof of Lemma 2.1.
2.5
Iterated and Simultaneous Inductive Deﬁnitions
Inductive deﬁnitions are often iterated, meaning that one inductive deﬁnition builds on top of
another. In an iterated inductive deﬁnition the premises of a rule
J1
. . .
Jk
J
may be instances of either a previously deﬁned judgment form, or the judgment form being de-
ﬁned. For example, the following rules deﬁne the judgment form −list, which states that a is a list
of natural numbers:
nil list
(2.7a)
a nat
b list
cons(a;b) list
(2.7b)
The ﬁrst premise of rule (2.7b) is an instance of the judgment form a nat, which was deﬁned
previously, whereas the premise b list is an instance of the judgment form being deﬁned by these
rules.
Frequently two or more judgments are deﬁned at once by a simultaneous inductive deﬁnition.
A simultaneous inductive deﬁnition consists of a set of rules for deriving instances of several
different judgment forms, any of which may appear as the premise of any rule. Because the rules
deﬁning each judgment form may involve any of the others, none of the judgment forms can be
taken to be deﬁned prior to the others. Instead we must understand that all of the judgment forms
are being deﬁned at once by the entire collection of rules. The judgment forms deﬁned by these
rules are, as before, the strongest judgment forms that are closed under the rules. Therefore the
principle of proof by rule induction continues to apply, albeit in a form that requires us to prove a
property of each of the deﬁned judgment forms simultaneously.
For example, consider the following rules, which constitute a simultaneous inductive deﬁni-
tion of the judgments a even, stating that a is an even natural number, and a odd, stating that a is
an odd natural number:
zero even
(2.8a)
b odd
succ(b) even
(2.8b)

2.6 Deﬁning Functions by Rules
19
a even
succ(a) odd
(2.8c)
The principle of rule induction for these rules states that to show simultaneously that P(a)
whenever a even and Q(b) whenever b odd, it is enough to show the following:
1. P(zero);
2. if Q(b), then P(succ(b));
3. if P(a), then Q(succ(a)).
As an example, we may use simultaneous rule induction to prove that (1) if a even, then either
a is zero or a is succ(b) with b odd, and (2) if a odd, then a is succ(b) with b even. We deﬁne P(a)
to hold iff a is zero or a is succ(b) for some b with b odd, and deﬁne Q(b) to hold iff b is succ(a)
for some a with a even. The desired result follows by rule induction, because we can prove the
following facts:
1. P(zero), which holds because zero is zero.
2. If Q(b), then succ(b) is succ(b′) for some b′ with Q(b′). Take b′ to be b and apply the
inductive assumption.
3. If P(a), then succ(a) is succ(a′) for some a′ with P(a′). Take a′ to be a and apply the
inductive assumption.
2.6
Deﬁning Functions by Rules
A common use of inductive deﬁnitions is to deﬁne a function by giving an inductive deﬁnition of
its graph relating inputs to outputs, and then showing that the relation uniquely determines the
outputs for given inputs. For example, we may deﬁne the addition function on natural numbers
as the relation sum(a;b;c), with the intended meaning that c is the sum of a and b, as follows:
b nat
sum(zero;b;b)
(2.9a)
sum(a;b;c)
sum(succ(a);b;succ(c))
(2.9b)
The rules deﬁne a ternary (three-place) relation sum(a;b;c) among natural numbers a, b, and c. We
may show that c is determined by a and b in this relation.
Theorem 2.4. For every a nat and b nat, there exists a unique c nat such that sum(a;b;c).
Proof. The proof decomposes into two parts:
1. (Existence) If a nat and b nat, then there exists c nat such that sum(a;b;c).
2. (Uniqueness) If sum(a;b;c), and sum(a;b;c′), then c is c′.

20
2.7 Notes
For existence, let P(a) be the proposition if b nat then there exists c nat such that sum(a;b;c). We
prove that if a nat then P(a) by rule induction on rules (2.2). We have two cases to consider:
Rule (2.2a) We are to show P(zero). Assuming b nat and taking c to be b, we obtain sum(zero;b;c)
by rule (2.9a).
Rule (2.2b) Assuming P(a), we are to show P(succ(a)). That is, we assume that if b nat then
there exists c such that sum(a;b;c), and are to show that if b′ nat, then there exists c′ such that
sum(succ(a);b′;c′). To this end, suppose that b′ nat. Then by induction there exists c such that
sum(a;b′;c). Taking c′ to be succ(c), and applying rule (2.9b), we obtain sum(succ(a);b′;c′),
as required.
For uniqueness, we prove that if sum(a;b;c1), then if sum(a;b;c2), then c1 is c2 by rule induction based
on rules (2.9).
Rule (2.9a) We have a is zero and c1 is b. By an inner induction on the same rules, we may show
that if sum(zero;b;c2), then c2 is b. By Lemma 2.2 we obtain b is b.
Rule (2.9b) We have that a is succ(a′) and c1 is succ(c′
1), where sum(a′;b;c′
1). By an inner induction
on the same rules, we may show that if sum(a;b;c2), then c2 is succ(c′
2) where sum(a′;b;c′
2). By
the outer inductive hypothesis c′
1 is c′
2 and so c1 is c2.
2.7
Notes
Aczel (1977) provides a thorough account of the theory of inductive deﬁnitions on which the
present account is based. A signiﬁcant difference is that we consider inductive deﬁnitions of judg-
ments over abt’s as deﬁned in Chapter 1, rather than with natural numbers. The emphasis on
judgments is inspired by Martin-L¨of’s logic of judgments (Martin-L¨of, 1983, 1987).
Exercises
2.1. Give an inductive deﬁnition of the judgment max(m;n;p), where m nat, n nat, and p nat, with
the meaning that p is the larger of m and n. Prove that every m and n are related to a unique
p by this judgment.
2.2. Consider the following rules, which deﬁne the judgment hgt(t;n) stating that the binary tree
t has height n.
hgt(empty;zero)
(2.10a)
hgt(t1;n1)
hgt(t2;n2)
max(n1;n2;n)
hgt(node(t1;t2);succ(n))
(2.10b)
Prove that the judgment hgt deﬁnes a function from trees to natural numbers.

2.7 Notes
21
2.3. Given an inductive deﬁnition of ordered variadic trees whose nodes have a ﬁnite, but variable,
number of children with a speciﬁed left-to-right ordering among them. Your solution should
consist of a simultaneous deﬁnition of two judgments, t tree, stating that t is a variadic tree,
and f forest, stating that f is a “forest” (ﬁnite sequence) of variadic trees.
2.4. Give an inductive deﬁnition of the height of a variadic tree of the kind deﬁned in Exercise 2.3.
Your deﬁnition should make use of an auxiliary judgment deﬁning the height of a forest of
variadic trees, and will be deﬁned simultaneously with the height of a variadic tree. Show
that the two judgments so deﬁned each deﬁne a function.
2.5. Give an inductive deﬁnition of the binary natural numbers, which are either zero, twice a
binary number, or one more than twice a binary number. The size of such a representation is
logarithmic, rather than linear, in the natural number it represents.
2.6. Give an inductive deﬁnition of addition of binary natural numbers as deﬁned in Exercise 2.5.
Hint: Proceed by analyzing both arguments to the addition, and make use of an auxiliary
function to compute the successor of a binary number. Hint: Alternatively, deﬁne both the
sum and the sum-plus-one of two binary numbers mutually recursively.

22
2.7 Notes

Chapter 3
Hypothetical and General Judgments
A hypothetical judgment expresses an entailment between one or more hypotheses and a conclusion.
We will consider two notions of entailment, called derivability and admissibility. Both express a
form of entailment, but they differ in that derivability is stable under extension with new rules,
admissibility is not. A general judgment expresses the universality, or genericity, of a judgment.
There are two forms of general judgment, the generic and the parametric. The generic judgment
expresses generality with respect to all substitution instances for variables in a judgment. The
parametric judgment expresses generality with respect to renamings of symbols.
3.1
Hypothetical Judgments
The hypothetical judgment codiﬁes the rules for expressing the validity of a conclusion conditional
on the validity of one or more hypotheses. There are two forms of hypothetical judgment that
differ according to the sense in which the conclusion is conditional on the hypotheses. One is
stable under extension with more rules, and the other is not.
3.1.1
Derivability
For a given set R of rules, we deﬁne the derivability judgment, written J1, . . . , Jk ⊢R K, where each
Ji and K are basic judgments, to mean that we may derive K from the expansion R ∪{ J1, . . . , Jk } of
the rules R with the axioms
J1
. . .
Jk
.
We treat the hypotheses, or antecedents, of the judgment, J1, . . . , Jk as “temporary axioms”, and de-
rive the conclusion, or consequent, by composing rules in R. Thus, evidence for a hypothetical
judgment consists of a derivation of the conclusion from the hypotheses using the rules in R.
We use capital Greek letters, usually Γ or ∆, to stand for a ﬁnite set of basic judgments, and
write R ∪Γ for the expansion of R with an axiom corresponding to each judgment in Γ. The

24
3.1 Hypothetical Judgments
judgment Γ ⊢R K means that K is derivable from rules R ∪Γ, and the judgment ⊢R Γ means that
⊢R J for each J in Γ. An equivalent way of deﬁning J1, . . . , Jn ⊢R J is to say that the rule
J1
. . .
Jn
J
(3.1)
is derivable from R, which means that there is a derivation of J composed of the rules in R aug-
mented by treating J1, . . . , Jn as axioms.
For example, consider the derivability judgment
a nat ⊢(2.2) succ(succ(a)) nat
(3.2)
relative to rules (2.2). This judgment is valid for any choice of object a, as shown by the derivation
a nat
succ(a) nat
succ(succ(a)) nat
(3.3)
which composes rules (2.2), starting with a nat as an axiom, and ending with succ(succ(a)) nat.
Equivalently, the validity of (3.2) may also be expressed by stating that the rule
a nat
succ(succ(a)) nat
(3.4)
is derivable from rules (2.2).
It follows directly from the deﬁnition of derivability that it is stable under extension with new
rules.
Theorem 3.1 (Stability). If Γ ⊢R J, then Γ ⊢R∪R′ J.
Proof. Any derivation of J from R ∪Γ is also a derivation from (R ∪R′) ∪Γ, because any rule in
R is also a rule in R ∪R′.
Derivability enjoys a number of structural properties that follow from its deﬁnition, indepen-
dently of the rules R in question.
Reﬂexivity Every judgment is a consequence of itself: Γ, J ⊢R J. Each hypothesis justiﬁes itself as
conclusion.
Weakening If Γ ⊢R J, then Γ, K ⊢R J. Entailment is not inﬂuenced by un-exercised options.
Transitivity If Γ, K ⊢R J and Γ ⊢R K, then Γ ⊢R J. If we replace an axiom by a derivation of it,
the result is a derivation of its consequent without that hypothesis.
Reﬂexivity follows directly from the meaning of derivability. Weakening follows directly from the
deﬁnition of derivability. Transitivity is proved by rule induction on the ﬁrst premise.

3.1 Hypothetical Judgments
25
3.1.2
Admissibility
Admissibility, written Γ |=R J, is a weaker form of hypothetical judgment stating that ⊢R Γ implies
⊢R J. That is, the conclusion J is derivable from rules R when the assumptions Γ are all derivable
from rules R. In particular if any of the hypotheses are not derivable relative to R, then the
judgment is vacuously true. An equivalent way to deﬁne the judgment J1, . . . , Jn |=R J is to state
that the rule
J1
. . .
Jn
J
(3.5)
is admissible relative to the rules in R. Given any derivations of J1, . . . , Jn using the rules in R, we
may build a derivation of J using the rules in R.
For example, the admissibility judgment
succ(a) even |=(2.8) a odd
(3.6)
is valid, because any derivation of succ(a) even from rules (2.2) must contain a sub-derivation of
a odd from the same rules, which justiﬁes the conclusion. This fact can be proved by induction on
rules (2.8). That judgment (3.6) is valid may also be expressed by saying that the rule
succ(a) even
a odd
(3.7)
is admissible relative to rules (2.8).
In contrast to derivability the admissibility judgment is not stable under extension to the rules.
For example, if we enrich rules (2.8) with the axiom
succ(zero) even
,
(3.8)
then rule (3.6) is inadmissible, because there is no composition of rules deriving zero odd. Admis-
sibility is as sensitive to which rules are absent from an inductive deﬁnition as it is to which rules
are present in it.
The structural properties of derivability ensure that derivability is stronger than admissibility.
Theorem 3.2. If Γ ⊢R J, then Γ |=R J.
Proof. Repeated application of the transitivity of derivability shows that if Γ ⊢R J and ⊢R Γ, then
⊢R J.
To see that the converse fails, note that
succ(zero) even ̸⊢(2.8) zero odd,
because there is no derivation of the right-hand side when the left-hand side is added as an axiom
to rules (2.8). Yet the corresponding admissibility judgment
succ(zero) even |=(2.8) zero odd

26
3.2 Hypothetical Inductive Deﬁnitions
is valid, because the hypothesis is false: there is no derivation of succ(zero) even from rules (2.8).
Even so, the derivability
succ(zero) even ⊢(2.8) succ(succ(zero)) odd
is valid, because we may derive the right-hand side from the left-hand side by composing rules (2.8).
Evidence for admissibility can be thought of as a mathematical function transforming deriva-
tions ▽1, . . . , ▽n of the hypotheses into a derivation ▽of the consequent. Therefore, the admissi-
bility judgment enjoys the same structural properties as derivability, and hence is a form of hypo-
thetical judgment:
Reﬂexivity If J is derivable from the original rules, then J is derivable from the original rules:
J |=R J.
Weakening If J is derivable from the original rules assuming that each of the judgments in Γ are
derivable from these rules, then J must also be derivable assuming that Γ and K are derivable
from the original rules: if Γ |=R J, then Γ, K |=R J.
Transitivity If Γ, K |=R J and Γ |=R K, then Γ |=R J. If the judgments in Γ are derivable, so is K,
by assumption, and hence so are the judgments in Γ, K, and hence so is J.
Theorem 3.3. The admissibility judgment Γ |=R J enjoys the structural properties of entailment.
Proof. Follows immediately from the deﬁnition of admissibility as stating that if the hypotheses
are derivable relative to R, then so is the conclusion.
If a rule r is admissible with respect to a rule set R, then ⊢R,r J is equivalent to ⊢R J. For if
⊢R J, then obviously ⊢R,r J, by simply disregarding r. Conversely, if ⊢R,r J, then we may replace
any use of r by its expansion in terms of the rules in R. It follows by rule induction on R, r that
every derivation from the expanded set of rules R, r can be transformed into a derivation from R
alone. Consequently, if we wish to prove a property of the judgments derivable from R, r, when
r is admissible with respect to R, it sufﬁces show that the property is closed under rules R alone,
because its admissibility states that the consequences of rule r are implicit in those of rules R.
3.2
Hypothetical Inductive Deﬁnitions
It is useful to enrich the concept of an inductive deﬁnition to allow rules with derivability judg-
ments as premises and conclusions. Doing so lets us introduce local hypotheses that apply only
in the derivation of a particular premise, and also allows us to constrain inferences based on the
global hypotheses in effect at the point where the rule is applied.
A hypothetical inductive deﬁnition consists of a set of hypothetical rules of the following form:
Γ Γ1 ⊢J1
. . .
Γ Γn ⊢Jn
Γ ⊢J
.
(3.9)

3.2 Hypothetical Inductive Deﬁnitions
27
The hypotheses Γ are the global hypotheses of the rule, and the hypotheses Γi are the local hypotheses
of the ith premise of the rule. Informally, this rule states that J is a derivable consequence of Γ when
each Ji is a derivable consequence of Γ, augmented with the hypotheses Γi. Thus, one way to show
that J is derivable from Γ is to show, in turn, that each Ji is derivable from Γ Γi. The derivation
of each premise involves a “context switch” in which we extend the global hypotheses with the
local hypotheses of that premise, establishing a new set of global hypotheses for use within that
derivation.
We require that all rules in a hypothetical inductive deﬁnition be uniform in the sense that they
are applicable in all global contexts. Uniformity ensures that a rule can be presented in implicit, or
local form,
Γ1 ⊢J1
. . .
Γn ⊢Jn
J
,
(3.10)
in which the global context has been suppressed with the understanding that the rule applies for
any choice of global hypotheses.
A hypothetical inductive deﬁnition is to be regarded as an ordinary inductive deﬁnition of a
formal derivability judgment Γ ⊢J consisting of a ﬁnite set of basic judgments Γ and a basic judgment
J. A set of hypothetical rules R deﬁnes the strongest formal derivability judgment that is structural
and closed under uniform rules R. Structurality means that the formal derivability judgment must
be closed under the following rules:
Γ, J ⊢J
(3.11a)
Γ ⊢J
Γ, K ⊢J
(3.11b)
Γ ⊢K
Γ, K ⊢J
Γ ⊢J
(3.11c)
These rules ensure that formal derivability behaves like a hypothetical judgment. We write Γ ⊢R J
to mean that Γ ⊢J is derivable from rules R.
The principle of hypothetical rule induction is just the principle of rule induction applied to the
formal hypothetical judgment. So to show that P(Γ ⊢J) when Γ ⊢R J, it is enough to show that P
is closed under the rules of R and under the structural rules.1 Thus, for each rule of the form (3.9),
whether structural or in R, we must show that
if P(Γ Γ1 ⊢J1) and . . . and P(Γ Γn ⊢Jn), then P(Γ ⊢J).
But this is just a restatement of the principle of rule induction given in Chapter 2, specialized to
the formal derivability judgment Γ ⊢J.
In practice we usually dispense with the structural rules by the method described in Sec-
tion 3.1.2. By proving that the structural rules are admissible any proof by rule induction may
restrict attention to the rules in R alone. If all rules of a hypothetical inductive deﬁnition are uni-
form, the structural rules (3.11b) and (3.11c) are clearly admissible. Usually, rule (3.11a) must be
postulated explicitly as a rule, rather than shown to be admissible on the basis of the other rules.
1Writing P(Γ ⊢J) is a mild abuse of notation in which the turnstile is used to separate the two arguments to P for the
sake of readability.

28
3.3 General Judgments
3.3
General Judgments
General judgments codify the rules for handling variables in a judgment. As in mathematics in
general, a variable is treated as an unknown ranging over a speciﬁed set of objects. A generic judg-
ment states that a judgment holds for any choice of objects replacing designated variables in the
judgment. Another form of general judgment codiﬁes the handling of symbolic parameters. A
parametric judgment expresses generality over any choice of fresh renamings of designated sym-
bols of a judgment. To keep track of the active variables and symbols in a derivation, we write
Γ ⊢U;X
R
J to say that J is derivable from Γ according to rules R, with objects consisting of abt’s over
symbols U and variables X .
The concept of uniformity of a rule must be extended to require that rules be closed under renam-
ing and substitution for variables and closed under renaming for parameters. More precisely, if R is a
set of rules containing a free variable x of sort s then it must also contain all possible substitution
instances of abt’s a of sort s for x, including those that contain other free variables. Similarly, if
R contains rules with a parameter u, then it must contain all instances of that rule obtained by
renaming u of a sort to any u′ of the same sort. Uniformity rules out stating a rule for a variable,
without also stating it all instances of that variable. It also rules out stating a rule for a parameter
without stating it for all possible renamings of that parameter.
Generic derivability judgment is deﬁned by
Y | Γ ⊢X
R J
iff
Γ ⊢X Y
R
J,
where Y ∩X = ∅. Evidence for generic derivability consists of a generic derivation ▽involving the
variables X Y. So long as the rules are uniform, the choice of Y does not matter, in a sense to be
explained shortly.
For example, the generic derivation ▽,
x nat
succ(x) nat
succ(succ(x)) nat
,
is evidence for the judgment
x | x nat ⊢X
(2.2) succ(succ(x)) nat
provided x /∈X . Any other choice of x would work just as well, as long as all rules are uniform.
The generic derivability judgment enjoys the following structural properties governing the be-
havior of variables, provided that R is uniform.
Proliferation If Y | Γ ⊢X
R J, then Y, y | Γ ⊢X
R J.
Renaming If Y, y | Γ ⊢X
R J, then Y, y′ | [y ↔y′]Γ ⊢X
R [y ↔y′]J for any y′ /∈X Y.
Substitution If Y, y | Γ ⊢X
R J and a ∈B[X Y], then Y | [a/y]Γ ⊢X
R [a/y]J.

3.4 Generic Inductive Deﬁnitions
29
Proliferation is guaranteed by the interpretation of rule schemes as ranging over all expansions of
the universe. Renaming is built into the meaning of the generic judgment. It is left implicit in the
principle of substitution that the substituting abt is of the same sort as the substituted variable.
Parametric derivability is deﬁned analogously to generic derivability, albeit by generalizing
over symbols, rather than variables. Parametric derivability is deﬁned by
V ∥Y | Γ ⊢U;X
R
J
iff
Y | Γ ⊢U V;X
R
J,
where V ∩U = ∅. Evidence for parametric derivability consists of a derivation ▽involving the
symbols V. Uniformity of R ensures that any choice of parameter names is as good as any other;
derivability is stable under renaming.
3.4
Generic Inductive Deﬁnitions
A generic inductive deﬁnition admits generic hypothetical judgments in the premises of rules, with
the effect of augmenting the variables, as well as the rules, within those premises. A generic rule
has the form
Y Y1 | Γ Γ1 ⊢J1
. . .
Y Yn | Γ Γn ⊢Jn
Y | Γ ⊢J
.
(3.12)
The variables Y are the global variables of the inference, and, for each 1 ≤i ≤n, the variables Yi are
the local variables of the ith premise. In most cases a rule is stated for all choices of global variables
and global hypotheses. Such rules can be given in implicit form,
Y1 | Γ1 ⊢J1
. . .
Yn | Γn ⊢Jn
J
.
(3.13)
A generic inductive deﬁnition is just an ordinary inductive deﬁnition of a family of formal
generic judgments of the form Y | Γ ⊢J. Formal generic judgments are identiﬁed up to renaming
of variables, so that the latter judgment is treated as identical to the judgment Y′ | bρ(Γ) ⊢bρ(J) for
any renaming ρ : Y ↔Y′. If R is a collection of generic rules, we write Y | Γ ⊢R J to mean that
the formal generic judgment Y | Γ ⊢J is derivable from rules R.
When specialized to a set of generic rules, the principle of rule induction states that to show
P(Y | Γ ⊢J) when Y | Γ ⊢R J, it is enough to show that P is closed under the rules R. Speciﬁcally,
for each rule in R of the form (3.12), we must show that
if P(Y Y1 | Γ Γ1 ⊢J1) . . . P(Y Yn | Γ Γn ⊢Jn) then P(Y | Γ ⊢J).
By the identiﬁcation convention (stated in Chapter 1) the property P must respect renamings of
the variables in a formal generic judgment.
To ensure that the formal generic judgment behaves like a generic judgment, we must always
ensure that the following structural rules are admissible:
Y | Γ, J ⊢J
(3.14a)

30
3.5 Notes
Y | Γ ⊢J
Y | Γ, J′ ⊢J
(3.14b)
Y | Γ ⊢J
Y, x | Γ ⊢J
(3.14c)
Y, x′ | [x ↔x′]Γ ⊢[x ↔x′]J
Y, x | Γ ⊢J
(3.14d)
Y | Γ ⊢J
Y | Γ, J ⊢J′
Y | Γ ⊢J′
(3.14e)
Y, x | Γ ⊢J
a ∈B[Y]
Y | [a/x]Γ ⊢[a/x]J
(3.14f)
The admissibility of rule (3.14a) is, in practice, ensured by explicitly including it. The admissibility
of rules (3.14b) and (3.14c) is assured if each of the generic rules is uniform, because we may
assimilate the added variable x to the global variables, and the added hypothesis J, to the global
hypotheses. The admissibility of rule (3.14d) is ensured by the identiﬁcation convention for the
formal generic judgment. Rule (3.14f) must be veriﬁed explicitly for each inductive deﬁnition.
The concept of a generic inductive deﬁnition extends to parametric judgments as well. Brieﬂy,
rules are deﬁned on formal parametric judgments of the form V ∥Y | Γ ⊢J, with symbols V, as
well as variables, Y. Such formal judgments are identiﬁed up to renaming of its variables and its
symbols to ensure that the meaning is independent of the choice of variable and symbol names.
3.5
Notes
The concepts of entailment and generality are fundamental to logic and programming languages.
The formulation given here builds on Martin-L¨of (1983, 1987) and Avron (1991). Hypothetical and
general reasoning are consolidated into a single concept in the AUTOMATH languages (Nederpelt
et al., 1994) and in the LF Logical Framework (Harper et al., 1993). These systems allow arbitrarily
nested combinations of hypothetical and general judgments, whereas the present account con-
siders only general hypothetical judgments over basic judgment forms. On the other hand we
consider here symbols, as well as variables, which are not present in these previous accounts.
Parametric judgments are required for specifying languages that admit the dynamic creation of
“new” objects (see Chapter 34).
Exercises
3.1. Combinators are inductively deﬁned by the rule set C given as follows:
s comb
(3.15a)

3.5 Notes
31
k comb
(3.15b)
a1 comb
a2 comb
ap(a1;a2) comb
(3.15c)
Give an inductive deﬁnition of the length of a combinator deﬁned as the number of occur-
rences of S and K within it.
3.2. The general judgment
x1, . . . , xn | x1 comb, . . . , xn comb ⊢C A comb
states that A is a combinator that may involve the variables x1, . . . , xn. Prove that if x |
x comb ⊢C a2 comb and a1 comb, then [a1/x]a2 comb by induction on the derivation of the
ﬁrst hypothesis of the implication.
3.3. Conversion, or equivalence, of combinators is expressed by the judgment A ≡B deﬁned by
the rule set E extending C as follows:2
a comb
a ≡a
(3.16a)
a2 ≡a1
a1 ≡a2
(3.16b)
a1 ≡a2
a2 ≡a3
a1 ≡a3
(3.16c)
a1 ≡a′
1
a2 ≡a′
2
a1 a2 ≡a′
1 a′
2
(3.16d)
a1 comb
a2 comb
k a1 a2 ≡a1
(3.16e)
a1 comb
a2 comb
a3 comb
s a1 a2 a3 ≡(a1 a3) (a2 a3)
(3.16f)
The no-doubt mysterious motivation for the last two equations will become clearer in a mo-
ment. For now, show that
x | x comb ⊢C∪E s k k x ≡x.
3.4. Show that if x | x comb ⊢C a comb, then there is a combinator a′, written [x] a and called
bracket abstraction, such that
x | x comb ⊢C∪E a′ x ≡a.
Consequently, by Exercise 3.2, if a′′ comb, then
([x] a) a′′ ≡[a′′/x]a.
2The combinator ap(a1;a2) is written a1 a2 for short, left-associatively when used in succession.

32
3.5 Notes
Hint: Inductively deﬁne the judgment
x | x comb ⊢absx a is a′,
where x | x comb ⊢a comb. Then argue that it deﬁnes a′ as a binary function of x and
a. The motivation for the conversion axioms governing k and s should become clear while
developing the proof of the desired equivalence.
3.5. Prove that bracket abstraction, as deﬁned in Exercise 3.4, is non-compositional by exhibiting a
and b such that a comb and
x y | x comb y comb ⊢C b comb
such that [a/y]([x] b) ̸= [x] ([a/y]b). Hint: Consider the case that b is y.
Suggest a modiﬁcation to the deﬁnition of bracket abstraction that is compositional by show-
ing under the same conditions given above that
[a/y]([x] b) = [x] ([a/y]b).
3.6. Consider the set B[X ] of abt’s generated by the operators ap, with arity (Exp, Exp)Exp, and
λ, with arity (Exp.Exp)Exp, and possibly involving variables in X , all of which are of sort
Exp. Give an inductive deﬁnition of the judgment b closed, which speciﬁes that b has no free
occurrences of the variables in X . Hint: it is essential to give an inductive deﬁnition of the
hypothetical, general judgment
x1, . . . , xn | x1 closed, . . . , xn closed ⊢b closed
in order to account for the binding of a variable by the λ operator. The hypothesis that a
variable is closed seems self-contradictory in that a variable obviously occurs free in itself.
Explain why this is not the case by examining carefully the meaning of the hypothetical and
general judgments.

Part II
Statics and Dynamics


Chapter 4
Statics
Most programming languages exhibit a phase distinction between the static and dynamic phases of
processing. The static phase consists of parsing and type checking to ensure that the program is
well-formed; the dynamic phase consists of execution of well-formed programs. A language is
said to be safe exactly when well-formed programs are well-behaved when executed.
The static phase is speciﬁed by a statics comprising a set of rules for deriving typing judgments
stating that an expression is well-formed of a certain type. Types mediate the interaction between
the constituent parts of a program by “predicting” some aspects of the execution behavior of the
parts so that we may ensure they ﬁt together properly at run-time. Type safety tells us that these
predictions are correct; if not, the statics is considered to be improperly deﬁned, and the language
is deemed unsafe for execution.
In this chapter we present the statics of a simple expression language, E, as an illustration of
the method that we will employ throughout this book.
4.1
Syntax
When deﬁning a language we shall be primarily concerned with its abstract syntax, speciﬁed by a
collection of operators and their arities. The abstract syntax provides a systematic, unambiguous
account of the hierarchical and binding structure of the language, and is considered the ofﬁcial
presentation of the language. However, for the sake of clarity, it is also useful to specify minimal
concrete syntax conventions, without going through the trouble to set up a fully precise grammar
for it.
We will accomplish both of these purposes with a syntax chart, whose meaning is best illus-

36
4.2 Type System
trated by example. The following chart summarizes the abstract and concrete syntax of E.
Typ
τ
::=
num
num
numbers
str
str
strings
Exp
e
::=
x
x
variable
num[n]
n
numeral
str[s]
”s”
literal
plus(e1; e2)
e1 + e2
addition
times(e1; e2)
e1 ∗e2
multiplication
cat(e1; e2)
e1 ^ e2
concatenation
len(e)
|e|
length
let(e1; x.e2)
let x be e1 in e2
deﬁnition
This chart deﬁnes two sorts, Typ, ranged over by τ, and Exp, ranged over by e. The chart de-
ﬁnes a set of operators and their arities. For example, it speciﬁes that the operator let has arity
(Exp, Exp.Exp)Exp, which speciﬁes that it has two arguments of sort Exp, and binds a variable of
sort Exp in the second argument.
4.2
Type System
The role of a type system is to impose constraints on the formations of phrases that are sensitive to
the context in which they occur. For example, whether the expression plus(x; num[n]) is sensible
depends on whether the variable x is restricted to have type num in the surrounding context of
the expression. This example is, in fact, illustrative of the general case, in that the only informa-
tion required about the context of an expression is the type of the variables within whose scope
the expression lies. Consequently, the statics of E consists of an inductive deﬁnition of generic
hypothetical judgments of the form
X | Γ ⊢e : τ,
where X is a ﬁnite set of variables, and Γ is a typing context consisting of hypotheses of the form
x : τ, one for each x ∈X . We rely on typographical conventions to determine the set of variables,
using the letters x and y to stand for them. We write x /∈dom(Γ) to say that there is no assumption
in Γ of the form x : τ for any type τ, in which case we say that the variable x is fresh for Γ.
The rules deﬁning the statics of E are as follows:
Γ, x : τ ⊢x : τ
(4.1a)
Γ ⊢str[s] : str
(4.1b)
Γ ⊢num[n] : num
(4.1c)
Γ ⊢e1 : num
Γ ⊢e2 : num
Γ ⊢plus(e1; e2) : num
(4.1d)
Γ ⊢e1 : num
Γ ⊢e2 : num
Γ ⊢times(e1; e2) : num
(4.1e)

4.3 Structural Properties
37
Γ ⊢e1 : str
Γ ⊢e2 : str
Γ ⊢cat(e1; e2) : str
(4.1f)
Γ ⊢e : str
Γ ⊢len(e) : num
(4.1g)
Γ ⊢e1 : τ1
Γ, x : τ1 ⊢e2 : τ2
Γ ⊢let(e1; x.e2) : τ2
(4.1h)
In rule (4.1h) we tacitly assume that the variable x is not already declared in Γ. This condition
may always be met by choosing a suitable representative of the α-equivalence class of the let
expression.
It is easy to check that every expression has at most one type by induction on typing, which is
rule induction applied to rules (4.1).
Lemma 4.1 (Unicity of Typing). For every typing context Γ and expression e, there exists at most one τ
such that Γ ⊢e : τ.
Proof. By rule induction on rules (4.1), making use of the fact that variables have at most one type
in any typing context.
The typing rules are syntax-directed in the sense that there is exactly one rule for each form
of expression. Consequently it is easy to give necessary conditions for typing an expression that
invert the sufﬁcient conditions expressed by the corresponding typing rule.
Lemma 4.2 (Inversion for Typing). Suppose that Γ ⊢e : τ. If e = plus(e1; e2), then τ = num,
Γ ⊢e1 : num, and Γ ⊢e2 : num, and similarly for the other constructs of the language.
Proof. These may all be proved by induction on the derivation of the typing judgment Γ ⊢e : τ.
In richer languages such inversion principles are more difﬁcult to state and to prove.
4.3
Structural Properties
The statics enjoys the structural properties of the generic hypothetical judgment.
Lemma 4.3 (Weakening). If Γ ⊢e′ : τ′, then Γ, x : τ ⊢e′ : τ′ for any x /∈dom(Γ) and any type τ.
Proof. By induction on the derivation of Γ ⊢e′ : τ′. We will give one case here, for rule (4.1h).
We have that e′ = let(e1; z.e2), where by the conventions on variables we may assume z is chosen
such that z /∈dom(Γ) and z ̸= x. By induction we have
1. Γ, x : τ ⊢e1 : τ1,
2. Γ, x : τ, z : τ1 ⊢e2 : τ′,
from which the result follows by rule (4.1h).

38
4.3 Structural Properties
Lemma 4.4 (Substitution). If Γ, x : τ ⊢e′ : τ′ and Γ ⊢e : τ, then Γ ⊢[e/x]e′ : τ′.
Proof. By induction on the derivation of Γ, x : τ ⊢e′ : τ′. We again consider only rule (4.1h). As in
the preceding case, e′ = let(e1; z.e2), where z is chosen so that z ̸= x and z /∈dom(Γ). We have by
induction and Lemma 4.3 that
1. Γ ⊢[e/x]e1 : τ1,
2. Γ, z : τ1 ⊢[e/x]e2 : τ′.
By the choice of z we have
[e/x]let(e1; z.e2) = let([e/x]e1; z.[e/x]e2).
It follows by rule (4.1h) that Γ ⊢[e/x]let(e1; z.e2) : τ′, as desired.
From a programming point of view, Lemma 4.3 allows us to use an expression in any context
that binds its free variables: if e is well-typed in a context Γ, then we may “import” it into any
context that includes the assumptions Γ. In other words introducing new variables beyond those
required by an expression e does not invalidate e itself; it remains well-formed, with the same
type.1 More importantly, Lemma 4.4 expresses the important concepts of modularity and linking.
We may think of the expressions e and e′ as two components of a larger system in which e′ is a client
of the implementation e. The client declares a variable specifying the type of the implementation,
and is type checked knowing only this information. The implementation must be of the speciﬁed
type to satisfy the assumptions of the client. If so, then we may link them to form the composite
system [e/x]e′. This implementation may itself be the client of another component, represented by
a variable y that is replaced by that component during linking. When all such variables have been
implemented, the result is a closed expression that is ready for execution (evaluation).
The converse of Lemma 4.4 is called decomposition. It states that any (large) expression can be
decomposed into a client and implementor by introducing a variable to mediate their interaction.
Lemma 4.5 (Decomposition). If Γ ⊢[e/x]e′ : τ′, then for every type τ such that Γ ⊢e : τ, we have
Γ, x : τ ⊢e′ : τ′.
Proof. The typing of [e/x]e′ depends only on the type of e wherever it occurs, if at all.
Lemma 4.5 tells us that any sub-expression can be isolated as a separate module of a larger
system. This property is especially useful when the variable x occurs more than once in e′, because
then one copy of e sufﬁces for all occurrences of x in e′.
The statics of E given by rules (4.1) exempliﬁes a recurrent pattern. The constructs of a language
are classiﬁed into one of two forms, the introduction and the elimination. The introduction forms
for a type determine the values, or canonical forms, of that type. The elimination forms determine
how to manipulate the values of a type to form a computation of another (possibly the same) type.
1This point may seem so obvious that it is not worthy of mention, but, surprisingly, there are useful type systems that
lack this property. Because they do not validate the structural principle of weakening, they are called substructural type
systems.

4.4 Notes
39
In the language E the introduction forms for the type num are the numerals, and those for the type
str are the literals. The elimination forms for the type num are addition and multiplication, and
those for the type str are concatenation and length.
The importance of this classiﬁcation will become clear once we have deﬁned the dynamics
of the language in Chapter 5. Then we will see that the elimination forms are inverse to the in-
troduction forms in that they “take apart” what the introduction forms have “put together.” The
coherence of the statics and dynamics of a language expresses the concept of type safety, the subject
of Chapter 6.
4.4
Notes
The concept of the static semantics of a programming language was historically slow to develop,
perhaps because the earliest languages had relatively few features and only very weak type sys-
tems. The concept of a static semantics in the sense considered here was introduced in the deﬁni-
tion of the Standard ML programming language (Milner et al., 1997), building on earlier work by
Church and others on the typed λ-calculus (Barendregt, 1992). The concept of introduction and
elimination, and the associated inversion principle, was introduced by Gentzen in his pioneer-
ing work on natural deduction (Gentzen, 1969). These principles were applied to the structure of
programming languages by Martin-L¨of (1984, 1980).
Exercises
4.1. It is sometimes useful to give the typing judgment Γ ⊢e : τ an “operational” reading that
speciﬁes more precisely the ﬂow of information required to derive a typing judgment (or
determine that it is not derivable). The analytic mode corresponds to the context, expression,
and type being given, with the goal to determine whether the typing judgment is derivable.
The synthetic mode corresponds to the context and expression being given, with the goal
to ﬁnd the unique type τ, if any, possessed by the expression in that context. These two
readings can be made explicit as judgments of the form e ↓τ, corresponding to the analytic
mode, and e ↑τ, corresponding to the synthetic mode.
Give a simultaneous inductive deﬁnition of these two judgments according to the following
guidelines:
(a) Variables are introduced in synthetic form.
(b) If we can synthesize a unique type for an expression, then we can analyze it with respect
to a given type by checking type equality.
(c) Deﬁnitions need care, because the type of the deﬁned expression is not given, even
when the type of the result is given.
There is room for variation; the point of the exercise is to explore the possibilities.

40
4.4 Notes
4.2. One way to limit the range of possibilities in the solution to Exercise 4.1 is to restrict and
extend the syntax of the language so that every expression is either synthetic or analytic
according to the following suggestions:
(a) Variables are analytic.
(b) Introduction forms are analytic, elimination forms are synthetic.
(c) An analytic expression can be made synthetic by introducing a type cast of the form
cast{τ}(e) specifying that e must check against the speciﬁed type τ, which is synthe-
sized for the whole expression.
(d) The deﬁning expression of a deﬁnition must be synthetic, but the scope of the deﬁnition
can be either synthetic or analytic.
Reformulate your solution to Exercise 4.1 to take account of these guidelines.

Chapter 5
Dynamics
The dynamics of a language describes how programs are executed. The most important way to de-
ﬁne the dynamics of a language is by the method of structural dynamics, which deﬁnes a transition
system that inductively speciﬁes the step-by-step process of executing a program. Another method
for presenting dynamics, called contextual dynamics, is a variation of structural dynamics in which
the transition rules are speciﬁed in a slightly different way. An equational dynamics presents the dy-
namics of a language by a collection of rules deﬁning when one program is deﬁnitionally equivalent
to another.
5.1
Transition Systems
A transition system is speciﬁed by the following four forms of judgment:
1. s state, asserting that s is a state of the transition system.
2. s ﬁnal, where s state, asserting that s is a ﬁnal state.
3. s initial, where s state, asserting that s is an initial state.
4. s 7−→s′, where s state and s′ state, asserting that state s may transition to state s′.
In practice we always arrange things so that no transition is possible from a ﬁnal state: if s ﬁnal,
then there is no s′ state such that s 7−→s′. A state from which no transition is possible is stuck.
Whereas all ﬁnal states are, by convention, stuck, there may be stuck states in a transition system
that are not ﬁnal. A transition system is deterministic iff for every state s there exists at most one
state s′ such that s 7−→s′, otherwise it is non-deterministic.
A transition sequence is a sequence of states s0, . . . , sn such that s0 initial, and si 7−→si+1 for
every 0 ≤i < n. A transition sequence is maximal iff there is no s such that sn 7−→s, and it is
complete iff it is maximal and sn ﬁnal. Thus every complete transition sequence is maximal, but
maximal sequences are not necessarily complete. The judgment s ↓means that there is a complete
transition sequence starting from s, which is to say that there exists s′ ﬁnal such that s 7−→∗s′.

42
5.2 Structural Dynamics
The iteration of transition judgment s 7−→∗s′ is inductively deﬁned by the following rules:
s 7−→∗s
(5.1a)
s 7−→s′
s′ 7−→∗s′′
s 7−→∗s′′
(5.1b)
When applied to the deﬁnition of iterated transition, the principle of rule induction states that
to show that P(s, s′) holds when s 7−→∗s′, it is enough to show these two properties of P:
1. P(s, s).
2. if s 7−→s′ and P(s′, s′′), then P(s, s′′).
The ﬁrst requirement is to show that P is reﬂexive. The second is to show that P is closed under
head expansion, or closed under inverse evaluation. Using this principle, it is easy to prove that 7−→∗
is reﬂexive and transitive.
The n-times iterated transition judgment s 7−→n s′, where n ≥0, is inductively deﬁned by the
following rules.
s 7−→0 s
(5.2a)
s 7−→s′
s′ 7−→n s′′
s 7−→n+1 s′′
(5.2b)
Theorem 5.1. For all states s and s′, s 7−→∗s′ iff s 7−→k s′ for some k ≥0.
Proof. From left to right, by induction on the deﬁnition of multi-step transition. From right to left,
by mathematical induction on k ≥0.
5.2
Structural Dynamics
A structural dynamics for the language E is given by a transition system whose states are closed
expressions. All states are initial. The ﬁnal states are the (closed) values, which represent the com-
pleted computations. The judgment e val, which states that e is a value, is inductively deﬁned by
the following rules:
num[n] val
(5.3a)
str[s] val
(5.3b)
The transition judgment e 7−→e′ between states is inductively deﬁned by the following rules:
n1 + n2 = n
plus(num[n1]; num[n2]) 7−→num[n]
(5.4a)
e1 7−→e′
1
plus(e1; e2) 7−→plus(e′
1; e2)
(5.4b)

5.2 Structural Dynamics
43
e1 val
e2 7−→e′
2
plus(e1; e2) 7−→plus(e1; e′
2)
(5.4c)
s1 ˆ s2 = s
cat(str[s1]; str[s2]) 7−→str[s]
(5.4d)
e1 7−→e′
1
cat(e1; e2) 7−→cat(e′
1; e2)
(5.4e)
e1 val
e2 7−→e′
2
cat(e1; e2) 7−→cat(e1; e′
2)
(5.4f)
"
e1 7−→e′
1
let(e1; x.e2) 7−→let(e′
1; x.e2)
#
(5.4g)
[e1 val]
let(e1; x.e2) 7−→[e1/x]e2
(5.4h)
We have omitted rules for multiplication and computing the length of a string, which follow a
similar pattern. Rules (5.4a), (5.4d), and (5.4h) are instruction transitions, because they correspond
to the primitive steps of evaluation. The remaining rules are search transitions that determine the
order of execution of instructions.
The bracketed rule, rule (5.4g), and bracketed premise on rule (5.4h), are included for a by-
value interpretation of let, and omitted for a by-name interpretation. The by-value interpretation
evaluates an expression before binding it to the deﬁned variable, whereas the by-name interpreta-
tion binds it in unevaluated form. The by-value interpretation saves work if the deﬁned variable
is used more than once, but wastes work if it is not used at all. Conversely, the by-name inter-
pretation saves work if the deﬁned variable is not used, and wastes work if it is used more than
once.
A derivation sequence in a structural dynamics has a two-dimensional structure, with the
number of steps in the sequence being its “width” and the derivation tree for each step being
its “height.” For example, consider the following evaluation sequence.
let(plus(num[1]; num[2]); x.plus(plus(x; num[3]); num[4]))
7−→
let(num[3]; x.plus(plus(x; num[3]); num[4]))
7−→
plus(plus(num[3]; num[3]); num[4])
7−→
plus(num[6]; num[4])
7−→
num[10]
Each step in this sequence of transitions is justiﬁed by a derivation according to rules (5.4). For
example, the third transition in the preceding example is justiﬁed by the following derivation:
plus(num[3]; num[3]) 7−→num[6] (5.4a)
plus(plus(num[3]; num[3]); num[4]) 7−→plus(num[6]; num[4]) (5.4b)
The other steps are similarly justiﬁed by composing rules.

44
5.3 Contextual Dynamics
The principle of rule induction for the structural dynamics of E states that to show P(e 7−→e′)
when e 7−→e′, it is enough to show that P is closed under rules (5.4). For example, we may show
by rule induction that the structural dynamics of E is determinate, which means that an expres-
sion may transition to at most one other expression. The proof requires a simple lemma relating
transition to values.
Lemma 5.2 (Finality of Values). For no expression e do we have both e val and e 7−→e′ for some e′.
Proof. By rule induction on rules (5.3) and (5.4).
Lemma 5.3 (Determinacy). If e 7−→e′ and e 7−→e′′, then e′ and e′′ are α-equivalent.
Proof. By rule induction on the premises e 7−→e′ and e 7−→e′′, carried out either simultaneously
or in either order. The primitive operators, such as addition, are assumed to have a unique value
when applied to values.
Rules (5.4) exemplify the inversion principle of language design, which states that the elimina-
tion forms are inverse to the introduction forms of a language. The search rules determine the
principal arguments of each elimination form, and the instruction rules specify how to evaluate
an elimination form when all of its principal arguments are in introduction form. For example,
rules (5.4) specify that both arguments of addition are principal, and specify how to evaluate an
addition once its principal arguments are evaluated to numerals. The inversion principle is cen-
tral to ensuring that a programming language is properly deﬁned, the exact statement of which is
given in Chapter 6.
5.3
Contextual Dynamics
A variant of structural dynamics, called contextual dynamics, is sometimes useful. There is no
fundamental difference between contextual and structural dynamics, rather one of style. The main
idea is to isolate instruction steps as a special form of judgment, called instruction transition, and
to formalize the process of locating the next instruction using a device called an evaluation context.
The judgment e val, deﬁning whether an expression is a value, remains unchanged.
The instruction transition judgment e1 →e2 for E is deﬁned by the following rules, together
with similar rules for multiplication of numbers and the length of a string.
m + n = p
plus(num[m]; num[n]) →num[p]
(5.5a)
s ˆ t = u
cat(str[s]; str[t]) →str[u]
(5.5b)
let(e1; x.e2) →[e1/x]e2
(5.5c)
The judgment E ectxt determines the location of the next instruction to execute in a larger
expression. The position of the next instruction step is speciﬁed by a “hole”, written ◦, into which

5.3 Contextual Dynamics
45
the next instruction is placed, as we shall detail shortly. (The rules for multiplication and length
are omitted for concision, as they are handled similarly.)
◦ectxt
(5.6a)
E1 ectxt
plus(E1; e2) ectxt
(5.6b)
e1 val
E2 ectxt
plus(e1; E2) ectxt
(5.6c)
The ﬁrst rule for evaluation contexts speciﬁes that the next instruction may occur “here”, at the
occurrence of the hole. The remaining rules correspond one-for-one to the search rules of the
structural dynamics. For example, rule (5.6c) states that in an expression plus(e1; e2), if the ﬁrst
argument, e1, is a value, then the next instruction step, if any, lies at or within the second argument,
e2.
An evaluation context is a template that is instantiated by replacing the hole with an instruction
to be executed. The judgment e′ = E{e} states that the expression e′ is the result of ﬁlling the hole
in the evaluation context E with the expression e. It is inductively deﬁned by the following rules:
e = ◦{e}
(5.7a)
e1 = E1{e}
plus(e1; e2) = plus(E1; e2){e}
(5.7b)
e1 val
e2 = E2{e}
plus(e1; e2) = plus(e1; E2){e}
(5.7c)
There is one rule for each form of evaluation context. Filling the hole with e results in e; otherwise
we proceed inductively over the structure of the evaluation context.
Finally, the contextual dynamics for E is deﬁned by a single rule:
e = E{e0}
e0 →e′
0
e′ = E{e′
0}
e 7−→e′
(5.8)
Thus, a transition from e to e′ consists of (1) decomposing e into an evaluation context and an
instruction, (2) execution of that instruction, and (3) replacing the instruction by the result of its
execution in the same spot within e to obtain e′.
The structural and contextual dynamics deﬁne the same transition relation. For the sake of
the proof, let us write e 7−→s e′ for the transition relation deﬁned by the structural dynam-
ics (rules (5.4)), and e 7−→c e′ for the transition relation deﬁned by the contextual dynamics
(rules (5.8)).
Theorem 5.4. e 7−→s e′ if, and only if, e 7−→c e′.

46
5.4 Equational Dynamics
Proof. From left to right, proceed by rule induction on rules (5.4). It is enough in each case to
exhibit an evaluation context E such that e = E{e0}, e′ = E{e′
0}, and e0 →e′
0. For example, for
rule (5.4a), take E = ◦, and note that e →e′. For rule (5.4b), we have by induction that there exists
an evaluation context E1 such that e1 = E1{e0}, e′
1 = E1{e′
0}, and e0 →e′
0. Take E = plus(E1; e2),
and note that e = plus(E1; e2){e0} and e′ = plus(E1; e2){e′
0} with e0 →e′
0.
From right to left, note that if e 7−→c e′, then there exists an evaluation context E such that
e = E{e0}, e′ = E{e′
0}, and e0 →e′
0. We prove by induction on rules (5.7) that e 7−→s e′. For
example, for rule (5.7a), e0 is e, e′
0 is e′, and e →e′. Hence e 7−→s e′. For rule (5.7b), we have
that E = plus(E1; e2), e1 = E1{e0}, e′
1 = E1{e′
0}, and e1 7−→s e′
1. Therefore e is plus(e1; e2), e′ is
plus(e′
1; e2), and therefore by rule (5.4b), e 7−→s e′.
Because the two transition judgments coincide, contextual dynamics can be considered an al-
ternative presentation of a structural dynamics. It has two advantages over structural dynam-
ics, one relatively superﬁcial, one rather less so. The superﬁcial advantage stems from writing
rule (5.8) in the simpler form
e0 →e′
0
E{e0} 7−→E{e′
0}
.
(5.9)
This formulation is superﬁcially simpler in that it does not make explicit how an expression is
decomposed into an evaluation context and a reducible expression. The deeper advantage of con-
textual dynamics is that all transitions are between complete programs. One need never consider a
transition between expressions of any type other than the observable type, which simpliﬁes certain
arguments, such as the proof of Lemma 47.16.
5.4
Equational Dynamics
Another formulation of the dynamics of a language regards computation as a form of equational
deduction, much in the style of elementary algebra. For example, in algebra we may show that
the polynomials x2 + 2 x + 1 and (x + 1)2 are equivalent by a simple process of calculation and
re-organization using the familiar laws of addition and multiplication. The same laws are enough
to determine the value of any polynomial, given the values of its variables. So, for example, we
may plug in 2 for x in the polynomial x2 + 2 x + 1 and calculate that 22 + 2 × 2 + 1 = 9, which is
indeed (2 + 1)2. We thus obtain a model of computation in which the value of a polynomial for a
given value of its variable is determined by substitution and simpliﬁcation.
Very similar ideas give rise to the concept of deﬁnitional, or computational, equivalence of expres-
sions in E, which we write as X | Γ ⊢e ≡e′ : τ, where Γ consists of one assumption of the form
x : τ for each x ∈X . We only consider deﬁnitional equality of well-typed expressions, so that
when considering the judgment Γ ⊢e ≡e′ : τ, we tacitly assume that Γ ⊢e : τ and Γ ⊢e′ : τ.
Here, as usual, we omit explicit mention of the variables X when they can be determined from the
forms of the assumptions Γ.
Deﬁnitional equality of expressions in E under the by-name interpretation of let is inductively
deﬁned by the following rules:
Γ ⊢e ≡e : τ
(5.10a)

5.4 Equational Dynamics
47
Γ ⊢e′ ≡e : τ
Γ ⊢e ≡e′ : τ
(5.10b)
Γ ⊢e ≡e′ : τ
Γ ⊢e′ ≡e′′ : τ
Γ ⊢e ≡e′′ : τ
(5.10c)
Γ ⊢e1 ≡e′
1 : num
Γ ⊢e2 ≡e′
2 : num
Γ ⊢plus(e1; e2) ≡plus(e′
1; e′
2) : num
(5.10d)
Γ ⊢e1 ≡e′
1 : str
Γ ⊢e2 ≡e′
2 : str
Γ ⊢cat(e1; e2) ≡cat(e′
1; e′
2) : str
(5.10e)
Γ ⊢e1 ≡e′
1 : τ1
Γ, x : τ1 ⊢e2 ≡e′
2 : τ2
Γ ⊢let(e1; x.e2) ≡let(e′
1; x.e′
2) : τ2
(5.10f)
n1 + n2 = n
Γ ⊢plus(num[n1]; num[n2]) ≡num[n] : num
(5.10g)
s1 ˆ s2 = s
Γ ⊢cat(str[s1]; str[s2]) ≡str[s] : str
(5.10h)
Γ ⊢let(e1; x.e2) ≡[e1/x]e2 : τ
(5.10i)
Rules (5.10a) through (5.10c) state that deﬁnitional equality is an equivalence relation. Rules (5.10d)
through (5.10f) state that it is a congruence relation, which means that it is compatible with all
expression-forming constructs in the language. Rules (5.10g) through (5.10i) specify the meanings
of the primitive constructs of E. We say that rules (5.10) deﬁne the strongest congruence closed
under rules (5.10g), (5.10h), and (5.10i).
Rules (5.10) sufﬁce to calculate the value of an expression by a deduction similar to that used
in high school algebra. For example, we may derive the equation
let x be 1 + 2 in x + 3 + 4 ≡10 : num
by applying rules (5.10). Here, as in general, there may be many different ways to derive the same
equation, but we need ﬁnd only one derivation in order to carry out an evaluation.
Deﬁnitional equality is rather weak in that many equivalences that we might intuitively think
are true are not derivable from rules (5.10). A prototypical example is the putative equivalence
x1 : num, x2 : num ⊢x1 + x2 ≡x2 + x1 : num,
(5.11)
which, intuitively, expresses the commutativity of addition. Although we shall not prove this here,
this equivalence is not derivable from rules (5.10). And yet we may derive all of its closed instances,
n1 + n2 ≡n2 + n1 : num,
(5.12)
where n1 nat and n2 nat are particular numbers.
The “gap” between a general law, such as Equation (5.11), and all of its instances, given by
Equation (5.12), may be ﬁlled by enriching the notion of equivalence to include a principle of proof
by mathematical induction. Such a notion of equivalence is sometimes called semantic equivalence,
because it expresses relationships that hold by virtue of the dynamics of the expressions involved.
(Semantic equivalence is developed rigorously for a related language in Chapter 46.)

48
5.5 Notes
Theorem 5.5. For the expression language E, the relation e ≡e′ : τ holds iff there exists e0 val such that
e 7−→∗e0 and e′ 7−→∗e0.
Proof. The proof from right to left is direct, because every transition step is a valid equation. The
converse follows from the following, more general, proposition, which is proved by induction on
rules (5.10): if x1 : τ1, . . . , xn : τn ⊢e ≡e′ : τ, then when e1 : τ1, e′
1 : τ1, . . . , en : τn, e′
n : τn, if for each
1 ≤i ≤n the expressions ei and e′
i evaluate to a common value vi, then there exists e0 val such that
[e1, . . . , en/x1, . . . , xn]e 7−→∗e0
and
[e′
1, . . . , e′
n/x1, . . . , xn]e′ 7−→∗e0.
5.5
Notes
The use of transition systems to specify the behavior of programs goes back to the early work of
Church and Turing on computability. Turing’s approach emphasized the concept of an abstract
machine consisting of a ﬁnite program together with unbounded memory. Computation proceeds
by changing the memory in accordance with the instructions in the program. Much early work on
the operational semantics of programming languages, such as the SECD machine (Landin, 1965),
emphasized machine models. Church’s approach emphasized the language for expressing com-
putations, and deﬁned execution in terms of the programs themselves, rather than in terms of aux-
iliary concepts such as memories or tapes. Plotkin’s elegant formulation of structural operational
semantics (Plotkin, 1981), which we use heavily throughout this book, was inspired by Church’s
and Landin’s ideas (Plotkin, 2004). Contextual semantics, which was introduced by Felleisen and
Hieb (1992), may be seen as an alternative formulation of structural semantics in which “search
rules” are replaced by “context matching”. Computation viewed as equational deduction goes
back to the early work of Herbrand, G¨odel, and Church.
Exercises
5.1. Prove that if s 7−→∗s′ and s′ 7−→∗s′′, then s 7−→∗s′′.
5.2. Complete the proof of Theorem 5.1 along the lines suggested there.
5.3. Complete the proof of Theorem 5.5 along the lines suggested there.

Chapter 6
Type Safety
Most programming languages are safe (or, type safe, or strongly typed). Informally, this means that
certain kinds of mismatches cannot arise during execution. For example, type safety for E states
that it will never arise that a number is added to a string, or that two numbers are concatenated,
neither of which is meaningful.
In general type safety expresses the coherence between the statics and the dynamics. The statics
may be seen as predicting that the value of an expression will have a certain form so that the
dynamics of that expression is well-deﬁned. Consequently, evaluation cannot “get stuck” in a
state for which no transition is possible, corresponding in implementation terms to the absence
of “illegal instruction” errors at execution time. Safety is proved by showing that each step of
transition preserves typability and by showing that typable states are well-deﬁned. Consequently,
evaluation can never “go off into the weeds,” and hence can never encounter an illegal instruction.
Type safety for the language E is stated precisely as follows:
Theorem 6.1 (Type Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val, or there exists e′ such that e 7−→e′.
The ﬁrst part, called preservation, says that the steps of evaluation preserve typing; the second,
called progress, ensures that well-typed expressions are either values or can be further evaluated.
Safety is the conjunction of preservation and progress.
We say that an expression e is stuck iff it is not a value, yet there is no e′ such that e 7−→e′. It
follows from the safety theorem that a stuck state is necessarily ill-typed. Or, putting it the other
way around, that well-typed states do not get stuck.
6.1
Preservation
The preservation theorem for E deﬁned in Chapters 4 and 5 is proved by rule induction on the
transition system (rules (5.4)).

50
6.2 Progress
Theorem 6.2 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. We will give the proof in two cases, leaving the rest to the reader. Consider rule (5.4b),
e1 7−→e′
1
plus(e1; e2) 7−→plus(e′
1; e2)
.
Assume that plus(e1; e2) : τ. By inversion for typing, we have that τ = num, e1 : num, and e2 : num.
By induction we have that e′
1 : num, and hence plus(e′
1; e2) : num. The case for concatenation is
handled similarly.
Now consider rule (5.4h),
let(e1; x.e2) 7−→[e1/x]e2
.
Assume that let(e1; x.e2) : τ2. By the inversion lemma 4.2, e1 : τ1 for some τ1 such that x : τ1 ⊢e2 :
τ2. By the substitution lemma 4.4 [e1/x]e2 : τ2, as desired.
It is easy to check that the primitive operations are all type-preserving; for example, if a nat
and b nat and a + b = c, then c nat.
The proof of preservation is naturally structured as an induction on the transition judgment,
because the argument hinges on examining all possible transitions from a given expression. In
some cases we may manage to carry out a proof by structural induction on e, or by an induction on
typing, but experience shows that this often leads to awkward arguments, or, sometimes, cannot
be made to work at all.
6.2
Progress
The progress theorem captures the idea that well-typed programs cannot “get stuck”. The proof
depends crucially on the following lemma, which characterizes the values of each type.
Lemma 6.3 (Canonical Forms). If e val and e : τ, then
1. If τ = num, then e = num[n] for some number n.
2. If τ = str, then e = str[s] for some string s.
Proof. By induction on rules (4.1) and (5.3).
Progress is proved by rule induction on rules (4.1) deﬁning the statics of the language.
Theorem 6.4 (Progress). If e : τ, then either e val, or there exists e′ such that e 7−→e′.
Proof. The proof proceeds by induction on the typing derivation. We will consider only one case,
for rule (4.1d),
e1 : num
e2 : num
plus(e1; e2) : num
,

6.3 Run-Time Errors
51
where the context is empty because we are considering only closed terms.
By induction we have that either e1 val, or there exists e′
1 such that e1 7−→e′
1. In the latter case it
follows that plus(e1; e2) 7−→plus(e′
1; e2), as required. In the former we also have by induction that
either e2 val, or there exists e′
2 such that e2 7−→e′
2. In the latter case we have that plus(e1; e2) 7−→
plus(e1; e′
2), as required. In the former, we have, by the Canonical Forms Lemma 6.3, e1 = num[n1]
and e2 = num[n2], and hence
plus(num[n1]; num[n2]) 7−→num[n1 + n2].
Because the typing rules for expressions are syntax-directed, the progress theorem could equally
well be proved by induction on the structure of e, appealing to the inversion theorem at each step
to characterize the types of the parts of e. But this approach breaks down when the typing rules
are not syntax-directed, that is, when there is more than one rule for a given expression form. Such
rules present no difﬁculites, so long as the proof proceeds by induction on the typing rules, and
not on the structure of the expression.
Summing up, the combination of preservation and progress together constitute the proof of
safety. The progress theorem ensures that well-typed expressions do not “get stuck” in an ill-
deﬁned state, and the preservation theorem ensures that if a step is taken, the result remains
well-typed (with the same type). Thus the two parts work together to ensure that the statics and
dynamics are coherent, and that no ill-deﬁned states can ever be encountered while evaluating a
well-typed expression.
6.3
Run-Time Errors
Suppose that we wish to extend E with, say, a quotient operation that is undeﬁned for a zero
divisor. The natural typing rule for quotients is given by the following rule:
e1 : num
e2 : num
div(e1; e2) : num
.
But the expression div(num[3]; num[0]) is well-typed, yet stuck! We have two options to correct this
situation:
1. Enhance the type system, so that no well-typed program may divide by zero.
2. Add dynamic checks, so that division by zero signals an error as the outcome of evaluation.
Either option is, in principle, practical, but the most common approach is the second. The ﬁrst
requires that the type checker prove that an expression be non-zero before permitting it to be used
in the denominator of a quotient. It is difﬁcult to do this without ruling out too many programs as
ill-formed. We cannot predict statically whether an expression will be non-zero when evaluated,
so the second approach is most often used in practice.
The overall idea is to distinguish checked from unchecked errors. An unchecked error is one
that is ruled out by the type system. No run-time checking is performed to ensure that such an

52
6.4 Notes
error does not occur, because the type system rules out the possibility of it arising. For example,
the dynamics need not check, when performing an addition, that its two arguments are, in fact,
numbers, as opposed to strings, because the type system ensures that this is the case. On the other
hand the dynamics for quotient must check for a zero divisor, because the type system does not
rule out the possibility.
One approach to modeling checked errors is to give an inductive deﬁnition of the judgment
e err stating that the expression e incurs a checked run-time error, such as division by zero. Here
are some representative rules that would be present in a full inductive deﬁnition of this judgment:
e1 val
div(e1; num[0]) err
(6.1a)
e1 err
div(e1; e2) err
(6.1b)
e1 val
e2 err
div(e1; e2) err
(6.1c)
Rule (6.1a) signals an error condition for division by zero. The other rules propagate this error
upwards: if an evaluated sub-expression is a checked error, then so is the overall expression.
Once the error judgment is available, we may also consider an expression, error, which forcibly
induces an error, with the following static and dynamic semantics:
Γ ⊢error : τ
(6.2a)
error err
(6.2b)
The preservation theorem is not affected by checked errors. However, the statement (and proof)
of progress is modiﬁed to account for checked errors.
Theorem 6.5 (Progress With Error). If e : τ, then either e err, or e val, or there exists e′ such that
e 7−→e′.
Proof. The proof is by induction on typing, and proceeds similarly to the proof given earlier, except
that there are now three cases to consider at each point in the proof.
6.4
Notes
The concept of type safety was ﬁrst formulated by Milner (1978), who invented the slogan “well-
typed programs do not go wrong.” Whereas Milner relied on an explicit notion of “going wrong”
to express the concept of a type error, Wright and Felleisen (1994) observed that we can instead
show that ill-deﬁned states cannot arise in a well-typed program, giving rise to the slogan “well-
typed programs do not get stuck.” However, their formulation relied on an analysis showing
that no stuck state is well-typed. The progress theorem, which relies on the characterization of
canonical forms in the style of Martin-L¨of (1980), eliminates this analysis.

6.4 Notes
53
Exercises
6.1. Complete the proof of Theorem 6.2 in full detail.
6.2. Complete the proof of Theorem 6.4 in full detail.
6.3. Give several cases of the proof of Theorem 6.5 to illustrate how checked errors are handled
in type safety proofs.

54
6.4 Notes

Chapter 7
Evaluation Dynamics
In Chapter 5 we deﬁned evaluation of expressions in E using a structural dynamics. Structural
dynamics is very useful for proving safety, but for some purposes, such as writing a user manual,
another formulation, called evaluation dynamics is preferable. An evaluation dynamics is a relation
between a phrase and its value that is deﬁned without detailing the step-by-step process of evalu-
ation. A cost dynamics enriches an evaluation dynamics with a cost measure specifying the resource
usage of evaluation. A prime example is time, measured as the number of transition steps required
to evaluate an expression according to its structural dynamics.
7.1
Evaluation Dynamics
An evaluation dynamics, consists of an inductive deﬁnition of the evaluation judgment e ⇓v stating
that the closed expression e evaluates to the value v. The evaluation dynamics of E is deﬁned by
the following rules:
num[n] ⇓num[n]
(7.1a)
str[s] ⇓str[s]
(7.1b)
e1 ⇓num[n1]
e2 ⇓num[n2]
n1 + n2 = n
plus(e1; e2) ⇓num[n]
(7.1c)
e1 ⇓str[s1]
e2 ⇓str[s2]
s1 ˆ s2 = s
cat(e1; e2) ⇓str[s]
(7.1d)
e ⇓str[s]
|s| = n
len(e) ⇓num[n]
(7.1e)
[e1/x]e2 ⇓v2
let(e1; x.e2) ⇓v2
(7.1f)

56
7.2 Relating Structural and Evaluation Dynamics
The value of a let expression is determined by substitution of the binding into the body. The rules
are not syntax-directed, because the premise of rule (7.1f) is not a sub-expression of the expression
in the conclusion of that rule.
Rule (7.1f) speciﬁes a by-name interpretation of deﬁnitions. For a by-value interpretation the
following rule should be used instead:
e1 ⇓v1
[v1/x]e2 ⇓v2
let(e1; x.e2) ⇓v2
(7.2)
Because the evaluation judgment is inductively deﬁned, we prove properties of it by rule in-
duction. Speciﬁcally, to show that the property P(e ⇓v) holds, it is enough to show that P is
closed under rules (7.1):
1. Show that P(num[n] ⇓num[n]).
2. Show that P(str[s] ⇓str[s]).
3. Show that P(plus(e1; e2) ⇓num[n]), if P(e1 ⇓num[n1]), P(e2 ⇓num[n2]), and n1 + n2 = n.
4. Show that P(cat(e1; e2) ⇓str[s]), if P(e1 ⇓str[s1]), P(e2 ⇓str[s2]), and s1 ˆ s2 = s.
5. Show that P(let(e1; x.e2) ⇓v2), if P([e1/x]e2 ⇓v2).
This induction principle is not the same as structural induction on e itself, because the evaluation
rules are not syntax-directed.
Lemma 7.1. If e ⇓v, then v val.
Proof. By induction on rules (7.1). All cases except rule (7.1f) are immediate. For the latter case, the
result follows directly by an appeal to the inductive hypothesis for the premise of the evaluation
rule.
7.2
Relating Structural and Evaluation Dynamics
We have given two different forms of dynamics for E. It is natural to ask whether they are equiv-
alent, but to do so ﬁrst requires that we consider carefully what we mean by equivalence. The
structural dynamics describes a step-by-step process of execution, whereas the evaluation dynam-
ics suppresses the intermediate states, focusing attention on the initial and ﬁnal states alone. This
remark suggests that the right correspondence is between complete execution sequences in the
structural dynamics and the evaluation judgment in the evaluation dynamics.
Theorem 7.2. For all closed expressions e and values v, e 7−→∗v iff e ⇓v.
How might we prove such a theorem? We will consider each direction separately. We consider
the easier case ﬁrst.
Lemma 7.3. If e ⇓v, then e 7−→∗v.

7.3 Type Safety, Revisited
57
Proof. By induction on the deﬁnition of the evaluation judgment.
For example, suppose that
plus(e1; e2) ⇓num[n] by the rule for evaluating additions. By induction we know that e1 7−→∗
num[n1] and e2 7−→∗num[n2]. We reason as follows:
plus(e1; e2)
7−→∗
plus(num[n1]; e2)
7−→∗
plus(num[n1]; num[n2])
7−→
num[n1 + n2]
Therefore plus(e1; e2) 7−→∗num[n1 + n2], as required. The other cases are handled similarly.
For the converse, recall from Chapter 5 the deﬁnitions of multi-step evaluation and complete
evaluation. Because v ⇓v when v val, it sufﬁces to show that evaluation is closed under converse
evaluation:1
Lemma 7.4. If e 7−→e′ and e′ ⇓v, then e ⇓v.
Proof. By induction on the deﬁnition of the transition judgment. For example, suppose that plus(e1; e2) 7−→
plus(e′
1; e2), where e1 7−→e′
1. Suppose further that plus(e′
1; e2) ⇓v, so that e′
1 ⇓num[n1], and
e2 ⇓num[n2], and n1 + n2 = n, and v is num[n]. By induction e1 ⇓num[n1], and hence plus(e1; e2) ⇓
num[n], as required.
7.3
Type Safety, Revisited
Type safety is deﬁned in Chapter 6 as preservation and progress (Theorem 6.1). These concepts are
meaningful when applied to a dynamics given by a transition system, as we shall do throughout
this book. But what if we had instead given the dynamics as an evaluation relation? How is type
safety proved in that case?
The answer, unfortunately, is that we cannot. Although there is an analog of the preservation
property for an evaluation dynamics, there is no clear analog of the progress property. Preser-
vation may be stated as saying that if e ⇓v and e : τ, then v : τ. It can be readily proved by
induction on the evaluation rules. But what is the analog of progress? We might be tempted to
phrase progress as saying that if e : τ, then e ⇓v for some v. Although this property is true for E,
it demands much more than just progress — it requires that every expression evaluate to a value!
If E were extended to admit operations that may result in an error (as discussed in Section 6.3), or
to admit non-terminating expressions, then this property would fail, even though progress would
remain valid.
One possible attitude towards this situation is to conclude that type safety cannot be properly
discussed in the context of an evaluation dynamics, but only by reference to a structural dynamics.
Another point of view is to instrument the dynamics with explicit checks for dynamic type errors,
and to show that any expression with a dynamic type fault must be statically ill-typed. Re-stated
in the contrapositive, this means that a statically well-typed program cannot incur a dynamic type
error. A difﬁculty with this point of view is that we must explicitly account for a form of error
1Converse evaluation is also known as head expansion.

58
7.4 Cost Dynamics
solely to prove that it cannot arise! Nevertheless, a semblance of type safety can be established
using evaluation dynamics.
We deﬁne a judgment e ?? stating that the expression e goes wrong when executed. The exact
deﬁnition of “going wrong” is given by a set of rules, but the intention is that it should cover all
situations that correspond to type errors. The following rules are representative of the general
case:
plus(str[s]; e2) ??
(7.3a)
e1 val
plus(e1; str[s]) ??
(7.3b)
These rules explicitly check for the misapplication of addition to a string; similar rules govern each
of the primitive constructs of the language.
Theorem 7.5. If e ??, then there is no τ such that e : τ.
Proof. By rule induction on rules (7.3). For example, for rule (7.3a), we note that str[s] : str, and
hence plus(str[s]; e2) is ill-typed.
Corollary 7.6. If e : τ, then ¬(e ??).
Apart from the inconvenience of having to deﬁne the judgment e ?? only to show that it is irrel-
evant for well-typed programs, this approach suffers a very signiﬁcant methodological weakness.
If we should omit one or more rules deﬁning the judgment e ??, the proof of Theorem 7.5 remains
valid; there is nothing to ensure that we have included sufﬁciently many checks for run-time type
errors. We can prove that the ones we deﬁne cannot arise in a well-typed program, but we cannot
prove that we have covered all possible cases. By contrast the structural dynamics does not spec-
ify any behavior for ill-typed expressions. Consequently, any ill-typed expression will “get stuck”
without our explicit intervention, and the progress theorem rules out all such cases. Moreover,
the transition system corresponds more closely to implementation—a compiler need not make
any provisions for checking for run-time type errors. Instead, it relies on the statics to ensure that
these cannot arise, and assigns no meaning to any ill-typed program. Therefore, execution is more
efﬁcient, and the language deﬁnition is simpler.
7.4
Cost Dynamics
A structural dynamics provides a natural notion of time complexity for programs, namely the num-
ber of steps required to reach a ﬁnal state. An evaluation dynamics, however, does not provide
such a direct notion of time. Because the individual steps required to complete an evaluation
are suppressed, we cannot directly read off the number of steps required to evaluate to a value.
Instead we must augment the evaluation relation with a cost measure, resulting in a cost dynamics.
Evaluation judgments have the form e ⇓k v, with the meaning that e evaluates to v in k steps.
num[n] ⇓0 num[n]
(7.4a)

7.5 Notes
59
e1 ⇓k1 num[n1]
e2 ⇓k2 num[n2]
plus(e1; e2) ⇓k1+k2+1 num[n1 + n2]
(7.4b)
str[s] ⇓0 str[s]
(7.4c)
e1 ⇓k1 s1
e2 ⇓k2 s2
cat(e1; e2) ⇓k1+k2+1 str[s1 ˆ s2]
(7.4d)
[e1/x]e2 ⇓k2 v2
let(e1; x.e2) ⇓k2+1 v2
(7.4e)
For a by-value interpretation of let, rule (7.4e) is replaced by the following rule:
e1 ⇓k1 v1
[v1/x]e2 ⇓k2 v2
let(e1; x.e2) ⇓k1+k2+1 v2
(7.5)
Theorem 7.7. For any closed expression e and closed value v of the same type, e ⇓k v iff e 7−→k v.
Proof. From left to right proceed by rule induction on the deﬁnition of the cost dynamics. From
right to left proceed by induction on k, with an inner rule induction on the deﬁnition of the struc-
tural dynamics.
7.5
Notes
The structural similarity between evaluation dynamics and typing rules was ﬁrst developed in
The Deﬁnition of Standard ML (Milner et al., 1997). The advantage of evaluation semantics is its
directness; its disadvantage is that it is not well-suited to proving properties such as type safety.
Robin Milner introduced the apt phrase “going wrong” as a description of a type error. Cost
dynamics was introduced by Blelloch and Greiner (1996) in a study of parallel computation (see
Chapter 37).
Exercises
7.1. Show that evaluation is deterministic: if e ⇓v1 and e ⇓v2, then v1 = v2.
7.2. Complete the proof of Lemma 7.3.
7.3. Complete the proof of Lemma 7.4. Then show that if e 7−→∗e′ with e′ val, then e ⇓e′.
7.4. Augment the evaluation dynamics with checked errors, along the lines sketched in Chap-
ter 5, using e ?? to say that e incurs a checked (or an unchecked) error. What remains unsat-
isfactory about the type safety proof? Can you think of a better alternative?

60
7.5 Notes
7.5. Consider generic hypothetical judgments of the form
x1 ⇓v1, . . . , xn ⇓vn ⊢e ⇓v
where v1 val, . . . , vn val, and v val. The hypotheses, written ∆, are called the environment of
the evaluation; they provide the values of the free variables in e. The hypothetical judgment
∆⊢e ⇓v is called an environmental evaluation dynamics.
Give a hypothetical inductive deﬁnition of the environmental evaluation dynamics without
making any use of substitution. In particular, you should include the rule
∆, x ⇓v ⊢x ⇓v
deﬁning the evaluation of a free variable.
Show that x1 ⇓v1, . . . , xn ⇓vn ⊢e ⇓v iff [v1, . . . , vn/x1, . . . , xn]e ⇓v (using the by-value
form of evaluation).

Part III
Total Functions


Chapter 8
Function Deﬁnitions and Values
In the language E we may perform calculations such as the doubling of a given expression, but we
cannot express doubling as a concept in itself. To capture the pattern of doubling a number, we
abstract away from the particular number being doubled using a variable to stand for a ﬁxed, but
unspeciﬁed, number, to express the doubling of an arbitrary number. Any particular instance of
doubling may then be obtained by substituting a numeric expression for that variable. In general
an expression may involve many distinct variables, necessitating that we specify which of several
possible variables is varying in a particular context, giving rise to a function of that variable.
In this chapter we will consider two extensions of E with functions. The ﬁrst, and perhaps
most obvious, extension is by adding function deﬁnitions to the language. A function is deﬁned by
binding a name to an abt with a bound variable that serves as the argument of that function. A
function is applied by substituting a particular expression (of suitable type) for the bound variable,
obtaining an expression.
The domain and range of deﬁned functions are limited to the types nat and str, because these
are the only types of expression. Such functions are called ﬁrst-order functions, in contrast to higher-
order functions, which permit functions as arguments and results of other functions. Because the
domain and range of a function are types, this requires that we introduce function types whose
elements are functions. Consequently, we may form functions of higher type, those whose domain
and range may themselves be function types.
8.1
First-Order Functions
The language ED extends E with function deﬁnitions and function applications as described by
the following grammar:
Exp
e
::=
apply{ f }(e)
f (e)
application
fun{τ1; τ2}(x1.e2; f.e)
fun f (x1 : τ1) : τ2 = e2 in e
deﬁnition
The expression fun{τ1; τ2}(x1.e2; f.e) binds the function name f within e to the pattern x1.e2, which
has argument x1 and deﬁnition e2. The domain and range of the function are, respectively, the

64
8.1 First-Order Functions
types τ1 and τ2. The expression apply{ f }(e) instantiates the binding of f with the argument e.
The statics of ED deﬁnes two forms of judgment:
1. Expression typing, e : τ, stating that e has type τ;
2. Function typing, f (τ1) : τ2, stating that f is a function with argument type τ1 and result type
τ2.
The judgment f (τ1) : τ2 is called the function header of f; it speciﬁes the domain type and the range
type of a function.
The statics of ED is deﬁned by the following rules:
Γ, x1 : τ1 ⊢e2 : τ2
Γ, f (τ1) : τ2 ⊢e : τ
Γ ⊢fun{τ1; τ2}(x1.e2; f.e) : τ
(8.1a)
Γ ⊢f (τ1) : τ2
Γ ⊢e : τ1
Γ ⊢apply{ f }(e) : τ2
(8.1b)
Function substitution, written [[x.e/ f ]]e′, is deﬁned by induction on the structure of e′ much like
ordinary substitution. However, a function name f does not stand for an expression, and can only
occur in an application of the form apply{ f }(e). Function substitution is deﬁned by the following
rule:
[[x.e/ f ]]apply{ f }(e′) = let([[x.e/ f ]]e′; x.e)
(8.2)
At application sites to f with argument e′, function substitution yields a let expression that binds
x to the result of expanding any further applications to f within e′.
Lemma 8.1. If Γ, f (τ1) : τ2 ⊢e : τ and Γ, x1 : τ1 ⊢e2 : τ2, then Γ ⊢[[x1.e2/ f ]]e : τ.
Proof. By rule induction on the ﬁrst premise, similarly to the proof of Lemma 4.4.
The dynamics of ED is deﬁned using function substitution:
fun{τ1; τ2}(x1.e2; f.e) 7−→[[x1.e2/ f ]]e
(8.3)
Because function substitution replaces all applications of f by appropriate let expressions, there
is no need to give a rule for application expressions (essentially, they behave like variables that are
replaced during evaluation, and not like a primitive operation of the language.)
The safety of ED may, with some effort, be derived from the safety theorem for higher-order
functions, which we discuss next.

8.2 Higher-Order Functions
65
8.2
Higher-Order Functions
The similarity between variable deﬁnitions and function deﬁnitions in ED is striking. Is it possible
to combine them? The gap that must be bridged is the segregation of functions from expressions.
A function name f is bound to an abstractor x.e specifying a pattern that is instantiated when f is
applied. To reduce function deﬁnitions to ordinary deﬁnitions, we reify the abstractor into a form
of expression, called a λ-abstraction, written lam{τ1}(x.e). Applications generalize to ap(e1; e2),
where e1 is an expression denoting a function, and not just a function name. λ-abstraction and
application are the introduction and elimination forms for the function type arr(τ1; τ2), which clas-
siﬁes functions with domain τ1 and range τ2.
The language EF enriches E with function types, as speciﬁed by the following grammar:
Typ
τ
::=
arr(τ1; τ2)
τ1 →τ2
function
Exp
e
::=
lam{τ}(x.e)
λ (x : τ) e
abstraction
ap(e1; e2)
e1(e2)
application
In EF functions are ﬁrst-class in that they are a form of expression that can be used like any other. In
particular functions may be passed as arguments to, and returned as results from, other functions.
For this reason ﬁrst-class functions are said to be higher-order, rather than ﬁrst-order.
The statics of EF is given by extending rules (4.1) with the following rules:
Γ, x : τ1 ⊢e : τ2
Γ ⊢lam{τ1}(x.e) : arr(τ1; τ2)
(8.4a)
Γ ⊢e1 : arr(τ2; τ)
Γ ⊢e2 : τ2
Γ ⊢ap(e1; e2) : τ
(8.4b)
Lemma 8.2 (Inversion). Suppose that Γ ⊢e : τ.
1. If e = lam{τ1}(x.e2), then τ = arr(τ1; τ2) and Γ, x : τ1 ⊢e2 : τ2.
2. If e = ap(e1; e2), then there exists τ2 such that Γ ⊢e1 : arr(τ2; τ) and Γ ⊢e2 : τ2.
Proof. The proof proceeds by rule induction on the typing rules. Observe that for each rule, exactly
one case applies, and that the premises of the rule provide the required result.
Lemma 8.3 (Substitution). If Γ, x : τ ⊢e′ : τ′, and Γ ⊢e : τ, then Γ ⊢[e/x]e′ : τ′.
Proof. By rule induction on the derivation of the ﬁrst judgment.
The dynamics of EF extends that of E with the following rules:
lam{τ}(x.e) val
(8.5a)
e1 7−→e′
1
ap(e1; e2) 7−→ap(e′
1; e2)
(8.5b)

66
8.2 Higher-Order Functions
"
e1 val
e2 7−→e′
2
ap(e1; e2) 7−→ap(e1; e′
2)
#
(8.5c)
[e2 val]
ap(lam{τ2}(x.e1); e2) 7−→[e2/x]e1
(8.5d)
The bracketed rule and premise are included for a call-by-value interpretation of function applica-
tion, and excluded for a call-by-name interpretation.1
When functions are ﬁrst-class, there is no need for function declarations: simply replace the
function declaration fun f (x1 : τ1) : τ2 = e2 in e by the deﬁnition let λ (x : τ1) e2 be f in e, and re-
place second-class function application f (e) by the ﬁrst-class function application f (e). Because
λ-abstractions are values, it makes no difference whether the deﬁnition is evaluated by-value or
by-name for this replacement to make sense. However, using ordinary deﬁnitions we may, for
example, give a name to a partially applied function, as in the following example:
let k be λ (x1 : num) λ (x2 : num) x1
in let kz be k(0) in kz(3) + kz(5).
Without ﬁrst-class functions, we cannot even form the function k, which returns a function as
result when applied to its ﬁrst argument.
Theorem 8.4 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. The proof is by induction on rules (8.5), which deﬁne the dynamics of the language.
Consider rule (8.5d),
ap(lam{τ2}(x.e1); e2) 7−→[e2/x]e1
.
Suppose that ap(lam{τ2}(x.e1); e2) : τ1. By Lemma 8.2 we have e2 : τ2 and x : τ2 ⊢e1 : τ1, so by
Lemma 8.3 [e2/x]e1 : τ1.
The other rules governing application are handled similarly.
Lemma 8.5 (Canonical Forms). If e : arr(τ1; τ2) and e val, then e = λ (x : τ1) e2 for some variable x
and expression e2 such that x : τ1 ⊢e2 : τ2.
Proof. By induction on the typing rules, using the assumption e val.
Theorem 8.6 (Progress). If e : τ, then either e val, or there exists e′ such that e 7−→e′.
Proof. The proof is by induction on rules (8.4). Note that because we consider only closed terms,
there are no hypotheses on typing derivations.
Consider rule (8.4b) (under the by-name interpretation). By induction either e1 val or e1 7−→e′
1.
In the latter case we have ap(e1; e2) 7−→ap(e′
1; e2). In the former case, we have by Lemma 8.5 that
e1 = lam{τ2}(x.e) for some x and e. But then ap(e1; e2) 7−→[e2/x]e.
1Although the term “call-by-value” is accurately descriptive, the origin of the term “call-by-name” remains shrouded
in mystery.

8.3 Evaluation Dynamics and Deﬁnitional Equality
67
8.3
Evaluation Dynamics and Deﬁnitional Equality
An inductive deﬁnition of the evaluation judgment e ⇓v for EF is given by the following rules:
lam{τ}(x.e) ⇓lam{τ}(x.e)
(8.6a)
e1 ⇓lam{τ}(x.e)
[e2/x]e ⇓v
ap(e1; e2) ⇓v
(8.6b)
It is easy to check that if e ⇓v, then v val, and that if e val, then e ⇓e.
Theorem 8.7. e ⇓v iff e 7−→∗v and v val.
Proof. In the forward direction we proceed by rule induction on rules (8.6), following along similar
lines as the proof of Theorem 7.2.
In the reverse direction we proceed by rule induction on rules (5.1). The proof relies on an
analog of Lemma 7.4, which states that evaluation is closed under converse execution, which is
proved by induction on rules (8.5).
Deﬁnitional equality for the call-by-name dynamics of EF is deﬁned by extension of rules (5.10).
Γ ⊢ap(lam{τ}(x.e2); e1) ≡[e1/x]e2 : τ2
(8.7a)
Γ ⊢e1 ≡e′
1 : τ2 →τ
Γ ⊢e2 ≡e′
2 : τ2
Γ ⊢ap(e1; e2) ≡ap(e′
1; e′
2) : τ
(8.7b)
Γ, x : τ1 ⊢e2 ≡e′
2 : τ2
Γ ⊢lam{τ1}(x.e2) ≡lam{τ1}(x.e′
2) : τ1 →τ2
(8.7c)
Deﬁnitional equality for call-by-value requires a bit more machinery. The main idea is to re-
strict rule (8.7a) to require that the argument be a value. In addition values must be expanded
to include variables, because in call-by-value, the argument variable of a function stands for the
value of its argument. The call-by-value deﬁnitional equality judgment takes the form
Γ ⊢e1 ≡e2 : τ,
where Γ consists of paired hypotheses x : τ, x val stating, for each variable x in scope, its type and
that it is a value. We write Γ ⊢e val to show that e is a value under these hypotheses, so that
x : τ, x val ⊢x val.

68
8.4 Dynamic Scope
8.4
Dynamic Scope
The dynamics of function application given by rules (8.5) is deﬁned only for expressions without
free variables. When a function is applied, the argument is substituted for the argument variable,
ensuring that the result remains closed. Moreover, because substitution of closed expressions can
never incur capture, the scopes of variables are not disturbed by the dynamics, ensuring that the
principles of binding and scope described in Chapter 1 are respected. This treatment of variables
is called static scoping, or static binding, to contrast it with an alternative approach that we now
describe.
Another approach, called dynamic scoping, or dynamic binding, is sometimes advocated as an
alternative to static binding. The crucial difference is that with dynamic scoping the principle of
identiﬁcation of abt’s up to renaming of bound variables is denied. Consequently, capture-avoiding
substitution is not available. Instead evaluation is deﬁned for open terms, with the bindings of
free variables provided by an environment mapping variable names to (possibly open) values. The
binding of a variable is determined as late as possible, at the point where the variable is evalu-
ated, rather than where it is bound. If the environment does not provide a binding for a variable,
evaluation is aborted with a run-time error.
For ﬁrst-order functions dynamic and static scoping coincide, but in the higher-order case the
two approaches diverge. For example, there is no difference between static and dynamic scope
when it comes to evaluation of an expression such as (λ (x : num) x + 7)(42). Whether 42 is substi-
tuted for x in the body of the function before evaluation, or the body is evaluated in the presence
of the binding of x to 42, the outcome is the same.
In the higher-order case the equivalence of static and dynamic scope breaks down. For exam-
ple, consider the expression
e ≜(λ (x : num) λ (y : num) x + y)(42).
With static scoping e evaluates to the closed value v ≜λ (y : num) 42 + y, which, if applied, would
add 42 to its argument. It makes no difference how the bound variable x is chosen, the outcome
will always be the same. With dynamic scoping e evaluates to the open value v′ ≜λ (y : num) x + y
in which the variable x occurs free. When this expression is evaluated, the variable x is bound
to 42, but this is irrelevant because the binding is not needed to evaluate the λ-abstraction. The
binding of x is not retrieved until such time as v′ is applied to an argument, at which point the
binding for x in force at that time is retrieved, and not the one in force at the point where e is
evaluated.
Therein lies the difference. For example, consider the expression
e′ ≜(λ ( f : num →num) (λ (x : num) f (0))(7))(e).
When evaluated using dynamic scope, the value of e′ is 7, whereas its value is 42 under static
scope. The discrepancy can be traced to the re-binding of x to 7 before the value of e, namely v′, is
applied to 0, altering the outcome.
Dynamic scope violates the basic principle that variables are given meaning by capture-avoiding
substitution as deﬁned in Chapter 1. Violating this principle has at least two undesirable conse-
quences. One is that the names of bound variables matter, in contrast to static scope which obeys

8.5 Notes
69
the identiﬁcation principle. For example, had the innermost λ-abstraction of e′ bound the vari-
able y, rather than x, then its value would have been 42, rather than 7. Thus one component of a
program may be sensitive to the names of bound variables chosen in another component, a clear
violation of modular decomposition.
Another problem is that dynamic scope is not, in general, type-safe. For example, consider the
expression
e′ ≜(λ ( f : num →num) (λ (x : str) f (”zero”))(7))(e).
Under dynamic scoping this expression gets stuck attempting to evaluate x + y with x bound to
the string ”zero”, and no further progress can be made. For this reason dynamic scope is only ever
advocated for so-called dynamically typed languages, which replace static consistency checks by
dynamic consistency checks to ensure a weak form of progress. Compile-time errors are thereby
transformed into run-time errors.
(For more on dynamic typing, see Chapter 22, and for more on dynamic scope, see Chapter 32.)
8.5
Notes
Nearly all programming languages provide some form of function deﬁnition mechanism of the
kind illustrated here. The main point of the present account is to demonstrate that a more natural,
and more powerful, approach is to separate the generic concept of a deﬁnition from the speciﬁc
concept of a function. Function types codify the general notion in a systematic way that encom-
passes function deﬁnitions as a special case, and moreover, admits passing functions as arguments
and returning them as results without special provision. The essential contribution of Church’s λ-
calculus (Church, 1941) was to take functions as primary, and to show that nothing more is needed
to get a fully expressive programming language.
Exercises
8.1. Formulate an environmental evaluation dynamics (see Exercise 7.5) for ED. Hint: Introduce
a new form of judgment for evaluation of function identiﬁers.
8.2. Consider an environmental dynamics for EF, which includes higher-order functions. What
difﬁculties arise? Can you think of a way to evade these difﬁculties? Hint: One approach is
to “substitute away” all free variables in a λ-abstraction at the point at which it is evaluated.
The second is to “freeze” the values of each of the free variables in a λ-abstraction, and to
“thaw” them when such a function is applied. What problems arise in each case?

70
8.5 Notes

Chapter 9
System T of Higher-Order Recursion
System T, well-known as G¨odel’s T, is the combination of function types with the type of natural
numbers. In contrast to E, which equips the naturals with some arbitrarily chosen arithmetic
operations, the language T provides a general mechanism, called primitive recursion, from which
these primitives may be deﬁned. Primitive recursion captures the essential inductive character of
the natural numbers, and hence may be seen as an intrinsic termination proof for each program in
the language. Consequently, we may only deﬁne total functions in the language, those that always
return a value for each argument. In essence every program in T “comes equipped” with a proof
of its termination. Although this may seem like a shield against inﬁnite loops, it is also a weapon
that can be used to show that some programs cannot be written in T. To do so would demand
a master termination proof for every possible program in the language, something that we shall
prove does not exist.
9.1
Statics
The syntax of T is given by the following grammar:
Typ
τ
::=
nat
nat
naturals
arr(τ1; τ2)
τ1 →τ2
function
Exp
e
::=
x
x
variable
z
z
zero
s(e)
s(e)
successor
rec{e0; x.y.e1}(e)
rec e {z ,→e0 | s(x) with y ,→e1}
recursion
lam{τ}(x.e)
λ (x : τ) e
abstraction
ap(e1; e2)
e1(e2)
application
We write n for the expression s(. . . s(z)), in which the successor is applied n ≥0 times to zero.
The expression rec{e0; x.y.e1}(e) is called the recursor. It represents the e-fold iteration of the

72
9.2 Dynamics
transformation x.y.e1 starting from e0. The bound variable x represents the predecessor and the
bound variable y represents the result of the x-fold iteration. The “with” clause in the concrete
syntax for the recursor binds the variable y to the result of the recursive call, as will become clear
shortly.
Sometimes the iterator, iter{e0; y.e1}(e), is considered as an alternative to the recursor. It has
essentially the same meaning as the recursor, except that only the result of the recursive call is
bound to y in e1, and no binding is made for the predecessor. Clearly the iterator is a special case
of the recursor, because we can always ignore the predecessor binding. Conversely, the recursor is
deﬁnable from the iterator, provided that we have product types (Chapter 10) at our disposal. To
deﬁne the recursor from the iterator, we simultaneously compute the predecessor while iterating
the speciﬁed computation.
The statics of T is given by the following typing rules:
Γ, x : τ ⊢x : τ
(9.1a)
Γ ⊢z : nat
(9.1b)
Γ ⊢e : nat
Γ ⊢s(e) : nat
(9.1c)
Γ ⊢e : nat
Γ ⊢e0 : τ
Γ, x : nat, y : τ ⊢e1 : τ
Γ ⊢rec{e0; x.y.e1}(e) : τ
(9.1d)
Γ, x : τ1 ⊢e : τ2
Γ ⊢lam{τ1}(x.e) : arr(τ1; τ2)
(9.1e)
Γ ⊢e1 : arr(τ2; τ)
Γ ⊢e2 : τ2
Γ ⊢ap(e1; e2) : τ
(9.1f)
As usual, admissibility of the structural rule of substitution is crucially important.
Lemma 9.1. If Γ ⊢e : τ and Γ, x : τ ⊢e′ : τ′, then Γ ⊢[e/x]e′ : τ′.
9.2
Dynamics
The closed values of T are deﬁned by the following rules:
z val
(9.2a)
[e val]
s(e) val
(9.2b)
lam{τ}(x.e) val
(9.2c)

9.2 Dynamics
73
The premise of rule (9.2b) is included for an eager interpretation of successor, and excluded for a
lazy interpretation.
The transition rules for the dynamics of T are as follows:

e 7−→e′
s(e) 7−→s(e′)

(9.3a)
e1 7−→e′
1
ap(e1; e2) 7−→ap(e′
1; e2)
(9.3b)
"
e1 val
e2 7−→e′
2
ap(e1; e2) 7−→ap(e1; e′
2)
#
(9.3c)
[e2 val]
ap(lam{τ}(x.e); e2) 7−→[e2/x]e
(9.3d)
e 7−→e′
rec{e0; x.y.e1}(e) 7−→rec{e0; x.y.e1}(e′)
(9.3e)
rec{e0; x.y.e1}(z) 7−→e0
(9.3f)
s(e) val
rec{e0; x.y.e1}(s(e)) 7−→[e, rec{e0; x.y.e1}(e)/x, y]e1
(9.3g)
The bracketed rules and premises are included for an eager successor and call-by-value applica-
tion, and omitted for a lazy successor and call-by-name application. Rules (9.3f) and (9.3g) specify
the behavior of the recursor on z and s(e). In the former case the recursor reduces to e0, and in
the latter case the variable x is bound to the predecessor e and y is bound to the (unevaluated)
recursion on e. If the value of y is not required in the rest of the computation, the recursive call is
not evaluated.
Lemma 9.2 (Canonical Forms). If e : τ and e val, then
1. If τ = nat, then either e = z or e = s(e′) for some e′.
2. If τ = τ1 →τ2, then e = λ (x : τ1) e2 for some e2.
Theorem 9.3 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val or e 7−→e′ for some e′.

74
9.3 Deﬁnability
9.3
Deﬁnability
A mathematical function f : N →N on the natural numbers is deﬁnable in T iff there exists an
expression e f of type nat →nat such that for every n ∈N,
e f (n) ≡f (n) : nat.
(9.4)
That is, the numeric function f : N →N is deﬁnable iff there is an expression e f of type nat →nat
such that, when applied to the numeral representing the argument n ∈N, the application is
deﬁnitionally equal to the numeral corresponding to f (n) ∈N.
Deﬁnitional equality for T, written Γ ⊢e ≡e′ : τ, is the strongest congruence containing these
axioms:
Γ, x : τ1 ⊢e2 : τ2
Γ ⊢e1 : τ1
Γ ⊢ap(lam{τ1}(x.e2); e1) ≡[e1/x]e2 : τ2
(9.5a)
Γ ⊢e0 : τ
Γ, x : τ ⊢e1 : τ
Γ ⊢rec{e0; x.y.e1}(z) ≡e0 : τ
(9.5b)
Γ ⊢e0 : τ
Γ, x : τ ⊢e1 : τ
Γ ⊢rec{e0; x.y.e1}(s(e)) ≡[e, rec{e0; x.y.e1}(e)/x, y]e1 : τ
(9.5c)
For example, the doubling function, d(n) = 2 × n, is deﬁnable in T by the expression ed :
nat →nat given by
λ (x : nat) rec x {z ,→z | s(u) with v ,→s(s(v))}.
To check that this deﬁnes the doubling function, we proceed by induction on n ∈N. For the basis,
it is easy to check that
ed(0) ≡0 : nat.
For the induction, assume that
ed(n) ≡d(n) : nat.
Then calculate using the rules of deﬁnitional equality:
ed(n + 1) ≡s(s(ed(n)))
≡s(s(2 × n))
= 2 × (n + 1)
= d(n + 1).
As another example, consider the following function, called Ackermann’s function, deﬁned by
the following equations:
A(0, n) = n + 1
A(m + 1, 0) = A(m, 1)
A(m + 1, n + 1) = A(m, A(m + 1, n)).

9.4 Undeﬁnability
75
The Ackermann function grows very quickly. For example, A(4, 2) ≈265,536, which is often cited
as being larger than the number of atoms in the universe! Yet we can show that the Ackermann
function is total by a lexicographic induction on the pair of arguments (m, n). On each recursive
call, either m decreases, or else m remains the same, and n decreases, so inductively the recursive
calls are well-deﬁned, and hence so is A(m, n).
A ﬁrst-order primitive recursive function is a function of type nat →nat that is deﬁned using the
recursor, but without using any higher order functions. Ackermann’s function is deﬁned so that it
is not ﬁrst-order primitive recursive, but is higher-order primitive recursive. The key to showing
that it is deﬁnable in T is to note that A(m + 1, n) iterates n times the function A(m, −), starting
with A(m, 1). As an auxiliary, let us deﬁne the higher-order function
it : (nat →nat) →nat →nat →nat
to be the λ-abstraction
λ ( f : nat →nat) λ (n : nat) rec n {z ,→id | s( ) with g ,→f ◦g},
where id = λ (x : nat) x is the identity, and f ◦g = λ (x : nat) f (g(x)) is the composition of f and
g. It is easy to check that
it( f )(n)(m) ≡f (n)(m) : nat,
where the latter expression is the n-fold composition of f starting with m. We may then deﬁne the
Ackermann function
ea : nat →nat →nat
to be the expression
λ (m : nat) rec m {z ,→s | s( ) with f ,→λ (n : nat) it( f )(n)( f (1))}.
It is instructive to check that the following equivalences are valid:
ea(0)(n) ≡s(n)
(9.6)
ea(m + 1)(0) ≡ea(m)(1)
(9.7)
ea(m + 1)(n + 1) ≡ea(m)(ea(s(m))(n)).
(9.8)
That is, the Ackermann function is deﬁnable in T.
9.4
Undeﬁnability
It is impossible to deﬁne an inﬁnite loop in T.
Theorem 9.4. If e : τ, then there exists v val such that e ≡v : τ.
Proof. See Corollary 46.15.

76
9.4 Undeﬁnability
Consequently, values of function type in T behave like mathematical functions: if e : τ1 →τ2
and e1 : τ1, then e(e1) evaluates to a value of type τ2. Moreover, if e : nat, then there exists a natural
number n such that e ≡n : nat.
Using this, we can show, using a technique called diagonalization, that there are functions on the
natural numbers that are not deﬁnable in T. We make use of a technique, called G¨odel-numbering,
that assigns a unique natural number to each closed expression of T. By assigning a unique num-
ber to each expression, we may manipulate expressions as data values in T so that T is able to
compute with its own programs.1
The essence of G¨odel-numbering is captured by the following simple construction on abstract
syntax trees. (The generalization to abstract binding trees is slightly more difﬁcult, the main com-
plication being to ensure that all α-equivalent expressions are assigned the same G¨odel number.)
Recall that a general ast a has the form o(a1, . . . , ak), where o is an operator of arity k. Enumer-
ate the operators so that every operator has an index i ∈N, and let m be the index of o in this
enumeration. Deﬁne the G¨odel number ⌜a⌝of a to be the number
2m 3n1 5n2 . . . pnk
k ,
where pk is the kth prime number (so that p0 = 2, p1 = 3, and so on), and n1, . . . , nk are the
G¨odel numbers of a1, . . . , ak, respectively. This procedure assigns a natural number to each ast.
Conversely, given a natural number, n, we may apply the prime factorization theorem to “parse”
n as a unique abstract syntax tree. (If the factorization is not of the right form, which can only be
because the arity of the operator does not match the number of factors, then n does not code any
ast.)
Now, using this representation, we may deﬁne a (mathematical) function funiv : N →N →N
such that, for any e : nat →nat, funiv(⌜e⌝)(m) = n iff e(m) ≡n : nat.2 The determinacy of the
dynamics, together with Theorem 9.4, ensure that funiv is a well-deﬁned function. It is called the
universal function for T because it speciﬁes the behavior of any expression e of type nat →nat.
Using the universal function, let us deﬁne an auxiliary mathematical function, called the diagonal
function δ : N →N, by the equation δ(m) = funiv(m)(m). The δ function is chosen so that
δ(⌜e⌝) = n iff e(⌜e⌝) ≡n : nat. (The motivation for its deﬁnition will become clear in a moment.)
The function funiv is not deﬁnable in T. Suppose that it were deﬁnable by the expression euniv,
then the diagonal function δ would be deﬁnable by the expression
eδ = λ (m : nat) euniv(m)(m).
But in that case we would have the equations
eδ(⌜e⌝) ≡euniv(⌜e⌝)(⌜e⌝)
≡e(⌜e⌝).
Now let e∆be the function expression
λ (x : nat) s(eδ(x)),
1The same technique lies at the heart of the proof of G¨odel’s celebrated incompleteness theorem. The undeﬁnability of
certain functions on the natural numbers within T may be seen as a form of incompleteness like that considered by G¨odel.
2The value of funiv(k)(m) may be chosen arbitrarily to be zero when k is not the code of any expression e.

9.5 Notes
77
so that we may deduce
e∆(⌜e∆⌝) ≡s(eδ(⌜e∆⌝))
≡s(e∆(⌜e∆⌝)).
But the termination theorem implies that there exists n such that e∆(⌜e∆⌝) ≡n, and hence we have
n ≡s(n), which is impossible.
We say that a language L is universal if it is possible to write an interpreter for L in L itself.
It is intuitively clear that funiv is computable in the sense that we can deﬁne it in some sufﬁciently
powerful programming language. But the preceding argument shows that T is not up to the
task; it is not a universal programming language. Examination of the foregoing proof reveals an
inescapable tradeoff: by insisting that all expressions terminate, we necessarily lose universality—
there are computable functions that are not deﬁnable in the language.
9.5
Notes
System T was introduced by G¨odel in his study of the consistency of arithmetic (G¨odel, 1980). He
showed how to “compile” proofs in arithmetic into well-typed terms of system T, and to reduce
the consistency problem for arithmetic to the termination of programs in T. It was perhaps the ﬁrst
programming language whose design was directly inﬂuenced by the veriﬁcation (of termination)
of its programs.
Exercises
9.1. Prove Lemma 9.2.
9.2. Prove Theorem 9.3.
9.3. Attempt to prove that if e : nat is closed, then there exists n such that e 7−→∗n under the
eager dynamics. Where does the proof break down?
9.4. Attempt to prove termination for all well-typed closed terms: if e : τ, then there exists e′ val
such that e 7−→∗e′. You are free to appeal to Lemma 9.2 and Theorem 9.3 as necessary.
Where does the attempt break down? Can you think of a stronger inductive hypothesis that
might evade the difﬁculty?
9.5. Deﬁne a closed term e of type τ in T to be hereditarily terminating at type τ by induction on
the structure of τ as follows:
(a) If τ = nat, then e is hereditarily terminating at type τ iff e is terminating (that is, iff
e 7−→∗n for some n.)
(b) If τ = τ1 →τ2, then e is hereditarily terminating iff when e1 is hereditarily terminating
at type τ1, then e(e1) is hereditarily terminating at type τ2.

78
9.5 Notes
Attempt to prove hereditary termination for well-typed terms: if e : τ, then e is hereditarily
terminating at type τ. The stronger inductive hypothesis bypasses the difﬁculty that arose
in Exercise 9.4, but introduces another obstacle. What is the complication? Can you think of
an even stronger inductive hypothesis that would sufﬁce for the proof?
9.6. Show that if e is hereditarily terminating at type τ, e′ : τ, and e′ 7−→e, then e is also hered-
itarily terminating at type τ. (The need for this result will become clear in the solution to
Exercise 9.5.)
9.7. Deﬁne an open, well-typed term
x1 : τ1, . . . , xn : τn ⊢e : τ
to be open hereditarily terminating iff every substitution instance
[e1, . . . , en/x1, . . . , xn]e
is closed hereditarily terminating at type τ when each ei is closed hereditarily terminating at
type τi for each 1 ≤i ≤n. Derive Exercise 9.3 from this result.

Part IV
Finite Data Types


Chapter 10
Product Types
The binary product of two types consists of ordered pairs of values, one from each type in the or-
der speciﬁed. The associated elimination forms are projections, which select the ﬁrst and second
component of a pair. The nullary product, or unit, type consists solely of the unique “null tuple”
of no values, and has no associated elimination form. The product type admits both a lazy and an
eager dynamics. According to the lazy dynamics, a pair is a value without regard to whether its
components are values; they are not evaluated until (if ever) they are accessed and used in another
computation. According to the eager dynamics, a pair is a value only if its components are values;
they are evaluated when the pair is created.
More generally, we may consider the ﬁnite product, ⟨τi⟩i∈I, indexed by a ﬁnite set of indices I.
The elements of the ﬁnite product type are I-indexed tuples whose ith component is an element
of the type τi, for each i ∈I. The components are accessed by I-indexed projection operations,
generalizing the binary case. Special cases of the ﬁnite product include n-tuples, indexed by sets
of the form I = { 0, . . . , n −1 }, and labeled tuples, or records, indexed by ﬁnite sets of symbols.
Similarly to binary products, ﬁnite products admit both an eager and a lazy interpretation.
10.1
Nullary and Binary Products
The abstract syntax of products is given by the following grammar:
Typ
τ
::=
unit
unit
nullary product
prod(τ1; τ2)
τ1 × τ2
binary product
Exp
e
::=
triv
⟨⟩
null tuple
pair(e1; e2)
⟨e1, e2⟩
ordered pair
pr[l](e)
e · l
left projection
pr[r](e)
e · r
right projection
There is no elimination form for the unit type, there being nothing to extract from the null tuple.

82
10.1 Nullary and Binary Products
The statics of product types is given by the following rules.
Γ ⊢⟨⟩: unit
(10.1a)
Γ ⊢e1 : τ1
Γ ⊢e2 : τ2
Γ ⊢⟨e1, e2⟩: τ1 × τ2
(10.1b)
Γ ⊢e : τ1 × τ2
Γ ⊢e · l : τ1
(10.1c)
Γ ⊢e : τ1 × τ2
Γ ⊢e · r : τ2
(10.1d)
The dynamics of product types is deﬁned by the following rules:
⟨⟩val
(10.2a)
[e1 val]
[e2 val]
⟨e1, e2⟩val
(10.2b)
"
e1 7−→e′
1
⟨e1, e2⟩7−→⟨e′
1, e2⟩
#
(10.2c)
"
e1 val
e2 7−→e′
2
⟨e1, e2⟩7−→⟨e1, e′
2⟩
#
(10.2d)
e 7−→e′
e · l 7−→e′ · l
(10.2e)
e 7−→e′
e · r 7−→e′ · r
(10.2f)
[e1 val]
[e2 val]
⟨e1, e2⟩· l 7−→e1
(10.2g)
[e1 val]
[e2 val]
⟨e1, e2⟩· r 7−→e2
(10.2h)
The bracketed rules and premises are omitted for a lazy dynamics, and included for an eager
dynamics of pairing.
The safety theorem applies to both the eager and the lazy dynamics, with the proof proceeding
along similar lines in each case.
Theorem 10.1 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ then either e val or there exists e′ such that e 7−→e′.
Proof. Preservation is proved by induction on transition deﬁned by rules (10.2). Progress is proved
by induction on typing deﬁned by rules (10.1).

10.2 Finite Products
83
10.2
Finite Products
The syntax of ﬁnite product types is given by the following grammar:
Typ
τ
::=
prod({i ,→τi}i∈I)
⟨τi⟩i∈I
product
Exp
e
::=
tpl({i ,→ei}i∈I)
⟨ei⟩i∈I
tuple
pr[i](e)
e · i
projection
The variable I stands for a ﬁnite index set over which products are formed. The type prod({i ,→τi}i∈I),
or ∏i∈I τi for short, is the type of I-tuples of expressions ei of type τi, one for each i ∈I. An I-tuple
has the form tpl({i ,→ei}i∈I), or ⟨ei⟩i∈I for short, and for each i ∈I the ith projection from an
I-tuple e is written pr[i](e), or e · i for short.
When I = { i1, . . . , in }, the I-tuple type may be written in the form
⟨i1 ,→τ1, . . . , in ,→τn⟩
where we make explicit the association of a type to each index i ∈I. Similarly, we may write
⟨i1 ,→e1, . . . , in ,→en⟩
for the I-tuple whose ith component is ei.
Finite products generalize empty and binary products by choosing I to be empty or the two-
element set { l, r }, respectively. In practice I is often chosen to be a ﬁnite set of symbols that serve
as labels for the components of the tuple to enhance readability.
The statics of ﬁnite products is given by the following rules:
Γ ⊢e1 : τ1
. . .
Γ ⊢en : τn
Γ ⊢⟨i1 ,→e1, . . . , in ,→en⟩: ⟨i1 ,→τ1, . . . , in ,→τn⟩
(10.3a)
Γ ⊢e : ⟨i1 ,→τ1, . . . , in ,→τn⟩
(1 ≤k ≤n)
Γ ⊢e · ik : τk
(10.3b)
In rule (10.3b) the index ik ∈I is a particular element of the index set I, whereas in rule (10.3a), the
indices i1, . . . , in range over the entire index set I.
The dynamics of ﬁnite products is given by the following rules:
[e1 val
. . .
en val]
⟨i1 ,→e1, . . . , in ,→en⟩val
(10.4a)


( e1 val
. . .
ej−1 val
e′
1 = e1
. . .
e′
j−1 = ej−1
ej 7−→e′
j
e′
j+1 = ej+1
. . .
e′
n = en
)
⟨i1 ,→e1, . . . , in ,→en⟩7−→⟨i1 ,→e′
1, . . . , in ,→e′
n⟩


(10.4b)
e 7−→e′
e · i 7−→e′ · i
(10.4c)

84
10.3 Primitive Mutual Recursion
[⟨i1 ,→e1, . . . , in ,→en⟩val]
⟨i1 ,→e1, . . . , in ,→en⟩· ik 7−→ek
(10.4d)
As formulated, rule (10.4b) speciﬁes that the components of a tuple are evaluated in some sequen-
tial order, without specifying the order in which the components are considered. It is not hard, but
a bit technically complicated, to impose an evaluation order by imposing a total ordering on the
index set and evaluating components according to this ordering.
Theorem 10.2 (Safety). If e : τ, then either e val or there exists e′ such that e′ : τ and e 7−→e′.
Proof. The safety theorem is decomposed into progress and preservation lemmas, which are proved
as in Section 10.1.
10.3
Primitive Mutual Recursion
Using products we may simplify the primitive recursion construct of T so that only the recursive
result on the predecessor, and not the predecessor itself, is passed to the successor branch. Writing
this as iter{e0; x.e1}(e), we may deﬁne rec{e0; x.y.e1}(e) to be e′ · r, where e’ is the expression
iter{⟨z, e0⟩; x′.⟨s(x′ · l), [x′ · r/x]e1⟩}(e).
The idea is to compute inductively both the number n and the result of the recursive call on n,
from which we can compute both n + 1 and the result of another recursion using e1. The base case
is computed directly as the pair of zero and e0. It is easy to check that the statics and dynamics of
the recursor are preserved by this deﬁnition.
We may also use product types to implement mutual primitive recursion, in which we deﬁne two
functions simultaneously by primitive recursion. For example, consider the following recursion
equations deﬁning two mathematical functions on the natural numbers:
e(0) = 1
o(0) = 0
e(n + 1) = o(n)
o(n + 1) = e(n)
Intuitively, e(n) is non-zero if and only if n is even, and o(n) is non-zero if and only if n is odd.
To deﬁne these functions in T enriched with products, we ﬁrst deﬁne an auxiliary function eeo
of type
nat →(nat × nat)
that computes both results simultaneously by swapping back and forth on recursive calls:
λ (n : nat × nat) iter n {z ,→⟨1, 0⟩| s(b) ,→⟨b · r, b · l⟩}.
We may then deﬁne eev and eod as follows:
eev ≜λ (n : nat) eeo(n) · l
eod ≜λ (n : nat) eeo(n) · r.

10.4 Notes
85
10.4
Notes
Product types are the most basic form of structured data. All languages have some form of product
type, but often in a form that is combined with other, separable, concepts. Common manifestations
of products include: (1) functions with “multiple arguments” or “multiple results”; (2) “objects”
represented as tuples of mutually recursive functions; (3) “structures,” which are tuples with mu-
table components. There are many papers on ﬁnite product types, which include record types
as a special case. Pierce (2002) provides a thorough account of record types, and their subtyping
properties (for which, see Chapter 24). Allen et al. (2006) analyzes many of the key ideas in the
framework of dependent type theory.
Exercises
10.1. A database schema may be thought of as a ﬁnite product type ∏i∈I τ, in which the columns, or
attributes. are labeled by the indices I whose values are restricted to atomic types, such as nat
and str. A value of a schema type is called a tuple, or instance, of that schema. A database
may be thought of as a ﬁnite sequence of such tuples, called the rows of the database. Give a
representation of a database using function, product, and natural numbers types, and deﬁne
the project operation that sends a database with columns I to a database with columns I′ ⊆I
by restricting each row to the speciﬁed columns.
10.2. Rather than choose between a lazy and an eager dynamics for products, we can instead
distinguish two forms of product type, called the positive and the negative. The statics of the
negative product is given by rules (10.1), and the dynamics is lazy. The statics of the positive
product, written τ1 ⊗τ2, is given by the following rules:
Γ ⊢e1 : τ1
Γ ⊢e2 : τ2
Γ ⊢fuse(e1; e2) : τ1 ⊗τ2
(10.5a)
Γ ⊢e0 : τ1 ⊗τ2
Γ x1 : τ1 x2 : τ2 ⊢e : τ
Γ ⊢split(e0; x1, x2.e) : τ
(10.5b)
The dynamics of fuse, the introduction form for the positive pair, is eager, essentially be-
cause the elimination form, split, extracts both components simultaneously.
Show that the negative product is deﬁnable in terms of the positive product using the unit
and function types to express the lazy semantics of negative pairing. Show that the positive
product is deﬁnable in terms of the negative product, provided that we have at our disposal a
let expression with a by-value dynamics so that we may enforce eager evaluation of positive
pairs.
10.3. Specializing Exercise 10.2 to nullary products, we obtain a positive and a negative unit type.
The negative unit type is given by rules (10.1), with no elimination forms and one introduc-
tion form. Give the statics and dynamics for a positive unit type, and show that the positive
and negative unit types are inter-deﬁnable without any further assumptions.

86
10.4 Notes

Chapter 11
Sum Types
Most data structures involve alternatives such as the distinction between a leaf and an interior
node in a tree, or a choice in the outermost form of a piece of abstract syntax. Importantly, the
choice determines the structure of the value. For example, nodes have children, but leaves do not,
and so forth. These concepts are expressed by sum types, speciﬁcally the binary sum, which offers a
choice of two things, and the nullary sum, which offers a choice of no things. Finite sums generalize
nullary and binary sums to allow an arbitrary number of cases indexed by a ﬁnite index set. As
with products, sums come in both eager and lazy variants, differing in how values of sum type are
deﬁned.
11.1
Nullary and Binary Sums
The abstract syntax of sums is given by the following grammar:
Typ
τ
::=
void
void
nullary sum
sum(τ1; τ2)
τ1 + τ2
binary sum
Exp
e
::=
abort{τ}(e)
abort(e)
abort
in[l]{τ1; τ2}(e)
l · e
left injection
in[r]{τ1; τ2}(e)
r · e
right injection
case(e; x1.e1; x2.e2)
case e {l · x1 ,→e1 | r · x2 ,→e2}
case analysis
The nullary sum represents a choice of zero alternatives, and hence admits no introduction form.
The elimination form, abort(e), aborts the computation in the event that e evaluates to a value,
which it cannot do. The elements of the binary sum type are labeled to show whether they are
drawn from the left or the right summand, either l · e or r · e. A value of the sum type is eliminated
by case analysis.
The statics of sum types is given by the following rules.
Γ ⊢e : void
Γ ⊢abort(e) : τ
(11.1a)

88
11.1 Nullary and Binary Sums
Γ ⊢e : τ1
Γ ⊢l · e : τ1 + τ2
(11.1b)
Γ ⊢e : τ2
Γ ⊢r · e : τ1 + τ2
(11.1c)
Γ ⊢e : τ1 + τ2
Γ, x1 : τ1 ⊢e1 : τ
Γ, x2 : τ2 ⊢e2 : τ
Γ ⊢case e {l · x1 ,→e1 | r · x2 ,→e2} : τ
(11.1d)
For the sake of readability, in rules (11.1b) and (11.1c) we have written l · e and r · e in place
of the abstract syntax in[l]{τ1; τ2}(e) and in[r]{τ1; τ2}(e), which includes the types τ1 and τ2
explicitly. In rule (11.1d) both branches of the case analysis must have the same type. Because
a type expresses a static “prediction” on the form of the value of an expression, and because an
expression of sum type could evaluate to either form at run-time, we must insist that both branches
yield the same type.
The dynamics of sums is given by the following rules:
e 7−→e′
abort(e) 7−→abort(e′)
(11.2a)
[e val]
l · e val
(11.2b)
[e val]
r · e val
(11.2c)

e 7−→e′
l · e 7−→l · e′

(11.2d)

e 7−→e′
r · e 7−→r · e′

(11.2e)
e 7−→e′
case e {l · x1 ,→e1 | r · x2 ,→e2} 7−→case e′ {l · x1 ,→e1 | r · x2 ,→e2}
(11.2f)
[e val]
case l · e {l · x1 ,→e1 | r · x2 ,→e2} 7−→[e/x1]e1
(11.2g)
[e val]
case r · e {l · x1 ,→e1 | r · x2 ,→e2} 7−→[e/x2]e2
(11.2h)
The bracketed premises and rules are included for an eager dynamics, and excluded for a lazy
dynamics.
The coherence of the statics and dynamics is stated and proved as usual.
Theorem 11.1 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val or e 7−→e′ for some e′.
Proof. The proof proceeds by induction on rules (11.2) for preservation, and by induction on
rules (11.1) for progress.

11.2 Finite Sums
89
11.2
Finite Sums
Just as we may generalize nullary and binary products to ﬁnite products, so may we also gener-
alize nullary and binary sums to ﬁnite sums. The syntax for ﬁnite sums is given by the following
grammar:
Typ
τ
::=
sum({i ,→τi}i∈I)
[τi]i∈I
sum
Exp
e
::=
in[i]{⃗τ}(e)
i · e
injection
case(e; {i ,→xi.ei}i∈I)
case e {i · xi ,→ei}i∈I
case analysis
The variable I stands for a ﬁnite index set over which sums are formed. The notation ⃗τ stands for
a ﬁnite function {i ,→τi}i∈I for some index set I. The type sum({i ,→τi}i∈I), or ∑i∈I τi for short,
is the type of I-classiﬁed values of the form in[i]{I}(ei), or i · ei for short, where i ∈I and ei is
an expression of type τi. An I-classiﬁed value is analyzed by an I-way case analysis of the form
case(e; {i ,→xi.ei}i∈I).
When I = { i1, . . . , in }, the type of I-classiﬁed values may be written
[i1 ,→τ1, . . . , in ,→τn]
specifying the type associated with each class li ∈I. Correspondingly, the I-way case analysis has
the form
case e {i1 · x1 ,→e1 | . . . | in · xn ,→en}.
Finite sums generalize empty and binary sums by choosing I to be empty or the two-element set
{ l, r }, respectively. In practice I is often chosen to be a ﬁnite set of symbols that serve as names
for the classes so as to enhance readability.
The statics of ﬁnite sums is deﬁned by the following rules:
Γ ⊢e : τk
(1 ≤k ≤n)
Γ ⊢ik · e : [i1 ,→τ1, . . . , in ,→τn]
(11.3a)
Γ ⊢e : [i1 ,→τ1, . . . , in ,→τn]
Γ, x1 : τ1 ⊢e1 : τ
. . .
Γ, xn : τn ⊢en : τ
Γ ⊢case e {i1 · x1 ,→e1 | . . . | in · xn ,→en} : τ
(11.3b)
These rules generalize the statics for nullary and binary sums given in Section 11.1.
The dynamics of ﬁnite sums is deﬁned by the following rules:
[e val]
i · e val
(11.4a)

e 7−→e′
i · e 7−→i · e′

(11.4b)
e 7−→e′
case e {i · xi ,→ei}i∈I 7−→case e′ {i · xi ,→ei}i∈I
(11.4c)
i · e val
case i · e {i · xi ,→ei}i∈I 7−→[e/xi]ei
(11.4d)
These again generalize the dynamics of binary sums given in Section 11.1.

90
11.3 Applications of Sum Types
Theorem 11.2 (Safety). If e : τ, then either e val or there exists e′ : τ such that e 7−→e′.
Proof. The proof is like that for the binary case, as described in Section 11.1.
11.3
Applications of Sum Types
Sum types have many uses, several of which we outline here. More interesting examples arise
once we also have induction and recursive types, which are introduced in Parts VI and Part VIII.
11.3.1
Void and Unit
It is instructive to compare the types unit and void, which are often confused with one another.
The type unit has exactly one element, ⟨⟩, whereas the type void has no elements at all. Con-
sequently, if e : unit, then if e evaluates to a value, that value is ⟨⟩— in other words, e has no
interesting value. On the other hand, if e : void, then e must not yield a value; if it were to have a
value, it would have to be a value of type void, of which there are none. Thus what is called the
void type in many languages is really the type unit because it indicates that an expression has no
interesting value, not that it has no value at all!
11.3.2
Booleans
Perhaps the simplest example of a sum type is the familiar type of Booleans, whose syntax is given
by the following grammar:
Typ
τ
::=
bool
bool
booleans
Exp
e
::=
true
true
truth
false
false
falsity
if(e; e1; e2)
if e then e1 else e2
conditional
The expression if(e; e1; e2) branches on the value of e : bool.
The statics of Booleans is given by the following typing rules:
Γ ⊢true : bool
(11.5a)
Γ ⊢false : bool
(11.5b)
Γ ⊢e : bool
Γ ⊢e1 : τ
Γ ⊢e2 : τ
Γ ⊢if e then e1 else e2 : τ
(11.5c)
The dynamics is given by the following value and transition rules:
true val
(11.6a)

11.3 Applications of Sum Types
91
false val
(11.6b)
if true then e1 else e2 7−→e1
(11.6c)
if false then e1 else e2 7−→e2
(11.6d)
e 7−→e′
if e then e1 else e2 7−→if e′ then e1 else e2
(11.6e)
The type bool is deﬁnable in terms of binary sums and nullary products:
bool = unit + unit
(11.7a)
true = l · ⟨⟩
(11.7b)
false = r · ⟨⟩
(11.7c)
if e then e1 else e2 = case e {l · x1 ,→e1 | r · x2 ,→e2}
(11.7d)
In the last equation above the variables x1 and x2 are chosen arbitrarily such that x1 /∈e1 and
x2 /∈e2. It is a simple matter to check that the readily-deﬁned statics and dynamics of the type
bool are engendered by these deﬁnitions.
11.3.3
Enumerations
More generally, sum types can be used to deﬁne ﬁnite enumeration types, those whose values are
one of an explicitly given ﬁnite set, and whose elimination form is a case analysis on the elements
of that set. For example, the type suit, whose elements are ♣, ♦, ♥, and ♠, has as elimination
form the case analysis
case e {♣,→e0 | ♦,→e1 | ♥,→e2 | ♠,→e3},
which distinguishes among the four suits. Such ﬁnite enumerations are easily representable as
sums. For example, we may deﬁne suit = [unit] ∈I, where I = { ♣, ♦, ♥, ♠} and the type family
is constant over this set. The case analysis form for a labeled sum is almost literally the desired
case analysis for the given enumeration, the only difference being the binding for the uninteresting
value associated with each summand, which we may ignore.
Other examples of enumeration types abound. For example, most languages have a type char
of characters, which is a large enumeration type containing all possible Unicode (or other such
standard classiﬁcation) characters. Each character is assigned a code (such as UTF-8) used for in-
terchange among programs. The type char is equipped with operations such as chcode(n) that
yield the char associated to the code n, and codech(c) that yield the code of character c. Using the
linear ordering on codes we may deﬁne a total ordering of characters, called the collating sequence
determined by that code.

92
11.3 Applications of Sum Types
11.3.4
Options
Another use of sums is to deﬁne the option types, which have the following syntax:
Typ
τ
::=
opt(τ)
τ opt
option
Exp
e
::=
null
null
nothing
just(e)
just(e)
something
ifnull{τ}{e1; x.e2}(e)
which e {null ,→e1 | just(x) ,→e2}
null test
The type opt(τ) represents the type of “optional” values of type τ. The introduction forms are
null, corresponding to “no value”, and just(e), corresponding to a speciﬁed value of type τ. The
elimination form discriminates between the two possibilities.
The option type is deﬁnable from sums and nullary products according to the following equa-
tions:1
τ opt = unit + τ
(11.8a)
null = l · ⟨⟩
(11.8b)
just(e) = r · e
(11.8c)
which e {null ,→e1 | just(x2) ,→e2} = case e {l · ,→e1 | r · x2 ,→e2}
(11.8d)
We leave it to the reader to check the statics and dynamics implied by these deﬁnitions.
The option type is the key to understanding a common misconception, the null pointer fallacy.
This fallacy arises from two related errors. The ﬁrst error is to deem values of certain types to
be mysterious entities called pointers. This terminology arises from suppositions about how these
values might be represented at run-time, rather than on their semantic role in the language. The
second error compounds the ﬁrst. A particular value of a pointer type is distinguished as the null
pointer, which, unlike the other elements of that type, does not stand for a value of that type at all,
but rather rejects all attempts to use it.
To help avoid such failures, such languages usually include a function, say null : τ →bool,
that yields true if its argument is null, and false otherwise. Such a test allows the programmer to
take steps to avoid using null as a value of the type it purports to inhabit. Consequently, programs
are riddled with conditionals of the form
if null(e) then ...error ... else ...proceed ....
(11.9)
Despite this, “null pointer” exceptions at run-time are rampant, in part because it is quite easy
to overlook the need for such a test, and in part because detection of a null pointer leaves little
recourse other than abortion of the program.
The underlying problem is the failure to distinguish the type τ from the type τ opt. Rather than
think of the elements of type τ as pointers, and thereby have to worry about the null pointer, we
instead distinguish between a genuine value of type τ and an optional value of type τ. An optional
1We often write an underscore in place of a bound variable that is not used within its scope.

11.4 Notes
93
value of type τ may or may not be present, but, if it is, the underlying value is truly a value of type
τ (and cannot be null). The elimination form for the option type,
which e {null ,→eerror | just(x) ,→eok},
(11.10)
propagates the information that e is present into the non-null branch by binding a genuine value
of type τ to the variable x. The case analysis effects a change of type from “optional value of type
τ” to “genuine value of type τ”, so that within the non-null branch no further null checks, explicit
or implicit, are necessary. Note that such a change of type is not achieved by the simple Boolean-
valued test exempliﬁed by expression (11.9); the advantage of option types is precisely that they
do so.
11.4
Notes
Heterogeneous data structures are ubiquitous. Sums codify heterogeneity, yet few languages sup-
port them in the form given here. The best approximation in commercial languages is the concept
of a class in object-oriented programming. A class is an injection into a sum type, and dispatch is
case analysis on the class of the data object. (See Chapter 26 for more on this correspondence.) The
absence of sums is the origin of C.A.R. Hoare’s self-described “billion dollar mistake,” the null
pointer (Hoare, 2009). Bad language designs put the burden of managing “null” values entirely at
run-time, instead of making the possibility or the impossibility of “null” apparent at compile time.
Exercises
11.1. Complete the deﬁnition of a ﬁnite enumeration type sketched in Section 11.3.3. Derive enu-
meration types from ﬁnite sum types.
11.2. The essence of Hoare’s mistake is the misidentiﬁcation of the type τ opt with the type bool ×
τ. Values of the latter type are pairs consisting of a boolean “ﬂag” and a value of type τ. The
idea is that the ﬂag indicates whether the associated value is “present”. When the ﬂag is
true, the second component is present, and, when the ﬂag is false, the second component
is absent.
Analyze Hoare’s mistake by attempting to deﬁne τ opt to be the type bool × τ by ﬁlling in
the following chart:
null ≜?
just(e) ≜?
which e {null ,→e1 | just(x) ,→e2} ≜?
Argue that even if we adopt Hoare’s convention of admitting a “null” value of every type,
the chart cannot be properly ﬁlled.

94
11.4 Notes
11.3. Databases have a version of the “null pointer” problem that arises when not every tuple
provides a value for every attribute (such as a person’s middle name). More generally, many
commercial databases are limited to a single atomic type for each attribute, presenting prob-
lems when the value of that attribute may have several types (for example, one may have
different sorts of postal codes depending on the country). Consider how to address these
problems using the methods discussed in Exercise 10.1. Suggest how to handle null val-
ues and heterogeneous values that avoids some of the complications that arise in traditional
formulations of databases.
11.4. A combinational circuit is an open expression of type
x1 : bool, . . . , xn : bool ⊢e : bool,
which computes a boolean value from n boolean inputs. Deﬁne a NOR and a NAND gate as
boolean circuits with two inputs and one output. There is no reason to restrict to a single
output. For example, deﬁne an HALF-ADDER that takes two boolean inputs, but produces
two boolean outputs, the sum and the carry outputs of the HALF-ADDER. Then deﬁne a
FULL-ADDER that takes three inputs, the addends and an incoming carry, and produces two
outputs, the sum and the outgoing carry. Deﬁne the type NYBBLE to be the product bool ×
bool × bool × bool. Deﬁne the combinational circuit NYBBLE-ADDER that takes two nybbles
as input and produces a nybble and a carry-out bit as output.
11.5. A signal is a time-varying sequence of booleans, representing the status of the signal at each
time instant. An RS latch is a fundamental digital circuit with two input signals and two
output signals. Deﬁne the type signal of signals to be the function type nat →bool of
inﬁnite sequences of booleans. Deﬁne an RS latch as a function of type
(signal × signal) →(signal × signal).

Part V
Types and Propositions


Chapter 12
Constructive Logic
Constructive logic codiﬁes the principles of mathematical reasoning as it is actually practiced. In
mathematics a proposition is judged true exactly when it has a proof, and is judged false exactly
when it has a refutation. Because there are, and always will be, unsolved problems, we cannot
expect in general that a proposition is either true or false, for in most cases we have neither a proof
nor a refutation of it. Constructive logic can be described as logic as if people matter, as distinct from
classical logic, which can be described as the logic of the mind of god.
From a constructive viewpoint a proposition is true when it has a proof. What is a proof is a
social construct, an agreement among people as to what is a valid argument. The rules of logic
codify a set of principles of reasoning that may be used in a valid proof. The valid forms of
proof are determined by the outermost structure of the proposition whose truth is asserted. For
example, a proof of a conjunction consists of a proof of each conjunct, and a proof of an implication
transforms a proof of its antecedent to a proof of its consequent. When spelled out in full, the forms
of proof are seen to correspond exactly to the forms of expression of a programming language. To
each proposition is associated the type of its proofs; a proof is then an expression of the associated
type. This association between programs and proofs induces a dynamics on proofs. In this way
proofs in constructive logic have computational content, which is to say that they are interpreted
as executable programs of the associated type. Conversely, programs have mathematical content as
proofs of the proposition associated to their type.
The uniﬁcation of logic and programming is called the propositions as types principle. It is a
central organizing principle of the theory of programming languages. Propositions are identiﬁed
with types, and proofs are identiﬁed with programs. A programming technique corresponds to
a method of proof; a proof technique corresponds to a method of programming. Viewing types
as behavioral speciﬁcations of programs, propositions are problem statements whose proofs are
solutions that implement the speciﬁcation.

98
12.1 Constructive Semantics
12.1
Constructive Semantics
Constructive logic is concerned with two judgments, namely φ prop, stating that φ expresses a
proposition, and φ true, stating that φ is a true proposition. What distinguishes constructive from
non-constructive logic is that a proposition is not conceived of as merely a truth value, but instead
as a problem statement whose solution, if it has one, is given by a proof. A proposition is true exactly
when it has a proof, in keeping with ordinary mathematical practice. In practice there is no other
criterion of truth than the existence of a proof.
Identifying truth with proof has important, and possibly surprising, consequences. The most
important consequence is that we cannot say, in general, that a proposition is either true or false.
If for a proposition to be true means to have a proof of it, what does it mean for a proposition
to be false? It means that we have a refutation of it, showing that it cannot be proved. That is,
a proposition is false if we can show that the assumption that it is true (has a proof) contradicts
known facts. In this sense constructive logic is a logic of positive, or afﬁrmative, information — we
must have explicit evidence in the form of a proof to afﬁrm the truth or falsity of a proposition.
In light of this it is clear that not every proposition is either true or false. For if φ expresses
an unsolved problem, such as the famous P
?= NP problem, then we have neither a proof nor a
refutation of it (the mere absence of a proof not being a refutation). Such a problem is undecided,
precisely because it has not been solved. Because there will always be unsolved problems (there
being inﬁnitely many propositions, but only ﬁnitely many proofs at a given point in time), we
cannot say that every proposition is decidable, that is, either true or false.
Of course, some propositions are decidable, and hence are either true or false. For example, if
φ expresses an inequality between natural numbers, then φ is decidable, because we can always
work out, for given natural numbers m and n, whether m ≤n or m ̸≤n — we can either prove or
refute the given inequality. This argument does not extend to the real numbers. To get an idea of
why not, consider the representation of a real number by its decimal expansion. At any ﬁnite time
we will have explored only a ﬁnite initial segment of the expansion, which is not enough to decide
if it is, say, less than 1. For if we have calculated the expansion to be 0.99 . . . 9, we cannot decide at
any time, short of inﬁnity, whether or not the number is 1.
The constructive attitude is simply to accept the situation as inevitable, and make our peace
with that. When faced with a problem we have no choice but to roll up our sleeves and try to prove
it or refute it. There is no guarantee of success! Life is hard, but we muddle through somehow.
12.2
Constructive Logic
The judgments φ prop and φ true of constructive logic are rarely of interest by themselves, but
rather in the context of a hypothetical judgment of the form
φ1 true, . . . , φn true ⊢φ true.
This judgment says that the proposition φ is true (has a proof), under the assumptions that each of
φ1, . . . , φn are also true (have proofs). Of course, when n = 0 this is just the same as the judgment
φ true.

12.2 Constructive Logic
99
The structural properties of the hypothetical judgment, when specialized to constructive logic,
deﬁne what we mean by reasoning under hypotheses:
Γ, φ true ⊢φ true
(12.1a)
Γ ⊢φ1 true
Γ, φ1 true ⊢φ2 true
Γ ⊢φ2 true
(12.1b)
Γ ⊢φ2 true
Γ, φ1 true ⊢φ2 true
(12.1c)
Γ, φ1 true, φ1 true ⊢φ2 true
Γ, φ1 true ⊢φ2 true
(12.1d)
Γ1, φ2 true, φ1 true, Γ2 ⊢φ true
Γ1, φ1 true, φ2 true, Γ2 ⊢φ true
(12.1e)
The last two rules are implicit in that we regard Γ as a set of hypotheses, so that two “copies” are
as good as one, and the order of hypotheses does not matter.
12.2.1
Provability
The syntax of propositional logic is given by the following grammar:
Prop
φ
::=
⊤
⊤
truth
⊥
⊥
falsity
∧(φ1; φ2)
φ1 ∧φ2
conjunction
∨(φ1; φ2)
φ1 ∨φ2
disjunction
⊃(φ1; φ2)
φ1 ⊃φ2
implication
The connectives of propositional logic are given meaning by rules that deﬁne (a) what constitutes
a “direct” proof of a proposition formed from that connective, and (b) how to exploit the existence
of such a proof in an “indirect” proof of another proposition. These are called the introduction and
elimination rules for the connective. The principle of conservation of proof states that these rules are
inverse to one another — the elimination rule cannot extract more information (in the form of a
proof) than was put into it by the introduction rule, and the introduction rules can reconstruct a
proof from the information extracted by the elimination rules.
Truth
Our ﬁrst proposition is trivially true. No information goes into proving it, and so no
information can be obtained from it.
Γ ⊢⊤true
(12.2a)
(no elimination rule)
(12.2b)

100
12.2 Constructive Logic
Conjunction
Conjunction expresses the truth of both of its conjuncts.
Γ ⊢φ1 true
Γ ⊢φ2 true
Γ ⊢φ1 ∧φ2 true
(12.3a)
Γ ⊢φ1 ∧φ2 true
Γ ⊢φ1 true
(12.3b)
Γ ⊢φ1 ∧φ2 true
Γ ⊢φ2 true
(12.3c)
Implication
Implication expresses the truth of a proposition under an assumption.
Γ, φ1 true ⊢φ2 true
Γ ⊢φ1 ⊃φ2 true
(12.4a)
Γ ⊢φ1 ⊃φ2 true
Γ ⊢φ1 true
Γ ⊢φ2 true
(12.4b)
Falsehood
Falsehood expresses the trivially false (refutable) proposition.
(no introduction rule)
(12.5a)
Γ ⊢⊥true
Γ ⊢φ true
(12.5b)
Disjunction
Disjunction expresses the truth of either (or both) of two propositions.
Γ ⊢φ1 true
Γ ⊢φ1 ∨φ2 true
(12.6a)
Γ ⊢φ2 true
Γ ⊢φ1 ∨φ2 true
(12.6b)
Γ ⊢φ1 ∨φ2 true
Γ, φ1 true ⊢φ true
Γ, φ2 true ⊢φ true
Γ ⊢φ true
(12.6c)
Negation
The negation, ¬φ, of a proposition φ is deﬁned as the implication φ ⊃⊥. As a result
¬φ true if φ true ⊢⊥true, which is to say that the truth of φ is refutable in that we may derive a
proof of falsehood from any purported proof of φ. Because constructive truth is deﬁned to be the
existence of a proof, the implied semantics of negation is rather strong. In particular, a problem φ
is open exactly when we can neither afﬁrm nor refute it. In contrast the classical conception of truth
assigns a ﬁxed truth value to each proposition so that every proposition is either true or false.

12.2 Constructive Logic
101
12.2.2
Proof Terms
The key to the propositions-as-types principle is to make explicit the forms of proof. The basic
judgment φ true, which states that φ has a proof, is replaced by the judgment p : φ, stating that p
is a proof of φ. (Sometimes p is called a “proof term”, but we will simply call p a “proof.”) The
hypothetical judgment is modiﬁed correspondingly, with variables standing for the presumed, but
unknown, proofs:
x1 : φ1, . . . , xn : φn ⊢p : φ.
We again let Γ range over such hypothesis lists, subject to the restriction that no variable occurs
more than once.
The syntax of proof terms is given by the following grammar:
Prf
p
::=
true-I
⟨⟩
truth intro
and-I(p1; p2)
⟨p1, p2⟩
conj. intro
and-E[l](p)
p · l
conj. elim
and-E[r](p)
p · r
conj. elim
imp-I(x.p)
λ (x) p
impl. intro
imp-E(p1; p2)
p1(p2)
impl. elim
false-E(p)
abort(p)
false elim
or-I[l](p)
l · p
disj. intro
or-I[r](p)
r · p
disj. intro
or-E(p; x1.p1; x2.p2)
case p {l · x1 ,→p1 | r · x2 ,→p2}
disj. elim
The concrete syntax of proof terms is chosen to stress the correspondence between propositions
and types discussed in Section 12.4 below.
The rules of constructive propositional logic can be restated using proof terms as follows.
Γ ⊢⟨⟩: ⊤
(12.7a)
Γ ⊢p1 : φ1
Γ ⊢p2 : φ2
Γ ⊢⟨p1, p2⟩: φ1 ∧φ2
(12.7b)
Γ ⊢p1 : φ1 ∧φ2
Γ ⊢p1 · l : φ1
(12.7c)
Γ ⊢p1 : φ1 ∧φ2
Γ ⊢p1 · r : φ2
(12.7d)
Γ, x : φ1 ⊢p2 : φ2
Γ ⊢λ (x) p2 : φ1 ⊃φ2
(12.7e)
Γ ⊢p : φ1 ⊃φ2
Γ ⊢p1 : φ1
Γ ⊢p(p1) : φ2
(12.7f)

102
12.3 Proof Dynamics
Γ ⊢p : ⊥
Γ ⊢abort(p) : φ
(12.7g)
Γ ⊢p1 : φ1
Γ ⊢l · p1 : φ1 ∨φ2
(12.7h)
Γ ⊢p2 : φ2
Γ ⊢r · p2 : φ1 ∨φ2
(12.7i)
Γ ⊢p : φ1 ∨φ2
Γ, x1 : φ1 ⊢p1 : φ
Γ, x2 : φ2 ⊢p2 : φ
Γ ⊢case p {l · x1 ,→p1 | r · x2 ,→p2} : φ
(12.7j)
12.3
Proof Dynamics
Proof terms in constructive logic are given a dynamics by Gentzen’s Principle. It states that the
elimination forms are inverse to the introduction forms. One aspect of Gentzen’s Principle is
the principle of conservation of proof, which states that the information introduced into a proof
of a proposition can be extracted without loss by elimination. For example, we may state that
conjunction elimination is post-inverse to conjunction introduction by the deﬁnitional equations:
Γ ⊢p1 : φ1
Γ ⊢p2 : φ2
Γ ⊢⟨p1, p2⟩· l ≡p1 : φ1
(12.8a)
Γ ⊢p1 : φ1
Γ ⊢p2 : φ2
Γ ⊢⟨p1, p2⟩· r ≡p2 : φ2
(12.8b)
Another aspect of Gentzen’s Principle is that principle of reversibility of proof, which states that
every proof can be reconstructed from the information that can be extracted from it by elimination.
In the case of conjunction this can be stated by the deﬁnitional equation
Γ ⊢p1 : φ1
Γ ⊢p2 : φ2
Γ ⊢⟨p · l, p · r⟩≡p : φ1 ∧φ2
(12.9)
Similar equivalences can be stated for the other connectives. For example, the conservation
and reversibility principles for implication are given by these rules:
Γ, x : φ1 ⊢p2 : φ2
Γ ⊢p2 : φ2
Γ ⊢(λ (x) p2)(p1) ≡[p1/x]p2 : φ2
(12.10a)
Γ ⊢p : φ1 ⊃φ2
Γ ⊢λ (x) (p(x)) ≡p : φ1 ⊃φ2
(12.10b)
The corresponding rules for disjunction and falsehood are given as follows:
Γ ⊢p : φ1 ∨φ2
Γ, x1 : φ1 ⊢p1 : ψ
Γ, x2 : φ2 ⊢p2 : ψ
Γ ⊢case l · p {l · x1 ,→p1 | r · x2 ,→p2} ≡[p/x1]p1 : ψ
(12.11a)

12.4 Propositions as Types
103
Γ ⊢p : φ1 ∨φ2
Γ, x1 : φ1 ⊢p1 : ψ
Γ, x2 : φ2 ⊢p2 : ψ
Γ ⊢case r · p {l · x1 ,→p1 | r · x2 ,→p2} ≡[p/x2]p2 : ψ
(12.11b)
Γ ⊢p : φ1 ∨φ2
Γ, x : φ1 ∨φ2 ⊢q : ψ
Γ ⊢[p/x]q ≡case p {l · x1 ,→[l · x1/x]q | r · x2 ,→[r · x2/x]q} : ψ
(12.11c)
Γ ⊢p : ⊥
Γ, x : ⊥⊢q : ψ
Γ ⊢[p/x]q ≡abort(p) : ψ
(12.11d)
12.4
Propositions as Types
Reviewing the statics and dynamics of proofs in constructive logic reveals a striking similarity
to the statics and dynamics of expressions of various types. For example, the introduction rule
for conjunction speciﬁes that a proof of a conjunction consists of a pair of proofs, one for each
conjunct, and the elimination rule inverts this, allowing us to extract a proof of each conjunct
from any proof of a conjunction. There is an obvious analogy with the static semantics of product
types, whose introduction form is a pair and whose elimination forms are projections. Gentzen’s
Principle extends the analogy to the dynamics as well, so that the elimination forms for conjunction
amount to projections that extract the appropriate components from an ordered pair.
The following chart summarizes the correspondence between propositions and types and be-
tween proofs and programs:
Prop
Type
⊤
unit
⊥
void
φ1 ∧φ2
τ1 × τ2
φ1 ⊃φ2
τ1 →τ2
φ1 ∨φ2
τ1 + τ2
The correspondence between propositions and types is a cornerstone of the theory of program-
ming languages. It exposes a deep connection between computation and deduction, and serves as
a framework for the analysis of language constructs and reasoning principles by relating them to
one another.
12.5
Notes
The propositions as types principle has its origins in the semantics of intuitionistic logic developed
by Brouwer, according to which the truth of a proposition is witnessed by a construction providing
computable evidence for it. The forms of evidence are determined by the form of the proposition,
so that evidence for an implication is a computable function transforming evidence for the hypoth-
esis into evidence for the conclusion. An explicit formulation of this semantics was introduced by
Heyting, and further developed by several people, including de Bruijn, Curry, Gentzen, Girard,
Howard, Kolmogorov, Martin-L¨of, and Tait. The propositions-as-types correspondence is some-
times called the Curry-Howard Isomorphism, but this terminology neglects the crucial contributions
of the others just mentioned. Moreover, the correspondence is not, in general, an isomorphism;

104
12.5 Notes
rather, it expresses Brouwer’s Dictum that the concept of proof is best explained by the more gen-
eral concept of construction (program).
Exercises
12.1. The law of the excluded middle (LEM) is the statement that every proposition φ is decidable in
the sense that φ ∨¬φ true. Constructively the law of the excluded middle states that, for
every proposition φ, we either have a proof of φ or a refutation of φ (proof of its negation).
Because this is manifestly not the case in general, one may suspect that the law of the ex-
cluded middle is not constructively valid. This is so, but not in the sense that the law is
refuted, but rather in the sense that it is not afﬁrmed. First, any proposition φ for which we
have a proof or a refutation is already decided, and so is decidable. Second, there are broad
classes of propositions for which we can, on demand, produce a proof or a refutation. For
example, it is decidable whether or not two integers are equal. Third, and most importantly,
there are, and always will be, propositions φ whose status is unresolved: it may turn out that
φ is true, or it may turn out that φ is false. For all these reasons constructive logic does not
refute the decidability propositions: ¬¬(φ ∨¬φ) true for any proposition φ. Prove it using
the rules given in this chapter.
12.2. The proposition ¬¬φ is no stronger than φ: prove φ ⊃¬¬φ true. The law of double-negation
elimination (DNE) states that (¬¬φ) ⊃φ true for every proposition φ. It follows immediately
from Exercise 12.1 that DNE entails LEM; prove the converse.
12.3. Deﬁne the relation φ ≤ψ to mean that φ true ⊢ψ true according to the rules of constructive
logic given above. With respect to this relation, show the following facts:
(a) It is a pre-order, which is say that it is reﬂexive and transitive.
(b) φ ∧ψ is the meet, or greatest lower bound, of φ and ψ, and ⊤is the top, or greatest, element.
(c) Show that φ ∨ψ is the join, or least upper bound, of φ and ψ, and that ⊥is the bottom, or
least, element.
(d) Show that φ ⊃ψ is an exponential, or pseudo-complement, in the sense that it is the largest
ρ such that φ ∧ρ ≤ψ. (The exponential φ ⊃ψ is sometimes written ψφ.)
Altogether these facts state that entailment in constructive propositional logic forms a Heyt-
ing algebra. Show that a general Heyting algebra (that is, an ordering with the above struc-
ture) is distributive in the sense that
φ ∧(ψ1 ∨ψ2) ≡(φ ∧ψ1) ∨(φ ∧ψ2)
φ ∨(ψ1 ∧ψ2) ≡(φ ∨ψ1) ∧(φ ∨ψ2),
where φ ≡ψ means φ ≤ψ and ψ ≤φ.

12.5 Notes
105
12.4. In any Heyting algebra we have φ ∧¬φ ≤⊥, which is to say that the negation is inconsistent
with the negated. But ¬φ is not necessarily the complement of φ in the sense that φ ∨¬φ ≤⊤.
A Boolean algebra is a Heyting algebra in which negation is always the complement of the
negated: ⊤≤φ ∨¬φ for every φ. Check that the two-element Boolean algebra for which
meets, joins, and exponentials are given by the classical truth tables (deﬁning φ ⊃ψ as
(¬φ) ∨ψ is Boolean algebra. Conclude that it is consistent to adjoin LEM to constructive
logic, which is to say that classical logic is a special case of constructive logic in which we
assume that every proposition is decidable. Being a Heyting algebra, every Boolean algebra
is clearly distributive. Show that every Boolean algebra also satisﬁes the de Morgan duality
laws:
¬(φ ∨ψ) ≡¬φ ∧¬ψ
¬(φ ∧ψ) ≡¬φ ∨¬ψ.
The ﬁrst of these is valid in any Heyting algebra; the second only in a Boolean algebra.

106
12.5 Notes

Chapter 13
Classical Logic
In constructive logic a proposition is true exactly when it has a proof, a derivation of it from axioms
and assumptions, and is false exactly when it has a refutation, a derivation of a contradiction from
the assumption that it is true. Constructive logic is a logic of positive evidence. To afﬁrm or deny
a proposition requires a proof, either of the proposition itself, or of a contradiction, under the
assumption that it has a proof. We are not always able to afﬁrm or deny a proposition. An open
problem is one for which we have neither a proof nor a refutation— constructively speaking, it is
neither true nor false.
In contrast classical logic (the one we learned in school) is a logic of perfect information where
every proposition is either true or false. We may say that classical logic corresponds to “god’s
view” of the world—there are no open problems, rather all propositions are either true or false. Put
another way, to assert that every proposition is either true or false is to weaken the notion of truth
to encompass all that is not false, dually to the constructively (and classically) valid interpretation
of falsity as all that is not true. The symmetry between truth and falsity is appealing, but there is a
price to pay for this: the meanings of the logical connectives are weaker in the classical case than
in the constructive.
The law of the excluded middle provides a prime example. Constructively, this principle is not
universally valid, as we have seen in Exercise 12.1. Classically, however, it is valid, because every
proposition is either false or not false, and being not false is the same as being true. Nevertheless,
classical logic is consistent with constructive logic in that constructive logic does not refute classical
logic. As we have seen, constructive logic proves that the law of the excluded middle is positively
not refuted (its double negation is constructively true). Consequently, constructive logic is stronger
(more expressive) than classical logic, because it can express more distinctions (namely, between
afﬁrmation and irrefutability), and because it is consistent with classical logic.
Proofs in constructive logic have computational content: they can be executed as programs,
and their behavior is described by their type. Proofs in classical logic also have computational con-
tent, but in a weaker sense than in constructive logic. Rather than positively afﬁrm a proposition,
a proof in classical logic is a computation that cannot be refuted. Computationally, a refutation
consists of a continuation, or control stack, that takes a proof of a proposition and derives a con-
tradiction from it. So a proof of a proposition in classical logic is a computation that, when given

108
13.1 Classical Logic
a refutation of that proposition derives a contradiction, witnessing the impossibility of refuting it.
In this sense the law of the excluded middle has a proof, precisely because it is irrefutable.
13.1
Classical Logic
In constructive logic a connective is deﬁned by giving its introduction and elimination rules. In
classical logic a connective is deﬁned by giving its truth and falsity conditions. Its truth rules
correspond to introduction, and its falsity rules to elimination. The symmetry between truth and
falsity is expressed by the principle of indirect proof. To show that φ true it is enough to show
that φ false entails a contradiction, and, conversely, to show that φ false it is enough to show that
φ true leads to a contradiction. Although the second of these is constructively valid, the ﬁrst is
fundamentally classical, expressing the principle of indirect proof.
13.1.1
Provability and Refutability
There are three basic judgment forms in classical logic:
1. φ true, stating that the proposition φ is provable;
2. φ false, stating that the proposition φ is refutable;
3. #, stating that a contradiction has been derived.
These are extended to hypothetical judgments in which we admit both provability and refutability
assumptions:
φ1 false, . . . , φm false ψ1 true, . . . , ψn true ⊢J.
The hypotheses are divided into two zones, one for falsity assumptions, ∆, and one for truth
assumptions, Γ.
The rules of classical logic are organized around the symmetry between truth and falsity, which
is mediated by the contradiction judgment.
The hypothetical judgment is reﬂexive:
∆, φ false Γ ⊢φ false
(13.1a)
∆Γ, φ true ⊢φ true
(13.1b)
The remaining rules are stated so that the structural properties of weakening, contraction, and
transitivity are admissible.
A contradiction arises when a proposition is judged both true and false. A proposition is true
if its falsity is absurd, and is false if its truth is absurd.
∆Γ ⊢φ false
∆Γ ⊢φ true
∆Γ ⊢#
(13.1c)
∆, φ false Γ ⊢#
∆Γ ⊢φ true
(13.1d)

13.1 Classical Logic
109
∆Γ, φ true ⊢#
∆Γ ⊢φ false
(13.1e)
Truth is trivially true, and cannot be refuted.
∆Γ ⊢⊤true
(13.1f)
A conjunction is true if both conjuncts are true, and is false if either conjunct is false.
∆Γ ⊢φ1 true
∆Γ ⊢φ2 true
∆Γ ⊢φ1 ∧φ2 true
(13.1g)
∆Γ ⊢φ1 false
∆Γ ⊢φ1 ∧φ2 false
(13.1h)
∆Γ ⊢φ2 false
∆Γ ⊢φ1 ∧φ2 false
(13.1i)
Falsity is trivially false, and cannot be proved.
∆Γ ⊢⊥false
(13.1j)
A disjunction is true if either disjunct is true, and is false if both disjuncts are false.
∆Γ ⊢φ1 true
∆Γ ⊢φ1 ∨φ2 true
(13.1k)
∆Γ ⊢φ2 true
∆Γ ⊢φ1 ∨φ2 true
(13.1l)
∆Γ ⊢φ1 false
∆Γ ⊢φ2 false
∆Γ ⊢φ1 ∨φ2 false
(13.1m)
Negation inverts the sense of each judgment:
∆Γ ⊢φ false
∆Γ ⊢¬φ true
(13.1n)
∆Γ ⊢φ true
∆Γ ⊢¬φ false
(13.1o)
An implication is true if its conclusion is true when the assumption is true, and is false if its
conclusion is false yet its assumption is true.
∆Γ, φ1 true ⊢φ2 true
∆Γ ⊢φ1 ⊃φ2 true
(13.1p)
∆Γ ⊢φ1 true
∆Γ ⊢φ2 false
∆Γ ⊢φ1 ⊃φ2 false
(13.1q)

110
13.1 Classical Logic
13.1.2
Proofs and Refutations
To explain the dynamics of classical proofs we ﬁrst introduce an explicit syntax for proofs and
refutations. We will deﬁne three hypothetical judgments for classical logic with explicit deriva-
tions:
1. ∆Γ ⊢p : φ, stating that p is a proof of φ;
2. ∆Γ ⊢k ÷ φ, stating that k is a refutation of φ;
3. ∆Γ ⊢k # p, stating that k and p are contradictory.
The falsity assumptions ∆are given by a context of the form
u1 ÷ φ1, . . . , um ÷ φm,
where m ≥0, in which the variables u1, . . . , un stand for refutations. The truth assumptions Γ are
given by a context of the form
x1 : ψ1, . . . , xn : ψn,
where n ≥0, in which the variables x1, . . . , xn stand for proofs.
The syntax of proofs and refutations is given by the following grammar:
Prf
p
::=
true-T
⟨⟩
truth
and-T(p1; p2)
⟨p1, p2⟩
conjunction
or-T[l](p)
l · p
disjunction left
or-T[r](p)
r · p
disjunction right
not-T(k)
not(k)
negation
imp-T(x.p)
λ (x) p
implication
ccr(u.(k # p))
ccr(u.(k # p))
contradiction
Ref
k
::=
false-F
abort
falsehood
and-F[l](k)
fst ; k
conjunction left
and-F[r](k)
snd ; k
conjunction right
or-F(k1; k2)
case(k1; k2)
disjunction
not-F(p)
not(p)
negation
imp-F(p; k)
ap(p) ; k
implication
ccp(x.(k # p))
ccp(x.(k # p))
contradiction
Proofs serve as evidence for truth judgments, and refutations serve as evidence for false judg-
ments. Contradictions are witnessed by the juxtaposition of a proof and a refutation.
A contradiction arises when a proposition is both true and false:
∆Γ ⊢k ÷ φ
∆Γ ⊢p : φ
∆Γ ⊢k # p
(13.2a)
Truth and falsity are deﬁned symmetrically in terms of contradiction:
∆, u ÷ φ Γ ⊢k # p
∆Γ ⊢ccr(u.(k # p)) : φ
(13.2b)

13.1 Classical Logic
111
∆Γ, x : φ ⊢k # p
∆Γ ⊢ccp(x.(k # p)) ÷ φ
(13.2c)
Reﬂexivity corresponds to the use of a variable hypothesis:
∆, u ÷ φ Γ ⊢u ÷ φ
(13.2d)
∆Γ, x : φ ⊢x : φ
(13.2e)
The other structure properties are admissible.
Truth is trivially true, and cannot be refuted.
∆Γ ⊢⟨⟩: ⊤
(13.2f)
A conjunction is true if both conjuncts are true, and is false if either conjunct is false.
∆Γ ⊢p1 : φ1
∆Γ ⊢p2 : φ2
∆Γ ⊢⟨p1, p2⟩: φ1 ∧φ2
(13.2g)
∆Γ ⊢k1 ÷ φ1
∆Γ ⊢fst ; k1 ÷ φ1 ∧φ2
(13.2h)
∆Γ ⊢k2 ÷ φ2
∆Γ ⊢snd ; k2 ÷ φ1 ∧φ2
(13.2i)
Falsity is trivially false, and cannot be proved.
∆Γ ⊢abort ÷ ⊥
(13.2j)
A disjunction is true if either disjunct is true, and is false if both disjuncts are false.
∆Γ ⊢p1 : φ1
∆Γ ⊢l · p1 : φ1 ∨φ2
(13.2k)
∆Γ ⊢p2 : φ2
∆Γ ⊢r · p2 : φ1 ∨φ2
(13.2l)
∆Γ ⊢k1 ÷ φ1
∆Γ ⊢k2 ÷ φ2
∆Γ ⊢case(k1; k2) ÷ φ1 ∨φ2
(13.2m)
Negation inverts the sense of each judgment:
∆Γ ⊢k ÷ φ
∆Γ ⊢not(k) : ¬φ
(13.2n)
∆Γ ⊢p : φ
∆Γ ⊢not(p) ÷ ¬φ
(13.2o)

112
13.2 Deriving Elimination Forms
An implication is true if its conclusion is true when the assumption is true, and is false if its
conclusion is false, yet its assumption is true.
∆Γ, x : φ1 ⊢p2 : φ2
∆Γ ⊢λ (x) p2 : φ1 ⊃φ2
(13.2p)
∆Γ ⊢p1 : φ1
∆Γ ⊢k2 ÷ φ2
∆Γ ⊢ap(p1) ; k2 ÷ φ1 ⊃φ2
(13.2q)
13.2
Deriving Elimination Forms
The price of achieving a symmetry between truth and falsity in classical logic is that we must very
often rely on the principle of indirect proof: to show that a proposition is true, we often must
derive a contradiction from the assumption of its falsity. For example, a proof of
(φ ∧(ψ ∧θ)) ⊃(θ ∧φ)
in classical logic has the form
λ (w) ccr(u.(k # w)),
where k is the refutation
fst ; ccp(x.(snd ; ccp(y.(snd ; ccp(z.(u # ⟨z, x⟩)) # y)) # w)).
And yet in constructive logic this proposition has a direct proof that avoids the circumlocutions of
proof by contradiction:
λ (w) ⟨w · r · r, w · l⟩.
But this proof cannot be expressed (as is) in classical logic, because classical logic lacks the elimi-
nation forms of constructive logic.
However, we may package the use of indirect proof into a slightly more palatable form by
deriving the elimination rules of constructive logic. For example, the rule
∆Γ ⊢φ ∧ψ true
∆Γ ⊢φ true
is derivable in classical logic:
∆, φ false Γ ⊢φ false
∆, φ false Γ ⊢φ ∧ψ false
∆Γ ⊢φ ∧ψ true
∆, φ false Γ ⊢φ ∧ψ true
∆, φ false Γ ⊢#
∆Γ ⊢φ true
The other elimination forms are derivable similarly, in each case relying on indirect proof to con-
struct a proof of the truth of a proposition from a derivation of a contradiction from the assumption
of its falsity.

13.3 Proof Dynamics
113
The derivations of the elimination forms of constructive logic are most easily exhibited using
proof and refutation expressions, as follows:
abort(p) = ccr(u.(abort # p))
p · l = ccr(u.(fst ; u # p))
p · r = ccr(u.(snd ; u # p))
p1(p2) = ccr(u.(ap(p2) ; u # p1))
case p1 {l · x ,→p2 | r · y ,→p} = ccr(u.(case(ccp(x.(u # p2)); ccp(y.(u # p))) # p1))
The expected elimination rules are valid for these deﬁnitions. For example, the rule
∆Γ ⊢p1 : φ ⊃ψ
∆Γ ⊢p2 : φ
∆Γ ⊢p1(p2) : ψ
(13.3)
is derivable using the deﬁnition of p1(p2) given above. By suppressing proof terms, we may derive
the corresponding provability rule
∆Γ ⊢φ ⊃ψ true
∆Γ ⊢φ true
∆Γ ⊢ψ true
.
(13.4)
13.3
Proof Dynamics
The dynamics of classical logic arises from the simpliﬁcation of the contradiction between a proof
and a refutation of a proposition. To make this explicit we will deﬁne a transition system whose
states are contradictions k # p consisting of a proof p and a refutation k of the same proposition.
The steps of the computation consist of simpliﬁcations of the contradictory state based on the form
of p and k.
The truth and falsity rules for the connectives play off one another in a pleasing way:
fst ; k # ⟨p1, p2⟩7−→k # p1
(13.5a)
snd ; k # ⟨p1, p2⟩7−→k # p2
(13.5b)
case(k1; k2) # l · p1 7−→k1 # p1
(13.5c)
case(k1; k2) # r · p2 7−→k2 # p2
(13.5d)
not(p) # not(k) 7−→k # p
(13.5e)
ap(p1) ; k # λ (x) p2 7−→k # [p1/x]p2
(13.5f)
The rules of indirect proof give rise to the following transitions:
ccp(x.(k1 # p1)) # p2 7−→[p2/x]k1 # [p2/x]p1
(13.5g)
k1 # ccr(u.(k2 # p2)) 7−→[k1/u]k2 # [k1/u]p2
(13.5h)
The ﬁrst of these deﬁnes the behavior of the refutation of φ that proceeds by contradicting the
assumption that φ is true. Such a refutation is activated by presenting it with a proof of φ, which

114
13.3 Proof Dynamics
is then substituted for the assumption in the new state. Thus, “ccp” stands for “call with current
proof.” The second transition deﬁnes the behavior of the proof of φ that proceeds by contradicting
the assumption that φ is false. Such a proof is activated by presenting it with a refutation of φ,
which is then substituted for the assumption in the new state. Thus, “ccr” stands for “call with
current refutation.”
Rules (13.5g) to (13.5h) overlap in that there are two transitions for a state of the form
ccp(x.(k1 # p1)) # ccr(u.(k2 # p2)),
one to the state [p/x]k1 # [p/x]p1, where p is ccr(u.(k2 # p2)), and one to the state [k/u]k2 #
[k/u]p2, where k is ccp(x.(k1 # p1)). The dynamics of classical logic is non-deterministic. To avoid
this one may impose a priority ordering among the two cases, preferring one transition over the
other when there is a choice. Preferring the ﬁrst corresponds to a “lazy” dynamics for proofs,
because we pass the unevaluated proof p to the refutation on the left, which is thereby activated.
Preferring the second corresponds to an “eager” dynamics for proofs, in which we pass the un-
evaluated refutation k to the proof, which is thereby activated.
All proofs in classical logic proceed by contradicting the assumption that it is false. In terms of
the classical logic machine the initial and ﬁnal states of a computation are as follows:
haltφ # p initial
(13.6a)
p canonical
haltφ # p ﬁnal
(13.6b)
where p is a proof of φ, and haltφ is the assumed refutation of φ. The judgment p canonical
states that p is a canonical proof, which holds of any proof other than an indirect proof. Execution
consists of driving a general proof to a canonical proof, under the assumption that the theorem is
false.
Theorem 13.1 (Preservation). If k ÷ φ, p : φ, and k # p 7−→k′ # p′, then there exists φ′ such that
k′ ÷ φ′ and p′ : φ′.
Proof. By rule induction on the dynamics of classical logic.
Theorem 13.2 (Progress). If k ÷ φ and p : φ, then either k # p ﬁnal or k # p 7−→k′ # p′.
Proof. By rule induction on the statics of classical logic.

13.4 Law of the Excluded Middle
115
13.4
Law of the Excluded Middle
The law of the excluded middle is derivable in classical logic:
φ ∨¬φ false, φ true ⊢φ true
φ ∨¬φ false, φ true ⊢φ ∨¬φ true
φ ∨¬φ false, φ true ⊢φ ∨¬φ false
φ ∨¬φ false, φ true ⊢#
φ ∨¬φ false ⊢φ false
φ ∨¬φ false ⊢¬φ true
φ ∨¬φ false ⊢φ ∨¬φ true
φ ∨¬φ false ⊢φ ∨¬φ false
φ ∨¬φ false ⊢#
φ ∨¬φ true
When written out using explicit proofs and refutations, we obtain the proof term p0 : φ ∨¬φ:
ccr(u.(u # r · not(ccp(x.(u # l · x))))).
To understand the computational meaning of this proof, let us juxtapose it with a refutation k ÷
φ ∨¬φ and simplify it using the dynamics given in Section 13.3. The ﬁrst step is the transition
k # ccr(u.(u # r · not(ccp(x.(u # l · x)))))
7−→
k # r · not(ccp(x.(k # l · x))),
wherein we have replicated k so that it occurs in two places in the result state. By virtue of its
type the refutation k must have the form case(k1; k2), where k1 ÷ φ and k2 ÷ ¬φ. Continuing the
reduction, we obtain:
case(k1; k2) # r · not(ccp(x.(case(k1; k2) # l · x)))
7−→
k2 # not(ccp(x.(case(k1; k2) # l · x))).
By virtue of its type k2 must have the form not(p2), where p2 : φ, and hence the transition proceeds
as follows:
not(p2) # not(ccp(x.(case(k1; k2) # l · x)))
7−→
ccp(x.(case(k1; k2) # l · x)) # p2.

116
13.5 The Double-Negation Translation
Observe that p2 is a valid proof of φ. Proceeding, we obtain
ccp(x.(case(k1; k2) # l · x)) # p2
7−→
case(k1; k2) # l · p2
7−→
k1 # p2
The ﬁrst of these two steps is the crux of the matter: the refutation, k = case(k1; k2), which was
replicated at the outset of the derivation, is re-used, but with a different argument. At the ﬁrst
use, the refutation k which is provided by the context of use of the law of the excluded middle, is
presented with a proof r · p1 of φ ∨¬φ. That is, the proof behaves as though the right disjunct of
the law is true, which is to say that φ is false. If the context is such that it inspects this proof, it
can only be by providing the proof p2 of φ that refutes the claim that φ is false. Should this occur,
the proof of the law of the excluded middle “backtracks” the context, providing instead the proof
l · p2 to k, which then passes p2 to k1 without further incident. The proof of the law of the excluded
middle boldly asserts ¬φ true, regardless of the form of φ. Then, if caught in its lie by the context
providing a proof of φ, it “changes its mind” and asserts φ to the original context k after all. No
further reversion is possible, because the context has itself provided a proof p2 of φ.
The law of the excluded middle illustrates that classical proofs are interactions between proofs
and refutations, which is to say interactions between a proof and the context in which it is used.
In programming terms this corresponds to an abstract machine with an explicit control stack, or
continuation, representing the context of evaluation of an expression. That expression may access
the context (stack, continuation) to backtrack so as to maintain the perfect symmetry between truth
and falsity. The penalty is that a closed proof of a disjunction no longer need show which disjunct
it proves, for as we have just seen, it may, on further inspection, “change its mind.”
13.5
The Double-Negation Translation
One consequence of the greater expressiveness of constructive logic is that classical proofs may be
translated systematically into constructive proofs of a classically equivalent proposition. There-
fore, by systematically reorganizing the classical proof, we may, without changing its meaning
from a classical perspective, turn it into a constructive proof of a constructively weaker propo-
sition. Consequently, there is no loss in adhering to constructive proofs, because every classical
proof is a constructive proof of a constructively weaker, but classically equivalent, proposition.
Moreover, it proves that classical logic is weaker (less expressive) than constructive logic, contrary
to a na¨ıve interpretation which would say that the added reasoning principles, such as the law
of the excluded middle, afforded by classical logic makes it stronger. In programming language
terms adding a “feature” does not necessarily strengthen (improve the expressive power) of your
language; on the contrary, it may weaken it.
We will deﬁne a translation φ∗of propositions that interprets classical into constructive logic

13.6 Notes
117
according to the following correspondences:
Classical
Constructive
∆Γ ⊢φ true
¬∆∗Γ∗⊢¬¬φ∗true
truth
∆Γ ⊢φ false
¬∆∗Γ∗⊢¬φ∗true
falsity
∆Γ ⊢#
¬∆∗Γ∗⊢⊥true
contradiction
Classical truth is weakened to constructive irrefutability, but classical falsehood is constructive
refutability, and classical contradiction is constructive falsehood. Falsity assumptions are negated
after translation to express their falsehood; truth assumptions are merely translated as is. Because
the double negations are classically cancelable, the translation will be easily seen to yield a classi-
cally equivalent proposition. But because ¬¬φ is constructively weaker than φ, we also see that a
proof in classical logic is translated to a constructive proof of a weaker statement.
There are many choices for the translation; here is one that makes the proof of the correspon-
dence between classical and constructive logic especially simple:
⊤∗= ⊤
(φ1 ∧φ2)∗= φ∗
1 ∧φ∗
2
⊥∗=⊥
(φ1 ∨φ2)∗= φ∗
1 ∨φ∗
2
(φ1 ⊃φ2)∗= φ∗
1 ⊃¬¬φ∗
2
(¬φ)∗= ¬φ∗
One may show by induction on the rules of classical logic that the correspondences summarized
above hold, using constructively valid entailments such as
¬¬φ true ¬¬ψ true ⊢¬¬(φ ∧ψ) true.
13.6
Notes
The computational interpretation of classical logic was ﬁrst explored by Grifﬁn (1990) and Murthy
(1991). The present account is inﬂuenced by Wadler (2003), transposed by Nanevski from sequent
calculus to natural deduction using multiple forms of judgment. The terminology is inspired by
Lakatos (1976), an insightful and inspiring analysis of the discovery of proofs and refutations of
conjectures in mathematics. Versions of the double-negation translation were originally given by
G¨odel and Gentzen. The computational content of the double negation translation was ﬁrst eluci-
dated by Murthy (1991), who established the important relationship with continuation passing.
Exercises

118
13.6 Notes
13.1. If the continuation type expresses negation, the types shown to be inhabited in Exercise 30.2,
when interpreted under the proposition-as-types interpretation, look suspiciously like the
following propositions:
(a) φ ∨¬φ.
(b) (¬¬φ) ⊃φ.
(c) (¬φ2 ⊃¬φ1) ⊃(φ1 ⊃φ2).
(d) ¬(φ1 ∨φ2) ⊃(¬φ1 ∧¬φ2).
None of these propositions is true, in general, in constructive logic. Show that each of these
propositions is true in classical logic by exhibiting a proof term for each. (The ﬁrst one is
done for you in Section 13.4; you need only do the other three.) Compare the proof term you
get for each with the inhabitant of the corresponding type that you gave in your solution to
Exercise 30.2.
13.2. Complete the proof of the double-negation interpretation sketched in Section 13.5, providing
explicit proof terms for clarity. Because (φ ∨¬φ)∗= φ∗∨¬φ∗, the double-negation trans-
lation applied to the proof of LEM (for φ) given in Section 13.4 yields a proof of the double
negation of LEM (for φ∗) in constructive logic. How does the translated proof compare to
the one you derived by hand in Exercise 12.1?

Part VI
Inﬁnite Data Types


Chapter 14
Generic Programming
14.1
Introduction
Many programs are instances of a pattern in a particular situation. Sometimes types determine
the pattern by a technique called (type) generic programming. For example, in Chapter 9 recursion
over the natural numbers is introduced in an ad hoc way. As we shall see, the pattern of recursion
on values of an inductive type is expressed as a generic program.
To get a ﬂavor of the concept, consider a function f of type ρ →ρ′, which transforms values of
type ρ into values of type ρ′. For example, f might be the doubling function on natural numbers.
We wish to extend f to a transformation from type [ρ/t]τ to type [ρ′/t]τ by applying f to various
spots in the input where a value of type ρ occurs to obtain a value of type ρ′, leaving the rest of
the data structure alone. For example, τ might be bool × t, in which case f could be extended to a
function of type bool × ρ →bool × ρ′ that sends the pairs ⟨a, b⟩to the pair ⟨a, f (b)⟩.
The foregoing example glosses over an ambiguity arising from the many-one nature of substi-
tution. A type can have the form [ρ/t]τ in many different ways, according to how many occur-
rences of t there are within τ. Given f as above, it is not clear how to extend it to a function from
[ρ/t]τ to [ρ′/t]τ. To resolve the ambiguity we must be given a template that marks the occurrences
of t in τ at which f is applied. Such a template is known as a type operator, t.τ, which is an abstrac-
tor binding a type variable t within a type τ. Given such an abstractor, we may unambiguously
extend f to instances of τ given by substitution for t in τ.
The power of generic programming depends on the type operators that are allowed. The sim-
plest case is that of a polynomial type operator, one constructed from sum and product of types,
including their nullary forms. These are extended to positive type operators, which also allow
certain forms of function types.
14.2
Polynomial Type Operators
A type operator is a type equipped with a designated variable whose occurrences mark the spots in
the type where a transformation is applied. A type operator is an abstractor t.τ such that t type ⊢

122
14.2 Polynomial Type Operators
τ type. An example of a type operator is the abstractor
t.unit + (bool × t)
in which occurrences of t mark the spots in which a transformation is applied. An instance of the
type operator t.τ is obtained by substituting a type ρ for the variable t within the type τ.
The polynomial type operators are those constructed from the type variable t the types void and
unit, and the product and sum type constructors τ1 × τ2 and τ1 + τ2. More precisely, the judgment
t.τ poly is inductively deﬁned by the following rules:
t.t poly
(14.1a)
t.unit poly
(14.1b)
t.τ1 poly
t.τ2 poly
t.τ1 × τ2 poly
(14.1c)
t.void poly
(14.1d)
t.τ1 poly
t.τ2 poly
t.τ1 + τ2 poly
(14.1e)
Exercise 14.1 asks for a proof that polynomial type operators are closed under substitution.
Polynomial type operators are templates describing the structure of a data structure with slots
for values of a particular type. For example, the type operator t.t × (nat + t) speciﬁes all types
ρ × (nat + ρ) for any choice of type ρ. Thus a polynomial type operator designates points of
interest in a data structure that have a common type. As we shall see shortly, this allows us
to specify a program that applies a given function to all values lying at points of interest in a
compound data structure to obtain a new one with the results of the applications at those points.
Because substitution is not injective, one cannot recover the type operator from its instances. For
example, if ρ were nat, then the instance would be nat × (nat + nat); it is impossible to know
which occurrences of nat are in designated spots unless we are given the pattern by the type
operator.
The generic extension of a polynomial type operator is a form of expression with the following
syntax
Exp
e
::=
map{t.τ}(x.e′)(e)
map{t.τ}(x.e′)(e)
generic extension.
Its statics is given as follows:
t.τ poly
Γ, x : ρ ⊢e′ : ρ′
Γ ⊢e : [ρ/t]τ
Γ ⊢map{t.τ}(x.e′)(e) : [ρ′/t]τ
(14.2)
The abstractor x.e′ speciﬁes a mapping that sends x : ρ to e′ : ρ′. The generic extension of t.τ
along x.e′ speciﬁes a mapping from [ρ/t]τ to [ρ′/t]τ. The latter mapping replaces values v of type

14.2 Polynomial Type Operators
123
ρ occurring at spots corresponding to occurrences of t in τ by the transformed value [v/x]e′ of
type ρ′ at the same spot. The type operator t.τ is a template in which certain spots, marked by
occurrences of t, show where to apply the transformation x.e′ to a value of type [ρ/t]τ to obtain a
value of type [ρ′/t]τ.
The following dynamics makes precise the concept of the generic extension of a polynomial
type operator.
map{t.t}(x.e′)(e) 7−→[e/x]e′
(14.3a)
map{t.unit}(x.e′)(e) 7−→e
(14.3b)
map{t.τ1 × τ2}(x.e′)(e)
7−→
⟨map{t.τ1}(x.e′)(e · l), map{t.τ2}(x.e′)(e · r)⟩
(14.3c)
map{t.void}(x.e′)(e) 7−→abort(e)
(14.3d)
map{t.τ1 + τ2}(x.e′)(e)
7−→
case e {l · x1 ,→l · map{t.τ1}(x.e′)(x1) | r · x2 ,→r · map{t.τ2}(x.e′)(x2)}
(14.3e)
Rule (14.3a) applies the transformation x.e′ to e itself, because the operator t.t speciﬁes that the
transformation is performed directly. Rule (14.3b) states that the empty tuple is transformed to
itself. Rule (14.3c) states that to transform e according to the operator t.τ1 × τ2, the ﬁrst component
of e is transformed according to t.τ1 and the second component of e is transformed according to
t.τ2. Rule (14.3d) states that the transformation of a value of type void aborts, because there are
no such values. Rule (14.3e) states that to transform e according to t.τ1 + τ2, case analyze e and
reconstruct it after transforming the injected value according to t.τ1 or t.τ2.
Consider the type operator t.τ given by t.unit + (bool × t). Let x.e be the abstractor x.s(x),
which increments a natural number. Using rules (14.3) we may derive that
map{t.τ}(x.e)(r · ⟨true, n⟩) 7−→∗r · ⟨true, n + 1⟩.
The natural number in the second component of the pair is incremented, because the type variable
t occurs in that spot in the type operator t.τ.
Theorem 14.1 (Preservation). If map{t.τ}(x.e′)(e) : τ′ and map{t.τ}(x.e′)(e) 7−→e′′, then e′′ : τ′.
Proof. By inversion of rule (14.2) we have
1. t type ⊢τ type;

124
14.3 Positive Type Operators
2. x : ρ ⊢e′ : ρ′ for some ρ and ρ′;
3. e : [ρ/t]τ;
4. τ′ is [ρ′/t]τ.
The proof proceeds by cases on rules (14.3). For example, consider rule (14.3c). It follows from
inversion that map{t.τ1}(x.e′)(e · l) : [ρ′/t]τ1, and similarly that map{t.τ2}(x.e′)(e · r) : [ρ′/t]τ2. It
is easy to check that
⟨map{t.τ1}(x.e′)(e · l), map{t.τ2}(x.e′)(e · r)⟩
has type [ρ′/t](τ1 × τ2), as required.
14.3
Positive Type Operators
The positive type operators extend the polynomial type operators to admit restricted forms of func-
tion type. Speciﬁcally, t.τ1 →τ2 is a positive type operator, if (1) t does not occur in τ1, and (2) t.τ2
is a positive type operator. In general, any occurrences of a type variable t in the domain of a
function type are negative occurrences, whereas any occurrences of t within the range of a function
type, or within a product or sum type, are positive occurrences.1 A positive type operator is one for
which only positive occurrences of the type variable t are allowed. Positive type operators, like
polynomial type operators, are closed under substitution.
We deﬁne the judgment t.τ pos, which states that the abstractor t.τ is a positive type operator
by the following rules:
t.t pos
(14.4a)
t.unit pos
(14.4b)
t.τ1 pos
t.τ2 pos
t.τ1 × τ2 pos
(14.4c)
t.void pos
(14.4d)
t.τ1 pos
t.τ2 pos
t.τ1 + τ2 pos
(14.4e)
τ1 type
t.τ2 pos
t.τ1 →τ2 pos
(14.4f)
In rule (14.4f), the type variable t is excluded from the domain of the function type by demanding
that it be well-formed without regard to t.
1The origin of this terminology is that a function type τ1 →τ2 is analogous to the implication φ1 ⊃φ2, which is
classically equivalent to ¬φ1 ∨φ2, so that occurrences in the domain are under the negation.

14.4 Notes
125
The generic extension of a positive type operator is deﬁned similarly to that of a polynomial
type operator, with the following dynamics on function types:
map+{t.τ1 →τ2}(x.e′)(e) 7−→λ (x1 : τ1) map+{t.τ2}(x.e′)(e(x1))
(14.5)
Because t is not allowed to occur within the domain type, the type of the result is τ1 →[ρ′/t]τ2,
assuming that e is of type τ1 →[ρ/t]τ2. It is easy to verify preservation for the generic extension
of a positive type operator.
It is instructive to consider what goes wrong if we try to extend the generic extension to an arbi-
trary type operator, without any positivity restriction. Consider the type operator t.τ1 →τ2, with-
out restriction on t, and suppose that x : ρ ⊢e′ : ρ′. The generic extension map{t.τ1 →τ2}(x.e′)(e)
should have type [ρ′/t]τ1 →[ρ′/t]τ2, given that e has type [ρ/t]τ1 →[ρ/t]τ2. The extension
should yield a function of the form
λ (x1 : [ρ′/t]τ1) . . .(e(. . .(x1)))
in which we apply e to a transformation of x1 and then transform the result. The trouble is that
we are given, inductively, that map{t.τ1}(x.e′)(−) transforms values of type [ρ/t]τ1 into values of
type [ρ′/t]τ1, but we need to go the other way around to make x1 suitable as an argument for e.
14.4
Notes
The generic extension of a type operator is an example of the concept of a functor in category
theory (MacLane, 1998). Generic programming is essentially functorial programming, exploiting
the functorial action of polynomial type operators (Hinze and Jeuring, 2003).
Exercises
14.1. Prove that if t.τ poly and t′.τ′ poly, then t.[τ/t′]τ′ poly.
14.2. Show that the generic extension of a constant type operator is essentially the identity in that
it sends each closed value to itself. More precisely, show that, for each value e of type τ, the
expression
map{ .τ}(x.e′)(e)
evaluates to e, regardless of the choice of e′. For simplicity, assume an eager dynamics for
products and sums, and consider only polynomial type operators. What complications arise
when extending this observation to positive type operators?
14.3. Consider Exercises 10.1 and 11.3 in which a database schema is represented by a ﬁnite prod-
uct type indexed by the attributes of the schema, and a database with that schema is a ﬁnite
sequence of instances of tuples of that type. Show that any database transformation that ap-
plies a function to one or more of the columns of each row of a database can be programmed
in two steps using generic programming according to the following plan:

126
14.4 Notes
(a) Specify a type operator whose type variable shows which columns are transformed.
All speciﬁed columns must be of the same type in order for the transformation to make
sense.
(b) Specify the transformation on the type of column that will be applied to each tuple of
the database to obtain an updated database.
(c) Form the generic extension of the type operator with the given transformation, and
apply it to the given database.
For speciﬁcity, consider a schema whose attributes I include the attributes first and last,
both of type str. Let c : str →str be a function that capitalizes strings according to some
convention. Use generic programming to capitalize the first and last attributes of each
row of a given database with the speciﬁed schema.
14.4. Whereas t occurs negatively in the type t →bool, and does not occur only positively
(t →bool) →bool, we may say that t does occur non-negatively in the latter type. This
example illustrates that occurrences of t in the domain of a function are negative, and that
occurrences in the domain of the domain of a function are non-negative. Every positive oc-
currence counts as non-negative, but not every non-negative occurrence is positive.2 Give a
simultaneous induction deﬁnition of the negative and non-negative type operators. Check
that the type operator t.(t →bool) →bool is non-negative according to your deﬁnition.
14.5. Using the deﬁnitions of negative and non-negative type operators requested in Exercise 14.4,
give the deﬁnition of the generic extension of a non-negative type operator. Speciﬁcally, de-
ﬁne simultaneously map−−{t.τ}(x.e′)(e) and map−{t.τ}(x.e)(e′) by induction on the struc-
ture of τ with the statics give by these rules:
t.τ non-neg
Γ, x : ρ ⊢e′ : ρ′
Γ ⊢e : [ρ/t]τ
Γ ⊢map−−{t.τ}(x.e′)(e) : [ρ′/t]τ
(14.6a)
t.τ neg
Γ, x : ρ ⊢e′ : ρ′
Γ ⊢e : [ρ′/t]τ
Γ ⊢map−{t.τ}(x.e′)(e) : [ρ/t]τ
(14.6b)
Note well the reversal of the types of e and the overall type in these two rules. Calculate the
generic extension of the type operator t.(t →bool) →bool.
2Often what we have called “positive” is called “strictly positive”, and what we have called “non-negative” is called
“positive”.

Chapter 15
Inductive and Coinductive Types
The inductive and the coinductive types are two important forms of recursive type. Inductive types
correspond to least, or initial, solutions of certain type equations, and coinductive types correspond
to their greatest, or ﬁnal, solutions. Intuitively, the elements of an inductive type are those that are
given by a ﬁnite composition of its introduction forms. Consequently, if we specify the behavior
of a function on each of the introduction forms of an inductive type, then its behavior is deﬁned
for all values of that type. Such a function is a recursor, or catamorphism. Dually, the elements of a
coinductive type are those that behave properly in response to a ﬁnite composition of its elimina-
tion forms. Consequently, if we specify the behavior of an element on each elimination form, then
we have fully speciﬁed a value of that type. Such an element is a generator, or anamorphism.
15.1
Motivating Examples
The most important example of an inductive type is the type of natural numbers as formalized in
Chapter 9. The type nat is the least type containing z and closed under s(−). The minimality con-
dition is expressed by the existence of the iterator, iter e {z ,→e0 | s(x) ,→e1}, which transforms a
natural number into a value of type τ, given its value for zero, and a transformation from its value
on a number to its value on the successor of that number. This operation is well-deﬁned precisely
because there are no other natural numbers.
With a view towards deriving the type nat as a special case of an inductive type, it is useful to
combine zero and successor into a single introduction form, and to correspondingly combine the
basis and inductive step of the iterator. The following rules specify the statics of this reformulation:
Γ ⊢e : unit + nat
Γ ⊢foldnat(e) : nat
(15.1a)
Γ, x : unit + τ ⊢e1 : τ
Γ ⊢e2 : nat
Γ ⊢recnat(x.e1; e2) : τ
(15.1b)
The expression foldnat(e) is the unique introduction form of the type nat. Using this, the ex-
pression z is foldnat(l · ⟨⟩), and s(e) is foldnat(r · e). The recursor, recnat(x.e1; e2), takes as

128
15.1 Motivating Examples
argument the abstractor x.e1 that combines the basis and inductive step into a single computation
that, given a value of type unit + τ, yields a value of type τ. Intuitively, if x is replaced by the
value l · ⟨⟩, then e1 computes the base case of the recursion, and if x is replaced by the value r · e,
then e1 computes the inductive step from the result e of the recursive call.
The dynamics of the combined representation of natural numbers is given by the following
rules:
foldnat(e) val
(15.2a)
e2 7−→e′
2
recnat(x.e1; e2) 7−→recnat(x.e1; e′
2)
(15.2b)
recnat(x.e1; foldnat(e2))
7−→
[map{t.unit + t}(y.recnat(x.e1; y))(e2)/x]e1
(15.2c)
Rule (15.2c) uses (polynomial) generic extension (see Chapter 14) to apply the recursor to the
predecessor, if any, of a natural number. If we expand the deﬁnition of the generic extension in
place, we obtain this rule:
recnat(x.e1; foldnat(e2))
7−→
[case e2 {l · ,→l · ⟨⟩| r · y ,→r · recnat(x.e1; y)}/x]e1
Exercise 15.2 asks for a derivation of the iterator, as deﬁned in Chapter 9, from the recursor just
given.
An illustrative example of a coinductive type is the type of streams of natural numbers. A
stream is an inﬁnite sequence of natural numbers such that an element of the stream can be com-
puted only after computing all preceding elements in that stream. That is, the computations of
successive elements of the stream are sequentially dependent in that the computation of one el-
ement inﬂuences the computation of the next. In this sense the introduction form for streams is
dual to the elimination form for natural numbers.
A stream is given by its behavior under the elimination forms for the stream type: hd(e) re-
turns the next, or head, element of the stream, and tl(e) returns the tail of the stream, the stream
resulting when the head element is removed. A stream is introduced by a generator, the dual of a
recursor, that deﬁnes the head and the tail of the stream in terms of the current state of the stream,
which is represented by a value of some type. The statics of streams is given by the following
rules:
Γ ⊢e : stream
Γ ⊢hd(e) : nat
(15.3a)
Γ ⊢e : stream
Γ ⊢tl(e) : stream
(15.3b)

15.1 Motivating Examples
129
Γ ⊢e : τ
Γ, x : τ ⊢e1 : nat
Γ, x : τ ⊢e2 : τ
Γ ⊢strgen x is e in <hd ,→e1,tl ,→e2> : stream
(15.3c)
In rule (15.3c) the current state of the stream is given by the expression e of some type τ, and the
head and tail of the stream are determined by the expressions e1 and e2, respectively, as a function
of the current state. (The notation for the generator is chosen to emphasize that every stream has
both a head and a tail.)
The dynamics of streams is given by the following rules:
strgen x is e in <hd ,→e1,tl ,→e2> val
(15.4a)
e 7−→e′
hd(e) 7−→hd(e′)
(15.4b)
hd(strgen x is e in <hd ,→e1,tl ,→e2>) 7−→[e/x]e1
(15.4c)
e 7−→e′
tl(e) 7−→tl(e′)
(15.4d)
tl(strgen x is e in <hd ,→e1,tl ,→e2>)
7−→
strgen x is [e/x]e2 in <hd ,→e1,tl ,→e2>
(15.4e)
Rules (15.4c) and (15.4e) express the dependency of the head and tail of the stream on its current
state. Observe that the tail is obtained by applying the generator to the new state determined by
e2 from the current state.
To derive streams as a special case of a coinductive type, we combine the head and the tail into
a single elimination form, and reorganize the generator correspondingly. Thus we consider the
following statics:
Γ ⊢e : stream
Γ ⊢unfoldstream(e) : nat × stream
(15.5a)
Γ, x : τ ⊢e1 : nat × τ
Γ ⊢e2 : τ
Γ ⊢genstream(x.e1; e2) : stream
(15.5b)
Rule (15.5a) states that a stream may be unfolded into a pair consisting of its head, a natural num-
ber, and its tail, another stream. The head hd(e) and tail tl(e) of a stream e are the projections
unfoldstream(e) · l and unfoldstream(e) · r, respectively. Rule (15.5b) states that a stream is gener-
ated from the state element e2 by an expression e1 that yields the head element and the next state
as a function of the current state.
The dynamics of streams is given by the following rules:
genstream(x.e1; e2) val
(15.6a)

130
15.2 Statics
e 7−→e′
unfoldstream(e) 7−→unfoldstream(e′)
(15.6b)
unfoldstream(genstream(x.e1; e2))
7−→
map{t.nat × t}(y.genstream(x.e1; y))([e2/x]e1)
(15.6c)
Rule (15.6c) uses generic extension to generate a new stream whose state is the second component
of [e2/x]e1. Expanding the generic extension we obtain the following reformulation of this rule:
unfoldstream(genstream(x.e1; e2))
7−→
⟨([e2/x]e1) · l, genstream(x.e1; ([e2/x]e1) · r)⟩
Exercise 15.3 asks for a derivation of strgen x is e in <hd ,→e1,tl ,→e2> from the coinductive
generation form.
15.2
Statics
We may now give a general account of inductive and coinductive types, which are deﬁned in terms
of positive type operators. We will consider a variant of T, which we will call M, with natural
numbers replaced by functions, products, sums, and a rich class of inductive and coinductive
types.
15.2.1
Types
The syntax of inductive and coinductive types involves type variables, which are, of course, vari-
ables ranging over types. The abstract syntax of inductive and coinductive types is given by the
following grammar:
Typ
τ
::=
t
t
self-reference
ind(t.τ)
µ(t.τ)
inductive
coi(t.τ)
ν(t.τ)
coinductive
Type formation judgments have the form
t1 type, . . . , tn type ⊢τ type,
where t1, . . . , tn are type names. We let ∆range over ﬁnite sets of hypotheses of the form t type,
where t is a type name. The type formation judgment is inductively deﬁned by the following rules:
∆, t type ⊢t type
(15.7a)

15.2 Statics
131
∆⊢unit type
(15.7b)
∆⊢τ1 type
∆⊢τ2 type
∆⊢prod(τ1; τ2) type
(15.7c)
∆⊢void type
(15.7d)
∆⊢τ1 type
∆⊢τ2 type
∆⊢sum(τ1; τ2) type
(15.7e)
∆⊢τ1 type
∆⊢τ2 type
∆⊢arr(τ1; τ2) type
(15.7f)
∆, t type ⊢τ type
∆⊢t.τ pos
∆⊢ind(t.τ) type
(15.7g)
∆, t type ⊢τ type
∆⊢t.τ pos
∆⊢coi(t.τ) type
(15.7h)
15.2.2
Expressions
The abstract syntax of M is given by the following grammar:
Exp
e
::=
fold{t.τ}(e)
foldt.τ(e)
constructor
rec{t.τ}(x.e1; e2)
rec(x.e1; e2)
recursor
unfold{t.τ}(e)
unfoldt.τ(e)
destructor
gen{t.τ}(x.e1; e2)
gen(x.e1; e2)
generator
The subscripts on the concrete syntax forms are often omitted when they are clear from context.
The statics for M is given by the following typing rules:
Γ ⊢e : [ind(t.τ)/t]τ
Γ ⊢fold{t.τ}(e) : ind(t.τ)
(15.8a)
Γ, x : [τ′/t]τ ⊢e1 : τ′
Γ ⊢e2 : ind(t.τ)
Γ ⊢rec{t.τ}(x.e1; e2) : τ′
(15.8b)
Γ ⊢e : coi(t.τ)
Γ ⊢unfold{t.τ}(e) : [coi(t.τ)/t]τ
(15.8c)
Γ ⊢e2 : τ2
Γ, x : τ2 ⊢e1 : [τ2/t]τ
Γ ⊢gen{t.τ}(x.e1; e2) : coi(t.τ)
(15.8d)

132
15.3 Dynamics
15.3
Dynamics
The dynamics of M is given in terms of the positive generic extension operation described in
Chapter 14. The following rules specify a lazy dynamics for M:
fold{t.τ}(e) val
(15.9a)
e2 7−→e′
2
rec{t.τ}(x.e1; e2) 7−→rec{t.τ}(x.e1; e′
2)
(15.9b)
rec{t.τ}(x.e1; fold{t.τ}(e2))
7−→
[map+{t.τ}(y.rec{t.τ}(x.e1; y))(e2)/x]e1
(15.9c)
gen{t.τ}(x.e1; e2) val
(15.9d)
e 7−→e′
unfold{t.τ}(e) 7−→unfold{t.τ}(e′)
(15.9e)
unfold{t.τ}(gen{t.τ}(x.e1; e2))
7−→
map+{t.τ}(y.gen{t.τ}(x.e1; y))([e2/x]e1)
(15.9f)
Rule (15.9c) states that to evaluate the recursor on a value of recursive type, we inductively apply
the recursor as guided by the type operator to the value, and then apply the inductive step to the
result. Rule (15.9f) is simply the dual of this rule for coinductive types.
Lemma 15.1. If e : τ and e 7−→e′, then e′ : τ.
Proof. By rule induction on rules (15.9).
Lemma 15.2. If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. By rule induction on rules (15.8).
Although a proof of this fact lies beyond our current reach, all programs in M terminate.
Theorem 15.3 (Termination for M). If e : τ, then there exists e′ val such that e 7−→∗e′.
It may, at ﬁrst, seem surprising that a language with inﬁnite data structures, such as streams,
can enjoy such a termination property. But bear in mind that inﬁnite data structures, such as
streams, are represented as in a continuing state of creation, and not as a completed whole.

15.4 Solving Type Equations
133
15.4
Solving Type Equations
For a positive type operator t.τ, we may say that the inductive type µ(t.τ) and the coinductive
type ν(t.τ) are both solutions (up to isomorphism) of the type equation t ∼= τ:
µ(t.τ) ∼= [µ(t.τ)/t]τ
ν(t.τ) ∼= [ν(t.τ)/t]τ.
Intuitively speaking, this means that every value of an inductive type is the folding of a value of
the unfolding of the inductive type, and that, similarly, every value of the unfolding of a coinduc-
tive type is the unfolding of a value of the coinductive type itself. It is a good exercise to deﬁne
functions back and forth between the isomorphic types, and to convince yourself informally that
they are mutually inverse to one another.
Whereas both are solutions to the same type equation, they are not isomorphic to each other.
To see why, consider the inductive type nat ≜µ(t.unit + t) and the coinductive type conat ≜
ν(t.unit + t). Informally, nat is the smallest (most restrictive) type containing zero, given by
fold(l · ⟨⟩), and closed under formation of the successor of any other e of type nat, given by
fold(r · e). Dually, conat is the largest (most permissive) type of expressions e for which the un-
folding, unfold.(e), is either zero, given by l · ⟨⟩, or to the successor of some other e′ of type conat,
given by r · e′.
Because nat is deﬁned by the composition of its introduction forms and sum injections, it is
clear that only ﬁnite natural numbers can be constructed in ﬁnite time. Because conat is deﬁned
by the composition of its elimination forms (unfoldings plus case analyses), it is clear that a co-
natural number can only be explored to ﬁnite depth in ﬁnite time—essentially we can only exam-
ine some ﬁnite number of predecessors of a given co-natural number in a terminating program.
Consequently,
1. there is a function i : nat →conat embedding every ﬁnite natural number into the type of
possibly inﬁnite natural numbers; and
2. there is an “actually inﬁnite” co-natural number ω that is essentially an inﬁnite composition
of successors.
Deﬁning the embedding of nat into conat is the subject of Exercise 15.1. The inﬁnite co-natural
number ω is deﬁned as follows:
ω ≜gen(x.r · x; ⟨⟩).
One may check that unfold.(ω) 7−→∗r · ω, which means that ω is its own predecessor. The
co-natural number ω is larger than any ﬁnite natural number in that any ﬁnite number of prede-
cessors of ω is non-zero.
Summing up, the mere fact of being a solution to a type equation does not uniquely character-
ize a type: there can be many different solutions to the same type equation, the natural and the
co-natural numbers being good examples of the discrepancy. However, we will show in Part VIII
that type equations have unique solutions (up to isomorphism), and that the restriction to polyno-
mial type operators is no longer required. The price we pay for the additional expressive power is
that programs are no longer guaranteed to terminate.

134
15.5 Notes
15.5
Notes
The language M is named after Mendler, on whose work the present treatment is based (Mendler,
1987). Mendler’s work is grounded in category theory, speciﬁcally the concept of an algebra for
a functor (MacLane, 1998; Taylor, 1999). The functorial action of a type constructor (described in
Chapter 14) plays a central role. Inductive types are initial algebras and coinductive types are ﬁnal
coalgebras for the functor given by a (polynomial or positive) type operator.
Exercises
15.1. Deﬁne a function i : nat →conat that sends every natural number to “itself” in the sense
that every ﬁnite natural number is sent to its correlate as a co-natural number.
(a) unfold.(i(z)) 7−→∗l · ⟨⟩.
(b) unfold.(i(s(n))) 7−→∗r · i(n).
15.2. Derive the iterator, iter e {z ,→e0 | s(x) ,→e1}, described in Chapter 9 from the recursor for
the inductive type of natural numbers given in Section 15.1.
15.3. Derive the stream generator, strgen x is e in <hd ,→e1,tl ,→e2> from the generator for the
coinductive stream type given in Section 15.1.
15.4. Consider the type seq ≜nat →nat of inﬁnite sequences of natural numbers. Every stream
can be turned into a sequence by the following function:
λ (stream : s) λ (n : nat) hd(iter n {z ,→s | s(x) ,→tl(x)}).
Show that every sequence can be turned into a stream whose nth element is the nth element
of the given sequence.
15.5. The type of lists of natural numbers is deﬁned by the following introduction and elimination
forms:
Γ ⊢nil : natlist
(15.10a)
Γ ⊢e1 : nat
Γ ⊢e2 : natlist
Γ ⊢cons(e1; e2) : natlist
(15.10b)
Γ ⊢e : natlist
Γ ⊢e0 : τ
Γ x : nat y : τ ⊢e1 : τ
Γ ⊢rec e {nil ,→e0 | cons(x; y) ,→e1} : τ
(15.10c)
The associated dynamics, whether eager or lazy, can be derived from that of the recursor for
the type nat given in Chapter 9. Give a deﬁnition of natlist as an inductive type, including
the deﬁnitions of its associated introduction and elimination forms. Check that they validate
the expected dynamics.

15.5 Notes
135
15.6. Consider the type itree of possibly inﬁnite binary trees with the following introduction and
elimination forms:
Γ ⊢e : itree
Γ ⊢view(e) : (itree × itree) opt
(15.11a)
Γ ⊢e : τ
Γ x : τ ⊢e′ : (τ × τ) opt
Γ ⊢itgen x is e in e′ : itree
(15.11b)
Because a possibly inﬁnite tree must be in a state of continual generation, viewing a tree
exposes only its top-level structure, an optional pair of possibly inﬁnite trees.1 If the view
is null, the tree is empty, and if it is just(e1)e2, then it is non-empty, with children given
by e1 and e2. To generate an inﬁnite tree, choose a type τ of its state of generation, and
provide its current state e and a state transformation e′ that, when applied to the current
state, announces whether or not generation is complete, and, if not, provides the state for
each of the children.
(a) Give a precise dynamics for the itree operations as just described informally. Hint: use
generic programming!
(b) Reformulate the type itree as a coinductive type, and derive the statics and dynamics
of its introduction and elimination forms.
15.7. Exercise 11.5 asked you to deﬁne an RS latch as a signal transducer, in which signals are
expressed explicitly as functions of time. Here you are asked again to deﬁne an RS latch
as a signal transducer, but this time with signals expressed as streams of booleans. Under
such a representation time is implicitly represented by the successive elements of the stream.
Deﬁne an RS latch as a transducer of signals consisting of pairs of booleans.
1See Chapter 11 for the deﬁnition of option types.

136
15.5 Notes

Part VII
Variable Types


Chapter 16
System F of Polymorphic Types
The languages we have considered so far are all monomorphic in that every expression has a unique
type, given the types of its free variables, if it has a type at all. Yet it is often the case that essentially
the same behavior is required, albeit at several different types. For example, in T there is a distinct
identity function for each type τ, namely λ (x : τ) x, even though the behavior is the same for each
choice of τ. Similarly, there is a distinct composition operator for each triple of types, namely
◦τ1,τ2,τ3 = λ ( f : τ2 →τ3) λ (g : τ1 →τ2) λ (x : τ1) f (g(x)).
Each choice of the three types requires a different program, even though they all have the same
behavior when executed.
Obviously it would be useful to capture the pattern once and for all, and to instantiate this
pattern each time we need it. The expression patterns codify generic (type-independent) behaviors
that are shared by all instances of the pattern. Such generic expressions are polymorphic. In this
chapter we will study the language F, which was introduced by Girard under the name System F
and by Reynolds under the name polymorphic typed λ-calculus. Although motivated by a simple
practical problem (how to avoid writing redundant code), the concept of polymorphism is central
to an impressive variety of seemingly disparate concepts, including the concept of data abstraction
(the subject of Chapter 17), and the deﬁnability of product, sum, inductive, and coinductive types
considered in the preceding chapters. (Only general recursive types extend the expressive power
of the language.)

140
16.1 Polymorphic Abstraction
16.1
Polymorphic Abstraction
The language F is a variant of T in which we eliminate the type of natural numbers, but add, in
compensation, polymorphic types:1
Typ
τ
::=
t
t
variable
arr(τ1; τ2)
τ1 →τ2
function
all(t.τ)
∀(t.τ)
polymorphic
Exp
e
::=
x
x
lam{τ}(x.e)
λ (x : τ) e
abstraction
ap(e1; e2)
e1(e2)
application
Lam(t.e)
Λ(t) e
type abstraction
App{τ}(e)
e[τ]
type application
A type abstraction Lam(t.e) deﬁnes a generic, or polymorphic, function with type variable t standing for
an unspeciﬁed type within e. A type application, or instantiation App{τ}(e) applies a polymorphic
function to a speciﬁed type, which is plugged in for the type variable to obtain the result. The
universal type, all(t.τ), classiﬁes polymorphic functions.
The statics of F consists of two judgment forms, the type formation judgment,
∆⊢τ type,
and the typing judgment,
∆Γ ⊢e : τ.
The hypotheses ∆have the form t type, where t is a variable of sort Typ, and the hypotheses Γ have
the form x : τ, where x is a variable of sort Exp.
The rules deﬁning the type formation judgment are as follows:
∆, t type ⊢t type
(16.1a)
∆⊢τ1 type
∆⊢τ2 type
∆⊢arr(τ1; τ2) type
(16.1b)
∆, t type ⊢τ type
∆⊢all(t.τ) type
(16.1c)
The rules deﬁning the typing judgment are as follows:
∆Γ, x : τ ⊢x : τ
(16.2a)
∆⊢τ1 type
∆Γ, x : τ1 ⊢e : τ2
∆Γ ⊢lam{τ1}(x.e) : arr(τ1; τ2)
(16.2b)
∆Γ ⊢e1 : arr(τ2; τ)
∆Γ ⊢e2 : τ2
∆Γ ⊢ap(e1; e2) : τ
(16.2c)
1Girard’s original version of System F included the natural numbers as a basic type.

16.1 Polymorphic Abstraction
141
∆, t type Γ ⊢e : τ
∆Γ ⊢Lam(t.e) : all(t.τ)
(16.2d)
∆Γ ⊢e : all(t.τ′)
∆⊢τ type
∆Γ ⊢App{τ}(e) : [τ/t]τ′
(16.2e)
Lemma 16.1 (Regularity). If ∆Γ ⊢e : τ, and if ∆⊢τi type for each assumption xi : τi in Γ, then
∆⊢τ type.
Proof. By induction on rules (16.2).
The statics admits the structural rules for a general hypothetical judgment. In particular, we
have the following critical substitution property for type formation and expression typing.
Lemma 16.2 (Substitution).
1. If ∆, t type ⊢τ′ type and ∆⊢τ type, then ∆⊢[τ/t]τ′ type.
2. If ∆, t type Γ ⊢e′ : τ′ and ∆⊢τ type, then ∆[τ/t]Γ ⊢[τ/t]e′ : [τ/t]τ′.
3. If ∆Γ, x : τ ⊢e′ : τ′ and ∆Γ ⊢e : τ, then ∆Γ ⊢[e/x]e′ : τ′.
The second part of the lemma requires substitution into the context Γ as well as into the term
and its type, because the type variable t may occur freely in any of these positions.
Returning to the motivating examples from the introduction, the polymorphic identity func-
tion, I, is written
Λ(t) λ (x : t) x;
it has the polymorphic type
∀(t.t →t).
Instances of the polymorphic identity are written I[τ], where τ is some type, and have the type
τ →τ.
Similarly, the polymorphic composition function, C, is written
Λ(t1) Λ(t2) Λ(t3) λ ( f : t2 →t3) λ (g : t1 →t2) λ (x : t1) f (g(x)).
The function C has the polymorphic type
∀(t1.∀(t2.∀(t3.(t2 →t3) →(t1 →t2) →(t1 →t3)))).
Instances of C are obtained by applying it to a triple of types, written C[τ1][τ2][τ3]. Each such
instance has the type
(τ2 →τ3) →(τ1 →τ2) →(τ1 →τ3).

142
16.1 Polymorphic Abstraction
Dynamics
The dynamics of F is given as follows:
lam{τ}(x.e) val
(16.3a)
Lam(t.e) val
(16.3b)
[e2 val]
ap(lam{τ1}(x.e); e2) 7−→[e2/x]e
(16.3c)
e1 7−→e′
1
ap(e1; e2) 7−→ap(e′
1; e2)
(16.3d)
"
e1 val
e2 7−→e′
2
ap(e1; e2) 7−→ap(e1; e′
2)
#
(16.3e)
App{τ}(Lam(t.e)) 7−→[τ/t]e
(16.3f)
e 7−→e′
App{τ}(e) 7−→App{τ}(e′)
(16.3g)
The bracketed premises and rule are included for a call-by-value interpretation, and omitted for a
call-by-name interpretation of F.
It is a simple matter to prove safety for F, using familiar methods.
Lemma 16.3 (Canonical Forms). Suppose that e : τ and e val, then
1. If τ = arr(τ1; τ2), then e = lam{τ1}(x.e2) with x : τ1 ⊢e2 : τ2.
2. If τ = all(t.τ′), then e = Lam(t.e′) with t type ⊢e′ : τ′.
Proof. By rule induction on the statics.
Theorem 16.4 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. By rule induction on the dynamics.
Theorem 16.5 (Progress). If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. By rule induction on the statics.

16.2 Polymorphic Deﬁnability
143
16.2
Polymorphic Deﬁnability
The language F is astonishingly expressive. Not only are all (lazy) ﬁnite products and sums de-
ﬁnable in the language, but so are all (lazy) inductive and coinductive types. Their deﬁnability is
most naturally expressed using deﬁnitional equality, which is the least congruence containing the
following two axioms:
∆Γ, x : τ1 ⊢e2 : τ2
∆Γ ⊢e1 : τ1
∆Γ ⊢(λ (x : τ) e2)(e1) ≡[e1/x]e2 : τ2
(16.4a)
∆, t type Γ ⊢e : τ
∆⊢ρ type
∆Γ ⊢Λ(t) e[ρ] ≡[ρ/t]e : [ρ/t]τ
(16.4b)
In addition there are rules omitted here specifying that deﬁnitional equality is a congruence rela-
tion (that is, an equivalence relation respected by all expression-forming operations).
16.2.1
Products and Sums
The nullary product, or unit, type is deﬁnable in F as follows:
unit ≜∀(r.r →r)
⟨⟩≜Λ(r) λ (x : r) x
The identity function plays the role of the null tuple, because it is the only closed value of this
type.
Binary products are deﬁnable in F by using encoding tricks similar to those described in Chap-
ter 21 for the untyped λ-calculus:
τ1 × τ2 ≜∀(r.(τ1 →τ2 →r) →r)
⟨e1, e2⟩≜Λ(r) λ (x : τ1 →τ2 →r) x(e1)(e2)
e · l ≜e[τ1](λ (x : τ1) λ (y : τ2) x)
e · r ≜e[τ2](λ (x : τ1) λ (y : τ2) y)
The statics given in Chapter 10 is derivable according to these deﬁnitions. Moreover, the following
deﬁnitional equalities are derivable in F from these deﬁnitions:
⟨e1, e2⟩· l ≡e1 : τ1
and
⟨e1, e2⟩· r ≡e2 : τ2.
The nullary sum, or void, type is deﬁnable in F:
void ≜∀(r.r)
abort{ρ}(e) ≜e[ρ]

144
16.2 Polymorphic Deﬁnability
Binary sums are also deﬁnable in F:
τ1 + τ2 ≜∀(r.(τ1 →r) →(τ2 →r) →r)
l · e ≜Λ(r) λ (x : τ1 →r) λ (y : τ2 →r) x(e)
r · e ≜Λ(r) λ (x : τ1 →r) λ (y : τ2 →r) y(e)
case e {l · x1 ,→e1 | r · x2 ,→e2} ≜
e[ρ](λ (x1 : τ1) e1)(λ (x2 : τ2) e2)
provided that the types make sense. It is easy to check that the following equivalences are deriv-
able in F:
case l · d1 {l · x1 ,→e1 | r · x2 ,→e2} ≡[d1/x1]e1 : ρ
and
case r · d2 {l · x1 ,→e1 | r · x2 ,→e2} ≡[d2/x2]e2 : ρ.
Thus the dynamic behavior speciﬁed in Chapter 11 is correctly implemented by these deﬁnitions.
16.2.2
Natural Numbers
As we remarked above, the natural numbers (under a lazy interpretation) are also deﬁnable in F.
The key is the iterator, whose typing rule we recall here for reference:
e0 : nat
e1 : τ
x : τ ⊢e2 : τ
iter{e1; x.e2}(e0) : τ
.
Because the result type τ is arbitrary, this means that if we have an iterator, then we can use it to
deﬁne a function of type
nat →∀(t.t →(t →t) →t).
This function, when applied to an argument n, yields a polymorphic function that, for any result
type, t, given the initial result for z and a transformation from the result for x into the result for
s(x), yields the result of iterating the transformation n times, starting with the initial result.
Because the only operation we can perform on a natural number is to iterate up to it, we may
simply identify a natural number, n, with the polymorphic iterate-up-to-n function just described.
Thus we may deﬁne the type of natural numbers in F by the following equations:
nat ≜∀(t.t →(t →t) →t)
z ≜Λ(t) λ (z : t) λ (s : t →t) z
s(e) ≜Λ(t) λ (z : t) λ (s : t →t) s(e[t](z)(s))
iter{e1; x.e2}(e0) ≜e0[τ](e1)(λ (x : τ) e2)
It is easy to check that the statics and dynamics of the natural numbers type given in Chapter 9
are derivable in F under these deﬁnitions. The representations of the numerals in F are called the
polymorphic Church numerals.

16.3 Parametricity Overview
145
The encodability of the natural numbers shows that F is at least as expressive as T. But is it more
expressive? Yes! It is possible to show that the evaluation function for T is deﬁnable in F, even
though it is not deﬁnable in T itself. However, the same diagonal argument given in Chapter 9
applies here, showing that the evaluation function for F is not deﬁnable in F. We may enrich F a bit
more to deﬁne the evaluator for F, but as long as all programs in the enriched language terminate,
we will once again have an undeﬁnable function, the evaluation function for that extension.
16.3
Parametricity Overview
A remarkable property of F is that polymorphic types severely constrain the behavior of their
elements. We may prove useful theorems about an expression knowing only its type—that is,
without ever looking at the code. For example, if i is any expression of type ∀(t.t →t), then it
is the identity function. Informally, when i is applied to a type, τ, and an argument of type τ, it
returns a value of type τ. But because τ is not speciﬁed until i is called, the function has no choice
but to return its argument, which is to say that it is essentially the identity function. Similarly, if
b is any expression of type ∀(t.t →t →t), then b is equivalent to either Λ(t) λ (x : t) λ (y : t) x or
Λ(t) λ (x : t) λ (y : t) y. Intuitively, when b is applied to two arguments of a given type, the only
value it can return is one of the givens.
Properties of a program in F that can be proved knowing only its type are called parametricity
properties. The facts about the functions i and b stated above are examples of parametricity prop-
erties. Such properties are sometimes called “free theorems,” because they come from typing “for
free”, without any knowledge of the code itself. It bears repeating that in F we prove non-trivial
behavioral properties of programs without ever examining the program text. The key to this in-
credible fact is that we are able to prove a deep property, called parametricity, about the language F,
that then applies to every program written in F. One may say that the type system “pre-veriﬁes”
programs with respect to a broad range of useful properties, eliminating the need to prove those
properties about every program separately. The parametricity theorem for F explains the remark-
able experience that if a piece of code type checks, then it “just works.” Parametricity narrows the
space of well-typed programs sufﬁciently that the opportunities for programmer error are reduced
to almost nothing.
So how does the parametricity theorem work? Without getting into too many technical details
(but see Chapter 48 for a full treatment), we can give a brief summary of the main idea. Any
function i : ∀(t.t →t) in F enjoys the following property:
For any type τ and any property P of the type τ, then if P holds of x : τ, then P holds of
i[τ](x).
To show that for any type τ, and any x of type τ, the expression i[τ](x) is equivalent to x, it sufﬁces
to ﬁx x0 : τ, and consider the property Px0 that holds of y : τ iff y is equivalent to x0. Obviously P
holds of x0 itself, and hence by the above-displayed property of i, it sends any argument satisfying
Px0 to a result satisfying Px0, which is to say that it sends x0 to x0. Because x0 is an arbitrary
element of τ, it follows that i[τ] is the identity function, λ (x : τ) x, on the type τ, and because τ is
itself arbitrary, i is the polymorphic identity function, Λ(t) λ (x : t) x.

146
16.4 Notes
A similar argument sufﬁces to show that the function b, deﬁned above, is either Λ(t) λ (x : t) λ (y : t) x
or Λ(t) λ (x : t) λ (y : t) y. By virtue of its type the function b enjoys the parametricity property
For any type τ and any property P of τ, if P holds of x : τ and of y : τ, then P holds of
b[τ](x)(y).
Choose an arbitrary type τ and two arbitrary elements x0 and y0 of type τ. Deﬁne Qx0,y0 to hold
of z : τ iff either z is equivalent to x0 or z is equivalent to y0. Clearly Qx0,y0 holds of both x0
and y0 themselves, so by the quoted parametricity property of b, it follows that Qx0,y0 holds of
b[τ](x0)(y0), which is to say that it is equivalent to either x0 or y0. Since τ, x0, and y0 are arbitrary,
it follows that b is equivalent to either Λ(t) λ (x : t) λ (y : t) x or Λ(t) λ (x : t) λ (y : t) y.
The parametricity theorem for F implies even stronger properties of functions such as i and
b considered above. For example, the function i of type ∀(t.t →t) also satisﬁes the following
condition:
If τ and τ′ are any two types, and R is a binary relation between τ and τ′, then for any x : τ
and x′ : τ′, if R relates x to x′, then R relates i[τ](x) to i[τ′](x′).
Using this property we may again prove that i is equivalent to the polymorphic identity function.
Speciﬁcally, if τ is any type and g : τ →τ is any function on that type, then it follows from the
type of i alone that i[τ](g(x)) is equivalent to g(i[τ](x)) for any x : τ. To prove this, simply choose
R to the be graph of the function g, the relation Rg that holds of x and x′ iff x′ is equivalent to
g(x). The parametricity property of i, when specialized to Rg, states that if x′ is equivalent to g(x),
then i[τ](x′) is equivalent to g(i[τ](x)), which is to say that i[τ](g(x)) is equivalent to g(i[τ](x)).
To show that i is equivalent to the identity function, choose x0 : τ arbitrarily, and consider the
constant function g0 on τ that always returns x0. Because x0 is equivalent to g0(x0), it follows that
i[τ](x0) is equivalent to x0, which is to say that i behaves like the polymorphic identity function.
16.4
Notes
System F was introduced by Girard (1972) in the context of proof theory and by Reynolds (1974)
in the context of programming languages. The concept of parametricity was originally isolated
by Strachey, but was not fully developed until the work of Reynolds (1983). The phrase “free
theorems” for parametricity theorems was introduced by Wadler (1989).
Exercises
16.1. Give polymorphic deﬁnitions and types to the s and k combinators deﬁned in Exercise 3.1.
16.2. Deﬁne in F the type bool of Church booleans. Deﬁne the type bool, and deﬁne true and false
of this type, and the conditional if e then e0 else e1, where e is of this type.
16.3. Deﬁne in F the inductive type of lists of natural numbers as deﬁned in Chapter 15. Hint:
Deﬁne the representation in terms of the recursor (elimination form) for lists, following the
pattern for deﬁning the type of natural numbers.

16.4 Notes
147
16.4. Deﬁne in F an arbitrary inductive type, µ(t.τ). Hint: generalize your answer to Exercise 16.3.
16.5. Deﬁne the type t list as in Exercise 16.3, with the element type, t, unspeciﬁed. Deﬁne the
ﬁnite set of elements of a list l to be those x given by the head of some number of tails of l.
Now suppose that f : ∀(t.t list →t list) is an arbitrary function of the stated type. Show
that the elements of f [τ](l) are a subset of those of l. Thus f may only permute, replicate, or
drop elements from its input list to obtain its output list.

148
16.4 Notes

Chapter 17
Abstract Types
Data abstraction is perhaps the most important technique for structuring programs. The main
idea is to introduce an interface that serves as a contract between the client and the implementor
of an abstract type. The interface speciﬁes what the client may rely on for its own work, and,
simultaneously, what the implementor must provide to satisfy the contract. The interface serves to
isolate the client from the implementor so that each may be developed in isolation from the other.
In particular one implementation can be replaced by another without affecting the behavior of the
client, provided that the two implementations meet the same interface and that each simulates
the other with respect to the operations of the interface. This property is called representation
independence for an abstract type.
Data abstraction is formalized by extending the language F with existential types. Interfaces
are existential types that provide a collection of operations acting on an unspeciﬁed, or abstract,
type. Implementations are packages, the introduction form for existential types, and clients are
uses of the corresponding elimination form. It is remarkable that the programming concept of
data abstraction is captured so naturally and directly by the logical concept of existential type
quantiﬁcation. Existential types are closely connected with universal types, and hence are often
treated together. The superﬁcial reason is that both are forms of type quantiﬁcation, and hence
both require the machinery of type variables. The deeper reason is that existential types are de-
ﬁnable from universals — surprisingly, data abstraction is actually just a form of polymorphism!
Consequently, representation independence is an application of the parametricity properties of
polymorphic functions discussed in Chapter 16.
17.1
Existential Types
The syntax of FE extends F with the following constructs:
Typ
τ
::=
some(t.τ)
∃(t.τ)
interface
Exp
e
::=
pack{t.τ}{ρ}(e)
pack ρ with e as ∃(t.τ)
implementation
open{t.τ}{ρ}(e1; t, x.e2)
open e1 as t with x:τ in e2
client

150
17.1 Existential Types
The introduction form ∃(t.τ) is a package of the form pack ρ with e as ∃(t.τ), where ρ is a type
and e is an expression of type [ρ/t]τ. The type ρ is the representation type of the package, and
the expression e is the implementation of the package.
The elimination form is the expression
open e1 as t with x:τ in e2, which opens the package e1 for use within the client e2 by binding its
representation type to t and its implementation to x for use within e2. Crucially, the typing rules
ensure that the client is type-correct independently of the actual representation type used by the
implementor, so that it can be varied without affecting the type correctness of the client.
The abstract syntax of the open construct speciﬁes that the type variable t and the expression
variable x are bound within the client. They may be renamed at will by α-equivalence without
affecting the meaning of the construct, provided, of course, that the names do not conﬂict with
any others in scope. In other words the type t is a “new” type, one that is distinct from all other
types, when it is introduced. This principle is sometimes called generativity of abstract types: the
use of an abstract type by a client “generates” a “new” type within that client. This behavior relies
on the identiﬁcation covnention stated in Chapter 1.
17.1.1
Statics
The statics of FE is given by these rules:
∆, t type ⊢τ type
∆⊢some(t.τ) type
(17.1a)
∆⊢ρ type
∆, t type ⊢τ type
∆Γ ⊢e : [ρ/t]τ
∆Γ ⊢pack{t.τ}{ρ}(e) : some(t.τ)
(17.1b)
∆Γ ⊢e1 : some(t.τ)
∆, t type Γ, x : τ ⊢e2 : τ2
∆⊢τ2 type
∆Γ ⊢open{t.τ}{τ2}(e1; t, x.e2) : τ2
(17.1c)
Rule (17.1c) is complex, so study it carefully! There are two important things to notice:
1. The type of the client, τ2, must not involve the abstract type t. This restriction prevents
the client from attempting to export a value of the abstract type outside of the scope of its
deﬁnition.
2. The body of the client, e2, is type checked without knowledge of the representation type, t.
The client is, in effect, polymorphic in the type variable t.
Lemma 17.1 (Regularity). Suppose that ∆Γ ⊢e : τ. If ∆⊢τi type for each xi : τi in Γ, then ∆⊢τ type.
Proof. By induction on rules (17.1), using substitution for expressions and types.
17.1.2
Dynamics
The dynamics of FE is deﬁned by the following rules (including the bracketed material for an
eager interpretation, and omitting it for a lazy interpretation):
[e val]
pack{t.τ}{ρ}(e) val
(17.2a)

17.2 Data Abstraction
151

e 7−→e′
pack{t.τ}{ρ}(e) 7−→pack{t.τ}{ρ}(e′)

(17.2b)
e1 7−→e′
1
open{t.τ}{τ2}(e1; t, x.e2) 7−→open{t.τ}{τ2}(e′
1; t, x.e2)
(17.2c)
[e val]
open{t.τ}{τ2}(pack{t.τ}{ρ}(e); t, x.e2) 7−→[ρ, e/t, x]e2
(17.2d)
It is important to see that, according to these rules, there are no abstract types at run time! The rep-
resentation type is propagated to the client by substitution when the package is opened, thereby
eliminating the abstraction boundary between the client and the implementor. Thus, data abstrac-
tion is a compile-time discipline that leaves no traces of its presence at execution time.
17.1.3
Safety
Safety of FE is stated and proved by decomposing it into progress and preservation.
Theorem 17.2 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. By rule induction on e 7−→e′, using substitution for both expression- and type variables.
Lemma 17.3 (Canonical Forms). If e : some(t.τ) and e val, then e = pack{t.τ}{ρ}(e′) for some type ρ
and some e′ such that e′ : [ρ/t]τ.
Proof. By rule induction on the statics, using the deﬁnition of closed values.
Theorem 17.4 (Progress). If e : τ then either e val or there exists e′ such that e 7−→e′.
Proof. By rule induction on e : τ, using the canonical forms lemma.
17.2
Data Abstraction
To illustrate the use of FE, we consider an abstract type of queues of natural numbers supporting
three operations:
1. Forming the empty queue.
2. Inserting an element at the tail of the queue.
3. Removing the head of the queue, which is assumed non-empty.
This is clearly a bare-bones interface, but sufﬁces to illustrate the main ideas of data abstraction.
Queue elements are natural numbers, but nothing depends on this choice.
The crucial property of this description is that nowhere do we specify what queues actually
are, only what we can do with them. The behavior of a queue is expressed by the existential type
∃(t.τ) which serves as the interface of the queue abstraction:
∃(t.⟨emp ,→t, ins ,→nat × t →t, rem ,→t →(nat × t) opt⟩).

152
17.3 Deﬁnability of Existential Types
The representation type t of queues is abstract — all that is known about it is that it supports the
operations emp, ins, and rem, with the given types.
An implementation of queues consists of a package specifying the representation type, together
with the implementation of the associated operations in terms of that representation. Internally
to the implementation, the representation of queues is known and relied upon by the operations.
Here is a very simple implementation el in which queues are represented as lists:
pack natlist with ⟨emp ,→nil, ins ,→ei, rem ,→er⟩as ∃(t.τ),
where
ei : nat × natlist →natlist = λ (x : nat × natlist) . . .,
and
er : natlist →nat × natlist = λ (x : natlist) . . ..
The elided body of ei conses the ﬁrst component of x, the element, onto the second component of
x, the queue, and the elided body of er reverses its argument, and returns the head element paired
with the reversal of the tail. Both of these operations “know” that queues are represented as values
of type natlist, and are programmed accordingly.
It is also possible to give another implementation ep of the same interface ∃(t.τ), but in which
queues are represented as pairs of lists, consisting of the “back half” of the queue paired with the
reversal of the “front half”. This two-part representation avoids the need for reversals on each call,
and, as a result, achieves amortized constant-time behavior:
pack natlist × natlist with ⟨emp ,→⟨nil, nil⟩, ins ,→ei, rem ,→er⟩as ∃(t.τ).
In this case ei has type
nat × (natlist × natlist) →(natlist × natlist),
and er has type
(natlist × natlist) →nat × (natlist × natlist).
These operations “know” that queues are represented as values of type natlist × natlist, and
are implemented accordingly.
The important point is that the same client type checks regardless of which implementation
of queues we choose, because the representation type is hidden, or held abstract, from the client
during type checking. Consequently, it cannot rely on whether it is natlist or natlist × natlist
or some other type. That is, the client is independent of the representation of the abstract type.
17.3
Deﬁnability of Existential Types
The language FE is not a proper extension of F, because existential types (under a lazy dynamics)
are deﬁnable in terms of universal types. Why should this be possible? Note that the client of an
abstract type is polymorphic in the representation type. The typing rule for
open e1 as t with x:τ in e2 : τ2,

17.4 Representation Independence
153
where e1 : ∃(t.τ), speciﬁes that e2 : τ2 under the assumptions t type and x : τ. In essence, the client
is a polymorphic function of type
∀(t.τ →τ2),
where t may occur in τ (the type of the operations), but not in τ2 (the type of the result).
This suggests the following encoding of existential types:
∃(t.τ) ≜∀(u.∀(t.τ →u) →u)
pack ρ with e as ∃(t.τ) ≜Λ(u) λ (x : ∀(t.τ →u)) x[ρ](e)
open e1 as t with x:τ in e2 ≜e1[τ2](Λ(t) λ (x : τ) e2)
An existential is encoded as a polymorphic function taking the overall result type u as argument,
followed by a polymorphic function representing the client with result type u, and yielding a
value of type u as overall result. Consequently, the open construct simply packages the client as
such a polymorphic function, instantiates the existential at the result type, τ2, and applies it to the
polymorphic client. (The translation therefore depends on knowing the overall result type τ2 of
the open construct.) Finally, a package consisting of a representation type ρ and an implementation
e is a polymorphic function that, when given the result type u and the client x instantiates x with
ρ and passes to it the implementation e.
17.4
Representation Independence
An important consequence of parametricity is that it ensures that clients are insensitive to the
representations of abstract types. More precisely, there is a criterion, bisimilarity, for relating two
implementations of an abstract type such that the behavior of a client is unaffected by swapping
one implementation by another that is bisimilar to it. This principle leads to a simple method for
proving the correctness of candidate implementation of an abstract type, which is to show that it
is bisimilar to an obviously correct reference implementation of it. Because the candidate and the
reference implementations are bisimilar, no client may distinguish them from one another, and
hence if the client behaves properly with the reference implementation, then it must also behave
properly with the candidate.
To derive the deﬁnition of bisimilarity of implementations, it is helpful to examine the deﬁni-
tion of existential types in terms of universals given in Section 17.3. It is immediately clear that
the client of an abstract type is polymorphic in the representation of the abstract type. A client
c of an abstract type ∃(t.τ) has type ∀(t.τ →τ2), where t does not occur free in τ2 (but may, of
course, occur in τ). Applying the parametricity property described informally in Chapter 16 (and
developed rigorously in Chapter 48), this says that if R is a bisimulation relation between any two
implementations of the abstract type, then the client behaves identically on them. The fact that t
does not occur in the result type ensures that the behavior of the client is independent of the choice
of relation between the implementations, provided that this relation is preserved by the operations
that implement it.

154
17.4 Representation Independence
Explaining what is a bisimulation is best done by example. Consider the existential type ∃(t.τ),
where τ is the labeled tuple type
⟨emp ,→t, ins ,→nat × t →t, rem ,→t →(nat × t) opt⟩.
This speciﬁes an abstract type of queues. The operations emp, ins, and rem specify, respectively, the
empty queue, an insert operation, and a remove operation. For the sake of simplicity the element
type is the type of natural numbers. The result of removal is an optional pair, according to whether
the queue is empty or not.
Theorem 48.12 ensures that if ρ and ρ′ are any two closed types, and if R is a relation between
expressions of these two types, then if the implementations e : [ρ/x]τ and e′ : [ρ′/x]τ respect R,
then c[ρ]e behaves the same as c[ρ′]e′. It remains to deﬁne when two implementations respect the
relation R. Let
e ≜⟨emp ,→em, ins ,→ei, rem ,→er⟩
and
e′ ≜⟨emp ,→e′
m, ins ,→e′
i, rem ,→e′
r⟩.
For these implementations to respect R means that the following three conditions hold:
1. The empty queues are related: R(em, e′m).
2. Inserting the same element on each of two related queues yields related queues: if d : τ and
R(q, q′), then R(ei(d)(q), e′
i(d)(q′)).
3. If two queues are related, then either they are both empty, or their front elements are the
same and their back elements are related: if R(q, q′), then either
(a) er(q) ∼= null ∼= e′r(q′), or
(b) er(q) ∼= just(⟨d, r⟩) and e′r(q′) ∼= just(⟨d′, r′⟩), with d ∼= d′ and R(r, r′).
If such a relation R exists, then the implementations e and e′ are bisimilar. The terminology stems
from the requirement that the operations of the abstract type preserve the relation: if it holds
before an operation is performed, then it must also hold afterwards, and the relation must hold for
the initial state of the queue. Thus each implementation simulates the other up to the relationship
speciﬁed by R.
To see how this works in practice, let us consider informally two implementations of the ab-
stract type of queues deﬁned earlier. For the reference implementation we choose ρ to be the type
natlist, and deﬁne the empty queue to be the empty list, deﬁne insert to add the given element
to the head of the list, and deﬁne remove to remove the last element of the list. The code is as
follows:
t ≜natlist
emp ≜nil
ins ≜λ (x : nat) λ (q : t) cons(x; q)
rem ≜λ (q : t) case rev(q) {nil ,→null | cons( f; qr) ,→just(⟨f, rev(qr)⟩)}.

17.5 Notes
155
Removing an element takes time linear in the length of the list, because of the reversal.
For the candidate implementation we choose ρ′ to be the type natlist × natlist of pairs of
lists ⟨b, f ⟩in which b is the “back half” of the queue, and f is the reversal of the “front half” of the
queue. For this representation we deﬁne the empty queue to be a pair of empty lists, deﬁne insert
to extend the back with that element at the head, and deﬁne remove based on whether the front
is empty or not. If it is non-empty, the head element is removed from it, and returned along with
the pair consisting of the back and the tail of the front. If it is empty, and the back is not, then we
reverse the back, remove the head element, and return the pair consisting of the empty list and the
tail of the now-reversed back. The code is as follows:
t ≜natlist × natlist
emp ≜⟨nil, nil⟩
ins ≜λ (x : nat) λ (⟨bs, f s⟩: t) ⟨cons(x; bs), f s⟩
rem ≜λ (⟨bs, f s⟩: t) case f s {nil ,→e | cons( f; f s′) ,→⟨bs, f s′⟩}, where
e ≜case rev(bs) {nil ,→null | cons(b; bs′) ,→just(⟨b, ⟨nil, bs′⟩⟩)}.
The cost of the occasional reversal is amortized across the sequence of inserts and removes to show
that each operation in a sequence costs unit time overall.
To show that the candidate implementation is correct, we show that it is bisimilar to the
reference implementation.
To do so, we specify a relation R between the types natlist and
natlist × natlist such that the two implementations satisfy the three simulation conditions
given earlier. The required relation states that R(l, ⟨b, f ⟩) iff the list l is the list app(b)(rev( f )),
where app is the evident append function on lists. That is, thinking of l as the reference represen-
tation of the queue, the candidate must ensure that the elements of b followed by the elements of f
in reverse order form precisely the list l. It is easy to check that the implementations just described
preserve this relation. Having done so, we are assured that the client c behaves the same regard-
less of whether we use the reference or the candidate. Because the reference implementation is
obviously correct (albeit inefﬁcient), the candidate must also be correct in that the behavior of any
client is not affected by using it instead of the reference.
17.5
Notes
The connection between abstract types in programming languages and existential types in logic
was made by Mitchell and Plotkin (1988). Closely related ideas were already present in Reynolds
(1974), but the connection with existential types was not explicitly drawn there. The present for-
mulation of representation independence follows closely Mitchell (1986).
Exercises
17.1. Show that the statics and dynamics of existential types are correctly simulated using the
interpretation given in Section 17.3.

156
17.5 Notes
17.2. Deﬁne in FE of the coinductive type of streams of natural numbers as deﬁned in Chapter 15.
Hint: Deﬁne the representation in terms of the generator (introduction form) for streams.
17.3. Deﬁne in FE an arbitrary coinductive type ν(t.τ). Hint: generalize your answer to Exer-
cise 17.2.
17.4. Representation independence for abstract types is a corollary of the parametricity theorem
for polymorphic types, using the interpretation of FE in F given in Section 17.3. Recast
the proof of equivalence of the two implementations of queues given in Section 17.4 as an
instance of parametricity as deﬁned informally in Chapter 16.

Chapter 18
Higher Kinds
The concept of type quantiﬁcation naturally leads to the consideration of quantiﬁcation over type
constructors, such as list, which are functions mapping types to types. For example, the abstract
type of queues of natural numbers considered in Section 17.4 could be generalized to an abstract
type constructor of queues that does not ﬁx the element type. In the notation that we shall develop
in this chapter such an abstraction is expressed by the existential type ∃q :: T →T.σ, where σ is the
labeled tuple type
⟨emp ,→∀t :: T.t, ins ,→∀t :: T.t × q[t] →q[t], rem ,→∀t :: T.q[t] →(t × q[t]) opt⟩.
The existential type quantiﬁes over the kind T →T of type constructors, which map types to types.
The operations are polymorphic, or generic, in the type of the elements of the queue. Their types
involve instances of the abstract queue constructor q[t] representing the abstract type of queues
whose elements are of type t. The client instantiates the polymorphic quantiﬁer to specify the ele-
ment type; the implementations are parametric in this choice (in that their behavior is the same in
any case). A package of the existential type given above consists of a representation type construc-
tor and an implementation of the operations in terms of this choice. Possible representations in-
clude the constructor λ (u :: T) u list and the constructor λ (u :: T) u list × u list, both of which
have kind T →T. It is easy to check that the implementations of the queue operations given in
Section 17.4 carry over to the more general case, almost without change, because they do not rely
on the type of the elements of the queue.
The language Fω enriches the language F with universal and existential quantiﬁcation over
kinds, such as T →T, used in the queues example. The extension accounts for deﬁnitional equality
of constructors. For example, an implementation of the existential given in the preceding para-
graph have to give implementations for the operations in terms of the choice of representation for
q. If, say, q is the constructor λ (u :: T) u list, then the ins operation takes a type argument speci-
fying the element type t and a queue of type (λ (u :: T) u list)[t], which should simplify to t list
by substitution of t for u in the body of the λ-abstraction. Deﬁnitional equality of constructors de-
ﬁnes the permissible rules of simpliﬁcation, and thereby deﬁnes when two types are equal. Equal
types should be interchangeable as classiﬁers, meaning that if e is of type τ and τ is deﬁnitionally

158
18.1 Constructors and Kinds
equal to τ′, then e should also have type τ′. In the queues example any expression of type t list
should also be of the unsimpliﬁed type to which it is deﬁnitionally equal.
18.1
Constructors and Kinds
The syntax of kinds of Fω is given by the following grammar:
Kind
κ
::=
Type
T
types
Unit
1
nullary product
Prod(κ1; κ2)
κ1 × κ2
binary product
Arr(κ1; κ2)
κ1 →κ2
function
The kinds consist of the kind of types T and the unit kind Unit and are closed under formation of
product and function kinds.
The syntax of constructors of Fω is deﬁned by this grammar:
Con
c
::=
u
u
variable
arr
→
function constructor
all{κ}
∀κ
universal quantiﬁer
some{κ}
∃κ
existential quantiﬁer
proj[l](c)
c · l
ﬁrst projection
proj[r](c)
c · r
second projection
app(c1; c2)
c1[c2]
application
unit
⟨⟩
null tuple
pair(c1; c2)
⟨c1,c2⟩
pair
lam(u.c)
λ (u) c
abstraction
The syntax of constructors follows the syntax of kinds in that there are introduction and elimi-
nation forms for all kinds. The constants →, ∀κ, and ∃κ are the introduction forms for the kind
T; there are no elimination forms, because types are only used to classify expressions. We use the
meta-variable τ for constructors of kind T, and write τ1 →τ2 for the application →[τ1][τ2], ∀u :: κ.τ
for ∀κ[λ (u :: κ) τ], and similarly for the existential quantiﬁer.
The statics of constructors and kinds of Fω is speciﬁed by the judgment
∆⊢c :: κ
which states that the constructor c is well-formed with kind κ. The hypotheses ∆consist of a ﬁnite
set of assumptions
u1 :: κ1, . . . , un :: κn,
where n ≥0, specifying the kinds of the active constructor variables.
The statics of constructors is deﬁned by the following rules:
∆, u :: κ ⊢u :: κ
(18.1a)

18.2 Constructor Equality
159
∆⊢→:: T →T →T
(18.1b)
∆⊢∀κ :: (κ →T) →T
(18.1c)
∆⊢∃κ :: (κ →T) →T
(18.1d)
∆⊢c :: κ1 × κ2
∆⊢c · l :: κ1
(18.1e)
∆⊢c :: κ1 × κ2
∆⊢c · r :: κ2
(18.1f)
∆⊢c1 :: κ2 →κ
∆⊢c2 :: κ2
∆⊢c1[c2] :: κ
(18.1g)
∆⊢⟨⟩:: 1
(18.1h)
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩:: κ1 × κ2
(18.1i)
∆, u :: κ1 ⊢c2 :: κ2
∆⊢λ (u) c2 :: κ1 →κ2
(18.1j)
The kinds of the three constants specify that they can be used to build constructors of kind T, the
kind of types, which, as usual, classify expressions.
18.2
Constructor Equality
The rules of deﬁnitional equality for Fω deﬁne when two constructors, in particular two types, are
interchangeable by differing only by simpliﬁcations that can be performed to obtain one from the
other. The judgment
∆⊢c1 ≡c2 :: κ
states that c1 and c2 are deﬁnitionally equal constructors of kind κ. When κ is the kind T, the
constructors c1 and c2 are deﬁnitionally equal types.
Deﬁnitional equality of constructors is deﬁned by these rules:
∆⊢c :: κ
∆⊢c ≡c :: κ
(18.2a)
∆⊢c ≡c′ :: κ
∆⊢c′ ≡c :: κ
(18.2b)
∆⊢c ≡c′ :: κ
∆⊢c′ ≡c′′ :: κ
∆⊢c ≡c′′ :: κ
(18.2c)
∆⊢c ≡c′ :: κ1 × κ2
∆⊢c · l ≡c′ · l :: κ1
(18.2d)

160
18.3 Expressions and Types
∆⊢c ≡c′ :: κ1 × κ2
∆⊢c · r ≡c′ · r :: κ2
(18.2e)
∆⊢c1 ≡c′
1 :: κ1
∆⊢c2 ≡c′
2 :: κ2
∆⊢⟨c1,c2⟩≡⟨c′
1,c′
2⟩:: κ1 × κ2
(18.2f)
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩· l ≡c1 :: κ1
(18.2g)
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩· r ≡c2 :: κ2
(18.2h)
∆⊢c1 ≡c′
1 :: κ2 →κ
∆⊢c2 ≡c′
2 :: κ2
∆⊢c1[c2] ≡c′
1[c′
2] :: κ
(18.2i)
∆, u :: κ ⊢c2 ≡c′
2 :: κ2
∆⊢λ (u :: κ) c2 ≡λ (u :: κ) c′
2 :: κ →κ2
(18.2j)
∆, u :: κ1 ⊢c2 :: κ2
∆⊢c1 :: κ1
∆⊢(λ (u :: κ) c2)[c1] ≡[c1/u]c2 :: κ2
(18.2k)
In short deﬁnitional equality of constructors is the strongest congruence containing the rules (18.2g), (18.2h),
and (18.2k).
18.3
Expressions and Types
The statics of expressions of Fω is deﬁned using two judgment forms:
∆⊢τ type
type formation
∆Γ ⊢e : τ
expression formation
Here, as before, Γ is a ﬁnite set of hypotheses of the form
x1 : τ1, . . . , xk : τk
such that ∆⊢τi type for each 1 ≤i ≤k.
The types of Fω are the constructors of kind T:
∆⊢τ :: T
∆⊢τ type
.
(18.3)
This being the only rule for introducing types, the only types are the constructors of kind T.
Deﬁnitionally equal types classify the same expressions:
∆Γ ⊢e : τ1
∆⊢τ1 ≡τ2 :: T
Γ ⊢e : τ2
.
(18.4)

18.4 Notes
161
This rule ensures that in situations such as that described in the introduction to this chapter, typing
is inﬂuenced by simpliﬁcation of types.
The language Fω extends F to permit universal quantiﬁcation over arbitrary kinds; the lan-
guage FEω extends Fω with existential quantiﬁcation over arbitrary kinds. The statics of the quan-
tiﬁers in FEω is deﬁned by the following rules:
∆, u :: κ Γ ⊢e : τ
∆Γ ⊢Λ(u :: κ) e : ∀u :: κ.τ
(18.5a)
∆Γ ⊢e : ∀u :: κ.τ
∆⊢c :: κ
∆Γ ⊢e[c] : [c/u]τ
(18.5b)
∆⊢c :: κ
∆, u :: κ ⊢τ type
∆Γ ⊢e : [c/u]τ
∆Γ ⊢pack c with e as ∃u :: κ.τ : ∃u :: κ.τ
(18.5c)
∆Γ ⊢e1 : ∃u :: κ.τ
∆, u :: κ Γ, x : τ ⊢e2 : τ2
∆⊢τ2 type
∆Γ ⊢open e1 as u :: κ with x : τ in e2 : τ2
(18.5d)
The dynamics of FEω is the subject of Exercise 18.2.
18.4
Notes
The language Fω given here is standard, apart from details of notation. The rule of invariance of
typing under deﬁnitional equality of types demands that a type checking algorithm must include
as a subroutine an algorithm for checking deﬁnitional equality. Numerous methods for checking
such equivalences are given in the literature, all of which proceed by various means to simplify
both sides of an equation, and check whether the results are the same. Another approach, pio-
neered by Watkins et al. (2008) in another setting, is to avoid deﬁnitional equality by maintaining
constructors in simpliﬁed form. The discussion in the introduction shows that substitution of a
simpliﬁed constructor into a simpliﬁed constructor is not necessarily simpliﬁed. The burden is
then shifted to deﬁning a form of simplifying substitution whose result is always in simpliﬁed
form.
Exercises
18.1. Adapt the two implementations of queues given in Chapter 17 to match the signature of
queue constructors given in the introduction,
∃q :: T →T.⟨emp ,→∀t :: T.t, ins ,→∀t :: T.t × q[t] →q[t], rem ,→∀t :: T.q[t] →(t × q[t]) opt⟩.
Consider the role played by deﬁnitional equality in ensuring that both implementations have
this type.
18.2. Give an equational dynamics for FEω. What role does deﬁnitional equality of constructors
play in it? Formulate a transition dynamics for FEω extended with a type of observable
results, say nat. What role does deﬁnitional equality play in the transition dynamics?

162
18.4 Notes

Part VIII
Partiality and Recursive Types


Chapter 19
System PCF of Recursive Functions
We introduced the language T as a basis for discussing total computations, those for which the
type system guarantees termination. The language M generalizes T to admit inductive and coin-
ductive types, while preserving totality. In this chapter we introduce PCF as a basis for discussing
partial computations, those that may not terminate when evaluated, even when they are well-
typed. At ﬁrst blush this may seem like a disadvantage, but as we shall see in Chapter 20 it admits
greater expressive power than is possible in T.
The source of partiality in PCF is the concept of general recursion, which permits the solution
of equations between expressions. The price for admitting solutions to all such equations is that
computations may not terminate—the solution to some equations might be undeﬁned (divergent).
In PCF the programmer must make sure that a computation terminates; the type system does not
guarantee it. The advantage is that the termination proof need not be embedded into the code
itself, resulting in shorter programs.
For example, consider the equations
f (0) ≜1
f (n + 1) ≜(n + 1) × f (n).
Intuitively, these equations deﬁne the factorial function. They form a system of simultaneous
equations in the unknown f which ranges over functions on the natural numbers. The function
we seek is a solution to these equations—a speciﬁc function f : N →N such that the above
conditions are satisﬁed.
A solution to such a system of equations is a ﬁxed point of an associated functional (higher-
order function). To see this, let us re-write these equations in another form:
f (n) ≜
(
1
if n = 0
n × f (n′)
if n = n′ + 1.

166
Re-writing yet again, we seek f given by
n 7→
(
1
if n = 0
n × f (n′)
if n = n′ + 1.
Now deﬁne the functional F by the equation F( f ) = f ′, where f ′ is given by
n 7→
(
1
if n = 0
n × f (n′)
if n = n′ + 1.
Note well that the condition on f ′ is expressed in terms of f, the argument to the functional F, and
not in terms of f ′ itself! The function f we seek is a ﬁxed point of F, a function f : N →N such
that f = F( f ). In other words e is deﬁned to be ﬁx(F), where ﬁx is a higher-order operator on
functionals F that computes a ﬁxed point for it.
Why should an operator such as F have a ﬁxed point? The key is that functions in PCF are
partial, which means that they may diverge on some (or even all) inputs. Consequently, a ﬁxed
point of a functional F is the limit of a series of approximations of the desired solution obtained
by iterating F. Let us say that a partial function φ on the natural numbers, is an approximation to
a total function f if φ(m) = n implies that f (m) = n. Let ⊥: N ⇀N be the totally undeﬁned
partial function—⊥(n) is undeﬁned for every n ∈N. This is the “worst” approximation to the
desired solution f of the recursion equations given above. Given any approximation φ of f, we
may “improve” it to φ′ = F(φ). The partial function φ′ is deﬁned on 0 and on m + 1 for every
m ≥0 on which φ is deﬁned. Continuing, φ′′ = F(φ′) = F(F(φ)) is an improvement on φ′, and
hence a further improvement on φ. If we start with ⊥as the initial approximation to f, then pass
to the limit
lim
i≥0 F(i)(⊥),
we will obtain the least approximation to f that is deﬁned for every m ∈N, and hence is the
function f itself. Turning this around, if the limit exists, it is the solution we seek.
Because this construction works for any functional F, we conclude that all such operators have
ﬁxed points, and hence that all systems of equations such as the one given above have solutions.
The solution is given by general recursion, but there is no guarantee that it is a total function
(deﬁned on all elements of its domain). For the above example it happens to be true, because we
can prove by induction that this is so, but in general the solution is a partial function that may
diverge on some inputs. It is our task as programmers to ensure that the functions deﬁned by
general recursion are total, or at least that we have a grasp of those inputs for which it is well-
deﬁned.

19.1 Statics
167
19.1
Statics
The syntax of PCF is given by the following grammar:
Typ
τ
::=
nat
nat
naturals
parr(τ1; τ2)
τ1 ⇀τ2
partial function
Exp
e
::=
x
x
variable
z
z
zero
s(e)
s(e)
successor
ifz{e0; x.e1}(e)
ifz e {z ,→e0 | s(x) ,→e1}
zero test
lam{τ}(x.e)
λ (x : τ) e
abstraction
ap(e1; e2)
e1(e2)
application
fix{τ}(x.e)
fix x : τ is e
recursion
The expression fix{τ}(x.e) is general recursion; it is discussed in more detail below. The expression
ifz{e0; x.e1}(e) branches according to whether e evaluates to z or not, binding the predecessor to
x in the case that it is not.
The statics of PCF is inductively deﬁned by the following rules:
Γ, x : τ ⊢x : τ
(19.1a)
Γ ⊢z : nat
(19.1b)
Γ ⊢e : nat
Γ ⊢s(e) : nat
(19.1c)
Γ ⊢e : nat
Γ ⊢e0 : τ
Γ, x : nat ⊢e1 : τ
Γ ⊢ifz{e0; x.e1}(e) : τ
(19.1d)
Γ, x : τ1 ⊢e : τ2
Γ ⊢lam{τ1}(x.e) : parr(τ1; τ2)
(19.1e)
Γ ⊢e1 : parr(τ2; τ)
Γ ⊢e2 : τ2
Γ ⊢ap(e1; e2) : τ
(19.1f)
Γ, x : τ ⊢e : τ
Γ ⊢fix{τ}(x.e) : τ
(19.1g)
Rule (19.1g) reﬂects the self-referential nature of general recursion. To show that fix{τ}(x.e) has
type τ, we assume that it is the case by assigning that type to the variable x, which stands for the
recursive expression itself, and checking that the body, e, has type τ under this very assumption.
The structural rules, including in particular substitution, are admissible for the static semantics.
Lemma 19.1. If Γ, x : τ ⊢e′ : τ′, Γ ⊢e : τ, then Γ ⊢[e/x]e′ : τ′.

168
19.2 Dynamics
19.2
Dynamics
The dynamic semantics of PCF is deﬁned by the judgments e val, specifying the closed values, and
e 7−→e′, specifying the steps of evaluation.
The judgment e val is deﬁned by the following rules:
z val
(19.2a)
[e val]
s(e) val
(19.2b)
lam{τ}(x.e) val
(19.2c)
The bracketed premise on rule (19.2b) is included for the eager interpretation of the successor oper-
ation, and omitted for the lazy interpretation. (See Chapter 36 for a further discussion of laziness.)
The transition judgment e 7−→e′ is deﬁned by the following rules:

e 7−→e′
s(e) 7−→s(e′)

(19.3a)
e 7−→e′
ifz{e0; x.e1}(e) 7−→ifz{e0; x.e1}(e′)
(19.3b)
ifz{e0; x.e1}(z) 7−→e0
(19.3c)
s(e) val
ifz{e0; x.e1}(s(e)) 7−→[e/x]e1
(19.3d)
e1 7−→e′
1
ap(e1; e2) 7−→ap(e′
1; e2)
(19.3e)
"
e1 val
e2 7−→e′
2
ap(e1; e2) 7−→ap(e1; e′
2)
#
(19.3f)
[e2 val]
ap(lam{τ}(x.e); e2) 7−→[e2/x]e
(19.3g)
fix{τ}(x.e) 7−→[fix{τ}(x.e)/x]e
(19.3h)
The bracketed rule (19.3a) is included for an eager interpretation of the successor, and omitted
otherwise. Bracketed rule (19.3f) and the bracketed premise on rule (19.3g) are included for a
call-by-value interpretation, and omitted for a call-by-name interpretation, of function applica-
tion. Rule (19.3h) implements self-reference by substituting the recursive expression itself for the
variable x in its body; this is called unwinding the recursion.

19.3 Deﬁnability
169
Theorem 19.2 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. The proof of preservation is by induction on the derivation of the transition judgment.
Consider rule (19.3h). Suppose that fix{τ}(x.e) : τ. By inversion and substitution we have
[fix{τ}(x.e)/x]e : τ, as required. The proof of progress proceeds by induction on the deriva-
tion of the typing judgment. For example, for rule (19.1g) the result follows because we may make
progress by unwinding the recursion.
It is easy to check that if e val, then e is irreducible in that there is no e′ such that e 7−→e′. The
safety theorem implies the converse, that an irreducible expression is a value, provided that it is
closed and well-typed.
Deﬁnitional equality for the call-by-name variant of PCF, written Γ ⊢e1 ≡e2 : τ, is the
strongest congruence containing the following axioms:
Γ ⊢ifz{e0; x.e1}(z) ≡e0 : τ
(19.4a)
Γ ⊢ifz{e0; x.e1}(s(e)) ≡[e/x]e1 : τ
(19.4b)
Γ ⊢fix{τ}(x.e) ≡[fix{τ}(x.e)/x]e : τ
(19.4c)
Γ ⊢ap(lam{τ1}(x.e2); e1) ≡[e1/x]e2 : τ
(19.4d)
These rules sufﬁce to calculate the value of any closed expression of type nat: if e : nat, then
e ≡n : nat iff e 7−→∗n.
19.3
Deﬁnability
Let us write fun x(y:τ1):τ2 is e for a recursive function within whose body, e : τ2, are bound two
variables, y : τ1 standing for the argument and x : τ1 ⇀τ2 standing for the function itself. The
dynamic semantics of this construct is given by the axiom
(fun x(y:τ1):τ2 is e)(e1) 7−→[fun x(y:τ1):τ2 is e, e1/x, y]e
.
That is, to apply a recursive function, we substitute the recursive function itself for x and the
argument for y in its body.

170
19.3 Deﬁnability
Recursive functions are deﬁned in PCF using recursive functions, writing
fix x : τ1 ⇀τ2 is λ (y : τ1) e
for fun x(y:τ1):τ2 is e. We may easily check that the static and dynamic semantics of recursive
functions are derivable from this deﬁnition.
The primitive recursion construct of T is deﬁned in PCF using recursive functions by taking
the expression
rec e {z ,→e0 | s(x) with y ,→e1}
to stand for the application e′(e), where e′ is the general recursive function
fun f (u:nat):τ is ifz u {z ,→e0 | s(x) ,→[ f (x)/y]e1}.
The static and dynamic semantics of primitive recursion are derivable in PCF using this expansion.
In general, functions deﬁnable in PCF are partial in that they may be undeﬁned for some
arguments. A partial (mathematical) function, φ : N ⇀N, is deﬁnable in PCF iff there is an
expression eφ : nat ⇀nat such that φ(m) = n iff eφ(m) ≡n : nat. So, for example, if φ is the
totally undeﬁned function, then eφ is any function that loops without returning when it is applied.
It is informative to classify those partial functions φ that are deﬁnable in PCF. The partial re-
cursive functions are deﬁned to be the primitive recursive functions extended with the minimization
operation: given φ(m, n), deﬁne ψ(n) to be the least m ≥0 such that (1) for m′ < m, φ(m′, n) is
deﬁned and non-zero, and (2) φ(m, n) = 0. If no such m exists, then ψ(n) is undeﬁned.
Theorem 19.3. A partial function φ on the natural numbers is deﬁnable in PCF iff it is partial recursive.
Proof sketch. Minimization is deﬁnable in PCF, so it is at least as powerful as the set of partial
recursive functions. Conversely, we may, with some tedium, deﬁne an evaluator for expressions of
PCF as a partial recursive function, using G¨odel-numbering to represent expressions as numbers.
Therefore PCF does not exceed the power of the set of partial recursive functions.
Church’s Law states that the partial recursive functions coincide with the set of effectively
computable functions on the natural numbers—those that can be carried out by a program written
in any programming language that is or will ever be deﬁned.1 Therefore PCF is as powerful as
any other programming language with respect to the set of deﬁnable functions on the natural
numbers.
The universal function φuniv for PCF is the partial function on the natural numbers deﬁned by
φuniv(⌜e⌝)(m) = n iff e(m) ≡n : nat.
In contrast to T, the universal function φuniv for PCF is partial (might be undeﬁned for some
inputs). It is, in essence, an interpreter that, given the code ⌜e⌝of a closed expression of type
nat ⇀nat, simulates the dynamic semantics to calculate the result, if any, of applying it to the m,
obtaining n. Because this process may fail to terminate, the universal function is not deﬁned for
all inputs.
1See Chapter 21 for further discussion of Church’s Law.

19.4 Finite and Inﬁnite Data Structures
171
By Church’s Law the universal function is deﬁnable in PCF. In contrast, we proved in Chap-
ter 9 that the analogous function is not deﬁnable in T using the technique of diagonalization. It is
instructive to examine why that argument does not apply in the present setting. As in Section 9.4,
we may derive the equivalence
e∆(⌜e∆⌝) ≡s(e∆(⌜e∆⌝))
for PCF. But now, instead of concluding that the universal function, euniv, does not exist as we did
for T, we instead conclude for PCF that euniv diverges on the code for e∆applied to its own code.
19.4
Finite and Inﬁnite Data Structures
Finite data types (products and sums), including their use in pattern matching and generic pro-
gramming, carry over verbatim to PCF. However, the distinction between the eager and lazy
dynamics for these constructs becomes more important. Rather than being a matter of preference,
the decision to use an eager or lazy dynamics affects the meaning of a program: the “same” types
mean different things in a lazy dynamics than in an eager dynamics. For example, the elements of
a product type in an eager language are pairs of values of the component types. In a lazy language
they are instead pairs of unevaluated, possibly divergent, computations of the component types,
a very different thing indeed. And similarly for sums.
The situation grows more acute for inﬁnite types such as the type nat of “natural numbers.”
The scare quotes are warranted, because the “same” type has a very different meaning under
an eager dynamics than under a lazy dynamics. In the former case the type nat is, indeed, the
authentic type of natural numbers—the least type containing zero and closed under successor.
The principle of mathematical induction is valid for reasoning about the type nat in an eager
dynamics. It corresponds to the inductive type nat deﬁned in Chapter 15.
On the other hand, under a lazy dynamics the type nat is no longer the type of natural numbers
at all. For example, it includes the value
ω ≜fix x : nat is s(x),
which has itself as predecessor! It is, intuitively, an “inﬁnite stack of successors”, growing without
end. It is clearly not a natural number (it is larger than all of them), so the principle of mathemat-
ical induction does not apply. In a lazy setting nat could be renamed lnat to remind us of the
distinction; it corresponds to the type conat deﬁned in Chapter 15.
19.5
Totality and Partiality
The advantage of a total programming language such as T is that it ensures, by type checking,
that every program terminates, and that every function is total. There is no way to have a well-
typed program that goes into an inﬁnite loop. This prohibition may seem appealing, until one
considers that the upper bound on the time to termination may be large, so large that it might
as well diverge for all practical purposes. But let us grant for the moment that it is a virtue of T
that it precludes divergence. Why, then, bother with a language such as PCF that does not rule

172
19.5 Totality and Partiality
out divergence? After all, inﬁnite loops are invariably bugs, so why not rule them out by type
checking? The notion seems appealing until one tries to write a program in a language such as T.
Consider the computation of the greatest common divisor (gcd) of two natural numbers. It can
be programmed in PCF by solving the following equations using general recursion:
gcd(m, 0) = m
gcd(0, n) = n
gcd(m, n) = gcd(m −n, n)
if m > n
gcd(m, n) = gcd(m, n −m)
if m < n
The type of gcd deﬁned this way is (nat × nat) ⇀nat, which suggests that it may not terminate
for some inputs. But we may prove by induction on the sum of the pair of arguments that it is, in
fact, a total function.
Now consider programming this function in T. It is, in fact, programmable using only primi-
tive recursion, but the code to do it is rather painful (try it!). One way to see the problem is that
in T the only form of looping is one that reduces a natural number by one on each recursive call;
it is not (directly) possible to make a recursive call on a smaller number other than the immedi-
ate predecessor. In fact one may code up more general patterns of terminating recursion using
only primitive recursion as a primitive, but if you check the details, you will see that doing so
comes at a price in performance and program complexity. Program complexity can be mitigated
by building libraries that codify standard patterns of reasoning whose cost of development should
be amortized over all programs, not just one in particular. But there is still the problem of perfor-
mance. Indeed, the encoding of more general forms of recursion into primitive recursion means
that, deep within the encoding, there must be a “timer” that goes down by ones to ensure that the
program terminates. The result will be that programs written with such libraries will be slower
than necessary.
But, one may argue, T is simply not a serious language. A more serious total programming
language would admit sophisticated patterns of control without performance penalty. Indeed, one
could easily envision representing the natural numbers in binary, rather than unary, and allowing
recursive calls by halving to get logarithmic complexity. Such a formulation is possible, as would
be quite a number of analogous ideas that avoid the awkwardness of programming in T. Could
we not then have a practical language that rules out divergence?
We can, but at a cost. We have already seen one limitation of total programming languages:
they are not universal. You cannot write an interpreter for T within T, and this limitation extends
to any total language whatever. If this does not seem important, then consider the Blum Size
Theorem (BST), which places another limitation on total languages. Fix any total language L that
permits writing functions on the natural numbers. Pick any blowup factor, say 22n. The BST states
that there is a total function on the natural numbers that is programmable in L, but whose shortest
program in L is larger by the given blowup factor than its shortest program in PCF!
The underlying idea of the proof is that in a total language the proof of termination of a program must
be baked into the code itself, whereas in a partial language the termination proof is an external veriﬁcation
condition left to the programmer. There are, and always will be, programs whose termination proof
is rather complicated to express, if you ﬁx in advance the means of proving it total. (In T it was
primitive recursion, but one can be more ambitious, yet still get caught by the BST.) But if you

19.6 Notes
173
leave room for ingenuity, then programs can be short, because they do not have to embed the
proof of their termination in their own running code.
19.6
Notes
The solution to recursion equations described here is based on Kleene’s ﬁxed point theorem for
complete partial orders, specialized to the approximation ordering of partial functions. The lan-
guage PCF is derived from Plotkin (1977) as a laboratory for the study of semantics of program-
ming languages. Many authors have used PCF as the subject of study of many problems in se-
mantics. It has thereby become the E. coli of programming languages.
Exercises
19.1. Consider the problem considered in Section 10.3 of how to deﬁne the mutually recursive
“even” and “odd” functions. There we gave a solution in terms of primitive recursion. You
are, instead, to give a solution in terms of general recursion. Hint: consider that a pair of
mutually recursive functions is a recursive pair of functions.
19.2. Show that minimization, as explained before the statement of Theorem 19.3, is deﬁnable in
PCF.
19.3. Consider the partial function φhalts such that if e : nat ⇀nat, then φhalts(⌜e⌝) evaluates to
zero iff e(⌜e⌝) converges, and evaluates to one otherwise. Prove that φhalts is not deﬁnable in
PCF.
19.4. Suppose that we changed the speciﬁcation of minimization given prior to Theorem 19.3 so
that ψ(n) is the least m such that φ(m, n) = 0, and is undeﬁned if no such m exists. Is this
“simpliﬁed” form of minimization deﬁnable in PCF?
19.5. Suppose that we wished to deﬁne, in the lazy variant of PCF, a version of the parallel or
function speciﬁed a function of two arguments that returns z if either of its arguments is
z, and s(z) otherwise. That is, we wish to ﬁnd an expression e satisfying the following
properties:
e(e1)(e2) 7−→∗z if e1 7−→∗z
e(e1)(e2) 7−→∗z if e2 7−→∗z
e(e1)(e2) 7−→∗s(z) otherwise
Thus, e deﬁnes a total function of its two arguments, even if one of the arguments diverges.
Clearly such a function cannot be deﬁned in the call-by-value variant of PCF, but can it be
deﬁned in the call-by-name variant? If so, show how; if not, prove that it cannot be, and
suggest an extension of PCF that would allow it to be deﬁned.

174
19.6 Notes
19.6. We appealed to Church’s Law to argue that the universal function for PCF is deﬁnable in
PCF. See what is behind this claim by considering two aspects of the problem: (1) G¨odel-
numbering, the representation of abstract syntax by a number; (2) evaluation, the process of
interpreting a function on its inputs. Part (1) is a technical issue arising from the limited data
structures available in PCF. Part (2) is the heart of the matter; explore its implementation in
terms of a solution to Part (1).

Chapter 20
System FPC of Recursive Types
In this chapter we study FPC, a language with products, sums, partial fucntions, and recursive
types. Recursive types are solutions to type equations t ∼= τ where there is no restriction on where
t may occur in τ. Equivalently, a recursive type is a ﬁxed point up to isomorphism of the associated
unrestricted type operator t.τ. By removing the restrictions on the type operator we may consider
the solution of a type equation such as t ∼= t ⇀t, which describes a type that is isomorphic to the
type of partial functions deﬁned on itself. If types were sets, such an equation could not be solved,
because there are more partial functions on a set than there are elements of that set. But types are
not sets: they classify computable functions, not arbitrary functions. With types we may solve such
“dubious” type equations, even though we cannot expect to do so with sets. The penalty is that
we must admit non-termination. For one thing, type equations involving functions have solutions
only if the functions involved are partial.
A beneﬁt of working in the setting of partial functions is that type equations have unique so-
lutions (up to isomorphism). Therefore it makes sense, as we shall do in this chapter, to speak of
the solution to a type equation. But what about the distinct solutions to a type equation given in
Chapter 15? These turn out to coincide for any ﬁxed dynamics, but give rise to different solutions
according to whether the dynamics is eager or lazy (as illustrated in Section 19.4 for the special
case of the natural numbers). Under a lazy dynamics (where all constructs are evaluated lazily),
recursive types have a coinductive ﬂavor, and the inductive analogs are inaccessible. Under an
eager dynamics (where all constructs are evaluated eagerly), recursive types have an inductive
ﬂavor. But the coinductive analogs are accessible as well, using function types to selectively im-
pose laziness. It follows that the eager dynamics is more expressive than the lazy dynamics, because
it is impossible to go the other way around (one cannot deﬁne inductive types in a lazy language).
20.1
Solving Type Equations
The language FPC has products, sums, and partial functions inherited from the preceding de-
velopment, extended with the new concept of recursive types. The syntax of recursive types is

176
20.1 Solving Type Equations
deﬁned as follows:
Typ
τ
::=
t
t
self-reference
rec(t.τ)
rec t is τ
recursive type
Exp
e
::=
fold{t.τ}(e)
fold(e)
fold
unfold(e)
unfold(e)
unfold
The subscript on the concrete syntax of fold is often omitted when it is clear from context.
Recursive types have the same general form as the inductive and coinductive types discussed
in Chapter 15, but without restriction on the type operator involved. Recursive type are formed
according to the rule:
∆, t type ⊢τ type
∆⊢rec(t.τ) type
(20.1)
The statics of folding and unfolding is given by the following rules:
Γ ⊢e : [rec(t.τ)/t]τ
Γ ⊢fold{t.τ}(e) : rec(t.τ)
(20.2a)
Γ ⊢e : rec(t.τ)
Γ ⊢unfold(e) : [rec(t.τ)/t]τ
(20.2b)
The dynamics of folding and unfolding is given by these rules:
[e val]
fold{t.τ}(e) val
(20.3a)

e 7−→e′
fold{t.τ}(e) 7−→fold{t.τ}(e′)

(20.3b)
e 7−→e′
unfold(e) 7−→unfold(e′)
(20.3c)
fold{t.τ}(e) val
unfold(fold{t.τ}(e)) 7−→e
(20.3d)
The bracketed premise and rule are included for an eager interpretation of the introduction form,
and omitted for a lazy interpretation. As mentioned in the introduction, the choice of eager or lazy
dynamics affects the meaning of recursive types.
Theorem 20.1 (Safety).
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val, or there exists e′ such that e 7−→e′.

20.2 Inductive and Coinductive Types
177
20.2
Inductive and Coinductive Types
Recursive types may be used to represent inductive types such as the natural numbers. Using an
eager dynamics for FPC, the recursive type
ρ = rec t is [z ,→unit, s ,→t]
satisﬁes the type equation
ρ ∼= [z ,→unit, s ,→ρ],
and is isomorphic to the type of eager natural numbers. The introduction and elimination forms
are deﬁned on ρ by the following equations:1
z ≜fold(z · ⟨⟩)
s(e) ≜fold(s · e)
ifz e {z ,→e0 | s(x) ,→e1} ≜case unfold(e) {z · ,→e0 | s · x ,→e1}.
It is a good exercise to check that the eager dynamics of natural numbers in PCF is correctly
simulated by these deﬁnitions.
On the other hand, under a lazy dynamics for FPC, the same recursive type ρ′,
rec t is [z ,→unit, s ,→t],
satisﬁes the same type equation,
ρ′ ∼= [z ,→unit, s ,→ρ′],
but is not the type of natural numbers! Rather, it is the type lnat of lazy natural numbers intro-
duced in Section 19.4. As discussed there, the type ρ′ contains the “inﬁnite number” ω, which is
of course not a natural number.
Similarly, using an eager dynamics for FPC, the type natlist of lists of natural numbers is
deﬁned by the recursive type
rec t is [n ,→unit, c ,→nat × t],
which satisﬁes the type equation
natlist ∼= [n ,→unit, c ,→nat × natlist].
The list introduction operations are given by the following equations:
nil ≜fold(n · ⟨⟩)
cons(e1; e2) ≜fold(c · ⟨e1, e2⟩).
1The “underscore” stands for a variable that does not occur free in e0.

178
20.3 Self-Reference
A conditional list elimination form is given by the following equation:
case e {nil ,→e0 | cons(x; y) ,→e1} ≜case unfold(e) {n · ,→e0 | c · ⟨x, y⟩,→e1},
where we have used pattern-matching syntax to bind the components of a pair for the sake of
clarity.
Now consider the same recursive type, but in the context of a lazy dynamics for FPC. What
type is it? If all constructs are lazy, then a value of the recursive type
rec t is [n ,→unit, c ,→nat × t],
has the form fold(e), where e is an unevaluated computation of the sum type, whose values are
injections of unevaluated computations of either the unit type or of the product type nat × t. And
the latter consists of pairs of an unevaluated computation of a (lazy!) natural number, and an
unevaluated computation of another value of this type. In particular, this type contains inﬁnite
lists whose tails go on without end, as well as ﬁnite lists that eventually reach an end. The type is,
in fact, a version of the type of inﬁnite streams deﬁned in Chapter 15, rather than a type of ﬁnite
lists as is the case under an eager dynamics.
It is common in textbooks to depict data structures using “box-and-pointer” diagrams. These
work well in the eager setting, provided that no functions are involved. For example, an eager list
of eager natural numbers may be depicted using this notation. We may think of fold as an abstract
pointer to a tagged cell consisting of either (a) the tag n with no associated data, or (b) the tag c
attached to a pair consisting of an authentic natural number and another list, which is an abstract
pointer of the same type. But this notation does not scale well to types involving functions, or to
languages with a lazy dynamics. For example, the recursive type of “lists” in lazy FPC cannot be
depicted using boxes and pointers, because of the unevaluated computations occurring in values
of this type. It is a mistake to limit one’s conception of data structures to those that can be drawn
on the blackboard using boxes and pointers or similar informal notations. There is no substitute
for a programming language to express data structures fully and accurately.
It is deceiving that the “same” recursive type can have two different meanings according to
whether the underlying dynamics is eager or lazy. For example, it is common for lazy languages
to use the name “list” for the recursive type of streams, or the name “nat” for the type of lazy
natural numbers. This terminology is misleading, considering that such languages do not (and
can not) have a proper type of ﬁnite lists or a type of natural numbers. Caveat emptor!
20.3
Self-Reference
In the general recursive expression fix{τ}(x.e) the variable x stands for the expression itself. Self-
reference is effected by the unrolling transition
fix{τ}(x.e) 7−→[fix{τ}(x.e)/x]e,
which substitutes the expression itself for x in its body during execution. It is useful to think of x
as an implicit argument to e that is instantiated to itself when the expression is used. In many well-
known languages this implicit argument has a special name, such as this or self, to emphasize
its self-referential interpretation.

20.3 Self-Reference
179
Using this intuition as a guide, we may derive general recursion from recursive types. This
derivation shows that general recursion may, like other language features, be seen as a manifes-
tation of type structure, instead of as an ad hoc language feature. The derivation isolates a type of
self-referential expressions given by the following grammar:
Typ
τ
::=
self(τ)
τ self
self-referential type
Exp
e
::=
self{τ}(x.e)
self x is e
self-referential expression
unroll(e)
unroll(e)
unroll self-reference
The statics of these constructs is given by the following rules:
Γ, x : self(τ) ⊢e : τ
Γ ⊢self{τ}(x.e) : self(τ)
(20.4a)
Γ ⊢e : self(τ)
Γ ⊢unroll(e) : τ
(20.4b)
The dynamics is given by the following rule for unrolling the self-reference:
self{τ}(x.e) val
(20.5a)
e 7−→e′
unroll(e) 7−→unroll(e′)
(20.5b)
unroll(self{τ}(x.e)) 7−→[self{τ}(x.e)/x]e
(20.5c)
The main difference, compared to general recursion, is that we distinguish a type of self-referential
expressions, instead of having self-reference at every type. However, as we shall see, the self-
referential type sufﬁces to implement general recursion, so the difference is a matter of taste.
The type self(τ) is deﬁnable from recursive types. As suggested earlier, the key is to consider
a self-referential expression of type τ to depend on the expression itself. That is, we seek to deﬁne
the type self(τ) so that it satisﬁes the isomorphism
self(τ) ∼= self(τ) ⇀τ.
We seek a ﬁxed point of the type operator t.t ⇀τ, where t /∈τ is a type variable standing for the
type in question. The required ﬁxed point is just the recursive type
rec(t.t ⇀τ),
which we take as the deﬁnition of self(τ).
The self-referential expression self{τ}(x.e) is the expression
fold(λ (x : self(τ)) e).

180
20.4 The Origin of State
We may check that rule (20.4a) is derivable according to this deﬁnition. The expression unroll(e)
is correspondingly the expression
unfold(e)(e).
It is easy to check that rule (20.4b) is derivable from this deﬁnition. Moreover, we may check that
unroll(self{τ}(y.e)) 7−→∗[self{τ}(y.e)/y]e.
This completes the derivation of the type self(τ) of self-referential expressions of type τ.
The self-referential type self(τ) can be used to deﬁne general recursion for any type. We may
deﬁne fix{τ}(x.e) to stand for the expression
unroll(self{τ}(y.[unroll(y)/x]e))
where the recursion at each occurrence of x is unrolled within e. It is easy to check that this veriﬁes
the statics of general recursion given in Chapter 19. Moreover, it also validates the dynamics, as
shown by the following derivation:
fix{τ}(x.e) = unroll(self{τ}(y.[unroll(y)/x]e))
7−→∗[unroll(self{τ}(y.[unroll(y)/x]e))/x]e
= [fix{τ}(x.e)/x]e.
It follows that recursive types can be used to deﬁne a non-terminating expression of every type,
fix{τ}(x.x).
20.4
The Origin of State
The concept of state in a computation—which will be discussed in Part XIV—has its origins in
the concept of recursion, or self-reference, which, as we have just seen, arises from the concept of
recursive types. For example, the concept of a ﬂip-ﬂop or a latch is a circuit built from combinational
logic elements (typically, nor or nand gates) that have the characteristic that they maintain an
alterable state over time. An RS latch, for example, maintains its output at the logical level of zero
or one in response to a signal on the R or S inputs, respectively, after a brief settling delay. This
behavior is achieved using feedback, which is just a form of self-reference, or recursion: the output
of the gate feeds back into its input so as to convey the current state of the gate to the logic that
determines its next state.
We can implement an RS latch using recursive types. The idea is to use self-reference to model
the passage of time, with the current output being computed from its input and its previous out-
puts. Speciﬁcally, an RS latch is a value of type τrsl given by
rec t is ⟨X ,→bool, Q ,→bool, N ,→t⟩.
The X and Q components of the latch represent its current outputs (of which Q represents the current
state of the latch), and the N component represents the next state of the latch.
If e is of type τrsl,

20.5 Notes
181
then we deﬁne e @ X to mean unfold(e) · X, and deﬁne e @ Q and e @ N similarly. The expressions
e @ X and e @ Q evaluate to the boolean outputs of the latch e, and e @ N evaluates to another latch
representing its evolution over time based on these inputs.
For given values r and s, a new latch is computed from an old latch by the recursive function
rsl deﬁned as follows:2
fix rsl is λ (l : τrsl) ersl,
where ersl is the expression
fix this is fold(⟨X ,→enor(⟨s, l @ Q⟩), Q ,→enor(⟨r, l @ X⟩), N ,→rsl(this)⟩),
where enor is the obvious binary function on booleans. The outputs of the latch are computed in
terms of the r and s inputs and the outputs of the previous state of the latch. To get the construction
started, we deﬁne an initial state of the latch in which the outputs are arbitrarily set to false, and
whose next state is determined by applying the recursive function rsl to that state:
fix this is fold(⟨X ,→false, Q ,→false, N ,→rsl(this)⟩).
Selection of the N component causes the outputs to be recalculated based on their current values.
Notice the role of self-reference in maintaining the state of the latch.
20.5
Notes
The systematic study of recursive types in programming was initiated by Scott (1976, 1982) to give
a mathematical model of the untyped λ-calculus. The derivation of recursion from recursive types
is an application of Scott’s theory. The category-theoretic view of recursive types was developed
by Wand (1979) and Smyth and Plotkin (1982). Implementing state using self-reference is funda-
mental to digital logic (Ward and Halstead, 1990). The example given in Section 20.4 is inspired
by Cook (2009) and Abadi and Cardelli (1996). The account of signals as streams (explored in the
exercises) is inspired by the pioneering work of Kahn (MacQueen, 2009). The language name FPC
is taken from Gunter (1992).
Exercises
20.1. Show that the recursive type D ≜rec t is t ⇀t is non-trivial by interpreting the sk-combinators
deﬁned in Exercise 3.1 into it. Speciﬁcally, deﬁne elements k : D and s : D and a (left-
associative) “application” function
x : D y : D ⊢x · y : D
such that
2For convenience we assume that fold is evaluated lazily.

182
20.5 Notes
(a) k · x · y 7−→∗x;
(b) s · x · y · z 7−→∗(x · z) · (y · z).
20.2. Recursive types admit the structure of both inductive and coinductive types. Consider the
recursive type τ ≜rec t is τ′ and the associated inductive and coinductive types µ(t.τ′) and
ν(t.τ′). Complete the following chart consistently with the statics of inductive and coinduc-
tive types on the left-hand side and with the statics of recursive types on the right:
foldt.t opt(e) ≜fold(e)
rec(x.e′; e) ≜?
unfoldt.t opt(e) ≜unfold(e)
gen(x.e′; e) ≜?
Check that the statics is derivable under these deﬁnitions. Hint: you will need to use general
recursion on the right to ﬁll in the missing cases. You may also ﬁnd it useful to use generic
programming.
Now consider the dynamics of these deﬁnitions, under both an eager and a lazy interpreta-
tion. What happens in each case?
20.3. Deﬁne the type signal of signals to be the coinductive type of inﬁnite streams of booleans
(bits). Deﬁne a signal transducer to be a function of type signal ⇀signal. Combinational
logic gates, such as the NOR gate, can be deﬁned as signal transducers. Give a coinductive
deﬁnition of the type signal, and deﬁne NOR as a signal transducer. Be sure to take account
of the underlying dynamics of PCF.
The passage from combinational to digital logic (circuit elements that maintain state) relies
on self-reference. For example, an RS latch can be built from NOR two nor gates in this way.
Deﬁne an RS latch using general recursion and two of the NOR gates just deﬁned.
20.4. The type τrsl given in Section 20.4 above is the type of streams of pairs of booleans. Give
another formulation of an RS latch as a value of type τrsl, but this time using the coinductive
interpretation of the recursive type proposed in Exercise 20.2 (using the lazy dynamics for
FPC). Expand and simplify this deﬁnition using your solution to Exercise 20.2, and compare
it with the formulation given in Section 20.4. Hint: the internal state of the stream is a pair of
booleans corresponding to the X and Q outputs of the latch.

Part IX
Dynamic Types


Chapter 21
The Untyped λ-Calculus
In this chapter we study the premier example of a uni-typed programming language, the (untyped)
λ-calculus. This formalism was introduced by Church in the 1930’s as a universal language of com-
putable functions. It is distinctive for its austere elegance. The λ-calculus has but one “feature”,
the higher-order function. Everything is a function, hence every expression may be applied to an
argument, which must itself be a function, with the result also being a function. To borrow a turn
of phrase, in the λ-calculus it’s functions all the way down.
21.1
The λ-Calculus
The abstract syntax of the untyped λ-calculus, called Λ, is given by the following grammar:
Exp
u
::=
x
x
variable
λ(x.u)
λ (x) u
λ-abstraction
ap(u1; u2)
u1(u2)
application
The statics of Λ is deﬁned by general hypothetical judgments of the form x1 ok, . . . , xn ok ⊢
u ok, stating that u is a well-formed expression involving the variables x1, . . . , xn. (As usual, we
omit explicit mention of the variables when they can be determined from the form of the hypothe-
ses.) This relation is inductively deﬁned by the following rules:
Γ, x ok ⊢x ok
(21.1a)
Γ ⊢u1 ok
Γ ⊢u2 ok
Γ ⊢u1(u2) ok
(21.1b)
Γ, x ok ⊢u ok
Γ ⊢λ (x) u ok
(21.1c)

186
21.2 Deﬁnability
The dynamics of Λ is given equationally, rather than via a transition system. Deﬁnitional equal-
ity for Λ is a judgment of the form Γ ⊢u ≡u′, where Γ = x1 ok, . . . , xn ok for some n ≥0, and
u and u′ are terms having at most the variables x1, . . . , xn free. It is inductively deﬁned by the
following rules:
Γ, u ok ⊢u ≡u
(21.2a)
Γ ⊢u ≡u′
Γ ⊢u′ ≡u
(21.2b)
Γ ⊢u ≡u′
Γ ⊢u′ ≡u′′
Γ ⊢u ≡u′′
(21.2c)
Γ ⊢u1 ≡u′
1
Γ ⊢u2 ≡u′
2
Γ ⊢u1(u2) ≡u′
1(u′
2)
(21.2d)
Γ, x ok ⊢u ≡u′
Γ ⊢λ (x) u ≡λ (x) u′
(21.2e)
Γ, x ok ⊢u2 ok
Γ ⊢u1 ok
Γ ⊢(λ (x) u2)(u1) ≡[u1/x]u2
(21.2f)
We often write just u ≡u′ when the variables involved need not be emphasized or are clear from
context.
21.2
Deﬁnability
Interest in the untyped λ-calculus stems from its surprising expressiveness. It is a Turing-complete
language in the sense that it has the same capability to express computations on the natural num-
bers as does any other known programming language. Church’s Law states that any conceivable
notion of computable function on the natural numbers is equivalent to the λ-calculus. This asser-
tion is true for all known means of deﬁning computable functions on the natural numbers. The
force of Church’s Law is that it postulates that all future notions of computation will be equiv-
alent in expressive power (measured by deﬁnability of functions on the natural numbers) to the
λ-calculus. Church’s Law is therefore a scientiﬁc law in the same sense as, say, Newton’s Law of
Universal Gravitation, which predicts the outcome of all future measurements of the acceleration
in a gravitational ﬁeld.1
We will sketch a proof that the untyped λ-calculus is as powerful as the language PCF de-
scribed in Chapter 19. The main idea is to show that the PCF primitives for manipulating the
natural numbers are deﬁnable in the untyped λ-calculus. In particular, we must show that the
natural numbers are deﬁnable as λ-terms in such a way that case analysis, which discriminates
between zero and non-zero numbers, is deﬁnable. The principal difﬁculty is with computing the
predecessor of a number, which requires a bit of cleverness. Finally, we show how to represent
general recursion, completing the proof.
1It is debatable whether there are any scientiﬁc laws in Computer Science. In the opinion of the author, Church’s Law,
which is usually called Church’s Thesis, is a strong candidate for being a scientiﬁc law.

21.2 Deﬁnability
187
The ﬁrst task is to represent the natural numbers as certain λ-terms, called the Church numerals.
0 ≜λ (b) λ (s) b
(21.3a)
n + 1 ≜λ (b) λ (s) s(n(b)(s))
(21.3b)
It follows that
n(u1)(u2) ≡u2(. . . (u2(u1))),
the n-fold application of u2 to u1. That is, n iterates its second argument (the induction step) n
times, starting with its ﬁrst argument (the basis).
Using this deﬁnition it is not difﬁcult to deﬁne the basic functions of arithmetic. For example,
successor, addition, and multiplication are deﬁned by the following untyped λ-terms:
succ ≜λ (x) λ (b) λ (s) s(x(b)(s))
(21.4)
plus ≜λ (x) λ (y) y(x)(succ)
(21.5)
times ≜λ (x) λ (y) y(0)(plus(x))
(21.6)
It is easy to check that succ(n) ≡n + 1, and that similar correctness conditions hold for the repre-
sentations of addition and multiplication.
To deﬁne ifz{u0; x.u1}(u) requires a bit of ingenuity. The key is to deﬁne the “cut-off prede-
cessor”, pred, such that
pred(0) ≡0
(21.7)
pred(n + 1) ≡n.
(21.8)
To compute the predecessor using Church numerals, we must show how to compute the result
for n + 1 in terms of its value for n. At ﬁrst glance this seems simple—just take the successor—
until we consider the base case, in which we deﬁne the predecessor of 0 to be 0. This formulation
invalidates the obvious strategy of taking successors at inductive steps, and necessitates some
other approach.
What to do? A useful intuition is to think of the computation in terms of a pair of “shift
registers” satisfying the invariant that on the nth iteration the registers contain the predecessor of
n and n itself, respectively. Given the result for n, namely the pair (n −1, n), we pass to the result
for n + 1 by shifting left and incrementing to obtain (n, n + 1). For the base case, we initialize the
registers with (0, 0), reﬂecting the stipulation that the predecessor of zero be zero. To compute the
predecessor of n we compute the pair (n −1, n) by this method, and return the ﬁrst component.
To make this precise, we must ﬁrst deﬁne a Church-style representation of ordered pairs.
⟨u1, u2⟩≜λ ( f ) f (u1)(u2)
(21.9)
u · l ≜u(λ (x) λ (y) x)
(21.10)
u · r ≜u(λ (x) λ (y) y)
(21.11)

188
21.3 Scott’s Theorem
It is easy to check that under this encoding ⟨u1, u2⟩· l ≡u1, and that a similar equivalence holds
for the second projection. We may now deﬁne the required representation, up, of the predecessor
function:
u′
p ≜λ (x) x(⟨0, 0⟩)(λ (y) ⟨y · r, succ (y · r)⟩)
(21.12)
up ≜λ (x) u′
p(x) · l
(21.13)
It is easy to check that this gives us the required behavior. Finally, deﬁne ifz{u0; x.u1}(u) to be
the untyped term
u(u0)(λ ( ) [up(u)/x]u1).
This deﬁnition gives us all the apparatus of PCF, apart from general recursion. But general
recursion is also deﬁnable in Λ using a ﬁxed point combinator. There are many choices of ﬁxed
point combinator, of which the best known is the Y combinator:
Y ≜λ (F) (λ ( f ) F( f ( f )))(λ ( f ) F( f ( f ))).
It is easy to check that
Y(F) ≡F(Y(F)).
Using the Y combinator, we may deﬁne general recursion by writing Y(λ (x) u), where x stands for
the recursive expression itself.
Although it is clear that Y as just deﬁned computes a ﬁxed point of its argument, it is probably
less clear why it works or how we might have invented it in the ﬁrst place. The main idea is quite
simple. If a function is recursive, it is given an extra ﬁrst argument, which is arranged at call sites
to be the function itself. Whenever we wish to call a self-referential function with an argument,
we apply the function ﬁrst to itself and then to its argument; this protocol is imposed on both the
“external” calls to the function and on the “internal” calls that the function may make to itself.
For this reason the ﬁrst argument is often called this or self, to remind you that it will be, by
convention, bound to the function itself.
With this in mind, it is easy to see how to derive the deﬁnition of Y. If F is the function whose
ﬁxed point we seek, then the function F′ = λ ( f ) F( f ( f )) is a variant of F in which the self-
application convention has been imposed internally by substituting for each occurrence of f in
F( f ) the self-application f ( f ). Now check that F′(F′) ≡F(F′(F′)), so that F′(F′) is the desired
ﬁxed point of F. Expanding the deﬁnition of F′, we have derived that the desired ﬁxed point of F
is
λ ( f ) F( f ( f ))(λ ( f ) F( f ( f ))).
To ﬁnish the derivation, we need only note that nothing depends on the particular choice of F,
which means that we can compute a ﬁxed point for F uniformly in F. That is, we may deﬁne a
single function, the term Y as deﬁned above, that computes the ﬁxed point of any F.
21.3
Scott’s Theorem
Scott’s Theorem states that deﬁnitional equality for the untyped λ-calculus is undecidable: there is
no algorithm to determine whether or not two untyped terms are deﬁnitionally equal. The proof

21.3 Scott’s Theorem
189
uses the concept of inseparability. Any two properties, A0 and A1, of λ-terms are inseparable if there
is no decidable property, B, such that A0 u implies that B u holds, and A1 u implies that B u does
not hold. We say that a property, A, of untyped terms is behavioral iff whenever u ≡u′, then A u
iff A u′.
The proof of Scott’s Theorem decomposes into two parts:
1. For any untyped λ-term u, we may ﬁnd an untyped term v such that u(⌜v⌝) ≡v, where ⌜v⌝
is the G¨odel number of v, and ⌜v⌝is its representation as a Church numeral. (See Chapter 9
for a discussion of G¨odel-numbering.)
2. Any two non-trivial2 behavioral properties A0 and A1 of untyped terms are inseparable.
Lemma 21.1. For any u there exists v such that u(⌜v⌝) ≡v.
Proof Sketch. The proof relies on the deﬁnability of the following two operations in the untyped
λ-calculus:
1. ap(⌜u1⌝)(⌜u2⌝) ≡⌜u1(u2)⌝.
2. nm(n) ≡⌜n⌝.
Intuitively, the ﬁrst takes the representations of two untyped terms, and builds the representation
of the application of one to the other. The second takes a numeral for n, and yields the repre-
sentation of the Church numeral n. Given these, we may ﬁnd the required term v by deﬁning
v ≜w(⌜w⌝), where w ≜λ (x) u(ap(x)(nm(x))). We have
v = w(⌜w⌝)
≡u(ap(⌜w⌝)(nm(⌜w⌝)))
≡u(⌜w(⌜w⌝)⌝)
≡u(⌜v⌝).
The deﬁnition is very similar to that of Y(u), except that u takes as input the representation of
a term, and we ﬁnd a v such that, when applied to the representation of v, the term u yields v
itself.
Lemma 21.2. Suppose that A0 and A1 are two non-trivial behavioral properties of untyped terms. Then
there is no untyped term w such that
1. For every u either w(⌜u⌝) ≡0 or w(⌜u⌝) ≡1.
2. If A0 u, then w(⌜u⌝) ≡0.
3. If A1 u, then w(⌜u⌝) ≡1.
2A property of untyped terms is trivial if it either holds for all untyped terms or never holds for any untyped term.

190
21.4 Untyped Means Uni-Typed
Proof. Suppose there is such an untyped term w. Let v be the untyped term
λ (x) ifz{u1; .u0}(w(x)),
where u0 and u1 are chosen such that A0 u0 and A1 u1. (Such a choice must exist by non-triviality
of the properties.) By Lemma 21.1 there is an untyped term t such that v(⌜t⌝) ≡t. If w(⌜t⌝) ≡0,
then t ≡v(⌜t⌝) ≡u1, and so A1 t, because A1 is behavioral and A1 u1. But then w(⌜t⌝) ≡1 by the
deﬁning properties of w, which is a contradiction. Similarly, if w(⌜t⌝) ≡1, then A0 t, and hence
w(⌜t⌝) ≡0, again a contradiction.
Corollary 21.3. There is no algorithm to decide whether u ≡u′.
Proof. For ﬁxed u, the property Eu u′ deﬁned by u′ ≡u is a non-trivial behavioral property of
untyped terms. So it is inseparable from its negation, and hence is undecidable.
21.4
Untyped Means Uni-Typed
The untyped λ-calculus can be faithfully embedded in a typed language with recursive types.
Thus every untyped λ-term has a representation as a typed expression in such a way that execution
of the representation of a λ-term corresponds to execution of the term itself. This embedding is
not a matter of writing an interpreter for the λ-calculus in FPC, but rather a direct representation
of untyped λ-terms as typed expressions in a language with recursive types.
The key observation is that the untyped λ-calculus is really the uni-typed λ-calculus. It is not the
absence of types that gives it its power, but rather that it has only one type, the recursive type
D ≜rec t is t ⇀t.
A value of type D is of the form fold(e) where e is a value of type D ⇀D — a function whose do-
main and range are both D. Any such function can be regarded as a value of type D by “folding”,
and any value of type D can be turned into a function by “unfolding”. As usual, a recursive type
is a solution to a type equation, which in the present case is the equation
D ∼= D ⇀D.
This isomorphism speciﬁes that D is a type that is isomorphic to the space of partial functions on
D itself, which is impossible if types are just sets.
This isomorphism leads to the following translation, of Λ into FPC:
x† ≜x
(21.14a)
λ (x) u† ≜fold(λ (x : D) u†)
(21.14b)
u1(u2)† ≜unfold(u†
1)(u†
2)
(21.14c)

21.5 Notes
191
Note that the embedding of a λ-abstraction is a value, and that the embedding of an application
exposes the function being applied by unfolding the recursive type. And so we have
λ (x) u1(u2)† = unfold(fold(λ (x : D) u†
1))(u†
2)
≡λ (x : D) u†
1(u†
2)
≡[u†
2/x]u†
1
= ([u2/x]u1)†.
The last step, stating that the embedding commutes with substitution, is proved by induction on
the structure of u1. Thus β-reduction is implemented by evaluation of the embedded terms.
Thus we see that the canonical untyped language, Λ, which by dint of terminology stands in
opposition to typed languages, turns out to be but a typed language after all. Rather than eliminat-
ing types, an untyped language consolidates an inﬁnite collection of types into a single recursive
type. Doing so renders static type checking trivial, at the cost of incurring dynamic overhead to
coerce values to and from the recursive type. In Chapter 22 we will take this a step further by
admitting many different types of data values (not just functions), each of which is a component
of a “master” recursive type. This generalization shows that so-called dynamically typed languages
are, in fact, statically typed. Thus this traditional distinction cannot be considered an opposition,
because dynamic languages are but particular forms of static languages in which undue emphasis
is placed on a single recursive type.
21.5
Notes
The untyped λ-calculus was introduced by Church (1941) as a formalization of the informal con-
cept of a computable function. Unlike the well-known machine models, such as the Turing ma-
chine or the random access machine, the λ-calculus codiﬁes mathematical and programming prac-
tice. Barendregt (1984) is the deﬁnitive reference for all aspects of the untyped λ-calculus; the proof
of Scott’s theorem is adapted from Barendregt’s account. Scott (1980a) gave the ﬁrst model of the
untyped λ-calculus in terms of an elegant theory of recursive types. This construction underlies
Scott’s apt description of the λ-calculus as “uni-typed”, rather than “untyped.” The idea to char-
acterize Church’s Law as such was communicated to the author, independently of each other, by
Robert L. Constable and Mark Lillibridge.
Exercises
21.1. Deﬁne an encoding of ﬁnite products as deﬁned in Chapter 10 in Λ.
21.2. Deﬁne the factorial function in Λ two ways, one without using Y, and one using Y. In both
cases show that your solution, u, has the property that u(n) ≡n!.
21.3. Deﬁne the “Church booleans” in Λ by deﬁning terms true and false such that

192
21.5 Notes
(a) true(u1)(u2) ≡u1.
(b) false(u1)(u2) ≡u2.
What is the encoding of if u then u1 else u2?
21.4. Deﬁne an encoding of ﬁnite sums as deﬁned in Chapter 11 in Λ.
21.5. Deﬁne an encoding of ﬁnite lists of natural numbers as deﬁned in Chapter 15 in Λ.
21.6. Deﬁne an encoding of the inﬁnite streams of natural numbers as deﬁned in Chapter 15 in Λ.
21.7. Show that Λ can be “compiled” to sk-combinators using bracket abstraction (see Exercises 3.4
and 3.5. Deﬁne a translation u∗from Λ into sk combinators such that
if u1 ≡u2, then u∗
1 ≡u∗
2.
Hint: Deﬁne u∗by induction on the structure of u, using the compositional form of bracket
abstraction considered in Exercise 3.5. Show that the translation is itself compositional in
that it commutes with substitution:
([u2/x]u1)∗= [u∗
2/x]u∗.
Then proceed by rule induction on rules (21.2) to show the required correctness condition.

Chapter 22
Dynamic Typing
We saw in Chapter 21 that an untyped language is a uni-typed language in which “untyped”
terms are just terms of single recursive type. Because all expressions of Λ are well-typed, type
safety ensures that no misinterpretation of a value is possible. When spelled out for Λ, type safety
follows from there being exactly one class of values, that of functions on values. No application
can get stuck, because every value is a function that may be applied to an argument.
This safety property breaks down once more than one class of value is admitted. For example,
if the natural numbers are added as a primitive to Λ, then it is possible to incur a run-time error
by attempting to apply a number to an argument. One way to manage this is to embrace the
possibility, treating class mismatches as checked errors, and weakening the progress theorem as
outlined in Chapter 6. Such languages are called dynamic languages because an error such as the
one described is postponed to run time, rather than precluded at compile time by type checking.
Languages of the latter sort are called static languages.
Dynamic languages are often considered in opposition to static languages, but the opposition
is illusory. Just as the untyped λ-calculus is uni-typed, so dynamic languages are but special cases
of static languages in which there is only one recursive type (albeit with multiple classes of value).

194
22.1 Dynamically Typed PCF
22.1
Dynamically Typed PCF
To illustrate dynamic typing we formulate a dynamically typed version of PCF, called DPCF. The
abstract syntax of DPCF is given by the following grammar:
Exp
d
::=
x
x
variable
num[n]
n
numeral
zero
zero
zero
succ(d)
succ(d)
successor
ifz{d0; x.d1}(d)
ifz d {zero ,→d0 | succ(x) ,→d1}
zero test
fun(x.d)
λ (x) d
abstraction
ap(d1; d2)
d1(d2)
application
fix(x.d)
fix x is d
recursion
There are two classes of values in DPCF, the numbers, which have the form num[n], and the func-
tions, which have the form fun(x.d). The expressions zero and succ(d) are not themselves values,
but rather are constructors that evaluate to values. General recursion is deﬁnable using a ﬁxed point
combinator, but is taken as primitive here to simplify the analysis of the dynamics in Section 22.3.
As usual, the abstract syntax of DPCF is what matters, but we use the concrete syntax to im-
prove readability. However, notational conveniences can obscure important details, such as the
tagging of values with their class and the checking of these tags at run-time. For example, the
concrete syntax for a number, n, suggests a “bare” representation, the abstract syntax reveals that
the number is labeled with the class num to distinguish it from a function. Correspondingly, the
concrete syntax for a function is λ (x) d, but its abstract syntax, fun(x.d), shows that it also sports
a class label. The class labels are required to ensure safety by run-time checking, and must not be
overlooked when comparing static with dynamic languages.
The statics of DPCF is like that of Λ; it merely checks that there are no free variables in the
expression. The judgment
x1 ok, . . . xn ok ⊢d ok
states that d is a well-formed expression with free variables among those in the hypotheses. If the
assumptions are empty, then we write just d ok to mean that d is a closed expression of DPCF.
The dynamics of DPCF must check for errors that would never arise in a language such as
PCF. For example, evaluation of a function application must ensure that the value being applied
is indeed a function, signaling an error if it is not. Similarly the conditional branch must ensure that
its principal argument is a number, signaling an error if it is not. To account for these possibilities,
the dynamics is given by several judgment forms, as summarized in the following chart:
d val
d is a (closed) value
d 7−→d′
d evaluates in one step to d′
d err
d incurs a run-time error
d is num n
d is of class num with value n
d isnt num
d is not of class num
d is fun x.d
d is of class fun with body x.d
d isnt fun
d is not of class fun

22.1 Dynamically Typed PCF
195
The last four judgment forms implement dynamic class checking. They are only relevant when d is
already a value. The afﬁrmative class-checking judgments have a second argument that represents
the underlying structure of a value; this argument is not itself an expression of DPCF.
The value judgment d val states that d is a evaluated (closed) expression:
num[n] val
(22.1a)
fun(x.d) val
(22.1b)
The afﬁrmative class-checking judgments are deﬁned by the following rules:
num[n] is num n
(22.2a)
fun(x.d) is fun x.d
(22.2b)
The negative class-checking judgments are correspondingly deﬁned by these rules:
num[n] isnt fun
(22.3a)
fun(x.d) isnt num
(22.3b)
The transition judgment d 7−→d′ and the error judgment d err are deﬁned simultaneously by
the following rules:
zero 7−→num[z]
(22.4a)
d 7−→d′
succ(d) 7−→succ(d′)
(22.4b)
d err
succ(d) err
(22.4c)
d is num n
succ(d) 7−→num[s(n)]
(22.4d)
d isnt num
succ(d) err
(22.4e)
d 7−→d′
ifz{d0; x.d1}(d) 7−→ifz{d0; x.d1}(d′)
(22.4f)
d err
ifz{d0; x.d1}(d) err
(22.4g)
d is num 0
ifz{d0; x.d1}(d) 7−→d0
(22.4h)
d is num n + 1
ifz{d0; x.d1}(d) 7−→[num[n]/x]d1
(22.4i)

196
22.2 Variations and Extensions
d isnt num
ifz{d0; x.d1}(d) err
(22.4j)
d1 7−→d′
1
ap(d1; d2) 7−→ap(d′
1; d2)
(22.4k)
d1 err
ap(d1; d2) err
(22.4l)
d1 is fun x.d
ap(d1; d2) 7−→[d2/x]d
(22.4m)
d1 isnt fun
ap(d1; d2) err
(22.4n)
fix(x.d) 7−→[fix(x.d)/x]d
(22.4o)
Rule (22.4i) labels the predecessor with the class num to maintain the invariant that variables are
bound to expressions of DPCF.
Lemma 22.1 (Class Checking). If d val, then
1. either d is num n for some n, or d isnt num;
2. either d is fun x.d′ for some x and d′, or d isnt fun.
Proof. By inspection of the rules deﬁning the class-checking judgments.
Theorem 22.2 (Progress). If d ok, then either d val, or d err, or there exists d′ such that d 7−→d′.
Proof. By induction on the structure of d. For example, if d = succ(d′), then we have by induction
either d′ val, or d′ err, or d′ 7−→d′′ for some d′′. In the last case we have by rule (22.4b) that
succ(d′) 7−→succ(d′′), and in the second-to-last case we have by rule (22.4c) that succ(d′) err.
If d′ val, then by Lemma 22.1, either d′ is num n or d′ isnt num. In the former case succ(d′) 7−→
num[s(n)], and in the latter succ(d′) err. The other cases are handled similarly.
Lemma 22.3 (Exclusivity). For any d in DPCF, exactly one of the following holds: d val, or d err, or
d 7−→d′ for some d′.
Proof. By induction on the structure of d, making reference to rules (22.4).
22.2
Variations and Extensions
The dynamic language DPCF deﬁned in Section 22.1 parallels the static language PCF deﬁned
in Chapter 19. One discrepancy, however, is in the treatment of natural numbers. Whereas in
PCF the zero and successor operations are introduction forms for the type nat, in DPCF they are
elimination forms that act on specially deﬁned numerals. The present formulation uses only a
single class of numbers.

22.2 Variations and Extensions
197
One could instead treat zero and succ(d) as values of separate classes, and introduce the obvi-
ous class checking judgments for them. When written in this style, the dynamics of the conditional
branch is given as follows:
d 7−→d′
ifz{d0; x.d1}(d) 7−→ifz{d0; x.d1}(d′)
(22.5a)
d is zero
ifz{d0; x.d1}(d) 7−→d0
(22.5b)
d is succ d′
ifz{d0; x.d1}(d) 7−→[d′/x]d1
(22.5c)
d isnt zero
d isnt succ
ifz{d0; x.d1}(d) err
(22.5d)
Notice that the predecessor of a value of the successor class need not be a number, whereas in the
previous formulation this possibility does not arise.
DPCF can be extended with structured data similarly. A classic example is to consider a class
nil, consisting of a “null” value, and a class cons, consisting of pairs of values.
Exp
d
::=
nil
nil
null
cons(d1; d2)
cons(d1; d2)
pair
ifnil(d; d0; x, y.d1)
ifnil d {nil ,→d0 | cons(x; y) ,→d1}
conditional
The expression ifnil(d; d0; x, y.d1) distinguishes the null value from a pair, and signals an error
on any other class of value.
Lists (ﬁnite sequences) can be encoded using null and pairing. For example, the list consisting
of three zeros can berepresented by the value
cons(zero; cons(zero; cons(zero; nil))).
But what to make of the following value?
cons(zero; cons(zero; cons(zero; λ (x) x)))
It is not a list, because it does not end with nil, but it is a permissible value in the enriched
language.
A difﬁculty with encoding lists using null and pair emerges when deﬁning functions on them.
For example, here is a deﬁnition of the function append that concatenates two lists:
fix a is λ (x) λ (y) ifnil(x; y; x1, x2.cons(x1; a(x2)(y)))
Nothing prevents us from applying this function to any two values, regardless of whether they
are lists. If the ﬁrst argument is not a list, then execution aborts with an error. But because the
function does not traverse its second argument, it can be any value at all. For example, we may
apply append with a list and a function to obtain the “list” that ends with a λ given above.

198
22.2 Variations and Extensions
It might be argued that the conditional branch that distinguishes null from a pair is inappro-
priate in DPCF, because there are more than just these two classes in the language. One approach
that avoids this criticism is to abandon pattern matching on the class of data, replacing it by a gen-
eral conditional branch that distinguishes null from all other values, and adding to the language
predicates1 that test the class of a value and destructors that invert the constructors of each class.
We could instead reformulate null and and pairing as follows:
Exp
d
::=
cond(d; d0; d1)
cond(d; d0; d1)
conditional
nil?(d)
nil?(d)
nil test
cons?(d)
cons?(d)
pair test
car(d)
car(d)
ﬁrst projection
cdr(d)
cdr(d)
second projection
The conditional cond(d; d0; d1) distinguishes d between nil and all other values. If d is not nil, the
conditional evaluates to d0, and otherwise evaluates to d1. In other words the value nil repre-
sents boolean falsehood, and all other values represent boolean truth. The predicates nil?(d) and
cons?(d) test the class of their argument, yielding nil if the argument is not of the speciﬁed class,
and yielding some non-nil if so. The destructors car(d) and cdr(d) decompose cons(d1; d2) into
d1 and d2, respectively.2
Written in this form, the function append is given by the expression
fix a is λ (x) λ (y) cond(x; cons(car(x); a(cdr(x))(y)); y).
The behavior of this formulation of append is no different from the earlier one; the only differ-
ence is that instead of dispatching on whether a value is either null or a pair, we instead allow
discrimination on any predicate of the value, which includes such checks as special cases.
An alternative, which is not widely used, is to enhance, and not restrict, the conditional branch
so that it includes cases for each possible class of value in the language. So in a language with num-
bers, functions, null, and pairing, the conditional would have four branches. The fourth branch,
for pairing, would deconstruct the pair into its constituent parts. The difﬁculty with this approach
is that in realistic languages there are many classes of data, and such a conditional would be rather
unwieldy. Moreover, even once we have dispatched on the class of a value, it is nevertheless neces-
sary for the primitive operations associated with that class to admit run-time checks. For example,
we may determine that a value d is of the numeric class, but there is no way to propagate this in-
formation into the branch of the conditional that then adds d to some other number. The addition
operation must still check the class of d, recover the underlying number, and create a new value
of numeric class. It is an inherent limitation of dynamic languages that they do not allow values
other than classiﬁed values.
1Predicates evaluate to the null value to mean that a condition is false, and some non-null value to mean that it is true.
2The terminology for the projections is archaic, but well-established. It is said that car originally stood for “contents of
the address register” and that cdr stood for “contents of the data register”, referring to the details of the original imple-
mentation of Lisp.

22.3 Critique of Dynamic Typing
199
22.3
Critique of Dynamic Typing
The safety theorem for DPCF is an advantage of dynamic over static typing. Unlike static lan-
guages, which rule out some candidate programs as ill-typed, every piece of abstract syntax in
DPCF is well-formed, and hence, by Theorem 22.2, has a well-deﬁned dynamics (albeit one with
checked errors). But this convenience is also a disadvantage, because errors that could be ruled
out at compile time by type checking are not signaled until run time.
Consider, for example, the addition function in DPCF, whose speciﬁcation is that, when passed
two values of class num, returns their sum, which is also of class num:3
fun(x.fix(p.fun(y.ifz{x; y′.succ(p(y′))}(y)))).
The addition function may, deceptively, be written in concrete syntax as follows:
λ (x) fix p is λ (y) ifz y {zero ,→x | succ(y′) ,→succ(p(y′))}.
It is deceptive, because it obscures the class tags on values, and the operations that check the
validity of those tags. Let us now examine the costs of these operations in a bit more detail.
First, note that the body of the ﬁxed point expression is labeled with class fun. The dynamics of
the ﬁxed point construct binds p to this function. Consequently, the dynamic class check incurred
by the application of p in the recursive call is guaranteed to succeed. But DPCF offers no means of
suppressing the redundant check, because it cannot express the invariant that p is always bound
to a value of class fun.
Second, note that the result of applying the inner λ-abstraction is either x, the argument of
the outer λ-abstraction, or the successor of a recursive call to the function itself. The successor
operation checks that its argument is of class num, even though this condition is guaranteed to
hold for all but the base case, which returns the given x, which can be of any class at all. In
principle we can check that x is of class num once, and note that it is otherwise a loop invariant
that the result of applying the inner function is of this class. However, DPCF gives us no way
to express this invariant; the repeated, redundant tag checks imposed by the successor operation
cannot be avoided.
Third, the argument y to the inner function is either the original argument to the addition
function, or is the predecessor of some earlier recursive call. But as long as the original call is to
a value of class num, then the dynamics of the conditional will ensure that all recursive calls have
this class. And again there is no way to express this invariant in DPCF, and hence there is no way
to avoid the class check imposed by the conditional branch.
Classiﬁcation is not free—storage is required for the class label, and it takes time to detach the
class from a value each time it is used and to attach a class to a value when it is created. Although
the overhead of classiﬁcation is not asymptotically signiﬁcant (it slows down the program only by
a constant factor), it is nevertheless non-negligible, and should be eliminated when possible. But
this is impossible within DPCF, because it cannot enforce the restrictions required to express the
required invariants. For that we need a static type system.
3This speciﬁcation imposes no restrictions on the behavior of addition on arguments that are not classiﬁed as numbers,
but we could make the further demand that the function abort when applied to arguments that are not classiﬁed by num.

200
22.4 Notes
22.4
Notes
The earliest dynamically typed language is Lisp (McCarthy, 1965), which continues to inﬂuence
language design a half century after its invention. Dynamic PCF is the core of Lisp, but with a
proper treatment of variable binding, correcting what McCarthy himself has described as an error
in the original design. Informal discussions of dynamic languages are often complicated by the
elision of the dynamic checks that are made explicit here. Although the surface syntax of dynamic
PCF is almost the same as that for PCF, minus the type annotations, the underlying dynamics is
different. It is for this reason that static PCF cannot be seen as a restriction of dynamic PCF by the
imposition of a type system.
Exercises
22.1. Surface syntax can be deceiving; even simple arithmetic expressions do not have the same
meaning in DPCF that they do in PCF. To see why, deﬁne the addition function, plus,
in DPCF, and examine the dynamics of evaluating expressions such as plus(5)(7). Even
though this expression might be written as “5 + 7” in both static and dynamic languages,
they have different meanings.
22.2. Give a precise dynamics to the data structuring primitives described informally in Sec-
tion 22.2. What class restrictions should cons impose on its arguments? Check the dynamics
of the append function when called with two lists as arguments.
22.3. To avoid the difﬁculties with the representation of lists using cons and nil, introduce a class
of lists that are constructed using revised versions of nil and cons that operate on the class
of lists. Revisit the dynamics of the append function when redeﬁned using the class of lists.
22.4. Allowing multiple arguments to, and multiple results from, functions is a notorious source
of trouble in dynamic languages. The restriction to a single type makes it impossible even
to distinguish n things from m things, let alone express more subtle properties of a program.
Numerous workarounds have been proposed. Explore the problem yourself by enriching
DPCF with multi-argument and multi-result functions. Be sure to consider these questions:
(a) If a function is deﬁned with n parameters, what should happen if it is called with more
or fewer than n arguments?
(b) What happens if one were to admit functions with a varying number of arguments?
How would you refer to these arguments within the body of such a function? How
does this relate to pattern matching?
(c) What if one wished to admit keyword parameter passing by giving names to the ar-
guments, and allowing them to be passed in any order by associating them with their
names?
(d) What notation would you suggest for functions returning multiple results? For exam-
ple, a division function might return the quotient and the remainder. How might one

22.4 Notes
201
notate this in the function body? How would a caller access the results individually or
collectively?
(e) How would one deﬁne the composition of two functions when either or both can take
multiple arguments or return multiple results?

202
22.4 Notes

Chapter 23
Hybrid Typing
A hybrid language is one that combines static and dynamic typing by enriching a statically typed
language with a distinguished type dyn of dynamic values. The dynamically typed language con-
sidered in Chapter 22 can be embedded into the hybrid language by viewing a dynamically typed
program as a statically typed program of type dyn. Static and dynamic types are not opposed to
one another, but may coexist harmoniously. The ad hoc device of adding the type dyn to a static
language is unnecessary in a language with recursive types, wherein it is deﬁnable as a particular
recursive type. Thus, one may say that dynamic typing is a mode of use of static typing, reconciling an
apparent opposition between them.
23.1
A Hybrid Language
Consider the language HPCF, which extends PCF with the following constructs:
Typ
τ
::=
dyn
dyn
dynamic
Exp
e
::=
new[l](e)
l ! e
construct
cast[l](e)
e @ l
destruct
inst[l](e)
l ? e
discriminate
Cls
l
::=
num
num
number
fun
fun
function
The type dyn is the type of dynamically classiﬁed values. The constructor attaches a classiﬁer to
a value of a type associated to that classifer, the destructor recovers the value classiﬁed with the
given classiﬁer, and the discriminator tests the class of a classiﬁed value.
The statics of HPCF extends that of PCF with the following rules:
Γ ⊢e : nat
Γ ⊢new[num](e) : dyn
(23.1a)
Γ ⊢e : dyn ⇀dyn
Γ ⊢new[fun](e) : dyn
(23.1b)

204
23.1 A Hybrid Language
Γ ⊢e : dyn
Γ ⊢cast[num](e) : nat
(23.1c)
Γ ⊢e : dyn
Γ ⊢cast[fun](e) : dyn ⇀dyn
(23.1d)
Γ ⊢e : dyn
Γ ⊢inst[num](e) : bool
(23.1e)
Γ ⊢e : dyn
Γ ⊢inst[fun](e) : bool
(23.1f)
The statics ensures that classiﬁers are attached to values of the right type, namely natural numbers
for num, and functions on classiﬁed values for fun.
The dynamics of HPCF extends that of PCF with the following rules:
e val
new[l](e) val
(23.2a)
e 7−→e′
new[l](e) 7−→new[l](e′)
(23.2b)
e 7−→e′
cast[l](e) 7−→cast[l](e′)
(23.2c)
new[l](e) val
cast[l](new[l](e)) 7−→e
(23.2d)
new[l′](e) val
l ̸= l′
cast[l](new[l′](e)) err
(23.2e)
e 7−→e′
inst[l](e) 7−→inst[l](e′)
(23.2f)
new[l](e) val
inst[l](new[l](e)) 7−→true
(23.2g)
new[l](e) val
l ̸= l′
inst[l′](new[l](e)) 7−→false
(23.2h)
Casting compares the class of the object to the required class, returning the underlying object if
these coincide, and signaling an error otherwise.1
Lemma 23.1 (Canonical Forms). If e : dyn and e val, then e = new[l](e′) for some class l and some e′ val.
If l = num, then e′ : nat, and if l = fun, then e′ : dyn ⇀dyn.
Proof. By rule induction on the statics of HPCF.
Theorem 23.2 (Safety). The language HPCF is safe:
1The judgment e err signals a checked error that is to be treated as described in Section 6.3.

23.2 Dynamic as Static Typing
205
1. If e : τ and e 7−→e′, then e′ : τ.
2. If e : τ, then either e val, or e err, or e 7−→e′ for some e′.
Proof. Preservation is proved by rule induction on the dynamics, and progress is proved by rule
induction on the statics, making use of the canonical forms lemma. The opportunities for run-time
errors are the same as those for DPCF—a well-typed cast might fail at run-time if the class of the
cast does not match the class of the value.
In a language such as FPC (Chapter 20) with recursive types, there is no need to add dyn as a
primitive type. Instead, it is deﬁned to be type
rec t is [num ,→nat, fun ,→t ⇀t].
(23.3)
The introduction and elimination forms for this deﬁnition of dyn are deﬁnable as follows:2
new[num](e) ≜fold(num · e)
(23.4)
new[fun](e) ≜fold(fun · e)
(23.5)
cast[num](e) ≜case unfold(e) {num · x ,→x | fun · x ,→error}
(23.6)
cast[fun](e) ≜case unfold(e) {num · x ,→error | fun · x ,→x}.
(23.7)
These deﬁnition simply decompose the class operations for dyn into recursive unfoldings and case
analyses on values of a sum type.
23.2
Dynamic as Static Typing
The language DPCF of Chapter 22 can be embedded into HPCF by a simple translation that makes
explicit the class checking in the dynamics of DPCF. Speciﬁcally, we may deﬁne a translation d†
of expressions of DPCF into expressions of HPCF according to the following static correctness
criterion:
Theorem 23.3. If x1 ok, . . . , xn ok ⊢d ok according to the statics of DPCF, then x1 : dyn, . . . , xn : dyn ⊢
d† : dyn in HPCF.
The proof of Theorem 23.3 is given by induction on the structure of d based on the following
2The expression error aborts the computation with an error; this can be accomplished using exceptions, which are the
subject of Chapter 29.

206
23.3 Optimization of Dynamic Typing
translation:
x† ≜x
num[n]† ≜new[num](n)
zero† ≜new[num](z)
succ(d)† ≜new[num](s(cast[num](d†)))
ifz{d0; x.d1}(d) ≜ifz{d†
0; x.[new[num](x)/x]d†
1}(cast[num](d†))
(fun(x.d))† ≜new[fun](λ (x : dyn) d†)
(ap(d1; d2))† ≜cast[fun](d†
1)(d†
2)
fix(x.d) ≜fix{dyn}(x.d†)
A rigorous proof of correctness of this translation requires methods like those in Chapter 47.
23.3
Optimization of Dynamic Typing
The language HPCF combines static and dynamic typing by enriching PCF with the type dyn of
classiﬁed values. It is, for this reason, called a hybrid language. Unlike a purely dynamic type sys-
tem, a hybrid type system can express invariants that are crucial to the optimization of programs
in HPCF.
Consider the addition function in DPCF given in Section 22.3, which we transcribe here for
convenience:
λ (x) fix p is λ (y) ifz y {zero ,→x | succ(y′) ,→succ(p(y′))}.
It is a value of type dyn in HPCF given as follows:
fun ! λ (x : dyn) fix p : dyn is fun ! λ (y : dyn) ex,p,y,
(23.8)
within which the fragment
x : dyn, p : dyn, y : dyn ⊢ex,p,y : dyn
stands for the expression
ifz (y @ num) {zero ,→x | succ(y′) ,→num ! (s((p @ fun)(num ! y′) @ num))}.
The embedding into HPCF makes explicit the run-time checks that are implicit in the dynamics of
DPCF.
Careful examination of the embedded formulation of addition reveals a great deal of redun-
dancy and overhead that can be eliminated in the statically typed version. Eliminating this redun-
dancy requires a static type discipline, because the intermediate computations involve values of a
type other than dyn. This transformation shows that the freedoms offered by dynamic languages

23.3 Optimization of Dynamic Typing
207
accruing from the absence of types are, instead, limitations on their expressive power arising from
the restriction to a single type.
The ﬁrst redundancy arises from the use of recursion in a dynamic language. In the above ex-
ample we use recursion to deﬁne the inner loop p of the computation. The value p is, by deﬁnition,
a λ-abstraction, which is explicitly tagged as a function. Yet the call to p within the loop checks
at run-time whether p is in fact a function before applying it. Because p is an internally deﬁned
function, all of its call sites are under the control of the addition function, which means that there
is no need for such pessimism at calls to p, provided that we change its type to dyn ⇀dyn, which
directly expresses the invariant that p is a function acting on dynamic values.
Performing this transformation, we obtain the following reformulation of the addition function
that eliminates this redundancy:
fun ! λ (x : dyn) fun ! fix p : dyn ⇀dyn is λ (y : dyn) e′
x,p,y,
where e′
x,p,y is the expression
ifz (y @ num) {zero ,→x | succ(y′) ,→num ! (s(p(num ! y′) @ num))}.
We have “hoisted” the function class label out of the loop, and suppressed the cast inside the loop.
Correspondingly, the type of p has changed to dyn ⇀dyn.
Next, note that the variable y of type dyn is cast to a number on each iteration of the loop before
it is tested for zero. Because this function is recursive, the bindings of y arise in one of two ways:
at the initial call to the addition function, and on each recursive call. But the recursive call is made
on the predecessor of y, which is a true natural number that is labeled with num at the call site,
only to be removed by the class check at the conditional on the next iteration. This observation
suggests that we hoist the check on y outside of the loop, and avoid labeling the argument to the
recursive call. Doing so changes the type of the function, however, from dyn ⇀dyn to nat ⇀dyn.
Consequently, further changes are required to ensure that the entire function remains well-typed.
Before doing so, let us make another observation. The result of the recursive call is checked to
ensure that it has class num, and, if so, the underlying value is incremented and labeled with class
num. If the result of the recursive call came from an earlier use of this branch of the conditional,
then obviously the class check is redundant, because we know that it must have class num. But
what if the result came from the other branch of the conditional? In that case the function returns
x, which need not be of class num because it is provided by the caller of the function. However,
we may reasonably insist that it is an error to call addition with a non-numeric argument. This
restriction can be enforced by replacing x in the zero branch of the conditional by x @ num.
Combining these optimizations we obtain the inner loop e′′
x deﬁned as follows:
fix p : nat ⇀nat is λ (y : nat) ifz y {zero ,→x @ num | succ(y′) ,→s(p(y′))}.
It has the type nat ⇀nat, and runs without class checks when applied to a natural number.
Finally, recall that the goal is to deﬁne a version of addition that works on values of type dyn.
Thus we need a value of type dyn ⇀dyn, but what we have at hand is a function of type nat ⇀nat.
It can be converted to the needed form by pre-composing with a cast to num and post-composing
with a coercion to num:
fun ! λ (x : dyn) fun ! λ (y : dyn) num ! (e′′
x(y @ num)).

208
23.4 Static Versus Dynamic Typing
The innermost λ-abstraction converts the function e′′
x from type nat ⇀nat to type dyn ⇀dyn by
composing it with a class check that ensures that y is a natural number at the initial call site, and
applies a label to the result to restore it to type dyn.
The outcome of these transformations is that the inner loop of the computation runs at “full
speed”, without any manipulation of tags on functions or numbers. But the outermost form of ad-
dition remains; it is a value of type dyn encapsulating a curried function that takes two arguments
of type dyn. Doing so preserves the correctness of all calls to addition, which pass and return
values of type dyn, while optimizing its execution during the computation. Of course, we could
strip the class tags from the addition function, changing its type from dyn to the more descriptive
dyn ⇀dyn ⇀dyn, but this imposes the obligation on the caller to treat addition not as a value of
type dyn, but rather as a function that must be applied to two successive values of type dyn whose
class is num. As long as the call sites to addition are under programmer control, there is no obstacle
to effecting this transformation. It is only when there are external call sites, not directly under
programmer control, that there is any need to package addition as a value of type dyn. Applying
this principle generally, we see that dynamic typing is only of marginal utility—it is used only at
the margins of a system where uncontrolled calls arise. Internally to a system there is no beneﬁt,
and considerable drawback, to restricting attention to the type dyn.
23.4
Static Versus Dynamic Typing
There have been many attempts by advocates of dynamic typing to distinguish dynamic from
static languages. It is useful to consider the supposed distinctions from the present viewpoint.
1. Dynamic languages associate types with values, whereas static languages associate types to variables.
Dynamic languages associate classes, not types, to values by tagging them with identiﬁers
such as num and fun. This form of classiﬁcation amounts to a use of recursive sum types
within a statically typed language, and hence cannot be seen as a distinguishing feature of
dynamic languages. Moreover, static languages assign types to expressions, not just vari-
ables. Because dynamic languages are just particular static languages (with a single type),
the same can be said of dynamic languages.
2. Dynamic languages check types at run-time, whereas static language check types at compile time.
Dynamic languages are just as surely statically typed as static languages, albeit for a degen-
erate type system with only one type. As we have seen, dynamic languages do perform
class checks at run-time, but so too do static languages that admit sum types. The difference
is only the extent to which we must use classiﬁcation: always in a dynamic language, only
as necessary in a static language.
3. Dynamic languages support heterogeneous collections, whereas static languages support homoge-
neous collections. The purpose of sum types is to support heterogeneity, so that any static
language with sums admits heterogeneous data structures. A typical example is a list such
as
cons(num[1]; cons(fun(x.x); nil))

23.5 Notes
209
(written in abstract syntax for emphasis). It is sometimes said that such a list is not repre-
sentable in a static language, because of the disparate nature of its components. Whether in
a static or a dynamic language, lists are type homogeneous, but can be class heterogeneous.
All elements of the above list are of type dyn; the ﬁrst is of class num, and the second is of
class fun.
Thus the seeming opposition between static and dynamic typing is an illusion. The question is
not whether to have static typing, but rather how best to embrace it. Conﬁning one’s attention to
a single recursive type seems pointlessly restrictive. Indeed, many so-called untyped languages
have implicit concessions to there being more than one type. The classic example is the ubiquitous
concept of “multi-argument functions”, which are a concession to the existence of products of
the type of values (with pattern matching). It is then a short path to considering “multi-result
functions”, and other ad hoc language features that amount to admitting a richer and richer static
type discipline.
23.5
Notes
Viewing dynamic languages as static languages with recursive types was ﬁrst proposed by Dana
Scott (Scott, 1980b), who also suggested glossing “untyped” as “uni-typed”. Most modern stati-
cally typed languages, such as Java or Haskell or OCaml or SML, include a type similar to dyn, or
admit recursive types with which to deﬁne it. For this reason one might expect that the opposition
between dynamic and static typing would fade away, but industrial and academic trends suggest
otherwise.
Exercises
23.1. Consider the extensions to DPCF described in Section 22.2 to admit null and pairing and
their associated operations. Extend the statics and dynamics of HPCF to account for these
extensions, and give a translation of the null and pairing operations described informally in
Chapter 22 in terms of this extension to HPCF.
23.2. Continue the interpretation of the null and pairing operations in HPCF given in Exercise 23.1
to provide an interpretation in FPC. Speciﬁcally, deﬁne the expanded dyn as a recursive type,
and give a direct implementation of the null and pairing primitives in terms of this recursive
type.
23.3. Consider the append function deﬁned in Chapter 22 using nil and cons to represent lists:
fix a is λ (x) λ (y) cond(x; cons(car(x); a(cdr(x))(y)); y).
Rewrite append in HPCF using the deﬁnitions given in Exercise 23.1. Then optimize the
implementation to eliminate unnecessary overhead while ensuring that append still has type
dyn.

210
23.5 Notes

Part X
Subtyping


Chapter 24
Structural Subtyping
A subtype relation is a pre-order (reﬂexive and transitive relation) on types that validates the sub-
sumption principle:
if τ′ is a subtype of τ, then a value of type τ′ may be provided when a value of type τ is required.
The subsumption principle relaxes the strictures of a type system to allow values of one type to be
treated as values of another.
Experience shows that the subsumption principle, although useful as a general guide, can be
tricky to apply correctly in practice. The key to getting it right is the principle of introduction and
elimination. To see whether a candidate subtyping relationship is sensible, it sufﬁces to consider
whether every introduction form of the subtype can be safely manipulated by every elimination
form of the supertype. A subtyping principle makes sense only if it passes this test; the proof of
the type safety theorem for a given subtyping relation ensures that this is the case.
A good way to get a subtyping principle wrong is to think of a type merely as a set of values
(generated by introduction forms), and to consider whether every value of the subtype can also
be considered to be a value of the supertype. The intuition behind this approach is to think of
subtyping as akin to the subset relation in ordinary mathematics. But, as we shall see, this can
lead to serious errors, because it fails to take account of the elimination forms that are applicable
to the supertype. It is not enough to think only of the introduction forms; subtyping is a matter of
behavior, and not containment.
24.1
Subsumption
A subtyping judgment has the form τ′ <: τ, and states that τ′ is a subtype of τ. At the least we
demand that the following structural rules of subtyping be admissible:
τ <: τ
(24.1a)
τ′′ <: τ′
τ′ <: τ
τ′′ <: τ
(24.1b)

214
24.2 Varieties of Subtyping
In practice we either tacitly include these rules as primitive, or prove that they are admissible for
a given set of subtyping rules.
The point of a subtyping relation is to enlarge the set of well-typed programs, which is accom-
plished by the subsumption rule:
Γ ⊢e : τ′
τ′ <: τ
Γ ⊢e : τ
(24.2)
In contrast to most other typing rules, the rule of subsumption is not syntax-directed, because
it does not constrain the form of e. That is, the subsumption rule can be applied to any form of
expression. In particular, to show that e : τ, we have two choices: either apply the rule appropriate
to the particular form of e, or apply the subsumption rule, checking that e : τ′ and τ′ <: τ.
24.2
Varieties of Subtyping
In this section we will informally explore several different forms of subtyping in the context of
extensions of the language FPC introduced in Chapter 20.
Numeric Types
We begin with an informal discussion of numeric types such as are common in many programming
languages. Our mathematical experience suggests subtyping relationships among numeric types.
For example, in a language with types int, rat, and real, representing the integers, the rationals,
and the reals, it is tempting to postulate the subtyping relationships
int <: rat <: real
by analogy with the set containments
Z ⊆Q ⊆R.
But are these subtyping relationships sensible? The answer depends on the representations
and interpretations of these types. Even in mathematics, the containments just mentioned are
usually not true—or are true only in a rough sense. For example, the set of rational numbers can
be considered to consist of ordered pairs (m, n), with n ̸= 0 and gcd(m, n) = 1, representing the
ratio m/n. The set Z of integers can be isomorphically embedded within Q by identifying n ∈Z
with the ratio n/1. Similarly, the real numbers are often represented as convergent sequences of
rationals, so that strictly speaking the rationals are not a subset of the reals, but rather can be
embedded in them by choosing a canonical representative (a particular convergent sequence) of
each rational.
For mathematical purposes it is entirely reasonable to overlook ﬁne distinctions such as that
between Z and its embedding within Q. Ignoring the difference is justiﬁed because the operations
on rationals restrict to the embedding in the expected way: if we add two integers thought of
as rationals in the canonical way, then the result is the rational associated with their sum. And
similarly for the other operations, provided that we take some care in deﬁning them to ensure that
it all works out properly. For the purposes of computing, however, we must also take account of
algorithmic efﬁciency and the ﬁniteness of machine representations. For example, what are often

24.2 Varieties of Subtyping
215
called “real numbers” in a programming language are, of course, ﬂoating point numbers, a ﬁnite
subset of the rational numbers. Not every rational can be exactly represented as a ﬂoating point
number, nor does ﬂoating point arithmetic restrict to rational arithmetic, even when its arguments
are exactly represented as ﬂoating point numbers.
Product Types
Product types give rise to a form of subtyping based on the subsumption principle. The only
elimination form applicable to a value of product type is a projection. Under mild assumptions
about the dynamics of projections, we may consider one product type to be a subtype of another by
considering whether the projections applicable to the supertype can be validly applied to values
of the subtype.
Consider a context in which a value of type τ = ⟨τj⟩j∈J is required. The statics of ﬁnite products
(rules (10.3)) ensures that the only operation we may perform on a value of type τ, other than to
bind it to a variable, is to take the jth projection from it for some j ∈J to obtain a value of type
τj. Now suppose that e is of type τ′. For the projection e · j to be well-formed, then τ′ is a ﬁnite
product type ⟨τ′
i ⟩i∈I such that j ∈I. Moreover, for the projection to be of type τj, it is enough to
require that τ′
j = τj. Because j ∈J is arbitrary, we arrive at the following subtyping rule for ﬁnite
product types:
J ⊆I
∏i∈I τi <: ∏j∈J τj
.
(24.3)
This rule suﬁces for the required subtyping, but not necessary; we will consider a more liberal form
of this rule in Section 24.3. The justiﬁcation for rule (24.3) is that we may evaluate e · i regardless
of the actual form of e, provided only that it has a ﬁeld indexed by i ∈I.
Sum Types
By an argument dual to the one given for ﬁnite product types we may derive a related subtyping
rule for ﬁnite sum types. If a value of type ∑j∈J τj is required, the statics of sums (rules (11.3))
ensures that the only non-trivial operation that we may perform on that value is a J-indexed case
analysis. If we provide a value of type ∑i∈I τ′
i instead, no difﬁculty will arise so long as I ⊆J and
each τ′
i is equal to τi. If the containment is strict, some cases cannot arise, but this does not disrupt
safety.
I ⊆J
∑i∈I τi <: ∑j∈J τj
.
(24.4)
Note well the reversal of the containment as compared to rule (24.3).
Dynamic Types
A popular form of subtyping is associated with the type dyn introduced in Chapter 23. The type
dyn provides no information about the class of a value of this type. One might argue that it is
whole the point of dynamic typing to suppress this information statically, making it available only

216
24.3 Variance
dynamically. On the other hand, it is not much trouble to introduce subtypes of dyn that specify
the class of a value, relying on subsumption to “forget” the class when it cannot be determined
statically.
Working in the context of Chapter 23 this amounts to introduce two new types, dyn[num] and
dyn[fun], governed by the following two subtyping axioms:
dyn[num] <: dyn
(24.5a)
dyn[fun] <: dyn
(24.5b)
Of course, in a richer language with more classes of dynamic values one would correspondingly
introduce more such subtypes of dyn, one for each additional class. As a matter of notation, the
type dyn is frequently spelled object, and its class-speciﬁc subtypes dyn[num] and dyn[fun], are
often written as num and fun, respectively. But doing so invites confusion between the separate
concepts of class and type, as discussed in detail in Chapters 22 and 23.
The class-speciﬁc subtypes of dyn come into play by reformulating the typing rules for intro-
ducing values of type dyn to note the class of the created value:
Γ ⊢e : nat
Γ ⊢new[num](e) : dyn[num]
(24.6a)
Γ ⊢e : dyn ⇀dyn
Γ ⊢new[fun](e) : dyn[fun]
(24.6b)
Thus, in this formulation, classiﬁed values “start life” with class-speciﬁc types, because in those
cases it is statically apparent what is the class of the introduced value. Subsumption is used to
weaken the type to dyn in those cases where no static prediction can be made—for example, when
the branches of a conditional evaluate to dynamic values of different classes it is necessary to
weaken the type of the branches to dyn.
The advantage of such a subtyping mechanism is that we can express more precise types, such
as the type dyn[num] ⇀dyn[num] of functions mapping a value of type dyn with class num to another
such value. This typing is more precise than, say, dyn ⇀dyn, which merely classiﬁes functions that
act on dynamically typed values. In this way weak invariants can be expressed and enforced, but
only insofar as it is possible to track the classes of the values involved in a computation. Subtyp-
ing is not nearly a powerful enough mechanism for practical situations, rendering the additional
speciﬁcity not worth the effort of including it. (A more powerful approach is developed in Chap-
ter 25.)
24.3
Variance
In addition to basic subtyping principles such as those considered in Section 24.2, it is also impor-
tant to consider the effect of subtyping on type constructors. A type constructor covariant in an

24.3 Variance
217
argument if subtyping in that argument is preserved by the constructor. It is contravariant if sub-
typing in that argument is reversed by the constructor. It is invariant in an argument if subtyping
for the constructed type is not affected by subtyping in that argument.
Product and Sum Types
Finite product types are covariant in each ﬁeld. For if e is of type ∏i∈I τ′
i , and the projection e · j is
to be of type τj, then it sufﬁces to require that j ∈I and τ′
j <: τj.
(∀i ∈I) τ′
i <: τi
∏i∈I τ′
i <: ∏i∈I τi
(24.7)
It is implicit in this rule that the dynamics of projection cannot be sensitive to the precise type of
any of the ﬁelds of a value of ﬁnite product type.
Finite sum types are also covariant, because each branch of a case analysis on a value of the
supertype expects a value of the corresponding summand, for which it sufﬁces to provide a value
of the corresponding subtype summand:
(∀i ∈I) τ′
i <: τi
∑i∈I τ′
i <: ∑i∈I τi
(24.8)
Partial Function Types
The variance of the function type constructors is a bit more subtle. Let us consider ﬁrst the variance
of the function type in its range. Suppose that e : τ1 ⇀τ′
2. Then if e1 : τ1, then e(e1) : τ′
2, and if
τ′
2 <: τ2, then e(e1) : τ2 as well.
τ′
2 <: τ2
τ1 ⇀τ′
2 <: τ1 ⇀τ2
(24.9)
Every function that delivers a value of type τ′
2 also delivers a value of type τ2, provided that
τ′
2 <: τ2. Thus the function type constructor is covariant in its range.
Now let us consider the variance of the function type in its domain. Suppose again that e :
τ1 ⇀τ2. Then e can be applied to any value of type τ1 to obtain a value of type τ2. Hence, by the
subsumption principle, it can be applied to any value of a subtype τ′
1 of τ1, and it will still deliver
a value of type τ2. Consequently, we may just as well think of e as having type τ′
1 ⇀τ2.
τ′
1 <: τ1
τ1 ⇀τ2 <: τ′
1 ⇀τ2
(24.10)
The function type is contravariant in its domain position. Note well the reversal of the subtyping
relation in the premise as compared to the conclusion of the rule!
Combining these rules we obtain the following general principle of contra- and covariance for
function types:
τ′
1 <: τ1
τ′
2 <: τ2
τ1 ⇀τ′
2 <: τ′
1 ⇀τ2
(24.11)

218
24.3 Variance
Beware of the reversal of the ordering in the domain!
Recursive Types
The language FPC has a partial function types, which behave the same under subtyping as total
function types, sums and products, which behave as described above, and recursive types, which
introduce some subtleties that have been the source of error in language design. To gain some
intuition, consider the type of labeled binary trees with natural numbers at each node,
rec t is [empty ,→unit, binode ,→⟨data ,→nat, lft ,→t, rht ,→t⟩],
and the type of “bare” binary trees, without data attached to the nodes,
rec t is [empty ,→unit, binode ,→⟨lft ,→t, rht ,→t⟩].
Is either a subtype of the other? Intuitively, we might expect the type of labeled binary trees to be
a subtype of the type of bare binary trees, because any use of a bare binary tree can simply ignore
the presence of the label.
Now consider the type of bare “two-three” trees with two sorts of nodes, those with two chil-
dren, and those with three:
rec t is [empty ,→unit, binode ,→τ2, trinode ,→τ3],
where
τ2 ≜⟨lft ,→t, rht ,→t⟩, and
τ3 ≜⟨lft ,→t, mid ,→t, rht ,→t⟩.
What subtype relationships should hold between this type and the preceding two tree types? In-
tuitively the type of bare two-three trees should be a supertype of the type of bare binary trees,
because any use of a two-three tree proceeds by three-way case analysis, which covers both forms
of binary tree.
To capture the pattern illustrated by these examples, we need a subtyping rule for recursive
types. It is tempting to consider the following rule:
t type ⊢τ′ <: τ
rec t is τ′ <: rec t is τ ??
(24.12)
That is, to check whether one recursive type is a subtype of the other, we simply compare their
bodies, with the bound variable treated as an argument. Notice that by reﬂexivity of subtyping,
we have t <: t, and hence we may use this fact in the derivation of τ′ <: τ.
Rule (24.12) validates the intuitively plausible subtyping between labeled binary tree and bare
binary trees just described. Deriving this requires checking that the subtyping relationship
⟨data ,→nat, lft ,→t, rht ,→t⟩<: ⟨lft ,→t, rht ,→t⟩,

24.3 Variance
219
holds generically in t, which is evidently the case.
Unfortunately, Rule (24.12) also underwrites incorrect subtyping relationships, as well as some
correct ones. As an example of what goes wrong, consider the recursive types
τ′ = rec t is ⟨a ,→t ⇀nat, b ,→t ⇀int⟩
and
τ = rec t is ⟨a ,→t ⇀int, b ,→t ⇀int⟩.
We assume for the sake of the example that nat <: int, so that by using rule (24.12) we may derive
τ′ <: τ, which is incorrect. Let e : τ′ be the expression
fold(⟨a ,→λ (x : τ′) 4, b ,→λ (x : τ′) q((unfold(x) · a)(x))⟩),
where q : nat ⇀nat is the discrete square root function. Because τ′ <: τ, it follows that e : τ as
well, and hence
unfold(e) : ⟨a ,→τ ⇀int, b ,→τ ⇀int⟩.
Now let e′ : τ be the expression
fold(⟨a ,→λ (x : τ) -4, b ,→λ (x : τ) 0⟩).
(The important point about e′ is that the a method returns a negative number; the b method is of
no signiﬁcance.) To ﬁnish the proof, observe that
(unfold(e) · b)(e′) 7−→∗q(-4),
which is a stuck state. We have derived a well-typed program that “gets stuck”, refuting type
safety!
Rule (24.12) is therefore incorrect. But what has gone wrong? The error lies in the choice of a
single variable to stand for both recursive types, which does not correctly model self-reference. In
effect we are treating two distinct recursive types as if they were equal while checking their bodies
for a subtyping relationship. But this is clearly wrong! It fails to take account of the self-referential
nature of recursive types. On the left side the bound variable stands for the subtype, whereas on
the right the bound variable stands for the super-type. Confusing them leads to the unsoundness
just illustrated.
As is often the case with self-reference, the solution is to assume what we are trying to prove,
and check that this assumption can be maintained by examining the bodies of the recursive types.
To do so we use hypothetical judgments of the form ∆⊢τ′ <: τ, where ∆consists of hypotheses
t type and t <: τ that declares a fresh type variable t that is not otherwise declared in ∆. Using such
hypothetical judgments we may state the correct rule for subtyping recursive types as follows:
∆, t type, t′ type, t′ <: t ⊢τ′ <: τ
∆, t′ type ⊢τ′ type
∆, t type ⊢τ type
∆⊢rec t′ is τ′ <: rec t is τ
.
(24.13)
That is, to check whether rec t′ is τ′ <: rec t is τ, we assume that t′ <: t, because t′ and t stand for
the corresponding recursive types, and check that τ′ <: τ under this assumption. It is instructive
to check that the unsound subtyping example given above is not derivable using this rule: the
subtyping assumption is at odds with the contravariance of the function type in its domain.

220
24.3 Variance
Quantiﬁed Types
Consider extending FPC with the universal and existential quantiﬁed types discussed in Chap-
ters 16 and 17. The variance principles for the quantiﬁers state that they are uniformly covariant
in the quantiﬁed types:
∆, t type ⊢τ′ <: τ
∆⊢∀(t.τ′) <: ∀(t.τ)
(24.14a)
∆, t type ⊢τ′ <: τ
∆⊢∃(t.τ′) <: ∃(t.τ)
(24.14b)
Consequently, we may derive the principle of substitution:
Lemma 24.1. If ∆, t type ⊢τ1 <: τ2, and ∆⊢τ type, then ∆⊢[τ/t]τ1 <: [τ/t]τ2.
Proof. By induction on the subtyping derivation.
It is easy to check that the above variance principles for the quantiﬁers are consistent with the
principle of subsumption. For example, a package of the subtype ∃(t.τ′) consists of a representa-
tion type ρ and an implementation e of type [ρ/t]τ′. But if t type ⊢τ′ <: τ, we have by substitution
that [ρ/t]τ′ <: [ρ/t]τ, and hence e is also an implementation of type [ρ/t]τ. So the package is also
of the supertype.
It is natural to extend subtyping to the quantiﬁers by allowing quantiﬁcation over all subtypes
of a speciﬁed type; this is called bounded quantiﬁcation.
∆, t type, t <: τ ⊢t <: τ
(24.15a)
∆⊢τ :: T
∆⊢τ <: τ
(24.15b)
∆⊢τ′′ <: τ′
∆⊢τ′ <: τ
∆⊢τ′′ <: τ
(24.15c)
∆⊢τ′
1 <: τ1
∆, t type, t <: τ′
1 ⊢τ2 <: τ′
2
∆⊢∀t <: τ1.τ2 <: ∀t <: τ′
1.τ′
2
(24.15d)
∆⊢τ1 <: τ′
1
∆, t type, t <: τ1 ⊢τ2 <: τ′
2
∆⊢∃t <: τ1.τ2 <: ∃t <: τ′
1.τ′
2
(24.15e)
Rule (24.15d) states that the universal quantiﬁer is contravariant in its bound, whereas rule (24.15e)
states that the existential quantiﬁer is covariant in its bound.

24.4 Dynamics and Safety
221
24.4
Dynamics and Safety
There is a subtle assumption in the deﬁnition of product subtyping in Section 24.2, namely that the
same projection operation from an I-tuple applies also to a J-tuple, provided J ⊇I. But this need
not be the case. One could represent I-tuples differently from J-tuples at will, so that the meaning
of the projection at position i ∈I ⊆J is different in the two cases. Nothing rules out this possibility,
yet product subtyping relies on it not being the case. From this point of view product subtyping
is not well-justiﬁed, but one may instead consider that subtyping limits possible implementations
to ensure that it makes sense.
Similar considerations apply to sum types. An J-way case analysis need not be applicable to
an I-way value of sum type, even when I ⊆J and all the types in common agree. For example,
one might represent values of a sum type with a “small” index set in a way that is not applicable
for a “large” index set. In that case the “large” case analysis would not make sense on a value
of “small” sum type. Here again we may consider either that subtyping is not justiﬁed, or that it
imposes limitations on the implementation that are not otherwise forced.
These considerations merit careful consideration of the safety of languages with subtyping. As
an illustrative case we consider the safety of FPC enriched with product subtyping. The main con-
cern is that the subsumption rule obscures the “true” type of a value, complicating the canonical
forms lemma. Moreover, we assume that the same projection makes sense for a wider tuple than
a narrower one, provided that it is within range.
Lemma 24.2 (Structurality).
1. The tuple subtyping relation is reﬂexive and transitive.
2. The typing judgment Γ ⊢e : τ is closed under weakening and substitution.
Proof.
1. Reﬂexivity is proved by induction on the structure of types. Transitivity is proved by in-
duction on the derivations of the judgments τ′′ <: τ′ and τ′ <: τ to obtain a derivation of
τ′′ <: τ.
2. By induction on rules (10.3), augmented by rule (24.2).
Lemma 24.3 (Inversion).
1. If e · j : τ, then e : ∏i∈I τi, j ∈I, and τj <: τ.
2. If ⟨ei⟩i∈I : τ, then ∏i∈I τ′
i <: τ where ei : τ′
i for each i ∈I.
3. If τ′ <: ∏j∈J τj, then τ′ = ∏i∈I τ′
i for some I and some types τ′
i for i ∈I.
4. If ∏i∈I τ′
i <: ∏j∈J τj, then J ⊆I and τ′
j <: τj for each j ∈J.
Proof. By induction on the subtyping and typing rules, paying special attention to rule (24.2).

222
24.5 Notes
Theorem 24.4 (Preservation). If e : τ and e 7−→e′, then e′ : τ.
Proof. By induction on rules (10.4). For example, consider rule (10.4d), so that e = ⟨ei⟩i∈I · k and
e′ = ek. By Lemma 24.3 we have ⟨ei⟩i∈I : ∏j∈J τj, with k ∈J and τk <: τ. By another application of
Lemma 24.3 for each i ∈I there exists τ′
i such that ei : τ′
i and ∏i∈I τ′
i <: ∏j∈J τj. By Lemma 24.3
again, we have J ⊆I and τ′
j <: τj for each j ∈J. But then ek : τk, as desired. The remaining cases
are similar.
Lemma 24.5 (Canonical Forms). If e val and e : ∏j∈J τj, then e is of the form ⟨ei⟩i∈I, where J ⊆I, and
ej : τj for each j ∈J.
Proof. By induction on rules (10.3) augmented by rule (24.2).
Theorem 24.6 (Progress). If e : τ, then either e val or there exists e′ such that e 7−→e′.
Proof. By induction on rules (10.3) augmented by rule (24.2). The rule of subsumption is han-
dled by appeal to the inductive hypothesis on the premise of the rule. rule (10.4d) follows from
Lemma 24.5.
24.5
Notes
Subtyping is perhaps the most widely misunderstood concept in programming languages. Sub-
typing is principally a convenience, akin to type inference, that makes some programs simpler to
write. But the subsumption rule cuts both ways. Inasmuch as it allows the implicit passage from
τ′ to τ when τ′ is a subtype of τ, it also weakens the meaning of a type assertion e : τ to mean
that e has some type contained in the type τ. Subsumption precludes expressing the requirement
that e has exactly the type τ, or that two expressions jointly have the same type. And it is just this
weakness that creates so many of the difﬁculties with subtyping.
Much has been written about subtyping, often in relation to object-oriented programming.
Standard ML (Milner et al., 1997) is one of the ﬁrst languages to make use of subtyping, in two
forms, called enrichment and realization. The former corresponds to product subtyping, and the
latter to the “forgetful” subtyping associated with type deﬁnitions (see Chapter 43). The ﬁrst sys-
tematic studies of subtyping include those by Mitchell (1984); Reynolds (1980), and Cardelli (1988).
Pierce (2002) give a thorough account of subtyping, especially of recursive and polymorphic types,
and proves that subtyping for bounded impredicative universal quantiﬁcation is undecidable.
Exercises
24.1. Check the variance of the type
(unit ⇀τ) × (τ ⇀unit).
When viewed as a constructor with argument τ, is it covariant or contravariant? Give a
precise proof or counterexample in each case.

24.5 Notes
223
24.2. Consider the two recursive types,
ρ1 ≜rec t is ⟨eq ,→(t ⇀bool)⟩,
and
ρ2 ≜rec t is ⟨eq ,→(t ⇀bool), f ,→bool⟩.
It is clear that ρ1 could not be a subtype of ρ2, because, viewed as a product after unrolling,
a value of the former type lacks a component that a value of the latter has. But is ρ2 a
subtype of ρ1? If so, prove it by exhibiting a derivation of this fact using the rules given
in Section 24.3. If not, give a counterexample showing that the suggested subtyping would
violate type safety.
24.3. Another approach to the dynamics of subtyping that ensures safety, but gives subsumption
dynamic signiﬁcance, associates a witness, called a coercion, to each subtyping relation, and
inserts a coercion wherever subsumption is used. More precisely,
(a) Assign to each valid subtyping τ <: τ′ a coercion function χ : τ ⇀τ′ that transforms a
value of type τ into a value of type τ′.
(b) Interpret the subsumption rule as implicit coercion. Speciﬁcally, when τ <: τ′ is wit-
nessed by χ : τ ⇀τ′, applying subsumption to e : τ inserts an application of χ to obtain
χ(e) : τ′.
Formulate this idea precisely for the case of a subtype relation generated by “width” sub-
typing for products, and the variance principles for product, sum and function types. Your
solution should make clear that it evades the tacit projection assumption mentioned above.
But there may be more than one coercion χ : τ ⇀τ′ corresponding to the subtyping τ <: τ′.
The meaning of a program would then depend on which coercion is chosen when subsump-
tion is used. If there is exactly one coercion for each subtyping relation, it is said to be
coherent. Is your coercion interpretation of product subtyping coherent? (A proper treatment
of coherence requires expression equivalence, which is discussed in Chapter 47.)

224
24.5 Notes

Chapter 25
Behavioral Typing
In Chapter 23 we demonstrated that dynamic typing is but a mode of use of static typing, one in
which dynamically typed values are of type dyn, a particular recursive sum type. A value of type
dyn is always of the form new[c](e), where c is its class and e is its underlying value. Importantly,
the class c determines the type of the underlying value of a dynamic value. The type system of the
hybrid language is rather weak in that every dynamically classiﬁed value has the same type, and
there is no mention of the class in its type. To correct this shortcoming it is common to enrich the
type system of the hybrid language so as to capture such information, for example as described in
Section 24.2.
In such a situation subtyping is used to resolve a fundamental tension between structure and
behavior in the design of type systems. On the one hand types determine the structure of a pro-
gramming language, and on the other serve a behavioral speciﬁcations of expressions written in
that language. Subtyping attempts to resolve this tension, unsuccessfully, by allowing certain
forms of retyping. Although subtyping works reasonably well for small examples, things get far
more complicated when we wish to specify the deep structure of a value, say that it is of a class c
and its underlying value is of another class d whose underlying value is a natural number. There is
no limit to the degree of speciﬁcity one may wish in such descriptions, which gives rise to endless
variations on type systems to accommodate various special situations.
Another resolution of the tension between structure and behavior in typing is to separate these
aspects by distinguishing types from type reﬁnements. Type reﬁnements specify the execution be-
havior of an expression of a particular type using speciﬁcations that capture whatever properties
are of interest, limited only by the difﬁculty of proving that a program satisﬁes the speciﬁcation
given by a reﬁnement.
Certain limited forms of behavioral speciﬁcations can express many useful properties of pro-
grams while remaining mechanically checkable. These include the fundamental behavioral prop-
erties determined by the type itself, but can be extended to include sharper conditions than just
these structural properties. In this chapter we will consider a particular notion of reﬁnement tai-
lored to the hybrid language of Chapter 23. It is based on two basic principles:
1. Type constructors, such as product, sum, and function space, act on reﬁnements of their com-

226
25.1 Statics
ponent types to induce a reﬁnement on the compound type formed by those constructors.
2. It is useful to track the class of a value of type dyn, and to assign multiple reﬁnements to
specify distinct behaviors of expressions involving values of dynamic type.
We will formulate a system of reﬁnements based on these principles that ensures that a well-
reﬁned program cannot incur a run-time error arising from the attempt to cast a value to a class
other than its own.
25.1
Statics
We will develop a system of reﬁnements for the extension with sums and products of the language
HPCF deﬁned in Chapter 23 in which there are but two classes of values of type dyn, namely num
and fun.1 The syntax of reﬁnements φ is given by the following grammar:
Ref
φ
::=
true{τ}
⊤τ
truth
and{τ}(φ1;φ2)
φ1 ∧τ φ2
conjunction
new[num](φ)
num ! φ
dynamic number
new[fun](φ)
fun ! φ
dynamic function
prod(φ1; φ2)
φ1 × φ2
product
sum(φ1; φ2)
φ1 + φ2
sum
parr(φ1; φ2)
φ1 ⇀φ2
function
Informally, a reﬁnement is a predicate specifying a property of the values of some type. Equiv-
alently, one may think of a reﬁnement as a subset of the values of a type, those that satisfy the
speciﬁed property. To expose the dependence of reﬁnements on types, the syntax of truth and
conjunction is parameterized by the type whose values they govern. In most cases the underlying
type is clear from context, in which case it is omitted. Note that the syntax of the product, sum,
and function reﬁnements is exactly the same as the syntax of the types they govern, but they are
reﬁnements, not types.
The judgment φ ⊑τ means that φ is a reﬁnement of the type τ. It is deﬁned by the following
rules:
⊤⊑τ
(25.1a)
φ1 ⊑τ
φ2 ⊑τ
φ1 ∧φ2 ⊑τ
(25.1b)
φ ⊑nat
num ! φ ⊑dyn
(25.1c)
φ ⊑dyn ⇀dyn
fun ! φ ⊑dyn
(25.1d)
1Of course, in a richer language there would be more classes than just these two, each with an associated type of the
underlying data that it classiﬁes.

25.1 Statics
227
φ1 ⊑τ1
φ2 ⊑τ2
φ1 × φ2 ⊑τ1 × τ2
(25.1e)
φ1 ⊑τ1
φ2 ⊑τ2
φ1 + φ2 ⊑τ1 + τ2
(25.1f)
φ1 ⊑τ1
φ2 ⊑τ2
φ1 ⇀φ2 ⊑τ1 ⇀τ2
(25.1g)
It is easy to see that each reﬁnement reﬁnes a unique type, the underlying type of that reﬁnement.
The concrete syntax num ! φ and fun ! φ is both concise and commonplace, but, beware, it tends to
obscure the critical distinctions between types, classes, and reﬁnements.
The reﬁnement satisfaction judgment, e ∈τ φ, where e : τ and φ ⊑τ, states that the well-typed
expression e exhibits the behavior speciﬁed by φ. The hypothetical form,
x1 ∈τ1 φ1, . . . , xn ∈τn φn ⊢e ∈τ φ,
constrains the expressions that may be substituted for a variable to satisfy its associated reﬁnement
type (which could be the trivial reﬁnement ⊤, that imposes no constraints). We write Φ for such
a ﬁnite sequence of reﬁnement assumptions on variables, called a reﬁnement context. Each such
Φ determines a typing context Γ given by x1 : τ1, . . . , xn : τn, specifying only the types of the
variables involved. We often write ΦΓ to state that Γ is the unique typing context determined by
Φ in this way.
The deﬁnition of the reﬁnement satisfaction judgment makes use of an auxiliary judgment,
φ1 ≤τ φ2, where φ1 ⊑τ and φ2 ⊑τ, which we shall often just write as φ1 ≤φ2 when τ is clear
from context. This judgment is called reﬁnement entailment. It states that the reﬁnement φ1 is at
least as strong as, or no weaker than, the reﬁnement φ2. Informally, this means that if e : τ satisﬁes φ1,
then it must also satisfy φ2. According to this interpretation, reﬁnement entailment is reﬂexive and
transitive. The reﬁnement ⊤, which holds of any well-typed expression, is greater than (entailed
by) any other reﬁnement, and the conjunction of two reﬁnements, φ1 ∧φ2 is the meet (greatest
lower bound) of φ1 and φ2. Because no value can be of two different classes, the conjunction of
num ! φ1 and fun ! φ2 entails any reﬁnement all. Finally, reﬁnement entailment satisﬁes the same
variance principles given for subtyping in Section 24.3.
φ ⊑τ
φ ≤τ φ
(25.2a)
φ1 ≤τ φ2
φ2 ≤τ φ3
φ1 ≤τ φ3
(25.2b)
φ ⊑τ
φ ≤τ ⊤
(25.2c)
φ1 ⊑τ
φ2 ⊑τ
φ1 ∧φ2 ≤τ φ1
(25.2d)
φ1 ⊑τ
φ2 ⊑τ
φ1 ∧φ2 ≤τ φ2
(25.2e)

228
25.1 Statics
φ ≤τ φ1
φ ≤τ φ2
φ ≤τ φ1 ∧φ2
(25.2f)
num ! φ1 ∧fun ! φ2 ≤dyn φ
(25.2g)
φ ≤nat φ′
num ! φ ≤dyn num ! φ′
(25.2h)
φ ≤dyn⇀dyn φ′
fun ! φ ≤dyn fun ! φ′
(25.2i)
φ1 ≤τ1 φ′
1
φ2 ≤τ2 φ′
2
φ1 × φ2 ≤τ1×τ2 φ′
1 × φ′
2
(25.2j)
φ1 ≤τ1 φ′
1
φ2 ≤τ2 φ′
2
φ1 + φ2 ≤τ1+τ2 φ′
1 + φ′
2
(25.2k)
φ′
1 ≤τ1 φ1
φ2 ≤τ2 φ′
2
φ1 ⇀φ2 ≤τ1⇀τ2 φ′
1 ⇀φ′
2
(25.2l)
For the sake of brevity we usually omit the type subscripts from reﬁnements and the reﬁnement
entailment relation.
We are now in a position to deﬁne the reﬁnement satisfaction judgment, ΦΓ ⊢e ∈τ φ, in which
we assume that Γ ⊢e : τ. When such a satisfaction judgment holds, we say that e is well-reﬁned,
a property that can be stated only for expressions that are well-typed. The goal is to ensure that
well-reﬁned expressions do not incur (checked) run-time errors. In the present setting reﬁnements
rule out casting a value of class c to a class c′ ̸= c. The formulation of satisfaction, though simple-
minded, will involve many important ideas.
To simplify the exposition, it is best to present the rules in groups, rather than all at once. The
ﬁrst group consists of the rules that pertain to expressions independently of their types.
ΦΓ, x ∈τ φ ⊢x ∈τ φ
(25.3a)
Φ ⊢e ∈τ φ′
φ′ ≤τ φ
Φ ⊢e ∈τ φ
(25.3b)
Φ ⊢e ∈τ φ1
Φ ⊢e ∈τ φ2
Φ ⊢e ∈τ φ1 ∧φ2
(25.3c)
Φ, x ∈τ φ ⊢e ∈τ φ
Φ ⊢fix x : τ is e ∈τ φ
(25.3d)
Rule (25.3a) expresses the obvious principle that if a variable is assumed to satisfy a reﬁnement φ,
then of course it does satisfy that reﬁnement. As usual, the principle of substitution is admissible.
It states that if a variable is assumed to satisfy φ, then we may substitute for it any expression that

25.1 Statics
229
does satisfy that reﬁnement, and the resulting instance will continue to satisfy the same reﬁnement
it had before the substitution.
Rule (25.3b) is analogous to the subsumption principle given in Chapter 24, although here
it has a subtly different meaning. Speciﬁcally, if an expression e satisﬁes a reﬁnement φ′, and
φ′ is stronger than some reﬁnement φ (as determined by rules (25.2)), then e must also satisfy the
reﬁnement φ. This inference is simply a matter of logic: the judgment φ′ ≤φ states that φ′ logically
entails φ.
Rule (25.3c) expresses the logical meaning of conjunction. If an expression e satisﬁes both φ1
and φ2, then it also satisﬁes φ1 ∧φ2. Rule (25.3b) ensures that the converse holds as well, noting
that by rules (25.2d) and (25.2e) a conjunction is stronger than either of its conjuncts. Similarly, the
same rule ensures that if e satisﬁes φ, then it also satisﬁes ⊤, but we do not postulate that every
well-typed expression satisﬁes ⊤, for that would defeat the goal of ensuring that well-reﬁned
expressions do not incur a run-time fault.
Rule (25.3d) states that reﬁnements are closed under general recursion (formation of ﬁxed
points). To show that fix x : τ is e satisﬁes a reﬁnement φ, it sufﬁces to show that e satisﬁes φ,
under the assumption that x, which stands for the recursive expression itself, satisﬁes φ. It is thus
obvious that non-terminating expressions, such as fix x : τ is x, satisfy any reﬁnement at all. In
particular, such a divergent expression does not incur a run-time error, the guarantee we are after
with the present system of reﬁnements.
The second group concerns the type dyn of classiﬁed values.
Φ ⊢e ∈nat φ
Φ ⊢num ! e ∈dyn num ! φ
(25.4a)
Φ ⊢e ∈dyn⇀dyn φ
Φ ⊢fun ! e ∈dyn fun ! φ
(25.4b)
Φ ⊢e ∈dyn num ! φ
Φ ⊢e @ num ∈nat φ
(25.4c)
Φ ⊢e ∈dyn⇀dyn fun ! φ
Φ ⊢e @ fun ∈dyn⇀dyn φ
(25.4d)
Φ ⊢e ∈dyn ⊤
Φ ⊢num ? e ∈bool ⊤
(25.4e)
Φ ⊢e ∈dyn ⊤
Φ ⊢fun ? e ∈bool ⊤
(25.4f)
Rules (25.4a) and (25.4b) state that a newly-created value of class c satisﬁes the reﬁnement of the
type dyn stating this fact, provided that the underlying value satisﬁes the given reﬁnement φ of
the type associated to that class (nat for num and dyn ⇀dyn for fun).
Rules (25.4c) and (25.4d) state that a value of type dyn may only be safely cast to a class c if the
value is known statically to be of this class. This condition is stated in the premises of these rules,
which require the class of the cast value to be known and suitable for the cast. The result of a well-
reﬁned cast satisﬁes the reﬁnement given to its underlying value. It is important to realize that in

230
25.1 Statics
the quest to avoid run-time faults, it is not possible to cast a value whose only known reﬁnement is
⊤, which imposes no restrictions on it. This limitation is burdensome, because in many situations
it is not possible to determine statically what is the class of a value. We will return to this critical
point shortly.
Rules (25.4e) and (25.4f) compute a boolean based on the class of its argument. We shall have
more to say about this in Section 25.2.
The third group of rules govern nullary and binary product types.
Φ ⊢⟨⟩∈unit ⊤
(25.5a)
Φ ⊢e1 ∈τ1 φ1
Φ ⊢e2 ∈τ2 φ2
Φ ⊢⟨e1, e2⟩∈τ1×τ2 φ1 × φ2
(25.5b)
Φ ⊢e ∈τ1×τ2 φ1 × φ2
Φ ⊢e · l ∈τ1 φ1
(25.5c)
Φ ⊢e ∈τ1×τ2 φ1 × φ2
Φ ⊢e · r ∈τ1 φ1
(25.5d)
Rule (25.5a) states the obvious: the null-tuple is well-reﬁned by the trivial reﬁnement. Because
unit contains only one element, little else can be said about it. Rule (25.5b) states that a pair satis-
ﬁes a product of reﬁnements if each component satisﬁes the corresponding reﬁnement. Rules (25.5c)
and (25.5d) state the converse.
The fourth group of rules govern nullary and binary sum types.
Φ ⊢e ∈void φ′
Φ ⊢e ∈void φ
(25.6a)
Φ ⊢e1 ∈τ1 φ1
Φ ⊢l · e1 ∈τ1+τ2 φ1 + φ2
(25.6b)
Φ ⊢e2 ∈τ2 φ2
Φ ⊢r · e2 ∈τ1+τ2 φ1 + φ2
(25.6c)
Φ ⊢e ∈τ1+τ2 φ1 + φ2
Φ, x1 ∈τ1 φ1 ⊢e1 ∈τ φ
Φ, x2 ∈τ2 φ2 ⊢e2 ∈τ φ
Φ ⊢case e {l · x1 ,→e1 | r · x2 ,→e2} ∈τ φ
(25.6d)
Rule (25.6a) states that if an expression of type void satisﬁes some reﬁnement (and hence is error-
free), then it satisﬁes every reﬁnement (of type void), because there are no values of this type, and
hence, being error-free, must diverge.
Rules (25.6b) and (25.6c) are similarly motivated. If e1 satisﬁes φ1, then l · e1 satisﬁes φ1 + φ2 for
any reﬁnement φ2, precisely because the latter reﬁnement is irrelevant to the injection. Similarly,
the right injection is independent of the reﬁnement of the left summand.
Rule (25.6d) is in some respects the most interesting rule of all, one that we shall have occa-
sion to revise shortly. The salient feature of this rule is that it propagates reﬁnement information

25.1 Statics
231
about the injected value into the corresponding branch by stating an assumption about the bound
variable of each branch. But it does not propagate any information into the branches about what is
known about e in each branch, namely that in the ﬁrst branch e must be of the form l · e1 and in
the second branch e must be of the form r · e2.
The failure to propagate this information may seem harmless, but it is, in fact, quite restrictive.
To see why, consider the special case of the type bool, which is deﬁned in Section 11.3.2 to be
unit + unit. The conditional expression if e then e1 else e2 is deﬁned to be a case analysis in
which there is no associated data to pass into the branches of the conditional. So, within the then
branch e1 it is not known statically that e is in fact true, nor is it known within the else branch e2
that e is in fact false.
The ﬁfth group of rules governs the function type.
Φ, x ∈τ1 φ1 ⊢e2 ∈τ2 φ2
Φ ⊢λ (x : τ1) e2 ∈τ1⇀τ2 φ1 ⇀φ2
(25.7a)
Φ ⊢e1 ∈τ2⇀τ φ2 ⇀φ
e2 ∈τ2 φ2
Φ ⊢e1(e2) ∈τ φ
(25.7b)
Rule (25.7a) states that a λ-abstraction satisﬁes a function reﬁnement if its body satisﬁes the range
reﬁnement, under the assumption that its argument satisﬁes the domain reﬁnement. This is only
to be expected.
Rule (25.7b) states the converse. If an expression of function type satisﬁes a function reﬁne-
ment, and it is applied to an argument satisfying the domain reﬁnement, then the application
must satisfy the range reﬁnement.
The last group of rules govern the type nat:
Φ ⊢z ∈nat ⊤
(25.8a)
Φ ⊢e ∈nat ⊤
Φ ⊢s(e) ∈nat ⊤
(25.8b)
Φ ⊢e ∈nat ⊤
Φ ⊢e0 ∈τ φ
Φ, x ∈nat ⊤⊢e1 ∈τ φ
Φ ⊢ifz e {z ,→e0 | s(x) ,→e1} ∈τ φ
(25.8c)
These rules are completely unambitious: they merely restate the typing rules as reﬁnement rules
that impose no requirements and make no guarantees of any properties of the natural numbers.
One could envision adding reﬁnements of the natural numbers, for instance stating that a natural
number is known to be zero or non-zero, for example.
To get a feel for the foregoing rules, it is useful to consider some simple examples. First, we
may combine rules (25.3b) and (25.4a) to derive the judgment
num ! n ∈dyn ⊤
for any natural number n. That is, we may “forget” the class of a value by applying subsumption,
and appealing to rule (25.2c). Second, such reasoning is essential in stating reﬁnement satisfac-
tion for a boolean conditional (or, more generally, any case analysis). For example, the following

232
25.2 Boolean Blindness
judgment is directly derivable without subsumption:
Φ, x ∈bool ⊤⊢if x then (num ! z) else (num ! s(z)) ∈dyn num ! ⊤.
But the following judgment is only derivable because we may weaken knowledge of the class of a
value in each branch to a common reﬁnement, in this case the weakest reﬁnement of all:
Φ, x ∈bool ⊤⊢if x then (num ! z) else (fun ! (λ (y : dyn) y)) ∈dyn ⊤.
In general conditionals attenuate the information we have about a value, except in those cases
where the same information is known about both branches. Conditionals are the main source of
lossiness in checking reﬁnement satisfaction.
Conjunction reﬁnements are used to express multiple properties of a single expression. For
example, the identity function on the type dyn satisﬁes the conjunctive reﬁnement
(num ! ⊤⇀num ! ⊤) ∧(fun ! ⊤⇀fun ! ⊤).
The occurrences of ⊤in the ﬁrst conjunct reﬁne the type nat, whereas the occurrences in the
second conjunct reﬁne the type dyn ⇀dyn. It is a good exercise to check that λ (x : dyn) x satisﬁes
the above reﬁnement of the type dyn ⇀dyn.
25.2
Boolean Blindness
Let us consider a very simple example that exposes a serious disease suffered by many program-
ming languages, a condition called boolean blindness. Suppose that x is a variable of type dyn with
reﬁnement ⊤, and consider the expression
if (num ? x) then x @ num else z.
Although it is clear that this expression has type nat, it is nevertheless ill-reﬁned (satisﬁes no re-
ﬁnement), even though it does not incur a run-time error. Speciﬁcally, within the then branch,
we as programmers know that x is a value of class num, but this fact is not propagated into the
then branch by rules (25.6) (of which the boolean conditional is a special case). Consequently,
rule (25.4c) does not apply (formally we do not know enough about x to cast it safely), so the
expression is ill-reﬁned. The branches of the conditional are “blind” both to the outcome and the
meaning of the boolean value computed by the test whether x is of class num.
Boolean blindness is endemic among programming languages. The difﬁculty is that a boolean
carries exactly one bit of information, which is not sufﬁcient to capture the meaning of that bit.
A boolean, which is (literally) a bit of data, ought to be distinguished from a proposition, which
expresses a fact. Taken by itself, a boolean conveys no information other than its value, whereas
a proposition expresses the reasoning that goes into ensuring that a piece of code is correct. In
terms of the above example knowing that num ? x evaluates dynamically to the boolean true is not
connected statically with the fact that the class of x is num or not. That information lives elsewhere,
in the speciﬁcation of the class test primitive. The question is how to connect the boolean value
returned by the class test with relevant facts about whether the class of x is num or not.

25.2 Boolean Blindness
233
Because the purpose of a type reﬁnement system is precisely to capture such facts in a form that
can be stated and veriﬁed, one may suspect that the difﬁculty with the foregoing example is that
the system of reﬁnements under consideration is too weak to capture the property that we need
to ensure that run-time faults do not occur. The example shows that something more is needed
if type reﬁnement is to be useful, so we ﬁrst consider what might be involved in enriching the
deﬁnition of reﬁnement satisfaction to ensure that the proper connections are made. The matter
boils down to two issues:
1. Propagating that num ? x returned true into the then branch, and that it returned false into
the else branch.
2. Connecting facts about the return value of a test to facts about the value being tested.
These are, in the present context, distinct aspects of the problem. Let us consider them in turn. The
upshot of the discussion will be to uncover a design ﬂaw in casting, and to suggest an alternative
formulation that does not suffer from boolean blindness.
To address the problem as stated, we ﬁrst need to enrich the language of reﬁnements to include
true ⊑bool and false ⊑bool, stating that a boolean is either true or false, respectively. That
way we can hope to express facts about boolean-valued expressions as reﬁnement judgments.
But what is the reﬁnement rule for the boolean conditional? As a ﬁrst guess one might consider
something like the following:
Φ, e ∈bool true ⊢e1 ∈τ φ
Φ, e ∈bool false ⊢e2 ∈τ φ
Φ ⊢if e then e1 else e2 ∈τ φ
(25.9)
Such a rule is a bit unusual in that it introduces a hypothesis about an expression, rather than a
variable, but let us ignore that for the time being and press on.
Having re-formulated the reﬁnement rule for the conditional, we immediately run into another
problem: how to deduce that x ∈dyn num ! ⊤, which is required for casting, from the assumption
that num ? x ∈bool true? One way to achieve this is by modifying the reﬁnement rule for the
conditional to account for the special case in which the boolean expression is literally num ? e for
some e. If it is, then we propagate into the then branch the additional assumption e ∈dyn num ! ⊤,
but if it is not, then no fact about the class of e is propagated into the else branch.2
This change is enough to ensure that the example under consideration is well-reﬁned. But
what if the boolean on which we are conditioning is not literally num ? e, but merely implies the
test would return true? How then are we to make the connection between such expressions and
relevant facts about the class of e? Clearly there is no end to the special cases we could consider to
restore vision to reﬁnements, but there is no unique best solution.
All is not lost, however! The foregoing analysis suggests that the fault lies not in our reﬁne-
ments, but in our language design. The sole source of run-time errors is the “naked cast” that
attempts to extract the underlying natural number from a value of type dyn—and signals a run-
time error if it cannot do so because the class of the value is not num. The boolean-valued test for
the class of a value seems to provide a way to avoid these errors. But, as we have just seen, the true
2For the special case of there being exactly two classes, we could propagate that the class of e is fun into the else branch,
but this approach does not generalize.

234
25.3 Reﬁnement Safety
cause of the problem is the attempt to separate the test from the cast. We may, instead, combine
these into a single form, say
ifofcl[num](e;x0.e0;e1),
that tests whether the class of e is num, and, if so, passes the underlying number to e0 by substituting
it for x0, and otherwise evaluates e1. No run-time error is possible, and hence no reﬁnements are
needed to ensure that it cannot occur.
But does this not mean that type reﬁnements are pointless? Not at all. It simply means that
in some cases veriﬁcation methods, such as type reﬁnements, are needed solely to remedy a lan-
guage design ﬂaw, and not provide a useful tool for programmers to help express and verify the
correctness of their programs. We may still wish to express invariants such as the property that a
particular function maps values of class c into values of class c′, simply for the purpose of stating
a programmer’s intention. Or we may enrich the system of reﬁnements to track properties such
as whether a number is even or odd, and to state conditions such as that a given function maps
evens to odds and odds to evens. There is no limit, in principle, to the variations and extensions
to help ensure that programs behave as expected.
25.3
Reﬁnement Safety
The judgment ΦΓ ⊢a ∈τ φ presupposes that Γ ⊢e : τ, so by adapting the proofs given earlier, we
may type preservation and progress for well-typed terms.
Theorem 25.1 (Type Safety). Suppose that e : τ for closed e.
1. If e 7−→e′, then e′ : τ.
2. Either e err, or e val, or there exists e′ such that e 7−→e′.
The proof of progress requires a canonical forms lemma, which for the type dyn is stated as
follows:
Lemma 25.2 (Canonical Forms). Suppose that e : dyn and e val. Then either e = num ! e′ or e = fun ! e′
for some e′.
The expression e′ would also be a value under an eager dynamics, but need not be under a lazy
dynamics. The proof of Lemma 25.2 proceeds as usual, by analyzing the typing rules for values.
The goal of the reﬁnement system introduced in Section 25.1 is to ensure that errors cannot arise
in a well-reﬁned program. To show this, we ﬁrst show that the dynamics preserves reﬁnements.
Lemma 25.3. Suppose that e val and e ∈dyn φ. If φ ≤num ! φ′, then e = num ! e′, where e′ ∈nat φ′, and
if φ ≤fun ! φ′, then e = fun ! e′, where e′ ∈dyn⇀dyn φ′.
Proof. The proof requires Lemma 25.2 to characterize the possible values of dyn, and an analysis
of the reﬁnement satisfaction rules. The lemma accommodates rule (25.3b), which appeals to the
transitivity of reﬁnement entailment.

25.4 Notes
235
Theorem 25.4 (Reﬁnement Preservation). If e ∈τ φ and e 7−→e′, then e′ ∈τ φ.
Proof. We know by the preceding theorem that e′ : τ. To show that e′ ∈τ φ we proceed by induction
on the deﬁnition of reﬁnement satisfaction given in Section 25.1. The type-independent group,
rules (25.3), are all easily handled, apart from the rule for ﬁxed points, which requires an appeal
to a substitution lemma, just as in Theorem 19.2. The remaining groups are all handled easily,
bearing in mind that an incorrect expression cannot make a transition.
Theorem 25.5 (Reﬁnement Error-Freedom). If e ∈τ φ, then ¬(e err).
Proof. By induction on the deﬁnition of reﬁnement satisfaction. The only interesting cases are
rules (25.4c) and (25.4d), which are handled by appeal to Lemma 25.3 in the case that the cast
expression is a value.
Corollary 25.6 (Reﬁnement Safety). If e ∈τ φ, then either e val or there exists e′ such that e′ ∈τ φ and
e 7−→e′. In particular, ¬(e err).
Proof. By Theorems 25.1, 25.4, and 25.5.
25.4
Notes
The distinction between types and reﬁnements is fundamental, yet the two are often conﬂated.
Types determine the structure of a programming language, including its statics and dynamics;
reﬁnements specify the behavior of well-typed programs. In full generality the satisfaction judg-
ment e ∈τ φ need not be decidable, whereas it is sensible to insist that the typing judgment e : τ
be decidable. The reﬁnement system presented in this chapter is decidable, but one may consider
many notions of reﬁnement that are not. For example, one may postulate that if e ∈τ φ and that e′
is indistinguishable from e by any program in the language,3 then e′ ∈τ φ. In contrast such a move
is not sensible in a type system, because the dynamics is derived from the statics by the inversion
principle. Therefore, reﬁnement is necessarily posterior to typing.
The syntactic formulation of type reﬁnements considered in this chapter was originally given
by Freeman and Pfenning (1991), and extended by Davies and Pfenning (2000); Davies (2005),
Xi and Pfenning (1998), Dunﬁeld and Pfenning (2003), and Mandelbaum et al. (2003). A more
general semantic formulation of type reﬁnement was given explicitly by Denney (1998) in the style
of the realizability interpretation of type theory on which NuPRL (Constable, 1986) is based. (See
the survey by van Oosten (2002) for the history of the realizability interpretations of constructive
logic.)
Exercises
25.1. Show that if φ1 ≤φ′
1 and φ2 ≤φ′
2, then φ1 ∧φ2 ≤φ′
1 ∧φ′
2.
3See Chapter 47 for a precise deﬁnition and development of this concept.

236
25.4 Notes
25.2. Show that φ ≤φ′ iff for every φ′′, if φ′′ ≤φ, then φ′′ ≤φ′. (This property of entailment is an
instance of the more general Yoneda Lemma in category theory.)
25.3. Extend the system of reﬁnements to recursive types by introducing a reﬁnement fold(φ)
that classiﬁes values of recursive type rec t is τ in terms of a reﬁnement of the unfolding of
that recursive type.
25.4. Consider the following two forms of reﬁnement for sum types, summand reﬁnements:
φ1 ⊑τ1
l · φ1 ⊑τ1 + τ2
(25.10a)
φ2 ⊑τ2
r · φ2 ⊑τ1 + τ2
(25.10b)
Informally, l · φ1 classiﬁes expressions of type τ1 + τ2 that lie within the left summand and
whose underlying value satisﬁes φ1, and similarly for r · φ2.
(a) State entailment rules governing summand reﬁnements.
(b) State reﬁnement rules assigning summand reﬁnements to the introduction forms of a
sum type.
(c) Give rules for the case analysis construct using summand reﬁnements to allow unreach-
able branches to be disregarded during reﬁnement checking.
(d) Modify rule (25.6d) so that the information “learned” by examining the value of e at
execution time is propagated into the appropriate branch of the case analysis.
Check the importance of this extension to the prospects for a cure for Boolean blindness.
25.5. Using the preceding exercise, derive the reﬁnements num ! φ and fun ! φ from the other reﬁne-
ment rules, including the reﬁnement fold(φ) considered in Exercise 25.3.
25.6. Show that the addition function (23.8), a value of type dyn, satisﬁes the reﬁnement
fun ! (num ! ⊤⇀fun ! (num ! ⊤⇀num ! ⊤)),
stating that
(a) It is itself a value of class fun.
(b) The so-classiﬁed function maps a value of class num to a result, if any, of class fun.
(c) The so-classiﬁed function maps a value of class num to a result, if any, of class num.
This description exposes the hidden complexity in the superﬁcial simplicity of a uni-typed
language.
25.7. Revisit the optimization process of the addition function carried out in Section 23.3 in view
of your answer to Exercise 25.6. Show that the validity of the optimizations is guaranteed by
the satisfaction of the stated type reﬁnement for addition.

Part XI
Dynamic Dispatch


Chapter 26
Classes and Methods
It often arises that the values of a type are partitioned into a variety of classes, each classifying data
with distinct internal structure. A simple example is provided by the type of points in the plane,
which are classiﬁed according to whether they are represented in cartesian or polar form. Both are
represented by a pair of real numbers, but in the cartesian case these are the x and y coordinates
of the point, whereas in the polar case these are its distance r from the origin and its angle θ with
the polar axis. A classiﬁed value is an object, or instance, of its class. The class determines the type
of the classiﬁed data, the instance type of the class; the classiﬁed data itself is the instance data of the
object.
Methods are functions that act on classiﬁed values. The behavior of a method is determined
by the class of its argument. The method dispatches on the class of the argument.1 Because the
selection is made at run-time, it is called dynamic dispatch. For example, the squared distance of
a point from the origin is calculated differently according to whether the point is represented in
cartesian or polar form. In the former case the required distance is x2 + y2, whereas in the latter
it is simply r2. Similarly, the quadrant of a cartesian point can be determined by examining the
sign of its x and y coordinates, and the quadrant of a polar point can be calculated by taking the
integral part of the angle θ divided by π/2.
Dynamic dispatch is often described in terms of a particular implementation strategy, which
we will call the class-based organization. In this organization each object is represented by a vec-
tor of methods specialized to the class of that object. We may equivalently use a method-based
organization in which each method branches on the class of an object to determine its behavior.
Regardless of the organization used, the fundamental idea is that (a) objects are classiﬁed, and (b)
methods dispatch on the class of an object. The class-based and method-based organizations are
interchangeable, and, in fact, related by a natural duality between sum and product types. We
explain this symmetry by focusing ﬁrst on the behavior of each method on each object, which is
given by a dispatch matrix. From this we derive both a class-based and a method-based organiza-
tion in such a way that their equivalence is obvious.
1More generally, we may dispatch on the class of multiple arguments simultaneously. We concentrate on single dispatch
for the sake of simplicity.

240
26.1 The Dispatch Matrix
26.1
The Dispatch Matrix
Because each method acts by dispatch on the class of its argument, we may envision the entire
system of classes and methods as a dispatch matrix edm whose rows are classes, whose columns are
methods, and whose (c, d)-entry deﬁnes the behavior of method d acting on an argument of class
c, expressed as a function of the instance data of the object. Thus, the dispatch matrix has a type
of the form
∏
c∈C ∏
d∈D
(τc ⇀ρd),
where C is the set of class names, D is the set of method names, τc is the instance type associated
with class c and ρd is the result type of method d. The instance type is the same for all methods
acting on a given class, and the result type is the same for all classes acted on by a given method.
As an illustrative example, let us consider the type of points in the plane classiﬁed into two
classes, cart and pol, corresponding to the cartesian and polar representations. The instance data
for a cartesian point has the type
τcart = ⟨x ,→float, y ,→float⟩,
and the instance data for a polar point has the type
τpol = ⟨r ,→float, th ,→float⟩.
Consider two methods acting on points, dist and quad, which compute, respectively, the
squared distance of a point from the origin and the quadrant of a point. The squared distance
method is given by the tuple edist = ⟨cart ,→ecart
dist, pol ,→epol
dist⟩of type
⟨cart ,→τcart ⇀ρdist, pol ,→τpol ⇀ρdist⟩,
where ρdist = float is the result type,
ecart
dist = λ (u : τcart) (u · x)2 + (u · y)2
is the squared distance computation for a cartesian point, and
epol
dist = λ (v : τpol) (v · r)2
is the squared distance computation for a polar point. Similarly, the quadrant method is given by
the tuple equad = ⟨cart ,→ecart
quad, pol ,→epol
quad⟩of type
⟨cart ,→τcart ⇀ρquad, pol ,→τpol ⇀ρquad⟩,
where ρquad = [I, II, III, IV] is the type of quadrants, and ecart
quad and epol
quad are expressions that
compute the quadrant of a point in rectangular and polar forms, respectively.
Now let C = { cart, pol } and let D = { dist, quad }, and deﬁne the dispatch matrix edm to
be the value of type
∏
c∈C ∏
d∈D
(τc ⇀ρd)

26.1 The Dispatch Matrix
241
such that, for each class c and method d,
edm · c · d 7−→∗ec
d.
That is, the entry in the dispatch matrix edm for class c and method d deﬁnes the behavior of that
method acting on an object of that class.
Dynamic dispatch is an abstraction given by the following components:
• An abstract type tobj of objects, which are classiﬁed by the classes on which the methods act.
• An operation new[c](e) of type tobj that creates an object of the class c with instance data
given by the expression e of type τc.
• An operation e ⇐d of type ρd that invokes method d on the object given by the expression e
of type tobj.
These operations must satisfy the deﬁning characteristic of dynamic dispatch,
(new[c](e)) ⇐d 7−→∗ec
d(e),
which states that invoking method d on an object of class c with instance data e amounts to apply-
ing ec
d, the code in the dispatch matrix for class c and method d to the instance data e.
In other words dynamic dispatch is an abstract type with interface given by the existential type
∃(tobj.⟨new ,→∏
c∈C
τc ⇀tobj, snd ,→∏
d∈D
tobj ⇀ρd⟩).
(26.1)
There are two main ways to implement this abstract type. The class-based organization, deﬁnes
objects as tuples of methods, and creates objects by specializing the methods to the given instance
data. The method-based organization creates objects by tagging the instance data with the class,
and deﬁnes methods by examining the class of the object. These two organizations are isomorphic
to one another, and hence can be interchanged at will. Nevertheless, many languages favor one
representation over the other, asymmetrizing an inherently symmetric situation.
The abstract type (26.1) calls attention to shortcoming of dynamic dispatch, namely that a mes-
sage can be sent to exactly one object at a time. This viewpoint seems natural in certain cases,
such as discrete event simulation in the language Simula-67. But often it is essential to act on sev-
eral classes of object at once. For example, the multiplication of a vector by a scalar combines the
elements of a ﬁeld and a commutative monoid; there is no natural way to associate scalar multipli-
cation with either the ﬁeld or the monoid, nor any way to anticipate that particular combination.
Moreover, the multiplication is not performed by checking at run-time that one has a scalar and a
vector in hand, for there is nothing inherent in a scalar or a vector that marks them as such. The
right tool for handling such situations is a module system (Chapters 44 and 45), and not dynamic
dispatch. The two mechanisms serve different purposes, and complement each other.
The same example serves to refute a widely held fallacy, namely that the values of an abstract
type cannot be heterogeneous. It is sometimes said that an abstract type of complex numbers must
commit to a single representation, say rectangular, and cannot accommodate multiple representa-
tions. This is a fallacy. Although it is true that an abstract type deﬁnes a single type, it is wrong
to say that only one representation of objects is possible. The abstract type can be implemented as
a sum, and the operations may correspondingly dispatch on the summand to compute the result.
Dynamic dispatch is a mode of use of data abstraction, and therefore cannot be opposed to it.

242
26.2 Class-Based Organization
26.2
Class-Based Organization
The class-based organization starts with the observation that the dispatch matrix can be reorga-
nized to “factor out” the instance data for each method acting on that class to obtain the class vector
ecv of type
τcv ≜∏
c∈C
(τc ⇀(∏
d∈D
ρd)).
Each entry of the class vector consists of a constructor that determines the result of each of the
methods when acting on given instance data.
An object has the type ρ = ∏d∈D ρd consisting of the product of the result types of the methods.
For example, in the case of points in the plane, the type ρ is the product type
⟨dist ,→ρdist, quad ,→ρquad⟩.
Each component speciﬁes the result of the methods acting on that object.
The message send operation e ⇐d is just the projection e · d. So, in the case of points in the
plane, e ⇐dist is the projection e · dist, and similarly e ⇐quad is the projection e · quad.
The class-based organization combines the implementation of each class into a class vector ecv a
tuple of type τcv consisting of the constructor of type τc ⇀ρ for each class c ∈C. The class vector
is deﬁned by ecv = ⟨c ,→ec⟩c∈C, where for each c ∈C the expression ec is
λ (u : τc) ⟨d ,→edm · c · d(u)⟩d∈D.
For example, the constructor for the class cart is the function ecart given by the expression
λ (u : τcart) ⟨dist ,→edm · cart · dist(u), quad ,→edm · cart · quad(u)⟩.
Similarly, the constructor for the class pol is the function epol given by the expression
λ (u : τpol) ⟨dist ,→edm · pol · dist(u), quad ,→edm · pol · quad(u)⟩.
The class vector ecv in this case is the tuple ⟨cart,→ecart, pol,→epol⟩of type ⟨cart,→τcart ⇀ρ, pol,→
τpol ⇀ρ⟩.
An object of a class is obtained by applying the constructor for that class to the instance data:
new[c](e) ≜ecv · c(e).
For example, a cartesian point is obtained by writing new[cart](⟨x ,→x0, y ,→y0⟩), which is de-
ﬁned by the expression
ecv · cart(⟨x ,→x0, y ,→y0⟩).
Similarly, a polar point is obtained by writing new[pol](r ,→r0, th ,→θ0), which is deﬁned by the
expression
ecv · pol(⟨r ,→r0, th ,→θ0⟩).
It is easy to check for this organization of points that for each class c and method d, we may derive
(new[c](e)) ⇐d 7−→∗(ecv · c(e)) · d
7−→∗edm · c · d(e).
That is, the message send evokes the behavior of the given method on the instance data of the
given object.

26.3 Method-Based Organization
243
26.3
Method-Based Organization
The method-based organization starts with the transpose of the dispatch matrix, which has the type
∏
d∈D ∏
c∈C
(τc ⇀ρd).
By observing that each row of the transposed dispatch matrix determines a method, we obtain the
method vector emv of type
τmv ≜∏
d∈D
(∑
c∈C
τc) ⇀ρd.
Each entry of the method vector consists of a dispatcher that determines the result as a function of
the instance data associated with a given object.
An object is a value of type τ = ∑c∈C τc, the sum over the classes of the instance types. For
example, the type of points in the plane is the sum type
[cart ,→τcart, pol ,→τpol].
Each point is labeled with its class, specifying its representation as having either cartesian or polar
form.
An object of a class c is just the instance data labeled with its class to form an element of the
object type:
new[c](e) ≜c · e.
For example, a cartesian point with coordinates x0 and y0 is given by the expression
new[cart](⟨x ,→x0, y ,→y0⟩) ≜cart · ⟨x ,→x0, y ,→y0⟩.
Similarly, a polar point with distance r0 and angle θ0 is given by the expression
new[pol](⟨r ,→r0, th ,→θ0⟩) ≜pol · ⟨r ,→r0, th ,→θ0⟩.
The method-based organization consolidates the implementation of each method into the method
vector emv of type τmv deﬁned by ⟨d ,→ed⟩d∈D, where for each d ∈D the expression ed : τ ⇀ρd is
λ (this : τ) case this {c · u ,→edm · c · d(u)}c∈C.
Each entry in the method vector is a dispatch function that deﬁnes the action of that method on each
class of object.
In the case of points in the plane, the method vector has the product type
⟨dist ,→τ ⇀ρdist, quad ,→τ ⇀ρquad⟩.
The dispatch function for the dist method has the form
λ (this : τ) case this {cart · u ,→edm · cart · dist(u) | pol · v ,→edm · pol · dist(v)},

244
26.4 Self-Reference
and the dispatch function for the quad method has the similar form
λ (this : τ) case this {cart · u ,→edm · cart · quad(u) | pol · v ,→edm · pol · quad(v)}.
The message send operation e ⇐d applies the dispatch function for method d to the object e:
e ⇐d ≜emv · d(e).
Thus we have, for each class c and method d
(new[c](e)) ⇐d 7−→∗emv · d(c · e)
7−→∗edm · c · d(e)
The result is, of course, the same as for the class-based organization.
26.4
Self-Reference
It is often useful to allow methods to create new objects or to send messages to objects. It is not
possible to do so using the simple dispatch matrix described in Section 26.1, for the simple reason
that there is no provision for self-reference within its entries. This deﬁciency may be remedied
by changing the type of the entries of the dispatch matrix to account for sending messages and
creating objects, as follows:
∏
c∈C ∏
d∈D
∀(tobj.τcv ⇀τmv ⇀τc ⇀ρd).
The type variable tobj represents the abstract object type.2 The types τcv and τmv, are, respectively,
the type of the class and method vectors, deﬁned in terms of the abstract type of objects tobj. They
are deﬁned by the equations
τcv ≜∏
c∈C
(τc ⇀tobj)
and
τmv ≜∏
d∈D
(tobj ⇀ρd).
The component of the class vector corresponding to a class c is a constructor that builds a value
of the abstract object type tobj from the instance data for c. The component of the method vector
corresponding to a method d is a dispatcher that yields a result of type ρd when applied to a value
of the abstract object type tobj.
In accordance with the revised type of the dispatch matrix the behavior associated to class c
and method d has the form
Λ(tobj) λ (cv : τcv) λ (mv : τmv) λ (u : τc) ec
d.
2The variable tobj is chosen not to occur in any τc or ρd. This restriction can be relaxed; see Exercise 26.4.

26.4 Self-Reference
245
The arguments cv and mv are used to create new objects and to send messages to objects. Within
the expression ec
d an object of class c′ with instance data e′ is created by writing cv · c′(e′), which
selects the appropriate constructor from the class vector cv and applies it to the given instance data.
The class c′ may well be the class c itself; this is one form of self-reference within ec
d. Similarly,
within ec
d a method d′ is invoked on e′ by writing mv · d′(e′). The method d′ may well be the
method d itself; this is another aspect of self-reference within ec
d.
To account for self-reference in the method-based organization, the method vector emv will be
deﬁned to have the self-referential type [τ/tobj]τmv self in which the object type τ is, as before,
the sum of the instance types of the classes ∑c∈C τc. The method vector is deﬁned by the following
equation:
emv ≜self mv is ⟨d ,→λ (this : τ) case this {c · u ,→edm · c · d[τ](e′
cv)(e′
mv)(u)}c∈C⟩d∈D,
where
e′
cv ≜⟨c ,→λ (u : τc) c · u⟩c∈C : [τ/tobj]τcv.
and
e′
mv ≜unroll(mv) : [τ/tobj]τmv.
Object creation is deﬁned by the equation
new[c](e) ≜c · e : τ
and message send is deﬁned by the equation
e ⇐d ≜unroll(emv) · d(e) : ρd.
To account for self-reference in the class-based organization, the class vector ecv will be deﬁned
to have the type [ρ/tobj]τcv self in which the object type ρ is, as before, the product of the result
types of the methods ∏d∈D ρd. The class vector is deﬁned by the following equation:
ecv ≜self cv is ⟨c ,→λ (u : τc) ⟨d ,→edm · c · d[ρ](e′′
cv)(e′′
mv)(u)⟩d∈D⟩c∈C,
where
e′′
cv ≜unroll(cv) : [ρ/tobj]τcv
and
e′′
mv ≜⟨d ,→λ (this : ρ) this · d⟩d∈D : [ρ/tobj]τmv.
Object creation is deﬁned by the equation
new[c](e) ≜unroll(ecv) · c(e) : ρ,
and message send is deﬁned by the equation
e ⇐d ≜e · d : ρd.
The symmetries between the two organizations are striking. They are a reﬂection of the funda-
mental symmetries between sum and product types.

246
26.5 Notes
26.5
Notes
The term “object-oriented” means many things to many people, but certainly dynamic dispatch,
the action of methods on instances of classes, is one of its central concepts. These characteristic
features emerge from the more general concepts of sum-, product-, and function types, which are
useful, alone and in combination, in a wider variety of circumstances. A bias towards either a
class- or method-based organization seems misplaced in view of the inherent symmetries of the
situation. The dynamic dispatch abstraction given by the type (26.1) admits either form of imple-
mentation, as demonstrated in Sections 26.2 and 26.3. The literature on object-oriented program-
ming, of which dynamic dispatch is a signiﬁcant aspect, is extensive. Abadi and Cardelli (1996)
and Pierce (2002) give a thorough account of much of this work.
Exercises
26.1. Consider the possibility that some methods may only be deﬁned on instances of some classes,
so that a message send operation may result in a “not understood” error at run-time. Use
the type τ opt deﬁned in Section 11.3.4 to rework the dispatch matrix to account for “not
understood” errors. Reformulate the class- and method-based implementations of dynamic
dispatch using the revised dispatch matrix representation. Proceed in two stages. In the ﬁrst
stage ignore the possibility of self-reference so that the behavior associated to a method on
an instance of a particular class cannot incur a “not understood” error. In the second stage
use your solution to the ﬁrst stage to further rework the dispatch matrix and the implemen-
tations of dynamic dispatch to account for the behavior of a method to include incurring a
“not understood” error.
26.2. Type reﬁnements can be used to ensure the absence of speciﬁed “not understood” errors that
may otherwise arise in the context of Exercise 26.1. To do so begin by specifying, for each
c ∈C, a subset Dc ⊆D of methods that must be well-deﬁned on instances of class c. This
deﬁnition determines, for each d ∈D, the set Cd ≜{ c ∈C | d ∈Dc } of classes on which
method d must be well-deﬁned. Using summand reﬁnements for the type τ opt, deﬁne the
type reﬁnement
φdm ≜∏
c∈C
( ∏
d∈Dc
just(⊤τc ⇀⊤ρd)) × ( ∏
d∈D\Dc
⊤(τc⇀ρd) opt),
which reﬁnes the type of dispatch matrix to within a permutation of its columns.3 It speciﬁes
that if d ∈Dc, then the dispatch matrix entry for class c and method d must be present, and
imposes no restriction on any other entry. Assume that edm ∈τdm φdm, as expected. Assume
a method-based organization in which the object type tobj is the sum over all classes of their
instance types.
3Working up to such a permutation is a notational convenience, and can be avoided at the expense of some clarity in
the presentation.

26.5 Notes
247
(a) Deﬁne the reﬁnements inst[c] and admits[d] of tobj stating that e ∈tobj is an instance
of class c ∈C and that e admits method d ∈D, respectively. Show that if d ∈Dc, then
inst[c] ≤admits[d], which is to say that any instance of class c admits any method d
for that class.
(b) Deﬁne φcv ⊑τcv and φmv ⊑τmv in terms of inst[c] and admits[d] so that ecv ∈τcv
φcv and emv ∈τmv φmv. Remember to use the class- and method vectors derived in
Exercise 26.1.
(c) Referring to the deﬁnitions of object creation and message send and of the class- and
method vectors derived in Exercise 26.1, conclude that if message d ∈Dc is sent to an
instance of c ∈C, then no “not understood” error can arise at run-time in a well-reﬁned
program.
26.3. Using self-reference set up a dispatch matrix in which two methods may call one another
mutually recursively when invoked on an instance of a class. Speciﬁcally, let num be a class
of numbers with instance type τnum = nat, and let ev and od be two methods with result type
ρev = ρod = bool. Deﬁne the dispatch entries for methods ev and od for the class num so that
they determine, by laborious mutual recursion, whether the instance datum is an even or an
odd natural number.
26.4. Generalize the account of self-reference to admit constructors whose arguments may involve
objects and methods whose results may involve objects. Speciﬁcally, allow the abstract object
type tobj to occur in the instance type τc of a class c or in the result type ρd of a method
d. Rework the development in Section 26.4 to account for this generalization. Hint: Use
recursive types as described in Chapter 20.

248
26.5 Notes

Chapter 27
Inheritance
In this chapter we build on Chapter 26 and consider the process of deﬁning the dispatch matrix
that determines the behavior of each method on each class. A common strategy is to build the
dispatch matrix incrementally by adding new classes or methods to an existing dispatch matrix.
To add a class requires that we deﬁne the behavior of each method on objects of that class, and to
deﬁne a method requires that we deﬁne the behavior of that method on objects of the classes. The
deﬁnition of these behaviors can be given by any means available in the language. However, it is
often suggested that a useful means of deﬁning a new class is to inherit the behavior of another
class on some methods, and to override its behavior on others, resulting in an amalgam of the old
and new behaviors. The new class is often called a subclass of the old class, which is then called the
superclass. Similarly, a new method can be deﬁned by inheriting the behavior of another method
on some classes, and overriding the behavior on others. By analogy we may call the new method
a sub-method of a given super-method. For the sake of clarity we restrict attention to the non-self-
referential case in the following development.
27.1
Class and Method Extension
We begin by extending a given dispatch matrix, edm, of type
∏
c∈C ∏
d∈D
(τc →ρd)
with a new class c∗/∈C and a new method d∗/∈D to obtain a new dispatch matrix e∗
dm of type
∏
c∈C∗∏
d∈D∗
(τc →ρd),
where C∗= C ∪{ c∗} and D∗= D ∪{ d∗}.
To add a new class c∗to the dispatch matrix, we must specify the following information:1
1The extension with a new method will be considered separately for the sake of clarity.

250
27.2 Class-Based Inheritance
1. The instance type τc∗of the new class c∗.
2. The behavior ec∗
d of each method d ∈D on an object of the new class c∗, a function of type
τc∗→ρd.
This data determines a new dispatch matrix e∗
dm such that the following conditions are satisﬁed:
1. For each c ∈C and d ∈D, the behavior e∗
dm · c · d is the same as the behavior edm · c · d.
2. For each d ∈D, the behavior e∗
dm · c∗· d is given by ec∗
d .
To deﬁne c∗as a subclass of some class c ∈C means to deﬁne the behavior ec∗
d to be ec
d for some
(perhaps many) d ∈D. It is sensible to inherit a method d in this way only if the subtype relation-
ship
τc →ρd <: τc∗→ρd
is valid, which will be the case if τc∗<: τc. This subtyping condition ensures that the inherited
behavior can be invoked on the instance data of the new class.
Similarly, to add a new method d∗to the dispatch matrix, we must specify the following infor-
mation:
1. The result type ρd∗of the new method d∗.
2. The behavior ec
d∗of the new method d∗on an object of each class c ∈C, a function of type
τc →ρd∗.
This data determines a new dispatch matrix e∗
dm such that the following conditions are satisﬁed:
1. For each c ∈C and d ∈D, the behavior e∗
dm · c · d is the same as edm · c · d.
2. The behavior e∗
dm · c · d∗is given by ec
d∗.
To deﬁne d∗as a sub-method of some d ∈D means to deﬁne the behavior ec
d∗to be ec
d for some
(perhaps many) classes c ∈C. This deﬁnition is only sensible if the subtype relationship
τc →ρd <: τc →ρd∗
holds, which is the case if ρd <: ρd∗. This subtyping relationship ensures that the result of the old
behavior sufﬁces for the new behavior.
We will now consider how inheritance relates to the method- and class-based organizations of
dynamic dispatch considered in Chapter 26.
27.2
Class-Based Inheritance
Recall that the class-based organization given in Chapter 26 consists of a class vector ecv of type
τcv ≜∏
c∈C
(τc →ρ),

27.2 Class-Based Inheritance
251
where the object type ρ is the ﬁnite product type ∏d∈D ρd. The class vector consists of a tuple of
constructors that specialize the methods to a given object of each class.
Let us consider the effect of adding a new class c∗as described in Section 27.1. The new class
vector e∗cv has type
τ∗
cv ≜∏
c∈C∗
(τc →ρ).
There is an isomorphism, written ( )†, between τ∗cv and the type
τcv × (τc∗→ρ),
which can be used to deﬁne the new class vector e∗cv as follows:
⟨ecv, λ (u : τc∗) ⟨d ,→ec∗
d (u)⟩d∈D⟩
†.
This deﬁnition makes clear that the old class vector ecv is reused intact in the new class vector,
which extends the old class vector with a new constructor.
Although the object type ρ is the same both before and after the extension with the new class,
the behavior of an object of class c∗may differ arbitrarily from that of any other object, even that
of the superclass from which it inherits its behavior. So, knowing that c∗inherits from c tells us
nothing about the behavior of its objects, but only about the means by which the class is deﬁned.
Inheritance carries no semantic signiﬁcance, but is only a record of the history of how a class is
deﬁned.
Now let us consider the effect of adding a new method d∗as described in Section 27.1. The
new class vector e∗cv has type
τ∗
cv ≜∏
c∈C
(τc →ρ∗),
where ρ∗is the product type ∏d∈D∗ρd. There is an isomorphism, written ( )‡, between ρ∗and the
type ρ × ρd∗, where ρ is the old object type. Using this the new class vector e∗cv is deﬁned by
⟨c ,→λ (u : τc) ⟨⟨d ,→((ecv · c)(u)) · d⟩d∈D, ec
d∗(u)⟩‡⟩c∈C.
Observe that each constructor must be re-deﬁned to account for the new method, but the deﬁnition
makes use of the old class vector for the deﬁnitions of the old methods.
By this construction the new object type ρ∗is a subtype of the old object type ρ. Consequently,
any objects with the new method can be used in situations expecting an object without the new
method, as might be expected. To avoid redeﬁning old classes when a new method is introduced,
we may restrict inheritance so that new methods are only added to new subclasses. Subclasses
may then have more methods than super-classes, and objects of the subclass can be provided
when an object of the superclass is required.

252
27.3 Method-Based Inheritance
27.3
Method-Based Inheritance
The method-based organization is dual to that of the class-based organization. Recall that the
method-based organization given in Chapter 26 consists of a method vector emv of type
τmv ≜∏
d∈D
τ →ρd,
where the instance type τ is the sum type ∑c∈C τc. The method vector consists of a tuple of func-
tions that dispatch on the class of the object to determine their behavior.
Let us consider the effect of adding a new method d∗as described in Section 27.1. The new
method vector e∗mv has type
τ∗
mv ≜∏
d∈D∗
τ →ρd.
There is an isomorphism, written ( )‡, between τ∗mv and the type
τmv × (τ →ρd∗).
Using this isomorphism, the new method vector e∗mv is deﬁned as
⟨emv, λ (this : τ) case this {c · u ,→ec
d∗(u)}c∈C⟩‡.
The old method vector is re-used intact, extended with a dispatch function for the new method.
The object type does not change under the extension with a new method, but because ρ∗<: ρ,
there is no difﬁculty using a new object in a context expecting an old object—the added method is
ignored.
Finally, let us consider the effect of adding a new class c∗as described in Section 27.1. The new
method vector, e∗mv, has the type
τ∗
mv ≜∏
d∈D
τ∗→ρd,
where τ∗is the new object type ∑c∈C∗τc, which is a super-type of the old object type τ. There is
an isomorphism, written ( )†, between τ∗and the sum type τ + τc∗, which we may use to deﬁne
the new method vector e∗mv as follows:
⟨d ,→λ (this : τ∗) case this† {l · u ,→(emv · d)(u) | r · u ,→ec∗
d (u)}⟩d∈D.
Every method must be redeﬁned to account for the new class, but the old method vector is reused.
27.4
Notes
Abadi and Cardelli (1996) and Pierce (2002) provide thorough accounts of the interaction of in-
heritance and subtyping. Liskov and Wing (1994) discuss it from a behavioral perspective. They
propose to require that subclasses respect the behavior of the superclass when inheritance is used.

27.4 Notes
253
Exercises
27.1. Consider the case of extending a dispatch matrix with self-reference by a new class c∗in
which a method d is inherited from an existing class c. What requirements ensure that such
an inheritance is properly deﬁned? What happens if we extend a self-referential dispatch
matrix with a new method, d∗that inherits its behavior on class c from another method d?
27.2. Consider the example of two mutually recursive methods given in Exercise 26.3. Suppose
that num∗is a new class with instance type τnum∗<: τnum that inherits the ev method from
num, but deﬁnes its own version of the od method. What happens when message ev is sent
to an instance of num∗? Will the revised od method ever be invoked?
27.3. Method specialization consists of deﬁning a new class by inheriting methods from another
class or classes, while redeﬁning some of the methods that the inherited methods might
invoke. The behavior of the inherited methods on instances of the new class is altered to the
extent that they invoke a method that is specialized to the new class. Reconsider Exercise 26.3
in light of Exercise 27.2, seeking to ensure that the specialization of od is invoked when the
inherited method ev is invoked on instances of the new class.
(a) Redeﬁne the class num along the following lines. The instance data of num is an object
admitting methods ev and od. The class num admits these methods, and simply hands
them off to the instance object.
(b) The classes zero or of succ admit both the ev and od methods, and are deﬁned using
message send to effect mutual recursion as necessary.
(c) Deﬁne a subclass succ∗of succ that overrides the od method. Show that ev on an
instance of succ∗correctly invokes the overridden od method.

254
27.4 Notes

Part XII
Control Flow


Chapter 28
Control Stacks
Structural dynamics is convenient for proving properties of languages, such as a type safety the-
orem, but is less convenient as a guide for implementation. A structural dynamics deﬁnes a tran-
sition relation using rules that determine where to apply the next instruction without spelling out
how to ﬁnd where the instruction lies within an expression. To make this process explicit we in-
troduce a mechanism, called a control stack, that records the work that remains to be done after an
instruction is executed. Using a stack eliminates the need for premises on the transition rules so
that the transition system deﬁnes an abstract machine whose steps are determined by information
explicit in its state, much as a concrete computer does.
In this chapter we develop an abstract machine K for evaluating expressions in PCF. The
machine makes explicit the context in which primitive instruction steps are executed, and the
process by which the results are propagated to determine the next step of execution. We prove
that K and PCF are equivalent in the sense that both achieve the same outcomes for the same
expressions.
28.1
Machine Deﬁnition
A state s of the stack machine K for PCF consists of a control stack k and a closed expression e.
States take one of two forms:
1. An evaluation state of the form k ▷e corresponds to the evaluation of a closed expression e on
a control stack k.
2. A return state of the form k ◁e, where e val, corresponds to the evaluation of a stack k on a
closed value e.
As an aid to memory, note that the separator “points to” the focal entity of the state, the expression
in an evaluation state and the stack in a return state.
The control stack represents the context of evaluation. It records the “current location” of eval-
uation, the context into which the value of the current expression is returned. Formally, a control

258
28.1 Machine Deﬁnition
stack is a list of frames:
ϵ stack
(28.1a)
f frame
k stack
k;f stack
(28.1b)
The frames of the K machine are inductively deﬁned by the following rules:
s(−) frame
(28.2a)
ifz{e0; x.e1}(−) frame
(28.2b)
ap(−; e2) frame
(28.2c)
The frames correspond to search rules in the dynamics of PCF. Thus, instead of relying on the
structure of the transition derivation to keep a record of pending computations, we make an ex-
plicit record of them in the form of a frame on the control stack.
The transition judgment between states of the PCF machine is inductively deﬁned by a set of
inference rules. We begin with the rules for natural numbers, using an eager semantics for the
successor.
k ▷z 7−→k ◁z
(28.3a)
k ▷s(e) 7−→k;s(−) ▷e
(28.3b)
k;s(−) ◁e 7−→k ◁s(e)
(28.3c)
To evaluate z we simply return it. To evaluate s(e), we push a frame on the stack to record the
pending successor, and evaluate e; when that returns with e′, we return s(e′) to the stack.
Next, we consider the rules for case analysis.
k ▷ifz{e0; x.e1}(e) 7−→k;ifz{e0; x.e1}(−) ▷e
(28.4a)
k;ifz{e0; x.e1}(−) ◁z 7−→k ▷e0
(28.4b)
k;ifz{e0; x.e1}(−) ◁s(e) 7−→k ▷[e/x]e1
(28.4c)
The test expression is evaluated, recording the pending case analysis on the stack. Once the value
of the test expression is determined, the zero or non-zero branch of the condition is evaluated,
substituting the predecessor in the latter case.

28.2 Safety
259
Finally, we give the rules for functions, which are evaluated by-name, and the rule for general
recursion.
k ▷lam{τ}(x.e) 7−→k ◁lam{τ}(x.e)
(28.5a)
k ▷ap(e1; e2) 7−→k;ap(−; e2) ▷e1
(28.5b)
k;ap(−; e2) ◁lam{τ}(x.e) 7−→k ▷[e2/x]e
(28.5c)
k ▷fix{τ}(x.e) 7−→k ▷[fix{τ}(x.e)/x]e
(28.5d)
It is important that evaluation of a general recursion requires no stack space.
The initial and ﬁnal states of the K machine are deﬁned by the following rules:
ϵ ▷e initial
(28.6a)
e val
ϵ ◁e ﬁnal
(28.6b)
28.2
Safety
To deﬁne and prove safety for the PCF machine requires that we introduce a new typing judgment,
k ◁: τ, which states that the stack k expects a value of type τ. This judgment is inductively deﬁned
by the following rules:
ϵ ◁: τ
(28.7a)
k ◁: τ′
f : τ ⇝τ′
k;f ◁: τ
(28.7b)
This deﬁnition makes use of an auxiliary judgment, f : τ ⇝τ′, stating that a frame f transforms a
value of type τ to a value of type τ′.
s(−) : nat ⇝nat
(28.8a)
e0 : τ
x : nat ⊢e1 : τ
ifz{e0; x.e1}(−) : nat ⇝τ
(28.8b)
e2 : τ2
ap(−; e2) : parr(τ2; τ) ⇝τ
(28.8c)

260
28.3 Correctness of the K Machine
The states of the PCF machine are well-formed if their stack and expression components match:
k ◁: τ
e : τ
k ▷e ok
(28.9a)
k ◁: τ
e : τ
e val
k ◁e ok
(28.9b)
We leave the proof of safety of the PCF machine as an exercise.
Theorem 28.1 (Safety).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s ﬁnal or there exists s′ such that s 7−→s′.
28.3
Correctness of the K Machine
Does evaluation of an expression e using the K machine yield the same result as does the structural
dynamics of PCF? The answer to this question can be derived from the following facts.
Completeness If e 7−→∗e′, where e′ val, then ϵ ▷e 7−→∗ϵ ◁e′.
Soundness If ϵ ▷e 7−→∗ϵ ◁e′, then e 7−→∗e′ with e′ val.
To prove completeness a plausible ﬁrst step is to consider a proof by induction on the deﬁnition
of multi-step transition, which reduces the theorem to the following two lemmas:
1. If e val, then ϵ ▷e 7−→∗ϵ ◁e.
2. If e 7−→e′, then, for every v val, if ϵ ▷e′ 7−→∗ϵ ◁v, then ϵ ▷e 7−→∗ϵ ◁v.
The ﬁrst can be proved easily by induction on the structure of e. The second requires an inductive
analysis of the derivation of e 7−→e′ that gives rise to two complications. The ﬁrst complication is
that we cannot restrict attention to the empty stack, for if e is, say, ap(e1; e2), then the ﬁrst step of
the K machine is
ϵ ▷ap(e1; e2) 7−→ϵ;ap(−; e2) ▷e1.
To handle such situations we consider the evaluation of e1 on any stack, not just the empty stack.
Speciﬁcally, we prove that if e 7−→e′ and k ▷e′ 7−→∗k ◁v, then k ▷e 7−→∗k ◁v. Reconsider
the case e = ap(e1; e2), e′ = ap(e′
1; e2), with e1 7−→e′
1. We are given that k ▷ap(e′
1; e2) 7−→∗k ◁v,
and we are to show that k ▷ap(e1; e2) 7−→∗k ◁v. It is easy to show that the ﬁrst step of the former
derivation is
k ▷ap(e′
1; e2) 7−→k;ap(−; e2) ▷e′
1.
We would like to apply induction to the derivation of e1 7−→e′
1, but to do so we need a value v1
such that e′
1 7−→∗v1, which is not at hand.
We therefore consider the value of each sub-expression of an expression. This information is
given by the evaluation dynamics described in Chapter 7, which has the property that e ⇓e′ iff
e 7−→∗e′ and e′ val.
Lemma 28.2. If e ⇓v, then for every k stack, k ▷e 7−→∗k ◁v.

28.3 Correctness of the K Machine
261
The desired result follows by the analog of Theorem 7.2 for PCF, which states that e ⇓v iff
e 7−→∗v.
To prove soundness, we note that it is awkward to reason inductively about a multi-step tran-
sition from ϵ ▷e 7−→∗ϵ ◁v. The intermediate steps could involve alternations of evaluation and
return states. Instead we consider a K machine state to encode an expression, and show that the
machine transitions are simulated by the transitions of the structural dynamics.
To do so we deﬁne a judgment, s ↬e, stating that state s “unravels to” expression e. It will
turn out that for initial states, s = ϵ ▷e, and ﬁnal states, s = ϵ ◁e, we have s ↬e. Then we show
that if s 7−→∗s′, where s′ ﬁnal, s ↬e, and s′ ↬e′, then e′ val and e 7−→∗e′. For this it is enough to
show the following two facts:
1. If s ↬e and s ﬁnal, then e val.
2. If s 7−→s′, s ↬e, s′ ↬e′, and e′ 7−→∗v, where v val, then e 7−→∗v.
The ﬁrst is quite simple, we need only note that the unraveling of a ﬁnal state is a value. For the
second, it is enough to prove the following lemma.
Lemma 28.3. If s 7−→s′, s ↬e, and s′ ↬e′, then e 7−→∗e′.
Corollary 28.4. e 7−→∗n iff ϵ ▷e 7−→∗ϵ ◁n.
28.3.1
Completeness
Proof of Lemma 28.2. The proof is by induction on an evaluation dynamics for PCF.
Consider the evaluation rule
e1 ⇓lam{τ2}(x.e)
[e2/x]e ⇓v
ap(e1; e2) ⇓v
(28.10)
For an arbitrary control stack k we are to show that k ▷ap(e1; e2) 7−→∗k ◁v. Applying both of the
inductive hypotheses in succession, interleaved with steps of the K machine, we obtain
k ▷ap(e1; e2) 7−→k;ap(−; e2) ▷e1
7−→∗k;ap(−; e2) ◁lam{τ2}(x.e)
7−→k ▷[e2/x]e
7−→∗k ◁v.
The other cases of the proof are handled similarly.
28.3.2
Soundness
The judgment s ↬e′, where s is either k ▷e or k ◁e, is deﬁned in terms of the auxiliary judgment
k ▷◁e = e′ by the following rules:
k ▷◁e = e′
k ▷e ↬e′
(28.11a)

262
28.3 Correctness of the K Machine
k ▷◁e = e′
k ◁e ↬e′
(28.11b)
In words, to unravel a state we wrap the stack around the expression to form a complete program.
The unraveling relation is inductively deﬁned by the following rules:
ϵ ▷◁e = e
(28.12a)
k ▷◁s(e) = e′
k;s(−) ▷◁e = e′
(28.12b)
k ▷◁ifz{e0; x.e1}(e) = e′
k;ifz{e0; x.e1}(−) ▷◁e = e′
(28.12c)
k ▷◁ap(e1; e2) = e
k;ap(−; e2) ▷◁e1 = e
(28.12d)
These judgments both deﬁne total functions.
Lemma 28.5. The judgment s ↬e relates every state s to a unique expression e, and the judgment k ▷◁e =
e′ relates every stack k and expression e to a unique expression e′.
We are therefore justiﬁed in writing k ▷◁e for the unique e′ such that k ▷◁e = e′.
The following lemma is crucial. It states that unraveling preserves the transition relation.
Lemma 28.6. If e 7−→e′, k ▷◁e = d, k ▷◁e′ = d′, then d 7−→d′.
Proof. The proof is by rule induction on the transition e 7−→e′. The inductive cases, where the
transition rule has a premise, follow easily by induction. The base cases, where the transition is an
axiom, are proved by an inductive analysis of the stack k.
For an example of an inductive case, suppose that e = ap(e1; e2), e′ = ap(e′
1; e2), and e1 7−→e′
1.
We have k ▷◁e = d and k ▷◁e′ = d′. It follows from rules (28.12) that k;ap(−; e2) ▷◁e1 = d and
k;ap(−; e2) ▷◁e′
1 = d′. So by induction d 7−→d′, as desired.
For an example of a base case, suppose that e = ap(lam{τ2}(x.e); e2) and e′ = [e2/x]e with
e 7−→e′ directly. Assume that k ▷◁e = d and k ▷◁e′ = d′; we are to show that d 7−→d′. We
proceed by an inner induction on the structure of k. If k = ϵ, the result follows immediately.
Consider, say, the stack k = k′;ap(−; c2). It follows from rules (28.12) that k′ ▷◁ap(e; c2) = d
and k′ ▷◁ap(e′; c2) = d′. But by the structural dynamics ap(e; c2) 7−→ap(e′; c2), so by the inner
inductive hypothesis we have d 7−→d′, as desired.
We may now complete the proof of Lemma 28.3.
Proof of Lemma 28.3. The proof is by case analysis on the transitions of the K machine. In each case,
after unraveling, the transition will correspond to zero or one transitions of the PCF structural
dynamics.
Suppose that s = k ▷s(e) and s′ = k;s(−) ▷e. Note that k ▷◁s(e) = e′ iff k;s(−) ▷◁e = e′, from
which the result follows immediately.

28.4 Notes
263
Suppose that s = k;ap(lam{τ}(x.e1); −) ◁e2 and s′ = k ▷[e2/x]e1.
Let e′ be such that
k;ap(lam{τ}(x.e1); −) ▷◁e2 = e′ and let e′′ be such that k ▷◁[e2/x]e1 = e′′. Observe that k ▷◁
ap(lam{τ}(x.e1); e2) = e′. The result follows from Lemma 28.6.
28.4
Notes
The abstract machine considered here is typical of a wide class of machines that make control ﬂow
explicit in the state. The prototype is the SECD machine (Landin, 1965), which is a linearization of
a structural operational semantics (Plotkin, 1981). The advantage of a machine model is that the
explicit treatment of control is needed for languages that allow the control state to be manipulated
(see Chapter 30 for a prime example). The disadvantage is that the control state of the computation
must be made explicit, necessitating rules for manipulating it that are left implicit in a structural
dynamics.
Exercises
28.1. Give the proof of Theorem 28.1 for conditional expressions.
28.2. Formulate a call-by-value variant of the PCF machine.
28.3. Analyze the worst-case asymptotic complexity of executing each instruction of the K ma-
chine.
28.4. Reﬁne the proof of Lemma 28.2 by bounding the number of machine steps taken for each
step of the PCF dynamics.

264
28.4 Notes

Chapter 29
Exceptions
Exceptions effect a non-local transfer of control from the point at which the exception is raised
to an enclosing handler for that exception. This transfer interrupts the normal ﬂow of control in
a program in response to unusual conditions. For example, exceptions can be used to signal an
error condition, or to signal the need for special handling in unusual circumstances. We could
use conditionals to check for and process errors or unusual conditions, but using exceptions is
often more convenient, particularly because the transfer to the handler is conceptually direct and
immediate, rather than indirect via explicit checks.
In this chapter we will consider two extensions of PCF with exceptions. The ﬁrst, FPCF, en-
riches PCF with the simplest form of exception, called a failure, with no associated data. A failure
can be intercepted, and turned into a success (or another failure!) by transferring control to an-
other expression. The second, XPCF, enriches PCF with exceptions, with associated data that is
passed to an exception handler that intercepts it. The handler may analyze the associated data to
determine how to recover from the exceptional condition. A key choice is to decide on the type of
the data associated to an exception.
29.1
Failures
The syntax of FPCF is deﬁned by the following extension of the grammar of PCF:
Exp
e
::=
fail
fail
signal a failure
catch(e1; e2)
catch e1 ow e2
catch a failure
The expression fail aborts the current evaluation, and the expression catch(e1; e2) catches any
failure in e1 by evaluating e2 instead. Either e1 or e2 may themselves abort, or they may diverge or
return a value as usual in PCF.
The statics of FPCF is given by these rules:
Γ ⊢fail : τ
(29.1a)

266
29.1 Failures
Γ ⊢e1 : τ
Γ ⊢e2 : τ
Γ ⊢catch(e1; e2) : τ
(29.1b)
A failure can have any type, because it never returns. The two expressions in a catch expression
must have the same type, because either might determine the value of that expression.
The dynamics of FPCF is given using a technique called stack unwinding. Evaluation of a catch
pushes a frame of the form catch(−; e) onto the control stack that awaits the arrival of a failure.
Evaluation of a fail expression pops frames from the control stack until it reaches a frame of
the form catch(−; e), at which point the frame is removed from the stack and the expression e is
evaluated. Failure propagation is expressed by a state of the form k ◀, which extends the two
forms of state considered in Chapter 28 to express failure propagation.
The FPCF machine extends the PCF machine with the following additional rules:
k ▷fail 7−→k ◀
(29.2a)
k ▷catch(e1; e2) 7−→k;catch(−; e2) ▷e1
(29.2b)
k;catch(−; e2) ◁v 7−→k ◁v
(29.2c)
k;catch(−; e2) ◀7−→k ▷e2
(29.2d)
( f ̸= catch(−; e))
k;f ◀7−→k ◀
(29.2e)
Evaluating fail propagates a failure up the stack. The act of failing itself, fail, will, of course,
give rise to a failure. Evaluating catch(e1; e2) consists of pushing the handler on the control stack
and evaluating e1. If a value reaches to the handler, the handler is removed and the value is passed
to the surrounding frame. If a failure reaches the handler, the stored expression is evaluated with
the handler removed from the control stack. Failures propagate through all frames other than the
catch frame.
The initial and ﬁnal states of the FPCF machine are deﬁned by the following rules:
ϵ initial
(29.3a)
e val
ϵ ◁e ﬁnal
(29.3b)
ϵ ◀ﬁnal
(29.3c)
The deﬁnition of stack typing given in Chapter 28 can be extended to account for the new forms
of frame so that safety can be proved in the same way as before. The only difference is that the
statement of progress must be weakened to take account of failure: a well-typed expression is
either a value, or may take a step, or may signal failure.

29.2 Exceptions
267
Theorem 29.1 (Safety for FPCF).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s ﬁnal or there exists s′ such that s 7−→s′.
29.2
Exceptions
The language XPCF enriches FPCF with exceptions, failures to which a value is attached. The
syntax of XPCF extends that of PCF with the following forms of expression:
Exp
e
::=
raise(e)
raise(e)
raise an exception
try(e1; x.e2)
try e1 ow x ,→e2
handle an exception
The argument to raise is evaluated to determine the value passed to the handler. The expression
try(e1; x.e2) binds a variable x in the handler e2. The associated value of the exception is bound to
that variable within e2, should an exception be raised when e1 is evaluated.
The statics of exceptions extends the statics of failures to account for the type of the value
carried with the exception:
Γ ⊢e : τexn
Γ ⊢raise(e) : τ
(29.4a)
Γ ⊢e1 : τ
Γ, x : τexn ⊢e2 : τ
Γ ⊢try(e1; x.e2) : τ
(29.4b)
The type τexn is some ﬁxed, but as yet unspeciﬁed, type of exception values. (The choice of τexn is
discussed in Section 29.3.)
The dynamics of XPCF is similar to that of FPCF, except that the failure state k ◀is replaced
by the exception state k ◀e which passes an exception value e to the stack k. There is only one
notion of exception, but the associated value can be used to identify the source of the exception.
We use a by-value interpretation to avoid the problem of imprecise exceptions that arises under a
by-name interpretation.
The stack frames of the PCF machine are extended to include raise(−) and try(−; x.e2).
These are used in the following rules:
k ▷raise(e) 7−→k;raise(−) ▷e
(29.5a)
k;raise(−) ◁e 7−→k ◀e
(29.5b)
k ▷try(e1; x.e2) 7−→k;try(−; x.e2) ▷e1
(29.5c)
k;try(−; x.e2) ◁e 7−→k ◁e
(29.5d)

268
29.3 Exception Values
k;try(−; x.e2) ◀e 7−→k ▷[e/x]e2
(29.5e)
( f ̸= try(−; x.e2))
k;f ◀e 7−→k ◀e
(29.5f)
The main difference compared to Rules (29.2) is that an exception passes a values to the stack,
whereas a failure does not.
The initial and ﬁnal states of the XPCF machine are deﬁned by the following rules:
ϵ ▷e initial
(29.6a)
e val
ϵ ◁e ﬁnal
(29.6b)
ϵ ◀e ﬁnal
(29.6c)
Theorem 29.2 (Safety for XPCF).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s ﬁnal or there exists s′ such that s 7−→s′.
29.3
Exception Values
The statics of XPCF is parameterized by the type τexn of values associated to exceptions. The
choice of τexn is important because it determines how the source of an exception is identiﬁed in a
program. If τexn is the one-element type unit, then exceptions degenerate to failures, which are
unable to identify their source. Thus τexn must have more than one value to be useful.
This fact suggests that τexn should be a ﬁnite sum. The classes of the sum identify the sources of
exceptions, and the classiﬁed value carries information about the particular instance. For example,
τexn might be a sum type of the form
[div ,→unit, fnf ,→string, . . .].
Here the class div might represent an arithmetic fault, with no associated data, and the class fnf
might represent a “ﬁle not found” error, with associated data being the name of the ﬁle that was
not found.
Using a sum means that an exception handler can dispatch on the class of the exception value
to identify its source and cause. For example, we might write
handle e1 ow x ,→
match x {
div ⟨⟩,→ediv
| fnf s ,→efnf }

29.4 Notes
269
to handle the exceptions speciﬁed by the above sum type. Because the exception and its associated
data are coupled in a sum type, there is no possibility of misinterpreting the data associated to one
exception as being that of another.
The disadvantage of choosing a ﬁnite sum for τexn is that it speciﬁes a closed world of possible
exception sources. All sources must be identiﬁed for the entire program, which impedes modular
development and evolution. A more modular approach admits an open world of exception sources
that can be introduced as the program evolves and even as it executes. A generalization of ﬁnite
sums, called dynamic classiﬁcation, deﬁned in Chapter 33, is required for an open world. (See that
Chapter for further discussion.)
When τexn is a type of classiﬁed values, its classes are often called exceptions, so that one may
speak of “the fnf exception” in the above example. This terminology is harmless, and all but
unavoidable, but it invites confusion between two separate ideas:
1. Exceptions as a control mechanism that allows the course of evaluation to be altered by raising
and handling exceptions.
2. Exceptions as a data value associated with such a deviation of control that allows the source
of the deviation to be identiﬁed.
As a control mechanism exceptions can be eliminated using explicit exception passing. A computa-
tion of type τ that may raise an exception is interpreted as an exception-free computation of type
τ + τexn; see Exercise 29.5 for more on this method.
29.4
Notes
Various forms of exceptions were considered in Lisp (Steele, 1990). The original formulation of
ML (Gordon et al., 1979) as a metalanguage for mechanized logic used failures to implement back-
tracking proof search. Most modern languages now have exceptions, but differ in the forms of
data that may be associated with them.
Exercises
29.1. Prove Theorem 29.2. Are any properties of τexn required for the proof?
29.2. Give an evaluation dynamics for XPCF using the following judgment forms:
• Normal evaluation: e ⇓v, where e : τ, v : τ, and v val.
• Exceptional evaluation: e ⇑v, where e : τ, and v : τexn, and v val.
The ﬁrst states that e evaluates normally to value v, the second that e raises an exception with
value v.
29.3. Give a structural operational dynamics to XPCF by inductively deﬁning the following judg-
ment forms:

270
29.4 Notes
• e 7−→e′, stating that expression e transitions to expression e′;
• e val, stating that expression e is a value.
Ensure that e ⇓v iff e 7−→∗v, and e ⇑v iff e 7−→∗raise(v), where v val in both cases.
29.4. The closed world assumption on exceptions amounts to choosing the type of exception val-
ues to be a ﬁnite sum type shared by the entire program. Under such an assumption it is
possible to track exceptions by placing an upper bound on the possible classes of an excep-
tion value.
Type reﬁnements (deﬁned in Chapter 25) can be used for exception tracking in a closed-
world setting. Deﬁne ﬁnite sum reﬁnements by the rule
X′ ⊆X
(∀x ∈X′) φx ⊑τx
[φx]x∈X′ ⊑[τx]x∈X
.
In particular, the reﬁnement ∅is the vacuous sum reﬁnement [] satisﬁed by no value. Entail-
ment of ﬁnite sum reﬁnements is deﬁned by the rule
X′ ⊆X′′
(∀x ∈X′) φx ≤φ′
x
[φx]x∈X′ ≤[φ′
x]x∈X′′
So, in particular, ∅≤φ for all sum reﬁnements φ of τexn. Entailment weakens knowledge of
the class of a value of sum type, which is crucial to their application to exception tracking.
The goal of this exercise is to develop a system of type reﬁnements for the modal formulation
of exceptions in MPCF using sum reﬁnements to perform exception tracking.
(a) Deﬁne the command reﬁnement judgment m ∈τ φ ow χ, where m ∼·· τ, φ ⊑τ, and
χ ⊑τexn, to mean that if m returns e, then e ∈τ φ, and if m raises e, then e ∈τexn χ.
(b) Deﬁne satisfaction and entailment for the expression reﬁnement cmd(φ; χ) ⊑cmd(τ),
where φ ⊑τ and χ ⊑τexn. This reﬁnement classiﬁes encapsulated commands that sat-
isfy the stated value and exception reﬁnements in the sense of the preceding problem.
29.5. Show that exceptions in MPCF can be eliminated by a translation into PCF enriched with
sum types by what is called the exception-passing style transformation. Each command m ∼·· τ
of MPCF is translated to a pure expression bm of type bτ + τexn whose value is either l · e,
where e : τ, for normal return, or r · e, where e : τexn, for an exceptional return. The command
translation is extended to an expression translation be that replaces occurrences of cmd(m) by
bm. The corresponding type translation, bτ, replaces cmd(τ) by bτ + τexn. Deﬁne the command
translation from MPCF to PCF enriched with sums, and show that it has the required type
and correctly simulates the behavior of exceptions.

Chapter 30
Continuations
The semantics of many control constructs (such as exceptions and coroutines) can be expressed in
terms of reiﬁed control stacks, a representation of a control stack as a value that can be reactivated at
any time, even if control has long since returned past the point of reiﬁcation. Reiﬁed control stacks
of this kind are called continuations; they are values that can be passed and returned at will in a
computation. Continuations never “expire”, and it is always sensible to reinstate a continuation
without compromising safety. Thus continuations support unlimited “time travel” — we can go
back to a previous step of the computation, then return to some point in its future.
Why are continuations useful? Fundamentally, they are representations of the control state of
a computation at a given time. Using continuations we can “checkpoint” the control state of a pro-
gram, save it in a data structure, and return to it later. In fact this is precisely what is necessary to
implement threads (concurrently executing programs) — the thread scheduler suspends a program
for later execution from where it left off.
30.1
Overview
We will consider the extension KPCF of PCF with the type cont(τ) of continuations accepting
values of type τ. The introduction form for cont(τ) is letcc{τ}(x.e), which binds the current
continuation (that is, the current control stack) to the variable x, and evaluates the expression e.
The corresponding elimination form is throw{τ}(e1; e2), which restores the value given by e1 to
the control stack given by e2.
To illustrate the use of these primitives, consider the problem of multiplying the ﬁrst n elements
of an inﬁnite sequence q of natural numbers, where q is represented by a function of type nat ⇀
nat. If zero occurs among the ﬁrst n elements, we would like to effect an “early return” with the
value zero, without further multiplication. This problem can be solved using exceptions, but we
will solve it with continuations to show how they are used.
Here is the solution in PCF, without short-cutting:

272
30.1 Overview
fix ms is
λ q : nat ⇀nat.
λ n : nat.
case n {
z ,→s(z)
| s(n’) ,→(q z) × (ms (q ◦succ) n’)
}
The recursive call composes q with the successor function to shift the sequence by one step.
Here is the solution in KPCF, with short-cutting:
λ q : nat ⇀nat.
λ n : nat.
letcc ret : nat cont in
let ms be
fix ms is
λ q : nat ⇀nat.
λ n : nat.
case n {
z ,→s(z)
| s(n’) ,→
case q z {
z ,→throw z to ret
| s(n’’) ,→(q z) × (ms (q ◦succ) n’)
}
}
in
ms q n
The letcc binds the return point of the function to the variable ret for use within the main loop
of the computation. If an element is zero, control is thrown to ret, effecting an early return with
the value zero.
To take another example, given that k has type τ cont and f has type τ′ ⇀τ, return k′ of type
return a continuation k′ of type τ′ cont such that throwing a value v′ of type τ′ to k′ throws the
value of f (v′) to k. Thus we seek to deﬁne a function compose of type
(τ′ ⇀τ) ⇀τ cont ⇀τ′ cont.
The continuation we seek is the one in effect at the point of the ellipsis in the expression throw
f(...) to k. It is the continuation that, when given a value v′, applies f to it, and throws the
result to k. We can seize this continuation using letcc by writing
throw f(letcc x:τ′ cont in ...) to k
The desired continuation is bound to x, but how can we return it as the result of compose? We use
the same idea as for short-circuit multiplication, writing

30.2 Continuation Dynamics
273
letcc ret:τ′ cont cont in
throw (f (letcc r in throw r to ret)) to k
as the body of compose. Note that the type of ret is τ cont cont, that of a continuation that expects
to be thrown a continuation!
30.2
Continuation Dynamics
The syntax of KPCF is as follows:
Type
τ
::=
cont(τ)
τ cont
continuation
Expr
e
::=
letcc{τ}(x.e)
letcc x in e
mark
throw{τ}(e1; e2)
throw e1 to e2
goto
cont(k)
cont(k)
continuation
The expression cont(k) is a reiﬁed control stack, which arises during evaluation.
The statics of KPCF is deﬁned by the following rules:
Γ, x : cont(τ) ⊢e : τ
Γ ⊢letcc{τ}(x.e) : τ
(30.1a)
Γ ⊢e1 : τ1
Γ ⊢e2 : cont(τ1)
Γ ⊢throw{τ}(e1; e2) : τ
(30.1b)
The result type of a throw expression is arbitrary because it does not return to the point of the call.
The statics of continuation values is given by the following rule:
k : τ
Γ ⊢cont(k) : cont(τ)
(30.2)
A continuation value cont(k) has type cont(τ) exactly if it is a stack accepting values of type τ.
To deﬁne the dynamics of KPCF we extend the PCF machine with two forms of stack frame:
throw{τ}(−; e2) frame
(30.3a)
e1 val
throw{τ}(e1; −) frame
(30.3b)
Every reiﬁed control stack is a value:
k stack
cont(k) val
(30.4)
The transition rules of the PCF machine governing continuations are as follows:
k ▷cont(k) 7−→k ◁cont(k)
(30.5a)

274
30.3 Coroutines from Continuations
k ▷letcc{τ}(x.e) 7−→k ▷[cont(k)/x]e
(30.5b)
k ▷throw{τ}(e1; e2) 7−→k;throw{τ}(−; e2) ▷e1
(30.5c)
e1 val
k;throw{τ}(−; e2) ◁e1 7−→k;throw{τ}(e1; −) ▷e2
(30.5d)
e val
k;throw{τ}(e; −) ◁cont(k′) 7−→k′ ◁e
(30.5e)
Evaluation of a letcc expression duplicates the control stack; evaluation of a throw expression
destroys the current control stack.
The safety of KPCF is proved by extending the safety proof for the K machine given in Chap-
ter 28.
We need only add typing rules for the two new forms of frame, which are as follows:
e2 : cont(τ)
throw{τ′}(−; e2) : τ ⇝τ′
(30.6a)
e1 : τ
e1 val
throw{τ′}(e1; −) : cont(τ) ⇝τ′
(30.6b)
The rest of the deﬁnitions remain as in Chapter 28.
Lemma 30.1 (Canonical Forms). If e : cont(τ) and e val, then e = cont(k) for some k such that k : τ.
Theorem 30.2 (Safety).
1. If s ok and s 7−→s′, then s′ ok.
2. If s ok, then either s ﬁnal or there exists s′ such that s 7−→s′.
30.3
Coroutines from Continuations
The distinction between a routine and a subroutine is the distinction between a manager and a
worker. The routine calls the subroutine to do some work, and the subroutine returns to the routine
when its work is done. The relationship is asymmetric in that there is a distinction between the
caller, the main routine, and the callee, the subroutine. It is useful to consider a symmetric situation
in which two routines each call the other to do some work. Such a pair of routines are called
coroutines; their relationship to one another is symmetric, not hierarchical.
A subroutine is implemented by having the caller pass to the callee a continuation representing
the work to be done once the subroutine ﬁnishes. When it does, it throws the return value to that
continuation, without the possibility of return. A coroutine is implemented by having two routines
each call each other as subroutines by providing a continuation when control is ceded from one to
the other. The only tricky part is how the entire process gets started.
Consider the type of each routine of the pair. A routine is a continuation accepting two argu-
ments, a data to be passed to the routine when it is resumed, and a continuation to be resumed
when the routine has ﬁnished its task. The datum represents the state of the computation, and the

30.3 Coroutines from Continuations
275
continuation is a coroutine that accepts arguments of the same form. Thus, the type of a coroutine
must satisfy the type isomorphism
τ coro ∼= (τ × τ coro) cont.
Therefore we deﬁne τ coro to be the recursive type
τ coro ≜rec t is (τ × t) cont.
Up to isomorphism, the type τ coro is the type of continuations that accept a value of type τ,
representing the state of the coroutine, and the partner coroutine, a value of the same type.
A coroutine r passes control to another coroutine r′ by evaluating the expression resume(⟨s, r′⟩),
where s is the current state of the computation. Doing so creates a new coroutine whose entry point
is the return point (calling site) of resume. Therefore the type of resume is
τ × τ coro ⇀τ × τ coro.
The deﬁnition of resume is as follows:
λ (⟨s, r′⟩: τ × τ coro) letcc k in throw ⟨s, fold(k)⟩to unfold(r′)
When applied, resume seizes the current continuation, and passes the state, s, and the seized con-
tinuation (packaged as a coroutine) to the called coroutine.
Because the state is explicitly passed from one routine to the other, a coroutine is a state trans-
formation function that, when activated with the current state, determines the next state of the
computation. A system of coroutines is created by establishing a joint exit point to which the re-
sult of the system is thrown, and creating a pair of coroutines that transform the state and pass
control to the partner routine. If either routine wishes to terminate the computation, it does so by
throwing a result value to their common exit point. Thus, a coroutine is a function of type
(τ′,τ) rout ≜τ′ cont ⇀τ ⇀τ,
where τ′ is the result type and τ is the state type of the system of coroutines.
To set up a system of coroutines we deﬁne a function run that, given two routines, creates
a function of type τ ⇀τ′ that, when applied to the initial state, computes a result of type τ′.
The computation consists of a cooperating pair of routines that share a common exit point. The
deﬁnition of run begins as follows:
λ (⟨r1, r2⟩) λ (s0) letcc x0 in let r′
1 be r1(x0) in let r′
2 be r2(x0) in . . .
Given two routines, run establishes their common exit point, and passes this continuation to both
routines. By throwing to this continuation either routine may terminate the computation with a
result of type τ′. The body of the run function continues as follows:
rep(r′
2)(letcc k in rep(r′
1)(⟨s0, fold(k)⟩))

276
30.3 Coroutines from Continuations
The auxiliary function rep creates an inﬁnite loop that transforms the state and passes control to
the other routine:
λ (t) fix l is λ (⟨s, r⟩) l(resume(⟨t(s), r⟩)).
The system is initialized by starting routine r1 with the initial state, and arranging that, when it
cedes control to its partner, it starts routine r2 with the resulting state. At that point the system is
bootstrapped: each routine will resume the other on each iteration of the loop.
A good example of coroutining is the interleaving of input and output in a computation. This
is done by coroutining between a producer routine and a consumer routine. The producer emits the
next element of the input, if any, and passes control to the consumer, removing that element from
the input. The consumer processes the next data item, and returns control to the producer, with the
result of processing attached to the output. For simplicity input and output are modeled as lists of
type τi list and τo list, respectively, which are passed back and forth between the routines. The
routines exchange messages according to the following protocol. The message OK(⟨i, o⟩) is sent
from the consumer to producer to acknowledge receipt of the previous message, and to pass back
the current state of the input and output channels. The message EMIT(⟨e, ⟨i, o⟩⟩), where e is a value
of type τi opt, is sent from the producer to the consumer to emit the next value (if any) from the
input, and to pass the current state of the input and output channels to the consumer.
Here is an implementation of the producer/consumer coroutines. The type τ of the state main-
tained by the routines is the labeled sum type
[OK ,→τi list × τo list, EMIT ,→τi opt × (τi list × τo list)].
The above type speciﬁes the message protocol between the producer and the consumer described
in the preceding paragraph.
The producer P is deﬁned by the expression
λ (x0) λ (msg) case msg {b1 | b2 | b3},
where the ﬁrst branch b1 is
OK · ⟨nil, os⟩,→EMIT · ⟨null, ⟨nil, os⟩⟩
and the second branch b2 is
OK · ⟨cons(i; is), os⟩,→EMIT · ⟨just(i), ⟨is, os⟩⟩,
and the third branch b3 is
EMIT · ,→error.
In words, if the input is exhausted, the producer emits the value null, along with the current
channel state. Otherwise, it emits just(i), where i is the ﬁrst remaining input, and removes that
element from the passed channel state. The producer cannot see an EMIT message, and signals an
error if it should occur.
The consumer C is deﬁned by the expression
λ (x0) λ (msg) case msg {b′
1 | b′
2 | b′
3},

30.4 Notes
277
where the ﬁrst branch b′
1 is
EMIT · ⟨null, ⟨, os⟩⟩,→throw os to x0,
the second branch b′
2 is
EMIT · ⟨just(i), ⟨is, os⟩⟩,→OK · ⟨is, cons( f (i); os)⟩,
and the third branch b′
3 is
OK · ,→error.
The consumer dispatches on the emitted datum. If it is absent, the output channel state is passed
to x0 as the overall value of the computation. If it is present, the function f (unspeciﬁed here)
of type τi ⇀τo is applied to transform the input to the output, and the result is added to the
output channel. If the message OK is received, the consumer signals an error, as the producer never
produces such a message.
The initial state s0 has the form OK · ⟨is, os⟩, where is and os are the initial input and output
channel state, respectively. The computation is created by the expression
run(⟨P, C⟩)(s0),
which sets up the coroutines as described earlier.
Although it is relatively easy to visualize and implement coroutines involving only two part-
ners, it is more complex, and less useful, to consider a similar pattern of control among n ≥2 par-
ticipants. In such cases it is more common to structure the interaction as a collection of n routines,
each of which is a coroutine of a central scheduler. When a routine resumes its partner, it passes
control to the scheduler, which determines which routine to execute next, again as a coroutine of
itself. When structured as coroutines of a scheduler, the individual routines are called threads. A
thread yields control by resuming its partner, the scheduler, which then determines which thread
to execute next as a coroutine of itself. This pattern of control is called cooperative multi-threading,
because it is based on voluntary yields, rather than forced suspensions by a scheduler.
30.4
Notes
Continuations are a ubiquitous notion in programming languages. Reynolds (1993) provides an
excellent account of the multiple discoveries of continuations. The formulation given here is in-
spired by Felleisen and Hieb (1992), who pioneered the development of linguistic theories of con-
trol and state.
Exercises
30.1. Type safety for KPCF follows almost directly from Theorem 28.1. Isolate the key observa-
tions required to extend the proof to include continuation types.

278
30.4 Notes
30.2. Exhibit a closed KPCF expression of each of the following types:
(a) τ + (τ cont).
(b) τ cont cont →τ.
(c) (τ2 cont →τ1 cont) →(τ1 →τ2).
(d) (τ1 + τ2) cont →(τ1 cont × τ2 cont).
Hint: you will need to use letcc and throw.
30.3. The type stream of inﬁnite streams of natural numbers deﬁned in Chapter 15 can be imple-
mented using continuations. Deﬁne stream to be the recursive type satisfying the isomor-
phism
stream ∼= (nat × stream) cont cont.
To examine the front of the stream, throw to it a continuation expecting a natural number and
another stream. When passed such a continuation, the stream throws to it the next number
in the stream, paired with another stream (that is, another continuation) representing the
stream of numbers following that number. Deﬁne the introduction and elimination forms
for streams deﬁned in Chapter 15 using this representation.

Part XIII
Symbolic Data


Chapter 31
Symbols
A symbol is an atomic datum with no internal structure. Whereas a variable is given meaning
by substitution, a symbol is given meaning by a family of operations indexed by symbols. A
symbol is just a name, or index, for a family of operations. Many different interpretations may
be given to symbols according to the operations we choose to consider, giving rise to concepts
such as ﬂuid binding, dynamic classiﬁcation, mutable storage, and communication channels. A
type is associated to each symbol whose interpretation depends on the particular application. For
example, in the case of mutable storage, the type of a symbol constrains the contents of the cell
named by that symbol to values of that type.
In this chapter we consider two constructs for computing with symbols. The ﬁrst is a means
of declaring new symbols for use within a speciﬁed scope. The expression new a ~ ρ in e introduces
a “new” symbol a with associated type ρ for use within e. The declared symbol a is “new” in
the sense that it is bound by the declaration within e, and so may be renamed at will to ensure
that it differs from any ﬁnite set of active symbols. Whereas the statics determines the scope of a
declared symbol, its range of signiﬁcance, or extent, is determined by the dynamics. There are two
different dynamic interpretations of symbols, the scoped and the free (short for scope-free) dynamics.
The scoped dynamics limits the extent of the symbol to its scope; the lifetime of the symbol is
restricted to the evaluation of its scope. Alternatively, under the free dynamics the extent of a
symbol exceeds its scope, extending to the entire computation of which it is a part. We may say
that in the free dynamics a symbol “escapes its scope,” but it is more accurate to say that its scope
widens to encompass the rest of the computation.
The second construct associated with symbols is the concept of a symbol reference, an expression
whose purpose is to refer to a particular symbol. Symbol references are values of a type ρ sym and
are written ’a for some symbol a with associated type ρ. The elimination form for the type ρ sym
is a conditional branch that determines whether a symbol reference refers to a statically speciﬁed
symbol. The statics of the elimination form ensures that, in the positive case, the type associated
to the referenced symbol is manifested, whereas in the negative case, no type information can be
gleaned from the test.

282
31.1 Symbol Declaration
31.1
Symbol Declaration
We will consider here an extension SPCF of PCF with the means to allocate new symbols. This
capability will be used in later chapters that use symbols for other purposes. Here we will only be
concerned with symbol allocation, and the introduction and elimination of symbols as values of a
type of plain symbols.
The syntax for symbol declaration in SPCF is given by the following grammar:
Exp
e
::=
new{τ}(a.e)
new a ~ τ in e
generation
The statics of symbol declaration makes use of a signature, or symbol context, that associates a type
to each of a ﬁnite set of symbols. We use the letter Σ to range over signatures, which are ﬁnite
sets of pairs a ~ τ, where a is a symbol and τ is a type. The typing judgment Γ ⊢Σ e : τ is
parameterized by a signature Σ associating types to symbols. In effect there is an inﬁnite family of
typing judgments, one for each choice of Σ. The expression new a ~ τ in e shifts from one instance
of the family to another by adding a new symbol to Σ.
The statics of symbol declaration makes use of a judgment, τ mobile, whose deﬁnition depends
on whether the dynamics is scoped. In a scoped dynamics mobility is deﬁned so that the computed
value of a mobile type cannot depend on any symbol. By constraining the scope of a declaration to
have mobile type, we can, under this interpretation, ensure that the extent of a symbol is conﬁned
to its scope. In a free dynamics every type is deemed mobile, because the dynamics ensures that
the scope of a symbol is widened to accommodate the possibility that the value returned from the
scope of a declaration may depend on the declared symbol. The term “mobile” reﬂects the infor-
mal idea that symbols may or may not be “moved” from the scope of their declaration according
to the dynamics given to them. A free dynamics allows symbols to be moved freely, whereas a
scoped dynamics limits their range of motion.
The statics of symbol declaration itself is given by the following rule:
Γ ⊢Σ,a~ρ e : τ
τ mobile
Γ ⊢Σ new{ρ}(a.e) : τ
(31.1)
As mentioned, the condition on τ ensures that the returned value does not escape its scope, if any.
31.1.1
Scoped Dynamics
The scoped dynamics of symbol declaration is given by a transition judgment of the form e 7−→
Σ e′
indexed by a signature Σ specifying the active symbols of the transition. Either e or e′ may involve
the symbols declared in Σ, but no others.
e 7−−−→
Σ,a~ρ e′
new{ρ}(a.e) 7−→
Σ new{ρ}(a.e′)
(31.2a)
e valΣ
new{ρ}(a.e) 7−→
Σ e
(31.2b)

31.1 Symbol Declaration
283
Rule (31.2a) speciﬁes that evaluation takes place within the scope of the declaration of a symbol.
Rule (31.2b) speciﬁes that the declared symbol is “forgotten” once its scope has been evaluated.
The deﬁnition of the judgment τ mobile must be chosen to ensure that the following mobility
condition is satisﬁed:
If τ mobile, ⊢Σ,a~ρ e : τ, and e valΣ,a~ρ, then ⊢Σ e : τ and e valΣ.
For example, in the presence of symbol references (see Section 31.2 below), a function type cannot
be deemed mobile, because a function may contain a reference to a local symbol. The type nat
may only be deemed mobile if the successor is evaluated eagerly, for otherwise a symbol reference
may occur within a value of this type, invalidating the condition.
Theorem 31.1 (Preservation). If ⊢Σ e : τ and e 7−→
Σ e′, then ⊢Σ e′ : τ.
Proof. By induction on the dynamics of symbol declaration. Rule (31.2a) follows by induction,
applying rule (31.1). Rule (31.2b) follows from the condition on mobility.
Theorem 31.2 (Progress). If ⊢Σ e : τ, then either e 7−→
Σ e′, or e valΣ.
Proof. There is only one rule to consider, Rule (31.1). By induction we have either e 7−−−→
Σ,a~ρ e′, in
which case rule (31.2a) applies, or e valΣ,a~ρ, in which case by the mobility condition we have
e valΣ, and hence rule (31.2b) applies.
31.1.2
Scope-Free Dynamics
The scope-free dynamics of symbols is deﬁned by a transition system between states of the form
ν Σ { e }, where Σ is a signature and e is an expression over this signature. The judgment ν Σ { e } 7−→
ν Σ′ { e′ } states that evaluation of e relative to symbols Σ results in the expression e′ in the exten-
sion Σ′ of Σ.
ν Σ { new{ρ}(a.e) } 7−→ν Σ, a ~ ρ { e }
(31.3)
Rule (31.3) speciﬁes that symbol generation enriches the signature with the newly introduced sym-
bol by extending the signature for all future transitions.
All other rules of the dynamics are changed to account for the allocated symbols. For example,
the dynamics of function application cannot be inherited from Chapter 19, but is reformulated as
follows:
ν Σ { e1 } 7−→ν Σ′ { e′
1 }
ν Σ { e1(e2) } 7−→ν Σ′ { e′
1(e2) }
(31.4a)
ν Σ { λ (x : τ) e(e2) } 7−→ν Σ { [e2/x]e }
(31.4b)
These rules shufﬂe around the signature to account for symbol declarations within the constituent
expressions of the application. Similar rules are required for all other constructs of SPCF.

284
31.2 Symbol References
Theorem 31.3 (Preservation). If ν Σ { e } 7−→ν Σ′ { e′ } and ⊢Σ e : τ, then Σ′ ⊇Σ and ⊢Σ′ e′ : τ.
Proof. There is only one rule to consider, rule (31.3), which is handled by inversion of rule (31.1).
Theorem 31.4 (Progress). If ⊢Σ e : τ, then either e valΣ or ν Σ { e } 7−→ν Σ′ { e′ } for some Σ′ and e′.
Proof. Immediate, by rule (31.3).
31.2
Symbol References
Symbols are not themselves values, but they may be used to form values. One useful example
is provided by the type τ sym of symbol references. A value of this type has the form ’a, where a
is a symbol in the signature. To compute with a reference we may branch according to whether
it is a reference to a speciﬁed symbol. The syntax of symbol references is given by the following
grammar:
Typ
τ
::=
sym(τ)
τ sym
symbols
Exp
e
quote[a]
’a
reference
is[a]{t.τ}(e; e1; e2)
if e is a then e1 ow e2
comparison
The expression quote[a] is a reference to the symbol a, a value of type sym(τ). The expression
is[a]{t.τ}(e; e1; e2) compares the value of e, which is a reference to some symbol b, with the given
symbol a. If b is a, the expression evaluates to e1, and otherwise to e2.
31.2.1
Statics
The typing rules for symbol references are as follows:
Γ ⊢Σ,a~ρ quote[a] : sym(ρ)
(31.5a)
Γ ⊢Σ,a~ρ e : sym(ρ′)
Γ ⊢Σ,a~ρ e1 : [ρ/t]τ
Γ ⊢Σ,a~ρ e2 : [ρ′/t]τ
Γ ⊢Σ,a~ρ is[a]{t.τ}(e; e1; e2) : [ρ′/t]τ
(31.5b)
Rule (31.5a) is the introduction rule for the type sym(ρ). It states that if a is a symbol with associated
type ρ, then quote[a] is an expression of type sym(ρ). Rule (31.5b) is the elimination rule for the
type sym(ρ). The type associated to the given symbol a need not be the same as the type of the
symbol referred to by the expression e. If e evaluates to a reference to a, then these types will
coincide, but if it refers to another symbol, b ̸= a, then these types may well differ.
With this in mind, consider rule (31.5b). A priori there is a discrepancy between the type ρ
of a and the type ρ′ of the symbol referred to by e. This discrepancy is mediated by the type
operator t.τ.1 Regardless of the outcome of the comparison, the overall type of the expression is
1See Chapter 14 for a discussion of type operators.

31.2 Symbol References
285
[ρ′/t]τ. If e evaluates to the symbol a, then we “learn” that the types ρ′ and ρ coincide, because the
speciﬁed and referenced symbol coincide. This coincidence is reﬂected by the type [ρ/t]τ for e1. If
e evaluates to some other symbol, a′ ̸= a, then the comparison evaluates to e2, which is required to
have type [ρ′/t]τ; no further information about the type of the symbol is acquired in this branch.
31.2.2
Dynamics
The (scoped) dynamics of symbol references is given by the following rules:
quote[a] valΣ,a~ρ
(31.6a)
is[a]{t.τ}(quote[a]; e1; e2) 7−−−→
Σ,a~ρ e1
(31.6b)
(a ̸= a′)
is[a]{t.τ}(quote[a′]; e1; e2) 7−−−−−−→
Σ,a~ρ,a′~ρ′ e2
(31.6c)
e 7−−−→
Σ,a~ρ e′
is[a]{t.τ}(e; e1; e2) 7−−−→
Σ,a~ρ is[a]{t.τ}(e′; e1; e2)
(31.6d)
Rules (31.6b) and (31.6c) specify that is[a]{t.τ}(e; e1; e2) branches according to whether the value
of e is a reference to the symbol a.
31.2.3
Safety
To ensure that the mobility condition is satisﬁed, it is important that symbol reference types not be
deemed mobile.
Theorem 31.5 (Preservation). If ⊢Σ e : τ and e 7−→
Σ e′, then ⊢Σ e′ : τ.
Proof. By rule induction on rules (31.6). The most interesting case is rule (31.6b). When the com-
parison is positive, the types ρ and ρ′ must be the same, because each symbol has at most one
associated type. Therefore, e1, which has type [ρ′/t]τ, also has type [ρ/t]τ, as required.
Lemma 31.6 (Canonical Forms). If ⊢Σ e : sym(ρ) and e valΣ, then e = quote[a] for some a such that
Σ = Σ′, a ~ ρ.
Proof. By rule induction on rules (31.5), taking account of the deﬁnition of values.
Theorem 31.7 (Progress). Suppose that ⊢Σ e : τ. Then either e valΣ, or there exists e′ such that e 7−→
Σ e′.

286
31.3 Notes
Proof. By rule induction on rules (31.5). For example, consider rule (31.5b), in which we have that
is[a]{t.τ}(e; e1; e2) has some type τ and that e : sym(ρ) for some ρ. By induction either rule (31.6d)
applies, or else we have that e valΣ, in which case we are assured by Lemma 31.6 that e is quote[a]
for some symbol b of type ρ declared in Σ. But then progress is assured by rules (31.6b) and (31.6c),
because equality of symbols is decidable (either a is b or it is not).
31.3
Notes
The concept of a symbol in a programming language was considered by McCarthy in the original
formulation of Lisp (McCarthy, 1965). Unfortunately, symbols were not clearly distinguished from
variables, leading to unexpected behaviors (see Chapter 32). The present account of symbols was
inﬂuenced by Pitts and Stark (1993) on the declaration of names in the π-calculus (Milner, 1999).
The associated type of a symbol may be used for applications that associate information with the
symbol, such as its ﬂuid binding (see Chapter 32) or its string representation (its “print name” in
Lisp jargon).
Exercises
31.1. The elimination form for symbol references given in Section 31.2 is “one-sided” in the sense
that one may compare a reference to an unknown symbol to a known symbol with a known
type. An alternative elimination form provides an equality test on symbol references. Formu-
late such a variation.
31.2. A list of type (τ sym × τ) list is called an association list. Using your solution to Exercise 31.1
deﬁne a function find that sends an association list to a mapping of type τ sym ⇀τ opt.
31.3. It would be more efﬁcient to represent an association list by a balanced tree associating values
to symbols, but to do so would require a total ordering on symbols (at least among the
symbols with the same associated type). What obstacles arise when introducing a linear
ordering on symbols?
31.4. In Lisp a symbolic expression, or s-expression, or sexpr, may be thought of as a value of the
recursive type
sexpr ≜rec s is [sym ,→sym(s) ; nil ,→unit ; cons ,→s × s].
It is customary to write cons(e0; e1) for fold(cons · ⟨e0, e1⟩), where e0 : sexpr and e1 : sexpr,
and to write nil for fold(nil · ⟨⟩). The list notation (e0, . . . , en−1) is then used as shorthand
for the s-expression
cons(e0; . . . cons(en−1; nil) . . .).
Because lists involving symbols arise often, it is customary to extend the quotation notation
from symbols to general s-expressions so that one need not quote each symbol contained
within it. Give the deﬁnition of this extension, and work out its meaning for the special case
of lists described above.

31.3 Notes
287
31.5. Considering symbol allocation to be an effect, give a modal formulation of SPCF along the
lines of MPCF described in Chapter 29. Consider both a scoped and a scope-free extent for
symbols.

288
31.3 Notes

Chapter 32
Fluid Binding
In this chapter we return to the concept of dynamic scoping of variables that was criticized in
Chapter 8. There it was observed that dynamic scoping is problematic for at least two reasons.
One is that renaming of bound variables is not respected; another is that dynamic scope is not
type safe. These violations of the expected behavior of variables is intolerable, because they are at
variance with mathematical practice and because they compromise modularity.
It is possible, however, to recover a type-safe analog of dynamic scoping by divorcing it from
the concept of a variable, and instead introducing a new mechanism, called ﬂuid binding. Fluid
binding associates to a symbol (and not a variable) a value of a speciﬁed type within a speciﬁed
scope. The identiﬁcation principle for bound variables is retained, type safety is not compromised,
yet some of the beneﬁts of dynamic scoping are preserved.
32.1
Statics
To account for ﬂuid binding we enrich SPCF deﬁned in Chapter 31 with these constructs to obtain
FSPCF:
Exp
e
::=
put[a](e1; e2)
put e1 for a in e2
binding
get[a]
get a
retrieval
The expression get[a] evaluates to the value of the current binding of a, if it has one, and is stuck
otherwise. The expression put[a](e1; e2) binds the symbol a to the value e1 for the duration of the
evaluation of e2, at which point the binding of a reverts to what it was prior to the execution. The
symbol a is not bound by the put expression, but is instead a parameter of it.
The statics of FSPCF is deﬁned by judgments of the form
Γ ⊢Σ e : τ,
much as in Chapter 31, except that here the signature associates a type to each symbol, instead of
just declaring the symbol to be in scope. Thus Σ is here deﬁned to be a ﬁnite set of declarations of
the form a ~ τ such that no symbol is declared more than once in the same signature. Note that the

290
32.2 Dynamics
association of a type to a symbol is not a typing assumption. In particular the signature Σ enjoys
no structural properties, and cannot be considered as a form of hypothesis as deﬁned in Chapter 3.
The following rules govern the new expression forms:
Γ ⊢Σ,a~τ get[a] : τ
(32.1a)
Γ ⊢Σ,a~τ1 e1 : τ1
Γ ⊢Σ,a~τ1 e2 : τ2
Γ ⊢Σ,a~τ1 put[a](e1; e2) : τ2
(32.1b)
Rule (32.1b) speciﬁes that the symbol a is a parameter of the expression that must be declared in
Σ.
32.2
Dynamics
The dynamics of FSPCF relies on a stack-like allocation of symbols in SPCF, and maintains an
association of values to symbols that tracks this stack-like allocation discipline. To do so we deﬁne
a family of transition judgments of the form e
µ7−→
Σ e′, where Σ is as in the statics, and µ is a ﬁnite
function mapping some subset of the symbols declared in Σ to values of the right type. If µ is
deﬁned for some symbol a, then it has the form µ′ ⊗a ,→e for some µ′ and value e. If µ is undeﬁned
for some symbol a, we may regard it as having the form µ′ ⊗a ,→•. We will write a ,→
to stand
for either a ,→• or a ,→e for some expression e.
The dynamics of FSPCF is deﬁned by the following rules:
get[a]
µ⊗a,→e
7−−−−→
Σ,a~τ
e
(32.2a)
e1
µ
7−−−→
Σ,a~τ e′
1
put[a](e1; e2)
µ
7−−−→
Σ,a~τ put[a](e′
1; e2)
(32.2b)
e1 valΣ,a~τ
e2
µ⊗a,→e1
7−−−−−→
Σ,a~τ
e′
2
put[a](e1; e2)
µ⊗a,→
7−−−−→
Σ,a~τ
put[a](e1; e′
2)
(32.2c)
e1 valΣ,a~τ
e2 valΣ,a~τ
put[a](e1; e2)
µ
7−−−→
Σ,a~τ e2
(32.2d)
Rule (32.2a) speciﬁes that get[a] evaluates to the current binding of a, if any. Rule (32.2b) speciﬁes
that the binding for the symbol a is evaluated before the binding is created. Rule (32.2c) evaluates
e2 in an environment where the symbol a is bound to the value e1, regardless of whether or not a is

32.3 Type Safety
291
already bound in the environment. Rule (32.2d) eliminates the ﬂuid binding for a once evaluation
of the extent of the binding has completed.
According to the dynamics of FSPCF given by rules (32.2), there is no transition of the form
get[a]
µ7−→
Σ
e if µ(a) = •. The judgment e unboundΣ states that execution of e will lead to such a
“stuck” state, and is inductively deﬁned by the following rules:
µ(a) = •
get[a] unboundµ
(32.3a)
e1 unboundµ
put[a](e1; e2) unboundµ
(32.3b)
e1 valΣ
e2 unboundµ
put[a](e1; e2) unboundµ
(32.3c)
In a larger language it would also be necessary to include error propagation rules of the sort
discussed in Chapter 6.
32.3
Type Safety
We ﬁrst deﬁne the auxiliary judgment µ : Σ by the following rules:
∅: ∅
(32.4a)
⊢Σ e : τ
µ : Σ
µ ⊗a ,→e : Σ, a ~ τ
(32.4b)
µ : Σ
µ ⊗a ,→• : Σ, a ~ τ
(32.4c)
These rules specify that if a symbol is bound to a value, then that value must be of the type associ-
ated to the symbol by Σ. No demand is made in the case that the symbol is unbound (equivalently,
bound to a “black hole”).
Theorem 32.1 (Preservation). If e
µ7−→
Σ e′, where µ : Σ and ⊢Σ e : τ, then ⊢Σ e′ : τ.
Proof. By rule induction on rules (32.2). Rule (32.2a) is handled by the deﬁnition of µ : Σ. Rule (32.2b)
follows by induction. Rule (32.2d) is handled by inversion of rules (32.1). Finally, rule (32.2c) is
handled by inversion of rules (32.1) and induction.
Theorem 32.2 (Progress). If ⊢Σ e : τ and µ : Σ, then either e valΣ, or e unboundµ, or there exists e′ such
that e
µ7−→
Σ e′.

292
32.4 Some Subtleties
Proof. By induction on rules (32.1). For rule (32.1a), we have Σ ⊢a ~ τ from the premise of the rule,
and hence, because µ : Σ, we have either µ(a) = • or µ(a) = e for some e such that ⊢Σ e : τ. In the
former case we have e unboundµ, and in the latter we have get[a]
µ7−→
Σ e. For rule (32.1b), we have
by induction that either e1 valΣ or e1 unboundµ, or e1
µ7−→
Σ e′
1. In the latter two cases we may apply
rule (32.2b) or rule (32.3b), respectively. If e1 valΣ, we apply induction to obtain that either e2 valΣ,
in which case rule (32.2d) applies; e2 unboundµ, in which case rule (32.3c) applies; or e2
µ7−→
Σ e′
2, in
which case rule (32.2c) applies.
32.4
Some Subtleties
The value of put e1 for a in e2 is the value of e2, calculated in a context where a is bound to the
value of e1. If e2 is of a basic type, such as nat, then the reversion of the binding of a cannot
inﬂuence the meaning of the result.1
But what if the type of put e1 for a in e2 is a function type, so that the returned value is a λ-
abstraction? The body of the returned λ may refer to the binding of a, which is reverted upon
return from the put. For example, consider the expression
put 17 for a in λ (x : nat) x + get a,
(32.5)
which has type nat ⇀nat, given that a is a symbol of type nat. Let us assume, for the sake of
discussion, that a is unbound at the point at which this expression is evaluated. Evaluating the put
binds a to the number 17, and returns the function λ (x : nat) x + get a. But because a is reverted
to its unbound state upon exiting the put, applying this function to an argument will result in an
error, unless a binding for a is given. Thus, if f is bound to the result of evaluating (32.5), then the
expression
put 21 for a in f (7)
(32.6)
will evaluate to 28, whereas evaluation of f (7) in the absence of a surrounding binding for a will
incur an error.
Contrast this with the similar expression
let y be 17 in λ (x : nat) x + y,
(32.7)
where we have replaced the ﬂuid-bound symbol a by a statically bound variable y. This expression
evaluates to λ (x : nat) x + 17, which adds 17 to its argument when applied. There is no possibility
of an unbound symbol arising at execution time, because variables are interpreted by substitution.
One way to think about this situation is to consider that ﬂuid-bound symbols serve as an al-
ternative to passing extra arguments to a function to specialize its value when it is called. To see
this, let e stand for the value of expression (32.5), a λ-abstraction whose body is dependent on the
1As long as the successor is evaluated eagerly; if not, the following examples are adaptable to situations where the value
of e2 is a lazily evaluated number.

32.5 Fluid References
293
binding of the symbol a. To use this function safely, it is necessary that the programmer provide a
binding for a prior to calling it. For example, the expression
put 7 for a in (e(9))
evaluates to 16, and the expression
put 8 for a in (e(9))
evaluates to 17. Writing just e(9), without a surrounding binding for a, results in a run-time error
attempting to retrieve the binding of the unbound symbol a.
This behavior can be simulated by adding an argument to the function value that will be bound
to the current binding of the symbol a at the point where the function is called. Instead of using
ﬂuid binding, we would provide an extra argument at each call site, writing
e′(7)(9)
and
e′(8)(9),
respectively, where e′ is the λ-abstraction
λ (y : nat) λ (x : nat) x + y.
Adding arguments can be cumbersome, though, especially when several call sites provide the
same binding for a. Using ﬂuid binding we may write
put 7 for a in ⟨e(8), e(9)⟩,
whereas using an extra argument we must write
⟨e′(7)(8), e′(7)(9)⟩.
However, such redundancy can be reduced by factoring out the common part, writing
let f be e′(7) in ⟨f (8), f (9)⟩.
The awkwardness of this simulation is usually taken as an argument in favor of including ﬂuid
binding in a language. The drawback, which is often perceived as an advantage, is that nothing in
the type of a function reveals its dependency on the binding of a symbol. It is therefore quite easy
to forget that such a binding is required, leading to run-time failures that might better be caught
at compile time.
32.5
Fluid References
The get and put operations for ﬂuid binding are indexed by a symbol that must be given as part
of the syntax of the operator. It is sometimes useful to defer until runtime the choice of ﬂuid on

294
32.5 Fluid References
which a get or put acts. References to ﬂuids allow the name the ﬂuid to be a value. References come
equipped with analogs of the get and put primitives, but for a dynamically determined symbol.
We may extend FSPCF with ﬂuid references by adding the following syntax:
Typ
τ
::=
fluid(τ)
τ fluid
ﬂuid
Exp
e
::=
fl[a]
& a
reference
getfl(e)
getfl e
retrieval
putfl(e; e1; e2)
putfl e is e1 in e2
binding
The expression fl[a] is the symbol a considered as a value of type fluid(τ). The expressions
getfl(e) and putfl(e; e1; e2) are analogs of the get and put operations for ﬂuid-bound symbols.
The statics of these constructs is given by the following rules:
Γ ⊢Σ,a~τ fl[a] : fluid(τ)
(32.8a)
Γ ⊢Σ e : fluid(τ)
Γ ⊢Σ getfl(e) : τ
(32.8b)
Γ ⊢Σ e : fluid(τ)
Γ ⊢Σ e1 : τ
Γ ⊢Σ e2 : τ2
Γ ⊢Σ putfl(e; e1; e2) : τ2
(32.8c)
Because we are using a scoped dynamics, references to ﬂuids cannot be deemed mobile.
The dynamics of references consists of resolving the referent and deferring to the underlying
primitives acting on symbols.
fl[a] valΣ,a~τ
(32.9a)
e
µ7−→
Σ e′
getfl(e)
µ7−→
Σ getfl(e′)
(32.9b)
getfl(fl[a])
µ7−→
Σ get[a]
(32.9c)
e
µ7−→
Σ e′
putfl(e; e1; e2)
µ7−→
Σ putfl(e′; e1; e2)
(32.9d)
putfl(fl[a]; e1; e2)
µ7−→
Σ put[a](e1; e2)
(32.9e)

32.6 Notes
295
32.6
Notes
Dynamic binding arose in early dialects of Lisp from not distinguishing variables from symbols.
When separated, variables retain their substitutive meaning, and symbols give rise to a separate
concept of ﬂuid binding. Allen (1978) discusses the implementation of ﬂuid binding. The present
formulation here draws on Nanevski (2003).
Exercises
32.1. Deep binding is an implementation of ﬂuid binding where the value associated to a symbol
is stored on the control stack as part of a put frame, and is retrieved by ﬁnding the most
recent such association. Deﬁne a stack machine for FSPCF that implements deep binding by
extending the FPCF machine. Be sure to consider new as well as put and get. Attempting to
get the binding for an unbound symbol signals a failure; otherwise, its most recent binding
is returned. Where do the issues discussed in Section 32.4 arise? Hint: you will need to
introduce an auxiliary judgment k ≥k′ ? a, which searches for the binding of the symbol a
on the stack k′, returning its value (or failure) to the stack k.
32.2. Shallow binding is an implementation of ﬂuid binding that maintains a mapping sending
each active symbol to a stack of values, the topmost being the active binding for that symbol.
Deﬁne a stack machine for FSPCF that maintains such a mapping to facilitate access to the
binding of a symbol. Hint: use evaluation states of the form k ∥µ ▷e, where µ is a mapping
each symbol a allocated on k to a stack of values, the topmost element of which, if any, is the
current binding of a. Use similar forms of return and fail states, and ensure that the mapping
invariant is maintained.
32.3. Exception handlers can be implemented by combining ﬂuid binding with continuations
(Chapter 30). Reserve a single ﬂuid-bound symbol hdlr that is always bound to the ac-
tive exception handler, which is represented by a continuation accepting a value of type τexn.
Raising an exception consists of throwing an exception value to this continuation. When en-
tering the scope of a handler, a continuation representing the “otherwise” clause is put as the
binding of hdlr. Give a precise formulation of exception handling based on this summary.
Hint: it is important to ensure that the current handler is maintained for both normal and
exceptional returns.

296
32.6 Notes

Chapter 33
Dynamic Classiﬁcation
In Chapters 11 and 26 we investigated the use of sums for classifying values of disparate type.
Every value of a classiﬁed type is labeled with a symbol that determines the type of the instance
data. A classiﬁed value is decomposed by pattern matching against a known class, which reveals
the type of the instance data. Under this representation the possible classes of an object are deter-
mined statically by its type. However, it is sometimes useful to allow the possible classes of data
value to be determined dynamically.
Dynamic generation of classes has many applications, most of which derive from the guaran-
tee that a newly allocated class is distinct from all others that have been or ever will be generated.
In this regard a dynamic class is a “secret” whose disclosure can be used to limit the ﬂow of infor-
mation in a program. In particular a dynamically classiﬁed value is opaque unless its identity has
been disclosed by its creator. Thus dynamic classiﬁcation can be used to ensure that an exception
reaches only its intended handler, or that a message on a communication channel reaches only the
intended recipient.
33.1
Dynamic Classes
A dynamic class is a symbol is generated at run-time. A classiﬁed value consists of a symbol of
type τ together with a value of that type. To compute with a classiﬁed value, it is compared with
a known class. If the value is of this class, the underlying instance data is passed to the positive
branch, otherwise the negative branch is taken, where it is matched against other known classes.
33.1.1
Statics
The syntax of dynamic classiﬁcation is given by the following grammar:
Typ
τ
::=
clsfd
clsfd
classiﬁed
Exp
e
::=
in[a](e)
a · e
instance
isin[a](e; x.e1; e2)
match e as a · x ,→e1 ow ,→e2
comparison

298
33.1 Dynamic Classes
The expression in[a](e) is a classiﬁed value with class a and underlying value e. The expression
isin[a](e; x.e1; e2) checks whether the class of the value given by e is a. If so, the classiﬁed value is
passed to e1; if not, the expression e2 is evaluated instead.
The statics of dynamic classiﬁcation is deﬁned by these rules:
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ in[a](e) : clsfd
(33.1a)
Γ ⊢Σ,a~τ e : clsfd
Γ, x : τ ⊢Σ,a~τ e1 : τ′
Γ ⊢Σ,a~τ e2 : τ′
Γ ⊢Σ,a~τ isin[a](e; x.e1; e2) : τ′
(33.1b)
The typing judgment is indexed by a signature associating a type to each symbol. Here the type
governs the instance data associated to each symbol.
33.1.2
Dynamics
To maximize the ﬂexibility in using dynamic classiﬁcation, we will consider a free dynamics for
symbol generation. Within this framework the dynamics of classiﬁcation is given by the following
rules:
e valΣ
in[a](e) valΣ
(33.2a)
ν Σ { e } 7−→ν Σ′ { e′ }
ν Σ { in[a](e) } 7−→ν Σ′ { in[a](e′) }
(33.2b)
e valΣ
ν Σ { isin[a](in[a](e); x.e1; e2) } 7−→ν Σ { [e/x]e1 }
(33.2c)
e′ valΣ
(a ̸= a′)
ν Σ { isin[a](in[a′](e′); x.e1; e2) } 7−→ν Σ { e2 }
(33.2d)
ν Σ { e } 7−→ν Σ′ { e′ }
ν Σ { isin[a](e; x.e1; e2) } 7−→ν Σ′ { isin[a](e′; x.e1; e2) }
(33.2e)
Throughout, if the states involved are well-formed, then there will be a declaration a ~ τ for some
type τ in Σ.
The dynamics of the elimination form for the type clsfd relies on disequality of names (specif-
ically, rule (33.2d)). Because disequality is not preserved under substitution, it is not sensible to
consider any language construct whose dynamics relies on such a substitution. To see what goes
wrong, consider the expression
match b · ⟨⟩as a ·
,→true ow ,→match b · ⟨⟩as b ·
,→false ow ,→true.
This expression evaluates to false, because the outer conditional is on the class a, which is a priori
different from b. However, if we substitute b for a in this expression we obtain
match b · ⟨⟩as b ·
,→true ow ,→match b · ⟨⟩as b ·
,→false ow ,→true,
which evaluate to true, because now the outer conditional governs the evaluation.

33.2 Class References
299
33.1.3
Safety
Theorem 33.1 (Safety).
1. If ⊢Σ e : τ and ν Σ { e } 7−→ν Σ′ { e′ }, then Σ′ ⊇Σ and ⊢Σ′ e′ : τ.
2. If ⊢Σ e : τ, then either e valΣ or ν Σ { e } 7−→ν Σ′ { e′ } for some e′ and Σ′.
Proof. Similar to the safety proofs given in Chapters 11 and 31.
33.2
Class References
The type cls(τ) has as values references to classes.
Typ
τ
::=
cls(τ)
τ cls
class reference
Exp
e
::=
cls[a]
& a
reference
mk(e1; e2)
mk(e1; e2)
instance
isof(e0; e1; x.e2; e3)
isof(e0; e1; x.e2; e3)
dispatch
The statics of these constructs is given by the following rules:
Γ ⊢Σ,a~τ cls[a] : cls(τ)
(33.3a)
Γ ⊢Σ e1 : cls(τ)
Γ ⊢Σ e2 : τ
Γ ⊢Σ mk(e1; e2) : clsfd
(33.3b)
Γ ⊢Σ e0 : cls(τ)
Γ ⊢Σ e1 : clsfd
Γ, x : τ ⊢Σ e2 : τ′
Γ ⊢Σ e3 : τ′
Γ ⊢Σ isof(e0; e1; x.e2; e3) : τ′
(33.3c)
The corresponding dynamics is given by these rules:
ν Σ { e1 } 7−→ν Σ′ { e′
1 }
ν Σ { mk(e1; e2) } 7−→ν Σ′ { mk(e′
1; e2) }
(33.4a)
e1 valΣ
ν Σ { e2 } 7−→ν Σ′ { e′
2 }
ν Σ { mk(e1; e2) } 7−→ν Σ′ { mk(e1; e′
2) }
(33.4b)
e valΣ
ν Σ { mk(cls[a]; e) } 7−→ν Σ { in[a](e) }
(33.4c)
ν Σ { e0 } 7−→ν Σ′ { e′
0 }
ν Σ { isof(e0; e1; x.e2; e3) } 7−→ν Σ′ { isof(e′
0; e1; x.e2; e3) }
(33.4d)
ν Σ { isof(cls[a]; e1; x.e2; e3) } 7−→ν Σ { isin[a](e1; x.e2; e3) }
(33.4e)
Rules (33.4d) and (33.4e) specify that the ﬁrst argument is evaluated to determine the target class,
which is then used to check whether the second argument, a classiﬁed data value, is of the target
class. This formulation is a two-stage process in which e0 determines the pattern against which to
match the classiﬁed value of e1.

300
33.3 Deﬁnability of Dynamic Classes
33.3
Deﬁnability of Dynamic Classes
The type clsfd can be deﬁned in terms of symbolic references, product types, and existential types
by the type expression
clsfd ≜∃(t.t sym × t).
The introduction form in[a](e), in which a is a symbol with associated type is τ and e is an expres-
sion of type τ, is deﬁned to be the package
pack τ with ⟨’a, e⟩as ∃(t.t sym × t).
(33.5)
The elimination form isin[a](e; x.e1; e2), of some type τ′, and where the type associated to a is τ,
is deﬁned in terms of symbol comparison (see Chapter 31), together with existential and product
elimination, and function types. By rule (33.1b) the type of e is clsfd, which is now the existential
type (33.5). Similarly, the branches both have the overall type τ′, and within e1 the variable x has
type τ. The elimination form for the type clsfd is deﬁned to be
open e as t with ⟨x, y⟩:t sym × t in (ebody(y)),
where ebody is an expression to be deﬁned shortly. It opens the package e which is an element of
the type (33.5), decomposing it into a type t a symbol reference x of type t sym, and an associated
value y of type t. The expression ebody will turn out to have the type t ⇀τ′ so that the application
to y will be type correct.
The expression ebody compares the symbolic reference x to the symbol a of type τ, yielding a
value of type t ⇀τ′. The expression ebody is
is[a]{u.u ⇀τ′}(x; e′
1; e′
2),
where, as speciﬁed by rule (31.5b), e′
1 has type [τ/u](u ⇀τ′) = τ ⇀τ′, and e′
2 has type [t/u](u ⇀τ′) =
t ⇀τ′. The expression e′
1 “knows” that the abstract type t is τ, the type associated to the symbol
a, because the comparison is positive. On the other hand e′
2 does not “learn” anything about the
type t.
It remains to choose the expressions e′
1 and e′
2. In the case of a positive comparison, we wish to
pass the classiﬁed value to the expression e1 by substitution for the variable x. We therefore deﬁne
e′
1 to be the expression
λ (x : τ) e1 : τ ⇀τ′.
In the case of a negative comparison no value is propagated to e2. We therefore deﬁne e′
2 to be the
expression
λ ( : t) e2 : t ⇀τ′.
We may then check that the statics and dynamics given in Section 33.1 are derivable under these
deﬁnitions.

33.4 Applications of Dynamic Classiﬁcation
301
33.4
Applications of Dynamic Classiﬁcation
Dynamic classiﬁcation has a number of interesting applications in programming. The most ob-
vious is to generalize dynamic dispatch (Chapter 26) to support computation over a dynamically
extensible type of heterogeneous values. Introducing a new class requires introducing a new row
in the dispatch matrix deﬁning the behavior of the methods on the newly deﬁned class. To allow
for this the rows of the matrix must be indexed by class references, rather than classes, so that it is
accessible without knowing statically the class.
Another application is to use dynamic classiﬁcation as a form of “perfect encryption” that
ensures that classiﬁed values can neither be constructed nor deconstructed without knowing the
class in question. Abstract encryption of this form can be used to ensure privacy of communication
among the parties in a computation. One example of such a scenario is in channel-based commu-
nication, as will be considered in Chapter 40. Another, less obvious, application is to ensure that
an exception value may only be received by the intended handler, and no other.
33.4.1
Classifying Secrets
Dynamic classiﬁcation can be used to enforce conﬁdentiality and integrity of data values in a pro-
gram. A value of type clsfd may only be constructed by sealing it with some class a and may only
be deconstructed by a case analysis that includes a branch for a. By controlling which parties in
a multi-party interaction have access to the classiﬁer a we may control how classiﬁed values are
created (ensuring their integrity) and how they are inspected (ensuring their conﬁdentiality). Any
party that lacks access to a cannot decipher a value classiﬁed by a, nor may it create a classiﬁed
value with this class. Because classes are dynamically generated symbols, they offer an absolute
conﬁdentiality guarantee among parties in a computation.1
Consider the following simple protocol for controlling the integrity and conﬁdentiality of data
in a program. A fresh symbol a is introduced, and we return a pair of functions of type
(τ ⇀clsfd) × (clsfd ⇀τ opt),
called the constructor and destructor functions for that class, which is accomplished by writing
new a ∼τ in
⟨λ (x : τ) a · x,
λ (x : clsfd) match x as a · y ,→just(y) ow ,→null ⟩.
The ﬁrst function creates a value classiﬁed by a, and the second function recovers the instance data
of a value classiﬁed by a. Outside of the scope of the declaration the symbol a is an unguessable
secret.
To enforce the integrity of a value of type τ it sufﬁces to ensure that only trusted parties have
access to the constructor. To enforce the conﬁdentiality of a value of type τ it sufﬁces to ensure that
only trusted parties have access to the destructor. Ensuring the integrity of a value amounts to
associating an invariant to it that is maintained by the trusted parties that may create an instance
1Of course, this guarantee is for programs written in conformance with the statics given here. If the abstraction imposed
by the type system is violated, no guarantees of conﬁdentiality can be made.

302
33.5 Notes
of that class. Ensuring the conﬁdentiality of a value amounts to propagating the invariant to
parties that may decipher it.
33.4.2
Exception Values
Exception handling is a communication between two agents, one that may raise an exception, and
one that may handle it. We wish to ensure that an exception can be caught only by a designated
handler, without fear that any intervening handler may intercept it. This secrecy property can
be ensured by using dynamic class allocation. A new class is declared, with the ability to create
an instance given only to the raising agent and the ability to match an instance given only to the
handler. The exception value cannot be intercepted by any other handler, because no other handler
is capable of matching it. This property is crucial to “black box” composition of programs from
components. Without dynamic classiﬁcation one can never be sure that alien code cannot intercept
an exception intended for a handler within one’s own code, or vice versa.
With this in mind, let us now reconsider the choice of the type τexn of exception values speciﬁed
in Chapter 29. There we distinguished the closed-world assumption, which amounts to deﬁning τexn
to be a ﬁnite sum type known to the whole program, from the open-world assumption, which is re-
alized by deﬁning τexn to be the type clsfd of dynamically classiﬁed values. This choice supports
modularity and evolution by allowing fresh exceptions (classes) to be allocated at will, avoiding
the need for an up-front agreement on the forms of exception. Another perspective is that dynamic
classiﬁcation treats an exception as a shared secret between the handler and the raiser of that ex-
ception. When an exception value is raised, it can only be intercepted and analyzed by a handler
that can match the value against the speciﬁed class. It is only by using dynamic classiﬁcation that
one can gain control over the ﬂow of information in a program that uses exceptions. Without it, an
unintended handler can intercept an exception that was not intended for it, disrupting the logic of
the program.
33.5
Notes
Dynamic classiﬁcation appears in Standard ML (Milner et al., 1997) as the type exn. The usefulness
of the type exn is obscured by its too-close association with the exception mechanism. The π-
calculus (Milner, 1999) popularized using “name generation” and “channel passing” to control
the connectivity and information ﬂow in a process network. In Chapter 40 we shall make explicit
that this aspect of the π-calculus is an application of dynamic classiﬁcation.
Exercises
33.1. Consider the following open-world named exception mechanism, which is typical of the excep-
tion mechanism found in many languages.

33.5 Notes
303
exception a of τ in e
declare an exception a of type τ in e
raise a with e
raise exception a with value e
try e ow a1(x1) ,→e1 | . . . | an(xn) ,→en | x ,→e′
handle exceptions a1, . . . , an
Exceptions are declared by name, specifying the type of their associated values. Each ex-
ecution of an exception declaration generates a fresh exception. An exception is raised by
specifying the exception name and a value to associate with it. The handler intercepts any ﬁ-
nite number of named exceptions, passing their associated values to handlers, and otherwise
propagates the exception to the default handler.
The following rules deﬁne the statics of these constructs:
Γ ⊢Σ,a~τ e : τ′
Γ ⊢Σ exception τ of a in e : τ′
(33.6a)
Σ ⊢a ~ τ
Γ ⊢Σ e : τ
Γ ⊢Σ raise a with e : τ′
(33.6b)
Σ ⊢a1 ~ τ1
. . .
Σ ⊢an ~ τn
Γ ⊢Σ e : τ′
Γ, x1 : τ1 ⊢Σ e1 : τ′
. . .
Γ, xn : τn ⊢en : τ′
Γ, x : τexn ⊢e′ : τ′
Γ ⊢Σ try e ow a1(x1) ,→e1 | . . . | an(xn) ,→en | x ,→e′ : τ′
(33.6c)
Give an implementation of named exceptions in terms of dynamic classiﬁcation and general
value-passing exceptions (Chapter 29).
33.2. Show that dynamic classiﬁcation with dynamic classes can be implemented in the combi-
nation of FPC and FEω enriched with references to free assignables, and with no modal
separation (so as to permit benign effects). Speciﬁcally, provide a package of the following
higher-kind existential type:
τ ≜∃clsfd :: T.∃class :: T →T.⟨new ,→τnew, mk ,→τmk, isof ,→τisof⟩.
where
τnew ≜∀(t.cls[t])
τmk ≜∀(t.(cls[t] × t) →clsfd)
τisof ≜∀(t.∀(u.(cls[t] × clsfd × (t →u) × u) →u)).
These operations correspond to the mechanisms of dynamic classiﬁcation described earlier
in this chapter. Hint: Deﬁne cls[t] to be t opt ref, and deﬁne clsfd so that a classiﬁed value
is represented by an encapsulated assignment to its class. Creating a new class allocates a
reference, creating a classiﬁed value creates an encapsulated assignment, and testing for a
class is implemented by assigning to the target class, then running the classiﬁed value, and
seeing whether the contents of the target class has changed.
33.3. Open-world named exceptions obstruct exception tracking (as described in Exercise 29.4).

304
33.5 Notes
(a) Show that it is not computable to track the exact set of exception names that might be
raised by an expression.
(b) Show that it is impossible to ﬁnitely bound set of exceptions that can be thrown by an
expression. Hint: Show that there are expressions for which any such upper bound is
inaccurate.
33.4. Exercise 33.3 may seem disappointing, until you realize that whereas positive exception track-
ing is impossible under the open-world assumption, negative exception tracking is not only
possible, but more desirable. It is often more useful to know that a speciﬁed exception cannot
be raised than it is to know that it can. Negative exception tracking can be expressed using
exclusion reﬁnements of the form X, where X is a ﬁnite set of dynamic classes. Informally, a
value satisﬁes such a reﬁnement only if its class is not among those in the set X. Deﬁne a
system of exclusion reﬁnements by deﬁning entailment and satisfaction for them. Be sure to
state reﬁnement rules for class allocation and for the introduction and elimination forms for
the type clsfd.

Part XIV
Mutable State


Chapter 34
Modernized Algol
Modernized Algol, or MA, is an imperative, block-structured programming language based on
the classic language Algol. MA extends PCF with a new syntactic sort of commands that act on
assignables by retrieving and altering their contents. Assignables are introduced by declaring them
for use within a speciﬁed scope; this is the essence of block structure. Commands are combined
by sequencing, and are iterated using recursion.
MA maintains a careful separation between pure expressions, whose meaning does not de-
pend on any assignables, and impure commands, whose meaning is given in terms of assignables.
The segregation of pure from impure ensures that the evaluation order for expressions is not con-
strained by the presence of assignables in the language, so that they can be manipulated just as in
PCF. Commands, on the other hand, have a constrained execution order, because the execution of
one may affect the meaning of another.
A distinctive feature of MA is that it adheres to the stack discipline, which means that assignables
are allocated on entry to the scope of their declaration, and deallocated on exit, using a conven-
tional stack discipline. Stack allocation avoids the need for more complex forms of storage man-
agement, at the cost of reducing the expressive power of the language.
34.1
Basic Commands
The syntax of the language MA of modernized Algol distinguishes pure expressions from impure
commands. The expressions include those of PCF (as described in Chapter 19), augmented with
one construct, and the commands are those of a simple imperative programming language based
on assignment. The language maintains a sharp distinction between variables and assignables. Vari-
ables are introduced by λ-abstraction and are given meaning by substitution. Assignables are
introduced by a declaration and are given meaning by assignment and retrieval of their contents,
which is, for the time being, restricted to natural numbers. Expressions evaluate to values, and
have no effect on assignables. Commands are executed for their effect on assignables, and return
a value. Composition of commands not only sequences their execution order, but also passes the
value returned by the ﬁrst to the second before it is executed. The returned value of a command

308
34.1 Basic Commands
is, for the time being, restricted to the natural numbers. (But see Section 34.3 for the general case.)
The syntax of MA is given by the following grammar, from which we have omitted repetition
of the expression syntax of PCF for the sake of brevity.
Typ
τ
::=
cmd
cmd
command
Exp
e
::=
cmd(m)
cmd m
encapsulation
Cmd
m
::=
ret(e)
ret e
return
bnd(e; x.m)
bnd x ←e ; m
sequence
dcl(e; a.m)
dcl a := e in m
new assignable
get[a]
@ a
fetch
set[a](e)
a := e
assign
The expression cmd(m) consists of the unevaluated command m thought of as a value of type
cmd. The command ret(e) returns the value of the expression e without having any effect on
the assignables. The command bnd(e; x.m) evaluates e to an encapsulated command, then this
command is executed for its effects on assignables, with its value substituted for x in m. The
command dcl(e; a.m) introduces a new assignable, a, for use within the command m whose initial
contents is given by the expression e. The command get[a] returns the current contents of the
assignable a and the command set[a](e) changes the contents of the assignable a to the value of e,
and returns that value.
34.1.1
Statics
The statics of MA consists of two forms of judgment:
1. Expression typing: Γ ⊢Σ e : τ.
2. Command formation: Γ ⊢Σ m ok.
The context Γ speciﬁes the types of variables, as usual, and the signature Σ consists of a ﬁnite set
of assignables. As with other uses of symbols, the signature cannot be interpreted as a form of
typing hypothesis (it enjoys no structural properties of entailment), but must be considered as an
index of a family of judgments, one for each choice of Σ.
The statics of MA is inductively deﬁned by the following rules:
Γ ⊢Σ m ok
Γ ⊢Σ cmd(m) : cmd
(34.1a)
Γ ⊢Σ e : nat
Γ ⊢Σ ret(e) ok
(34.1b)
Γ ⊢Σ e : cmd
Γ, x : nat ⊢Σ m ok
Γ ⊢Σ bnd(e; x.m) ok
(34.1c)
Γ ⊢Σ e : nat
Γ ⊢Σ,a m ok
Γ ⊢Σ dcl(e; a.m) ok
(34.1d)

34.1 Basic Commands
309
Γ ⊢Σ,a get[a] ok
(34.1e)
Γ ⊢Σ,a e : nat
Γ ⊢Σ,a set[a](e) ok
(34.1f)
Rule (34.1a) is the introduction rule for the type cmd, and rule (34.1c) is the corresponding elimi-
nation form. Rule (34.1d) introduces a new assignable for use within a speciﬁed command. The
name a of the assignable is bound by the declaration, and so may be renamed to satisfy the im-
plicit constraint that it not already occur in Σ. Rule (34.1e) states that the command to retrieve
the contents of an assignable a returns a natural number. Rule (34.1f) states that we may assign a
natural number to an assignable.
34.1.2
Dynamics
The dynamics of MA is deﬁned in terms of a memory µ a ﬁnite function assigning a numeral to
each of a ﬁnite set of assignables.
The dynamics of expressions consists of these two judgment forms:
1. e valΣ, stating that e is a value relative to Σ.
2. e 7−→
Σ e′, stating that the expression e steps to the expression e′.
These judgments are inductively deﬁned by the following rules, together with the rules deﬁning
the dynamics of PCF (see Chapter 19). It is important, however, that the successor operation be
given an eager, instead of lazy, dynamics so that a closed value of type nat is a numeral (for reasons
that will be explained in Section 34.3).
cmd(m) valΣ
(34.2a)
Rule (34.2a) states that an encapsulated command is a value.
The dynamics of commands is deﬁned in terms of states m ∥µ, where µ is a memory mapping
assignables to values, and m is a command. There are two judgments governing such states:
1. m ∥µ ﬁnalΣ. The state m ∥µ is complete.
2. m ∥µ 7−→
Σ m′ ∥µ′. The state m ∥µ steps to the state m′ ∥µ′; the set of active assignables is
given by the signature Σ.
These judgments are inductively deﬁned by the following rules:
e valΣ
ret(e) ∥µ ﬁnalΣ
(34.3a)
e 7−→
Σ e′
ret(e) ∥µ 7−→
Σ ret(e′) ∥µ
(34.3b)

310
34.1 Basic Commands
e 7−→
Σ e′
bnd(e; x.m) ∥µ 7−→
Σ bnd(e′; x.m) ∥µ
(34.3c)
e valΣ
bnd(cmd(ret(e)); x.m) ∥µ 7−→
Σ [e/x]m ∥µ
(34.3d)
m1 ∥µ 7−→
Σ m′
1 ∥µ′
bnd(cmd(m1); x.m2) ∥µ 7−→
Σ bnd(cmd(m′
1); x.m2) ∥µ′
(34.3e)
get[a] ∥µ ⊗a ,→e 7−→
Σ,a ret(e) ∥µ ⊗a ,→e
(34.3f)
e 7−→
Σ,a e′
set[a](e) ∥µ 7−→
Σ,a set[a](e′) ∥µ
(34.3g)
e valΣ,a
set[a](e) ∥µ ⊗a ,→
7−→
Σ,a ret(e) ∥µ ⊗a ,→e
(34.3h)
e 7−→
Σ e′
dcl(e; a.m) ∥µ 7−→
Σ dcl(e′; a.m) ∥µ
(34.3i)
e valΣ
m ∥µ ⊗a ,→e 7−→
Σ,a m′ ∥µ′ ⊗a ,→e′
dcl(e; a.m) ∥µ 7−→
Σ dcl(e′; a.m′) ∥µ′
(34.3j)
e valΣ
e′ valΣ,a
dcl(e; a.ret(e′)) ∥µ 7−→
Σ ret(e′) ∥µ
(34.3k)
Rule (34.3a) speciﬁes that a ret command is ﬁnal if its argument is a value. Rules (34.3c) to (34.3e)
specify the dynamics of sequential composition. The expression e must, by virtue of the type
system, evaluate to an encapsulated command, which is executed to ﬁnd its return value, which
is then substituted into the command m before executing it.
Rules (34.3i) to (34.3k) deﬁne the concept of block structure in a programming language. Dec-
larations adhere to the stack discipline in that an assignable is allocated during evaluation of the
body of the declaration, and deallocated after evaluation of the body is complete. Therefore the
lifetime of an assignable can be identiﬁed with its scope, and hence we may visualize the dynamic
lifetimes of assignables as being nested inside one another, in the same way as their static scopes
are nested inside one another. The stack-like behavior of assignables is a characteristic feature of
what are known as Algol-like languages.

34.1 Basic Commands
311
34.1.3
Safety
The judgment m ∥µ okΣ is deﬁned by the rule
⊢Σ m ok
µ : Σ
m ∥µ okΣ
(34.4)
where the auxiliary judgment µ : Σ is deﬁned by the rule
∀a ∈Σ
∃e
µ(a) = e and e val∅and ⊢∅e : nat
µ : Σ
(34.5)
That is, the memory must bind a number to each assignable in Σ.
Theorem 34.1 (Preservation).
1. If e 7−→
Σ e′ and ⊢Σ e : τ, then ⊢Σ e′ : τ.
2. If m ∥µ 7−→
Σ m′ ∥µ′, with ⊢Σ m ok and µ : Σ, then ⊢Σ m′ ok and µ′ : Σ.
Proof. Simultaneously, by induction on rules (34.2) and (34.3).
Consider rule (34.3j). Assume that ⊢Σ dcl(e; a.m) ok and µ : Σ. By inversion of typing we have
⊢Σ e : nat and ⊢Σ,a m ok. Because e valΣ and µ : Σ, we have µ ⊗a ,→e : Σ, a. By induction we have
⊢Σ,a m′ ok and µ′ ⊗a ,→e′ : Σ, a, from which the result follows immediately.
Consider rule (34.3k). Assume that ⊢Σ dcl(e; a.ret(e′)) ok and µ : Σ. By inversion we have
⊢Σ e : nat, and ⊢Σ,a ret(e′) ok, and so ⊢Σ,a e′ : nat. But because e′ valΣ,a, and e′ is a numeral, and
we also have ⊢Σ e′ : nat, as required.
Theorem 34.2 (Progress).
1. If ⊢Σ e : τ, then either e valΣ, or there exists e′ such that e 7−→
Σ e′.
2. If ⊢Σ m ok and µ : Σ, then either m ∥µ ﬁnalΣ or m ∥µ 7−→
Σ m′ ∥µ′ for some µ′ and m′.
Proof. Simultaneously, by induction on rules (34.1). Consider rule (34.1d). By the ﬁrst inductive
hypothesis we have either e 7−→
Σ e′ or e valΣ. In the former case rule (34.3i) applies. In the latter, we
have by the second inductive hypothesis,
m ∥µ ⊗a ,→e ﬁnalΣ,a
or
m ∥µ ⊗a ,→e 7−→
Σ,a m′ ∥µ′ ⊗a ,→e′.
In the former case we apply rule (34.3k), and in the latter, rule (34.3j).

312
34.2 Some Programming Idioms
34.2
Some Programming Idioms
The language MA is designed to expose the elegant interplay between the execution of an expres-
sion for its value and the execution of a command for its effect on assignables. In this section we
show how to derive several standard idioms of imperative programming in MA.
We deﬁne the sequential composition of commands, written {x ←m1 ; m2}, to stand for the com-
mand bnd x ←cmd (m1) ; m2. Binary composition readily generalizes to an n-ary form by deﬁning
{x1 ←m1 ; . . . xn−1 ←mn−1 ; mn},
to stand for the iterated composition
{x1 ←m1 ; . . . {xn−1 ←mn−1 ; mn}}.
We sometimes write just {m1 ; m2} for the composition { ←m1 ; m2} where the returned value
from m1 is ignored; this generalizes in the obvious way to an n-ary form.
A related idiom, the command do e, executes an encapsulated command and returns its result.
By deﬁnition do e stands for the command bnd x ←e ; ret x.
The conditional command if (m) m1 else m2 executes either m1 or m2 according to whether the
result of executing m is zero or not:
{x ←m ; do (ifz x {z ,→cmd m1 | s( ) ,→cmd m2})}.
The returned value of the conditional is the value returned by the selected command.
The while loop command while (m1) m2 repeatedly executes the command m2 while the com-
mand m1 yields a non-zero number. It is deﬁned as follows:
do (fix loop : cmd is cmd (if (m1) {ret z} else {m2 ; do loop})).
This commands runs the self-referential encapsulated command that, when executed, ﬁrst exe-
cutes m1, branching on the result. If the result is zero, the loop returns zero (arbitrarily). If the
result is non-zero, the command m2 is executed and the loop is repeated.
A procedure is a function of type τ ⇀cmd that takes an argument of some type τ and yields
an unexecuted command as result. Many procedures have the form λ (x : τ) cmd m, which we
abbreviate to proc (x : τ) m. A procedure call is the composition of a function application with the
activation of the resulting command. If e1 is a procedure and e2 is its argument, then the procedure
call call e1(e2) is deﬁned to be the command do (e1(e2)), which immediately runs the result of
applying e1 to e2.
As an example, here is a procedure of type nat ⇀cmd that returns the factorial of its argument:

34.3 Typed Commands and Typed Assignables
313
proc (x:nat) {
dcl r := 1 in
dcl a := x in
{ while ( @ a ) {
y ←@ r
; z ←@ a
; r := (x-z+1)× y
; a := z-1
}
; x ←@ r
; ret x
}
}
The loop maintains the invariant that the contents of r is the factorial of x minus the contents
of a. Initialization makes this invariant true, and it is preserved by each iteration of the loop, so
that upon completion of the loop the assignable a contains 0 and r contains the factorial of x, as
required.
34.3
Typed Commands and Typed Assignables
So far we have restricted the type of the returned value of a command, and the contents of an
assignable, to be nat. Can this restriction be relaxed, while adhering to the stack discipline?
The key to admitting returned and assignable values of other types may be uncovered by a
close examination of the proof of Theorem 34.1. For the proof to go through it is crucial that
values of type nat, the type of assignables and return values, cannot contain an assignable, for
otherwise the embedded assignable would escape the scope of its declaration. This property is
self-evidently true for eagerly evaluated natural numbers, but fails when they are evaluated lazily.
Thus the safety of MA hinges on the evaluation order for the successor operation, in contrast to
most other situations where either interpretation is also safe.
When extending MA to admit assignables and returned values of other types, it is necessary
to pay close attention to whether assignables can be embedded in a value of a candidate type.
For example, if return values of procedure type are allowed, then the following command violates
safety:
dcl a := z in {ret (proc (x : nat) {a := x})}.
This command, when executed, allocates a new assignable a and returns a procedure that, when
called, assigns its argument to a. But this makes no sense, because the assignable a is deallocated
when the body of the declaration returns, but the returned value still refers to it. If the returned
procedure is called, execution will get stuck in the attempt to assign to a.
A similar example shows that admitting assignables of procedure type is also unsound. For
example, suppose that b is an assignable whose contents are of type nat ⇀cmd, and consider the
command
dcl a := z in {b := proc (x : nat) cmd(a := x) ; ret z}.

314
34.3 Typed Commands and Typed Assignables
We assign to b a procedure that uses a locally declared assignable a and then leaves the scope of
the declaration. If we then call the procedure stored in b, execution will get stuck attempting to
assign to the non-existent assignable a.
To admit declarations that return values other than nat and to admit assignables with contents
of types other than nat, we must rework the statics of MA to record the returned type of a com-
mand and to record the type of the contents of each assignable. First, we generalize the ﬁnite set
Σ of active assignables to assign a mobile type to each active assignable so that Σ has the form
of a ﬁnite set of assumptions of the form a ~ τ, where a is an assignable. Second, we replace the
judgment Γ ⊢Σ m ok by the more general form Γ ⊢Σ m ∼·· τ, stating that m is a well-formed com-
mand returning a value of type τ. Third, the type cmd is generalized to cmd(τ), which is written in
examples as τ cmd, to specify the return type of the encapsulated command.
The statics given in Section 34.1.1 is generalized to admit typed commands and typed assignables
as follows:
Γ ⊢Σ m ∼·· τ
Γ ⊢Σ cmd(m) : cmd(τ)
(34.6a)
Γ ⊢Σ e : τ
Γ ⊢Σ ret(e) ∼·· τ
(34.6b)
Γ ⊢Σ e : cmd(τ)
Γ, x : τ ⊢Σ m ∼·· τ′
Γ ⊢Σ bnd(e; x.m) ∼·· τ′
(34.6c)
Γ ⊢Σ e : τ
τ mobile
Γ ⊢Σ,a~τ m ∼·· τ′
τ′ mobile
Γ ⊢Σ dcl(e; a.m) ∼·· τ′
(34.6d)
Γ ⊢Σ,a~τ get[a] ∼·· τ
(34.6e)
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ set[a](e) ∼·· τ
(34.6f)
Apart from the generalization to track returned types and content types, the most important
change is that in Rule (34.6d) both the type of a declared assignable and the return type of the
declaration is required to be mobile. The deﬁnition of the judgment τ mobile is guided by the
following mobility condition:
if τ mobile, ⊢Σ e : τ and e valΣ, then ⊢∅e : τ and e val∅.
(34.7)
That is, a value of mobile type may not depend on any active assignables.
As long as the successor operation is evaluated eagerly, the type nat is mobile:
nat mobile
(34.8)
Similarly, a product of mobile types may safely be deemed mobile, if pairs are evaluated eagerly:
τ1 mobile
τ2 mobile
τ1 × τ2 mobile
(34.9)

34.4 Notes
315
And the same goes for sums, if the injections are evaluated eagerly:
τ1 mobile
τ2 mobile
τ1 + τ2 mobile
(34.10)
In each of these cases laziness defeats mobility, because values may contain suspended computa-
tions that depend on an assignable. For example, if the successor operation for the natural num-
bers were evaluated lazily, then s(e) would be a value for any expression e including one that
refers to an assignable a.
Because the body of a procedure may involve an assignable, no procedure type is mobile, nor
is any command type. What about function types other than procedure types? We may think
they are mobile, because a pure expression cannot depend on an assignable. Although this is the
case, the mobility condition need not hold. For example, consider the following value of type
nat ⇀nat:
λ (x : nat) (λ ( : τ cmd) z)(cmd {@ a}).
Although the assignable a is not actually needed to compute the result, it nevertheless occurs in
the value, violating the mobility condition.
The mobility restriction on the statics of declarations ensures that the type associated to an
assignable is always mobile. We may therefore assume, without loss of generality, that the types
associated to the assignables in the signature Σ are mobile.
Theorem 34.3 (Preservation for Typed Commands).
1. If e 7−→
Σ e′ and ⊢Σ e : τ, then ⊢Σ e′ : τ.
2. If m ∥µ 7−→
Σ m′ ∥µ′, with ⊢Σ m ∼·· τ and µ : Σ, then ⊢Σ m′ ∼·· τ and µ′ : Σ.
Theorem 34.4 (Progress for Typed Commands).
1. If ⊢Σ e : τ, then either e valΣ, or there exists e′ such that e 7−→
Σ e′.
2. If ⊢Σ m ∼·· τ and µ : Σ, then either m ∥µ ﬁnalΣ or m ∥µ 7−→
Σ m′ ∥µ′ for some µ′ and m′.
The proofs of Theorems 34.3 and 34.4 follows very closely the proof of Theorems 34.1 and 34.2.
The main difference is that we appeal to the mobility condition to ensure that returned values and
stored values are independent of the active assignables.
34.4
Notes
Modernized Algol is a derivative of Reynolds’s Idealized Algol (Reynolds, 1981). In contrast to
Reynolds’s formulation, Modernized Algol maintains a separation between computations that
depend on the memory and those that do not, and does not rely on call-by-name for function
application, but rather has a type of encapsulated commands that can be used where call-by-name

316
34.4 Notes
would otherwise be required. The modal distinction between expressions and commands was
present in the original formulation of Algol 60, but is developed here in light of the concept of
monadic effects introduced by Moggi (1989). Its role in functional programming was emphasized
by Wadler (1992). The modal separation in MA is adapted directly from Pfenning and Davies
(2001), which stresses the connection to lax modal logic.
What are called assignables here are invariably called variables elsewhere. The distinction be-
tween variables and assignables is blurred in languages that allow assignables as forms of expres-
sion. (Indeed, Reynolds himself1 regards this as a deﬁning feature of Algol, in opposition to the
formulation given here.) In MA we choose to make the distinction between variables, which are
given meaning by substitution, and assignables, which are given meaning by mutation. Draw-
ing this distinction requires new terminology; the term assignable seems apt for the imperative
programming concept.
The concept of mobility of a type was introduced in the ML5 language for distributed comput-
ing (Murphy et al., 2004), with the similar meaning that a value of a mobile type cannot depend
on local resources. Here the mobility restriction is used to ensure that the language adheres to the
stack discipline.
Exercises
34.1. Originally Algol had both scalar assignables, whose contents are atomic values, and array
assignables, which is a ﬁnite sequence of scalar assignables. Like scalar assignables, array
assignables are stack-allocated. Extend MA with array assignables, ensuring that the lan-
guage remains type safe, but allowing that computation may abort if a non-existent array
element is accessed.
34.2. Consider carefully the behavior of assignable declarations within recursive procedures, as in
the following expression
fix p is λ (p : τ) dcl a := e in cmd(m)
of type τ ⇀ρ cmd for some ρ. Because p is recursive, the body m of the procedure may
call itself during its execution, causing the same declaration to be executed more than once.
Explain the dynamics of getting and setting a in such a situation.
34.3. Originally Algol considered assignables as expressions that stand for their contents in mem-
ory. Thus, if a is an assignable containing a number, one could write expressions such as
a + a that would evaluate to twice the contents of a. Moreover, one could write commands
such as a := a + a to double the contents of a. These conventions encouraged programmers
to think of assignables as variables, quite the opposite of their separation in MA. This con-
vention, combined with an over-emphasis on concrete syntax, led to a conundrum about the
different roles of a in the above assignment command: its meaning on the left of the assign-
ment is different from its meaning on the right. These came to be called the left-, or l-value,
1Personal communication, 2012.

34.4 Notes
317
and the right-, or r-value of the assignable a, corresponding to its position in the assignment
statement. When viewed as abstract syntax, though, there is no ambiguity to be explained:
the assignment operator is indexed by its target assignable, instead of taking as argument
an expression that happens to be an assignable, so that the command is set[a](a + a), not
set(a; a + a).
This still leaves the puzzle of how to regard assignables as forms of expression. As a ﬁrst
cut, reformulate the dynamics of MA to account for this. Reformulate the dynamics of ex-
pressions in terms of the judgments e ∥µ 7−→
Σ e′ ∥µ′ and e ∥µ ﬁnal that allow evaluation of e
to depend on the contents of the memory. Each use of an assignable as an expression should
require one access to the memory. Then prove memory invariance:: if e ∥µ 7−→
Σ e′ ∥µ′, then
µ′ = µ.
A natural generalization is to allow any sequence of commands to be considered as an ex-
pression, if they are all passive in the sense that no assignments are allowed. Write do {m},
where m is a passive command, for a passive block whose evaluation consists of executing the
command m on the current memory, using its return value as the value of the expression.
Observe that memory invariance holds for passive blocks.
The use of an assignable a as an expression may now be rendered as the passive block
do {@ a}. More complex uses of assignables as expressions admit several different inter-
pretations using passive blocks. For example, an expression such as a + a might be rendered
in one of two ways:
(a) do {@ a} + do {@ a}, or
(b) let x be do {@ a} in x + x.
The latter formulation accesses a only once, but uses its value twice. Comment on there
being two different interpretations of a + a.
34.4. Recursive procedures in Algol are declared using a command of the form proc p(x : τ) : ρ is
m in m′, which is governed by the typing rule
Γ, p : τ ⇀ρ cmd, x : τ ⊢Σ m ∼·· ρ
Γ, p : τ ⇀ρ cmd ⊢Σ m′ ∼·· τ′
Γ ⊢Σ proc p(x : τ) : ρ is m in m′ ∼·· τ′
.
(34.11)
From the present viewpoint it is peculiar to insist on declaring procedures at all, because they
are simply values of procedure type, and even more peculiar to insist that they be conﬁned
for use within a command. One justiﬁcation for this limitation, though, is that Algol included
a peculiar feature, called an own variable2 that was declared for use within the procedure, but
whose state persisted across calls to the procedure. One application would be to a procedure
that generated pseudo-random numbers based on a stored seed that inﬂuenced the behavior
of successive calls to it. Give a formulation in MA of the extended declaration
proc p(x : τ) : ρ is {own a := e in m} in m′
2That is to say, an own assignable.

318
34.4 Notes
where a is declared as an “own” of the procedure p. Contrast the meaning of the foregoing
declaration with the following one:
proc p(x : τ) : ρ is {dcl a := e in m} in m′.
34.5. A natural generalization of own assignables is to allow the creation of many such scenarios
for a single procedure (or mutually recursive collection of procedures), with each instance
creating its own persistent state. This ability motivated the concept of a class in Simula-67 as
a collection of procedures, possibly mutually recursive, that shared common persistent state.
Each instance of a class is called an object of that class; calls to its constituent procedures mu-
tate the private persistent state. Formulate this 1967 precursor of imperative object-oriented
programming in the context of MA.
34.6. There are several ways to formulate an abstract machine for MA that accounts for both the
control stack, which sequences execution (as described in Chapter 28 for PCF), and the data
stack, which records the contents of the assignables. A consolidated stack combines these two
separate concepts into one, whereas separated stacks keeps the memory separate from the
control stack, much as we have done in the structural dynamics given by Rules (34.3). In
either case the storage required for an assignable is deallocated when exiting the scope of
that assignable, a key beneﬁt of the stack discipline for assignables in MA.
With a modal separation between expressions and commands it is natural to use a structural
dynamics for expressions (given by the transition and value judgments, e 7−→e′ and e val),
and a stack machine dynamics for commands.
(a) Formulate a consolidated stack machine where both assignables and stack frames are
recorded on the same stack. Consider states k ▷Σ m, where ⊢Σ k ◁: τ and ⊢Σ m ∼·· τ,
and k ◁Σ e, where ⊢Σ k ◁: τ and ⊢Σ e : τ. Comment on the implementation methods
required for a consolidated stack.
(b) Formulate a separated stack machine where the memory is maintained separately from
the control stack. Consider states of the form µ ∥k ▷Σ m, where µ : Σ, ⊢Σ k ◁: τ, and
⊢Σ m ∼·· τ, and of the form µ ∥k ◁Σ e, where ⊢Σ k ◁: τ, ⊢Σ e : τ, and e val.

Chapter 35
Assignable References
A reference to an assignable a is a value, written &a, of reference type that determines the assignable a.
A reference to an assignable provides the capability to get or set the contents of that assignable, even
if the assignable itself is not in scope when it is used. Two references can be compared for equality
to test whether they govern the same underlying assignable. If two references are equal, then
setting one will affect the result of getting the other; if they are not equal, then setting one cannot
inﬂuence the result of getting from the other. Two references that govern the same underlying
assignable are aliases. Aliasing complicates reasoning about programs that use references, because
any two references may refer to the assignable.
Reference types are compatible with both a scoped and a scope-free allocation of assignables.
When assignables are scoped, the range of signiﬁcance of a reference type is limited to the scope
of the assignable to which it refers. Reference types are therefore immobile, so that they cannot be
returned from the body of a declaration, nor stored in an assignable. Although ensuring adherence
to the stack discipline, this restriction precludes using references to create mutable data structures,
those whose structure can be altered during execution. Mutable data structures have a number of
applications in programming, including improving efﬁciency (often at the expense of expressive-
ness) and allowing cyclic (self-referential) structures to be created. Supporting mutability requires
that assignables be given a scope-free dynamics, so that their lifetime persists beyond the scope of
their declaration. Consequently, all types are mobile, so that a value of any type may be stored in
an assignable or returned from a command.
35.1
Capabilities
The commands get[a] and set[a](e) in MA operate on statically speciﬁed assignable a. Even to
write these commands requires that the assignable a be in scope where the command occurs. But
suppose that we wish to deﬁne a procedure that, say, updates an assignable to double its previous
value, and returns the previous value. We can write such a procedure for any given assignable, a,
but what if we wish to write a generic procedure that works for any given assignable?

320
35.2 Scoped Assignables
One way to do this is give the procedure the capability to get and set the contents of some caller-
speciﬁed assignable. Such a capability is a pair consisting of a getter and a setter for that assignable.
The getter for an assignable a is a command that, when executed, returns the contents of a. The
setter for an assignable a is a procedure that, when applied to a value of suitable type, assigns that
value to a. Thus, a capability for an assignable a containing a value of type τ is a value of type
τ cap ≜τ cmd × (τ ⇀τ cmd).
A capability for getting and setting an assignable a containing a value of type τ is given by the
pair
⟨cmd (@ a), proc (x : τ) a := x⟩
of type τ cap. Because a capability type is a product of a command type and a procedure type,
no capability type is mobile. Thus, a capability cannot be returned from a command, nor stored
into an assignable. This is as it should be, for otherwise we would violate the stack discipline for
allocating assignables.
The proposed generic doubling procedure is programmed using capabilities as follows:
proc (⟨get, set⟩: nat cmd × (nat ⇀nat cmd)) {x ←do get ; y ←do (set(x + x)) ; ret x}.
The procedure is called with the capability to access an assignable a. When executed, it invokes the
getter to obtain the contents of a, and then invokes the setter to assign to a, returning the previous
value. Observe that the assignable a need not be accessible by this procedure; the capability given
by the caller comprises the commands required to get and set a.
35.2
Scoped Assignables
A weakness of using a capability to give indirect access to an assignable is that there is no guaran-
tee that a given getter/setter pair are in fact the capability for a particular assignable. For example,
we might pair the getter for a with the setter for b, leading to unexpected behavior. There is noth-
ing in the type system that prevents creating such mismatched pairs.
To avoid this we introduce the concept of a reference to an assignable. A reference is a value
from which we may obtain the capability to get and set a particular assignable. Moreover, two
references can be tested for equality to see whether they act on the same assignable.1 The reference
type ref(τ) has as values references to assignables of type τ. The introduction and elimination
forms for this type are given by the following syntax chart:
Typ
τ
::=
ref(τ)
τ ref
assignable
Exp
e
::=
ref[a]
&a
reference
Cmd
m
::=
getref(e)
∗e
contents
setref(e1; e2)
e1 ∗= e2
update
1The getter and setter do not sufﬁce to deﬁne equality, because not all types admit a test for equality. When they do,
and when there are at least two distinct values of their type, we can determine whether they are aliases by assigning to one
and checking whether the contents of the other is changed.

35.2 Scoped Assignables
321
The statics of reference types is deﬁned by the following rules:
Γ ⊢Σ,a~τ ref[a] : ref(τ)
(35.1a)
Γ ⊢Σ e : ref(τ)
Γ ⊢Σ getref(e) ∼·· τ
(35.1b)
Γ ⊢Σ e1 : ref(τ)
Γ ⊢Σ e2 : τ
Γ ⊢Σ setref(e1; e2) ∼·· τ
(35.1c)
Rule (35.1a) speciﬁes that the name of any active assignable is an expression of type ref(τ).
The dynamics of reference types defers to the corresponding operations on assignables, and
does not alter the underlying dynamics of assignables:
ref[a] valΣ,a~τ
(35.2a)
e 7−→
Σ e′
getref(e) ∥µ 7−→
Σ getref(e′) ∥µ
(35.2b)
getref(ref[a]) ∥µ 7−−−→
Σ,a~τ get[a] ∥µ
(35.2c)
e1 7−→
Σ e′
1
setref(e1; e2) ∥µ 7−→
Σ setref(e′
1; e2) ∥µ
(35.2d)
setref(ref[a]; e) ∥µ 7−−−→
Σ,a~τ set[a](e) ∥µ
(35.2e)
A reference to an assignable is a value. The getref and setref operations on references defer to
the corresponding operations on assignables once the referent has been resolved.
Because references give rise to capabilities, the reference type is immobile. As a result refer-
ences cannot be stored in assignables or returned from commands. The immobility of references
ensures safety, as can be seen by extending the safety proof given in Chapter 34.
As an example of using references, the generic doubling procedure discussed in the preceding
section is programmed using references as follows:
proc (r : nat ref) {x ←∗r ; r ∗= x + x ; ret x}.
Because the argument is a reference, rather than a capability, there is no possibility that the getter
and setter refer to different assignables.

322
35.3 Free Assignables
The ability to pass references to procedures comes at a price, because any two references might
refer to the same assignable (if they have the same type). Consider a procedure that, when given
two references x and y, adds twice the contents of y to the contents of x. One way to write this
code creates no complications:
λ (x : nat ref) λ (y : nat ref) cmd {x′ ←∗x ; y′ ←∗y ; x ∗= x′ + y′ + y′}.
Even if x and y refer to the same assignable, the effect will be to set the contents of the assignable
referenced by x to the sum of its original contents and twice the contents of the assignable refer-
enced by y.
But now consider the following seemingly equivalent implementation of this procedure:
λ (x : nat ref) λ (y : nat ref) cmd {x += y ; x += y},
where x += y is the command
{x′ ←∗x ; y′ ←∗y ; x ∗= x′ + y′}
that adds the contents of y to the contents of x. The second implementation works right, as long
as x and y do not refer to the same assignable. If they do refere to a common assignable a, with
contents n, the result is that a is to set 4 × n, instead of the intended 3 × n. The second get of y is
affected by the ﬁrst set of x.
In this case it is clear how to avoid the problem: use the ﬁrst implementation, rather than
the second. But the difﬁculty is not in ﬁxing the problem once it has been discovered, but in
noticing the problem in the ﬁrst place. Wherever references (or capabilities) are used, the problems
of interference lurk. Avoiding them requires very careful consideration of all possible aliasing
relationships among all of the references in play. The problem is that the number of possible
aliasing relationships among n references grows combinatorially in n.
35.3
Free Assignables
Although it is interesting to note that references and capabilities are compatible with the stack
discipline, for references to be useful requires that this restriction be relaxed. With immobile ref-
erences it is impossible to build data structures containing references, or to return references from
procedures. To allow this we must arrange that the lifetime of an assignable extend beyond its
scope. In other words we must give up stack allocation for heap allocation. Assignables that
persist beyond their scope of declaration are called scope-free, or just free, assignables. When all
assignables are free, every type is mobile and so any value, including a reference, may be used in
a data structure.
Supporting free assignables amounts to changing the dynamics so that allocation of assignables
persists across transitions. We use transition judgments of the form
ν Σ { m ∥µ } 7−→ν Σ′ { m′ ∥µ′ }.

35.3 Free Assignables
323
Execution of a command may allocate new assignables, may alter the contents of existing assignables,
and may give rise to a new command to be executed at the next step. The rules deﬁning the dy-
namics of free assignables are as follows:
e valΣ
ν Σ { ret(e) ∥µ } ﬁnal
(35.3a)
e 7−→
Σ e′
ν Σ { ret(e) ∥µ } 7−→ν Σ { ret(e′) ∥µ }
(35.3b)
e 7−→
Σ e′
ν Σ { bnd(e; x.m) ∥µ } 7−→ν Σ { bnd(e′; x.m) ∥µ }
(35.3c)
e valΣ
ν Σ { bnd(cmd(ret(e)); x.m) ∥µ } 7−→ν Σ { [e/x]m ∥µ }
(35.3d)
ν Σ { m1 ∥µ } 7−→ν Σ′ { m′
1 ∥µ′ }
ν Σ { bnd(cmd(m1); x.m2) ∥µ } 7−→ν Σ′ { bnd(cmd(m′
1); x.m2) ∥µ′ }
(35.3e)
ν Σ, a ~ τ { get[a] ∥µ ⊗a ,→e } 7−→ν Σ, a ~ τ { ret(e) ∥µ ⊗a ,→e }
(35.3f)
e 7−→
Σ e′
ν Σ { set[a](e) ∥µ } 7−→ν Σ { set[a](e′) ∥µ }
(35.3g)
e valΣ,a~τ
ν Σ, a ~ τ { set[a](e) ∥µ ⊗a ,→} 7−→ν Σ, a ~ τ { ret(e) ∥µ ⊗a ,→e }
(35.3h)
e 7−→
Σ e′
ν Σ { dcl(e; a.m) ∥µ } 7−→ν Σ { dcl(e′; a.m) ∥µ }
(35.3i)
e valΣ
ν Σ { dcl(e; a.m) ∥µ } 7−→ν Σ, a ~ τ { m ∥µ ⊗a ,→e }
(35.3j)
The language RMA extends MA with references to free assignables. Its dynamics is similar to
that of references to scoped assignables given earlier.
e 7−→
Σ e′
ν Σ { getref(e) ∥µ } 7−→ν Σ { getref(e′) ∥µ }
(35.4a)
ν Σ { getref(ref[a]) ∥µ } 7−→ν Σ { get[a] ∥µ }
(35.4b)
e1 7−→
Σ e′
1
ν Σ { setref(e1; e2) ∥µ } 7−→ν Σ { setref(e′
1; e2) ∥µ }
(35.4c)

324
35.4 Safety
ν Σ { setref(ref[a]; e2) ∥µ } 7−→ν Σ { set[a](e2) ∥µ }
(35.4d)
The expressions cannot alter or extend the memory, only commands may do so.
As an example of using RMA, consider the command newref[τ](e) deﬁned by
dcl a := e in ret (&a).
(35.5)
This command allocates a fresh assignable, and returns a reference to it. Its static and dynamics
are derived from the foregoing rules as follows:
Γ ⊢Σ e : τ
Γ ⊢Σ newref[τ](e) ∼·· ref(τ)
(35.6)
e 7−→
Σ e′
ν Σ { newref[τ](e) ∥µ } 7−→ν Σ { newref[τ](e′) ∥µ }
(35.7a)
e valΣ
ν Σ { newref[τ](e) ∥µ } 7−→ν Σ, a ~ τ { ret(ref[a]) ∥µ ⊗a ,→e }
(35.7b)
Oftentimes the command newref[τ](e) is taken as primitive, and the declaration command is omit-
ted. In that case all assignables are accessed by reference, and no direct access to assignables is
provided.
35.4
Safety
Although the proof of safety for references to scoped assignables presents few difﬁculties, the
safety for free assignables is tricky. The main difﬁculty is to account for cyclic dependencies within
data structures. The contents of one assignable may contain a reference to itself, or a reference to
another assignable that contains a reference to it, and so forth. For example, consider the following
procedure e of type nat ⇀nat cmd:
proc (x : nat) {if (x) ret (1) else { f ←@ a ; y ←f (x −1) ; ret (x ∗y)}}.
Let µ be a memory of the form µ′ ⊗a ,→e in which the contents of a contains, via the body of the
procedure, a reference to a itself. Indeed, if the procedure e is called with a non-zero argument, it
will “call itself” by indirect reference through a.
Cyclic dependencies complicate the deﬁnition of the judgment µ : Σ. It is deﬁned by the
following rule:
⊢Σ m ∼·· τ
⊢Σ µ : Σ
ν Σ { m ∥µ } ok
(35.8)
The ﬁrst premise of the rule states that the command m is well-formed relative to Σ. The second
premise states that the memory µ conforms to Σ, relative to all of Σ so that cyclic dependencies are
permitted. The judgment ⊢Σ′ µ : Σ is deﬁned as follows:
∀a ~ τ ∈Σ
∃e
µ(a) = e and ⊢Σ′ e : τ
⊢Σ′ µ : Σ
(35.9)

35.4 Safety
325
Theorem 35.1 (Preservation).
1. If ⊢Σ e : τ and e 7−→
Σ e′, then ⊢Σ e′ : τ.
2. If ν Σ { m ∥µ } ok and ν Σ { m ∥µ } 7−→ν Σ′ { m′ ∥µ′ }, then ν Σ′ { m′ ∥µ′ } ok.
Proof. Simultaneously, by induction on transition. We prove the following stronger form of the
second statement:
If ν Σ { m ∥µ } 7−→ν Σ′ { m′ ∥µ′ }, where ⊢Σ m ∼·· τ, ⊢Σ µ : Σ, then Σ′ extends Σ, and
⊢Σ′ m′ ∼·· τ, and ⊢Σ′ µ′ : Σ′.
Consider the transition
ν Σ { dcl(e; a.m) ∥µ } 7−→ν Σ, a ~ ρ { m ∥µ ⊗a ,→e }
where e valΣ. By assumption and inversion of rule (34.6d) we have ⊢Σ e : ρ, ⊢Σ,a~ρ m ∼·· τ, and
⊢Σ µ : Σ. But because extension of Σ with a fresh assignable does not affect typing, we also have
⊢Σ,a~ρ µ : Σ and ⊢Σ,a~ρ e : ρ, from which it follows by rule (35.9) that ⊢Σ,a~ρ µ ⊗a ,→e : Σ, a ~ ρ.
The other cases follow a similar pattern, and are left as an exercise for the reader.
Theorem 35.2 (Progress).
1. If ⊢Σ e : τ, then either e valΣ or there exists e′ such that e 7−→
Σ e′.
2. If ν Σ { m ∥µ } ok then either ν Σ { m ∥µ } ﬁnal or ν Σ { m ∥µ } 7−→ν Σ′ { m′ ∥µ′ } for some Σ′,
µ′, and m′.
Proof. Simultaneously, by induction on typing. For the second statement we prove the stronger
form
If ⊢Σ m ∼·· τ and ⊢Σ µ : Σ, then either ν Σ { m ∥µ } ﬁnal, or ν Σ { m ∥µ } 7−→ν Σ′ { m′ ∥µ′ }
for some Σ′, µ′, and m′.
Consider the typing rule
Γ ⊢Σ e : ρ
Γ ⊢Σ,a~ρ m ∼·· τ
Γ ⊢Σ dcl(e; a.m) ∼·· τ
We have by the ﬁrst inductive hypothesis that either e valΣ or e 7−→
Σ e′ for some e′. In the latter case
we have by rule (35.3i)
ν Σ { dcl(e; a.m) ∥µ } 7−→ν Σ { dcl(e′; a.m) ∥µ }.
In the former case we have by rule (35.3j) that
ν Σ { dcl(e; a.m) ∥µ } 7−→ν Σ, a ~ ρ { m ∥µ ⊗a ,→e }.

326
35.5 Benign Effects
Now consider the typing rule
Γ ⊢Σ,a~τ get[a] ∼·· τ
By assumption ⊢Σ,a~τ µ : Σ, a ~ τ, and hence there exists e valΣ,a~τ such that µ = µ′ ⊗a ,→e and
⊢Σ,a~τ e : τ. By rule (35.3f)
ν Σ, a ~ τ { get[a] ∥µ′ ⊗a ,→e } 7−→ν Σ, a ~ τ { ret(e) ∥µ′ ⊗a ,→e },
as required. The other cases are handled similarly.
35.5
Benign Effects
The modal separation between commands and expressions ensures that the meaning of an expres-
sion does not depend on the (ever-changing) contents of assignables. Although this is helpful in
many, perhaps most, situations, it also precludes programming techniques that use storage effects
to implement purely functional behavior. A prime example is memoization. Externally, a sus-
pended computation behaves exactly like the underlying computation; internally, an assignable
is associated with the computation that stores the result of any evaluation of the computation for
future use. Other examples are self-adjusting data structures, which use state to improve their efﬁ-
ciency without changing their functional behavior. For example, a splay tree is a binary search tree
that uses mutation internally to re-balance the tree as elements are inserted, deleted, and retrieved,
so that lookup takes time proportional to the logarithm of the number of elements.
These are examples of benign storage effects, uses of mutation in a data structure to improve
efﬁciency without disrupting its functional behavior. One class of examples are self-adjusting data
structures that reorganize themselves during one use to improve efﬁciency of later uses. Another
class of examples are memoized, or lazy, data structures, which are discussed in Chapter 36. Be-
nign effects such as these are impossible to implement if a strict separation between expressions
and commands is maintained. For example, a self-adjusting tree involves mutation, but is a value
just like any other, and this cannot be achieved in MA. Although several special-case techniques
are known, the most general solution is to do away with the modal distinction, coalescing expres-
sions and commands into a single syntactic category. The penalty is that the type system no longer
ensures that an expression of type τ denotes a value of that type; it might also have storage effects
during its evaluation. The beneﬁt is that one may freely use benign effects, but it is up to the
programmer to ensure that they truly are benign.
The language RPCF extends PCF with references to free assignables. The following rules de-
ﬁne the statics of the distinctive features of RPCF:
Γ ⊢Σ e1 : τ1
Γ ⊢Σ,a~τ1 e2 : τ2
Γ ⊢Σ dcl(e1; a.e2) : τ2
(35.10a)
Γ ⊢Σ,a~τ get[a] : τ
(35.10b)

35.6 Notes
327
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ set[a](e) : τ
(35.10c)
Correspondingly, the dynamics of RPCF is given by transitions of the form
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ },
where e is an expression, and not a command. The rules deﬁning the dynamics are very similar to
those for RMA, but with commands and expressions integrated into a single category.
To illustrate the concept of a benign effect, consider the technique of back-patching to implement
recursion. Here is an implementation of the factorial function that uses an assignable to implement
recursive calls:
dcl a := λn:nat.0 in
{ f ←a := λn:nat.ifz(n, 1, n′.n×(@a)(n′))
; ret(f)
}
This declaration returns a function of type nat ⇀nat that is obtained by (a) allocating a free
assignable initialized arbitrarily with a function of this type, (b) deﬁning a λ-abstraction in which
each “recursive call” consists of retrieving and applying the function stored in that assignable, (c)
assigning this function to the assignable, and (d) returning that function. The result is a function
on the natural numbers, even though it uses state in its implementation.
Backpatching is not expressible in RMA, because it relies on assignment. Let us attempt to
recode the previous example in RMA:
dcl a := proc(n:nat){ret 0} in
{ f ←a := . . .
; ret(f)
},
where the elided procedure assigned to a is given by
proc(n:nat) {if (ret(n)) {ret(1)} else {f←@a; x←f(n-1); ret(n×x)}}.
The difﬁculty is that what we have is a command, not an expression. Moreover, the result of
the command is of the procedure type nat ⇀(nat cmd), and not of the function type nat ⇀nat.
Consequently, we cannot use the factorial procedure in an expression, but have to execute it as a
command using code such as this:
{ f ←fact ; x ←f(n); ret(x) }.
35.6
Notes
Reynolds (1981) uses capabilities to provide indirect access to assignables; references are just an
abstract form of capability. References are often permitted only for free assignables, but with

328
35.6 Notes
mobility restrictions one may also have references to scoped assignables. The proof of safety of
free references outlined here follows those given by Wright and Felleisen (1994) and Harper (1994).
Benign effects are central to the distinction between Haskell, which provides an Algol-like sep-
aration between commands and expressions, and ML, which integrates evaluation with execution.
The choice between them is classic trade-off, with neither superior to the other in all respects.
Exercises
35.1. Consider scoped array assignables as described in Exercise 34.1. Extend the treatment of
array assignables in Exercise 34.1, to account for array assignable references.
35.2. References to scope-free assignables are often used to implement recursive data structures
such as mutable lists and trees. Examine such data structures in the context of RMA enriched
with sum, product, and recursive types.
Give six different types that could be considered a type of linked lists, according to the fol-
lowing characteristics:
(a) A mutable list may only be updated in toto by replacing it with another (immutable)
list.
(b) A mutable list can be altered in one of two ways, to make it empty, or to change both its
head and tail element simultaneously. The tail element is any other such mutable list,
so circularities may arise.
(c) A mutable list is, permanently, either empty or non-empty. If not, both its head and tail
can be modiﬁed simultaneously.
(d) A mutable list is, permanently, either empty or non-empty. If not, its tail, but not its
head, can be set to another such list.
(e) A mutable list is, permanently, either empty or non-empty. If not, either its head or its
tail elements can be modiﬁed independently.
(f) A mutable list can be altered to become either empty or non-empty. If it is non-empty,
either it head, or its tail, can be modiﬁed independently of one another.
Discuss the merits and deﬁciencies of each representation.

Chapter 36
Lazy Evaluation
Lazy evaluation comprises a variety of methods to defer evaluation of an expression until it is re-
quired, and to share the results of any such evaluation among all uses of a deferred computation.
Laziness is not merely an implementation device, but it also affects the meaning of a program.
One form of laziness is the by-need evaluation strategy for function application. Recall from
Chapter 8 that the by-name evaluation order passes the argument to a function in unevaluated
form so that it is only evaluated if it is actually used. But because the argument is replicated by
substitution, it might be evaluated more than once. By-need evaluation ensures that the argument
to a function is evaluated at most once, by ensuring that all copies of an argument share the result
of evaluating any one copy.
Another form of laziness is the concept of a lazy data structure. As we have seen in Chap-
ters 10, 11, and 20, we may choose to defer evaluation of the components of a data structure until
they are actually required, and not when the data structure is created. But if a component is re-
quired more than once, then the same computation will, without further provision, be repeated
on each use. To avoid this, the deferred portions of a data structure are shared so an access to one
will propagate its result to all occurrences of the same computation.
Yet another form of laziness arises from the concept of general recursion considered in Chap-
ter 19. Recall that the dynamics of general recursion is given by unrolling, which replicates the
recursive computation on each use. It would be preferable to share the results of such compu-
tation across unrollings. A lazy implementation of recursion avoids such replications by sharing
those results.
Traditionally, languages are biased towards either eager or lazy evaluation. Eager languages
use a by-value dynamics for function applications, and evaluate the components of data structures
when they are created. Lazy languages adopt the opposite strategy, preferring a by-name dynam-
ics for functions, and a lazy dynamics for data structures. The overhead of laziness is reduced by
managing sharing to avoid redundancy. Experience has shown, however, that the distinction is
better drawn at the level of types. It is important to have both lazy and eager types, so that the
programmer controls the use of laziness, rather than having it enforced by the language dynamics.

330
36.1 PCF By-Need
36.1
PCF By-Need
We begin by considering a lazy variant of PCF, called LPCF, in which functions are called by
name, and the successor operator is evaluated lazily. Under a lazy interpretation variables are
bound to unevaluated expressions, and the argument to the successor left unevaluated: any suc-
cessor is a value, regardless of whether the predecessor is or not. By-name function applica-
tion replicates the unevaluated argument by substitution, which means that there can arise many
copies of the same expression, each evaluated separately, if at all. By-need evaluation uses a device
called memoization to share all such copies of an argument and to ensure that if it is evaluated at all,
its value is stored so that all other uses of it will avoid re-computation. Computations are named
during evaluation, and are accessed by a level of indirection using this name to index the memo
table, which records the expression and, if it is every evaluated, its value.
The dynamics of LPCF is based on a transition system with states of the form ν Σ { e ∥µ },
where Σ is a ﬁnite set of hypotheses a1 ~ τ1, . . . , an ~ τn associating types to symbols, e is an ex-
pression that can involve the symbols in Σ, and µ maps each symbol declared in Σ to either an
expression or a special symbol, •, called the black hole. (The role of the black hole is explained be-
low.) As a notational convenience, we use a bit of legerdemain with the concrete syntax similar to
that used in Chapter 34. Speciﬁcally, the concrete syntax for the expression via(a), which fetches
the contents of the assignable a, is @ a.
The dynamics of LPCF is given by he following two forms of judgment:
1. e valΣ, stating that e is a value that can involve the symbols in Σ.
2. ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }, stating that one step of evaluation of the expression e relative
to memo table µ with the symbols declared in Σ results in the expression e′ relative to the
memo table µ′ with symbols declared in Σ′.
The dynamics is deﬁned so that the active symbols grow during evaluation. The memo table may
be altered destructively during execution to show progress in the evaluation of the expression
associated with a symbol.
The judgment e valΣ expressing that e is a closed value is deﬁned by the following rules:
z valΣ
(36.1a)
s(@ a) valΣ,a~nat
(36.1b)
λ (x : τ) e valΣ
(36.1c)
Rules (36.1a) through (36.1c) specify that z is a value, any expression of the form s(@ a), where
a is a symbol, is a value, and that any λ-abstraction, possibly containing symbols, is a value. It
is important that symbols themselves are not values, rather they stand for (possibly unevaluated)
expressions as speciﬁed by the memo table. The expression @ a, which is short for via(a), is not a
value. Rather, it is accessed to obtain, and possibly update, the binding of the symbol a in memory.

36.1 PCF By-Need
331
The initial and ﬁnal states of evaluation are deﬁned as follows:
ν ∅{ e ∥∅} initial
(36.2a)
e valΣ
ν Σ { e ∥µ } ﬁnal
(36.2b)
Rule (36.2a) speciﬁes that an initial state consists of an expression evaluated relative to an
empty memo table. Rule (36.2b) speciﬁes that a ﬁnal state has the form ν Σ { e ∥µ }, where e is a
value relative to Σ.
The transition judgment for the dynamics of LPCF is deﬁned by the following rules:
e valΣ,a~τ
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→e } 7−→ν Σ, a ~ τ { e ∥µ ⊗a ,→e }
(36.3a)
ν Σ, a ~ τ { e ∥µ ⊗a ,→• } 7−→ν Σ′, a ~ τ { e′ ∥µ′ ⊗a ,→• }
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→e } 7−→ν Σ′, a ~ τ { @ a ∥µ′ ⊗a ,→e′ }
(36.3b)
ν Σ { s(e) ∥µ } 7−→ν Σ, a ~ nat { s(@ a) ∥µ ⊗a ,→e }
(36.3c)
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { ifz e {z ,→e0 | s(x) ,→e1} ∥µ } 7−→ν Σ′ { ifz e′ {z ,→e0 | s(x) ,→e1} ∥µ′ }
(36.3d)
ν Σ { ifz z {z ,→e0 | s(x) ,→e1} ∥µ } 7−→ν Σ { e0 ∥µ }
(36.3e)





ν Σ, a ~ nat { ifz s(@ a) {z ,→e0 | s(x) ,→e1} ∥µ ⊗a ,→e }
7−→
ν Σ, a ~ nat { [@ a/x]e1 ∥µ ⊗a ,→e }





(36.3f)
ν Σ { e1 ∥µ } 7−→ν Σ′ { e′
1 ∥µ′ }
ν Σ { e1(e2) ∥µ } 7−→ν Σ′ { e′
1(e2) ∥µ′ }
(36.3g)





ν Σ { (λ (x : τ) e)(e2) ∥µ }
7−→
ν Σ, a ~ τ { [@ a/x]e ∥µ ⊗a ,→e2 }





(36.3h)
ν Σ { fix x : τ is e ∥µ } 7−→ν Σ, a ~ τ { @ a ∥µ ⊗a ,→[@ a/x]e }
(36.3i)

332
36.2 Safety of PCF By-Need
Rule (36.3a) governs a symbol whose associated expression is a value; the value of the symbol is
the value associated to that symbol in the memo table. Rule (36.3b) speciﬁes that if the expression
associated to a symbol is not a value, then it is evaluated “in place” until such time as rule (36.3a)
applies. This is achieved by switching the focus of evaluation to the associated expression, while
at the same time associating the black hole to that symbol. The black hole represents the absence of
a value for that symbol, so that any attempt to use it during evaluation of its associated expression
cannot make progress. The black hole signals a circular dependency that, if not caught using a
black hole, would initiate an inﬁnite regress.
Rule (36.3c) speciﬁes that evaluation of s(e) allocates a fresh symbol a for the expression e, and
yields the value s(@ a). The value of e is not determined until such time as the predecessor is re-
quired in a later computation, implementing a lazy dynamics for the successor. Rule (36.3f), which
governs a conditional branch on a successor, substitutes @ a for the variable x when computing the
predecessor of a non-zero number, ensuring that all occurrences of x share the same predecessor
computation.
Rule (36.3g) speciﬁes that the value of the function position of an application must be deter-
mined before the application can be executed. Rule (36.3h) speciﬁes that to evaluate an application
of a λ-abstraction we allocate a fresh symbol a for the argument, and substitute @ a for the argu-
ment variable of the function. The argument is evaluated only if it is needed in the later computa-
tion, and then that value is shared among all occurrences of the argument variable in the body of
the function.
General recursion is implemented by rule (36.3i). Recall from Chapter 19 that the expression
fix x : τ is e stands for the solution of the recursion equation x = e. Rule (36.3i) computes this
solution by associating a fresh symbol a with the body e substituting @ a for x within e to effect the
self-reference. It is this substitution that permits a named expression to depend on its own name.
For example, the expression fix x : τ is x associates the expression a to a in the memo table, and
returns @ a. The next step of evaluation is stuck, because it seeks to evaluate @ a with a bound
to the black hole. In contrast an expression such as fix f : τ′ ⇀τ is λ (x : τ′) e does not get stuck,
because the self-reference is “hidden” within the λ-abstraction, and hence need not be evaluated
to determine the value of the binding.
36.2
Safety of PCF By-Need
We write Γ ⊢Σ e : τ to mean that e has type τ under the assumptions Γ, treating symbols declared
in Σ as expressions of their associated type. The rules are as in Chapter 19, extended with the
following rule for symbols:
Γ ⊢Σ,a~τ @ a : τ
(36.4)
This rule states that the demand for the binding of a symbol, @ a, is a form of expression. It is a
“delayed substitution” that lazily replaces a demand for a by its binding.
The judgment ν Σ { e ∥µ } ok is deﬁned by the following rules:
⊢Σ e : τ
⊢Σ µ : Σ
ν Σ { e ∥µ } ok
(36.5a)

36.2 Safety of PCF By-Need
333
∀a ~ τ ∈Σ
µ(a) = e ̸= • =⇒⊢Σ′ e : τ
⊢Σ′ µ : Σ
(36.5b)
Rule (36.5b) permits self-reference through the memo table by allowing the expression associated
to a symbol a to contain occurrences of @ a. A symbol that is bound to the “black hole” is consid-
ered to be of any type.
Theorem 36.1 (Preservation). If ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ } and ν Σ { e ∥µ } ok, then ν Σ′ { e′ ∥µ′ } ok.
Proof. We prove by induction on rules (36.3) that if ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ } and ⊢Σ µ : Σ
and ⊢Σ e : τ, then Σ′ ⊇Σ and ⊢Σ′ µ′ : Σ′ and ⊢Σ′ e′ : τ.
Consider rule (36.3b), for which we have e = e′ = @ a, µ = µ0 ⊗a ,→e0, µ′ = µ′
0 ⊗a ,→e′
0, and
ν Σ, a ~ τ { e0 ∥µ0 ⊗a ,→• } 7−→ν Σ′, a ~ τ { e′
0 ∥µ′
0 ⊗a ,→• }.
Assume that ⊢Σ,a~τ µ : Σ, a ~ τ. It follows that ⊢Σ,a~τ e0 : τ and ⊢Σ,a~τ µ0 : Σ, and hence that
⊢Σ,a~τ µ0 ⊗a ,→• : Σ, a ~ τ.
We have by induction that Σ′ ⊇Σ and ⊢Σ′,a~τ e′
0 : τ′ and
⊢Σ′,a~τ µ0 ⊗a ,→• : Σ, a ~ τ.
But then
⊢Σ′,a~τ µ′ : Σ′, a ~ τ,
which sufﬁces for the result.
Consider rule (36.3g), so that e is the application e1(e2) and
ν Σ { e1 ∥µ } 7−→ν Σ′ { e′
1 ∥µ′ }.
Suppose that ⊢Σ µ : Σ and ⊢Σ e : τ. By inversion of typing ⊢Σ e1 : τ2 ⇀τ for some type τ2 such
that ⊢Σ e2 : τ2. By induction Σ′ ⊇Σ and ⊢Σ′ µ′ : Σ′ and ⊢Σ′ e′
1 : τ2 ⇀τ. By weakening we have
⊢Σ′ e2 : τ2, so that ⊢Σ′ e′
1(e2) : τ, which is enough for the result.
The statement of the progress theorem allows for the occurrence of a black hole, representing
a checkable form of non-termination. The judgment ν Σ { e ∥µ } loops, stating that e diverges by
virtue of encountering the black hole, is deﬁned by the following rules:
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→• } loops
(36.6a)
ν Σ, a ~ τ { e ∥µ ⊗a ,→• } loops
ν Σ, a ~ τ { @ a ∥µ ⊗a ,→e } loops
(36.6b)
ν Σ { e ∥µ } loops
ν Σ { ifz e {z ,→e0 | s(x) ,→e1} ∥µ } loops
(36.6c)

334
36.3 FPC By-Need
ν Σ { e1 ∥µ } loops
ν Σ { e1(e2) ∥µ } loops
(36.6d)
There are other ways of forming an inﬁnite loop. The looping judgment simply codiﬁes those
cases in which the looping behavior is a self-dependency, which is mediated by a black hole.
Theorem 36.2 (Progress). If ν Σ { e ∥µ } ok, then either ν Σ { e ∥µ } ﬁnal, or ν Σ { e ∥µ } loops, or
there exists µ′ and e′ such that ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }.
Proof. We proceed by induction on the derivations of ⊢Σ e : τ and ⊢Σ µ : Σ implicit in the deriva-
tion of ν Σ { e ∥µ } ok.
Consider rule (19.1a), where the symbol a is declared in Σ. Thus Σ = Σ0, a ~ τ and ⊢Σ µ : Σ. It
follows that µ = µ0 ⊗a ,→e0 with ⊢Σ µ0 : Σ0 and ⊢Σ e0 : τ. Note that ⊢Σ µ0 ⊗a ,→• : Σ. Applying
induction to the derivation of ⊢Σ e0 : τ, we consider three cases:
1. ν Σ { e0 ∥µ ⊗a ,→• } ﬁnal.
By inversion of rule (36.2b) we have e0 valΣ, and hence by
rule (36.3a) we obtain ν Σ { @ a ∥µ } 7−→ν Σ { e0 ∥µ }.
2. ν Σ { e0 ∥µ0 ⊗a ,→• } loops. By applying rule (36.6b) we obtain ν Σ { @ a ∥µ } loops.
3. ν Σ { e0 ∥µ0 ⊗a ,→• } 7−→ν Σ′ { e′
0 ∥µ′
0 ⊗a ,→• }. By applying rule (36.3b) we obtain
ν Σ { @ a ∥µ ⊗a ,→e0 } 7−→ν Σ′ { @ a ∥µ′ ⊗a ,→e′
0 }.
36.3
FPC By-Need
The language LFPC is FPC but with a by-need dynamics. For example, the dynamics of product
types in LFPC is given by the following rules:
⟨@ a1, @ a2⟩valΣ,a1~τ1,a2~τ2
(36.7a)





ν Σ { ⟨e1, e2⟩∥µ }
7−→
ν Σ, a1 ~ τ1, a2 ~ τ2 { ⟨@ a1, @ a2⟩∥µ ⊗a1 ,→e1 ⊗a2 ,→e2 }





(36.7b)
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { e · l ∥µ } 7−→ν Σ′ { e′ · l ∥µ′ }
(36.7c)
ν Σ { e ∥µ } loops
ν Σ { e · l ∥µ } loops
(36.7d)

36.4 Suspension Types
335





ν Σ, a1 ~ τ1, a2 ~ τ2 { ⟨@ a1, @ a2⟩· l ∥µ }
7−→
ν Σ, a1 ~ τ1, a2 ~ τ2 { @ a1 ∥µ }





(36.7e)
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { e · r ∥µ } 7−→ν Σ′ { e′ · r ∥µ′ }
(36.7f)
ν Σ { e ∥µ } loops
ν Σ { e · r ∥µ } loops
(36.7g)





ν Σ, a1 ~ τ1, a2 ~ τ2 { ⟨@ a1, @ a2⟩· r ∥µ }
7−→
ν Σ, a1 ~ τ1, a2 ~ τ2 { @ a2 ∥µ }





(36.7h)
A pair is considered a value only if its arguments are symbols (rule (36.7a)), which are introduced
when the pair is created (rule (36.7b)). The ﬁrst and second projections evaluate to one or the other
symbol in the pair, inducing a demand for the value of that component (rules (36.7e) and (36.7h)).
Similar ideas can be used to give a by-need dynamics to sums and recursive types.
36.4
Suspension Types
The dynamics of LFPC outlined in the previous section imposes a by-need interpretation on every
type. A more ﬂexible approach is to isolate the machinery of by-need evaluation by introducing a
type τ susp of memoized computations, called suspensions, of a value of type τ to an eager variant
of FPC. Doing so allows the programmer to choose the extent to which a by-need dynamics is
imposed.
Informally, the type τ susp has as introduction form susp x : τ is e representing a suspended,
self-referential, computation, e, of type τ. It has as elimination form the operation force(e) that
evaluates the suspended computation presented by e, records the value in a memo table, and
returns that value as result. Using suspension types we can construct lazy types at will. For
example, the type of lazy pairs with components of type τ1 and τ2 is expressible as the type
τ1 susp × τ2 susp
and the type of by-need functions with domain τ1 and range τ2 is expressible as the type
τ1 susp ⇀τ2.
We may also express more complex combinations of eagerness and laziness, such as the type of
“lazy lists” consisting of computations that, when forced, evaluate either to the empty list, or a
non-empty list consisting of a natural number and another lazy list:
rec t is (unit + (nat × t)) susp.

336
36.4 Suspension Types
Contrast this preceding type with this one:
rec t is (unit + (nat × t susp)).
Values of the latter type are the empty list and a pair consisting of a natural number and a compu-
tation of another such value.
The language SFPC extends FPC with a type of suspensions:
Typ
τ
::=
susp(τ)
τ susp
suspension
Exp
e
::=
susp{τ}(x.e)
susp x : τ is e
delay
force(e)
force(e)
force
lcell[a]
lcell[a]
indirection
Suspended computations are potentially self-referential; the bound variable x refers to the suspen-
sion itself. The expression lcell[a] is a reference to the suspension named a.
The statics of SFPC is given using a judgment of the form Γ ⊢Σ e : τ, where Σ assigns types to
the names of suspensions. It is deﬁned by the following rules:
Γ, x : susp(τ) ⊢Σ e : τ
Γ ⊢Σ susp{τ}(x.e) : susp(τ)
(36.8a)
Γ ⊢Σ e : susp(τ)
Γ ⊢Σ force(e) : τ
(36.8b)
Γ ⊢Σ,a~τ lcell[a] : susp(τ)
(36.8c)
Rule (36.8a) checks that the expression, e, has type τ under the assumption that x, which stands
for the suspension itself, has type susp(τ).
The dynamics of SFPC is eager, with memoization conﬁned to the suspension type as de-
scribed by the following rules:
lcell[a] valΣ,a~τ
(36.9a)





ν Σ { susp{τ}(x.e) ∥µ }
7−→
ν Σ, a ~ τ { lcell[a] ∥µ ⊗a ,→[lcell[a]/x]e }





(36.9b)
ν Σ { e ∥µ } 7−→ν Σ′ { e′ ∥µ′ }
ν Σ { force(e) ∥µ } 7−→ν Σ′ { force(e′) ∥µ′ }
(36.9c)
e valΣ,a~τ





ν Σ, a ~ τ { force(lcell[a]) ∥µ ⊗a ,→e }
7−→
ν Σ, a ~ τ { e ∥µ ⊗a ,→e }





(36.9d)

36.5 Notes
337
ν Σ, a ~ τ { e ∥µ ⊗a ,→• }
7−→
ν Σ′, a ~ τ { e′ ∥µ′ ⊗a ,→• }





ν Σ, a ~ τ { force(lcell[a]) ∥µ ⊗a ,→e }
7−→
ν Σ′, a ~ τ { force(lcell[a]) ∥µ′ ⊗a ,→e′ }





(36.9e)
Rule (36.9a) speciﬁes that a reference to a suspension is a value. Rule (36.9b) speciﬁes that evalu-
ation of a delayed computation consists of allocating a fresh symbol for it in the memo table, and
returning a reference to that suspension. Rules (36.9c) to (36.9e) specify that demanding the value
of a suspension forces evaluation of the suspended computation, which is then stored in the memo
table and returned as the result.
36.5
Notes
The by-need dynamics given here is inspired by Ariola and Felleisen (1997), but with the differ-
ence that by-need cells are regarded as assignables, rather than variables. Doing so maintains the
principle that variables are given meaning by substitution. In contrast by-need cells are a form of
assignable to which at most one assignment is ever done.
Exercises
36.1. Recall from Chapter 20 that, under a lazy interpretation, the recursive type
rec t is [z ,→unit, s ,→t]
contains the “inﬁnite number” ω ≜fix x : nat is s(x). Contrast the behavior of ω under
the by-need interpretation given in this chapter with that by-name interpretation given in
Chapters 19 and 20.
36.2. In LFPC the putative recursive type of “lists” of natural numbers,
rec t is [nil ,→unit, cons ,→nat × t],
is, rather, the type of ﬁnite or inﬁnite streams of natural numbers. To prove this, exhibit the
stream of all natural numbers as an element of this type.
36.3. Complete the deﬁnition of LFPC by giving the by-need dynamics for unit, void, sum, and
recursive types.

338
36.5 Notes
36.4. LFPC can be interpreted into SFPC. Complete the following chart deﬁning the interpreta-
tion, bτ, of the type τ:
unit ≜. . .
τ1 × τ2 ≜. . .
void ≜. . .
τ1 + τ2 ≜. . .
rec t is τ ≜. . . .
Hint: Characterize the values of the lazy types in the left column, and express those values as
eager types in the right column, using suspensions where necessary.

Part XV
Parallelism


Chapter 37
Nested Parallelism
Parallel computation seeks to reduce the running times of programs by allowing many computa-
tions to be carried out simultaneously. For example, if we wish to add two numbers, each given by
a complex computation, we may consider evaluating the addends simultaneously, then comput-
ing their sum. The ability to exploit parallelism is limited by the dependencies among parts of a
program. Obviously, if one computation depends on the result of another, then we have no choice
but to execute them sequentially so that we may propagate the result of the ﬁrst to the second.
Consequently, the fewer dependencies among sub-computations, the greater the opportunities for
parallelism.
In this chapter we discuss the language PPCF, which is the extension of PCF with nested par-
allelism. Nested parallelism has a hierarchical structure arising from forking two (or more) parallel
computations, then joining these computations to combine their results before proceeding. Nested
parallelism is also known as fork-join parallelism. We will consider two forms of dynamics for
nested parallelism. The ﬁrst is a structural dynamics in which a single transition on a compound
expression may involve multiple transitions on its constituent expressions. The second is a cost dy-
namics (introduced in Chapter 7) that focuses attention on the sequential and parallel complexity
(also known as the work and the depth, or span) of a parallel program by associating a series-parallel
graph with each computation.
37.1
Binary Fork-Join
The syntax of PPCF extends that of PCF with the following construct:
Exp
e
::=
par(e1; e2; x1.x2.e)
par x1 = e1 and x2 = e2 in e
parallel let
The variables x1 and x2 are bound only within e, and not within e1 or e2, which ensures that they
are not mutually dependent and hence can be evaluated simultaneously. The variable bindings
represent a fork of two parallel computations e1 and e2, and the body e represents their join.

342
37.1 Binary Fork-Join
The static of PPCF enriches that of PCF with the following rule for parallel let:
Γ ⊢e1 : τ1
Γ ⊢e2 : τ2
Γ, x1 : τ1, x2 : τ2 ⊢e : τ
Γ ⊢par(e1; e2; x1.x2.e) : τ
(37.1)
The sequential structural dynamics of PPCF is deﬁned by a transition judgment of the form
e 7→seq e′ deﬁned by these rules:
e1 7→seq e′
1
par(e1; e2; x1.x2.e) 7→seq par(e′
1; e2; x1.x2.e)
(37.2a)
e1 val
e2 7→seq e′
2
par(e1; e2; x1.x2.e) 7→seq par(e1; e′
2; x1.x2.e)
(37.2b)
e1 val
e2 val
par(e1; e2; x1.x2.e) 7→seq [e1, e2/x1, x2]e
(37.2c)
The parallel structural dynamics of PPCF is given by a transition judgment of the form e 7→par e′,
deﬁned as follows:
e1 7→par e′
1
e2 7→par e′
2
par(e1; e2; x1.x2.e) 7→par par(e′
1; e′
2; x1.x2.e)
(37.3a)
e1 7→par e′
1
e2 val
par(e1; e2; x1.x2.e) 7→par par(e′
1; e2; x1.x2.e)
(37.3b)
e1 val
e2 7→par e′
2
par(e1; e2; x1.x2.e) 7→par par(e1; e′
2; x1.x2.e)
(37.3c)
e1 val
e2 val
par(e1; e2; x1.x2.e) 7→par [e1, e2/x1, x2]e
(37.3d)
The parallel dynamics abstracts away from any limitations on processing capacity; such limitations
are considered in Section 37.4.
The implicit parallelism theorem states that the sequential and the parallel dynamics coincide.
Consequently, we need never be concerned with the semantics of a parallel program (its meaning
is given by the sequential dynamics), but only with its efﬁciency. As a practical matter, this means
that a program can be developed on a sequential platform, even if it is meant to run on a parallel
platform, because the behavior is not affected by whether we execute it using a sequential or a
parallel dynamics. Because the sequential dynamics is deterministic (every expression has at most
one value), the implicit parallelism theorem implies that the parallel dynamics is also determin-
istic. For this reason the implicit parallelism theorem is also known as the deterministic parallelism
theorem. This terminology emphasizes the distinction between deterministic parallelism, the subject
of this chapter, from non-deterministic concurrency, the subject of Chapters 39 and 40.
A proof of the implicit parallelism theorem can be given by giving an evaluation dynamics
e ⇓v in the style of Chapter 7, and showing that
e 7→∗
par v
iff
e ⇓v
iff
e 7→∗
seq v

37.1 Binary Fork-Join
343
(where v is a closed expression such that v val). The most important rule of the evaluation dynam-
ics is for the evaluation of a parallel let:
e1 ⇓v1
e2 ⇓v2
[v1, v2/x1, x2]e ⇓v
par(e1; e2; x1.x2.e) ⇓v
(37.4)
The other rules are easily derived from the structural dynamics of PCF as in Chapter 7.
It is possible to show that the sequential dynamics of PPCF agrees with its evaluation dynam-
ics by extending the proof of Theorem 7.2.
Lemma 37.1. For all v val, e 7→∗seq v if, and only if, e ⇓v.
Proof. It sufﬁces to show that if e 7→seq e′ and e′ ⇓v, then e ⇓v, and that if e1 7→∗seq v1 and
e2 7→∗seq v2 and [v1, v2/x1, x2]e 7→∗seq v, then
par x1 = e1 and x2 = e2 in e 7→∗
seq v.
By a similar argument we may show that the parallel dynamics also agrees with the evaluation
dynamics, and hence with the sequential dynamics.
Lemma 37.2. For all v val, e 7→∗par v if, and only if, e ⇓v.
Proof. It sufﬁces to show that if e 7→par e′ and e′ ⇓v, then e ⇓v, and that if e1 7→∗par v1 and
e2 7→∗par v2 and [v1, v2/x1, x2]e 7→∗par v, then
par x1 = e1 and x2 = e2 in e 7→∗
par v.
The proof of the ﬁrst is by induction on the parallel dynamics. The proof of the second proceeds
by simultaneous induction on the derivations of e1 7→∗par v1 and e2 7→∗par v2. If e1 = v1 with v1 val
and e2 = v2 with v2 val, then the result follows immediately from the third premise. If e2 = v2
but e1 7→par e′
1 7→∗par v1, then by induction we have that par x1 = e′
1 and x2 = v2 in e 7→∗par v, and
hence the result follows by an application of rule (37.3b). The symmetric case follows similarly by
an application of rule (37.3c), and in the case that both e1 and e2 transition, the result follows by
induction and rule (37.3a).
Theorem 37.3 (Implicit Parallelism). The sequential and parallel dynamics coincide: for all v val, e 7→∗seq
v iff e 7→∗par v.
Proof. By Lemmas 37.1 and 37.2.
The implicit parallelism theorem states that parallelism does not affect the semantics of a pro-
gram, only the efﬁciency of its execution. Correctness is not affected by parallelism, only efﬁciency.

344
37.2 Cost Dynamics
37.2
Cost Dynamics
In this section we deﬁne a parallel cost dynamics that assigns a cost graph to the evaluation of a PPCF
expression. Cost graphs are deﬁned by the following grammar:
Cost
c
::=
0
zero cost
1
unit cost
c1 ⊗c2
parallel combination
c1 ⊕c2
sequential combination
A cost graph is a series-parallel ordered directed acyclic graph, with a designated source node and
sink node. For 0 the graph consists of one node and no edges, with the source and sink both being
the node itself. For 1 the graph consists of two nodes and one edge directed from the source to
the sink. For c1 ⊗c2, if g1 and g2 are the graphs of c1 and c2, respectively, then the graph has two
extra nodes, a source node with two edges to the source nodes of g1 and g2, and a sink node, with
edges from the sink nodes of g1 and g2 to it. The children of the source are ordered according to
the sequential evaluation order. Finally, for c1 ⊕c2, where g1 and g2 are the graphs of c1 and c2,
the graph has as source node the source of g1, as sink node the sink of g2, and an edge from the
sink of g1 to the source of g2.
The intuition behind a cost graph is that nodes represent subcomputations of an overall com-
putation, and edges represent sequentiality constraints stating that one computation depends on
the result of another, and hence cannot be started before the one on which it depends completes.
The product of two graphs represents parallelism opportunities in which there are no sequentiality
constraints between the two computations. The assignment of source and sink nodes reﬂects the
overhead of forking two parallel computations and joining them after they have both completed.
At the structural level, we note that only the root has no ancestors, and only the ﬁnal node of
the cost graph has no descendents. Interior nodes may have one or two descendents, the former
representing a sequential dependency, and the latter representing a fork point. Such nodes may
have one or two ancestors, the former corresponding to a sequential dependency and the latter
representing a join point.
We associate with each cost graph two numeric measures, the work, wk(c), and the depth, dp(c).
The work is deﬁned by the following equations:
wk(c) =









0
if c = 0
1
if c = 1
wk(c1) + wk(c2)
if c = c1 ⊗c2
wk(c1) + wk(c2)
if c = c1 ⊕c2
(37.5)
The depth is deﬁned by the following equations:
dp(c) =









0
if c = 0
1
if c = 1
max(dp(c1), dp(c2))
if c = c1 ⊗c2
dp(c1) + dp(c2)
if c = c1 ⊕c2
(37.6)

37.2 Cost Dynamics
345
Informally, the work of a cost graph determines the total number of computation steps repre-
sented by the cost graph, and thus corresponds to the sequential complexity of the computation.
The depth of the cost graph determines the critical path length, the length of the longest depen-
dency chain within the computation, which imposes a lower bound on the parallel complexity of a
computation. The critical path length is a lower bound on the number of steps required to com-
plete the computation.
In Chapter 7 we introduced cost dynamics to assign time complexity to computations. The proof
of Theorem 7.7 shows that e ⇓k v iff e 7−→k v. That is, the step complexity of an evaluation of e to
a value v is just the number of transitions required to derive e 7−→∗v. Here we use cost graphs
as the measure of complexity, then relate these cost graphs to the structural dynamics given in
Section 37.1.
The judgment e ⇓c v, where e is a closed expression, v is a closed value, and c is a cost graph
speciﬁes the cost dynamics. By deﬁnition we arrange that e ⇓0 e when e val. The cost assignment
for let is given by the following rule:
e1 ⇓c1 v1
e2 ⇓c2 v2
[v1, v2/x1, x2]e ⇓c v
par(e1; e2; x1.x2.e) ⇓(c1⊗c2)⊕1⊕c v
(37.7)
The cost assignment speciﬁes that, under ideal conditions, e1 and e2 are evaluated in parallel, and
that their results are passed to e. The cost of fork and join is implicit in the parallel combination
of costs, and assign unit cost to the substitution because we expect it to be implemented by a
constant-time mechanism for updating an environment. The cost dynamics of other language
constructs is speciﬁed in a similar way, using only sequential combination to isolate the source of
parallelism to the let construct.
Two simple facts about the cost dynamics are important to keep in mind. First, the cost assign-
ment does not inﬂuence the outcome.
Lemma 37.4. e ⇓v iff e ⇓c v for some c.
Proof. From right to left, erase the cost assignments to construct an evaluation derivation. From
left to right, decorate the evaluation derivations with costs as determined by the rules deﬁning the
cost dynamics.
Second, the cost of evaluating an expression is uniquely determined.
Lemma 37.5. If e ⇓c v and e ⇓c′ v, then c is c′.
Proof. By induction on the derivation of e ⇓c v.
The link between the cost dynamics and the structural dynamics is given by the following
theorem, which states that the work cost is the sequential complexity, and the depth cost is the
parallel complexity, of the computation.
Theorem 37.6. If e ⇓c v, then e 7→wseq v and e 7→dpar v, where w = wk(c) and d = dp(c). Conversely, if
e 7→wseq v, then there exists c such that e ⇓c v with wk(c) = w, and if e 7→dpar v′, then there exists c′ such
that e ⇓c′ v′ with dp(c′) = d.

346
37.2 Cost Dynamics
Proof. The ﬁrst part is proved by induction on the derivation of e ⇓c v, the interesting case being
rule (37.7). By induction we have e1 7→w1
seq v1, e2 7→w2
seq v2, and [v1, v2/x1, x2]e 7→wseq v, where
w1 = wk(c1), w2 = wk(c2), and w = wk(c). By pasting together derivations we get a derivation
par(e1; e2; x1.x2.e) 7→w1
seq par(v1; e2; x1.x2.e)
7→w2
seq par(v1; v2; x1.x2.e)
7→seq [v1, v2/x1, x2]e
7→w
seq v.
Noting that wk((c1 ⊗c2) ⊕1 ⊕c) = w1 + w2 + 1 + w completes the proof. Similarly, we have by
induction that e1 7→d1
par v1, e2 7→d2
par v2, and [v1, v2/x1, x2]e 7→dpar v, where d1 = dp(c1), d2 = dp(c2),
and d = dp(c). Assume, without loss of generality, that d1 ≤d2 (otherwise simply swap the roles
of d1 and d2 in what follows). We may paste together derivations as follows:
par(e1; e2; x1.x2.e) 7→d1
par par(v1; e′
2; x1.x2.e)
7→d2−d1
par
par(v1; v2; x1.x2.e)
7→par [v1, v2/x1, x2]e
7→d
par v.
Calculating dp((c1 ⊗c2) ⊕1 ⊕c) = max(d1, d2) + 1 + d completes the proof.
Turning to the second part, it sufﬁces to show that if e 7→seq e′ with e′ ⇓c′ v, then e ⇓c v with
wk(c) = wk(c′) + 1, and if e 7→par e′ with e′ ⇓c′ v, then e ⇓c v with dp(c) = dp(c′) + 1.
Suppose that e = par(e1; e2; x1.x2.e0) with e1 val and e2 val. Then e 7→seq e′, where e =
[e1, e2/x1, x2]e0 and there exists c′ such that e′ ⇓c′ v. But then e ⇓c v, where c = (0 ⊗0) ⊕1 ⊕c′,
and a simple calculation shows that wk(c) = wk(c′) + 1, as required. Similarly, e 7→par e′ for e′ as
above, and hence e ⇓c v for some c such that dp(c) = dp(c′) + 1, as required.
Suppose that e = par(e1; e2; x1.x2.e0) and e 7→seq e′, where e′ = par(e′
1; e2; x1.x2.e0) and e1 7→seq
e′
1.
From the assumption that e′ ⇓c′ v, we have by inversion that e′
1 ⇓c′
1 v1, e2 ⇓c′
2 v2, and
[v1, v2/x1, x2]e0 ⇓c′
0 v, with c′ = (c′
1 ⊗c′
2) ⊕1 ⊕c′
0. By induction there exists c1 such that wk(c1) =
1 + wk(c′
1) and e1 ⇓c1 v1. But then e ⇓c v, with c = (c1 ⊗c′
2) ⊕1 ⊕c′
0.
By a similar argument, suppose that e = par(e1; e2; x1.x2.e0) and e 7→par e′, where e′ = par(e′
1; e′
2; x1.x2.e0)
and e1 7→par e′
1, e2 7→par e′
2, and e′ ⇓c′ v. Then by inversion e′
1 ⇓c′
1 v1, e′
2 ⇓c′
2 v2, [v1, v2/x1, x2]e0 ⇓c0
v. But then e ⇓c v, where c = (c1 ⊗c2) ⊕1 ⊕c0, e1 ⇓c1 v1 with dp(c1) = 1 + dp(c′
1), e2 ⇓c2 v2 with
dp(c2) = 1 + dp(c′
2), and [v1, v2/x1, x2]e0 ⇓c0 v. Calculating, we get
dp(c) = max(dp(c′
1) + 1, dp(c′
2) + 1) + 1 + dp(c0)
= max(dp(c′
1), dp(c′
2)) + 1 + 1 + dp(c0)
= dp((c′
1 ⊗c′
2) ⊕1 ⊕c0) + 1
= dp(c′) + 1,
which completes the proof.

37.3 Multiple Fork-Join
347
Corollary 37.7. If e 7→wseq v and e 7→dpar v′, then v is v′ and e ⇓c v for some c such that wk(c) = w and
dp(c) = d.
37.3
Multiple Fork-Join
So far we have conﬁned attention to binary fork/join parallelism induced by the parallel let con-
struct. A generalizaton, called data parallelism, allows the simultaneous creation of any number of
tasks that compute on the components of a data structure. The main example is a sequence of val-
ues of a speciﬁed type. The primitive operations on sequences are a natural source of unbounded
parallelism. For example, we may consider a parallel map construct that applies a given function
to every element of a sequence simultaneously, forming a sequence of the results.
We will consider here a simple language of sequence operations to illustrate the main ideas.
Typ
τ
::=
seq(τ)
τ seq
sequence
Exp
e
::=
seq(e0, . . . ,en−1)
[e0, . . . ,en−1]
sequence
len(e)
|e|
size
sub(e1; e2)
e1[e2]
element
tab(x.e1; e2)
tab(x.e1; e2)
tabulate
map(x.e1; e2)
[e1 | x ∈e2]
map
cat(e1; e2)
cat(e1; e2)
concatenate
The expression seq(e0, . . . ,en−1) evaluates to an n-sequence whose elements are given by the ex-
pressions e0, . . . , en−1. The operation len(e) returns the number of elements in the sequence given
by e. The operation sub(e1; e2) retrieves the element of the sequence given by e1 at the index given
by e2. The tabulate operation, tab(x.e1; e2), yields the sequence of length given by e2 whose ith
element is given by [i/x]e1. The operation map(x.e1; e2) computes the sequence whose ith element
is given by [e/x]e1, where e is the ith element of the sequence given by e2. The operation cat(e1; e2)
concatenates two sequences of the same type.
The statics of these operations is given by the following typing rules:
Γ ⊢e0 : τ
. . .
Γ ⊢en−1 : τ
Γ ⊢seq(e0, . . . ,en−1) : seq(τ)
(37.8a)
Γ ⊢e : seq(τ)
Γ ⊢len(e) : nat
(37.8b)
Γ ⊢e1 : seq(τ)
Γ ⊢e2 : nat
Γ ⊢sub(e1; e2) : τ
(37.8c)
Γ, x : nat ⊢e1 : τ
Γ ⊢e2 : nat
Γ ⊢tab(x.e1; e2) : seq(τ)
(37.8d)
Γ ⊢e2 : seq(τ)
Γ, x : τ ⊢e1 : τ′
Γ ⊢map(x.e1; e2) : seq(τ′)
(37.8e)

348
37.4 Bounded Implementations
Γ ⊢e1 : seq(τ)
Γ ⊢e2 : seq(τ)
Γ ⊢cat(e1; e2) : seq(τ)
(37.8f)
The cost dynamics of these constructs is deﬁned by the following rules:
e0 ⇓c0 v0
. . .
en−1 ⇓cn−1 vn−1
seq(e0, . . . ,en−1) ⇓
Nn−1
i=0 ci seq(v0, . . . ,vn−1)
(37.9a)
e ⇓c seq(v0, . . . ,vn−1)
len(e) ⇓c⊕1 num[n]
(37.9b)
e1 ⇓c1 seq(v0, . . . ,vn−1)
e2 ⇓c2 num[i]
(0 ≤i < n)
sub(e1; e2) ⇓c1⊕c2⊕1 vi
(37.9c)
e2 ⇓c num[n]
[num[0]/x]e1 ⇓c0 v0
. . .
[num[n −1]/x]e1 ⇓cn−1 vn−1
tab(x.e1; e2) ⇓c⊕Nn−1
i=0 ci seq(v0, . . . ,vn−1)
(37.9d)
e2 ⇓c seq(v0, . . . ,vn−1)
[v0/x]e1 ⇓c0 v′
0
. . .
[vn−1/x]e1 ⇓cn−1 v′
n−1
map(x.e1; e2) ⇓c⊕Nn−1
i=0 ci seq(v′
0, . . . ,v′
n−1)
(37.9e)
e1 ⇓c1 seq(v0, . . . , vm−1)
e2 ⇓c2 seq(v′
0, . . . , v′
n−1)
cat(e1; e2) ⇓c1⊕c2⊕Nm+n
i=0 1 seq(v0, . . . , vm−1, v′
0, . . . , v′
n−1)
(37.9f)
The cost dynamics for sequence operations is validated by introducing a sequential and parallel
cost dynamics and extending the proof of Theorem 37.6 to cover this extension.
37.4
Bounded Implementations
Theorem 37.6 states that the cost dynamics accurately models the dynamics of the parallel let
construct, whether executed sequentially or in parallel. The theorem validates the cost dynamics
from the point of view of the dynamics of the language, and permits us to draw conclusions
about the asymptotic complexity of a parallel program that abstracts away from the limitations
imposed by a concrete implementation. Chief among these is the restriction to a ﬁxed number,
p > 0, of processors on which to schedule the workload. Besides limiting the available parallelism
this also imposes some synchronization overhead that must be taken into account. A bounded
implementation is one for which we may establish an asymptotic bound on the execution time once
these overheads are taken into account.
A bounded implementation must take account of the limitations and capabilities of the hard-
ware on which the program is run. Because we are only interested in asymptotic upper bounds,
it is convenient to formulate an abstract machine model, and to show that the primitives of the
language can be implemented on this model with guaranteed time (and space) bounds. One ex-
ample of such a model is the shared-memory multiprocessor, or SMP, model. The basic assumption

37.4 Bounded Implementations
349
of the SMP model is that there are some ﬁxed p > 0 processors coordinated by an interconnect
that permits constant-time access to any object in memory shared by all p processors.1 An SMP
is assumed to provide a constant-time synchronization primitive with which to control simulta-
neous access to a memory cell. There are a variety of such primitives, any of which are enough
to provide a parallel fetch-and-add instruction that allows each processor to get the current con-
tents of a memory cell and update it by adding a ﬁxed constant in a single atomic operation—the
interconnect serializes any simultaneous accesses by more than one processor.
Building a bounded implementation of parallelism involves two major tasks. First, we must
show that the primitives of the language can be implemented efﬁciently on the abstract machine
model. Second, we must show how to schedule the workload across the processors to minimize
execution time by maximizing parallelism. When working with a low-level machine model such
as an SMP, both tasks involve a fair bit of technical detail to show how to use low-level machine
instructions, including a synchronization primitive, to implement the language primitives and to
schedule the workload. Collecting these together, we may then give an asymptotic bound on the
time complexity of the implementation that relates the abstract cost of the computation to cost
of implementing the workload on a p-way multiprocessor. The prototypical result of this kind is
Brent’s Theorem.
Theorem 37.8. If e ⇓c v with wk(c) = w and dp(c) = d, then e can be evaluated on a p-processor SMP
in time O(max(w/p, d)).
The theorem tells us that we can never execute a program in fewer steps than its depth d and
that, at best, we can divide the work up evenly into w/p rounds of execution by the p processors.
Note that if p = 1 then the theorem establishes an upper bound of O(w) steps, the sequential
complexity of the computation. Moreover, if the work is proportional to the depth, then we are
unable to exploit parallelism, and the overall time is proportional to the work alone.
Theorem 37.8 motivates consideration of a useful ﬁgure of merit, the parallelizability ratio, which
is the ratio w/d of work to depth. If w/d ≫p, then the program is parallelizable, because then
w/p ≫d, and we may therefore reduce running time by using p processors at each step. If the
parallelizability ratio is a constant, then d will dominate w/p, and we will have little opportunity
to exploit parallelism to reduce running time. It is not known, in general, whether a problem
admits a parallelizable solution. The best we can say, on present knowledge, is that there are
algorithms for some problems that have a high degree of parallelizability, and there are problems
for which no such algorithm is known. It is a difﬁcult problem in complexity theory to analyze
which problems are parallelizable, and which are not.
Proving Brent’s Theorem for an SMP would take us much too far aﬁeld for the present pur-
poses. Instead we shall prove a Brent-type Theorem for an abstract machine, the P machine. The
machine is unrealistic in that it is deﬁned at a very high level of abstraction. But it is designed
to match well the cost semantics given earlier in this chapter. In particular, there are mechanisms
that account for both sequential and parallel dependencies in a computation.
At the highest level, the state of the P machine consists of a global task graph whose struc-
ture corresponds to a “diagonal cut” through the cost graph of the overall computation. Nodes
1A slightly weaker assumption is that each access may require up to lg p time to account for the overhead of synchro-
nization, but we shall neglect this reﬁnement in the present, simpliﬁed account.

350
37.4 Bounded Implementations
immediately above the cut are eligible to be executed, higher ancestors having already been com-
pleted, and whose immediate descendents are waiting for their ancestors to complete. Further
descendents in the full task graph are tasks yet to be created, once the immediate descendents
are ﬁnished. The P machine discards completed tasks, and future tasks beyond the immediate
dependents are only created as execution proceeds. Thus it is only those nodes next to the cut line
through the cost graph that are represented in the P machine state.
The global state of the P machine is a conﬁguration of the form ν Σ { µ }, where Σ is degenerated
to just a ﬁnite set of (pairwise distinct) task names and µ is a ﬁnite mapping the task names in Σ to
local states, representing the state of an individual task. A local state is either a closed PCF expres-
sion, or one of two special join points that implement the sequential and parallel dependencies of a
task on one or two ancestors, respectively.2 Thus, when expanded out, a global state has the form
ν a1, . . . , an { a1 ,→s1 ⊗. . . ⊗an ,→sn },
where n ≥1, and each si is a local state. The ordering of the tasks in a state, like the order of
declarations in the signature, is not signiﬁcant.
A P machine state transition has the form ν Σ { µ } 7−→ν Σ′ { µ′ }. There are two forms of
such transitions, the global and the local. A global step selects as many tasks as are available, up
to a pre-speciﬁed parameter p > 0, which represents the number of processors available at each
round. (Such a scheduler is greedy in the sense that it never fails to execute an available task, up
to the speciﬁed limit for each round.) A task is ﬁnished if it consists of a closed PCF value, or
is a join point whose dependents are not yet ﬁnished; otherwise a task is available, or ready. A
ready task is always capable of taking a local step consisting of either a step of PCF, expressed in
the setting of the P machine, or a synchronization step that manages the join-point logic. Because
the P machine employs a greedy scheduler, it must complete execution in time proportional to
max(w/p, d) steps by doing up to p steps of work at a time, insofar as it is possible within the
limits of the depth of the computation. We thus get a Brent-type Theorem for the abstract machine
that illustrates more sophisticated Brent-type Theorems for other models, such as the PRAM, that
are used in the analysis of parallel algorithms.
The local transitions of the P machine corresponding to the steps of PCF itself are illustrated
by the following example rules for application; the others follow a similar pattern.3
¬(e1 val)
ν a { a ,→e1(e2) } 7−→loc ν a a1 { a ,→join[a1](x1.x1(e2)) ⊗a1 ,→e1 }
(37.10a)
e1 val
ν a { a ,→e1(e2) } 7−→loc ν a a2 { a ,→join[a2](x2.e1(x2)) ⊗a2 ,→e2 }
(37.10b)
e1 val
e2 val
ν a1 a2 { a1 ,→join[a2](x2.e1(x2)) ⊗a2 ,→e2 } 7−→loc ν a1 { a1 ,→[e2/x2]e1 }
(37.10c)
2The use of join points for each sequential dependency is proﬂigate, but aligns the machine with the cost semantics.
Realistically, individual tasks manage sequential dependencies without synchronization, by using local control stacks as in
Chapter 28.
3Here and elsewhere typing information is omitted from Σ, because it is not relevant to the dynamics.

37.4 Bounded Implementations
351
e2 val
ν a { a ,→(λ (x : τ2) e)(e2) } 7−→loc ν a { a ,→[e2/x]e }
(37.10d)
Rules (37.10a) and (37.10b) create create tasks for the evaluation of the function and argument of
an expression. Rule (37.10c) propagates the result of evaluation of the function or argument of an
application to the appropriate application expression. This rule mediates between the ﬁrst two
rules and Rule (37.10d), which effects a β-reduction in-place.
The local transitions of the P machine corresponding to binary fork and join are as follows:





ν a { a ,→par(e1; e2; x1.x2.e) }
7−→loc
ν a1, a2, a { a1 ,→e1 ⊗a2 ,→e2 ⊗a ,→join[a1; a2](x1; x2.e) }





(37.11a)
e1 val
e2 val





ν a1, a2, a { a1 ,→e1 ⊗a2 ,→e2 ⊗a ,→join[a1; a2](x1; x2.e) }
7−→loc
ν a { a ,→[e1, e2/x1, x2]e }





(37.11b)
Rule (37.11a) creates two parallel tasks on which the executing task depends. The expression
join[a1; a2](x1; x2.e) is blocked on tasks a1 and a2, so that no local step applies to it. Rule (37.11b)
synchronizes a task with the tasks on which it depends once their execution has completed; those
tasks are no longer required, and are eliminated from the state.
Each global transition is the simultaneous execution of one step of computation on as many as
p ≥1 processors.
ν Σ1 a1 { µ1 ⊗a1 ,→s1 } 7−→loc ν Σ′
1 a1 { µ′
1 ⊗a1 ,→s′
1 }
. . .
ν Σn an { µn ⊗an ,→sn } 7−→loc ν Σ′
n an { µ′
n ⊗an ,→s′
n }





ν Σ0 Σ1 a1 . . . Σn an { µ0 ⊗µ1 ⊗a1 ,→s1 ⊗. . . ⊗µn ⊗an ,→sn }
7−→glo
ν Σ0 Σ′
1 a1 . . . Σ′
n an { µ0 ⊗µ′
1 ⊗a1 ,→s′
1 ⊗. . . ⊗µ′
n ⊗an ,→s′
n }





(37.12)
At each global step some number 1 ≤n ≤p of ready tasks are scheduled for execution, where n is
maximal among the number of ready tasks. Because no two distinct tasks may depend on the same
task, we may partition the n tasks so that each scheduled task is grouped with the tasks on which
it depends as necessary for any local join step. Any local fork step adds two fresh tasks to the state
resulting from the global transition; any local join step eliminates two tasks whose execution has
completed. A subtle point is that it is implicit in our name binding conventions that the names
of any created tasks are globally unique, even though they are locally created. In implementation
terms this requires a synchronization step among the processors to ensure that task names are not
accidentally reused among the parallel tasks.

352
37.5 Scheduling
The proof of a Brent-type Theorem for the P machine is now obvious. We need only ensure
that the parameter n of Rule (37.12) is chosen as large as possible at each step, limited only by
the parameter p and the number of ready tasks. A scheduler with this property is greedy; it never
allows a processor to go idle if work remains to be done. Consequently, if there are always p
available tasks at each global step, then the evaluation will complete in w/p steps, where w is
the work complexity of the program. If, at some stage, fewer than p tasks are available, then
performance degrades according to the sequential dependencies among the sub-computations. In
the limiting case the P machine must take at least d steps, where d is the depth of the computation.
37.5
Scheduling
The global transition relation of the P machine deﬁned in Section 37.4 affords wide latitude in the
choice of tasks that are advanced by taking a local transition. Doing so abstracts from implemen-
tation details that are irrelevant to the proof of the Brent-type Theorem given later in that section,
the only requirement being that the number of tasks chosen be as large as possible up to the spec-
iﬁed bound p, representing the number of available processors. When taking into account factors
not considered here, it is necessary to specify the scheduling policy more precisely—for example,
different scheduling policies may have asymptotically different space requirements. The overall
idea is to consider scheduling a computation on p processors as a p-way parallel traversal of its cost
graph, visiting up to p nodes at a time in an order consistent with the dependency ordering. In
this section we will consider one such traversal, p-way parallel depth-ﬁrst-search, or p-DFS, which
specializes to the familiar depth-ﬁrst traversal in the case that p = 1.
Recall that the depth ﬁrst-search of a directed graph maintain a stack of unvisited nodes, which
is initialized with the start node. At each round, a node is popped from the stack and visited, and
then its unvisited children are pushed on the stack (in reverse order in the case of ordered graphs),
completing that round. The traversal terminates when the stack is empty. When viewed as a
scheduling strategy, visiting a node of a cost graph consists of scheduling the work associated
with that node on a processor. The job of such as scheduler is to do the work of the computation in
depth-ﬁrst order, visiting the children of a node from left to right, consistently with the sequential
dynamics (which would, in particular, treat a parallel binding as two sequential bindings). Notice
that because a cost graph is directed acyclic, there are no “back edges” arising from the traversal,
and because it is series-parallel in structure, there are no “cross edges”. Thus, all children of a node
are unvisited, and no task is considered more than once.
Although evocative, viewing scheduling as graph traversal invites one to imagine that the
cost graph is given explicitly as a data structure, which is not at all the case. Instead the graph
is created dynamically as the sub-computations are executed. At each round the computation
associated with a node may complete (when it has achieved its value), continue (when more work
is yet to be done), or fork (when it generates parallel sub-computations with a speciﬁed join point).
Once a computation has completed and its value has been passed to the associated join point, its
node in the cost graph is discarded. Furthermore, the children of a node only come into existence
as a result of its execution, according to whether it completes (no children), continues (one child),
or forks (two children). Thus one may envision that the cost graph “exists” as a cut through the
abstract cost graph representing pending tasks that have not yet been activated by the traversal.

37.5 Scheduling
353
A parallel depth-ﬁrst search works much the same way, except that as many as p nodes are
visited at each round, constrained only by the presence of unvisited (yet-to-be-scheduled) nodes.
One might naively think that this simply means popping up to p nodes from the stack on each
round, visiting them all simultaneously, and pushing their dependents on the stack in reverse or-
der, just as for conventional depth-ﬁrst search. But a moment’s thought reveals that this is not
correct. Because the cost graphs are ordered, the visited nodes form a sequence determined by
the left-to-right ordering of the children of a node. If a node completes, it has no children and is
removed from its position in the sequence in the next round. If a node continues, it has one child
that occupies the same relative position as its parent in the next round. And if a node forks two
children, they are inserted into the sequence after the predecessor, and immediately prior to that
node, related to each other by the left-to-right ordering of the children. The task associated to the
visited node itself becomes the join point of the immediately preceding pair of tasks, with which
it will synchronize when they complete. Thus the visited sequence of k ≤p nodes becomes, on
the next round, anywhere from 0 (if all nodes completes) to 3 × k nodes (if each node forks). These
are placed into consideration, in the speciﬁed order, for the next round to ensure that they are pro-
cessed in depth-ﬁrst order. Importantly, the data structure maintaining the unvisited nodes of the
graph is not a simple pushdown stack, because of the “in-place” replacement of each visited node
by zero, one, or two nodes in between its predecessor and successor in the sequential ordering of
the visited nodes.
Consider a variant of the P machine in which the order of the tasks is signiﬁcant. A task is
ﬁnished if it is a value, blocked if it is a join, and ready otherwise. Local transitions remain the same
as in Section 37.4, bearing in mind that the ordering is signiﬁcant. A global transition, however,
consists of making a local transition on each of the ﬁrst k ≤p ready tasks.4 After this selection the
global state is depicted as follows:
ν Σ0 a1 Σ1 . . . ak Σk Σ { µ0 ⊗a1 ,→e1 ⊗µ1 ⊗. . . ak ,→ek ⊗µ }
where each µi consists of ﬁnished or blocked tasks, and each ei is ready. A schedule is greedy If
k < p only when no task in µ is ready.
After a local transition is made on each of the k selected tasks, the resulting global state has the
form
ν Σ0Σ′
1 a1 Σ1 . . . Σ′
k ak Σk Σ { µ0 ⊗µ′
1 ⊗a1 ,→e′
1 ⊗µ1 ⊗. . . µ′
k ⊗ak ,→e′
k ⊗µ }
where each µ′
i represents the newly created task(s) of the local transition on task ai ,→ei, and each
e′
i is the expression resulting from the transition on that task. Next, all possible synchronizations
are made by replacing each occurrence of an adjacent triple of the form
ai,1 ,→e1 ⊗ai,2 ,→e2 ⊗ai ,→join[ai,1; ai,2](x1; x2.e)
(with e1 and e2 ﬁnished) by the task ai ,→[e1, e2/x1, x2]e. Doing so propagates the values of tasks
ai,1 and ai,2 to the join point, enabling the computation to continue. The two ﬁnished tasks are
removed from the state, and the join point is no longer blocked.
4Thus the local transition given by Rule (37.11b) is never applicable; the dynamics of joins will be described shortly.

354
37.6 Notes
37.6
Notes
Parallelism is a high-level programming concept that increases efﬁciency by carrying out multiple
computations simultaneously when they are mutually independent. Parallelism does not change
the meaning of a program, but only how fast it is executed. The cost semantics speciﬁes the num-
ber of steps required to execute a program sequentially and with maximal parallelism. A bounded
implementation provides a bound on the number of steps when the number of processors is lim-
ited, limiting the degree of parallelism that can be realized. This formulation of parallelism was
introduced by Blelloch (1990). The concept of a cost semantics and the idea of a bounded imple-
mentation studied here are derived from Blelloch and Greiner (1995, 1996).
Exercises
37.1. Consider extending PPCF with exceptions, as described in Chapter 29, under the assump-
tion that τexn has at least two exception values. Give a sequential and a parallel structural
dynamics to parallel let in such a way that determinacy continues to hold.
37.2. Give a matching cost semantics to PPCF extended with exceptions (descibed in Exercise 37.1)
by inductively deﬁning the following two judgments:
(a) e ⇓c v, stating that e evaluates to value v with cost c;
(b) e ⇑c v, stating that e raises the value v with cost c.
The analog of Theorem 37.6 remains valid for the dynamics. In particular, if e ⇑c v, then both
e 7→wseq raise(v), where w = wk(c), and e 7→dpar raise(v), where d = dp(c), and conversely.
37.3. Extend the P machine to admit exceptions to match your solution to Exercise 37.2. Argue
that the revised machine supports a Brent-type validation of the cost semantics.
37.4. Another way to express the dynamics of PPCF enriched with exceptions is by rewriting
par(e1; e2; x1.x2.e) into another such parallel binding, par(e′
1; e′
2; x′
1.x′
2.e′), which implements
the correct dynamics to ensure determinacy. Hint: Extend XPCF with sums (Chapter 11),
using them to record the outcome of each parallel sub-computation (e′
1 derived from e1, and
e′
2 derived from e2), then check the outcomes (e′ derived from e) in such a way to ensure
determinacy.

Chapter 38
Futures and Speculations
A future is a computation that is performed before it is value is needed. Like a suspension, a future
represents a value that is to be determined later. Unlike a suspension, a future is always evaluated,
regardless of whether its value is required. In a sequential setting futures are of little interest; a
future of type τ is just an expression of type τ. In a parallel setting, however, futures are of interest
because they provide a means of initiating a parallel computation whose result is not needed until
later, by which time it will have been completed.
The prototypical example of the use of futures is to implementing pipelining, a method for over-
lapping the stages of a multistage computation to the fullest extent possible. Pipelining minimizes
the latency caused by one stage waiting for a previous stage to complete by allowing the two
stages to execute in parallel until an explicit dependency arises. Ideally, the computation of the
result of an earlier stage is ﬁnished by the time a later stage needs it. At worst the later stage is
delayed until the earlier stage completes, incurring what is known as a pipeline stall.
A speculation is a delayed computation whose result might be needed for the overall compu-
tation to ﬁnish. The dynamics for speculations executes suspended computations in parallel with
the main thread of computation, without regard to whether the value of the speculation is needed
by the main thread. If the value of the speculation is needed, then such a dynamics pays off, but if
not, the effort to compute it is wasted.
Futures are work efﬁcient in that the overall work done by a computation involving futures is no
more than the work done by a sequential execution. Speculations, in contrast, are work inefﬁcient
in that speculative execution might be in vain—the overall computation may involve more steps
than the work needed to compute the result. For this reason speculation is a risky strategy for
exploiting parallelism. It can make use available resources, but perhaps only at the expense of
doing more work than necessary!

356
38.1 Futures
38.1
Futures
The syntax of futures is given by the following grammar:
Typ
τ
::=
fut(τ)
τ fut
future
Exp
e
::=
fut(e)
fut(e)
future
fsyn(e)
fsyn(e)
synchronize
fcell[a]
fcell[a]
indirection
The type τ fut is the type of futures of type τ. Futures are introduced by the expression fut(e),
which schedules e for evaluation and returns a reference to it. Futures are eliminated by the ex-
pression fsyn(e), which synchronizes with the future referred to by e, returning its value. Indirect
references to future values are represented by fcell[a], indicating a future value to be stored at a.
38.1.1
Statics
The statics of futures is given by the following rules:
Γ ⊢e : τ
Γ ⊢fut(e) : fut(τ)
(38.1a)
Γ ⊢e : fut(τ)
Γ ⊢fsyn(e) : τ
(38.1b)
These rules are unsurprising, because futures add no new capabilities to the language beyond
providing an opportunity for parallel evaluation.
38.1.2
Sequential Dynamics
The sequential dynamics of futures is easily deﬁned. Futures are evaluated eagerly; synchroniza-
tion returns the value of the future.
e val
fut(e) val
(38.2a)
e 7−→e′
fut(e) 7−→fut(e′)
(38.2b)
e 7−→e′
fsyn(e) 7−→fsyn(e′)
(38.2c)
e val
fsyn(fut(e)) 7−→e
(38.2d)
Under a sequential dynamics futures have little purpose: they introduce a pointless level of
indirection.

38.2 Speculations
357
38.2
Speculations
The syntax of (non-recursive) speculations is given by the following grammar:1
Typ
τ
::=
spec(τ)
τ spec
speculation
Exp
e
::=
spec(e)
spec(e)
speculate
ssyn(e)
ssyn(e)
synchronize
scell[a]
scell[a]
indirection
The type τ spec is the type of speculations of type τ. The introduction form spec(e) creates a
computation that can be speculatively evaluated, and the elimination form ssyn(e) synchronizes
with a speculation. A reference to the result of a speculative computation stored at a is written
scell[a].
38.2.1
Statics
The statics of speculations is given by the following rules:
Γ ⊢e : τ
Γ ⊢spec(e) : spec(τ)
(38.3a)
Γ ⊢e : spec(τ)
Γ ⊢ssyn(e) : τ
(38.3b)
Thus, the statics for speculations as given by rules (38.3) is equivalent to the statics for futures
given by rules (38.1).
38.2.2
Sequential Dynamics
The deﬁnition of the sequential dynamics of speculations is like that of futures, except that specu-
lations are values.
spec(e) val
(38.4a)
e 7−→e′
ssyn(e) 7−→ssyn(e′)
(38.4b)
ssyn(spec(e)) 7−→e
(38.4c)
Under a sequential dynamics speculations are simply a re-formulation of suspensions.
1We conﬁne ourselves to the non-recursive case to ease the comparison with futures.

358
38.3 Parallel Dynamics
38.3
Parallel Dynamics
Futures are only interesting insofar as they admit a parallel dynamics that allows the computation
of the future to go ahead concurrently with some other computation. In this section we give a
parallel dynamics of futures and speculation in which the creation, execution, and synchronization
of tasks is made explicit. The parallel dynamics of futures and speculations is identical, except for
the termination condition. Whereas futures require that all tasks are completed before termination,
speculations may be abandoned before they are completed. For the sake of concision we will
give the parallel dynamics of futures, remarking only where alterations are made for the parallel
dynamics of speculations.
The parallel dynamics of futures relies on a modest extension to the language given in Sec-
tion 38.1 to introduce names for tasks. Let Σ be a ﬁnite mapping assigning types to names. As
mentioned earlier, the expression fcell[a] is a value referring to the outcome of task a. The statics
of this expression is given by the following rule:2
Γ ⊢Σ,a~τ fcell[a] : fut(τ)
(38.5)
Rules (38.1) carry over in the obvious way with Σ recording the types of the task names.
States of the parallel dynamics have the form ν Σ { e ∥µ }, where e is the focus of evaluation,
and µ records the active parallel futures (or speculations). Formally, µ is a ﬁnite mapping assigning
expressions to the task names declared in Σ. A state is well-formed according to the following rule:
⊢Σ e : τ
(∀a ∈dom(Σ)) ⊢Σ µ(a) : Σ(a)
ν Σ { e ∥µ } ok
(38.6)
As discussed in Chapter 35 this rule admits self-referential and mutually referential futures. A
more reﬁned condition could as well be given that avoids circularities; we leave this as an exercise
for the reader.
The parallel dynamics is divided into two phases, the local phase, which deﬁnes the basic steps
of evaluation of an expression, and the global phase, which executes all possible local steps in
parallel. The local dynamics of futures is deﬁned by the following rules:3
fcell[a] valΣ,a~τ
(38.7a)
ν Σ { fut(e) ∥µ } 7−→loc ν Σ, a ~ τ { fcell[a] ∥µ ⊗a ,→e }
(38.7b)
ν Σ { e ∥µ } 7−→loc ν Σ′ { e′ ∥µ′ }
ν Σ { fsyn(e) ∥µ } 7−→loc ν Σ′ { fsyn(e′) ∥µ′ }
(38.7c)
2A similar rule applies to scell[a] in the case of speculations.
3These rules are augmented by a reformulation of the dynamics of the other constructs of the language phrased in terms
of the present notion of state.

38.3 Parallel Dynamics
359
e′ valΣ,a~τ





ν Σ, a ~ τ { fsyn(fcell[a]) ∥µ ⊗a ,→e′ }
7−→loc
ν Σ, a ~ τ { e′ ∥µ ⊗a ,→e′ }





(38.7d)
Rule (38.7b) activates a future named a executing the expression e and returns a reference to it.
Rule (38.7d) synchronizes with a future whose value has been determined. Note that a local tran-
sition always has the form
ν Σ { e ∥µ } 7−→loc ν Σ Σ′ { e′ ∥µ ⊗µ′ }
where Σ′ is either empty or declares the type of a single symbol, and µ′ is either empty or of the
form a ,→e′ for some expression e′.
A global step of the parallel dynamics consists of at most one local step for the focal expression
and one local step for each of up to p futures, where p > 0 is a ﬁxed parameter representing the
number of processors.
µ = µ0 ⊗a1 ,→e1 ⊗. . . ⊗an ,→en
µ′′ = µ0 ⊗a1 ,→e′
1 ⊗. . . ⊗an ,→e′
n
ν Σ { e ∥µ } 7−→0,1
loc ν Σ Σ′ { e′ ∥µ ⊗µ′ }
(∀1 ≤i ≤n ≤p)
ν Σ { ei ∥µ } 7−→loc ν Σ Σ′
i { e′
i ∥µ ⊗µ′
i }





ν Σ { e ∥µ }
7−→glo
ν Σ Σ′ Σ′
1 . . . Σ′
n { e′ ∥µ′′ ⊗µ′ ⊗µ′
1 ⊗. . . ⊗µ′
n }





(38.8a)
Rule (38.8a) allows the focus expression to take either zero or one steps because it might be blocked
awaiting the completion of evaluation of a parallel future (or synchronizing with a speculation).
The futures allocated by the local steps of execution are consolidated in the result of the global
step. We assume without loss of generality that the names of the new futures in each local step
are pairwise disjoint so that the combination makes sense. In implementation terms satisfying this
disjointness assumption means that the processors must synchronize their access to memory.
The initial state of a computation, for futures or speculations, is deﬁned by the rule
ν ∅{ e ∥∅} initial
(38.9)
For futures a state is ﬁnal only if the focus and all parallel futures have completed evaluation:
e valΣ
µ valΣ
ν Σ { e ∥µ } ﬁnal
(38.10a)
(∀a ∈dom(Σ)) µ(a) valΣ
µ valΣ
(38.10b)

360
38.4 Pipelining With Futures
For speculations a state is ﬁnal only if the focus is a value, regardless of whether any other specu-
lations have completed:
e valΣ
ν Σ { e ∥µ } ﬁnal
(38.11)
All futures must terminate to ensure that the work performed in parallel matches that performed
sequentially; no future is created whose value is not needed according to the sequential semantics.
In contrast, speculations can be abandoned when their values are not needed.
38.4
Pipelining With Futures
Pipelining is an interesting example of the use of parallel futures. Consider a situation in which a
producer builds a list whose elements represent units of work, and a consumer traverses the work
list and acts on each element of that list. The elements of the work list can be thought of as “in-
structions” to the consumer, which maps a function over that list to carry out those instructions.
An obvious sequential implementation ﬁrst builds the work list, then traverses it to perform the
work indicated by the list. This strategy works well provided that the elements of the list can be
produced quickly, but if each element needs a lot of computation, it would be preferable to over-
lap production of the next list element with execution of the previous unit of work, which can be
programmed using futures.
Let flist be the recursive type rec t is unit + (nat × t fut), whose elements are nil, deﬁned
to be fold(l · ⟨⟩), and cons(e1,e2), deﬁned to be fold(r · ⟨e1, fut(e2)⟩). The producer is a recursive
function that generates a value of type flist:
fix produce : (nat →nat opt) →nat →flist is
λ f. λ i.
case f(i) {
null ,→nil
| just x ,→cons(x, fut (produce f (i+1)))
}
On each iteration the producer generates a parallel future to produce the tail. The future continues
to execute after the producer returns so that its evaluation overlaps with subsequent computation.
The consumer folds an operation over the work list as follows:
fix consume : ((nat×nat)→nat) →nat →flist →nat is
λ g. λ a. λ xs.
case xs {
nil ,→a
| cons (x, xs) ,→consume g (g (x, a)) (fsyn xs)
}
The consumer synchronizes with the tail of the work list just at the point where it makes a recursive
call and hence needs the head element of the tail to continue processing. At this point the consumer
will block, if necessary, to await computation of the tail before continuing the recursion.

38.5 Notes
361
Speculations arise naturally in lazy languages. But although they provide opportunities for
parallelism, they are not, in general, work efﬁcient: a speculation might be evaluated even though
its value is never needed. An alternative is to combine suspensions (see Chapter 36) with futures
so that the programmer may specify which suspensions ought to be evaluated in parallel. The
notion of a spark is designed to achieve this. A spark evaluates a computation in parallel only for
its effect on suspensions that are likely to be needed later. Speciﬁcally, we may deﬁne
spark(e1; e2) ≜letfut be force(e1) in e2,
where e1 : τ1 susp and e2 : τ2.4 The expression force(e1) is evaluated in parallel, forcing the
evaluation of e1, in hopes that it will have completed evaluation before its value is needed by e2.
As an example, consider the type strm of streams of numbers deﬁned by the recursive type
rec t is (unit + (nat × t)) susp. Elements of this type are suspended computations that, when
forced, either signals the end of stream, or produces a number and another such stream. Suppose
that s is such a stream, and assume that we know, for reasons of its construction, that it is ﬁnite.
We wish to compute map( f )(s) for some function f, and to overlap this computation with the
production of the stream elements. We will make use of a function mapforce that forces successive
elements of the input stream, but yields no useful output. The computation
letfut be map(force)(s) in map( f )(s)
forces the elements of the stream in parallel with the computation of map( f )(s), with the intention
that all suspensions in s are forced before their values are needed by the main computation.
38.5
Notes
Futures were introduced by Friedman and Wise (1976), and featured in the MultiLisp language
(Halstead, 1985) for parallel programming. A similar concept is proposed by Arvind et al. (1986)
under the name “I-structures.” The formulation given here is derived from Greiner and Blelloch
(1999). Sparks were introduced by Trinder et al. (1998).
Exercises
38.1. Use futures to deﬁne letfut x be e1 in e2, a parallel let in which e2 is evaluated in parallel
with e1 up to the point that e2 needs the value of x.
38.2. Use futures to encode binary nested parallelism by giving a deﬁnition of par(e1; e2; x1.x2.e).
Hint: Only one future is needed if you are careful.
4The expression evaluates e1 simultaneously with e2, up to the point that the value of x is needed. Its deﬁnition in terms
of futures is the subject of Exercise 38.1.

362
38.5 Notes

Part XVI
Concurrency and Distribution


Chapter 39
Process Calculus
So far we have studied the statics and dynamics of programs in isolation, without regard to their
interaction with each other or the world. But to extend this analysis to even the most rudimentary
forms of input and output requires that we consider external agents that interact with the program.
After all, the purpose of a computer is, ultimately, to interact with a person!
To extend our investigations to interactive systems, we develop a small language, called PiC,
which is derived from a variety of similar formalisms, called process calculi, that give an abstract
formulation of interaction among independent agents. The development will be carried out in
stages, starting with simple action models, then extending to interacting concurrent processes,
and ﬁnally to synchronous and asynchronous communication. The calculus consists of two main
syntactic categories, processes and events. The basic form of process is one that awaits the arrival
of an event. Processes are formed by concurrent composition, replication, and declaration of a
channel. The basic forms of event are signaling on a channel and querying a channel; these are
later generalized to sending and receiving data on a channel. Events are formed from send and
receive events by ﬁnite non-deterministic choice.
39.1
Actions and Events
Concurrent interaction is based on events, which specify the actions that a process can take. Two
processes interact by taking two complementary actions, a signal and a query on a channel. The
processes synchronize when one signals on a channel that the other is querying, after which they
continue to interact with other processes.
To begin with we will focus on sequential processes, which simply await the arrival of one of
several possible actions, known as an event.
Proc
P
::=
await(E)
$ E
synchronize
Evt
E
::=
null
0
null
or(E1; E2)
E1 + E2
choice
que[a](P)
?a;P
query
sig[a](P)
!a;P
signal

366
39.1 Actions and Events
The variable a ranges over symbols serving as channels that mediate communication among the
processes.
We will not distinguish between events that differ only up to structural congruence, which is
deﬁned to be the strongest equivalence relation closed under these rules:
E ≡E′
$ E ≡$ E′
(39.1a)
E1 ≡E′
1
E2 ≡E′
2
E1 + E2 ≡E′
1 + E′
2
(39.1b)
P ≡P′
?a;P ≡?a;P′
(39.1c)
P ≡P′
!a;P ≡!a;P′
(39.1d)
E + 0 ≡E
(39.1e)
E1 + E2 ≡E2 + E1
(39.1f)
E1 + (E2 + E3) ≡(E1 + E2) + E3
(39.1g)
Imposing structural congruence on sequential processes enables us to think of an event as having
the form
!a;P1 + . . . + ?a;Q1 + . . .
consisting of a sum of signal and query events, with the sum of no events being the null event 0.
An illustrative example of Milner’s is a simple vending machine that may take in a 2p coin,
then optionally either allow a request for a cup of tea, or take another 2p coin, then allow a request
for a cup of coffee.
V = $ (?2p;$ (!tea;V + ?2p;$ (!cof;V)))
(39.2)
As the example indicates, we allow recursive deﬁnitions of processes, with the understanding that
a deﬁned identiﬁer may always be replaced with its deﬁnition wherever it occurs. (Later we will
show how to avoid reliance on recursive deﬁnitions.)
Because the computation occurring within a process is suppressed, sequential processes have
no dynamics on their own, but only through their interaction with other processes. For the vend-
ing machine to work there must be another process (you) who initiates the events expected by
the machine, causing both your state (the coins in your pocket) and its state (as just described) to
change as a result.

39.2 Interaction
367
39.2
Interaction
Processes become interesting when they are allowed to interact with one another to achieve a
common goal. To account for interaction we enrich the language of processes with concurrent
composition:
Proc
P
::=
await(E)
$ E
synchronize
stop
1
inert
conc(P1; P2)
P1 ⊗P2
composition
The process 1 represents the inert process, and the process P1 ⊗P2 represents the concurrent com-
position of P1 and P2. We may identify 1 with $ 0, the process that awaits the event that will never
occur, but we prefer to treat the inert process as a primitive concept.
We will identify processes up to structural congruence, the strongest equivalence relation closed
under these rules:
P ⊗1 ≡P
(39.3a)
P1 ⊗P2 ≡P2 ⊗P1
(39.3b)
P1 ⊗(P2 ⊗P3) ≡(P1 ⊗P2) ⊗P3
(39.3c)
P1 ≡P′
1
P2 ≡P′
2
P1 ⊗P2 ≡P′
1 ⊗P′
2
(39.3d)
Up to structural congruence every process has the form
$ E1 ⊗. . . ⊗$ En
for some n ≥0, it being understood that when n = 0 this stands for the null process 1.
Interaction between processes consists of synchronization of two complementary actions. The
dynamics of interaction is deﬁned by two forms of judgment. The transition judgment P 7−→P′
states that the process P evolves to the process P′ as a result of a single step of computation. The
family of transition judgments, P
α7−→P′, where α is an action, states that the process P may evolve
to the process P′ as long as the action α is permissible in the context in which the transition occurs.
As a notational convenience, we often regard the unlabeled transition to be the labeled transition
corresponding to the special silent action.
The possible actions are given by the following grammar:
Act
α
::=
que[a]
a ?
query
sig[a]
a !
signal
sil
ε
silent
The query action a ? and the signal action a ! are complementary, and the silent action ε, is self-
complementary. We deﬁne the complementary action to α to be the action α given by the equations
a ? = a !, a ! = a ?, and ε = ε.

368
39.2 Interaction
$ (!a;P + E) a !
7−→P
(39.4a)
$ (?a;P + E) a ?
7−→P
(39.4b)
P1
α7−→P′
1
P1 ⊗P2
α7−→P′
1 ⊗P2
(39.4c)
P1
α7−→P′
1
P2
α7−→P′
2
P1 ⊗P2 7−→P′
1 ⊗P′
2
(39.4d)
Rules (39.4a) and (39.4b) specify that any of the events on which a process is synchronizing
may occur. Rule (39.4d) synchronizes two processes that take complementary actions.
As an example, let us consider the vending machine V, given by equation (39.2), interacting
with the user process U deﬁned as follows:
U = $ !2p;$ !2p;$ ?cof;1.
Here is a trace of the interaction between V and U:
V ⊗U 7−→$ (!tea;V + ?2p;$ !cof;V) ⊗$ !2p;$ ?cof;1
7−→$ !cof;V ⊗$ ?cof;1
7−→V
These steps are justiﬁed by the following pairs of labeled transitions:
U
2p !
7−−→U′ = $ !2p;$ ?cof;1
V
2p ?
7−−→V′ = $ (!tea;V + ?2p;$ !cof;V)
U′ 2p !
7−−→U′′ = $ ?cof;1
V′ 2p ?
7−−→V′′ = $ !cof;V
U′′ cof ?
7−−→1
V′′ cof !
7−−→V
We have suppressed uses of structural congruence in the foregoing derivation to avoid clutter, but
it is important to see its role in managing the non-deterministic choice of events by a process.

39.3 Replication
369
39.3
Replication
Some presentations of process calculi forego reliance on deﬁning equations for processes in favor
of a replication construct, which we write as ∗P. This process stands for as many concurrently
executing copies of P as needed. Implicit replication can be expressed by the structural congruence
∗P ≡P ⊗∗P.
(39.5)
Understood as a principle of structural congruence, this rule hides the steps of process creation,
and gives no hint as to how often it should be applied. We could alternatively build replication
into the dynamics to model the details of replication more closely:
∗P 7−→P ⊗∗P.
(39.6)
Because there is no constraint on the use of this rule, it can at any time create a new copy of the
replicated process P. It is also possible to tie its use to send and receive events so that replication
is causal, rather than spontaneous.
So far we have used recursive process deﬁnitions to deﬁne processes that interact repeatedly
according to some protocol. Rather than take recursive deﬁnition as a primitive notion, we may
instead use replication to model repetition. We do so by introducing an “activator” process that
is used to cause the replication. Consider the recursive deﬁnition X = P(X), where P is a process
expression that may refer to itself as X. Such a self-referential process can be simulated by deﬁning
the activator process
A = ∗$ (?a;P($ (!a;1))),
in which we have replaced occurrences of X within P by an initiator process that signals the event
a to the activator. Note that the activator A is structurally congruent to the process A′ ⊗A, where
A′ is the process
$ (?a;P($ (!a;1))).
To start process P we concurrently compose the activator A with an initiator process, $ (!a;1). Note
that
A ⊗$ (!a;1) 7−→A ⊗P($ !a;1),
which starts the process P while maintaining a running copy of the activator, A.
As an example, let us consider Milner’s vending machine written using replication, instead of
recursive process deﬁnition:
V0 = $ (!v;1)
(39.7)
V1 = ∗$ (?v;V2)
(39.8)
V2 = $ (?2p;$ (!tea;V0 + ?2p;$ (!cof;V0)))
(39.9)
The process V1 is a replicated server that awaits a signal on channel v to create another instance of
the vending machine. The recursive calls are replaced by signals along v to re-start the machine.
The original machine V is simulated by the concurrent composition V0 ⊗V1.

370
39.4 Allocating Channels
This example motivates replacing spontaneous replication by replicated synchronization, which
is deﬁned by the following rules:
∗$ (!a;P + E) a !
7−→P ⊗∗$ (!a;P + E)
(39.10a)
∗$ (?a;P + E) a ?
7−→P ⊗∗$ (?a;P + E)
(39.10b)
The process ∗$ (E) is to be regarded not as a composition of replication and synchronization, but as
the inseparable combination of these two constructs. The advantage is that the replication occurs
only as needed, precisely when a synchronization with another process is possible, avoiding the
need “guess” when replication is needed.
39.4
Allocating Channels
It is often useful (particularly once we have introduced inter-process communication) to introduce
new channels within a process, and not assume that all channels of interaction are given a priori.
To allow for this, we enrich the syntax of processes with channel declaration:
Proc
P
::=
new(a.P)
ν a.P
new channel
The channel a is bound within the process P. To simplify notation we sometimes write ν a1, . . . , ak.P
for the iterated declaration ν a1.. . . ν ak.P.
We then extend structural congruence with the following rules:
P =α P′
P ≡P′
(39.11a)
P ≡P′
ν a.P ≡ν a.P′
(39.11b)
a /∈P2
(ν a.P1) ⊗P2 ≡ν a.(P1 ⊗P2)
(39.11c)
ν a.ν b.P ≡ν b.ν a.P
(39.11d)
(a /∈P)
ν a.P ≡P
(39.11e)
Rule (39.11c), called scope extrusion, will be especially important in Section 39.6. Rule (39.11e) states
that channels are de-allocated once they are no longer in use.

39.4 Allocating Channels
371
To account for the scopes of channels we extend the statics of PiC with a signature Σ comprising
a ﬁnite set of active channels. The judgment ⊢Σ P proc states that a process P is well-formed
relative to the channels declared in the signature Σ.
⊢Σ 1 proc
(39.12a)
⊢Σ P1 proc
⊢Σ P2 proc
⊢Σ P1 ⊗P2 proc
(39.12b)
⊢Σ E event
⊢Σ $ E proc
(39.12c)
⊢Σ,a P proc
⊢Σ ν a.P proc
(39.12d)
The foregoing rules make use of an auxiliary judgment, ⊢Σ E event, stating that E is a well-formed
event relative to Σ.
⊢Σ 0 event
(39.13a)
⊢Σ,a P proc
⊢Σ,a ?a;P event
(39.13b)
⊢Σ,a P proc
⊢Σ,a !a;P event
(39.13c)
⊢Σ E1 event
⊢Σ E2 event
⊢Σ E1 + E2 event
(39.13d)
The judgment ⊢Σ α action states that α is a well-formed action relative to Σ:
⊢Σ,a a ? action
(39.14a)
⊢Σ,a a ! action
(39.14b)
⊢Σ ε action
(39.14c)
The dynamics of the current fragment of PiC is correspondingly generalized to keep track of
the set of active channels. The judgment P
α7−→
Σ
P′ states that P transitions to P′ with action α
relative to channels Σ. The dynamics of this extension is obtained by indexing the transitions by
the signature, and adding a rule for channel declaration.
$ (!a;P + E)
a !
7−→
Σ,a P
(39.15a)

372
39.5 Communication
$ (?a;P + E)
a ?
7−→
Σ,a P
(39.15b)
P1
α7−→
Σ P′
1
P1 ⊗P2
α7−→
Σ P′
1 ⊗P2
(39.15c)
P1
α7−→
Σ P′
1
P2
α7−→
Σ P′
2
P1 ⊗P2 7−→
Σ P′
1 ⊗P′
2
(39.15d)
P
α
7−→
Σ,a P′
⊢Σ α action
ν a.P α7−→
Σ ν a.P′
(39.15e)
Rule (39.15e) ensures that no process may interact with ν a.P along the channel a by using the
identiﬁcation convention to choose a /∈Σ.
Consider again the deﬁnition of the vending machine using replication instead of recursion.
The channel v used to initialize the machine is private to the machine itself. The process V =
ν v.(V0 ⊗V1) declares a new channel v for use by V0 and V1, which are deﬁned essentially as
before. The interaction of the user process U with V begins as follows:
(ν v.(V0 ⊗V1)) ⊗U 7−→
Σ (ν v.V2) ⊗U ≡ν v.(V2 ⊗U).
The interaction continues within the scope of the declaration, which ensures that v does not occur
within U.
39.5
Communication
Synchronization coordinates the execution of two processes that take the complementary actions of
signaling and querying a common channel. Synchronous communication generalizes synchroniza-
tion to pass a data value betwen two synchronizing processes, one of which is the sender of the
value and the other its receiver. The type of the data is immaterial to the communication.
To account for interprocess communication we enrich the language of processes to include
variables, as well as channels, in the formalism. Variables range, as always, over types, and are
given meaning by substitution. Channels, on the other hand, are assigned types that classify the
data carried on that channel, and are given meaning by send and receive events that generalize the
signal and query events considered in Section 39.2. The abstract syntax of communication events
is given by the following grammar:
Evt
E
::=
snd[a](e; P)
! a(e ; P)
send
rcv[a](x.P)
? a(x.P)
receive

39.5 Communication
373
The event rcv[a](x.P) represents the receipt of a value x on the channel a, passing x to the process
P. The variable x is bound within P. The event snd[a](e; P) represents the transmission of e on a
and continuing with P.
We modify the syntax of declarations to account for the type of value sent on a channel.
Proc
P
::=
new{τ}(a.P)
ν a ~ τ.P
typed channel
The process new{τ}(a.P) introduces a new channel a with associated type τ for use within the
process P. The channel a is bound within P.
The statics is extended to account for the type of a channel. The judgment Γ ⊢Σ P proc states
that P is a well-formed process involving the channels declared in Σ and the variables declared in
Γ. It is inductively deﬁned by the following rules, wherein we assume that the typing judgment
Γ ⊢Σ e : τ is given separately.
Γ ⊢Σ 1 proc
(39.16a)
Γ ⊢Σ P1 proc
Γ ⊢Σ P2 proc
Γ ⊢Σ P1 ⊗P2 proc
(39.16b)
Γ ⊢Σ,a~τ P proc
Γ ⊢Σ ν a ~ τ.P proc
(39.16c)
Γ ⊢Σ E event
Γ ⊢Σ $ E proc
(39.16d)
Rules (39.16) make use of the auxiliary judgment Γ ⊢Σ E event, stating that E is a well-formed
event relative to Γ and Σ, which is deﬁned as follows:
Γ ⊢Σ 0 event
(39.17a)
Γ ⊢Σ E1 event
Γ ⊢Σ E2 event
Γ ⊢Σ E1 + E2 event
(39.17b)
Γ, x : τ ⊢Σ,a~τ P proc
Γ ⊢Σ,a~τ ? a(x.P) event
(39.17c)
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ P proc
Γ ⊢Σ,a~τ ! a(e ; P) event
(39.17d)
Rule (39.17d) makes use of a typing judgment for expressions that ensures that the type of a chan-
nel is respected by communication.
The dynamics of communication extends that of synchronization by enriching send and receive
actions with the value sent or received.
Act
α
::=
rcv[a](e)
a ? e
receive
snd[a](e)
a ! e
send
sil
ε
silent

374
39.5 Communication
Complementarity is deﬁned as before, by switching the orientation of an action: a ? e = a ! e,
a ! e = a ? e, and ε = ε.
The statics ensures that the expression associated with these actions is a value of a type suitable
for the channel:
⊢Σ,a~τ e : τ
e valΣ,a~τ
⊢Σ,a~τ a ! e action
(39.18a)
⊢Σ,a~τ e : τ
e valΣ,a~τ
⊢Σ,a~τ a ? e action
(39.18b)
⊢Σ ε action
(39.18c)
The dynamics is deﬁned by replacing the synchronization rules (39.15a) and (39.15b) with the
following communication rules:
e 7−−−→
Σ,a~τ e′
$ (! a(e ; P) + E) 7−−−→
Σ,a~τ $ (! a(e′ ; P) + E)
(39.19a)
e valΣ,a~τ
$ (! a(e ; P) + E)
a!e
7−−−→
Σ,a~τ P
(39.19b)
e valΣ,a~τ
$ (? a(x.P) + E)
a?e
7−−−→
Σ,a~τ [e/x]P
(39.19c)
Rule (39.19c) is non-deterministic in that it “guesses” the value e to be received along channel a.
Rules (39.19) make reference to the dynamics of expressions, which is left unspeciﬁed because
nothing depends on it.
Using synchronous communication, both the sender and the receiver of a message are blocked
until the interaction is completed. Therefore the sender must be notiﬁed whenever a message is
received, which means that there must be an implicit reply channel from receiver to sender that
carries the notiﬁcation. This means that synchronous communication can be decomposed into a
simpler asynchronous send operation, which sends a message on a channel without waiting for its
receipt, together with channel passing to send an acknowledgment channel along with the message
data.
Asynchronous communication is deﬁned by removing the synchronous send event from the pro-
cess calculus, and adding a new form of process that simply sends a message on a channel. The
syntax of asynchronous send is as follows:
Proc
P
::=
asnd[a](e)
! a(e)
send
The process asnd[a](e) sends the message e on channel a, and then terminates immediately. With-
out the synchronous send event, every event is, up to structural congruence, a choice of zero or

39.6 Channel Passing
375
more read events. The statics of asynchronous send is given by the following rule:
Γ ⊢Σ,a~τ e : τ
Γ ⊢Σ,a~τ ! a(e) proc
(39.20)
The dynamics is given similarly:
e valΣ
! a(e) a!e
7−→
Σ
1
(39.21)
The rule for communication remains unchanged. A pending asynchronous send is essentially a
buffer holding the value to be sent once a receiver is available.
39.6
Channel Passing
An interesting case of interprocess communication arises when one process passes channel refer-
ence, a form of value, to another along a common channel. The receiving process need not have
any direct access to the channel referred to by the reference. It merely operates on it using send
and receive operations that act on channel references instead of ﬁxed channels. Doing so allows for
new patterns of communication to be established among processes. For example, two processes,
P and Q, may share a channel a along which they may send and receive messages. If the scope of
a is conﬁned to these processes, then no other process R may communicate on that channel; it is,
in effect, a private channel between P and Q.
The following process expression illustrates such a situation:
(ν a ~ τ.(P ⊗Q)) ⊗R.
The process R is excluded from the scope of the channel a, which however includes both P and
Q. The processes P and Q may communicate with each other on channel a, but R has no access
to this channel. If P and Q wish to allow R to communicate along a, they may do so by sending a
reference to a to R along some channel b known to all three processes. Thus we have the following
situation:
ν b ~ τ chan.((ν a ~ τ.(P ⊗Q)) ⊗R).
Assuming that P initiates the inclusion of R into its communication with Q along a, it has the
form $ (! b(& a ; P′)). The process R correspondingly takes the form $ (? b(x.R′)). The system of
processes therefore has the form
ν b ~ τ chan.(ν a ~ τ.($ (! b(& a ; P′)) ⊗Q) ⊗$ (? b(x.R′))).
Sending a reference to a to R would seem to violate the scope of a. The communication of the
reference would seem to escape the scope of the referenced channel, which would be nonsensical.
It is here that the concept of scope extrusion, introduced in Section 39.4 comes into play:
ν b ~ τ chan.ν a ~ τ.($ (! b(& a ; P′)) ⊗Q ⊗$ (? b(x.R′))).

376
39.6 Channel Passing
The scope of a expands to encompass R, preparing the ground for communication between P and
R, resulting in
ν b ~ τ chan.ν a ~ τ.(P′ ⊗Q ⊗[& a/x]R′).
The reference to the channel a is substituted for the variable x within R′.
The process R may now communicate with P and Q by sending and receiving messages along
the channel reference substituted for the variable x. For this we use dynamic forms of send and
receive in which the channel on which to communicate is determined by evaluation of an expres-
sion. For example, to send a message e of type τ along the channel referred to by x, the process R′
would have the form
$ (!! (x ; e ; R′′)).
Similarly, to receive along the referenced channel, the process R′ would have the form
$ (?? (x ; y.R′′)).
In both cases the dynamic communication forms evolve to the static communication forms once
the referenced channel has been determined.
The syntax of channel reference types is given by the following grammar:
Typ
τ
::=
chan(τ)
τ chan
channel type
Exp
e
::=
chref[a]
& a
reference
Evt
E
::=
sndref(e1; e2; P)
!! (e1 ; e2 ; P)
send
rcvref(e; x.P)
?? (e ; x.P)
receive
The events sndref(e1; e2; P) and rcvref(e; x.P) are dynamic versions of the events snd[a](e; P)
and rcv[a](x.P) in which the channel reference is determined dynamically by evaluation of an
expression.
The statics of channel references is given by the following rules:
Γ ⊢Σ,a~τ & a : τ chan
(39.22a)
Γ ⊢Σ e1 : τ chan
Γ ⊢Σ e2 : τ
Γ ⊢Σ P proc
Γ ⊢Σ !! (e1 ; e2 ; P) event
(39.22b)
Γ ⊢Σ e : τ chan
Γ, x : τ ⊢Σ P proc
Γ ⊢Σ ?? (e ; x.P) event
(39.22c)
Because channel references are forms of expression, events must be evaluated to determine the
channel to which they refer.
E 7−−−→
Σ,a~τ E′
$ (E) 7−−−→
Σ,a~τ $ (E′)
(39.23a)
e valΣ,a~τ
$ (!! (& a ; e ; P) + E) 7−−−→
Σ,a~τ $ (! a(e ; P) + E)
(39.23b)

39.7 Universality
377
e valΣ,a~τ
$ (?? (& a ; x.P) + E) 7−−−→
Σ,a~τ $ (? a(x.P) + E)
(39.23c)
Events must similarly be evaluated; see Chapter 40 for guidance on how to formulate such a
dynamics.
39.7
Universality
The process calculus PiC developed in this chapter is universal in the sense that the untyped λ-
calculus can be encoded within it. Consequently, via this encoding, the same functions on the
natural numbers are deﬁnable in PiC as are deﬁnable in Λ and hence, by Church’s Law, any
known programming language. This claim is remarkable because PiC has so few capabilities that
one might suspect that it is too weak to be a useful programming language. The key to seeing that
PiC is universal is to note that communication allows processes to send and receive values of an
arbitrary type. So long as recursive and channel reference types are available, then it is a purely
technical matter to show that Λ is encodable within it. After all, what makes Λ universal is that
its one type is a recursive type (see Chapter 21), so it is natural to guess that with messages of
recursive type available then PiC would be universal. And indeed it is.
To prove universality it sufﬁces to give an encoding of the untyped λ-calculus under a call-
by-name dynamics into PiC. To motivate the translation, consider a call-by-name stack machine
for evaluating λ-terms. A stack is a composition of frames, each of which have the form −(e2)
corresponding to the evaluation of the function part of an application. A stack is represented in PiC
by a reference to a channel that expects an expression (the function to apply) and another channel
reference (the stack on which to evaluate the result of the application). A λ-term is represented by
a reference to a channel that expects a stack on which the expression is evaluated.
Let κ be the type of continuations. It should be isomorphic to the type of references to channels
that carry a pair of values, an argument, whose type is a reference to a channel carrying a contin-
uation, and another continuation to which to deliver the result of the application. Thus we seek to
have the following type isomorphism:
κ ∼= (κ chan × κ) chan.
The solution is a recursive type, as described in Chapter 20. Thus, just as for Λ itself, the key to
the universality of PiC is the use of the recursive type κ.
We now give the translation of Λ into PiC. For the sake of the induction the translation of a
Λ expression u is given relative to a variable of type κ, representing the continuation to which the
result will be sent. The representation is given by the following equations:
x @ k ≜!! (x ; k)
λ (x) u @ k ≜$ ?? (unfold(k) ; ⟨x, k′⟩.u @ k′)
u1(u2) @ k ≜
ν a1 ~ κ chan × κ.(u1 @ fold(& a1)) ⊗ν a ~ κ.∗$ ? a(k2.u2 @ k2) ⊗! a1(⟨& a, k⟩)

378
39.8 Notes
We use pattern matching on pairs for the sake of readability. Only asynchronous sends are needed.
The use of static and dynamic communication operations in the translation merits close con-
sideration. The call site of a λ-term is determined dynamically; we cannot predict at translation
time the continuation of the term. In particular, the binding of a variable can be used at several
call sites, corresponding to uses of that variable. On the other hand, the channel associated to
an argument is determined statically. The server associated to the variable listens on a statically
determined channel for a continuation, which is determined dynamically.
As a check on the correctness of the representation, consider the following derivation:
(λ (x) x)(y) @ k 7−→∗
ν a1 ~ τ.($ ? a1(⟨x, k′⟩.!! (x ; k′))) ⊗ν a ~ κ.∗$ ? a(k2.!! (y ; k2)) ⊗! a1(⟨& a, k⟩)
7−→∗ν a ~ κ.∗$ ? a(k2.!! (y ; k2)) ⊗! a(k)
7−→∗ν a ~ κ.∗$ ? a(k2.!! (y ; k2)) ⊗!! (y ; k)
Apart from the idle server process listening on channel a, this is just the translation y @ k. (Using
the methods to be developed in detail in Chapter 49, we may show that the result of the computa-
tion step is “bisimilar” to the translation of y @ k, and hence equivalent to it for all purposes.)
39.8
Notes
Process calculi as models of concurrency and interaction were introduced and extensively devel-
oped by Hoare (1978) and Milner (1999). Milner’s original formulation, CCS, was introduced to
model pure synchronization, whereas Hoare’s, CSP, included value-passing. CCS was extended
to become the π-calculus (Milner, 1999), which includes channel-passing. Dozens upon dozens of
variations and extensions of CSP, CCS, and the π-calculus have been considered in the literature,
and continue to be a subject of intensive study. (See Engberg and Nielsen (2000) for an account of
some of the critical developments in the area.)
The process calculus considered here is derived from the π-calculus as presented in Milner
(1999). The overall line of development, and the vending machine example and the λ-calculus en-
coding, are adapted from Milner (1999). The distinction drawn here between static and dynamic
events (that is, those that are given syntactically versus those that arise by evaluation) ﬂows nat-
urally from the distinction between variables and channels. It is possible to formulate PiC using
only channel references, suppressing any mention of channels themselves. The present formula-
tion coheres with the formulation of assignables and assignable references in Chapters 34 and 35.
The concept of dynamic events is taken one step further in Concurrent ML (Reppy, 1999), wherein
events are values of an event type (see also Chapter 40).
Exercises
39.1. Booleans can be represented in the process calculus similarly to the way in they are repre-
sented in Λ (Chapter 21), called the Milner booleans. Speciﬁcally, a boolean can be represented

39.8 Notes
379
by a channel carrying a pair of channel references that are signaled (sent a trivial value) to
indicate whether the boolean is true or false. Give a deﬁnition of processes corresponding
to truth, falsehood, and conditional branch between two processes, each parameterized by a
channel a representing the boolean.
39.2. Deﬁne the sequential composition P ; Q of processes P and Q in PiC. Hint: Deﬁne an auxil-
iary translation P ▷p, where p is a channel value, such that P ▷p behaves like P, but sends
the unit value on p just before termination.
39.3. Consider again an RS latch, which was the subject of Exercises 11.4, 15.7, and 20.3. Im-
plement an RS latch as a process L(i, o) that takes a pair of booleans as input on channel i,
representing the R and S inputs to the latch, and outputs a pair of booleans on channel o, rep-
resenting the outputs Q and Z, with Q being the output of interest. Consider the following
input processes:
I(i) ≜∗! i(⟨false, false⟩)
Ireset(i) ≜! i(⟨true, false⟩) ; Ireset
Iset(i) ≜! i(⟨true, false⟩) ; Iset
The ﬁrst quiesces the inputs by holding both R and S at false forever. The second asserts
the R input (only), and then quiesces; the third asserts the S input (only), and then quiesces.
Show that the process L(i, o) ⊗Ireset(i) evolves to a process capable of taking the action
o ! ⟨false, false⟩, and is then forever capable of evolving to a process taking the same ac-
tion. Similarly, show that L(i, o) ⊗Iset(i) evolves to a process capable of taking the action
o ! ⟨true, false⟩, and is then forever capable of evolving to a process taking the same ac-
tion.
39.4. Some versions of process calculus to note have a distinction between events and processes.
They instead consider the non-deterministic choice P1 + P2 of two processes, which is deﬁned
by the following silent transition rules:
P1 + P2 7−→
Σ P1
(39.24a)
P1 + P2 7−→
Σ P2
(39.24b)
Thus P1 + P2 may spontaneously evolve into either P1 or P2, without interacting with any
other process. Show that non-deterministic choice is deﬁnable in the asynchronous process
calculus in such a way that the given transitions are possible.
39.5. In the asynchronous process calculus events are ﬁnite sums of inputs, called an input choice,
of the form
? a1(x1.P1) + . . . + ? an(xn.Pn).

380
39.8 Notes
The behavior of the process
P ≜$ (? a1(x1.P1) + . . . + ? an(xn.Pn))
is tantalizingly close to that of the concurrent composition of processes receiving on a single
channel,
Q ≜$ ? a1(x1.P1) ⊗. . . ⊗$ ? an(xn.Pn).
The processes P and Q are similar in that both may synchronize with a concurrently execut-
ing sender on any of the speciﬁed receive channels. They are different in that P abandons the
other receives once one has synchronized with a sender, whereas Q leaves the other choices
available for further synchronization. So a receive-only choice can be deﬁned in terms of
the concurrent composition of single-choice receives by arranging that the other choices are
deactivated once one has been chosen. Show that this is the case. Hint: Associate a Milner
boolean (Exercise 39.1) with each choice group that limits synchronization to at most one
sender.
39.6. The polyadic π-calculus is a process calculus in which all channels are constrained to carry
values of the recursive type π satisfying the isomorphism
π ∼= ∑
n∈N
π chan × . . . × π chan
|
{z
}
n
.
Thus, a message value has the form
n · ⟨& a1, . . . , & an
|
{z
}
n
⟩
in which the tag n indicates the size of the tuple of channel references associated with it.
Show that the encoding of Λ given in Section 39.7 can be given using only channels of type
π, proving the universality of the polyadic π-calculus.

Chapter 40
Concurrent Algol
In this chapter we integrate concurrency into the framework of Modernized Algol described in
Chapter 34. The resulting language, called Concurrent Algol, or CA, illustrates the integration of
the mechanisms of the process calculus described in Chapter 39 into a practical programming lan-
guage. To avoid distracting complications, we drop assignables from Modernized Algol entirely.
(There is no loss of generality, however, because free assignables are deﬁnable in Concurrent Algol
using processes as cells.)
The process calculus described in Chapter 39 is intended as a self-standing model of concurrent
computation. When viewed in the context of a programming language, however, it is possible to
streamline the machinery to take full advantage of types that are in any case required for other
purposes. In particular the concept of a channel, which features prominently in Chapter 39, is
identiﬁed with the concept of a dynamic class as described in Chapter 33. More precisely, we take
broadcast communication of dynamically classiﬁed values as the basic synchronization mechanism
of the language. Being dynamically classiﬁed, messages consist of a payload tagged with a class,
or channel. The type of the channel determines the type of the payload. Importantly, only those
processes that have access to the channel may decode the message; all others must treat it as
inscrutable data that can be passed around but not examined. In this way we can model not only
the mechanisms described in Chapter 39, but also formulate an abstract account of encryption and
decryption in a network using the methods described in Chapter 39.
Concurrent Algol features a modal separation between commands and expressions like in
Modernized Algol. It is also possible to combine these two levels (so as to allow benign con-
currency effects), but we do not develop this approach in detail here.

382
40.1 Concurrent Algol
40.1
Concurrent Algol
The syntax of CA is obtained by removing assignables from MA, and adding a syntactic level of
processes to represent the global state of a program:
Typ
τ
::=
cmd(τ)
τ cmd
commands
Exp
e
::=
cmd(m)
cmd m
command
Cmd
m
::=
ret e
ret e
return
bnd(e; x.m)
bnd x ←e ; m
sequence
Proc
p
::=
stop
1
idle
run(m)
run(m)
atomic
conc(p1; p2)
p1 ⊗p2
concurrent
new{τ}(a.p)
ν a ~ τ.p
new channel
The process run(m) is an atomic process executing the command m. The other forms of process are
adapted from Chapter 39. If Σ has the form a1 ~ τ1, . . . , an ~ τn, then we sometimes write ν Σ{p}
for the iterated form ν a1 ~ τ1.. . . ν an ~ τn.p.
The statics of CA is given by these judgments:
Γ ⊢Σ e : τ
expression typing
Γ ⊢Σ m ∼·· τ
command typing
Γ ⊢Σ p proc
process formation
Γ ⊢Σ α action
action formation
The expression and command typing judgments are essentially those of MA, augmented with the
constructs described below.
Process formation is deﬁned by the following rules:
⊢Σ 1 proc
(40.1a)
⊢Σ m ∼·· τ
⊢Σ run(m) proc
(40.1b)
⊢Σ p1 proc
⊢Σ p2 proc
⊢Σ p1 ⊗p2 proc
(40.1c)
⊢Σ,a~τ p proc
⊢Σ ν a ~ τ.p proc
(40.1d)
Processes are identiﬁed up to structural congruence, as described in Chapter 39.
Action formation is deﬁned by the following rules:
⊢Σ ε action
(40.2a)

40.1 Concurrent Algol
383
⊢Σ e : clsfd
e valΣ
⊢Σ e ! action
(40.2b)
⊢Σ e : clsfd
e valΣ
⊢Σ e ? action
(40.2c)
Messages are values of the type clsfd deﬁned in Chapter 33.
The dynamics of CA is deﬁned by transitions between processes, which represent the state of
the computation. More precisely, the judgment p
α7−→
Σ
p′ states that the process p evolves in one
step to the process p′ while undertaking action α.
m
α=⇒
Σ ν Σ′ { m′ ⊗p }
run(m) α7−→
Σ ν Σ′{run(m′) ⊗p}
(40.3a)
e valΣ
run(ret e)
ε7−→
Σ 1
(40.3b)
p1
α7−→
Σ p′
1
p1 ⊗p2
α7−→
Σ p′
1 ⊗p2
(40.3c)
p1
α7−→
Σ p′
1
p2
α7−→
Σ p′
2
p1 ⊗p2
ε7−→
Σ p′
1 ⊗p′
2
(40.3d)
p
α
7−−−→
Σ,a~τ p′
⊢Σ α action
ν a ~ τ.p α7−→
Σ ν a ~ τ.p′
(40.3e)
Rule (40.3a) states that a step of execution of the atomic process run(m) consists of a step of execu-
tion of the command m, which may allocate some set Σ′ of symbols or create a concurrent process
p. This rule implements scope extrusion for classes (channels) by expanding the scope of the chan-
nel declaration to the context in which the command m occurs. Rule (40.3b) states that a completed
command evolves to the inert (stopped) process; processes are executed solely for their effect, and
not for their value.
Executing a command in CA may, in addition to evolving to another command, allocate a new
channel or may spawn a new process. More precisely, the judgment1
m
α=⇒
Σ ν Σ′ { m′ ⊗p′ }
1The right-hand side of this judgment is a triple consisting of Σ′, m′, and p′, not a process expression comprising these
parts.

384
40.2 Broadcast Communication
states that the command m transitions to the command m′ while creating new channels Σ′ and
new processes p′. The action α speciﬁes the interactions of which m is capable when executed.
As a notational convenience we drop mention of the new channels or processes when either are
trivial.
The following rules deﬁne the execution of the basic forms of command inherited from MA:
e 7−→
Σ e′
ret e
ε=⇒
Σ ret e′
(40.4a)
m1
α=⇒
Σ ν Σ′ { m′
1 ⊗p′ }
bnd x ←cmd m1 ; m2
α=⇒
Σ ν Σ′{bnd x ←cmd m′
1 ; m2 ⊗p′}
(40.4b)
e valΣ
bnd x ←cmd (ret e) ; m2
ε=⇒
Σ [e/x]m2
(40.4c)
e1 7−→
Σ e′
1
bnd x ←e1 ; m2
ε=⇒
Σ bnd x ←e′
1 ; m2
(40.4d)
These rules are supplemented by rules governing communication and synchronization among
processes in the next two sections.
40.2
Broadcast Communication
In this section we consider a very general form of process synchronization called broadcast. Pro-
cesses emit and accept messages of type clsfd, the type of dynamically classiﬁed values consid-
ered in Chapter 33. A message consists of a channel, which is its class, and a payload, which is
a value of the type associated with the channel (class). Recipients may pattern match against a
message to determine whether it is of a given class, and, if so, recover the associated payload. No
process that lacks access to the class of a message may recover the payload of that message. (See
Section 33.4.1 for a discussion of how to enforce conﬁdentiality and integrity restrictions using
dynamic classiﬁcation).
The syntax of the commands pertinent to broadcast communication is given by the following
grammar:
Cmd
m
::=
spawn(e)
spawn(e)
spawn
emit(e)
emit(e)
emit message
acc
acc
accept message
newch{τ}
newch
new channel
The command spawn(e) spawns a process that executes the encapsulated command given by e.
The commands emit(e) and acc emit and accept messages, which are classiﬁed values whose

40.2 Broadcast Communication
385
class is the channel on which the message is sent. The command newch{τ} returns a reference to a
fresh class carrying values of type τ.
The statics of broadcast communication is given by the following rules:
Γ ⊢Σ e : cmd(unit)
Γ ⊢Σ spawn(e) ∼·· unit
(40.5a)
Γ ⊢Σ e : clsfd
Γ ⊢Σ emit(e) ∼·· unit
(40.5b)
Γ ⊢Σ acc ∼·· clsfd
(40.5c)
Γ ⊢Σ newch{τ} ∼·· cls(τ)
(40.5d)
Execution of these commands is deﬁned as follows:
spawn(cmd(m))
ε=⇒
Σ ret ⟨⟩⊗run(m)
(40.6a)
e 7−→
Σ e′
spawn(e)
ε=⇒
Σ spawn(e′)
(40.6b)
e valΣ
emit(e) e !
=⇒
Σ ret ⟨⟩
(40.6c)
e 7−→
Σ e′
emit(e)
ε=⇒
Σ emit(e′)
(40.6d)
e valΣ
acc
e ?
=⇒
Σ ret e
(40.6e)
newch{τ}
ε=⇒
Σ ν a ~ τ { ret (& a) }
(40.6f)
Rule (40.6c) speciﬁes that emit(e) has the effect of emitting the message e.
Correspondingly,
rule (40.6e) speciﬁes that acc may accept (any) message that is being sent.
As usual, the preservation theorem for CA ensures that well-typed programs remain well-
typed during execution. The proof of preservation requires a lemma about command execution.
Lemma 40.1. If m
α=⇒
Σ ν Σ′ { m′ ⊗p′ }, ⊢Σ m ∼·· τ, then ⊢Σ α action, ⊢Σ Σ′ m′ ∼·· τ, and ⊢Σ Σ′ p′ proc.

386
40.3 Selective Communication
Proof. By induction on rules (40.4).
With this in hand the proof of preservation goes along familiar lines.
Theorem 40.2 (Preservation). If ⊢Σ p proc and p 7−→
Σ p′, then ⊢Σ p′ proc.
Proof. By induction on transition, appealing to Lemma 40.1 for the crucial steps.
Typing does not, however, guarantee progress with respect to unlabeled transition, for the
simple reason that there may be no other process with which to communicate. By extending
progress to labeled transitions we may state that this is the only way for process exceution to
get stuck. But some care must be taken to account for allocating new channels.
Theorem 40.3 (Progress). If ⊢Σ p proc, then either p ≡1, or p ≡ν Σ′{p′} such that p′
α
7−−→
Σ Σ′
p′′ for
some ⊢Σ Σ′ p′′ and some ⊢Σ Σ′ α action.
Proof. By induction on rules (40.1) and (40.5).
The progress theorem says that no process can get stuck for any reason other than ithe nability
to communicate with another process. For example, a process that receives on a channel for which
there is no sender is “stuck”, but this does not violate Theorem 40.3.
40.3
Selective Communication
Broadcast communication provides no means of restricting acceptance to messages of a particu-
lar class (that is, of messages on a particular channel). Using broadcast communication we may
restrict attention to a particular channel a of type τ by running the following command:
fix loop : τ cmd is {x ←acc ; match x as a · y ,→ret y ow ,→emit(x) ; do loop}
This command is always capable of receiving a broadcast message. When one arrives, it is exam-
ined to see whether it is classiﬁed by a. If so, the underlying classiﬁed value is returned; otherwise
the message is re-broadcast so that another process may consider it. Polling consists of repeatedly
executing the above command until a message of channel a is successfully accepted, if ever it is.
Polling is evidently impractical in most situations. An alternative is to change the language
to allow for selective communication. Rather than accept any broadcast message, we may conﬁne
attention to messages sent only on certain channels. The type event(τ) of events consists of a ﬁnite
choice of accepts, all of whose payloads are of type τ.
Typ
τ
::=
event(τ)
τ event
events
Exp
e
::=
rcv[a]
? a
selective read
never{τ}
never
null
or(e1; e2)
e1 or e2
choice
wrap(e1; x.e2)
e1 as x in e2
post-composition
Cmd
m
::=
sync(e)
sync(e)
synchronize

40.3 Selective Communication
387
Events in CA are similar to those of the asynchronous process calculus described in Chapter 39.
The chief difference is that post-composition is considered as a general operation on events, in-
stead of one tied to the receive event itself.
The statics of event expressions is given by the following rules:
Σ ⊢a ~ τ
Γ ⊢Σ rcv[a] : event(τ)
(40.7a)
Γ ⊢Σ never{τ} : event(τ)
(40.7b)
Γ ⊢Σ e1 : event(τ)
Γ ⊢Σ e2 : event(τ)
Γ ⊢Σ or(e1; e2) : event(τ)
(40.7c)
Γ ⊢Σ e1 : event(τ1)
Γ, x : τ1 ⊢Σ e2 : τ2
Γ ⊢Σ wrap(e1; x.e2) : event(τ2)
(40.7d)
The corresponding dynamics is deﬁned by these rules:
Σ ⊢a ~ τ
rcv[a] valΣ
(40.8a)
never{τ} valΣ
(40.8b)
e1 valΣ
e2 valΣ
or(e1; e2) valΣ
(40.8c)
e1 7−→
Σ e′
1
or(e1; e2) 7−→
Σ or(e′
1; e2)
(40.8d)
e1 valΣ
e2 7−→
Σ e′
2
or(e1; e2) 7−→
Σ or(e1; e′
2)
(40.8e)
e1 7−→
Σ e′
1
wrap(e1; x.e2) 7−→
Σ wrap(e′
1; x.e′
2)
(40.8f)
e1 valΣ
wrap(e1; x.e2) valΣ
(40.8g)
Event values are identiﬁed up to structural congruence as described in Chapter 39.
The statics of the synchronization command is given by the following rule:
Γ ⊢Σ e : event(τ)
Γ ⊢Σ sync(e) ∼·· τ
(40.9a)

388
40.4 Free Assignables as Processes
The type of the event determines the type of value returned by the synchronization command.
Execution of a synchronization command depends on the event.
e 7−→
Σ e′
sync(e)
ε=⇒
Σ sync(e′)
(40.10a)
e valΣ
⊢Σ e : τ
Σ ⊢a ~ τ
sync(rcv[a]) a·e ?
==⇒
Σ
ret(e)
(40.10b)
sync(e1)
α=⇒
Σ m1
sync(or(e1; e2))
α=⇒
Σ m1
(40.10c)
sync(e2)
α=⇒
Σ m2
sync(or(e1; e2))
α=⇒
Σ m2
(40.10d)
sync(e1)
α=⇒
Σ m1
sync(wrap(e1; x.e2))
α=⇒
Σ bnd(cmd(m1); x.ret(e2))
(40.10e)
Rule (40.10b) states that an acceptance on a channel a may synchronize only with messages classi-
ﬁed by a. When combined with structural congruence, Rules (40.10c) and (40.10d) state that either
event between two choices may engender an action. Rule (40.10e) yields the command that per-
forms the command m1 resulting from the action α taken by the event e1, then returns e2 with x
bound to the return value of m1.
Selective communication and dynamic events can be used together to implement a communi-
cation protocol in which a channel reference is passed on a channel in order to establish a com-
munication path with the recipient. Let a be a channel carrying values of type cls(τ), and let b
be a channel carrying values of type τ, so that & b can be passed as a message along channel a.
A process that wishes to accept a channel reference on a and then accept on that channel has the
form
{x ←sync(? a) ; y ←sync(?? x) ; . . .}.
The event ? a speciﬁes a selective receipt on channel a. Once the value x is accepted, the event ?? x
speciﬁes a selective receipt on the channel referenced by x. So, if & b is sent along a, then the event
?? & b evaluates to ? b, which accepts selectively on channel b, even though the receiving process
may have no direct access to the channel b itself.
40.4
Free Assignables as Processes
Scope-free assignables are deﬁnable in CA by associating to each assignable a server process that
sets and gets the contents of the assignable. To each assignable a of type τ is associated a server
that selectively accepts a message on channel a with one of two forms:

40.4 Free Assignables as Processes
389
1. get · (& b), where b is a channel of type τ. This message requests that the contents of a be
sent on channel b.
2. set· (⟨e, & b⟩), where e is a value of type τ, and b is a channel of type τ. This message requests
that the contents of a be set to e, and that the new contents be transmitted on channel b.
In other words, a is a channel of type τsrvr given by
[get ,→τ cls, set ,→τ × τ cls].
The server selectively accepts on channel a, then dispatches on the class of the message to satisfy
the request.
The server associated with the assignable a of type τ maintains the contents of a using recur-
sion. When called with the current contents of the assignable, the server selectively accepts on
channel a, dispatching on the associated request, and calling itself recursively with the (updated,
if necessary) contents:
λ (u : τsrvr cls) fix srvr : τ ⇀void cmd is λ (x : τ) cmd {y ←sync(?? u) ; e(40.12)}.
(40.11)
The server is a procedure that takes an argument of type τ, the current contents of the assignable,
and yields a command that never terminates, because it restarts the server loop after each request.
The server selectively accepts a message on channel a, and dispatches on it as follows:
case y {get · z ,→e(40.13) | set · ⟨x′, z⟩,→e(40.14)}.
(40.12)
A request to get the contents of the assignable a is served as follows:
{ ←emit(mk(z; x)) ; do srvr(x)}
(40.13)
A request to set the contents of the assignable a is served as follows:
{ ←emit(mk(z; x′)) ; do srvr(x′)}
(40.14)
The type τ ref is deﬁned to be τsrvr cls, the type of channels (classes) to servers providing a
cell containing a value of type τ. A new free assignable is created by the command ref e0, which
is deﬁned to be
{x ←newch ; ←spawn(e(40.11)(x)(e0)) ; ret x}.
(40.15)
A channel carrying a value of type τsrvr is allocated to serve as the name of the assignable, and a
new server is spawned that accepts requests on that channel, with initial value e0 of type τ0.
The commands ∗e0 and e0 ∗= e1 send a message to the server to get and set the contents of an
assignable. The code for ∗e0 is as follows:
{x ←newch ; ←emit(mk(e0; get · x)) ; sync(?? (x))}
(40.16)
A channel is allocated for the return value, the server is contacted with a get message specifying
this channel, and the result of receiving on this channel is returned. Similarly, the code for e0 ∗= e1
is as follows:
{x ←newch ; ←emit(mk(e0; set · ⟨e1, x⟩)) ; sync(?? (x))}
(40.17)

390
40.5 Notes
40.5
Notes
Concurrent Algol is a synthesis of process calculus and Modernized Algol; is essentially an “Algol-
like” formulation of Concurrent ML (Reppy, 1999). The design is inﬂuenced by Parallel Algol
(Brookes, 2002). Much work on concurrent interaction takes communication channels as a basic
concept, but see Linda (Gelernter, 1985) for an account similar to the one suggested here.
Exercises
40.1. In Section 40.2 channels are allocated using the command newch, which returns a channel
reference. Alternatively one may extend CA with a means of declaring channels just as
assignables are declared in MA. Formulate the syntax, statics, and dynamics of such a con-
struct, and derive newch using this extension.
40.2. Extend selective communication (Section 40.3) to account for channel references, which give
rise to a new form of event. Give the syntax, statics, and semantics of this extension.
40.3. Adapt the implementation of an RS latch given in Exercise 39.3 to CA.

Chapter 41
Distributed Algol
A distributed computation is one that takes place at many sites, each of which controls some re-
sources at that site. For example, the sites might be nodes on a network, and a resource might be
a device or sensor at that site, or a database controlled by that site. Only programs that execute at
a particular site may access the resources situated at that site. Consequently, command execution
always takes place at a particular site, called the locus of execution. Access to resources at a remote
site from a local site is achieved by moving the locus of execution to the remote site, running code
to access the local resource, and returning a value to the local site.
In this chapter we consider the language DA, which extends Concurrent Algol with a spatial
type system that mediates access to resources on a network. The type safety theorem ensures that
all accesses to a resource controlled by a site are through a program executing at that site, even
though references to local resources can be freely passed around to other sites on the network. The
main idea is that channels and events are located at a particular site, and that synchronization on an
event can only occur at the proper site for that event. Issues of concurrency, which are temporal,
are thereby separated from those of distribution, which are spatial.
The concept of location in DA is sufﬁciently abstract that it admits another useful interpretation
that can be useful in computer security settings. The “location” of a computation can be considered
to be the principal on whose behalf the computation is executing. From this point of view, a local
resource is one that is accessible to a particular principal, and a mobile computation is one that can
be executed by any principal. Movement from one location to another may then be interpreted as
executing a piece of code on behalf of another principal, returning its result to the principal that
initiated the transfer.
41.1
Statics
The statics of DA is inspired by the possible worlds interpretation of modal logic. Under that inter-
pretation the truth of a proposition is considered relative to a world, which determines the state
of affairs described by that proposition. A proposition may be true in one world, and false in an-
other. For example, one may use possible worlds to model counter-factual reasoning, where one

392
41.1 Statics
postulates that certain facts that happen to be true in this, the actual, world, might be otherwise in
some other, possible, world. For instance, in the actual world you, the reader, are reading this book,
but in a possible world you may never have taken up the study of programming languages at all.
Of course not everything is possible: there is no possible world in which 2 + 2 is other than 4, for
example. Moreover, once a commitment has been made to one counter-factual, others are ruled
out. We say that one world is accessible from another when the ﬁrst is a sensible counter-factual
relative to the ﬁrst. So, for example, one may consider that relative to a possible world in which
you are the king, there is no further possible world in which someone else is also the king (there
being only one sovereign).
In DA we shall interpret possible worlds as sites on a network, with accessibility between
worlds expressing network connectivity. We postulate that every site is connected to itself (re-
ﬂexivity); that if one site is reachable from another, then the second is also reachable from the ﬁrst
(symmetry); and that if a site is reachable from a reachable site, then this site is itself reachable from
the ﬁrst (transitivity). From the point of view of modal logics, the type system of DA is derived
from the logic S5, for which accessibility is an equivalence relation.
The syntax of DA derives from that of CA. The following grammar summarizes the important
changes:
Typ
τ
::=
cmd[w](τ)
τ cmd[w]
commands
event[w](τ)
τ event[w]
events
Cmd
m
::=
at[w](m)
at w do m
change site
The command and event types are indexed by the site w at which they make sense. The command
at[w](m) changes the locus of execution from one site to another.
A signature Σ in DA consists of a ﬁnite set of declarations of the form a ~ τ @ w, where τ is a type
and w is a site. Such a declaration speciﬁes that a is a channel at site w carrying a payload of type
τ. We may think of a signature Σ as a family of signatures Σw one for each world w, containing the
declarations of the channels at that world. Partitioning channels in this way corresponds to the
idea that channels are located at a particular site. They may be handled passively at other sites,
but their only active role is at the site at which they are declared.
The statics of DA is given by the following judgment forms:
Γ ⊢Σ e : τ
expression typing
Γ ⊢Σ m ∼·· τ @ w
command typing
Γ ⊢Σ p proc @ w
process formation
Γ ⊢Σ α action @ w
action formation
The expression typing judgment is independent of the site, expressing the requirement that the
values of a type be meaningful at any site. On the other hand commands can only be executed at
a particular site, because their meaning depends on the resources at a site. Processes are similarly
conﬁned to execution at a site. Actions are site-speciﬁc; there is no inter-site synchronization.
The expressions of the command and event types of DA are deﬁned by the following rules:
Γ ⊢Σ m ∼·· τ @ w
Γ ⊢Σ cmd(m) : cmd[w](τ)
(41.1a)

41.1 Statics
393
Γ ⊢Σ never{τ} : event[w](τ)
(41.1b)
Σ ⊢a ~ τ @ w
Γ ⊢Σ rcv[a] : event[w](τ)
(41.1c)
Γ ⊢Σ e1 : event[w](τ)
Γ ⊢Σ e2 : event[w](τ)
Γ ⊢Σ or(e1; e2) : event[w](τ)
(41.1d)
Γ ⊢Σ e1 : event[w](τ1)
Γ, x : τ1 ⊢Σ e2 : τ2
Γ ⊢Σ wrap(e1; x.e2) : event[w](τ2)
(41.1e)
Rule (41.1a) states that the type of an encapsulated command records the site at which the com-
mand is executed. Rules (41.1b) to (41.1e) specify that events are attached to a site because channels
are. Communication among processes is conﬁned to a site; there is no inter-site synchronization.
The statics of the commands of DA is given by the following rules:
Γ ⊢Σ e : τ
Γ ⊢Σ ret(e) ∼·· τ @ w
(41.2a)
Γ ⊢Σ e1 : τ1 @ w
Γ, x : τ1 ⊢Σ m2 ∼·· τ2 @ w
Γ ⊢Σ bnd(e1; x.m2) ∼·· τ2 @ w
(41.2b)
Γ ⊢Σ e : cmd[w](unit)
Γ ⊢Σ spawn(e) ∼·· unit @ w
(41.2c)
Γ ⊢Σ e : τ
Σ ⊢a ~ τ @ w
Γ ⊢Σ snd[a](e) ∼·· unit @ w
(41.2d)
Γ ⊢Σ e : event[w](τ)
Γ ⊢Σ sync(e) ∼·· τ @ w
(41.2e)
Γ ⊢Σ m′ ∼·· τ′ @ w′
Γ ⊢Σ at[w′](m′) ∼·· τ′ @ w
(41.2f)
Rule (41.2a) states that an expression may be returned at any site, because its meaning is indepen-
dent of the site. Rule (41.2b) ensures that the sequential composition of commands is allowed only
within a site, and not across sites. Rule (41.2e) states that the sync command returns a value of
the same type as that of the event, and can be executed only at the site to which the given event
pertains. Rule (41.2d) states that a message can be sent along a channel available at the site from
which it is sent. Finally, rule (41.2f) states that to execute a command at a site w′ requires that the
command pertain to that site. The returned value is then passed to the original site.
Process formation is deﬁned as follows:
⊢Σ 1 proc @ w
(41.3a)
⊢Σ m ∼·· unit @ w
⊢Σ run(m) proc @ w
(41.3b)

394
41.2 Dynamics
⊢Σ p1 proc @ w
⊢Σ p2 proc @ w
⊢Σ p1 ⊗p2 proc @ w
(41.3c)
⊢Σ,a~τ@w p proc @ w
⊢Σ ν a ~ τ.p proc @ w
(41.3d)
These rules state that processes are sited. In particular an atomic process consists of a command
suitable for the site at which the process is run, and a new channel is allocated at the site of the
process that allocates it.
Action formation is deﬁned as follows:
⊢Σ ε action @ w
(41.4a)
⊢Σ e : τ
e valΣ
Σ ⊢a ~ τ @ w
⊢Σ a · e ! action @ w
(41.4b)
⊢Σ e : τ
e valΣ
Σ ⊢a ~ τ @ w
⊢Σ a · e ? action @ w
(41.4c)
Messages are values of type clsfd, and are meaningful only at the site at which the channel is
allocated. Locality of actions corresponds to conﬁnement of communication to a single site.
41.2
Dynamics
The dynamics of DA is a labeled transition judgment between processes at a site. Thus, the judg-
ment
p α @ w
7−−−→
Σ
p′
states that at site w the process p steps to the process p′, engendering the action α. It is deﬁned by
the following rules:
m α @ w
==⇒
Σ
ν Σ′ { m′ ⊗p }
run(m) α @ w
7−−−→
Σ
ν Σ′{run(m′) ⊗p}
(41.5a)
e valΣ
run(ret e) ε @ w
7−−→
Σ
1
(41.5b)
p1
α @ w
7−−−→
Σ
p′
1
p1 ⊗p2
α @ w
7−−−→
Σ
p′
1 ⊗p2
(41.5c)
p1
α @ w
7−−−→
Σ
p′
1
p2
α @ w
7−−−→
Σ
p′
2
p1 ⊗p2
ε @ w
7−−→
Σ
p′
1 ⊗p′
2
(41.5d)

41.2 Dynamics
395
p
α @ w
7−−−−−→
Σ,a~τ@w p′
⊢Σ α action @ w
ν a ~ τ.p α @ w
7−−−→
Σ
ν a ~ τ.p′
(41.5e)
These rules are like Rules (40.3), but for the sensitivity to the site at which execution takes place.
The site comes into play in Rules (41.5a) and (41.5e).
Rule (41.5a) makes use of the command execution judgment
m α @ w
==⇒
Σ
ν Σ′ { m′ ⊗p },
which states that the command m when executed at site w may engender the action α and in the
process create new channels, Σ′, and a new process p. (The result of the transition is not a process
expression, but a triple comprising the newly allocated channels, the newly created processes, and
a new command)
Command execution is deﬁned by the following rules:
spawn(cmd(m)) ε @ w
==⇒
Σ
ret(⟨⟩) ⊗run(m)
(41.6a)
e valΣ
⊢Σ e : τ
Σ ⊢a ~ τ @ w
snd[a](e) a·e ! @ w
====⇒
Σ
ret ⟨⟩
(41.6b)
e valΣ
⊢Σ e : τ
Σ ⊢a ~ τ @ w
sync(rcv[a]) a·e ? @ w
====⇒
Σ
ret(e)
(41.6c)
sync(e1) α @ w
==⇒
Σ
m1
sync(or(e1; e2)) α @ w
==⇒
Σ
m1
(41.6d)
sync(e2) α @ w
==⇒
Σ
m2
sync(or(e1; e2)) α @ w
==⇒
Σ
m2
(41.6e)
sync(e1) α @ w
==⇒
Σ
m1
sync(wrap(e1; x.e2)) α @ w
==⇒
Σ
bnd(cmd(m1); x.ret(e2))
(41.6f)
m α @ w′
===⇒
Σ
ν Σ′ { m′ ⊗p′ }
at[w′](m) α @ w
==⇒
Σ
ν Σ′ { at[w′](m′) ⊗p′ }
(41.6g)

396
41.3 Safety
e valΣ
at[w′](ret(e)) ε @ w
==⇒
Σ
ret(e)
(41.6h)
Rule (41.6a) states that new processes created at a site stay at that site—the new process executes
the given command at the current site. Rule (41.6b) speciﬁes that a send generates an event speciﬁc
to the site at which it occurs. Rules (41.6c) to (41.6f) specify that receive events occur only for chan-
nels allocated at the execution site. Rules (41.6g) and (41.6h) state that the command at[w′](m) is
executed at site w by executing m at site w′, and returning the result to the site w.
41.3
Safety
The safety theorem for DA ensures that synchronization on a channel may only occur at the site
on which the channel resides, even though channel references may be propagated from one site
to another during a computation. By the time the reference is resolved and synchronization is
attempted the computation will be executing at the right site.
Lemma 41.1 (Execution). If m
α @ w
==⇒
Σ
ν Σ′ { m′ ⊗p′ }, and ⊢Σ m ∼·· τ @ w, then ⊢Σ α action @ w,
⊢Σ Σ′ m′ ∼·· τ @ w, and ⊢Σ Σ′ p′ proc @ w.
Proof. By induction on rules (41.6).
Theorem 41.2 (Preservation). If p
α
7−−−→
Σ @ w p′ and ⊢Σ p proc @ w, then ⊢Σ p′ proc @ w.
Proof. By induction on the statics of DA, appealing to Lemma 41.1 for atomic processes.
The progress theorem states that the only impediment to execution of a well-typed program is
synchronizing on an event that never occurs.
Theorem 41.3 (Progress). If ⊢Σ p proc @ w, then either p ≡1 or there exists α and p′ such that
p
α
7−−−→
Σ @ w p′.
Proof. By induction on the dynamics of DA.
41.4
Notes
The use of a spatial modality to express locality and mobility constraints in a distributed program
was introduced in the experimental language ML5 (Murphy et al., 2004). Some languages for dis-
tributed computing consolidate concurrency with distribution by allowing cross-site interaction.
The idea of DA is to separate temporal from spatial considerations, limiting synchronization to a
single site, but allowing movement of the locus of execution from one site to another.

41.4 Notes
397
Exercises
41.1. The deﬁnition of DA given in this chapter has no means of allocating new channels, or send-
ing and receiving on them. Remedy this shortcoming by adding a command to create chan-
nel references. Give the statics and dynamics of this extension, and of any associated exten-
sions needed to account for it. Hint: the type of channel references, chan[w](τ), should be
indexed by the site of the channel to which the reference refers.
41.2. Given a channel reference e : chan[w′](τ), it is sensible to send a message asynchronously
from site w along this channel by providing a payload e′ : τ. It is also possible to implement
a synchronous remote send (also known as a remote procedure call) that sends a message e′ : τ
on a remote channel e : chan[w′](τ) and returns a result of type τ′ in response to the message.
Implement both of these capabilities in DA. Hint: Implement synchronous communication
using a reply channel as described in Chapter 39.

398
41.4 Notes

Part XVII
Modularity


Chapter 42
Modularity and Linking
Modularity is the most important technique for controlling the complexity of programs. Programs
are decomposed into separate components with precisely speciﬁed, and tightly controlled, interac-
tions. The pathways for interaction among components determine dependencies that constrain
the process by which the components are integrated, or linked, to form a complete system. Differ-
ent systems may use the same components, and a single system may use multiple instances of a
single component. Sharing of components amortizes the cost of their development across systems,
and helps limit errors by limiting coding effort.
Modularity is not limited to programming languages. In mathematics the proof of a theorem
is decomposed into a collection of deﬁnitions and lemmas. References among the lemmas deter-
mine a dependency relation that constrains their integration to form a complete proof of the main
theorem. Of course, one person’s theorem is another person’s lemma; there is no intrinsic limit
on the depth and complexity of the hierarchies of results in mathematics. Mathematical structures
are themselves composed of separable parts, for example, a ring comprises a group and a monoid
structure on the same underlying set.
Modularity arises from the structural properties of the hypothetical and general judgments.
Dependencies among components are expressed by free variables whose typing assumptions state
the presumed properties of the component. Linking amounts to substitution to discharge the
hypothesis.
42.1
Simple Units and Linking
Decomposing a program into units amounts to exploiting the transitivity of the hypothetical judg-
ment (see Chapter 3). The decomposition may be described as an interaction between two parties,
the client and the implementor, mediated by an agreed-upon contract, an interface. The client assumes
that the implementor upholds the contract, and the implementor guarantees that the contract will
be upheld. The assumption made by the client amounts to a declaration of its dependence on the
implementor discharged by linking the two parties accordng to their agreed-upon contract.

402
42.2 Initialization and Effects
The interface that mediates the interaction between a client and an implementor is a type. Link-
ing is the implementation of the composite structural rules of substitution and transitivity:
Γ ⊢eimpl : τintf
Γ, x : τintf ⊢eclient : τclient
Γ ⊢[eimpl/x]eclient : τclient
(42.1)
The type τintf is the interface type. It deﬁnes the operations provided by the implementor eimpl and
relied upon by the client eclient. The free variable x expresses the dependency of eclient on eimpl. That
is, the client accesses the implementation by using the variable x.
The interface type τintf is the contract between the client and the implementor. It determines
the properties of the implementation on which the client may depend and, at the same time, de-
termines the obligations that the implementor must fulﬁll. The simplest form of interface type is a
ﬁnite product type of the form ⟨f1 ,→τ1, . . . , fn ,→τn⟩, specifying a component with components
fi of type τi. Such a type is an application program interface, or API, because it determines the op-
erations that the client (application) may expect from the implementor. A more advanced form of
interface is one that deﬁnes an abstract type of the form ∃(t.⟨f1 ,→τ1, . . . , fn ,→τn⟩), which deﬁnes
an abstract type t representing the internal state of an “abstract machine” whose “instruction set”
consists of the operations f1, . . . , fn whose types may involve t. Being abstract, the type t is not
revealed to the client, but is known only to the implementor.1
Conceptually, linking is just substitution, but practically this can be implemented in many
ways. One method is separate compilation. The expressions eclient and eimpl, the source modules,
are translated (compiled) into another, lower-level, language, resulting in object modules. Linking
consists of performing the required substitution at the level of the object language in such a way
that the result corresponds to translating [eimpl/x]eclient. Another method, separate checking, shifts
the requirement for translation to the linker. The client and implementor units are checked for
type correctness with respect to the interface, but are not translated into lower-level form. Linking
then consists of translating the composite program as a whole, often resulting in a more efﬁcient
outcome than would be possible when compiling separately.
The foregoing are all forms of static linking because the program is composed before it is ex-
ecuted. Another method, dynamic linking, defers program composition until run-time, so that a
component is loaded only if it is actually required during execution. This might seem to involve
executing programs with free variables, but it does not. Each client implemented by a stub that
forwards accesses to a stored implementation (typically, in an ambient ﬁle system). The difﬁculty
with dynamic linking is that it refers to components by name (say, a path in a ﬁle system), and the
binding of that name may change at any time, wreaking havoc on program behavior.
42.2
Initialization and Effects
Linking resolves the dependencies among the components of a program by substitution. This
view is valid so long as the components are given by pure expressions, those that evaluate to a
value without inducing any effects. For in such cases there is no problem with the replication, or
1See Chapters 17 and 48 for a discussion of type abstraction.

42.2 Initialization and Effects
403
complete omission, of a component arising from repeated, or absent, uses of a variable represent-
ing it. But what if the expression deﬁning the implementation of a component has an effect when
evaluated? At a minimum replication of the component implies replication of its effects. Worse,
effects introduce implicit dependencies among components that are not apparent from their types.
For example, if each of two components mutates a shared assignable, the order in which they are
linked with a client program affects the behavior of the whole.
This may raise doubts about the treatment of linking as substitution, but on closer inspection it
becomes clear that implicit dependencies are naturally expressed by the modal distinction between
expressions and commands introduced in Chapter 34. Speciﬁcally, a component that may have
an effect when executed does not have type τintf of implementations of the interface type, but
rather the type τintf cmd of encapsulated commands that, when executed, have effects and yield
implementations. Being encapsulated, a value of this type is itself free of effects, but it may have
effects when evaluated.
The distinction between the types τintf and τintf cmd is mediated by the sequencing command
introduced in Chapter 34. For the sake of generality, let us assume that the client is itself an
encapsulated command of type τclient cmd, so that it may itself have effects when executed, and may
serve as a component of a yet larger system. Assuming that the client refers to the encapsulated
implementation by the variable x, the command
bnd x ←x ; do eclient
ﬁrst determines the implementation of the interface by running the encapsulated command x then
running the client code with the result bound to x. The implicit dependencies of the client on
the implementor are made explicit by the sequencing command, which ensures that the imple-
mentor’s effects occur prior to those of the client, precisely because the client depends on the
implementor for its execution.
More generally, to manage such interactions in a large program it is common to isolate an
initialization procedure whose role is to stage the effects engendered by the various components
according to some policy or convention. Rather than attempt to survey all possible policies, let us
just note that the upshot of such conventions is that the initialization procedure is a command of
the form
{x1 ←x1 ; . . . xn ←xn ; mmain},
where x1, . . . , xn represent the components of the system and mmain is the main (startup) routine.
After linking the initialization procedure has the form
{x1 ←e1 ; . . . xn ←en ; mmain},
where e1, . . . , en are the encapsulated implementations of the linked components. When the ini-
tialization procedure is executed, it results in the substitution
[v1, . . . , vn/x1, . . . , xn]mmain,
where the expressions v1, . . . , vn represent the values resulting from executing e1, . . . , en, respec-
tively, and the implicit effects have occurred in the order speciﬁed by the initializer.

404
42.3 Notes
42.3
Notes
The relationship between the structural properties of entailment and the practical problem of sep-
arate development was implicit in much early work on programming languages, but became ex-
plicit once the correspondence between propositions and types was developed. There are many
indications of this correspondence in sources such as Proofs and Types (Girard, 1989) and Intuition-
istic Type Theory (Martin-L¨of, 1984), but it was ﬁrst made explicit by Cardelli (1997).

Chapter 43
Singleton Kinds and Subkinding
The expression let e1 : τ be x in e2 is a form of abbreviation mechanism by which we may bind e1
to the variable x for use within e2. In the presence of function types this expression is deﬁnable
as the application (λ (x : τ) e2)(e1), which accomplishes the same thing. It is natural to consider
an analogous form of let expression which binds a type to a type variable within a scope. Using
def t is τ in e to bind the type variable t to τ within the expression e, we may write expressions
such as
def t is nat × nat in λ (x : t) s(x · l),
which introduces a type abbreviation within an expression. To ensure that this expression is well-
typed, the type variable t is to be synonymous with the type nat × nat, for otherwise the body of
the λ-abstraction is not type correct.
Following the pattern of the expression-level let, we might guess that def t is τ in e abbrevi-
ates the polymorphic instantiation Λ(t) e[τ], which binds t to τ within e. Doing so captures the
dynamics of type abbreviation, but it fails to adhere to the intended statics. The difﬁculty is that,
according to this interpretation of type deﬁnitions, the expression e is type-checked in the absence
of any knowledge of the binding of t, rather than in the knowledge that t is synonymous with τ.
Thus, in the above example, the expression s(x · l) would fail to type check, unless the binding of
t were exposed.
Interpreting type deﬁnition in terms of type abstraction and type application fails. One solution
is to consider type abbreviation to be a primitive notion with the following statics:
Γ ⊢[τ/t]e : τ′
Γ ⊢def t is τ in e : τ′
(43.1)
This formulation would solve the problem of type abbreviation, but in an ad hoc way. Is there a
more general solution?
There is, by introducing singleton kinds, which classify type constructors by revealing their
identity. Singletons solve not only the type deﬁnition problem, but play a crucial role in the design
of module systems (as described in Chapters 44 and 45.)

406
43.1 Overview
43.1
Overview
The central organizing principle of type theory is compositionality. To ensure that a program can be
decomposed into separable parts, we ensure that the composition of a program from constituent
parts is mediated by the types of those parts. Put in other terms, the only thing that one part of a
program “knows” about another is its type. For example, the formation rule for addition of natural
numbers depends only on the type of its arguments (both have type nat), and not on their speciﬁc
form or value. But in the case of a type abbreviation of the form def t is τ in e, the principle of
compositionality dictates that the only thing that e “knows” about the type variable t is its kind,
namely T, and not its binding, namely τ. The proposed representation of type abbreviation as the
combination of type abstraction and type application meets this requirement, but it does not have
the intended meaning!
We could, as suggested in the introduction, abandon the core principles of type theory, and
introduce type abbreviations as a primitive notion. But there is no need to do so. Instead we need
a kind for t that captures its identity; such a kind is called a singleton kind. Informally, the kind S(τ)
is the kind of types that are deﬁnitionally equal to τ. That is, up to deﬁnitional equality, this kind
has only one inhabitant, namely τ. Consequently, if u :: S(τ) is a variable of singleton kind, then
within its scope, the variable u is synonymous with τ. Thus we may represent def t is τ in e by
Λ(t :: S(τ)) e[τ], which correctly propagates the identity of t, namely τ, to e during type checking.
The formalization of singleton kinds requires some more machinery at the constructor and kind
level. First, we capture the idea that a constructor of singleton kind is a fortiori a constructor of kind
T, and hence is a type. Otherwise, a variable u of singleton kind cannot be used as a type, even
though it is explicitly deﬁned to be one! To avoid this problem, we introduce a subkinding relation
κ1 <:: κ2. The fundamental axiom of subkinding is S(τ) <:: T, stating that every constructor of
singleton kind is a type. Second, we account for constructors occurring within kinds. A singleton
kind is a dependent kind in that its meaning depends on a constructor. Put another way, S(τ) is
a family of kinds indexed by constructors of kind T. Products and functions are generalized to
dependent products and dependent functions of families. The dependent product kind, Σ u :: κ1.κ2,
classiﬁes pairs ⟨c1,c2⟩such that c1 :: κ1, as might be expected, and c2 :: [c1/u]κ2, in which the
kind of the second component is sensitive to the ﬁrst component itself, and not just its kind. The
dependent function kind, Π u :: κ1.κ2, classiﬁes functions that, when applied to a constructor c1 ::
κ1, results in a constructor of kind [c1/u]κ2. Note that the kind of the result is sensitive to the
argument, and not just to its kind.
Third, it is useful to consider singletons not just of kind T, but also of higher kinds. To support
this we introduce higher singletons, written S(c :: κ), where κ is a kind and c is a constructor of kind
κ. These are deﬁnable in terms of the basic form of singleton kinds using dependent function and
product kinds.
43.2
Singletons
A singleton kind has the form S(c), where c is a constructor. The singleton classiﬁes all constructors
that are equivalent to the constructor c. For the time being we consider singleton kinds in the
context of the language Fω described in Chapter 18, which includes a kind of types, and is closed

43.2 Singletons
407
under product and function kinds. In Section 43.3 we will enrich the language of kinds in a way
that will ensure that the product and function kinds of Fω are deﬁnable.
The statics of singletons uses the following judgment forms:
∆⊢κ kind
kind formation
∆⊢κ1 ≡κ2
kind equivalence
∆⊢c :: κ
constructor formation
∆⊢c1 ≡c2 :: κ
constructor equivalence
∆⊢κ1 <:: κ2
subkinding
These judgments are deﬁned simultaneously by a collection of rules including the following:
∆⊢c :: Type
∆⊢S(c) kind
(43.2a)
∆⊢c :: Type
∆⊢c :: S(c)
(43.2b)
∆⊢c :: S(d)
∆⊢c ≡d :: Type
(43.2c)
∆⊢c :: κ1
∆⊢κ1 <:: κ2
∆⊢c :: κ2
(43.2d)
∆⊢c :: Type
∆⊢S(c) <:: Type
(43.2e)
∆⊢c ≡d :: Type
∆⊢S(c) ≡S(d)
(43.2f)
∆⊢κ1 ≡κ2
∆⊢κ1 <:: κ2
(43.2g)
∆⊢κ1 <:: κ2
∆⊢κ2 <:: κ3
∆⊢κ1 <:: κ3
(43.2h)
Omitted for brevity are rules stating that constructor and kind equivalence are reﬂexive, symmet-
ric, transitive, and preserved by kind and constructor formation.
Rule (43.2b) expresses the principle of “self-recognition,” which states that every constructor
c of kind Type also has the kind S(c). By rule (43.2c) any constructor of kind S(c) is deﬁnition-
ally equal to c. Consequently, self-recognition expresses the reﬂexivity of constructor equiva-
lence. Rule (43.2e) is just the subsumption principle re-stated at the level of constructors and
kinds. Rule (43.2f) states that the singleton kind respects equivalence of its constructors, so that
equivalent constructors determine the same singletons. Rules (43.2g) and (43.2h) state that the
subkinding relation is a pre-order that respects kind equivalence.
To see these rules in action let us consider a few illustrative examples. First, consider the
behavior of variables of singleton kind. Suppose that ∆⊢u :: S(c) is such a variable. Then by

408
43.3 Dependent Kinds
rule (43.2c) we may deduce that ∆⊢u ≡c :: T. Thus, declaring u with a singleton kind deﬁnes it
to be the constructor speciﬁed by its kind.
Taking this a step further, the existential type ∃u :: S(c).τ is the type of packages whose repre-
sentation type is (equivalent to) c—it is an abstract type whose identity is revealed by assigning
it a singleton kind. By the general principles of equivalence we have that the type ∃u :: S(c).τ is
equivalent to the type ∃:: S(c).[c/u]τ, wherein we have propagated the equivalence of u and c
into the type τ. On the other hand we may also “forget” the deﬁnition of u, because the subtyping
∃u :: S(c).τ <: ∃u :: T.τ
is derivable using the following variance rule for existential types over a kind:
∆⊢κ1 <:: κ2
∆, u :: κ1 ⊢τ1 <: τ2
∆⊢∃u :: κ1.τ1 <: ∃u :: κ2.τ2
(43.3)
Similarly, we may derive the subtyping
∀u :: T.τ <: ∀u :: S(c).τ
from the following variance rule for universals over a kind:
∆⊢κ2 <:: κ1
∆, u :: κ2 ⊢τ1 <: τ2
∆⊢∀u :: κ1.τ1 <: ∀u :: κ2.τ2
(43.4)
Informally, the displayed subtyping states that a polymorphic function that may be applied to any
type is one that may only be applied to a particular type c.
These examples show that singleton kinds express the idea of a scoped deﬁnition of a type
variable in a way that is not tied to an ad hoc deﬁnition mechanism, but arises naturally from
general principles of binding and scope. We will see in Chapters 44 and 45 more advanced uses of
singletons to manage the interaction among program modules.
43.3
Dependent Kinds
Although it is perfectly possible to add singleton kinds to the framework of higher kinds intro-
duced in Chapter 18, to do so would be to short-change the expressiveness of the language. Using
higher kinds we can express the kind of constructors that, when applied to a type, yield a spe-
ciﬁc type, say int, as result, namely T →S(int). But we cannot express the kind of constructors
that, when applied to a type, yield that very type as result, for there is no way for the result kind
to refer to the argument of the function. Similarly, using product kinds we can express the kind
of pairs whose ﬁrst component is int and whose second component is an arbitrary type, namely
S(int) × T. But we cannot express the kind of pairs whose second component is equivalent to its
ﬁrst component, for there is no way for the kind of the second component to make reference to the
ﬁrst component itself.
To express such concepts requires that product and function kinds be generalized so that the
kind of the second component of a pair may mention the ﬁrst component of that pair, or the kind

43.3 Dependent Kinds
409
of the result of a function may mention the argument to which it is applied. Such kinds are called
dependent kinds because they involve kinds that mention, or depend upon, constructors (of kind T).
The syntax of dependent kinds is given by the following grammar:
Kind
κ
::=
S(c)
S(c)
singleton
Σ(κ1; u.κ2)
Σ u :: κ1.κ2
dependent product
Π(κ1; u.κ2)
Π u :: κ1.κ2
dependent function
Con
c
::=
u
u
variable
pair(c1; c2)
⟨c1,c2⟩
pair
proj[l](c)
c · l
ﬁrst projection
proj[r](c)
c · r
second projection
lam{κ}(u.c)
λ (u :: κ) c
abstraction
app(c1; c2)
c1[c2]
application
As a notational convenience, when there is no dependency in a kind we write κ1 × κ2 for Σ :: κ1.κ2,
and κ1 →κ2 for Π :: κ1.κ2, where the “blank” stands for an irrelevant variable.
The dependent product kind Σ u :: κ1.κ2 classiﬁes pairs ⟨c1,c2⟩of constructors in which c1 has
kind κ1 and c2 has kind [c1/u]κ2. For example, the kind Σ u :: T.S(u) classiﬁes pairs ⟨c,c⟩, where c is
a constructor of kind T. More generally, this kind classiﬁes pairs of the form ⟨c1,c2⟩where c1 and c2
are equivalent, but not necessarily identical, constructors. The dependent function kind Π u :: κ1.κ2
classiﬁes constructors c that, when applied to a constructor c1 of kind κ1 yield a constructor of
kind [c1/u]κ2. For example, the kind Π u :: T.S(u) classiﬁes constructors that, when applied to a
constructor c yield a constructor equivalent to c; a constructor of this kind is essentially the identity
function. We may, of course, combine these to form kinds such as
Π u :: T × T.S(u · r) × S(u · l),
which classiﬁes functions that swap the components of a pair of types. (Such examples suggest
that the behavior of a constructor may be pinned down precisely using dependent kinds. We shall
see in Section 43.4 that this is the case.)
The formation, introduction, and elimination rules for the product kind are as follows:
∆⊢κ1 kind
∆, u :: κ1 ⊢κ2 kind
∆⊢Σ u :: κ1.κ2 kind
(43.5a)
∆⊢c1 :: κ1
∆⊢c2 :: [c1/u]κ2
∆⊢⟨c1,c2⟩:: Σ u :: κ1.κ2
(43.5b)
∆⊢c :: Σ u :: κ1.κ2
∆⊢c · l :: κ1
(43.5c)
∆⊢c :: Σ u :: κ1.κ2
∆⊢c · r :: [c · l/u]κ2
(43.5d)
In rule (43.5a), note that the variable u may occur in the kind κ2 by appearing in a singleton kind.
Correspondingly, rules (43.5b), (43.5c), and (43.5d) substitute a constructor for this variable.

410
43.3 Dependent Kinds
The following equivalence axioms govern the constructors associated with the dependent prod-
uct kind:
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩· l ≡c1 :: κ1
(43.6a)
∆⊢c1 :: κ1
∆⊢c2 :: κ2
∆⊢⟨c1,c2⟩· r ≡c2 :: κ2
(43.6b)
The subkinding rule for the dependent product kind speciﬁes that it is covariant in both posi-
tions:
∆⊢κ1 <:: κ′
1
∆, u :: κ1 ⊢κ2 <:: κ′
2
∆⊢Σ u :: κ1.κ2 <:: Σ u :: κ′
1.κ′
2
(43.7)
The congruence rule for equivalence of dependent product kinds is formally similar:
∆⊢κ1 ≡κ′
1
∆, u :: κ1 ⊢κ2 ≡κ′
2
∆⊢Σ u :: κ1.κ2 ≡Σ u :: κ′
1.κ′
2
(43.8)
Notable consequences of these rules include the subkindings
Σ u :: S(int).S(u) <:: Σ u :: T.S(u)
and
Σ u :: T.S(u) <:: T × T,
and the equivalence
Σ u :: S(int).S(u) ≡S(int) × S(int).
Subkinding is used to “forget” information about the identity of the components of a pair, and
equivalence is used to propagate such information within a kind.
The formation, introduction, and elimination rules for dependent function kinds are as follows:
∆⊢κ1 kind
∆, u :: κ1 ⊢κ2 kind
∆⊢Π u :: κ1.κ2 kind
(43.9a)
∆, u :: κ1 ⊢c :: κ2
∆⊢λ (u :: κ1) c :: Π u :: κ1.κ2
(43.9b)
∆⊢c :: Π u :: κ1.κ2
∆⊢c1 :: κ1
∆⊢c[c1] :: [c1/u]κ2
(43.9c)
Rule (43.9b) speciﬁes that the result kind of a λ-abstraction depends uniformly on the argument u.
Correspondingly, rule (43.9c) speciﬁes that the kind of an application is obtained by substitution
of the argument into the result kind of the function itself.
The following rule of equivalence governs the constructors associated with the dependent
product kind:
∆, u :: κ1 ⊢c :: κ2
∆⊢c1 :: κ1
∆⊢(λ (u :: κ1) c)[c1] ≡[c1/u]c :: κ2
(43.10)

43.4 Higher Singletons
411
The subkinding rule for the dependent function kind speciﬁes that it is contravariant in its
domain and covariant in its range:
∆⊢κ′
1 <:: κ1
∆, u :: κ′
1 ⊢κ2 <:: κ′
2
∆⊢Π u :: κ1.κ2 <:: Π u :: κ′
1.κ′
2
(43.11)
The equivalence rule is similar, except that the symmetry of equivalence obviates a choice of vari-
ance:
∆⊢κ1 ≡κ′
1
∆, u :: κ1 ⊢κ2 ≡κ′
2
∆⊢Π u :: κ1.κ2 ≡Π u :: κ′
1.κ′
2
(43.12)
Rule (43.11) gives rise to the subkinding
Π u :: T.S(int) <:: Π u :: S(int).T,
which illustrates the co- and contravariance of the dependent function kind. In particular a func-
tion that takes any type and delivers the type int is also a function that takes the type int and
delivers a type. Rule (43.12) gives rise to the equivalence
Π u :: S(int).S(u) ≡S(int) →S(int),
which propagates information about the argument into the range kind. Combining these two rules
we may derive the subkinding
Π u :: T.S(u) <:: S(int) →S(int).
Intuitively, a constructor function that yields its argument is, in particular, a constructor function
that may only be applied to int, and yields int. Formally, by contravariance we have the sub-
kinding
Π u :: T.S(u) <:: Π u :: S(int).S(u),
and by sharing propagation we may derive the indicated superkind.
43.4
Higher Singletons
Although singletons are restricted to constructors of kind T, we may use dependent product and
function kinds to deﬁne singletons of every kind. Speciﬁcally, we wish to deﬁne the kind S(c :: κ),
where c is of kind κ, that classiﬁes constructors equivalent to c. When κ = T this is, of course, just
S(c); the problem is to deﬁne singletons for the higher kinds Σ u :: κ1.κ2 and Π u :: κ1.κ2.
Suppose that c :: κ1 × κ2. The singleton kind S(c :: κ1 × κ2) classiﬁes constructors equivalent
to c. If we assume, inductively, that singletons are deﬁned for κ1 and κ2, then we need only
note that c is equivalent to ⟨c · l,c · r⟩. For then the singleton S(c :: κ1 × κ2) can be deﬁned to
be S(c · l :: κ1) × S(c · r :: κ2). Similarly, suppose that c :: κ1 →κ2. Using the equivalence of c and
λ (u :: κ1 →κ2) c[u], we may deﬁne S(c :: κ1 →κ2) to be Π u :: κ1.S(c[u] :: κ2).

412
43.4 Higher Singletons
In general the kind S(c :: κ) is deﬁned by induction on the structure of κ by the following kind
equivalences:
∆⊢c :: S(c′)
∆⊢S(c :: S(c′)) ≡S(c)
(43.13a)
∆⊢c :: Σ u :: κ1.κ2
∆⊢S(c :: Σ u :: κ1.κ2) ≡Σ u :: S(c · l :: κ1).S(c · r :: κ2)
(43.13b)
∆⊢c :: Π u :: κ1.κ2
∆⊢S(c :: Π u :: κ1.κ2) ≡Π u :: κ1.S(c[u] :: κ2)
(43.13c)
The sensibility of these equations relies on rule (43.2c) together with the following principles of
constructor equivalence, called extensionality principles:
∆⊢c :: Σ u :: κ1.κ2
∆⊢c ≡⟨c · l,c · r⟩:: Σ u :: κ1.κ2
(43.14a)
∆⊢c :: Π u :: κ1.κ2
∆⊢c ≡λ (u :: κ1) c[u] :: Π u :: κ1.κ2
(43.14b)
Rule (43.2c) states that the only constructors of kind S(c′) are those equivalent to c′, and rules (43.14a)
and (43.14b) state that the only members of the dependent product and function types are, respec-
tively, pairs and λ-abstractions of the right kinds.
Finally, the following self-recognition rules are required to ensure that rule (43.2b) extends to
higher kinds.
∆⊢c · l :: κ1
∆⊢c · r :: [c · l/u]κ2
∆⊢c :: Σ u :: κ1.κ2
(43.15a)
∆, u :: κ1 ⊢c[u] :: κ2
∆⊢c :: Π u :: κ1.κ2
(43.15b)
An illustrative case arises when u is a constructor variable of kind Σ v :: T.S(v). We may derive that
u · l :: S(u · l) using rule (43.2b). We may also derive u · r :: S(u · l) using rule (43.5d). Therefore,
by rule (43.15a), we may derive u :: Σ v :: S(u · l).S(u · l), which is a subkind of Σ v :: T.S(v). This
more precise kind is a correct kinding for u, because the ﬁrst component of u is u · l, and the
second component of u is equivalent to the ﬁrst component, and hence is also u · l. But without
rule (43.15a) it is impossible to derive this fact.
The point of introducing higher singletons is to ensure that every constructor can be classiﬁed
by a kind that determines it up to deﬁnitional equality. Viewed as extending singleton types, we
would expect that higher singletons enjoy similar properties.
Theorem 43.1. If ∆⊢c :: κ, then ∆⊢S(c :: κ) <:: κ and ∆⊢c :: S(c :: κ).
The proof of this theorem is beyond the scope of this text.

43.5 Notes
413
43.5
Notes
Singleton kinds were introduced by Stone and Harper (2006) to isolate the concept of type sharing
that arises in the ML module system (Milner et al., 1997; Harper and Lillibridge, 1994; Leroy, 1994).
The meta-theory of singleton kinds is surprisingly intricate. The main source of complexity arises
from constructor-indexed families of kinds. If u :: κ ⊢c′ :: κ′, and if c1 :: κ and c2 : κ are distinct,
but equivalent, then so are the instances [c1/u]κ′ and [c2/u]κ′. Managing kind equivalence raises
signiﬁcant technical difﬁculties in the proofs.
Exercises
43.1. Show that Rules (43.5c) and (43.5d) are inter-derivable with the following two rules:
∆⊢c :: κ1 × κ2
∆⊢c · l :: κ1
(43.16a)
∆⊢c :: κ1 × κ2
∆⊢c · r :: κ2
.
(43.16b)
43.2. Show that Rule (43.9c) is inter-derivable with the rule
∆⊢c :: κ1 →κ2
∆⊢c1 :: κ1
∆⊢c[c1] :: κ2
.
(43.17)
43.3. It is useful to modify a kind κ by imposing on κ a deﬁnition of one of its components.
A component of a kind is speciﬁed by a simple path consisting of a ﬁnite, possibly empty,
sequence of symbols l and r thought of as a tree address within a kind. The path projection
c · p of a constructor c of kind κ by a path p is inductively deﬁned by these equations:
c · ε ≜c
c · (l p) ≜(c · l) · p
c · (r p) ≜(c · r) · p
If ∆, u :: κ ⊢c :: κr, then the patched kind κ{r := c} is the kind κ1 × S(c :: κr). It has the
property that
∆, u :: κ{r := c} ⊢u · r ≡c :: κr.
Deﬁne ∆⊢κ{p := c} kind, where ∆⊢κ kind, ∆, u :: κ ⊢u · p :: κp, and ∆⊢c :: κc, to be such
that ∆⊢κ{p := c} <:: κ and ∆, u :: κ{p := c} ⊢u · p ≡c :: κc.

414
43.5 Notes
43.4. Patching is used to constrain a component of a kind to be equivalent to a speciﬁed construc-
tor. A sharing speciﬁcation imposed on a kind ensures that a deﬁnitional equality holds of any
constructor of that kind. Informally, the kind u :: κ / u · p ≡u · q is a subkind κ′ of κ such that
∆, u :: κ′ ⊢u · p ≡u · q :: κ′′.
(43.18)
For example, the kind u :: T × T / u · l ≡u · r classiﬁers pairs of types whose left and right
components are deﬁnitionally equal.
Suppose that ∆⊢κ kind is a well-formed kind, and that ∆, u :: κ ⊢u · p :: κp and ∆, u :: κ ⊢
u · q :: κq are well-formed paths. Deﬁne ∆⊢u :: κ / p ≡q kind to be the kind κ′ speciﬁed by
Equation (43.18). Hint: Make use of the answer to Exercise 43.3.

Chapter 44
Type Abstractions and Type Classes
An interface is a contract that speciﬁes the rights of a client and the responsibilities of an imple-
mentor. Being a speciﬁcation of behavior, an interface is a type. In principle any type may serve
as an interface, but in practice it is usual to structure code into modules consisting of separable
and reusable components. An interface speciﬁes the behavior of a module expected by a client
and imposed on the implementor. It is the fulcrum balancing the tension between separation and
integration. As a rule, a module ought to have a well-deﬁned behavior that can be understood
separately, but it is equally important that it be easy to combine modules to form an integrated
whole.
A fundamental question is, what is the type of a module? That is, what form should an interface
take? One long-standing idea is that an interface is a labeled tuple of functions and procedures
with speciﬁed types. The types of the ﬁelds of the tuple are often called function headers, because
they summarize the call and return types of each function. Using interfaces of this form is called
procedural abstraction, because it limits the dependencies between modules to a speciﬁed set of
procedures. We may think of the ﬁelds of the tuple as being the instruction set of a virtual machine.
The client makes use of these instructions in its code, and the implementor agrees to provide their
implementations.
The problem with procedural abstraction is that it does not provide as much insulation as one
might like. For example, a module that implements a dictionary must expose in the types of its
operations the exact representation of the tree as, say, a recursive type (or, in more rudimentary
languages, a pointer to a structure that itself may contain such pointers). Yet the client ought not
depend on this representation: the purpose of abstraction is to get rid of pointers. The solution,
as discussed in Chapter 17, is to extend the abstract machine metaphor to allow the internal state
of the machine to be hidden from the client. In the case of a dictionary the representation of the
dictionary as a binary search tree is hidden by existential quantiﬁcation. This concept is called type
abstraction, because the type of the underlying data (state of the abstract machine) is hidden.
Type abstraction is a powerful method for limiting the dependencies among the modules that
constitute a program. It is very useful in many circumstances, but is not universally applicable.
It is often useful to expose, rather than obscure, type information across a module boundary. A
typical example is the implementation of a dictionary, which is a mapping from keys to values. To

416
44.1 Type Abstraction
use, say, a binary search tree to implement a dictionary, we require that the key type admit a total
ordering with which keys can be compared. The dictionary abstraction does not depend on the
exact type of the keys, but only requires that the key type be constrained to provide a comparison
operation. A type class is a speciﬁcation of such a requirement. The class of comparable types, for
example, speciﬁes a type t together with an operation leq of type (t × t) →bool with which to
compare them. Superﬁcially, such a speciﬁcation looks like a type abstraction, because it speciﬁes
a type and one or more operations on it, but with the important difference that the type t is not
hidden from the client. For if it were, the client would only be able to compare keys using leq, but
would have no means of obtaining keys to compare. A type class, in contrast to a type abstraction,
is not intended to be an exhaustive speciﬁcation of the operations on a type, but as a constraint
on its behavior expressed by demanding that certain operations, such as comparison, be available,
without limiting the other operations that might be deﬁned on it.
Type abstractions and type classes are the extremal cases of a general concept of module type
that we shall discuss in detail in this chapter. The crucial idea is the controlled revelation of type
information across module boundaries.
Type abstractions are opaque; type classes are trans-
parent. These are both instances of translucency, which arises from combining existential types
(Chapter 17), subtyping (Chapter 24), and singleton kinds and subkinding (Chapter 43). Unlike in
Chapter 17, however, we will distinguish the types of modules, which are called signatures, from
the types of ordinary values. The distinction is not essential, but it will be helpful to keep the two
concepts separate at the outset, deferring discussion of how to ease the segregation once the basic
concepts are in place.
44.1
Type Abstraction
Type abstraction is captured by a form of existential type quantiﬁcation similar to that described
in Chapter 17. For example, a dictionary with keys of type τkey and values of type τval implements
the signature σdict deﬁned by Jt :: T ; τdictK, where τdict is the labeled tuple type
⟨emp ,→t , ins ,→τkey × τval × t →t , fnd ,→τkey × t →τval opt⟩.
The type variable t occurring in τdict and bound by σdict is the abstract type of dictionaries on which
are deﬁned three operations emp, ins, and fnd with the speciﬁed types. The type τval is immaterial
to the discussion, because the dictionary operations impose no restrictions on the values that are
associated to keys. However, it is important that the type τkey be some ﬁxed type, such as str,
equipped with a suite of operations such as comparison. Observe that the signature σdict merely
speciﬁes that a dictionary is a value of some type that admits the operations emp, ins, and fnd with
the types given by τdict.
An implementation of the signature σdict is a structure Mdict of the form Jρdict ; edictK, where ρdict
is some concrete representation of dictionaries, and edict is a labeled tuple of type [ρdict/t]τdict of
the general form
⟨emp ,→. . . , ins ,→. . . , fnd ,→. . .⟩.
The elided parts implement the dictionary operations in terms of the chosen representation type
ρdict making use of the comparison operation that we assume is available of values of type τkey. For

44.1 Type Abstraction
417
example, the type ρdict might be a recursive type deﬁning a balanced binary search tree, such as a
red-black tree. The dictionary operations work on the underlying representation of the dictionary
as such a tree, just as would a package of existential type (see Chapter 17). The supposition about
τkey is temporary, and is lifted in Section 44.2.
To ensure that the representation of the dictionary is hidden from a client, the structure Mdict
is sealed with the signature σdict to obtain the module
Mdict ↿σdict.
The effect of sealing is to ensure that the only information about Mdict that propagates to the client
is given by σdict. In particular, because σdict only speciﬁes that the type t have kind T, no informa-
tion about the choice of t as ρdict in Mdict is made available to the client.
A module is a two-phase object consisting of a static part and a dynamic part. The static part is
a constructor of a speciﬁed kind; the dynamic part is a value of a speciﬁed type. There are two
elimination forms that extract the static and dynamic parts of a module. These are, respectively, a
form of constructor and a form of expression. More precisely, the constructor M · s stands for the
static part of M, and the expression M · d stands for its dynamic part. According to the inversion
principle, if a module M has introduction form, then M · s should be equivalent to the static part
of M. So, for example, Mdict · s should be equivalent to ρdict.
But consider the static part of a sealed module, which has the form (Mdict ↿σdict) · s. Because
sealing hides the representation of an abstract type, this constructor should not be equivalent
to ρdict.
If M′
dict is another implementation of σdict, should (Mdict ↿σdict) · s be equivalent to
(M′
dict ↿σdict) · s? To ensure reﬂexivity of type equivalence this equation should hold when M
and M′ are equivalent modules. But this violates representation independence for abstract types
by making equivalence of abstract types sensitive to their implementation.
It would seem, then, that there is a contradiction between two very fundamental concepts,
type equivalence and representation independence. The way out of this conundrum is to disallow
reference to the static part of a sealed module: the type expression M ↿σ · s is deemed ill-formed.
More generally, the formation of M · s is disallowed unless M is a module value, whose static part
is always manifest. An explicit structure is a module value, as is any module variable (provided
that module variables are bound by-value).
One effect of this restriction is that sealed modules must be bound to a variable before they
are used. Because module variables are bound by-value, doing so has the effect of imposing ab-
straction at the binding site. In fact, we may think of sealing as a kind of computational effect
that “occurs” at the binding site, much as the bind operation in Algol discussed in Chapter 34
engenders the effects induced by an encapsulated command. As a consequence two bindings of
the same sealed module result in two abstract types. The type system willfully ignores the iden-
tity of the two occurrences of the same module in order to ensure that their representations can be
changed independently of one another without disrupting the behavior of any client code (because
the client cannot rely on their identity, it must regard them as different).

418
44.2 Type Classes
44.2
Type Classes
Type abstraction is an essential tool for limiting dependencies among modules in a program. The
signature of a type abstraction determines all that is known about a module by a client; no other
uses of the values of an abstract type are permissible. A complementary tool is to use a signature to
partially specify the capabilities of a module. Such a signature is a type class, or a view; an instance
of the type class is an implementation of it. Because the signature of a type class only constrains
the minimum capabilities of an unknown module, there must be some other means of working
with values of that type. The way to achieve this is to expose, rather than hide, the identity of
the static part of a module. In this sense type classes are the “opposite” of type abstractions, but
we shall see below that there is a smooth progression between them, mediated by a subsignature
judgment.
Let us consider the implementation of dictionaries as a client of the implementation of its
keys. To implement a dictionary using a binary search tree the only requirement is that keys
come equipped with a total ordering given by a comparison operation. This requirement can be
expressed by a signature σord given by
Jt :: T ; ⟨leq ,→(t × t) →bool⟩K .
Because a given type can be ordered in many ways, it is essential that the ordering be packaged
with the type to determine a type of keys.
The implementation of dictionaries as binary search trees takes the form
X : σord ⊢MX
bstdict : σX
dict.
Here σX
dict is the signature
q
t :: T ; τX
dict
y
, whose body, τX
dict, is the tuple type
⟨emp ,→t , ins ,→X · s × τval × t →t , fnd ,→X · s × t →τval opt⟩,
and MX
bstdict is a structure (not given explicitly here) that implements the dictionary operations
using binary search trees.1 Within MX
bstdict, the static and dynamic parts of the module X are
accessed by writing X · s and X · d, respectively. In particular, the comparison operation on keys is
accessed by the projection X · d · leq.
The declared signature of the module variable X expresses a constraint on the capabilities of a
key type by specifying an upper bound on its signature in the subsignature ordering. So any mod-
ule bound to X must provide a type of keys and a comparison operation on that type, but nothing
else is assumed of it. Because this is all we know about the unknown module X the dictionary
implementation is constrained to rely only on these speciﬁed capabilities, and no others. When
linking with a module deﬁning X, the implementation need not be sealed with this signature, but
must instead have a signature that is no larger than it in the subsignature relation. Indeed, the
signature σord is useless for sealing, as is easily seen by example. Suppose that Mnatord : σord is
an instance of the class of ordered types under the usual ordering. If we seal Mnatord with σord by
writing
Mnatord ↿σord,
1Here and elsewhere in this chapter and the next, the superscript X serves as a reminder that the module variable X
may occur free in the annotated module or signature.

44.2 Type Classes
419
the resulting module is useless, because we would then have no way to create values of the key
type.
We see, then, that a type class is a description (or view) of a pre-existing type, and is not a means
of introducing a new type. Rather than obscure the identity of the static part of Mnatord, we wish
to propagate its identity as nat while specifying a comparison with which to order them. Type
identity propagation is achieved using singleton kinds (as described in Chapter 43). Speciﬁcally,
the most precise, or principal, signature of a structure is the one that exposes its static part using a
singleton kind. In the case of the module Mnatord, the principal signature is the signature σnatord
given by
Jt :: S(nat) ; leq ,→(t × t) →boolK ,
which, by the rules of equivalence (deﬁned formally in Section 44.3), is equivalent to the signature
J :: S(nat) ; leq ,→(nat × nat) →boolK .
The derivation of such an equivalence is called equivalence propagation, because it propagates the
identity of the type t into its scope.
The dictionary implementation MX
bstdict expects a module X with signature σord, but the mod-
ule Mnatord provides the signature σnatord. Applying the rules of subkinding given in Chapter 43,
together with the covariance principle for signatures, we obtain the subsignature relationship
σnatord <: σord.
By the subsumption principle, a module of signature σnatord may be provided when a module of
signature σord is required. Therefore Mnatord may be linked to X in MX
bstdict.
Combining subtyping with sealing provides a smooth gradation between type classes and type
abstractions. The principal signature for MX
bstdict is the signature ρX
dict given by
r
t :: S(τX
bst) ; ⟨emp ,→t , ins ,→X · s × τval × t →t , fnd ,→X · s × t →τval opt⟩
z
,
where τX
bst is the type of binary search trees with keys given by the module X of signature σord.
This signature is a subsignature of σX
dict given earlier, so that the sealed module
MX
bstdict ↿σX
dict
is well-formed, and has type σX
dict, which hides the representation type of the dictionary abstrac-
tion.
After linking X to Mnatord, the signature of the dictionary is specialized by propagating the
identity of the static part of Mnatord using the subsignature judgment. As remarked earlier, the
dictionary implementation satisﬁes the typing
X : σord ⊢MX
bstdict : σX
dict.
But because σnatord <: σord, we have, by contravariance, that
X : σnatord ⊢MX
bstdict : σX
dict.

420
44.3 A Module Language
is also a valid typing judgment. If X : σnatord, then X · s is equivalent to nat, because it has kind
S(nat), so that the typing
X : σnatord ⊢MX
bstdict : σnatdict
is also valid. The closed signature σnatdict is given explicitly by
Jt :: T ; ⟨emp ,→t , ins ,→nat × τval × t →t , fnd ,→nat × t →τval opt⟩K .
The representation of dictionaries is hidden, but the representation of keys as natural numbers is
not. The dependency on X has been eliminated by replacing all occurrences of X · s within σX
dict by
the type nat. Having derived this typing we may link X with Mnatord as described in Chapter 42
to obtain a composite module, Mnatdict, of signature σnatdict, in which keys are natural numbers
ordered as speciﬁed by Mnatord.
It is convenient to exploit subtyping for labeled tuple types to avoid creating an ad hoc module
specifying the standard ordering on the natural numbers. Instead we can extract the required
module directly from the implementation of the abstract type of numbers using subsumption. As
an illustration, let Xnat be a module variable of signature σnat, which has the form
Jt :: T ; ⟨zero ,→t , succ ,→t →t , leq ,→(t × t) →bool , . . . ⟩K
The ﬁelds of the tuple provide all and only the operations that are available on the abstract type of
natural numbers. Among them is the comparison operation leq, which is required by the dictio-
nary module. Applying the subtyping rules for labeled tuples given in Chapter 24, together with
the covariance of signatures, we obtain the subsignature relationship
σnat <: σord,
so that by subsumption the variable Xnat may be linked to the variable X postulated by the dictio-
nary implementation. Subtyping takes care of extracting the required leq ﬁeld from the abstract
type of natural numbers, demonstrating that the natural numbers are an instance of the class of
ordered types. Of course, this approach only works if we wish to order the natural numbers in the
natural way provided by the abstract type. If, instead, we wish to use another ordering, then we
must construct instances of σord “by hand” to deﬁne the appropriate ordering.
44.3
A Module Language
The module language Mod formalizes theideas outlined in the preceding section. The syntax is
divided into ﬁve levels: expressions classiﬁed by types, constructors classiﬁed by kinds, and mod-
ules classiﬁed by signatures. The expression and type level consists of various language mecha-
nisms described earlier in this book, including at least product, sum, and partial function types.
The constructor and kind level is as described in Chapters 18 and 43, with singleton and dependent

44.3 A Module Language
421
kinds. The following grammar summarizes the syntax of modules.
Sig
σ
::=
sig{κ}(t.τ)
Jt :: κ ; τK
signature
Mod
M
::=
X
X
variable
str(c;e)
Jc ; eK
structure
seal{σ}(M)
M ↿σ
seal
let{σ}(M1; X.M2)
(let X be M1 in M2) : σ
deﬁnition
Con
c
::=
stat(M)
M · s
static part
Exp
e
::=
dyn(M)
M · d
dynamic part
The statics of Mod consists of the following forms of judgment:
Γ ⊢σ sig
well-formed signature
Γ ⊢σ1 ≡σ2
equivalent signatures
Γ ⊢σ1 <: σ2
subsignature
Γ ⊢M : σ
well-formed module
Γ ⊢M val
module value
Γ ⊢e val
expression value
Rather than segregate hypotheses into zones, we instead admit the following three forms of hy-
pothesis groups:
X : σ, X val
module value variable
u :: κ
constructor variable
x : τ, x val
expression value variable
It is important that module and expression variables are always regarded as values to ensure
that type abstraction is properly enforced. Correspondingly, each module and expression variable
appears in Γ paired with the hypothesis that it is a value. As a notational convenience we will not
explicitly state the value hypotheses associated with module and expression variables, under the
convention that all such variables implicitly come paired with such an assumption.
The following rules deﬁne the formation, equivalence, and subsignature judgments.
Γ ⊢κ kind
Γ, u :: κ ⊢τ type
Γ ⊢Ju :: κ ; τK sig
(44.1a)
Γ ⊢κ1 ≡κ2
Γ, u :: κ1 ⊢τ1 ≡τ2
Γ ⊢Ju :: κ1 ; τ1K ≡Ju :: κ2 ; τ2K
(44.1b)
Γ ⊢κ1 <:: κ2
Γ, u :: κ1 ⊢τ1 <: τ2
Γ ⊢Ju :: κ1 ; τ1K <: Ju :: κ2 ; τ2K
(44.1c)
Most importantly, signatures are covariant in both the kind and type positions: subkinding and
subtyping are preserved by the formation of a signature. It follows from rule (44.1b) that
Ju :: S(c) ; τK ≡J :: S(c) ; [c/u]τK

422
44.3 A Module Language
and, further, it follows from rule (44.1c) that
J :: S(c) ; [c/u]τK <: J :: T ; [c/u]τK
and so
Ju :: S(c) ; τK <: J :: T ; [c/u]τK.
It is also the case that
Ju :: S(c) ; τK <: Ju :: T ; τK.
But the two supersignatures of Ju :: S(c) ; τK are incomparable with respect to the subsignature judg-
ment.
The statics of expressions of Mod is given by the following rules:
Γ, X : σ ⊢X : σ
(44.2a)
Γ ⊢c :: κ
Γ ⊢e : [c/u]τ
Γ ⊢Jc ; eK : Ju :: κ ; τK
(44.2b)
Γ ⊢σ sig
Γ ⊢M : σ
Γ ⊢M ↿σ : σ
(44.2c)
Γ ⊢σ sig
Γ ⊢M1 : σ1
Γ, X : σ1 ⊢M2 : σ
Γ ⊢(let X be M1 in M2) : σ : σ
(44.2d)
Γ ⊢M : σ
Γ ⊢σ <: σ′
Γ ⊢M : σ′
(44.2e)
In rule (44.2b) it is always possible to choose κ to be the most speciﬁc kind of c in the subkind
ordering, which uniquely determines c up to constructor equivalence. For such a choice, the sig-
nature Ju :: κ ; τK is equivalent to J :: κ ; [c/u]τK, which propagates the identity of the static part
of the module expression into the type of its dynamic part. Rule (44.2c) is used together with the
subsumption (rule (44.2e)) to ensure that M has the speciﬁed signature.
The need for a signature annotation on a module deﬁnition is a manifestation of the avoidance
problem. Rule (44.2d) would be perfectly sensible were the signature σ omitted from the syntax of
the deﬁnition. However, omitting this information greatly complicates type checking. If σ were
omitted from the syntax of the deﬁnition, the type checker would be required to ﬁnd a signature σ
for the body of the deﬁnition that avoids the module variable X. Inductively, we may suppose that
we have found a signature σ1 for the module M1, and a signature σ2 for the module M2, under the
assumption that X has signature σ1. To ﬁnd a signature for an unadorned deﬁnition, we must ﬁnd
a supersignature σ of σ2 that avoids X. To ensure that all possible choices of σ are accounted for, we
seek the least (most precise) such signature with respect to the subsignature relation; this is called
the principal signature of a module. The problem is that there may not be a least supersignature
of a given signature that avoids a speciﬁed variable. (Consider the example above of a signature
with two incomparable supersignatures. The example can be chosen so that the supersignatures
avoid a variable X that occurs in the subsignature.) Consequently, modules do not have principal
signatures, a signiﬁcant complication for type checking. To avoid this problem, we insist that the

44.3 A Module Language
423
avoiding supersignature σ be given by the programmer so that the type checker is not required to
ﬁnd one.
Modules give rise to a new form of constructor expression, M · s, and a new form of value ex-
pression, M · d. These operations, respectively, extract the static and dynamic parts of the module
M. Their formation rules are as follows:
Γ ⊢M val
Γ ⊢M : Ju :: κ ; τK
Γ ⊢M · s :: κ
(44.3a)
Γ ⊢M : J :: κ ; τK
Γ ⊢M · d : τ
(44.3b)
Rule (44.3a) requires that the module expression M be a value according to the following rules:
Γ, X : σ, X val ⊢X val
(44.4a)
Γ ⊢e val
Γ ⊢Jc ; eK val
(44.4b)
(It is not strictly necessary to insist that the dynamic part of a structure be a value for the structure
to itself be a value.)
Rule (44.3a) speciﬁes that only structure values have well-deﬁned static parts, and hence pre-
cludes reference to the static part of a sealed structure, which is not a value. This property ensures
representation independence for abstract types, as discussed in Section 44.1. For if M · s were ad-
missible when M is a sealed module, it would be a type whose identity depends on the underlying
implementation, in violation of the abstraction principle. Module variables are, on the other hand,
values, so that if X : Jt :: T ; τK is a module variable, then X · s is a well-formed type. What this
means in practice is that sealed modules must be bound to variables before they can be used. It is
for this reason that we include deﬁnitions among module expressions.
Rule (44.3b) requires that the signature of the module, M, be non-dependent, so that the result
type, τ, does not depend on the static part of the module. This independence may not always
be the case. For example, if M is a sealed module, say N ↿Jt :: T ; tK for some module N, then
projection M · d is ill-formed. For if it were well-formed, its type would be M · s, which would
violate representation independence for abstract types. But if M is a module value, then it is
always possible to derive a non-dependent signature for it, provided that we include the following
rule of self-recognition:
Γ ⊢M : Ju :: κ ; τK
Γ ⊢M val
Γ ⊢M : Ju :: S(M · s :: κ) ; τK
(44.5)
This rule propagates the identity of the static part of a module value into its signature. The depen-
dency of the type of the dynamic part on the static part is then eliminable by sharing propagation.
The following rule of constructor equivalence states that a type projection from a module value
is eliminable:
Γ ⊢Jc ; eK : Jt :: κ ; τK
Γ ⊢Jc ; eK val
Γ ⊢Jc ; eK · s ≡c :: κ
(44.6)

424
44.4 First- and Second-Class
The requirement that the expression e be a value, which is implicit in the second premise of the
rule, is not strictly necessary, but does no harm. A consequence is that apparent dependencies of
closed constructors (or kinds) on modules may always be eliminated. In particular the identity of
the constructor Jc ; eK · s is independent of e, as would be expected if representation independence
is to be assured.
The dynamics of modules is given as follows:
e 7−→e′
Jc ; eK 7−→Jc ; e′K
(44.7a)
e val
Jc ; eK · d 7−→e
(44.7b)
There is no need to evaluate constructors at run-time, because the dynamics of expressions does
not depend on their types. It is not difﬁcult to prove type safety for this dynamics relative to the
foregoing statics.
44.4
First- and Second-Class
It is common to draw a distinction between ﬁrst-class and second-class modules based on whether
signatures are types, and hence whether modules are just a form of expression like any other.
When modules are ﬁrst-class their values can depend on the state of the world at run-time. When
modules are second-class signatures are a separate form of classiﬁer from types, and module ex-
pressions may not be used in the same way as ordinary expressions. For example, it may not be
possible to compute a module based on the phase of the moon.
Superﬁcially, it seems as though ﬁrst-class modules are uniformly superior to second-class
modules, because you can do more with them. But on closer examination we see that the “less
is more” principle applies here as well, much as in the distinction between dynamic and static
languages discussed in Chapters 22 and 23. In particular if modules are ﬁrst-class, then one must
adopt a “pessimistic” attitude towards expressions that compute them, precisely because they
represent fully general, even state-dependent, computations. One consequence is that it is difﬁcult,
or even impossible, to track the identity of the static part of a module during type checking. A
general module expression need not have a well-deﬁned static component, precluding its use in
type expressions. Second-class modules, on the other hand, can be permissive with the use of the
static components of modules in types, precisely because the range of possible computations is
reduced. In this respect second-class modules are more powerful than ﬁrst-class, despite initial
impressions. More importantly, a second-class module system can always be enriched to allow
ﬁrst-class modules, without requiring that they be ﬁrst-class. Thus we have the best of both worlds:
the ﬂexibility of ﬁrst-class modules and the precision of second-class modules. In short you pay
for only what you use: if you use ﬁrst-class capabilities, you should expect to pay a cost, but if you
do not, you should not be taxed on the unrealized gain.
First-class modules are added to Mod in the following way. First, enrich the type system with
existential types, as described in Chapter 17, so that “ﬁrst-class modules” are just packages of
existential type. A second-class module M of signature Jt :: κ ; τK is made ﬁrst-class by forming

44.5 Notes
425
the package pack M · s with M · d as ∃(t.τ) of type ∃t :: κ.τ consisting of the static and dynamic
parts of M. Second, to allow packages to act like modules, we introduce the module expression
open e that opens the contents of a package as a module:
Γ ⊢e : ∃t :: κ.τ
Γ ⊢open e : Jt :: κ ; τK
(44.8)
Because the package e is an arbitrary expression of existential type, the module expression open e
may not be regarded as a value, and hence does not have a well-deﬁned static part. Instead we
must generally bind it to a variable before it is used, mimicking the composite behavior of the
existential elimination form given in Chapter 17.
44.5
Notes
The use of dependent types to express modularity was ﬁrst proposed by MacQueen (1986). Later
studies extended this proposal to model the phase distinction between compile- and run-time (Harper
et al., 1990), and to account for type abstraction as well as type classes (Harper and Lillibridge,
1994; Leroy, 1994). The avoidance problem was ﬁrst isolated by Castagna and Pierce (1994) and
by Harper and Lillibridge (1994). It has come to play a central role in subsequent work on mod-
ules, such as Lillibridge (1997) and Dreyer (2005). The self-recognition rule was introduced by
Harper and Lillibridge (1994) and by Leroy (1994). That rule was later identiﬁed as a manifesta-
tion of higher-order singletons (Stone and Harper, 2006). A consolidation of these ideas is used as
the foundation for a mechanization of the meta-theory of modules (Lee et al., 2007). A thorough
summary of the main issues in module system design is given in Dreyer (2005).
The presentation given here focuses attention on the type structure required to support modu-
larity. An alternative formulation uses elaboration, a translation of modularity constructs into more
primitive notions such as polymorphism and higher-order functions. The Deﬁnition of Standard
ML (Milner et al., 1997) pioneered the elaboration approach. Building on earlier work of Russo,
a more rigorous type-theoretic formulation was given by Rossberg et al. (2010). The advantage
of the elaboration-based approach is that it can make do with a simpler type theory as the target
language, but at the expense of making the explanation of modularity more complex.
Exercises
44.1. Consider the type abstraction σset of ﬁnite sets of elements of type τelt given by the following
equations:
σset ≜Jt :: T ; τsetK
τset ≜⟨emp ,→t , ins ,→τelt × t →t , mem ,→τelt × t →bool⟩.
Deﬁne an implementation
Γ, D : σdict ⊢Mset : σset

426
44.5 Notes
of ﬁnite sets of elements in terms of a dictionary whose key and value types are chosen
appropriately.
44.2. Fix an ordered type τnod of nodes, and consider the type abstraction σgrph of ﬁnite graphs given
by the following equations:
σgrph ≜
q
tgrph :: T ;
q
tedg :: S(τedg) ; τgrph
yy
τedg ≜τnod × τnod
τgrph ≜⟨emp ,→tgrph , ins ,→τedg × tgrph →tgrph , mem ,→τedg × tgrph →bool⟩.
The signature σgrph is translucent, with both opaque and transparent type components:
graphs themselves are abstract, but edges are pairs of nodes.
Deﬁne an implementation
N : σord, S : σnodset, D : σnodsetdict ⊢Mgrph : σgrph
in terms of an implementation of nodes, sets of nodes, and a dictionary mapping nodes to
sets of nodes. Represent the graph by a dictionary assigning to each node the set of nodes
incident upon it. Deﬁne the node type τnod to be the type N · s, and choose the signatures of
the set and dictionary abstractions appropriately in terms of this choice of node type.
44.3. Deﬁne signature modiﬁcation, a variant of kind modiﬁcation deﬁned in Exercise 43.3, in which
a deﬁnition of a constructor component can be imposed on a signature. Let P stand for a
composition of static and dynamic projections of the form · d . . . · d · s, so that X · P stands
for X · d . . . · d · s. Assume that Γ ⊢σ sig, Γ, X : σ ⊢X · P :: κ, and Γ ⊢c :: κ. Deﬁne signature
σ{P := c} such that Γ ⊢σ{P := c} <: σ and Γ, X : σ{P := c} ⊢X · P ≡c :: κ.
44.4. The signature σgrph is a subsignature (instance) of the type class
σgrphcls ≜
q
tgrph :: T ;
q
tedg :: T ; τgrph
yy
in which the deﬁnition of tedg has been made explicit as the product of two nodes.
Check that Γ ⊢σgrph ≡σgrphcls{ · d · s := τnod × τnod}, so that the former can be deﬁned as
the latter.

Chapter 45
Hierarchy and Parameterization
To be adequately expressive it is essential that a module system support module hierarchies. Hierar-
chical structure arises naturally in programming, both as an organizational device for partitioning
of a large program into manageable pieces, and as a localization device that allows one type ab-
straction or type class to be layered on top of another. In such a scenario the lower layer plays an
auxiliary role relative to the upper layer, and we may think of the upper layer as being abstracted
over the lower in the sense that any implementation of the lower layer induces an instance of the
upper layer corresponding to that instance. The pattern of dependency of one abstraction on an-
other is captured by an abstraction mechanism that allows the implementation of one abstraction
to be considered a function of the implementation of another. Hierarchies and abstraction work in
tandem to offer an expressive language for organizing programs.
45.1
Hierarchy
It is common in modular programming to layer a type class or a type abstraction on top of a type
class. For example, the class of equality types, which are those that admit a boolean equivalence
test, is described by the signature σeq deﬁned as follows:
Jt :: T ; ⟨eq ,→(t × t) →bool⟩K .
Instances of this class consist of a type together with a binary equality operation deﬁned on it.
Such instances are modules with a subsignature of σeq; the signature σnateq given by
Jt :: S(nat) ; ⟨eq ,→(t × t) →bool⟩K
is one example. A module value of this signature has the form
Jnat ; ⟨eq ,→. . .⟩K ,
where the elided expression implements an equivalence relation on the natural numbers. All
other instance values of the class σeq have a similar form, differing in the choice of type, and/or
the choice of comparison operation.

428
45.1 Hierarchy
The class of ordered types are an extension of the class of equality types with a binary opera-
tion for the (strict) comparison of two elements of that type. One way to formulate this is as the
signature
Jt :: T ; ⟨eq ,→(t × t) →bool,lt ,→(t × t) →bool⟩K ,
which is a subsignature of σeq according to the rules of subtyping given in Chapter 24. This rela-
tionship amounts to the requirement that every ordered type is a fortiori an equality type.
This situation is well and good, but it would be even better if there were a way to incrementally
extend the equality type class to the ordered type class without having to rewrite the signature as
we have done in the foregoing example. Instead, we would like to layer the comparison aspect on
top of an equality type class to obtain the ordered type class. For this we use a hierarchical signature
σeqord of the form
∑X : σeq . σX
ord.
In this signature we write σX
ord for the signature
Jt :: S(X · s) ; ⟨lt ,→(t × t) →bool⟩K ,
which refers to the static part of X, namely the type on which the equality relation is deﬁned. The
notation σX
ord emphasizes that this signature has a free module variable X occurring within it, and
hence is only meaningful in a context in which X has been declared.
A value of the signature σeqord is a pair of modules, ⟨Meq ; Mord⟩, in which Meq comprises a
type equipped with an equality relation on it, and the second comprises a type equipped with an
ordering relation on it. Crucially, the second type is constrained by the singleton kind in σX
ord to be
the same as the ﬁrst type. Such a constraint is a sharing speciﬁcation. The process of drawing out of
the consequences of a sharing speciﬁcation is called sharing propagation.
Sharing propagation is achieved by combining subkinding (as described in Chapter 43) with
subtyping for signatures. For example, a particular ordering Mnatord of the natural numbers is a
module with signature
∑X : σnateq . σX
ord.
By covariance of the hierarchical signature, this signature is a subsignature of σeqord, so that by
subsumption we may regard Mnatord as a module of the latter signature. The static part of the
subsignature is a singleton, so we may apply the rules of sharing propagation given in Chapter 43
to show that the subsignature is equivalent to the signature
∑X : σnateq . σnatord,
where σnatord is the closed signature
Jt :: S(nat) ; ⟨lt ,→(t × t) →bool⟩K .
Notice that sharing propagation has replaced the type X · s in the signature with nat, eliminat-
ing the dependency on the module variable X. After another round of sharing propagation, this
signature is equivalent to the signature ρnatord given by
J :: S(nat) ; ⟨lt ,→(nat × nat) →bool⟩K .

45.1 Hierarchy
429
Here we have replaced both occurrences of t in the type of the comparison operation with nat as a
consequence of the kind of t. The net effect is to propagate the identity of the static part of Mnatord
to the signature of the second component of Mnatord.
Although its value is a pair, which seems symmetric, a module of signature σeqord is asymmetric
in that the signature of the second component is dependent on the ﬁrst component itself. The
dependence is displayed by the occurrence of the module variable X in the signature σord. Thus,
for ⟨Meq ; Mord⟩to be a well-formed module of signature σeqord, the ﬁrst component Meq must
have signature σeq, which is meaningful independently of the other component of the pair. On the
other hand, the second component, Mord, must have signature σX
eq, with the understanding that
X stands for the module Meq. In general this signature is not meaningful independently of Meq
itself, and hence it may not be possible to handle Mord independently of Meq.
Turning this the other way around, if M is any module of signature σeqord, then it is always
sensible to project it onto its ﬁrst coordinate to obtain a module M · 1 of signature σeq. But it is
not always sensible to project it onto its second coordinate, because it may not be possible to give
a signature to M · 2 in the case that the dependency on the ﬁrst component cannot be resolved
statically. This problem can arise if the M · 1 is a sealed module, whose static part cannot be
formed in order to ensure representation independence. In such a situation the dependence of the
signature σX
ord on the module variable X cannot be eliminated, and so no signature can be given
to the second projection. For this reason the ﬁrst component of a module hierarchy is a submodule
of the hierarchy, whereas the second component may or may not be a submodule of it. Put in
other terms, the second component of a hierarchy is “projectible” exactly when the dependence of
its signature on the ﬁrst component is eliminable by sharing propagation. That is, we may know
enough about the ﬁrst component statically to ensure that an independent type for the second
component can be given. In that case the second component can be considered to be a submodule
of the pair; otherwise, the second is inseparable from the ﬁrst, and therefore cannot be projected
from the pair.
Consider a module Mnatord of signature σnatord, which, we noted earlier, is a subsignature of
σeqord. The ﬁrst projection Mnatord · 1 is a well-formed module of closed signature σeq and hence
is a submodule of Mnatord. The situation is less clear for the second projection, Mnatord · 2, because
its signature, σX
ord, depends on the ﬁrst component via the variable X. However, we noted above
that the signature σnatord is equivalent to the signature
∑: σnateq . ρnatord
in which the dependency on X is eliminated by sharing propagation. This, too, is a valid signature
for Mnatord, and hence the second projection Mnatord · 2 is a well-formed module of closed sig-
nature ρnatord. Otherwise, if the only signature available for Mnatord were σeqord, then the second
projection would be ill-formed—the second component would not be separable from the ﬁrst, and
hence could not be considered a submodule of the pair.
The hierarchical dependency of the signature of the second component of a pair on the ﬁrst
component gives rise to a useful alternative interpretation of a hierarchical module signature as
describing a family of modules given by the second component thought of as being indexed by
the ﬁrst component. In the case at hand, the collection of modules of the signature σeqord gives
rise to a family of modules of signature σX
ord, where X ranges over σeq. That is, to each choice,

430
45.2 Abstraction
Meq, of signature σeq, we associate the collection of choices Mord coherent with the ﬁrst choice in
accordance with the sharing constraint in σX
ord, taking X to be Mord. This collection is the ﬁber over
Meq, and the collection of modules of signature σeqord is ﬁbered over σeq (by the ﬁrst projection).
The preceding example illustrates the layering of one type class on top of another. It is also
useful to layer a type abstraction over a type class. A good example is given by a dictionary
abstraction in which the type of keys is an instance of the class of ordered types, but is otherwise
unspeciﬁed. The signature σkeydict of such a dictionary is given as follows:
∑K : σeqord . σK
dict,
where σeqord is the signature of ordered equality types (in either of the two forms discussed above),
and σK
dict is the signature of dictionaries of some type τ given as follows:
Jt :: T ; ⟨emp ,→t, ins ,→K · s × τ × t →t, fnd ,→K · s × t →τ opt⟩K .
The ins and fnd operations make use of the type K · s of keys given by the submodule of the
dictionary module. We may think of σkeydict as specifying a family of dictionary modules, one for
each choice of the ordered type of keys. Regardless of the interpretation, an implementation of the
signature σkeydict consists of a two-level hierarchy of the form ⟨M1 ; M2⟩, where M1 speciﬁes the
key type and its ordering, and M2 implements the dictionary for keys of this type in terms of this
ordering.
45.2
Abstraction
The signature σkeydict describes a family of dictionary modules indexed by a module of ordered
keys. Such modules evaluate to pairs consisting of the ordered type of keys together with the
dictionary per se, specialized to that choice of keys. Although it is possible that the code of the
dictionary operations differs for each choice of keys, it is more often the case that the same im-
plementation can be used for all choices of keys, the only difference being that references to, say,
X · lt refers to a different function for each choice of key module X.
Such a uniform implementation of dictionaries is given by an abstracted module, or functor. A
functor is a module expressed as a function of an unknown module of speciﬁed signature. The
uniform dictionary module would be expressed as a functor abstracted over the module imple-
menting keys, which is to say as a λ-abstraction
Mdictfun ≜λ Z : σeqord . MZ
keydict.
Here MZ
keydict is the generic implementation of dictionaries in terms of an unspeciﬁed module Z of
signature σeqord. The signature of Z expresses the requirement that the dictionary implementation
relies on the keys being an ordered type, but makes no other requirement on it.
A functor is a form of module, and hence has a signature as well, a functor signature. The
signature σdictfun of the functor MZ
keydict has the form
∏Z : σeqord . ρZ
keydict,

45.2 Abstraction
431
which speciﬁes that its domain is the signature, σeqord, of ordered types, and whose range is a
signature ρZ
keydict that depends on the module Z. Using a mild extension of the notation introduced
in Exercise 44.3, we may deﬁne
ρkeydict ≜σkeydict{ · 1 · 1 · s := Z · 1 · s}.
This deﬁnition ensures that the required sharing constraint between the key type in the result of
the functor and the key type given as its argument.
The dictionary functor Mdictfun deﬁnes a generic implementation of dictionaries in terms of an
ordered type of keys. An instance of the dictionary for a speciﬁc choice of keys is obtained by
applying, or instantiating, it with a module of its domain signature σeqord. For example, because
Mnatord, the type of natural numbers ordered in the usual way, is such a module, we may form the
instance Mdictfun (Mnatord) to obtain a dictionary with numeric keys. By choosing other modules of
signature σeqord we may obtain corresponding instances of the dictionary functor. More generally,
if M is any module of signature σdictfun, then it is a functor that we may apply it to any module
Mkey of signature σeqord to obtain the instance M (Mkey).
But what is the signature of such an instance, and how may it be deduced? Recall that the result
signature of σdictfun is dependent on the argument itself, and not just its signature. It is therefore
not immediately clear what signature to assign to the instance; the dependency on the argument
must be resolved in order to obtain a signature that makes sense independently of the argument.
The situation is broadly similar to the problem of computing the signature of the second compo-
nent of a hierarchical module, and similar methods are used to resolve the dependencies, namely
to exploit subtyping for signatures specialize the result signature according to the argument.
Let us consider an illustrative example. Note that by contravariance of subtyping for functor
signatures, we may weaken a functor signature by strengthening its domain signature. In the case
of the signature σdictfun of the dictionary functor, we may obtain a supersignature σnatdictfun by
strengthening its domain to require that the key type be the type of natural numbers:
∏Z : σnatord . ρZ
keydict.
Fixing Z to be a module variable of the specialized signature σnatord, the range signature ρZ
keydict is
given by the modiﬁcation
σkeydict{ · 1 · 1 · s := Z · 1 · s}.
By sharing propagation this is equivalent to the closed signature, ρnatdict, given by
σkeydict{ · 1 · 1 · s := nat},
because we may derive the equivalence of Z · 1 · s and nat once the signature of Z is specialized to
σnatord.
Now by subsumption if M is a module of signature σdictfun, then M is also a module of the
supersignature
∏Z : σnatord . ρZ
keydict.
We have just shown that the latter signature is equivalent to the non-dependent functor signature
∏: σnatord . ρnatdict.

432
45.3 Hierarchy and Abstraction
The range is now given independently of the argument, so we may deduce that if Mnatkey has
signature σnatord, then the application M (Mnatkey) has the signature ρnatdict.
The crucial point is that the dependence of the range signature on the domain signature is
eliminated by propagating knowledge about the type components of the argument itself. Absent
this knowledge, the functor application cannot be regarded as well-formed, much as the second
projection from a hierarchy cannot be admitted if the dependency of its signature on the ﬁrst com-
ponent cannot be eliminated. If the argument to the functor is a value, then it is always possible
to ﬁnd a signature for it that maximizes the propagation of type sharing information so that the
dependency of the range on the argument can always be eliminated.
45.3
Hierarchy and Abstraction
In this section we sketch the extension of the module language introduced in Chapter 44 to account
for module hierarchies and module abstraction.
The syntax of Mod is enriched with the following clauses:
Sig
σ
::=
sub(σ1;X.σ2)
∑X : σ1 . σ2
hierarchy
fun(σ1;X.σ2)
∏X : σ1 . σ2
functor
Mod
M
::=
sub(M1;M2)
⟨M1 ; M2⟩
hierarchy
fst(M)
M · 1
ﬁrst component
snd(M)
M · 2
second component
fun{σ}(X.M)
λ X : σ . M
functor
app(M1;M2)
M1 (M2)
instance
The syntax of signatures is extended to include hierarchies and functors, and the syntax of modules
is correspondingly extended with introduction and elimination forms for these signatures.
The judgment M projectible states that the module, M, is projectible in the sense that its con-
stituent types may be referenced by compositions of projections, including the static part of a
structure. This judgment is inductively deﬁned by the following rules:
Γ, X : σ ⊢x projectible
(45.1a)
Γ ⊢M1 projectible
Γ ⊢M2 projectible
Γ ⊢⟨M1 ; M2⟩projectible
(45.1b)
Γ ⊢M projectible
Γ ⊢M · 1 projectible
(45.1c)
Γ ⊢M projectible
Γ ⊢M · 2 projectible
(45.1d)
All module variables are considered projectible, even though this condition is only relevant for
hierarchies of basic structures. Because the purpose of sealing is to hide the representation of an
abstract type, no sealed module is considered projectible. Furthermore, no functor is projectible,

45.3 Hierarchy and Abstraction
433
because there is no concept of projection for a functor. More importantly, no functor instance is
projectible, which ensures that any two instances of the same functor deﬁne distinct abstract types.
Functors are therefore generative. (See Section 45.4 for a discussion of an alternative treatment of
functors.)
The signature formation judgment is extended to include these rules:
Γ ⊢σ1 sig
Γ, X : σ1 ⊢σ2 sig
Γ ⊢∑X : σ1 . σ2 sig
(45.2a)
Γ ⊢σ1 sig
Γ, X : σ1 ⊢σ2 sig
Γ ⊢∏X : σ1 . σ2 sig
(45.2b)
Signature equivalence is deﬁned to be compatible with the two new forms of signature:
Γ ⊢σ1 ≡σ′
1
Γ, X : σ1 ⊢σ2 ≡σ′
2
Γ ⊢∑X : σ1 . σ2 ≡∑X : σ′
1 . σ′
2
(45.3a)
Γ ⊢σ1 ≡σ′
1
Γ, X : σ1 ⊢σ2 ≡σ′
2
Γ ⊢∏X : σ1 . σ2 ≡∏X : σ′
1 . σ′
2
(45.3b)
The subsignature judgment is augmented with the following rules:
Γ ⊢σ1 <: σ′
1
Γ, X : σ1 ⊢σ2 <: σ′
2
Γ ⊢∑X : σ1 . σ2 <: ∑X : σ′
1 . σ′
2
(45.4a)
Γ ⊢σ′
1 <: σ1
Γ, X : σ′
1 ⊢σ2 <: σ′
2
Γ ⊢∏X : σ1 . σ2 <: ∏X : σ′
1 . σ′
2
(45.4b)
Rule (45.4a) speciﬁes that the hierarchical signature is covariant in both positions, whereas rule (45.4b)
speciﬁes that the functor signature is contravariant in its domain and covariant in its range.
The statics of module expressions is extended by the following rules:
Γ ⊢M1 : σ1
Γ ⊢M2 : σ2
Γ ⊢⟨M1 ; M2⟩: ∑: σ1 . σ2
(45.5a)
Γ ⊢M : ∑X : σ1 . σ2
Γ ⊢M · 1 : σ1
(45.5b)
Γ ⊢M : ∑: σ1 . σ2
Γ ⊢M · 2 : σ2
(45.5c)
Γ, X : σ1 ⊢M2 : σ2
Γ ⊢λ X : σ1 . M2 : ∏X : σ1 . σ2
(45.5d)
Γ ⊢M1 : ∏: σ2 . σ
Γ ⊢M2 : σ2
Γ ⊢M1 (M2) : σ
(45.5e)
Rule (45.5a) states that an explicit module hierarchy is given a signature in which there is no de-
pendency of the signature of the second component on the ﬁrst component (indicated here by the

434
45.4 Applicative Functors
underscore in place of the module variable). A dependent signature can be given to a hierarchy by
sealing, which makes it into a non-value, even if the components are values. Rule (45.5b) states that
the ﬁrst projection is deﬁned for general hierarchical signatures. On the other hand, rule (45.5c) re-
stricts the second projection to non-dependent hierarchies, as discussed in the preceding section.
Similarly, rule (45.5e) restricts instantiation to functors whose types are non-dependent, forcing
any dependencies to be resolved using the subsignature relation and sharing propagation before
application.
The self-recognition rules given in Chapter 44 are extended to account for the formation of
hierarchical module value by the following rules:
Γ ⊢M projectible
Γ ⊢M : ∑X : σ1 . σ2
Γ ⊢M · 1 : σ′
1
Γ ⊢M : ∑X : σ′
1 . σ2
(45.6a)
Γ ⊢M projectible
Γ ⊢M : ∑: σ1 . σ2
Γ ⊢M · 2 : σ′
2
Γ ⊢M : ∑: σ1 . σ′
2
(45.6b)
Rules (45.6a) and (45.6b) allow the specialization of the signature of a hierarchical module value to
express that its constructor components are equivalent to their projections from the module itself.
45.4
Applicative Functors
In the module language just described functors are regarded as generative in the sense that any
two instances, even with arguments, are considered to “generate” distinct abstract types. Genera-
tivity is achieved by treating a functor application M (M1) to be non-projectible, so that if it deﬁnes
an abstract type in the result, that type cannot be referenced without ﬁrst binding the application
to a variable. Any two such bindings are necessarily to distinct variables X and Y, and so the
abstract types X · s and Y · s are distinct, regardless of their bindings.
The justiﬁcation for this design decision merits careful consideration. By treating functors as
generative, we are ensuring that a client of the functor cannot in any way rely on the implemen-
tation of that functor. That is, we are extending the principle of representation independence for
abstract types to functors in a natural way. A consequence of this policy is that the module lan-
guage is compatible with extensions such as a conditional module that branches on an arbitrary
dynamic condition that might even depend on external conditions such as the phase of the moon!
A functor with such an implementation must be considered generative, because the abstract types
arising from any instance cannot be regarded as well-deﬁned until the moment when the appli-
cation is evaluated, which amounts to the point at which it is bound to a variable. By regarding
all functors as generative we are, in effect, maximizing opportunities to exploit changes of repre-
sentation without disrupting the behavior of clients of the functor, a bedrock principle of modular
decomposition.
But because the module language considered in the preceding section does not include any-
thing so powerful as a conditional module, we might consider that the restriction to generative
functors is too severe, and can be usefully relaxed. One such alternative is the concept of an
applicative functor. An applicative functor is one for which instances by values are regarded as

45.5 Notes
435
projectible:1
M projectible
M1 val
M (M1) projectible
(45.7)
It is important that, because of this rule, applicative functors are not compatible with conditional
modules. Thus, a module language based on applicative functors is inherently restricted as com-
pared to one based on generative functors.
The beneﬁt of consdiering a functor instance to be projectible is that we may form types such as
(M (M1)) · s, which projects the static part of the instance. But this raises the question of when two
such type expressions are equivalent? The difﬁculty is that the answer to this question depends on
the functor argument. For suppose that F is an applicative functor variable, under what conditions
should (F (M1)) · s and (F (M2)) · s be regarded as the same type? In the case of generative functors
we did not have face this question, because the instances are not projectible, but for applicative
functors the question cannot be dodged, but must be addressed. We will return to this point in a
moment, after considering one further complication that raises a similar issue.
The difﬁculty is that the body of an applicative functor cannot be sealed to impose abstraction,
and, according to the rules given in the preceding section, no sealed module is projectible. Because
sealing is the only means of imposing abstraction, we must relax this condition and allow sealed
projectible modules to be projectible:
M projectible
M ↿σ projectible
(45.8)
Thus, we may form type expressions of the form (M ↿σ) · s, which project the static part of a sealed
module. And once again we are faced with the issue that the equivalence of two such types must
involve the equivalence of the sealed modules themselves, in seeming violation of representation
independence.
Summarizing, if we are to treat functors as applicative, then some compromise of the principle
of representation independence for abstract types is required. We must deﬁne equivalence for the
static parts of sealed modules, and doing so requires at least checking whether the underlying
modules are identical. Because the underlying modules have both static and dynamic parts, this
means comparing their executable code for equivalence during type checking. More signiﬁcantly,
because the formation of a client may depend on the equivalence of two modules, we cannot
change the representation of a sealed module without fear of disrupting the typing or behavior of
the client. But such a dependency undermines the very purpose of having a module system in the
ﬁrst place!
45.5
Notes
Module hierarchies and functors in the form discussed here were introduced by Milner et al.
(1997), which also employed the reading of a module hierarchy as an indexed family of mod-
ules. The theory of hierarchies and functors was ﬁrst studied by Harper and Lillibridge (1994) and
1We may also consider functor abstractions to be projectible, but because all variables are projectible.

436
45.5 Notes
Leroy (1994), building on earlier work by Mitchell and Plotkin (1988) on existential types. The con-
cept of an applicative functor was introduced by Leroy (1995) and is central to the module system
of O’Caml (OCaml).
Exercises
45.1. Consider the following signature σorddict of dictionaries equipped with their type of ordered
keys and their type of values:
σorddict ≜∑K : σord .∑V : σtyp . σK,V
dict
σK,V
dict ≜Jt :: T ; ⟨emp ,→t, ins ,→K · s × V · s × t →t, fnd ,→K · s × t →V · s opt⟩K
σtyp ≜Jt :: T ; unitK .
Deﬁne a functor Morddictfun that implements a dictionary in terms of an ordered type of keys
and a type of values. It should have signature
σorddictfun ≜∏⟨K ; V⟩: ∑: σord . σtyp . σK,V
dict,
where
σK,V
dict ≜σorddict{ · 1 · s := K · s}{ · 2 · 1 · s := V · s}.
45.2. Deﬁne a signature σordset of ﬁnite sets that comes equipped with its own ordered type of ele-
ments. Deﬁne the signature σsetfun of a functor that implements this set abstraction in terms
of an instance of the class of ordered types. Be sure to propagate type sharing information!
Give a functor Msetfun with signature σsetfun implementing the ﬁnite set abstraction in terms
of a given ordered element type. Hint: use the dictionary functor Mdictfun from Exercise 45.1.
45.3. Deﬁne a signature σordgrph of graphs that comes equipped with an ordered type of nodes.
Deﬁne the signature of a functor that implements graphs in terms of an ordered type of
nodes. Deﬁne a functor with this signature that implements graphs in terms of an ordered
types of nodes. Hint: use the functor Mdictfun from Exercise 45.1 and the functor Msetfun of
Exercise 45.2.

Part XVIII
Equational Reasoning


Chapter 46
Equality for System T
The beauty of functional programming is that equality of expressions in a functional language
corresponds follows the familiar patterns of mathematical reasoning. For example, in the language
T of Chapter 9 in which we can express addition as the function plus, the expressions
λ (x : nat) λ (y : nat) plus(x)(y)
and
λ (x : nat) λ (y : nat) plus(y)(x)
are equal. In other words, the addition function as programmed in T is commutative.
Commutativity of addition may seem self-evident, but why is it true? What does it mean for
two expressions to be equal? These two expressions are not deﬁnitionally equal; their equality re-
quires proof, and is not merely a matter of calculation. Yet the two expressions are interchangeable
because they given the same result when applied to the same number. In general two functions
are logically equivalent if they give equal results for equal arguments. Because this is all that mat-
ters about a function, we may expect that logically equivalent functions are interchangeable in
any program. Thinking of the programs in which these functions occur as observations of their
behavior, these functions are said to be observationally equivalent. The main result of this chapter
is that observational and logical equivalence coincide for a variant of T in which the successor is
evaluated eagerly, so that a value of type nat is a numeral.
46.1
Observational Equivalence
When are two expressions equal? Whenever we cannot tell them apart! It may seem tautological to
say so, but it is not, because it all depends on what we consider to be a means of telling expressions
apart. What “experiment” are we permitted to perform on expressions in order to distinguish
them? What counts as an observation that, if different for two expressions, is a sure sign that they
are different?

440
46.1 Observational Equivalence
If we permit ourselves to consider the syntactic details of the expressions, then very few ex-
pressions could be considered equal. For example, if it is signiﬁcant that an expression contains,
say, more than one function application, or that it has an occurrence of λ-abstraction, then very few
expressions would come out as equivalent. But such considerations seem silly, because they con-
ﬂict with the intuition that the signiﬁcance of an expression lies in its contribution to the outcome of
a computation, and not to the process of obtaining that outcome. In short, if two expressions make
the same contribution to the outcome of a complete program, then they ought to be regarded as
equal.
We must ﬁx what we mean by a complete program. Two considerations inform the deﬁnition.
First, the dynamics of T is deﬁned only for expressions without free variables, so a complete
program should clearly be a closed expression. Second, the outcome of a computation should be
observable, so that it is evident whether the outcome of two computations differs or not. We deﬁne
a complete program to be a closed expression of type nat, and deﬁne the observable behavior of the
program to be the numeral to which it evaluates.
An experiment on, or observation about, an expression is any means of using that expression
within a complete program. We deﬁne an expression context to be an expression with a “hole”
in it serving as a place-holder for another expression. The hole is permitted to occur anywhere,
including within the scope of a binder. The bound variables within whose scope the hole lies are
exposed to capture by the expression context. A program context is a closed expression context of
type nat—that is, it is a complete program with a hole in it. The meta-variable C stands for any
expression context.
Replacement is the process of ﬁlling a hole in an expression context C with an expression e
which is written C{e}. Importantly, the free variables of e that are exposed by C are captured by
replacement (which is why replacement is not a form of substitution, which is deﬁned so as to
avoid capture). If C is a program context, then C{e} is a complete program iff all free variables of
e are captured by the replacement. For example, if C = λ (x : nat) ◦, and e = x + x, then
C{e} = λ (x : nat) x + x.
The free occurrences of x in e are captured by the λ-abstraction as a result of the replacement of
the hole in C by e.
We sometimes write C{◦} to emphasize the occurrence of the hole in C. Expression contexts
are closed under composition in that if C1 and C2 are expression contexts, then so is
C{◦} ≜C1{C2{◦}},
and we have C{e} = C1{C2{e}}. The trivial, or identity, expression context is the “bare hole”,
written ◦, for which ◦{e} = e.
The statics of expressions of T is extended to expression contexts by deﬁning the typing judg-
ment
C : (Γ ▷τ) ⇝(Γ′ ▷τ′)
so that if Γ ⊢e : τ, then Γ′ ⊢C{e} : τ′. This judgment is inductively deﬁned by a collection of rules
derived from the statics of T (see rules (9.1)). Some representative rules are as follows:
◦: (Γ ▷τ) ⇝(Γ ▷τ)
(46.1a)

46.1 Observational Equivalence
441
C : (Γ ▷τ) ⇝(Γ′ ▷nat)
s(C) : (Γ ▷τ) ⇝(Γ′ ▷nat)
(46.1b)
C : (Γ ▷τ) ⇝(Γ′ ▷nat)
Γ′ ⊢e0 : τ′
Γ′, x : nat, y : τ′ ⊢e1 : τ′
rec C {z ,→e0 | s(x) with y ,→e1} : (Γ ▷τ) ⇝(Γ′ ▷τ′)
(46.1c)
Γ′ ⊢e : nat
C0 : (Γ ▷τ) ⇝(Γ′ ▷τ′)
Γ′, x : nat, y : τ′ ⊢e1 : τ′
rec e {z ,→C0 | s(x) with y ,→e1} : (Γ ▷τ) ⇝(Γ′ ▷τ′)
(46.1d)
Γ′ ⊢e : nat
Γ′ ⊢e0 : τ′
C1 : (Γ ▷τ) ⇝(Γ′, x : nat, y : τ′ ▷τ′)
rec e {z ,→e0 | s(x) with y ,→C1} : (Γ ▷τ) ⇝(Γ′ ▷τ′)
(46.1e)
C2 : (Γ ▷τ) ⇝(Γ′, x : τ1 ▷τ2)
λ (x : τ1) C2 : (Γ ▷τ) ⇝(Γ′ ▷τ1 →τ2)
(46.1f)
C1 : (Γ ▷τ) ⇝(Γ′ ▷τ2 →τ′)
Γ′ ⊢e2 : τ2
C1(e2) : (Γ ▷τ) ⇝(Γ′ ▷τ′)
(46.1g)
Γ′ ⊢e1 : τ2 →τ′
C2 : (Γ ▷τ) ⇝(Γ′ ▷τ2)
e1(C2) : (Γ ▷τ) ⇝(Γ′ ▷τ′)
(46.1h)
Lemma 46.1. If C : (Γ ▷τ) ⇝(Γ′ ▷τ′), then Γ′ ⊆Γ, and if Γ ⊢e : τ, then Γ′ ⊢C{e} : τ′.
Contexts are closed under composition, with the trivial context acting as an identity for it.
Lemma 46.2. If C : (Γ ▷τ) ⇝(Γ′ ▷τ′), and C′ : (Γ′ ▷τ′) ⇝(Γ′′ ▷τ′′), then C′{C{◦}} : (Γ ▷τ) ⇝(Γ′′ ▷τ′′).
Lemma 46.3. If C : (Γ ▷τ) ⇝(Γ′ ▷τ′) and x /∈dom(Γ), then C : (Γ, x : τ′′ ▷τ) ⇝(Γ′, x : τ′′ ▷τ′).
Proof. By induction on rules (46.1).
A complete program is a closed expression of type nat.
Deﬁnition 46.4. Two complete programs, e and e′, are Kleene equal, written e ≃e′, iff there exists n ≥0
such that e 7−→∗n and e′ 7−→∗n.
Kleene equality is obviously reﬂexive and symmetric; transitivity follows from determinacy
of evaluation. Closure under converse evaluation follows similarly. It is immediate from the
deﬁnition that 0 ̸≃1.
Deﬁnition 46.5. Suppose that Γ ⊢e : τ and Γ ⊢e′ : τ are two expressions of the same type. Two such
expressions are observationally equivalent, written Γ ⊢e ∼= e′ : τ, iff C{e} ≃C{e′} for every program
context C : (Γ ▷τ) ⇝(∅▷nat).
In other words, for all possible experiments, the outcome of an experiment on e is the same as the
outcome on e′, which is an equivalence relation. For the sake of brevity, we often write e ∼=τ e′ for
∅⊢e ∼= e′ : τ.
A family of equivalence relations Γ ⊢e1 E e2 : τ is a congruence iff it is preserved by all contexts.
That is,
if Γ ⊢e E e′ : τ, then Γ′ ⊢C{e} E C{e′} : τ′
for every expression context C : (Γ ▷τ) ⇝(Γ′ ▷τ′). Such a family of relations is consistent iff
∅⊢e E e′ : nat implies e ≃e′.

442
46.2 Logical Equivalence
Theorem 46.6. Observational equivalence is the coarsest consistent congruence on expressions.
Proof. Consistency follows from the deﬁnition by noting that the trivial context is a program con-
text. Observational equivalence is clearly an equivalence relation. To show that it is a congruence,
we need only observe that type-correct composition of a program context with an arbitrary expres-
sion context is again a program context. Finally, it is the coarsest such equivalence relation, for if
Γ ⊢e E e′ : τ for some consistent congruence E, and if C : (Γ ▷τ) ⇝(∅▷nat), then by congruence
∅⊢C{e} E C{e′} : nat, and hence by consistency C{e} ≃C{e′}.
A closing substitution γ for the typing context Γ = x1 : τ1, . . . , xn : τn is a ﬁnite function assigning
closed expressions e1 : τ1, . . . , en : τn to x1, . . . , xn, respectively. We write ˆγ(e) for the substitution
[e1, . . . , en/x1, . . . , xn]e, and write γ : Γ to mean that if x : τ occurs in Γ, then there exists a closed
expression e such that γ(x) = e and e : τ. We write γ ∼=Γ γ′, where γ : Γ and γ′ : Γ, to express that
γ(x) ∼=Γ(x) γ′(x) for each x declared in Γ.
Lemma 46.7. If Γ ⊢e ∼= e′ : τ and γ : Γ, then ˆγ(e) ∼=τ ˆγ(e′). Moreover, if γ ∼=Γ γ′, then ˆγ(e) ∼=τ bγ′(e)
and ˆγ(e′) ∼=τ bγ′(e′).
Proof. Let C : (∅▷τ) ⇝(∅▷nat) be a program context; we are to show that C{ ˆγ(e)} ≃C{ ˆγ(e′)}.
Because C has no free variables, this is equivalent to showing that ˆγ(C{e}) ≃ˆγ(C{e′}). Let D be
the context
λ (x1 : τ1) . . . λ (xn : τn) C{◦}(e1) . . .(en),
where Γ = x1 : τ1, . . . , xn : τn and γ(x1) = e1, . . . , γ(xn) = en. By Lemma 46.3 we have C :
(Γ ▷τ) ⇝(Γ ▷nat), from which it follows that D : (Γ ▷τ) ⇝(∅▷nat). Because Γ ⊢e ∼= e′ : τ, we
have D{e} ≃D{e′}. But by construction D{e} ≃ˆγ(C{e}), and D{e′} ≃ˆγ(C{e′}), so ˆγ(C{e}) ≃
ˆγ(C{e′}). Because C is arbitrary, it follows that ˆγ(e) ∼=τ ˆγ(e′).
Deﬁning D′ like D, but based on γ′, rather than γ, we may also show that D′{e} ≃D′{e′},
and hence bγ′(e) ∼=τ bγ′(e′). Now if γ ∼=Γ γ′, then by congruence we have D{e} ∼=nat D′{e}, and
D{e′} ∼=nat D′{e′}. It follows that D{e} ∼=nat D′{e′}, and so, by consistency of observational
equivalence, we have D{e} ≃D′{e′}, which is to say that ˆγ(e) ∼=τ bγ′(e′).
Theorem 46.6 licenses the principle of proof by coinduction: to show that Γ ⊢e ∼= e′ : τ, it is
enough to exhibit a consistent congruence E such that Γ ⊢e E e′ : τ. It can be difﬁcult to construct
such a relation. In the next section we will provide a general method for doing so that exploits
types.
46.2
Logical Equivalence
The key to simplifying reasoning about observational equivalence is to exploit types. Informally,
we may classify the uses of expressions of a type into two broad categories, the passive and the
active uses. The passive uses are those that manipulate expressions without inspecting them. For
example, we may pass an expression of type τ to a function that simply returns it. The active
uses are those that operate on the expression itself; these are the elimination forms associated
with the type of that expression. For the purposes of distinguishing two expressions, it is only

46.2 Logical Equivalence
443
the active uses that matter; the passive uses manipulate expressions at arm’s length, affording no
opportunities to distinguish one from another.
Logical equivalence is therefore deﬁned as follows.
Deﬁnition 46.8. Logical equivalence is a family of relations e ∼τ e′ between closed expressions of type
τ. It is deﬁned by induction on τ as follows:
e ∼nat e′
iff
e ≃e′
e ∼τ1→τ2 e′
iff
if e1 ∼τ1 e′
1, then e(e1) ∼τ2 e′(e′
1)
The deﬁnition of logical equivalence at type nat licenses the following principle of proof by
nat-induction. To show that E (e, e′) whenever e ∼nat e′, it is enough to show that
1. E (0, 0), and
2. if E (n, n), then E (n + 1, n + 1).
This assertion is justiﬁed by mathematical induction on n ≥0, where e 7−→∗n and e′ 7−→∗n by
the deﬁnition of Kleene equivalence.
Lemma 46.9. Logical equivalence is symmetric and transitive: if e ∼τ e′, then e′ ∼τ e, and if e ∼τ e′ and
e′ ∼τ e′′, then e ∼τ e′′.
Proof. Simultaneously, by induction on the structure of τ. If τ = nat, the result is immediate. If τ =
τ1 →τ2, then we may assume that logical equivalence is symmetric and transitive at types τ1 and
τ2. For symmetry, assume that e ∼τ e′; we wish to show e′ ∼τ e. Assume that e′
1 ∼τ1 e1; it sufﬁces
to show that e′(e′
1) ∼τ2 e(e1). By induction we have that e1 ∼τ1 e′
1. Therefore by assumption
e(e1) ∼τ2 e′(e′
1), and hence by induction e′(e′
1) ∼τ2 e(e1). For transitivity, assume that e ∼τ e′ and
e′ ∼τ e′′; we are to show e ∼τ e′′. Suppose that e1 ∼τ1 e′′
1; it is enough to show that e(e1) ∼τ e′′(e′′
1 ).
By symmetry and transitivity we have e1 ∼τ1 e1, so by assumption e(e1) ∼τ2 e′(e1). We also have
by assumption e′(e1) ∼τ2 e′′(e′′
1 ). By transitivity we have e′(e1) ∼τ2 e′′(e′′
1 ), which sufﬁces for the
result.
Logical equivalence is extended to open terms by substitution of related closed terms to obtain
related results. If γ and γ′ are two substitutions for Γ, we deﬁne γ ∼Γ γ′ to hold iff γ(x) ∼Γ(x)
γ′(x) for every variable, x, such that Γ ⊢x : τ. Open logical equivalence, written Γ ⊢e ∼e′ : τ, is
deﬁned to mean that ˆγ(e) ∼τ bγ′(e′) whenever γ ∼Γ γ′.
Lemma 46.10. Open logical equivalence is symmetric and transitive.
Proof. Follows from Lemma 46.9 and the deﬁnition of open logical equivalence.
At this point we are “two thirds of the way” to justifying the use of the name “open logical
equivalence.” The remaining third, reﬂexivity, is established in the next section.

444
46.3 Logical and Observational Equivalence Coincide
46.3
Logical and Observational Equivalence Coincide
In this section we prove the coincidence of observational and logical equivalence.
Lemma 46.11 (Converse Evaluation). Suppose that e ∼τ e′. If d 7−→e, then d ∼τ e′, and if d′ 7−→e′,
then e ∼τ d′.
Proof. By induction on the structure of τ. If τ = nat, then the result follows from the closure of
Kleene equivalence under converse evaluation. If τ = τ1 →τ2, then suppose that e ∼τ e′, and
d 7−→e. To show that d ∼τ e′, we assume e1 ∼τ1 e′
1 and show d(e1) ∼τ2 e′(e′
1). It follows from the
assumption that e(e1) ∼τ2 e′(e′
1). Noting that d(e1) 7−→e(e1), the result follows by induction.
Lemma 46.12 (Consistency). If e ∼nat e′, then e ≃e′.
Proof. Immediate, from Deﬁnition 46.8.
Theorem 46.13 (Reﬂexivity). If Γ ⊢e : τ, then Γ ⊢e ∼e : τ.
Proof. We are to show that if Γ ⊢e : τ and γ ∼Γ γ′, then ˆγ(e) ∼τ bγ′(e). The proof proceeds by
induction on typing derivations; we consider two representative cases.
Consider the case of rule (8.4a), in which τ = τ1 →τ2 and e = λ (x : τ1) e2. We are to show that
λ (x : τ1) ˆγ(e2) ∼τ1→τ2 λ (x : τ1) bγ′(e2).
Assume that e1 ∼τ1 e′
1; by Lemma 46.11, it is enough to show that [e1/x] ˆγ(e2) ∼τ2 [e′
1/x] bγ′(e2).
Let γ2 = γ ⊗x ,→e1 and γ′
2 = γ′ ⊗x ,→e′
1, and observe that γ2 ∼Γ,x:τ1 γ′
2. Therefore, by induction
we have ˆγ2(e2) ∼τ2 ˆγ′
2(e2), from which the result follows easily.
Now consider the case of rule (9.1d), for which we are to show that
rec{ ˆγ(e0); x.y. ˆγ(e1)}( ˆγ(e)) ∼τ rec{ bγ′(e0); x.y. bγ′(e1)}( bγ′(e)).
By the induction hypothesis applied to the ﬁrst premise of rule (9.1d), we have
ˆγ(e) ∼nat bγ′(e).
We proceed by nat-induction. It sufﬁces to show that
rec{ ˆγ(e0); x.y. ˆγ(e1)}(z) ∼τ rec{ bγ′(e0); x.y. bγ′(e1)}(z),
(46.2)
and that
rec{ ˆγ(e0); x.y. ˆγ(e1)}(s(n)) ∼τ rec{ bγ′(e0); x.y. bγ′(e1)}(s(n)),
(46.3)
assuming
rec{ ˆγ(e0); x.y. ˆγ(e1)}(n) ∼τ rec{ bγ′(e0); x.y. bγ′(e1)}(n).
(46.4)
To show (46.2), by Lemma 46.11 it is enough to show that ˆγ(e0) ∼τ bγ′(e0). This condition is
assured by the outer inductive hypothesis applied to the second premise of rule (9.1d).

46.3 Logical and Observational Equivalence Coincide
445
To show (46.3), deﬁne
δ = γ ⊗x ,→n ⊗y ,→rec{ ˆγ(e0); x.y. ˆγ(e1)}(n)
and
δ′ = γ′ ⊗x ,→n ⊗y ,→rec{ bγ′(e0); x.y. bγ′(e1)}(n).
By (46.4) we have δ ∼Γ,x:nat,y:τ δ′. Consequently, by the outer inductive hypothesis applied to the
third premise of rule (9.1d), and Lemma 46.11, the required follows.
Corollary 46.14 (Equivalence). Open logical equivalence is an equivalence relation.
Corollary 46.15 (Termination). If e : nat, then e 7−→∗e′ for some e′ val.
Lemma 46.16 (Congruence). If C0 : (Γ ▷τ) ⇝(Γ0 ▷τ0), and Γ ⊢e ∼e′ : τ, then Γ0 ⊢C0{e} ∼C0{e′} :
τ0.
Proof. By induction on the derivation of the typing of C0. We consider a representative case in
which C0 = λ (x : τ1) C2 so that C0 : (Γ ▷τ) ⇝(Γ0 ▷τ1 →τ2) and C2 : (Γ ▷τ) ⇝(Γ0, x : τ1 ▷τ2).
Assuming Γ ⊢e ∼e′ : τ, we are to show that
Γ0 ⊢C0{e} ∼C0{e′} : τ1 →τ2,
which is to say
Γ0 ⊢λ (x : τ1) C2{e} ∼λ (x : τ1) C2{e′} : τ1 →τ2.
We know, by induction, that
Γ0, x : τ1 ⊢C2{e} ∼C2{e′} : τ2.
Suppose that γ0 ∼Γ0 γ′
0, and that e1 ∼τ1 e′
1. Let γ1 = γ0 ⊗x ,→e1, γ′
1 = γ′
0 ⊗x ,→e′
1, and observe
that γ1 ∼Γ0,x:τ1 γ′
1. By Deﬁnition 46.8 it is enough to show that
ˆγ1(C2{e}) ∼τ2 ˆγ′
1(C2{e′}),
which follows from the inductive hypothesis.
Theorem 46.17. If Γ ⊢e ∼e′ : τ, then Γ ⊢e ∼= e′ : τ.
Proof. By Lemmas 46.12 and 46.16, and Theorem 46.6.
Corollary 46.18. If e : nat, then e ∼=nat n, for some n ≥0.
Proof. By Theorem 46.13 we have e ∼nat e. Hence for some n ≥0, we have e ∼nat n, and so by
Theorem 46.17, e ∼=nat n.
Lemma 46.19. For closed expressions e : τ and e′ : τ, if e ∼=τ e′, then e ∼τ e′.

446
46.4 Some Laws of Equality
Proof. We proceed by induction on the structure of τ. If τ = nat, consider the empty context
to obtain e ≃e′, and hence e ∼nat e′. If τ = τ1 →τ2, then we are to show that whenever
e1 ∼τ1 e′
1, we have e(e1) ∼τ2 e′(e′
1). By Theorem 46.17 we have e1 ∼=τ1 e′
1, and hence by congruence
of observational equivalence it follows that e(e1) ∼=τ2 e′(e′
1), from which the result follows by
induction.
Theorem 46.20. If Γ ⊢e ∼= e′ : τ, then Γ ⊢e ∼e′ : τ.
Proof. Assume that Γ ⊢e ∼= e′ : τ, and that γ ∼Γ γ′. By Theorem 46.17 we have γ ∼=Γ γ′, so by
Lemma 46.7 ˆγ(e) ∼=τ bγ′(e′). Therefore, by Lemma 46.19, ˆγ(e) ∼τ ˆγ(e′).
Corollary 46.21. Γ ⊢e ∼= e′ : τ iff Γ ⊢e ∼e′ : τ.
Deﬁnitional equality is sufﬁcient for observational equivalence:
Theorem 46.22. If Γ ⊢e ≡e′ : τ, then Γ ⊢e ∼e′ : τ, and hence Γ ⊢e ∼= e′ : τ.
Proof. By an argument similar to that used in the proof of Theorem 46.13 and Lemma 46.16, then
appealing to Theorem 46.17.
Corollary 46.23. If e ≡e′ : nat, then there exists n ≥0 such that e 7−→∗n and e′ 7−→∗n.
Proof. By Theorem 46.22 we have e ∼nat e′ and hence e ≃e′.
46.4
Some Laws of Equality
In this section we summarize some useful principles of observational equivalence for T. For the
most part these are laws of logical equivalence, and then transferred to observational equivalence
by appeal to Corollary 46.21. The laws are presented as inference rules with the meaning that if all
of the premises are true judgments about observational equivalence, then so are the conclusions.
In other words each rule is admissible as a principle of observational equivalence.
46.4.1
General Laws
Logical equivalence is indeed an equivalence relation: it is reﬂexive, symmetric, and transitive.
Γ ⊢e ∼= e : τ
(46.5a)
Γ ⊢e′ ∼= e : τ
Γ ⊢e ∼= e′ : τ
(46.5b)
Γ ⊢e ∼= e′ : τ
Γ ⊢e′ ∼= e′′ : τ
Γ ⊢e ∼= e′′ : τ
(46.5c)
Reﬂexivity is an instance of a more general principle, that all deﬁnitional equalities are obser-
vational equivalences.
Γ ⊢e ≡e′ : τ
Γ ⊢e ∼= e′ : τ
(46.6a)

46.5 Notes
447
Observational equivalence is a congruence: we may replace equals by equals anywhere in an
expression.
Γ ⊢e ∼= e′ : τ
C : (Γ ▷τ) ⇝(Γ′ ▷τ′)
Γ′ ⊢C{e} ∼= C{e′} : τ′
(46.7a)
Equivalence is stable under substitution for free variables, and substituting equivalent expres-
sions in an expression gives equivalent results.
Γ ⊢e : τ
Γ, x : τ ⊢e2 ∼= e′
2 : τ′
Γ ⊢[e/x]e2 ∼= [e/x]e′
2 : τ′
(46.8a)
Γ ⊢e1 ∼= e′
1 : τ
Γ, x : τ ⊢e2 ∼= e′
2 : τ′
Γ ⊢[e1/x]e2 ∼= [e′
1/x]e′
2 : τ′
(46.8b)
46.4.2
Equality Laws
Two functions are equal if they are equal on all arguments.
Γ, x : τ1 ⊢e(x) ∼= e′(x) : τ2
Γ ⊢e ∼= e′ : τ1 →τ2
(46.9)
Consequently, every expression of function type is equal to a λ-abstraction:
Γ ⊢e ∼= λ (x : τ1) e(x) : τ1 →τ2
(46.10)
46.4.3
Induction Law
An equation involving a free variable x of type nat can be proved by induction on x.
Γ ⊢[n/x]e ∼= [n/x]e′ : τ (for every n ∈N)
Γ, x : nat ⊢e ∼= e′ : τ
(46.11a)
To apply the induction rule, we proceed by mathematical induction on n ∈N, which reduces
to showing:
1. Γ ⊢[z/x]e ∼= [z/x]e′ : τ, and
2. Γ ⊢[s(n)/x]e ∼= [s(n)/x]e′ : τ, if Γ ⊢[n/x]e ∼= [n/x]e′ : τ.
46.5
Notes
The method of logical relations interprets types as relations (here, equivalence relations) by asso-
ciating with each type constructor a relational action that transforms the relation interpreting its
arguments to the relation interpreting the constructed type. Logical relations (Statman, 1985) are a

448
46.5 Notes
fundamental tool in proof theory and provide the foundation for the semantics of the NuPRL type
theory (Constable, 1986; Allen, 1987; Harper, 1992). The use of logical relations to characterize ob-
servational equivalence is an adaptation of the NuPRL semantics to the simpler setting of G¨odel’s
System T.

Chapter 47
Equality for System PCF
In this Chapter we develop the theory of observational equivalence for PCF, with an eager inter-
pretation of the type of natural numbers. The development proceeds along lines similar to those
in Chapter 46, but is complicated by the presence of general recursion. The proof depends on the
concept of an admissible relation, one that admits the principle of proof by ﬁxed point induction.
47.1
Observational Equivalence
The deﬁnition of observational equivalence, along with the auxiliary notion of Kleene equivalence,
are deﬁned similarly to Chapter 46, but modiﬁed to account for the possibility of non-termination.
The collection of well-formed PCF contexts is inductively deﬁned in a manner directly anal-
ogous to that in Chapter 46. Speciﬁcally, we deﬁne the judgment C : (Γ ▷τ) ⇝(Γ′ ▷τ′) by rules
similar to rules (46.1), modiﬁed for PCF. (We leave the precise deﬁnition as an exercise for the
reader.) When Γ and Γ′ are empty, we write just C : τ ⇝τ′.
A complete program is a closed expression of type nat.
Deﬁnition 47.1. We say that two complete programs, e and e′, are Kleene equal, written e ≃e′, iff for
every n ≥0, e 7−→∗n iff e′ 7−→∗n.
Kleene equality is clearly an equivalence relation and is closed under converse evaluation.
Moreover, 0 ̸≃1 and, if e and e′ are both divergent, then e ≃e′.
Observational equivalence is deﬁned just as it is in Chapter 46.
Deﬁnition 47.2. We say that Γ ⊢e : τ and Γ ⊢e′ : τ are observationally, or contextually, equivalent
iff for every program context C : (Γ ▷τ) ⇝(∅▷nat), C{e} ≃C{e′}.
Theorem 47.3. Observational equivalence is the coarsest consistent congruence.
Proof. See the proof of Theorem 46.6.
Lemma 47.4 (Substitution and Functionality). If Γ ⊢e ∼= e′ : τ and γ : Γ, then ˆγ(e) ∼=τ ˆγ(e′).
Moreover, if γ ∼=Γ γ′, then ˆγ(e) ∼=τ ˆγ′(e) and ˆγ(e′) ∼=τ ˆγ′(e′).

450
47.2 Logical Equivalence
Proof. See Lemma 46.7.
47.2
Logical Equivalence
Deﬁnition 47.5. Logical equivalence, e ∼τ e′, between closed expressions of type τ is deﬁned by induc-
tion on τ as follows:
e ∼nat e′
iff
e ≃e′
e ∼τ1⇀τ2 e′
iff
e1 ∼τ1 e′
1 implies e(e1) ∼τ2 e′(e′
1)
Formally, logical equivalence is deﬁned as in Chapter 46, except that the deﬁnition of Kleene
equivalence is altered to account for non-termination. Logical equivalence is extended to open
terms by substitution. Speciﬁcally, we deﬁne Γ ⊢e ∼e′ : τ to mean that ˆγ(e) ∼τ bγ′(e′) whenever
γ ∼Γ γ′.
By the same argument as given in the proof of Lemma 46.9 logical equivalence is symmetric
and transitive, as is its open extension.
Lemma 47.6 (Strictness). If e : τ and e′ : τ are both divergent, then e ∼τ e′.
Proof. By induction on the structure of τ. If τ = nat, then the result follows immediately from
the deﬁnition of Kleene equivalence. If τ = τ1 ⇀τ2, then e(e1) and e′(e′
1) diverge, so by induction
e(e1) ∼τ2 e′(e′
1), as required.
Lemma 47.7 (Converse Evaluation). Suppose that e ∼τ e′. If d 7−→e, then d ∼τ e′, and if d′ 7−→e′,
then e ∼τ d′.
47.3
Logical and Observational Equivalence Coincide
The proof of coincidence of logical and observational equivalence relies on the concept of bounded
recursion, which we deﬁne by induction on m ≥0 as follows:
fix0 x : τ is e ≜fix x : τ is x
fixm+1 x : τ is e ≜[fixm x : τ is e/x]e
When m = 0, bounded recursion is deﬁned to be a divergent expression of type τ. When m > 0,
bounded recursion is deﬁned by unrolling the recursion m times by iterated substitution. Intu-
itively, the bounded recursive expression fixm x : τ is e is as good as fix x : τ is e for up to m
unrollings, after which it is divergent.
It is easy to check that the follow rule is derivable for each m ≥0:
Γ, x : τ ⊢e : τ
Γ ⊢fixm{τ}(x.e) : τ
.
(47.1a)
The proof is by induction on m ≥0, and amounts to an iteration of the substitution lemma for the
statics of PCF.

47.3 Logical and Observational Equivalence Coincide
451
The key property of bounded recursion is the principle of ﬁxed point induction, which permits
reasoning about a recursive computation by induction on the number of unrollings required to
reach a value. The proof relies on compactness, which will be stated and proved in Section 47.4
below.
Theorem 47.8 (Fixed Point Induction). Suppose that x : τ ⊢e : τ. If
(∀m ≥0) fixm x : τ is e ∼τ fixm x : τ is e′,
then fix x : τ is e ∼τ fix x : τ is e′.
Proof. Deﬁne an applicative context A to be either a hole, ◦, or an application of the form A(e),
where A is an applicative context. The typing judgment for applicative contexts, A : τ0 ⇝τ, is a
special case of the general typing judgment for contexts. Deﬁne logical equivalence of applicative
contexts, A ∼A′ : τ0 ⇝τ, by induction on the structure of A as follows:
1. ◦∼◦: τ0 ⇝τ0;
2. if A ∼A′ : τ0 ⇝τ2 ⇀τ and e2 ∼τ2 e′
2, then A(e2) ∼A′(e′
2) : τ0 ⇝τ.
We prove by induction on the structure of τ, if A ∼A′ : τ0 ⇝τ and
for every m ≥0, A{fixm x : τ0 is e} ∼τ A′{fixm x : τ0 is e′},
(47.2)
then
A{fix x : τ0 is e} ∼τ A′{fix x : τ0 is e′}.
(47.3)
Choosing A = A′ = ◦with τ0 = τ completes the proof.
If τ = nat, then assume that A ∼A′ : τ0 ⇝nat and (47.2). By Deﬁnition 47.5, we are to show
A{fix x : τ0 is e} ≃A′{fix x : τ0 is e′}.
By Corollary 47.17 there exists m ≥0 such that
A{fix x : τ0 is e} ≃A{fixm x : τ0 is e}.
By (47.2) we have
A{fixm x : τ0 is e} ≃A′{fixm x : τ0 is e′}.
By Corollary 47.17
A′{fixm x : τ0 is e′} ≃A′{fix x : τ0 is e′}.
The result follows by transitivity of Kleene equivalence.
If τ = τ1 ⇀τ2, then by Deﬁnition 47.5, it is enough to show
A{fix x : τ0 is e}(e1) ∼τ2 A′{fix x : τ0 is e′}(e′
1)
whenever e1 ∼τ1 e′
1. Let A2 = A(e1) and A′
2 = A′(e′
1). It follows from (47.2) that for every m ≥0
A2{fixm x : τ0 is e} ∼τ2 A′
2{fixm x : τ0 is e′}.
Noting that A2 ∼A′
2 : τ0 ⇝τ2, we have by induction
A2{fix x : τ0 is e} ∼τ2 A′
2{fix x : τ0 is e′},
as required.

452
47.3 Logical and Observational Equivalence Coincide
Lemma 47.9 (Reﬂexivity). If Γ ⊢e : τ, then Γ ⊢e ∼e : τ.
Proof. The proof proceeds along the same lines as the proof of Theorem 46.13. The main differ-
ence is the treatment of general recursion, which is proved by ﬁxed point induction. Consider
rule (19.1g). Assuming γ ∼Γ γ′, we are to show that
fix x : τ is ˆγ(e) ∼τ fix x : τ is bγ′(e).
By Theorem 47.8 it is enough to show that, for every m ≥0,
fixm x : τ is ˆγ(e) ∼τ fixm x : τ is bγ′(e).
We proceed by an inner induction on m. When m = 0 the result is immediate, because both sides
of the desired equivalence diverge. Assuming the result for m, and applying Lemma 47.7, it is
enough to show that ˆγ(e1) ∼τ bγ′(e1), where
e1 = [fixm x : τ is ˆγ(e)/x] ˆγ(e), and
(47.4)
e′
1 = [fixm x : τ is bγ′(e)/x] bγ′(e).
(47.5)
But this follows directly from the inner and outer inductive hypotheses. For by the outer inductive
hypothesis, if
fixm x : τ is ˆγ(e) ∼τ fixm x : τ is bγ′(e),
then
[fixm x : τ is ˆγ(e)/x] ˆγ(e) ∼τ [fixm x : τ is bγ′(e)/x] bγ′(e).
But the hypothesis holds by the inner inductive hypothesis, from which the result follows.
To handle the conditional ifz e {z ,→e0 | s(x) ,→e1}, we proceed by cases on whether e di-
verges, in which case the conditional is divergent and therefore self-related by Lemma 47.6, or e
converges, in which case we can proceed by an inner mathematical induction on its value, ap-
pealing to the inductive hypotheses governing the branches of the conditional to complete the
argument.
Symmetry and transitivity of eager logical equivalence are easily established by induction on
types, noting that Kleene equivalence is symmetric and transitive. Eager logical equivalence is
therefore an equivalence relation.
Lemma 47.10 (Congruence). If C0 : (Γ ▷τ) ⇝(Γ0 ▷τ0), and Γ ⊢e ∼e′ : τ, then Γ0 ⊢C0{e} ∼C0{e′} :
τ0.
Proof. By induction on the derivation of the typing of C0, following along similar lines to the proof
of Lemma 47.9.
Logical equivalence is consistent, by deﬁnition. Consequently, it is contained in observational
equivalence.
Theorem 47.11. If Γ ⊢e ∼e′ : τ, then Γ ⊢e ∼= e′ : τ.

47.4 Compactness
453
Proof. By consistency and congruence of logical equivalence.
Lemma 47.12. If e ∼=τ e′, then e ∼τ e′.
Proof. By induction on the structure of τ. If τ = nat, then the result is immediate, because the
empty expression context is a program context. If τ = τ1 ⇀τ2, then suppose that e1 ∼τ1 e′
1.
We are to show that e(e1) ∼τ2 e′(e′
1). By Theorem 47.11 e1 ∼=τ1 e′
1, and hence by Lemma 47.4
e(e1) ∼=τ2 e′(e′
1), from which the result follows by induction.
Theorem 47.13. If Γ ⊢e ∼= e′ : τ, then Γ ⊢e ∼e′ : τ.
Proof. Assume that Γ ⊢e ∼= e′ : τ. Suppose that γ ∼Γ γ′. By Theorem 47.11 we have γ ∼=Γ γ′, and
so by Lemma 47.4 we have
ˆγ(e) ∼=τ ˆγ′(e′).
Therefore by Lemma 47.12 we have
ˆγ(e) ∼τ ˆγ′(e′).
Corollary 47.14. Γ ⊢e ∼= e′ : τ iff Γ ⊢e ∼e′ : τ.
47.4
Compactness
The principle of ﬁxed point induction is derived from a critical property of PCF, called compactness.
This property states that only ﬁnitely many unwindings of a ﬁxed point expression are needed in
a complete evaluation of a program. Although intuitively obvious (one cannot complete inﬁnitely
many recursive calls in a ﬁnite computation), it is rather tricky to state and prove rigorously.
The proof of compactness (Theorem 47.16) makes use of the stack machine for PCF deﬁned in
Chapter 28, augmented with the following transitions for bounded recursive expressions:
k ▷fix0 x : τ is e 7−→k ▷fix0 x : τ is e
(47.6a)
k ▷fixm+1 x : τ is e 7−→k ▷[fixm x : τ is e/x]e
(47.6b)
It is not difﬁcult to extend the proof of Corollary 28.4 to account for bounded recursion.
To get a feel for what is involved in the compactness proof, consider ﬁrst the factorial function
f in PCF:
fix f : nat ⇀nat is λ (x : nat) ifz x {z ,→s(z) | s(x′) ,→x ∗f (x′)}.
Obviously evaluation of f (n) requires n recursive calls to the function itself. That is, for a given
input n we may place a bound m on the recursion that is sufﬁcient to ensure termination of the
computation. This property can be expressed formally using the m-bounded form of general re-
cursion,
fixm f : nat ⇀nat is λ (x : nat) ifz x {z ,→s(z) | s(x′) ,→x ∗f (x′)}.

454
47.4 Compactness
Call this expression f (m). It follows from the deﬁnition of f that if f (n) 7−→∗p, then f (m)(n) 7−→∗
p for some m ≥0 (in fact, m = n sufﬁces).
When considering expressions of higher type, we cannot expect to get the same result from the
bounded recursion as from the unbounded. For example, consider the addition function a of type
τ = nat ⇀(nat ⇀nat), given by the expression
fix p : τ is λ (x : nat) ifz x {z ,→id | s(x′) ,→s ◦(p(x′))},
where id = λ (y : nat) y is the identity, e′ ◦e = λ (x : τ) e′(e(x)) is composition, and s = λ (x : nat) s(x)
is the successor function. The application a(n) terminates after three transitions, regardless of the
value of n, resulting in a λ-abstraction. When n is positive, the result contains a residual copy of
a itself, which is applied to n −1 as a recursive call. The m-bounded version of a, written a(m),
is also such that a(m)(n) terminates in three steps, provided that m > 0. But the result is not the
same, because the residuals of a appear as a(m−1), rather than as a itself.
Turning now to the proof of compactness, it is helpful to introduce some notation. Suppose
that x : τ ⊢ex : τ for some arbitrary abstractor x.ex. Let f (ω) = fix x : τ is ex, and let f (m) =
fixm x : τ is ex. Observe that f (ω) : τ and f (m) : τ for any m ≥0.
The following technical lemma governing the stack machine allows the bound on occurrences
of a recursive expression to be raised without affecting the outcome of evaluation.
Lemma 47.15. For every m ≥0, if [ f (m)/y]k ▷[ f (m)/y]e 7−→∗ϵ ◁n, then [ f (m+1)/y]k ▷[ f (m+1)/y]e 7−→∗
ϵ ◁n.
Proof. By induction on m ≥0, and then induction on transition.
Theorem 47.16 (Compactness). Suppose that y : τ ⊢e : nat where y /∈f (ω). If [ f (ω)/y]e 7−→∗n,
then there exists m ≥0 such that [ f (m)/y]e 7−→∗n.
Proof. We prove simultaneously the stronger statements that if
[ f (ω)/y]k ▷[ f (ω)/y]e 7−→∗ϵ ◁n,
then for some m ≥0,
[ f (m)/y]k ▷[ f (m)/y]e 7−→∗ϵ ◁n,
and if
[ f (ω)/y]k ◁[ f (ω)/y]e 7−→∗ϵ ◁n
then for some m ≥0,
[ f (m)/y]k ◁[ f (m)/y]e 7−→∗ϵ ◁n.
(Note that if [ f (ω)/y]e val, then [ f (m)/y]e val for all m ≥0.) The result then follows by the correct-
ness of the stack machine (Corollary 28.4).
We proceed by induction on transition. Suppose that the initial state is
[ f (ω)/y]k ▷f (ω),

47.5 Lazy Natural Numbers
455
which arises when e = y, and the transition sequence is as follows:
[ f (ω)/y]k ▷f (ω) 7−→[ f (ω)/y]k ▷[ f (ω)/x]ex 7−→∗ϵ ◁n.
Noting that [ f (ω)/x]ex = [ f (ω)/y][y/x]ex, we have by induction that there exists m ≥0 such that
[ f (m)/y]k ▷[ f (m)/x]ex 7−→∗ϵ ◁n.
By Lemma 47.15
[ f (m+1)/y]k ▷[ f (m)/x]ex 7−→∗ϵ ◁n
and we need only recall that
[ f (m+1)/y]k ▷f (m+1) = [ f (m+1)/y]k ▷[ f (m)/x]ex
to complete the proof. If, on the other hand, the initial step is an unrolling, but e ̸= y, then we have
for some z /∈f (ω) and z ̸= y
[ f (ω)/y]k ▷fix z : τ is dω 7−→[ f (ω)/y]k ▷[fix z : τ is dω/z]dω 7−→∗ϵ ◁n.
where dω = [ f (ω)/y]d. By induction there exists m ≥0 such that
[ f (m)/y]k ▷[fix z : τ is dm/z]dm 7−→∗ϵ ◁n,
where dm = [ f (m)/y]d. But then by Lemma 47.15 we have
[ f (m+1)/y]k ▷[fix z : τ is dm+1/z]dm+1 7−→∗ϵ ◁n,
where dm+1 = [ f (m+1)/y]d, from which the result follows directly.
Corollary 47.17. There exists m ≥0 such that [ f (ω)/y]e ≃[ f (m)/y]e.
Proof. If [ f (ω)/y]e diverges, then it sufﬁces to take m to be zero. Otherwise, apply Theorem 47.16
to obtain m, and note that the required Kleene equivalence follows.
47.5
Lazy Natural Numbers
Recall from Chapter 19 that if the successor is evaluated lazily, then the type nat changes its mean-
ing to that of the lazy natural numbers, which we shall write lnat for emphasis. This type contains
an “inﬁnite number” ω, which is essentially an endless stack of successors.
To account for the lazy successor the deﬁnition of logical equivalence must be reformulated.
Rather than being deﬁned inductively as the strongest relation closed under speciﬁed conditions, it
is now deﬁned coinductively as the weakest relation consistent with two analogous conditions. We
may then show that two expressions are related using the principle of proof by coinduction.

456
47.5 Lazy Natural Numbers
The deﬁnition of Kleene equivalence must be altered to account for the lazily evaluated succes-
sor operation. To account for ω, two computations are compared based solely on the outermost
form of their values, if any. We deﬁne e ≃e′ to hold iff (a) if e 7−→∗z, then e′ 7−→∗z, and vice versa;
and (b) if e 7−→∗s(e1), then e′ 7−→∗s(e′
1), and vice versa.
Corollary 47.17 can be proved for the co-natural numbers by essentially the same argument as
before.
The deﬁnition of logical equivalence at type lnat is deﬁned to be the weakest equivalence
relation E between closed terms of type lnat satisfying the following consistency conditions: if
e E e′ : lnat, then
1. If e 7−→∗z, then e′ 7−→∗z, and vice versa.
2. If e 7−→∗s(e1), then e′ 7−→∗s(e′
1) with e1 E e′
1 : lnat, and vice versa.
It is immediate that if e ∼lnat e′, then e ≃e′, and so logical equivalence is consistent. It is also
strict in that if e and e′ are both divergent expressions of type lnat, then e ∼lnat e′.
The principle of proof by coinduction states that to show e ∼lnat e′, it sufﬁces to exhibit a
relation, E, such that
1. e E e′ : lnat, and
2. E satisﬁes the above consistency conditions.
If these requirements hold, then E is contained in logical equivalence at type lnat, and hence
e ∼lnat e′, as required.
As an application of coinduction, let us consider the proof of Theorem 47.8. The overall ar-
gument remains as before, but the proof for the type lnat must be altered as follows. Suppose
that A ∼A′ : τ0 ⇝lnat, and let a = A{fix x : τ0 is e} and a′ = A′{fix x : τ0 is e′}. Writing
a(m) = A{fixm x : τ0 is e} and a′(m) = A′{fixm x : τ0 is e′}, assume that
for every m ≥0, a(m) ∼lnat a′(m).
We are to show that
a ∼lnat a′.
Deﬁne the functions pn for n ≥0 on closed terms of type lnat by the following equations:
p0(d) = d
p(n+1)(d) =
(
d′
if pn(d) 7−→∗s(d′)
undeﬁned
otherwise
For n ≥0, let an = pn(a) and a′
n = pn(a′). Correspondingly, let a(m)
n
= pn(a(m)) and a′
n
(m) =
pn(a′(m)). Deﬁne E to be the strongest relation such that an E a′
n : lnat for all n ≥0. We will show
that the relation E satisﬁes the consistency conditions, and so it is contained in logical equivalence.
Because a E a′ : lnat (by construction), the result follows immediately.

47.6 Notes
457
To show that E is consistent, suppose that an E a′
n : lnat for some n ≥0. We have by Corol-
lary 47.17 an ≃a(m)
n
, for some m ≥0, and hence, by the assumption, an ≃a′
n
(m), and so by
Corollary 47.17 again, a′
n
(m) ≃a′
n. Now if an 7−→∗s(bn), then a(m)
n
7−→∗s(b(m)
n
) for some b(m)
n
, and
hence there exists b′
n
(m) such that a′
n
(m) 7−→∗b′
n
(m), and so there exists b′
n such that a′
n 7−→∗s(b′
n).
But bn = pn+1(a) and b′
n = pn+1(a′), and we have bn E b′
n : lnat by construction, as required.
47.6
Notes
The use of logical relations to characterize observational equivalence for PCF is inspired by the
treatment of partiality in type theory by Constable and Smith (1987) and by the studies of observa-
tional equivalence by Pitts (2000). Although the technical details differ, the proof of compactness
here is inspired by Pitts’s structurally inductive characterization of termination using an abstract
machine. It is critical to restrict attention to transition systems whose states are complete programs
(closed expressions of observable type). Structural operational semantics usually does not fulﬁll
this requirement, thereby requiring a considerably more complex argument than given here.

458
47.6 Notes

Chapter 48
Parametricity
The main motivation for polymorphism is to enable more programs to be written — those that are
“generic” in one or more types, such as the composition function given in Chapter 16. If a program
does not depend on the choice of types, we can code it using polymorphism. Moreover, if we wish
to insist that a program can not depend on a choice of types, we demand that it be polymorphic.
Thus polymorphism can be used both to expand the collection of programs we may write, and
also to limit the collection of programs that are permissible in a given context.
The restrictions imposed by polymorphic typing give rise to the experience that in a polymor-
phic functional language, if the types are correct, then the program is correct. Roughly speaking,
if a function has a polymorphic type, then the strictures of type genericity cut down the set of
programs with that type. Thus if you have written a program with this type, it is more likely to be
the one you intended!
The technical foundation for these remarks is called parametricity. The goal of this chapter is to
give an account of parametricity for F under a call-by-name interpretation.
48.1
Overview
We will begin with an informal discussion of parametricity based on a “seat of the pants” under-
standing of the set of well-formed programs of a type.
Suppose that a function value f has the type ∀(t.t →t). What function could it be? When
instantiated at a type τ it should evaluate to a function g of type τ →τ that, when further applied
to a value v of type τ returns a value v′ of type τ. Because f is polymorphic, g cannot depend on
v, so v′ must be v. In other words, g must be the identity function at type τ, and f must therefore
be the polymorphic identity.
Suppose that f is a function of type ∀(t.t). What function could it be? A moment’s thought
reveals that it cannot exist at all. For it must, when instantiated at a type τ, return a value of that
type. But not every type has a value (including this one), so this is an impossible assignment. The
only conclusion is that ∀(t.t) is an empty type.

460
48.2 Observational Equivalence
Let N be the type of polymorphic Church numerals introduced in Chapter 16, namely ∀(t.t →(t →t) →t).
What are the values of this type? Given any type τ, and values z : τ and s : τ →τ, the expression
f [τ](z)(s)
must yield a value of type τ. Moreover, it must behave uniformly with respect to the choice of τ.
What values could it yield? The only way to build a value of type τ is by using the element z and
the function s passed to it. A moment’s thought reveals that the application must amount to the
n-fold composition
s(s(. . . s(z) . . .)).
That is, the elements of N are in one-to-one correspondence with the natural numbers.
48.2
Observational Equivalence
The deﬁnition of observational equivalence given in Chapters 46 and 47 is based on identifying a
type of answers that are observable outcomes of complete programs. Values of function type are
not regarded as answers, but are treated as “black boxes” with no internal structure, only input-
output behavior. In F, however, there are no (closed) base types. Every type is either a function
type or a polymorphic type, and hence no types suitable to serve as observable answers.
One way to manage this difﬁculty is to augment F with a base type of answers to serve as the
observable outcomes of a computation. The only requirement is that this type have two elements
that can be immediately distinguished from each other by evaluation. We may achieve this by
enriching F with a base type 2 containing two constants, tt and ff, that serve as possible answers
for a complete computation. A complete program is a closed expression of type 2.
Kleene equality is deﬁned for complete programs by requiring that e ≃e′ iff either (a) e 7−→∗tt
and e′ 7−→∗tt; or (b) e 7−→∗ff and e′ 7−→∗ff. This relation is an equivalence, and it is immediate
that tt ̸≃ff, because these are two distinct constants. As before, we say that a type-indexed family
of equivalence relations between closed expressions of the same type is consistent if it implies
Kleene equality at the answer type 2.
To deﬁne observational equivalence, we must ﬁrst deﬁne the concept of an expression context
for F as an expression with a “hole” in it. More precisely, we may give an inductive deﬁnition of
the judgment
C : (∆; Γ ▷τ) ⇝(∆′; Γ′ ▷τ′),
which states that C is an expression context that, when ﬁlled with an expression ∆; Γ ⊢e : τ
yields an expression ∆′; Γ′ ⊢C{e} : τ′. (We leave the precise deﬁnition of this judgment, and the
veriﬁcation of its properties, as an exercise for the reader.)
Deﬁnition 48.1. Two expressions of the same type are observationally equivalent, written ∆; Γ ⊢
e ∼= e′ : τ, iff C{e} ≃C{e′} whenever C : (∆; Γ ▷τ) ⇝(∅; ∅▷2).
Lemma 48.2. Observational equivalence is the coarsest consistent congruence.
Proof. Essentially the same as the the proof of Theorem 46.6.

48.3 Logical Equivalence
461
Lemma 48.3.
1. If ∆, t; Γ ⊢e ∼= e′ : τ and τ0 type, then ∆; [τ0/t]Γ ⊢[τ0/t]e ∼= [τ0/t]e′ : [τ0/t]τ.
2. If ∅; Γ, x : τ0 ⊢e ∼= e′ : τ and d : τ0, then ∅; Γ ⊢[d/x]e ∼= [d/x]e′ : τ. Moreover, if d ∼=τ0 d′, then
∅; Γ ⊢[d/x]e ∼= [d′/x]e : τ and ∅; Γ ⊢[d/x]e′ ∼= [d′/x]e′ : τ.
Proof.
1. Let C : (∆; [τ0/t]Γ ▷[τ0/t]τ) ⇝(∅▷2) be a program context. We are to show that
C{[τ0/t]e} ≃C{[τ0/t]e′}.
Because C is closed, this is equivalent to
[τ0/t]C{e} ≃[τ0/t]C{e′}.
Let C′ be the context Λ(t) C{◦}[τ0], and observe that
C′ : (∆, t; Γ ▷τ) ⇝(∅▷2).
Therefore, from the assumption,
C′{e} ≃C′{e′}.
But C′{e} ≃[τ0/t]C{e}, and C′{e′} ≃[τ0/t]C{e′}, from which the result follows.
2. By an argument similar to that for Lemma 46.7.
48.3
Logical Equivalence
In this section we introduce a form of logical equivalence that captures the informal concept of
parametricity, and also provides a characterization of observational equivalence. This characteri-
zation will permit us to derive properties of observational equivalence of polymorphic programs
of the kind suggested earlier.
The deﬁnition of logical equivalence for F is somewhat more complex than for T. The main
idea is to deﬁne logical equivalence for a polymorphic type ∀(t.τ) to satisfy a very strong condi-
tion that captures the essence of parametricity. As a ﬁrst approximation, we might say that two
expressions e and e′ of this type should be logically equivalent if they are logically equivalent for
“all possible” interpretations of the type t. More precisely, we might require that e[ρ] be related to
e′[ρ] at type [ρ/t]τ, for any choice of type ρ. But this runs into two problems, one technical, the
other conceptual. The same device will be used to solve both problems.
The technical problem stems from impredicativity. In Chapter 46 logical equivalence is deﬁned
by induction on the structure of types. But when polymorphism is impredicative, the type [ρ/t]τ
might well be larger than ∀(t.τ). At the very least we would have to justify the deﬁnition of logical
equivalence on some other grounds, but no criterion appears to be available. The conceptual
problem is that, even if we could make sense of the deﬁnition of logical equivalence, it would be
too restrictive. For such a deﬁnition amounts to saying that the unknown type t is interpreted as

462
48.3 Logical Equivalence
logical equivalence at whatever type it is when instantiated. To obtain useful parametricity results,
we shall ask for much more than this. What we shall do is to consider separately instances of e and
e′ by types ρ and ρ′, and treat the type variable t as standing for any relation (of some form) between
ρ and ρ′. We may suspect that this is asking too much: perhaps logical equivalence is the empty
relation. Surprisingly, this is not the case, and indeed it is this very feature of the deﬁnition that
we shall exploit to derive parametricity results about the language.
To manage both of these problems we will consider a generalization of logical equivalence
that is parameterized by a relational interpretation of the free type variables of its classiﬁer. The
parameters determine a separate binding for each free type variable in the classiﬁer for each side
of the equation, with the discrepancy being mediated by a speciﬁed relation between them. Thus
related expressions need not have the same type, with the differences between them mediated by
the given relation.
We will restrict attention to a certain collection of “admissible” binary relations between closed
expressions. The conditions are imposed to ensure that logical equivalence and observational
equivalence coincide.
Deﬁnition 48.4 (Admissibility). A relation R between expressions of types ρ and ρ′ is admissible, writ-
ten R : ρ ↔ρ′, iff it satisﬁes two requirements:
1. Respect for observational equivalence: if R(e, e′) and d ∼=ρ e and d′ ∼=ρ′ e′, then R(d, d′).
2. Closure under converse evaluation: if R(e, e′), then if d 7−→e, then R(d, e′) and if d′ 7−→e′, then
R(e, d′).
Closure under converse evaluation is a consequence of respect for observational equivalence, but
we are not yet in a position to establish this fact.
The judgment δ : ∆states that δ is a type substitution that assigns a closed type to each type
variable t ∈∆. A type substitution δ induces a substitution function ˆδ on types given by the
equation
ˆδ(τ) = [δ(t1), . . . , δ(tn)/t1, . . . , tn]τ,
and similarly for expressions. Substitution is extended to contexts point-wise by deﬁning ˆδ(Γ)(x) =
ˆδ(Γ(x)) for each x ∈dom(Γ).
Let δ and δ′ be two type substitutions of closed types to the type variables in ∆. An admissible
relation assignment η between δ and δ′ is an assignment of an admissible relation η(t) : δ(t) ↔δ′(t)
to each t ∈∆. The judgment η : δ ↔δ′ states that η is an admissible relation assignment between
δ and δ′.
Logical equivalence is deﬁned in terms of its generalization, called parametric logical equivalence,
written e ∼τ e′ [η : δ ↔δ′], deﬁned as follows.
Deﬁnition 48.5 (Parametric Logical Equivalence). The relation e ∼τ e′ [η : δ ↔δ′] is deﬁned by

48.3 Logical Equivalence
463
induction on the structure of τ by the following conditions:
e ∼t e′ [η : δ ↔δ′]
iff
η(t)(e, e′)
e ∼2 e′ [η : δ ↔δ′]
iff
e ≃e′
e ∼τ1→τ2 e′ [η : δ ↔δ′]
iff
e1 ∼τ1 e′
1 [η : δ ↔δ′] implies
e(e1) ∼τ2 e′(e′
1) [η : δ ↔δ′]
e ∼∀(t.τ) e′ [η : δ ↔δ′]
iff
for every ρ, ρ′, and every admissible R : ρ ↔ρ′,
e[ρ] ∼τ e′[ρ′] [η ⊗t ,→R : δ ⊗t ,→ρ ↔δ′ ⊗t ,→ρ′]
Logical equivalence is deﬁned in terms of parametric logical equivalence by considering all
possible interpretations of its free type- and expression variables. An expression substitution γ for
a context Γ, written γ : Γ, is a substitution of a closed expression γ(x) : Γ(x) to each variable
x ∈dom(Γ). An expression substitution γ : Γ induces a substitution function, ˆγ, deﬁned by the
equation
ˆγ(e) = [γ(x1), . . . , γ(xn)/x1, . . . , xn]e,
where the domain of Γ consists of the variables x1, . . . , xn. The relation γ ∼Γ γ′ [η : δ ↔δ′] is
deﬁned to hold iff dom(γ) = dom(γ′) = dom(Γ), and γ(x) ∼Γ(x) γ′(x) [η : δ ↔δ′] for every
variable x in their common domain.
Deﬁnition 48.6 (Logical Equivalence). The expressions ∆; Γ ⊢e : τ and ∆; Γ ⊢e′ : τ are logically
equivalent, written ∆; Γ ⊢e ∼e′ : τ iff, for every assignment δ and δ′ of closed types to type variables
in ∆, and every admissible relation assignment η : δ ↔δ′, if γ ∼Γ γ′ [η : δ ↔δ′], then ˆγ( ˆδ(e)) ∼τ
bγ′(bδ′(e′)) [η : δ ↔δ′].
When e, e′, and τ are closed, this deﬁnition states that e ∼τ e′ iff e ∼τ e′ [∅: ∅↔∅], so that logical
equivalence is indeed a special case of its generalization.
Lemma 48.7 (Closure under Converse Evaluation). Suppose that e ∼τ e′ [η : δ ↔δ′]. If d 7−→e,
then d ∼τ e′, and if d′ 7−→e′, then e ∼τ d′.
Proof. By induction on the structure of τ. When τ = t, the result holds by the deﬁnition of admis-
sibility. Otherwise the result follows by induction, making use of the deﬁnition of the transition
relation for applications and type applications.
Lemma 48.8 (Respect for Observational Equivalence). Suppose that e ∼τ e′ [η : δ ↔δ′]. If d ∼= ˆδ(τ) e
and d′ ∼=bδ′(τ) e′, then d ∼τ d′ [η : δ ↔δ′].
Proof. By induction on the structure of τ, relying on the deﬁnition of admissibility, and the con-
gruence property of observational equivalence. For example, if τ = ∀(t.τ2), then we are to show
that for every admissible R : ρ ↔ρ′,
d[ρ] ∼τ2 d′[ρ′] [η ⊗t ,→R : δ ⊗t ,→ρ ↔δ′ ⊗t ,→ρ′].
Because observational equivalence is a congruence, we have d[ρ] ∼=[ρ/t] ˆδ(τ2) e[ρ], and d′[ρ′] ∼=[ρ′/t]bδ′(τ2)
e′[ρ]. It follows that
e[ρ] ∼τ2 e′[ρ′] [η ⊗t ,→R : δ ⊗t ,→ρ ↔δ′ ⊗t ,→ρ′],
from which the result follows by induction.

464
48.3 Logical Equivalence
Corollary 48.9. The relation e ∼τ e′ [η : δ ↔δ′] is an admissible relation between closed types ˆδ(τ) and
bδ′(τ).
Proof. By Lemmas 48.7 and 48.8.
Corollary 48.10. If ∆; Γ ⊢e ∼e′ : τ, and ∆; Γ ⊢d ∼= e : τ and ∆; Γ ⊢d′ ∼= e′ : τ, then ∆; Γ ⊢d ∼d′ : τ.
Proof. By Lemma 48.3 and Corollary 48.9.
Lemma 48.11 (Compositionality). Let R : ˆδ(ρ) ↔bδ′(ρ) be the relational interpretation of some type ρ,
which is to say R(d, d′) holds iff d ∼ρ d′ [η : δ ↔δ′]. Then e ∼[ρ/t]τ e′ [η : δ ↔δ′] if, and only if,
e ∼τ e′ [η ⊗t ,→R : δ ⊗t ,→ˆδ(ρ) ↔δ′ ⊗t ,→bδ′(ρ)].
Proof. By induction on the structure of τ. When τ = t, the result is immediate from the choice
of the relation R. When τ = t′ ̸= t, the result follows from Deﬁnition 48.5. When τ = τ1 →τ2,
the result follows by induction, using Deﬁnition 48.5. Similarly, when or τ = ∀(u.τ1), the result
follows by induction, noting that we may assume, without loss of generality, that u ̸= t and
u /∈ρ.
Despite the strong conditions on polymorphic types, logical equivalence is not too restrictive—
every expression satisﬁes its constraints. This result is often called the parametricity theorem or the
abstraction theorem:
Theorem 48.12 (Parametricity). If ∆; Γ ⊢e : τ, then ∆; Γ ⊢e ∼e : τ.
Proof. By rule induction on the statics of F given by rules (16.2).
We consider two representative cases here.
Rule (16.2d) Suppose δ : ∆, δ′ : ∆, η : δ ↔δ′, and γ ∼Γ γ′ [η : δ ↔δ′]. By induction we have that
for all ρ, ρ′, and admissible R : ρ ↔ρ′,
[ρ/t] ˆγ( ˆδ(e)) ∼τ [ρ′/t] bγ′(bδ′(e)) [η∗: δ∗↔δ′
∗],
where η∗= η ⊗t ,→R, δ∗= δ ⊗t ,→ρ, and δ′∗= δ′ ⊗t ,→ρ′. Because
Λ(t) ˆγ( ˆδ(e))[ρ] 7−→∗[ρ/t] ˆγ( ˆδ(e))
and
Λ(t) bγ′(bδ′(e))[ρ′] 7−→∗[ρ′/t] bγ′(bδ′(e)),
the result follows by Lemma 48.7.
Rule (16.2e) Suppose δ : ∆, δ′ : ∆, η : δ ↔δ′, and γ ∼Γ γ′ [η : δ ↔δ′]. By induction we have
ˆγ( ˆδ(e)) ∼∀(t.τ) bγ′(bδ′(e)) [η : δ ↔δ′]

48.3 Logical Equivalence
465
Let ˆρ = ˆδ(ρ) and ˆρ′ = bδ′(ρ). Deﬁne the relation R : ˆρ ↔ˆρ′ by R(d, d′) iff d ∼ρ d′ [η : δ ↔δ′].
By Corollary 48.9, this relation is admissible.
By the deﬁnition of logical equivalence at polymorphic types, we obtain
ˆγ( ˆδ(e))[ ˆρ] ∼τ bγ′(bδ′(e))[ ˆρ′] [η ⊗t ,→R : δ ⊗t ,→ˆρ ↔δ′ ⊗t ,→ˆρ′].
By Lemma 48.11
ˆγ( ˆδ(e))[ ˆρ] ∼[ρ/t]τ bγ′(bδ′(e))[ ˆρ′] [η : δ ↔δ′]
But
ˆγ( ˆδ(e))[ ˆρ] = ˆγ( ˆδ(e))[ ˆδ(ρ)]
(48.1)
= ˆγ( ˆδ(e[ρ])),
(48.2)
and similarly
bγ′(bδ′(e))[ ˆρ′] = bγ′(bδ′(e))[bδ′(ρ)]
(48.3)
= bγ′(bδ′(e[ρ])),
(48.4)
from which the result follows.
Corollary 48.13. If ∆; Γ ⊢e ∼= e′ : τ, then ∆; Γ ⊢e ∼e′ : τ.
Proof. By Theorem 48.12 ∆; Γ ⊢e ∼e : τ, and hence by Corollary 48.10, ∆; Γ ⊢e ∼e′ : τ.
Lemma 48.14 (Congruence). If ∆; Γ ⊢e ∼e′ : τ and C : (∆; Γ ▷τ) ⇝(∆′; Γ′ ▷τ′), then ∆′; Γ′ ⊢
C{e} ∼C{e′} : τ′.
Proof. By induction on the structure of C, following along very similar lines to the proof of Theo-
rem 48.12.
Lemma 48.15 (Consistency). Logical equivalence is consistent.
Proof. Follows from the deﬁnition of logical equivalence.
Corollary 48.16. If ∆; Γ ⊢e ∼e′ : τ, then ∆; Γ ⊢e ∼= e′ : τ.
Proof. By Lemma 48.15 Logical equivalence is consistent, and by Lemma 48.14, it is a congruence,
and hence is contained in observational equivalence.
Corollary 48.17. Logical and observational equivalence coincide.
Proof. By Corollaries 48.13 and 48.16.
If d : τ and d 7−→e, then d ∼τ e, and hence by Corollary 48.16, d ∼=τ e. Therefore if a relation
respects observational equivalence, it must also be closed under converse evaluation. The second
condition on admissibility is superﬂuous, now that we have established the coincidence of logical
and observational equivalence.

466
48.4 Parametricity Properties
Corollary 48.18 (Extensionality).
1. e ∼=τ1→τ2 e′ iff for all e1 : τ1, e(e1) ∼=τ2 e′(e1).
2. e ∼=∀(t.τ) e′ iff for all ρ, e[ρ] ∼=[ρ/t]τ e′[ρ].
Proof. The forward direction is immediate in both cases, because observational equivalence is a
congruence, by deﬁnition. The backward direction is proved similarly in both cases, by appeal to
Theorem 48.12. In the ﬁrst case, by Corollary 48.17 it sufﬁces to show that e ∼τ1→τ2 e′. To this
end suppose that e1 ∼τ1 e′
1. We are to show that e(e1) ∼τ2 e′(e′
1). By the assumption we have
e(e′
1) ∼=τ2 e′(e′
1). By parametricity we have e ∼τ1→τ2 e, and hence e(e1) ∼τ2 e(e′
1). The result
then follows by Lemma 48.8. In the second case, by Corollary 48.17 it is sufﬁcient to show that
e ∼∀(t.τ) e′. Suppose that R : ρ ↔ρ′ for some closed types ρ and ρ′. It sufﬁces to show that
e[ρ] ∼τ e′[ρ′] [η : δ ↔δ′], where η(t) = R, δ(t) = ρ, and δ′(t) = ρ′. By the assumption we have
e[ρ′] ∼=[ρ′/t]τ e′[ρ′]. By parametricity e ∼∀(t.τ) e, and hence e[ρ] ∼τ e′[ρ′] [η : δ ↔δ′]. The result
then follows by Lemma 48.8.
Lemma 48.19 (Identity Extension). Let η : δ ↔δ be such that η(t) is observational equivalence at type
δ(t) for each t ∈dom(δ). Then e ∼τ e′ [η : δ ↔δ] iff e ∼= ˆδ(τ) e′.
Proof. The backward direction follows from Theorem 48.12 and respect for observational equiv-
alence. The forward direction is proved by induction on the structure of τ, appealing to Corol-
lary 48.18 to establish observational equivalence at function and polymorphic types.
48.4
Parametricity Properties
The parametricity theorem enables us to deduce properties of expressions of F that hold solely
because of their type. The stringencies of parametricity ensure that a polymorphic type has very
few inhabitants. For example, we may prove that every expression of type ∀(t.t →t) behaves like
the identity function.
Theorem 48.20. Let e : ∀(t.t →t) be arbitrary, and let id be Λ(t) λ (x : t) x. Then e ∼=∀(t.t→t) id.
Proof. By Corollary 48.17 it is sufﬁcient to show that e ∼∀(t.t→t) id. Let ρ and ρ′ be arbitrary closed
types, let R : ρ ↔ρ′ be an admissible relation, and suppose that e0 R e′
0. We are to show
e[ρ](e0) R id[ρ](e′
0),
which, given the deﬁnition of id and closure under converse evaluation, is to say
e[ρ](e0) R e′
0.
It sufﬁces to show that e[ρ](e0) ∼=ρ e0, for then the result follows by the admissibility of R and the
assumption e0 R e′
0.
By Theorem 48.12 we have e ∼∀(t.t→t) e. Let the relation S : ρ ↔ρ be deﬁned by d S d′ iff
d ∼=ρ e0 and d′ ∼=ρ e0. This relation is clearly admissible, and we have e0 S e0. It follows that
e[ρ](e0) S e[ρ](e0),

48.4 Parametricity Properties
467
and so, by the deﬁnition of the relation S, e[ρ](e0) ∼=ρ e0.
In Chapter 16 we showed that product, sum, and natural numbers types are all deﬁnable in
F. The proof of deﬁnability in each case consisted of showing that the type and its associated
introduction and elimination forms are encodable in F. The encodings are correct in the (weak)
sense that the dynamics of these constructs as given in the earlier chapters is derivable from the
dynamics of F via these deﬁnitions. By taking advantage of parametricity we may extend these
results to obtain a strong correspondence between these types and their encodings.
As a ﬁrst example, let us consider the representation of the unit type, unit, in F, as deﬁned in
Chapter 16 by the following equations:
unit = ∀(r.r →r)
⟨⟩= Λ(r) λ (x : r) x
It is easy to see that ⟨⟩: unit according to these deﬁnitions. But this says that the type unit is
inhabited (has an element). What we would like to know is that, up to observational equivalence,
the expression ⟨⟩is the only element of that type. But this is the content of Theorem 48.20. We say
that the type unit is strongly deﬁnable within F.
Continuing in this vein, let us examine the deﬁnition of the binary product type in F, also given
in Chapter 16:
τ1 × τ2 = ∀(r.(τ1 →τ2 →r) →r)
⟨e1, e2⟩= Λ(r) λ (x : τ1 →τ2 →r) x(e1)(e2)
e · l = e[τ1](λ (x : τ1) λ (y : τ2) x)
e · r = e[τ2](λ (x : τ1) λ (y : τ2) y)
It is easy to check that ⟨e1, e2⟩· l ∼=τ1 e1 and ⟨e1, e2⟩· r ∼=τ2 e2 by a direct calculation.
We wish to show that the ordered pair, as deﬁned above, is the unique such expression, and
hence that Cartesian products are strongly deﬁnable in F. We will make use of a lemma governing
the behavior of the elements of the product type whose proof relies on Theorem 48.12.
Lemma 48.21. If e : τ1 × τ2, then e ∼=τ1×τ2 ⟨e1, e2⟩for some e1 : τ1 and e2 : τ2.
Proof. Expanding the deﬁnitions of pairing and the product type, and applying Corollary 48.17,
we let ρ and ρ′ be arbitrary closed types, and let R : ρ ↔ρ′ be an admissible relation between
them. Suppose further that
h ∼τ1→τ2→t h′ [η : δ ↔δ′],
where η(t) = R, δ(t) = ρ, and δ′(t) = ρ′ (and each is undeﬁned on t′ ̸= t). We are to show that for
some e1 : τ1 and e2 : τ2,
e[ρ](h) ∼t h′(e1)(e2) [η : δ ↔δ′],
which is to say
e[ρ](h) R h′(e1)(e2).
Now by Theorem 48.12 we have e ∼τ1×τ2 e. Deﬁne the relation S : ρ ↔ρ′ by d S d′ iff the following
conditions are satisﬁed:

468
48.4 Parametricity Properties
1. d ∼=ρ h(d1)(d2) for some d1 : τ1 and d2 : τ2;
2. d′ ∼=ρ′ h′(d′
1)(d′
2) for some d′
1 : τ1 and d′
2 : τ2;
3. d R d′.
This relation is clearly admissible. Noting that
h ∼τ1→τ2→t h′ [η′ : δ ↔δ′],
where η′(t) = S and η′(t′) is undeﬁned for t′ ̸= t, we conclude that e[ρ](h) S e[ρ′](h′), and hence
e[ρ](h) R h′(d′
1)(d′
2),
as required.
Now suppose that e : τ1 × τ2 is such that e · l ∼=τ1 e1 and e · r ∼=τ2 e2. We wish to show that
e ∼=τ1×τ2 ⟨e1, e2⟩. From Lemma 48.21 it follows that e ∼=τ1×τ2 ⟨e · l, e · r⟩by congruence and direct
calculation. Hence, by congruence we have e ∼=τ1×τ2 ⟨e1, e2⟩.
By a similar line of reasoning we may show that the Church encoding of the natural numbers
given in Chapter 16 strongly deﬁnes the natural numbers in that the following properties hold:
1. iter z {z ,→e0 | s(x) ,→e1} ∼=ρ e0.
2. iter s(e) {z ,→e0 | s(x) ,→e1} ∼=ρ [iter e {z ,→e0 | s(x) ,→e1}/x]e1.
3. Suppose that x : nat ⊢r(x) : ρ. If
(a) r(z) ∼=ρ e0, and
(b) r(s(e)) ∼=ρ [r(e)/x]e1,
then for every e : nat, r(e) ∼=ρ iter e {z ,→e0 | s(x) ,→e1}.
The ﬁrst two equations, which constitute weak deﬁnability, are easily established by calculation,
using the deﬁnitions given in Chapter 16. The third property, the unicity of the iterator, is proved
using parametricity by showing that every closed expression of type nat is observationally equiv-
alent to a numeral n. We then argue for unicity of the iterator by mathematical induction on n ≥0.
Lemma 48.22. If e : nat, then either e ∼=nat z, or there exists e′ : nat such that e ∼=nat s(e′). Conse-
quently, there exists n ≥0 such that e ∼=nat n.
Proof. By Theorem 48.12 we have e ∼nat e. Deﬁne the relation R : nat ↔nat to be the strongest
relation such that d R d′ iff either d ∼=nat z and d′ ∼=nat z, or d ∼=nat s(d1) and d′ ∼=nat s(d′
1)
and d1 R d′
1. It is easy to see that z R z, and if e R e′, then s(e) R s(e′). Letting zero = z and
succ = λ (x : nat) s(x), we have
e[nat](zero)(succ) R e[nat](zero)(succ).
The result follows by the induction principle arising from the deﬁnition of R as the strongest
relation satisfying its deﬁning conditions.

48.5 Representation Independence, Revisited
469
48.5
Representation Independence, Revisited
In Section 17.4 we discussed the property of representation independence for abstract types. If two
implementations of an abstract type are “similar”, then the client behavior is not affected by replac-
ing one for the other. The crux of the matter is the deﬁnition of similarity of two implementations.
Informally, two implementations of an abstract type are similar if there is a relation R between
their representation types that is preserved by the operations of the type. The relation R may be
thought of as expressing the “equivalence” of the two representations; checking that each opera-
tion preserves R amounts to checking that the result of performing that operation on equivalent
representations yields equivalent results.
As an example, we argued in Section 17.4 that two implementations of a queue abstraction are
similar. The two representations of queues are related by a relation R such that q R (b, f ) iff q is b
followed by the reversal of f. When then argued that the operations preserve this relationship, and
then claimed, without proof, that the behavior of the client would not be disrupted by changing
one implementation to the other.
The proof of this claim relies on parametricity, as may be seen by considering the deﬁnability
of existential types in F given in Section 17.3. According to that deﬁnition, the client, e, of an
abstract type ∃(t.τ) is a polymorphic function of type ∀(t.τ →τ2), where τ2, the result type of
the computation, does not involve the type variable t. Being polymorphic, the client enjoys the
parametricity property given by Theorem 48.12. Speciﬁcally, suppose that ρ1 and ρ2 are two closed
representation types and that R : ρ1 ↔ρ2 is an admissible relation between them. For example, in
the case of the queue abstraction, ρ1 is the type of lists of elements of the queue, ρ2 is the type of
a pair of lists of elements, and R is the relation given above. Suppose further that e1 : [ρ1/t]τ and
e2 : [ρ2/t]τ are two implementations of the operations such that
e1 ∼τ e2 [η : δ1 ↔δ2],
(48.5)
where η(t) = R, δ1(t) = ρ1, and δ2(t) = ρ2. In the case of the queues example the expression e1 is
the implementation of the queue operations in terms of lists, and the e2 is the implementation in
terms of pairs of lists described earlier. Condition (48.5) states that the two implementations are
similar in that they preserve the relation R between the representation types. By Theorem 48.12 it
follows that the client e satisﬁes
e ∼τ2 e [η : δ1 ↔δ2].
But because τ2 is a closed type (in particular, does not involve t), this is equivalent to
e ∼τ2 e [∅: ∅↔∅].
But then by Lemma 48.19 we have
e[ρ1](e1) ∼=τ2 e[ρ2](e2).
That is, the client behavior is not affected by the change of representation.

470
48.6 Notes
48.6
Notes
The concept of parametricity is latent in the proof of normalization for System F (Girard, 1972).
Reynolds (1983), though technically ﬂawed due to its reliance on a (non-existent) set-theoretic
model of polymorphism, emphasizes the centrality of logical equivalence for characterizing equal-
ity of polymorphic programs. The application of parametricity to representation independence
was suggested by Reynolds, and developed for existential types by Mitchell (1986) and Pitts (1998).
The extension of System F with a “positive” (inductively deﬁned) observable type appears to be
needed to even deﬁne observational equivalence, but this point seems not to have been made
elsewhere in the literature.

Chapter 49
Process Equivalence
As the name implies a process is an ongoing computation that may interact with other processes
by sending and receiving messages. From this point of view a concurrent computation has no
deﬁnite “ﬁnal outcome” but rather affords an opportunity for interaction that may well continue
indeﬁnitely. The notion of equivalence of processes must therefore be based on their potential
for interaction, rather than on the “answer” that they may compute. Let P and Q be such that
⊢Σ P proc and ⊢Σ Q proc. We say that P and Q are equivalent, written P ≈Σ Q, iff there is a
bisimulation R such that P RΣ Q. A family of relations R = { RΣ }Σ is a bisimulation iff whenever
P may evolve to P′ taking the action α, then Q may also evolve to some process Q′ taking the same
action such that P′ RΣ Q′, and, conversely, if Q may evolve to Q′ taking action α, then P may
evolve to P′ taking the same action, and P′ RΣ Q′. This correspondence captures the idea that the
two processes afford the same opportunities for interaction in that they each simulate each other’s
behavior with respect to their ability to interact with their environment.
49.1
Process Calculus
We will consider a process calculus that consolidates the main ideas explored in Chapters 39
and 40. We assume as given an ambient language of expressions that includes the type clsfd
of classiﬁed values (see Chapter 33). Channels are treated as dynamically generated classes with
which to build messages, as described in Chapter 40.
The syntax of the process calculus is given by the following grammar:
Proc
P
::=
stop
1
inert
conc(P1; P2)
P1 ⊗P2
composition
await(E)
$ E
synchronize
new{τ}(a.P)
ν a ~ τ.P
allocation
emit(e)
! e
broadcast
Evt
E
::=
null
0
null
or(E1; E2)
E1 + E2
choice
acc(x.P)
? (x.P)
acceptance

472
49.1 Process Calculus
The statics is given by the judgments Γ ⊢Σ P proc and Γ ⊢Σ E event deﬁned by the following
rules. We assume as given a judgment Γ ⊢Σ e : τ for τ a type including the type clsfd of classiﬁed
values.
Γ ⊢Σ 1 proc
(49.1a)
Γ ⊢Σ P1 proc
Γ ⊢Σ P2 proc
Γ ⊢Σ P1 ⊗P2 proc
(49.1b)
Γ ⊢Σ E event
Γ ⊢Σ $ E proc
(49.1c)
Γ ⊢Σ,a~τ P proc
Γ ⊢Σ ν a ~ τ.P proc
(49.1d)
Γ ⊢Σ e : clsfd
Γ ⊢Σ ! e proc
(49.1e)
Γ ⊢Σ 0 event
(49.1f)
Γ ⊢Σ E1 event
Γ ⊢Σ E2 event
Γ ⊢Σ E1 + E2 event
(49.1g)
Γ, x : clsfd ⊢Σ P proc
Γ ⊢Σ ? (x.P) event
(49.1h)
The dynamics is given by the judgments P
α7−→
Σ P′ and E
α=⇒
Σ P, deﬁned as in Chapter 39. We as-
sume as given the judgments e 7−→
Σ e′ and e valΣ for expressions. Processes and events are identiﬁed
up to structural congruence, as described in Chapter 39.
P1
α7−→
Σ P′
1
P1 ⊗P2
α7−→
Σ P′
1 ⊗P2
(49.2a)
P1
α7−→
Σ P′
1
P2
α7−→
Σ P′
2
P1 ⊗P2
ε7−→
Σ P′
1 ⊗P′
2
(49.2b)
E
α=⇒
Σ P
$ E α7−→
Σ P
(49.2c)
P
α
7−−−→
Σ,a~τ P′
⊢Σ α action
ν a ~ τ.P α7−→
Σ ν a ~ τ.P′
(49.2d)

49.2 Strong Equivalence
473
e valΣ
⊢Σ e : clsfd
! e e !
7−→
Σ 1
(49.2e)
E1
α=⇒
Σ P
E1 + E2
α=⇒
Σ P
(49.2f)
e valΣ
? (x.P) e ?
=⇒
Σ [e/x]P
(49.2g)
Assuming that substitution is valid for expressions, it is also valid for processes and events.
Lemma 49.1.
1. If Γ, x : τ ⊢Σ P proc and Γ ⊢Σ e : τ, then Γ ⊢Σ [e/x]P proc.
2. If Γ, x : τ ⊢Σ E event and Γ ⊢Σ e : τ, then Γ ⊢Σ [e/x]E event.
Transitions preserve well-formedness of processes and events.
Lemma 49.2.
1. If ⊢Σ P proc and P α7−→
Σ P′, then ⊢Σ P′ proc.
2. If ⊢Σ E event and E
α=⇒
Σ P, then ⊢Σ P proc.
49.2
Strong Equivalence
Bisimilarity makes precise the informal idea that two processes are equivalent if they each can take
the same actions and, in doing so, evolve into equivalent processes. A process relation P is a family
{ PΣ } of binary relations between processes P and Q such that ⊢Σ P proc and ⊢Σ Q proc, and an
event relation E is a family { EΣ } of binary relations between events E and F such that ⊢Σ E event
and ⊢Σ F event. A (strong) bisimulation is a pair (P, E) consisting of a process relation P and an
event relation E satisfying the following conditions:
1. If P PΣ Q, then
(a) if P α7−→
Σ P′, then there exists Q′ such that Q α7−→
Σ Q′ with P′ PΣ Q′, and
(b) if Q α7−→
Σ Q′, then there exists P′ such that P α7−→
Σ P′ with P′ PΣ Q′.
2. If E EΣ F, then
(a) if E
α=⇒
Σ P, then there exists Q such that F
α=⇒
Σ Q with P PΣ Q, and
(b) if F
α=⇒
Σ Q, then there exists P such that E
α=⇒
Σ P with P PΣ Q.

474
49.2 Strong Equivalence
The qualiﬁer “strong” refers to the fact that the action α in the conditions on being a bisimulation
include the silent action ε. (In Section 49.3 we discuss another notion of bisimulation in which the
silent actions are treated specially.)
(Strong) equivalence is the pair (≈, ≈) of process and event relations such that P ≈Σ Q and
E ≈Σ F iff there exists a strong bisimulation (P, E) such that P PΣ Q, and E EΣ F.
Lemma 49.3. Strong equivalence is a strong bisimulation.
Proof. Follows immediately from the deﬁnition.
The deﬁnition of strong equivalence gives rise to the principle of proof by coinduction. To show
that P ≈Σ Q, it is enough to give a bisimulation (P, E) such that P PΣ Q (and similarly for events).
An instance of coinduction that arises fairly often is to choose (P, E) to be (≈∪P0, ≈∪E0) for
some P0 and E0 such that P P0 Q, and show that this expansion is a bisimulation. Because strong
equivalence is itself a bisimulation, this reduces to show that if P′ P0 Q′ and P′
α7−→
Σ
P′′, then
Q′
α7−→
Σ Q′′ for some Q′′ such that either P′′ ≈Σ Q′′ or P′′ P0 Q′′ (and analogously for transitions
from Q′, and similarly for event transitions). This proof method amounts to assuming what we are
trying to prove and showing that this assumption is tenable. The proof that the expanded relation
is a bisimulation may make use of the assumptions P0 and E0; in this sense “circular reasoning” is
a perfectly valid method of proof.
Lemma 49.4. Strong equivalence is an equivalence relation.
Proof. For reﬂexivity and symmetry, it sufﬁces to note that the identity relation is a bisimulation,
as is the converse of a bisimulation. For transitivity we need that the composition of two bisimu-
lations is again a bisimulation, which follows directly from the deﬁnition.
It remains to verify that strong equivalence is a congruence, which means that each of the
process- and event-forming constructs respects strong equivalence. To show this we require the
open extension of strong equivalence to processes and events with free variables. The relation Γ ⊢Σ
P ≈Q is deﬁned for processes P and Q such that Γ ⊢Σ P proc and Γ ⊢Σ Q proc to mean that
ˆγ(P) ≈Σ ˆγ(Q) for every substitution, γ, of closed values of appropriate type for the variables Γ.
Lemma 49.5. If Γ, x : clsfd ⊢Σ P ≈Q, then Γ ⊢Σ ? (x.P) ≈? (x.Q).
Proof. Fix a closing substitution γ for Γ, and let ˆP = ˆγ(P) and ˆQ = ˆγ(Q). By assumption we have
x : clsfd ⊢Σ ˆP ≈ˆQ. We are to show that ? (x. ˆP) ≈Σ ? (x. ˆQ). The proof is by coinduction, taking
P = ≈and E = ≈∪E0, where
E0 = { (? (x.P′), ? (x.Q′)) | x : clsfd ⊢Σ P′ ≈Q′ }.
Clearly ? (x. ˆP) E0 ? (x. ˆQ). Suppose that ? (x.P′) E0 ? (x.Q′). By inspection of rules (49.2), if
? (x.P′)
α=⇒
Σ
P′′, then α = v ? and P′′ = [v/x]P′ for some v valΣ such that ⊢Σ v : clsfd. But
? (x.Q′)
v ?
=⇒
Σ
[v/x]Q′, and we have that [v/x]P′ ≈Σ [v/x]Q′ by the deﬁnition of E0, and hence
[v/x]P′ E0 [v/x]Q′, as required. The symmetric case follows symmetrically, completing the proof.

49.2 Strong Equivalence
475
Lemma 49.6. If Γ ⊢Σ,a~τ P ≈Q, then Γ ⊢Σ ν a ~ τ.P ≈ν a ~ τ.Q.
Proof. Fix a closing value substitution γ for Γ, and let ˆP = ˆγ(P) and ˆQ = ˆγ(Q). Assuming that
ˆP ≈Σ,a~τ
ˆQ, we are to show that ν a ~ τ. ˆP ≈Σ ν a ~ τ. ˆQ. The proof is by coinduction, taking
P = ≈∪P0 and E = ≈, where
P0 = { (ν a ~ τ.P′, ν a ~ τ.Q′) | P′ ≈Σ,a~τ Q′ }.
Clearly ν a ~ τ. ˆP P0 ν a ~ τ. ˆQ. Suppose that ν a ~ τ.P′ P0 ν a ~ τ.Q′, and that ν a ~ τ.P′
α7−→
Σ P′′. By
inspection of rules (49.2), we see that ⊢Σ α action and that P′′ = ν a ~ τ.P′′′ for some P′′′ such
that P′
α
7−−−→
Σ,a~τ
P′′′. But by deﬁnition of P0 we have P′ ≈Σ,a~τ Q′, and hence Q′
α
7−−−→
Σ,a~τ Q′′′ with
P′′′ ≈Σ,a~τ Q′′′. Letting Q′′ = ν a ~ τ.Q′, we have that ν a ~ τ.Q′
α
7−−−→
Σ,a~τ
Q′′ and by deﬁnition of
P0 we have P′′ P0 Q′′, as required. The symmetric case is proved symmetrically, completing the
proof.
Lemmas 49.5 and 49.6 capture two different cases of binding, the former of variables, and
the latter of classes. The hypothesis of Lemma 49.5 relates all substitutions for the variable x in
the recipient processes, whereas the hypothesis of Lemma 49.6 relates the constituent processes
schematically in the class name, a. This makes all the difference, for if we were to consider all
substitution instances of a class name by another class name, then a class would no longer be
“new” within its scope, because we could identify it with an “old” class by substitution. On
the other hand we must consider substitution instances for variables, because the meaning of a
variable is given in such terms. This shows that classes and variables must be distinct concepts.
(See Chapter 33 for an example of what goes wrong when the two concepts are confused.)
Lemma 49.7. If Γ ⊢Σ P1 ≈Q1 and Γ ⊢Σ P2 ≈Q2, then Γ ⊢Σ P1 ⊗P2 ≈Q1 ⊗Q2.
Proof. Let γ be a closing value substitution for Γ, and let ˆPi = ˆγ(Pi) and ˆQi = ˆγ(Qi) for i = 1, 2.
The proof is by coinduction, considering the relation P = ≈∪P0 and E = ≈, where
P0 = { (P′
1 ⊗P′
2, Q′
1 ⊗Q′
2) | P′
1 ≈Σ Q′
1 and P′
2 ≈Σ Q′
2 }.
Suppose that P′
1 ⊗P′
2 P0 Q′
1 ⊗Q′
2, and that P′
1 ⊗P′
2
α7−→
Σ
P′′. There are two cases to consider, the
interesting one being rule (49.2b). In this case we have P′′ = P′′
1 ⊗P′′
2 with P′
1
α7−→
Σ P′′
1 and P′
2
α7−→
Σ P′′
2 .
By deﬁnition of P0 we have that Q′
1
α7−→
Σ Q′′
1 and Q′
2
α7−→
Σ Q′′
2 with P′′
1 ≈Σ Q′′
1 and P′′
2 ≈Σ Q′′
2. Letting
Q′′ = Q′′
1 ⊗Q′′
2, we have that P′′ P0 Q′′, as required. The symmetric case is handled symmetrically,
and rule (49.2a) is handled similarly.
Lemma 49.8. If Γ ⊢Σ E1 ≈F1 and Γ ⊢Σ E2 ≈F2, then Γ ⊢Σ E1 + E2 ≈F1 + F2.
Proof. Follows immediately from rules (49.2) and the deﬁnition of bisimulation.
Lemma 49.9. If Γ ⊢Σ E ≈F, then Γ ⊢Σ $ E ≈$ F.

476
49.3 Weak Equivalence
Proof. Follows immediately from rules (49.2) and the deﬁnition of bisimulation.
Lemma 49.10. If Γ ⊢Σ d ∼= e : clsfd, then Γ ⊢Σ ! d ≈! e.
Proof. The process calculus introduces no new observations on expressions, so that d and e remain
indistinguishable as actions.
Theorem 49.11. Strong equivalence is a congruence.
Proof. Follows immediately from the preceding lemmas, which cover each case separately.
49.3
Weak Equivalence
Strong equivalence expresses the idea that two processes are equivalent if they simulate each other
step-by-step. Every action taken by one process is matched by a corresponding action taken by the
other. This seems natural for the non-trivial actions e ! and e ?, but is arguably overly restrictive for
the silent action ε. Silent actions correspond to the actual steps of computation, whereas the send
and receive actions express the potential to interact with another process. Silent steps are therefore
of a very different ﬂavor than the other forms of action, and therefore might usefully be treated
differently from them. Weak equivalence seeks to do just that.
Silent actions arise within the process calculus itself (when two processes communicate), but
they play an even more important role when the dynamics of expressions is considered explicitly
(as in Chapter 40). For then each step e 7−→
Σ e′ of evaluation of an expression corresponds to a silent
transition for any process in which it is embedded. In particular, ! e
ε7−→
Σ ! e′ whenever e 7−→
Σ e′. We
may also consider atomic processes of the form run(m) consisting of a command to be executed in
accordance with the rules of some underlying dynamics. Here again we would expect that each
step of command execution induces a silent transition from one atomic process to another.
From the point of view of equivalence, it therefore seems sensible to allow that a silent action by
one process may be mimicked by one or more silent actions by another. For example, there is little
to be gained by distinguishing, say, run(ret 3+4) from run(ret (1+2)+(2+2)) merely because the
latter takes more steps to compute the same value than the former! The purpose of weak equiva-
lence is precisely to disregard such trivial distinctions by allowing a transition to be matched by a
matching transition, possibly preceded by any number of silent transitions.
A weak bisimulation is a pair (P, E) consisting of a process relation P and an event relation E
satisfying the following conditions:
1. If P PΣ Q, then
(a) if P
α7−→
Σ
P′, where α ̸= ε, then there exists Q′′ and Q′ such that Q
ε7−→
Σ
∗Q′′
α7−→
Σ Q′ with
P′ PΣ Q′, and if P
ε7−→
Σ P′, then Q
ε7−→
Σ
∗Q′ with P′ PΣ Q′;
(b) if Q
α7−→
Σ
Q′, where α ̸= ε, then there exists P′′ and P′ such that P
ε7−→
Σ
∗P′′
α7−→
Σ
P′ with
P′ PΣ Q′, and if Q
ε7−→
Σ Q′, then P
ε7−→
Σ
∗P′ with P′ PΣ Q′;

49.4 Notes
477
2. If E EΣ F, then
(a) if E
α=⇒
Σ P, then there exists Q such that F
α=⇒
Σ Q with P PΣ Q, and
(b) if F
α=⇒
Σ Q, then there exists P such that E
α=⇒
Σ P with P PΣ Q.
(The conditions on the event relation are the same as for strong bisimilarity because there are, in
this calculus, no silent actions on events.)
Weak equivalence is the pair (∼, ∼) of process and event relations deﬁned by P ∼Σ Q and E ∼Σ F
iff there exists a weak bisimulation (P, E) such that P PΣ Q, and E EΣ F. The open extension of
weak equivalence, written Γ ⊢Σ P ∼Q and Γ ⊢Σ E ∼F, is deﬁned exactly as is the open extension
of strong equivalence.
Theorem 49.12. Weak equivalence is an equivalence relation and a congruence.
Proof. The proof proceeds along similar lines to that of Theorem 49.11.
49.4
Notes
The literature on process equivalence is extensive. Numerous variations have been considered for
an equally numerous array of formalisms. Milner recounts the history and development of the
concept of bisimilarity in his monograph on the π-calculus (Milner, 1999), crediting David Park
with its original conception (Park, 1981). The development in this chapter is inspired by Milner,
and by a proof of congruence of strong bisimilarity given by Bernardo Toninho for the process
calculus considered in Chapter 39.

478
49.4 Notes

Part XIX
Appendices


Appendix A
Background on Finite Sets
We make frequent use of the concepts of a ﬁnite set of discrete objects and of ﬁnite functions between
them. A set X is discrete iff equality of its elements is decidable: for every x, y ∈X, either x =
y ∈X or x ̸= y ∈X. This condition is to be understood constructively as stating that we may
effectively determine whether any two elements of the set X are equal or not. Perhaps the most
basic example of a discrete set is the set N of natural numbers. A set X is countable iff there is a
bijection f : X ∼= N between X and the set of natural numbers, and it is ﬁnite iff there is a bijection,
f : X ∼= { 0, . . . , n −1 }, where n ∈N, between it and some inital segment of the natural numbers.
This condition is again to be understood constructively in terms of computable mappings, so that
countable and ﬁnite sets are computably enumerable and, in the ﬁnite case, have a computable
size.
Given countable sets, U and V, a ﬁnite function is a computable partial function φ : U →V
between them. The domain dom(φ) of φ is the set { u ∈U | φ(u) ↓}, of objects u ∈U such
that φ(u) = v for some v ∈V. Two ﬁnite functions, φ and ψ, between U and V are disjoint iff
dom(φ) ∩dom(ψ) = ∅. The empty ﬁnite function, ∅, between U and V is the totally undeﬁned
partial function between them. If u ∈U and v ∈V, the ﬁnite function, u ,→v, between U and V
sends u to v, and is undeﬁned otherwise; its domain is therefore the singleton set { u }. In some
situations we write u ~ v for the ﬁnite function u ,→v.
If φ and ψ are two disjoint ﬁnite functions from U to V, then φ ⊗ψ is the ﬁnite function from U
to V deﬁned by the equation
(φ ⊗ψ)(u) =





φ(u)
if u ∈dom(φ)
ψ(v)
if v ∈dom(ψ)
undeﬁned
otherwise
If u1, . . . , un ∈U are pairwise distinct, and v1, . . . , vn ∈V, then we sometimes write u1 ,→v1, . . . , un ,→
vn, or u1 ~ v1, . . . , un ~ vn, for the ﬁnite function u1 ,→v1 ⊗. . . ⊗un ,→vn.

482

Bibliography
Mart´ın Abadi and Luca Cardelli. A Theory of Objects. Springer-Verlag, 1996. 181, 246, 252
Peter Aczel. An introduction to inductive deﬁnitions. In Jon Barwise, editor, Handbook of Mathe-
matical Logic, chapter C.7, pages 783–818. North-Holland, 1977. 20
John Allen. Anatomy of LISP. Computer Science Series. McGraw-Hill, 1978. 10, 295
S.F. Allen, M. Bickford, R.L. Constable, R. Eaton, C. Kreitz, L. Lorigo, and E. Moran. Innovations
in computational type theory using Nuprl. Journal of Applied Logic, 4(4):428–469, 2006. ISSN
1570-8683. doi: 10.1016/j.jal.2005.10.005. 85
Stuart Allen. A non-type-theoretic deﬁnition of Martin-L¨of’s types. In LICS, pages 215–221, 1987.
448
Zena M. Ariola and Matthias Felleisen. The call-by-need lambda calculus. J. Funct. Program., 7(3):
265–301, 1997. 337
Arvind, Rishiyur S. Nikhil, and Keshav Pingali. I-structures: Data structures for parallel comput-
ing. In Joseph H. Fasel and Robert M. Keller, editors, Graph Reduction, volume 279 of Lecture
Notes in Computer Science, pages 336–369. Springer, 1986. ISBN 3-540-18420-1. 361
Arnon Avron. Simple consequence relations. Information and Computation, 92:105–139, 1991. 30
Henk Barendregt. The Lambda Calculus, Its Syntax and Semantics, volume 103 of Studies in Logic and
the Foundations of Mathematics. North-Holland, 1984. 191
Henk Barendregt. Lambda calculi with types. In S. Abramsky, D. M. Gabbay, and T. S. E Maibaum,
editors, Handbook of Logic in Computer Science, volume 2, Computational Structures. Oxford Uni-
versity Press, 1992. 39
Yves Bertot, G´erard Huet, Jean-Jacques L´evy, and Gordon Plotkin, editors. From Semantics to Com-
puter Science: Essays in Honor of Gilles Kahn. Cambridge University Press, 2009. 487
Guy E. Blelloch. Vector Models for Data-Parallel Computing. MIT Press, 1990. ISBN 0-262-02313-X.
354

484
BIBLIOGRAPHY
Guy E. Blelloch and John Greiner. Parallelism in sequential functional languages. In FPCA, pages
226–237, 1995. 354
Guy E. Blelloch and John Greiner. A provable time and space efﬁcient implementation of NESL.
In ICFP, pages 213–225, 1996. 59, 354
Manuel Blum. On the size of machines. Information and Control, 11(3):257–265, September 1967.
Stephen D. Brookes. The essence of parallel algol. Inf. Comput., 179(1):118–149, 2002. 390
Samuel R. Buss, editor. Handbook of Proof Theory. Elsevier, Amsterdam, 1998. 484
Luca Cardelli. Structural subtyping and the notion of power type. In Proc. ACM Symposium on
Principles of Programming Languages, pages 70–79, 1988. 222
Luca Cardelli. Program fragments, linking, and modularization. In Proc. ACM Symposium on
Principles of Programming Languages, pages 266–277, 1997. 404
Giuseppe Castagna and Benjamin C. Pierce. Decidable bounded quantiﬁcation. In Proc. ACM
Symposium on Principles of Programming Languages, pages 151–162, 1994. 425
Alonzo Church. The Calculi of Lambda-Conversion. Princeton University Press, 1941. 10, 69, 191
R. L. Constable. Implementing Mathematics with the Nuprl Proof Development System. Prentice-Hall,
Englewood Cliffs, NJ, 1986. v, 10, 235, 448
Robert L. Constable. Types in logic, mathematics, and programming. In Buss (1998), chapter X. v
Robert L. Constable and Scott F. Smith. Partial objects in constructive type theory. In LICS, pages
183–193. IEEE Computer Society, 1987. 457
William R. Cook. On understanding data abstraction, revisited. In OOPSLA, pages 557–572, 2009.
181
Rowan Davies. Practical Reﬁnement-Type Checking. PhD thesis, Carnegie Mellon University School
of Computer Science, May 2005. Available as Technical Report CMU–CS–05–110. 235
Rowan Davies and Frank Pfenning. Intersection types and computational effects. In Martin Oder-
sky and Philip Wadler, editors, ICFP, pages 198–208. ACM, 2000. ISBN 1-58113-202-6. 235
Ewen Denney. Reﬁnement types for speciﬁcation. In David Gries and Willem P. de Roever, editors,
PROCOMET, volume 125 of IFIP Conference Proceedings, pages 148–166. Chapman & Hall, 1998.
ISBN 0-412-83760-9. 235
Derek Dreyer. Understanding and Evolving the ML Module System. PhD thesis, Carnegie Mellon
University, Pittsburgh, PA, May 2005. 425
Joshua Dunﬁeld and Frank Pfenning. Type assignment for intersections and unions in call-by-
value languages. In Andrew D. Gordon, editor, FoSSaCS, volume 2620 of Lecture Notes in Com-
puter Science, pages 250–266. Springer, 2003. ISBN 3-540-00897-7. 235

BIBLIOGRAPHY
485
Uffe Engberg and Mogens Nielsen. A calculus of communicating systems with label passing - ten
years after. In Gordon D. Plotkin, Colin Stirling, and Mads Tofte, editors, Proof, Language, and
Interaction, Essays in Honour of Robin Milner, pages 599–622. The MIT Press, 2000. 378
Matthias Felleisen and Robert Hieb. The revised report on the syntactic theories of sequential
control and state. TCS: Theoretical Computer Science, 103, 1992. 48, 277
Tim Freeman and Frank Pfenning. Reﬁnement types for ml. In David S. Wise, editor, PLDI, pages
268–277. ACM, 1991. ISBN 0-89791-428-7. 235
Daniel Friedman and David Wise. The impact of applicative programming on multiprocessing. In
International Conference on Parallel Processing, 1976. 361
David Gelernter. Generative communication in Linda. ACM Trans. Program. Lang. Syst., 7(1):80–
112, 1985. 390
Gerhard Gentzen. Investigations into logical deduction. In M. E. Szabo, editor, The Collected Papers
of Gerhard Gentzen, pages 68–213. North-Holland, Amsterdam, 1969. 39
J.-Y. Girard. Interpretation fonctionelle et elimination des coupures de l’arithmetique d’ordre superieur.
These d’etat, Universite Paris VII, 1972. 146, 470
Jean-Yves Girard. Proofs and Types. Cambridge University Press, 1989. Translated by Paul Taylor
and Yves Lafont. v, 404
Kurt G¨odel. On a hitherto unexploited extension of the ﬁnitary standpoint. Journal of Philosphical
Logic, 9:133–142, 1980. Translated by Wilfrid Hodges and Bruce Watson. 77
Michael J. Gordon, Arthur J. Milner, and Christopher P. Wadsworth. Edinburgh LCF, volume 78 of
Lecture Notes in Computer Science. Springer-Verlag, 1979. 10, 269
John Greiner and Guy E. Blelloch. A provably time-efﬁcient parallel implementation of full spec-
ulation. ACM Trans. Program. Lang. Syst., 21(2):240–285, 1999. 361
Timothy Grifﬁn. A formulae-as-types notion of control. In Proc. ACM Symposium on Principles of
Programming Languages, pages 47–58, 1990. 117
Carl Gunter. Semantics of Programming Languages. Foundations of Computing Series. MIT Press,
1992. 181
Robert H. Halstead, Jr. Multilisp: A language for concurrent symbolic computation. ACM Trans.
Program. Lang. Syst., 7(4):501–538, 1985. 361
Robert Harper. Constructing type systems over an operational semantics. J. Symb. Comput., 14(1):
71–84, 1992. 448
Robert Harper. A simpliﬁed account of polymorphic references. Inf. Process. Lett., 51(4):201–206,
1994. 328

486
BIBLIOGRAPHY
Robert Harper and Mark Lillibridge. A type-theoretic approach to higher-order modules with
sharing. In Proc. ACM Symposium on Principles of Programming Languages, pages 123–137, 1994.
413, 425, 435
Robert Harper, John C. Mitchell, and Eugenio Moggi. Higher-order modules and the phase dis-
tinction. In Proc. ACM Symposium on Principles of Programming Languages, pages 341–354, 1990.
425
Robert Harper, Furio Honsell, and Gordon Plotkin. A framework for deﬁning logics. Journal of the
Association for Computing Machinery, 40:194–204, 1993. 10, 30
Ralf Hinze and Johan Jeuring. Generic haskell: Practice and theory. In Roland Carl Backhouse and
Jeremy Gibbons, editors, Generic Programming, volume 2793 of Lecture Notes in Computer Science,
pages 1–56. Springer, 2003. ISBN 3-540-20194-7. 125
C. A. R. Hoare. Communicating sequential processes. Commun. ACM, 21(8):666–677, 1978. 378
Tony Hoare. Null references: The billion dollar mistake. Presentation at QCon 2009, August 2009.
93
S. C. Kleene. Introduction to Metamathematics. van Nostrand, 1952. 10
Imre Lakatos. Proofs and Refutations: The Logic of Mathematical Discovery. Cambridge University
Press, 1976. 117
P. J. Landin. A correspondence between Algol 60 and Church’s lambda notation. CACM, 8:89–101;
158–165, 1965. 48, 263
Daniel K. Lee, Karl Crary, and Robert Harper. Towards a mechanized metatheory of standard ml.
In Proc. ACM Symposium on Principles of Programming Languages, pages 173–184, 2007. 425
Xavier Leroy. Manifest types, modules, and separate compilation. In Proc. ACM Symposium on
Principles of Programming Languages, pages 109–122, 1994. 413, 425, 436
Xavier Leroy. Applicative functors and fully transparent higher-order modules. In Proc. ACM
Symposium on Principles of Programming Languages, pages 142–153, 1995. 436
Mark Lillibridge. Translucent Sums: A Foundation for Higher-Order Module Systems. PhD thesis,
Carnegie Mellon University School of Computer Science, Pittsburgh, PA, May 1997. 425
Barbara Liskov and Jeannette M. Wing. A behavioral notion of subtyping. ACM Trans. Program.
Lang. Syst., 16(6):1811–1841, 1994. 252
Saunders MacLane.
Categories for the Working Mathematician.
Graduate Texts in Mathematics.
Springer-Verlag, second edition, 1998. 125, 134
David B. MacQueen. Using dependent types to express modular structure. In Proc. ACM Sympo-
sium on Principles of Programming Languges, pages 277–286, 1986. 425

BIBLIOGRAPHY
487
David B. MacQueen. Kahn networks at the dawn of functional programming. In Bertot et al.
(2009), chapter 5. 181
Yitzhak Mandelbaum, David Walker, and Robert Harper. An effective theory of type reﬁnements.
In Runciman and Shivers (2003), pages 213–225. ISBN 1-58113-756-7. 235
Per Martin-L¨of. Constructive mathematics and computer programming. In Logic, Methodology and
Philosophy of Science IV, pages 153–175. North-Holland, 1980. 39, 52
Per Martin-L¨of. On the meanings of the logical constants and the justiﬁcations of the logical laws.
Unpublished Lecture Notes, 1983. 20, 30
Per Martin-L¨of. Intuitionistic Type Theory. Studies in Proof Theory. Bibliopolis, Naples, Italy, 1984.
v, 39, 404
Per Martin-L¨of. Truth of a proposition, evidence of a judgement, validity of a proof. Synthese, 73
(3):407–420, 1987. 20, 30
John McCarthy. LISP 1.5 Programmer’s Manual. MIT Press, 1965. 10, 200, 286
N. P. Mendler. Recursive types and type constraints in second-order lambda calculus. In LICS,
pages 30–36, 1987. 134
Robin Milner. A theory of type polymorphism in programming. JCSS, 17:348–375, 1978. 52
Robin Milner. Communicating and mobile systems - the Pi-calculus. Cambridge University Press,
1999. ISBN 978-0-521-65869-0. 286, 302, 378, 477
Robin Milner, Mads Tofte, Robert Harper, and David MacQueen. The Deﬁnition of Standard ML
(Revised). The MIT Press, 1997. 39, 59, 222, 302, 413, 425, 435
John C. Mitchell. Coercion and type inference. In Proc. ACM Symposium on Principles of Program-
ming Languages, pages 175–185, 1984. 222
John C. Mitchell. Representation independence and data abstraction. In Proc. ACM Symposium on
Principles of Programming Languages, pages 263–276, 1986. 155, 470
John C. Mitchell. Foundations for Programming Languages. MIT Press, 1996. v
John C. Mitchell and Gordon D. Plotkin. Abstract types have existential type. ACM Trans. Program.
Lang. Syst., 10(3):470–502, 1988. 155, 436
Eugenio Moggi. Computational lambda-calculus and monads. In LICS, pages 14–23. IEEE Com-
puter Society, 1989. ISBN 0-8186-1954-6. 316
Tom Murphy, VII, Karl Crary, Robert Harper, and Frank Pfenning. A symmetric modal lambda
calculus for distributed computing. In LICS, pages 286–295, 2004. 316, 396
Chetan R. Murthy. An evaluation semantics for classical proofs. In LICS, pages 96–107. IEEE
Computer Society, 1991. 117

488
BIBLIOGRAPHY
Aleksandar Nanevski. From dynamic binding to state via modal possibility. In PPDP, pages 207–
218. ACM, 2003. ISBN 1-58113-705-2. 295
R. P. Nederpelt, J. H. Geuvers, and R. C. de Vrijer, editors. Selected Papers on Automath, volume 133
of Studies in Logic and the Foundations of Mathematics. North-Holland, 1994. 10, 30
B. Nordstrom, K. Petersson, and J. M. Smith. Programming in Martin-L¨of’s Type Theory. Oxford
University Press, 1990. URL http://www.cs.chalmers.se/Cs/Research/Logic/book. 10
OCaml. Ocaml, 2012. URL http://caml.inria.fr/ocaml/. 436
David Michael Ritchie Park. Concurrency and automata on inﬁnite sequences. In Peter Deussen,
editor, Theoretical Computer Science, volume 104 of Lecture Notes in Computer Science, pages 167–
183. Springer, 1981. ISBN 3-540-10576-X. 477
Frank Pfenning and Rowan Davies. A judgmental reconstruction of modal logic. Mathematical
Structures in Computer Science, 11(4):511–540, 2001. 316
Benjamin C. Pierce. Types and Programming Languages. The MIT Press, 2002. v, 85, 222, 246, 252
Benjamin C. Pierce. Advanced Topics in Types and Programming Languages. The MIT Press, 2004. v
Andrew M. Pitts. Existential types: Logical relations and operational equivalence. In Kim Guld-
strand Larsen, Sven Skyum, and Glynn Winskel, editors, ICALP, volume 1443 of Lecture Notes in
Computer Science, pages 309–326. Springer, 1998. ISBN 3-540-64781-3. 470
Andrew M. Pitts. Operational semantics and program equivalence. In Gilles Barthe, Peter Dybjer,
Luis Pinto, and Jo˜ao Saraiva, editors, APPSEM, volume 2395 of Lecture Notes in Computer Science,
pages 378–412. Springer, 2000. ISBN 3-540-44044-5. 457
Andrew M. Pitts and Ian D. B. Stark. Observable properties of higher order functions that dynam-
ically create local names, or what’s new? In Andrzej M. Borzyszkowski and Stefan Sokolowski,
editors, MFCS, volume 711 of Lecture Notes in Computer Science, pages 122–141. Springer, 1993.
ISBN 3-540-57182-5. 10, 286
G. D. Plotkin. A structural approach to operational semantics. Technical Report DAIMI FN-19,
Aarhus University Computer Science Department, 1981. 48, 263
Gordon D. Plotkin. LCF considered as a programming language. Theor. Comput. Sci., 5(3):223–255,
1977. 173
Gordon D. Plotkin. The origins of structural operational semantics. J. of Logic and Algebraic Pro-
gramming, 60:3–15, 2004. 48
John H. Reppy. Concurrent Programming in ML. Cambridge University Press, 1999. 378, 390
J. C. Reynolds. Types, abstraction, and parametric polymorphism. In Information Processing ’83,
pages 513–523. North-Holland, Amsterdam, 1983. 146, 470

BIBLIOGRAPHY
489
John C. Reynolds. Towards a theory of type structure. In Bernard Robinet, editor, Symposium
on Programming, volume 19 of Lecture Notes in Computer Science, pages 408–423. Springer, 1974.
ISBN 3-540-06859-7. 146, 155
John C. Reynolds. Using category theory to design implicit conversions and generic operators.
In Neil D. Jones, editor, Semantics-Directed Compiler Generation, volume 94 of Lecture Notes in
Computer Science, pages 211–258. Springer, 1980. ISBN 3-540-10250-7. 222
John C. Reynolds. The essence of Algol. In Proceedings of the 1981 International Symposium on
Algorithmic Languages, pages 345–372. North-Holland, 1981. 315, 327
John C. Reynolds. The discoveries of continuations. Lisp and Symbolic Computation, 6(3-4):233–248,
1993. 277
John C. Reynolds. Theories of Programming Languages. Cambridge University Press, Cambridge,
England, 1998. v
Andreas Rossberg, Claudio V. Russo, and Derek Dreyer. F-ing modules. In Andrew Kennedy and
Nick Benton, editors, TLDI, pages 89–102. ACM, 2010. ISBN 978-1-60558-891-9. 425
Colin Runciman and Olin Shivers, editors. Proceedings of the Eighth ACM SIGPLAN International
Conference on Functional Programming, ICFP 2003, Uppsala, Sweden, August 25-29, 2003, 2003.
ACM. ISBN 1-58113-756-7. 487, 490
Dana Scott. Lambda calculus: Some models, some philosophy. In J. Barwise, H. J. Keisler, and
K. Kunen, editors, The Kleene Symposium, pages 223–265. North Holland, Amsterdam, 1980a.
191
Dana S. Scott. Data types as lattices. SIAM J. Comput., 5(3):522–587, 1976. 181
Dana S Scott. Relating theories of the lambda calculus. To HB Curry: Essays on combinatory logic,
lambda calculus and formalism, pages 403–450, 1980b. 209
Dana S. Scott.
Domains for denotational semantics.
In Mogens Nielsen and Erik Meineche
Schmidt, editors, ICALP, volume 140 of Lecture Notes in Computer Science, pages 577–613.
Springer, 1982. ISBN 3-540-11576-5. 181
Michael B. Smyth and Gordon D. Plotkin. The category-theoretic solution of recursive domain
equations. SIAM J. Comput., 11(4):761–783, 1982. 181
Richard Statman. Logical relations and the typed lambda-calculus. Information and Control, 65
(2/3):85–97, 1985. 447
Guy L. Steele. Common Lisp: The Language. Digital Press, 2nd edition edition, 1990. 269
Christopher A. Stone and Robert Harper. Extensional equivalence and singleton types. ACM
Trans. Comput. Log., 7(4):676–722, 2006. 413, 425

490
BIBLIOGRAPHY
Paul Taylor. Practical Foundations of Mathematics. Cambridge Studies in Advanced Mathematics.
Cambridge University Press, Cambridge, 1999. 134
P.W. Trinder, K. Hammond, H.-W. Loidl, and S.L. Peyton Jones. Algorithm + strategy = parallelism.
JOURNAL OF FUNCTIONAL PROGRAMMING, 8:23–60, 1998. 361
Jaap van Oosten. Realizability: A historical essay. Mathematical Structures in Computer Science, 12
(3):239–263, 2002. 235
Philip Wadler. Theorems for free! In FPCA, pages 347–359, 1989. 146
Philip Wadler. Comprehending monads. Mathematical Structures in Computer Science, 2(4):461–493,
1992. 316
Philip Wadler. Call-by-value is dual to call-by-name. In Runciman and Shivers (2003), pages
189–201. ISBN 1-58113-756-7. 117
Mitchell Wand. Fixed-point constructions in order-enriched categories. Theor. Comput. Sci., 8:13–
30, 1979. 181
Stephen A. Ward and Robert H. Halstead. Computation structures. MIT electrical engineering and
computer science series. MIT Press, 1990. ISBN 978-0-262-23139-8. 181
Kevin Watkins, Iliano Cervesato, Frank Pfenning, and David Walker. Specifying properties of
concurrent computations in clf. Electr. Notes Theor. Comput. Sci., 199:67–87, 2008. 161
Andrew K. Wright and Matthias Felleisen. A syntactic approach to type soundness. Inf. Comput.,
115(1):38–94, 1994. 52, 328
Hongwei Xi and Frank Pfenning. Eliminating array bound checking through dependent types.
In Jack W. Davidson, Keith D. Cooper, and A. Michael Berman, editors, PLDI, pages 249–257.
ACM, 1998. ISBN 0-89791-987-4. 235

Index
FPC, see recursive types
M, see inductive types, coinductive types
Λ, see untyped λ-calculus
F, see universal types
Fω, see higher kinds
MA, see Modernized Algol
PCF, see Plotkin’s PCF
PPCF, see parallelism
T, see G¨odel’s T
abstract binding tree, 3, 6
abstractor, 7
valence, 7
α-equivalence, 8
bound variable, 8
capture, 9
free variable, 8
graph representation, 11
operator, 7
arity, 7
parameter, 9
structural induction, 8
substitution, 9
weakening, 11
abstract binding trees
closed, 32
abstract syntax tree, 3–5
operator, 4
arity, 4
index, 9
parameter, 9
structural induction, 5
substitution, 6
variable, 4
weakening, 10
abstract types, see existential types, see also sig-
natures
abstracted modules, see signatures
abt, see abstract binding tree
assignables, see Modernized Algol
ast, see abstract syntax tree
back-patching, see references
benign effects, see references
bidirectional typing, 39, 40
boolean blindness, see type reﬁnements
boolean type, 90
call-by-need, see laziness
capabilities, 319
channel types, see Concurrent Algol
class types, 299
deﬁnability, 300
dynamics, 299
statics, 299
classes, see dynamic dispatch
classical logic, 107
classical laws, 118
contradiction, 108, 110
derivability of elimination forms, 112
double-negation translation, 116, 118
dynamics, 113
excluded middle, 115
judgments, 108
proof, 110
conjunction, 111
disjunction, 111
implication, 111
negation, 111
truth, 111

492
INDEX
variable, 111
provability, 108
conjunction, 109
disjunction, 109
hypothesis, 108
implication, 109
negation, 109
truth, 109
refutability, 108
conjunction, 109
disjunction, 109
falsehood, 109
hypothesis, 108
implication, 109
negation, 109
refutation, 110
conjunction, 111
disjunction, 111
falsehood, 111
implication, 111
negation, 111
variable, 111
safety, 114
classiﬁed type, 297
applications, 301
conﬁdentiality, 301
dynamics, 298
exception values, 302
integrity, 301
named exceptions, 302
safety, 299
statics, 297
coinductive types
dynamics, 132
inﬁnite trees, 135
statics, 131
streams, 128
combinators
sk basis, 30
bracket abstraction, 31, 32
conversion, 31
substitution, 31
command types, see Modernized Algol
compactness, see equality
Concurrent Algol, 381
broadcast communication, 384
dynamics, 385
safety, 385
statics, 385
class declaration, 390
deﬁnability of free assignables, 388
dynamics, 383
RS latch, 390
selective communication, 386
dynamics, 388
statics, 387
statics, 382
constructive logic, 97
Boolean algebra, 105
conservation of proof, 102
decidable proposition, 104
double-negation elimination, 104
Gentzen’s Principle, 102
Heyting algebra, 104
judgment forms, 98
law of the excluded middle, 104
proof, 101
conjunction, 101
disjunction, 102
falsehood, 102
implication, 101
truth, 101
proofs-as-programs, 103
propositions-as-types, 103
provability, 99
conjunction, 100
disjunction, 100
falsehood, 100
implication, 100
negation, 100
truth, 99
reversibility of proof, 102
semantics, 98
continuation types, 271
classical logic, 278
coroutines, 274
dynamics, 273
safety, 274

INDEX
493
statics, 273
streams, 278
syntax, 273
continuations types
safety, 277
contravariance, see subtyping
covariance, see subtyping
deﬁnitional equality, see equality
Distributed Algol, 391
dynamics, 394
safety, 396
statics, 391
dynamic binding, see ﬂuids
dynamic classiﬁcation, see classiﬁed type
dynamic dispatch, 239, 240
class-based, 241, 242
class vector, 242
instance, 242
message send, 242
object type, 242
self-reference, 245
dispatch matrix, 240
message not understood, 246
method-based, 241
message send, 244
method vector, 243
object type, 243
self-reference, 245
self-reference, 244, 247
instances, 247
results, 247
dynamic types, 194
as static types, 205
class dispatch, 198
cons, 197
critique, 199
destructors, 197
dynamics, 194
lists, 200
multiple arguments, 200
multiple results, 200
nil, 197
numeric classes, 196
pairs, 200
predicates, 197
safety, 196
statics, 194
subtyping, 215
dynamic typing
vs static typing, 208
dynamics, 35, 41
checked errors, 51
contextual, 44
cost, 58
deﬁnitional equality, 46
determinacy, 44
environmental evaluation, 60
equational, 46
equivalence theorem, 45
evaluation, 55
equivalence to transition, 56
evaluation context, 44
induction on transition, 42
inversion principle, 44
structural, 42
transition system, 41
unchecked errors, 51
dynamics types
arithmetic, 200
enumeration types, 91
equality, 439
coinduction, 442, 455
compactness, 450, 453, 454
congruence, 441
contexts, 440, 449
deﬁnitional, 46, 143, 169, 185, 406, 439
equational laws, 446
ﬁxed point induction, 450
Kleene equality, 449
Kleene equivalence, 441
logical equivalence, 439, 442, 444, 450
closed, 443
observation, 440
observational equivalence, 439, 441, 444, 449
event types, see Concurrent Algol
exceptions, 265, 267

494
INDEX
dynamics, 267
evaluation dynamics, 269
exception type, 268, 269, 302
safety, 268, 269
statics, 267
structural dynamics, 269
syntax, 267
type reﬁnements, 270
existential types
representation independence, 469
existential types, 149
coinductive types, 156
deﬁnability from universals, 152
dynamics, 150
modeling data abstraction, 151
parametricity, 156
representation independence, 153, 156
safety, 151
statics, 150
streams, 156
subtyping, 220
failures, see also exceptions, 265
dynamics, 266
failure-passing style, 270
safety, 266
statics, 265
ﬁnite function, 481
ﬁxed point induction, see equality
ﬂuid binding, see ﬂuids
ﬂuid reference types, 293
dynamics, 294
statics, 294
ﬂuids, 289
deep binding, 295
dynamics, 290
exception handling, 295
safety, 291
shallow binding, 295
statics, 289
subtleties, 292
function types
deﬁnitions, 63
dynamic binding, 68
ﬁrst order, 63
dynamics, 64
safety, 64
statics, 64
ﬁrst-class functions, 65
higher order, 65
dynamics, 65
safety, 66
statics, 65
second class functions, 63
static binding, 68
subtyping, 217
functors, see signatures
future types, 355, 356
future let, 361
parallel dynamics, 358
parallel let, 361
pipelining, 360
sequential dynamics, 356
sparks, 361
statics, 356
futures, see future types
G¨odel’s T, 71
canonical forms, 77
deﬁnability, 74
deﬁnitional equality, 74
dynamics, 72
hereditary termination, 77
iterator, 72
recursor, 71
safety, 73, 77
statics, 72
termination, 77
undeﬁnability, 75
general judgment, 23, 28
generic derivability, 28
proliferation, 28
structurality, 28
substitution, 28
parametric derivability, 29
general recursion, 167
generic inductive deﬁnition, 29
formal generic judgment, 29

INDEX
495
rule, 29
rule induction, 29
structurality, 29
Girard’s System F, see universal types, see uni-
versal types
higher kinds, 157
constructor equality, 159
constructor statics, 158
constructors, 158
expression statics, 160
expressions, 160
hybrid types, 203
as recursive types, 205
dynamics, 204
optimization of dynamic types, 206
safety, 204
statics, 203
hypothetical inductive deﬁnition, 26
formal derivability, 27
rule, 26
rule induction, 27
uniformity of rules, 27
hypothetical judgment, 23
admissibility, 25
reﬂexivity, 26
structurality, 26
transitivity, 26
weakening, 26
derivability, 23
reﬂexivity, 24
stability, 24
structurality, 24
transitivity, 24
weakening, 24
inductive deﬁnition, 13, 14
admissible rule, 25
backward chaining, 16
derivable rule, 23
derivation, 15
forward chaining, 16
function, 19
iterated, 18
rule, 14
axiom, 14
conclusion, 14
premise, 14
rule induction, 15, 16
rule scheme, 14
instance, 14
simultaneous, 18
inductive types
dynamics, 132
lists, 134
natural numbers, 127
statics, 131
inheritance, 249
class extension, 249
class-based, 250
method extension, 250
method specialization, 253
method-based, 252
self-reference, 253
simple method override, 253
sub-method, 249
subclass, 249
super-method, 249
superclass, 249
interface, see separate compilation
judgment, 13
judgment form, 13
predicate, 13
subject, 13
kinds
dependent, see singleton kinds, Σ kinds, Π
kinds
Kleene equality, see equality
laziness, 329
by-need general recursion, 332
data structures, 337
lists, 337
parallel or, 173
recursion, 337
safety, 332

496
INDEX
suspension interpretation, 337
suspension types, 335
linking, see separate compilation
logical equivalence, see equality
methods, see dynamic dispatch
mobile types, 314
mobility condition, 314
rules, 314
Modernized Algol
separated and consolidated stacks, 318
Modernized Algol, 307
arrays, 316
assignables, 307, 320
block structure, 310
classes and objects, 318
command types, 314
commands, 307, 313
control stack, 318
data stack, 318
expressions, 307
free assignables, 322
free dynamics, 322
idioms
conditionals, 312
iteration, 312
procedures, 312
sequential composition, 312
multiple declaration instances, 316
own assignables, 317
passive commands, 316
recursive procedures, 316
scoped dynamics, 309
scoped safety, 311
stack discipline, 310
stack machine, 318
statics, 308, 314
modules, see signatures
mutual primitive recursion, 84
null, see option types
objects, see dynamic dispatch
observational equivalence, see equality
option types, 92
parallelism, 341
binary fork-join, 341
Brent’s Theorem, 349
cost dynamics, 344, 354
cost dynamics vs. transition dynamics, 345
cost graphs, 344
exceptions, 354
implicit parallelism theorem, 343
multiple fork-join, 347
parallel complexity, 345
parallel dynamics, 342
parallelizability, 349
provably efﬁcient implementation, 348
sequence types, 347
cost dynamics, 348
statics, 347
sequential complexity, 345
sequential dynamics, 342
statics, 341
structural dynamics, 354
task dynamics, 349, 354
work vs. depth, 345
parametricity, see equality
phase distinction, 35, see also signatures
Π kinds, 406, 408, 409
elimination rules, 410
equivalence, 410
formation rules, 410
introduction rules, 410
subkinding, 410
Plotkin’s PCF, 165
Blum size theorem, 172
bounded recursion, 450
deﬁnability, 170
deﬁnitional equality, 169
dynamics, 168
eager natural numbers, 171
eagerness and laziness, 171
halting problem, 173
induction, 171
mutual recursion, 173
safety, 168

INDEX
497
statics, 167
totality and partiality, 171
polarity, 85
polymorphic types, see universal types
primitive recursion, 84
process calculus, 365, 471
actions, 365
asynchronous communication, 374
bisimilarity, 471
channel types, 375
dynamics, 376
statics, 376
channels, 370, 373
coinduction, see strong and weak bisimilar-
ity
concurrent composition, 367
dynamics, 367, 371, 373, 472
equivalence, see bisimilarity
events, 365
input choice, 379
Milner booleans, 378
polyadic π-calculus, 380
process choice, 379
replication, 369
RS latch, 379
sequential composition, 379
statics, 370, 373, 472
strong bisimilarity, 473, 474
strong bisimulation, 473
structural congruence, 366, 367
synchronization, 367
synchronous communication, 372
syntax, 471
universality, 377
weak bisimilarity, 476
coinduction, 477
weak bisimulation, 476
process equivalence, see process calculus
product types, 81
dynamics, 82
ﬁnite, 83
safety, 82
statics, 81
subtyping, 215, 217
recursive types, see also type recursion
dynamics, 176
eager data structures, 177
eager lists, 177
eager natural numbers, 177
lazy data structures, 177
lazy lists, 178
lazy natural numbers, 177
RS latch, 182
self-reference, 178
signals, 182
statics, 175
subtyping, 218, 223
reference types, 319
aliasing, 321
free dynamics, 323
safety, 321, 324
scoped dynamics, 321
statics, 321
references
arrays, 328
back-patching, 327
benign effects, 326
mutable data structures, 328
reﬁnement types, see type reﬁnements
reﬁnements, see type reﬁnements
representation independence, see existential types
representation independence, see also parametric-
ity
Reynolds’s Algol, see Modernized Algol
safety
evaluation, 57, 58
scoped assignables, see Modernized Algol
self types, 178
as recursive types, 179
deriving general recursion, 180
self-reference, 178
unrolling, 178
separate compilation, 401
initialization, 402
interface, 401
linking, 401
units, 401

498
INDEX
Σ kinds, 406, 408, 409
elimination rules, 409
equivalence, 409
formation rules, 409
introduction rules, 409
subkinding, 410
signatures, 415
abstracted modules, see functors
abstraction, 430
applicative functor, 434
ascription, see sealing
avoidance problem, 422
dictionary functor, 436
dynamic part, 417
dynamics, 424
ﬁrst- vs second-class, 424
functors, 430, 431
generative functor, 433
graph abstraction, 426
graph class, 426
graph functor, 436
hierarchies, 427, 428
instances, 418
opacity, 416
principal signature, 419
revelation, 416
sealing, 417
self-recognition, 423, 434
set abstraction, 425
set functor, 436
sharing propagation, 428
sharing speciﬁcation, 428
signature modiﬁcation, 426
static part, 417
statics, 421, 432
structures, 416
submodule, 429
subsignature, 418–420
syntax, 420, 432
translucency, 416
transparency, 416
type abstractions, 415, 416
type classes, 415, 418
views, 418
singleton kinds, 406
as type deﬁnitions, 408
constructor equivalence, 407
function kinds, 413
higher singletons, 406, 411
kind equivalence, 407
kind formation, 407
kind modiﬁcation, 413
product kinds, 413
sharing speciﬁcation, 414
subkinding, 407
sparks, see future types
speculation types, 357
parallel dynamics, 358
sequential dynamics, 357
statics, 357
speculations, see speculation types
stack machine, 257
correctness, 260
completeness, 261
soundness, 261
unraveling, 261
dynamics, 258
frame, 257
safety, 259
stack, 257
state, 257
state, 180
from recursion, 180
RS latch, 180
statics, 35
canonical forms, 38
decomposition, 38
induction on typing, 37
introduction and elimination, 38
structurality, 37
substitution, 37
type system, 36
unicity, 37
weakening, 37
structural subtyping, see subtyping
subkinding, 406
Π kinds, 410
Σ kinds, 410

INDEX
499
singleton kinds, 407
submodules, see signatures
subtyping, 213
behavioral vs structural, 225
bounded quantiﬁcation, 220
class types, 215
coercion, 223
coherence, 223
dynamic types, 215
dynamics, 221
function types, 217
numeric types, 214
product types, 215, 217
quantiﬁed types, 220
recursive types, 218, 223
safety, 221
subsumption, 213
sum types, 215, 217
variance, 216, 222
sum types, 87
dynamics, 88
ﬁnite, 89
statics, 87
subtyping, 215, 217
suspension types, see laziness
symbol reference, see symbol types
symbol types, 284
association lists, 286
dynamics, 285
equality, 286
safety, 285
statics, 284
symbolic expressions, 286
symbols, 281
mobility, 283
modal separation, 287
safety, 283
scope-free dynamics, 283
scoped dynamics, 282
statics, 282
total ordering, 286
syntax, 3
abstract, 3
binding, 3
chart, 35
concrete, 3
structural, 3
surface, 3
System F, see universal types
System Fω, see higher kinds
type abstractions, see also existential types
type classes, see signatures
type operators, 121
composition, 125
database programming, 125
generic extension, 122, 124
polynomial, 122
positive, 124
identity, 125
non-negative, 126
generic extension, 126
polynomial, 121
positive, 124
type recursion, see recursive types
type reﬁnements, 225
addition reﬁnement, 236
boolean blindness, 232
booleans vs. propositions, 232
conjunction covariance, 235
dynamic type, 229, 236
entailment, 227
error-freedom, 235
examples, 231
function types, 231
general satisfaction, 228
natural numbers, 231
optimization of dynamic typing, 236
pre-order, 227
preservation, 234
product types, 230
recursive types, 236
reﬁned canonical forms, 234
reﬁnements vs. types, 225
safety, 234
safety theorem, 235
satisfaction, 227, 228
sum types, 230, 236

500
INDEX
summand reﬁnements, 236
syntax, 226
types vs. reﬁnements, 235
underlying type, 226
Yoneda Lemma, 236
type safety, 49
canonical forms, 50
checked errors, 52
errors, 53
preservation, 49, 53
progress, 50, 53
uni-typed λ-calculus, 190
as untyped, 190
unit
dynamics, 82
statics, 81
unit type, 81
vs void type, 90
units, see separate compilation
universal types, 140
sk combinators, 146
abstraction theorem, 464
admissible relation, 462
Church numerals, 144
deﬁnability, 143
booleans, 146
inductive types, 147
lists, 146
natural numbers, 144
products, 143
sums, 143
deﬁnitional equality, 143
dynamics, 142
equivalence candidate, see admissible rela-
tion
function extensionality, 465
Kleene equality, 460
logical equivalence, 461
closed, 462
compositionality, 464
open, 463
observational equivalence, 460
parametricity, 145, 147, 459, see equality, 466,
469
parametricity theorem, 464
safety, 142
statics, 140
subtyping, 220
untyped λ-calculus, 185
Y combinator, 188
as uni-typed, 190
booleans, 191
bracket abstraction, 192
Church numerals, 187
deﬁnability, 186
deﬁnitional equality, 185
dynamics, 185
lists, 192
products, 191
Scott’s Theorem, 188
statics, 185
streams, 192
sums, 192
variance, see subtyping
void type, 87
vs unit type, 90
dynamics, 88
statics, 87


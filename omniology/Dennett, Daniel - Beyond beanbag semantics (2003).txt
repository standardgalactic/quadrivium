1. Introduction
In the 1960s, when I became a graduate student in linguis-
tics, generative grammar was the hot new topic. Everyone
from philosophers to psychologists to anthropologists to ed-
ucators to literary theorists was reading about transforma-
tional grammar. But by the late 1970s, the bloom was off
the rose, although most linguists didn’t realize it; and by the
1990s, linguistics was arguably far on the periphery of the
action in cognitive science. To some extent, of course, such
a decline in fortune was simply a matter of fashion and the
arrival of new methodologies such as connectionism and
brain imaging. However, there are deeper reasons for lin-
guistics’ loss of prestige, some historical and some scientific.
The basic questions I want to take up here, then, are:
What was right about generative grammar in the 1960s,
that it held out such promise?
What was wrong about it, such that it didn’t fulfill its
promise?
How can we fix it, so as to restore its value to the other
cognitive sciences?
The goal is to integrate linguistics with the other cognitive
sciences, not to eliminate the insights achieved by any of
them. To understand language and the brain, we need all
the tools we can get. But everyone will have to give a little
in order for the pieces to fit together properly.
The position developed in Foundations of Language
(Jackendoff 2002) is that the overall program of generative
grammar was correct, as was the way this program was in-
tended to fit in with psychology and biology. However, a ba-
sic technical mistake at the heart of the formal implemen-
tation, concerning the overall role of syntax in the grammar,
led to the theory being unable to make the proper connec-
tions both within linguistic theory and with neighboring
fields. Foundations of Language develops an alternative,
the parallel architecture, which offers far richer opportuni-
ties for integration of the field. To understand the motiva-
tion for the parallel architecture, it is necessary to go
through some history.
2. Three founding themes of generative grammar
The remarkable first chapter of Noam Chomsky’s Aspects
of the Theory of Syntax (1965, henceforth Aspects) set the
agenda for everything that has happened in generative lin-
guistics since. Three theoretical pillars support the enter-
prise: mentalism, combinatoriality, and acquisition.
2.1. Mentalism
Before Aspects, the predominant view among linguists – if
it was even discussed – was that language is something that
exists either as an abstraction, in texts, or in some sense “in
the community” (the latter being the influential view of de
Saussure [1915], for example). Chomsky urged the view
that the appropriate object of study is the linguistic system
BEHAVIORAL AND BRAIN SCIENCES (2003) 26, 651–707
Printed in the United States of America
© 2004 Cambridge University Press
0140-525X/04 $12.50
651
Précis of Foundations of Language:
Brain, Meaning, Grammar, Evolution
Ray Jackendoff
Program in Linguistics, Brandeis University, Waltham, MA 02454
jackendoff@brandeis.edu
Abstract: The goal of this study is to reintegrate the theory of generative grammar into the cognitive sciences. Generative grammar was
right to focus on the child’s acquisition of language as its central problem, leading to the hypothesis of an innate Universal Grammar.
However, generative grammar was mistaken in assuming that the syntactic component is the sole course of combinatoriality, and that
everything else is “interpretive.” The proper approach is a parallel architecture, in which phonology, syntax, and semantics are au-
tonomous generative systems linked by interface components. The parallel architecture leads to an integration within linguistics, and to
a far better integration with the rest of cognitive neuroscience. It fits naturally into the larger architecture of the mind/brain and per-
mits a properly mentalistic theory of semantics. It results in a view of linguistic performance in which the rules of grammar are directly
involved in processing. Finally, it leads to a natural account of the incremental evolution of the language capacity.
Keywords: evolution of language; generative grammar; parallel architecture; semantics, syntax
Ray Jackendoff is Professor of Linguistics at Bran-
deis University, where he has taught since 1971. His
books include, among others, Semantics and Cognition
(1983); Consciousness and the Computational Mind
(1987); Semantic Structures (1990); The Architecture of
the Language Faculty (1997); and, with Fred Lerdahl,
A Generative Theory of Tonal Music (1983). Jackendoff
has been President of theLinguistic Society of America
and of the Society for Philosophy and Psychology. He 
is a Fellow of the American Academy of Arts and Sci-
ences and of the American Association for the Ad-
vancement of Science. His CD, Romanian Music for
Clarinet and Piano, was issued by Albany Records in
2003.
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

in the mind/brain of the individual speaker. According to
this stance, a community has a common language by virtue
of all speakers in the community having essentially the same
linguistic system in their minds/brains.1
The term most often used for this linguistic system is
“knowledge,” perhaps an unfortunate choice. However,
within the theoretical discourse of the time, the alternative
was thinking of language as an ability, a “knowing how” in
the sense of Ryle (1949), which carried overtones of be-
haviorism and stimulus-response learning, a sense from
which Chomsky with good reason wished to distance him-
self. It must be stressed, though, that whatever term is used,
the linguistic system in a speaker’s mind/brain is deeply un-
conscious and largely unavailable to introspection, in the
same way that our processing of visual signals is deeply un-
conscious. Thus, language is a kind of mind/brain property
hard to associate with the term “knowledge,” which com-
monly implies accessibility to introspection. Foundations of
Language (hereafter Foundations) compromises with tra-
dition by systematically using the term f-knowledge (func-
tional knowledge) to describe whatever is in speakers’
heads that enables them to speak and understand their na-
tive language(s).
There still are linguists, especially those edging off to-
ward semiotics and hermeneutics, who reject the mentalist
stance and assert that the only sensible way to study lan-
guage is in terms of the communication between individu-
als (a random example is Dufva & Lähteenmäki 1996). But,
on the whole, the mentalistic outlook of generative gram-
mar has continued to be hugely influential throughout lin-
guistics and cognitive neuroscience.
More controversial has been an important distinction
made in Aspects between the study of competence – a
speaker’s f-knowledge of language – and performance, the
actual processes (viewed computationally or neurally) tak-
ing place in the mind/brain that put this f-knowledge to use
in speaking and understanding sentences. I think the orig-
inal impulse behind the distinction was methodological
convenience. A competence theory permits linguists to do
what they have always done, namely, study phenomena like
Polish case marking and Turkish vowel harmony, without
worrying too much about how the brain actually processes
them. Unfortunately, in response to criticism from many
different quarters (especially in response to the collapse of
the derivational theory of complexity as detailed in Fodor
et al. 1974), linguists tended to harden the distinction into
a firewall: competence theories were taken to be immune
to evidence from performance. So began a gulf between lin-
guistics and the rest of cognitive science that has persisted
until the present.
Foundations does not abandon the competence-perfor-
mance distinction, but does return it to its original status as
a methodological rather than ideological distinction. Al-
though the innovations in Foundations are largely in the
realm of competence theory, one of their important conse-
quences is that there is a far closer connection to theories
of processing, as well as the possibility of a two-way dialogue
between competence and performance theories. We return
to this issue in section 9.3.
2.2. Combinatoriality
The earliest published work in generative grammar, Chom-
sky’s Syntactic Structures (1957), began with the observa-
tion that a language contains an arbitrarily large number of
sentences. Therefore, in addition to the finite list of words,
a characterization of a language must contain a set of rules
(or grammar) that collectively describe or “generate” the
sentences of the language. Syntactic Structures showed
that the rules of natural language could not be character-
ized in terms of a finite-state Markov process, nor in terms
of a context-free phrase structure grammar. Chomsky pro-
posed that the appropriate form for the rules of a natural
language is a context-free phrase structure grammar sup-
plemented by transformational rules. Not all subsequent
traditions of generative grammar (e.g., Head-Driven
Phrase Structure Grammar, Pollard & Sag 1994; Lexical-
Functional Grammar, Bresnan 1982; 2001) have main-
tained the device of transformational rules; but they all con-
tain machinery designed to overcome the shortcomings of
context-free grammars pointed out in Chomsky 1957.2
Transferred into the mentalistic framework of Chomsky
1965, the consequence of combinatoriality is that speakers
of the language must have rules of language (or mental
grammars) in their heads as part of their f-knowledge.
Again there is a certain amount of controversy arising from
the term “rules.” Rules of grammar in the sense of genera-
tive grammar are not like any of the sorts of rules or laws in
ordinary life: rules of etiquette, rules of chess, traffic laws,
or laws of physics. They are unconscious principles that play
a role in the production and understanding of sentences.
Again, to ward off improper analogies, Foundations uses
the term f-rules for whatever the combinatorial principles
in the head may be. Generative linguistics leaves open how
directly the f-rules are involved in processing, but, as sug-
gested above, the unfortunate tendency among linguists
has been not to care. The theory in Foundations, though,
makes it possible to regard the rules as playing a direct role
in processing (again, see sect. 9.3).
An important reason for the spectacular reception of
early generative grammar was that it went beyond merely
claiming that language needs rules. It offered rigorous for-
mal techniques for characterizing the rules, based on ap-
proaches to the foundations of mathematics and com-
putability developed earlier in the century. The technology
suddenly made it possible to say lots of interesting things
about language and ask lots of interesting questions. For the
first time ever it was possible to provide detailed descrip-
tions of the syntax of natural languages (not only English
but German, French, Turkish, Mohawk, Hidatsa, and Jap-
anese were studied early on). In addition, generative
phonology took off rapidly, adapting elements of Prague
School phonology of the 1930s to the new techniques. With
Chomsky and Halle’s 1968 Sound Pattern of English as its
flagship, generative phonology quickly supplanted the
phonological theory of the American structuralist tradition.
2.3. Acquisition
Mentalism and combinatoriality together lead to the crucial
question: How do children get the f-rules into their heads?
Given that the f-rules are unconscious, parents and peers
cannot verbalize them; and even if they could, children
would not understand, because they don’t know language
yet. The best the environment can do for a language learner
is provide examples of the language in a context. From
there on it is up to the language learner to construct the
principles on his or her own – unconsciously, of course.
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
652
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

Chomsky (1965) asked the prescient question: What
does the child have to “[f-]know in advance” in order to ac-
complish this feat? He phrased the problem in terms of the
“poverty of the stimulus”: Many different generalizations
are consistent with the data presented to the child, but the
child somehow comes up with the “right” one, that is, the
one that puts him or her in tune with the generalizations of
the language community. I like to put the problem a bit
more starkly: The whole community of linguists, working
together for decades with all sorts of crosslinguistic and psy-
cholinguistic data unavailable to children, has still been un-
able to come up with a complete characterization of the
grammar of a single natural language. Yet every child does
it by the age of ten or so. Children don’t have to make the
choices we do. They don’t have to decide whether the
“right” choice of grammar is in the style of transformational
grammar, the Minimalist Program, Optimality Theory, Role
and Reference Grammar, Tree-Adjoining Grammar, Cog-
nitive Grammar, connectionist networks, or some as yet
unarticulated alternative. They already f-know it in ad-
vance.
One of the goals of linguistic theory, then, is to solve this
“Paradox of Language Acquisition” by discovering what as-
pects of linguistic f-knowledge are not learned, but rather
form the basis for the child’s learning. The standard term
for the unlearned component is Universal Grammar or UG,
a term that again perhaps carries too much unwanted bag-
gage. In particular, UG should not be confused with uni-
versals of language; it is rather what shapes the acquisition
of language. I prefer to think of it as a toolkit for construct-
ing language, out of which the child (or better, the child’s
brain) f-selects tools appropriate to the job at hand. If the
language in the environment happens to have a case system
(like German), UG will help shape the child’s acquisition of
case; if it has a tone system (like Mandarin), UG will help
shape the child’s acquisition of tone. But if the language in
the environment happens to be English, which lacks case
and tone, these parts of UG will simply be silent.
What then is the source of language universals? Some of
them will indeed be determined by UG, for example, the
overall “architecture” of the grammatical system: the parts
of the mental grammar and their relations (of which much
more below). Other universals, especially what are often
called “statistical” or “implicational” universals, may be the
result of biases imposed by UG. For example, UG may say
that if a language has a case system, the simplest such sys-
tems are thus-and-so; these will be widespread systems
crosslinguistically; they will be acquired earlier by children
than other systems; and they will create pressures for his-
torical change. Other universals may be a consequence of
the functional properties of any relatively efficient commu-
nication system: for instance, the most frequently used sig-
nals tend to be short. UG doesn’t have to say anything about
these universals at all; they will come about through the dy-
namics of language use in the community (a process which
of course is not very well understood).
If UG is not learned, how does the child acquire it? The
only alternative is through the structure of the brain, which
is determined through a combination of genetic inheritance
and the biological processes resulting from expression of
the genes, the latter in turn determined by some combina-
tion of inherent structure and environmental input. Here
contemporary science is pretty much at an impasse. We
know little about how genes determine brain structure and
nothing about how the details of brain structure determine
anything about language structure, even aspects of lan-
guage as simple as speech sounds. Filling out this part of the
picture is a long-term challenge for cognitive neuroscience.
It is premature to reject the hypothesis of Universal Gram-
mar, as some have (e.g., Deacon 1997; Elman et al. 1996),
arguing that we don’t know how genes could code for lan-
guage acquisition. After all, we don’t know how genes code
for birdsong or sexual behavior or sneezing either, but we
don’t deny that there is a genetic basis behind these.
There next arises the question of how much of UG is a
human cognitive specialization for language and how much
is a consequence of more general capacities. The question
has often been oversimplified to a binary decision between
language being entirely special or entirely general, with a
strong bias toward the former inside generative linguistics
and toward the latter outside the field. The truth of the mat-
ter undoubtedly lies somewhere in between. To be sure,
many people (including myself) would find it satisfying if a
substantial part of language acquisition were a consequence
of general human cognitive factors; but the possibility of
some specialization overlaying the general factors must not
be discounted. My view is that we cannot determine what
is general and what is special until we have comparable the-
ories of other cognitive capacities, including other learned
cognitive capacities. To claim that language is parasitic on,
say, motor control, perhaps because both have hierarchical
and temporal structures (this seems to be the essence of
Corballis’s [1991] position) – but without stating a theory of
the f-knowledge involved in motor control – is to coarsen
the fabric of linguistic theory to the point of unrecogniz-
ability. The closest approach to a comparable theory is the
music theory of Lerdahl and Jackendoff (1983), which dis-
plays some striking parallels and some striking differences
with language (Pinker and Jackendoff, to appear).
Of course, if UG – the ability to learn language – is in
part a human cognitive specialization, it must be deter-
mined by some specifically human genes, which in turn had
to have come into existence sometime since the hominid
line separated from the other great apes. One would there-
fore like to be able to tell some reasonable story about how
UG could be shaped by natural selection or other evolu-
tionary processes. We return to this issue in section 9.4.
This approach to the acquisition of language has given
rise to a flourishing tradition of developmental research
(references far too numerous to mention) and a small but
persistent tradition in learnability theory (e.g., Baker & Mc-
Carthy 1981; Wexler & Culicover 1980). And certainly,
even if the jury is still out on the degree to which language
acquisition is a cognitive specialization, there have been all
manner of phenomena investigated that bear on the issue,
for example:
(1) The sensitive period for language learning and the
consequences for first and second language acquisition at a
later age (Curtiss 1977; Flynn & O’Neill 1988; Klein & Per-
due 1997; Lenneberg 1967; Newport 1990).
(2) The limited ability of apes to acquire even rudimen-
tary versions of human language, even with extensive train-
ing (Premack 1976; Savage-Rumbaugh et al. 1998; Seiden-
berg & Petitto 1978; Terrace 1979).
(3) The characteristic brain localization of language
functions, resulting in characteristic aphasias (Zurif 1990).
(4) The grammatical parallels between spoken and
signed languages and the parallels in acquisition and apha-
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
653
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

sia (Bellugi et al. 1989; Fischer & Siple 1990; Klima & Bel-
lugi 1979).
(5) The existence of characteristic language deficits as-
sociated with various genetic conditions (Bellugi et al. 1994;
Clahsen & Almazan 1998; Gopnik 1999).
(6) The creation of creole languages by communities of
pidgin-speaking children (Bickerton 1981; DeGraff 1999).
(7) Most strikingly, the creation of a signed language de
novo by a newly assembled community of deaf children in
Nicaragua (Kegl et al. 1999).
My impression is that, although there are questions about
all of these cases, en masse they offer an overwhelming case
for some degree of genetic specialization for language
learning in humans.
These three foundational issues of generative grammar –
mentalism, combinatoriality, and acquisition – have stood
the test of time; if anything, they have become even more
important through the years in the rest of cognitive science.
It is these three issues that connect linguistics intimately
with psychology, brain science, and genetics. Much of the
promise of generative linguistics arose from this new and
exciting potential for scientific unification.
3. The broken promise: Deep Structure would be
the key to the mind
A fourth major point of Aspects, and the one that seeped
most deeply into the awareness of the wider public, con-
cerned the notion of Deep Structure. A basic claim of the
Chomsky 1965 version of generative grammar was that in
addition to the surface form of sentences, that is, the form
we hear, there is another level of syntactic structure, called
Deep Structure, which expresses underlying syntactic reg-
ularities of sentences. For example, a passive sentence like
(1a) has a Deep Structure in which the noun phrases are in
the order of the corresponding active (1b).
(1a) The bear was chased by the lion.
(1b) The lion chased the bear.
Similarly, a question such as (2a) has a Deep Structure
closely resembling that of the corresponding declarative
(2b).
(2a) Which martini did Harry drink?
(2b) Harry drank that martini.
In the years preceding Aspects, the question arose of how
syntactic structure is connected to meaning. Following a
hypothesis first proposed by Katz and Postal (1964), As-
pects made the striking claim that the relevant level of syn-
tax for determining meaning is Deep Structure.
In its weakest version, this claim was only that regulari-
ties of meaning are most directly encoded in Deep Struc-
ture, and this can be seen in examples (1) and (2). However,
the claim was sometimes taken to imply much more – that
Deep Structure is meaning, an interpretation that Chom-
sky did not at first discourage.3 This was the part of gen-
erative linguistics that got everyone really excited. For if 
the techniques of transformational grammar lead us to
meaning, we can uncover the nature of human thought.
Moreover, if Deep Structure is innate – being dictated by
Universal Grammar – then linguistic theory gives us un-
paralleled access to the essence of human nature. No won-
der everyone wanted to learn linguistics.
What happened next was that a group of generative lin-
guists, notably George Lakoff, John Robert Ross, James
McCawley, and Paul Postal, pushed very hard on the idea
that Deep Structure should directly encode meaning. The
outcome, the theory of Generative Semantics (e.g., Lakoff
1971; McCawley 1968; Postal 1970), increased the “ab-
stractness” and complexity of Deep Structure, to the point
that the example Floyd broke the glass was famously posited
to have eight underlying clauses, each corresponding to
some feature of the semantics. All the people who admired
Aspects for what it said about meaning loved Generative Se-
mantics, and it swept the country. But Chomsky himself re-
acted negatively, and with the aid of his then-current stu-
dents (full disclosure: present author included), argued
vigorously against Generative Semantics. When the dust of
the ensuing “Linguistics Wars” cleared around 1973 (cf.
Harris 1993; Huck & Goldsmith 1995; Newmeyer 1980),
Chomsky had won – but with a twist: he no longer claimed
that Deep Structure was the sole level that determines
meaning (Chomsky 1972). Then, having won the battle, he
turned his attention not to meaning, but to relatively tech-
nical constraints on movement transformations (e.g.,
Chomsky 1973; 1977).
The reaction in the larger community was shock: for one
thing, at the fact that the linguists had behaved so badly; but
more substantively, at the sense that there had been a “bait
and switch.” Chomsky had promised Meaning with a capi-
tal M and then had withdrawn the offer. Many researchers,
both inside and outside linguistics, turned away from gen-
erative grammar with disgust, rejecting not only Deep
Structure but also mentalism, innateness, and sometimes
even combinatoriality. And when, later in the 1970s, Chom-
sky started talking about meaning again, in terms of a syn-
tactic level of Logical Form (e.g., Chomsky 1981), it was too
late – the damage had been done. From this point on, the
increasingly abstract technical apparatus of generative
grammar was of no interest to more than a tiny minority of
cognitive scientists, much less to the general public.
Meanwhile, various non-Chomskyan traditions of gener-
ative grammar developed, most notably Relational Gram-
mar (Perlmutter 1983), Head-Driven Phrase Structure
Grammar (Pollard & Sag 1987; 1994), Lexical-Functional
Grammar (Bresnan 1982; 2001), Formal Semantics (Chier-
chia & McConnell-Ginet 1990, Heim & Kratzer 1998; Par-
tee 1976), Optimality Theory (Prince & Smolensky 1993),
Construction Grammar (Fillmore et al. 1988; Goldberg
1995), and Cognitive Grammar (Lakoff 1987; Langacker
1987; Talmy 2000). On the whole, these approaches to lin-
guistics (with the possible exception of Cognitive Gram-
mar) have made even less contact with philosophy, psy-
chology, and neuroscience than the recent Chomskyan
tradition. My impression is that many linguists have simply
returned to the traditional concerns of the field: describing
languages, with as little theoretical and cognitive baggage
as possible. Although this is perfectly fine – issues of in-
nateness don’t play too big a role when you’re trying to
record an endangered language before its speakers all die
– the sense of excitement that comes from participating in
the integration of fields has become attenuated.
4. The scientific mistake: Syntactocentrism
So much for pure intellectual history. We now turn to what
I think was an important mistake at the core of generative
grammar, one that in retrospect lies behind much of the
alienation of linguistic theory from the cognitive sciences.
Chomsky did demonstrate that language requires a gener-
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
654
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

ative system that makes possible an infinite variety of sen-
tences. However, he explicitly assumed, without argument
(Chomsky 1965), that generativity is localized in the syn-
tactic component of the grammar – the construction of
phrases from words – and that phonology (the organization
of speech sounds) and semantics (the organization of mean-
ing) are purely “interpretive,” that is, that their combinato-
rial properties are derived strictly from the combinatorial-
ity of syntax.
In 1965 this was a perfectly reasonable view. The impor-
tant issue at that time was to show that something in lan-
guage is generative. Generative syntax had provided pow-
erful new tools, which were yielding copious and striking
results. At the time, it looked as though phonology could be
treated as a sort of low-level derivative of syntax: The syn-
tax gets the words in the right order, then phonology mas-
sages their pronunciation to adjust them to their local envi-
ronment. As for semantics, virtually nothing was known:
The only things on the table were the rudimentary propos-
als of Katz and Fodor (1963) and some promising work by
people such as Bierwisch (1967; 1969) and Weinreich
(1966). So the state of the theory offered no reason to ques-
tion the assumption that all combinatorial complexity arises
from syntax.
Subsequent shifts in mainstream generative linguistics
stressed major differences in outlook. But one thing that re-
mained unchanged was the assumption that syntax is the
sole source of combinatoriality. Figure 1 diagrams the ar-
chitecture of components in three major stages of Chom-
skyan syntactic theory: the Aspects theory, Principles and
Parameters (or Government-Binding) Theory (Chomsky
1981), and the Minimalist Program (Chomsky 1995). The
arrows denote direction of derivation.
These theoretical shifts alter the components of syntax
and their relation to sound and meaning. What remains
constant throughout, though, is that (a) there is an initial
stage of derivation in which words or morphemes are com-
bined into syntactic structures; (b) these structures are then
massaged by various syntactic operations; and (c) certain
syntactic structures are shipped off to phonology/phonetics
to be pronounced and other syntactic structures are
shipped off to “semantic interpretation” to be understood.
In short, syntax is the source of all linguistic organization.
I believe that this assumption of “syntactocentrism” –
which, I repeat, was never explicitly grounded – was an im-
portant mistake at the heart of the field.4 The correct ap-
proach is to regard linguistic structure as the product of a
number of parallel but interacting generative capacities –
at the very least, one each for phonology, syntax, and se-
mantics. As we will see, elements of such a “parallel archi-
tecture” have been implicit in practice in the field for years.
What is novel in the present work is bringing these prac-
tices out into the open, stating them as a foundational prin-
ciple of linguistic organization, and exploring the large-
scale consequences.
5. Phonology as an exemplar of the parallel
architecture
An unnoticed crack in the assumption of syntactocentrism
appeared in the middle to late 1970s, when the theory of
phonology underwent a major sea change. Before then, the
sound system of language had been regarded essentially as
a sequence of speech sounds. Any further structure, such
as the division into words, was thought of as simply inher-
ited from syntax. However, beginning with work such as
Goldsmith (1979) and Liberman and Prince (1977),
phonology rapidly came to be thought of as having its own
autonomous structure, in fact multiple structures or tiers.
Figure 2 provides a sample, the structure of the phrase “the
big apple.” The phonological segments appear at the bot-
tom, as terminal elements of the syllabic tree.
There are several innovations here. First, syllabic struc-
ture is seen as hierarchically organized. At the center of the
syllable (notated as s) is a syllabic nucleus (notated N),
which is usually a vowel but sometimes a syllabic consonant
such as the l in “apple.” The material following the nucleus
is the syllabic coda (notated C); this groups with the nucleus
to form the rhyme (notated R), the part involved in
rhyming. In turn, the rhyme groups with the syllabic onset
(notated O) to form the entire syllable. Syllables are
grouped together into larger units such as feet and phono-
logical words (the bracketing subscripted Wd). Notice that
in Figure 2, the word “the” does not constitute a phonolog-
ical word on its own; it is attached (or cliticized) to the word
“big.” Finally, phonological words group into larger units
such as phonological phrases. Languages differ in their
repertoire of admissible nuclei, onsets, and codas, but the
basic hierarchical organization and the principles by which
strings of segments are divided into syllables are universal.
(It should also be mentioned that signed languages have
parallel syllabic organization, except that the syllables are
built out of manual rather than vocal constituents, Fischer
& Siple 1990; Klima &Bellugi 1979.)
These hierarchical structures are not built out of syntac-
tic primitives such as nouns, verbs, and determiners; their
units are intrinsically phonological. In addition, the struc-
tures, though hierarchical, are not recursive.5 Therefore,
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
655
Figure 1.
Architecture of Chomsky’s theories through the years.
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

the principles governing these structures are not derivable
from syntactic structures; they are an autonomous system
of generative rules.
Next, consider the metrical grid in Figure 2. Its units are
beats, notated as columns of x’s. A column with only one x
is a weak beat, and more x’s in a column indicate a relatively
stronger beat. Each beat is associated with a syllable; the
strength of a beat indicates the relative stress on that sylla-
ble, so that in Figure 2 the first syllable of apple receives
maximum stress. The basic principles of metrical grids are
in part autonomous of language: They also appear, for ex-
ample, in music (Lerdahl & Jackendoff 1983), where they
are associated with notes instead of syllables. Metrical grids
place a high priority on rhythmicity: An optimum grid pre-
sents an alternation of strong and weak beats, as is found in
music and in much poetry. On the other hand, the structure
of syllables exerts an influence on the associated metrical
grid: Syllables with heavy rhymes (i.e., containing a coda or
a long vowel) “want” to be associated with relatively heavy
stress. The stress rules of a language concern the way syl-
labic structure comes to be associated with a metrical grid;
languages differ in ways that are now quite well understood
(e.g., Halle & Idsardi 1995; Kager 1995).
Again, metrical grids are built of nonsyntactic units. As
they are to some degree independent of syllabic structure,
they turn out to be a further autonomous “tier” of phono-
logical structure.
At a larger scale of phonological organization we find
prosodic units over which intonation contours are defined.
These are comparable in size to syntactic phrases but do not
coincide with them. Here are two examples:
(3) Syntactic bracketing:
[Sesame Street] [is [a production [of [the Children’s
Television Workshop]]]]
Prosodic bracketing (two pronunciations):
a. [Sesame Street is a production of] [the Children’s
Television Workshop]
b. [Sesame Street] [is a production] [of the Chil-
dren’s Television Workshop]
(4) Syntactic bracketing
[This] [is [the cat [that chased [the rat [that ate [the
cheese]]]]]]
Prosodic bracketing:
[This is the cat] [that chased the rat] [that ate the
cheese]
The two pronunciations of (3) are both acceptable, and
other prosodic bracketings are also possible. However, the
choice of prosodic bracketing is not entirely free, given that,
for example, [Sesame] [Street is a production of the] [Chil-
dren’s Television Workshop] is an impossible phrasing. Now
notice that the first constituent of (3a) and the second con-
stituent of (3b) do not correspond to any syntactic con-
stituent. We would be hard pressed to know what syntactic
label to give to Sesame Street is a production of. But as an
intonational constituent it is perfectly fine. Similarly in (4)
the syntax is relentlessly right-embedded, but the prosody
is flat and perfectly balanced into three parts. Again, the
first two constituents of the prosody do not correspond to
syntactic constituents of the sentence.
The proper way to deal with this lack of correspondence
is to posit a phonological category of Intonational Phrase,
which plays a role in the assignment of intonation contours
and the distribution of stress (Beckman & Pierrehumbert
1986; Ladd 1996). Intonational Phrases are to some degree
correlated with syntax; their boundaries tend to be at the
beginning of major syntactic constituents; but their ends do
not necessarily correlate with the ends of the correspond-
ing syntactic constituents. At the same time, Intonational
Phrases have their own autonomous constraints, in partic-
ular a strong preference for rhythmicity and parallelism (as
evinced in [4], for example), and a preference for saving the
longest prosodic constituent for the end of the sentence.6
Another example of mismatch between syntax and
phonology comes from contractions such as I’m and Lisa’s
(as in Lisa’s a doctor). These are clearly phonological words,
but what is their syntactic category? It is implausible to see
them either as noun phrases that incidentally contain a verb
or to see them as verbs that incidentally contain a noun.
Keeping phonological and syntactic structure separate al-
lows us to say the natural thing: they are phonological words
that correspond to two separate syntactic constituents.
(5) Syntactic structure:
[NP I] [V (a)m] [NP Lisa] [V (i)s]
Phonological structure: [Wd I’m]
[Wd Lisa’s]
Given that every different sentence of the language has
a different phonological structure, and phonological struc-
tures cannot be derived from syntax, the usual arguments for
combinatoriality lead us to the conclusion that phonological
structure is generative. However, in addition to the genera-
tive principles that describe these structures, it is necessary
to introduce a new kind of principle into the grammar, what
might be called “correspondence rules” or “interface rules.”
These rules (I revert to the standard term “rules” rather than
being obsessive about “f-rules”) regulate the way the inde-
pendent structures correspond with each other. For in-
stance, the relation between syllable weight and metrical
weight is regulated by an interface rule between syllabic and
metrical structure; the relation between syntactic and into-
national constituents is regulated by an interface rule be-
tween syntactic and prosodic structure.
An important property of interface rules is that they don’t
“see” every aspect of the structures they are connecting. For
example, the rules that connect syllabic content to metrical
grids are totally insensitive to syllable onset. Universally,
stress rules care only about what happens in the rhyme. Sim-
ilarly, although the connection between syntax and phonol-
ogy “sees” certain syntactic boundaries, it is insensitive to the
depth of syntactic embedding, Moreover, syntactic structure
is totally insensitive to the segmental content of the words it
is arranging (e.g., there is no syntactic rule that applies only
to words that begin with b). Thus, interface rules implement
not isomorphisms between the structures they relate, but
rather only partial homomorphisms.
This is not to say that we should think of speakers as
thinking up phonological and syntactic structures indepen-
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
656
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
Figure 2.
Phonological structure of the big apple.
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

dently, in the hope they can be matched up by the inter-
faces. That would be the same sort of mistake as thinking
that speakers start with the symbol S and generate a syn-
tactic tree, finally putting in words so they know what the
sentence is about. At the moment we are not thinking in
terms of production; rather we are stating the principles (of
“competence”) in terms of which sentences are well-
formed. We will get back to how this is related to process-
ing in section 9.3.
Now the main point of this section: This view of phono-
logical structure, developed in the late 1970s and almost
immediately adopted as standard, is deeply subversive of
the syntactocentric assumption that all linguistic combina-
toriality originates in syntax. According to this view, phono-
logical structure is not just a passive hand-me-down derived
from low-level syntax – it has its own role in shaping the to-
tality of linguistic structure. But no great commotion was
made about this most radical aspect of the new phonology.
Phonologists for the most part were happy to get on with
exploring this exciting way of doing things, and for them,
the consequences for syntax didn’t matter. Syntacticians, for
their part, simply found phonology irrelevant to their con-
cerns of constraining movement rules and the like, espe-
cially since phonology had now developed its own arcane
technical machinery. So neither subdiscipline really took
notice; and as the technologies diverged, the relation be-
tween syntax and phonology became a no-man’s-land (or
perhaps only a very-few-men’s-land). Tellingly, as far as I
can determine, in all of Chomsky’s frequent writings on the
character of the human language capacity, there is virtually
no reference to post-1975 phonology – much less to the
challenge that it presents to his overall syntactocentric view
of language.
6. The syntax-semantics interface
I have treated the developments in phonology first because
they are less controversial. But in fact the same thing hap-
pened in semantics. During the course of the 1970s and
1980s, several radically different approaches to semantics
developed. Within linguistics, these developments at least
included Formal Semantics (growing out of formal logic,
Chierchia & McConnell-Ginet 1990, Heim & Kratzer
1998; Partee 1976), Cognitive Grammar (Lakoff 1987; Lan-
gacker 1987; Talmy 2000), and Conceptual Semantics
(Jackendoff 1983; 1990; Pinker 1989; Pustejovsky 1995),
plus approaches within computational linguistics and cog-
nitive psychology. Whatever their differences, all these ap-
proaches take meaning to be deeply combinatorial. None
of them take the units of semantic structure to be syntactic
units such as NPs and VPs; rather, the units are intrinsically
semantic entities like objects, events, actions, properties,
and quantifiers.7 Therefore, whichever semantic theory we
choose, it is necessary to grant semantics an independent
generative organization, and it is necessary to include in the
theory of grammar an interface component that correlates
semantic structures with syntactic and phonological struc-
tures. In other words, the relation of syntax to semantics is
qualitatively parallel to the relation of syntax to phonology.
However, apparently no one pointed out the challenge to
syntactocentrism – except the Cognitive Grammarians,
who mostly went to the other extreme and denied syntax
any independent role, and who have been steadfastly ig-
nored by mainstream generative linguistics.
The organization of phonological structure into semi-in-
dependent tiers finds a parallel in semantics as well. Lin-
guistic meaning can be readily partialled into two indepen-
dent aspects. On one hand, there is what might be called
“propositional structure”: who did what to whom and so on.
For example, in The bear chased the lion, there is an event
of chasing in which the bear is the chaser and the lion is
“chasee.” On the other hand, there is also what is now called
“information structure”: the partitioning of the message
into old versus new information, topic versus comment,
presupposition versus focus, and so forth. We can leave the
propositional structure of a sentence intact but change its
information structure, by using stress (6a–c) or various fo-
cusing constructions (6d–f):
(6) a. The BEAR chased the lion.
b. The bear chased the LION.
c. The bear CHASED the lion.
d. It was the bear that chased the lion.
e. What the bear did was chase the lion.
f. What happened to the lion was the bear chased it.
Thus the propositional structure and the information struc-
ture are orthogonal dimensions of meaning, and can prof-
itably be regarded as autonomous tiers. (Foundations
proposes a further split of propositional structure into de-
scriptive and referential tiers, an issue too complex for the
present context.)
Like the interface between syntax and phonology, that
between syntax and semantics is not an isomorphism. Some
aspects of syntax make no difference in semantics. For in-
stance, the semantic structure of a language is the same
whether or not the syntax marks subject-verb agreement,
verb-object agreement, or nominative and accusative case.
The semantic structure of a language does not care whether
the syntax calls for the verb to be after the subject (as in En-
glish), at the end of the clause (as in Japanese), or second in
a main clause and final in a subordinate clause (as in Ger-
man). As these aspects of syntax are not correlated with or
derivable from semantics, the interface component disre-
gards them.
Similarly, some aspects of semantics have little if any sys-
tematic effect in syntax. Here are a few well-known exam-
ples:
First, the syntactic form of a question can be used to elicit
information (7a), test someone’s knowledge (7b), request
an action (7c), or express sarcasm (7d). Therefore, these
choices of illocutionary force are not mapped into syntactic
structure.
(7) a. Where is my hat?
b. (Now, Billy:) What’s the capital of New York?
c. Would you open the window?
d. Is the Pope Catholic?
Second, in example (8a), the interpretation is that Jill
jumped multiple times. This aspect of interpretation does
not arise from any single word in the sentence, nor from the
syntactic structure. If we change the verb to sleep, as in
(8b), we don’t interpret the sentence as implying multiple
acts of sleeping. If we change until to when, as in (8c), only
a single jump is entailed.
(8) a. Jill jumped until the alarm went off.
b. Jill slept until the alarm went off.
c. Jill jumped when the alarm went off.
The standard account of this contrast (Jackendoff 1997;
Pustejovsky 1995; Talmy 2000; Verkuyl 1993) is that the
meaning of until sets a temporal bound on an ongoing
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
657
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

process. When the verb phrase already denotes an ongoing
process, such as sleeping, all is well. But when the verb
phrase denotes an action that has a natural temporal end-
ing, such as jumping, then its interpretation is “coerced”
into repeated action – a sort of ongoing process – which in
turn can have a temporal bound set on it by until. For pre-
sent purposes, the point is that the sense of repetition arises
from semantic combination, without any direct syntactic re-
flex. (On the other hand, there are languages such as Amer-
ican Sign Language that have a grammatical marker of it-
eration; this will have to be used in the translation of [8a].)
A third case of semantic combinatoriality that is not re-
flected in syntax appears in (9), where the “understood”
subject of the sentence is not the entity normally denoted
by the actual subject (Nunberg 1979).
(9) a. [One waitress says to another]:
The ham sandwich wants another cup of coffee.
[Interpretation: “the person who ordered/is eat-
ing the ham sandwich . . .”]
b. Chomsky is on the top shelf next to Plato.
[Interpretation: “the book by Chomsky . . .”]
Such cases of “reference transfer” contain no syntactic
reflex of the italicized parts of the interpretation. One might
be tempted to dismiss these phenomena as “mere prag-
matics,” hence outside the grammatical system. But this
proves impossible, because reference transfer can have in-
direct grammatical effects. A clear example involves imag-
ining that Richard Nixon went to see the opera Nixon in
China (yes, a real opera!), and what happened was that:
(10) Nixon was astonished to see himself sing a foolish
duet with Pat.
The singer of the duet, of course, is the actor playing Nixon;
thus the interpretation of himself involves a reference trans-
fer. However, we cannot felicitously say that what happened
next was that:
(11) *(Up on stage), Nixon was astonished to see himself
get up and walk out.
That is, a reflexive pronoun referring to the acted character
can have the real person as antecedent, but not vice versa
(Fauconnier 1985; Jackendoff 1992b). As the use of reflex-
ive pronouns is central to grammar, reference transfer can-
not be seen as “extragrammatical.”
A fourth case of syntax-semantics mismatch concerns
one of the most persistent issues within generative gram-
mar, the syntactic status of quantifier scope. Consider the
two interpretations of (12).
(12) Everyone in this room knows at least two languages.
a. “John knows English and French; Sue knows
Hebrew and Hausa; . . .”
b. “. . . namely, Mandarin and Navajo.”
Should there be two different syntactic structures associ-
ated with these two interpretations? Chomsky 1957 said no;
Chomsky 1981 said yes; Generative Semantics said yes; I
am inclined to say no (Jackendoff 1996c; 2002 [Founda-
tions], Ch. 12). The problem with finding two different syn-
tactic structures is that it requires systematic and drastic
distortions of the syntactic tree that never show up in the
surface syntax of any language. The problem with having
only one syntactic structure is that it makes the syntax-se-
mantics interface more complex. The point to be made here
is that the scope of quantification may well be a further ex-
ample of the “dirtiness” of the interface between syntax and
semantics. This continues to be an important issue in lin-
guistic theory.
In each of these cases, a syntactocentric theory is forced
to derive the semantic distinctions from syntactic distinc-
tions. Hence, it is forced into artificial solutions such as
empty syntactic structure and elaborate movement, which
have no independent motivation beyond providing grist for
the semantics. On the other hand, if the semantics is
treated as independent from syntax but correlated with it,
it is possible to permit a less than perfect correlation; it is
then an empirical issue to determine how close the match
is.
If we abandon syntactocentrism, it is logically possible
that there are aspects of semantics that have no impact on
syntax but do have an effect on phonology through a direct
phonology-semantics interface. Such a treatment is at-
tractive for the correlation between prosody and informa-
tion structure. For example, the differences among (6a-c)
do not show up in syntax at all – only in the stress and in-
tonation in phonology, and in the focus-presupposition re-
lations in semantics. In a syntactocentric theory, one is
forced to generate these sentences with a dummy syntac-
tic element such as [1Focus], which serves only to corre-
late phonology and meaning and does not affect word or-
der or inflection. (Such was the approach in Jackendoff
[1972], for instance.) But this element does no work in syn-
tax per se; it is only present to account for the correlation
between phonology and semantics. By introducing a direct
phonology-to-semantics interface sensitive to this correla-
tion, we can account for it with minimal extra machinery;
but of course this requires us to abandon syntactocen-
trism.
7. The outcome: Parallel architecture
The argument so far has been that theoretical thinking in
both phonology and semantics has proceeded in practice as
though their structures are a result of independent gener-
ative capacities. What has attracted far less notice among
syntacticians, phonologists, and semanticists alike is that
such an organization logically requires the grammar to con-
tain interface components that correlate the independent
structures. Carrying this observation through the entire ar-
chitecture of grammar, we arrive at an overall picture like
Figure 3. Here the grammar contains multiple sets of for-
mation rules (the “generative” components), each deter-
mining its own characteristic type of structure; and the
structures are linked or correlated by interface compo-
nents.
In the syntactocentric architecture, a sentence is well-
formed when its initial syntactic tree is well-formed and all
the steps of derivation from this to phonology and seman-
tics are well-formed. In the parallel architecture, a sentence
is well-formed when all three of its structures – phonolog-
ical, syntactic, and semantic – are independently well-
formed and a well-formed correspondence among them
has been established by the interfaces.
One of the primary interface rules between phonology
and syntax is that the linear order of units in phonology cor-
responds to the linear order of the corresponding units in
syntax. One of the primary interface rules between syntax
and semantics is that a syntactic head (such as a verb, noun,
adjective, or preposition) corresponds to a semantic func-
tion, and that the syntactic arguments of the head (subject,
object, etc.) correspond to the arguments of the semantic
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
658
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

function. The consequence of these two primary interface
principles is that for the most part, syntax has the linear or-
der of phonology but the embedding structure of seman-
tics.
An illustration of some of these properties of the parallel
architecture appears in Figure 4, the structure of the phrase
the cats. The three independent structures are displayed
side by side;8 the subscripting indicates the connections es-
tablished by the interfaces between the parts of the three
structures. For example, the clitic pronounced dƒ is coin-
dexed with the determiner in the syntax and with the defi-
niteness feature in semantics. Notice that the lowest nodes
in the syntactic tree are syntactic features, not the custom-
ary notation the cat-s. The reasons for this are explained in
the next section.
The overall architecture laid out in Figure 3 provides a
model within which many different theories of grammar
can be embedded and compared. For example, Figure 3
does not dictate whether the syntactic formation rules are
along the lines of transformational grammar, the Minimal-
ist Program, Head-Driven Phrase Structure Grammar, or
many other alternatives. Moreover, the syntactocentric
framework is a version of Figure 3 in which the phonologi-
cal and semantic formation rules are null, so that everything
in phonological and semantic structures is determined only
by their interfaces with syntax. The framework favored by
many in Cognitive Linguistics minimizes or even eliminates
the syntactic formation rules, so that syntax is determined
entirely by meaning.
The organization into parallel generative components is
not new here. In addition to the innovations in phonology
discussed in section 5, Lexical-Functional Grammar di-
vides syntax into two tiers, c-structure and f-structure; Au-
tolexical Syntax (Sadock 1991) has a different division into
morphosyntax and phrasal syntax; Role and Reference
Grammar (Van Valin & LaPolla 1997) has, in addition to a
morphosyntax/phrasal syntax division, the propositional/
information tier division in semantics, with interfaces go-
ing every which way among the tiers. In other words, var-
ious elements of this architecture are widely present in the
literature. What is novel here is recognizing that this or-
ganization runs through the entire grammar, from phonol-
ogy through semantics (and further, as we will see in sect.
9).9
It might well be argued that the standard syntactocen-
tric framework has served the field well for forty years.
Why should anyone want to give it up? A reply might come
in five parts. First, no one has ever argued for the syntac-
tocentric model. In Aspects, it was explicitly only an as-
sumption, which quickly hardened into dogma and then
became part of the unstated background. By contrast, the
parallel architecture now has been argued for, in part
based on well-established results in phonology and seman-
tics, which have never played a role in syntactocentric ar-
gumentation.
Second, an advocate might argue that the syntactocentric
model is a priori simpler: Why should we admit so many dif-
ferent components into the grammar? The reply would be
that the choice among theories must be determined by em-
pirical adequacy as well as a priori simplicity. If the parallel
architecture allows a more perspicuous account of, say, in-
tonation contours or the relation of focus to stress, these are
arguments in its favor.
A third point concerns the relation of syntax and seman-
tics. As syntax is now not responsible for determining every
semantic distinction, it is to some degree liberated from 
semantics and can therefore be considerably simplified.
However, some compensating complexity must be intro-
duced into the syntax-semantics interface, so it doesn’t dis-
appear from the grammar entirely. It now becomes an em-
pirical question how to parcel the complexity out, and this
question can be addressed; it is not just an issue of opinion
or preference (see e.g., Culicover 1999b; Culicover & Jack-
endoff 1995; 1997; 1999; Culicover & Jackendoff, forth-
coming). At the same time, syntax does not go away entirely
(as opponents of Chomsky would often like). The syntax of
a language still has to say where the verb goes, whether the
verb agrees with the subject, how to form relative clauses
and questions, and so on. The differences among languages
in these respects are not predictable from semantics, and
children have to learn them.
A fourth point concerns the nature of Universal Gram-
mar. In the parallel architecture, the issues of acquisition
and innateness don’t go away, they are exactly the same,
namely: How does the child acquire the grammar of its na-
tive language on the basis of environmental evidence?
However, as just suggested, the sorts of questions that most
often arise concern the balance of power among compo-
nents. We don’t find ourselves invariably asking: What do
we have to add to syntax to account for such-and-such a
phenomenon? Rather, we find ourselves asking: In which
component does this phenomenon belong? Is it a fact of
syntax, of semantics, or of the interfaces? And to what ex-
tent is it realistic to attribute such a bias to the child learn-
ing the language?
A final point concerns not linguistic structure itself but
its connection to the rest of the theory of the brain/mind.
On the face of it (at least in my opinion), one should favor
approaches that permit theoretical integration. Section 9
will show four ways that the parallel architecture invites
such integration but the syntactocentric theory does not.
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
659
Figure 3.
The parallel architecture.
Figure 4.
The structure of the cats in the parallel architecture.
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

8. The lexicon and the words versus rules
controversy
Every theory of language has to view a word as a stored
complex of phonological, syntactic, and semantic features
or structure; commonly the store of words is called the lex-
icon. However, theories differ in the role of the lexicon in
the construction of sentences. In all of the syntactocentric
architectures shown in Figure 1, words are inserted into
syntactic trees at the beginning of a syntactic derivation, at
the point when syntactic trees are being built and before
they begin to be manipulated and fed to phonology and se-
mantics. Thus, the traditional notation for trees in Figure
5a is actually intended as an abbreviation of Figure 5b, in
which the lexical items are spelled out in full. The conse-
quence is that the syntax is carrying around with it all the
phonological and semantic features of words, which are to-
tally invisible to syntactic rules and are of use to the gram-
mar only when handed over and “interpreted” by the
proper component.
The parallel architecture, by contrast, insists that each
kind of feature belongs only in its own structure. In this
framework, the traditional syntactic notation in Figure 5 is
formally incoherent, because it has phonological and se-
mantic features in a syntactic structure. Thus, it is formally
impossible to insert full lexical items into syntactic struc-
ture. How then do words get into linguistic structures? The
answer is that each of the three structures making up a word
is inserted into its proper structure, and each of them car-
ries with it an index that connects it to the others. So, for
example, the word cat is notated as in Figure 6; its contri-
bution to the larger structure in Figure 4 should be evident.
Thus, a word is best regarded as a type of interface rule
that establishes partial correspondences among pieces of
phonological, syntactic, and semantic structure (each piece
in turn conforming to the formation rules of its own com-
ponent). In other words, the language does not consist of a
lexicon plus rules of grammar; rather, lexical items are
among the rules of grammar – very particular rules to be
sure, but rules nonetheless.
This treatment of the lexicon offers an attractive account
for a number of previously troublesome phenomena. For
example, consider an idiom such as kick the bucket. This
can be treated as a lexically listed VP that is coindexed with
phonology in the normal way, but which lacks indices con-
necting the individual words to semantics. Instead, the VP
as a whole is coindexed with the semantic structure DIE. As
a consequence, the individual words kick, the, and bucket
do not contribute individually to meaning. This is precisely
what an idiom is supposed to be: a stored unit in which the
words do not have their normal meaning.
A sort of converse is found in irregular morphology. Con-
sider something like the irregular plural feet. It has to be
listed syntactically as a plural noun, and the two syntactic
parts are coindexed in the normal way to semantics: the
word denotes multiple entities of the type FOOT. However,
the syntactic parts are not connected in normal fashion to
phonology; rather the whole syntactic complex is coindexed
with the undifferentiated lump feet in phonology.
Notice by contrast how the regular plural is coded in Fig-
ure 4. The regular plural consists of a piece of meaning,
namely plurality, plus a piece of syntax, namely an affix at-
tached to nouns, plus a piece of phonology, namely a suffix
s or z or ƒz, the choice determined contextually. That is, the
regular plural has all the same parts as a word, and it de-
termines a connection between them. We can notate this as
a lexical item along the lines of Figure 7. (The italicized bits
denote contextual features that determine how this item is
combined with its environment.) The contribution of this
item to the overall structure in Figure 4 is entirely parallel
to the contribution of the word cat.
This view of regular morphology puts a new and unex-
pected spin on the by now hoary “words versus rules” con-
troversy (e.g., Elman et al. 1996; Pinker 1999; Rumelhart &
McClelland 1986). Traditionally, everyone agrees that ir-
regular plural nouns like feet have to be listed in the lexi-
con. The issues are taken to be: (1) Are regular plurals all
listed as well, or is there a separate rule for the regular cases
that says “To form the plural of a noun, add -z?” And, there-
fore, (2) When children learn to form regular plurals, are
they learning something qualitatively different from learn-
ing the rough-and-ready generalizations among irregular
plurals?
In the present view, words are rules – interface rules that
help connect phonological, syntactic, and semantic struc-
tures. Figure 7, the “rule” for the regular English plural af-
fix, is qualitatively no different. Its contextual features are
qualitatively not unlike those of, say, transitive verbs. It
combines with nouns the same way a transitive verb com-
bines with its object. Thus, the formation of regular plurals
is an instance of ordinary combinatoriality. In this approach,
the issues come to be restated like this: (1) Are regular plu-
rals all listed, or is there a separate lexical item that encodes
the regular affix, which combines with any singular noun to
form a plural noun? And, therefore, (2) when children learn
to form regular plurals, are they learning this new lexical
item by extracting it as a regularity from the contexts in
which it appears – in the same way that they extract verbs
from the phrasal contexts in which they appear?
I submit that even to the most committed of connec-
tionists, this latter way of framing the question can hardly
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
660
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
Figure 5a.
Traditional notion for the cat. 5b. What the tradi-
tional notation abbreviates.
Figure 6.
The structure of the word cat.
Figure 7.
The English regular plural as a lexical item.
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

be objectionable. Yet the advocates of rules, such as Pinker,
have not made the case nearly as strong as it can be. The
connectionist argument has been to the effect: We can
make a device that learns all English past tenses without
making use of a rule, and we can find evidence from acqui-
sition and processing that supports this account. The best
version of the anti-connectionist argument has been: Con-
nectionist modeling offers important innovations over stan-
dard models of language in dealing with case-by-case learn-
ing and analogy for the irregular past tenses. But – you still
need rule learning to account for children’s acquisition of
regular past tense, and we can find evidence from acquisi-
tion and processing that supports this account. The prob-
lem is that the debate has often been framed as though only
the past tense were at issue, while the subtext behind the
connectionist position is that if this can be learned without
rules, then it is a good bet that the rest of language can be
too.
But not only the past tense is at stake. To deal with the
whole of language, it is necessary to account for the creative
formation of things like verb phrases and relative clauses –
linguistic entities that cannot be listed in the lexicon. In the
present view, the way that the regular past tense affix com-
bines with verbs is exactly like the way nouns combine with
relative clauses and the way noun phrases combine with
verbs and the way subordinate clauses combine with main
clauses – it is just another case of free combinatoriality. In
the decade and a half since the original connectionist past
tense model, there has been no demonstration that the
model scales up to acquisition of the full free combinatori-
ality of language – the issue that grounds generative lin-
guistics.
At the same time, notice that within the parallel archi-
tecture, the terms of the dispute become far less con-
tentious. The regular past tense is no longer a qualitatively
different phenomenon from words: Words are a type of
rule, and the posited regular past tense morpheme, shown
in Figure 7, is in the relevant respects just like a word. It
differs only in that it happens to be grammatically smaller
and it requires a word as its grammatical host. So the issue
is only whether there is such a separate lexical item, not
whether there are two wholly different kinds of linguistic
animal, namely words and rules. Thus, in the end, the fate
of the past tense doesn’t seem like such a big deal.
9. Four ways the parallel architecture helps
integrate linguistics with cognitive
neuroscience
The parallel architecture may be an intriguing technical al-
ternative to the Chomskyan orthodoxy in linguistics, but is
there any reason why it should be of interest to anyone
other than linguists? The end of the previous section may
have begun to offer some hints. This section will sketch out
a little more fully some ways in which the parallel architec-
ture offers opportunities to unify linguistics with the other
cognitive sciences.
9.1. The place of the parallel framework in the larger
architecture of the mind/brain
To sum up the larger picture: The parallel architecture
claims that language is organized into a number of semi-in-
dependent combinatorial systems, each of which has its
own organizing principles. These systems are linked by sys-
tems of interface principles. Interface principles establish a
correlation between pieces of structure in two (or more) of
the combinatorial systems. Some interface principles deal
with large-scale and general correspondences such as the
parallel between linear order in syntax and in phonology.
On the other hand, some of them are extremely specialized,
for instance individual words, idioms, and regular affixes.
The interface principles as a whole do not implement an
isomorphism between the structures they connect. Rather,
they implement a partial homomorphism, a “dirty” corre-
spondence in which not all parts of the structures in ques-
tion are correlated, and in which many-to-many mappings
are altogether common.
This conception of the interfaces within language is per-
fectly in tune with the way linguistic structures connect to
the rest of the mind. Consider how phonology interacts
with the auditory system in speech perception and with the
motor system in speech production. As is well known (to the
dismay of fifty years of computer scientists working on au-
tomated speech recognition), the mapping between a fre-
quency analysis of the speech signal and the phonological
structure of an utterance is frighteningly complex. In par-
ticular, some aspects of the speech signal play no role in
phonological structure and must be factored out, for exam-
ple, the individual timbre of the speaker’s voice, the
speaker’s tone of voice, and the speed of production, not to
mention ambient noise. These aspects of the speech signal
are put to use for other cognitive purposes, but not for
speech. Moreover, having factored all these things out from
the acoustic signal, still not every part of the phonological
structure is predictable from what is left: Most prominently,
word boundaries are not present as pauses in the signal.
Thus, the auditory-to-phonological mapping has the same
general characteristics as the interfaces inside language: It
establishes a “dirty” correspondence between certain as-
pects of two disparate mental structures.
Speech production has similar properties. Not every as-
pect of phonological structure corresponds to an aspect of
the motor control involved in operating the vocal tract. In
particular, word boundaries do not correspond at all con-
sistently to pauses in production. And not every aspect of
motor control is controlled by phonological structure. For
example, one can talk intelligibly with a pipe in one’s mouth,
which hugely distorts the motor commands involved in
speech without changing the phonological structure a bit.
And of course the same muscles in the vocal tract are used
for chewing, swallowing, and so on. Without going into
more detail, it should be clear that again the same sort of
interface is in play here.
Next, consider the visual system. Beyond the very early
levels of vision, there is little detailed theory of the f-knowl-
edge involved in vision – the necessary levels of represen-
tation and so on (I take Marr [1982] to have been attempt-
ing to lay out such a theory, but the enterprise has been
largely abandoned since his death). On the other hand, the
neuroscience of vision reveals a qualitatively similar pic-
ture: numerous independent brain areas, each specializing
in a particular aspect of vision such as shape, motion, color,
and spatial relations, each interacting with certain others by
dedicated pathways, and no area where “it all comes to-
gether” to form a full representation of the visual field. This
has precisely the flavor of the parallel architecture in lin-
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
661
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

guistics, where the notion of a “sentence” or “phrase” is dis-
tributed among several structures, communicating with
each other via dedicated interfaces, as shown in Figure 4.10
A syntactocentric architecture, by comparison, shows no
resemblance to the rest of the mind/brain. A master “com-
putational system” that generates syntactic structures,
which in turn determine phonological structures and mean-
ings, simply has no known parallel in the brain. Even the
connection of language to speech is markedly different
from the connections among the components inside of lan-
guage.
9.2. The role of semantics
Another important advantage of the parallel architecture is
the connection of semantics to the rest of the mind/brain.
Foundations of Language (Chs. 9 and 10) advocates that if
generative grammar is to truly adopt the mentalist stance,
this stance must be applied to meaning as well. According
to this stance, the basic function of language is to convert
thoughts into communicable form; the virtue of human lan-
guage over other natural communication systems is that it
is so broad in the range of messages it can convey. Each of
the indefinitely many sentences of a language conveys a dif-
ferent thought. As not all these thoughts can be stored in a
single head, it is necessary that thoughts be constructed
combinatorially. Therefore, a goal for semantic theory is to
uncover the combinatorial system underlying human con-
cepts. Such a goal converges with important trends in psy-
chology and philosophy.
However, another influential strain in semantics (and the
predominant one in Anglo-American philosophy, dating
back to Frege 1892 and shared by people as different as
Jerry Fodor 1987; David Lewis 1972; Hilary Putnam 1975;
and John Searle 1980) takes it that semantics is the study of
the connection of language to the world. On this view, a
proper semantics has to be concerned above all with how
the noise kæt is connected with cats. How language users
make that connection is quite a different issue (and to many
semanticists, not of interest). There is no room here to be-
gin the critique of this view; Chapters 9 and 10 of Founda-
tions take up the argument in detail. My overall conclusion
is that even if it is worthwhile undertaking such a “realist”
semantics, the enterprise of discovering how language users
do it is also worthwhile. I don’t care whether you call the
latter enterprise semantics or shmenantics or whatever– it
is this enterprise whose central issues intercalate naturally
with those of generative linguistics, cognitive psychology,
and neuroscience. Just to be clear, I will call this enterprise
conceptualist semantics.
Conceptualist semantics requires us to rethink the tradi-
tional issue of reference, which takes as its starting point the
unshakeable intuition that the phrase my cat does indeed
pick out an individual in the world. In a mentalist linguistic
theory, the language user’s linguistic system connects the
phonological string /maykæt / to the concept of a feline an-
imal, and to the concept of this feline animal being pos-
sessed by the speaker of the phrase. How then does the lan-
guage user get from there to the actual individual out there
in the world? The answer in brief is that it isn’t just language
users who have to connect something in their head to a
sense of individuals in the world: Any organism with a vi-
sual system approximately like ours (e.g., babies and apes)
has precisely the same problem. The environment acting on
the visual system produces some set of activations in the
brain, resulting in the organism experiencing real objects
out there. In other words, conceptualist semantics allows us
to recognize that the problem of reference is not a problem
about language; it is, at bottom, a problem about perception
and cognition which has to be solved by psychology and
neuroscience. By contrast, conventional realist theories of
reference divorce reference from the mind and make no
contact whatsoever with research on perception.
In order for the system of meaning to be influenced by
perception, of course, there has to be an interface between
conceptual/semantic structure and the “upper end” of the
perceptual systems, where “the world” (i.e., the perceiver’s
conceptualization of the physical world) is organized in
terms of stable three-dimensional objects that are located
in space with respect to the perceiver and each other. This
interface too can be shown to have the standard character-
istics: It is a partial homomorphism between the quasi-al-
gebraic format in which linguistic meanings are encoded
and the quasi-geometric/topological format(s) in which
spatial understanding is encoded. Thus, at the semantic end
of the language faculty, just as at the phonological end, the
relation between language and the rest of the mind is of the
same general character as the interfaces within the lan-
guage faculty itself.
Studying the conceptual system as a combinatorial sys-
tem leads to the same questions about acquisition as study-
ing syntax. How does the child learning language acquire
the meanings of all those thousands of words on the basis
of experience, both perceptual and linguistic? What per-
ceptual biases and innate structures does the child bring to
the task of interpreting the world? Here conceptualist se-
mantics makes contact with a rich literature on word and
concept learning and its innate bases (Baillargeon 1986;
Bloom 2000; Carey 1985; Gleitman & Landau 1994; Keil
1989; Macnamara 1982; Spelke et al. 1994, to mention only
a few examples). Moreover, because humans doubtless
share with monkeys and apes at least the parts of the con-
ceptual system dealing with physical space and perhaps
some of the parts dealing with social relations and other
minds, conceptualist semantics further makes contact with
research on primate cognition (Cheney & Seyfarth 1990;
Hauser 2000; Köhler 1927; Povinelli 2002; Premack 1976;
Tomasello 2000).
Again, these are issues that conventional realist seman-
tics cannot address. Nor are they particularly accessible to
semantics studied in a syntactocentric linguistic theory. For
if the combinatorial properties of semantics were com-
pletely attributable to the combinatorial properties of syn-
tax, then it would be impossible for nonlinguistic organisms
to have combinatorial thoughts. There are of course im-
portant strains of philosophy that have embraced this view,
identifying the capability for thought with the capability for
overt language (Descartes comes to mind, for instance).
But I think contemporary cognitive neuroscience has out-
grown such a view, and linguistics ought to be able to fol-
low suit gracefully.
9.3. The relation of grammar to processing
A theory of linguistic competence is supposed to simply de-
fine the permissible structures in the language, without say-
ing how those structures are produced in real time. How-
ever, as pointed out in section 2, a competence theory ought
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
662
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

to lend itself to being embedded in a theory of processing:
We ought to be able to say how the f-knowledge that con-
stitutes the competence theory is actually put to use.
There turns out to be an inherent structural reason why
competence has to be isolated from performance in the syn-
tactocentric view of language. If we flatten out and simplify
all the syntactocentric architectures in Figure 1, they all
have a logical directionality proceeding outward from syn-
tax in the middle:
(13) Logical directionality of syntactocentric architecture
sound ← phonology ← syntax → meaning
What I mean by logical directionality is that the possible
phonological structures and meanings cannot be deter-
mined without first determining syntactic structures. Syn-
tacticians may insist that they are being “metaphorical”
when they talk about things happening “before” and “after”
other things in a derivation; but the logical dependence is
there nevertheless. Now contrast this to the logical direc-
tionality of language processing: Language perception goes
consistently from left to right, and language production
from right to left.
(14) a. Logical directionality of language perception
sound →phonology →syntax →meaning
b. Logical directionality of language production
sound ←phonology ←syntax ←meaning
Hence, there is no way that the logical directionality in (13)
can serve the purposes of both perception and production.
Going from syntax to phonology in (13) seems inherently
like production – but only part of production; going from
syntax to semantics in (13) seems inherently like perception
– but only part of it.
The parallel architecture, by contrast, is inherently
nondirectional. The “information flow” between sound and
meaning is through the sequence of interfaces, each of
which is a system of correlations between two structures,
not a derivation of one from the other. The correlations can
be used in either direction (which is why they are drawn
with double arrows in Fig. 3). This makes it possible to think
of speech perception as a process where structures are ac-
tivated first at the auditory end of the chain of structures,
“clamped” by the environmental input. The interfaces
propagate activation rightward through the chain, each in-
terface principle creating a partial resonance between the
structures it connects. Eventually the structured activation
reaches semantic structure, at which point it can interact
with the hearer’s understanding of the context to produce
the understanding of the heard utterance. Similarly, in
speech production, the speaker begins with a thought to
convey, that is, meaning is “clamped” by the speaker’s com-
municative intent. Then the interface principles propagate
activation leftward through the chain, eventually activating
motor control of the vocal tract and producing speech. Cru-
cially, except for the specifically auditory and vocal parts of
the chain, the very same structures and the very same in-
terface principles are invoked in perception and produc-
tion, just in opposite directions.
There is no need in this system for all of one level to be
totally processed before activation of the next level sets in.
Any activation of a level, no matter how incomplete, if it can
be detected by the next interface, will start to propagate to
the next level in the chain. Therefore, processing can be
thought of as “incremental” or “opportunistic” rather than
rigidly regulated. In addition, because the interfaces are
trying to achieve “resonance,” that is, optimal mapping be-
tween levels, there is ample room in the processing theory
for feedback in processing – semantics affecting syntactic
processing in perception, and vice versa in production.
A crucial tenet of this theory, though, is that the rules of
grammar are the only source of information flow in lan-
guage processing. For example, knowledge of context can-
not directly affect phonological processing, because there
are no interface rules that directly relate contextual under-
standing to phonological structure. On the other hand, con-
text can indirectly affect phonological processing – via the
interfaces linking them through semantics and syntax. The
prediction is that such feedback will take effect some time
after constraints directly from phonology, because it has to
go up the chain of interfaces and down again. On the whole
such a prediction seems consistent with the experimental
literature (Cutler & Clifton 1999; Levelt 1989); Founda-
tions works out many details, in particular the relation of
long-term memory to working memory during language
processing.
The role of the lexicon in the processing theory is entirely
parallel to that in the competence theory. Recall that words
are little interface rules, providing partial routes for map-
ping between sound and meaning. Now consider the logic
of language perception. The auditory system and the inter-
face from audition to phonology produce some string of
speech sounds in the hearer’s head, and this activates a call
to the lexicon: “Do any of you guys in there sound like this?”
Then various items “raise their hands,” that is, are activated.
At this point the processor has no way of knowing which of
these items is semantically appropriate, because no contact
has yet been made with semantics. However, each item
over time activates a connection to potential syntactic and
semantic structures, which can be integrated with previous
words and with context to determine which candidate word
makes most sense in context. This scenario corresponds
precisely to the results in lexical access experiments (Swin-
ney 1979; Tanenhaus et al. 1979), in which at first every pos-
sible sense of a given phonological string is activated, later
to be pruned down by semantic context.
A parallel story can be told for speech production. The
speaker has activated some conceptual structure that s/he
wishes to communicate. The first step is to call the lexicon:
“Do any of you guys in there mean this?” Then various
items raise their hands. All the lexical retrieval and speech
error literature now comes to bear in showing us the flow
of information from this point to actual vocal production;
for the most part, it proves to correspond nicely to the op-
tions made possible by the components of the parallel
framework (Levelt 1989; 1999).
It is significant that the parallel architecture accords
words a very active role in determining the structure of sen-
tences, in concurrence with evidence from the psycholin-
guistic literature. By contrast, the syntactocentric architec-
ture views words as essentially passive: They simply ride
around at the bottom of syntactic trees, while the deriva-
tional rules of syntax do all the interesting work. Thus again,
in the area of the lexicon, the syntactocentric framework
makes it hard to connect competence and performance.
The conclusion here is that the parallel architecture per-
mits a far closer relation between competence and perfor-
mance theories. The rules of the language, including the
words, are posited to be precisely what the processing sys-
tem uses in constructing mappings between sound and
meaning. This opens the door for a two-way dialogue be-
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
663
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

tween linguistics and psycholinguistics. Linguistics has al-
ways dictated the structures that psycholinguistics should
be investigating. But now there is the possibility that psy-
cholinguistic experiments may help determine what com-
ponent of the grammar is responsible for a particular phe-
nomenon. For example, Pin˜ ango et al. (1999) have shown
that aspectual coercion (e.g., the sense of repetition in Jill
jumped until the alarm went off, example [8a]) causes a pro-
cessing load at a point in time that is appropriate to seman-
tic, not syntactic processing. This result conforms to the
theoretical claim that aspectual coercion is a matter of ad-
justing semantic well-formedness, not a matter of syntactic
deletion of an iterative morpheme. In short, the idealization
of a competence theory is not a rigid abstraction; rather, it
is a convenient methodological move, to be bridged freely
when the occasion arises. (More detail on the relation of
rules to processing is offered in Foundations, Ch. 7.)
9.4. Evolution of language
Let us return to a point from section 2. If Universal Gram-
mar is a human cognitive specialization, it has to be trans-
mitted by genes that have emerged in the course of our evo-
lutionary divergence from the chimpanzees. Of course, the
actual evidence for the evolution of the language faculty is
practically nonexistent. There is some evidence about the
evolution of the human vocal tract (Fitch 2000), but the
ability to make speech sounds is only one element of lan-
guage – and of course there are signed languages, which
don’t involve speech at all. In addition, it has begun to look
like many of the mechanisms for auditory perception are al-
ready in place in other mammals (Hauser et al. 2002). But
the real issue is: How did the ability to systematically map
combinations of concepts into sequences of speech sounds
and back again develop in our species, and how did the abil-
ity to learn such systematic combinatorial mappings de-
velop?
In the absence of evidence, we would like at least to be
able to tell a plausible story about the emergence of Uni-
versal Grammar, an important aspect of which is the over-
all architecture of language. In particular, we would not like
to have to explain language through miraculous emergence,
given that (as argued by Pinker & Bloom 1990) it has the
hallmarks of being shaped by natural selection. Pinker and
Bloom, however, do not offer any concrete proposals as to
how language evolved. As is well known, Chomsky himself
has been notably evasive on the issue of the evolution of the
language faculty, often seeming to cast aspersions on the
theory of natural selection (Newmeyer 1998 collects repre-
sentative quotes). Chomsky is correct that other factors be-
sides natural selection play a role in evolution, for example,
the mechanics of physical bodies and the biochemistry of
proteins. Nevertheless, there is nothing in these other fac-
tors that provides any helpful hints on what brought about
the emergence of language.
The logic of the syntactocentric architecture suggests a
reason why such evasion has been necessary. The problem
is in providing a route for incremental evolution, such that
some primitive version of the faculty could still be useful to
the organism. In the syntactocentric architecture, every-
thing depends on syntax. Meaning cannot have evolved be-
fore syntax, because its structure is totally dependent on the
syntactic structure from which it is derived. For the same
reason, phonological structure cannot have evolved before
syntax. Therefore, the complexity of syntax had to evolve
before the complexity of the other components. But what
would confer an adaptive advantage on a syntactic faculty
that just generated meaningless and imperceptible syntac-
tic structures? And what would enable children to acquire
such syntactic structure if there were no perceptible output
to which they could attach it? We quickly see that, at this
very crude level at least, the syntactocentric theory is stuck:
There is no logical way to build it incrementally, such that
the earlier stages are useful.
The parallel architecture offers a better alternative. The
system of concepts that language expresses is an indepen-
dent generative component in the mind/brain. Given that
this system is believed to exist, in some degree at least, in
other primates as well, it also could have existed in our an-
cestors, prior to language. That is, our ancestors had inter-
esting thoughts but lacked any way to say them: Meaning
therefore would be the first generative component of lan-
guage to emerge (a similar view is urged by Hauser 2000).
Most speculation on language evolution goes on to say
that the earliest stage would have been the symbolic use 
of simple vocalization, without grammatical organization.
Such a stage is theoretically impossible in the syntactocen-
tric theory, because even single-word utterances have to
arise from syntactic structure. But such a stage is quite nat-
ural in the parallel architecture: It consists of stored associ-
ations of vocalizations and concepts, a “paleo-lexicon.” Lex-
ical items that can serve on their own as utterances still exist
in modern language, for example, hello, oops, ouch, and
gadzooks. The provision for them in language might be
viewed as an evolutionary relic of this earliest stage.
Assuming that there would be an adaptive advantage to
a larger number of signals, a regimentation of vocalization
along the lines of phonological structure would be the next
generative component of language to emerge. Phonologi-
cal organization in effect digitizes vocalizations, making a
large vocabulary reliably discriminable and learnable.
(Proto-)words at this point would be simply duples of
phonological and semantic structure, without syntax.
A next innovation might be the provision of concatenat-
ing words into larger utterances. However, when words are
concatenated, the issue arises of how the meanings of words
in a string are related to each other semantically. In a string
like eat apple Fred, it is pretty clear on pragmatic grounds
that Fred is eating the apple and not the reverse. But prag-
matics can only go so far: in chase lion bear, who is the
chaser? Something as elaborate as English syntax is not en-
tirely necessary to fix this. One can actually get considerable
mileage from simple functional principles of linear word or-
der. For example, the principle “Agent First” would tell us
that the lion is chasing the bear and not the reverse. Such a
principle is a straight phonology-to-semantics mapping, re-
lating linear order to semantic function. And principles like
this appear to be widespread in pidgin languages (Bicker-
ton 1981) and the grammars of speakers who have acquired
their languages late in life, after the sensitive period (Klein
& Perdue 1997).
Finally, principles like Agent First have their limitations,
too. One can imagine the capacity for modern syntactic
structure evolving last, as a way of making more complex
semantic relations among the words of utterances more
precisely mappable to linear word order in phonology. That
is, syntax develops in evolution as a refinement, a “super-
charger” of a preexisting interface between phonology and
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
664
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

semantics. This seems exactly appropriate to its function
within the parallel architecture.
In short, the parallel architecture cannot tell us exactly
how language evolved – I don’t think anything can ever tell
us that. But it does afford a far more plausible hypothesis
than the syntactocentric architecture (Foundations, Ch. 8,
develops this story in considerably more detail). Therefore,
the parallel architecture opens the door for linguistics to
participate far more fully in the mainstream of evolutionary
psychology, yet another desirable connection.
10. Conclusions
Putting this all together, the parallel architecture makes it
possible both to internally integrate linguistic theory, es-
tablishing the proper relation between phonology, syntax,
semantics, and the lexicon, and also to integrate linguistic
theory more comprehensively with the brain and with biol-
ogy. In addition, by liberating semantics from its syntactic
shackles, the parallel architecture makes it possible to de-
velop a fully psychological theory of meaning and its rela-
tion to perception. As observed above, these points of con-
nection were precisely what early generative grammar
promised but ultimately couldn’t bring off. I have tried to
show here why syntactocentrism was a major reason behind
this disappointment.
Of course, to propose a new architecture only begins the
work. It opens major questions about exactly what compo-
nents the grammar requires and what interfaces connect
them. Vast numbers of phenomena have been studied in
the context of the traditional architecture; to what extent
can the analyses proposed there be duplicated or even im-
proved upon? In particular, a thorough overhaul of syntac-
tic theory is necessary to overcome decades of accretions
motivated solely by syntactocentric assumptions (Culicover
& Jackendoff [forthcoming] begin to undertake this task).
Perhaps the hardest part of all this will be maintaining a
sense of global integration, keeping the subdomains of the
field in closer touch than they have recently been.
But linguistics alone cannot sustain the weight of the in-
quiry. We need all the help we can get from every possible
quarter. And in return, one would hope that linguistic the-
ory might be a more fruitful source of evidence and puzzles
for other fields. Above all, my aspiration for Foundations is
that it can help encourage the necessary culture of collab-
oration.
ACKNOWLEDGMENTS
Foundations of Language was written during 1999–2000, while I
was a Fellow at the Wissenschaftskolleg zu Berlin, a year for which
I am profoundly grateful. Both the book and this version of it were
supported in part by NIH Grant 03660 to Brandeis University. I
also must thank Marc Hauser and Dan Dennett for their help and
encouragement on this version.
NOTES
1. “Essentially the same” is a matter of perspective. When we
are talking about “English speakers” as a whole we can treat them
all as essentially the same. But if we’re talking about dialect dif-
ferences, dialect contact, or language change, we can just as eas-
ily switch to treating different speakers as having (slightly) differ-
ent linguistic systems in their heads. And of course when we’re
talking about language acquisition, we take it for granted that the
young child has a different system than the adults.
2. To some extent Chomsky’s point has been lost on the larger
cognitive neuroscience community. For instance, the widely cited
connectionist parser of Elman (1990) is a variation of a finite-state
Markov device, and is subject to some of the same objections
raised by Chomsky in 1957. See Marcus (2001) and Pinker (1999)
for extensive discussion.
3. For example: “The deep structure that expresses the mean-
ing is common to all languages, so it is claimed [by the Port-Royal
grammarians – who of course did not use the term “deep struc-
ture”], being a simple reflection of the forms of thought” (Chom-
sky 1966).
4. Some opponents of Chomskyan generative grammar (for in-
stance some Cognitive Grammarians) have rightly objected to syn-
tactocentrism, but proposed instead that all properties of language
are derivable from meaning. I take this to be equally misguided,
for reasons that should be evident as we proceed.
5. A standard mark of recursivity is a constituent occurring
within another constituent of the same type. For instance, a clause
can appear within another clause: The man who comes from New
York is tall; and a noun phrase can appear within a noun phrase:
the king of the Cannibal Islands. In phonology this sort of situ-
ation does not occur nearly so freely: In particular, a syllable can-
not occur within another syllable.
6. Interestingly, Chomsky (1965) brings up an example like (4)
and analyzes the prosody as a fact of performance: Speakers don’t
pronounce the sentence in accordance with its syntactic structure.
This is about the only way he can analyze it, given that he does not
have independent principles of intonational constituency at his
disposal. Contemporary theory allows us to say (correctly, I be-
lieve) that (4) is well-formed both syntactically and prosodically,
with a well-formed but non-isomorphic correspondence between
the two structures.
7. It is important to distinguish two interpretations of “syntac-
tic” here. In the broader sense, every combinatorial system has a
syntax: mathematics, computer languages, music, and even
phonology and semantics. In the narrower sense of technical lin-
guistics, “syntactic” denotes the organization of units such as NPs,
VPs, and prepositions. I am reserving “syntactic” for this narrower
sense and using “combinatorial” for the broader sense.
8. For the semantics I have used the Conceptual Structure no-
tation of Jackendoff (1983; 1990); readers invested in other frame-
works should feel free to substitute their own notations.
9. Stratificational Grammar (Lamb 1966) also proposed a thor-
oughgoing organization into independent generative components
linked by interfaces.
10. The lexicon, a large collection of learned arbitrary associa-
tions between very particular bits of structure, also has parallels in
other domains of memory. For instance, it is an arbitrary fact that
the sound kæt means “feline animal,” and the child must learn it
from the environment. Now consider the association between the
appearance of foods and their tastes. It is similarly arbitrary (from
the point of view of the organism) that something that looks like a
cucumber tastes the way it does, and organisms learn probably
hundreds or thousands of such associations. (There are even am-
biguous looking foods: Think of mashed potatoes and vanilla ice
cream.)
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
665
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

Open Peer Commentary
Commentary submitted by the qualified professional readership of this
journal will be considered for publication in a later issue as Continuing
Commentary on this article. Integrative overviews and syntheses are es-
pecially encouraged.
Semantic paralysis
Fred Adams
Department of Philosophy, University of Delaware, Newark, DE 19716.
fa@udel.edu
http://www.udel.edu/Philosophy/famain.html
Abstract: I challenge Jackendoff’s claim that semantics should not be par-
alyzed by a failure to solve Brentano’s problem of intentionality. I argue
that his account of semantics is in fact paralyzed because it fails to live up
to his own standards of naturalization, has no account of falsity, and gives
the wrong semantic objects for words and thoughts.
There is no reason to be paralyzed by the absence of a
solution for intentionality . . . (Jackendoff 2002, p. 280)
Of late, there are two big ideas at the extremes in cognitive sci-
ence. One is that the mind itself, not just its referential content,
extends beyond the head and into the environment (Clark &
Chalmers 1998). The other is that not even the content of thoughts
extends into the environment, for that requires solving the prob-
lem of intentionality – how thoughts come to be about things and
mean things outside the head. Jackendoff defends this second idea
in Chapters 9 and 10 of his recent book, Foundations of Language:
Brain, Meaning, Grammar, Evolution (henceforth Jackendoff
2002). Elsewhere (Adams & Aizawa 2001), I have said why the
first idea is a bad one. Here I’ll say why the second idea is an un-
happy one, as well.
Jackendoff accepts the following:
1. “People find sentences . . . meaningful because of some-
thing going on in their brains” (Jackendoff 2002, p. 268).
2. “There is no magic . . . we seek a thoroughly naturalistic ex-
planation [of meaning] that ultimately can be embedded in our
understanding of the physical world” (p. 268).
3. “[T]he basic problem [is to] situate the study of meaning in
the study of the f-mind”1 (p. 271).
4. Meaningful f-mental entities in cognition direct attention
and make judgments on the world as perceived through the senses
(p. 271).
5. Meaningful f-mental entities in cognitive processes connect
linguistically conveyed messages with one’s physical actions
(p. 272).
Jackendoff also signals a departure from Jerry Fodor’s views.
Fodor (1990) wants syntactic items in the language of thought
(LOT, Fodor’s version of f-mind) to represent things – entities in
the world. The meaningful entities, in this view, are meaningful
because they represent things, are about things in the world. “Nat-
uralized semantics” is all about how purely natural conditions and
natural causes can make this happen (make things in the head
mean or be about things outside the head). Finding a satisfactory
account of the relations between the representing item and the
represented is notoriously difficult. Jackendoff finds it so difficult
(“one cannot make naturalistic sense of intentionality” [p. 300])
that he is ready to throw in the towel (“there is no physically real-
izable causal connections between concepts and objects” [p. 300]).
He says:
Fodor’s problems arise from treating the combinatorial structures that
constitute meanings/thoughts as symbols for something, representa-
tions of something, information about something. Instead, I am going
to try to take them just as pure non-intentional structure . . . with
phonology and syntax. The problem will then be to reconstruct the in-
tuitions that the notion of intentionality is supposed to account for.
(p. 279)
Jackendoff thinks that one can simply push “the world” into the
head as a conceptual structure or reconstruction, and dispense
with the hard problem of naturalizing semantics in terms of causal
relations to an external world (p. 303ff). He spends a good deal of
Chapters 9 and 10 explaining why his constructivist semantics is
not guilty of solipsism. Nevertheless, I think that he should leap at
the chance for solipsism. After all, solipsists may wonder whether
there is a world beyond their minds, but at least their terms have
perfectly stable semantic contents. Their worry is largely episte-
mological (“How do I know there is more than just me?”), but the
semantics of the terms in which they worry are perfectly ordinary
meaningful terms. “Tree” means tree when they wonder whether
there really are trees.
What would symbols in the conceptual semantics of Jackendoff
mean? He says “A speaker (or thinker) S judges that a term or
phrase in the f-mind refers to an entity E in the world conceptu-
alized by S” (p. 304). He is proposing a mapping from terms in the
f-mind to other objects in the f-mind E, where the first set of ob-
jects are the representational vehicles and the second set are the
meanings. This way we don’t have to worry about counterfactuals
or causal chains or what information is: “in a conceptualist theory,
reference is taken to be . . . dependent on a language user”
(p. 304). Jackendoff retreats to the friendly confines of the head
because this will somehow make semantics easier and because the
conceptual structures inside the head “do exactly the things mean-
ing is supposed to do” (p. 306). Language is meaningful because
it connects to such conceptual structures. As per the numbered
list above, we just have to construe “world” and “physical” as re-
ferring to conceptual structures of a mental model when we do the
semantics of terms in the f-mind.
So why is this not going to work? I have space to state only a few
reasons (but there are more). First, even for objects inside the
head, Jackendoff has to give the naturalistic conditions under
which one object represents another. He gives none. Second, what
he does say violates his own principle (2) above. How can the ori-
gin of reference depend on a language user, unless there is already
language with meaning to be used? It would be magic to invoke
meaning in the explanation of the origin of a system of language
use. Naturalized accounts of meaning must avoid magic.
Third, since everyone is familiar with Searle’s (1980) example of
the Chinese Room, through reference to it I can register my
strongest complaints. Jackendoff admits that “On this picture our
thoughts seem to be trapped in our own brains” (p. 305), but
things are even worse – as if that weren’t bad enough. There is no
sense in calling what is trapped in the brain thoughts. At most
there are structures, perhaps even information-bearing structures
delivered by the senses. But there seems little reason to think
these are more than semantically uninterpreted squiggles and
squoggles (in Searle’s terminology) that come in through the sen-
sory oracles. They might as well be Chinese characters to non-Chi-
nese speakers.
Here is an example: I see beer and say “beer, please” because I
want a beer. Now in Jackendoff’s view there is in the f-mind a syn-
tactic object that I would call my symbol for beer. He can’t call it
that because it is not a symbol for beer. It is a symbol for a per-
ceptual structure that may occur in me in the presence of beer
(but also may not). There is no nomic semantic intentional rela-
tion between “beer” and beer in his picture. Normally we would
say that it was because I wanted beer that I said “beer, please.” It
was because of the semantic content of my thought (there is beer
here) that I intentionally tried to order beer. Thoughts do that.
They cause things like behavior because of their contents and they
derive their contents, at least in part, from their causal connec-
tions to their environments. And they can be falsely tokened – I
could mistakenly have thought there was beer.
Now, how can any of these things constitutive of thoughts be
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
666
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

true of Jackendoff’s conceptual structures? They can’t. Take just
the last case. If I apply “beer” to the structure conceptualized by
me now present in my head and that normally is tokened in the
presence of beer (but which can be tokened whether or not there
is beer actually nearby), how could my thought be false? It can’t.
There is no mismatch with my reality and no falsity according to
me. So it is not thoughts that are trapped in the brain, according
to Jackendoff’s picture. Thoughts really can be false (not just con-
ceived false, whatever that comes to in his semantics [p. 329]).
Finally, at the end of the day one often wants a beer. In Jack-
endoff’s proposal, what one actually wants is a beer percept or an
as-perceived-beerly-by-me conceptual structure to be tokened.
Not for me – I just want a beer.
NOTE
1. Editor’s note: “f-mind” stands for “functional mind” (Cf. Founda-
tions, p. 21).
On the role of frame-based knowledge in
lexical representation
József Andor
Department of English Linguistics, University of Pécs, Pécs, H-7624
Hungary. andor@btk.pte.hu
Abstract: In this commentary I discuss the role of types of knowledge and
conceptual structures in lexical representation, revealing the explanatory
potential of frame-based knowledge. Although frame-based lexical se-
mantics is not alien to the theoretical model outlined in Jackendoff’s con-
ceptual semantics, testing its relevance to the analysis of the lexical evi-
dence presented in his book has been left out of consideration.
Through the years, Jackendoff’s approach to describing lexical
representation and characterizing the nature of lexical storage and
retrieval has been strictly conceptualist. However, in Foundations
of Language (Jackendoff 2002) he has not addressed several fac-
tors of the mental representation of lexical information exten-
sively, and consequently, various important details have remained
unexplained or have been overlooked. One of these concerns the
relation between the linguistic (i.e., the “dictionary”) versus the
encyclopedic meaning of lexical items; that is, as Jackendoff refers
to them in discussing the views of others, their semantic versus
pragmatic potential (Jackendoff 2002, pp. 285–86). I would argue
that in discussing this conceptual facet of lexical representation we
are not strictly facing meaning. Rather, in my view, we are facing
here various types of knowledge and their conceptually based role
in lexical representation and the mapping of meaning. As outlined
by Clark (1992; 1996) and by Andor (1985; 2003), however, the re-
lation does not only hold between “dictionary” (i.e., lexical) and
encyclopedic types of knowledge, but is manifold and can occur
as a result of the interaction of multiple types of knowledge, in-
cluding generic and private or socio-cognitively based communal
and expert knowledge during communication. All of these types
of knowledge contribute to the common ground shared by speak-
ers of a linguistic community (Andor 1985; 2003; Clark 1992;
1996, p. 92–121). In Foundations, Jackendoff does not address in
detail the complex issue of the relation between these types of
knowledge based on empirical evidence. For instance, how exactly
does encyclopedic knowledge, a body of stereotypically-based
knowledge, serve as a source for lexically represented knowledge?
Conversely, does the latter type of knowledge serve as a source for
the saturation of lexical meaning embodied by the lexical items
represented in a given language?
Nor is the issue of the role of frame-based, scenic and scriptal
knowledge in lexical storage and retrieval, as well as in the repre-
sentation of lexical and encyclopedic knowledge types, discussed,
although Chapters 9, 10, and 11 abound in traces of this domain.
Jackendoff refers to difficulties in separating domains of encyclo-
pedic and lexical semantics, for instance, in clarifying the differ-
ence between the lexical meanings of murder and assassinate. He
argues that the “latter implies a political motive on the part of the
agent” (Jackendoff 2002, p. 286), but fails to identify the real core
of difference: These verbs belong to the lexical networks in the
representation of different conceptual scenes and frames, and
thus have different scripts of associated performance in their con-
ceptual makeup.
This is an important issue to be taken into account in studying
the criteria and borderlines of synonymy. Although words that are
members of a given lexical field may fall into different types of syn-
onym sets, some of them may be freely substitutable by another
member of the same field and may even show the same patterns
of syntactic alternations, and hence be identified as absolute syn-
onyms; others in the same domain may be near or partial syn-
onyms only (Cruse 2000, p. 156–60). Absolute synonymy is known
to be quite rare. According to Jackendoff, items are synonymous
in case they are mutually subordinate (1983, p. 104). But perhaps
the most important issue concerning the set of criteria of syn-
onymy has been overlooked by researchers of the field: Although
lexical items belonging to a given lexical field may share similar de-
notational, categorical, subcategorization, and perhaps even se-
lectional and relational features (i.e., argument structure), they
may still reveal different grades of distance in prototypicality due
to differences in their frame relatedness and the scriptal makeup
of their background concepts. The higher the frame dominance,
the greater the distance from the prototypical instance within the
given lexical domain, and the looser the synonym relatedness to
other members of the field.
This can be tested experimentally. For instance, within the do-
main of verbs of cutting, mow, trim, and prune are quite distant
from cut, the prototypical member of the group, whereas slice is
nearer. Concerning verbs of jumping, bounce is lower down in the
gradience of prototypicality than are spring and hop, whereas
prance and dance are even further away from the prototypical
member jump in this lexical domain. Features of categories and
their lexical representation in a certain domain occur as clusters,
as pointed out by Jackendoff and others. However, an important
property is overlooked: The more types and kinds of features are
shared by members, the higher the rate of prototypicality mani-
fested, but at the same time, a high coincidence of feature clus-
ters results in a lower rate of frame dominance. In Jackendoff’s
view “the prototype is simply an instance that happens to maxi-
mally satisfy the cluster conditions” (2002, p. 356). I believe that
the role of the prototype lexical concept in a lexical field is more
marked: It is the item that provides the criteria of coherence
within the lexical domain and sets boundary conditions on mem-
bership in its lexis.
Finally, let me briefly address Jackendoff’s approach to the in-
teresting issue of frame-based reference, the case of frame-based
lexical items. In his conceptualist view, “reference is taken to be at
its foundation dependent on a language user, . . . being in the real
world is not a necessary condition” (2002, p. 304). Such is the case
of unicorns, dwarfs, trolls, goblins, chimera, and so forth. All such
entities require some rate of conceptualization, as Jackendoff sug-
gests, in at least some minimal way to gain reference (2002,
p. 304). However, he fails to provide adequate terminology for
such cases of items. As frames are types of conceptual structures
which are based on global and stereotypical information, are dom-
inantly dependent on encyclopedic knowledge, and are acquired
in lack of direct exposure to empirical experience contrary to
scenic knowledge (Andor 1985), the above lexical items are typi-
cally acquired and retained in memory on such grounds. A great
many lexical concepts such as marmots, but even tigers or cows
may first be acquired on such grounds, and then, based on expo-
sure to direct experience, scenic knowledge, their content is mod-
ified and standardized upon speakers’ gaining full lexical compe-
tence. Thus, their feature makeup may show analogies to those
acquired on the basis of scenic knowledge.
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
667
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

ACKNOWLEDGMENTS
This work was supported by a Fulbright research grant (No. 1202204)
given to the author by CIES in the United States and the Hungarian-
American Commission for Educational Exchange. Special thanks to
Robert M. Harnish of the University of Arizona, Tucson, for useful sug-
gestions in preparing the manuscript.
brain, Meaning, Grammar, evolution
Michael A. Arbib
Computer Science Department, Neuroscience Program, and USC Brain
Project, University of Southern California, Los Angeles, CA 90089-2520.
arbib@pollux.usc.edu
http://www-hbp.usc.edu/
Abstract: I reject Jackendoff’s view of Universal Grammar as something
that evolved biologically but applaud his integration of blackboard archi-
tectures. I thus recall the HEARSAY speech understanding system - the
AI system that introduced the concept of “blackboard” - to provide another
perspective on Jackendoff’s architecture.
The subtitle “Brain, Meaning, Grammar, Evolution” for Founda-
tions of Language (Jackendoff 2002) suggested that Jackendoff
would devote major portions of his book to brain and evolution.
Alas, there is no serious discussion of the brain (beyond a few pass-
ing references to aphasia) and the discussion of evolution (Ch. 8)
focuses on an incremental account of Universal Grammar (UG)
that ignores brain evolution. Space does not permit proper dis-
cussion of the brain here. Instead, I lament Jackendoff’s view of
Universal Grammar as something that evolved biologically; and
then recall the HEARSAY speech understanding system to pro-
vide another perspective on Jackendoff’s architecture.
Concerns about Universal Grammar. Jackendoff (2002, p. 263)
views UG as “the unlearned basis from which language is learned”
and argues that “it had better be available to help children learn
case systems, agreement systems, fixed word order, and gram-
matical functions in case the language in the environment hap-
pens to have them.”
I find this view incoherent if it implies that evolution yielded
adaptations specific to each of these systems. What selective pres-
sure would cause humans whose language does not use cases to
evolve a brain with a device specialized for learning case systems?!
Instead, I think we should seek to understand what made the brain
“language ready,” providing capacities that make possible the dis-
covery of Jackendoff’s language “components” over the course of
many millennia, and their acquisition by the child over the course
of a few years. One listing of such capacities (based on Arbib
2002b) follows:
Complex imitation: the ability to recognize another’s perfor-
mance as a combination of familiar movements and then repeat it.
Symbolization: The ability to associate an arbitrary symbol with
a class of episodes, objects or actions. (These symbols may have
been unitary utterances, rather than words in the modern sense,
and may have been based on manual and facial gestures rather
than being vocalized.)
Parity (mirror property): What counts for the “speaker” must
count for the “listener.”
Intentional communication: Communication is intended by the
utterer to have a particular effect on the recipient, rather than be-
ing involuntary or a side effect of praxis.
From hierarchical structuring to temporal ordering: Perceiving
that objects and actions have sub-parts; finding the appropriate
timing of actions to achieve goals in relation to those hierarchically
structured objects.
Beyond the here-and-now: The ability to recall past events or
imagine future ones.
Paedomorphy and sociality: A prolonged period of infant de-
pendency combines with social structures for caregiving to pro-
vide the conditions for complex social learning.
In hindsight we may see these as preadaptations for language
but they were adaptive in their own right, and underlie many mod-
ern human capacities other than language. In this view, Universal
Grammar is only tenable as a descriptive umbrella for the im-
mense variety of human languages, not as a “genetic reality” or
“neural reality” that implausibly contains all possible grammatical
structures in embryo (one is reminded of the “little man” that sev-
enteenth century spermists “saw” inside the head of the sperma-
tozoon [Pinto-Correia 1996; 1997]). I applaud Jackendoff’s at-
tempt to provide an evolutionary sequence for language but argue
(e.g., Arbib 2002b) that case systems, agreement systems, and so
on, are to be seen as human inventions that required no change in
brain structure for their discovery and cultural transmission.
Moreover, I see these as coarse grain compared to the actual in-
ventions that were made across the millennia and which eventu-
ally coalesced into the more-or-less coherent structures that Jack-
endoff and other linguists tend to treat as natural and indivisible.
What is universal is the need for expression, not the choice of lin-
guistic structure for meeting those needs. The evolution of lan-
guage from protolanguage is part of the history, not the biology, of
Homo sapiens.
Déjà-entendu. Jackendoff makes much of the AI notion of
blackboard in presenting his architecture for language, but does
not cite HEARSAY-II (Erman et al. 1980; Lesser et al. 1975), per-
haps the first AI system to develop a blackboard architecture.
While obviously not the state of the art, it is of interest because 
it foreshadows features of Jackendoff’s architecture. Digitized
speech data provide input at the parameter level; the output at the
phrasal level interprets the speech signal as a sequence of words
with associated syntactic and semantic structure. Because of am-
biguities in the spoken input, a variety of hypotheses must be con-
sidered. To keep track of all these hypotheses, HEARSAY uses a
dynamic global data structure, called the blackboard, partitioned
into various levels; processes called knowledge sources act upon
hypotheses at one level to generate hypotheses at another.
First, a knowledge source takes data from the parameter level
to hypothesize a phoneme at the surface-phonemic level. Many
different phonemes may be posted as possible interpretations of
the same speech segment. A lexical knowledge source takes pho-
neme hypotheses and finds words in its dictionary that are con-
sistent with the phoneme data - thus posting hypotheses at the 
lexical level and allowing certain phoneme hypotheses to be dis-
carded. To obtain hypotheses at the phrasal level, knowledge
sources embodying syntax and semantics are brought to bear.
Each hypothesis is annotated with a number expressing the cur-
rent confidence level assigned to it. Each hypothesis is explicitly
linked to those it supports at another level. Knowledge sources co-
operate and compete to limit ambiguities. In addition to data-driv-
en processing which works upward, HEARSAY also uses hypoth-
esis-driven processing so that when a hypothesis is formed on the
basis of partial data, a search may be initiated to find supporting
data at lower levels. A hypothesis activated with sufficient confi-
dence will provide the context for determination of other hy-
potheses. However, such an island of reliability need not survive
into the final interpretation of the sentence. All we can ask is that
it forwards the process which eventually yields this interpretation.
Hanson and Riseman (1987) based the architecture of their
computer vision system VISIONS on the HEARSAY architecture
as well as neurally inspired schema theory (Arbib 1981; Arbib et
al. 1998). Such a conceptual rapprochement between visual per-
ception and speech understanding offers a computational frame-
work for further exploration of the Saussurean sign (Arbib 2003;
Hurford 2003). Arbib and Caplan (1979) discussed how the
knowledge sources of HEARSAY, which were scheduled serially,
might be replaced by schemas distributed across the brain to cap-
ture the spirit of “distributed localization” of Luria (e.g., Luria
1973). Today, advances in the understanding of distributed com-
putation and the flood of brain imaging data make the time ripe
for a new push at a neurolinguistics informed by the understand-
ing of distributed computation. Despite its disappointing inatten-
tion to the brain, Jackendoff’s book could make a valuable contri-
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
668
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

bution to this effort by showing generative linguists how to break
out of the straitjacket of syntactocentrism by integrating their
work into a rich multi-modal architecture.
ACKNOWLEDGMENT
Preparation of this commentary was supported in part by a Fellowship
from the Center for Interdisciplinary Research of the University of South-
ern California.
Language evolution without evolution
Derek Bickerton
Department of Linguistics, University of Hawaii, Honolulu, HI 96822.
bickertond@prodigy.net
Abstract: Jackendoff’s major syntactic exemplar is deeply unrepresenta-
tive of most syntactic relations and operations. His treatment of language
evolution is vulnerable to Occam’s Razor, hypothesizing stages of dubious
independence and unexplained adaptiveness, and effectively divorcing the
evolution of language from other aspects of human evolution. In particu-
lar, it ignores connections between language and the massive discontinu-
ities in human cognitive evolution.
I approach Jackendoff’s ambitious and many-faceted Foundations
of Language: Brain, Meaning, Grammar, Evolution (Jackendoff
2002) as an unashamed syntactocentrist. Jackendoff, however, is
far from being that, and the main example he picks to illustrate
syntactic relations could hardly have been better chosen had he
deliberately intended to marginalize and trivialize syntax:
(1) The little star’s beside the big star.
This sentence, first analyzed on pages 5 through 6, is returned to
repeatedly throughout the text.
But, copular sentences like (1), sentences with the verb “to be,”
form a small and highly idiosyncratic subset of sentences; their
properties differ sharply from those of the vast majority of sen-
tences. The latter describe actions, events, or a variety of states,
and deploy a rich variety of argument structures; copular sen-
tences express only identity, location, or the attribution of quali-
ties (The rose is red/in the vase/a Molly Perkins) and take only a
theme argument. In a non-copular clause, no two noun-phrases
will have the same referent (unless a specifically reflexive form
such as himself is used), and transposition of noun-phrases in-
evitably changes meaning:
(2) a. John hit the captain.
b. The captain hit John.
In copular clauses, no two noun-phrases will have different ref-
erents; consequently, transposition of noun-phrases inevitably
leaves meaning unchanged:
(3) a. John is the captain.
b. The captain is John.
There are many more syntactic relations that can’t be illustrated
via copular sentences, too many to list here. Perhaps in his re-
sponse to commentary Jackendoff will tell us why he chose such
an atypical sentence as his prime syntactic exemplar.
Much more could be said about Jackendoff’s treatment of syn-
tax, but I must reserve the bulk of this commentary for his chap-
ter on language evolution. Right off, Jackendoff confuses the is-
sues with a straw-man version of “the common view of Universal
Grammar” (p. 233). According to him, that view treats phonology
and syntax as “passive handmaidens of syntax” that could not,
therefore, have evolved prior to syntax. But syntax without phonol-
ogy and semantics would be useless, so this view is absurd.
In fact the current status of semantics and phonology (what-
ever that may be) carries no entailment for their order of evolu-
tion. No one disputes that apes and hominids had some sort of
conceptual structure, therefore semantics (in some form) had to
precede syntax (indeed, this is made quite explicit in my own writ-
ings, from Bickerton 1990 on). As for phonology, this (at least in
some primitive form) was presumably present in protolanguage,
which had no syntax. But the emergence of syntax selected for a
sophisticated phonology, while the capacity to assemble seman-
tic units into complex propositions radically expanded conceptual
structure.
Jackendoff then turns to the proposal of Bickerton (1990) that
language developed in two steps, an asyntactic protolanguage and
syntacticized modern language, and instead opts for “a more
graceful, incremental evolution” (p. 236). But are the incremen-
tal stages he proposes really stages at all?
Take the three stages: (1) “use of symbols in a non-situation-spe-
cific fashion,” (2) “use of an open, unlimited class of symbols,” and
(3) “development of a phonological combinatorial system” that
supposedly intervene between an alingual state and protolan-
guage. No real difference exists between the first two. A symbol
freed from the here and now has to be cultural rather than bio-
logical; if you can invent one, you can invent an unlimited num-
ber. A protolanguage adequate for the needs of hominids two mil-
lion years ago wouldn’t have needed many. Nothing suggests that
an insatiable demand for new symbols would have driven the
emergence of a phonological combinatorial system.
As Jackendoff is well aware, at least one current framework
(Optimality Theory) proposes “a united grammatical framework
for syntax and phonology” (Smolensky 1999). Whether or not one
buys the theory itself, it seems highly likely that language’s two
combinatorial systems came in together, perhaps exploiting some
single underlying capacity, but more likely with phonology em-
ploying mechanisms derived directly or indirectly from syntax.
This pushes the third of Jackendoff’s stages to a post-protolan-
guage position.
“Concatenation of symbols” is supposed to constitute another
intermediate between call systems and protolanguage. But since
“language-trained” apes appear to have concatenated symbols
with no explicit training and minimal modeling, why is this stage
not implicit in the development of symbols? And why invoke, as a
distinct stage, “use of symbol position to convey basic semantic re-
lations”? In every variety of protolanguage I know of, such use is
not principle-based but merely a statistical tendency. The real evo-
lution in language was not from unordered symbols to regularly
ordered symbols to modern syntax. It was from concatenation in
linear strings to concatenation in hierarchical structures (Bicker-
ton 2002). Between these two types there is no intermediate,
therefore, not even the possibility of a gradual evolution from one
to the other.
Regarding post-protolanguage changes, I have already con-
ceded (Bickerton 2000, sect. 4) that the original two-stage model
has to be supplemented by a third stage, the grammaticization of
a morphologically bare syntax to enhance parsability. I see no point
in arbitrarily dividing this third stage into several sub-stages, as
Jackendoff does in his Figure 8.1, especially as Creole languages
quickly create both grammatical (albeit unbound) morphology
and symbols encoding semantic relations through demotion of
regular lexical items. Moreover, each hypothetical stage requires
its own selectional history; it will not do merely to suppose that any
improvement in a system is automatically selected for.
Whatever its defects, the three-stage model sought to ground
itself in known human-evolutionary developments and anchor it-
self at least provisionally in time. Jackendoff rejects these con-
straints (explicitly, in the case of time) in the belief that they “make
little difference” (p. 236). I’m sorry, they make a lot of difference.
The most striking fact about human evolution is the massive
cognitive and behavioral difference between our species and all
antecedent species. Moreover, most writers agree that language
was strongly contributory to, if not wholly constitutive of, that dif-
ference. But if language was evolving gradually over a long period,
as Jackendoff’s account implies, then why did improvements in
language yield no apparent changes in cognition or behavior until
the last hundred thousand years?
The gross mismatch between the archaeological record and any
gradualist account of language evolution is something that lin-
guists and nonlinguists alike have been studiously avoiding or
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
669
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

evading ever since I pointed it out more than a decade ago (Bick-
erton 1990). The cognitive discontinuity between humans and
prehumans precisely mirrors the linguistic discontinuity between
linear and hierarchical concatenation. Can this be mere coinci-
dence?
Whether it is or not, any gradualist account of language evolu-
tion that does not even try to explain why, if language evolved
gradually, human cognition and behavior did not evolve equally
gradually has little explanatory value. I do not wish to single out
Jackendoff in this respect. He himself says, “I see no need at the
moment to hold myself to a higher standard than the rest of the
field” (p. 237). But if somebody doesn’t do just that, we might as
well give up on language evolution.
Why behavior should matter to linguists
A. Charles Catania
Department of Psychology, University of Maryland at Baltimore County,
Baltimore, MD 21250. catania@umbc.edu
http://www.umbc.edu/psyc/personal/catania/catania.htm
Abstract: Jackendoff’s Foundations of Language: Brain, Meaning, Gram-
mar, Evolution has many points of similarity with Skinner’s analysis of ver-
bal behavior, though the former emphasizes structure whereas the latter
emphasizes function. The parallels are explored in the context of a selec-
tionist account of behavior in general and of verbal behavior in particular.
Part of the argument is that behavior drives evolution and therefore also
drives brain organization. Another concerns itself with the nature of ex-
planation. Recent experimental developments in behavior analysis are re-
viewed as potential contributions to an understanding of language that in-
corporates its functional as well as structural dimensions.
It is easy to see where the constructive collaboration Jackendoff
invites in his Preface (Jackendoff 2002) can be offered; but to pre-
sent the relevant material within brief compass is hard. Despite
many affinities outlined below, I argue that more is to be gained
by focusing on how linguistic structures can be illuminated by be-
havioral functions than by using linguistic structures to illuminate
hypothetical brain mechanisms.
It helps that Jackendoff places his account firmly within an 
evolutionary context, because evolution is driven by behavior.
Whether an organism survives and reproduces depends on what
it can do and the conditions under which it does it. Its environ-
ment consists not only of the physical world but also members of
its own and other species. Its brains and muscles and other organ
systems all evolved in the service of its behavior. Therefore, it is a
reasonable proposition that behavior drives brain structure, not
only through evolutionary contingencies that select behaving or-
ganisms with certain kinds of brains, but also through environ-
mental contingencies that shape different patterns of behavior
and alter brains within the lifetimes of individuals. Jackendoff ac-
knowledges this when he states that “perceptual systems have
evolved in order that organisms may act reliably in the real world”
(p. 308).1 But if behavior drives brain organization, behavior is the
place to start (Catania 1972; 1995b; 1997; Catania & Harnad 1988;
Skinner 1988).
Let us first dispose of some common misconceptions. Behavior
is not defined by muscle movements or by glandular secretions. It
is defined by function rather than form. Shifts of attention are be-
haviors, for example, even without overt movement; what matters
is that they are modified by their consequences. So, also, are see-
ing and looking. You can look without seeing and see without hav-
ing looked; both are subject to contingencies and either can occur
in the absence of visual stimulation (Jackendoff calls these actions
percepts, as in his bug example on pp. 311–12, but thinking of
them as actions rather than states has advantages).
In biology, studies of structure and function are respectively
called anatomy and physiology. Their priorities were once an issue
(Russell 1916). Behavior also has both structure and function. For
example, when a horse runs, muscle flexions combine to produce
coordinated leg movements that change with shifts from one gait
to another. All gaits, either natural (trotting) or trained (the rack),
are constrained by neurophysiological and mechanical factors and
constitute a grammar of the horse’s running. But that grammar is
orthogonal to function: for example, when and where the horse
runs; with which gait; what consequences follow. As organs differ
in anatomy and physiology, so also varieties of behavior differ in
what they look like and what they do. A horse may overtake an-
other at lope or gallop, and gallop in overtaking others or in es-
caping from predators. In the former, actions of different form
have similar functions; in the latter, actions of similar form have
different functions. Language too has both structure and function.
Beyond the structure-function distinction is the issue of selec-
tion. Within individual lifetimes behavior is selected by its conse-
quences, much as organisms are selected over generations by evo-
lutionary contingencies. Operants, classes of behavior selected by
their consequences, are fundamental units of behavior defined by
function. All operants participate in three-term contingencies in
which discriminative stimuli set occasions on which responses
have consequences (e.g., at traffic intersections, the consequences
that follow from stepping on the gas or the brakes vary with the
colors of the traffic lights). Parallels between natural selection in
phylogeny and in ontogeny have been explored in detail (Catania
1973a; 1973b; 1987; 1996b; Skinner 1935; 1938; 1981; Smith
1986). Behavioral accounts are often identified with S-R associa-
tions, but behavior analysis is a selectionist rather than associa-
tionist account (for a more detailed discussion, see Catania 1998;
2000).
The poverty of the stimulus (Chomsky 1959; Crain 1991) takes
on a different aspect in the context of selection. The selection of
operant classes by their consequences does not depend on exten-
sive sampling of negative instances. Consider the evolutionary
analogy: Populations are not selected from pools exposed to all
possible environments, and not all variations are included in the
pools upon which selection operates. It remains reasonable to con-
sider structural constraints on what is selected, but those con-
straints do not negate genealogy. As Darwin made abundantly
clear, both structure and function must be viewed through the lens
of selection.
Other biological analogies are also relevant. For example, or-
ganisms have been characterized as theories of their environ-
ments. Jackendoff exemplifies this view when he pushes the world
into the mind. But it is a risky move (Andresen 1990), and paral-
lel moves in biology have not fared well. For example, genetic ma-
terial is no longer said to carry blueprints of organisms, nor does
it reveal properties of the environments within which it was se-
lected; it is instead best regarded as a recipe for development
(Dawkins 1982). It is, similarly, a useful move to think of what is
remembered as a recipe rather than a blueprint for recall.
With these preliminaries, let us compare Jackendoff and Skin-
ner. In this undertaking, it is on the one hand not reassuring that
Jackendoff disposes of behaviorism with a 1913 reference to John
B. Watson (p. 280) and comments on Skinner only in passing with-
out citation (p. 372). On the other hand, it is intriguing that so
many of Jackendoff’s distinctions and categories have clear paral-
lels in Skinner’s (1957) account. Both present modular systems
and their modules are necessarily heterogeneous (cf. Jackendoff
2002, p. 160). Both consider how the modules can arise and how
they are coordinated with each other. When Jackendoff says
“reading, for example, acts like a module in an accomplished
reader, but it requires extensive training for most people in a way
that the phonology-syntax module does not” (p. 227), he parallels
Skinner’s textual, tact and echoic classes of verbal responses. Con-
sistent with the status of operant classes, Skinner’s modules are
based on considerations of function rather than form: “we cannot
tell from form alone into which class a response falls” (Skinner
1957, p. 186).
Both Skinner and Jackendoff wrestle with the problem of defin-
ing verbal classes in terms of reference or meaning or environ-
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
670
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

mental determinants, arguing that nouns and verbs cannot be
viewed as words corresponding to things and actions, respectively.
Both are concerned with how terms for nonexistent entities (e.g.,
unicorns) arise as extensions from verbal units acquired in the con-
text of real entities (e.g., horns), and with how entities that cannot
be pointed to can come into existence through words (e.g.,
months, nations). While doing so, both also grapple with the tran-
sition from the local and concrete to the abstract and metaphori-
cal (e.g., Jackendoff 2002, p. 299; Skinner 1957, pp. 91–116). Be-
cause his classes are defined by function, Skinner easily handles
cases where members do not share physical properties: “Some-
times a genuine extension seems to occur when no similarity be-
tween stimuli expressible in the terms of physical science can be
demonstrated” (1957, p. 97).
Both Jackendoff and Skinner reject chaining as a general basis
for generating utterances, consistent with Lashley’s (1951) argu-
ments about sequential structure, but both allow sequential de-
pendencies (rote sequences) in specific instances. That Skinner
made use of such a module, which he called intraverbal, demon-
strates that he did not regard such sequential processes as univer-
sal. Both discuss ways in which sequences, first generated as
chains, can become units in their own right. Both recognize that
some but not all large verbal units are constructed as they are pro-
duced (constructed online versus memorized).
Both deal with single-word “primitive” verbal units, such as
hello and ouch and psst. Both allow verbal units of varying size,
from phonemes through syllables and words to sentences and
larger forms. Both classify and interpret verbal devices such as
metonymy and metaphor, and both use errors and pathologies for
distinguishing among classes of verbal behavior.
Both are especially concerned with the coordinations that pro-
duce new verbal instances. Jackendoff deals with them in terms of
multiple components and interface rules, and Skinner, in terms of
multiple causation, the simultaneous determination of different
aspects of an utterance by different variables (as when participants
and timings and actions simultaneously determine sentence fea-
tures such as nouns and verbs and tenses). Compare Jackendoff
on generative components:
What is new in the present approach is that the idea of multiple gener-
ative components . . . has been extended in thoroughgoing fashion to
every part of the grammar, so that it becomes a fundamental architec-
tural design principle. (Jackendoff 2002, pp. 129–30)
and Skinner on the active editing of ongoing speech:
In the processes of composition and editing the speaker arranges, qual-
ifies, withholds, or releases verbal behavior which already exists in some
strength in his repertoire. Much of the behavior emitted upon any oc-
casion “just grows” – it springs from the current changing environment
and from other verbal behavior in progress. (Skinner 1957, p. 228)
Perhaps most significant, both deal with the hierarchical struc-
ture of verbal behavior, and, in particular, with nestings in which
higher order structures depend on the speaker’s awareness of
other levels (as in specifying one’s confidence in something said in
a phrase like “I am sure that . . .”). Some of these higher order
units cannot stand alone. Skinner (1957, pp. 311–43) wrote of
them as autoclitic processes – in the sense of verbal behavior that
leans upon other verbal behavior – and distinguished between re-
lational and descriptive forms further divided into qualitative and
quantitative and other dimensions (Catania 1980). Jackendoff
makes similar distinctions, though applying the relational and de-
scriptive labels somewhat differently. Though both discuss struc-
ture mainly in terms of rearrangements and transformations of
units, they also allow a role for frames within which units can be
placed (e.g., Jackendoff 2002, pp. 63, 176; Skinner 1957, pp. 336,
346).
When Jackendoff says “we must consider the domain of lin-
guistic semantics to be continuous with human conceptualization
as a whole” (p. 282), it is reminiscent of Skinner’s argument that
thinking and behavior are coextensive; when he says “the seams of
the mind must be determined empirically” (p. 283), he seems to
address what Skinner (1938) called natural lines of fracture in be-
havior.
The commonalities have not been exhausted (e.g., Jackendoff’s
activation and Skinner’s priming, or concern with verbal dimen-
sions like tone of voice or with the fuzzy boundaries of verbal
classes, or appeals to the practices of verbal communities). But it
is also crucial to acknowledge the vast differences, while noting
that the convergences evident in such divergent approaches may
themselves be of particular significance. Having already consid-
ered their different stances on brain and behavior, I concentrate
on modes of explanation.
Jackendoff often offers explanations, when what he has pro-
vided is description (e.g., pp. 336–42). But the relation between
sentence and structural diagram is similar to that between sen-
tence and paraphrase: Diagrams may make subtle structural fea-
tures easier to see and may help in taxonomic development, but
they do not specify where the features came from or how they
work or what effects they may have (cf. Skinner 1957, p. 388, on
paraphrase).
It is good that Jackendoff is explicit about rules being in the
heads of linguists rather than language users: “rules are nowhere
present in the f-mind in the form we write them. Rather, these
rules are indeed just descriptions of regularities in the organiza-
tion of linguistic memory” (p. 57; cf. p. 167). Structure alone can-
not justify explanatory appeals to conformity (p. 171), spontaneous
generation (p. 188), convenience of usage (p. 358), or insight
(p. 390).
I can only touch on the problems raised when language is in-
terpreted in terms of the metaphors of meaning and communica-
tion (Catania 1998, pp. 239–78; Lakoff 1987; Lakoff & Johnson
1980). Those ancient philosophers who thought that vision de-
pended on something traveling from the eye to the thing seen had
it backwards, but we can be similarly misled when our language of
reference leads us to speak of words as referring to things in the
world, and therefore to neglect the other direction, in which
events occasion what we say or provide conditions under which we
learn what words mean (cf. Day 1969; Wittgenstein 1953). Jack-
endoff occasionally seems to move in this direction: “We do not
have to worry about whether the phrase really refers, only about
how language users treat it” (p. 325, n. 24). Furthermore, to speak
of communication as the sharing of meanings is to neglect the ir-
reducible function of all verbal behavior, which is that it is a way
in which one individual can affect the behavior of another. This is
not to dispose of meaning and communication, but rather to rec-
ognize that both are derivatives of that more fundamental func-
tion (Catania 1990; 1991; 1995b; 2001).
With regard to description as a form of explanation, Jackend-
off’s statement that “We can determine properties of ‘language in
the world’ only through its manifestations in human linguistic in-
tuition and behavior” (p. 298) seems to share something with Skin-
ner’s (1957, p. 6) remark that: “There is obviously something sus-
picious in the ease with which we discover in a set of ideas
precisely those properties needed to account for the behavior
which expresses them.”
Skinner, instead, looks to the environment. In accounting for
the difference between offering a teapot to Nancy and offering
Nancy a teapot (p. 54), we need to know whether a teapot was of-
fered to Nancy, not Jane, or whether Nancy was offered a teapot,
not a teacup. The practices of verbal communities will be more
likely than brain structure to tell us whether the Frisbee on top of
the house has been roofed or rooved (p. 158). Though it is unusual
to say “John stayed the same distance from me he always stays”
(p. 321), the sentence may tell us more about how often we inter-
act with people with bodyguards than about how in general we talk
about distance. Verbal religious practices will tell us more about
the truth value of “God is a trinity” or “God is a man” than will
questions about how these sentences relate to the world (p. 294).
And the circumstances under which people say or respond to the
word “stop” may be more important than whether the word should
be regarded as a symbol (p. 239). If the above instances are to be
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
671
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

paraphrased or diagrammed, environmental antecedents should
be incorporated into those forms.
But commentary would be only of historical interest if it were
just that Jackendoff has developed a system whose features Skin-
ner had anticipated. It is more important that the behavioral
stance has since expanded to new topics that must be taken into
account. For example, Skinner hinted at how multiple causation
can yield productivity: “We turn now to a different type of multi-
ple control, in which functional relations, established separately,
combine possibly for the first time upon a given occasion” (Skin-
ner 1957, p. 229). But he did not go far enough. Experimental
studies have since addressed the spontaneous coming together of
responses learned separately, in the phenomenon called adduc-
tion (e.g., Catania et al. 2000; Esper 1973; Johnson & Layng 1992).
Shaping is another source of novel behavior, and variability itself
can be selected (Neuringer 2002; Pryor et al. 1969). Higher order
classes provide still another source (Catania 1995a; 1996a), illus-
trated by generalized imitation, as when a child imitates an action
never before seen or imitated (Baer et al. 1967; Gewirtz & Stin-
gle 1968; Poulson & Kymissis 1988; Poulson et al. 1991). Other
higher order examples are those of equivalence classes, in which
new behavior emerges from reflexivity, symmetry, and transitivity
relations among the members of stimulus sets (Catania et al. 1989;
D’Amato et al. 1985; Dube et al. 1993). These relations cannot be
derived from stimulus properties, and so can only be dealt with in
terms of the environmental contingencies that created them
(Catania 1996b; Vaughan 1988). They are of particular relevance
for interpreting relations among words and other events (in other
words, meanings), and provide an easy bridge to many hierarchi-
cal structures discussed by Jackendoff.
Other extensions grounded in experimental findings are to the
roles of echoic behavior and of responses to pointing in the de-
velopment of naming in children (Horne & Lowe 1996), func-
tional effects of naming (Wright et al. 1990), developmental tran-
sitions from nonverbal to nonverbal behavior (Bentall & Lowe
1987; Bentall et al. 1985; Moerk 1992), the shaping of verbal be-
havior and correlated changes in subsequent nonverbal respond-
ing in verbal governance (Catania, 2003; Catania et al. 1982; 1990;
Chadwick et al. 1994; Greenspoon 1955; Lovaas 1964; Rosenfarb
et al. 1992; Shimoff & Catania 1998; Skinner 1969), and ways in
which verbal governance depends on differential attention to dif-
ferent kinds of verbal stimuli, as when the bringer of bad news is
poorly received (Dinsmoor 1983).
Jackendoff has offered “an open-mindedness to insights from
whatever quarter” (p. xiii) and has asked for “all the help we can
get from every possible quarter” (p. 429), so my hope is that the
news offered here in return will not be poorly received. The be-
havioral bathwater is gone but the baby has thrived and is ready
to rejoin the company of linguists to help them with their work.
NOTE
1. Unless otherwise noted, pages refer to Jackendoff (2002).
“Grammar box” in the brain
Valéria Csépe
Department of Psychophysiology, Group of Developmental
Psychophysiology Research Institute for Psychology of the Hungarian
Academy of Sciences, Budapest, H-1068 Hungary. csepe@cogpsyphy.hu
http://humlab.cogpsyphy.hu/humlabe.htm
Abstract: Brain activity data prove the existence of qualitatively different
structures in the brain. However, the question is whether the human brain
acts as linguists assume in their models. The modular architecture of gram-
mar that has been claimed by many linguists raises some empirical ques-
tions. One of the main questions is whether the threefold abstract parti-
tion of language (into syntactic, phonological, and semantic domains) has
distinct neural correlates.
There is a growing number of data-giving evidence on brain spe-
cialization for language, although many language processes, in
spite of their distinct function in the architecture, cannot be local-
ized to just one particular area of the brain. However, as we know
from brain measures and especially from brain-imaging data, one
particular area or part of the network is involved in different tasks,
and there is a spatial and temporal overlapping of the processes.
Brain-activity data seem to prove the existence of qualitatively dif-
ferent structures in the brain processing phonological, syntactic,
and semantic information. However, the question is whether the
human brain acts as linguists assume it does in their models.
Jackendoff has many well-elaborated questions about the ner-
vous system serving language functions, eight of them listed in his
concluding remarks (pp. 422–23). His questions will attract the at-
tention of neuroscientists, as Chomsky’s concept of Universal
Grammar has given place to discussions and studies on relating 
abstract entities with physiological correlates. According to Jack-
endoff’s statement, Universal Grammar is a limited set of “at-
tractor” structures that guides language acquisition through in-
heritance. However, the question is what do we mean with
inheritance, innateness, and wiring, when referring to the biolog-
ical relevance of Jackendoff’s reconfigured generative grammar.
New findings in genetics further strengthen the belief that lan-
guage is specified by biological factors. The recent discovery of the
FOXP2 gene (Lai et al. 2001) supports the assumption of linguists
that the development of language is set by innate factors. As re-
vealed by the data of Cecilia Lai and her coworkers, a mutant ver-
sion of the FOXP2 within chromosome 7 provokes Specific Lan-
guage Impairment (SLI). However, the FOXP2 data may irritate
some linguists rather than satisfy them, because SLI is a hetero-
geneous class of verbal disturbances and does not correspond to a
single domain of rule applications. Therefore, I think, Jackendoff
is correct when he refers to a language toolkit, and assumes innate
capacities instead of a language system lodged in the brain.
The modular architecture of grammar claimed by many lin-
guists raises some empirical questions. One of the main questions
is whether the threefold abstract partition of language (into syn-
tactic, phonological, and semantic domains) has distinct neural
correlates. There are experimental data that prove semantic in-
formation has a distinct representation in the brain. Another fun-
damental question is whether syntactic processing is associated
with dedicated neural networks. Syntactic processing during sen-
tence reading has been investigated in several functional neu-
roimaging studies and showed consistent activation of the pars op-
ercularis of Broca’s area (Caplan et al. 1998; Just et al. 1996).
However, sentences presented in the auditory modality (Caplan
et al. 1999) lead to activation of the pars triangularis. Moreover,
in visual tasks the anterior cingulate gyrus and the right medial
frontal gyrus were activated. This finding was interpreted as a cor-
relate of phonological encoding and subvocal rehearsal. A current
study by Newman et al. (2003) adds further empirical evidence to
partly distinct networks specialized for syntactic and semantic pro-
cessing. Their fMRI data suggest that separable subregions of the
Broca’s area contribute to thematic and syntactic processing. In
their study, the pars triangularis was more involved in thematic
processing and the pars opercularis in syntactic processing.
Dapretto and Bookheimer (1999) tried to separate the syntac-
tic and lexicosemantic processing in an fMRI experiment. In the
semantic condition single words, in the syntactic condition full se-
quences, were changed. The authors used passive constructions
for syntactic change; and, I am sure Jackendoff would argue, pas-
sive constructions do not necessarily preserve the semantic con-
tent of their active counterpart. In spite of the assumed semantic
change in the passive construction, Dapretto and Bookheimer
(1999) found activation in the Broca’s pars opercularis. In a recent
study, Moro et al. (2001) applied syntactic, morphosyntactic, and
phonotactic tasks for “pseudosentences” and found activation in
the Broca’s area pars opercularis and in the right inferior frontal
region during syntactic and morphosyntactic processing. A local
network shared by morphological and syntactic computations
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
672
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

proves that syntax and morphosyntax are closely related in the
brain, as it is assumed in the model of modern architecture of lan-
guage proposed by Jackendoff (p. 261). However, this does not
mean that syntactic capacities are implemented in a single area.
The lack of complete overlap of brain areas involved in syntactic
and morphosyntactic processing is in agreement with most of the
linguistic models. It must be underlined, however, that the role of
working memory in syntactic processing is more or less ignored by
the linguistic models. It seems to be “understandable” if we take
into account the complexity of the relationship of working memory
and sentence comprehension. Working memory may play a differ-
ent role in assigning the syntactic structure of a sentence, and in us-
ing this structure to determine the meaning of it. The complex re-
lationship of syntactic complexity and working memory load is
proven by patients’ data. Pickett et al. (1998) report on a patient with
mild Parkinsonism who showed perseverations in rule applications,
impaired comprehension in sentence meaning conveyed by syntax,
and intact verbal and visual short-term memory. The striking disso-
ciation shown by the patient was that her sentence-comprehension
performance increased proportionally with syntactic complexity.
We may assume that the most probable areas playing a crucial role
in such a memory-syntax interface are frontal regions of the cortex.
Jackendoff mentions the possible role of working memory (WM)
in language processes several times in his book and his most elabo-
rate remarks are related to the distinction between Baddeley’s WM
model and his own linguistic working-memory concept. I agree
working memory is not just a “shelf where the brain stores mater-
ial” (p. 207), but also a workbench that has a complex relationship
with constructing verbal structures. From this point of view, Bad-
deley’s model has a limited capacity in explaining the relationship
between WM and the integrative and interface processes.
However, a different model of working memory from Just and
Carpenter (1992) may fit better with Jackendoff’s parallel grammar
model. In the Just and Carpenter model of functional working
memory, henceforth referred to as f-WM, storage is defined as
temporal retention of verbal information already processed, while
processing is defined as computations generating various types of
linguistic representations (lexical, morphological, grammatical). In
one of the f-WM studies by Montgomery (2000), the relation of
WM and immediate processing of simple sentence structures was
investigated in SLI children and two control groups, age matched
and receptive syntax-matched controls. The SLI group showed
deficits in all f-WM tasks and was very slow as compared to the con-
trol groups. However, immediate processing of simple sentences
does not rely heavily on f-WM resources, so the problem may be
more related to integrating the resources associated with different
subsystems of the linguistic working memory.
Given the distinctions between Baddeley’s WM model and the
f-WM model we may assume that the f-WM model is closer to
Jackendoff’s assumption on linguistic working memory than to
Baddeley’s previous or recent models (Baddeley 2003). The Just
and Carpenter model assumes that items activated in the working
memory are integrated into larger chunks. The model is not far
from that of Jackendoff’s idea on the linguistic working memory
included in the parallel grammar that heavily relies on item inte-
gration. The task of neuroscience would be to shed light on possi-
ble neural functions related to the subsystems assumed. If Jack-
endoff is right about the integrative function of linguistic working
memory as an inherent part of the three linguistic structures, brain
activity correlates should be associated with it. It is really mysteri-
ous how the items retrieved from long-term memory undergo
transient processing in working memory and how they are related
to brain mechanisms. However, I do think that the problem is that
we haven’t yet found the right experimental paradigms for inves-
tigating these processes.
ACKNOWLEDGMENTS
The author’s work is supported by a research grant given by the National
RD Program of the Hungarian Ministry of Education (Project No. 5/054).
Beyond beanbag semantics
Daniel C. Dennett
Center for Cognitive Studies, Tufts University, Medford, MA 02155.
ddennett@tufts.edu
http://ase.tufts.edu/cogstud/
Abstract: Jackendoff’s “mentalistic” semantics looks more radical than it
is. It can best be understood as a necessary corrective to the traditional
oversimplification that holds that psychological variation “cancels out” on
the path from word to world. This reform parallels the “evo-devo” reform
in evolutionary biology.
Mendel’s genes were a brilliant simplification that permitted many
of the fundamental principles and constraints of inheritance to be
clearly described and tested. But if you took them too literally,
imagining them to have exact counterparts lined up like simple
beads strung on the chromosomes, you got “beanbag genetics,” as
Ernst Mayr once dismissively called it. The working parts of the
DNA inheritance machinery encountered in contemporary mole-
cular genetics are so much more subtle and active than Mendelian
genes, that some would declare that genes – the genes Mendel in-
troduced to us – do not exist at all! Eliminative materialism re-
garding genes in the Age of Genes? An unlikely terminological re-
form. We don’t throw the Mendelian ladder away; we continue to
use it, with due circumspection and allowances (Crow 2001; Hal-
dane 1964).
Jackendoff’s masterpiece Foundations of Language (Jackend-
off 2002) poses a counterpart question: Isn’t it time to trade in
Chomsky’s pathfinding syntactocentric vision for something more
complex in some ways and more natural in others? In the syntac-
tocentric picture, a word is a simple, inert sort of thing, a sound
plus a meaning sitting in its pigeonhole in the lexicon waiting to
be attached to a twig on a syntactic tree. In Jackendoff’s alterna-
tive vision, words are active: “little interface rules” (target article,
sect. 9.3, para. 6) with lots of attachment prospects, links, con-
straints, affinities, and so on, carrying many of their combinator-
ial powers with them. Jackendoff’s proposed parallel architecture,
with its three simultaneous and semi-autonomous generative
processes, is biologically plausible, both neuroscientifically and
evolutionarily. It opens up a space for theory modeling in which
hypotheses about opponent processes, recurrence, and other sorts
of mutual interaction, can be formulated and tested. The Univer-
sal Grammar (UG) doesn’t need to be written down as rules to be
consulted. It is partly embodied in the architecture, and partly
fixed by culturally evolved attractors homed-in on by individual
learning. The epicycles of syntactocentric theories largely evapo-
rate, as the division of labor between syntax, semantics, and
phonology gets re-allotted.
Any revolution is apt to look more outrageous in prospect than
it turns out to be in retrospect. I would like to propose a friendly
amendment, softening the blow of Jackendoff’s “mentalistic” se-
mantics. Semantics, as traditionally conceived by logicians,
philosophers, and linguists, is where the rubber meets the road,
where language gets all the way to the world and words refer to
the things and events therein. The winding path by which a word
“gets to” the world, when it does, surely lies in the mind (or brain)
of a language user, but tradition has it that this messy intermedi-
ary can and should be largely ignored. There are several influen-
tial bad arguments as to why this should be so, but here’s one that
can stand for them all:
“My uncle is suing his stockbroker.” When you hear that sen-
tence, and understand it, you perhaps engage in some imagery,
picturing an adult male (in a suit?) with some papers in his hand,
confronting, somehow, some other man (why a man?), and so on.
There would no doubt be wide variation in the imagery in the
minds of different hearers, and some might claim that they en-
gaged in no imaging at all and yet still understood the sentence
just fine. Moreover, such imagery as people did indulge in would
be unable on its own to fix the meaning of the sentence (there is
nothing an uncle looks like that distinguishes him from a father or
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
673
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

brother). Clearly, goes the argument, the idiosyncrasies of imagery
or other mental processes we each indulge in are irrelevant to the
issue of semantics – the word-world relation that specifies, some-
how, the set of objects in the world correctly referred to by “un-
cle,” “sue,” and “stockbroker.” So, we cancel out all the conflicting
and irrelevant mental states and processes and leave the messy
minds out of semantics altogether. In any case, since we semanti-
cists have to get all the way to the world in the end, it won’t do to
stop short in the mind (or the brain), so why tarry?
This is strikingly like the justification that has been offered by
evolutionists for habitually ignoring developmental biology: We
choose to go from the gene directly to the adaptation, the pheno-
typic structure or behavior that is actually selected for, because
that is, in evolution, where the rubber meets the road. A gene for
x, a gene for y, and we can postpone indefinitely the tricky job of
charting the winding path from gene transcription to operational
phenotypic asset. This is in fact a very valuable simplification, but
it can be overdone. Reacting against it – today’s “evo-devo” band-
wagon – can overshoot, too.
Jackendoff says, in italics, “it is necessary to thoroughly psy-
chologize not just language, but also ‘the world’” (p. 294) and adds:
“the perceptual world is reality for us” (p. 308). As he recognizes,
this looks as if he’s stopping semantics in the brain, saddling his
brilliant view of language with some weird sort of materialistic ide-
alism. Let me try to put the matter more mundanely. Most people
go through life without ever giving semantics any thought. You
don’t have to figure out the semantics of your own language to use
it, but if you do try to, you soon discover the set of issues that ex-
ercise Jackendoff. It helps keep the quandaries at bay to go het-
ero-, to do the semantics of some other guy’s language (and mind).
Like this:
The words of his language refer to things. We mustn’t presup-
pose that his semantic system matches ours – the meta-language
we use to describe his psychology. If we want to say what his words
refer to, we have to see how his brain is designed by evolution (in-
cluding cultural evolution) and by individual learning, to parse out
his perceptual and conceptual world. Once we’ve done this we can
ask: Do his terms refer to things in the world as we parse it, or
“just” to things in the world as he experiences it (and as his con-
specifics and companions experience it)? (For if there is a lan-
guage, there is a shared system even if it isn’t our shared system.)
If the former is true, then we share the world with him; our man-
ifest image (Sellars 1963) is (roughly) the same as his, and theirs.
If not, then we have to maintain something like scare-quotes when
we refer to the “things” in his world. But either way, we eventu-
ally get all the way out to the world – where the rubber meets the
road. What we can’t express in our terms, we can describe in our
terms.
Jackendoff insists, rightly in my opinion, that it is only by taking
this indirect path that analyzes the manifest image implicit in the
language-users’ brains that we can complete the task of linguistics.
For most purposes, however, we can continue using the traditional
semantical talk about the word-world relation, just as biologists
can continue to talk about genes for myopia or even dyslexia
(Dawkins 1982; Dennett 1995), because we know how to take the
longer, more complicated path when necessary.
A conceptuocentric shift in the
characterization of language
Peter Ford Dominey
Institut des Sciences Cognitives, CNRS UMR 5015, 69675 Bron, France.
dominey@isc.cnrs.fr
http://www.isc.cnrs.fr/dom/dommenu.htm
Abstract: Recognizing limitations of the “syntactocentric” perspective,
Jackendoff proposes a model in which phonology, syntax, and conceptual
systems are each independently combinatorial. We can ask, however,
whether he has taken this issue to its logical conclusion. The fundamental
question that is not fully addressed is whether the combinatorial aspect of
syntax originated in, and derives from, the indeed “far richer” conceptual
system, a question to be discussed.
In Foundations of Language, Jackendoff (2002) has undertaken
what is finally a rather profound reconfiguration of the generative
framework in a manner that allows a potentially much more in-
teresting interaction with related aspects of the other cognitive
sciences. Recognizing limitations of the “syntactocentric” per-
spective, in which the free combinatoriality of language is attrib-
uted to syntax alone, Jackendoff proposes to correct the situation
by promoting a model in which phonology, syntax, and the con-
ceptual system are each independently combinatorial.
Of particular interest is the status of the conceptual system as a
“combinatorial system independent of, and far richer than, syn-
tactic structure” (p. 123) in the parallel architecture, and the re-
sulting questions concerning the functional relation between the
conceptual and the syntactic components. In this aspect, Jack-
endoff has initiated an interesting debate, but in a certain sense
he has failed to take his position to its logical conclusion. The fun-
damental question that is not fully addressed is whether the com-
binatorial capability originated in the indeed “far richer” concep-
tual system. This is consistent with the consideration that language
arose primarily to enhance communication (p. 236) of thoughts,
which assumes the precondition of a combinatorial conceptual
structure system (p. 238).
If the combinatoriality of language serves the purpose of trans-
mitting messages constructed from an equally combinatorial sys-
tem of thoughts (p. 272, and Ch. 3), then the precedence for com-
binatoriality appears to lie in the thought or conceptual system. In
this case, it would have been more interesting to see Chapter 3 on
combinatoriality organized around the combinatoriality of the
conceptual system, with an analysis of the extent to which the
combinatoriality of syntax derives from that of its predecessor.
In any event, Jackendoff’s view of the conceptual system invites
one to consider things from a more conceptuocentric perspective.
Indeed, Jackendoff notes that (p. 417) “languages differ in their
syntactic strategies for expressing phrasal semantics; but the or-
ganization of what is to be expressed seems universal,” again sug-
gesting that the origin of the universal combinatorial capacity lies
more in the independent combinatorial capability of the concep-
tual system than in syntax. In this context, one could consider the
syntactic integrative processor as an algorithm for reading or tra-
versing the conceptual structure data structure in order to gener-
ate a linear string that would be processed in parallel by the phono-
logical integrative processor. In this sense, the observed generative
component of syntax would derive from that of the conceptual sys-
tem. Indeed, on page 417 Jackendoff indicates that “what is part
of Universal Grammar, of course, is the architecture of the inter-
face components that allow conceptual structures to be expressed
in syntactic and phonological structures.” The interesting part of
what is universal then, is the conceptual system and its interfaces.
If this were the case, then the syntactic integrative processor
would perform an interface between conceptual and phonological
structures. This perspective focuses on the relation between the
structure of language and the structure of meaning, more than the
syntactocentric approach does. In this context, one would expect
a certain degree of isomorphism between conceptual structures
and the linguistic structures that communicate them. Jackendoff
thus notes that for “simple compositional” structure based on ar-
gument satisfaction, modification, and lambda extraction and vari-
able binding, there is a “close correspondence between the con-
figurations of lexical items in syntax and conceptual structure”
(p. 387). Enriched composition such as the reference transfer de-
picted in Nunberg’s (1979) sentence “The ham sandwich over in
the corner wants more coffee” manifests situations in which this
iconicity is claimed to break down. Indeed, the development and
use of this type of “verbal shorthand” will lead to the development
of grammatical constructions that partially circumvent iconicity,
here simply referring to an individual by his or her most contex-
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
674
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

tually salient property, his or her restaurant order. Still, from a de-
velopmental perspective, we should explore how far the infant can
go with the “close correspondence” hypothesis.
The issue of the source of compositionality in the conceptual
system or in syntax is not a trivial issue, as it has massive impact on
learnability. Learnability issues generally evoked in the “poverty
of stimulus” framework focus largely on the complexity of induc-
ing regularities derived from syntactic compositionality. This com-
plexity could be significantly reduced if the compositionality were
already present in the conceptual system. In this context, acquisi-
tion does not necessarily imply that the child perform a (demon-
strably impossible) task of grammar induction on reduced input
(see target article, sect. 4.6). Rather, it implies that the child learns
how to interpret the meaning of sentences by any method. In his
discussion of lexical storage versus online construction (p. 188)
Jackendoff outlines an approach in which the infant initially is
“storing everything,” and begins to generalize regular patterns
“and extract explicit patterns containing typed variables;” allowing
the system to “go productive,” via variable-based structures simi-
lar to those discussed by Marcus (2001). The resulting lexical con-
struction-based developmental trajectory described in section 6.9
makes interesting contact with the usage-based account of lan-
guage acquisition as developed by Tomasello (1999b; 2003). In
making this connection, Jackendoff has quietly performed a re-
markable stunt in theoretical diplomacy, by (at least partially) in-
tegrating the construction grammar framework into the parallel
architecture.
What becomes interesting from this dual perspective of (1) the
combinatorial precedence of the conceptual system, and (2) the
use of a construction grammar style approach as suggested in
Chapter 6, is the potential reduction in the processing complexity
associated with language acquisition. Across languages, meaning
is encoded by individual words, word order, grammatical marking,
and prosody (Bates & MacWhinney 1982). Within a language,
grammatical constructions will be identifiable based on their char-
acteristic configurations of these cues. These grammatical con-
structions will each have their respective form-to-meaning corre-
spondences – which the learner is expected to acquire. Thus, the
mappings can be learned and subsequently accessed, based on the
configuration of grammatical cues that serves as an index into the
lexicon of stored constructions. A model based on these principles
made interesting predictions concerning the neural bases of these
operations (Dominey et al. 2003), and has also been effective in
miniature language acquisition contexts, in which grammatical
constructions are learned and productively generalized to new
sentences (Dominey 2000; 2003). This suggests that when the
brunt of the compositional load is put on the conceptual repre-
sentation, a reliable scaffolding is thus in place, upon which syn-
tactic compositionality may naturally repose.
Generative grammar with a human face?
Shimon Edelman
Department of Psychology, Cornell University, Ithaca, NY 14853-7601.
se37@cornell.edu
http://kybele.psych.cornell.edu/~edelman/
Abstract: The theoretical debate in linguistics during the past half-cen-
tury bears an uncanny parallel to the politics of the (now defunct) Com-
munist Bloc. The parallels are not so much in the revolutionary nature of
Chomsky’s ideas as in the Bolshevik manner of his takeover of linguistics
(Koerner 1994) and in the Trotskyist (“permanent revolution”) flavor of
the subsequent development of the doctrine of Transformational Gener-
ative Grammar (TGG) (Townsend & Bever 2001, pp. 37–40). By those
standards, Jackendoff is quite a party faithful (a Khrushchev or a Dubcek,
rather than a Solzhenitsyn or a Sakharov) who questions some of the com-
ponents of the dogma, yet stops far short of repudiating it.
In Foundations of Language, Jackendoff (2002) offers his version
of TGG, in which the primacy of syntax (“an important mistake,”
p. 107) is abolished, the related notions of Deep Structure and
Logical Form (“the broken promise,” cf. Précis, sect. 3) are set
aside, the links to other domains of cognition are discussed, and
a hand is extended in peace to psychologists and other cognitive
scientists. Foundations is an enjoyable, thought-provoking and
useful book that fulfills the promise of its title by presenting – and
attempting to tackle – foundational issues in linguistics. It is an
excellent overview of the ground that must be covered by any se-
rious contender for a linguistic “theory of everything.” Its non-
dogmatic style engages skeptical readers of cognitive and empiri-
cist persuasions (“can my theory explain this set of facts better?”)
instead of alienating them.
Among the more positive aspects of Jackendoff’s stance in Foun-
dations are: the emancipation of semantics as one of the three
equal-status components of the “parallel architecture” (p. 125); the
realization that not all rules are fully productive (admitting con-
structions p. 189); and the construal of meaning as a system of con-
ceptual structures (p. 306). The pervasiveness of TGG dogma is,
however, very prominent throughout the book. On the most ab-
stract level, the dogma manifests itself in the bizarre mentalistic
nomenclature ( f-knowledge, etc.) that Jackendoff uses instead of
the standard explanatory machinery of representation found in all
cognitive sciences. Jackendoff shuns a representational account of
linguistic knowledge because of his (understandable) wish to avoid
joining Fodor and Searle in the philosophical quagmire of inten-
tionality. There exist, however, psychophysically and neurobiolog-
ically plausible accounts of symbolic representation that hinge on
counterfactual causality and manage to stay clear of the Fodorian
mire (Clark 2000; Edelman 1999).
The preponderance of Chomskian bricks in Foundations is re-
vealed in Jackendoff’s official insistence, in the introductory chap-
ters, on rule-based combinatoriality. His initial formulation of this
concept (pp. 38–57) is so strong as to be incompatible with his
own views on constructions (pp. 152–87) and on their graded en-
trenchment (p. 189), expressed later in the book. It is satisfying to
observe that those latter views are on a convergence course with
some of the best-known and most promising work in cognitive lin-
guistics (Goldberg 1998; Langacker 1987). As such, they can stand
on their own: Computationally explicit construction-based ac-
counts of linguistic productivity need no extra propping (Solan et
al. 2003). In any case, Jackendoff should not count on any help
from TGG, a Protean theory that, despite decades of effort, has
failed to garner empirical support for the psychological reality of
the processes and entities postulated by its successive versions,
such as movement and traces (Edelman, in press; Edelman &
Christiansen 2003). In a recent attempt to obtain psycholinguistic
evidence for traces, for example (Nakano et al. 2002), only 24 sub-
jects out of the original 80 performed consistently with the pre-
dictions of a trace/movement theory, while 39 subjects exhibited
the opposite behavior (the data from the rest of the subjects were
discarded because their error rate was too high). Jackendoff’s con-
tinuing to cling to TGG (complete with movement and traces), de-
spite its empirical bankruptcy and despite his self-proclaimed
openness to reform, is difficult to explain.
Even Jackendoff’s highly commendable effort to treat seman-
tics seriously may be undermined by his continuing commitment
to TGG. Conceptualist semantics is an exciting idea, but to de-
velop it fully one must listen to what cognitive psychologists have
to say about the nature of concepts. Instead, Jackendoff erects his
own theory of concepts around scaffolding left by the generative
linguists, which, in turn, is only as sound as those decades-old in-
tuitions of Chomsky and Fodor. In particular, incorporating Marr’s
and Biederman’s respective theories of visual structure (pp. 346–
47), themselves patterned on TGG-style syntax, into the founda-
tions of semantics cannot be a good idea. Jackendoff’s acknowl-
edgment, in a footnote 10 on p. 347, that Marr is “out of fashion”
with the vision community holds a key to a resolution of this issue:
Current perceptually grounded theories of vision (Edelman 1999;
2002) and symbol systems (Barsalou 1999) are a safe, additive-free
alternative to TGG-style semantics.
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
675
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

In summary, Jackendoff’s book is one of several recent mani-
festations in linguistics of the equivalent of the Prague Spring of
1968, when calls for putting a human face on Soviet-style “social-
ism” began to be heard (cf. the longing for “linguistics with a hu-
man face” expressed by Werth [1999, p. 18]). Jackendoff’s stance,
according to which the “mistakes” that were made do not invali-
date the TGG framework, amounts to a bid to change the system
from within. In a totalitarian political system, this may only work
if the prime mover behind the change is at the very top of the
power pyramid: Czechoslovakia’s Dubcek in 1968 merely brought
the Russian tanks to the streets of Prague, whereas Russia’s Gor-
bachev in 1987 succeeded in dismantling the tyranny that had sent
in the tanks. In generative linguistics, it may be too late for any fur-
ther attempts to change the system from within, seeing that pre-
vious rounds of management-initiated reforms did little more than
lead the field in circles (Edelman & Christiansen 2003). If so,
transformational generative grammar, whose foundations Jack-
endoff ventures to repair, may have to follow the fate of the Com-
munist Bloc to clear the way for real progress in understanding
language and the brain.
Complexity underestimated?
Péter Érdi
Center for Complex Systems Studies, Kalamazoo College, Kalamazoo, MI
49006; and Department of Biophysics, KFKI Research Institute for Particle
and Nuclear Physics of the Hungarian Academy of Sciences, H-1525
Budapest, Hungary. perdi@kzoo.edu
erdi@rmki.kfki.huhttp://cc.kzoo.edu/~perdi
http://www.rmki.kfki.hu/biofiz/cneuro/cneuro.htm
Abstract: Instead of commenting directly on Foundations of Language:
Brain, Meaning, Grammar, Evolution, I provide some remarks from an in-
terdisciplinary view. Language theory is examined from the perspective of
the theory of complex systems. The gestural-vocal dichotomy, network
theory, evolutionary mechanisms/algorithms, chaos theory, and construc-
tive approach are briefly mentioned.
1. The perspective. I do not have a background in generative
linguistics, and read the book Foundations of Language (Jack-
endoff 2002) from the perspective of how the author managed to
embed linguistics into an interdisciplinary framework. I remain
slightly disappointed. The author clearly abandoned Chomsky’s
grand isolation decades ago, but the real integrative approach is
missing. For example, the title of the first chapter is “The Com-
plexity of Linguistic Structure,” but the author gives only a few ref-
erences from the community of complex-system researchers. Still,
though the book seems to be primarily a text written by a linguist
for linguists, I have learned very much from it. My comments here
are directed not so much at the book itself as at articulating the
potential ingredients for a more interdisciplinary approach.
2. The gestural-vocal dichotomy. Jackendoff assumes that lan-
guage arose in the vocal-auditory modality, and states (in my view,
surprisingly) that “a gesture-visual origin would not materially
change my story” (p. 236). Based on the fascinating findings of
mirror neurons (for reviews, see Rizzolati & Arbib 1998), the mir-
ror system hypothesis of language evolution has been suggested
(e.g., Arbib 2002a). Mirror neurons in monkeys are active both in
order to execute motor actions and to observe similar actions of
other monkeys or humans. The neural region involved in these op-
erations is considered to be the homologue of Broca’s area, the
crucial speech area of humans. Language in humans evolved from
a basic mechanism that was originally not related to communica-
tion, namely, the “capacity to recognize actions” (Rizzolatti & Ar-
bib 1998). Should we believe now, in light of these newer results,
that the gestural-visual systems implemented in the action-per-
ception cycle might have a more important role in language evo-
lution than was earlier thought? While I might see the difficulties
in explaining the transfer from gestural to vocal modality, I don’t
see why we should not consider these findings as a big step toward
a new Neurolinguistics.
3. Network theory: Static and (statistical) characterization;
self-organizing algorithms. Real world systems in many cases can
be represented by networks, and complex networks can be seen
everywhere. The organization of biological, technological, and so-
cial structures might be better understood by using network the-
oretical approaches (Albert & Barabasi 2002; Newmann 2003).
“Small-world” graph properties (highly clustered and small aver-
age length between nodes) and power-law distributions are the
key properties of the networks. Complex networks are neither
purely ordered nor purely random.
Motivated by the big success of network theory, several works
have shown that certain networks assigned to human language
have the characteristic patterns of complex organization. Cancho
and Solé (2001) analyzed the British National Corpus, and a net-
work of interacting words has been constructed by taking into ac-
count only short-distance correlations. The authors don’t deny
that their algorithm is based on the analysis of the surface struc-
tures of sentences. Another network of words was constructed
from a thesaurus by Motter et al. (2002). Roughly speaking, words
are connected if they express “similar” concepts. In any case, both
networks showed statistical properties very similar to those of
other complex networks.
Dorogovtsev and Mendes (2001) gave a self-organizing algo-
rithm for the development of word networks based on elementary
interactions between words. This algorithm might be the basis of
a mechanism to produce a kernel lexicon of the language.
4. Evolutionary mechanisms/algorithms. Jackendoff certainly
gives some credit to recent work “on mathematical and computa-
tional modeling of communities of communicating organisms”
(p. 81). At least from the perspective of integrative approaches, it
is interesting to see how model frameworks of population dynam-
ics and evolutionary game theory can be extended to describe lan-
guage evolution (e.g., Nowak & Krakauer 1999), and specifically
grammar acquisition (Komarova et al. 2001), which offers a model
framework for describing signal-object association, word forma-
tion, and the emergence of syntax with coherent concepts.
5. Chaos theory. Chaos theory might have some role in lin-
guistics. It certainly contributed to the explanation of the occur-
rence of the celebrated Zipf’s law (Nicolis & Tsuda 1989). (I un-
derstand that statistical-empirical laws might have nothing to do
with architectures, so Zipf’s law should not necessarily be men-
tioned in the book.) The population-dynamical/game-theoretical
models elaborated for the acquisition and evolution of language
might lead to chaotic behavior under certain conditions. Mitch-
ener and Nowak (2003) recently argued that small learning errors
may lead to unpredictable language changes.
6. Constructive approach. While there are different strategies
to simulate language evolution, the constructive approach seems
to be particularly interesting (e.g., Hashimoto 2001). Language, as
a complex dynamical system, can be studied at different hierar-
chical levels. The origin of the first linguistic systems, the evolu-
tion of various languages and language structures, the normal de-
velopment and acquisition of language in children and adults, and
the sense-making process of giving meanings to words during
communication take place in different levels of language organi-
zation. The constructive approach takes into account both the sub-
jective language-users and the communication among them. The
prerequisites of simulating language evolution are language-users,
that is, communicative individuals with an established communi-
cation system.
Recent efforts to understand emergent biological and social
structures adopt the constructive approach. Accordingly, struc-
tures and processes emerge as a result of the interaction between
the components of complex systems. Specifically, one can under-
stand the emergence of linguistic structures and behaviors. These
components consist of interacting autonomous agents, their
neural, sensorimotor, cognitive, and communication abilities, and
their physical and social environment.
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
676
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

Jackendoff might be right: “Linguistics alone cannot sustain the
weight of the inquiry. We need all the help we can get from every
possible quarter” (p. 429).
7. Afterthought. Jackendoff’s Foundations is a result of an in-
credible intellectual effort. I am very curious to see how the au-
thor reacts to remarks coming from an external world.
ACKNOWLEDGMENTS
Support from the Henry R. Luce foundation, and the National Science Re-
search Council (OTKA) grant No. T038140 are acknowledged.
Imaginary mistakes versus real problems in
generative grammar
Robert Freidin
Program in Linguistics, Princeton University, Princeton, NJ 08544.
freidin@princeton.edu
Abstract: Jackendoff claims that current theories of generative grammar
commit a “scientific mistake” by assuming that syntax is the sole source of
linguistic organization (“syntactocentrism”). The claim is false, and fur-
thermore, Jackendoff’s solution to the alleged problem, the parallel archi-
tecture, creates a real problem that exists in no other theory of generative
grammar.
Jackendoff’s Foundations of Language (Jackendoff 2002) begins
with a polemic about a perceived “scientific mistake” in standard
generative grammar, which is corrected in his new proposal for the
architecture of grammatical theory. The mistake, dubbed “syntac-
tocentricism,” concerns theories in which the only formation rules
(i.e., mechanisms that create linguistic representations) are those
of the syntactic component. “In short, syntax is the source of all
linguistic organization.” In contrast, Jackendoff proposes a model
in which there are three independent sets of formation rules (for
phonology, syntax, and semantics), a model he calls the parallel ar-
chitecture. The three independent representations thereby gen-
erated must then be related by interface (or correspondence)
rules, including rules that relate phonological representations di-
rectly to semantic representations.
Before discussing the parallel architecture proposed as a solu-
tion to the purportedly flawed standard theory, it is useful to con-
sider exactly how current theories of generative grammar are syn-
tactocentric, given Jackendoff’s characterization. Let us consider
the case of the minimalist program (cf., Chomsky 1995; 2000;
2001), which is inaccurately represented in Figure 1.1
Within a minimalist derivation (e.g., Chomsky 1995, Ch. 4), the
first step is the selection from the lexicon of a lexical array, a set of
lexical items designated the numeration. This lexical array is then
used to build linguistic structures via the iterated application of
the concatenation operation Merge. Merge builds syntactic struc-
tures bottom-up by concatenating two syntactic objects (lexical
items from the numeration, or phrases constructed from previous
applications of Merge) and labeling the concatenation with the
syntactic category label of one of the two concatenated objects,
thus creating a new syntactic object.2 The syntactic object gener-
ated eventually produces a Phonetic Form (PF) that is interpreted
at the sensory-motor interface and a Logical Form (LF) that is in-
terpreted at the conceptual-intensional interface.3 Within the de-
rivation of a linguistic expression, there is a point called “Spell-
Out” (S/O) where the phonetic features of the expression are sent
to the phonological component for further processing, and the
rest of the structure moves on to the LF interface. Any changes to
the structure of the expression after S/O are covert, because their
effects cannot be seen in PF.
Even if Merge is the only formation rule available in the de-
rivation, it does not follow that syntax is the sole source of linguis-
tic organization. The charge of “syntactocentrism” ignores the
contribution of the lexicon. Given that the lexicon specifies the
phonological, morphological, syntactic, and semantic structure of
lexical items, it too constitutes a major source of “linguistic orga-
nization.” If lexical items enter the syntactic derivation with a
specification of their syllable structure, then there is no need to
independently generate a syllable structure for the whole linguis-
tic expression generated.4 The charge of syntactocentrism is sim-
ply false for this theory, and as far as I can tell, for any previous
theory of generative grammar that has ever been proposed. The
notion is little more than a phantom.
Given that lexical entries contain phonological and semantic in-
formation, as well as syntactic information – the standard model
since Chomsky 1965 – Jackendoff’s parallel architecture creates a
serious dilemma. Presumably, the parallel architecture lexicon
that feeds the syntactic component contains no phonological or se-
mantic information. Otherwise, the parallel derivations of phono-
logical and semantic representations would redundantly specify
information that is already part of the syntactic derivation, thereby
undermining the need for parallel derivations in the first place.
Ironically, the syntactic derivation under the parallel architecture
must be “syntactocentric” – in just the same way that the phono-
logical derivation is “phonocentric” and the semantic derivation is
“semantocentric.”
The parallel architecture puts an enormous burden on the in-
terface/correspondence rules, one that they must surely fail to
carry in even the simplest cases. If, as Jackendoff seems to be
claiming, phonological representations contain no syntactic infor-
mation, then there must be a correspondence rule that links the
phonological representation of persuade to the lexical category V,
rather than some other lexical category. However, the phonetic la-
bels of words in a language are fundamentally arbitrary – what
Chomsky (1993) calls “Sausseurian arbitrariness” – so there is no
systematic way (i.e., via rules) to correlate phonetic labels and lex-
ical categories. The same point applies to the connections be-
tween phonological and semantic representations. Given the par-
allel architecture, nothing in the phonological representation of
persuade tells us that it corresponds to the semantic representa-
tion of persuade rather than the semantic representation of try.
The standard solution to the problem of Sausseurian arbitrariness
is to list the correspondences in the lexicon, traditionally the
repository for idiosyncratic properties of a language. But once we
do this, the motivation for the parallel architecture evaporates.
NOTES
1. It is important to note that the minimalist program is a program for
research investigating very general questions concerning the optimality (in
some interesting sense) of the computational system for human language
and more generally the possible “perfection” of language design. (See
Chomsky 1995; Freidin 1997 for discussion.) These questions by them-
selves do not provide a theoretical framework or a particular model, let
alone a specific theory. At present, the minimalist program is being inves-
tigated in a variety of ways, where specific proposals are often mutually ex-
clusive, as is normally the case in linguistics, and rational inquiry more gen-
erally.
2. Thus phrase structure is constructed via transformation and there-
fore there is no phrase structure rule component. Movement transforma-
tions in this theory also involve a form of merger, where the syntactic ob-
ject moved is concatenated with the root of the phrase containing it. When
two independent objects are merged, this is called external Merge;
whereas when a syntactic object is displaced to an edge of the constituent
containing it, this is called internal Merge. The two types of Merge corre-
spond to the distinction between generalized versus singulary [sic, techni-
cal term] transformations in Chomsky (1957 and earlier).
3. There is no further conversion of LF to “semantic representation” as
indicated in Figure 1. Furthermore, following up on Note 1, recent pro-
posals have questioned the existence of any level of representation like LF
(see Chomsky 2002).
4. The same argument can be made regarding semantic representation.
Assuming that the structures Jackendoff proposes for the semantic repre-
sentation of verbs are on the right track, these structures could just as eas-
ily be part of the semantic specification of the lexical entry for predicates
where the elements labeled “Object” in Jackendoff’s lexical representa-
tions are variables to be replaced with constant terms from the actual sen-
tence in which the predicate occurs. Again, there is no need to generate
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
677
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

these semantic representations independently of the syntax and then have
the problem of relating the two independent representations.
Linguistics fit for dialogue
Simon Garroda and Martin J. Pickeringb
aDepartment of Psychology, University of Glasgow, Glasgow G12 8QT,
United Kingdom; bDepartment of Psychology, University of Edinburgh,
Edinburgh EH8 9JZ, United Kingdom. simon@psy.gla.ac.uk
martin.pickering@ed.ac.uk
http://www.psy.gla.ac.uk/~simon/
http://www.psy.ed.ac.uk/Staff/academics.html#pickeringmartin
Abstract: Foundations of Language (Jackendoff 2002) sets out to recon-
cile generative accounts of language structure with psychological accounts
of language processing. We argue that Jackendoff’s “parallel architecture”
is a particularly appropriate linguistic framework for the interactive align-
ment account of dialogue processing. It offers a helpful definition of lin-
guistic levels of representation, it gives an interesting account of routine
expressions, and it supports radical incrementality in processing.
It is easy to argue that dialogue is the basic setting for language
use (Clark 1996). Yet historically, generative linguistics has devel-
oped theories of isolated, decontextualized sentences that are
used in texts or speeches, in other words, in monologue. In turn,
this failure to address dialogue at a linguistic level is one of the
main reasons why psycholinguistics have also ignored dialogue. In
contrast, Pickering and Garrod (in press) propose a specific mech-
anistic account of language processing in dialogue, called the in-
teractive alignment model. This account assumes that in dialogue,
interlocutors align their linguistic representations at many levels
through a largely automatic process. It also assumes that align-
ment at one level can promote alignment at other levels. This ex-
plains why coming to a mutual understanding in dialogue is gen-
erally much easier than interpreting or producing utterances in
monologue. In this commentary we consider how Jackendoff’s
framework in Foundations relates to this account.
Jackendoff considers how linguistic theory can elucidate lan-
guage processing (Ch. 7), a surprisingly fresh approach from a
generative linguist. However, he does not explicitly consider how
his “parallel architecture” might relate to language processing in
dialogue. Here, we argue that the architecture turns out to be par-
ticularly helpful in understanding how interactive alignment
comes about. First, it is consistent with multiple independent lev-
els of representation with links between the levels. Second, it of-
fers interesting insights into the linguistic representation of semi-
fixed or routine expressions such as idioms, which we argue play
an important role in dialogue processing. Finally, it is consistent
with incrementality in both production and comprehension,
which appears necessary for understanding dialogue.
Independent levels and the interfaces between them. Jack-
endoff assumes that phonological, syntactic, and semantic forma-
tion rules generate phonological, syntactic, and semantic struc-
tures respectively, and these are brought into correspondence by
interface rules, which encode the relationship between different
systems (Ch. 5). This produces an architecture which is “logically
non-directional” and hence not inherently biased toward either
perception or production (Ch. 7, p. 198). These two general fea-
tures of Jackendoff’s account make it especially attractive as a lin-
guistic framework for interactive alignment. First, interlocutors
can align representations at different linguistic levels (e.g., Brani-
gan et al. 2000; Garrod & Anderson 1987). These researchers ar-
gue that the alignment process is largely automatic (operating
through so-called alignment channels) and that alignment at one
level (e.g., the syntactic) reinforces alignment at other levels (e.g.,
the semantic) (e.g., Cleland & Pickering 2003). Hence, alignment
channels can affect the application of the formation rules, and in-
terface rules are encoded in the links between the levels. It would
be difficult to find such a correspondence with traditional gener-
ative approaches where only syntax is generative and where
phonology and semantics are “read off” syntactic structures (e.g.,
Chomsky 1981). Second, the non-directional character of Jack-
endoff’s architecture explains how perception of structure at one
level can enhance subsequent production of structure at that level
as the literature on alignment in dialogue demonstrates. In other
words, so long as the linguistic structures called upon in compre-
hension and production are the same, there can be priming from
comprehension to production and therefore alignment between
interlocutors.
The structure of routine expressions. Pickering and Garrod (in
press) argue that the interactive alignment process naturally leads
to the development of routine expressions in dialogue. In other
words, dialogue utterances become like stock phrases or idioms
with semi-fixed structure and interpretation. This is reflected in
the degree of lexical and structural repetition in dialogue corpora
(Aijmer 1996; Tannen 1989). We argue that routinization greatly
simplifies language processing because it allows interlocutors to
call upon stored representations, which already encode many of
the decisions normally required in production or comprehension,
rather than having to compute everything from scratch.
Jackendoff provides an interesting discussion of the contrast be-
tween lexical storage and on-line construction (Ch. 6). In section
6.5 he specifically addresses the structure of idioms, and in sec-
tion 6.6, what he calls constructional idioms. Constructional id-
ioms are weakly generative constructions such as take NP to task
or put NP in (his, her, or their) place. These behave like complex
VPs but include a free variable position inside the complex struc-
ture. Of course, all such idioms are assumed to be represented in
long-term memory, either as complete packages (i.e., for standard
idioms) or as frames with variables (i.e., for constructional idioms).
In our framework we assume that routines of all these kinds are
constructed through alignment processes. They can therefore be
“set up” for a particular conversation, with a particular meaning
that holds for that interchange alone. In other words, routines can
be transient.
Radical incrementality in processing. A crucial feature of Jack-
endoff’s account for dialogue is that it supports radically incre-
mental processing. Of course, there are good reasons for assum-
ing incrementality in monologue comprehension, as well. Here,
we merely point out that the fact that interlocutors can complete
each other’s utterances or clarify what they have just heard
strongly suggests that it must be possible to comprehend frag-
ments of language as they are encountered, and the fact that such
contributions are constrained by the syntax of the original frag-
ment indicates that incremental syntactic analysis must occur (see
Pickering & Garrod, in press).
Where is the lexicon?
Judit Gervain
Cognitive Neuroscience Sector, Scuola Internazionale Superiore di Studi
Avanzati, Trieste 34014, Italy. gervain@sissa.it
Abstract: In an attempt to provide a unified model of language-related
mental processes, Jackendoff puts forward significant modifications to the
generative architecture of the language faculty. While sympathetic to the
overall objective of the book, my review points out that one aspect of the
proposal – the status of the lexicon – lacks sufficient empirical support.
In Foundations of Language, Jackendoff (2002) proposes a sub-
stantial “reconceptualization” of the generative architecture of
language in order to better integrate linguistics into the study of
the mind and the brain. This move is attractive because it allows
the author to embrace a wide range of findings within the broader
framework of cognitive neurosciences. Thus previously unrelated
phenomena, such as grammaticalization in Creole languages, tip
of the tongue states, or referential dependencies within sentences
are discussed in a unified mental model. While I am in perfect
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
678
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

agreement with the general objective and some of the main con-
clusions of the book, I call into question Jackendoff’s account of
the lexicon. My criticism will focus on three points, comparing the
traditional account with the author’s from a linguistic, a neuronal,
and an architectural point of view. The comments are not intended
as a rejection or a rebuttal of Jackendoff’s proposal as a whole.
Rather, the aim is to highlight certain points in the text where the
author’s attempt at overall integration results in loss of explanatory
power within specific domains.
As Jackendoff himself acknowledges (p. 425), the most dra-
matic modification he brings to the linguistic architecture estab-
lished in generative grammar concerns the function and content
of the lexicon (Chs. 5 and 6). The usual distinction (e.g., Chomsky
1995), between rules as syntactic functions and lexical entries as
tokens on which these functions operate, is blurred. In the new
model, both types of entities are contained in the lexicon as triplets
of phonological, syntactic, and semantic representations. The only
difference between the lexical items and the l(exical)-rules is
whether they contain typed variables: The former don’t, the latter
do.
The first problem with such an analysis is that Jackendoff does
not provide enough linguistic evidence to ground it. His main ar-
gument is that even apparently fixed and memorized expressions,
such as idioms and other semi-productive items, follow syntactic
regularities, for example, the past tense of take in take something
for granted is took, just as in any other case. Moreover, some of
the semi-productive items even contain syntactic variables, as the
noun phrase (NP) slot (something) in the previous example illus-
trates. Following the same logic, syntactic rules, that is, fully pro-
ductive items, can be conceived of as structures with variable slots
only, and can be included in the lexicon, just as set phrases or fixed
idioms are included. While this lexicalist and representationalist
approach might prove to be a good analysis of idioms, it certainly
does not do justice to a wide range of phenomena syntactic re-
search has been concerned within the last fifty years. As it stands,
the model cannot handle derivational rules, for example, move-
ment and constraints thereof, or purely structural-relational no-
tions, such as c-command or government. How is one to account
for the difference in grammaticality between the possible sen-
tence Who do you think will win? and the complex NP violation
*Who do you know [the fact that will win] in Jackendoff’s model?
The description of the syntactic component and the two syntactic
interfaces is not explicit enough to provide an answer. Since issues
of this sort make up the bulk of syntactic research, this lacuna is
not negligible. (Of course, this is not to say that representational-
ist or lexical accounts of syntax are in principle not possible. The
question is, How much empirical material they are able to cover?)
Secondly, Jackendoff’s proposal to conflate semi-productive
and productive processes goes against neuropsychological find-
ings. Pinker (1991; 1997) and Clahsen (1999) present convincing
evidence that the mental dictionary and the mental grammar may
be kept in different parts of the brain (Pinker 1997, p. 547). A dou-
ble dissociation is found between lexically stored and rule-gener-
ated past tense forms in patients with specific brain lesions, and
neuro-imaging studies also reveal a differential recruitment of
brain areas. Although Jackendoff mentions some of these data, he
has little to say about how to reconcile them with his fully lexical-
ized model.
Thirdly, the lexicalization of syntactic patterns is not without un-
welcome consequences for the whole of the linguistic architec-
ture. In the proposed model, lexical items and the l-rules have a
double function; they act as interface conditions, but also provide
material for the three generative components. In other words, the
lexicon is constantly called upon during the derivation by the in-
dependent generative components, as well as the interfaces. Con-
sequently, there is no clear-cut distinction between the subsys-
tems of the grammar; the lexicon seems to have devoured the
tripartite architecture. In his attempt to do away with syntacto-
centrism, Jackendoff seems to introduce heavy lexicocentrism in
the design.
This architectural problem is especially acute when the model
is extended to explain performance, that is, processing. “Now,
when the lexicon is called, should we think of the processor call-
ing the lexicon for a match? Or should we think of the lexicon, as
part of the interface processor, as actively attempting to impose it-
self on the input? This perhaps awaits a better understanding of
brain dynamics” (p. 207). Note that the two options make distinct
empirical predictions about the relationship between the lexicon
and the grammar or the mental processes underlying lexical ac-
cess and retrieval. For example, one would expect lexical access to
be a slower, two-step process in the first scenario (call to the lexi-
con plus word retrieval), whereas according to the second, access
is immediate. Unfortunately, these predictions are not explored in
detail, therefore the proposal is not comparable with existing ac-
counts, which formulate empirically testable predictions (e.g.,
Levelt 1993).
In the foregoing discussion, I have been arguing that Jacken-
doff’s reformulation of the status of the lexicon in the generative
design of language lacks empirical support from linguistic, neuro-
psychological, and architectural viewpoint. As a consequence,
more research is needed before Jackendoff’s framework can be
evaluated against rival theories of the language faculty. Although
unification is a welcome development in the history of sciences,
and the cognitive domain should be no exception, as Jackendoff
convincingly argues, we have to make sure that we are not paying
too high a price for it.
ACKNOWLEDGMENTS
I wish to thank Jacques Mehler, Luca Bonatti, Gabor Zemplen, Agnes
Kovacs, and Mohinish Shukla for comments on an earlier version of this
commentary. All remaining errors are mine.
Language shares neural prerequisites with
non-verbal capacities
Georg Goldenberg
Neuropsychological Department, Bogenhausen Hospital, D 81925 Munich,
Germany. Georg.Goldenberg@extern.lrz-muenchen.de
Abstract: Based on neuropsychological evidence of nonverbal impair-
ment accompanying aphasia, I propose that the neural prerequisites for
language acquisition are shared with a range of nonverbal capacities. Their
commonality concerns the ability to recognize a limited number of finite
elements in manifold perceptual entities and to combine them for con-
structing manifold entities.
Although the brain figures prominently in the title of the book
Foundations of Language: Brain, Meaning, Grammar, Evolution
(Jackendoff 2002), little attention is devoted to the available em-
pirical evidence on the neural substrate of linguistic competence.
One of the most robust facts in neuropsychology is the cerebral
asymmetry of the neural substrate of language. In the great ma-
jority of people, left brain damage (LBD) causes aphasia, but nei-
ther does aphasia affect all aspects of language nor is it the only
sequel of LBD.
Aphasia affects syntax, phonology, and semantics, which can all
be conceptualized as being based on combinatorial systems of fi-
nite elements. Other components of verbal communication can-
not easily be reduced to combinations of finite elements because
they demand fine-grained distinctions within distinct elements or
categories. Such components, like emotional prosody or the prag-
matics of communicative exchange, are relatively spared in apha-
sia but vulnerable to diffuse or right-sided brain lesions, which do
not cause aphasia (McDonald 1993; Starkstein et al. 1994).
At the same time, most aphasic patients have difficulties with
nonverbal tasks that require the extraction of a limited number of
finite elements from a rich perceptual diversity. Such tasks are, for
example, color sorting where colors have to be sorted according to
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
679
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

categories rather than to perceptual similarity (e.g., light green
may be perceptually closer to yellow than to dark green but has to
be sorted with the greens); or matching objects by type rather than
perceptual appearance, as for example when an analogue clock
has to be matched with a digital clock rather than a (visually more
similar) compass; or matching images of objects with their char-
acteristic sound (Vignolo 1990). By contrast, matching tasks that
require consideration of variations within a category such as, for
example, matching of individual faces, do not crucially depend on
left hemisphere integrity (Benton & Van Allen 1968).
There are symptoms of LBD, which on first sight, do not fit into
a left-hemisphere dominance for extraction and combination of fi-
nite elements. These are “high level” disorders of motor control
traditionally termed “apraxia.” These symptoms have led to the
proposal that left-hemisphere dominance concerns primarily mo-
tor control. Attempts to deduce language dominance from motor
dominance have either emphasized the motor demands of speak-
ing (Kimura 1983) or postulated that language evolved from ges-
tural communication (Corballis 2002). Recent research suggests
that apraxia has more to do with the application of combinatorial
systems of finite elements than with motor control. Apraxia affects
three domains of actions: imitation of gestures, performance of
meaningful gestures on command, and use of tools and objects.
Evidence has been provided that LBD patients fail imitation of
novel gestures because they cannot reduce them to combinations
of a limited number of defined body parts (Goldenberg 1996;
Goldenberg & Strauss 2002). They have similar problems when
this body part coding is required to match photographed gestures
(Goldenberg 1999) or to replicate gestures on a mannequin
(Goldenberg 1995), although motor control is trivial for pointing
to photographs and very different from imitation for manipulating
a mannequin. By contrast, the exclusive role of LBD is mitigated
or vanishes completely when imitation puts fewer demands on
body-part coding and requires instead fine-grained distinctions
within one category of body parts (e.g., the fingers of one hand).
Performance of meaningful gestures to command is frequently
tested by asking for a pantomime of object use (e.g., “Show me
how you would use a toothbrush”). Here the crucial difficulty of
LBD patients seems to concern the demonstration of the object
and its use by selecting distinctive features of the motor action as-
sociated with that use (Goldenberg et al. 2003). Use of tools and
objects poses demands on many cognitive functions and can be
impaired by brain lesions in many locations (Schwartz et al. 1999),
but one component which is exclusively bound to left hemisphere
integrity is the inference of possible functions from structural
properties of objects. For example, LBD patients may fail to dis-
cover that a hook can be fixed to a ring by inserting it (Goldenberg
& Hagmann 1998). Such failures can be attributed to an inability
to detect a limited number of functionally relevant features and to
solve mechanical problems by reducing them to basic functional
relationships.
There is controversy concerning whether the co-occurrence of
these difficulties with aphasia in LBD patients is a result of simi-
larities between the affected functions or of anatomical contiguity
between their neural substrates, but this opposition may be ill-
conceived. Anatomical contiguity is unlikely to have arisen from
arbitrary placement of unrelated functions. Presumably it reflects
a deeper affinity of their neural substrate. It may be more fruitful
to ask for the functional properties corresponding to this neural
commonality. I propose that this commonality is to be sought in
the ability to recognize a limited number of finite elements in
manifold perceptual entities, and to combine them for recon-
structing manifold entities. In this account, the neurally designed
predisposition for language acquisition is not specific for language
but also supports a range of nonverbal capacities.
Jackendoff’s conceptualism
James Higginbotham
School of Philosophy, University of Southern California, Los Angeles, CA
90089-0451. higgy@usc.edu
Abstract: In this commentary, I concentrate upon Ray Jackendoff’s view
of the proper foundations for semantics within the context of generative
grammar. Jackendoff (2002) favors a form of internalism that he calls “con-
ceptualism.” I argue that a retreat from realism to conceptualism is not
only unwarranted, but even self-defeating, in that the issues that prompt
his view will inevitably reappear if the latter is adopted.
In Foundations of Language: Brain, Meaning, Grammar, Evolu-
tion (henceforth Foundations), Jackendoff is sympathetic – more
sympathetic than I, for one, would have expected him to be – to
the view that the theory of meaning in empirical linguistics should
link language to human action and communication, and that the
notions of reference and truth are indispensable both as explain-
ing relations of sentences to one another, as in implication, and
their relations to their subject matter and conditions on their use.
Jackendoff holds, however, that the proper implementation of this
view requires the adoption of a variety of irrealism about what we
refer to, and what makes what we say true or false. In Part III of
Foundations he offers a variety of reasons for this irrealism, or
conceptualism, as he calls it. None of these seem to me effective;
I will consider a few below. More than this, however: Jackendoff’s
irrealism threatens to be self-defeating, in that the problems that
he discerns for realist accounts are bound to return, in just the
same form, under the interpretation of reference that he offers.
Having remarked, in my view rightly, that the signal contribu-
tion of generative grammar was to take for the subject of linguis-
tics not the formal properties of language but rather the basis for
human knowledge and capacity for language, Jackendoff is wary (to
the point of abhorrence) of saying that languages themselves are
abstract objects whose properties we know (or “cognize,” to use
Chomsky’s suggestion of a more neutral terminology). He is wary
of this, not because he rejects the notion of implicit or tacit knowl-
edge, but rather because he thinks that, once we say that languages
are abstract, we have cut ourselves off from the psychological in-
vestigation that is to be the core of the enterprise (p. 297). He is
also repelled (p. 299) by the idea that these abstract objects have
always been lying around, waiting for people to “grasp” them. Ab-
stract objects in general, he thinks, must be “human creations.”
The conflicts here are illusory, however. What comes to hold
only through human organization and activity is not the existence
of abstract objects, but empirical identities: That language L has
property P, may be a fact on a par with the truths of arithmetic;
but that Higginbotham’s language or Jackendoff’s language 5 L,
and therefore that Higginbotham’s language or Jackendoff’s lan-
guage has property P, is a psychological contingency, to which all
the available evidence, about them and other humans, is relevant.
I suppose we may agree that a primitive mechanism of “grasping”
is, if true, a counsel of despair. But how is the slogan that abstract
objects are “human creations” supposed to help? Everyone knows
on a moment’s reflection that to enclose the largest area with a
piece of string, you should form it into a circle. Supposing that cir-
cles are human creations brings us no closer to an explanation of
why this should be so.
Jackendoff opposes what he calls common-sense realism about
reference – according to which (simplifying only a bit) words re-
fer to things – to his own conceptualist account, according to
which speakers judge words to refer to things in “the world as con-
ceptualized” by them. The basis for the substitution of the con-
ceptualist view for the standard one is a variety of questions about
reference given in Chapter 10, section 3, (pp. 300–303). All our
old friends are there: Sherlock Holmes, the unicorn in my dream,
the value of my watch, virtual squares, “politically constructed en-
tities” such as Wyoming, and so forth. There is no space here to
consider all of these, but I make two remarks.
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
680
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

First, Jackendoff ignores the point that nominal reference must
be considered, not in isolation, but in the context of a sentence.
Thus, take the value of my watch, or the distance between New
York and Boston. The things are identified with “mixed numbers,”
$50 for my watch, 204 miles for the distance. However, as ob-
served originally by Carnap (1926), any mystification about them
disappears when we observe that the reference is to the value of
my watch in dollars, which is just the number 50, or the distance
between New York and Boston in miles, which is 204. The virtual
square formed by four properly placed dots, Jackendoff says, “isn’t
there physically.” True enough, there are no lines, only the dots
that are to be construed as the vertices of the square. There is,
however, within the limits of perceptual accuracy, exactly one
square of which they are the vertices, so that to say that the square
is “formed by the four dots” indicates, not that there is no square,
but rather how we are to understand the notion “x is formed by y.”
Second, and more critically, Jackendoff urges (p. 304) that “ref-
erence” need not be to things in the “real world” (his emphasis).
So statements about Sherlock Holmes or the unicorn in my dream
can be taken on a par with statements about Derek Jeter or the
unicorn in the garden. But this, I think, conceals a mistake: The
distinction between names like Sherlock Holmes on the one hand,
and Derek Jeter on the other, is a distinction that is made within
our speech, not outside it. If you think there is a serious question
whether Sherlock Holmes ever visited Mongolia, or that what is
responsible for the truth of the statement that he lived in London
is the same sort of thing that is responsible for the truth of the
statement that Derek Jeter lives in New York, then you don’t un-
derstand the name; for it is part of understanding the name Sher-
lock Holmes that Sherlock Holmes is a fictional character. The case
is similar with dream-objects, but rather more interesting, since
one could believe that one saw them, interacted with them, and
so forth; but for us anyway there is common recognition that state-
ments about some of the contents of dreams are made true or false
in virtue of our representations alone, so that their superficial
grammatical form is not a guide to their truth conditions. But the
truth conditions are known, and known to be different from those
of apparently similar statements. It is, therefore, no advance, and
in fact an obscuring of the issues, to adopt for these reasons a con-
ceptualist semantics. Jackendoff’s thought seems to be that, if we
are casual enough about objects of reference, on the ground that
they are in the merely conceptualized world, the problems of dis-
tinguishing the truth conditions of fictions, some statements about
dreams, and so forth will go away. But they won’t.
The latter part of Foundations involves often very interesting dis-
cussion of semantic phenomena, both lexical and combinatoric.
None of these, so far as I can see, require making a distinction such
as he envisages between the “conceptualized world” and – the world.
Four challenges for cognitive neuroscience
and the cortico-hippocampal division of
memory
Harry Howard
Department of Spanish and Portuguese, Tulane University, New Orleans, LA
70118. howard@tulane.edu
http://www.tulane.edu/~howard/
Abstract: Jackendoff’s criticisms of the current state of theorization in
cognitive neuroscience are defused by recent work on the computational
complementarity of the hippocampus and neocortex. Such considerations
lead to a grounding of Jackendoff’s processing model in the complemen-
tary methods of pattern analysis effected by independent component
analysis (ICA) and principle component analysis (PCA).
Jackendoff elaborates four challenges for cognitive neuroscience
whose consequences reverberate throughout his book, Founda-
tions of Language (Jackendoff 2002). In a nutshell, if spreading
activation (SA), firing synchrony of neural units (FS), and multi-
layer perceptrons trained by back-propagation of error (BP) con-
stitute the apogee of current neurotheory, then it has a long way
to go to reach even the lowest echelons of descriptive adequacy
for human language. In this commentary, I briefly review a neu-
rologically realistic alternative to the SA/FS/BP trio that meets
most of Jackendoff’s challenges. Known as the Complementary
Learning Systems (CLS) model, it was first developed by Mc-
Clelland et al. (1995), and has been refined several times since
then (see O’Reilly & Norman 2002). Its computational principles
have been applied to a wide range of learning and memory phe-
nomena (impaired and preserved learning capacities with hip-
pocampal lesions in conditioning, habituation, contextual learn-
ing, recognition memory, recall, and retrograde amnesia) across
several species (rats, monkeys, and humans). To explain how CLS
works, we start at the end, with Jackendoff’s fourth challenge.
Jackendoff sees it as contradictory that a compositional phrase,
such as “lift the shovel” should be encoded in short-term memory
via SA or FS, while a structurally-equivalent idiomatic phrase such
as “kick the bucket” should be stored in long-term memory by the
slow modulation of synaptic weights via BP. The CLS literature
implicitly raises a comparable objection, which is resolved as the
computational difference between hippocampal and neocortical
function. By way of illustration, let us call on a linguistic example
that neither Jackendoff nor CLS discusses, but which has consid-
erable empirical depth (see Bowerman 1996; Bowerman & Choi
2001; Choi & Bowerman 1991).
Imagine a child viewing two events, one in which a cassette is
put in a bag and another in which the same cassette is put in its
case. Korean, in contrast, lexicalizes the events with separate
items, namely, the verbs nehta, “put loosely in or around” for the
former and kkita, “interlock, fit tightly” for the latter. Thus, the
brain must keep both events separate, and presumably with their
full complement of real-world detail, in order to account for the
specificity of Korean. Nevertheless, both events have overlapping
parts, such as the cassette, the motion of bringing two things to-
gether, and maybe even the person performing the motion. The
brain must therefore ensure that parts shared among events do not
interfere with one another. CLS asserts that these characteristics
define episodic memory and the function of the hippocampus: the
fast and automatic learning of sparse representations. Neverthe-
less, if all events are kept separate, there will be no way to gener-
alize across them. Yet humans do indeed generalize; witness the
fact that English uses the same spatial vocabulary for both events,
namely, the preposition in, with the aid of the motion verb put.
The brain must therefore be able to integrate events so as to ab-
stract away from their specifics and encode the overall statistical
structure of the environment. CLS asserts that these characteris-
tics define semantic memory and the function of neocortex: the
slow and task-driven learning of overlapping representations.
Returning to as “lift the shovel” versus “kick the bucket,” we
may conclude that the hippocampus makes the initial, short-term
binding of the disparate features of the phrases from which the
neocortex extracts any statistical regularities, such as the parallel
[V [Det N]] structures. The idiomatic phrase sports an additional
regularity, namely, the fact that it has a noncompositional reading
which presumably can only be learned by the slow (i.e., multiple
exposure) modulation of synaptic weights. The conclusion is that
CLS avoids any inconsistent treatment of compositional and non-
compositional phrases.
Turning to the first challenge, Jackendoff cites the multiple
embedding of linguistic entities as leading to temporal incoher-
ence for any solution to the feature-binding problem that relies
on FS. The CLS model has made this same criticism from a more
neurologically informed perspective. Its alternative is to return to
the notion of conjunctive features, with a twist. The twist is to
avoid the combinatorial explosion of units encoding a single fea-
ture conjunction by distributing the conjunctions across many
units (O’Reilly & Busby 2002),
where each unit encodes some possibly-difficult to describe amalgam
of input features, such that individual units are active at different levels
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
681
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

for different inputs, and many such units are active for each input (Hin-
ton et al. 1986). Therefore, the input is represented by a complex dis-
tributed pattern of activation over units, and each unit can exhibit vary-
ing levels of sensitivity to the featural conjunctions present in the input.
(O’Reilly & Busby 2002)
The conclusion is that Jackendoff’s objection is vindicated, and re-
solved.
The “problem of 2” presumably disappears with a distributed
encoding, since each repeated item would be represented by dif-
ferent levels of activation of the neural population encoding the
feature conjunction according to the item’s context. However, the
current CLS literature does not address this issue directly, leaving
the reader uncertain whether the approach will scale up correctly.
Finally, the derivation of typed variables is a goal of CLS, in the
guise of the learning of relational structures. Its supporters echo
Jackendoff’s reiterated protestation that freely-combining typed
variables are fundamental to higher cognition. Unfortunately, the
CLS simulations in which a relational structure is learned suffer
from the general opaqueness of distributed encoding. That is to
say, the network appears to have learned the relations that it was
exposed to, but from the text of the reports, one does not under-
stand how it is done.
In this respect, the work of two other researchers is highly rel-
evant. Jackendoff cites Shastri and Ajjanagadde (1993) as one of
the few computational models that grapples with the representa-
tion of typed variables. Shastri’s more recent model SMRITI
(Shastri 2002) takes these ideas a step further by specifically at-
tributing to the hippocampus the responsibility for creating role-
entity bindings, so that it will assign to an event in which John gives
Mary a book in the library on Tuesday, the representation GIVE:
giver 5 John, recipient 5 Mary, give-object 5 a book, location 5
library, temporal-location 5 Tuesday. This is tantamount to the
relational structure of first-order predicate logic, if not Event Se-
mantics (see Parsons 1990). In a slightly different vein, Pulver-
muller (2002) reviews and expands on the concept of syn-fire
chains as a neurologically plausible mechanism for serial order in
language. Such chains explicitly encode the relational structure of
syntax, though Pulvermüller does not localize them to any partic-
ular cortical area. In fact, Pulvermüller provides a fascinating neu-
roscientifically grounded foil to Jackendoff, and reading the two
of them together is a rewarding intellectual exercise. More to the
point, both Shastri and Pulvermüller wind up invoking freely-
combining typed variables in a way that is more transparent than
CLS.
So what of the other 418 or so pages of Jackendoff’s text? The
various modules of the grammar-based processing architecture
presumably reflect independent clusters of statistical regularities
in the linguistic input, learned by the gradual adjustment of synap-
tic weights. This suggests a further distillation of the CLS: The
hippocampus performs independent component analysis (ICA)
on its input patterns in order to orthogonalize them, that is, to re-
move their common features and so make them maximally unre-
lated (Kempermann & Wiskott 2004). At the very least, this would
separate a linguistic pattern into its phonological, syntactic, and
conceptual components, and then into the independent subcom-
ponents thereof, such as Shastri’s role-entity bindings in the con-
ceptual module. The drawback of ICA is that it separates linguis-
tic patterns into an enormous number of dimensions, for example,
temporal-location 5 Monday, temporal-location 5 Tuesday, and
so on. It is the function of the neocortex to reduce these dimen-
sions to the most relevant or informative ones, which suggests that
neocortex performs principal component analysis (PCA, see Si-
moncelli & Olshausen 2001) on the hippocampal output. For in-
stance, the independent temporal components mentioned two
sentences ago could be reduced to the single principal component
temporal-location 5 day of the week, thereby synthesizing a typed
variable.
Psychologism and conceptual semantics
Luke Jerzykiewicza and Sam Scottb
aCognitive Science Department, Carleton University, Ottawa K1S 5B6,
Canada; bPhilosophy-Neuroscience-Psychology Program, Washington
University in St. Louis, St. Louis, MO 63130. ljerzyki@uwo.ca
sscott@artsci.wustl.edu
http://www.carleton.ca/~ljerzyki
http://www.artsci.wustl.edu/~sscott
Abstract: Psychologism is the attempt to account for the necessary truths
of mathematics in terms of contingent psychological facts. It is widely re-
garded as a fallacy. Jackendoff’s view of reference and truth entails psy-
chologism. Therefore, he needs to either provide a defense of the doctrine,
or show that the charge doesn’t apply.
Jackendoff’s vision of the language faculty in Foundations of Lan-
guage: Brain, Meaning, Grammar, Evolution (2002) is impressive
in scope and rich in insightful detail. However, his account of ab-
stract objects (sect. 10.9.3) requires substantial elaboration and
defense before it can safely avoid the fallacy of psychologism.
In Jackendoff’s account, conceptual structures within the gen-
erative semantic module are not themselves interpreted – they do
not have a semantics. They just are the semantics of natural lan-
guage. The fine-grained data-structures that inhabit the semantic
module interface richly with perceptual modalities and with mo-
tor outputs, while individually not necessarily representing any-
thing in the world as such (cf. Brooks 1991). The familiar appear-
ance that words refer to entities and events can be explained – for
concrete referents, at least – in terms of the relationship between
semantic constructs and the outputs of perceptual faculties. It is
these outputs that we experience as our “world.” Now, in the case
of abstract objects (like beliefs, mortgages, obligations, and num-
bers), which manifestly lack perceptual features, the theory makes
only slightly different provisions: The data-structures that encode
them possess inferential rather than perceptual features. Inter-
faces to syntax and phonology treat all conceptual structures 
similarly, regardless of whether their constitutive features are 
exclusively inferential or, in part, perceptual. Thus, Jackendoff’s
naturalistic theory of concepts rejects Platonism and identifies ab-
stract objects with the cognitive structures that express them.
The paradigm cases of abstract objects are mathematical and
logical entities. Oddly, Jackendoff does not discuss these cases ex-
plicitly. Yet if the Conceptual Semantics (CS) account of abstract
objects is to work at all, it must work for them. The trouble is that
CS entails psychologism, the view that the necessary truths of
mathematics and logic are to be accounted for in terms of contin-
gent facts about human cognition. According to psychologism, 2
1 2 5 4 is a fact of human psychology, not a fact that is indepen-
dent of human beings. Frege (1953) raised seminal objections to
this doctrine and today psychologism is typically viewed as a
patent fallacy (Dartnall 2000). There is room for discussion, how-
ever. Haack (1978) points out that it is far from obvious whether
Frege’s objections continue to apply. Frege’s target was the intro-
spectionist psychology of the day, and Jackendoff (1987; 2002)
carefully avoids this approach. But to get off the ground, a psy-
chologistic account of abstract entities must cope with three chal-
lenges:
(1) Universality. Some norms derive their authority from com-
munity standards. Those norms are no less real for their conven-
tional nature (traffic rules come to mind), but they are only true
by agreement. By way of contrast, norms governing the behavior
of abstract logical and mathematical entities are universal (a point
stressed by Nagel 1997). Community standards derive their au-
thority from these norms, and not vice versa. Even people with un-
tutored intuitions can come to recognize the truth of a law of logic
or mathematics, though they may require quite a bit of reflection
to do so. CS needs an explanation of how some abstract objects
(which are supposed to be mental entities) come to possess these
inferential features. Are they innate? If so, Jackendoff’s rejection
of Fodor loses some of its bite. Are they learned? If so, the poverty
of stimulus problem rears its ugly head.
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
682
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

(2) Objectivity. Logic, geometry, and mathematics are not 
uninterpreted formal systems that people happen to universally
assent to regardless of which community they inhabit. Formal in-
terpretations of physical phenomena permit predictions concern-
ing the behavior of objective reality even in contexts vastly beyond
the scope of actual (or possible) human experience. How then
does mathematical reasoning manage to preserve truth about dis-
tant contexts if mathematical objects are merely psychological
data structures with local inferential features? In other words,
quite apart from its universality, how, in the psychologistic ac-
count, does mathematics come by its objectivity (cf. Smith 1996)?
(3) Error. It is tempting to account for the validity of logical in-
ference in terms of the way that (normal, healthy) cognitive sys-
tems actually reason. But we can make mistakes regarding the
properties of abstract objects. Even professional mathematicians
occasionally draw false inferences about mathematical objects.
And a real feeling of surprise and discovery can accompany math-
ematical innovation, that moment when humanity discovers that
we have been conceiving of some mathematical construct incor-
rectly all along. The intuition that mathematical objects can have
properties quite different from those imputed to them, even by
professionals, fuels Platonist intuitions (Dummett 1978). Validity
cannot merely consist in conformity with the way people actually
reason; it is a property of arguments that conform to the way we
ought to reason. How psychologism can account for this remains
uncertain.
Jackendoff (pp. 330–32) suggests several mechanisms of social
“tuning” that can serve to establish (universal) norms within a
community – norms against which error may be judged and the
appearance of objectivity can arise. So when Joe mistakes a platy-
pus for a duck (p. 329), his error is relative to the impressions of
the rest of his community. “Objective” fact and the appearance of
universality is established by community consensus. Unfortu-
nately, this account does quite poorly with logic and mathematics.
A mathematical or logical discovery happens when one member
of the community realizes that something is wrong with the way
the community conceptualizes some aspect of the field, and
demonstrates that error to the other members of the community.
The issue here is how a whole community can be shown to be in
error when the objective reality against which the error is judged
is mere community consensus. Platonism has an obvious solution
to this issue, but CS will have to work for one.
We are by no means arguing that universality, objectivity, and
error cannot be accommodated by CS. But Jackendoff does sug-
gest that CS can provide insight into the appeal of formal ap-
proaches to semantics. Before it can explain the success of its ri-
val, it must itself account for the nature of the logical apparatus on
which formal work rests. We suspect that this can indeed be done.
But until it is, CS remains incomplete in an important way.
ACKNOWLEDGMENT
We thank Robert Stainton for his helpful comments and discussion.
Delegation, subdivision, and modularity: 
How rich is Conceptual Structure?
Damián Justoa, Julien Dutantb, Benoît Hardy-Valléec,
David Nicolasd, and Benjamin Q. Sylvande
a,b,c,d,eInstitut Jean Nicod, Centre National de la Recherche Scientifique –
École des Hautes Études en Sciences Sociales – École Normale Supérieure
(CNRS – EHESS – ENS), 75007 Paris, France; aÉcole des Hautes Études
en Sciences Sociales, 75006 Paris, France; b,eU.F.R. de Philosophie,
Sorbonne Paris IV, 75005 Paris, France; bÉcole Normale Supérieure –
Lettres et Sciences Humaines, 69342 Lyon, France; cDépartement de
Philosophie, Université du Québec à Montréal, Montréal, Québec H3C 3P8,
Canada. aledam@noos.fr
Julien.Dutant@paris4.sorbonne.fr
benoithv@iquebec.com
dnicolas@gmx.net
Benjamin.Sylvand@paris4.sorbonne.fr
http://www.institutnicod.org
http://benoithv.free.fr
http://d.a.nicolas.free.fr/research
Abstract: Contra Jackendoff, we argue that within the parallel architec-
ture framework, the generality of language does not require a rich con-
ceptual structure. To show this, we put forward a delegation model of spe-
cialization. We find Jackendoff’s alternative, the subdivision model,
insufficiently supported. In particular, the computational consequences of
his representational notion of modularity need to be clarified.
In Jackendoff’s framework in Foundations of Language (2002),
understanding the meaning of a sentence consists in constructing
a representation in a specific cognitive structure, namely, Con-
ceptual Structure (CS). CS is not dedicated to language, though.
It is the structure that carries out most of our reasoning about the
world. According to Jackendoff, this follows from what we call the
Generality of Language Argument (GLA):
1. Language allows us to talk about virtually anything.
2. Every distinct meaning should be represented within CS.
3. CS must contain our knowledge about everything it repre-
sents.
4. Hence, CS contains large bodies of world knowledge: CS is
“rich.”
For instance, if the difference between “to murder” and “to as-
sassinate” is that the second requires a political motive, then CS
contains knowledge about what it is to be a political motive (Jack-
endoff 2002, p. 286).
GLA excludes the idea that there is a specifically linguistic level
of semantics, containing only a “dictionary meaning” as opposed
to “encyclopedic information” (Jackendoff 2002, p. 285). It also
excludes a minimal view of CS. We call minimal a CS that is able
to represent all distinct meanings, but is not able to carry out com-
putations other than the logical ones. A minimal CS could repre-
sent the meanings of “x is an elephant” and “x likes peanuts,” but
would not be able to infer the second from the first.
We think that GLA is wrong: The generality of language is com-
patible with a minimal CS. Indeed, it is a viable possibility within
Jackendoff’s general architecture of the mind. Consider the sen-
tence: “The elephant fits in the mailbox.” To know that it is wrong
is to represent its meaning and judge it to be false. Jackendoff
would say that these two steps are carried out by different struc-
tures, namely, CS and Spatial Structure (SpS). Since only CS in-
teracts directly with language, the sentence has to be translated
into CS. From there it can in turn be translated into a represen-
tation in SpS. This would be done by dedicated interfaces. SpS is
the place where the sentence is found false, for it is impossible to
create a well-formed spatial representation of an elephant in a
mailbox. We regard this as an instance of a delegation model:
(DM) Domain-specific computations are carried out outside CS, but
their result is represented in CS, and may thus be expressed in lan-
guage.
In this case the computation is very simple. It consists of checking
whether an adequate SpS representation can be formed. Never-
theless, it is done outside CS. CS only represents its result, namely
that the elephant does not fit in the mailbox.
It is a priori possible that DM applies to all the computations
involved in our knowledge about physical objects, biological kinds,
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
683
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

other minds and so on. The resulting CS would be minimal.
Hence, premise (3) is false: CS could represent meanings without
containing world knowledge.
Jackendoff does not address this question. Instead, he directly
proposes an alternative model for specialization. For instance, he
takes social cognition as involving a specialized mental structure.
But he claims that this is a substructure of CS, a “sub-specializa-
tion” (Jackendoff 1992a, Ch. 4). We call this the subdivision
model:
(SM) Domain-specific computations are carried out within parts of CS,
and can thus be expressed in language.
If most of our reasoning about specific domains has to be car-
ried out within parts of CS, then CS has to be rich. But why should
it be so? Jackendoff could put forward two distinct hypotheses.
The computational unity hypothesis claims that CS is a compu-
tational module, with a unique processor, and that sub-specializa-
tions are representational modules, that is, knowledge bases about
specific domains.1 On this hypothesis, domain-specific inferences
are construed as logical inferences based on domain-specific
premises and effected by a single processor, and this is why they
are part of CS. However, such a claim is far from being uncontro-
versial. Many cognitive psychologists argue that putative “sub-spe-
cializations” such as Theory of Mind, carry out their computations
independently of each other in a relatively autonomous way, and
are possibly situated in distinct, dedicated neural structures
(Leslie 1994; Segal 1996). Moreover, if the processor were dam-
aged, it seems that one would lose all propositional computational
abilities at once. But this pathology has not been observed.
A weaker hypothesis is that of a unique representational format.
Jackendoff (2002, p. 220) seems to endorse it. It merely claims that
all sub-specializations of CS share a common, propositional for-
mat and that all corresponding computations are of a quantifica-
tional-predicational character. Their computations need not be
carried out by a common processor. However, we do not think that
this view has any more plausibility than the hypothesis that some
sub-specializations have their computations carried out in sui
generis formats that are designed for the tasks that they solve. Our
understanding of each other’s minds plausibly involves proposi-
tional representations, but this may be the exception rather than
the rule. Moreover, it is not clear whether CS would, in this view,
constitute a module in any interesting sense, or whether the hy-
pothesis really differs from generalized delegation and a minimal
CS.
To conclude, within Jackendoff’s architecture of the mind, the
generality of language is compatible with either a rich or a mini-
mal CS. The choice of the former requires that the computational
consequences of Jackendoff’s representational notion of modu-
larity be at the very least clarified.
ACKNOWLEDGMENTS
Thanks to Roberto Casati for setting up a workshop on Jackendoff’s work,
and to Ray Jackendoff for discussing issues related to the present argu-
ment. Justo acknowledges support by CONICET and Fundación Antor-
chas, Argentina.
NOTE
1. For further discussion of representational (or intentional) and com-
putational modularity, see Segal (1996).
Neuropsychological evidence for the
distinction between grammatically relevant
and irrelevant components of meaning
David Kemmerer
Deparment of Audiology and Speech Sciences and Department of
Psychological Sciences, Purdue University, West Lafayette, IN 47907-1353.
kemmerer@purdue.edu
Abstract: Jackendoff (2002) argues that grammatically relevant and irrel-
evant components of meaning do not occupy distinct levels of the seman-
tic system. However, neuropsychological studies have found that the two
components doubly dissociate in brain-damaged subjects, suggesting that
they are in fact segregated. Neural regionalization of these multidimen-
sional semantic subsystems might take place during language develop-
ment.
Jackendoff’s Foundations of Language is, without a doubt, a mon-
umental achievement. It both clarifies and begins to fulfill the
deeply pressing need for integration not only within linguistics but
also between linguistics and the connected disciplines of psychol-
ogy, neuroscience, and evolutionary biology.
Here I concentrate on the relation between linguistics and neu-
roscience. Although Jackendoff points out that a great deal has
been learned about the functional organization of various aspects
of language in the brain, he doesn’t devote much space to explor-
ing how these findings can shed light on current issues in linguis-
tic theory. To illustrate the potential applications of recent neu-
rolinguistic research, I present an example that bears directly on
two theoretical topics that are near to Jackendoff’s heart: the syn-
tax-semantics interface, and the basic architecture of the seman-
tic system.
As Jackendoff observes, many linguists have been converging
on the notion that grammatical constructions consist of mor-
phosyntactic patterns that are directly associated with schematic
meanings; and, in order for a word to occur in a given construc-
tion, its own meaning must be compatible with that of the con-
struction (Goldberg 2003). Consider the well-known locative al-
ternation:
(1) a. Sam sprayed water on the flowers.
b. Sam dripped water on the flowers.
c. *Sam drenched water on the flowers.
(2) a. Sam sprayed the flowers with water.
b. *Sam dripped the flowers with water.
c. Sam drenched the flowers with water.
The construction in (1) has the broad-range meaning “X causes
Y to go to Z in some manner,” whereas the one in (2) has the broad-
range meaning “X causes Z to change state in some way by adding
Y”; each construction also has a network of more restricted nar-
row-range meanings that are essentially generalizations over verb
classes (Pinker 1989). Spray can occur in both constructions be-
cause it encodes not only a particular manner of motion (a sub-
stance moves in a mist) but also a particular change of state (a sur-
face becomes covered with a substance). However, drip and
drench are in complementary distribution, for the following rea-
sons. One of the narrow-range meanings of the first construction
is “X enables a mass Y to go to Z via the force of gravity,” and this
licenses expressions like drip/dribble/pour/spill water on the
flowers and excludes expressions like *drench water on the flow-
ers. Similarly, one of the narrow-range meanings of the second
construction is “X causes a solid or layer-like medium Z to have a
mass Y distributed throughout it,” and this licenses expressions
like drench/douse/soak/saturate the flowers with water and ex-
cludes expressions like *drip the flowers with water.
According to the Grammatically Relevant Subsystem Hypoth-
esis (GRSH), a fundamental division exists between, on the one
hand, semantic features that determine the compatibility between
verb meanings and constructional meanings, and on the other, se-
mantic features that capture idiosyncratic nuances of verb mean-
ings, for example, the featural distinctions between drip, dribble,
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
684
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

pour, and spill, and between drench, douse, soak, and saturate
(Pinker 1989; see also Davis 2001; Hale & Keyser 1993; Mohanan
& Wee 1999; Rappaport Hovav & Levin 1998).
Jackendoff cites Pinker’s (1989) analysis of verb-based con-
structions approvingly, but he is apparently skeptical of the
GRSH. In Foundations he states that the hypothesized indepen-
dent level for grammatically relevant meaning “exhibits no inter-
esting semantic constraints beyond its coarseness relative to lexi-
cal distinctions” (p. 290), and he offers the following alternative
proposal: “The subset of semantic features relevant to grammar is
just the subset that is (or can be) mentioned in phrasal interface
rules” the part of conceptualization that is “visible” to these rules?
(p. 291).
Now, if grammatically relevant and irrelevant components of
meaning are segregated, as the GRSH maintains, then they are
probably subserved by at least partially distinct neural structures.
Therefore, it should be possible for them to be impaired inde-
pendently of each other by brain damage. I have been conducting
a series of studies with aphasic subjects to test this prediction, and
have obtained results that are consistent with it. The first study fo-
cused on the locative alternation and revealed the following dou-
ble dissociation (Kemmerer 2000a). One subject failed a verb-pic-
ture matching test that evaluated her ability to discriminate
between grammatically irrelevant aspects of verb meanings (e.g.,
drip-pour-spill) but passed a grammaticality judgment test that
evaluated her knowledge of the grammatically relevant semantic
features that determine which constructions the very same verbs
can occur in (e.g., Sam spilled beer on his pants vs. *Sam spilled
his pants with beer). In contrast, two other subjects manifested
the opposite pattern: They passed the matching test but failed the
judgment test. Moreover, their errors on the judgment test were
most likely due to grammatical-semantic rather purely syntactic
deficits, because they performed well on a separate test that ad-
dressed simple clausal syntax. Three subsequent studies focusing
on various constructions found robust one-way dissociations in-
volving subjects who passed tests of grammatically irrelevant
meaning but failed tests of grammatical relevant meaning (Kem-
merer 2000b; 2003; Kemmerer & Wright 2002; see Breedin &
Saffran 1999; Marshall et al. 1996, for additional reports of the re-
verse type of dissociation; see Druks & Masterson 2003; Shapiro
& Caramazza 2002, for other pertinent studies).
Although this research has just begun, the initial findings sup-
port the GRSH and challenge Jackendoff’s view. It is possible,
however, that the two competing positions could eventually be
reconciled in the following way. The neural structures that im-
plement grammatical semantics might not be genetically pro-
grammed for this function; instead, through as yet unknown
mechanisms of self-organization (perhaps like those simulated by
Kohonen networks), these structures might become functionally
specialized over the course of language development as the child
formulates increasingly abstract semantic generalizations over
verb classes that are associated with certain morphosyntactic
frames. This kind of approach could accommodate not only the
neuropsychological data, but also recent typological data on ex-
tensive crosslinguistic variation in grammatical semantics (Croft
2001; Haspelmath 2003; Slobin 1997; Zhang 1998), as well as re-
cent psycholinguistic data on the acquisition of grammatical con-
structions (Tomasello 2003).
Finally, and on a more positive note for Jackendoff, neurosci-
entific studies strongly support his proposal (p. 350) that certain
semantic features of action verbs are not algebraic but rather mo-
toric and visuospatial in character (e.g., Breedin & Saffran 1994;
Kable et al. 2002; Kemmerer & Tranel 2003; Pulvermuller et al.
2001; Rizzolatti et al. 2001; Stamenov & Gallese 2002; Tranel et
al. 2003).
Interestingly, these semantic features tend to be grammatically
irrelevant, a point that Jackendoff recognizes and that deserves
closer attention from scholars in both linguistics and cognitive
neuroscience.
A mixed treatment of categoricity and
regularity: Solutions that don’t do justice to a
well-exposed complexity
René Joseph Lavie
UMR 7114 Modèles, Dynamiques, Corpus (MODYCO), Université Paris 10 et
CNRS, 92000 Nanterre, France. rlavie@waika9.com
Abstract: Jackendoff’s position with respect to categories (for lexical items
and larger constituents) is unclear. Positing categories is (1) implausible in
several respects; (2) it makes the binding problem in language seem more
massive than it actually is; and (3) it makes it difficult to explain language
acquisition. Waiting for connectionism to fulfill its promise, a different
track is sketched which is residually symbolic, exemplarist, and analogy-
based.
This commentary bears only on Jackendoff’s position on cate-
gories in Foundations of Language (Jackendoff 2002), although
there would be much to say on other subjects. (For example, how
is the simplest metonymy to be accounted for with the overly sim-
plistic vision of semantics that is advocated?) I will understand
“category” – following conventional usage in linguistics – as lexi-
cal categories, grammatical categories (including rules), and func-
tional categories.
While several authors today are giving up categories – or mak-
ing efforts to that end – Foundations takes a position on cate-
gories which is not entirely clear to me. On p. 24, speaking about
“the theoretical claims” that “words belong to syntactic catego-
ries” and that “words group hierarchically into larger constituents
that also belong to syntactic categories,” Jackendoff reminds us
that many different notations (trees, bracketed expressions,
boxes) may be used. A possible reading of the passage is that Jack-
endoff is endorsing the claim itself (besides the variety of nota-
tions, there would be, unarguably, a categorical structure). But, in
many other places in the book, it is clear that the author takes the
necessary distance with respect to categories. However, in Chap-
ter 5 “The parallel architecture,” which is central to the definition
of Jackendoff’s proposal, lexical categories are pervasive in the
text; there isn’t an explicit statement that they are rejected by this
theory, nor is there an explicit statement showing how linguistic
phenomenology is to be accounted for without categories. In gen-
eral, the author’s statement of the “massiveness of the binding
problem” (addressed below in this commentary) can be under-
stood only under the assumption of categories. In short, the book
ultimately seems to me to be ambiguous as to whether it endorses
lexical categories (then, how would that be compatible with the
difficulties that Jackendoff himself raises?), or whether it rejects
them (in which case, I am not sure I perceive what theoretical de-
vices are called for, for a precise account of linguistic phenome-
nology).
In any case, there is a theoretical obstacle to positing categories:
that of implausibility, recognized by Jackendoff himself. “It is ob-
vious that speakers don’t have a direct counterpart of the symbol
NP in their heads” (p. 24).
There is also the obstacle of coping with the linguistic facts. The
evidence is abundant, for example, in the decades of work done
by Maurice Gross at the University of Paris 7, which showed that
in French there are no two verbs with exactly the same distribu-
tional behaviour (Gross 1975, p. 214). It may be the case, however,
that attaching lexical items to several categories, with multiple in-
heritance – as proposed in Foundations – makes it possible to ad-
dress the variety of distributional behaviours, but this remains to
be shown through detailed work on extensive linguistic data. Still,
there would remain problems with plausibility, learnability, and
language change.
Constructions, as proposed in Foundations, are categorical in
the sense that they are abstract, and based on the lexical cate-
gories. However, the proposed theory seemingly accepts – as does
Goldberg (1995) – as many constructions as wanted, and orga-
nizes them into an inheritance lattice (pp. 183–87). This reduces
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
685
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

the categoriality of the theory without nullifying it. No doubt it
provides the model with enough flexibility for a faithful synchronic
account of a language: Make as many constructions as needed,
with as many inheritance links as needed. But the prediction is
that it will resist explaining language change and acquisition be-
cause the process of modifying a lattice of constructions – to say
nothing of just establishing it – can only be a complicated one. The
prediction turns out to be true: In pages 189–90, Jackendoff ad-
dresses learnability issues; he makes a fair summary of the data on
acquisition which is available and concludes that “the gap is still
not yet bridged” and that he has not “provided a thorough account
of language acquisition.” I come back to this point below.
The issue of reducing categoriality is also at stake, in a way, with
the proposition “to blur the distinction between lexical items and
what have traditionally been regarded as rules of grammar”
(p. 180). I have not evaluated to what degree this is workable, but
it may well be, and if it is, it certainly reduces categoriality in an
interesting way: It is a valuable step in the direction of the much-
wanted reconciliation of data (the static face of linguistic knowl-
edge) and processes (its dynamic face).
To view the matter simplistically, connectionist modelling is
where an alternative to categoric accounts is most likely to obtain,
ultimately. Yet, Markus (2001) showed that connectionist models
have not yet provided three base mechanisms which are manda-
tory to account for cognition in general, and language in particu-
lar – this point is very well recalled in Foundations, pp. 62–64. So,
today, it is not possible to simply abandon symbolic accounts for
connectionist accounts.
In my doctoral dissertation, Le Locuteur Analogique (The Ana-
logical Speaker, Lavie 2003), I provide a ruleless and category-free
account of language productivity. It is residually symbolic, and
willingly so. It greatly alleviates the problem of binding as stated
in Foundations, pages 58–60. In effect, among the several causes
generating a need for binding, Foundations includes the need to
bind instances and types (i.e., categories) together. Jackendoff
identifies this as the main cause of “the massiveness of the bind-
ing problem.” The model I propose posits no categories (and, as a
corollary, no rules); all the computation takes place among exem-
plars and occurrences. This alone suppresses the need to bind in-
stances to types. Therefore, there is still a certain amount of bind-
ing required, but it ceases to be as massive as deemed by
Jackendoff. Reducing the want for binding in this way makes a
step toward plausibility.
On page 186, Jackendoff writes:
I am [sic] must admit to being uneasy with claiming that the pressure
on lexical items from regular l-rules plus historical contingency are to-
gether enough to account for the overwhelming syntactic regularity of
idioms. Historical contingencies surely are responsible for some irreg-
ular idioms . . . evidence from lexical memory can now be brought to
bear on the properties of general categorization. I take such potential
unification to be a reason for optimism, as it draws a principled con-
nection between two pre-existing robust lines of research.
I think that there is not that much about which to be uneasy. If
rules and categories are excluded from the explanation, and con-
tingency (historical and otherwise) is re-acknowledged as under-
lying all language dynamics, then it becomes possible to see lexi-
cal items, far from undergoing “pressure from regular l-rules,”
rather, as actively participating in productive processes that are
mixed in the sense that they will produce outcomes that some-
times exhibit regularities and sometimes irregularities (as per-
ceived from a given analytical standpoint). The way to achieve this
is perhaps through recognition of inheritance, but not by installing
inheritance hierarchies explicitly in the theory (Jackendoff him-
self claims [pp. 185–86] that “there are no overt inheritance hier-
archies in the brain”). On the contrary, lexical contingency and the
empowerment of the lexicon are achieved by obtaining inheri-
tance effects (along with categorization effects, regularization ef-
fects, etc.) and by founding the base inscriptions (I do not write
“representations”) and base dynamics on something antecedent:
analogy. The latter has to be backed by contingent, exemplarist
paradigmatic links, exerting exemplarist co-positionings of terms,
and by abductive movements, the combination of which produces
the overall language effects we are seeking. Doing so does indeed
“draw principled connections between pre-existing robust lines of
research,” one of them being analogy, a respectable, bimillenary
theme in linguistics (e.g., studied by Varro, Paul, Brugmann, Saus-
sure, Bloomfield, etc.; cf. also Householder [1971]; Itkonen &
Haukioja [1997]), which has been despised and unfortunately
ruled out by other influential theoreticians of linguistics through
most of the twentieth century. It also connects interestingly with
more recent work in neighboring fields (cf. Gentner et al. 2001;
Choe 2003; for the latter, one important function of the thalamus
is to process analogies).
A theory based on exemplarist inscriptions (and therefore, re-
jecting rules, templates, constructions, etc.) has another impor-
tant benefit. Above I quoted Jackendoff refraining from pre-
tending to have filled the gap of language acquisition. As he
summarizes acquisition data (pp. 189–90), he rightly mentions re-
sults, notably Tomasello’s, which show that the emergence of a
new construction happens one word at a time instead of “popping
into place.” This constitutes a strong push to dismiss rules and ab-
stract constructions, favoring instead mechanisms based on ex-
emplars, such as the ones I propose. Doing so also provides a
straightforward explanation of the sigmoid curve (or logistic
curve), which governs the appearance, spreading, and generaliza-
tion of a new “structure” in the observed productions of young
speakers.
The good news with Foundations is that, except for a timid “per-
haps” (p. 57), it makes no claim that probabilities would play an
explanatory role in linguistic theory – contra a number of authors
who called on probabilities over the last decade, in a desperate ef-
fort to cope with variety and variation after realizing that categor-
ical theories fall short on this count.
Finally, if I have sounded negative in my critique, this is because
I chose to concentrate solely on categoricity. This must not hide a
global esteem for Foundations. In particular, the idea (after
Selkirk [1984], van Vallin [2001], and Sadock [1991]) that linguis-
tic structure is multidimensional – that is, that it is made up of sev-
eral complementary, simple hierarchical structures – is certainly
a very sound and important one. It deserves being fleshed out in
a noncategorical manner.
“Parallel architecture” as a variety 
of stratificationalism
David G. Lockwood
Department of Linguistics and Germanic, Slavic, Asian, and African
Languages, Michigan State University, East Lansing, MI 48824.
lockwoo8@msu.edu
Abstract: The model of parallel architecture for language presented by
Jackendoff is a kind of stratificational model in the spirit of Sydney Lamb.
It differs from the more usual stratificationalism most importantly in its
clear commitment to nativism, though the variety of nativism is greatly
modified from what is more usual among Chomskyans. The revised model
presents a potential for fruitful discussion with proponents of stratifica-
tionalism, and the potential for enrichment via a relational implementa-
tion.
The striking thing about Jackendoff’s Foundations of Language:
Brain, Meaning, Grammar, Evolution (2002), from my viewpoint,
is its similarity to the work of Sydney Lamb – to such a point 
that dialogue between supporters of the respective views be-
comes much more feasible than in the past. It can honestly be said
that the “parallel architecture” model that Jackendoff proposes
amounts to a variety of stratificational theory.
The only citation of Lamb’s work in the book, however, is Lamb
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
686
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

1966, mentioned (p. 128) as an early model outside the tradition
of generative grammar, sharing Jackendoff’s current view of par-
allel architecture for different structural components. But apart
from this fundamental similarity, several other important points of
similarity emerge in this work:
1. Retreat from the insistence on derivational/transformational
rules in recognition of their lack of realism.
2. Serious attention to the need for a model of linguistic com-
petence to be made usable as the basis for a model of perfor-
mance.
3. Adoption of a more constrained view of Universal Grammar
falling far short of the innateness of abstract grammatical cate-
gories, as envisioned in the Principles and Parameters model. Be-
sides being constrained, Jackendoff’s view is much more articu-
lated, and is presented with a scenario explaining how it might
have developed incrementally in human evolution, rather than ap-
pearing suddenly and mysteriously as a whole.
4. Conclusion that the distinction between “rules” of language
and lexical items does not have to be seen as so fundamental, as in
more orthodox Chomskyan models.
5. A view of the lexical item more like Lamb’s version of the lex-
eme, including the consideration of a possible separate internal
syntax for the word (morphotactics).
6. A view of semantics that includes many aspects treated as
syntactic in more orthodox generativism and sees the possibility of
going beyond single sentences to take discourse relations into ac-
count.
7. Serious attention to the relation of language to other matters
of neural functioning like visual perception.
There is no indication that Jackendoff is aware of Lamb’s more
recent work (as summarized in Lamb 1999). Lamb’s interest in re-
lating his model of language to the brain began in classroom pre-
sentations and public lectures in the late 1960s. It took about 30
years, however, before Lamb was sufficiently satisfied to publish
the results, though his product is a textbook introduction to what
he now terms “neuro-cognitive linguistics,” rather than a research
monograph. The neuro-cognitive model is a development of the
stratificational which “uses mainly linguistic evidence but at-
tempts also to integrate the findings from psycholinguistics and
neurolinguistics” (1999, pp. 7–8).
Lamb’s relational networks are more sophisticated and less lim-
ited than those of the connectionists cited by Jackendoff. In dis-
cussing recent attacks against this form of connectionism, Lamb
states: “We shall see that some of them are based on misunder-
standing of connectionism – or at least, of what connectionism
ought to be – while some of them, along with additional evidence,
oblige us to refine the theory” (1999, p. 4).
Though excluding Lamb’s more recent work, Jackendoff nev-
ertheless attempts to synthesize a remarkable breadth of research
areas from different disciplines. More attention to Lamb’s work,
however, would not only open up possibilities of an enriched dia-
logue among scholars, it would provide a potential for bringing in
an implementation of the model in a way that has a chance to ul-
timately relate to the neural connections involved in the brain.
The parallel architecture model has justifiably retreated from the
old model of derivational rules, which constituted more a mathe-
matical abstraction than a realistic way to look at language as a sys-
tem acquired and used by humans. Lamb’s more recent model,
however, provides a way to relate language modeling more posi-
tively to neural facts, and it would be well worth examining how
relational networks of the Lambian sort could be used to imple-
ment this model.
The most fundamental difference between Jackendoff’s strati-
ficational model and Lamb’s concerns nativism. Lamb has always
been skeptical of claims of innate universal grammar, while Jack-
endoff sees nativism as the most essential feature of the older
Chomskyism to be retained. Still, he presents a modified and ar-
ticulated variety of the latter, and Lamb has always been more
concerned with questions of language structure than with lan-
guage acquisition, meaning that his model is not totally incom-
patible with nativism in general. Dialogue based on this model is
much more feasible than with relation to older, more monolithic
forms of nativism.
Cartesian and empirical linguistics: 
The growing gulf
Eoghan MacAogáin
Linguistics Institute of Ireland, Dublin 2, Ireland. eoghan@ite.ie
http://www.ite.ie
Abstract: Jackendoff’s Foundations of Language: Brain, Meaning, Gram-
mar, Evolution (2002) achieves a major shift in the focus and methods of
Generative Linguistics (GL). Yet some of the original restrictive features
of GL, cognitivism and Cartesianism in particular, remain intact in the new
work and take on a more extreme form with the addition of a phenome-
nalist ontology.
Jackendoff’s Foundations presents a striking new view of language
as a component in a general theory of mind. By taking a more
piecemeal interpretation of the traditional formalisms of Genera-
tive Linguistics (GL) and supplementing them with a new se-
mantics, it opens up GL to a wide range of research areas in cog-
nitive science. Since Foundations is also a true product of the GL
tradition, certain of its chapters (notably Chs. 3, 5, 6, 11, and 12)
also make an excellent introduction to state-of-the-art GL for lan-
guage researchers in other disciplines.
But some traditional principles of GL, strongly at variance with
the objectives of interdisciplinarity, have passed unchanged into
Foundations. The first is “cognitivism” (Keijzer & Bem 1996;
MacAogáin 1999), the practice of referring to all linguistic com-
petences as “knowledge” or “cognition,” including those that are
unconscious. The difficulty with cognitivism is that it leaves us
with only one form of activation, regardless of how levels and in-
terfaces were ascribed to the structure so activated. All we ever
have is the whole lot “running off” as a unit in f-mind. But in or-
der to model the most elementary of behaviour systems, we al-
ready need several forms of activation that are irreducibly differ-
ent; two at the very least to correspond to perceptions and wants.
In addition, we need a separation between forms of activation that
are belief-forming or want-forming from those that merely deter-
mine content. In spite of the mentalistic idiom, these distinctions
are well established in neurology, down to the invertebrates, and
are separable also in psychological models of cognition, inference,
learning, and decision-making, which embody notions of rein-
forcement and adaptation.
While cognitivism can be defended in a lot of cognitive science,
wherever truth, value, and reinforcement are well-defined in the
task environment, in GL, where grammaticality is all we have, its
effect is to split language off irretrievably from behaviour and the
environment, as is acknowledged by the f-prefixes of Foundations.
The cognitive linguist can claim to be already working on the phys-
icalist account, in conjunction with the brain sciences. But adding
neurological glosses to the cognitivist account leaves it just as iso-
lated as it was before from the quantitative study of language as a
form of response to the environment.
The isolation is compounded in Foundations by the addition of
a phenomenalist or “conceptualist” ontology, most explicitly in the
attack on the notion of external object (Ch. 10). Phenomenalism
retreats from the external world to the world as perceived by in-
dividuals, or in the language of Foundations, it pushes the world
back into the mind (p. 303). This is necessary, according to Jack-
endoff, to open up the border between GL and psychology and
thus to “integrate semantics with the other human sciences”
(p. 329).
The suggestion is that psychology, and perhaps other human
sciences, are phenomenalist in nature. “Psychological” (as op-
posed to “philosophical” or “truth conditional”) is Jackendoff’s
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
687
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

preferred term for “phenomenalist” in other contexts (e.g., Jack-
endoff 1992a, p. 158). An argument offered in Foundations
(p. 304) is that the study of mental events independently of possi-
ble referents in the real world – undoubtedly a common practice
in psychology and linguistics – is already the beginnings of phe-
nomenalism.
But while some areas of psychology may be able to get by with
a phenomenalist ontology, those of most interest to linguists,
namely perception, cognition, and language, are not among them.
Here the psychologist very often doesn’t even care about the ex-
act nature of the internal events, as long as the responses come out
right. The important thing about language is that its phrases and
sentences are keyed to the same events in the environment for all
of its users, regardless of what goes on in their heads.
The phenomenalism of Foundations will ensure that it will pre-
serve the traditional Cartesian aloofness of GL from behaviourist
and corpus-based approaches to language, in spite of the new in-
terdisciplinary forum it has created. Neither does Foundations of-
fer any points of contact with applied linguistics. To take a single
example, the search for a sensible lexical and phrasal semantics,
so central to Foundations, is going on with equal urgency in mod-
ern lexicography, a new corpus-based discipline with strong links
to the empirical study of second-language learning (e.g., Humblé
2001). Yet it seems that neither can help the other.
The difference between Cartesian and empirical linguistics can
be illustrated by asking how “valid” Foundations is, in the old psy-
chological sense of really being about what it claims to be about.
How representative are the numbered phrases and sentences of
English in Foundations, of the English language as a whole? With
the ready availability of corpora of high validity for the major lan-
guages, it is significant that linguists in general are still not re-
quired to estimate for us the percentage of the language that their
structures will cover, and the size of the remaining piece that will
require a different treatment. The relativity of structures to indi-
vidual languages poses the same problem. It is fortunate that En-
glish had the international status to allow GL to direct so much of
its efforts on a single language. But even if English were the only
language in the world we would still have the validity problem. As
Jackendoff puts it, “there are just so many goddamned words”
(2002, p. 377). We need some assurance that our semantic struc-
tures have invariance over a good part of the language we are
studying, and are not trapped in lexical pockets. And ideally we
would like to know which structures have some chance of being
invariant over languages, or at least a few languages from differ-
ent groups.
Perhaps the explanation for the gulf between Cartesian and em-
pirical approaches is that the terms “pure” and “applied” have a
special meaning in linguistics that does not imply that the theory
and its referent remain the same across domains. “Pure” in lin-
guistics could also mean that we are dealing with a different kind
of reality. Foundations makes the case more strongly than ever be-
fore.
How Jackendoff helps us think
Carlos Molina
Instituto de Filosofía, Pontificia Universidad Católica de Chile, Santiago,
Casilla 316, correo 22, Chile. cmolinac@puc.cl
Abstract: The nature of the relationship between language and thought
has been quite elusive. We believe that its understanding is crucially de-
pendent on the available notions of language and thought. Foundations of
Language offers an unusually clear and complete account of both, provid-
ing a fruitful and much needed framework for future research. No doubt
it will help us think better about these elusive complexities.
In a recent article published in this journal, philosopher Peter
Carruthers put forward the hypothesis that natural language
(more specifically, the Logical Forms [LF] underlying natural lan-
guage sentences) is the mechanism that enables what he calls “in-
termodular and non-domain-specific thinking” (Carruthers 2002).
According to this view, each domain-specific-module can translate
its mentalese thoughts into LFs due to the language faculty. This
common format, in turn, is what enables the combination and in-
tegration of information from different modalities to occur.
I believe that one of the reasons that Carruthers appeals to LFs
is the prestige of Chomsky’s theories. In some respects this pres-
tige is fully deserved, but because of his selection, Carruthers is
“limited” by a theory that has no semantic motivations, and which
is not concerned with linguistic performance (only competence).
And all this in spite of the fact that his main concern is to under-
stand “how we think.”1 On the other hand, Carruthers’ hypothe-
sis is based on a syntactocentric theory: All generative power
comes from syntactic structure alone; the semantic and phono-
logical components are derived from it.
One of Jackendoff’s main concerns in his new book, Founda-
tions (Jackendoff 2002), is to provide a critical view of the syntac-
tocentric viewpoint that permeates modern linguistics and the iso-
lation from the discipline that it has imposed on the rest of the
mind/brain sciences. In what I see as an important methodologi-
cal lesson of Foundations, we must begin our theorizing by estab-
lishing the boundary conditions of a specific problem (e.g., by
“thought” we understand such and such . . .). Only then will we be
able to see what kind of architecture may arise from it, while mak-
ing as few assumptions as possible. In a certain sense, this is what
Foundations is all about – and on a massive scale.
In Foundations, as well as in the author’s previous work (Jack-
endoff 1987; 1996; 1997, Ch. 8), the whole problem of the rela-
tionship between language and thought is expressed with what I
consider to be unusual clarity. In this framework, semantics,
phonology, and syntax are different and autonomous computa-
tional spaces, each connected to the others by interface rules.
Here, the locus of thought is at the level of Conceptual Structure,
and this is where the integration of information – one of Car-
ruthers’s main concerns – takes place. It is the combinatoriality at
the level of Conceptual Structure, and not at the level of Syntac-
tic Structure (or LF based on syntactic structures), which enables
the integration of conceptual information in the generation of
more complex thoughts. In addition, Jackendoff delves into Con-
ceptual Structures and shows us a rich landscape of substructures
composed of different tiers (descriptive, referential, and informa-
tional tiers; Spatial Structure; Qualia structure).
This architecture leaves language with a role in thought which
we believe to be more interesting than the one that a coarse syn-
tactic structure would be capable of providing. It is also a more in-
teresting role than what a hypothetical “semantic level” – distinct
from the conceptual level – would be capable of fulfilling. Jack-
endoff’s proposal is basically that the tripartite nature of language
permits the mapping of unconscious conceptual representations
(through syntactic representations) onto conscious phonological
representations.2 As Jackendoff puts it:
Linguistic form provides one means for thought to be made available to
awareness (another is visual imagery); we “hear the little voice in the
head” and thereby “know what we are thinking.” Notice however that
the form of the awareness in question is essentially phonological. What
we “hear” is words, pronounced with stress patterns. At the same time,
one cannot define rules of inference over phonological structure, so it
is not an appropriate medium for reasoning. The correct level for car-
rying out reasoning is conceptual structure, and reasoning can take
place even without any connection to language, in which case it is un-
conscious. (Jackendoff 2002, p. 274)
In a stimulating article entitled “How Language Helps Us
Think” the author provides some cues on the role of language on
thought:
Language is the only modality of consciousness that makes perceptible
the relational (or predicational) form of thought and the abstract ele-
ments of thought. Through these elements being present as isolable en-
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
688
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

tities in consciousness, they can serve as the focus of attention, which
permits higher-power-processing, anchoring, and, perhaps most im-
portant, retrievable storage of these otherwise nonperceptible ele-
ments. (Jackendoff 1996a, p. 27)
This way of language helping thought seems to be compatible with
phenomenology:
In particular, in speaking, one’s choice of words at the beginning of a
sentence may by feedback refine the formulation of subsequent parts
of the thought; one’s choice of a syntactic structure for realizing the
words affects the order in which the rest of the thought must be refined
. . . As the expression of the thought reaches conscious form (in my the-
ory, phonological structure), one can “hear” it as “inner language” in ad-
vance of uttering it, and quickly re-evaluate it, revise it, or repair it be-
fore producing it publicly. This is experienced as “finding out what one
is thinking by trying to say it.” It is also possible at this point for one to
discover that an utterance is “not exactly what one meant.” (Jackendoff
1996b, p. 204)3
On the other hand, Jackendoff’s framework liberates narrow
syntax from the burden of having to account for the richness of
thought. All semantic distinctions that are reflected in grammar
(morphology, syntax, and phonology) are carried out by mappings
between different levels (which may vary between languages). We
believe such an architecture is highly adaptable to future evidence
on how language might affect thought. It is also compatible with
the idea that learning vocabulary and grammar (i.e., mappings be-
tween phonology, syntax, and meaning) might shape the “inner
conceptual landscape” in a manner that differs substantially from
cognitive systems that lack such devices. As Spelke put it:
Natural languages have a magical property. Once a speaker has learned
the terms of a language and the rules by which those terms combine,
she can represent the meanings of all grammatical combinations of
those terms without further learning. The compositional semantics of
natural languages allows speakers to know the meanings of new wholes
from the meanings of their parts. (Spelke, 2003, p. 306, emphasis
added)
Jackendoff’s ideas seem to run along these lines, with the ex-
ception (I believe) that what Spelke calls the compositional se-
mantics of natural language would be called the compositional or
combinatorial character of thought in Jackendoff’s framework,
and the achievements mentioned are made not by language but
with the help of language, that is, with the help that a lexicon – and
the possibility of mapping conceptual structures onto syntactic
and phonological (conscious) structures – provides in terms of an-
choring, manipulation, and explicitness.
Finally, some remarks on Jackendoff’s methodology. Although
one may not agree with everything he says, his manner of theo-
rizing has one undeniably rare quality: The reader will always un-
derstand what is being said. His concepts are well defined and
troublesome issues are left open rather than being artificially
“solved.” I believe that explicitness and clarity are an important
part of what we call science. Nature is full of patterns, the mind/
brain is a sort of pattern processing device, and thus, when hu-
mans begin to speak clearly about something, suddenly, voilà! –
you have the makings of science. Besides its original ideas on lan-
guage and cognition, and its impressive integrative power, I see
Foundations as a tremendous lesson on scientific discourse.
NOTES
1. Carruthers’s proposals are at least problematic: How does an account
based solely on domain-specific modules and LFs deal with the complex-
ities of “language production,” for which it has been necessary to postu-
late non-verbal processes such as “macroplanning” and “microplanning”?
(Molina 2002). On the other hand, how does this account deal with the fact
that we can have bare or wordless concepts (i.e., concepts that do not have
a word associated with them), such as “the pathetic strands of hair that
some men drape carefully but ineffectively over their bald spots” (Den-
nett 1998, p. 286) or “the moist residue left on a window after a dog presses
its nose to it” (Murphy 2003, p. 389)?
2. For Jackendoff’s concept of consciousness see Jackendoff (1987;
1997, Ch. 8).
3. I am, however, somewhat uncomfortable with the idea that in lan-
guage production, “feedback and attention [are] not possible until there is
a conscious phonological structure available” (Jackendoff 1996b, p. 205).
This is because it is stated that in language production, besides being ca-
pable of monitoring the phonology, syntax, and semantics of the sentences
that reach our inner speech, it also appears to be possible to monitor the
construction of the preverbal message, for which no overt conscious clues
are still available. In other words, it appears that the speaker can directly
monitor the preverbal messages he is preparing to express, and he may re-
ject a message before its formulation has started. As Levelt puts it:
The speaker no doubt also monitors messages before they are sent into
the formulator, considering whether they will have the intended effect
in view of the present state of the discourse and the knowledge shared
with the interlocutors . . . The main work is done by the Conceptual-
izer, which can attend to internally generated messages and to the out-
put of the speech-Comprehension System.” (Levelt 1989, p. 14, em-
phasis added)
What kind of “unconscious” monitoring would this be? Would it be part of
what could be called the “dynamic of thought”?
Grammar and brain
Helmut Schnelle
Sprachwissenschafliches Institut, Ruhr-Universitat Bochum, D-44780
Bochum, Germany. helmut.schnelle@ruhr-uni-bochum.de
Abstract: Jackendoff’s account of relating linguistic structure and brain
structure is too restricted in concentrating on formal features of compu-
tational requirements, neglecting the achievements of various types of
neuroscientific modelling. My own approaches to neuronal models of syn-
tactic organization show how these requirements could be met. The book’s
lack of discussion of a sound philosophy of the relation is briefly men-
tioned.
I agree with Jackendoff (2002) on the main principles outlined in
Foundations of Language: Brain, Meaning, Grammar, Evolution:
The discussion of the foundations of language should be based on
considerations of the brain’s neural circuitry along with its func-
tional properties, on a “two way street” (p. 22); strict reductionism
and autonomous functionalism are inappropriate. The challenges
to cognitive neuroscience presented in Chapter 3, section 3.5, and
the list of basic questions on pp. 422–23, are well selected.
I disagree on the following points: (1) It is not true that only
structures built of (formal symbolic) discrete combinatorial units
(p. 423) can explain the productivity of language (pp. 38–39). (2)
The competing design feature of “brain style modelling” is inap-
propriately characterized by mere reference to a few models
(p. 23). (3) It is not correct that “we don’t even have an idea of how
a single speech sound such as /p/ – much less a category like NP
– is instantiated in neural firings or synaptic connections” (see be-
low). (4) A book on the foundations of language should find some
place for basic philosophical and methodological discussion, and
not merely presuppose standards (Cartesianism, the formal view
of axiomatization). (For a contrasting Leibnizean view, see
Schnelle [1991a; 1991b, Part III; 1996b], and, for an intuitionistic
computational foundation, Schnelle [1988].)
My specific critique will elaborate point 3:
1. The possibility of an analysis based on active and interactive
feature representation units in terms of neuronal groups,
columns, and modules is briefly mentioned in the author’s refer-
ence to Hubel and Wiesel 1968 (see Foundations, p. 23). How-
ever, the author disregards its important role for the representa-
tion of actively interactive features in the Jakobsonian sense
(Schnelle 1997) and their fruitful analyses by Szentagothai,
Mountcastle, Arbib, and others (cf. Arbib & Erdi 2000, Schnelle
1980; 1981), as well as the related computational Theory of Neu-
ronal Group Selection of Edelman (1987, and his subsequent
books).
2. The author also completely neglects neuroanatomic and
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
689
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

neuropsychological approaches relevant for language. (For an
overview, see Schnelle 1996a.)
3. Jackendoff concentrates on computational problems. His list
of eight critical questions about how language is “lodged in the
brain” (p. 422) and his four challenges for cognitive neuroscience
(p. 58) are very much to the point. His main question is whether
and how essential properties of computational combinatoric the-
ories of grammar could be instantiated in active unit interaction
networks, that is, in “brain-style modelling.” He deplores that to
his knowledge these questions and challenges “have not been
widely recognized in the cognitive neuroscience community”
(p. 58). Unfortunately, the author doesn’t seem to have investi-
gated this with care. Among others, my own approaches since
1979, and those of my young colleagues, have addressed precisely
these questions in many publications, including two books. Let me
briefly explain.
4. The first question is basic: “How are discrete structural units
(such as phonological features and syntactic categories) instanti-
ated in the nervous system?” My answer, following the basic idea
of Jakobson (cf. Schnelle 1997), is: Each feature and each category
is represented by a group of (hundreds of) neurons placed in a ge-
netically prestructured internal network (like a Szentagothai col-
umn) and connected by inhibitory and excitatory connections to
other group networks (i.e., representations of other features or
categories). (For the computational technicalities, see my articles
in Theoretical Linguistics during the eighties, but mainly Schnelle
& Doust 1992, Wilkens & Schnelle 1990, and the computationally
elaborate book of Wilkens 1997.) As a result of this external inter-
action with other groups, each group participates at each time in
a distributed network of currently interactive “features.” In the
simplest cases (those of nonsyntactic patterns) the binding proce-
dures often discussed by neuroscientists are sufficient in building
a synchronous activity pattern.
5. In syntactic processes, the situation is different. The com-
putational details can be studied in my publications. Here I em-
phasize their neurobiological organization. Each group or column
representing a syntactic feature contains a subset of neurons func-
tioning as a component of the distributed working memory. Their
current activity states – stable for a time span of a few seconds –
mark the activation status of the feature use as being either in-
choate, in process, or successfully terminated. At the same time,
other neurons of the same group indicate how often a given syn-
tactic process for a sentence has made use of the same syntactic
category represented by the group. The interdependencies of
these working memory neurons also mark the temporal sequence
of these uses. In other words, the collection of the working mem-
ory neurons of the syntactic category group gives the group the
power of an activity unit functioning like a pushdown store with
storage capacity in the range of seconds. In theory, the limitations
to seconds and finite sets of neurons could be neglected. This
would give the system the capacity of a Turing machine.
6. Because this organization represents a syntactic category (by
connected working memory marks of complex activation states), a
category occurring several times in a syntactic structure is not
stored as many times, but rather, is marked by a storage pattern of
working memory neurons, a subset of the group. Groups of this
marked type occur only in certain areas of the brain, for example,
in sulci of frontal areas (such as Broca’s) and of the superior tem-
poral sulcus. Both are involved in syntactic and lexical language
processing.
7. In this way, hierarchical structures are not represented as
composed from passive units but as distributions of syntactic cat-
egories’ modules marked by their sequence of activation. Thus,
working memory does not store but rather, distributively marks,
acts of category involvement.
8. The binding problem and the problem of “brain-style mod-
elling” are easily solved by the appropriate connectivity of the syn-
tactic category representation groups.
9. The solution to the problem of variables is solved by the fact
that distant areas in the brain are connected by bundles of axons
where each axon activates many distant groups (i.e., a class of rep-
resentations for lexeme units, each being represented like a con-
vergence unit, à la Damasio). Only those that have an appropriate
state of (lexemic) pre-activation combine the “request” signal with
pre-activation to generate actual activation.
Thus, I believe I have provided technically and empirically pos-
sible answers to the critical questions and challenges listed in the
book.
Rescuing generative linguistics: 
Too little, too late?
Michael J. Spivey and Monica Gonzalez-Marquez
Department of Psychology, Cornell University, Ithaca, NY 14853.
spivey@cornell.edu
mg246@cornell.edu
http://www.psych.cornell.edu/people/Faculty/mjs41.html
http://www.psych.cornell.edu/people/Graduate_Students/mg246.html
Abstract: Jackendoff’s Foundations of Language: Brain, Meaning, Gram-
mar, Evolution attempts to reconnect generative linguistics to the rest of
cognitive science. However, by minimally acknowledging decades of work
in cognitive linguistics, treating dynamical systems approaches somewhat
dismissively, and clinging to certain fundamental dogma while revising
others, he clearly risks satisfying no one by almost pleasing everyone.
Jackendoff (2002) promises integration. He vows “open-minded-
ness to insights from whatever quarter, a willingness to recognize
tensions among apparently competing insights, and a joint com-
mitment to fight fair in the interests of deeper understanding”
(p. xiii). Yet the most long-standing opposition to generative lin-
guistics, the cognitive linguistics paradigm, and its key insight that
linguistic structure is not separable from meaning, receives scant
recognition in Foundations. In fact, quite a few “tensions and com-
peting insights” are actually given little more than lip service (fre-
quently relegated to footnotes) throughout the book. Jackendoff
regularly acknowledges that generative linguistics has made a
great many problems for itself, both ideologically and empirically,
but insists on maintaining several of its core principles, adding a
great many software patches, as it were, and keeping the name.
With this book, Jackendoff has, perhaps despite his best inten-
tions, allied himself more than ever before with a program whose
rhetorical high-ground and untouchability have faded (indeed, he
recounts much of that fading process himself), and whose time
may be running out. Indeed, it is somewhat surprising that Jack-
endoff, a frequent challenger of certain aspects of the mainstream
doctrine, would be the one to organize a rescue attempt of the
generative linguistics expedition (which has arguably been
stranded at its own “Donner Pass” for some time now).
Early on, Jackendoff reduces the competence/performance
distinction to a “soft methodological” separation (which, with his
addition of “neural instantiation,” begins to resemble Marr’s
[1982] tripartite division of levels of analysis, with crosstalk en-
couraged). With this subtle revision, he manages to reify the no-
tion of linguistic competence at the same time that he takes away
a valuable and convenient defense mechanism that generative lin-
guistics has relied on for decades. He also lists numerous criti-
cisms of the “software versus hardware” analogy for “mind versus
brain” (which has so frequently been used as an excuse to ignore
neurophysiology), but somehow still manages to refer to it as “a
robust analogy.” These, and many similar instances, are clearly the
wafflings of a torn scientist who senses the future but cannot let
go of the past.
For example, Jackendoff’s approach to morphological produc-
tivity, the “remembering” of idioms such as “he kicked the bucket”
and their morphological architecture (Ch. 6) would be a perfect
place for a merger between cognitive linguistics and generative
linguistics. However, what he instead presents are syntactic prob-
lematizations of the issues. He argues to his chagrin that there
must be two kinds of rules at play, those that are fully productive
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
690
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

and those that are semi-productive. Though he admits that even
with these two variations the meanings of, say, denominal verbs,
will not be fully predictable, he maintains that this is the best that
can be done and that the inconsistent outputs must simply be
listed in long-term memory (p. 159). No attempt is made to ac-
count for the unpredictability by other means, that is, conceptual
metaphor, prototype theory. Jackendoff does not make the 
connection because he is still too attached to the notion that lin-
guistic structure is separable from meaning (cf. Janda 1993;
Wierzbicka 1985). If he acknowledged the possibility that struc-
ture can contain meaning, a hypothesis basic to so many thinkers
outside of the generative framework (Halliday 1985; Hopper
1998; Langacker 1987; 1991; Talmy 2000), the problems he posits
in Chapter 6 would not only be tremendously simplified, genuine
strides could be made toward his promised integration.
But the real troubles with Jackendoff’s position run deeper than
that. Two of the basic tenets of generative linguistics that he insists
on perpetuating, mentalism and discrete-variable combinatorial-
ity, do not actually belong solely to Chomsky and generative lin-
guistics; they are in fact deeply rooted in much of cognitive psy-
chology and psycholinguistics. Ironically, these very tenets are
now at the forefront of a massive challenge to traditional cognitive
science, coming from the recent confluence of dynamical, eco-
logical, and embodied approaches to cognition in general (Elman
et al. 1996; Port & Van Gelder 1995; Van Orden et al. 2003).
In his discussion of mentalism (Ch. 2), and how to avoid the ho-
muncular problems endemic to it, Jackendoff sidesteps into an el-
egant description of how all the phonological, syntactic, and se-
mantic properties of a sentence could be encoded as a location in
a high-dimensional space (he even gets amazingly close to de-
scribing graded attractor basins, without using that term). But he
then laments that this static high-dimensional space framework
cannot address the time-dependent nature of one variable being
followed by another variable. In fact, in recent dynamical-systems
accounts of syntax, that time-dependent nature of sentence pro-
cessing is quite nicely accommodated by a trajectory (rather than
a single location) moving through the state-space (e.g., Culicover
1999a; Tabor & Tanenhaus 1999).
In his discussion of combinatoriality and language processing
(Chs. 3 and 7), rather than turning a blind eye to psycholinguistic
data, or invoking a hard competence/performance distinction,
Jackendoff responsibly embraces experimental evidence for con-
tinuous interaction between linguistic “modules” (curiously pos-
tulating “interface modules,” rather than questioning the modu-
larity framework itself; cf. Uttal [2001]). Yet he still maintains the
notion of discrete-variable combinatoriality in language, as though
he were unaware of its collision course with a combinatorial ex-
plosion when faced with the on-line incremental processing of
real-time language comprehension. This combinatorial overload
of working memory is precisely what led researchers to propose
modular syntactic heuristics (e.g., Frazier & Fodor 1978), which,
in the end, did not stand up to tests of interaction with semantics
and discourse (e.g., Spivey & Tanenhaus 1998; Trueswell et al.
1994). Therefore, if incremental language comprehension does
indeed involve an interactive constraint-satisfaction process, then
in order to avoid a combinatorial explosion of discrete “cognitive
entities,” it cannot possibly be using discrete-variable combinato-
riality. Instead, language processing is perhaps better described as
a single trajectory through a high-dimensional space of multiple
competing attractor basins. In such a framework, the combinato-
rial expansion resulting from incremental temporary ambiguity
becomes an issue of graded state-space resolvability (admittedly,
no small problem), rather than an issue of exceeding a countable
capacity limitation in a “working memory module,” and hence no
system overload.
Overall, this courageous and wonderfully written attempt by
Jackendoff to please both sides of a rather bitter war may instead
leave both sides teased and unsatisfied. The book serves as a de-
tailed confession of the limitations of generative linguistics, with
only hints about alternative frameworks that are more successfully
grappling with crucial issues that generative linguistics has either
complicated into oblivion or chosen to ignore. It is unfortunate
that one of the few opportunities he allows himself to directly ad-
dress theories outside of generative linguistics is used to accuse
them of not making “full contact with the overall goals of genera-
tive linguistics” (p. 269). Somehow Jackendoff maintains his alle-
giance to generative linguistics, despite revealing, again and again,
deep insights into the failures and weaknesses of the framework.
One cannot help but wonder, when his rescue attempt reaches this
lost expedition, will they just eat him too?
From Frege to dynamic theories of meaning
Alice G. B. ter Meulen
Center for Language and Cognition, University of Groningen, 9700 AS
Groningen, The Netherlands. atm@let.rug.nl
http://atm.nemit.net
Abstract: In designing stratified models of human language, understand-
ing notions of logical consequence and validity of inference require sepa-
rating the aspects of meaning that vary between models from logical con-
stants. Modelling meaning requires choices regarding the primitives,
where the Fregean program is still offering us the fundamental insights on
the role of truth, judgement, and grasping or sharing of thoughts.
Kudos to Ray Jackendoff for proclaiming so vigorously in Foun-
dations of Language: Brain, Meaning, Grammar, Evolution that
the time has come to haul down the “firewalls” separating linguis-
tic theories into a fragmented multitude of explanatory models,
which restrains us from advancing the ecologically sound research
program of how we understand language (p. 329). It is very much
en vogue indeed to bid farewell to narrow syntax-centered expla-
nation, to make room for interfaces not only to other modules of
grammar, but also to results of psycholinguistic experiments and
of other scientific advances in neuroscience and cognition. The
tangible bottleneck still is to characterize a feasible account of how
these interfaces are to be modelled, what these models are in-
tended to represent and how core concepts at one tier in such
stratified models are mapped onto a next higher tier. Unfortu-
nately, Jackendoff’s efforts to help us pass this bottleneck cannot
serve this otherwise laudable purpose, as explained below.
Frege’s agenda, set well over a century ago (Frege 1892), char-
acterized meaning as whatever it was, disregarding the subjective
associations and contextual factors that determined reference. Ex-
pressions of various syntactic categories, including sentences,
could co-refer and yet differ in meaning. Thoughts, that is, mean-
ings of sentences, could hence be shared in communication to
produce mutual understanding. Concepts, that is, the meaning of
predicates, were considered to be unsaturated functions needing
objects to determine the truth-value of a proposition. Much of the
subsequent century of semantics research was given to explaining
Frege’s notion of meaning, modelled in possible world semantics
of the Carnap-Kaplan-Montague-Kripke-Lewis lineage as func-
tions from primitive possible worlds to extensions of the appro-
priate type. Jackendoff proposes an interesting slant on Fregean
theories of meaning when stating: “Frege’s problem is not a
uniquely linguistic problem; rather, it lies in a more general the-
ory of how the f-mind keeps track of individuated entities”
(p. 314). This more dynamic view of Frege’s theory of meaning has
been developed and advocated by Gareth Evans (1985) – not
mentioned at all in Foundations – who did more to advance our
understanding of meaning and interpretation in his short life span
than most of us with longer life may ever hope to do.
Jackendoff pleads for the primacy of speaker reference, rather
reminiscent of Grice, Donnellan, and Kripke (cf. Hale & Wright
1999), correctly emphasizing a very Fregean notion of judgement
to connect concepts to “reality” (Jackendoff’s percepts). He ad-
mits to coming close to Kamp’s Discourse Representation Theory
(DRT) and its variants of dynamic semantics, since reference gives
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
691
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

the interpreter something to bind descriptive information to. Al-
though DRT accounted nicely for the fact that indefinites in the
scope of negation cannot bind pronouns in subsequent sentences,
for example, “Joan did not buy [a car]1. *[It]1 is a Jaguar,” the box
notation Jackendoff introduces does not use boxes to characterize
accessibility constraints for pronoun resolution, neither does it
have a proper semantics in which valid inferences can be charac-
terized.
We part company when, rather surprisingly, Jackendoff is lead
to conclude that “there is little to be gained in theoretical econ-
omy or explanatory adequacy by making a strict cut between log-
ical and non-logical properties . . . logical properties are just some
heterogeneous subset of semantic properties” (p. 289). What is
logical and what is not depends on one’s choices in modelling
meaning, but anyone must make such choices. If one desires to
model inference as a matter of form, irrespective of content, an
inference is logically valid just in case the premises entail the con-
clusion in every possible model, for its validity must depend only
on its logical constants. Choices of the modeller also determine
the furniture of the models: I concur entirely with Jackendoff’s
“yes” to events and “no” to possible worlds. But he erroneously be-
lieves that variables for places are new with his conceptual se-
mantics (p. 322), for Situation Semantics already introduced loca-
tions as constituents of their situation types (Barwise & Perry
1983). Truly puzzling is his later usage of lambda-abstraction, usu-
ally forming functions from truth-value denoting expressions, but
now filling terminal nodes in conceptual tree structures with se-
mantic objects as labels of higher nodes (p. 389). For this tech-
nique familiar from model theoretic semantics to do any work, the
tree structures must be given a semantics, specifying in a Fregean,
compositional way how functions and arguments may be com-
posed to compute appropriately typed values. It can be adopted
in DRT as well, assisting in separating compositional aspects of
meaning from constraining accessibility in declaring reference
markers, and opening a route to alleviate representation by dele-
gating more to inference rules, as in Blackburn and Bos (1999).
No semantics, no inference; no inference, no interpretation; no in-
terpretation, no understanding!
What we badly need in order to advance the scientific study of
human language understanding is a novel language, a scientific
pidgin to communicate across different tiers of modelling mean-
ing. Our explanatory theories need not be incommensurable, if we
strive to integrate the various explanatory toolkits carefully and
systematically. Because Jackendoff has written his book to pro-
mote a culture of collaboration, he should be responded to by in-
corporating the many useful insights, such as the three aspects,
sensory, material, and constitutive, of his “telic quale” into current
theories of meaning and interpretation.
The architecture is not exactly parallel: Some
modules are more equal than others
Boris B. Velichkovsky,a Andrej A. Kibrik,b and Boris M.
Velichkovskyc
aInstitute of Psychology, Russian Academy of Education, 125009 Moscow,
Russia; bInstitute of Linguistics, Russian Academy of Sciences, 125003 Mos-
cow, Russia; cPsychology Department, Dresden University of Technology, D-
01062, Germany. velich@pirao.ru
kibrik@iling.msk.su
velich@psychologie.tu-dresden.de
http://www.pirao.ru
http://www.philol.msu.ru/rus/kaf/otipl/home/
aakibrik.htm
http://www.applied-cognition.org
Abstract: Despite its computational elegancy, Jackendoff’s proposal to
reconcile competing approaches by postulating a parallel architecture for
phonological, syntactic, and semantic modules is disappointing. We argue
that it is a pragmatic version of the leading module which Jackendoff
would probably prefer, but which he does not explicitly acknowledge. This
internal conflict leads to several shortcomings and even distortions of in-
formation presented in the book.
Foundations of Language: Brain, Meaning, Grammar, Evolution
(Jackendoff 2002) presents a major revision of the syntactocentric
approaches in contemporary linguistics. It may well become an es-
tablished interpretation for the next decades, especially in view of
difficulties with an empirical refutation of such a multicomponent
model. Taking apart the delicate question of what belongs to an in-
terface and what is a core module of one or another domain (the an-
swer may in fact vary for different languages), we would like to com-
ment on another issue central to Jackendoff’s approach: the overall
architecture and its relations to the function of linguistic mecha-
nisms. As in the case of several other, presumably parallel, mecha-
nisms of processing (e.g., dorsal and ventral streams of visual pro-
cessing, or semantic and episodic memories), what is stressed in the
book is the parallelism, not the hierarchical organization of the mod-
ules. Clearly, Jackendoff also undermines systematic asymmetries
in the relationships of the constituent mechanisms.
When considering language use from a functional point of view,
that is, as goal-oriented activity, a distinction between leading ver-
sus background functions should be made. For the whole domain
of language, such a leading function is the communication of
meaning and enhancement of social coordination. Variable and
progressively automated syntactic and phonological processes are
only means to achieve this general goal. This is also what Jack-
endoff emphasizes when he writes, “Phonology and syntax, after
all, evolved to express meaning, which has a far longer evolution-
ary pedigree. Moreover . . . much of conveyed meaning is tacit,
not expressed at all in the words uttered or the syntactic structure
combining them” (p. 428). The testimony to a priority of meaning
evaporates when the modular architecture starts to be discussed.
The problem is not so much that semantics is “only equated” in
its role to phonology and syntax. Strangely, the upper level – var-
iously named as pragmatic, executive, or metacognitive – is nearly
overlooked. When Jackendoff writes that interpersonal tuning
(“attunement,” or also “alignment”; see Pickering & Garrod, in
press) is “the last piece” (p. 322) in the overall picture, our im-
pression is that it has to be the first, and not only in evolutionary
or ontogenetic considerations. For instance, recent experimental
data consistently prove that interpersonal (pragmatic) variables,
such as addressees’ needs, influence syntactic forms early in on-
line processing (Lockridge & Brennan 2002).
There are even some distortions and omissions, amounting to
cognitive scotomata, in this otherwise precise and almost exhaus-
tive text. For example, take Jackendoff’s entire line of analysis of
communication and interpersonal cognition. Jackendoff mentions
the “theory of mind” only once, in a footnote on page 323, where
its development is attributed to the fifth year of life; whereas in
fact it culminates (with a relatively difficult false-belief task) al-
ready in the fourth year (see Perner & Lang 1999, among many
others) and, what is important, it has a long history of earlier
achievements, such as a “social cognitive revolution” at nine
months (documented by Tomasello 1999a), when children start to
consider other people as intentional agents. The related steps are
self-recognition in a mirror test and empathy for others, both im-
portant motives for communication. Further, Jackendoff ignores
completely the rich research literature on joint-attention effects
(not to be confused with the less specific idea of joint intention)
in language development and practical interaction.
The second scotoma is the virtual lack of neurolinguistic and
neuropsychological data although, ironically, “brain” is the first
word in the subtitle of the book. The prefrontal cortex is of par-
ticular importance as the crucial part of interpersonal neurocog-
nition in that its damages correlate with autism and the underde-
velopment of language (e.g., Stuss et al. 2001). Relevant data
demonstrate that prefrontal executive functions go beyond work-
ing memory, being connected to attentive processing, task switch-
ing, and resource allocation (the dorsolateral prefrontal cortex)
and to emotions, self-reference, and social behavior (ventromedial
and frontopolar areas). This is where we should look for brain
mechanisms of not only joint intention, but also joint attention and
creative usage of language in situated collaborative actions.
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
692
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

These examples show how thorny is the path from the one-sided
conceptualizations of the last half-century. The book demon-
strates certain first cautious steps. Universal Grammar (UG) is
now seen as an attractor state. Semantics has been equated to
phonology and syntax. The latter has been reduced, in its proto-
form, to simple (and thoroughly pragmatic) heuristics of the type:
“first noun is the actor” and “focus first” – which is already known
from the literature on child development, as well as from aphasi-
ology (e.g., Velichkovsky 1996). However, for a reader from the
discourse-and-activity-oriented camp, this formidable work is also
an illustration of the attractor power of generative grammar, which
is still preventing the due revision and proper integration of lin-
guistics with the rest of cognitive science. Sooner or later, we will
certainly come from UG to something like UP, that is, the univer-
sal pragmatics of cooperative action, with exact solutions for syn-
tactic peculiarities of different languages (Greenberg 1986; Kib-
rik 2001).
Linear order and its place in grammar
Richard Wiese
Institut für Germanistische Sprachwissenschaft, Philipps-Universität Marburg,
35032 Marburg, Germany wiese@mailer.uni-marburg.de
http://staff-www.uni-marburg.de/~wiese
Abstract: This commentary discusses the division of labor between syn-
tax and phonology, starting with the parallel model of grammar developed
by Jackendoff. It is proposed that linear, left-to-right order of linguistic
items is not represented in syntax, but in phonology. Syntax concerns the
abstract relations of categories alone. All components of grammar con-
tribute to linear order, by means of the interface rules.
1. The problem of linear order. One of the important lines of
thought in Jackendoff’s (2002) book Foundations of Language:
Brain, Meaning, Grammar, Evolution is that Generative Gram-
mar (GG) was wrong in presupposing syntactocentrism, the view
that only syntax generates structures and that these structures are
merely interpreted by the other components of grammar, phonol-
ogy, and semantics. Syntactocentrism should be replaced by a par-
allel architecture of grammar, which gives phonology, semantics,
and syntax equal rights in the grammar.
In this commentary, I will argue that the case against syntacto-
centrism should be carried one step further: The division of labor
between syntax and phonology is not complete in Jackendoff’s
present description of these components. I propose that linear or-
der has no place in syntax; order (what remains of it after syntac-
tic and semantic constraints have had their say) is under the con-
trol of phonology or the syntax-phonology interface rules. In other
words, while Jackendoff claims that syntactic structure represents
(a) categories, (b) linear order, (c) constituency (p. 24), I would
like to propose that (b) be removed from this list.
2. Why syntax is neither necessary nor sufficient. In dis-
cussing possible steps in the evolution of language (Ch. 8,
pp. 255ff), Jackendoff uses a set of examples in which syntax does
not specify the position of a phrase within the sentence. As
demonstrated here, sentence adverbials are free to occur before
or after the sentence, or in the major break of the sentence. That
is, the model proposed by Jackendoff is very explicit about the fact
that syntax underdetermines the actual linear order of con-
stituents in a sentence.
It is also well known in general that some ordering phenomena
crucially relate to nonsyntactic information. Perhaps foremost
among these phenomena are the so-called “heaviness shifts,” that
is, presumed movement operations which shift heavy, that is, long
constituents toward the left or right edge of a sentence. I claim
that length in these cases is phonological length, measured per-
haps in number of syllables or phonological words, and cannot be
measured in terms of number of grammatical words or syntactic
complexity. In considering the syntax-phonology relation, Jacken-
doff points to examples in which prosodic demands, such as heav-
iness, are active in overridng syntactic requirements (pp. 120f).
For cases such as these, the conclusion must either be that pros-
ody (heaviness) can override syntactic rules, or that syntax simply
has nothing to say about the actual order.
The parallel structures proposed by Jackendoff are synchro-
nized by the use of subscripts identifying corresponding pieces of
information (such as words). Now, given the fact that the phono-
logical structures as given in Figure 1.1 of Foundations and
throughout the book are fully specified with respect to linear or-
der, all other order information, in the syntax or in the semantics,
is redundant and superfluous. Any information on the linear or-
der of a linguistic expression can be read off the phonological
structure. We note in passing that semantic structure is ignorant
as to linear order. It is only in the syntax that ordering information
is duplicated. Within a discussion of constraints on syntactic rules
(Ch. 3, sect. 3.2.3), Jackendoff actually lifts this observation into
an interface constraint for the correspondence of syntactic and
phonological structures (17b, p. 50): “The linear order of elements
in phonological structure (normally) corresponds to the linear or-
der of corresponding elements in syntactic structure.” This does
not do away with the redundancy.
Second, the sort of syntax advocated by Jackendoff right from
the beginning of his book is characterized by a considerable
amount of abstractness, by structural elements which do not con-
tain actual words in their actual order. For example, the syntactic
structure of The little star’s beside a big star (p. 6 and later) has
terminal elements consisting of grammatical features alone. Their
position in the tree bears little relation to the fact that they are (of-
ten, but not always) realized at the end of their respective words.
Some syntactic items can also be “unpronounceable,” as noted on
page 13. For such elements, it makes no sense to specify ordering
information, whereas information on the dominating constituent
is relevant, in fact crucial. The same point holds for lexical items:
While it makes sense to place the past tense morpheme in the
word devoured at the end of the syntactic structure for this word
(see [6] in Ch. 6.2.3), it does not make sense for the irregular verb
ate, as in (7). As the placement of the regular -ed suffix is also spec-
ified in the phonological structure of devoured, I conclude that it
is more adequate to let the syntax be ignorant about the linear or-
der of inflectional morphemes.
Finally, in his discussion of a possible time-course of incremen-
tal language evolution, Jackendoff characterizes the protolan-
guage “concatenation of symbols” as the “beginning of syntax”
(p. 241). It is not clear whether this connection is a necessary one.
A juxtaposition of symbols in a protolanguage could well be “syn-
tagmatic phonology,” for example syllabic concatenation, going
hand in hand with “the beginning of semantics.” In a similar vein,
Jackendoff explicitly questions (pp. 252–53, Note 11) the useful-
ness of versions of syntax with unordered trees. Here, he argues
that it would be against the spirit of an evolutionist account of lan-
guage competence not to assume that syntax contains information
on linear order. I fail to see why linear order in a protolanguage
must be in the domain of syntax. If the assumed protolanguage has
some linear order, this order can just as well be under the control
of other components.
3. The proper role of syntax in grammar. The elementary for-
mal notions of phrase structure syntax are those of domination and
sisterhood (co-domination), but not that of linear order. Two
nodes A and B co-dominated by a node C are necessarily adjacent
in a binary structure, but no right-to-left relationship need be as-
sumed. Consider an elementary (but quite powerful) phrase struc-
ture syntax allowing for a head to license a complement phrase,
with the result to be modified by an adjunct phrase. Schematically,
this gives {{X YP} ZP} in an a-temporal syntax (with “{ }” marking
nonlinear constituency, and with X, Y, Z as variables for syntactic
categories). Translated into linear-order standard syntax, four pos-
sibilities arise, namely [ZP [X YP]], [ZP [YP X]], [[X YP] ZP], [[YP
X] ZP]. Generally, for any structure with n binary-branching
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
693
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

nodes, there are 2n linear orders. This can quickly lead to large
numbers, but these are still smaller than the m(!) possible permu-
tations that would result from m constituents (for m 5 3 as in the
present example, this gives six).
The present proposal is that syntax does indeed provide only for
the more modest constraints given by a-temporal syntax. A-tem-
poral syntax is sufficient to specify a crucial ingredient of syntax,
called structure-dependence in many of Chomsky’s publications.
Structure dependence is decidedly not the specification of linear
order, but the specification of domination and sisterhood alone.
Order of constituents is only partially determined by structure
dependence. The remaining task is that of phonology, semantics,
and pragmatics combined. I have nothing to say about the latter
two, but will assume that principles of information structure (such
as “Agent First” and “Focus Last,” Foundations, Ch. 8, sect. 8.7)
are of primary importance here. Again, avoidance of duplication
seems to make a syntactic determination of order superfluous at
best in those cases in which other principles are at work already.
4. The role of phonology. As for linear order in phonology, it is
indisputable that phonology (in contrast to syntax) needs linear or-
der as a core concept. The string of phonemes /pit/ is in contrast
with the string /tip/, while /ipt/ is a possible, but unrealized word
in English, and any other permutation of the three phonemes is
ill-formed in English. In other words, the elementary notions of
contrast, distinctiveness, and well-formedness in phonology in-
clude linear order. Structuralist phonology used the term “syntag-
matic relation” in this connection; here, “syntagmatic” literally
means “in accordance to the time axis.” Furthermore, a number
of phonological rules are generally cast in terms of linear order.
For example, the basic rule of compound stress in English or Ger-
man says that the first of two parts in a compound carries main
stress. For stress in phrases, the reverse holds (simplifying con-
siderably): the second of two constituents in a phrase receives
main stress. In other words, phonology is very much about the
temporal line-up of chunks of speech. Given that it is grounded in
the phonetics of speech, this does not come as a surprise.
Furthermore, some of the syntactic movement operations as-
sumed in syntactic theory are clearly related, at least functionally,
to either information structure (as in “topic first”) or to preferred
positions for constituents with either strong stress (focus posi-
tions) or weak stress (deaccentuation). Given that syntax is not
conceived as “knowing” about nonsyntactic principles such as
stress, it is almost inevitable to assign the respective movement op-
erations to some other domain.
5. Where does order come from? If the present hypothesis
about temporally unordered syntactic constituents should be cor-
rect, it would leave us with one crucial question: From what rules
or principles does the actual order (encoded in phonological struc-
tures) derive? No complete answer can possibly be given here, but
parts of the answer have been identified already: Jackendoff points
out in several places that there are principles of ordering which
are part of semantics, information structure in particular, and of
phonology, heaviness constraints and stress preferences in partic-
ular.
Lexical information (either on individual items or on more or
less extended lexical classes) must be another source of temporal
order: Prepositions versus postpositions are an obvious example,
prenominal versus postnominal adjectives might provide a further
case.
Next, phonology itself provides ordering information, as we can
see from principles, such as the one requiring long constituents to
follow short ones (Behaghel’s law).
Setting aside the cases just enumerated, there are substantial
remaining problems. My formal proposal at this point is that the
rules providing the interface between syntax and phonology –
Jackendoff’s “PS-SS interface rules” (Ch. 5, sect. 5.6) – provide
the natural locus for stating the constraints on linear order for syn-
tactic and/or semantic constituents. Such rules are, by necessity,
sensitive to information stemming from both of the components
between which they mediate. Here again, the architecture of
grammar proposed by Jackendoff provides a fruitful base for fur-
ther research.
How did we get from there to here in the
evolution of language?
Willem Zuidemaa and Bart de Boerb
aLanguage Evolution and Computation Research Unit, School of Philosophy,
Psychology and Language Sciences, and Institute of Animal, Cell and
Population Biology, University of Edinburgh, Edinburgh EH8 9LL, United
Kingdom; bKunstmatige Intelligentie, Rijksuniversiteit Groningen, 9712 TS
Groningen, The Netherlands. jelle@ling.ed.ac.uk
b.de.boer@ai.rug.nl
http://www.ling.ed.ac.uk/~jelle
http://www.ai.rug.nl/~bart
Abstract: Jackendoff’s scenario of the evolution of language is a major
contribution towards a more rigorous theory of the origins of language, be-
cause it is theoretically constrained by a testable theory of modern lan-
guage. However, the theoretical constraints from evolutionary theory are
not really recognized in his work. We hope that Jackendoff’s lead will be
followed by intensive cooperation between linguistic theorists and evolu-
tionary modellers.
There has been a vigorous debate in the evolution of language lit-
erature on whether the human capacity for language evolved grad-
ually or with an abrupt “big bang.” One of the arguments in favor
of the latter position has been that human language is an all or
nothing phenomenon that is of no value when only part of its ap-
paratus is in place. From a developmental perspective this has al-
ways been a peculiar argument, seemingly at odds with the grad-
ual development of phonological, syntactic, and semantic skills of
infants. In the context of the evolution of language, the argument
was eloquently refuted in a seminal paper by Pinker and Bloom
(1990). However, Pinker and Bloom did not go much further than
stating that a gradual evolution of Universal Grammar was possi-
ble. They did not explore the consequences of such a view for lin-
guistic theory, and their approach was criticized by both the or-
thodox generativists and the latter’s long-term opponents.
Jackendoff (2002) has now gone one step further. If linguistic
theory is incompatible with gradual evolution and development,
perhaps linguistic theory needs to be revised. Jackendoff has writ-
ten a powerful book around the thesis that the language capacity
is a collection of skills (“a toolbox”). Some of these skills are lan-
guage-specific, some not, and each of them is functional even
without all or some of the other skills present. From his decom-
position of linguistic skills follow a number of hypotheses on plau-
sible intermediate stages in the evolution of language, that fit in
neatly with many other theories, models, and findings in this field.
Jackendoff’s book therefore presents a significant departure
from the generative, “formalist” tradition, where the evolution of
language has received little attention. In this tradition, the struc-
ture of human language has often been viewed as accidental rather
than as adapted to the functions that language fulfills in life.
Chomsky and others have been dismissive about attempts to re-
construct the evolution of language, which they regard as unsci-
entific speculation. Chomsky famously observed that “we know
very little about what happens when 1010 neurons are crammed
into something the size of a basketball” (Chomsky 1975).
In contrast, Jackendoff presents the different tools from the
“toolbox” as adaptations for better communication. Moreover, he
gives a rather complete scenario of successive, incremental adap-
tations that is consistent with his view on how modern language
works, and how it can be decomposed. Interestingly, he argues
that present-day languages show “fossils” of each of the earlier
stages: expressions and constructions that do not exploit the full
combinatorial apparatus of modern language. Jackendoff’s book is
therefore a major contribution towards a more rigorous, scientific
theory of the evolution of language, in part because it leads to
some testable predictions, but more importantly because it is the-
oretically constrained by a testable theory of modern language.
Commentary/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
694
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

However, Jackendoff does not really recognize that, in addition,
evolutionary theory brings stringent theoretical constraints (Bar-
ton & Partridge 2000). Good evolutionary explanations specify the
assumptions on genotypic and phenotypic variation and selection
pressures, of which the consequences can be worked out in math-
ematical and computational models. For instance, Nowak et al.
(2001) derive a “coherence threshold” for the evolution of lan-
guage, which poses a strict constraint on the accuracy of both ge-
netic and cultural transmission of language for linguistic coher-
ence in a population to be possible. In this type of work, one often
finds that “adaptive explanations” that seem so obvious in a verbal
treatment such as Jackendoff’s, are in fact insufficient.
Cavalli-Sforza and Feldman (1983) studied a “conformism con-
straint” that arises from the positive frequency dependency of lan-
guage evolution: Linguistic innovations are not advantageous in a
population where that innovation is very infrequent. Imagine, for
instance, a population that is in the second state of Jackendoff’s
scenario. That is, individuals can use a large vocabulary of learned
signals in a non-situation-specific manner, but their language is
not compositional: Signals cannot be analyzed as consisting of
meaningful parts. Suppose that a child is born with a genetic mu-
tation that makes her more inclined to analyze sentences compo-
sitionally. Would this child profit significantly from this mutation,
even if the language of the population she is born into is not at all
compositional? If not – and it takes some creativity to come up
with reasons why she would – evolutionary theory predicts that
the new gene will disappear through negative selection or random
drift (Fisher 1922).
That is not to say that language did not evolve according to Jack-
endoff’s scenario, but just to emphasize that each of the transitions
between the phases he proposes is a challenge in itself. The evo-
lution of language is not, as is sometimes suggested, a domain for
just-so stories. Rather, it turns out that it is very difficult to find
even a single plausible scenario for the evolutionary path from pri-
mate-like communication to the sophisticated toolbox of human
language that will survive close scrutiny from mathematical and
computational modeling. Recently, this insight has led to a surge
in the interest in “explorative,” computational models (see Kirby
2002b; Steels 1997; for reviews). They have yielded intriguing
ideas on adaptive and nonadaptive explanations for the emergence
of shared, symbolic vocabularies (e.g., Oliphant & Batali 1996),
combinatorial phonology (e.g., de Boer 2000; Oudeyer 2002),
compositionality and recursive phrase-structure (e.g., Batali 2002;
Kirby 2002a).
For instance, the suggestion of Kirby (2000) – referred to but
not discussed in Jackendoff’s book – is that a process of cultural
evolution might facilitate the emergence of compositionality. If a
language is transmitted culturally from generation to generation,
signals might frequently get lost through a bottleneck effect (that
arises from the finite number of learning opportunities for the
child). Signals that can be inferred from other signals in the lan-
guage, because they follow some or other systematicity, have an
inherent advantage over signals that compete for transmission
through the bottleneck. With some sort of generalization mecha-
nism in place (not necessarily adapted for language), one always
expects a language to become more compositional (Kirby 2000),
and, more generally, better adapted to the idiosyncrasies of the in-
dividual learning skills (Zuidema 2003).
Throughout his book, Jackendoff uses metaphors and termi-
nology from computer science. Terms like processing, working
memory, and interface make it sometimes appear as if he is de-
scribing a computer rather than processes in the human brain.
However, nowhere do his descriptions become sufficiently formal
and exact to make them really implementable as a computer pro-
gram. In this light, his criticism of neural network models of lan-
guage acquisition and his mentioning only in passing of computa-
tional models of the evolution of language is unsatisfactory.
Jackendoff’s challenges for connectionists are interesting and to
the point, but it is equally necessary for theories such as Jackend-
off’s, especially their implications for development and evolution,
to be made more precise and to be extended in computational and
mathematical models.
In sum, in the effort to find a plausible scenario for the evolu-
tion of human language, a book like Jackendoff’s Foundations of
Language, based on a broad and thorough review of linguistic the-
ory and facts, is extremely welcome. But as explorative computa-
tional models such as the ones discussed have been very fruitful
in showing new opportunities and constraints for evolutionary ex-
planations of human language, we hope that Jackendoff’s lead will
be followed by intensive cooperation between linguistic theorists
and evolutionary modellers.
ACKNOWLEDGMENT
Willem Zuidema is funded by a Marie Curie fellowship of the European
Union.
Author’s Response
Toward better mutual understanding
Ray Jackendoff
Program in Linguistics, Brandeis University, Waltham, MA 02454.
jackendoff@brandeis.edu
Abstract. The commentaries show the wide variety of incom-
mensurable viewpoints on language that Foundations of Lan-
guage attempts to integrate. In order to achieve a more compre-
hensive framework that preserves genuine insights coming from
all sides, everyone will have to give a little.
R1. Goals
My goal in writing Foundations of Language was threefold.
First, I wished to develop a framework for studying lan-
guage – the parallel architecture – which would permit a
better integration of all the subfields and theoretical frame-
works of linguistics with each other and with the other cog-
nitive neurosciences. Second, I wished to persuade lin-
guists to join more fully in this integrative enterprise. Third,
I wished to persuade cognitive neuroscientists outside lin-
guistics that the past forty years have brought genuine in-
sights in linguistic description – albeit somewhat obscured
by the technical opacity of linguistic theory – and that the
parallel architecture offers better prospects for renewed di-
alogue. The commentaries suggest that I have succeeded to
some extent, but that there still is a long way to go and a lot
of preconceptions to overcome (including, no doubt, my
own). The difficulties of integration are legion: The study
of language, more than any other cognitive capacity,
stretches the limits of interdisciplinarity, all the way from
neuroscience and genetics to social policy and literary the-
ory, with linguistics, psychology, and anthropology in be-
tween.
Many of the commentators focus on issues in Founda-
tions that are touched upon only tangentially or not at all in
the précis appearing here. In this response I will do my best
to make clear what is at stake. My hope, of course, is that
readers will thereby be engaged enough to want to tackle
the whole book.
Response/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
695
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

R2. Sociology
Let me clear some sociological remarks out of the way first.
Some commentators found the book fatally flawed because
it has abandoned traditional generative grammar and phi-
losophy of language (Adams, Freidin, Higginbotham,
Jerzykiewicz & Scott, ter Meulen), whereas others
found the book fatally flawed because it clings to traditional
generative grammar, which to them is clearly a dead letter
(Edelman, Lavie, MacAogáin, Spivey & Gonzalez-
Marquez). Edelman compares me to Khrushchev prop-
ping up the doomed Soviet regime, neglecting the fact that
I concur with him in emphatically rejecting Chomsky’s
Minimalist Program (and also neglecting the fact that I have
no secret police or gulag with which to suppress dissent).
Spivey and Gonzalez-Marquez compare generative gram-
mar to the Donner Pass party, who ended up eating each
other. Although I acknowledge that linguists can often be
less than civil (see the discussion of the generative seman-
tics dispute, Foundations, pp. 73–74), I submit that the de-
bate isn’t always that much better elsewhere.
In both groups who dismiss the work, I find a reluctance
to acknowledge my larger goals. As the Précis says, “To un-
derstand language and the brain, we need all the tools we
can get. But everyone will have to give a little in order for
the pieces to fit together properly” (sect. 1, para. 1). On one
hand, the structures that linguists have discovered represent
real empirical generalizations and real problems for learn-
ing; on the other hand, we do have to figure out how the neu-
rons do it. I think a total integration of these inquiries is still
quite a few years off, but I am trying to find a way to move
each of them in a direction that recognizes the mutual value
of the other in achieving the common goal.
I am also delighted that several commentators see possi-
bilities for linking my parallel architecture to their own con-
cerns. For instance, Garrod & Pickering see the architec-
ture as an opening for connecting linguistics with the study
of discourse, an issue Foundations touches on very superfi-
cially at the end of Chapter 12. Érdi offers a smorgasbord
of issues in network theory, chaos theory, and dynamic sys-
tems theory that make contact with points in my discussion.
R3. Literature of which I was unaware
As I acknowledged in the Preface, the undertaking is by this
point too large for one person to grasp all of its parts equally.
If nothing else, there just is too much literature and it grows
far faster than anyone can read much less digest it. Several
commentators suggest that there is relevant literature out
there that I should have read. Guilty as charged, but after
all life is short and you have to make your choices: either
read for another 90 years or write the damn book, knowing
you haven’t covered all the bases.
In particular, Catania points out many parallels between
my thought and Skinner’s. These may well be valid and
worth examination. On the other hand, I am not aware of
any work emerging from the Skinnerian tradition that ap-
proaches the level of detail of linguistic organization rou-
tinely investigated by generative grammar (and discussed in
Chs. 1, 5, 6, 11, and 12 of Foundations). This may be my ig-
norance, or it may be a sociological consequence of behav-
iorism’s eclipse since the cognitive revolution, or it may be
an inherent insufficiency in behaviorist theory. Likewise,
Lockwood points out parallels between my work and Syd-
ney Lamb’s, some of which I briefly alluded to in Founda-
tions (cf. Précis, Note 9), and much of which I was unaware
of. In this case I am happy to report that I had a productive
discussion with Lamb at a conference in 2002 after the pub-
lication of Foundations. We could see the commonalities,
but he also acknowledged that his neurally inspired ap-
proach could not solve my Problem of 2 (“How does a
neural network encode multiple tokens of a known category
in working memory?”), crucial to an account of language
processing (cf. Foundations, pp. 61–63).
Csépe observes that there are other models of working
memory besides the one I take to task, citing in particular
that of Just and Carpenter (1992) as better fitting my ac-
count. I am pleased. Zuidema & de Boer refer to litera-
ture on mathematical constraints in evolutionary theory
that I was only vaguely aware of; certainly I am not conver-
sant enough with the mathematics to bring this research to
bear usefully on my approach. Again, I would be happy if
my work could add greater linguistic sophistication to this
valuable line of inquiry.
Howard and Schnelle each point out theories of neural
architecture and function with which I was unacquainted.
I am glad to see that they acknowledge the challenges for
neuroscience presented by the combinatoriality of lan-
guage, and I am glad that they claim to have solved some of
them. This is all to the good. However, I don’t think it is for
me to evaluate their models; it is a long intellectual stretch
from details of syntax to details of neurons. Rather, I can
hope that they might use their models’ convergences with
the demands of language processing to try to win greater
acceptance in the broader neuroscience community.
Velichkovsky, Kibrik, & Velichkovsky (Velichkovsky
et al.) allude to a literature on language as communication
and enhancement of social coordination with which I was not
acquainted. Again, I see no conflict here. I think the parallel
architecture, in particular the independence of semantics
from syntax, opens the door to connections being made in a
way that is impossible within the syntactocentric architec-
ture. Arbib draws attention to the HEARSAY architecture
for speech understanding, which introduced the notion of a
“blackboard” for parallel computation of different levels of
language structure. I was aware of the term as something in
common currency but was not aware of its original source.
ter Meulen reminds us of the notion of Place in semantics,
as proposed within Situation Semantics by Barwise and Perry
(1983), so I was not alone. I do not remember whether Bar-
wise and Perry refer to the discussion in Jackendoff (1978).
In any event, I am sure ter Meulen would agree that the no-
tion has not exactly taken hold in formal semantics as a whole.
Thanks to all of these commentators for pointing out the fur-
ther connections.
R4. Localization of brain function
Several commentators take seriously the possibility of con-
necting components of the parallel architecture to brain lo-
cation, and raise interesting issues about whether the the-
ory makes the right distinctions. Csépe asks what could be
meant in a biological sense by inheritance, innateness, and
wiring. This question applies of course to any formal theory
of grammar, and Foundations (Chs. 2–4) stresses that it is
one of the major problems to be faced in unifying neuro-
Response/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
696
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

science with a formal theory of language or of any other cog-
nitive capacity. Csépe goes on to ask whether the three gen-
erative components have neural correlates, and she cites
various imaging data concerning the separability of seman-
tic, syntactic, and phonological processing, as well as finer
distinctions such as morphosyntax versus phrasal syntax.
The parallel architecture also bids us ask how the interfaces
among the generative components are neurally realized,
presumably as connections among areas that subserve dif-
ferent formats of structure. On the other hand, there is the
possibility that some of the components interweave in the
brain or that some of them (especially semantics) are spread
out among various brain areas. Whatever the answers, the
parallel architecture makes for a better potential correla-
tion between components of the theory and areas of the
brain, in the same way that Foundations demonstrates bet-
ter correlations between components of the theory and
components of processing.
Goldenberg observes that left brain damage often re-
sults not just in some variety of aphasia, but also in dimin-
ished performance in certain nonverbal sorting tasks and
high-level disorders of motor control (apraxias). He sug-
gests that the common element among these three is re-
combination of a finite repertoire of elements into new
combinations. He is wise enough to not say that language
reduces to these other functions, but rather to say that all
three are special applications of this common function. In
itself this proposal has no bearing on which theory of gram-
mar one adopts, except that it may help explain the general
location of language in the brain. Still, I find this approach
a friendly addendum to the parallel architecture, with a (to
me) novel proposal about the evolutionary antecedents of
combinatoriality.
Gervain wonders whether the double dissociation of
lexically stored and rule-generated past tense forms is a
problem for my treatment of productive versus semi-pro-
ductive regularities, which she says I conflate. In fact, my
claim (Foundations, Ch. 6; Précis, sect. 8) is that the semi-
productive forms are stored in long-term memory as a
whole, but the productive forms arise from free combina-
tion of a stored stem with a stored affix. Thus, the process-
ing involved in relating a semi-regular past to its present is
lexical association, but the processing involved in relating a
regular past to its present is variable instantiation. I think
this provides room in the theory for the observed double
dissociation in processing.
Kemmerer, in one of the most interesting of the com-
mentaries, shows how evidence from neuroscience might
be brought to bear on fine details of the theory. Founda-
tions claims (Ch. 11) that certain aspects of meaning are en-
coded not in the algebraic format of conceptual structure
but, rather, in some visuospatial format; Kemmerer cites
references from the neurolinguistics literature that support
this view.
On the other hand, Kemmerer takes issue with my claim
(Ch. 9) that within conceptual structure there is no princi-
pled distinction of format between those aspects of seman-
tics that are relevant to grammar (time, person, evidentiary
status, etc.) and those that are not (the distinction between
dog and kangaroo or between five and six). In particular, the
distinction between caused motion and caused change of
property is correlated with the syntactic difference between
(1a) and (1b), which can however describe the same event.
(1) a. Sam sprayed water on the flowers. [water moves to
position on flowers]
b. Sam sprayed the flowers with water. [flowers come
to have water on them]
However, other verbs permit only one construal:
(2) a. Sam dripped/poured/spilled water on the flowers.
b. *Sam dripped/poured/spilled the flowers with
water. [unacceptable]
(3) a. *Sam drenched/doused/soaked water on the flow-
ers. [unacceptable]
b. Sam drenched/doused/soaked the flowers with
water.
Kemmerer’s experiments reveal a double dissociation in
different aphasias: Some patients could distinguish the
meanings of verbs within classes (2) and (3) but could not
judge (2b) or (3a) to be ungrammatical, and some patients
were just the reverse. This suggests that the grammatically
relevant aspect of meaning that distinguishes (2) from (3) is
neurally segregated from the grammatically irrelevant as-
pect of meaning that distinguishes verbs within the classes.
Kemmerer offers a reconciliation of this finding with my
position: that the neural structures that implement gram-
matical semantics might not be genetically programmed for
this function, but they become functionally specialized as
the child learns. He observes that this resolution accom-
modates the fact that such grammatically relevant aspects
of meaning vary considerably from one language to the
next. I am sympathetic to this suggestion. If a piece of
meaning is relevant to grammar, it must be encoded as one
component of an interface rule, perhaps as constructional
meaning; the other end of this interface rule is a bit of syn-
tactic and/or phonological structure with which this mean-
ing correlates. Many such rules must be learned, although
some, such as the preference for Agents to be subjects, are
so widespread as to suggest that they are wired in. By con-
trast, grammatically irrelevant aspects of meaning will ap-
pear not as part of a general interface rule, but only in the
mapping of individual word meanings from semantics to
phonology. This difference might be the basis for the dis-
sociation Kemmerer observes.
The argument in Foundations, therefore, was that gram-
matically relevant aspects of meaning don’t differ in format
from grammatically irrelevant aspects of meaning – that is,
there is no separate level of linguistic semantics distinct
from general-purpose meaning. What makes a particular
piece of meaning grammatically relevant is its playing a role
in a relatively general interface rule between meaning and
syntax.
The larger question for linguistic theory and cognitive
neuroscience is to determine the exact range of possible
grammatically relevant aspects of meaning. Person is always
relevant, and number, relative status of the speaker and
hearer, causation, agenthood, patienthood, evidentiary sta-
tus, time, and many other things often appear. The distinc-
tion between (2) and (3) appears to be a special case of the
more general principle that direct objects are construed as
Patients if possible; the special use of with in (3) appears to
be a construction of English (Jackendoff 1990, Ch. 8) with
parallels in other languages as well. In any event, Kem-
merer’s work is exactly the sort of research in neurolin-
guistics that is pertinent to the parallel architecture; the in-
terest arises from the close contact between the theoretical
model and its possible interpretation in brain terms.
Response/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
697
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

R5. Evolution
A number of commentators addressed themselves to my
discussion of the evolution of language (Foundations, Ch.
8; Précis, sect. 9.4). I ought to make my goal in this chapter
clear. One of the issues for the evolution of the language
faculty is how its apparent complexity could have evolved
incrementally, in such a way that each stage was a usable
form of communication and each successive innovation was
an improvement on the previous system. I offered a hy-
pothesis about such a sequence of innovations, with no pre-
sumptions about the absolute timing. Various stages could
have been nearly simultaneous or widely spaced in time: It
is the relative order that makes a difference. In addition to
the standard sorts of evidence offered for the evolution of
language, I was able to connect some of my hypothesized
stages with the present-day architecture of grammar. In
particular, certain grammatical phenomena appear as “fos-
sils” of earlier stages.
Érdi is enthusiastic about the idea that language arose in
the visual-gestural modality and changed later into the au-
ditory-vocal modality, citing the recent investigations of
“mirror neurons.” Foundations said that this would not ma-
terially change my story, and I stand by this statement. Even
if visual-gestural language did emerge first, it is still neces-
sary to account for the emergence of the auditory-vocal
modality, in particular the digitization of the speech signal
into phonological segments – a major innovation, for which
there are no animal homologues or analogues. Visual-ges-
tural origins might permit some differences in the ordering:
The amazing expansion of the vocabulary (which in my
story is interdependent with phonology) could precede the
initiation of phonology, and some of the syntactic innova-
tions could as well. In the end, however, all the innovations
must still take place, and at the moment I know of no non-
speculative evidence for the primacy of the visual-gestural
modality in language (possibly my ignorance of course), so
I would prefer to remain agnostic.
Arbib mentions a number of capacities that had to exist
prior to getting language off the ground at all: imitation
(which I mention); symbolization (which I take to be the es-
sential move); parity (which I neglected, but about which he
is right); intentional communication (for me, probably part
of symbolization, but worth separating out); beyond the
here-and-now (which I take to be a characteristic of primate
thought); paedomorphy and sociality (with which I agree);
and the ability to time actions in relation to hierarchical goals
(I agree here too). But he seems to think this is all one needs
to get language: “What is universal is the need for expres-
sion, not the choice of linguistic structure for meeting those
needs.” This – again – does not take into account the digiti-
zation of phonology, which calls for something more than
just a need to express oneself. (Foundations, p. 244: “As
many linguists [references omitted] – but not many nonlin-
guists – have recognized, the innovation of phonological
structure is a major cognitive advance.”) Chapter 4 (cf. Pré-
cis, sect. 2) offers a list of well-known symptoms that collec-
tively suggest that language is a biological specialization that
goes far beyond just a need for expression.
Arbib takes the position that case and agreement sys-
tems are cultural inventions, whereas I supposed that they
are a product of late stages of evolution in the language fac-
ulty. This is an interesting topic for future research. It would
certainly be nicer if these elements of grammar were not
partly specified by the toolkit of Universal Grammar: there
would be less needed in the genome, and less necessary in-
novation for evolution. On the other hand, one would want
to account for the linguistically widespread properties of
case and agreement systems – what happens and, crucially,
what doesn’t happen. One would also want to account for
the fragility of these systems in agrammatic aphasia, Spe-
cific Language Impairment, and second language learning:
the impairments appear not to be due simply to phonolog-
ical difficulties.
Bickerton, to whom Foundations gives grateful credit
for his insight into the evidence for an evolutionary stage of
protolanguage, complains that there is no evidence for my
further decomposition of the evolutionary process. But he
flatly denies, without argument, the difference between the
use of symbols (which might be limited to a small innate or
learned vocabulary, like primate calls) and the use of an
open and unlimited class of symbols, which requires the
possibility of imitating others and learning fast, and, at least
for some individuals, the possibility of innovating symbols.
He similarly denies any difference between these and the
innovation of the phonological combinatorial system, ig-
noring my argument that the digitization of phonology is
necessary in order to keep a vocabulary of thousands of
symbols separate in memory and perception. Then he of-
fers the raw speculation: “It seems highly likely that lan-
guage’s two combinatorial systems came in together, per-
haps exploiting some single underlying capacity, but more
likely with phonology employing mechanisms derived di-
rectly or indirectly from syntax.” I honestly don’t see what
makes this highly likely, other than a prejudice about the
primacy of syntax. In addition, Bickerton himself has ar-
gued that protolanguage (like pidgins) lacked syntax; but
pidgins and agrammatic aphasia certainly don’t lack phonol-
ogy. Thus, the logic of Bickerton’s original position de-
mands that phonology belongs to an earlier stratum of lan-
guage than syntax, in concurrence with my position, and in
conflict with the position he takes in this commentary.
Bickerton also complains that I do not address his evi-
dence about the timing of the development of protolan-
guage and modern language. I do not deny his evidence or
the interest of the issue. It’s just that you can’t do everything.
Zuidema & de Boer raise the important issue that every
transition between phases is a challenge. Suppose one
speaker has a mutation that allows her to construct fancier
sentences. What good will it do her if no one else can per-
ceive them advantageously? This is, of course, a problem,
whether the evolution of language was in many phases, or
all at once (as Chomsky often seems to think), or in two
phases, à la Bickerton. In fact, this is a potential problem
for any cognitive system that requires mutuality. Founda-
tions recognized this problem and declined to address it,
citing ignorance. One possibility, suggested by Chomsky (in
his plenary address to the Linguistic Society of America,
January 2004), is that some of the offspring of this single in-
dividual will share the relevant gene, and it is they who will
reap the communicative advantage that leads to compara-
tive reproductive success. I look forward to further discus-
sion of this issue.
R6. Syntax
At the core of Foundations (Précis, sect. 4), is the argument
that generative grammar since its inception has labored un-
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
698
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

der an incorrect assumption, never argued for: that the
combinatorial complexity of language arises from the syn-
tactic component alone. Foundations proposes instead
(Précis, sects. 5–7) a parallel architecture, in which phonol-
ogy, syntax, and semantics are equally generative, and ar-
gues that such an organization both reflects actual practice
(outside syntactic theory) and also provides a more reveal-
ing account of language as a whole. I am gratified that so
many of the commentaries appreciated the value of this ar-
gument.
On the other hand, there were dissenters. Bickerton de-
scribes himself as “an unashamed syntactocentrist,” though
he offers no defense of syntactocentrism. He more or less
accuses me of marginalizing and trivializing syntax by ana-
lyzing such a trivial sentence in Chapter 1 (whose point was
to show the richness of linguistic structure in even the most
trivial sentence); he overlooks all the discussion of syntac-
tic detail in Chapters 5, 6, and 12. Of course, there is a lot
more to syntax (and phonology and semantics) than I have
presented in Foundations. Culicover and Jackendoff (forth-
coming) addresses what might be left of syntax in the new
framework; it is hardly marginal or trivial, though far less
complex than the Chomskyan models of the last thirty
years.
Freidin attempts to defend the standard Chomskyan
models. He claims that my term “syntactocentric” mischar-
acterizes these models, in that the phonological and se-
mantic content of the lexical items embedded in a syntac-
tic tree provides a source for phonological and semantic
combinatoriality (as described in the Précis, sects. 4 and 8).
But he misses my point. Much of Chapters 5, 6, and 12 (and
also the Précis, sects. 5 and 6, as well as a large proportion
of my previously published work) demonstrates that the in-
dependent combinatorial structure of phonology and se-
mantics simply cannot be derived from syntactic con-
stituency, because the correspondence between the three
components is imperfect in many interesting ways. Freidin
does not address any of these phenomena, and as far as I
know no one else in the recent Chomskyan tradition has ad-
dressed them either.
Freidin also offers the argument that the parallel archi-
tecture puts a huge burden on the interface/correspon-
dence rules that establish the relations among the parallel
structures, because each word requires an interface rule
linking its phonological structure, its syntactic features, and
its semantic structure. Then he says that since the linking
of phonology with meaning is arbitrary, there cannot be any
such rules. He misses the point of section 5.7 in Founda-
tions (Précis, sect. 8): words are interface rules! They are
not general and systematic, of course. However, Chapter 6
demonstrates a cline of phenomena ranging from the very
specific to the very general, such that words are on one end
and general rules of phrase structure are on the other. So it
is no longer possible to make a sharp distinction between
the truly exceptional and the truly general, as the Chom-
skyan theoretical technology has always done. Freidin ig-
nores all the arguments for this view of the lexicon (e.g.,
idioms, constructions, regular morphology, role in process-
ing); again, as far as I know, no one else in the Chomskyan
tradition has addressed them either.
Gervain worries that Foundations provides no account
of the standard syntactic phenomena usually treated as
movement and constraints on movement. This is correct.
Foundations acknowledges their existence (Chs. 1 and 5)
but does not discuss them in detail. As observed in the Pré-
cis, section 7, the parallel architecture still leaves open the
possibility that the syntactic component involves movement
rules with the standard definitions and the standard con-
straints. But it also allows for the possibility that there is no
syntactic movement per se, and that passive, raising, wh-
fronting, and so on are accounted for in a fashion akin to
that of Lexical-Functional Grammar (Bresnan 1982) or
Head-Driven Phrase Structure Grammar (Pollard & Sag
1987; 1994), where the constraints are on configurations
rather than on movement. My inclinations as a whole are
for the latter possibility, but Foundations, being long
enough already, was not the place to argue the point. Culi-
cover and Jackendoff (forthcoming) take up these matters
in more detail.
Gervain also worries that in an effort to do away with
syntactocentrism, I have substituted “lexicocentrism.” Per-
haps, but to call something “X-centric” doesn’t make it bad
(consider “heliocentric” in reference to Copernicus). Ger-
vain observes that the parallel architecture’s claims about
the lexicon become crucial when the model is extended to
explain processing. I certainly agree, and Chapter 7 of
Foundations is devoted to working out some of the impli-
cations. There is clearly much more to be done, but my im-
pression is that psycholinguists are on the whole enthusias-
tic about this model because of the connections it makes
with processing. (By contrast, a prominent psycholinguist
once told me that he had not referred to Chomsky at all in
his influential book on processing, because he could find no
way to make contact with Chomsky’s recent work; and
Chomsky and his close colleagues have likewise done little
to make contact with processing concerns, so far as I know.)
Wiese, on the other hand, suggests that my slimming
down of syntax does not go far enough, and that linear or-
der should also be purged from syntactic structure. The
consequence would be that a rule like “verb goes at the be-
ginning of verb phrase” would be not a rule of syntax, but
rather, a rule of the syntax-phonology interface. I have no
objection to such a move in principle, and it would be in-
teresting to see how it works out. This is precisely the sort
of question that the parallel architecture encourages: the
balance of power among components of the grammar.
Many other such issues are addressed in Foundations, for
instance, the balance of syntax and semantics in determin-
ing the overt argument structure of verbs (Ch. 5); the con-
tribution of syntax in semantic coercion (Ch. 12); and the
balance of syntax and phonology in determining intonation
contours (Ch. 5). I hope I will be forgiven for drawing the
line on what is included in the book.
Lavie, too, thinks I retained too much of traditional gen-
erative grammar. In his case, the issue is the notion of syn-
tactic categories. I have some difficulty following his argu-
ment, but the key seems to be that in his ruleless model “all
the computation takes place among exemplars and occur-
rences” with no reference to types; computations are based
on “analogy.” However, it is not clear to me from the com-
mentary alone how Lavie’s idea differs, for example, from
the connectionist approaches attacked by Marcus (2001),
and how it solves my “Problem of 2” (Foundations, Ch. 4).
Lavie also does not address my direct argument against
analogy in Chapter 3:
Nor can we figure out . . . rhymes by analogy, reasoning for ex-
ample, “Well, ling sounds sort of like the word link, and link
rhymes with think, so maybe ling rhymes with think.” The only
Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
699
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

words for which such an analogical argument works are the
words with which ling already rhymes – which is of course no
help. (Foundations, p. 64)
Lavie notices that I mention Tomasello’s work on acqui-
sition (e.g., Tomasello & Merriman 1995), which points out
that at an early stage, children learn new constructions one
word at a time; therefore, he asks why I need rules and cat-
egories at all. However, Chapter 6 endorses Tomasello’s po-
sition only at the outset of rule acquisition: There has to be
a further stage where some similarities coalesce as regular-
ities, producing a lexical item with a variable that functions
in a rule-like fashion. Lavie cites Maurice Gross (1975) to
the effect that no two verbs have exactly the same distribu-
tional behavior (which wouldn’t surprise me); he concludes
from this that it is a mistake to have a category such as verb.
However, all verbs go in the same position in the sentence,
and with very few exceptions they all have full inflectional
paradigms (regular or irregular, as the case may be). These
absolute uniformities among verbs are a function of their
syntactic category; the differences in distribution follow
from differences in meaning, subcategorization, participa-
tion in idioms and collocations, and so forth. I’ve never
heard of a language in which different verbs occur in differ-
ent positions, say, one class of verbs that occurs at the be-
ginning of the verb phrase (VP) and another class that oc-
curs at the end – certainly a logical possibility. This suggests
that some of the distributional behavior of verbs is a func-
tion of their being verbs, and that Universal Grammar spec-
ifies that within a given language all verbs have their posi-
tion specified in the same way. I don’t see how this can be
done without categories, but perhaps Lavie has a more so-
phisticated notion of analogy than I am imagining.
R7. Semantics
Finally, I turn to the issue that Foundations characterizes as
the “holy grail” of linguistics and cognitive science: the pur-
suit of a theory of meaning. Foundations contends (Précis,
sect. 9.2) that just as it is necessary to study syntax and
phonology as mental structure, it is necessary to abandon
the realist conception of meaning bequeathed to us by stan-
dard philosophical approaches and to situate meaning in
the mind of the meaner. This aspect of Foundations drew
the most outcries from commentators.
Adams, Higginbotham, Jerzykiewicz & Scott, Mac-
Aogáin, and ter Meulen all offer essentially the same ar-
gument: A proper theory of meaning cannot merely ac-
count for the relation between linguistic expressions and
the language user’s mind, it must also account for the rela-
tion between linguisic expressions and the world. They cite
Frege, Searle’s Chinese Room, the universality of truths of
arithmetic, and the need for an objective notion of logical
validity as evidence for this position. Much of Chapters 9
and 10 of Foundations is devoted to answering this argu-
ment. However, I explicitly say (pp. 279–80):
My approach might in a way be seen as hedging one’s bets. I am
hoping that we can arrive at a naturalized view of meaning with-
out invoking intentionality. On the other hand . . . conceptual
structure might indeed need to be intentional in some sense.
Whichever is the case, we still have to work out the details of
the combinatorial system constituting semantic/conceptual
structure/LoT, as well as its interfaces with language, inference,
perception, and action – which is what I take as the task of con-
ceptualist semantics.
(A similar statement appears in the Précis, sect. 9.2.) For
example, whether or not there are universal Platonic truths
to which arithmetic statements such as “212 5 4” refer, we
still have to account for how human beings conceptualize
number such that they grasp these truths as universal and
timeless. This is something the Anglo-American philosoph-
ical tradition pretty much ignores but cognitive neuro-
science cannot.
In addition, Chapter 10 (Précis, sect. 9.2) argues that our
sense of contact with the external world is not just a prob-
lem for semantics, it is a problem for perception as well.
Everyone studying visual perception knows that our visual
sense of the “world out there” is the result of fantastically
complex brain processing going on between the retina and
central cognition. I am unaware of literature in the philos-
ophy of language that addresses this problem. My own po-
sition is that linguistic reference to objects in the world sim-
ply piggybacks on the output of the perceptual systems:
“the world” as given to us by the perceptual systems is what
we refer to. This does indeed leave the issue of what it is to
refer to a number (as Foundations acknowledges). How-
ever, I take it that the first-order problem, the one that lan-
guage had to deal with as it evolved in our ancestors, is ref-
erence to people and trees and things to eat and things to
do, and this falls in naturally with the problem of percep-
tion.
Jerzykiewicz & Scott think I have identified truth with
“community consensus.” Far from it. As Chapter 10 of
Foundations says, community consensus is one way we have
of checking our judgments of truth, especially when we
have no personal experience with the matters at hand. But
if that were all there was, I couldn’t logically claim that the
community consensus on reference is false, could I? (The
case in Foundations was the Emperor’s New Clothes.) In
the general case, one’s judgments of truth require a delicate
balancing of evidential sources from personal experience,
memory, inference, and community opinion. And, given
that “absolute truth” (about most matters, anyway) is in
principle inaccessible to us, our judgments of truth are all
we have to work with in dealing with life.
Higginbotham says that if all reference is via concepts,
as I claim, then the theory can’t distinguish Sherlock
Holmes (fictional) from Derek Jeter (nonfictional) and
from the unicorn in my dream. However, Chapter 10
specifically does offer a distinction in terms of “valuation
features,” which Higginbotham evidently overlooks. This
account may or may not be correct, but it is there.
Higginbotham also offers what he calls a “demystifica-
tion” of the reference of “the distance between New York
and Boston”: If the distance is 204 miles, the actual refer-
ential expression is “the distance between New York and
Boston in miles,” which refers unproblematically to the
number 204. This approach, which he attributes to Carnap
(who was of course innocent of modern linguistics), is sim-
ply not feasible. First of all, it leaves unexplained what
“mile” refers to, not an easy task without a referential cate-
gory of distances. Second, it does not address my example
(given in Ch. 10 and cited in my works since 1983): The fish
that got away was this [demonstrating] long. This example
invokes no numbers. Rather, the distance referred to by this
long (a spatial extent, not a number) is picked up from ob-
servation of the position of the speaker’s hands. Third, as
Foundations (p. 301) observes, the truth-conditions de-
pend on how the distance between New York and Boston is
Response/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
700
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

measured: “From center to center, from nearest border to
nearest border, along some particular highway, on a straight
line through the earth’s crust? Much depends on one’s pur-
pose.”
Dennett, in an attempt to head off critics of my concep-
tualist view of reference, offers the suggestion that we con-
sider it an account of “some other guy’s language (and
mind).” He suggests that such an approach is necessary for
a proper theory of linguistics, but for everyday purposes
“we can continue using the traditional semantical talk about
the word-world relation.” He compares this stance-switch-
ing to the practice in evolutionary psychology, where every-
one talks about “the gene for such-and-such” (everyday
talk), knowing full well that there is really a very intricate
biochemical story to be told (technical talk). I am reminded
also of Dennett’s own rhetoric about consciousness: He
constantly insists that the everyday talk of “you, perceiving
the world there in your brain” (or the “Cartesian theater”)
has to be carefully expunged from technical talk about how
the brain produces awareness of the world. In fact, he and
I indeed face the same problem: People accuse him of
claiming we’re not conscious, and accuse me of believing
language doesn’t refer to the world – in both cases because
they are unwilling to engage in the proper technical talk
necessary to approach the problem scientifically.
Fortunately, there are some commentators who find the
conceptualist approach appealing. Molina and Dominey
contrast the parallel architecture’s account of meaning with
one based on the syntactocentric framework for language.
Molina observes, correctly I think, that the Chomskyan the-
ory of LF does not provide an adequate vehicle for thought
in general. Dominey suggests that I have gone “conceptuo-
centric” (there’s that X-centric again!): In the parallel ar-
chitecture it is natural to suppose that the hierarchical com-
plexity of syntax is but a pale reflection of that in meaning,
and it exists only insofar as it helps express thought more
precisely. Moreover, Dominey says, access to the composi-
tionality of meaning provides a scaffolding for the child’s
discovery of syntactic structure. I concur. It is nice to hear
that he is developing experimental tests of these proposi-
tions.
Velichkovsky et al., on the other hand, think that I have
not gone far enough. For them, the whole theory of lan-
guage should be based on the pragmatic, executive, or
metacognitive aspects of communication. They predict that
the notion of Universal Grammar will come to be replaced
by Universal Pragmatics of cooperative action. I am fully in
agreement with them in thinking that pragmatic and com-
municative aspects of language should play a role in the ac-
count of the system as a whole. However, a full theory of
language still must account for all the details of syntax,
phonology, morphology, and word and phrasal meaning –
as well as how they are all learned. If there are pragmatic
and communicative aspects that interact with these, well,
we need more cognitive structures and more interfaces.
Among such aspects briefly discussed in Chapter 12 of
Foundations (and sect. 6 of the Précis) are information
structure (topic vs. focus, old vs. new information) and the
“reference transfer” constructions in sentences like Plato is
on the top shelf of the bookcase, in which a speaker uses a
person’s name to refer to a book by that person. Again, this
is not an area in which I’ve done much research, but that
doesn’t mean I believe it’s unimportant.
Finally, other commentators propose further articulation
of conceptual structure. Andor advocates closer attention
to frame-based semantics, which I allude to at the end of
Chapter 11. True, there is much more to do. I hope others
will join in the work. Justo et al. chew over the idea that
conceptual structure can be subdivided into a number of
different and more restricted components. If spatial un-
derstanding is regarded as outside conceptual structure,
why is social understanding regarded as inside conceptual
structure? What do such decisions imply about processing?
These are exactly the right questions, and they can only be
settled if researchers accept the overall framework so we
can work on its details. Such questions cannot be settled by
the logicians’ insistence on realism and/or intentionality.
Ter Meulen takes issue with my claim that there is no
strict cut between logical and nonlogical properties, insist-
ing that
If one desires to model inference as a matter of form, irrespec-
tive of content, an inference is logically valid just in case the
premises entail the conclusion in every possible model, for its
validity must depend only on its logical constants. (para. 4)
Similarly, Justo et al. advocate a “minimal view” of con-
ceptual structure, in which it “is able to represent all dis-
tinct meanings, but is not able to carry out computations
other than the logical ones.” My own take on the logical/
nonlogical distinction (Ch. 9) is that certainly it must be ac-
counted for, but not by a difference in level, say, between
truly semantic structure and some sort of pragmatic struc-
ture. For instance, I am interested in the fact that (4a) leads
to the inference (4b), but (5a) leaves both (5b) and (5c) as
possible continuations.
(4) a. Bill forced Harry to leave.
b. Harry left.
(5) a. Bill pressured Harry to leave.
b. . . . and Harry left.
c. . . . but Harry didn’t leave
Now it turns out, at least according to the “force-dy-
namic” theory of the semantics of causation (Jackendoff
1990; Talmy 1988), that force and pressure have almost the
same semantic content: they both mean roughly “apply
force to an individual toward the end of that individual per-
forming some action.” They differ only in that force indi-
cates that the application of force is successful, whereas
pressure leaves the issue of success open, while biasing to-
ward a successful result (that is why (5b) uses and and (5c)
uses but). The result is that force has a logical property
whereas pressure has a merely heuristic property. Unlike lo-
gicians in the truth-conditional tradition, I wish to capture
both these properties, in a way that brings out their simi-
larity. More generally, I think that an account of logical in-
ference is only one aspect of the theory of meaning, an im-
portant one to be sure, but not important enough as to
demand precedence over everything else.
R8. Final remarks
Overall, these commentaries illustrate, more vividly than I
could have done myself, the huge rifts among the many
communities engaged in the study of language, as well as
the attitudes that keep these communities isolated from
each other. I wrote Foundations in an effort to throw some
ropes across the rifts, to provide some slender basis for
communication. However, it is clearly impossible for one
individual alone to bridge all the gaps, fill in all the missing
Response/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
701
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

parts, and make all the connections. I hope the present dis-
cussion, along with Foundations itself, can serve as an invi-
tation to others to take part in the enterprise.
References
Letters “a” and “r” appearing before authors’ initials refer to target article
and response, respectively.
Adams, F. & Aizawa, K. (2001) “The bounds of cognition.” Philosophical
Psychology 14:43–64.
[FA]
Aijmer, K. (1996) Conversational routines in English: Convention and creativity.
Longman.
[SG]
Albert, R. & Barabasi A.-L. (2002) Statistical mechanics of complex networks.
Reviews of Modern Physics 74:47–97.
[PE]
Andor, J. (1985) On the psychological relevance of frames. Quaderni di Semantica
6:212–21.
[JA]
(2003) On the social-cognitive and linguistic status of various types of knowledge
and knowledge structures. In: Communication and culture: Argumentative,
cognitive and linguistic perspectives, pp. 115–30, ed. L. I. Komlósi, P.
Houtlosser & M. Leezenberg. SicSat.
[JA]
Andresen, J. T. (1990) Skinner and Chomsky thirty years later. Historiographia
Linguistica 17:145–66.
[ACC]
Arbib, M. A. (1981) Perceptual structures and distributed motor control. In:
Handbook of physiology, section 2: The nervous system, vol. II: Motor control,
Part 1, pp. 1449–80, ed. V. B. Brooks. American Physiological Society.
[MAA]
(2002a) Language evolution: The mirror system hypothesis. In: The handbook of
brain theory and neural networks, pp. 606–611, ed. M. A. Arbib. MIT Press.
[PE]
(2002b) The mirror system, imitation, and the evolution of language. In:
Imitation in animals and artifacts, pp. 229–80, ed. C. Nehaniv & K.
Dautenhahn. MIT Press.
[MAA]
(2003) How far is language beyond our grasp? A response to Hurford. In:
Evolution of communication systems: A comparative approach, ed. D.
Kimbrough Oller & U. Griebel. MIT Press.
[MAA]
Arbib, M. A. & Caplan, D. (1979) Neurolinguistics must be computational.
Behavioral and Brain Sciences 2:449–83.
[MAA]
Arbib, M. A. & Érdi, P. (2000) Précis of Neural organization: Structure, function,
and dynamics. Behavioral and Brain Sciences 23:513–17.
[HS]
Arbib, M. A., Érdi, P. & Szentágothai, J. (1998) Neural organization: Structure,
function, and dynamics. The MIT Press.
[MAA]
Baddeley, A. (2003) Working memory and language: An overview. Journal of
Communication Disorders 36:189–208.
[VC]
Baer, D. M., Peterson, R. F. & Sherman, J. A. (1967) The development of imitation
by reinforcing behavioral similarity to a model. Journal of the Experimental
Analysis of Behavior 10:405–16.
[ACC]
Baillargeon, R. (1986) Representing the existence and the location of hidden
objects: Object permanence in 6- and 8-month-old infants. Cognition 23:21–
41.
[aRJ]
Baker, C. L. & McCarthy, J. J., eds. (1981) The logical problem of language
acquisition. MIT Press.
[aRJ]
Barsalou, L. W. (1999) Perceptual symbol systems. Behavioral and Brain Sciences
22:577–660.
[SE]
Barton, N. & Partridge, L. (2000) Limits to natural selection. BioEssays 22:1075–
84.
[WZ]
Barwise, J. & Perry, J. (1983) Situations and attitudes. MIT Press.
[rRJ, AGBtM]
Batali, J. (2002) The negotiation and acquisition of recursive grammars as a result
of competition among exemplars. In: Linguistic evolution through language
acquisition: Formal and computational models, ed. T. Briscoe. Cambridge
University Press.
[WZ]
Bates, E. & MacWhinney, B. (1982) Functionalist approaches to grammar. In:
Language acquisition: The state of the art, pp. 173–218, ed. E. Wanner & L.
Gleitman. Cambridge University Press
[PFD]
Beckman, M. & Pierrehumbert, J. (1986) Intonational structure in English and
Japanese. Phonology 3:255–309.
[aRJ]
Bellugi, U., Klima, E. S. & Poizner, H. (1989) Language, modality, and the brain.
Trends in Neurosciences 12:380–88.
[aRJ]
Bellugi, U., Wang, P. & Jernigan, T. (1994) Williams syndrome: An unusual
neuropsychological profile. In: Atypical cognitive deficits in developmental
disorders: Implications for brain function, ed. S. Broman & J. Grafman.
Erlbaum.
[aRJ]
Bentall, R. P. & Lowe, C. F. (1987) The role of verbal behavior in human learning:
III. Instructional effects in children. Journal of the Experimental Analysis of
Behavior 47:177–90.
[ACC]
Bentall, R. P., Lowe, C. F. & Beasty, A. (1985) The role of verbal behavior in
human learning: II. Developmental differences. Journal of the Experimental
Analysis of Behavior 43:165–81.
[ACC]
Benton, A. L. & van Allen, M. W. (1968) Impairment in facial recognition in
patients with cerebral disease. Cortex 4:344–58.
[GG]
Bickerton, D. (1981) Roots of language. Karoma.
[aRJ]
(1990) Language and species. University of Chicago Press.
[DB]
(2000) How protolanguage became language. In: The evolutionary emergence of
language, ed. C. Knight, M. Studdert-Kennedy & J. R. Hurford. Cambridge
University Press.
[DB]
Bierwisch, M. (1967) Some semantic universals of German adjectivals.
Foundations of Language 3:1–36.
[aRJ]
(1969) On certain problems of semantic representation. Foundations of
Language 5:153–84.
[aRJ]
Blackburn, P. & Bos, J. (1999) Representation and inference for natural language.
A first course in computational semantics, vol. II; working with discourse
representation structures. University of Saarland. (Unpublished manuscript.)
[AGBtM]
Bloom, P. (2000) How children learn the meanings of words. MIT Press.
[aRJ]
Bowerman, M. (1996) Learning how to structure space for language: A
crosslinguistic perspective. In: Language and space, ed. P. Bloom, M. A.
Peterson, L. Nadel & M. F. Garrett. MIT Press.
[HH]
Bowerman, M. & Choi, S. (2001) Shaping meanings for language: Universal and
language-specific in the acquisition of spatial semantic categories. In:
Language acquisition and conceptual development, pp. 475–511, ed. M.
Bowerman & S. C. Levinson. Cambridge University Press.
[HH]
Branigan, H. P., Pickering, M. J. & Cleland, A. A. (2000) Syntactic coordination in
dialogue. Cognition 75:B13–25.
[SG]
Breedin, S. D. & Saffran, E. M. (1994) Reversal of the concreteness effect in a
patient with semantic dementia. Cognitive Neuropsychology 11:617–60.
[DK]
(1999) Sentence processing in the face of semantic loss: A case study. Journal of
Experimental Psychology: General 128:547–62.
[DK]
Bresnan, J. W., ed. (1982) The mental representation of grammatical relations. MIT
Press.
[arRJ]
(2001) Lexical-functional syntax. Blackwell.
[aRJ]
Brooks, R. A. (1991) Intelligence without representation. In: Mind design II,
pp. 395–420, ed. J. Haugeland. Bradford Books/MIT Press.
[LJ]
Cancho, R. F. & Sole, R. V. (2001) The small-world of human language.
Proceedings of the Royal Society of London, Series B: Biological Sciences
268:2261–65.
[PE]
Caplan, D., Alpert, N. & Waters, G. (1998) Effects of syntactic structure and
propositional number on patterns of regional cerebral blood flow. Journal of
Cognitive Neuroscience 10:541–52.
[VC]
(1999) PET studies of syntactic processing with auditory sentence presentation.
NeuroImage 9:343–51.
[VC]
Carey, S. (1985) Conceptual change in childhood. MIT Press.
[aRJ]
Carnap, R. (1926) Physikalische Begriffsbildung. Karlsruhe.
[JH]
Carruthers, P. (2002) The cognitive functions of language. Behavioral and Brain
Sciences 25(6):657–726.
[CM]
Catania, A. C. (1972) Chomsky’s formal analysis of natural languages: A behavioral
translation. Behaviorism 1:1–15.
[ACC]
(1973a) The concept of the operant in the analysis of behavior. Behaviorism
1:103–16.
[ACC]
(1973b) The psychologies of structure, function, and development. American
Psychologist 28:434–43.
[ACC]
(1980) Autoclitic processes and the structure of behavior. Behaviorism 8:175–
86.
[ACC]
(1987) Some Darwinian lessons for behavior analysis. A review of Peter J.
Bowler’s “The eclipse of Darwinism.” Journal of the Experimental Analysis of
Behavior 47:249–57.
[ACC]
(1990) What good is five percent of a language competence? Behavioral and
Brain Sciences 13:729–31.
[ACC]
(1991) The phylogeny and ontogeny of language function. In: Biological and
behavioral determinants of language development, ed. N. A. Krasnegor, 
D. M. Rumbaugh, R. L. Schiefelbusch & M. Studdert-Kennedy. Erlbaum.
[ACC]
(1995a) Higher-order behavior classes: Contingencies, beliefs, and verbal
behavior. Journal of Behavior Therapy and Experimental Psychiatry 26:191–
200.
[ACC]
(1995b) Single words, multiple words, and the functions of language. Behavioral
and Brain Sciences 18:184–85.
[ACC]
(1996a) Natural contingencies in the creation of naming as a higher-order
behavior class. Journal of the Experimental Analysis of Behavior 65:276–79.
[ACC]
(1996b) On the origins of behavior structure. In: Stimulus class formation in
humans and animals, pp. 3–12, ed. T. R. Zentall and P. Smeets. Elsevier.
[ACC]
References/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
702
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

(1997) An orderly arrangement of well-known facts: Retrospective review of B.
F. Skinner’s “Verbal behavior.” Contemporary Psychology 42:967–70.
[ACC]
(1998) Learning, 4th edition. Prentice-Hall.
[ACC]
(2000) From behavior to brain and back again: Review of Orbach on Lashley-
Hebb. Psycoloquy 11, No. 27. (Online publication: psyc.00.11.027.lashley-
hebb.14.catania).
[ACC]
(2001) Three varieties of selection and their implications for the origins of
language. In: Language evolution: Biological, linguistic and philosophical
perspectives, pp. 55–71, ed. G. Györi. Peter Lang.
[ACC]
(2003) Verbal governance, verbal shaping, and attention to verbal stimuli. In:
Behavior theory and philosophy, pp. 301–21, ed. K. A. Lattal & P. N. Chase.
Kluwer/Academic Press.
[ACC]
Catania, A. C. & Harnad, S., eds. (1988) The selection of behavior: The operant
behaviorism of B. F. Skinner. Cambridge University Press.
[ACC]
Catania, A. C., Horne, P. & Lowe, C. F. (1989) Transfer of function across
members of an equivalence class. Analysis of Verbal Behavior 7:99–110.
[ACC]
Catania, A. C., Lowe, C. F. & Horne, P. (1990) Nonverbal behavior correlated with
the shaped verbal behavior of children. Analysis of Verbal Behavior 8:43–55.
[ACC]
Catania, A. C., Matthews, B. A. & Shimoff, E. (1982) Instructed versus shaped
human verbal behavior: Interactions with nonverbal responding. Journal of
the Experimental Analysis of Behavior 38:233–48.
[ACC]
Catania, A. C., Ono, K. & de Souza, D. (2000) Sources of novel behavior: Stimulus
control arranged for different response dimensions. European Journal of
Behavior Analysis 1:23–32.
[ACC]
Cavalli-Sforza, L. & Feldman, M. (1983) Paradox of the evolution of
communication and of social interactivity. Proceedings of the National
Academy of Sciences USA 80:2017–21.
[WZ]
Chadwick, P. D. J., Lowe, C. F., Horne, P. J. & Higson, P. J. (1994) Modifying
delusions: The role of empirical testing. Behavior Therapy 25:35–49.
[ACC]
Cheney, D. & Seyfarth, R. (1990) How monkeys see the world. University of
Chicago Press.
[aRJ]
Chierchia, G. & McConnell-Ginet, S. (1990) Meaning and grammar: An
introduction to semantics. MIT Press.
[aRJ]
Choe, Y. (2003) Processing of analogy in the thalamocortical circuit. In:
Proceedings of the International Joint Conference on Neural Networks 2003,
pp. 1480–85.
[RJL]
Choi, S. & Bowerman, M. (1991) Learning to express motion events in English and
Korean: The influence of language specific lexicalization patterns. In: Lexical
and conceptual semantics, pp. 83–121, ed. B. Levin & S. Pinker. Blackwell.
[HH]
Chomsky, N. (1957) Syntactic structures. Mouton.
[RF, aRJ]
(1959) Review of B. F. Skinner’s Verbal behavior. Language 35:26–58.
[ACC]
(1965) Aspects of the theory of syntax. MIT Press.
[RF, aRJ]
(1966) Cartesian linguistics. Harper & Row.
[aRJ]
(1972) Studies on semantics in generative grammar. Mouton.
[aRJ]
(1973) Constraints on transformations. In: A Festschrift for Morris Halle, ed. S.
Anderson & P. Kiparsky. Holt, Rinehart & Winston.
[aRJ]
(1975) Reflections on language. Pantheon.
[WZ]
(1977) On wh-movement. In: Formal syntax, ed. P. Culicover, T. Wasow & A.
Akmajian. Academic Press.
[aRJ]
(1981) Lectures on government and binding. Foris.
[SG, aRJ]
(1993) A minimalist program for linguistic theory. In: The view from Building
20: Essays in linguistics in honor of Sylvain Bromberger, pp. 1–52, ed. K.
Hale & S. J. Keyser. MIT Press.
[RF]
(1995) The minimalist program. MIT Press.
[RF, JG, aRJ]
(2000) Minimalist inquiries: The framework. In: Step by step: Essays on
minimalist syntax in honor of Howard Lasnik, pp. 89–155, ed. R. Martin, D.
Michaels & J. Uriagereka. MIT Press.
[RF]
(2001) Derivation by phase. In: Ken Hale: A life in language, pp. 1–52, ed. M.
Kenstowicz. MIT Press.
[RF]
(2002) Beyond explanatory adequacy. (MIT unpublished manuscript.)
[RF]
Chomsky, N. & Halle, M. (1968) The sound pattern of English. Harper & Row.
[aRJ]
Clahsen, H. (1999) Lexical entries and rules of language: A multidisciplinary study
of German inflection. Behavioral and Brain Sciences 22:991–1013.
[JG]
Clahsen, H. & Almazan, M. (1998) Syntax and morphology in Williams syndrome.
Cognition 68:167–98.
[aRJ]
Clark, A. (2000) A theory of sentience. Oxford University Press.
[SE]
Clark, A. & Chalmers, D. (1998) The extended mind. Analysis 58:10–23.
[FA]
Clark, H. H. (1992) Arenas of language use. The University of Chicago Press.
[JA]
(1996) Using language. Cambridge University Press.
[JA, SG]
Cleland, A. A. & Pickering, M. J. (2003) The use of lexical and syntactic
information in language production: Evidence from the priming of noun-
phrase structure. Journal of Memory and Language 49:214–30.
[SG]
Corballis, M. C. (1991) The lopsided ape. Oxford University Press.
[aRJ]
(2002) From hand to mouth: The origins of language. Princeton University Press.
[GG]
Crain, S. (1991) Language acquisition in the absence of experience. Behavioral and
Brain Sciences 14:597–650.
[ACC]
Croft, W. (2001) Radical construction grammar: Syntactic theory in typological
perspective. Oxford University Press.
[DK]
Crow, J. F. (2001) The beanbag lives on. Nature 409:771.
[DCD]
Cruse, A. (2000) Meaning in language. Oxford University Press.
[JA]
Culicover, P. W. (1999a) Review article: Minimalist architectures. Journal of
Linguistics 35:137–50.
[MJS]
(1999b) Syntactic nuts: Hard cases in syntax. Oxford University Press.
[aRJ]
Culicover, P. W. & Jackendoff, R. (1995) Something else for the binding theory.
Linguistic Inquiry 26:249–75.
[aRJ]
(1997) Semantic subordination despite syntactic coordination. Linguistic Inquiry
28:195–217.
[aRJ]
(1999) The view from the periphery: The English correlative conditional.
Linguistic Inquiry 30:543–71.
[aRJ]
(forthcoming) Syntax made simple(r). Oxford University Press.
[arRJ]
Curtiss, S. (1977) Genie: A linguistic study of a modern-day “wild child.” Academic
Press.
[aRJ]
Cutler, A. & Clifton, C., Jr. (1999) Comprehending spoken language: A blueprint
of the listener. In: The neurocognition of language, ed. C. M. Brown & P.
Hagoort. Oxford University Press.
[aRJ]
D’Amato, M. R., Salmon, D. P., Loukas, E. & Tomie, A. (1985) Symmetry and
transitivity of conditional relations in monkeys (Cebus apella) and pigeons
(Columba livia). Journal of the Experimental Analysis of Behavior 44:35–47.
[ACC]
Dapretto, M. & Bookheimer, S. Z. (1999) Form and content: Dissociating syntax
and semantics in sentence comprehension. Neuron 24427–32.
[VC]
Dartnall, T. (2000) Reverse psychologism, cognition and content. Minds and
Machines 10:31–52.
[LJ]
Davis, A. R. (2001) Linking by types in the hierarchical lexicon. CSLI (Center for
the Study of Language and Information).
[DK]
Dawkins, R. (1982) The extended phenotype. Freeman.
[ACC, DCD]
Day, W. F. (1969) On certain similarities between the philosophical investigations
of Ludwig Wittgenstein and the operationism of B. F. Skinner. Journal of the
Experimental Analysis of Behavior 12:489–506.
[ACC]
Deacon, T. W. (1997) The symbolic species. Norton.
[aRJ]
De Boer, B. (2000) Self organization in vowel systems. Journal of Phonetics
28:441–65.
[WZ]
Degraff, M., ed. (1999) Language creation and language change: Creolization,
diachrony, and development. MIT Press.
[aRJ]
Dennett, D. C. (1998) Reflections on language and mind. In: Language and
thought: Interdisciplinary themes, pp. 284–94, ed. P. Carruthers & J. Boucher.
Cambridge University Press.
[CM]
Dennett, D. C. (1995) Darwin’s dangerous idea. Simon & Schuster.
[DCD]
de Saussure, F. (1915) Cours de linguistique générale, ed. C. Bally & A. Sechehaye.
(English translation. Course in general linguistics.) Philosophical Library.
[aRJ]
Dinsmoor, J. A. (1983) Observing and conditioned reinforcement. Behavioral and
Brain Sciences 6:693–728.
[ACC]
Dominey, P. F. (2000) Conceptual grounding in simulation studies of language
acquisition. Evolution of Communication 4(1):57–85.
[PFD]
(2003) Learning grammatical constructions in a miniature language from
narrated video events. In: Proceedings of the 25th Annual Meeting of the
Cognitive Science Society, Boston, 2003.
[PFD]
Dominey, P. F., Hoen, M., Blanc, J. M. & Lelekov-Boissard, T. (in press)
Neurological basis of language and sequential cognition: Evidence from
simulation, aphasia and ERP studies. Brain and Language 86(2):207–25.
[PFD]
Dorogovtsev, S. N. & Mendes, J. F. F. (2001) Language as an evolving word web.
Proceedings of the Royal Society of London, Series B: Biological Sciences
268:2603–606.
[PE]
Druks, J. & Masterson, J., eds. (2003) The neural basis of verbs. Journal of
Neurolinguistics (Special Issue), Vol. 16, Nos. 2–3.
[DK]
Dube, W. V., McIlvane, W. J., Callahan, T. D. & Stoddard, L. T. (1993) The search
for stimulus equivalence in nonverbal organisms. Psychological Record
43:761–78.
[ACC]
Dufva, H. & Lähteenmäki, M. (1996) But who killed Harry? A dialogical approach
to language and consciousness. Pragmatics and Cognition 4:35–53.
[aRJ]
Dummett, M. (1978) Truth and other enigmas. Harvard University Press.
[LJ]
Edelman, G. M. (1987) Neural Darwinism: The theory of neuronal group selection.
Basic Books.
[HS]
Edelman, S. (1999) Representation and recognition in vision. MIT Press.
[SE]
(2002) Constraining the neural representation of the visual world. Trends in
Cognitive Sciences 6:125–31.
[SE]
(in press) Bridging language with the rest of cognition: Computational,
algorithmic and neurobiological issues and methods. In: Methods in cognitive
References/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
703
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

linguistics, ed. M. Gonzalez-Marquez, I. Mittelberg, S. Coulson & M. J.
Spivey. John Benjamins.
[SE]
Edelman, S. & Christiansen, M. H. (2003) How seriously should we take
minimalist syntax? A comment on Lasnik. Trends in Cognitive Science 7:60–
61.
[SE]
Elman, J. (1990) Finding structure in time. Cognitive Science 14:179–211.
[aRJ]
Elman, J., Bates, E., Johnson, M., Karmiloff-Smith, A., Parisi, D. & Plunkett, K.
(1996) Rethinking innateness: A connectionist perspective on development.
MIT Press.
[aRJ, MJS]
Erman, L. D., Hayes-Roth, F. A., Lesser, V. R. & Reddy, D. R. (1980) The
HEARSAY-II speech-understanding system: Integrating knowledge to resolve
uncertainty. Computing Surveys 12:213–53.
[MAA]
Esper, E. A. (1973) Analogy and association in linguistics and psychology.
University of Georgia Press.
[ACC]
Evans, G. (1985) Collected papers. Oxford University Press.
[AGBtM]
Fauconnier, G. (1985) Mental spaces: Aspects of meaning construction in natural
language. MIT Press.
[aRJ]
Fillmore, C., Kay, P. & O’Connor, M. C. (1988) Regularity and idiomaticity in
grammatical constructions: The case of let alone. Language 64:501–38.
[aRJ]
Fischer, S. D. & Siple, P., eds. (1990) Theoretical issues in sign language research
1. University of Chicago Press.
[aRJ]
Fisher, R. (1922) On the dominance ratio. Proceedings of the Royal Society of
Edinburgh 42:321–431.
[WZ]
Fitch, W. T. (2000) The evolution of speech: A comparative review. Trends in
Cognitive Sciences 4:258–67.
[aRJ]
Flynn, S. & O’Neill, W., eds. (1988) Linguistic theory in second language
acquisition. Reidel.
[aRJ]
Fodor, J. (1987) Psychosemantics: The problem of meaning in the philosophy of
mind. MIT Press.
[aRJ]
(1990) A theory of content and other essays. MIT/Bradford.
[FA]
Fodor, J., Bever, T. & Garrett, M. (1974) The psychology of language. McGraw-
Hill.
[aRJ]
Frazier, L. & Fodor, J. D. (1978) The sausage machine: A new two-stage parsing
model. Cognition 6:291–325.
[MJS]
Frege, G. (1892) Über sinn und bedeutung. Zeitschrift für Philosophie und
Philosophische Kritik 100:25–50. (English translation in: Translations from
the philosophical writings of Gottlob Frege, ed. P. Geach & M. Black.
Blackwell).
[aRJ, AGBtM]
(1953) The foundations of arithmetic, 2nd edition, trans. J. L. Austin. Blackwell.
(Original work published 1884.)
[LJ]
Freidin, R. (1997) Review of Noam Chomsky’s The minimalist program. Language
73(3):571–582.
[RF]
Garrod, S. & Anderson, A. (1987) Saying what you mean in dialogue: A study in
conceptual and semantic co-ordination. Cognition 27:181–218.
[SG]
Gentner D., Kolyoak, K. J. & Konokiv, B. N. (2001) The analogical mind:
Perspectives from cognitive science. MIT Press.
[RJL]
Gewirtz, J. L. & Stingle, K. G. (1968) Learning of generalized imitation as the basis
for identification. Psychological Review 75:374–97.
[ACC]
Gleitman, L. R. & Landau, B., eds. (1994) The acquisition of the lexicon. MIT
Press.
[aRJ]
Goldberg, A. (1995) Constructions: A construction grammar approach to argument
structure. University of Chicago Press.
[aRJ, RJL]
Goldberg, A. E. (1998) Patterns of experience in patterns of language. In: The new
psychology of language, pp. 203–19, ed. M. Tomasello. Erlbaum.
[SE]
(2003) Constructions: A new theoretical approach to language. Trends in
Cognitive Sciences 7:219–24.
[DK]
Goldenberg, G. (1995) Imitating gestures and manipulating a mannikin – the
representation of the human body in ideomotor apraxia. Neuropsychologia
33:63–72.
[GG]
(1996) Defective imitation of gestures in patients with damage in the left or right
hemisphere. Journal of Neurology, Neurosurgery, and Psychiatry 61:176–80.
[GG]
(1999) Matching and imitation of hand and finger postures in patients with
damage in the left or right hemisphere. Neuropsychologia 37:559–66.
[GG]
Goldenberg, G. & Hagmann, S. (1998) Tool use and mechanical problem solving in
apraxia. Neuropsychologia 36:581–89.
[GG]
Goldenberg, G., Hartmann, K. & Schlott, I. (2003) Defective pantomime of object
use in left brain damage: Apraxia or asymbolia? Neuropsychologia 41:1565–
73.
[GG]
Goldenberg, G. & Strauss, S. (2002) Hemisphere asymmetries for imitation of
novel gestures. Neurology 59:893–97.
[GG]
Goldsmith, J. (1979) Autosegmental phonology. Garland Press.
[aRJ]
Gopnik, M. (1999) Some evidence for impaired grammars. In: Language, logic,
and concepts, ed. R. Jackendoff, P. Bloom & K. Wynn. MIT Press.
[aRJ]
Greenberg, J. H. (1986) On being a linguistic anthropologist. Annual Review of
Anthropology 15:1–24.
[BBV]
Greenspoon, J. (1955) The reinforcing effect of two spoken sounds on the
frequency of two responses. American Journal of Psychology 68:409–16.
[ACC]
Gross, M. (1975) Méthodes en syntaxe. Hermann.
[rRJ, RJL]
Haack, S. (1978) Philosophy of logics. Cambridge University Press.
[LJ]
Haldane, J. B. S. (1964) A defense of beanbag genetics. Perspectives in Biological
Medicine 8:343–59. (Reprinted in: Selected papers of J. B. S. Haldane, pp. 1–
17, ed. C. R. Dronamraju. Garland.)
[DCD]
Hale, B. & Wright, C., eds. (1999) A companion to the philosophy of language.
Blackwell Companions to Philosophy. Blackwell.
[AGBtM]
Hale, K. & Keyser, S. (1993) On argument structure and the lexical expression of
syntactic relations. In: The view from Building 20, ed. K. Hale & S. Keyser.
MIT Press.
[DK]
Halle, M. & Idsardi, W. (1995) Stress and metrical structure. In: Handbook of
theoretical phonology, ed. J. Goldsmith. Blackwell.
[aRJ]
Halliday, M. A. K. (1985) Spoken and written language. Oxford University Press.
[MJS]
Hanson, A. R. & Riseman, E. (1987) A methodology for the development of
general knowledge-based vision systems. In: Vision, brain, and cooperative
computation, pp. 285–328, ed. M. A. Arbib & A. R. Hanson. MIT Press.
[MAA]
Harris, R. A. (1993) The linguistics wars. Oxford University Press.
[aRJ]
Hashimoto, T. (2001) The constructive approach to the dynamical view of
language. In: Simulating the evolution of language, pp. 307–24, ed. A.
Cangelosi & D. Parisi. Springer Verlag.
[PE]
Haspelmath, M. (2003) The geometry of grammatical meaning: Semantic maps
and cross-linguistic comparison. In: The new psychology of language:
Cognitive and functional approaches to language structure, vol. 2, ed. M.
Tomasello. Erlbaum.
[DK]
Hauser, M. D. (2000) Wild minds: What animals really think. Henry Holt.
[aRJ]
Hauser, M. D., Chomsky, N. & Fitch, T. (2002) The faculty of language: What is it,
who has it, and how did it evolve? Science 298:1569–79.
[aRJ]
Heim, I. & Kratzer, A (1998) Semantics in generative grammar. Blackwell.
[aRJ]
Hinton, G. E., McClelland, J. L. & Rumelhart, D. E. (1986) Distributed
representations. In: Parallel distributed processing: Explorations in the
microstructure of cognition. Vol. 1: Foundations, ed. D. E. Rumelhart, J. L.
McClelland and the PDP Research Group. MIT Press.
[HH]
Hopper, P. J. (1998) Emergent grammar. In: The new psychology of language, ed.
M. Tomasello. Erlbaum.
[MJS]
Horne, P. J. & Lowe, C. F. (1996) On the origins of naming and other symbolic
behavior. Journal of the Experimental Analysis of Behavior 65:185–241.
[ACC]
Householder, F. W. (1971) Linguistic speculations. Cambridge University Press.
[RJL]
Huck, G. & Goldsmith, J. (1995) Ideology and linguistic theory. University of
Chicago Press.
[aRJ]
Humblé, P. (2001) Dictionaries and language learners. Haag and Herchen.
[EM]
Hurford (2003) Language beyond our grasp: What mirror neurons can, and cannot,
do for language evolution. In: Evolution of communication systems: A
comparative approach, ed. D. Kimbrough Oller & U. Griebel. MIT Press.
[MAA]
Itkonen, E. & Haukioja, J. (1997) A rehabilitation of analogy in syntax (and
elsewhere). In: Metalinguistik im Wandel: Die kognitive Wende in
Wissenschaftstheorie und Linguistik, pp. 131–77, ed. A. Kertesz. Peter Lang
Verlag.
[RJL]
Jackendoff, R. (1972) Semantic interpretation in generative grammar. MIT Press.
[aRJ]
(1978) Grammar as evidence for conceptual structure. In: Linguistic theory and
psychological reality, pp. 201–28, ed. M. Halle, J. Bresnan & G. Miller. MIT
Press.
[rRJ]
(1983) Semantics and cognition. MIT Press.
[JA, aRJ]
(1987) Consciousness and the computational mind. Bradford Books/MIT Press.
[CJ, CM]
(1990) Semantic structures. MIT Press.
[arRJ]
(1992a) Languages of the mind: Essays on mental representation. MIT Press.
[DJ, EM]
(1992b) Mme. Tussaud meets the binding theory. Natural Language and
Linguistic Theory 10.1:1–31.
[aRJ]
(1996a) How language helps us think. Pragmatics and Cognition 4(1):1–34.
[CM]
(1996b) Preliminaries to discussing how language helps us think. Pragmatics and
Cognition 4(1):197–213.
[CM]
(1996c) The proper treatment of measuring out, telicity, and possibly even
quantification in English. Natural Language and Linguistic Theory 14:305–
54.
[aRJ]
(1997) The architecture of the language faculty. MIT Press.
[aRJ]
(2002) Foundations of language: Brain, meaning, grammar, evolution. Oxford
University Press.
[FA, JA, MAA, ACC, DCD, PFD, RF, SG, JG, GG, JH,
arRJ, DJ, LJ, DK, RJL, DGL, CM, EM, MJS, BV, RW, WZ]
References/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
704
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

Janda, L. (1993) A geography of case semantics. Mouton de Gruyter.
[MJS]
Johnson, K. R. & Layng, T. V. J. (1992) Breaking the structuralist barrier: Literacy
and numeracy with fluency. American Psychologist 47:1475–90.
[ACC]
Just, M. A. & Carpenter, P. A. (1992) A capacity theory of comprehension:
Individual differences in working memory. Psychological Review 99:122–49.
[VC, rRJ]
Just, M., Carpenter, P., Keller, T. A., Eddy, W. F. & Thulborn, K. R. (1996) Brain
activation modulated by sentence comprehension. Science 274:114–16.
[VC]
Kable, J. W., Lease-Spellmeyer, J. & Chatterjee, A. (2002) Neural substrates of
action event knowledge. Journal of Cognitive Neuroscience 14:795–805.
[DK]
Kager, R. (1995) The metrical theory of word stress. In: Handbook of theoretical
phonology, ed. J. Goldsmith. Blackwell.
[aRJ]
Katz, J. & Fodor, J (1963) The structure of a semantic theory. Language 39:170–
210.
[aRJ]
Katz, J. & Postal, P. M. (1964) An integrated theory of linguistic descriptions. MIT
Press.
[aRJ]
Kegl, J., Senghas, A. & Coppola, M. (1999) Creations through contact: Sign
language emergence and sign language change in Nicaragua. DeGraff.
[aRJ]
Keijzer, F. A. & Bem, S. (1996) Behavioral systems interpreted as autonomous
agents and as coupled dynamic systems. Philosophical Psychology 9:323–46.
[EM]
Keil, F. C. (1989) Concepts, kinds, and cognitive development. MIT Press.
[aRJ]
Kemmerer, D. (2000a) Grammatically relevant and grammatically irrelevant
features of verb meaning can be independently impaired. Aphasiology
14:997–1020.
[DK]
(2000b) Selective impairment of knowledge underlying prenominal adjective
order: Evidence for the autonomy of grammatical semantics. Journal of
Neurolinguistics 13:57–82.
[DK]
(2003) Why can you hit someone on the arm but not break someone on the arm?
A neuropsychological investigation of the English body-part possessor
ascension construction. Journal of Neurolinguistics 16:13–36.
[DK]
Kemmerer, D. & Tranel, D. (2003) A double dissociation between the meanings of
action verbs and locative prepositions. Neurocase 9:421–35.
[DK]
Kemmerer, D. & Wright, S. K. (2002) Selective impairment of knowledge
underlying un-prefixation: Further evidence for the autonomy of grammatical
semantics. Journal of Neurolinguistics 15:403–32.
[DK]
Kempermann, G. & Wiskott, L. (2004) What is the functional role of new neurons
in the adult dentate gyrus? In: Stem cells in the nervous system: Function and
clinical implications. Springer-Verlag.
[HH]
Kibrik, A. E. (2001) Subject-oriented vs. subjectless languages. In: Language
typology and language universals: An international handbook, vol 2,
pp. 1413–24, ed. M. Haspelmath, E. König, W. Oesterreicher & W. Raible.
Mouton de Gruyter.
[BBV]
Kimura, D. (1983) Neuromotor mechanisms in human communication. Oxford
University Press/Clarendon Press.
[GG]
Kirby, S. (2000) Syntax without natural selection: How compositionality emerges
from vocabulary in a population of learners. In: The evolutionary emergence of
language: Social function and the origins of linguistic form, ed. C. Knight, J.
Hurford & M. Studdert-Kennedy. Cambridge University Press.
[WZ]
(2002a) Learning, bottlenecks and the evolution of recursive syntax. In:
Linguistic evolution through language acquisition: formal and computational
models, ed. T. Briscoe. Cambridge University Press.
[WZ]
(2002b) Natural language from artificial life. Artificial Life 8:185–215.
[WZ]
Klein, W. & Perdue, C. (1997) The basic variety, or: Couldn’t language be much
simpler? Second Language Research 13:301–47.
[aRJ]
Klima, E. S. & Bellugi, U. (1979) The signs of language. Harvard University Press.
[aRJ]
Koerner, E. F. K. (1994) The anatomy of a revolution in the social sciences:
Chomsky in 1962. Dhumbadji! 1(4):3–17.
[SE]
Köhler, W. (1927) The mentality of apes. Routledge & Kegan Paul.
[aRJ]
Komarova, N. L., Niyogi, P. & Nowak, M. A. (2001) The evolutionary dynamics of
grammar acquisition. Journal of Theoretical Biology 209:43–59.
[PE]
Ladd, D. R. (1996) Intonational phonology. Cambridge University Press.
[aRJ]
Lai, C., Fisher, S., Hurst, J., Vargha-Kadem, F. & Monaco, A. (2001) A fork-head-
domain gene is mutated in a severe speech and language disorder. Nature
413:519–23.
[VC]
Lakoff, G. (1971) On generative semantics. In: Semantics: An interdisciplinary
reader in philosophy, linguistics, and psychology, ed. D. Steinberg & L.
Jakobovits. Cambridge University Press.
[aRJ]
(1987) Women, fire, and dangerous things. University of Chicago Press.
[ACC,
aRJ]
Lakoff, G. & Johnson, M. (1980) Metaphors we live by. University of Chicago
Press.
[ACC]
Lamb, S. (1966) Outline of stratificational grammar. Georgetown University Press.
[aRJ, DGL]
(1999) Pathways of the brain: The neurocognitive basis of language. John
Benjamins.
[DGL]
Langacker, R. W. (1987) Foundations of cognitive grammar, vol. 1: Theoretical
prerequisites. Stanford University Press.
[SE, aRJ, MJS]
(1991) Foundations of cognitive grammar, vol. 2. Stanford University Press.
[MJS]
Lashley, K. S. (1951) The problem of serial order in behavior. In: Cerebral
mechanisms in behavior, pp. 112–46, ed. L. A. Jeffress. Wiley.
[ACC]
Lavie, R. J. (2003) Le locuteur Analogique ou la grammaire mise à sa place. Thèse
de l’Université de Paris 10 Nanterre, France.
[English edition: The
analogical speaker, or grammar put in its place. Doctoral Dissertation,
University of Paris 10, Nanterre, France.] Eho Productions.
[RJL]
Lenneberg, E. (1967) Biological foundations of language. Wiley.
[aRJ]
Lerdahl, F. & Jackendoff, R. (1983) A Generative theory of tonal music. MIT
Press.
[aRJ]
Leslie, A. M. (1994) ToMM, ToBy, and agency: Core architecture and domain
specificity. In: Mapping the mind, pp. 119–48, ed. L. Hirschfeld & S. Gelman.
Cambridge University Press.
[DJ]
Lesser, V. R., Fennel, R. D., Erman, L. D. & Reddy, D. R. (1975) Organization of
the HEARSAY-II speech understanding system. IEEE Transactions on
Acoustics, Speech, and Signal Processing 23:11–23.
[MAA]
Levelt, W. J. M. (1989) Speaking: From intention to articulation. MIT Press.
[aRJ, CM]
(1993) Lexical access in speech production. Blackwell.
[JG]
(1999) Producing spoken language: A blueprint of the speaker. In: The
neurocognition of language, ed. C. M. Brown & P. Hagoort. Oxford University
Press.
[aRJ]
Lewis, D. (1972) General semantics: Semantics for natural language, ed. D.
Davidson & G. Harman. Reidel.
[aRJ]
Liberman, M. & Prince, A (1977) On stress and linguistic rhythm. Linguistic
Inquiry 8:249–336.
[aRJ]
Lockridge, C. B. & Brennan, S. E. (2002) Addresses’ needs influence speakers’
early syntactic choices. Psychonomic Bulletin and Review 9(3):550–57.
[BBV]
Lovaas, O. I. (1964) Cue properties of words: The control of operant responding
by rate and content of verbal operants. Child Development 35:245–56.
[ACC]
Luria, A. R. (1973) The working brain. Penguin Books.
[MAA]
MacAogáin, E. (1999) Information and appearance. Behavioral and Brain Sciences
22:159–60.
[EM]
Macnamara, J. (1982) Names for things. MIT Press.
[aRJ]
Marcus, G. (2001) The algebraic mind: Integrating connectionism and cognitive
science. MIT Press.
[PFD, arRJ, RJL,]
Marr, D. (1982) Vision. Freeman.
[aRJ, MJS]
Marshall, J., Chiat, S., Robson, J. & Pring, T. (1996) Calling a salad a federation: An
investigation of semantic jargon. Part 2: Verbs. Journal of Neurolinguistics
9:251–60.
[DK]
McCawley, J. D. (1968) Lexical insertion in a transformational grammar without
deep structure. In: Papers from the Fourth Meeting of the Chicago Linguistic
Society, ed. B. Darden, C.-J. N. Bailey & A. Davison. University of Chicago
Department of Linguistics.
[aRJ]
McDonald, S. (1993) Viewing the brain sideways? Frontal versus right hemisphere
explanations of non-aphasic language disorders. Aphasiology 7:535–49.
[GG]
Mitchener, W. G. & Nowak, M. A. (2003) Chaos and language. Proceedings of the
Royal Society of London B: Biological Sciences. (in press)
http://www.journals.royalsoc.ac.uk/app/home/contribution.asp
[PE]
Moerk, E. L. (1992) First language: Taught and learned. Brookes.
[ACC]
Mohanan, T. & Wee, L., eds. (1999) Grammatical semantics: Evidence for
structure in meaning. CSLI (Center for the Study of Language and
Information).
[DK]
Molina, C. (2002) Could you think Carruthers’ ideas without having to speak
them? Talk with yourself if you want to have any thought on that.
(Commentary on Carruthers 2002) Behavioral and Brain Sciences 25(6):692–
93.
[CM]
Montgomery, J. (2000) Relation of working memory to off-line and real-time
sentence processing in children with specific language impairment. Applied
Psycholinguistics 21:117–48.
[VC]
Moro, A., Tettamanti, M., Perani, D., Donati, C., Cappa, S. F. & Fazio, F. (2001)
Syntax and the brain? Disentangling grammar by selective anomalies.
Neuroimage 13:110–18.
[VC]
Motter, A. E., de Moura, A. P. S., Lai, Y.-C. & Dasgupta, P. (2002) Topology of the
conceptual network of language. Physical Review E 65(065102):1–4.
[PE]
Murphy, G. (2003) The big book of concepts. MIT Press.
[CM]
Nagel, T. (1997) The last word. Oxford University Press.
[LJ]
Nakano, Y., Felser, C. & Clahsen, H. (2002) Antecedent priming at trace positions
in Japanese long-distance scrambling. Journal of Psycholinguistic Research
31:531–70.
[SE]
References/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
705
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

Neuringer, A. (2002) Operant variability: Evidence, functions, and theory.
Psychonomic Bulletin and Review 9:672–705.
[ACC]
Newman, S. D., Just, M. A., Keller, T. A., Roth, J. & Carpenter, P. A. (2003)
Differential effects of syntactic and semantic processing on the subregions of
Broca’s area. Cognitive Brain Research 16:297–307.
[VC]
Newmann, M. E. J. (2003) The structure and function of complex networks. SIAM
Review 45:167–256.
[PE]
Newmeyer, F. J. (1980) Linguistic theory in America: The first quarter-century of
transformational-generative grammar. Academic Press.
[aRJ]
(1998) On the supposed “counterfunctionality” of Universal Grammar: Some
evolutionary implications. In: Approaches to the evolution of language, ed. J.
Hurford, M. Studdert-Kennedy & C. Knight. Cambridge University Press.
[aRJ]
Newport, E. (1990) Maturational constraints on language learning. Cognitive
Science 14:11–28.
[aRJ]
Nicolis, J. & Tsuda, I. (1989) On the parallel between Zipf’s law and 1/f processes
in chaotic systems possessing coexisting attractors a possible mechanism for
language formation in the cerebral cortex Progress of Theoretical Physics
82:254–74. http://www2.yukawa.kyoto-u.ac.jp/~ptpwww/
[PE]
Nowak, M. A., Komarova, N. & Niyogi, P. (2001) Evolution of universal grammar.
Science 291:114–18.
[WZ]
Nowak, M. A. & Krakauer, D. (1999) The evolution of language. Proceedings of the
National Academy of Sciences USA 96:8028–33.
[PE]
Nunberg, G. (1979) The non-uniqueness of semantic solutions: Polysemy.
Linguistics and Philosophy 3:143–84.
[PFD, aRJ]
Oliphant, M. & Batali, J. (1996) Learning and the emergence of coordinated
communication. Center for Research on Language Newsletter 11(1):1–46.
[WZ]
O’Reilly, R. C. & Busby, R. S. (2002) Generalizable relational binding from coarse-
coded distributed representations. In: Advances in neural information
processing systems, ed. T. G. Dietterich, S. Becker & Z. Ghahramani. MIT
Press.
[HH]
O’Reilly, R. C. & Norman, K. A. (2002) Hippocampal and neocortical
contributions to memory: Advances in the complementary learning systems
framework. Trends in Cognitive Sciences 6:505–10.
[HH]
Oudeyer, P.-Y. (2002) A unified model for the origins of phonemically coded
syllable systems. In: Proceedings of the 24th Annual Conference of the
Cognitive Science Society, ed. B. Bel & I. Marlien. Erlbaum.
[WZ]
Parsons, T. (1990) Events in the semantics of English: A study in subatomic
semantics. MIT Press.
[HH]
Partee, B., ed. (1976) Montague grammar. Academic Press.
[aRJ]
Perlmutter, D. M., ed. (1983) Studies in relational grammar 1. University of
Chicago Press.
[aRJ]
Perner, J. & Lang B. (1999) Development of theory of mind and executive control.
Trends in Cognitive Sciences 3:337–44.
[BBV]
Pickering, M. J. & Garrod, S. (in press) Toward a mechanistic psychology of
dialogue. Behavioral and Brain Sciences.
[SG, BBV]
Pickett, E. R., Kuniholm, E., Protopapas, A., Friedman, J. & Lieberman, P. (1998)
Selective speech motor, syntax and cognitive deficits associated with bilateral
damage to the putamen and the head of the caudate nucleus: A case study.
Neuropsychologia 36:173–88.
[VC]
Piñango, M. M., Zurif, E. & Jackendoff, R. (1999) Real-time processing
implications of enriched composition at the syntax-semantics interface.
Journal of Psycholinguistic Research 28:395–414.
[aRJ]
Pinker, S. (1989) Learnability and cognition: The acquisition of argument
structure. MIT Press.
[aRJ; DK]
(1991) Rules of language. Science 253:530–35.
[JG]
(1997) Words and rules in the human brain. Nature 387:547–48.
[JG]
(1999) Words and rules. Basic Books.
[aRJ]
Pinker, S. & Bloom, P. (1990) Natural language and natural selection. Behavioral
and Brain Sciences 13:707–26.
[aRJ, WZ]
Pinker, S. & Jackendoff R. (forthcoming) The faculty of language: What’s special
about it? Cognition [aRJ]
Pinto-Correia, C. (1996) Homunculus: Historiographic misunderstandings of 
preformationist terminology; an essay abstracted from Pinto-Correia (1997).
Online publication, available at: http://zygote.swarthmore.edu/fert1b.html.
[MAA]
(1997) The ovary of Eve: Egg and sperm and preformation. University of
Chicago Press.
[MAA]
Pollard, C. & Sag, I. (1987) Information-based syntax and semantics. Center for
the Study of Language and Information.
[arRJ]
(1994) Head-driven phrase structure grammar. University of Chicago Press.
[arRJ]
Port, R. F. & van Gelder, T., eds. (1995) Mind as motion. MIT Press.
[MJS]
Postal, P. M. (1970) On the surface verb “remind.” Linguistic Inquiry 1:37–120.
[aRJ]
Poulson, C. L. & Kymissis, E. (1988) Generalized imitation in infants. Journal of
Experimental Child Psychology 46:324–36.
[ACC]
Poulson, C. L., Kymissis, E., Reeve, K. F., Andreatos, M. & Reeve, L. (1991)
Generalized vocal imitation in infants. Journal of Experimental Child
Psychology 5:267–79.
[ACC]
Povinelli, D. (2002) Folk physics for apes. Oxford University Press.
[aRJ]
Premack, D. (1976) Intelligence in ape and man. Erlbaum.
[aRJ]
Prince, A. & Smolensky, P. (1993) Optimality theory: Constraint interaction in
generative grammar. Rutgers University Center for Cognitive Science.
[aRJ]
Pryor, K. W., Haag, R. & O’Reilly, J. (1969) The creative porpoise: Training for
novel behavior. Journal of the Experimental Analysis of Behavior 12:653–61.
[ACC]
Pulvermuller, F. (2002) The neuroscience of language. On brain circuits of words
and serial order. Cambridge University Press.
[HH]
Pulvermuller, F., Hummel F., Harle, M. (2001) Walking or talking? Behavioral and
neurophysiological correlates of action verb processing. Brain and Language
78:143–68.
[DK]
Pustejovsky, J. (1995) The generative lexicon. MIT Press.
[aRJ]
Putnam, H. (1975) The meaning of “meaning.” In: Language, mind, and
knowledge, ed. K. Gunderson. University of Minnesota Press.
[aRJ]
Rappaport Hovav, M. & Levin, B. (1998) Building verb meanings. In: The
projection of arguments: Lexical and compositional factors, ed. M. Butt & W.
Geuder. CSLI (Center for the Study of Language and Information).
[DK]
Rizzolatti, G. & Arbib, M. A. (1998) Language within our grasp. Trends in
Neuroscience 21(5):188–94.
[PE]
Rizzolatti, G. Fogassi, L. & Gallese, V. (2001) Neurophysiological mechanisms
underlying the understanding and imitation of action. Nature Reviews
Neuroscience 2:661–70.
[DK]
Rosenfarb, I. S., Newland, M. C., Brannon, S. E. & Howey, D. S. (1992) Effects of
self-generated rules on the development of schedule-controlled behavior.
Journal of the Experimental Analysis of Behavior 58:107–21.
[ACC]
Rumelhart, D. & McClelland, J. (1986) On learning the past tense of English
verbs. In: Parallel distributed processing, vol. 2, ed. J. McClelland, D.
Rumelhart & the PDP Research Group. MIT Press.
[aRJ]
Russell, E. S. (1916) Form and function. John Murray.
[ACC]
Ryle, G. (1949) The concept of mind. University of Chicago Press.
[aRJ]
Sadock, J. (1991) Autolexical syntax: A theory of parallel grammatical
representations. University of Chicago Press.
[aRJ, RJL]
Savage-Rumbaugh, S., Shanker, S. & Taylor, T. (1998) Apes, language, and the
human mind. Oxford University Press.
[aRJ]
Schnelle, H. (1980) Introductory remarks on theoretical neurolinguistics.
Language Research (Seoul) 16(2):225–36.
[HS]
(1981) Elements of theoretical net-linguistics. Theoretical Linguistics 8:67–100.
[HS]
(1988) Turing naturalized. Von Neumann’s unfinished project. In: The universal
Turing machine: A half-century survey, ed. R. Herken. Oxford University
Press, and Kammerer & Unverzagt.
[HS]
(1991a) From Leibniz to artificial intelligence. In: Topics in philosophy and
artificial intelligence, ed. L. Albertazzi & R. Poli. Istituto Mitteleuropeo di
Cultura.
[HS]
(1991b) Die Natur der Sprache. W. de Gruyter (2nd ed. 1996).
[HS]
(1996a) Approaches to computational brain theories of language. A review of
recent proposals. Theoretical Linguistics 22:49–104.
[HS]
(1996b) Reflections on Chomsky’s Language and thought. Theoretical
Linguistics 22:105–24.
[HS]
(1997) Linguistic structure, brain topography and cerebral process. Acta
Linguistica Hafniensia (The Roman Jakobson Centennial) 29:271–303.
[HS]
Schnelle, H. & Doust, R. (1992) A net-linguistic “Early” parser. In: Connectionist
approaches to natural language processing, ed. R. G. Reilly & N. E. Sharkey.
Erlbaum.
[HS]
Schwartz, M. F., Buxbaum, L. J., Montgomery, M. W., Fitzpatrick-DeSalme, E. J.,
Hart, T., Ferraro, M., Lee, S. S. & Coslett, H. B. (1999) Naturalistic action
production following right hemisphere stroke. Neuropsychologia 37:51–66.
[GG]
Searle, J. (1980) Minds, brains, and programs. Behavioral and Brain Sciences
3:417–24.
[FA, aRJ]
Segal, G. (1996) The modularity of theory of mind. In: Theories of theories of
mind, pp. 141–57, ed. P. Carruthers & P. K. Smith. Cambridge University
Press.
[DJ]
Seidenberg, M. S. & Petitto, L. (1978) Signing behavior in apes: A critical review.
Cognition l7:177–215.
[aRJ]
Selkirk E. O. (1984) Phonology and syntax: The relation between sound and
structure. MIT Press.
[RJL]
Sellars, W. (1963) Science, perception and reality. Routlege.
[DCD]
Shapiro, K. & Caramazza, A., eds. (2002) The role and neural representation of
grammatical class. Journal of Neurolinguistics: Special Issue Vol. 15, Nos. 3–5.
[DK]
Shastri, L. (2002) Episodic memory and cortico-hippocampal interactions. Trends
in Cognitive Sciences 6:162–68.
[HH]
References/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
706
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

Shastri, L. & Ajjanagadde, V. (1993) From simple associations to systematic
reasoning: A connectionist representation of rules, variables and dynamic
bindings. Behavioral and Brain Sciences 16:417–94.
[HH]
Shimoff, E. & Catania, A. C. (1998) The verbal governance of behavior. In:
Handbook of research methods in human operant behavior, pp. 371–404, ed.
K. A. Lattal & M. Perone. Plenum.
[ACC]
Simoncelli, E. P. & Olshausen, B. A. (2001) Natural image statistics and neural
representation. Annual Review of Neuroscience 24(1):1193–216.
[HH]
Skinner, B. F. (1935) The generic nature of the concepts of stimulus and response.
Journal of General Psychology 12:40–65.
[ACC]
(1938) The behavior of organisms: An experimental analysis. Appleton-Century-
Crofts.
[ACC]
(1957) Verbal behavior. Appleton-Century-Crofts.
[ACC]
(1969) An operant analysis of problem solving. In: B. F. Skinner, Contingencies
of reinforcement, pp. 133–57. Appleton-Century-Crofts.
[ACC]
(1981) Selection by consequences. Science 213:501–504.
[ACC]
(1988) Responses to commentaries. In: The selection of behavior: The operant
behaviorism of B. F. Skinner, ed. A. C. Catania & S. Harnad. Cambridge
University Press.
[ACC]
Slobin, D. I. (1997) The origins of grammaticizable notions: Beyond the individual
mind. In: The crosslinguistic study of language acquisition, vol. 5, ed. D. I.
Slobin. Erlbaum.
[DK]
Smith, B. C. (1996) On the origin of objects. MIT Press.
[LJ]
Smith, T. L. (1986) Biology as allegory: A review of Elliott Sober’s “The nature of
selection.” Journal of the Experimental Analysis of Behavior 46:105–12.
[ACC]
Smolensky, P. (1999) Optimality theory. In: The MIT encyclopedia of the cognitive
sciences, ed. R. A. Wilson & F. C. Keil. MIT Press.
[DB]
Solan, Z., Ruppin, E., Horn, D. & Edelman, S. (2003) Unsupervised efficient
learning and representation of language structure. In: Proceedings of the 25th
Conference of the Cognitive Science Society, ed. R. Alterman & D. Kirsh.
Erlbaum.
[Available on CDROM.]
[SE]
Spelke, E. (2003) What makes us smart. In: Language in mind: Advances in the
study of language and thought, pp. 277–311, ed. D. Genter & S. Goldin-
Meadow. MIT Press.
[CM]
Spelke, E., Katz, G., Purcell, S., Ehrlich, S. & Breinlinger, K. (1994) Early
knowledge of object motion: Continuity and inertia. Cognition 51:131–76.
[aRJ]
Spivey, M. J. & Tanenhaus, M. K. (1998) Syntactic ambiguity resolution in
discourse: Modeling the effects of referential context and lexical frequency.
Journal of Experimental Psychology: Learning, Memory, and Cognition
24:1521–43.
[MJS]
Stamenov, M. I. & Gallese, V., eds. (2002) Mirror neurons and the evolution of
brain and language. John Benjamins.
[DK]
Starkstein, S. E., Federoff, J. P., Price, T. R., Leiguarda, R. C. & Robinson, R. G.
(1994) Neuropsychological and neuroradiologic correlates of emotional
prosody comprehension. Neurology 44:515–22.
[GG]
Steels, L. (1997) The synthetic modeling of language origins. Evolution of
Communication 1:1–35.
[WZ]
Stuss, D. T., Gallup, G. G. & Alexander, M. P. (2001) Frontal lobes are necessary
for “theory of mind.” Brain 124:279–86.
[BBV]
Swinney, D. (1979) Lexical access during sentence comprehension:
(Re)consideration of context effects. Journal of Verbal Learning and Verbal
Behavior 18:645–59.
[aRJ]
Tabor, W. & Tanenhaus, M. K. (1999) Dynamical models of sentence processing.
Cognitive Science 23:491–515.
[MJS]
Talmy, L. (1988) Force-dynamics in language and thought. Cognitive Science
12:49–100.
[rRJ]
(2000) Towards a cognitive semantics, vols. 1 and 2. MIT Press.
[aRJ, MJS]
Tanenhaus, M. K., Leiman, J. M. & Seidenberg, M. (1979) Evidence for multiple
stages in the processing of ambiguous words in syntactic contexts. Journal of
Verbal Learning and Verbal Behavior 18:427–40.
[aRJ]
Tannen, D. (1989) Talking voices: Repetition, dialogue, and imagery in
conversational discourse. Cambridge University Press.
[SG]
Terrace, H. (1979) Nim. Knopf.
[aRJ]
Tomasello, M. (1999a) The cultural origins of human cognition. Harvard University
Press.
[BBV]
(1999b) The item-based nature of children’s early syntactic development. Trends
in Cognitive Science 4(4):156–63.
[PFD]
ed. (2000) Primate cognition (special issue). Cognitive Science 24:3.
[aRJ]
(2003) Constructing a language: A usage-based theory of language acquisition.
Harvard University Press.
[PFD, DK]
Tomasello, M. & Merriman, W. E., eds. (1995) Beyond names for things: Young
children’s acquisition of verbs. Erlbaum.
[rRJ]
Townsend, D. J. & Bever, T. G. (2001) Sentence comprehension. MIT Press.
[SE]
Tranel, D., Kemmerer, D., Adolphs, R., Damasio, H. & Damasio, A. (2003) Neural
correlates of conceptual knowledge for actions. Cognitive Neuropsychology
20:409–32.
[DK]
Trueswell, J. C., Tanenhaus, M. K. & Garnsey, S. M. (1994) Semantic influences on
parsing: Use of thematic role information in syntactic ambiguity resolution.
Journal of Memory and Language 33:285–318.
[MJS]
Uttal, W. R. (2001) The new phrenology. MIT Press.
[MJS]
Van Orden, G. C., Holden, J. C. & Turvey, M. T. (2003) Self-organization of
cognitive performance. Journal of Experimental Psychology: General
132:331–50.
[MJS]
Van Valin, R. D. (2001) An introduction to syntax. Cambridge University Press.
[RJL]
Van Valin, R. D. & LaPolla, R. (1997) Syntax: Structure, meaning and function.
Cambridge University Press.
[aRJ]
Vaughan, W., Jr. (1988) Formation of equivalence sets in pigeons. Journal of
Experimental Psychology: Animal Behavior Processes 14:36–42.
[ACC]
Velichkovsky, B. M. (1996) Language development at the crossroad of biological
and cultural interactions. In: Communicating meaning: Evolution and
development of language, pp. 1–26, ed. B. M. Velichkovsky & D. M.
Rumbaugh. Erlbaum.
[BBV]
Verkuyl, H. (1993) A theory of aspectuality: The interaction between temporal and
atemporal structure. Cambridge University Press.
[aRJ]
Vignolo, L. A. (1990) Non-verbal conceptual impairment in aphasia. In: Handbook
of clinical neuropsychology, pp. 185–206, ed. F. Boller & J. Grafman.
Elsevier.
[GG]
Weinreich, U. (1966) Explorations in semantic theory. In: Current trends in
linguistics, vol. 3, ed. T. Sebeok. Mouton. (Reprinted in: U. Weinreich (1980)
On semantics. University of Pennsylvania Press.)
[aRJ]
Werth, P. (1999) Text worlds: Representing conceptual space in discourse. Pearson
Education.
[SE]
Wexler, K. & Culicover, P. (1980) Formal principles of language acquisition. MIT
Press.
[aRJ]
Wierzbicka, A. (1985) Oats and wheat: Mass nouns, iconicity and human
categorization. In: Iconicity in syntax, ed. J. Haiman. John Benjamins.
[MJS]
Wilkens, R. (1997) Spontanes versus reflektiertes Sprachverstehen: Deklarative
Grammatiktheorie in einem neuronalen Modell. Deutscher Universitatsverlag.
[HS]
Wilkens, R. & Schnelle, H. (1990) A connectionist parser for connectionist phrase
structure grammars. In: Konnektionism in artificial Intelligence and
Kognitionsforschung, ed. C. Dorffner. Springer.
[HS]
Wittgenstein, L. (1953) Philosophical investigations. Macmillan.
[ACC]
Wright, A. A., Cook, R. G., Rivera, J. J., Shyan, M. R., Neiworth, J. J. & Jitsumori,
M. (1990) Naming, rehearsal, and interstimulus interval effects in memory
processing. Journal of Experimental Psychology: Learning, Memory, and
Cognition 16:1043–59.
[ACC]
Zhang, N. (1998) The interactions between construction meaning and lexical
meaning. Linguistics 36:957–80.
[DK]
Zuidema, W. (2003) How the poverty of the stimulus solves the poverty of the
stimulus. In: Advances in neural information processing systems 15
(Proceedings of NIPS’02), pp. 51–58, ed. S. Becker, S. Thrun & K.
Obermayer. MIT Press.
[WZ]
Zurif, E. (1990) Language and the brain. In: An introduction to cognitive science,
vol. 1: Language, ed. D. Osherson & H. Lasnik. MIT Press.
[aRJ]
References/Jackendoff: Précis of Foundations of Language: Brain, Meaning, Grammar, Evolution
BEHAVIORAL AND BRAIN SCIENCES (2003) 26:6
707
https://www.cambridge.org/core/terms. https://doi.org/10.1017/S0140525X03280158
Downloaded from https://www.cambridge.org/core. Western Sydney University Library, on 05 Nov 2019 at 12:55:17, subject to the Cambridge Core terms of use, available at

